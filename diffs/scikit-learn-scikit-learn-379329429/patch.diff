diff --git a/.circleci/config.yml b/.circleci/config.yml
index 52fcd6712942..7ad3e5427b27 100644
--- a/.circleci/config.yml
+++ b/.circleci/config.yml
@@ -47,6 +47,7 @@ jobs:
       - SCIPY_VERSION: 0.14
       - MATPLOTLIB_VERSION: 1.3
       - SCIKIT_IMAGE_VERSION: 0.9.3
+      - PANDAS_VERSION: 0.13.1
     steps:
       - checkout
       - run: ./build_tools/circle/checkout_merge_commit.sh
diff --git a/CONTRIBUTING.md b/CONTRIBUTING.md
index ece0ef947ef2..8ae29353a5cc 100644
--- a/CONTRIBUTING.md
+++ b/CONTRIBUTING.md
@@ -4,7 +4,7 @@ Contributing to scikit-learn
 
 **Note: This document is a 'getting started' summary for contributing code,
 documentation, testing, and filing issues.** Visit the [**Contributing
-page**](http://scikit-learn.org/stable/developers/contributing.html)
+page**](http://scikit-learn.org/dev/developers/contributing.html)
 for the full contributor's guide. Please read it carefully to help make
 the code review process go as smoothly as possible and maximize the
 likelihood of your contribution being merged.
@@ -168,7 +168,7 @@ following rules before submitting:
 
 -  If you are submitting an algorithm or feature request, please verify that
    the algorithm fulfills our
-   [new algorithm requirements](http://scikit-learn.org/stable/faq.html#what-are-the-inclusion-criteria-for-new-algorithms).
+   [new algorithm requirements](http://scikit-learn.org/dev/faq.html#what-are-the-inclusion-criteria-for-new-algorithms).
 
 -  Please ensure all code snippets and error messages are formatted in
    appropriate code blocks.
@@ -235,6 +235,6 @@ illustrating it.
 Further Information
 -------------------
 
-Visit the [Contributing Code](http://scikit-learn.org/stable/developers/index.html#coding-guidelines)
+Visit the [Contributing Code](http://scikit-learn.org/stable/developers/contributing.html#coding-guidelines)
 section of the website for more information including conforming to the
 API spec and profiling contributed code.
diff --git a/README.rst b/README.rst
index 90fa97beec64..cf011277f539 100644
--- a/README.rst
+++ b/README.rst
@@ -54,7 +54,7 @@ scikit-learn requires:
 - SciPy (>= 0.13.3)
 
 For running the examples Matplotlib >= 1.3.1 is required. A few examples
-require scikit-image >= 0.9.3 as well.
+require scikit-image >= 0.9.3 and a few examples require pandas >= 0.13.1.
 
 scikit-learn also uses CBLAS, the C interface to the Basic Linear Algebra
 Subprograms library. scikit-learn comes with a reference implementation, but
diff --git a/benchmarks/bench_plot_lasso_path.py b/benchmarks/bench_plot_lasso_path.py
index 60a76017118c..7c698fdc32b9 100644
--- a/benchmarks/bench_plot_lasso_path.py
+++ b/benchmarks/bench_plot_lasso_path.py
@@ -32,7 +32,7 @@ def compute_bench(samples_range, features_range):
             dataset_kwargs = {
                 'n_samples': n_samples,
                 'n_features': n_features,
-                'n_informative': n_features / 10,
+                'n_informative': n_features // 10,
                 'effective_rank': min(n_samples, n_features) / 10,
                 #'effective_rank': None,
                 'bias': 0.0,
diff --git a/benchmarks/bench_plot_randomized_svd.py b/benchmarks/bench_plot_randomized_svd.py
index 96a0e91fa440..ae4b7e64bd3b 100644
--- a/benchmarks/bench_plot_randomized_svd.py
+++ b/benchmarks/bench_plot_randomized_svd.py
@@ -31,7 +31,7 @@
     goal: study whether the rank of the matrix and the number of components
     extracted by randomized SVD affect "the optimal" number of power iterations
 
-(c) plot: time vs norm, varing datasets
+(c) plot: time vs norm, varying datasets
     data: many datasets
     goal: compare default configurations
 
diff --git a/build_tools/circle/build_doc.sh b/build_tools/circle/build_doc.sh
index 014672263256..86231f8de40c 100755
--- a/build_tools/circle/build_doc.sh
+++ b/build_tools/circle/build_doc.sh
@@ -118,12 +118,11 @@ conda update --yes --quiet conda
 conda create -n $CONDA_ENV_NAME --yes --quiet python="${PYTHON_VERSION:-*}" \
   numpy="${NUMPY_VERSION:-*}" scipy="${SCIPY_VERSION:-*}" cython \
   pytest coverage matplotlib="${MATPLOTLIB_VERSION:-*}" sphinx=1.6.2 pillow \
-  scikit-image="${SCIKIT_IMAGE_VERSION:-*}"
+  scikit-image="${SCIKIT_IMAGE_VERSION:-*}" pandas="${PANDAS_VERSION:-*}"
 
 source activate testenv
 pip install sphinx-gallery
-# Use numpydoc master (for now)
-pip install git+https://github.com/numpy/numpydoc
+pip install numpydoc==0.8
 
 # Build and install scikit-learn in dev mode
 python setup.py develop
diff --git a/build_tools/travis/install.sh b/build_tools/travis/install.sh
index 9a5b65ce225b..443bfb9d812a 100755
--- a/build_tools/travis/install.sh
+++ b/build_tools/travis/install.sh
@@ -62,6 +62,11 @@ if [[ "$DISTRIB" == "conda" ]]; then
     conda create -n testenv --yes $TO_INSTALL
     source activate testenv
 
+    # for python 3.4, conda does not have recent pytest packages
+    if [[ "$PYTHON_VERSION" == "3.4" ]]; then
+        pip install pytest==3.5
+    fi
+
 elif [[ "$DISTRIB" == "ubuntu" ]]; then
     # At the time of writing numpy 1.9.1 is included in the travis
     # virtualenv but we want to use the numpy installed through apt-get
diff --git a/doc/conf.py b/doc/conf.py
index f09759abe84d..b72e9b4bde79 100644
--- a/doc/conf.py
+++ b/doc/conf.py
@@ -44,13 +44,15 @@
 numpydoc_class_members_toctree = False
 
 
-# pngmath / imgmath compatibility layer for different sphinx versions
-import sphinx
-from distutils.version import LooseVersion
-if LooseVersion(sphinx.__version__) < LooseVersion('1.4'):
-    extensions.append('sphinx.ext.pngmath')
-else:
+# For maths, use mathjax by default and svg if NO_MATHJAX env variable is set
+# (useful for viewing the doc offline)
+if os.environ.get('NO_MATHJAX'):
     extensions.append('sphinx.ext.imgmath')
+    imgmath_image_format = 'svg'
+else:
+    extensions.append('sphinx.ext.mathjax')
+    mathjax_path = ('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/'
+                    'MathJax.js?config=TeX-AMS_SVG')
 
 
 autodoc_default_flags = ['members', 'inherited-members']
diff --git a/doc/developers/contributing.rst b/doc/developers/contributing.rst
index 7117d9ade434..0d6174f3b2d4 100644
--- a/doc/developers/contributing.rst
+++ b/doc/developers/contributing.rst
@@ -34,15 +34,6 @@ See :ref:`new_contributors` to get started.
 
 |
 
-Submitting a bug report
-=======================
-
-In case you experience issues using this package, do not hesitate to submit a
-ticket to the
-`Bug Tracker <https://github.com/scikit-learn/scikit-learn/issues>`_. You are
-also welcome to post feature requests or pull requests.
-
-
 Ways to contribute
 ==================
 
@@ -59,15 +50,74 @@ investigating bugs, and :ref:`reviewing other developers' pull requests
 <code_review>` are very valuable contributions that decrease the burden on the
 project maintainers.
 
-Another way to contribute is to report issues you're facing, and give a "thumbs up"
-on issues that others reported and that are relevant to you.
-It also helps us if you spread the word: reference the project from your blog
-and articles, link to it from your website, or simply say "I use it":
+Another way to contribute is to report issues you're facing, and give a "thumbs
+up" on issues that others reported and that are relevant to you.  It also helps
+us if you spread the word: reference the project from your blog and articles,
+link to it from your website, or simply say "I use it":
 
 .. raw:: html
 
    <script type="text/javascript" src="http://www.ohloh.net/p/480792/widgets/project_users.js?style=rainbow"></script>
 
+Submitting a bug report or a feature request
+============================================
+
+In case you experience issues using this package, do not hesitate to submit a
+ticket to the
+`Bug Tracker <https://github.com/scikit-learn/scikit-learn/issues>`_. You are
+also welcome to post feature requests or pull requests.
+
+It is recommended to check that your issue complies with the
+following rules before submitting:
+
+-  Verify that your issue is not being currently addressed by other
+   `issues <https://github.com/scikit-learn/scikit-learn/issues?q=>`_
+   or `pull requests <https://github.com/scikit-learn/scikit-learn/pulls?q=>`_.
+
+-  If you are submitting an algorithm or feature request, please verify that
+   the algorithm fulfills our
+   `new algorithm requirements
+   <http://scikit-learn.org/stable/faq.html#what-are-the-inclusion-criteria-for-new-algorithms>`_.
+
+-  If you are submitting a bug report, we strongly encourage you to follow the guidelines in 
+   :ref:`filing_bugs`.
+
+.. _filing_bugs:
+
+How to make a good bug report
+-----------------------------
+
+When you submit an issue to `Github
+<https://github.com/scikit-learn/scikit-learn/issues>`_, please do your best to
+follow these guidelines! This will make it a lot easier to provide you with good
+feedback:
+
+- The ideal bug report contains a **short reproducible code snippet**, this way
+  anyone can try to reproduce the bug easily (see `this
+  <http://stackoverflow.com/help/mcve>`_ for more details). If your snippet is
+  longer than around 50 lines, please link to a `gist
+  <https://gist.github.com>`_ or a github repo.
+
+- If not feasible to include a reproducible snippet, please be specific about
+  what **estimators and/or functions are involved and the shape of the data**.
+
+- If an exception is raised, please **provide the full traceback**.
+
+- Please include your **operating system type and version number**, as well as
+  your **Python, scikit-learn, numpy, and scipy versions**. This information
+  can be found by running the following code snippet::
+
+     import platform; print(platform.platform())
+     import sys; print("Python", sys.version)
+     import numpy; print("NumPy", numpy.__version__)
+     import scipy; print("SciPy", scipy.__version__)
+     import sklearn; print("Scikit-Learn", sklearn.__version__)
+
+- Please ensure all **code snippets and error messages are formatted in
+  appropriate code blocks**.  See `Creating and highlighting code blocks
+  <https://help.github.com/articles/creating-and-highlighting-code-blocks>`_
+  for more details.
+
 
 .. _git_repo:
 
@@ -331,47 +381,6 @@ and Cython optimizations.
      [doc build]            Docs built including example gallery plots
      ====================== ===================
 
-.. _filing_bugs:
-
-Filing Bugs
------------
-
-We use GitHub issues to track all bugs and feature requests; feel free to
-open an issue if you have found a bug or wish to see a feature implemented.
-
-It is recommended to check that your issue complies with the
-following rules before submitting:
-
--  Verify that your issue is not being currently addressed by other
-   `issues <https://github.com/scikit-learn/scikit-learn/issues?q=>`_
-   or `pull requests <https://github.com/scikit-learn/scikit-learn/pulls?q=>`_.
-
--  If you are submitting an algorithm or feature request, please verify that
-   the algorithm fulfills our
-   `new algorithm requirements
-   <http://scikit-learn.org/stable/faq.html#what-are-the-inclusion-criteria-for-new-algorithms>`_.
-
--  Please ensure all code snippets and error messages are formatted in
-   appropriate code blocks.
-   See `Creating and highlighting code blocks
-   <https://help.github.com/articles/creating-and-highlighting-code-blocks>`_.
-
--  Please include your operating system type and version number, as well
-   as your Python, scikit-learn, numpy, and scipy versions. This information
-   can be found by running the following code snippet::
-
-     import platform; print(platform.platform())
-     import sys; print("Python", sys.version)
-     import numpy; print("NumPy", numpy.__version__)
-     import scipy; print("SciPy", scipy.__version__)
-     import sklearn; print("Scikit-Learn", sklearn.__version__)
-
--  Please be specific about what estimators and/or functions are involved
-   and the shape of the data, as appropriate; please include a
-   `reproducible <http://stackoverflow.com/help/mcve>`_ code snippet
-   or link to a `gist <https://gist.github.com>`_. If an exception is raised,
-   please provide the traceback.
-
 .. _new_contributors:
 
 Issues for New Contributors
@@ -451,7 +460,9 @@ while. To save some time, you can use:
       matching ``your_regex_goes_here`` will be run. This is particularly
       useful if you are modifying a few examples.
 
-That should create all the documentation in the ``_build/html/stable`` directory.
+That should create all the documentation in the ``_build/html/stable``
+directory.  Set the environment variable `NO_MATHJAX=1` if you intend to view
+the documentation in an offline setting.
 
 To build the PDF manual, run::
 
diff --git a/doc/developers/maintainer.rst b/doc/developers/maintainer.rst
index 9efe99786252..d0d0db8a041b 100644
--- a/doc/developers/maintainer.rst
+++ b/doc/developers/maintainer.rst
@@ -84,5 +84,5 @@ The definition of what gets run in the Cron job is done in the .travis.yml
 config file, exactly the same way as the other Travis jobs. We use a ``if: type
 = cron`` filter in order for the build to be run only in Cron jobs.
 
-The branch targetted by the Cron job and the frequency of the Cron job is set
+The branch targeted by the Cron job and the frequency of the Cron job is set
 via the web UI at https://www.travis-ci.org/scikit-learn/scikit-learn/settings.
diff --git a/doc/developers/tips.rst b/doc/developers/tips.rst
index 0c23bb74c8ed..7e97bcfb2a2c 100644
--- a/doc/developers/tips.rst
+++ b/doc/developers/tips.rst
@@ -64,8 +64,22 @@ will be displayed as a color background behind the line number.
 Useful pytest aliases and flags
 -------------------------------
 
-We recommend using pytest to run unit tests. When a unit tests fail, the
-following tricks can make debugging easier:
+The full test suite takes fairly long to run. For faster iterations,
+it is possibly to select a subset of tests using pytest selectors.
+In particular, one can run a `single test based on its node ID
+<https://docs.pytest.org/en/latest/example/markers.html#selecting-tests-based-on-their-node-id>`_::
+
+  pytest -v sklearn/linear_model/tests/test_logistic.py::test_sparsify
+
+or use the `-k pytest parameter
+<https://docs.pytest.org/en/latest/example/markers.html#using-k-expr-to-select-tests-based-on-their-name>`_
+to select tests based on their name. For instance,::
+
+  pytest sklearn/tests/test_common.py -v -k LogisticRegression
+
+will run all :term:`common tests` for the ``LogisticRegression`` estimator.
+
+When a unit tests fail, the following tricks can make debugging easier:
 
   1. The command line argument ``pytest -l`` instructs pytest to print the local
      variables when a failure occurs.
diff --git a/doc/glossary.rst b/doc/glossary.rst
index 9105251f5980..a8f31c1b3fdf 100644
--- a/doc/glossary.rst
+++ b/doc/glossary.rst
@@ -990,7 +990,7 @@ Target Types
         :term:`outputs`, each one a finite floating point number, for a
         fixed int ``n_outputs > 1`` in a particular dataset.
 
-        Continous multioutput targets are represented as multiple
+        Continuous multioutput targets are represented as multiple
         :term:`continuous` targets, horizontally stacked into an array
         of shape ``(n_samples, n_outputs)``.
 
diff --git a/doc/index.rst b/doc/index.rst
index 77b70b9d52f0..cfcaedc38017 100644
--- a/doc/index.rst
+++ b/doc/index.rst
@@ -22,7 +22,7 @@
         <strong>Applications</strong>: Spam detection, Image recognition.</br>
         <strong>Algorithms</strong>:&nbsp;
 
-    :ref:`SVM<svm>`, :ref:`nearest neighbors<classification>`, :ref:`random forest<forest>`, ...
+    :ref:`SVM<svm_classification>`, :ref:`nearest neighbors<classification>`, :ref:`random forest<forest>`, ...
 
     .. raw:: html
 
@@ -52,7 +52,7 @@
         <strong>Applications</strong>: Drug response, Stock prices.</br>
         <strong>Algorithms</strong>:&nbsp;
 
-    :ref:`SVR<svm>`, :ref:`ridge regression<ridge_regression>`, :ref:`Lasso<lasso>`, ...
+    :ref:`SVR<svm_regression>`, :ref:`ridge regression<ridge_regression>`, :ref:`Lasso<lasso>`, ...
 
     .. raw:: html
 
diff --git a/doc/modules/calibration.rst b/doc/modules/calibration.rst
index d7bb10479ce6..6abffac9b73d 100644
--- a/doc/modules/calibration.rst
+++ b/doc/modules/calibration.rst
@@ -105,7 +105,7 @@ in the middle, i.e., 0.5.
 .. currentmodule:: sklearn.metrics
 
 The following experiment is performed on an artificial dataset for binary
-classification with 100.000 samples (1.000 of them are used for model fitting)
+classification with 100,000 samples (1,000 of them are used for model fitting)
 with 20 features. Of the 20 features, only 2 are informative and 10 are
 redundant. The figure shows the estimated probabilities obtained with
 logistic regression, a linear support-vector classifier (SVC), and linear SVC with
diff --git a/doc/modules/classes.rst b/doc/modules/classes.rst
index 2e7dcba82e84..68494051041b 100644
--- a/doc/modules/classes.rst
+++ b/doc/modules/classes.rst
@@ -646,6 +646,7 @@ Kernels:
    :template: class.rst
 
    impute.SimpleImputer
+   impute.MICEImputer
 
 .. _kernel_approximation_ref:
 
@@ -745,7 +746,6 @@ Kernels:
    linear_model.enet_path
    linear_model.lars_path
    linear_model.lasso_path
-   linear_model.lasso_stability_path
    linear_model.logistic_regression_path
    linear_model.orthogonal_mp
    linear_model.orthogonal_mp_gram
@@ -1499,6 +1499,7 @@ To be removed in 0.21
    :template: deprecated_function.rst
 
    datasets.load_mlcomp
+   linear_model.lasso_stability_path
 
 
 To be removed in 0.20
diff --git a/doc/modules/clustering.rst b/doc/modules/clustering.rst
index f8b36f746ebc..9318b243e121 100644
--- a/doc/modules/clustering.rst
+++ b/doc/modules/clustering.rst
@@ -1502,7 +1502,7 @@ cluster analysis.
  * Peter J. Rousseeuw (1987). "Silhouettes: a Graphical Aid to the
    Interpretation and Validation of Cluster Analysis". Computational
    and Applied Mathematics 20: 53–65.
-   `doi:10.1016/0377-0427(87)90125-7 <http://dx.doi.org/10.1016/0377-0427(87)90125-7>`_.
+   `doi:10.1016/0377-0427(87)90125-7 <https://doi.org/10.1016/0377-0427(87)90125-7>`_.
 
 
 Advantages
@@ -1533,9 +1533,9 @@ Calinski-Harabaz Index
 ----------------------
 
 If the ground truth labels are not known, the Calinski-Harabaz index
-(:func:`sklearn.metrics.calinski_harabaz_score`) can be used to evaluate the
-model, where a higher Calinski-Harabaz score relates to a model with better
-defined clusters.
+(:func:`sklearn.metrics.calinski_harabaz_score`) - also known as the Variance 
+Ratio Criterion - can be used to evaluate the model, where a higher 
+Calinski-Harabaz score relates to a model with better defined clusters.
 
 For :math:`k` clusters, the Calinski-Harabaz score :math:`s` is given as the
 ratio of the between-clusters dispersion mean and the within-cluster
@@ -1595,7 +1595,7 @@ Drawbacks
 
  *  Caliński, T., & Harabasz, J. (1974). "A dendrite method for cluster
     analysis". Communications in Statistics-theory and Methods 3: 1-27.
-    `doi:10.1080/03610926.2011.560741 <http://dx.doi.org/10.1080/03610926.2011.560741>`_.
+    `doi:10.1080/03610926.2011.560741 <https://doi.org/10.1080/03610926.2011.560741>`_.
 
 .. _contingency_matrix:
 
diff --git a/doc/modules/covariance.rst b/doc/modules/covariance.rst
index 8325720fe565..1eb74b51cd16 100644
--- a/doc/modules/covariance.rst
+++ b/doc/modules/covariance.rst
@@ -7,14 +7,13 @@ Covariance estimation
 .. currentmodule:: sklearn.covariance
 
 
-Many statistical problems require at some point the estimation of a
+Many statistical problems require the estimation of a
 population's covariance matrix, which can be seen as an estimation of
 data set scatter plot shape. Most of the time, such an estimation has
 to be done on a sample whose properties (size, structure, homogeneity)
-has a large influence on the estimation's quality. The
-`sklearn.covariance` package aims at providing tools affording
-an accurate estimation of a population's covariance matrix under
-various settings.
+have a large influence on the estimation's quality. The
+`sklearn.covariance` package provides tools for accurately estimating
+a population's covariance matrix under various settings.
 
 We assume that the observations are independent and identically
 distributed (i.i.d.).
@@ -24,22 +23,22 @@ Empirical covariance
 ====================
 
 The covariance matrix of a data set is known to be well approximated
-with the classical *maximum likelihood estimator* (or "empirical
+by the classical *maximum likelihood estimator* (or "empirical
 covariance"), provided the number of observations is large enough
 compared to the number of features (the variables describing the
 observations). More precisely, the Maximum Likelihood Estimator of a
-sample is an unbiased estimator of the corresponding population
+sample is an unbiased estimator of the corresponding population's
 covariance matrix.
 
 The empirical covariance matrix of a sample can be computed using the
 :func:`empirical_covariance` function of the package, or by fitting an
 :class:`EmpiricalCovariance` object to the data sample with the
-:meth:`EmpiricalCovariance.fit` method.  Be careful that depending
-whether the data are centered or not, the result will be different, so
-one may want to use the ``assume_centered`` parameter accurately. More precisely
-if one uses ``assume_centered=False``, then the test set is supposed to have the
-same mean vector as the training set. If not so, both should be centered by the
-user, and ``assume_centered=True`` should be used.
+:meth:`EmpiricalCovariance.fit` method. Be careful that results depend
+on whether the data are centered, so one may want to use the
+``assume_centered`` parameter accurately. More precisely, if
+``assume_centered=False``, then the test set is supposed to have the
+same mean vector as the training set. If not, both should be centered
+by the user, and ``assume_centered=True`` should be used.
 
 .. topic:: Examples:
 
@@ -64,17 +63,17 @@ empirical covariance matrix cannot be inverted for numerical
 reasons. To avoid such an inversion problem, a transformation of the
 empirical covariance matrix has been introduced: the ``shrinkage``.
 
-In the scikit-learn, this transformation (with a user-defined shrinkage
+In scikit-learn, this transformation (with a user-defined shrinkage
 coefficient) can be directly applied to a pre-computed covariance with
 the :func:`shrunk_covariance` method. Also, a shrunk estimator of the
 covariance can be fitted to data with a :class:`ShrunkCovariance` object
-and its :meth:`ShrunkCovariance.fit` method.  Again, depending whether
-the data are centered or not, the result will be different, so one may
-want to use the ``assume_centered`` parameter accurately.
+and its :meth:`ShrunkCovariance.fit` method. Again, results depend on
+whether the data are centered, so one may want to use the
+``assume_centered`` parameter accurately.
 
 
 Mathematically, this shrinkage consists in reducing the ratio between the
-smallest and the largest eigenvalue of the empirical covariance matrix.
+smallest and the largest eigenvalues of the empirical covariance matrix.
 It can be done by simply shifting every eigenvalue according to a given
 offset, which is equivalent of finding the l2-penalized Maximum
 Likelihood Estimator of the covariance matrix. In practice, shrinkage
@@ -95,7 +94,7 @@ bias/variance trade-off, and is discussed below.
 Ledoit-Wolf shrinkage
 ---------------------
 
-In their 2004 paper [1]_, O. Ledoit and M. Wolf propose a formula so as
+In their 2004 paper [1]_, O. Ledoit and M. Wolf propose a formula
 to compute the optimal shrinkage coefficient :math:`\alpha` that
 minimizes the Mean Squared Error between the estimated and the real
 covariance matrix.
@@ -190,10 +189,10 @@ The matrix inverse of the covariance matrix, often called the precision
 matrix, is proportional to the partial correlation matrix. It gives the
 partial independence relationship. In other words, if two features are
 independent conditionally on the others, the corresponding coefficient in
-the precision matrix will be zero. This is why it makes sense to estimate
-a sparse precision matrix: by learning independence relations from the
-data, the estimation of the covariance matrix is better conditioned. This
-is known as *covariance selection*.
+the precision matrix will be zero. This is why it makes sense to
+estimate a sparse precision matrix: the estimation of the covariance
+matrix is better conditioned by learning independence relations from
+the data. This is known as *covariance selection*.
 
 In the small-samples situation, in which ``n_samples`` is on the order
 of ``n_features`` or smaller, sparse inverse covariance estimators tend to work
@@ -273,13 +272,13 @@ paper. It is the same algorithm as in the R ``glasso`` package.
 Robust Covariance Estimation
 ============================
 
-Real data set are often subjects to measurement or recording
+Real data sets are often subject to measurement or recording
 errors. Regular but uncommon observations may also appear for a variety
-of reason. Every observation which is very uncommon is called an
-outlier.
+of reasons. Observations which are very uncommon are called
+outliers.
 The empirical covariance estimator and the shrunk covariance
 estimators presented above are very sensitive to the presence of
-outlying observations in the data. Therefore, one should use robust
+outliers in the data. Therefore, one should use robust
 covariance estimators to estimate the covariance of its real data
 sets. Alternatively, robust covariance estimators can be used to
 perform outlier detection and discard/downweight some observations
diff --git a/doc/modules/cross_validation.rst b/doc/modules/cross_validation.rst
index e889515da092..f64f533238bc 100644
--- a/doc/modules/cross_validation.rst
+++ b/doc/modules/cross_validation.rst
@@ -87,7 +87,7 @@ is then the average of the values computed in the loop.
 This approach can be computationally expensive,
 but does not waste too much data
 (as it is the case when fixing an arbitrary test set),
-which is a major advantage in problem such as inverse inference
+which is a major advantage in problems such as inverse inference
 where the number of samples is very small.
 
 
diff --git a/doc/modules/decomposition.rst b/doc/modules/decomposition.rst
index 4243c4799606..d897377d1626 100644
--- a/doc/modules/decomposition.rst
+++ b/doc/modules/decomposition.rst
@@ -817,7 +817,7 @@ stored components::
     .. [5] `"Fast local algorithms for large scale nonnegative matrix and tensor
       factorizations."
       <http://www.bsp.brain.riken.jp/publications/2009/Cichocki-Phan-IEICE_col.pdf>`_
-      A. Cichocki, P. Anh-Huy, 2009
+      A. Cichocki, A. Phan, 2009
 
     .. [6] `"Algorithms for nonnegative matrix factorization with the beta-divergence"
       <https://arxiv.org/pdf/1010.1763.pdf>`_
@@ -848,7 +848,7 @@ a corpus with :math:`D` documents and :math:`K` topics:
   3. For each word :math:`i` in document :math:`d`:
 
     a. Draw a topic index :math:`z_{di} \sim \mathrm{Multinomial}(\theta_d)`
-    b. Draw the observed word :math:`w_{ij} \sim \mathrm{Multinomial}(beta_{z_{di}}.)`
+    b. Draw the observed word :math:`w_{ij} \sim \mathrm{Multinomial}(\beta_{z_{di}})`
 
 For parameter estimation, the posterior distribution is:
 
diff --git a/doc/modules/impute.rst b/doc/modules/impute.rst
index 75aa7e27f48c..f16182510597 100644
--- a/doc/modules/impute.rst
+++ b/doc/modules/impute.rst
@@ -15,6 +15,10 @@ values. However, this comes at the price of losing data which may be valuable
 (even though incomplete). A better strategy is to impute the missing values,
 i.e., to infer them from the known part of the data.
 
+
+Univariate feature imputation
+=============================
+
 The :class:`SimpleImputer` class provides basic strategies for imputing missing
 values, either using the mean, the median or the most frequent value of
 the row or column in which the missing values are located. This class
@@ -26,9 +30,9 @@ that contain the missing values::
 
     >>> import numpy as np
     >>> from sklearn.impute import SimpleImputer
-    >>> imp = SimpleImputer(missing_values='NaN', strategy='mean', axis=0)
+    >>> imp = SimpleImputer(missing_values='NaN', strategy='mean')
     >>> imp.fit([[1, 2], [np.nan, 3], [7, 6]])       # doctest: +NORMALIZE_WHITESPACE
-    SimpleImputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)
+    SimpleImputer(copy=True, missing_values='NaN', strategy='mean', verbose=0)
     >>> X = [[np.nan, 2], [6, np.nan], [7, 6]]
     >>> print(imp.transform(X))           # doctest: +NORMALIZE_WHITESPACE  +ELLIPSIS
     [[4.          2.        ]
@@ -39,9 +43,9 @@ The :class:`SimpleImputer` class also supports sparse matrices::
 
     >>> import scipy.sparse as sp
     >>> X = sp.csc_matrix([[1, 2], [0, 3], [7, 6]])
-    >>> imp = SimpleImputer(missing_values=0, strategy='mean', axis=0)
+    >>> imp = SimpleImputer(missing_values=0, strategy='mean')
     >>> imp.fit(X)                  # doctest: +NORMALIZE_WHITESPACE
-    SimpleImputer(axis=0, copy=True, missing_values=0, strategy='mean', verbose=0)
+    SimpleImputer(copy=True, missing_values=0, strategy='mean', verbose=0)
     >>> X_test = sp.csc_matrix([[0, 2], [6, 0], [7, 6]])
     >>> print(imp.transform(X_test))      # doctest: +NORMALIZE_WHITESPACE  +ELLIPSIS
     [[4.          2.        ]
@@ -52,5 +56,35 @@ Note that, here, missing values are encoded by 0 and are thus implicitly stored
 in the matrix. This format is thus suitable when there are many more missing
 values than observed values.
 
-:class:`SimpleImputer` can be used in a Pipeline as a way to build a composite
-estimator that supports imputation. See :ref:`sphx_glr_auto_examples_plot_missing_values.py`.
+.. _mice:
+
+Multivariate feature imputation
+===============================
+
+A more sophisticated approach is to use the :class:`MICEImputer` class, which
+implements the Multivariate Imputation by Chained Equations technique. MICE
+models each feature with missing values as a function of other features, and
+uses that estimate for imputation. It does so in a round-robin fashion: at
+each step, a feature column is designated as output `y` and the other feature
+columns are treated as inputs `X`. A regressor is fit on `(X, y)` for known `y`.
+Then, the regressor is used to predict the unknown values of `y`. This is
+repeated for each feature, and then is done for a number of imputation rounds.
+Here is an example snippet::
+
+    >>> import numpy as np
+    >>> from sklearn.impute import MICEImputer
+    >>> imp = MICEImputer(n_imputations=10, random_state=0)
+    >>> imp.fit([[1, 2], [np.nan, 3], [7, np.nan]])
+    MICEImputer(imputation_order='ascending', initial_strategy='mean',
+          max_value=None, min_value=None, missing_values='NaN', n_burn_in=10,
+          n_imputations=10, n_nearest_features=None, predictor=None,
+          random_state=0, verbose=False)
+    >>> X_test = [[np.nan, 2], [6, np.nan], [np.nan, 6]]
+    >>> print(np.round(imp.transform(X_test)))
+    [[ 1.  2.]
+     [ 6.  4.]
+     [13.  6.]]
+
+Both :class:`SimpleImputer` and :class:`MICEImputer` can be used in a Pipeline
+as a way to build a composite estimator that supports imputation.
+See :ref:`sphx_glr_auto_examples_plot_missing_values.py`.
diff --git a/doc/modules/lda_qda.rst b/doc/modules/lda_qda.rst
index ad9a9c2be29b..3d45dd78f317 100644
--- a/doc/modules/lda_qda.rst
+++ b/doc/modules/lda_qda.rst
@@ -42,7 +42,7 @@ perform supervised dimensionality reduction, by projecting the input data to a
 linear subspace consisting of the directions which maximize the separation
 between classes (in a precise sense discussed in the mathematics section
 below). The dimension of the output is necessarily less than the number of
-classes, so this is a in general a rather strong dimensionality reduction, and
+classes, so this is, in general, a rather strong dimensionality reduction, and
 only makes senses in a multiclass setting.
 
 This is implemented in
@@ -73,7 +73,9 @@ More specifically, for linear and quadratic discriminant analysis,
 :math:`P(X|y)` is modelled as a multivariate Gaussian distribution with
 density:
 
-.. math:: p(X | y=k) = \frac{1}{(2\pi)^n |\Sigma_k|^{1/2}}\exp\left(-\frac{1}{2} (X-\mu_k)^t \Sigma_k^{-1} (X-\mu_k)\right)
+.. math:: P(X | y=k) = \frac{1}{(2\pi)^{d/2} |\Sigma_k|^{1/2}}\exp\left(-\frac{1}{2} (X-\mu_k)^t \Sigma_k^{-1} (X-\mu_k)\right) 
+
+where :math:`d` is the number of features.
 
 To use this model as a classifier, we just need to estimate from the training
 data the class priors :math:`P(y=k)` (by the proportion of instances of class
diff --git a/doc/modules/linear_model.rst b/doc/modules/linear_model.rst
index 4edb19a83372..83554c4363a8 100644
--- a/doc/modules/linear_model.rst
+++ b/doc/modules/linear_model.rst
@@ -837,8 +837,8 @@ while with ``loss="hinge"`` it fits a linear support vector machine (SVM).
 Perceptron
 ==========
 
-The :class:`Perceptron` is another simple algorithm suitable for large scale
-learning. By default:
+The :class:`Perceptron` is another simple classification algorithm suitable for
+large scale learning. By default:
 
     - It does not require a learning rate.
 
diff --git a/doc/modules/mixture.rst b/doc/modules/mixture.rst
index 69b5471103c2..fb8e897270f0 100644
--- a/doc/modules/mixture.rst
+++ b/doc/modules/mixture.rst
@@ -218,7 +218,7 @@ fit only one component.
 
 On the following figure we are fitting a dataset not well-depicted by a
 Gaussian mixture. Adjusting the ``weight_concentration_prior``, parameter of the
-class:`BayesianGaussianMixture` controls the number of components used to fit
+:class:`BayesianGaussianMixture` controls the number of components used to fit
 this data. We also present on the last two plots a random sampling generated
 from the two resulting mixtures.
 
diff --git a/doc/themes/scikit-learn/static/nature.css_t b/doc/themes/scikit-learn/static/nature.css_t
index e696696f4a2c..6712ec71950b 100644
--- a/doc/themes/scikit-learn/static/nature.css_t
+++ b/doc/themes/scikit-learn/static/nature.css_t
@@ -786,7 +786,7 @@ div.warning-wrapper p {
 }
 
 
-/*-----------------------The Code Sprint Sponser Banner---------------------*/
+/*-----------------------The Code Sprint Sponsor Banner---------------------*/
 
 div.sprint-wrapper {
     font-weight: bold;
diff --git a/doc/tutorial/machine_learning_map/ML_MAPS_README.txt b/doc/tutorial/machine_learning_map/ML_MAPS_README.txt
index 679419bb96c3..069cc6be4de2 100644
--- a/doc/tutorial/machine_learning_map/ML_MAPS_README.txt
+++ b/doc/tutorial/machine_learning_map/ML_MAPS_README.txt
@@ -19,7 +19,7 @@ so I'll try to make it as simple as possible.
 
 Use a Graphics editor like Inkscape Vector Graphics Editor
 to open the ml_map.svg file, in this folder. From there
-you can move objects around, ect. as you need.
+you can move objects around, etc. as you need.
 
 Save when done, and make sure to export a .PNG file
 to replace the old-outdated ml_map.png, as that file
diff --git a/doc/tutorial/machine_learning_map/pyparsing.py b/doc/tutorial/machine_learning_map/pyparsing.py
index 9905a890bd81..ba9833d6d613 100644
--- a/doc/tutorial/machine_learning_map/pyparsing.py
+++ b/doc/tutorial/machine_learning_map/pyparsing.py
@@ -1302,7 +1302,7 @@ def pa(s,l,t):
 
     def setFailAction( self, fn ):
         """Define action to perform if parsing fails at this expression.
-           Fail acton fn is a callable function that takes the arguments
+           Fail action fn is a callable function that takes the arguments
            C{fn(s,loc,expr,err)} where:
             - s = string being parsed
             - loc = location where expression match was attempted and failed
@@ -1604,7 +1604,7 @@ def parseString( self, instring, parseAll=False ):
            (see L{I{parseWithTabs}<parseWithTabs>})
          - define your parse action using the full C{(s,loc,toks)} signature, and
            reference the input string using the parse action's C{s} argument
-         - explictly expand the tabs in your input string before calling
+         - explicitly expand the tabs in your input string before calling
            C{parseString}
         
         Example::
diff --git a/doc/whats_new/v0.19.rst b/doc/whats_new/v0.19.rst
index a2ddc9367cd9..c28199716528 100644
--- a/doc/whats_new/v0.19.rst
+++ b/doc/whats_new/v0.19.rst
@@ -807,8 +807,8 @@ Trees and ensembles
 - All tree based estimators now accept a ``min_impurity_decrease``
   parameter in lieu of the ``min_impurity_split``, which is now deprecated.
   The ``min_impurity_decrease`` helps stop splitting the nodes in which
-  the weighted impurity decrease from splitting is no longer alteast
-  ``min_impurity_decrease``.  :issue:`8449` by `Raghav RV`_.
+  the weighted impurity decrease from splitting is no longer at least
+  ``min_impurity_decrease``. :issue:`8449` by `Raghav RV`_.
 
 Linear, kernelized and related models
 
diff --git a/doc/whats_new/v0.20.rst b/doc/whats_new/v0.20.rst
index e71a8af0d78d..13de05ada836 100644
--- a/doc/whats_new/v0.20.rst
+++ b/doc/whats_new/v0.20.rst
@@ -53,7 +53,7 @@ Classifiers and regressors
   via ``n_iter_no_change``, ``validation_fraction`` and ``tol``. :issue:`7071`
   by `Raghav RV`_
 
-- :class:`dummy.DummyRegressor` now has a ``return_std`` option in its 
+- :class:`dummy.DummyRegressor` now has a ``return_std`` option in its
   ``predict`` method. The returned standard deviations will be zeros.
 
 - Added :class:`naive_bayes.ComplementNB`, which implements the Complement
@@ -81,11 +81,22 @@ Preprocessing
   :issue:`10210` by :user:`Eric Chang <ericchang00>` and
   :user:`Maniteja Nandana <maniteja123>`.
 
+- :class:`preprocessing.QuantileTransformer` handles and ignores NaN values.
+  :issue:`10404` by :user:`Guillaume Lemaitre <glemaitre>`.
+
 - Added the :class:`compose.TransformedTargetRegressor` which transforms
   the target y before fitting a regression model. The predictions are mapped
   back to the original space via an inverse transform. :issue:`9041` by
   `Andreas Müller`_ and :user:`Guillaume Lemaitre <glemaitre>`.
 
+- Added :class:`MICEImputer`, which is a strategy for imputing missing
+  values by modeling each feature with missing values as a function of
+  other features in a round-robin fashion. :issue:`8478` by
+  :user:`Sergey Feldman <sergeyf>`.
+
+- Updated :class:`preprocessing.MinMaxScaler` to pass through NaN values. :issue:`10404`
+  by :user:`Lucija Gregov <LucijaGregov>`.
+
 Model evaluation
 
 - Added the :func:`metrics.balanced_accuracy_score` metric and a corresponding
@@ -142,6 +153,18 @@ Classifiers and regressors
   only require X to be an object with finite length or shape.
   :issue:`9832` by :user:`Vrishank Bhardwaj <vrishank97>`.
 
+- :class:`neighbors.RadiusNeighborsRegressor` and
+  :class:`neighbors.RadiusNeighborsClassifier` are now
+  parallelized according to ``n_jobs`` regardless of ``algorithm``.
+  :issue:`8003` by :user:`Joël Billaud <recamshak>`.
+  
+- Memory usage improvement for :func:`_class_means` and :func:`_class_cov`
+  in :class:`discriminant_analysis`.
+  :issue:`10898` by :user:`Nanxin Chen <bobchennan>`.`
+
+- :func:`manifold.t_sne.trustworthiness` accepts metrics other than
+  Euclidean. :issue:`9775` by :user:`William de Vazelhes <wdevazelhes>`.
+
 Cluster
 
 - :class:`cluster.KMeans`, :class:`cluster.MiniBatchKMeans` and
@@ -149,6 +172,10 @@ Cluster
   row-major ordering, improving runtime.
   :issue:`10471` by :user:`Gaurav Dhingra <gxyd>`.
 
+- :class:`cluster.DBSCAN` now is parallelized according to ``n_jobs``
+  regardless of ``algorithm``.
+  :issue:`8003` by :user:`Joël Billaud <recamshak>`.
+
 Datasets
 
 - In :func:`datasets.make_blobs`, one can now pass a list to the `n_samples`
@@ -160,6 +187,15 @@ Preprocessing
 
 - :class:`preprocessing.PolynomialFeatures` now supports sparse input.
   :issue:`10452` by :user:`Aman Dalmia <dalmia>` and `Joel Nothman`_.
+  
+- Enable the call to :meth:`get_feature_names` in unfitted
+  :class:`feature_extraction.text.CountVectorizer` initialized with a
+  vocabulary. :issue:`10908` by :user:`Mohamed Maskani <maskani-moh>`.
+
+- The ``transform`` method of :class:`sklearn.preprocessing.MultiLabelBinarizer`
+  now ignores any unknown classes. A warning is raised stating the unknown classes
+  classes found which are ignored.
+  :issue:`10913` by :user:`Rodrigo Agundez <rragundez>`.
 
 Model evaluation and meta-estimators
 
@@ -195,12 +231,34 @@ Metrics
 - :func:`metrics.roc_auc_score` now supports binary ``y_true`` other than
   ``{0, 1}`` or ``{-1, 1}``. :issue:`9828` by :user:`Hanmin Qin <qinhanmin2014>`.
 
+- :func:`metrics.label_ranking_average_precision_score` now supports vector ``sample_weight``.
+  :issue:`10845` by :user:`Jose Perez-Parras Toledano <jopepato>`.
+
+- Add ``dense_output`` parameter to :func:`metrics.pairwise.linear_kernel`. When
+  False and both inputs are sparse, will return a sparse matrix.
+  :issue:`10999` by :user:`Taylor G Smith <tgsmith61591>`.
+
 Linear, kernelized and related models
 
 - Deprecate ``random_state`` parameter in :class:`svm.OneClassSVM` as the
   underlying implementation is not random.
   :issue:`9497` by :user:`Albert Thomas <albertcthomas>`.
 
+Decomposition, manifold learning and clustering
+
+- Deprecate ``precomputed`` parameter in function
+  :func:`manifold.t_sne.trustworthiness`. Instead, the new parameter
+  ``metric`` should be used with any compatible metric including
+  'precomputed', in which case the input matrix ``X`` should be a matrix of
+  pairwise distances or squared distances. :issue:`9775` by
+  :user:`William de Vazelhes <wdevazelhes>`.
+
+Utils
+
+- Avoid copying the data in :func:`utils.check_array` when the input data is a
+  memmap (and ``copy=False``). :issue:`10663` by :user:`Arthur Mensch
+  <arthurmensch>` and :user:`Loïc Estève <lesteve>`.
+
 Miscellaneous
 
 - Add ``filename`` attribute to datasets that have a CSV file.
@@ -259,9 +317,9 @@ Classifiers and regressors
 - Fixed a bug in :class:`linear_model.RidgeClassifierCV` where
   the parameter ``store_cv_values`` was not implemented though
   it was documented in ``cv_values`` as a way to set up the storage
-  of cross-validation values for different alphas. :issue:`10297` by 
+  of cross-validation values for different alphas. :issue:`10297` by
   :user:`Mabel Villalba-Jiménez <mabelvj>`.
-  
+
 - Fixed a bug in :class:`naive_bayes.MultinomialNB` which did not accept vector
   valued pseudocounts (alpha).
   :issue:`10346` by :user:`Tobias Madsen <TobiasMadsen>`
@@ -286,6 +344,10 @@ Classifiers and regressors
 - Fixed a bug in :class:`linear_model.RidgeCV` where using integer ``alphas``
   raised an error. :issue:`10393` by :user:`Mabel Villalba-Jiménez <mabelvj>`.
 
+- Fixed condition triggering gap computation in :class:`linear_model.Lasso`
+  and :class:`linear_model.ElasticNet` when working with sparse matrices.
+  :issue:`10992` by `Alexandre Gramfort`_.
+
 Decomposition, manifold learning and clustering
 
 - Fix for uninformative error in :class:`decomposition.IncrementalPCA`:
@@ -335,6 +397,11 @@ Decomposition, manifold learning and clustering
   by :user:`Jan Margeta <jmargeta>`, :user:`Guillaume Lemaitre <glemaitre>`,
   and :user:`Devansh D. <devanshdalal>`.
 
+- Fixed a bug in :class:`mixture.BaseMixture` where the reported `n_iter_` was
+  missing an iteration. It affected :class:`mixture.GaussianMixture` and
+  :class:`mixture.BayesianGaussianMixture`. :issue:`10740` by :user:`Erich
+  Schubert <kno10>` and :user:`Guillaume Lemaitre <glemaitre>`.
+
 Metrics
 
 - Fixed a bug in :func:`metrics.precision_precision_recall_fscore_support`
@@ -354,6 +421,12 @@ Metrics
   :func:`mutual_info_score`.
   :issue:`9772` by :user:`Kumar Ashutosh <thechargedneutron>`.
 
+- Fixed a bug in :func:`metrics.cluster.fowlkes_mallows_score` to avoid integer
+  overflow. Casted return value of `contingency_matrix` to `int64` and computed
+  product of square roots rather than square root of product.
+  :issue:`9515` by :user:`Alan Liddell <aliddell>` and
+  :user:`Manh Dao <manhdao>`.
+
 Neighbors
 
 - Fixed a bug so ``predict`` in :class:`neighbors.RadiusNeighborsRegressor` can
@@ -391,6 +464,12 @@ Preprocessing
   ``inverse_transform`` on unseen labels. :issue:`9816` by :user:`Charlie Newey
   <newey01c>`.
 
+Model evaluation and meta-estimators
+
+- Add improved error message in :func:`model_selection.cross_val_score` when
+  multiple metrics are passed in ``scoring`` keyword.
+  :issue:`11006` by :user:`Ming Li <minggli>`.
+
 Datasets
 
 - Fixed a bug in :func:`dataset.load_boston` which had a wrong data point.
@@ -422,6 +501,19 @@ Linear, kernelized and related models
   better for unscaled features. :issue:`8361` by :user:`Gaurav Dhingra <gxyd>`
   and :user:`Ting Neo <neokt>`.
 
+- Added convergence warning to :class:`svm.LinearSVC` and
+  :class:`linear_model.logistic.LogisticRegression` when ``verbose`` is set to 0.
+  :issue:`10881` by :user:`Alexandre Sevin <AlexandreSev>`.
+
+Decomposition, manifold learning and clustering
+
+- Deprecate ``precomputed`` parameter in function
+  :func:`manifold.t_sne.trustworthiness`. Instead, the new parameter
+  ``metric`` should be used with any compatible metric including
+  'precomputed', in which case the input matrix ``X`` should be a matrix of
+  pairwise distances or squared distances. :issue:`9775` by
+  :user:`William de Vazelhes <wdevazelhes>`.
+
 Metrics
 
 - Deprecate ``reorder`` parameter in :func:`metrics.auc` as it's no longer required
@@ -441,6 +533,13 @@ Imputer
   :class:`impute.SimpleImputer`. :issue:`9726` by :user:`Kumar Ashutosh
   <thechargedneutron>`.
 
+- The ``axis`` parameter in :class:`impute.SimpleImputer` was removed. The
+  behavior is equivalent to ``axis=0`` (impute along columns). Row-wise
+  imputation can be performed with FunctionTransformer (e.g.,
+  ``FunctionTransformer(lambda X: Imputer().fit_transform(X.T).T)``).
+  :issue:`10829` by :user:`Guillaume Lemaitre <glemaitre>` and :user:`Gilberto
+  Olimpio <gilbertoolimpio>`.
+
 Outlier Detection models
 
 - More consistent outlier detection API:
@@ -461,8 +560,8 @@ Outlier Detection models
 
 Covariance
 
-- The :func:`covariance.graph_lasso`, :class:`covariance.GraphLasso` and 
-  :class:`covariance.GraphLassoCV` have been renamed to 
+- The :func:`covariance.graph_lasso`, :class:`covariance.GraphLasso` and
+  :class:`covariance.GraphLassoCV` have been renamed to
   :func:`covariance.graphical_lasso`, :class:`covariance.GraphicalLasso` and
   :class:`covariance.GraphicalLassoCV` respectively and will be removed in version 0.22.
   :issue:`9993` by :user:`Artiem Krinitsyn <artiemq>`
@@ -478,6 +577,12 @@ Misc
   :class:`cluster.AffinityPropagation`, and :class:`cluster.Birch`.
   :issue:`#10306` by :user:`Jonathan Siebert <jotasi>`.
 
+- Changed ValueError exception raised in :class:`model_selection.ParameterSampler`
+  to a UserWarning for case where the class is instantiated with a greater value of
+  ``n_iter`` than the total space of parameters in the parameter grid. ``n_iter`` now
+  acts as an upper bound on iterations.
+  :issue:`#10982` by :user:`Juliet Lawton <julietcl>`
+
 Changes to estimator checks
 ---------------------------
 
@@ -495,3 +600,7 @@ Changes to estimator checks
 
 - Add invariance tests for clustering metrics. :issue:`8102` by :user:`Ankita
   Sinha <anki08>` and :user:`Guillaume Lemaitre <glemaitre>`.
+
+- Add tests in :func:`estimator_checks.check_estimator` to check that an
+  estimator can handle read-only memmap input data. :issue:`10663` by
+  :user:`Arthur Mensch <arthurmensch>` and :user:`Loïc Estève <lesteve>`.
diff --git a/examples/applications/skip_stock_market.py b/examples/applications/plot_stock_market.py
similarity index 60%
rename from examples/applications/skip_stock_market.py
rename to examples/applications/plot_stock_market.py
index cc1b0d244c20..379efb8e4dfe 100644
--- a/examples/applications/skip_stock_market.py
+++ b/examples/applications/plot_stock_market.py
@@ -70,160 +70,92 @@
 import numpy as np
 import matplotlib.pyplot as plt
 from matplotlib.collections import LineCollection
-from six.moves.urllib.request import urlopen
-from six.moves.urllib.parse import urlencode
-from sklearn import cluster, covariance, manifold
 
-print(__doc__)
+import pandas as pd
 
+from sklearn import cluster, covariance, manifold
 
-def retry(f, n_attempts=3):
-    "Wrapper function to retry function calls in case of exceptions"
-    def wrapper(*args, **kwargs):
-        for i in range(n_attempts):
-            try:
-                return f(*args, **kwargs)
-            except Exception:
-                if i == n_attempts - 1:
-                    raise
-    return wrapper
-
-
-def quotes_historical_google(symbol, start_date, end_date):
-    """Get the historical data from Google finance.
-
-    Parameters
-    ----------
-    symbol : str
-        Ticker symbol to query for, for example ``"DELL"``.
-    start_date : datetime.datetime
-        Start date.
-    end_date : datetime.datetime
-        End date.
-
-    Returns
-    -------
-    X : array
-        The columns are ``date`` -- date, ``open``, ``high``,
-        ``low``, ``close`` and ``volume`` of type float.
-    """
-    params = {
-        'q': symbol,
-        'startdate': start_date.strftime('%Y-%m-%d'),
-        'enddate': end_date.strftime('%Y-%m-%d'),
-        'output': 'csv',
-    }
-    url = 'https://finance.google.com/finance/historical?' + urlencode(params)
-    response = urlopen(url)
-    dtype = {
-        'names': ['date', 'open', 'high', 'low', 'close', 'volume'],
-        'formats': ['object', 'f4', 'f4', 'f4', 'f4', 'f4']
-    }
-    converters = {
-        0: lambda s: datetime.strptime(s.decode(), '%d-%b-%y').date()}
-    data = np.genfromtxt(response, delimiter=',', skip_header=1,
-                         dtype=dtype, converters=converters,
-                         missing_values='-', filling_values=-1)
-    min_date = min(data['date']) if len(data) else datetime.min.date()
-    max_date = max(data['date']) if len(data) else datetime.max.date()
-    start_end_diff = (end_date - start_date).days
-    min_max_diff = (max_date - min_date).days
-    data_is_fine = (
-        start_date <= min_date <= end_date and
-        start_date <= max_date <= end_date and
-        start_end_diff - 7 <= min_max_diff <= start_end_diff)
-
-    if not data_is_fine:
-        message = (
-            'Data looks wrong for symbol {}, url {}\n'
-            '  - start_date: {}, end_date: {}\n'
-            '  - min_date:   {}, max_date: {}\n'
-            '  - start_end_diff: {}, min_max_diff: {}'.format(
-                symbol, url,
-                start_date, end_date,
-                min_date, max_date,
-                start_end_diff, min_max_diff))
-        raise RuntimeError(message)
-    return data
+print(__doc__)
 
 
 # #############################################################################
 # Retrieve the data from Internet
 
-# Choose a time period reasonably calm (not too long ago so that we get
-# high-tech firms, and before the 2008 crash)
+# The data is from 2003 - 2008. This is reasonably calm: (not too long ago so
+# that we get high-tech firms, and before the 2008 crash). This kind of
+# historical data can be obtained for from APIs like the quandl.com and
+# alphavantage.co ones.
 start_date = datetime(2003, 1, 1).date()
 end_date = datetime(2008, 1, 1).date()
 
 symbol_dict = {
-    'NYSE:TOT': 'Total',
-    'NYSE:XOM': 'Exxon',
-    'NYSE:CVX': 'Chevron',
-    'NYSE:COP': 'ConocoPhillips',
-    'NYSE:VLO': 'Valero Energy',
-    'NASDAQ:MSFT': 'Microsoft',
-    'NYSE:IBM': 'IBM',
-    'NYSE:TWX': 'Time Warner',
-    'NASDAQ:CMCSA': 'Comcast',
-    'NYSE:CVC': 'Cablevision',
-    'NASDAQ:YHOO': 'Yahoo',
-    'NASDAQ:DELL': 'Dell',
-    'NYSE:HPQ': 'HP',
-    'NASDAQ:AMZN': 'Amazon',
-    'NYSE:TM': 'Toyota',
-    'NYSE:CAJ': 'Canon',
-    'NYSE:SNE': 'Sony',
-    'NYSE:F': 'Ford',
-    'NYSE:HMC': 'Honda',
-    'NYSE:NAV': 'Navistar',
-    'NYSE:NOC': 'Northrop Grumman',
-    'NYSE:BA': 'Boeing',
-    'NYSE:KO': 'Coca Cola',
-    'NYSE:MMM': '3M',
-    'NYSE:MCD': 'McDonald\'s',
-    'NYSE:PEP': 'Pepsi',
-    'NYSE:K': 'Kellogg',
-    'NYSE:UN': 'Unilever',
-    'NASDAQ:MAR': 'Marriott',
-    'NYSE:PG': 'Procter Gamble',
-    'NYSE:CL': 'Colgate-Palmolive',
-    'NYSE:GE': 'General Electrics',
-    'NYSE:WFC': 'Wells Fargo',
-    'NYSE:JPM': 'JPMorgan Chase',
-    'NYSE:AIG': 'AIG',
-    'NYSE:AXP': 'American express',
-    'NYSE:BAC': 'Bank of America',
-    'NYSE:GS': 'Goldman Sachs',
-    'NASDAQ:AAPL': 'Apple',
-    'NYSE:SAP': 'SAP',
-    'NASDAQ:CSCO': 'Cisco',
-    'NASDAQ:TXN': 'Texas Instruments',
-    'NYSE:XRX': 'Xerox',
-    'NYSE:WMT': 'Wal-Mart',
-    'NYSE:HD': 'Home Depot',
-    'NYSE:GSK': 'GlaxoSmithKline',
-    'NYSE:PFE': 'Pfizer',
-    'NYSE:SNY': 'Sanofi-Aventis',
-    'NYSE:NVS': 'Novartis',
-    'NYSE:KMB': 'Kimberly-Clark',
-    'NYSE:R': 'Ryder',
-    'NYSE:GD': 'General Dynamics',
-    'NYSE:RTN': 'Raytheon',
-    'NYSE:CVS': 'CVS',
-    'NYSE:CAT': 'Caterpillar',
-    'NYSE:DD': 'DuPont de Nemours'}
+    'TOT': 'Total',
+    'XOM': 'Exxon',
+    'CVX': 'Chevron',
+    'COP': 'ConocoPhillips',
+    'VLO': 'Valero Energy',
+    'MSFT': 'Microsoft',
+    'IBM': 'IBM',
+    'TWX': 'Time Warner',
+    'CMCSA': 'Comcast',
+    'CVC': 'Cablevision',
+    'YHOO': 'Yahoo',
+    'DELL': 'Dell',
+    'HPQ': 'HP',
+    'AMZN': 'Amazon',
+    'TM': 'Toyota',
+    'CAJ': 'Canon',
+    'SNE': 'Sony',
+    'F': 'Ford',
+    'HMC': 'Honda',
+    'NAV': 'Navistar',
+    'NOC': 'Northrop Grumman',
+    'BA': 'Boeing',
+    'KO': 'Coca Cola',
+    'MMM': '3M',
+    'MCD': 'McDonald\'s',
+    'PEP': 'Pepsi',
+    'K': 'Kellogg',
+    'UN': 'Unilever',
+    'MAR': 'Marriott',
+    'PG': 'Procter Gamble',
+    'CL': 'Colgate-Palmolive',
+    'GE': 'General Electrics',
+    'WFC': 'Wells Fargo',
+    'JPM': 'JPMorgan Chase',
+    'AIG': 'AIG',
+    'AXP': 'American express',
+    'BAC': 'Bank of America',
+    'GS': 'Goldman Sachs',
+    'AAPL': 'Apple',
+    'SAP': 'SAP',
+    'CSCO': 'Cisco',
+    'TXN': 'Texas Instruments',
+    'XRX': 'Xerox',
+    'WMT': 'Wal-Mart',
+    'HD': 'Home Depot',
+    'GSK': 'GlaxoSmithKline',
+    'PFE': 'Pfizer',
+    'SNY': 'Sanofi-Aventis',
+    'NVS': 'Novartis',
+    'KMB': 'Kimberly-Clark',
+    'R': 'Ryder',
+    'GD': 'General Dynamics',
+    'RTN': 'Raytheon',
+    'CVS': 'CVS',
+    'CAT': 'Caterpillar',
+    'DD': 'DuPont de Nemours'}
 
 
 symbols, names = np.array(sorted(symbol_dict.items())).T
 
-# retry is used because quotes_historical_google can temporarily fail
-# for various reasons (e.g. empty result from Google API).
 quotes = []
 
 for symbol in symbols:
     print('Fetching quote history for %r' % symbol, file=sys.stderr)
-    quotes.append(retry(quotes_historical_google)(
-        symbol, start_date, end_date))
+    url = ('https://raw.githubusercontent.com/scikit-learn/examples-data/'
+           'master/financial-data/{}.csv')
+    quotes.append(pd.read_csv(url.format(symbol)))
 
 close_prices = np.vstack([q['close'] for q in quotes])
 open_prices = np.vstack([q['open'] for q in quotes])
diff --git a/examples/calibration/plot_calibration_curve.py b/examples/calibration/plot_calibration_curve.py
index d47e035674ee..e096a7017b4f 100644
--- a/examples/calibration/plot_calibration_curve.py
+++ b/examples/calibration/plot_calibration_curve.py
@@ -10,7 +10,7 @@
 uncalibrated classifier.
 
 The experiment is performed on an artificial dataset for binary classification
-with 100.000 samples (1.000 of them are used for model fitting) with 20
+with 100,000 samples (1,000 of them are used for model fitting) with 20
 features. Of the 20 features, only 2 are informative and 10 are redundant. The
 first figure shows the estimated probabilities obtained with logistic
 regression, Gaussian naive Bayes, and Gaussian naive Bayes with both isotonic
diff --git a/examples/cluster/plot_digits_linkage.py b/examples/cluster/plot_digits_linkage.py
index f170149043fe..d9009bbc3b3d 100644
--- a/examples/cluster/plot_digits_linkage.py
+++ b/examples/cluster/plot_digits_linkage.py
@@ -56,7 +56,7 @@ def nudge_images(X, y):
 
 #----------------------------------------------------------------------
 # Visualize the clustering
-def plot_clustering(X_red, X, labels, title=None):
+def plot_clustering(X_red, labels, title=None):
     x_min, x_max = np.min(X_red, axis=0), np.max(X_red, axis=0)
     X_red = (X_red - x_min) / (x_max - x_min)
 
@@ -87,7 +87,7 @@ def plot_clustering(X_red, X, labels, title=None):
     clustering.fit(X_red)
     print("%s :\t%.2fs" % (linkage, time() - t0))
 
-    plot_clustering(X_red, X, clustering.labels_, "%s linkage" % linkage)
+    plot_clustering(X_red, clustering.labels_, "%s linkage" % linkage)
 
 
 plt.show()
diff --git a/examples/gaussian_process/plot_gpc_isoprobability.py b/examples/gaussian_process/plot_gpc_isoprobability.py
index 0639a65a384a..41a409f0e980 100644
--- a/examples/gaussian_process/plot_gpc_isoprobability.py
+++ b/examples/gaussian_process/plot_gpc_isoprobability.py
@@ -46,7 +46,7 @@ def g(x):
 # Observations
 y = np.array(g(X) > 0, dtype=int)
 
-# Instanciate and fit Gaussian Process Model
+# Instantiate and fit Gaussian Process Model
 kernel = C(0.1, (1e-5, np.inf)) * DotProduct(sigma_0=0.1) ** 2
 gp = GaussianProcessClassifier(kernel=kernel)
 gp.fit(X, y)
diff --git a/examples/gaussian_process/plot_gpr_noisy_targets.py b/examples/gaussian_process/plot_gpr_noisy_targets.py
index 8d9b15ed0484..68f735887147 100644
--- a/examples/gaussian_process/plot_gpr_noisy_targets.py
+++ b/examples/gaussian_process/plot_gpr_noisy_targets.py
@@ -49,7 +49,7 @@ def f(x):
 # its MSE
 x = np.atleast_2d(np.linspace(0, 10, 1000)).T
 
-# Instanciate a Gaussian Process model
+# Instantiate a Gaussian Process model
 kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
 gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)
 
diff --git a/examples/manifold/plot_lle_digits.py b/examples/manifold/plot_lle_digits.py
index faa81a5da114..133d81bab0f6 100644
--- a/examples/manifold/plot_lle_digits.py
+++ b/examples/manifold/plot_lle_digits.py
@@ -48,14 +48,14 @@ def plot_embedding(X, title=None):
     plt.figure()
     ax = plt.subplot(111)
     for i in range(X.shape[0]):
-        plt.text(X[i, 0], X[i, 1], str(digits.target[i]),
+        plt.text(X[i, 0], X[i, 1], str(y[i]),
                  color=plt.cm.Set1(y[i] / 10.),
                  fontdict={'weight': 'bold', 'size': 9})
 
     if hasattr(offsetbox, 'AnnotationBbox'):
         # only print thumbnails with matplotlib > 1.0
         shown_images = np.array([[1., 1.]])  # just something big
-        for i in range(digits.data.shape[0]):
+        for i in range(X.shape[0]):
             dist = np.sum((X[i] - shown_images) ** 2, 1)
             if np.min(dist) < 4e-3:
                 # don't show points that are too close
diff --git a/examples/model_selection/plot_confusion_matrix.py b/examples/model_selection/plot_confusion_matrix.py
index 4b7c36098807..339d961f4dbb 100644
--- a/examples/model_selection/plot_confusion_matrix.py
+++ b/examples/model_selection/plot_confusion_matrix.py
@@ -79,9 +79,10 @@ def plot_confusion_matrix(cm, classes,
                  horizontalalignment="center",
                  color="white" if cm[i, j] > thresh else "black")
 
-    plt.tight_layout()
     plt.ylabel('True label')
     plt.xlabel('Predicted label')
+    plt.tight_layout()
+
 
 # Compute confusion matrix
 cnf_matrix = confusion_matrix(y_test, y_pred)
diff --git a/examples/model_selection/plot_precision_recall.py b/examples/model_selection/plot_precision_recall.py
index 5510b4538bf1..7010e96737c6 100644
--- a/examples/model_selection/plot_precision_recall.py
+++ b/examples/model_selection/plot_precision_recall.py
@@ -137,7 +137,7 @@
 # ................................
 from sklearn.metrics import precision_recall_curve
 import matplotlib.pyplot as plt
-from sklearn.externals.funcsigs import signature
+from sklearn.utils.fixes import signature
 
 precision, recall, _ = precision_recall_curve(y_test, y_score)
 
diff --git a/examples/plot_feature_stacker.py b/examples/plot_feature_stacker.py
index d1d32dfb9e2f..4798617f40cb 100644
--- a/examples/plot_feature_stacker.py
+++ b/examples/plot_feature_stacker.py
@@ -19,6 +19,7 @@
 #
 # License: BSD 3 clause
 
+from __future__ import print_function
 from sklearn.pipeline import Pipeline, FeatureUnion
 from sklearn.model_selection import GridSearchCV
 from sklearn.svm import SVC
@@ -42,6 +43,7 @@
 
 # Use combined features to transform dataset:
 X_features = combined_features.fit(X, y).transform(X)
+print("Combined space has", X_features.shape[1], "features")
 
 svm = SVC(kernel="linear")
 
diff --git a/examples/plot_missing_values.py b/examples/plot_missing_values.py
index 811be48c161a..e32c19fae084 100644
--- a/examples/plot_missing_values.py
+++ b/examples/plot_missing_values.py
@@ -1,73 +1,127 @@
 """
-======================================================
+====================================================
 Imputing missing values before building an estimator
-======================================================
-
-This example shows that imputing the missing values can give better
-results than discarding the samples containing any missing value.
-Imputing does not always improve the predictions, so please check via
-cross-validation.  Sometimes dropping rows or using marker values is
-more effective.
+====================================================
 
 Missing values can be replaced by the mean, the median or the most frequent
-value using the ``strategy`` hyper-parameter.
+value using the basic ``SimpleImputer``.
 The median is a more robust estimator for data with high magnitude variables
 which could dominate results (otherwise known as a 'long tail').
 
-Script output::
-
-  Score with the entire dataset = 0.56
-  Score without the samples containing missing values = 0.48
-  Score after imputation of the missing values = 0.55
-
-In this case, imputing helps the classifier get close to the original score.
-
+Another option is the MICE imputer. This uses round-robin linear regression,
+treating every variable as an output in turn. The version implemented assumes
+Gaussian (output) variables. If your features are obviously non-Normal,
+consider transforming them to look more Normal so as to improve performance.
 """
+
 import numpy as np
+import matplotlib.pyplot as plt
 
+from sklearn.datasets import load_diabetes
 from sklearn.datasets import load_boston
 from sklearn.ensemble import RandomForestRegressor
 from sklearn.pipeline import Pipeline
-from sklearn.impute import SimpleImputer
+from sklearn.impute import SimpleImputer, MICEImputer
 from sklearn.model_selection import cross_val_score
 
 rng = np.random.RandomState(0)
 
-dataset = load_boston()
-X_full, y_full = dataset.data, dataset.target
-n_samples = X_full.shape[0]
-n_features = X_full.shape[1]
-
-# Estimate the score on the entire dataset, with no missing values
-estimator = RandomForestRegressor(random_state=0, n_estimators=100)
-score = cross_val_score(estimator, X_full, y_full).mean()
-print("Score with the entire dataset = %.2f" % score)
-
-# Add missing values in 75% of the lines
-missing_rate = 0.75
-n_missing_samples = int(np.floor(n_samples * missing_rate))
-missing_samples = np.hstack((np.zeros(n_samples - n_missing_samples,
-                                      dtype=np.bool),
-                             np.ones(n_missing_samples,
-                                     dtype=np.bool)))
-rng.shuffle(missing_samples)
-missing_features = rng.randint(0, n_features, n_missing_samples)
-
-# Estimate the score without the lines containing missing values
-X_filtered = X_full[~missing_samples, :]
-y_filtered = y_full[~missing_samples]
-estimator = RandomForestRegressor(random_state=0, n_estimators=100)
-score = cross_val_score(estimator, X_filtered, y_filtered).mean()
-print("Score without the samples containing missing values = %.2f" % score)
-
-# Estimate the score after imputation of the missing values
-X_missing = X_full.copy()
-X_missing[np.where(missing_samples)[0], missing_features] = 0
-y_missing = y_full.copy()
-estimator = Pipeline([("imputer", SimpleImputer(missing_values=0,
-                                                strategy="mean",
-                                                axis=0)),
-                      ("forest", RandomForestRegressor(random_state=0,
-                                                       n_estimators=100))])
-score = cross_val_score(estimator, X_missing, y_missing).mean()
-print("Score after imputation of the missing values = %.2f" % score)
+
+def get_results(dataset):
+    X_full, y_full = dataset.data, dataset.target
+    n_samples = X_full.shape[0]
+    n_features = X_full.shape[1]
+
+    # Estimate the score on the entire dataset, with no missing values
+    estimator = RandomForestRegressor(random_state=0, n_estimators=100)
+    full_scores = cross_val_score(estimator, X_full, y_full,
+                                  scoring='neg_mean_squared_error')
+
+    # Add missing values in 75% of the lines
+    missing_rate = 0.75
+    n_missing_samples = int(np.floor(n_samples * missing_rate))
+    missing_samples = np.hstack((np.zeros(n_samples - n_missing_samples,
+                                          dtype=np.bool),
+                                 np.ones(n_missing_samples,
+                                         dtype=np.bool)))
+    rng.shuffle(missing_samples)
+    missing_features = rng.randint(0, n_features, n_missing_samples)
+
+    # Estimate the score after replacing missing values by 0
+    X_missing = X_full.copy()
+    X_missing[np.where(missing_samples)[0], missing_features] = 0
+    y_missing = y_full.copy()
+    estimator = RandomForestRegressor(random_state=0, n_estimators=100)
+    zero_impute_scores = cross_val_score(estimator, X_missing, y_missing,
+                                         scoring='neg_mean_squared_error')
+
+    # Estimate the score after imputation (mean strategy) of the missing values
+    X_missing = X_full.copy()
+    X_missing[np.where(missing_samples)[0], missing_features] = 0
+    y_missing = y_full.copy()
+    estimator = Pipeline([("imputer", SimpleImputer(missing_values=0,
+                                                    strategy="mean")),
+                          ("forest", RandomForestRegressor(random_state=0,
+                                                           n_estimators=100))])
+    mean_impute_scores = cross_val_score(estimator, X_missing, y_missing,
+                                         scoring='neg_mean_squared_error')
+
+    # Estimate the score after imputation (MICE strategy) of the missing values
+    estimator = Pipeline([("imputer", MICEImputer(missing_values=0,
+                                                  random_state=0)),
+                          ("forest", RandomForestRegressor(random_state=0,
+                                                           n_estimators=100))])
+    mice_impute_scores = cross_val_score(estimator, X_missing, y_missing,
+                                         scoring='neg_mean_squared_error')
+
+    return ((full_scores.mean(), full_scores.std()),
+            (zero_impute_scores.mean(), zero_impute_scores.std()),
+            (mean_impute_scores.mean(), mean_impute_scores.std()),
+            (mice_impute_scores.mean(), mice_impute_scores.std()))
+
+
+results_diabetes = np.array(get_results(load_diabetes()))
+mses_diabetes = results_diabetes[:, 0] * -1
+stds_diabetes = results_diabetes[:, 1]
+
+results_boston = np.array(get_results(load_boston()))
+mses_boston = results_boston[:, 0] * -1
+stds_boston = results_boston[:, 1]
+
+n_bars = len(mses_diabetes)
+xval = np.arange(n_bars)
+
+x_labels = ['Full data',
+            'Zero imputation',
+            'Mean Imputation',
+            'MICE Imputation']
+colors = ['r', 'g', 'b', 'orange']
+
+# plot diabetes results
+plt.figure(figsize=(12, 6))
+ax1 = plt.subplot(121)
+for j in xval:
+    ax1.barh(j, mses_diabetes[j], xerr=stds_diabetes[j],
+             color=colors[j], alpha=0.6, align='center')
+
+ax1.set_title('Feature Selection Techniques with Diabetes Data')
+ax1.set_xlim(left=np.min(mses_diabetes) * 0.9,
+             right=np.max(mses_diabetes) * 1.1)
+ax1.set_yticks(xval)
+ax1.set_xlabel('MSE')
+ax1.invert_yaxis()
+ax1.set_yticklabels(x_labels)
+
+# plot boston results
+ax2 = plt.subplot(122)
+for j in xval:
+    ax2.barh(j, mses_boston[j], xerr=stds_boston[j],
+             color=colors[j], alpha=0.6, align='center')
+
+ax2.set_title('Feature Selection Techniques with Boston Data')
+ax2.set_yticks(xval)
+ax2.set_xlabel('MSE')
+ax2.invert_yaxis()
+ax2.set_yticklabels([''] * n_bars)
+
+plt.show()
diff --git a/examples/preprocessing/plot_scaling_importance.py b/examples/preprocessing/plot_scaling_importance.py
index b131bc417df1..15a134d0fd22 100644
--- a/examples/preprocessing/plot_scaling_importance.py
+++ b/examples/preprocessing/plot_scaling_importance.py
@@ -89,7 +89,7 @@
 pca = unscaled_clf.named_steps['pca']
 pca_std = std_clf.named_steps['pca']
 
-# Show first principal componenets
+# Show first principal components
 print('\nPC 1 without scaling:\n', pca.components_[0])
 print('\nPC 1 with scaling:\n', pca_std.components_[0])
 
diff --git a/examples/svm/plot_rbf_parameters.py b/examples/svm/plot_rbf_parameters.py
index 3a909b2b422b..fa0702538c6a 100644
--- a/examples/svm/plot_rbf_parameters.py
+++ b/examples/svm/plot_rbf_parameters.py
@@ -31,7 +31,7 @@
 
 Note that the heat map plot has a special colorbar with a midpoint value close
 to the score values of the best performing models so as to make it easy to tell
-them appart in the blink of an eye.
+them apart in the blink of an eye.
 
 The behavior of the model is very sensitive to the ``gamma`` parameter. If
 ``gamma`` is too large, the radius of the area of influence of the support
diff --git a/examples/tree/plot_unveil_tree_structure.py b/examples/tree/plot_unveil_tree_structure.py
index 161d3983530c..b9ae45e9010b 100644
--- a/examples/tree/plot_unveil_tree_structure.py
+++ b/examples/tree/plot_unveil_tree_structure.py
@@ -106,7 +106,7 @@
 
 print('Rules used to predict sample %s: ' % sample_id)
 for node_id in node_index:
-    if leave_id[sample_id] != node_id:
+    if leave_id[sample_id] == node_id:
         continue
 
     if (X_test[sample_id, feature[node_id]] <= threshold[node_id]):
diff --git a/setup.cfg b/setup.cfg
index f96e9cf9f85a..b02383bae3b5 100644
--- a/setup.cfg
+++ b/setup.cfg
@@ -7,6 +7,7 @@ test = pytest
 addopts =
     --doctest-modules
     --disable-pytest-warnings
+    -rs
 
 [wheelhouse_uploader]
 artifact_indexes=
diff --git a/sklearn/cluster/_k_means_elkan.pyx b/sklearn/cluster/_k_means_elkan.pyx
index f79f3011abba..5dad5f0c2481 100644
--- a/sklearn/cluster/_k_means_elkan.pyx
+++ b/sklearn/cluster/_k_means_elkan.pyx
@@ -155,7 +155,7 @@ def k_means_elkan(np.ndarray[floating, ndim=2, mode='c'] X_,
     upper_bounds_ = np.empty(n_samples, dtype=dtype)
     cdef floating[:] upper_bounds = upper_bounds_
 
-    # Get the inital set of upper bounds and lower bounds for each sample.
+    # Get the initial set of upper bounds and lower bounds for each sample.
     update_labels_distances_inplace(X_p, centers_p, center_half_distances,
                                     labels, lower_bounds, upper_bounds,
                                     n_samples, n_features, n_clusters)
diff --git a/sklearn/cluster/bicluster.py b/sklearn/cluster/bicluster.py
index 6c61d6b983bb..81f2f411a7f4 100644
--- a/sklearn/cluster/bicluster.py
+++ b/sklearn/cluster/bicluster.py
@@ -238,11 +238,10 @@ class SpectralCoclustering(BaseSpectral):
         (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
         are used.
 
-    random_state : int, RandomState instance or None, optional, default: None
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
+    random_state : int, RandomState instance or None (default)
+        Used for randomizing the singular value decomposition and the k-means
+        initialization. Use an int to make the randomness deterministic.
+        See :term:`Glossary <random_state>`.
 
     Attributes
     ----------
@@ -370,11 +369,10 @@ class SpectralBiclustering(BaseSpectral):
         (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
         are used.
 
-    random_state : int, RandomState instance or None, optional, default: None
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
+    random_state : int, RandomState instance or None (default)
+        Used for randomizing the singular value decomposition and the k-means
+        initialization. Use an int to make the randomness deterministic.
+        See :term:`Glossary <random_state>`.
 
     Attributes
     ----------
diff --git a/sklearn/cluster/k_means_.py b/sklearn/cluster/k_means_.py
index 94f033f1cf0a..e2ee29147366 100644
--- a/sklearn/cluster/k_means_.py
+++ b/sklearn/cluster/k_means_.py
@@ -56,8 +56,10 @@ def _k_init(X, n_clusters, x_squared_norms, random_state, n_local_trials=None):
     x_squared_norms : array, shape (n_samples,)
         Squared Euclidean norm of each data point.
 
-    random_state : RandomState
-        The generator used to initialize the centers.
+    random_state : int, RandomState instance
+        The generator used to initialize the centers. Use an int to make the
+        randomness deterministic.
+        See :term:`Glossary <random_state>`.
 
     n_local_trials : integer, optional
         The number of seeding trials for each center (except the first),
@@ -171,7 +173,7 @@ def _check_sample_weight(X, sample_weight):
     if sample_weight is None:
         return np.ones(n_samples, dtype=X.dtype)
     else:
-        # verify that the number of samples is equal to the number of weights
+        sample_weight = np.asarray(sample_weight)
         if n_samples != len(sample_weight):
             raise ValueError("n_samples=%d should be == len(sample_weight)=%d"
                              % (n_samples, len(sample_weight)))
@@ -244,11 +246,10 @@ def k_means(X, n_clusters, sample_weight=None, init='k-means++',
     tol : float, optional
         The relative increment in the results before declaring convergence.
 
-    random_state : int, RandomState instance or None, optional, default: None
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
+    random_state : int, RandomState instance or None (default)
+        Determines random number generation for centroid initialization. Use
+        an int to make the randomness deterministic.
+        See :term:`Glossary <random_state>`.
 
     copy_x : boolean, optional
         When pre-computing distances it is more numerically accurate to center
@@ -499,11 +500,10 @@ def _kmeans_single_lloyd(X, sample_weight, n_clusters, max_iter=300,
     precompute_distances : boolean, default: True
         Precompute distances (faster but takes more memory).
 
-    random_state : int, RandomState instance or None, optional, default: None
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
+    random_state : int, RandomState instance or None (default)
+        Determines random number generation for centroid initialization. Use
+        an int to make the randomness deterministic.
+        See :term:`Glossary <random_state>`.
 
     Returns
     -------
@@ -703,11 +703,10 @@ def _init_centroids(X, k, init, random_state=None, x_squared_norms=None,
     init : {'k-means++', 'random' or ndarray or callable} optional
         Method for initialization
 
-    random_state : int, RandomState instance or None, optional, default: None
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
+    random_state : int, RandomState instance or None (default)
+        Determines random number generation for centroid initialization. Use
+        an int to make the randomness deterministic.
+        See :term:`Glossary <random_state>`.
 
     x_squared_norms :  array, shape (n_samples,), optional
         Squared euclidean norm of each data point. Pass it if you have it at
@@ -820,11 +819,10 @@ class KMeans(BaseEstimator, ClusterMixin, TransformerMixin):
     verbose : int, default 0
         Verbosity mode.
 
-    random_state : int, RandomState instance or None, optional, default: None
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
+    random_state : int, RandomState instance or None (default)
+        Determines random number generation for centroid initialization. Use
+        an int to make the randomness deterministic.
+        See :term:`Glossary <random_state>`.
 
     copy_x : boolean, optional
         When pre-computing distances it is more numerically accurate to center
@@ -1124,11 +1122,11 @@ def _mini_batch_step(X, sample_weight, x_squared_norms, centers, weight_sums,
         the distances of each sample to its closest center.
         May not be None when random_reassign is True.
 
-    random_state : int, RandomState instance or None, optional, default: None
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
+    random_state : int, RandomState instance or None (default)
+        Determines random number generation for centroid initialization and to
+        pick new clusters amongst observations with uniform probability. Use
+        an int to make the randomness deterministic.
+        See :term:`Glossary <random_state>`.
 
     random_reassign : boolean, optional
         If True, centers with very low counts are randomly reassigned
@@ -1341,11 +1339,10 @@ class MiniBatchKMeans(KMeans):
         Compute label assignment and inertia for the complete dataset
         once the minibatch optimization has converged in fit.
 
-    random_state : int, RandomState instance or None, optional, default: None
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
+    random_state : int, RandomState instance or None (default)
+        Determines random number generation for centroid initialization and
+        random reassignment. Use an int to make the randomness deterministic.
+        See :term:`Glossary <random_state>`.
 
     tol : float, default: 0.0
         Control early stopping based on the relative center changes as
diff --git a/sklearn/cluster/mean_shift_.py b/sklearn/cluster/mean_shift_.py
index 3238fa358e3e..332531b13078 100644
--- a/sklearn/cluster/mean_shift_.py
+++ b/sklearn/cluster/mean_shift_.py
@@ -47,11 +47,11 @@ def estimate_bandwidth(X, quantile=0.3, n_samples=None, random_state=0,
     n_samples : int, optional
         The number of samples to use. If not given, all samples are used.
 
-    random_state : int, RandomState instance or None, optional (default=None)
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
+    random_state : int, RandomState instance or None (default)
+        The generator used to randomly select the samples from input points
+        for bandwidth estimation. Use an int to make the randomness
+        deterministic.
+        See :term:`Glossary <random_state>`.
 
     n_jobs : int, optional (default = 1)
         The number of parallel jobs to run for neighbors search.
diff --git a/sklearn/cluster/spectral.py b/sklearn/cluster/spectral.py
index 5051043c3147..58ef1ba6c9d0 100644
--- a/sklearn/cluster/spectral.py
+++ b/sklearn/cluster/spectral.py
@@ -38,11 +38,10 @@ def discretize(vectors, copy=True, max_svd_restarts=30, n_iter_max=20,
         Maximum number of iterations to attempt in rotation and partition
         matrix search if machine precision convergence is not reached
 
-    random_state : int, RandomState instance or None, optional, default: None
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
+    random_state : int, RandomState instance or None (default)
+        Determines random number generation for rotation matrix initialization.
+        Use an int to make the randomness deterministic.
+        See :term:`Glossary <random_state>`.
 
     Returns
     -------
@@ -195,13 +194,12 @@ def spectral_clustering(affinity, n_clusters=8, n_components=None,
         to be installed. It can be faster on very large, sparse problems,
         but may also lead to instabilities
 
-    random_state : int, RandomState instance or None, optional, default: None
+    random_state : int, RandomState instance or None (default)
         A pseudo random number generator used for the initialization of the
         lobpcg eigen vectors decomposition when eigen_solver == 'amg' and by
-        the K-Means initialization. If int, random_state is the seed used by
-        the random number generator; If RandomState instance, random_state is
-        the random number generator; If None, the random number generator is
-        the RandomState instance used by `np.random`.
+        the K-Means initialization. Use an int to make the randomness
+        deterministic.
+        See :term:`Glossary <random_state>`.
 
     n_init : int, optional, default: 10
         Number of time the k-means algorithm will be run with different
@@ -309,13 +307,12 @@ class SpectralClustering(BaseEstimator, ClusterMixin):
         to be installed. It can be faster on very large, sparse problems,
         but may also lead to instabilities
 
-    random_state : int, RandomState instance or None, optional, default: None
+    random_state : int, RandomState instance or None (default)
         A pseudo random number generator used for the initialization of the
         lobpcg eigen vectors decomposition when eigen_solver == 'amg' and by
-        the K-Means initialization.  If int, random_state is the seed used by
-        the random number generator; If RandomState instance, random_state is
-        the random number generator; If None, the random number generator is
-        the RandomState instance used by `np.random`.
+        the K-Means initialization. Use an int to make the randomness
+        deterministic.
+        See :term:`Glossary <random_state>`.
 
     n_init : int, optional, default: 10
         Number of time the k-means algorithm will be run with different
diff --git a/sklearn/cluster/tests/test_hierarchical.py b/sklearn/cluster/tests/test_hierarchical.py
index 3dcc415424cb..83ddc9729ceb 100644
--- a/sklearn/cluster/tests/test_hierarchical.py
+++ b/sklearn/cluster/tests/test_hierarchical.py
@@ -296,7 +296,7 @@ def test_scikit_vs_scipy():
             children_ = out[:, :2].astype(np.int)
             children, _, n_leaves, _ = _TREE_BUILDERS[linkage](X, connectivity)
 
-            # Sort the order of of child nodes per row for consistency
+            # Sort the order of child nodes per row for consistency
             children.sort(axis=1)
             assert_array_equal(children, children_, 'linkage tree differs'
                                                     ' from scipy impl for'
diff --git a/sklearn/compose/_target.py b/sklearn/compose/_target.py
index 0537bf43a526..cb3e1cedd0eb 100644
--- a/sklearn/compose/_target.py
+++ b/sklearn/compose/_target.py
@@ -23,13 +23,16 @@ class TransformedTargetRegressor(BaseEstimator, RegressorMixin):
     ``exp``.
 
     The computation during ``fit`` is::
+
         regressor.fit(X, func(y))
     or::
-        regressor.fit(X, transformer.transform(y))
 
+        regressor.fit(X, transformer.transform(y))
     The computation during ``predict`` is::
+
         inverse_func(regressor.predict(X))
     or::
+
         transformer.inverse_transform(regressor.predict(X))
 
     Read more in the :ref:`User Guide <preprocessing_targets>`.
diff --git a/sklearn/covariance/elliptic_envelope.py b/sklearn/covariance/elliptic_envelope.py
index 633f102fc006..874155bdd28d 100644
--- a/sklearn/covariance/elliptic_envelope.py
+++ b/sklearn/covariance/elliptic_envelope.py
@@ -161,7 +161,6 @@ def score_samples(self, X):
             Opposite of the Mahalanobis distances.
         """
         check_is_fitted(self, 'offset_')
-        X = check_array(X)
         return -self.mahalanobis(X)
 
     def predict(self, X):
diff --git a/sklearn/covariance/empirical_covariance_.py b/sklearn/covariance/empirical_covariance_.py
index 650719815e6b..84ad65af7a5a 100644
--- a/sklearn/covariance/empirical_covariance_.py
+++ b/sklearn/covariance/empirical_covariance_.py
@@ -18,6 +18,7 @@
 from ..base import BaseEstimator
 from ..utils import check_array
 from ..utils.extmath import fast_logdet
+from ..metrics.pairwise import pairwise_distances
 
 
 def log_likelihood(emp_cov, precision):
@@ -263,26 +264,25 @@ def error_norm(self, comp_cov, norm='frobenius', scaling=True,
 
         return result
 
-    def mahalanobis(self, observations):
+    def mahalanobis(self, X):
         """Computes the squared Mahalanobis distances of given observations.
 
         Parameters
         ----------
-        observations : array-like, shape = [n_observations, n_features]
+        X : array-like, shape = [n_samples, n_features]
             The observations, the Mahalanobis distances of the which we
             compute. Observations are assumed to be drawn from the same
             distribution than the data used in fit.
 
         Returns
         -------
-        mahalanobis_distance : array, shape = [n_observations,]
+        dist : array, shape = [n_samples,]
             Squared Mahalanobis distances of the observations.
 
         """
         precision = self.get_precision()
         # compute mahalanobis distances
-        centered_obs = observations - self.location_
-        mahalanobis_dist = np.sum(
-            np.dot(centered_obs, precision) * centered_obs, 1)
+        dist = pairwise_distances(X, self.location_[np.newaxis, :],
+                                  metric='mahalanobis', VI=precision)
 
-        return mahalanobis_dist
+        return np.reshape(dist, (len(X),)) ** 2
diff --git a/sklearn/datasets/rcv1.py b/sklearn/datasets/rcv1.py
index afe239782365..f66823471277 100644
--- a/sklearn/datasets/rcv1.py
+++ b/sklearn/datasets/rcv1.py
@@ -24,12 +24,15 @@
 from ..utils import Bunch
 
 
-# The original data can be found at:
-# http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a13-vector-files/lyrl2004_vectors_test_pt0.dat.gz
-# http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a13-vector-files/lyrl2004_vectors_test_pt1.dat.gz
-# http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a13-vector-files/lyrl2004_vectors_test_pt2.dat.gz
-# http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a13-vector-files/lyrl2004_vectors_test_pt3.dat.gz
-# http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a13-vector-files/lyrl2004_vectors_train.dat.gz
+# The original vectorized data can be found at:
+#    http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/a13-vector-files/lyrl2004_vectors_test_pt0.dat.gz
+#    http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/a13-vector-files/lyrl2004_vectors_test_pt1.dat.gz
+#    http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/a13-vector-files/lyrl2004_vectors_test_pt2.dat.gz
+#    http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/a13-vector-files/lyrl2004_vectors_test_pt3.dat.gz
+#    http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/a13-vector-files/lyrl2004_vectors_train.dat.gz
+# while the original stemmed token files can be found
+# in the README, section B.12.i.:
+#    http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/lyrl2004_rcv1v2_README.htm
 XY_METADATA = (
     RemoteFileMetadata(
         url='https://ndownloader.figshare.com/files/5976069',
diff --git a/sklearn/datasets/samples_generator.py b/sklearn/datasets/samples_generator.py
index a3628324aace..5cd54b438f13 100644
--- a/sklearn/datasets/samples_generator.py
+++ b/sklearn/datasets/samples_generator.py
@@ -705,7 +705,7 @@ def make_blobs(n_samples=100, n_features=2, centers=None, cluster_std=1.0,
     Parameters
     ----------
     n_samples : int or array-like, optional (default=100)
-        If int, it is the the total number of points equally divided among
+        If int, it is the total number of points equally divided among
         clusters.
         If array-like, each element of the sequence indicates
         the number of samples per cluster.
diff --git a/sklearn/datasets/tests/test_base.py b/sklearn/datasets/tests/test_base.py
index 091735b986a3..9b63190f1b76 100644
--- a/sklearn/datasets/tests/test_base.py
+++ b/sklearn/datasets/tests/test_base.py
@@ -248,7 +248,7 @@ def test_bunch_pickle_generated_with_0_16_and_read_with_0_17():
     bunch = Bunch(key='original')
     # This reproduces a problem when Bunch pickles have been created
     # with scikit-learn 0.16 and are read with 0.17. Basically there
-    # is a suprising behaviour because reading bunch.key uses
+    # is a surprising behaviour because reading bunch.key uses
     # bunch.__dict__ (which is non empty for 0.16 Bunch objects)
     # whereas assigning into bunch.key uses bunch.__setattr__. See
     # https://github.com/scikit-learn/scikit-learn/issues/6196 for
diff --git a/sklearn/decomposition/fastica_.py b/sklearn/decomposition/fastica_.py
index 7bc78e4e31b8..f2484672abe6 100644
--- a/sklearn/decomposition/fastica_.py
+++ b/sklearn/decomposition/fastica_.py
@@ -182,10 +182,11 @@ def fastica(X, n_components=None, algorithm="parallel", whiten=True,
         or 'cube'.
         You can also provide your own function. It should return a tuple
         containing the value of the function, and of its derivative, in the
-        point. Example:
+        point. The derivative should be averaged along its last dimension.
+        Example:
 
         def my_g(x):
-            return x ** 3, 3 * x ** 2
+            return x ** 3, np.mean(3 * x ** 2, axis=-1)
 
     fun_args : dictionary, optional
         Arguments to send to the functional form.
@@ -454,6 +455,9 @@ def __init__(self, n_components=None, algorithm='parallel', whiten=True,
                  fun='logcosh', fun_args=None, max_iter=200, tol=1e-4,
                  w_init=None, random_state=None):
         super(FastICA, self).__init__()
+        if max_iter < 1:
+            raise ValueError("max_iter should be greater than 1, got "
+                             "(max_iter={})".format(max_iter))
         self.n_components = n_components
         self.algorithm = algorithm
         self.whiten = whiten
diff --git a/sklearn/decomposition/nmf.py b/sklearn/decomposition/nmf.py
index d78dcc678ba5..5eee44a7396b 100644
--- a/sklearn/decomposition/nmf.py
+++ b/sklearn/decomposition/nmf.py
@@ -467,7 +467,7 @@ def _fit_coordinate_descent(X, W, H, tol=1e-4, max_iter=200, l1_reg_W=0,
 
     References
     ----------
-    Cichocki, Andrzej, and P. H. A. N. Anh-Huy. "Fast local algorithms for
+    Cichocki, Andrzej, and Phan, Anh-Huy. "Fast local algorithms for
     large scale nonnegative matrix and tensor factorizations."
     IEICE transactions on fundamentals of electronics, communications and
     computer sciences 92.3: 708-721, 2009.
@@ -891,7 +891,8 @@ def non_negative_factorization(X, W=None, H=None, n_components=None,
 
     solver : 'cd' | 'mu'
         Numerical solver to use:
-        'cd' is a Coordinate Descent solver.
+        'cd' is a Coordinate Descent solver that uses Fast Hierarchical
+            Alternating Least Squares (Fast HALS).
         'mu' is a Multiplicative Update solver.
 
         .. versionadded:: 0.17
diff --git a/sklearn/decomposition/online_lda.py b/sklearn/decomposition/online_lda.py
index b725bfad0a81..bf35b20f35b8 100644
--- a/sklearn/decomposition/online_lda.py
+++ b/sklearn/decomposition/online_lda.py
@@ -42,7 +42,7 @@ def _update_doc_distribution(X, exp_topic_word_distr, doc_topic_prior,
         Document word matrix.
 
     exp_topic_word_distr : dense matrix, shape=(n_topics, n_features)
-        Exponential value of expection of log topic word distribution.
+        Exponential value of expectation of log topic word distribution.
         In the literature, this is `exp(E[log(beta)])`.
 
     doc_topic_prior : float
diff --git a/sklearn/decomposition/pca.py b/sklearn/decomposition/pca.py
index c5ceb40672db..1c93f6b00134 100644
--- a/sklearn/decomposition/pca.py
+++ b/sklearn/decomposition/pca.py
@@ -243,7 +243,7 @@ class PCA(_BasePCA):
         from Tipping and Bishop 1999. See "Pattern Recognition and
         Machine Learning" by C. Bishop, 12.2.1 p. 574 or
         http://www.miketipping.com/papers/met-mppca.pdf. It is required to
-        computed the estimated data covariance and score samples.
+        compute the estimated data covariance and score samples.
 
         Equal to the average of (min(n_features, n_samples) - n_components)
         smallest eigenvalues of the covariance matrix of X.
@@ -326,7 +326,7 @@ def fit(self, X, y=None):
         Parameters
         ----------
         X : array-like, shape (n_samples, n_features)
-            Training data, where n_samples in the number of samples
+            Training data, where n_samples is the number of samples
             and n_features is the number of features.
 
         y : Ignored
diff --git a/sklearn/decomposition/sparse_pca.py b/sklearn/decomposition/sparse_pca.py
index 68db09b5d277..b8be8fdc4b51 100644
--- a/sklearn/decomposition/sparse_pca.py
+++ b/sklearn/decomposition/sparse_pca.py
@@ -244,9 +244,6 @@ class MiniBatchSparsePCA(SparsePCA):
     components_ : array, [n_components, n_features]
         Sparse components extracted from the data.
 
-    error_ : array
-        Vector of errors at each iteration.
-
     n_iter_ : int
         Number of iterations run.
 
diff --git a/sklearn/decomposition/tests/test_fastica.py b/sklearn/decomposition/tests/test_fastica.py
index 591c4a7615b2..b237f4a15def 100644
--- a/sklearn/decomposition/tests/test_fastica.py
+++ b/sklearn/decomposition/tests/test_fastica.py
@@ -14,6 +14,7 @@
 from sklearn.utils.testing import assert_equal
 from sklearn.utils.testing import assert_warns
 from sklearn.utils.testing import assert_raises
+from sklearn.utils.testing import assert_raises_regex
 
 from sklearn.decomposition import FastICA, fastica, PCA
 from sklearn.decomposition.fastica_ import _gs_decorrelation
@@ -259,3 +260,21 @@ def test_inverse_transform():
             # reversibility test in non-reduction case
             if n_components == X.shape[1]:
                 assert_array_almost_equal(X, X2)
+
+
+def test_fastica_errors():
+    n_features = 3
+    n_samples = 10
+    rng = np.random.RandomState(0)
+    X = rng.random_sample((n_samples, n_features))
+    w_init = rng.randn(n_features + 1, n_features + 1)
+    assert_raises_regex(ValueError, 'max_iter should be greater than 1',
+                        FastICA, max_iter=0)
+    assert_raises_regex(ValueError, r'alpha must be in \[1,2\]',
+                        fastica, X, fun_args={'alpha': 0})
+    assert_raises_regex(ValueError, 'w_init has invalid shape.+'
+                        r'should be \(3L?, 3L?\)',
+                        fastica, X, w_init=w_init)
+    assert_raises_regex(ValueError,
+                        'Invalid algorithm.+must be.+parallel.+or.+deflation',
+                        fastica, X, algorithm='pizza')
diff --git a/sklearn/discriminant_analysis.py b/sklearn/discriminant_analysis.py
index 9ff65677dd86..edb17294fa1a 100644
--- a/sklearn/discriminant_analysis.py
+++ b/sklearn/discriminant_analysis.py
@@ -83,18 +83,18 @@ def _class_means(X, y):
 
     Returns
     -------
-    means : array-like, shape (n_features,)
+    means : array-like, shape (n_classes, n_features)
         Class means.
     """
-    means = []
-    classes = np.unique(y)
-    for group in classes:
-        Xg = X[y == group, :]
-        means.append(Xg.mean(0))
-    return np.asarray(means)
+    classes, y = np.unique(y, return_inverse=True)
+    cnt = np.bincount(y)
+    means = np.zeros(shape=(len(classes), X.shape[1]))
+    np.add.at(means, y, X)
+    means /= cnt[:, None]
+    return means
 
 
-def _class_cov(X, y, priors=None, shrinkage=None):
+def _class_cov(X, y, priors, shrinkage=None):
     """Compute class covariance matrix.
 
     Parameters
@@ -120,11 +120,11 @@ def _class_cov(X, y, priors=None, shrinkage=None):
         Class covariance matrix.
     """
     classes = np.unique(y)
-    covs = []
-    for group in classes:
+    cov = np.zeros(shape=(X.shape[1], X.shape[1]))
+    for idx, group in enumerate(classes):
         Xg = X[y == group, :]
-        covs.append(np.atleast_2d(_cov(Xg, shrinkage)))
-    return np.average(covs, axis=0, weights=priors)
+        cov += priors[idx] * np.atleast_2d(_cov(Xg, shrinkage))
+    return cov
 
 
 class LinearDiscriminantAnalysis(BaseEstimator, LinearClassifierMixin,
diff --git a/sklearn/ensemble/forest.py b/sklearn/ensemble/forest.py
index 8293141ef1ba..05ba33faab37 100644
--- a/sklearn/ensemble/forest.py
+++ b/sklearn/ensemble/forest.py
@@ -745,7 +745,7 @@ class RandomForestClassifier(ForestClassifier):
     """A random forest classifier.
 
     A random forest is a meta estimator that fits a number of decision tree
-    classifiers on various sub-samples of the dataset and use averaging to
+    classifiers on various sub-samples of the dataset and uses averaging to
     improve the predictive accuracy and control over-fitting.
     The sub-sample size is always the same as the original
     input sample size but the samples are drawn with replacement if
@@ -1019,7 +1019,7 @@ class RandomForestRegressor(ForestRegressor):
     """A random forest regressor.
 
     A random forest is a meta estimator that fits a number of classifying
-    decision trees on various sub-samples of the dataset and use averaging
+    decision trees on various sub-samples of the dataset and uses averaging
     to improve the predictive accuracy and control over-fitting.
     The sub-sample size is always the same as the original
     input sample size but the samples are drawn with replacement if
@@ -1258,7 +1258,7 @@ class ExtraTreesClassifier(ForestClassifier):
 
     This class implements a meta estimator that fits a number of
     randomized decision trees (a.k.a. extra-trees) on various sub-samples
-    of the dataset and use averaging to improve the predictive accuracy
+    of the dataset and uses averaging to improve the predictive accuracy
     and control over-fitting.
 
     Read more in the :ref:`User Guide <forest>`.
@@ -1502,7 +1502,7 @@ class ExtraTreesRegressor(ForestRegressor):
 
     This class implements a meta estimator that fits a number of
     randomized decision trees (a.k.a. extra-trees) on various sub-samples
-    of the dataset and use averaging to improve the predictive accuracy
+    of the dataset and uses averaging to improve the predictive accuracy
     and control over-fitting.
 
     Read more in the :ref:`User Guide <forest>`.
diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py
index 636640b19511..dc89d18af6da 100644
--- a/sklearn/ensemble/iforest.py
+++ b/sklearn/ensemble/iforest.py
@@ -283,7 +283,7 @@ def decision_function(self, X):
             positive scores represent inliers.
 
         """
-        # We substract self.offset_ to make 0 be the threshold value for being
+        # We subtract self.offset_ to make 0 be the threshold value for being
         # an outlier:
 
         return self.score_samples(X) - self.offset_
diff --git a/sklearn/ensemble/tests/test_forest.py b/sklearn/ensemble/tests/test_forest.py
index 551c811849a7..420d10bc10be 100644
--- a/sklearn/ensemble/tests/test_forest.py
+++ b/sklearn/ensemble/tests/test_forest.py
@@ -223,8 +223,8 @@ def check_importances(name, criterion, dtype, tolerance):
     # Check with parallel
     importances = est.feature_importances_
     est.set_params(n_jobs=2)
-    importances_parrallel = est.feature_importances_
-    assert_array_almost_equal(importances, importances_parrallel)
+    importances_parallel = est.feature_importances_
+    assert_array_almost_equal(importances, importances_parallel)
 
     # Check with sample weights
     sample_weight = check_random_state(0).randint(1, 10, len(X))
diff --git a/sklearn/ensemble/weight_boosting.py b/sklearn/ensemble/weight_boosting.py
index b75e607b5b46..f13d16befb14 100644
--- a/sklearn/ensemble/weight_boosting.py
+++ b/sklearn/ensemble/weight_boosting.py
@@ -26,7 +26,6 @@
 from abc import ABCMeta, abstractmethod
 
 import numpy as np
-from numpy.core.umath_tests import inner1d
 
 from .base import BaseEnsemble
 from ..base import ClassifierMixin, RegressorMixin, is_regressor, is_classifier
@@ -522,8 +521,8 @@ def _boost_real(self, iboost, X, y, sample_weight, random_state):
 
         # Boost weight using multi-class AdaBoost SAMME.R alg
         estimator_weight = (-1. * self.learning_rate
-                                * (((n_classes - 1.) / n_classes) *
-                                   inner1d(y_coding, np.log(y_predict_proba))))
+                            * ((n_classes - 1.) / n_classes)
+                            * (y_coding * np.log(y_predict_proba)).sum(axis=1))
 
         # Only boost the weights if it will fit again
         if not iboost == self.n_estimators - 1:
diff --git a/sklearn/externals/joblib/numpy_pickle_utils.py b/sklearn/externals/joblib/numpy_pickle_utils.py
index 7196c0cbc85c..27d759be77b2 100644
--- a/sklearn/externals/joblib/numpy_pickle_utils.py
+++ b/sklearn/externals/joblib/numpy_pickle_utils.py
@@ -101,7 +101,7 @@ def _detect_compressor(fileobj):
     # Read the magic number in the first bytes of the file.
     if hasattr(fileobj, 'peek'):
         # Peek allows to read those bytes without moving the cursor in the
-        # file whic.
+        # file which.
         first_bytes = fileobj.peek(_MAX_PREFIX_LEN)
     else:
         # Fallback to seek if the fileobject is not peekable.
diff --git a/sklearn/externals/joblib/pool.py b/sklearn/externals/joblib/pool.py
index 290363a36b55..ef3838e7e559 100644
--- a/sklearn/externals/joblib/pool.py
+++ b/sklearn/externals/joblib/pool.py
@@ -573,7 +573,7 @@ def _cleanup():
 
         if np is not None:
             # Register smart numpy.ndarray reducers that detects memmap backed
-            # arrays and that is alse able to dump to memmap large in-memory
+            # arrays and that is else able to dump to memmap large in-memory
             # arrays over the max_nbytes threshold
             if prewarm == "auto":
                 prewarm = not use_shared_mem
diff --git a/sklearn/feature_extraction/tests/test_text.py b/sklearn/feature_extraction/tests/test_text.py
index 22fbd3f17bea..ff4dd6bd86a7 100644
--- a/sklearn/feature_extraction/tests/test_text.py
+++ b/sklearn/feature_extraction/tests/test_text.py
@@ -270,7 +270,7 @@ def test_countvectorizer_custom_vocabulary_pipeline():
     assert_equal(X.shape[1], len(what_we_like))
 
 
-def test_countvectorizer_custom_vocabulary_repeated_indeces():
+def test_countvectorizer_custom_vocabulary_repeated_indices():
     vocab = {"pizza": 0, "beer": 0}
     try:
         CountVectorizer(vocabulary=vocab)
@@ -544,7 +544,9 @@ def test_feature_names():
 
     # test for Value error on unfitted/empty vocabulary
     assert_raises(ValueError, cv.get_feature_names)
+    assert_false(cv.fixed_vocabulary_)
 
+    # test for vocabulary learned from data
     X = cv.fit_transform(ALL_FOOD_DOCS)
     n_samples, n_features = X.shape
     assert_equal(len(cv.vocabulary_), n_features)
@@ -558,6 +560,19 @@ def test_feature_names():
     for idx, name in enumerate(feature_names):
         assert_equal(idx, cv.vocabulary_.get(name))
 
+    # test for custom vocabulary
+    vocab = ['beer', 'burger', 'celeri', 'coke', 'pizza',
+             'salad', 'sparkling', 'tomato', 'water']
+
+    cv = CountVectorizer(vocabulary=vocab)
+    feature_names = cv.get_feature_names()
+    assert_array_equal(['beer', 'burger', 'celeri', 'coke', 'pizza', 'salad',
+                        'sparkling', 'tomato', 'water'], feature_names)
+    assert_true(cv.fixed_vocabulary_)
+
+    for idx, name in enumerate(feature_names):
+        assert_equal(idx, cv.vocabulary_.get(name))
+
 
 def test_vectorizer_max_features():
     vec_factories = (
@@ -942,6 +957,35 @@ def test_pickling_transformer():
         orig.fit_transform(X).toarray())
 
 
+def test_transformer_idf_setter():
+    X = CountVectorizer().fit_transform(JUNK_FOOD_DOCS)
+    orig = TfidfTransformer().fit(X)
+    copy = TfidfTransformer()
+    copy.idf_ = orig.idf_
+    assert_array_equal(
+        copy.transform(X).toarray(),
+        orig.transform(X).toarray())
+
+
+def test_tfidf_vectorizer_setter():
+    orig = TfidfVectorizer(use_idf=True)
+    orig.fit(JUNK_FOOD_DOCS)
+    copy = TfidfVectorizer(vocabulary=orig.vocabulary_, use_idf=True)
+    copy.idf_ = orig.idf_
+    assert_array_equal(
+        copy.transform(JUNK_FOOD_DOCS).toarray(),
+        orig.transform(JUNK_FOOD_DOCS).toarray())
+
+
+def test_tfidfvectorizer_invalid_idf_attr():
+    vect = TfidfVectorizer(use_idf=True)
+    vect.fit(JUNK_FOOD_DOCS)
+    copy = TfidfVectorizer(vocabulary=vect.vocabulary_, use_idf=True)
+    expected_idf_len = len(vect.idf_)
+    invalid_idf = [1.0] * (expected_idf_len + 1)
+    assert_raises(ValueError, setattr, copy, 'idf_', invalid_idf)
+
+
 def test_non_unique_vocab():
     vocab = ['a', 'b', 'c', 'a', 'a']
     vect = CountVectorizer(vocabulary=vocab)
diff --git a/sklearn/feature_extraction/text.py b/sklearn/feature_extraction/text.py
index 9b1ebd6320b2..df0582d3d4f5 100644
--- a/sklearn/feature_extraction/text.py
+++ b/sklearn/feature_extraction/text.py
@@ -541,6 +541,26 @@ def transform(self, X):
             X = normalize(X, norm=self.norm, copy=False)
         return X
 
+    def fit_transform(self, X, y=None):
+        """Transform a sequence of documents to a document-term matrix.
+
+        Parameters
+        ----------
+        X : iterable over raw text documents, length = n_samples
+            Samples. Each sample must be a text document (either bytes or
+            unicode strings, file name or file object depending on the
+            constructor argument) which will be tokenized and hashed.
+        y : any
+            Ignored. This parameter exists only for compatibility with
+            sklearn.pipeline.Pipeline.
+
+        Returns
+        -------
+        X : scipy.sparse matrix, shape = (n_samples, self.n_features)
+            Document-term matrix.
+        """
+        return self.fit(X, y).transform(X)
+
     def _get_hasher(self):
         return FeatureHasher(n_features=self.n_features,
                              input_type='string', dtype=self.dtype,
@@ -993,6 +1013,9 @@ def inverse_transform(self, X):
 
     def get_feature_names(self):
         """Array mapping from feature integer indices to feature name"""
+        if not hasattr(self, 'vocabulary_'):
+            self._validate_vocabulary()
+
         self._check_vocabulary()
 
         return [t for t, i in sorted(six.iteritems(self.vocabulary_),
@@ -1062,6 +1085,12 @@ class TfidfTransformer(BaseEstimator, TransformerMixin):
     sublinear_tf : boolean, default=False
         Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).
 
+    Attributes
+    ----------
+    idf_ : array, shape (n_features)
+        The inverse document frequency (IDF) vector; only defined
+        if  ``use_idf`` is True.
+
     References
     ----------
 
@@ -1157,6 +1186,13 @@ def idf_(self):
         # which means hasattr(self, "idf_") is False
         return np.ravel(self._idf_diag.sum(axis=0))
 
+    @idf_.setter
+    def idf_(self, value):
+        value = np.asarray(value, dtype=np.float64)
+        n_features = value.shape[0]
+        self._idf_diag = sp.spdiags(value, diags=0, m=n_features,
+                                    n=n_features, format='csr')
+
 
 class TfidfVectorizer(CountVectorizer):
     """Convert a collection of raw documents to a matrix of TF-IDF features.
@@ -1295,9 +1331,9 @@ class TfidfVectorizer(CountVectorizer):
     vocabulary_ : dict
         A mapping of terms to feature indices.
 
-    idf_ : array, shape = [n_features], or None
-        The learned idf vector (global term weights)
-        when ``use_idf`` is set to True, None otherwise.
+    idf_ : array, shape (n_features)
+        The inverse document frequency (IDF) vector; only defined
+        if  ``use_idf`` is True.
 
     stop_words_ : set
         Terms that were ignored because they either:
@@ -1386,6 +1422,16 @@ def sublinear_tf(self, value):
     def idf_(self):
         return self._tfidf.idf_
 
+    @idf_.setter
+    def idf_(self, value):
+        self._validate_vocabulary()
+        if hasattr(self, 'vocabulary_'):
+            if len(self.vocabulary_) != len(value):
+                raise ValueError("idf length = %d must be equal "
+                                 "to vocabulary size = %d" %
+                                 (len(value), len(self.vocabulary)))
+        self._tfidf.idf_ = value
+
     def fit(self, raw_documents, y=None):
         """Learn vocabulary and idf from training set.
 
diff --git a/sklearn/gaussian_process/kernels.py b/sklearn/gaussian_process/kernels.py
index 009eb1d7718e..4581b87ba948 100644
--- a/sklearn/gaussian_process/kernels.py
+++ b/sklearn/gaussian_process/kernels.py
@@ -30,7 +30,7 @@
 from ..metrics.pairwise import pairwise_kernels
 from ..externals import six
 from ..base import clone
-from sklearn.externals.funcsigs import signature
+from ..utils.fixes import signature
 
 
 def _check_length_scale(X, length_scale):
diff --git a/sklearn/gaussian_process/tests/test_kernels.py b/sklearn/gaussian_process/tests/test_kernels.py
index a07a406a415d..8c40139480e1 100644
--- a/sklearn/gaussian_process/tests/test_kernels.py
+++ b/sklearn/gaussian_process/tests/test_kernels.py
@@ -3,10 +3,9 @@
 # Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
 # License: BSD 3 clause
 
-from sklearn.externals.funcsigs import signature
-
 import numpy as np
 
+from sklearn.utils.fixes import signature
 from sklearn.gaussian_process.kernels import _approx_fprime
 
 from sklearn.metrics.pairwise \
diff --git a/sklearn/impute.py b/sklearn/impute.py
index 69fba61f3d8f..fe772d6a3a0c 100644
--- a/sklearn/impute.py
+++ b/sklearn/impute.py
@@ -1,16 +1,23 @@
 """Transformers for missing value imputation"""
 # Authors: Nicolas Tresegnie <nicolas.tresegnie@gmail.com>
+#          Sergey Feldman <sergeyfeldman@gmail.com>
 # License: BSD 3 clause
 
+from __future__ import division
+
 import warnings
+from time import time
 
 import numpy as np
 import numpy.ma as ma
 from scipy import sparse
 from scipy import stats
+from collections import namedtuple
 
 from .base import BaseEstimator, TransformerMixin
-from .utils import check_array
+from .base import clone
+from .preprocessing import normalize
+from .utils import check_array, check_random_state, safe_indexing
 from .utils.sparsefuncs import _get_median
 from .utils.validation import check_is_fitted
 from .utils.validation import FLOAT_DTYPES
@@ -20,8 +27,13 @@
 zip = six.moves.zip
 map = six.moves.map
 
+MICETriplet = namedtuple('MICETriplet', ['feat_idx',
+                                         'neighbor_feat_idx',
+                                         'predictor'])
+
 __all__ = [
     'SimpleImputer',
+    'MICEImputer',
 ]
 
 
@@ -77,17 +89,11 @@ class SimpleImputer(BaseEstimator, TransformerMixin):
         The imputation strategy.
 
         - If "mean", then replace missing values using the mean along
-          the axis.
+          each column.
         - If "median", then replace missing values using the median along
-          the axis.
+          each column.
         - If "most_frequent", then replace missing using the most frequent
-          value along the axis.
-
-    axis : integer, optional (default=0)
-        The axis along which to impute.
-
-        - If `axis=0`, then impute along columns.
-        - If `axis=1`, then impute along rows.
+          value along each column.
 
     verbose : integer, optional (default=0)
         Controls the verbosity of the imputer.
@@ -99,27 +105,23 @@ class SimpleImputer(BaseEstimator, TransformerMixin):
 
         - If X is not an array of floating values;
         - If X is sparse and `missing_values=0`;
-        - If `axis=0` and X is encoded as a CSR matrix;
-        - If `axis=1` and X is encoded as a CSC matrix.
+        - If X is encoded as a CSR matrix.
 
     Attributes
     ----------
     statistics_ : array of shape (n_features,)
-        The imputation fill value for each feature if axis == 0.
+        The imputation fill value for each feature.
 
     Notes
     -----
-    - When ``axis=0``, columns which only contained missing values at `fit`
-      are discarded upon `transform`.
-    - When ``axis=1``, an exception is raised if there are rows for which it is
-      not possible to fill in the missing values (e.g., because they only
-      contain missing values).
+    Columns which only contained missing values at `fit` are discarded upon
+    `transform`.
+
     """
     def __init__(self, missing_values="NaN", strategy="mean",
-                 axis=0, verbose=0, copy=True):
+                 verbose=0, copy=True):
         self.missing_values = missing_values
         self.strategy = strategy
-        self.axis = axis
         self.verbose = verbose
         self.copy = copy
 
@@ -143,46 +145,29 @@ def fit(self, X, y=None):
                              " got strategy={1}".format(allowed_strategies,
                                                         self.strategy))
 
-        if self.axis not in [0, 1]:
-            raise ValueError("Can only impute missing values on axis 0 and 1, "
-                             " got axis={0}".format(self.axis))
-
-        # Since two different arrays can be provided in fit(X) and
-        # transform(X), the imputation data will be computed in transform()
-        # when the imputation is done per sample (i.e., when axis=1).
-        if self.axis == 0:
-            X = check_array(X, accept_sparse='csc', dtype=FLOAT_DTYPES,
-                            force_all_finite='allow-nan'
-                            if self.missing_values == 'NaN'
-                            or np.isnan(self.missing_values) else True)
+        X = check_array(X, accept_sparse='csc', dtype=FLOAT_DTYPES,
+                        force_all_finite='allow-nan'
+                        if self.missing_values == 'NaN'
+                        or np.isnan(self.missing_values) else True)
 
-            if sparse.issparse(X):
-                self.statistics_ = self._sparse_fit(X,
-                                                    self.strategy,
-                                                    self.missing_values,
-                                                    self.axis)
-            else:
-                self.statistics_ = self._dense_fit(X,
-                                                   self.strategy,
-                                                   self.missing_values,
-                                                   self.axis)
+        if sparse.issparse(X):
+            self.statistics_ = self._sparse_fit(X,
+                                                self.strategy,
+                                                self.missing_values)
+        else:
+            self.statistics_ = self._dense_fit(X,
+                                               self.strategy,
+                                               self.missing_values)
 
         return self
 
-    def _sparse_fit(self, X, strategy, missing_values, axis):
+    def _sparse_fit(self, X, strategy, missing_values):
         """Fit the transformer on sparse data."""
-        # Imputation is done "by column", so if we want to do it
-        # by row we only need to convert the matrix to csr format.
-        if axis == 1:
-            X = X.tocsr()
-        else:
-            X = X.tocsc()
-
         # Count the zeros
         if missing_values == 0:
-            n_zeros_axis = np.zeros(X.shape[not axis], dtype=int)
+            n_zeros_axis = np.zeros(X.shape[1], dtype=int)
         else:
-            n_zeros_axis = X.shape[axis] - np.diff(X.indptr)
+            n_zeros_axis = X.shape[0] - np.diff(X.indptr)
 
         # Mean
         if strategy == "mean":
@@ -209,7 +194,7 @@ def _sparse_fit(self, X, strategy, missing_values, axis):
                 n_non_missing = np.add(n_non_missing, s)
 
             else:
-                sums = X.sum(axis=axis)
+                sums = X.sum(axis=0)
                 n_non_missing = np.diff(X.indptr)
 
             # Ignore the error, columns with a np.nan statistics_
@@ -249,7 +234,7 @@ def _sparse_fit(self, X, strategy, missing_values, axis):
 
                 return most_frequent
 
-    def _dense_fit(self, X, strategy, missing_values, axis):
+    def _dense_fit(self, X, strategy, missing_values):
         """Fit the transformer on dense data."""
         X = check_array(X, force_all_finite='allow-nan'
                         if self.missing_values == 'NaN'
@@ -259,7 +244,7 @@ def _dense_fit(self, X, strategy, missing_values, axis):
 
         # Mean
         if strategy == "mean":
-            mean_masked = np.ma.mean(masked_X, axis=axis)
+            mean_masked = np.ma.mean(masked_X, axis=0)
             # Avoid the warning "Warning: converting a masked element to nan."
             mean = np.ma.getdata(mean_masked)
             mean[np.ma.getmask(mean_masked)] = np.nan
@@ -268,7 +253,7 @@ def _dense_fit(self, X, strategy, missing_values, axis):
 
         # Median
         elif strategy == "median":
-            median_masked = np.ma.median(masked_X, axis=axis)
+            median_masked = np.ma.median(masked_X, axis=0)
             # Avoid the warning "Warning: converting a masked element to nan."
             median = np.ma.getdata(median_masked)
             median[np.ma.getmaskarray(median_masked)] = np.nan
@@ -283,9 +268,8 @@ def _dense_fit(self, X, strategy, missing_values, axis):
             # See https://github.com/scipy/scipy/issues/2636
 
             # To be able access the elements by columns
-            if axis == 0:
-                X = X.transpose()
-                mask = mask.transpose()
+            X = X.transpose()
+            mask = mask.transpose()
 
             most_frequent = np.empty(X.shape[0])
 
@@ -304,55 +288,29 @@ def transform(self, X):
         X : {array-like, sparse matrix}, shape = [n_samples, n_features]
             The input data to complete.
         """
-        if self.axis == 0:
-            check_is_fitted(self, 'statistics_')
-            X = check_array(X, accept_sparse='csc', dtype=FLOAT_DTYPES,
-                            force_all_finite='allow-nan'
-                            if self.missing_values == 'NaN'
-                            or np.isnan(self.missing_values) else True,
-                            copy=self.copy)
-            statistics = self.statistics_
-            if X.shape[1] != statistics.shape[0]:
-                raise ValueError("X has %d features per sample, expected %d"
-                                 % (X.shape[1], self.statistics_.shape[0]))
-
-        # Since two different arrays can be provided in fit(X) and
-        # transform(X), the imputation data need to be recomputed
-        # when the imputation is done per sample
-        else:
-            X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES,
-                            force_all_finite='allow-nan'
-                            if self.missing_values == 'NaN'
-                            or np.isnan(self.missing_values) else True,
-                            copy=self.copy)
-
-            if sparse.issparse(X):
-                statistics = self._sparse_fit(X,
-                                              self.strategy,
-                                              self.missing_values,
-                                              self.axis)
-
-            else:
-                statistics = self._dense_fit(X,
-                                             self.strategy,
-                                             self.missing_values,
-                                             self.axis)
-
-        # Delete the invalid rows/columns
+        check_is_fitted(self, 'statistics_')
+        X = check_array(X, accept_sparse='csc', dtype=FLOAT_DTYPES,
+                        force_all_finite='allow-nan'
+                        if self.missing_values == 'NaN'
+                        or np.isnan(self.missing_values) else True,
+                        copy=self.copy)
+        statistics = self.statistics_
+        if X.shape[1] != statistics.shape[0]:
+            raise ValueError("X has %d features per sample, expected %d"
+                             % (X.shape[1], self.statistics_.shape[0]))
+
+        # Delete the invalid columns
         invalid_mask = np.isnan(statistics)
         valid_mask = np.logical_not(invalid_mask)
         valid_statistics = statistics[valid_mask]
         valid_statistics_indexes = np.flatnonzero(valid_mask)
-        missing = np.arange(X.shape[not self.axis])[invalid_mask]
+        missing = np.arange(X.shape[1])[invalid_mask]
 
-        if self.axis == 0 and invalid_mask.any():
+        if invalid_mask.any():
             if self.verbose:
                 warnings.warn("Deleting features without "
                               "observed values: %s" % missing)
             X = X[:, valid_statistics_indexes]
-        elif self.axis == 1 and invalid_mask.any():
-            raise ValueError("Some rows only contain "
-                             "missing values: %s" % missing)
 
         # Do actual imputation
         if sparse.issparse(X) and self.missing_values != 0:
@@ -367,14 +325,551 @@ def transform(self, X):
                 X = X.toarray()
 
             mask = _get_mask(X, self.missing_values)
-            n_missing = np.sum(mask, axis=self.axis)
+            n_missing = np.sum(mask, axis=0)
             values = np.repeat(valid_statistics, n_missing)
 
-            if self.axis == 0:
-                coordinates = np.where(mask.transpose())[::-1]
-            else:
-                coordinates = mask
+            coordinates = np.where(mask.transpose())[::-1]
 
             X[coordinates] = values
 
         return X
+
+
+class MICEImputer(BaseEstimator, TransformerMixin):
+    """MICE transformer to impute missing values.
+
+    Basic implementation of MICE (Multivariate Imputations by Chained
+    Equations) package from R. This version assumes all of the features are
+    Gaussian.
+
+    Read more in the :ref:`User Guide <mice>`.
+
+    Parameters
+    ----------
+    missing_values : int or "NaN", optional (default="NaN")
+        The placeholder for the missing values. All occurrences of
+        ``missing_values`` will be imputed. For missing values encoded as
+        np.nan, use the string value "NaN".
+
+    imputation_order : str, optional (default="ascending")
+        The order in which the features will be imputed. Possible values:
+
+        "ascending"
+            From features with fewest missing values to most.
+        "descending"
+            From features with most missing values to fewest.
+        "roman"
+            Left to right.
+        "arabic"
+            Right to left.
+        "random"
+            A random order for each round.
+
+    n_imputations : int, optional (default=100)
+        Number of MICE rounds to perform, the results of which will be
+        used in the final average.
+
+    n_burn_in : int, optional (default=10)
+        Number of initial MICE rounds to perform the results of which
+        will not be returned.
+
+    predictor : estimator object, default=BayesianRidge()
+        The predictor to use at each step of the round-robin imputation.
+        It must support ``return_std`` in its ``predict`` method.
+
+    n_nearest_features : int, optional (default=None)
+        Number of other features to use to estimate the missing values of
+        the each feature column. Nearness between features is measured using
+        the absolute correlation coefficient between each feature pair (after
+        initial imputation). Can provide significant speed-up when the number
+        of features is huge. If ``None``, all features will be used.
+
+    initial_strategy : str, optional (default="mean")
+        Which strategy to use to initialize the missing values. Same as the
+        ``strategy`` parameter in :class:`sklearn.preprocessing.Imputer`
+        Valid values: {"mean", "median", or "most_frequent"}.
+
+    min_value : float, optional (default=None)
+        Minimum possible imputed value. Default of ``None`` will set minimum
+        to negative infinity.
+
+    max_value : float, optional (default=None)
+        Maximum possible imputed value. Default of ``None`` will set maximum
+        to positive infinity.
+
+    verbose : int, optional (default=0)
+        Verbosity flag, controls the debug messages that are issued
+        as functions are evaluated. The higher, the more verbose. Can be 0, 1,
+        or 2.
+
+    random_state : int, RandomState instance or None, optional (default=None)
+        The seed of the pseudo random number generator to use when shuffling
+        the data.  If int, random_state is the seed used by the random number
+        generator; If RandomState instance, random_state is the random number
+        generator; If None, the random number generator is the RandomState
+        instance used by ``np.random``.
+
+    Attributes
+    ----------
+    initial_imputer_ : object of class :class:`sklearn.preprocessing.Imputer`'
+        The imputer used to initialize the missing values.
+
+    imputation_sequence_ : list of tuples
+        Each tuple has ``(feat_idx, neighbor_feat_idx, predictor)``, where
+        ``feat_idx`` is the current feature to be imputed,
+        ``neighbor_feat_idx`` is the array of other features used to impute the
+        current feature, and ``predictor`` is the trained predictor used for
+        the imputation.
+
+    Notes
+    -----
+    The R version of MICE does not have inductive functionality, i.e. first
+    fitting on ``X_train`` and then transforming any ``X_test`` without
+    additional fitting. We do this by storing each feature's predictor during
+    the round-robin ``fit`` phase, and predicting without refitting (in order)
+    during the ``transform`` phase.
+
+    Features which contain all missing values at ``fit`` are discarded upon
+    ``transform``.
+
+    Features with missing values in transform which did not have any missing
+    values in fit will be imputed with the initial imputation method only.
+
+    References
+    ----------
+    .. [1] `Stef van Buuren, Karin Groothuis-Oudshoorn (2011). "mice:
+        Multivariate Imputation by Chained Equations in R". Journal of
+        Statistical Software 45: 1-67.
+        <https://www.jstatsoft.org/article/view/v045i03>`_
+    """
+
+    def __init__(self,
+                 missing_values='NaN',
+                 imputation_order='ascending',
+                 n_imputations=100,
+                 n_burn_in=10,
+                 predictor=None,
+                 n_nearest_features=None,
+                 initial_strategy="mean",
+                 min_value=None,
+                 max_value=None,
+                 verbose=False,
+                 random_state=None):
+
+        self.missing_values = missing_values
+        self.imputation_order = imputation_order
+        self.n_imputations = n_imputations
+        self.n_burn_in = n_burn_in
+        self.predictor = predictor
+        self.n_nearest_features = n_nearest_features
+        self.initial_strategy = initial_strategy
+        self.min_value = min_value
+        self.max_value = max_value
+        self.verbose = verbose
+        self.random_state = random_state
+
+    def _impute_one_feature(self,
+                            X_filled,
+                            mask_missing_values,
+                            feat_idx,
+                            neighbor_feat_idx,
+                            predictor=None,
+                            fit_mode=True):
+        """Impute a single feature from the others provided.
+
+        This function predicts the missing values of one of the features using
+        the current estimates of all the other features. The ``predictor`` must
+        support ``return_std=True`` in its ``predict`` method for this function
+        to work.
+
+        Parameters
+        ----------
+        X_filled : ndarray
+            Input data with the most recent imputations.
+
+        mask_missing_values : ndarray
+            Input data's missing indicator matrix.
+
+        feat_idx : int
+            Index of the feature currently being imputed.
+
+        neighbor_feat_idx : ndarray
+            Indices of the features to be used in imputing ``feat_idx``.
+
+        predictor : object
+            The predictor to use at this step of the round-robin imputation.
+            It must support ``return_std`` in its ``predict`` method.
+            If None, it will be cloned from self._predictor.
+
+        fit_mode : boolean, default=True
+            Whether to fit and predict with the predictor or just predict.
+
+        Returns
+        -------
+        X_filled : ndarray
+            Input data with ``X_filled[missing_row_mask, feat_idx]`` updated.
+
+        predictor : predictor with sklearn API
+            The fitted predictor used to impute
+            ``X_filled[missing_row_mask, feat_idx]``.
+        """
+
+        # if nothing is missing, just return the default
+        # (should not happen at fit time because feat_ids would be excluded)
+        missing_row_mask = mask_missing_values[:, feat_idx]
+        if not np.any(missing_row_mask):
+            return X_filled, predictor
+
+        if predictor is None and fit_mode is False:
+            raise ValueError("If fit_mode is False, then an already-fitted "
+                             "predictor should be passed in.")
+
+        if predictor is None:
+            predictor = clone(self._predictor)
+
+        if fit_mode:
+            X_train = safe_indexing(X_filled[:, neighbor_feat_idx],
+                                    ~missing_row_mask)
+            y_train = safe_indexing(X_filled[:, feat_idx],
+                                    ~missing_row_mask)
+            predictor.fit(X_train, y_train)
+
+        # get posterior samples
+        X_test = safe_indexing(X_filled[:, neighbor_feat_idx],
+                               missing_row_mask)
+        mus, sigmas = predictor.predict(X_test, return_std=True)
+        good_sigmas = sigmas > 0
+        imputed_values = np.zeros(mus.shape, dtype=X_filled.dtype)
+        imputed_values[~good_sigmas] = mus[~good_sigmas]
+        imputed_values[good_sigmas] = self.random_state_.normal(
+            loc=mus[good_sigmas], scale=sigmas[good_sigmas])
+
+        # clip the values
+        imputed_values = np.clip(imputed_values,
+                                 self._min_value,
+                                 self._max_value)
+
+        # update the feature
+        X_filled[missing_row_mask, feat_idx] = imputed_values
+        return X_filled, predictor
+
+    def _get_neighbor_feat_idx(self,
+                               n_features,
+                               feat_idx,
+                               abs_corr_mat):
+        """Get a list of other features to predict ``feat_idx``.
+
+        If self.n_nearest_features is less than or equal to the total
+        number of features, then use a probability proportional to the absolute
+        correlation between ``feat_idx`` and each other feature to randomly
+        choose a subsample of the other features (without replacement).
+
+        Parameters
+        ----------
+        n_features : int
+            Number of features in ``X``.
+
+        feat_idx : int
+            Index of the feature currently being imputed.
+
+        abs_corr_mat : ndarray, shape (n_features, n_features)
+            Absolute correlation matrix of ``X``. The diagonal has been zeroed
+            out and each feature has been normalized to sum to 1. Can be None.
+
+        Returns
+        -------
+        neighbor_feat_idx : array-like
+            The features to use to impute ``feat_idx``.
+        """
+        if (self.n_nearest_features is not None and
+                self.n_nearest_features < n_features):
+            p = abs_corr_mat[:, feat_idx]
+            neighbor_feat_idx = self.random_state_.choice(
+                np.arange(n_features), self.n_nearest_features, replace=False,
+                p=p)
+        else:
+            inds_left = np.arange(feat_idx)
+            inds_right = np.arange(feat_idx + 1, n_features)
+            neighbor_feat_idx = np.concatenate((inds_left, inds_right))
+        return neighbor_feat_idx
+
+    def _get_ordered_idx(self, mask_missing_values):
+        """Decide in what order we will update the features.
+
+        As a homage to the MICE R package, we will have 4 main options of
+        how to order the updates, and use a random order if anything else
+        is specified.
+
+        Also, this function skips features which have no missing values.
+
+        Parameters
+        ----------
+        mask_missing_values : array-like, shape (n_samples, n_features)
+            Input data's missing indicator matrix, where "n_samples" is the
+            number of samples and "n_features" is the number of features.
+
+        Returns
+        -------
+        ordered_idx : ndarray, shape (n_features,)
+            The order in which to impute the features.
+        """
+        frac_of_missing_values = mask_missing_values.mean(axis=0)
+        missing_values_idx = np.nonzero(frac_of_missing_values)[0]
+        if self.imputation_order == 'roman':
+            ordered_idx = missing_values_idx
+        elif self.imputation_order == 'arabic':
+            ordered_idx = missing_values_idx[::-1]
+        elif self.imputation_order == 'ascending':
+            n = len(frac_of_missing_values) - len(missing_values_idx)
+            ordered_idx = np.argsort(frac_of_missing_values,
+                                     kind='mergesort')[n:][::-1]
+        elif self.imputation_order == 'descending':
+            n = len(frac_of_missing_values) - len(missing_values_idx)
+            ordered_idx = np.argsort(frac_of_missing_values,
+                                     kind='mergesort')[n:]
+        elif self.imputation_order == 'random':
+            ordered_idx = missing_values_idx
+            self.random_state_.shuffle(ordered_idx)
+        else:
+            raise ValueError("Got an invalid imputation order: '{0}'. It must "
+                             "be one of the following: 'roman', 'arabic', "
+                             "'ascending', 'descending', or "
+                             "'random'.".format(self.imputation_order))
+        return ordered_idx
+
+    def _get_abs_corr_mat(self, X_filled, tolerance=1e-6):
+        """Get absolute correlation matrix between features.
+
+        Parameters
+        ----------
+        X_filled : ndarray, shape (n_samples, n_features)
+            Input data with the most recent imputations.
+
+        tolerance : float, optional (default=1e-6)
+            ``abs_corr_mat`` can have nans, which will be replaced
+            with ``tolerance``.
+
+        Returns
+        -------
+        abs_corr_mat : ndarray, shape (n_features, n_features)
+            Absolute correlation matrix of ``X`` at the beginning of the
+            current round. The diagonal has been zeroed out and each feature's
+            absolute correlations with all others have been normalized to sum
+            to 1.
+        """
+        n_features = X_filled.shape[1]
+        if (self.n_nearest_features is None or
+                self.n_nearest_features >= n_features):
+            return None
+        abs_corr_mat = np.abs(np.corrcoef(X_filled.T))
+        # np.corrcoef is not defined for features with zero std
+        abs_corr_mat[np.isnan(abs_corr_mat)] = tolerance
+        # ensures exploration, i.e. at least some probability of sampling
+        abs_corr_mat[abs_corr_mat < tolerance] = tolerance
+        # features are not their own neighbors
+        np.fill_diagonal(abs_corr_mat, 0)
+        # needs to sum to 1 for np.random.choice sampling
+        abs_corr_mat = normalize(abs_corr_mat, norm='l1', axis=0, copy=False)
+        return abs_corr_mat
+
+    def _initial_imputation(self, X):
+        """Perform initial imputation for input X.
+
+        Parameters
+        ----------
+        X : ndarray, shape (n_samples, n_features)
+            Input data, where "n_samples" is the number of samples and
+            "n_features" is the number of features.
+
+        Returns
+        -------
+        Xt : ndarray, shape (n_samples, n_features)
+            Input data, where "n_samples" is the number of samples and
+            "n_features" is the number of features.
+
+        X_filled : ndarray, shape (n_samples, n_features)
+            Input data with the most recent imputations.
+
+        mask_missing_values : ndarray, shape (n_samples, n_features)
+            Input data's missing indicator matrix, where "n_samples" is the
+            number of samples and "n_features" is the number of features.
+        """
+        X = check_array(X, dtype=FLOAT_DTYPES, order="F",
+                        force_all_finite='allow-nan'
+                        if self.missing_values == 'NaN'
+                        or np.isnan(self.missing_values) else True)
+
+        mask_missing_values = _get_mask(X, self.missing_values)
+        if self.initial_imputer_ is None:
+            self.initial_imputer_ = SimpleImputer(
+                                            missing_values=self.missing_values,
+                                            strategy=self.initial_strategy)
+            X_filled = self.initial_imputer_.fit_transform(X)
+        else:
+            X_filled = self.initial_imputer_.transform(X)
+
+        valid_mask = np.flatnonzero(np.logical_not(
+            np.isnan(self.initial_imputer_.statistics_)))
+        Xt = X[:, valid_mask]
+        mask_missing_values = mask_missing_values[:, valid_mask]
+
+        return Xt, X_filled, mask_missing_values
+
+    def fit_transform(self, X, y=None):
+        """Fits the imputer on X and return the transformed X.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+            Input data, where "n_samples" is the number of samples and
+            "n_features" is the number of features.
+
+        y : ignored.
+
+        Returns
+        -------
+        Xt : array-like, shape (n_samples, n_features)
+             The imputed input data.
+        """
+        self.random_state_ = getattr(self, "random_state_",
+                                     check_random_state(self.random_state))
+
+        if self.predictor is None:
+            from .linear_model import BayesianRidge
+            self._predictor = BayesianRidge()
+        else:
+            self._predictor = clone(self.predictor)
+
+        self._min_value = np.nan if self.min_value is None else self.min_value
+        self._max_value = np.nan if self.max_value is None else self.max_value
+
+        self.initial_imputer_ = None
+        X, X_filled, mask_missing_values = self._initial_imputation(X)
+
+        # edge case: in case the user specifies 0 for n_imputations,
+        # then there is no need to do burn in and the result should be
+        # just the initial imputation (before clipping)
+        if self.n_imputations < 1:
+            return X_filled
+
+        X_filled = np.clip(X_filled, self._min_value, self._max_value)
+
+        # order in which to impute
+        # note this is probably too slow for large feature data (d > 100000)
+        # and a better way would be good.
+        # see: https://goo.gl/KyCNwj and subsequent comments
+        ordered_idx = self._get_ordered_idx(mask_missing_values)
+
+        abs_corr_mat = self._get_abs_corr_mat(X_filled)
+
+        # impute data
+        n_rounds = self.n_burn_in + self.n_imputations
+        n_samples, n_features = X_filled.shape
+        Xt = np.zeros((n_samples, n_features), dtype=X.dtype)
+        self.imputation_sequence_ = []
+        if self.verbose > 0:
+            print("[MICE] Completing matrix with shape %s" % (X.shape,))
+        start_t = time()
+        for i_rnd in range(n_rounds):
+            if self.imputation_order == 'random':
+                ordered_idx = self._get_ordered_idx(mask_missing_values)
+
+            for feat_idx in ordered_idx:
+                neighbor_feat_idx = self._get_neighbor_feat_idx(n_features,
+                                                                feat_idx,
+                                                                abs_corr_mat)
+                X_filled, predictor = self._impute_one_feature(
+                    X_filled, mask_missing_values, feat_idx, neighbor_feat_idx,
+                    predictor=None, fit_mode=True)
+                predictor_triplet = MICETriplet(feat_idx,
+                                                neighbor_feat_idx,
+                                                predictor)
+                self.imputation_sequence_.append(predictor_triplet)
+
+            if i_rnd >= self.n_burn_in:
+                Xt += X_filled
+            if self.verbose > 0:
+                print('[MICE] Ending imputation round '
+                      '%d/%d, elapsed time %0.2f'
+                      % (i_rnd + 1, n_rounds, time() - start_t))
+
+        Xt /= self.n_imputations
+        Xt[~mask_missing_values] = X[~mask_missing_values]
+        return Xt
+
+    def transform(self, X):
+        """Imputes all missing values in X.
+
+        Note that this is stochastic, and that if random_state is not fixed,
+        repeated calls, or permuted input, will yield different results.
+
+        Parameters
+        ----------
+        X : array-like, shape = [n_samples, n_features]
+            The input data to complete.
+
+        Returns
+        -------
+        Xt : array-like, shape (n_samples, n_features)
+             The imputed input data.
+        """
+        check_is_fitted(self, 'initial_imputer_')
+
+        X, X_filled, mask_missing_values = self._initial_imputation(X)
+
+        # edge case: in case the user specifies 0 for n_imputations,
+        # then there is no need to do burn in and the result should be
+        # just the initial imputation (before clipping)
+        if self.n_imputations < 1:
+            return X_filled
+
+        X_filled = np.clip(X_filled, self._min_value, self._max_value)
+
+        n_rounds = self.n_burn_in + self.n_imputations
+        n_imputations = len(self.imputation_sequence_)
+        imputations_per_round = n_imputations // n_rounds
+        i_rnd = 0
+        Xt = np.zeros(X.shape, dtype=X.dtype)
+        if self.verbose > 0:
+            print("[MICE] Completing matrix with shape %s" % (X.shape,))
+        start_t = time()
+        for it, predictor_triplet in enumerate(self.imputation_sequence_):
+            X_filled, _ = self._impute_one_feature(
+                X_filled,
+                mask_missing_values,
+                predictor_triplet.feat_idx,
+                predictor_triplet.neighbor_feat_idx,
+                predictor=predictor_triplet.predictor,
+                fit_mode=False
+            )
+            if not (it + 1) % imputations_per_round:
+                if i_rnd >= self.n_burn_in:
+                    Xt += X_filled
+                if self.verbose > 1:
+                    print('[MICE] Ending imputation round '
+                          '%d/%d, elapsed time %0.2f'
+                          % (i_rnd + 1, n_rounds, time() - start_t))
+                i_rnd += 1
+
+        Xt /= self.n_imputations
+        Xt[~mask_missing_values] = X[~mask_missing_values]
+        return Xt
+
+    def fit(self, X, y=None):
+        """Fits the imputer on X and return self.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+            Input data, where "n_samples" is the number of samples and
+            "n_features" is the number of features.
+
+        y : ignored
+
+        Returns
+        -------
+        self : object
+            Returns self.
+        """
+        self.fit_transform(X)
+        return self
diff --git a/sklearn/linear_model/cd_fast.pyx b/sklearn/linear_model/cd_fast.pyx
index 6641a8f96628..4fbfe8b2489d 100644
--- a/sklearn/linear_model/cd_fast.pyx
+++ b/sklearn/linear_model/cd_fast.pyx
@@ -480,8 +480,9 @@ def sparse_enet_coordinate_descent(floating [:] w,
                 if d_w_ii > d_w_max:
                     d_w_max = d_w_ii
 
-                if w[ii] > w_max:
-                    w_max = w[ii]
+                if fabs(w[ii]) > w_max:
+                    w_max = fabs(w[ii])
+
             if w_max == 0.0 or d_w_max / w_max < d_w_tol or n_iter == max_iter - 1:
                 # the biggest coordinate update of this iteration was smaller than
                 # the tolerance: check the duality gap as ultimate stopping
diff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py
index 26eaeaa029f1..3e8a104d57d7 100644
--- a/sklearn/linear_model/logistic.py
+++ b/sklearn/linear_model/logistic.py
@@ -707,7 +707,7 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
                 func, w0, fprime=None,
                 args=(X, target, 1. / C, sample_weight),
                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
-            if info["warnflag"] == 1 and verbose > 0:
+            if info["warnflag"] == 1:
                 warnings.warn("lbfgs failed to converge. Increase the number "
                               "of iterations.", ConvergenceWarning)
             # In scipy <= 1.0.0, nit may exceed maxiter.
@@ -1034,17 +1034,17 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,
         instance used by `np.random`. Used when ``solver`` == 'sag' or
         'liblinear'.
 
-    solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},
+    solver : str, {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},
         default: 'liblinear'
         Algorithm to use in the optimization problem.
 
         - For small datasets, 'liblinear' is a good choice, whereas 'sag' and
-            'saga' are faster for large ones.
+          'saga' are faster for large ones.
         - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'
-            handle multinomial loss; 'liblinear' is limited to one-versus-rest
-            schemes.
+          handle multinomial loss; 'liblinear' is limited to one-versus-rest
+          schemes.
         - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas
-            'liblinear' and 'saga' handle L1 penalty.
+          'liblinear' and 'saga' handle L1 penalty.
 
         Note that 'sag' and 'saga' fast convergence is only guaranteed on
         features with approximately the same scale. You can
@@ -1421,19 +1421,19 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,
         that can be used, look at :mod:`sklearn.metrics`. The
         default scoring option used is 'accuracy'.
 
-    solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},
+    solver : str, {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},
         default: 'lbfgs'
         Algorithm to use in the optimization problem.
 
         - For small datasets, 'liblinear' is a good choice, whereas 'sag' and
-            'saga' are faster for large ones.
+          'saga' are faster for large ones.
         - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'
-            handle multinomial loss; 'liblinear' is limited to one-versus-rest
-            schemes.
+          handle multinomial loss; 'liblinear' is limited to one-versus-rest
+          schemes.
         - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas
-            'liblinear' and 'saga' handle L1 penalty.
+          'liblinear' and 'saga' handle L1 penalty.
         - 'liblinear' might be slower in LogisticRegressionCV because it does
-            not handle warm-starting.
+          not handle warm-starting.
 
         Note that 'sag' and 'saga' fast convergence is only guaranteed on
         features with approximately the same scale. You can preprocess the data
diff --git a/sklearn/linear_model/perceptron.py b/sklearn/linear_model/perceptron.py
index 2ecb5af96b90..a09663e4873f 100644
--- a/sklearn/linear_model/perceptron.py
+++ b/sklearn/linear_model/perceptron.py
@@ -97,8 +97,9 @@ class Perceptron(BaseSGDClassifier):
     Notes
     -----
 
-    `Perceptron` and `SGDClassifier` share the same underlying implementation.
-    In fact, `Perceptron()` is equivalent to `SGDClassifier(loss="perceptron",
+    ``Perceptron`` is a classification algorithm which shares the same
+    underlying implementation with ``SGDClassifier``. In fact,
+    ``Perceptron()`` is equivalent to `SGDClassifier(loss="perceptron",
     eta0=1, learning_rate="constant", penalty=None)`.
 
     See also
diff --git a/sklearn/linear_model/sgd_fast.pyx b/sklearn/linear_model/sgd_fast.pyx
index 08e60a11bb6e..384ad25673be 100644
--- a/sklearn/linear_model/sgd_fast.pyx
+++ b/sklearn/linear_model/sgd_fast.pyx
@@ -386,8 +386,8 @@ def plain_sgd(np.ndarray[double, ndim=1, mode='c'] weights,
         (1) constant, eta = eta0
         (2) optimal, eta = 1.0/(alpha * t).
         (3) inverse scaling, eta = eta0 / pow(t, power_t)
-        (4) Passive Agressive-I, eta = min(alpha, loss/norm(x))
-        (5) Passive Agressive-II, eta = 1.0 / (norm(x) + 0.5*alpha)
+        (4) Passive Aggressive-I, eta = min(alpha, loss/norm(x))
+        (5) Passive Aggressive-II, eta = 1.0 / (norm(x) + 0.5*alpha)
     eta0 : double
         The initial learning rate.
     power_t : double
diff --git a/sklearn/linear_model/stochastic_gradient.py b/sklearn/linear_model/stochastic_gradient.py
index b3f8a34fe6a8..c6ad4c2754c5 100644
--- a/sklearn/linear_model/stochastic_gradient.py
+++ b/sklearn/linear_model/stochastic_gradient.py
@@ -802,8 +802,6 @@ def __init__(self, loss="hinge", penalty='l2', alpha=0.0001, l1_ratio=0.15,
             average=average, n_iter=n_iter)
 
     def _check_proba(self):
-        check_is_fitted(self, "t_")
-
         if self.loss not in ("log", "modified_huber"):
             raise AttributeError("probability estimates are not available for"
                                  " loss=%r" % self.loss)
@@ -848,6 +846,8 @@ def predict_proba(self):
         return self._predict_proba
 
     def _predict_proba(self, X):
+        check_is_fitted(self, "t_")
+
         if self.loss == "log":
             return self._predict_proba_lr(X)
 
diff --git a/sklearn/linear_model/tests/test_logistic.py b/sklearn/linear_model/tests/test_logistic.py
index 5812fd5ecf7f..a179c89e199a 100644
--- a/sklearn/linear_model/tests/test_logistic.py
+++ b/sklearn/linear_model/tests/test_logistic.py
@@ -800,15 +800,6 @@ def test_logistic_regression_class_weights():
         assert_array_almost_equal(clf1.coef_, clf2.coef_, decimal=6)
 
 
-def test_logistic_regression_convergence_warnings():
-    # Test that warnings are raised if model does not converge
-
-    X, y = make_classification(n_samples=20, n_features=20, random_state=0)
-    clf_lib = LogisticRegression(solver='liblinear', max_iter=2, verbose=1)
-    assert_warns(ConvergenceWarning, clf_lib.fit, X, y)
-    assert_equal(clf_lib.n_iter_, 2)
-
-
 def test_logistic_regression_multinomial():
     # Tests for the multinomial option in logistic regression
 
@@ -1033,7 +1024,6 @@ def test_logreg_predict_proba_multinomial():
     assert_greater(clf_wrong_loss, clf_multi_loss)
 
 
-@ignore_warnings
 def test_max_iter():
     # Test that the maximum number of iteration is reached
     X, y_bin = iris.data, iris.target.copy()
@@ -1049,7 +1039,7 @@ def test_max_iter():
                 lr = LogisticRegression(max_iter=max_iter, tol=1e-15,
                                         multi_class=multi_class,
                                         random_state=0, solver=solver)
-                lr.fit(X, y_bin)
+                assert_warns(ConvergenceWarning, lr.fit, X, y_bin)
                 assert_equal(lr.n_iter_[0], max_iter)
 
 
diff --git a/sklearn/linear_model/tests/test_sgd.py b/sklearn/linear_model/tests/test_sgd.py
index 80b3ca394f99..9f372f706ca7 100644
--- a/sklearn/linear_model/tests/test_sgd.py
+++ b/sklearn/linear_model/tests/test_sgd.py
@@ -1,5 +1,6 @@
 import pickle
 import unittest
+import pytest
 
 import numpy as np
 import scipy.sparse as sp
@@ -467,6 +468,29 @@ def test_set_coef_multiclass(self):
         # Provided intercept_ does match dataset.
         clf = self.factory().fit(X2, Y2, intercept_init=np.zeros((3,)))
 
+    def test_sgd_predict_proba_method_access(self):
+        # Checks that SGDClassifier predict_proba and predict_log_proba methods
+        # can either be accessed or raise an appropriate error message
+        # otherwise. See
+        # https://github.com/scikit-learn/scikit-learn/issues/10938 for more
+        # details.
+        for loss in SGDClassifier.loss_functions:
+            clf = SGDClassifier(loss=loss)
+            if loss in ('log', 'modified_huber'):
+                assert hasattr(clf, 'predict_proba')
+                assert hasattr(clf, 'predict_log_proba')
+            else:
+                message = ("probability estimates are not "
+                           "available for loss={!r}".format(loss))
+                assert not hasattr(clf, 'predict_proba')
+                assert not hasattr(clf, 'predict_log_proba')
+                with pytest.raises(AttributeError,
+                                   message=message):
+                    clf.predict_proba
+                with pytest.raises(AttributeError,
+                                   message=message):
+                    clf.predict_log_proba
+
     def test_sgd_proba(self):
         # Check SGD.predict_proba
 
diff --git a/sklearn/manifold/spectral_embedding_.py b/sklearn/manifold/spectral_embedding_.py
index e399c75708fc..bc367b4e5af9 100644
--- a/sklearn/manifold/spectral_embedding_.py
+++ b/sklearn/manifold/spectral_embedding_.py
@@ -208,7 +208,7 @@ def spectral_embedding(adjacency, n_components=8, eigen_solver=None,
     * Toward the Optimal Preconditioned Eigensolver: Locally Optimal
       Block Preconditioned Conjugate Gradient Method
       Andrew V. Knyazev
-      http://dx.doi.org/10.1137%2FS1064827500366124
+      https://doi.org/10.1137%2FS1064827500366124
     """
     adjacency = check_symmetric(adjacency)
 
diff --git a/sklearn/manifold/t_sne.py b/sklearn/manifold/t_sne.py
index b2235fe7d9ac..bcaaa68e9909 100644
--- a/sklearn/manifold/t_sne.py
+++ b/sklearn/manifold/t_sne.py
@@ -9,6 +9,7 @@
 #   http://cseweb.ucsd.edu/~lvdmaaten/workshops/nips2010/papers/vandermaaten.pdf
 from __future__ import division
 
+import warnings
 from time import time
 import numpy as np
 from scipy import linalg
@@ -394,7 +395,8 @@ def _gradient_descent(objective, p0, it, n_iter,
     return p, error, i
 
 
-def trustworthiness(X, X_embedded, n_neighbors=5, precomputed=False):
+def trustworthiness(X, X_embedded, n_neighbors=5,
+                    precomputed=False, metric='euclidean'):
     r"""Expresses to what extent the local structure is retained.
 
     The trustworthiness is within [0, 1]. It is defined as
@@ -431,15 +433,28 @@ def trustworthiness(X, X_embedded, n_neighbors=5, precomputed=False):
     precomputed : bool, optional (default: False)
         Set this flag if X is a precomputed square distance matrix.
 
+        ..deprecated:: 0.20
+            ``precomputed`` has been deprecated in version 0.20 and will be
+            removed in version 0.22. Use ``metric`` instead.
+
+    metric : string, or callable, optional, default 'euclidean'
+        Which metric to use for computing pairwise distances between samples
+        from the original input space. If metric is 'precomputed', X must be a
+        matrix of pairwise distances or squared distances. Otherwise, see the
+        documentation of argument metric in sklearn.pairwise.pairwise_distances
+        for a list of available metrics.
+
     Returns
     -------
     trustworthiness : float
         Trustworthiness of the low-dimensional embedding.
     """
     if precomputed:
-        dist_X = X
-    else:
-        dist_X = pairwise_distances(X, squared=True)
+        warnings.warn("The flag 'precomputed' has been deprecated in version "
+                      "0.20 and will be removed in 0.22. See 'metric' "
+                      "parameter instead.", DeprecationWarning)
+        metric = 'precomputed'
+    dist_X = pairwise_distances(X, metric=metric)
     ind_X = np.argsort(dist_X, axis=1)
     ind_X_embedded = NearestNeighbors(n_neighbors).fit(X_embedded).kneighbors(
         return_distance=False)
@@ -851,7 +866,7 @@ def _tsne(self, P, degrees_of_freedom, n_samples, X_embedded,
         self.n_iter_ = it
 
         if self.verbose:
-            print("[t-SNE] Error after %d iterations: %f"
+            print("[t-SNE] KL divergence after %d iterations: %f"
                   % (it + 1, kl_divergence))
 
         X_embedded = params.reshape(n_samples, self.n_components)
diff --git a/sklearn/manifold/tests/test_t_sne.py b/sklearn/manifold/tests/test_t_sne.py
index 67b0853463c9..6b1d87bb18bf 100644
--- a/sklearn/manifold/tests/test_t_sne.py
+++ b/sklearn/manifold/tests/test_t_sne.py
@@ -14,6 +14,8 @@
 from sklearn.utils.testing import assert_greater
 from sklearn.utils.testing import assert_raises_regexp
 from sklearn.utils.testing import assert_in
+from sklearn.utils.testing import assert_warns
+from sklearn.utils.testing import assert_raises
 from sklearn.utils.testing import skip_if_32bit
 from sklearn.utils import check_random_state
 from sklearn.manifold.t_sne import _joint_probabilities
@@ -288,11 +290,39 @@ def test_preserve_trustworthiness_approximately_with_precomputed_distances():
                     early_exaggeration=2.0, metric="precomputed",
                     random_state=i, verbose=0)
         X_embedded = tsne.fit_transform(D)
-        t = trustworthiness(D, X_embedded, n_neighbors=1,
-                            precomputed=True)
+        t = trustworthiness(D, X_embedded, n_neighbors=1, metric="precomputed")
         assert t > .95
 
 
+def test_trustworthiness_precomputed_deprecation():
+    # FIXME: Remove this test in v0.23
+
+    # Use of the flag `precomputed` in trustworthiness parameters has been
+    # deprecated, but will still work until v0.23.
+    random_state = check_random_state(0)
+    X = random_state.randn(100, 2)
+    assert_equal(assert_warns(DeprecationWarning, trustworthiness,
+                              pairwise_distances(X), X, precomputed=True), 1.)
+    assert_equal(assert_warns(DeprecationWarning, trustworthiness,
+                              pairwise_distances(X), X, metric='precomputed',
+                              precomputed=True), 1.)
+    assert_raises(ValueError, assert_warns, DeprecationWarning,
+                  trustworthiness, X, X, metric='euclidean', precomputed=True)
+    assert_equal(assert_warns(DeprecationWarning, trustworthiness,
+                              pairwise_distances(X), X, metric='euclidean',
+                              precomputed=True), 1.)
+
+
+def test_trustworthiness_not_euclidean_metric():
+    # Test trustworthiness with a metric different from 'euclidean' and
+    # 'precomputed'
+    random_state = check_random_state(0)
+    X = random_state.randn(100, 2)
+    assert_equal(trustworthiness(X, X, metric='cosine'),
+                 trustworthiness(pairwise_distances(X, metric='cosine'), X,
+                                 metric='precomputed'))
+
+
 def test_early_exaggeration_too_small():
     # Early exaggeration factor must be >= 1.
     tsne = TSNE(early_exaggeration=0.99)
diff --git a/sklearn/metrics/classification.py b/sklearn/metrics/classification.py
index 02346da8a683..a7c535530724 100644
--- a/sklearn/metrics/classification.py
+++ b/sklearn/metrics/classification.py
@@ -499,7 +499,7 @@ def matthews_corrcoef(y_true, y_pred, sample_weight=None):
     ----------
     .. [1] `Baldi, Brunak, Chauvin, Andersen and Nielsen, (2000). Assessing the
        accuracy of prediction algorithms for classification: an overview
-       <http://dx.doi.org/10.1093/bioinformatics/16.5.412>`_
+       <https://doi.org/10.1093/bioinformatics/16.5.412>`_
 
     .. [2] `Wikipedia entry for the Matthews Correlation Coefficient
        <https://en.wikipedia.org/wiki/Matthews_correlation_coefficient>`_
@@ -1009,6 +1009,7 @@ def precision_recall_fscore_support(y_true, y_pred, beta=1.0, labels=None,
 
     It is possible to compute per-label precisions, recalls, F1-scores and
     supports instead of averaging:
+
     >>> precision_recall_fscore_support(y_true, y_pred, average=None,
     ... labels=['pig', 'dog', 'cat'])
     ... # doctest: +ELLIPSIS,+NORMALIZE_WHITESPACE
diff --git a/sklearn/metrics/cluster/supervised.py b/sklearn/metrics/cluster/supervised.py
index b73303ad623f..19bc461c9e9f 100644
--- a/sklearn/metrics/cluster/supervised.py
+++ b/sklearn/metrics/cluster/supervised.py
@@ -852,11 +852,12 @@ def fowlkes_mallows_score(labels_true, labels_pred, sparse=False):
     labels_true, labels_pred = check_clusterings(labels_true, labels_pred)
     n_samples, = labels_true.shape
 
-    c = contingency_matrix(labels_true, labels_pred, sparse=True)
+    c = contingency_matrix(labels_true, labels_pred,
+                           sparse=True).astype(np.int64)
     tk = np.dot(c.data, c.data) - n_samples
     pk = np.sum(np.asarray(c.sum(axis=0)).ravel() ** 2) - n_samples
     qk = np.sum(np.asarray(c.sum(axis=1)).ravel() ** 2) - n_samples
-    return tk / np.sqrt(pk * qk) if tk != 0. else 0.
+    return np.sqrt(tk / pk) * np.sqrt(tk / qk) if tk != 0. else 0.
 
 
 def entropy(labels):
diff --git a/sklearn/metrics/cluster/tests/test_supervised.py b/sklearn/metrics/cluster/tests/test_supervised.py
index 8da03d1e1745..8be39cd220d2 100644
--- a/sklearn/metrics/cluster/tests/test_supervised.py
+++ b/sklearn/metrics/cluster/tests/test_supervised.py
@@ -173,15 +173,16 @@ def test_expected_mutual_info_overflow():
     assert expected_mutual_information(np.array([[70000]]), 70000) <= 1
 
 
-def test_int_overflow_mutual_info_score():
-    # Test overflow in mutual_info_classif
+def test_int_overflow_mutual_info_fowlkes_mallows_score():
+    # Test overflow in mutual_info_classif and fowlkes_mallows_score
     x = np.array([1] * (52632 + 2529) + [2] * (14660 + 793) + [3] * (3271 +
                  204) + [4] * (814 + 39) + [5] * (316 + 20))
     y = np.array([0] * 52632 + [1] * 2529 + [0] * 14660 + [1] * 793 +
                  [0] * 3271 + [1] * 204 + [0] * 814 + [1] * 39 + [0] * 316 +
                  [1] * 20)
 
-    assert_all_finite(mutual_info_score(x.ravel(), y.ravel()))
+    assert_all_finite(mutual_info_score(x, y))
+    assert_all_finite(fowlkes_mallows_score(x, y))
 
 
 def test_entropy():
diff --git a/sklearn/metrics/cluster/unsupervised.py b/sklearn/metrics/cluster/unsupervised.py
index f4da109f16e2..635aa42b0c11 100644
--- a/sklearn/metrics/cluster/unsupervised.py
+++ b/sklearn/metrics/cluster/unsupervised.py
@@ -211,6 +211,8 @@ def silhouette_samples(X, labels, metric='euclidean', **kwds):
 def calinski_harabaz_score(X, labels):
     """Compute the Calinski and Harabaz score.
 
+    It is also known as the Variance Ratio Criterion.
+
     The score is defined as ratio between the within-cluster dispersion and
     the between-cluster dispersion.
 
diff --git a/sklearn/metrics/pairwise.py b/sklearn/metrics/pairwise.py
index e13bcbfeb804..58bd3f1627f2 100644
--- a/sklearn/metrics/pairwise.py
+++ b/sklearn/metrics/pairwise.py
@@ -722,7 +722,7 @@ def paired_distances(X, Y, metric="euclidean", **kwds):
 
 
 # Kernels
-def linear_kernel(X, Y=None):
+def linear_kernel(X, Y=None, dense_output=True):
     """
     Compute the linear kernel between X and Y.
 
@@ -734,12 +734,18 @@ def linear_kernel(X, Y=None):
 
     Y : array of shape (n_samples_2, n_features)
 
+    dense_output : boolean (optional), default True
+        Whether to return dense output even when the input is sparse. If
+        ``False``, the output is sparse if both input arrays are sparse.
+
+        .. versionadded:: 0.20
+
     Returns
     -------
     Gram matrix : array of shape (n_samples_1, n_samples_2)
     """
     X, Y = check_pairwise_arrays(X, Y)
-    return safe_sparse_dot(X, Y.T, dense_output=True)
+    return safe_sparse_dot(X, Y.T, dense_output=dense_output)
 
 
 def polynomial_kernel(X, Y=None, degree=3, gamma=None, coef0=1):
diff --git a/sklearn/metrics/ranking.py b/sklearn/metrics/ranking.py
index 8800561f0cfa..5039c5f874a5 100644
--- a/sklearn/metrics/ranking.py
+++ b/sklearn/metrics/ranking.py
@@ -418,7 +418,7 @@ def _binary_clf_curve(y_true, y_score, pos_label=None, sample_weight=None):
     tps = stable_cumsum(y_true * weight)[threshold_idxs]
     if sample_weight is not None:
         # express fps as a cumsum to ensure fps is increasing even in
-        # the presense of floating point errors
+        # the presence of floating point errors
         fps = stable_cumsum((1 - y_true) * weight)[threshold_idxs]
     else:
         fps = 1 + threshold_idxs - tps
@@ -639,7 +639,7 @@ def roc_curve(y_true, y_score, pos_label=None, sample_weight=None,
     return fpr, tpr, thresholds
 
 
-def label_ranking_average_precision_score(y_true, y_score):
+def label_ranking_average_precision_score(y_true, y_score, sample_weight=None):
     """Compute ranking-based average precision
 
     Label ranking average precision (LRAP) is the average over each ground
@@ -664,6 +664,9 @@ def label_ranking_average_precision_score(y_true, y_score):
         class, confidence values, or non-thresholded measure of decisions
         (as returned by "decision_function" on some classifiers).
 
+    sample_weight : array-like of shape = [n_samples], optional
+        Sample weights.
+
     Returns
     -------
     score : float
@@ -679,7 +682,7 @@ def label_ranking_average_precision_score(y_true, y_score):
     0.416...
 
     """
-    check_consistent_length(y_true, y_score)
+    check_consistent_length(y_true, y_score, sample_weight)
     y_true = check_array(y_true, ensure_2d=False)
     y_score = check_array(y_score, ensure_2d=False)
 
@@ -710,9 +713,17 @@ def label_ranking_average_precision_score(y_true, y_score):
         scores_i = y_score[i]
         rank = rankdata(scores_i, 'max')[relevant]
         L = rankdata(scores_i[relevant], 'max')
-        out += (L / rank).mean()
+        aux = (L / rank).mean()
+        if sample_weight is not None:
+            aux = aux * sample_weight[i]
+        out += aux
+
+    if sample_weight is None:
+        out /= n_samples
+    else:
+        out /= np.sum(sample_weight)
 
-    return out / n_samples
+    return out
 
 
 def coverage_error(y_true, y_score, sample_weight=None):
diff --git a/sklearn/metrics/scorer.py b/sklearn/metrics/scorer.py
index 05231826a899..c13d59a8daa2 100644
--- a/sklearn/metrics/scorer.py
+++ b/sklearn/metrics/scorer.py
@@ -19,6 +19,7 @@
 # License: Simplified BSD
 
 from abc import ABCMeta, abstractmethod
+from collections import Iterable
 import warnings
 
 import numpy as np
@@ -300,6 +301,10 @@ def check_scoring(estimator, scoring=None, allow_none=False):
                 "If no scoring is specified, the estimator passed should "
                 "have a 'score' method. The estimator %r does not."
                 % estimator)
+    elif isinstance(scoring, Iterable):
+        raise ValueError("For evaluating multiple scores, use "
+                         "sklearn.model_selection.cross_validate instead. "
+                         "{0} was passed.".format(scoring))
     else:
         raise ValueError("scoring value should either be a callable, string or"
                          " None. %r was passed" % scoring)
diff --git a/sklearn/metrics/tests/test_common.py b/sklearn/metrics/tests/test_common.py
index 84d5ebe3fbf5..680b78c3dd43 100644
--- a/sklearn/metrics/tests/test_common.py
+++ b/sklearn/metrics/tests/test_common.py
@@ -306,6 +306,7 @@
     "macro_average_precision_score",
 
     "coverage_error", "label_ranking_loss",
+    "label_ranking_average_precision_score",
 ]
 
 # Classification metrics with  "multilabel-indicator" format
diff --git a/sklearn/metrics/tests/test_pairwise.py b/sklearn/metrics/tests/test_pairwise.py
index 242523034e7a..799b3e4fe9bf 100644
--- a/sklearn/metrics/tests/test_pairwise.py
+++ b/sklearn/metrics/tests/test_pairwise.py
@@ -43,6 +43,8 @@
 from sklearn.preprocessing import normalize
 from sklearn.exceptions import DataConversionWarning
 
+import pytest
+
 
 def test_pairwise_distances():
     # Test the pairwise_distance helper function.
@@ -560,21 +562,29 @@ def test_laplacian_kernel():
     assert_true(np.all(K - np.diag(np.diag(K)) < 1))
 
 
-def test_cosine_similarity_sparse_output():
-    # Test if cosine_similarity correctly produces sparse output.
-
+@pytest.mark.parametrize('metric, pairwise_func',
+                         [('linear', linear_kernel),
+                          ('cosine', cosine_similarity)])
+def test_pairwise_similarity_sparse_output(metric, pairwise_func):
     rng = np.random.RandomState(0)
     X = rng.random_sample((5, 4))
     Y = rng.random_sample((3, 4))
     Xcsr = csr_matrix(X)
     Ycsr = csr_matrix(Y)
 
-    K1 = cosine_similarity(Xcsr, Ycsr, dense_output=False)
+    # should be sparse
+    K1 = pairwise_func(Xcsr, Ycsr, dense_output=False)
     assert_true(issparse(K1))
 
-    K2 = pairwise_kernels(Xcsr, Y=Ycsr, metric="cosine")
+    # should be dense, and equal to K1
+    K2 = pairwise_func(X, Y, dense_output=True)
+    assert not issparse(K2)
     assert_array_almost_equal(K1.todense(), K2)
 
+    # show the kernel output equal to the sparse.todense()
+    K3 = pairwise_kernels(X, Y=Y, metric=metric)
+    assert_array_almost_equal(K1.todense(), K3)
+
 
 def test_cosine_similarity():
     # Test the cosine_similarity.
diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py
index 3f032e45e90d..a9f66740f2b8 100644
--- a/sklearn/mixture/base.py
+++ b/sklearn/mixture/base.py
@@ -208,7 +208,7 @@ def fit(self, X, y=None):
                 self._initialize_parameters(X, random_state)
                 self.lower_bound_ = -np.infty
 
-            for n_iter in range(self.max_iter):
+            for n_iter in range(1, self.max_iter + 1):
                 prev_lower_bound = self.lower_bound_
 
                 log_prob_norm, log_resp = self._e_step(X)
diff --git a/sklearn/mixture/tests/test_gaussian_mixture.py b/sklearn/mixture/tests/test_gaussian_mixture.py
index 821a7ce412ad..08a083abf71e 100644
--- a/sklearn/mixture/tests/test_gaussian_mixture.py
+++ b/sklearn/mixture/tests/test_gaussian_mixture.py
@@ -1,6 +1,6 @@
 # Author: Wei Xue <xuewei4d@gmail.com>
 #         Thierry Guillemot <thierry.guillemot.work@gmail.com>
-# License: BSD 3 clauseimport warnings
+# License: BSD 3 clause
 
 import sys
 import warnings
diff --git a/sklearn/mixture/tests/test_mixture.py b/sklearn/mixture/tests/test_mixture.py
new file mode 100644
index 000000000000..a79cafe3bcce
--- /dev/null
+++ b/sklearn/mixture/tests/test_mixture.py
@@ -0,0 +1,23 @@
+# Author: Guillaume Lemaitre <g.lemaitre58@gmail.com>
+# License: BSD 3 clause
+
+import pytest
+import numpy as np
+
+from sklearn.mixture import GaussianMixture
+from sklearn.mixture import BayesianGaussianMixture
+
+
+@pytest.mark.parametrize(
+    "estimator",
+    [GaussianMixture(),
+     BayesianGaussianMixture()]
+)
+def test_gaussian_mixture_n_iter(estimator):
+    # check that n_iter is the number of iteration performed.
+    rng = np.random.RandomState(0)
+    X = rng.rand(10, 5)
+    max_iter = 1
+    estimator.set_params(max_iter=max_iter)
+    estimator.fit(X)
+    assert estimator.n_iter_ == max_iter
diff --git a/sklearn/model_selection/_search.py b/sklearn/model_selection/_search.py
index d9e7d3166b8f..7bcab9c6a642 100644
--- a/sklearn/model_selection/_search.py
+++ b/sklearn/model_selection/_search.py
@@ -242,13 +242,16 @@ def __iter__(self):
             # look up sampled parameter settings in parameter grid
             param_grid = ParameterGrid(self.param_distributions)
             grid_size = len(param_grid)
-
-            if grid_size < self.n_iter:
-                raise ValueError(
-                    "The total space of parameters %d is smaller "
-                    "than n_iter=%d. For exhaustive searches, use "
-                    "GridSearchCV." % (grid_size, self.n_iter))
-            for i in sample_without_replacement(grid_size, self.n_iter,
+            n_iter = self.n_iter
+
+            if grid_size < n_iter:
+                warnings.warn(
+                    'The total space of parameters %d is smaller '
+                    'than n_iter=%d. Running %d iterations. For exhaustive '
+                    'searches, use GridSearchCV.'
+                    % (grid_size, self.n_iter, grid_size), UserWarning)
+                n_iter = grid_size
+            for i in sample_without_replacement(grid_size, n_iter,
                                                 random_state=rnd):
                 yield param_grid[i]
 
@@ -694,7 +697,7 @@ def _store(key_name, array, weights=None, splits=False, rank=False):
         for cand_i, params in enumerate(candidate_params):
             for name, value in params.items():
                 # An all masked empty array gets created for the key
-                # `"param_%s" % name` at the first occurence of `name`.
+                # `"param_%s" % name` at the first occurrence of `name`.
                 # Setting the value at an index also unmasks that index
                 param_results["param_%s" % name][cand_i] = value
 
@@ -1124,6 +1127,12 @@ class RandomizedSearchCV(BaseSearchCV):
     It is highly recommended to use continuous distributions for continuous
     parameters.
 
+    Note that before SciPy 0.16, the ``scipy.stats.distributions`` do not
+    accept a custom RNG instance and always use the singleton RNG from
+    ``numpy.random``. Hence setting ``random_state`` will not guarantee a
+    deterministic iteration whenever ``scipy.stats`` distributions are used to
+    define the parameter search space.
+
     Read more in the :ref:`User Guide <randomized_parameter_search>`.
 
     Parameters
@@ -1381,7 +1390,7 @@ class RandomizedSearchCV(BaseSearchCV):
         Does exhaustive search over a grid of parameters.
 
     :class:`ParameterSampler`:
-        A generator over parameter settins, constructed from
+        A generator over parameter settings, constructed from
         param_distributions.
 
     """
diff --git a/sklearn/model_selection/_split.py b/sklearn/model_selection/_split.py
index fce5fc48c9cf..866cb4cc53aa 100644
--- a/sklearn/model_selection/_split.py
+++ b/sklearn/model_selection/_split.py
@@ -1096,7 +1096,7 @@ class RepeatedKFold(_RepeatedSplits):
 
     See also
     --------
-    RepeatedStratifiedKFold: Repeates Stratified K-Fold n times.
+    RepeatedStratifiedKFold: Repeats Stratified K-Fold n times.
     """
     def __init__(self, n_splits=5, n_repeats=10, random_state=None):
         super(RepeatedKFold, self).__init__(
@@ -1929,7 +1929,7 @@ def train_test_split(*arrays, **options):
         Allowed inputs are lists, numpy arrays, scipy-sparse
         matrices or pandas dataframes.
 
-    test_size : float, int, None, optional
+    test_size : float, int or None, optional (default=0.25)
         If float, should be between 0.0 and 1.0 and represent the proportion
         of the dataset to include in the test split. If int, represents the
         absolute number of test samples. If None, the value is set to the
@@ -1938,7 +1938,7 @@ def train_test_split(*arrays, **options):
         if ``train_size`` is unspecified, otherwise it will complement
         the specified ``train_size``.
 
-    train_size : float, int, or None, default None
+    train_size : float, int, or None, (default=None)
         If float, should be between 0.0 and 1.0 and represent the
         proportion of the dataset to include in the train split. If
         int, represents the absolute number of train samples. If None,
@@ -1954,7 +1954,7 @@ def train_test_split(*arrays, **options):
         Whether or not to shuffle the data before splitting. If shuffle=False
         then stratify must be None.
 
-    stratify : array-like or None (default is None)
+    stratify : array-like or None (default=None)
         If not None, data is split in a stratified fashion, using this as
         the class labels.
 
diff --git a/sklearn/model_selection/_validation.py b/sklearn/model_selection/_validation.py
index ceddce37781a..50af9b5dd550 100644
--- a/sklearn/model_selection/_validation.py
+++ b/sklearn/model_selection/_validation.py
@@ -194,6 +194,10 @@ def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv=None,
     :func:`sklearn.model_selection.cross_val_score`:
         Run cross-validation for single metric evaluation.
 
+    :func:`sklearn.model_selection.cross_val_predict`:
+        Get predictions from each split of cross-validation for diagnostic
+        purposes.
+
     :func:`sklearn.metrics.make_scorer`:
         Make a scorer from a performance metric or loss function.
 
@@ -341,6 +345,10 @@ def cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv=None,
         To run cross-validation on multiple metrics and also to return
         train scores, fit times and score times.
 
+    :func:`sklearn.model_selection.cross_val_predict`:
+        Get predictions from each split of cross-validation for diagnostic
+        purposes.
+
     :func:`sklearn.metrics.make_scorer`:
         Make a scorer from a performance metric or loss function.
 
@@ -606,6 +614,9 @@ def cross_val_predict(estimator, X, y=None, groups=None, cv=None, n_jobs=1,
                       method='predict'):
     """Generate cross-validated estimates for each input data point
 
+    It is not appropriate to pass these predictions into an evaluation
+    metric. Use :func:`cross_validate` to measure generalization error.
+
     Read more in the :ref:`User Guide <cross_validation>`.
 
     Parameters
@@ -677,6 +688,12 @@ def cross_val_predict(estimator, X, y=None, groups=None, cv=None, n_jobs=1,
     predictions : ndarray
         This is the result of calling ``method``
 
+    See also
+    --------
+    cross_val_score : calculate score for each CV split
+
+    cross_validate : calculate one or more scores and timings for each CV split
+
     Notes
     -----
     In the case that one or more classes are absent in a training portion, a
@@ -1097,7 +1114,7 @@ def learning_curve(estimator, X, y, groups=None,
 
     Returns
     -------
-    train_sizes_abs : array, shape = (n_unique_ticks,), dtype int
+    train_sizes_abs : array, shape (n_unique_ticks,), dtype int
         Numbers of training examples that has been used to generate the
         learning curve. Note that the number of ticks might be less
         than n_ticks because duplicate entries will be removed.
diff --git a/sklearn/model_selection/tests/test_search.py b/sklearn/model_selection/tests/test_search.py
index 87d93894683b..8bb60afe4a45 100644
--- a/sklearn/model_selection/tests/test_search.py
+++ b/sklearn/model_selection/tests/test_search.py
@@ -33,6 +33,7 @@
 from sklearn.base import BaseEstimator
 from sklearn.base import clone
 from sklearn.exceptions import NotFittedError
+from sklearn.exceptions import ConvergenceWarning
 from sklearn.datasets import make_classification
 from sklearn.datasets import make_blobs
 from sklearn.datasets import make_multilabel_classification
@@ -350,7 +351,9 @@ def test_return_train_score_warn():
     for estimator in estimators:
         for val in [True, False, 'warn']:
             estimator.set_params(return_train_score=val)
-            result[val] = assert_no_warnings(estimator.fit, X, y).cv_results_
+            fit_func = ignore_warnings(estimator.fit,
+                                       category=ConvergenceWarning)
+            result[val] = assert_no_warnings(fit_func, X, y).cv_results_
 
     train_keys = ['split0_train_score', 'split1_train_score',
                   'split2_train_score', 'mean_train_score', 'std_train_score']
@@ -1214,10 +1217,10 @@ def test_fit_grid_point():
             assert_equal(n_test_samples, test.size)
 
     # Should raise an error upon multimetric scorer
-    assert_raise_message(ValueError, "scoring value should either be a "
-                         "callable, string or None.", fit_grid_point, X, y,
-                         svc, params, train, test, {'score': scorer},
-                         verbose=True)
+    assert_raise_message(ValueError, "For evaluating multiple scores, use "
+                         "sklearn.model_selection.cross_validate instead.",
+                         fit_grid_point, X, y, svc, params, train, test,
+                         {'score': scorer}, verbose=True)
 
 
 def test_pickle():
@@ -1382,10 +1385,18 @@ def test_grid_search_failing_classifier_raise():
 
 
 def test_parameters_sampler_replacement():
-    # raise error if n_iter too large
+    # raise warning if n_iter is bigger than total parameter space
     params = {'first': [0, 1], 'second': ['a', 'b', 'c']}
     sampler = ParameterSampler(params, n_iter=7)
-    assert_raises(ValueError, list, sampler)
+    n_iter = 7
+    grid_size = 6
+    expected_warning = ('The total space of parameters %d is smaller '
+                        'than n_iter=%d. Running %d iterations. For '
+                        'exhaustive searches, use GridSearchCV.'
+                        % (grid_size, n_iter, grid_size))
+    assert_warns_message(UserWarning, expected_warning,
+                         list, sampler)
+
     # degenerates to GridSearchCV if n_iter the same as grid_size
     sampler = ParameterSampler(params, n_iter=6)
     samples = list(sampler)
diff --git a/sklearn/multiclass.py b/sklearn/multiclass.py
index a8510cf0a0a8..0fc2907fe481 100644
--- a/sklearn/multiclass.py
+++ b/sklearn/multiclass.py
@@ -197,7 +197,7 @@ def fit(self, X, y):
         self
         """
         # A sparse LabelBinarizer, with sparse_output=True, has been shown to
-        # outpreform or match a dense label binarizer in all cases and has also
+        # outperform or match a dense label binarizer in all cases and has also
         # resulted in less or equal memory consumption in the fit_ovr function
         # overall.
         self.label_binarizer_ = LabelBinarizer(sparse_output=True)
diff --git a/sklearn/neighbors/ball_tree.pyx b/sklearn/neighbors/ball_tree.pyx
index bd0a476f2b4c..909e50ca47ba 100644
--- a/sklearn/neighbors/ball_tree.pyx
+++ b/sklearn/neighbors/ball_tree.pyx
@@ -105,7 +105,7 @@ cdef inline DTYPE_t max_dist(BinaryTree tree, ITYPE_t i_node,
 
 
 cdef inline int min_max_dist(BinaryTree tree, ITYPE_t i_node, DTYPE_t* pt,
-                             DTYPE_t* min_dist, DTYPE_t* max_dist) except -1:
+                             DTYPE_t* min_dist, DTYPE_t* max_dist) nogil except -1:
     """Compute the minimum and maximum distance between a point and a node"""
     cdef DTYPE_t dist_pt = tree.dist(pt, &tree.node_bounds[0, i_node, 0],
                                      tree.data.shape[1])
diff --git a/sklearn/neighbors/base.py b/sklearn/neighbors/base.py
index 196092c6e6fa..d983c124679f 100644
--- a/sklearn/neighbors/base.py
+++ b/sklearn/neighbors/base.py
@@ -258,6 +258,12 @@ def _fit(self, X):
                     "Expected n_neighbors > 0. Got %d" %
                     self.n_neighbors
                 )
+            else:
+                if not np.issubdtype(type(self.n_neighbors), np.integer):
+                    raise TypeError(
+                        "n_neighbors does not take %s value, "
+                        "enter integer value" %
+                        type(self.n_neighbors))
 
         return self
 
@@ -327,6 +333,17 @@ class from an array representing our data set and ask who's
 
         if n_neighbors is None:
             n_neighbors = self.n_neighbors
+        elif n_neighbors <= 0:
+            raise ValueError(
+                "Expected n_neighbors > 0. Got %d" %
+                n_neighbors
+            )
+        else:
+            if not np.issubdtype(type(n_neighbors), np.integer):
+                raise TypeError(
+                    "n_neighbors does not take %s value, "
+                    "enter integer value" %
+                    type(n_neighbors))
 
         if X is not None:
             query_is_train = False
@@ -619,10 +636,18 @@ class from an array representing our data set and ask who's
                 raise ValueError(
                     "%s does not work with sparse matrices. Densify the data, "
                     "or set algorithm='brute'" % self._fit_method)
-            results = self._tree.query_radius(X, radius,
-                                              return_distance=return_distance)
+
+            n_jobs = _get_n_jobs(self.n_jobs)
+            results = Parallel(n_jobs, backend='threading')(
+                delayed(self._tree.query_radius, check_pickle=False)(
+                    X[s], radius, return_distance)
+                for s in gen_even_slices(X.shape[0], n_jobs)
+            )
             if return_distance:
-                results = results[::-1]
+                neigh_ind, dist = tuple(zip(*results))
+                results = np.hstack(dist), np.hstack(neigh_ind)
+            else:
+                results = np.hstack(results)
         else:
             raise ValueError("internal: _fit_method not recognized")
 
diff --git a/sklearn/neighbors/binary_tree.pxi b/sklearn/neighbors/binary_tree.pxi
index 1f6510ac3f23..edf78257c9b2 100755
--- a/sklearn/neighbors/binary_tree.pxi
+++ b/sklearn/neighbors/binary_tree.pxi
@@ -144,6 +144,8 @@
 cimport cython
 cimport numpy as np
 from libc.math cimport fabs, sqrt, exp, cos, pow, log
+from libc.stdlib cimport calloc, malloc, free
+from libc.string cimport memcpy
 from sklearn.utils.lgamma cimport lgamma
 
 import numpy as np
@@ -156,6 +158,11 @@ from typedefs import DTYPE, ITYPE
 from dist_metrics cimport (DistanceMetric, euclidean_dist, euclidean_rdist,
                            euclidean_dist_to_rdist, euclidean_rdist_to_dist)
 
+cdef extern from "numpy/arrayobject.h":
+    void PyArray_ENABLEFLAGS(np.ndarray arr, int flags)
+
+np.import_array()
+
 # some handy constants
 cdef DTYPE_t INF = np.inf
 cdef DTYPE_t NEG_INF = -np.inf
@@ -545,7 +552,7 @@ cdef inline void swap(DITYPE_t* arr, ITYPE_t i1, ITYPE_t i2):
 
 
 cdef inline void dual_swap(DTYPE_t* darr, ITYPE_t* iarr,
-                           ITYPE_t i1, ITYPE_t i2):
+                           ITYPE_t i1, ITYPE_t i2) nogil:
     """swap the values at inex i1 and i2 of both darr and iarr"""
     cdef DTYPE_t dtmp = darr[i1]
     darr[i1] = darr[i2]
@@ -670,7 +677,7 @@ cdef class NeighborsHeap:
 
 
 cdef int _simultaneous_sort(DTYPE_t* dist, ITYPE_t* idx,
-                            ITYPE_t size) except -1:
+                            ITYPE_t size) nogil except -1:
     """
     Perform a recursive quicksort on the dist array, simultaneously
     performing the same swaps on the idx array.  The equivalent in
@@ -1341,7 +1348,7 @@ cdef class BinaryTree:
         else:
             return indices.reshape(X.shape[:X.ndim - 1] + (k,))
 
-    def query_radius(self, X, r, return_distance=False,
+    def query_radius(self, X, r, int return_distance=False,
                      int count_only=False, int sort_results=False):
         """
         query_radius(self, X, r, count_only = False):
@@ -1406,6 +1413,8 @@ cdef class BinaryTree:
         cdef DTYPE_t[::1] dist_arr_i
         cdef ITYPE_t[::1] idx_arr_i, counts
         cdef DTYPE_t* pt
+        cdef ITYPE_t** indices = NULL
+        cdef DTYPE_t** distances = NULL
 
         # validate X and prepare for query
         X = check_array(X, dtype=DTYPE, order='C')
@@ -1429,11 +1438,15 @@ cdef class BinaryTree:
         rarr_np = r.reshape(-1)  # store explicitly to keep in scope
         cdef DTYPE_t[::1] rarr = get_memview_DTYPE_1D(rarr_np)
 
-        # prepare variables for iteration
         if not count_only:
-            indices = np.zeros(Xarr.shape[0], dtype='object')
+            indices = <ITYPE_t**>calloc(Xarr.shape[0], sizeof(ITYPE_t*))
+            if indices == NULL:
+                raise MemoryError()
             if return_distance:
-                distances = np.zeros(Xarr.shape[0], dtype='object')
+                distances = <DTYPE_t**>calloc(Xarr.shape[0], sizeof(DTYPE_t*))
+                if distances == NULL:
+                    free(indices)
+                    raise MemoryError()
 
         np_idx_arr = np.zeros(self.data.shape[0], dtype=ITYPE)
         idx_arr_i = get_memview_ITYPE_1D(np_idx_arr)
@@ -1445,33 +1458,89 @@ cdef class BinaryTree:
         counts = get_memview_ITYPE_1D(counts_arr)
 
         pt = &Xarr[0, 0]
-        for i in range(Xarr.shape[0]):
-            counts[i] = self._query_radius_single(0, pt, rarr[i],
-                                                  &idx_arr_i[0],
-                                                  &dist_arr_i[0],
-                                                  0, count_only,
-                                                  return_distance)
-            pt += n_features
+        memory_error = False
+        with nogil:
+            for i in range(Xarr.shape[0]):
+                counts[i] = self._query_radius_single(0, pt, rarr[i],
+                                                      &idx_arr_i[0],
+                                                      &dist_arr_i[0],
+                                                      0, count_only,
+                                                      return_distance)
+                pt += n_features
+
+                if count_only:
+                    continue
 
-            if count_only:
-                pass
-            else:
                 if sort_results:
                     _simultaneous_sort(&dist_arr_i[0], &idx_arr_i[0],
                                        counts[i])
 
-                indices[i] = np_idx_arr[:counts[i]].copy()
+                # equivalent to: indices[i] = np_idx_arr[:counts[i]].copy()
+                indices[i] = <ITYPE_t*>malloc(counts[i] * sizeof(ITYPE_t))
+                if indices[i] == NULL:
+                    memory_error = True
+                    break
+                memcpy(indices[i], &idx_arr_i[0], counts[i] * sizeof(ITYPE_t))
+
                 if return_distance:
-                    distances[i] = np_dist_arr[:counts[i]].copy()
+                    # equivalent to: distances[i] = np_dist_arr[:counts[i]].copy()
+                    distances[i] = <DTYPE_t*>malloc(counts[i] * sizeof(DTYPE_t))
+                    if distances[i] == NULL:
+                        memory_error = True
+                        break
+                    memcpy(distances[i], &dist_arr_i[0], counts[i] * sizeof(DTYPE_t))
+
+        try:
+            if memory_error:
+                raise MemoryError()
+
+            if count_only:
+                # deflatten results
+                return counts_arr.reshape(X.shape[:X.ndim - 1])
+            elif return_distance:
+                indices_npy = np.zeros(Xarr.shape[0], dtype='object')
+                distances_npy = np.zeros(Xarr.shape[0], dtype='object')
+                for i in range(Xarr.shape[0]):
+                    # make a new numpy array that wraps the existing data
+                    indices_npy[i] = np.PyArray_SimpleNewFromData(1, &counts[i], np.NPY_INTP, indices[i])
+                    # make sure the data will be freed when the numpy array is garbage collected
+                    PyArray_ENABLEFLAGS(indices_npy[i], np.NPY_OWNDATA)
+                    # make sure the data is not freed twice
+                    indices[i] = NULL
+
+                    # make a new numpy array that wraps the existing data
+                    distances_npy[i] = np.PyArray_SimpleNewFromData(1, &counts[i], np.NPY_DOUBLE, distances[i])
+                    # make sure the data will be freed when the numpy array is garbage collected
+                    PyArray_ENABLEFLAGS(distances_npy[i], np.NPY_OWNDATA)
+                    # make sure the data is not freed twice
+                    distances[i] = NULL
+
+                # deflatten results
+                return (indices_npy.reshape(X.shape[:X.ndim - 1]),
+                        distances_npy.reshape(X.shape[:X.ndim - 1]))
+            else:
+                indices_npy = np.zeros(Xarr.shape[0], dtype='object')
+                for i in range(Xarr.shape[0]):
+                    # make a new numpy array that wraps the existing data
+                    indices_npy[i] = np.PyArray_SimpleNewFromData(1, &counts[i], np.NPY_INTP, indices[i])
+                    # make sure the data will be freed when the numpy array is garbage collected
+                    PyArray_ENABLEFLAGS(indices_npy[i], np.NPY_OWNDATA)
+                    # make sure the data is not freed twice
+                    indices[i] = NULL
+
+                # deflatten results
+                return indices_npy.reshape(X.shape[:X.ndim - 1])
+        except:
+            # free any buffer that is not owned by a numpy array
+            for i in range(Xarr.shape[0]):
+                free(indices[i])
+                if return_distance:
+                    free(distances[i])
+            raise
+        finally:
+            free(indices)
+            free(distances)
 
-        # deflatten results
-        if count_only:
-            return counts_arr.reshape(X.shape[:X.ndim - 1])
-        elif return_distance:
-            return (indices.reshape(X.shape[:X.ndim - 1]),
-                    distances.reshape(X.shape[:X.ndim - 1]))
-        else:
-            return indices.reshape(X.shape[:X.ndim - 1])
 
     def kernel_density(self, X, h, kernel='gaussian',
                        atol=0, rtol=1E-8,
@@ -1971,7 +2040,7 @@ cdef class BinaryTree:
                                       DTYPE_t* distances,
                                       ITYPE_t count,
                                       int count_only,
-                                      int return_distance) except -1:
+                                      int return_distance) nogil:
         """recursive single-tree radius query, depth-first"""
         cdef DTYPE_t* data = &self.data[0, 0]
         cdef ITYPE_t* idx_array = &self.idx_array[0]
@@ -1999,8 +2068,7 @@ cdef class BinaryTree:
             else:
                 for i in range(node_info.idx_start, node_info.idx_end):
                     if (count < 0) or (count >= self.data.shape[0]):
-                        raise ValueError("Fatal: count too big: "
-                                         "this should never happen")
+                        return -1
                     indices[count] = idx_array[i]
                     if return_distance:
                         distances[count] = self.dist(pt, (data + n_features
@@ -2019,8 +2087,7 @@ cdef class BinaryTree:
                                      n_features)
                 if dist_pt <= reduced_r:
                     if (count < 0) or (count >= self.data.shape[0]):
-                        raise ValueError("Fatal: count out of range. "
-                                         "This should never happen.")
+                        return -1
                     if count_only:
                         pass
                     else:
diff --git a/sklearn/neighbors/classification.py b/sklearn/neighbors/classification.py
index 39735c0e1480..ace8590b0815 100644
--- a/sklearn/neighbors/classification.py
+++ b/sklearn/neighbors/classification.py
@@ -289,6 +289,10 @@ class RadiusNeighborsClassifier(NeighborsBase, RadiusNeighborsMixin,
     metric_params : dict, optional (default = None)
         Additional keyword arguments for the metric function.
 
+    n_jobs : int, optional (default = 1)
+        The number of parallel jobs to run for neighbors search.
+        If ``-1``, then the number of jobs is set to the number of CPU cores.
+
     Examples
     --------
     >>> X = [[0], [1], [2], [3]]
@@ -317,12 +321,13 @@ class RadiusNeighborsClassifier(NeighborsBase, RadiusNeighborsMixin,
 
     def __init__(self, radius=1.0, weights='uniform',
                  algorithm='auto', leaf_size=30, p=2, metric='minkowski',
-                 outlier_label=None, metric_params=None, **kwargs):
+                 outlier_label=None, metric_params=None, n_jobs=1, **kwargs):
         super(RadiusNeighborsClassifier, self).__init__(
               radius=radius,
               algorithm=algorithm,
               leaf_size=leaf_size,
-              metric=metric, p=p, metric_params=metric_params, **kwargs)
+              metric=metric, p=p, metric_params=metric_params,
+              n_jobs=n_jobs, **kwargs)
         self.weights = _check_weights(weights)
         self.outlier_label = outlier_label
 
diff --git a/sklearn/neighbors/dist_metrics.pxd b/sklearn/neighbors/dist_metrics.pxd
index fdbc7e154744..4ce405e1e0e8 100644
--- a/sklearn/neighbors/dist_metrics.pxd
+++ b/sklearn/neighbors/dist_metrics.pxd
@@ -39,7 +39,7 @@ cdef inline DTYPE_t euclidean_dist_to_rdist(DTYPE_t dist) nogil except -1:
     return dist * dist
 
 
-cdef inline DTYPE_t euclidean_rdist_to_dist(DTYPE_t dist) except -1:
+cdef inline DTYPE_t euclidean_rdist_to_dist(DTYPE_t dist) nogil except -1:
     return sqrt(dist)
 
 
@@ -72,6 +72,6 @@ cdef class DistanceMetric:
     cdef int cdist(self, DTYPE_t[:, ::1] X, DTYPE_t[:, ::1] Y,
                    DTYPE_t[:, ::1] D) except -1
 
-    cdef DTYPE_t _rdist_to_dist(self, DTYPE_t rdist) except -1
+    cdef DTYPE_t _rdist_to_dist(self, DTYPE_t rdist) nogil except -1
 
     cdef DTYPE_t _dist_to_rdist(self, DTYPE_t dist) nogil except -1
diff --git a/sklearn/neighbors/dist_metrics.pyx b/sklearn/neighbors/dist_metrics.pyx
index 69c1e6ed40e2..440187d60e49 100755
--- a/sklearn/neighbors/dist_metrics.pyx
+++ b/sklearn/neighbors/dist_metrics.pyx
@@ -330,7 +330,7 @@ cdef class DistanceMetric:
                 D[i1, i2] = self.dist(&X[i1, 0], &Y[i2, 0], X.shape[1])
         return 0
 
-    cdef DTYPE_t _rdist_to_dist(self, DTYPE_t rdist) except -1:
+    cdef DTYPE_t _rdist_to_dist(self, DTYPE_t rdist) nogil except -1:
         """Convert the reduced distance to the distance"""
         return rdist
 
@@ -418,7 +418,7 @@ cdef class EuclideanDistance(DistanceMetric):
                               ITYPE_t size) nogil except -1:
         return euclidean_rdist(x1, x2, size)
 
-    cdef inline DTYPE_t _rdist_to_dist(self, DTYPE_t rdist) except -1:
+    cdef inline DTYPE_t _rdist_to_dist(self, DTYPE_t rdist) nogil except -1:
         return sqrt(rdist)
 
     cdef inline DTYPE_t _dist_to_rdist(self, DTYPE_t dist) nogil except -1:
@@ -462,7 +462,7 @@ cdef class SEuclideanDistance(DistanceMetric):
                              ITYPE_t size) nogil except -1:
         return sqrt(self.rdist(x1, x2, size))
 
-    cdef inline DTYPE_t _rdist_to_dist(self, DTYPE_t rdist) except -1:
+    cdef inline DTYPE_t _rdist_to_dist(self, DTYPE_t rdist) nogil except -1:
         return sqrt(rdist)
 
     cdef inline DTYPE_t _dist_to_rdist(self, DTYPE_t dist) nogil except -1:
@@ -551,7 +551,7 @@ cdef class MinkowskiDistance(DistanceMetric):
                              ITYPE_t size) nogil except -1:
         return pow(self.rdist(x1, x2, size), 1. / self.p)
 
-    cdef inline DTYPE_t _rdist_to_dist(self, DTYPE_t rdist) except -1:
+    cdef inline DTYPE_t _rdist_to_dist(self, DTYPE_t rdist) nogil except -1:
         return pow(rdist, 1. / self.p)
 
     cdef inline DTYPE_t _dist_to_rdist(self, DTYPE_t dist) nogil except -1:
@@ -610,7 +610,7 @@ cdef class WMinkowskiDistance(DistanceMetric):
                              ITYPE_t size) nogil except -1:
         return pow(self.rdist(x1, x2, size), 1. / self.p)
 
-    cdef inline DTYPE_t _rdist_to_dist(self, DTYPE_t rdist) except -1:
+    cdef inline DTYPE_t _rdist_to_dist(self, DTYPE_t rdist) nogil except -1:
         return pow(rdist, 1. / self.p)
 
     cdef inline DTYPE_t _dist_to_rdist(self, DTYPE_t dist) nogil except -1:
@@ -683,7 +683,7 @@ cdef class MahalanobisDistance(DistanceMetric):
                              ITYPE_t size) nogil except -1:
         return sqrt(self.rdist(x1, x2, size))
 
-    cdef inline DTYPE_t _rdist_to_dist(self, DTYPE_t rdist) except -1:
+    cdef inline DTYPE_t _rdist_to_dist(self, DTYPE_t rdist) nogil except -1:
         return sqrt(rdist)
 
     cdef inline DTYPE_t _dist_to_rdist(self, DTYPE_t dist) nogil except -1:
@@ -998,7 +998,7 @@ cdef class HaversineDistance(DistanceMetric):
         return 2 * asin(sqrt(sin_0 * sin_0
                              + cos(x1[0]) * cos(x2[0]) * sin_1 * sin_1))
 
-    cdef inline DTYPE_t _rdist_to_dist(self, DTYPE_t rdist) except -1:
+    cdef inline DTYPE_t _rdist_to_dist(self, DTYPE_t rdist) nogil except -1:
         return 2 * asin(sqrt(rdist))
 
     cdef inline DTYPE_t _dist_to_rdist(self, DTYPE_t dist) nogil except -1:
diff --git a/sklearn/neighbors/graph.py b/sklearn/neighbors/graph.py
index add4f241e7b4..b794e2059a7b 100644
--- a/sklearn/neighbors/graph.py
+++ b/sklearn/neighbors/graph.py
@@ -68,7 +68,7 @@ def kneighbors_graph(X, n_neighbors, mode='connectivity', metric='minkowski',
     include_self : bool, default=False.
         Whether or not to mark each sample as the first nearest neighbor to
         itself. If `None`, then True is used for mode='connectivity' and False
-        for mode='distance' as this will preserve backwards compatibilty.
+        for mode='distance' as this will preserve backwards compatibility.
 
     n_jobs : int, optional (default = 1)
         The number of parallel jobs to run for neighbors search.
@@ -143,7 +143,7 @@ def radius_neighbors_graph(X, radius, mode='connectivity', metric='minkowski',
     include_self : bool, default=False
         Whether or not to mark each sample as the first nearest neighbor to
         itself. If `None`, then True is used for mode='connectivity' and False
-        for mode='distance' as this will preserve backwards compatibilty.
+        for mode='distance' as this will preserve backwards compatibility.
 
     n_jobs : int, optional (default = 1)
         The number of parallel jobs to run for neighbors search.
diff --git a/sklearn/neighbors/kd_tree.pyx b/sklearn/neighbors/kd_tree.pyx
index 481662c97787..08d2ff8ef0e3 100644
--- a/sklearn/neighbors/kd_tree.pyx
+++ b/sklearn/neighbors/kd_tree.pyx
@@ -147,7 +147,7 @@ cdef DTYPE_t max_dist(BinaryTree tree, ITYPE_t i_node, DTYPE_t* pt) except -1:
 
 
 cdef inline int min_max_dist(BinaryTree tree, ITYPE_t i_node, DTYPE_t* pt,
-                             DTYPE_t* min_dist, DTYPE_t* max_dist) except -1:
+                             DTYPE_t* min_dist, DTYPE_t* max_dist) nogil except -1:
     """Compute the minimum and maximum distance between a point and a node"""
     cdef ITYPE_t n_features = tree.data.shape[1]
 
diff --git a/sklearn/neighbors/quad_tree.pxd b/sklearn/neighbors/quad_tree.pxd
index 0dc4bd3fe5f3..7c146aca96c7 100644
--- a/sklearn/neighbors/quad_tree.pxd
+++ b/sklearn/neighbors/quad_tree.pxd
@@ -26,7 +26,7 @@ cdef float EPSILON = 1e-6
 # have is_leaf and max_width consecutive as it permits to avoid padding by
 # the compiler and keep the size coherent for both C and numpy data structures.
 cdef struct Cell:
-    # Base storage stucture for cells in a QuadTree object
+    # Base storage structure for cells in a QuadTree object
 
     # Tree structure
     SIZE_t parent              # Parent cell of this cell
diff --git a/sklearn/neighbors/quad_tree.pyx b/sklearn/neighbors/quad_tree.pyx
index 8267c13da7aa..fbe736636c89 100644
--- a/sklearn/neighbors/quad_tree.pyx
+++ b/sklearn/neighbors/quad_tree.pyx
@@ -422,7 +422,7 @@ cdef class _QuadTree:
             the query point:
             - results[idx:idx+n_dimensions] contains the coordinate-wise
                 difference between the query point and the summary cell idx.
-                This is usefull in t-SNE to compute the negative forces.
+                This is useful in t-SNE to compute the negative forces.
             - result[idx+n_dimensions+1] contains the squared euclidean
                 distance to the summary cell idx.
             - result[idx+n_dimensions+2] contains the number of point of the
diff --git a/sklearn/neighbors/regression.py b/sklearn/neighbors/regression.py
index 7181c44efbfc..d9f5682d49bf 100644
--- a/sklearn/neighbors/regression.py
+++ b/sklearn/neighbors/regression.py
@@ -238,6 +238,10 @@ class RadiusNeighborsRegressor(NeighborsBase, RadiusNeighborsMixin,
     metric_params : dict, optional (default = None)
         Additional keyword arguments for the metric function.
 
+    n_jobs : int, optional (default = 1)
+        The number of parallel jobs to run for neighbors search.
+        If ``-1``, then the number of jobs is set to the number of CPU cores.
+
     Examples
     --------
     >>> X = [[0], [1], [2], [3]]
@@ -266,12 +270,14 @@ class RadiusNeighborsRegressor(NeighborsBase, RadiusNeighborsMixin,
 
     def __init__(self, radius=1.0, weights='uniform',
                  algorithm='auto', leaf_size=30,
-                 p=2, metric='minkowski', metric_params=None, **kwargs):
+                 p=2, metric='minkowski', metric_params=None, n_jobs=1,
+                 **kwargs):
         super(RadiusNeighborsRegressor, self).__init__(
               radius=radius,
               algorithm=algorithm,
               leaf_size=leaf_size,
-              p=p, metric=metric, metric_params=metric_params, **kwargs)
+              p=p, metric=metric, metric_params=metric_params,
+              n_jobs=n_jobs, **kwargs)
         self.weights = _check_weights(weights)
 
     def predict(self, X):
diff --git a/sklearn/neighbors/tests/test_neighbors.py b/sklearn/neighbors/tests/test_neighbors.py
index ec4371ce82a9..a95a906ad3cb 100644
--- a/sklearn/neighbors/tests/test_neighbors.py
+++ b/sklearn/neighbors/tests/test_neighbors.py
@@ -18,6 +18,7 @@
 from sklearn.utils.testing import assert_greater
 from sklearn.utils.testing import assert_in
 from sklearn.utils.testing import assert_raises
+from sklearn.utils.testing import assert_raises_regex
 from sklearn.utils.testing import assert_true
 from sklearn.utils.testing import assert_warns
 from sklearn.utils.testing import assert_warns_message
@@ -108,6 +109,21 @@ def test_unsupervised_inputs():
         assert_array_almost_equal(ind1, ind2)
 
 
+def test_n_neighbors_datatype():
+    # Test to check whether n_neighbors is integer
+    X = [[1, 1], [1, 1], [1, 1]]
+    expected_msg = "n_neighbors does not take .*float.* " \
+                   "value, enter integer value"
+    msg = "Expected n_neighbors > 0. Got -3"
+
+    neighbors_ = neighbors.NearestNeighbors(n_neighbors=3.)
+    assert_raises_regex(TypeError, expected_msg, neighbors_.fit, X)
+    assert_raises_regex(ValueError, msg,
+                        neighbors_.kneighbors, X=X, n_neighbors=-3)
+    assert_raises_regex(TypeError, expected_msg,
+                        neighbors_.kneighbors, X=X, n_neighbors=3.)
+
+
 def test_precomputed(random_state=42):
     """Tests unsupervised NearestNeighbors with a distance matrix."""
     # Note: smaller samples may result in spurious test success
@@ -1273,6 +1289,36 @@ def check_same_knn_parallel(algorithm):
         yield check_same_knn_parallel, algorithm
 
 
+def test_same_radius_neighbors_parallel():
+    X, y = datasets.make_classification(n_samples=30, n_features=5,
+                                        n_redundant=0, random_state=0)
+    X_train, X_test, y_train, y_test = train_test_split(X, y)
+
+    def check_same_radius_neighbors_parallel(algorithm):
+        clf = neighbors.RadiusNeighborsClassifier(radius=10,
+                                                  algorithm=algorithm)
+        clf.fit(X_train, y_train)
+        y = clf.predict(X_test)
+        dist, ind = clf.radius_neighbors(X_test)
+        graph = clf.radius_neighbors_graph(X_test, mode='distance').toarray()
+
+        clf.set_params(n_jobs=3)
+        clf.fit(X_train, y_train)
+        y_parallel = clf.predict(X_test)
+        dist_parallel, ind_parallel = clf.radius_neighbors(X_test)
+        graph_parallel = \
+            clf.radius_neighbors_graph(X_test, mode='distance').toarray()
+
+        assert_array_equal(y, y_parallel)
+        for i in range(len(dist)):
+            assert_array_almost_equal(dist[i], dist_parallel[i])
+            assert_array_equal(ind[i], ind_parallel[i])
+        assert_array_almost_equal(graph, graph_parallel)
+
+    for algorithm in ALGORITHMS:
+        yield check_same_radius_neighbors_parallel, algorithm
+
+
 def test_dtype_convert():
     classifier = neighbors.KNeighborsClassifier(n_neighbors=1)
     CLASSES = 15
diff --git a/sklearn/neighbors/unsupervised.py b/sklearn/neighbors/unsupervised.py
index 605db227302c..db19e8df6b9f 100644
--- a/sklearn/neighbors/unsupervised.py
+++ b/sklearn/neighbors/unsupervised.py
@@ -77,7 +77,6 @@ class NearestNeighbors(NeighborsBase, KNeighborsMixin,
     n_jobs : int, optional (default = 1)
         The number of parallel jobs to run for neighbors search.
         If ``-1``, then the number of jobs is set to the number of CPU cores.
-        Affects only :meth:`kneighbors` and :meth:`kneighbors_graph` methods.
 
     Examples
     --------
diff --git a/sklearn/preprocessing/data.py b/sklearn/preprocessing/data.py
index e8a2e92299c3..fb8f443e9c7a 100644
--- a/sklearn/preprocessing/data.py
+++ b/sklearn/preprocessing/data.py
@@ -25,7 +25,7 @@
 from ..utils import check_array
 from ..utils.extmath import row_norms
 from ..utils.extmath import _incremental_mean_and_var
-from ..utils.fixes import _argmax
+from ..utils.fixes import _argmax, nanpercentile
 from ..utils.sparsefuncs_fast import (inplace_csr_row_normalize_l1,
                                       inplace_csr_row_normalize_l2)
 from ..utils.sparsefuncs import (inplace_column_scale,
@@ -276,6 +276,9 @@ class MinMaxScaler(BaseEstimator, TransformerMixin):
 
     Notes
     -----
+    NaNs are treated as missing values: disregarded in fit, and maintained in
+    transform.
+
     For a comparison of the different scalers, transformers, and normalizers,
     see :ref:`examples/preprocessing/plot_all_scaling.py
     <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.
@@ -340,10 +343,11 @@ def partial_fit(self, X, y=None):
                             "You may consider to use MaxAbsScaler instead.")
 
         X = check_array(X, copy=self.copy, warn_on_dtype=True,
-                        estimator=self, dtype=FLOAT_DTYPES)
+                        estimator=self, dtype=FLOAT_DTYPES,
+                        force_all_finite="allow-nan")
 
-        data_min = np.min(X, axis=0)
-        data_max = np.max(X, axis=0)
+        data_min = np.nanmin(X, axis=0)
+        data_max = np.nanmax(X, axis=0)
 
         # First pass
         if not hasattr(self, 'n_samples_seen_'):
@@ -373,7 +377,8 @@ def transform(self, X):
         """
         check_is_fitted(self, 'scale_')
 
-        X = check_array(X, copy=self.copy, dtype=FLOAT_DTYPES)
+        X = check_array(X, copy=self.copy, dtype=FLOAT_DTYPES,
+                        force_all_finite="allow-nan")
 
         X *= self.scale_
         X += self.min_
@@ -389,7 +394,8 @@ def inverse_transform(self, X):
         """
         check_is_fitted(self, 'scale_')
 
-        X = check_array(X, copy=self.copy, dtype=FLOAT_DTYPES)
+        X = check_array(X, copy=self.copy, dtype=FLOAT_DTYPES,
+                        force_all_finite="allow-nan")
 
         X -= self.min_
         X /= self.scale_
@@ -956,11 +962,10 @@ class RobustScaler(BaseEstimator, TransformerMixin):
     The IQR is the range between the 1st quartile (25th quantile)
     and the 3rd quartile (75th quantile).
 
-    Centering and scaling happen independently on each feature (or each
-    sample, depending on the ``axis`` argument) by computing the relevant
-    statistics on the samples in the training set. Median and  interquartile
-    range are then stored to be used on later data using the ``transform``
-    method.
+    Centering and scaling happen independently on each feature by
+    computing the relevant statistics on the samples in the training
+    set. Median and interquartile range are then stored to be used on
+    later data using the ``transform`` method.
 
     Standardization of a dataset is a common requirement for many
     machine learning estimators. Typically this is done by removing the mean
@@ -2195,6 +2200,9 @@ class QuantileTransformer(BaseEstimator, TransformerMixin):
 
     Notes
     -----
+    NaNs are treated as missing values: disregarded in fit, and maintained in
+    transform.
+
     For a comparison of the different scalers, transformers, and normalizers,
     see :ref:`examples/preprocessing/plot_all_scaling.py
     <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.
@@ -2235,7 +2243,7 @@ def _dense_fit(self, X, random_state):
                                                     size=self.subsample,
                                                     replace=False)
                 col = col.take(subsample_idx, mode='clip')
-            self.quantiles_.append(np.percentile(col, references))
+            self.quantiles_.append(nanpercentile(col, references))
         self.quantiles_ = np.transpose(self.quantiles_)
 
     def _sparse_fit(self, X, random_state):
@@ -2280,8 +2288,7 @@ def _sparse_fit(self, X, random_state):
                 # quantiles. Force the quantiles to be zeros.
                 self.quantiles_.append([0] * len(references))
             else:
-                self.quantiles_.append(
-                    np.percentile(column_data, references))
+                self.quantiles_.append(nanpercentile(column_data, references))
         self.quantiles_ = np.transpose(self.quantiles_)
 
     def fit(self, X, y=None):
@@ -2350,30 +2357,36 @@ def _transform_col(self, X_col, quantiles, inverse):
             #  for inverse transform, match a uniform PDF
             X_col = output_distribution.cdf(X_col)
         # find index for lower and higher bounds
-        lower_bounds_idx = (X_col - BOUNDS_THRESHOLD <
-                            lower_bound_x)
-        upper_bounds_idx = (X_col + BOUNDS_THRESHOLD >
-                            upper_bound_x)
-
+        with np.errstate(invalid='ignore'):  # hide NaN comparison warnings
+            lower_bounds_idx = (X_col - BOUNDS_THRESHOLD <
+                                lower_bound_x)
+            upper_bounds_idx = (X_col + BOUNDS_THRESHOLD >
+                                upper_bound_x)
+
+        isfinite_mask = ~np.isnan(X_col)
+        X_col_finite = X_col[isfinite_mask]
         if not inverse:
             # Interpolate in one direction and in the other and take the
             # mean. This is in case of repeated values in the features
             # and hence repeated quantiles
             #
             # If we don't do this, only one extreme of the duplicated is
-            # used (the upper when we do assending, and the
+            # used (the upper when we do ascending, and the
             # lower for descending). We take the mean of these two
-            X_col = .5 * (np.interp(X_col, quantiles, self.references_)
-                          - np.interp(-X_col, -quantiles[::-1],
-                                      -self.references_[::-1]))
+            X_col[isfinite_mask] = .5 * (
+                np.interp(X_col_finite, quantiles, self.references_)
+                - np.interp(-X_col_finite, -quantiles[::-1],
+                            -self.references_[::-1]))
         else:
-            X_col = np.interp(X_col, self.references_, quantiles)
+            X_col[isfinite_mask] = np.interp(X_col_finite,
+                                             self.references_, quantiles)
 
         X_col[upper_bounds_idx] = upper_bound_y
         X_col[lower_bounds_idx] = lower_bound_y
         # for forward transform, match the output PDF
         if not inverse:
-            X_col = output_distribution.ppf(X_col)
+            with np.errstate(invalid='ignore'):  # hide NaN comparison warnings
+                X_col = output_distribution.ppf(X_col)
             # find the value to clip the data to avoid mapping to
             # infinity. Clip such that the inverse transform will be
             # consistent
@@ -2388,13 +2401,15 @@ def _transform_col(self, X_col, quantiles, inverse):
     def _check_inputs(self, X, accept_sparse_negative=False):
         """Check inputs before fit and transform"""
         X = check_array(X, accept_sparse='csc', copy=self.copy,
-                        dtype=[np.float64, np.float32])
+                        dtype=FLOAT_DTYPES,
+                        force_all_finite='allow-nan')
         # we only accept positive sparse matrix when ignore_implicit_zeros is
         # false and that we call fit or transform.
-        if (not accept_sparse_negative and not self.ignore_implicit_zeros and
-                (sparse.issparse(X) and np.any(X.data < 0))):
-            raise ValueError('QuantileTransformer only accepts non-negative'
-                             ' sparse matrices.')
+        with np.errstate(invalid='ignore'):  # hide NaN comparison warnings
+            if (not accept_sparse_negative and not self.ignore_implicit_zeros
+                    and (sparse.issparse(X) and np.any(X.data < 0))):
+                raise ValueError('QuantileTransformer only accepts'
+                                 ' non-negative sparse matrices.')
 
         # check the output PDF
         if self.output_distribution not in ('normal', 'uniform'):
@@ -2583,6 +2598,9 @@ def quantile_transform(X, axis=0, n_quantiles=1000,
 
     Notes
     -----
+    NaNs are treated as missing values: disregarded in fit, and maintained in
+    transform.
+
     For a comparison of the different scalers, transformers, and normalizers,
     see :ref:`examples/preprocessing/plot_all_scaling.py
     <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.
@@ -2773,7 +2791,7 @@ def _check_input(self, X, check_positive=False, check_shape=False,
         X : array-like, shape (n_samples, n_features)
 
         check_positive : bool
-            If True, check that all data is postive and non-zero.
+            If True, check that all data is positive and non-zero.
 
         check_shape : bool
             If True, check that n_features matches the length of self.lambdas_
diff --git a/sklearn/preprocessing/label.py b/sklearn/preprocessing/label.py
index 11b52280f88b..7f95a1426c87 100644
--- a/sklearn/preprocessing/label.py
+++ b/sklearn/preprocessing/label.py
@@ -9,6 +9,7 @@
 from collections import defaultdict
 import itertools
 import array
+import warnings
 
 import numpy as np
 import scipy.sparse as sp
@@ -684,6 +685,7 @@ class MultiLabelBinarizer(BaseEstimator, TransformerMixin):
     sklearn.preprocessing.OneHotEncoder : encode categorical integer features
         using a one-hot aka one-of-K scheme.
     """
+
     def __init__(self, classes=None, sparse_output=False):
         self.classes = classes
         self.sparse_output = sparse_output
@@ -794,9 +796,19 @@ def _transform(self, y, class_mapping):
         """
         indices = array.array('i')
         indptr = array.array('i', [0])
+        unknown = set()
         for labels in y:
-            indices.extend(set(class_mapping[label] for label in labels))
+            index = set()
+            for label in labels:
+                try:
+                    index.add(class_mapping[label])
+                except KeyError:
+                    unknown.add(label)
+            indices.extend(index)
             indptr.append(len(indices))
+        if unknown:
+            warnings.warn('unknown class(es) {0} will be ignored'
+                          .format(sorted(unknown, key=str)))
         data = np.ones(len(indices), dtype=int)
 
         return sp.csr_matrix((data, indices, indptr),
diff --git a/sklearn/preprocessing/tests/test_common.py b/sklearn/preprocessing/tests/test_common.py
new file mode 100644
index 000000000000..1488ceaba12c
--- /dev/null
+++ b/sklearn/preprocessing/tests/test_common.py
@@ -0,0 +1,83 @@
+import pytest
+import numpy as np
+
+from scipy import sparse
+
+from sklearn.datasets import load_iris
+from sklearn.model_selection import train_test_split
+
+from sklearn.base import clone
+
+from sklearn.preprocessing import QuantileTransformer
+from sklearn.preprocessing import MinMaxScaler
+
+from sklearn.utils.testing import assert_array_equal
+from sklearn.utils.testing import assert_allclose
+
+iris = load_iris()
+
+
+def _get_valid_samples_by_column(X, col):
+    """Get non NaN samples in column of X"""
+    return X[:, [col]][~np.isnan(X[:, col])]
+
+
+@pytest.mark.parametrize(
+    "est, support_sparse",
+    [(MinMaxScaler(), False),
+     (QuantileTransformer(n_quantiles=10, random_state=42), True)]
+)
+def test_missing_value_handling(est, support_sparse):
+    # check that the preprocessing method let pass nan
+    rng = np.random.RandomState(42)
+    X = iris.data.copy()
+    n_missing = 50
+    X[rng.randint(X.shape[0], size=n_missing),
+      rng.randint(X.shape[1], size=n_missing)] = np.nan
+    X_train, X_test = train_test_split(X, random_state=1)
+    # sanity check
+    assert not np.all(np.isnan(X_train), axis=0).any()
+    assert np.any(np.isnan(X_train), axis=0).all()
+    assert np.any(np.isnan(X_test), axis=0).all()
+    X_test[:, 0] = np.nan  # make sure this boundary case is tested
+
+    Xt = est.fit(X_train).transform(X_test)
+    # missing values should still be missing, and only them
+    assert_array_equal(np.isnan(Xt), np.isnan(X_test))
+
+    # check that the inverse transform keep NaN
+    Xt_inv = est.inverse_transform(Xt)
+    assert_array_equal(np.isnan(Xt_inv), np.isnan(X_test))
+    # FIXME: we can introduce equal_nan=True in recent version of numpy.
+    # For the moment which just check that non-NaN values are almost equal.
+    assert_allclose(Xt_inv[~np.isnan(Xt_inv)], X_test[~np.isnan(X_test)])
+
+    for i in range(X.shape[1]):
+        # train only on non-NaN
+        est.fit(_get_valid_samples_by_column(X_train, i))
+        # check transforming with NaN works even when training without NaN
+        Xt_col = est.transform(X_test[:, [i]])
+        assert_array_equal(Xt_col, Xt[:, [i]])
+        # check non-NaN is handled as before - the 1st column is all nan
+        if not np.isnan(X_test[:, i]).all():
+            Xt_col_nonan = est.transform(
+                _get_valid_samples_by_column(X_test, i))
+            assert_array_equal(Xt_col_nonan,
+                               Xt_col[~np.isnan(Xt_col.squeeze())])
+
+    if support_sparse:
+        est_dense = clone(est)
+        est_sparse = clone(est)
+
+        Xt_dense = est_dense.fit(X_train).transform(X_test)
+        Xt_inv_dense = est_dense.inverse_transform(Xt_dense)
+        for sparse_constructor in (sparse.csr_matrix, sparse.csc_matrix,
+                                   sparse.bsr_matrix, sparse.coo_matrix,
+                                   sparse.dia_matrix, sparse.dok_matrix,
+                                   sparse.lil_matrix):
+            # check that the dense and sparse inputs lead to the same results
+            Xt_sparse = (est_sparse.fit(sparse_constructor(X_train))
+                         .transform(sparse_constructor(X_test)))
+            assert_allclose(Xt_sparse.A, Xt_dense)
+            Xt_inv_sparse = est_sparse.inverse_transform(Xt_sparse)
+            assert_allclose(Xt_inv_sparse.A, Xt_inv_dense)
diff --git a/sklearn/preprocessing/tests/test_data.py b/sklearn/preprocessing/tests/test_data.py
index 51c37097adca..e3bf4096750d 100644
--- a/sklearn/preprocessing/tests/test_data.py
+++ b/sklearn/preprocessing/tests/test_data.py
@@ -1210,6 +1210,20 @@ def test_quantile_transform_and_inverse():
     assert_array_almost_equal(X, X_trans_inv)
 
 
+def test_quantile_transform_nan():
+    X = np.array([[np.nan, 0,  0, 1],
+                  [np.nan, np.nan, 0, 0.5],
+                  [np.nan, 1, 1, 0]])
+
+    transformer = QuantileTransformer(n_quantiles=10, random_state=42)
+    transformer.fit_transform(X)
+
+    # check that the quantile of the first column is all NaN
+    assert np.isnan(transformer.quantiles_[:, 0]).all()
+    # all other column should not contain NaN
+    assert not np.isnan(transformer.quantiles_[:, 1:]).any()
+
+
 def test_robust_scaler_invalid_range():
     for range_ in [
         (-1, 90),
diff --git a/sklearn/preprocessing/tests/test_label.py b/sklearn/preprocessing/tests/test_label.py
index 2bc3ae17ea2d..14788b14b521 100644
--- a/sklearn/preprocessing/tests/test_label.py
+++ b/sklearn/preprocessing/tests/test_label.py
@@ -14,6 +14,7 @@
 from sklearn.utils.testing import assert_true
 from sklearn.utils.testing import assert_raises
 from sklearn.utils.testing import assert_raise_message
+from sklearn.utils.testing import assert_warns_message
 from sklearn.utils.testing import ignore_warnings
 
 from sklearn.preprocessing.label import LabelBinarizer
@@ -307,10 +308,17 @@ def test_multilabel_binarizer_empty_sample():
 def test_multilabel_binarizer_unknown_class():
     mlb = MultiLabelBinarizer()
     y = [[1, 2]]
-    assert_raises(KeyError, mlb.fit(y).transform, [[0]])
-
-    mlb = MultiLabelBinarizer(classes=[1, 2])
-    assert_raises(KeyError, mlb.fit_transform, [[0]])
+    Y = np.array([[1, 0], [0, 1]])
+    w = 'unknown class(es) [0, 4] will be ignored'
+    matrix = assert_warns_message(UserWarning, w,
+                                  mlb.fit(y).transform, [[4, 1], [2, 0]])
+    assert_array_equal(matrix, Y)
+
+    Y = np.array([[1, 0, 0], [0, 1, 0]])
+    mlb = MultiLabelBinarizer(classes=[1, 2, 3])
+    matrix = assert_warns_message(UserWarning, w,
+                                  mlb.fit(y).transform, [[4, 1], [2, 0]])
+    assert_array_equal(matrix, Y)
 
 
 def test_multilabel_binarizer_given_classes():
diff --git a/sklearn/src/cblas/atlas_misc.h b/sklearn/src/cblas/atlas_misc.h
index aa8fa2a1f8a5..55da67c93482 100644
--- a/sklearn/src/cblas/atlas_misc.h
+++ b/sklearn/src/cblas/atlas_misc.h
@@ -184,7 +184,7 @@
 #elif defined(QCPLX)
    #define TYPE ATL_QTYPE
    #define PRE e
-   #deffine UPR q
+   #define UPR q
    #define PREU E
    #define PATL ATL_e
    #define PATLU ATL_q
diff --git a/sklearn/svm/base.py b/sklearn/svm/base.py
index 289064718bf6..1a5b9b18c88b 100644
--- a/sklearn/svm/base.py
+++ b/sklearn/svm/base.py
@@ -907,7 +907,7 @@ def _fit_liblinear(X, y, C, fit_intercept, intercept_scaling, class_weight,
     # on 32-bit platforms, we can't get to the UINT_MAX limit that
     # srand supports
     n_iter_ = max(n_iter_)
-    if n_iter_ >= max_iter and verbose > 0:
+    if n_iter_ >= max_iter:
         warnings.warn("Liblinear failed to converge, increase "
                       "the number of iterations.", ConvergenceWarning)
 
diff --git a/sklearn/svm/classes.py b/sklearn/svm/classes.py
index f6f8f9ed4b80..c1070b4f6981 100644
--- a/sklearn/svm/classes.py
+++ b/sklearn/svm/classes.py
@@ -504,7 +504,8 @@ class SVC(BaseSVC):
         Whether to return a one-vs-rest ('ovr') decision function of shape
         (n_samples, n_classes) as all other classifiers, or the original
         one-vs-one ('ovo') decision function of libsvm which has shape
-        (n_samples, n_classes * (n_classes - 1) / 2).
+        (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one
+        ('ovo') is used as multi-class strategy.
 
         .. versionchanged:: 0.19
             decision_function_shape is 'ovr' by default.
diff --git a/sklearn/svm/tests/test_svm.py b/sklearn/svm/tests/test_svm.py
index d3384ffe6a5e..ead2d1cd27fd 100644
--- a/sklearn/svm/tests/test_svm.py
+++ b/sklearn/svm/tests/test_svm.py
@@ -871,13 +871,17 @@ def test_consistent_proba():
     assert_array_almost_equal(proba_1, proba_2)
 
 
-def test_linear_svc_convergence_warnings():
+def test_linear_svm_convergence_warnings():
     # Test that warnings are raised if model does not converge
 
-    lsvc = svm.LinearSVC(max_iter=2, verbose=1)
+    lsvc = svm.LinearSVC(random_state=0, max_iter=2)
     assert_warns(ConvergenceWarning, lsvc.fit, X, Y)
     assert_equal(lsvc.n_iter_, 2)
 
+    lsvr = svm.LinearSVR(random_state=0, max_iter=2)
+    assert_warns(ConvergenceWarning, lsvr.fit, iris.data, iris.target)
+    assert_equal(lsvr.n_iter_, 2)
+
 
 def test_svr_coef_sign():
     # Test that SVR(kernel="linear") has coef_ with the right sign.
diff --git a/sklearn/tests/test_common.py b/sklearn/tests/test_common.py
index a2f17da48779..a0e7ff82ad43 100644
--- a/sklearn/tests/test_common.py
+++ b/sklearn/tests/test_common.py
@@ -13,6 +13,8 @@
 import re
 import pkgutil
 
+import pytest
+
 from sklearn.utils.testing import assert_false, clean_warning_registry
 from sklearn.utils.testing import all_estimators
 from sklearn.utils.testing import assert_equal
@@ -41,34 +43,57 @@ def test_all_estimator_no_base_class():
 
 
 def test_all_estimators():
-    # Test that estimators are default-constructible, cloneable
-    # and have working repr.
     estimators = all_estimators(include_meta_estimators=True)
 
     # Meta sanity-check to make sure that the estimator introspection runs
     # properly
     assert_greater(len(estimators), 0)
 
-    for name, Estimator in estimators:
-        # some can just not be sensibly default constructed
-        yield check_parameters_default_constructible, name, Estimator
 
+@pytest.mark.parametrize(
+        'name, Estimator',
+        all_estimators(include_meta_estimators=True)
+)
+def test_parameters_default_constructible(name, Estimator):
+    # Test that estimators are default-constructible
+    check_parameters_default_constructible(name, Estimator)
 
-def test_non_meta_estimators():
-    # input validation etc for non-meta estimators
-    estimators = all_estimators()
-    for name, Estimator in estimators:
+
+def _tested_non_meta_estimators():
+    for name, Estimator in all_estimators():
         if issubclass(Estimator, BiclusterMixin):
             continue
         if name.startswith("_"):
             continue
+        yield name, Estimator
+
+
+def _generate_checks_per_estimator(check_generator, estimators):
+    for name, Estimator in estimators:
         estimator = Estimator()
-        # check this on class
-        yield check_no_attributes_set_in_init, name, estimator
+        for check in check_generator(name, estimator):
+            yield name, Estimator, check
 
-        for check in _yield_all_checks(name, estimator):
-            set_checking_parameters(estimator)
-            yield check, name, estimator
+
+@pytest.mark.parametrize(
+        "name, Estimator, check",
+        _generate_checks_per_estimator(_yield_all_checks,
+                                       _tested_non_meta_estimators())
+)
+def test_non_meta_estimators(name, Estimator, check):
+    # Common tests for non-meta estimators
+    estimator = Estimator()
+    set_checking_parameters(estimator)
+    check(name, estimator)
+
+
+@pytest.mark.parametrize("name, Estimator",
+                         _tested_non_meta_estimators())
+def test_no_attributes_set_in_init(name, Estimator):
+    # input validation etc for non-meta estimators
+    estimator = Estimator()
+    # check this on class
+    check_no_attributes_set_in_init(name, estimator)
 
 
 def test_configure():
@@ -95,19 +120,21 @@ def test_configure():
         os.chdir(cwd)
 
 
-def test_class_weight_balanced_linear_classifiers():
+def _tested_linear_classifiers():
     classifiers = all_estimators(type_filter='classifier')
 
     clean_warning_registry()
     with warnings.catch_warnings(record=True):
-        linear_classifiers = [
-            (name, clazz)
-            for name, clazz in classifiers
+        for name, clazz in classifiers:
             if ('class_weight' in clazz().get_params().keys() and
-                issubclass(clazz, LinearClassifierMixin))]
+                    issubclass(clazz, LinearClassifierMixin)):
+                yield name, clazz
+
 
-    for name, Classifier in linear_classifiers:
-        yield check_class_weight_balanced_linear_classifier, name, Classifier
+@pytest.mark.parametrize("name, Classifier",
+                         _tested_linear_classifiers())
+def test_class_weight_balanced_linear_classifiers(name, Classifier):
+    check_class_weight_balanced_linear_classifier(name, Classifier)
 
 
 @ignore_warnings
diff --git a/sklearn/tests/test_impute.py b/sklearn/tests/test_impute.py
index 802ab82e406e..954a016a835b 100644
--- a/sklearn/tests/test_impute.py
+++ b/sklearn/tests/test_impute.py
@@ -1,14 +1,19 @@
+from __future__ import division
+
+import pytest
 
 import numpy as np
 from scipy import sparse
 
-from sklearn.utils.testing import assert_equal
+from sklearn.utils.testing import assert_allclose
 from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_raises
 from sklearn.utils.testing import assert_false
 
-from sklearn.impute import SimpleImputer
+from sklearn.impute import SimpleImputer, MICEImputer
+from sklearn.dummy import DummyRegressor
+from sklearn.linear_model import BayesianRidge, ARDRegression
 from sklearn.pipeline import Pipeline
 from sklearn.model_selection import GridSearchCV
 from sklearn import tree
@@ -34,25 +39,15 @@ def _check_statistics(X, X_true,
     if X.dtype.kind == 'f' or X_true.dtype.kind == 'f':
         assert_ae = assert_array_almost_equal
 
-    # Normal matrix, axis = 0
-    imputer = SimpleImputer(missing_values, strategy=strategy, axis=0)
+    # Normal matrix
+    imputer = SimpleImputer(missing_values, strategy=strategy)
     X_trans = imputer.fit(X).transform(X.copy())
     assert_ae(imputer.statistics_, statistics,
               err_msg=err_msg.format(0, False))
     assert_ae(X_trans, X_true, err_msg=err_msg.format(0, False))
 
-    # Normal matrix, axis = 1
-    imputer = SimpleImputer(missing_values, strategy=strategy, axis=1)
-    imputer.fit(X.transpose())
-    if np.isnan(statistics).any():
-        assert_raises(ValueError, imputer.transform, X.copy().transpose())
-    else:
-        X_trans = imputer.transform(X.copy().transpose())
-        assert_ae(X_trans, X_true.transpose(),
-                  err_msg=err_msg.format(1, False))
-
-    # Sparse matrix, axis = 0
-    imputer = SimpleImputer(missing_values, strategy=strategy, axis=0)
+    # Sparse matrix
+    imputer = SimpleImputer(missing_values, strategy=strategy)
     imputer.fit(sparse.csc_matrix(X))
     X_trans = imputer.transform(sparse.csc_matrix(X.copy()))
 
@@ -63,21 +58,6 @@ def _check_statistics(X, X_true,
               err_msg=err_msg.format(0, True))
     assert_ae(X_trans, X_true, err_msg=err_msg.format(0, True))
 
-    # Sparse matrix, axis = 1
-    imputer = SimpleImputer(missing_values, strategy=strategy, axis=1)
-    imputer.fit(sparse.csc_matrix(X.transpose()))
-    if np.isnan(statistics).any():
-        assert_raises(ValueError, imputer.transform,
-                      sparse.csc_matrix(X.copy().transpose()))
-    else:
-        X_trans = imputer.transform(sparse.csc_matrix(X.copy().transpose()))
-
-        if sparse.issparse(X_trans):
-            X_trans = X_trans.toarray()
-
-        assert_ae(X_trans, X_true.transpose(),
-                  err_msg=err_msg.format(1, True))
-
 
 def test_imputation_shape():
     # Verify the shapes of the imputed matrix for different strategies.
@@ -86,10 +66,14 @@ def test_imputation_shape():
 
     for strategy in ['mean', 'median', 'most_frequent']:
         imputer = SimpleImputer(strategy=strategy)
-        X_imputed = imputer.fit_transform(X)
-        assert_equal(X_imputed.shape, (10, 2))
         X_imputed = imputer.fit_transform(sparse.csr_matrix(X))
-        assert_equal(X_imputed.shape, (10, 2))
+        assert X_imputed.shape == (10, 2)
+        X_imputed = imputer.fit_transform(X)
+        assert X_imputed.shape == (10, 2)
+
+        mice_imputer = MICEImputer(initial_strategy=strategy)
+        X_imputed = mice_imputer.fit_transform(X)
+        assert X_imputed.shape == (10, 2)
 
 
 def safe_median(arr, *args, **kwargs):
@@ -232,8 +216,7 @@ def test_imputation_pipeline_grid_search():
                          ('tree', tree.DecisionTreeRegressor(random_state=0))])
 
     parameters = {
-        'imputer__strategy': ["mean", "median", "most_frequent"],
-        'imputer__axis': [0, 1]
+        'imputer__strategy': ["mean", "median", "most_frequent"]
     }
 
     X = sparse_random_matrix(100, 100, density=0.10)
@@ -268,44 +251,242 @@ def test_imputation_copy():
     Xt[0, 0] = -1
     assert_array_almost_equal(X, Xt)
 
-    # copy=False, sparse csr, axis=1 => no copy
-    X = X_orig.copy()
-    imputer = SimpleImputer(missing_values=X.data[0], strategy="mean",
-                            copy=False, axis=1)
-    Xt = imputer.fit(X).transform(X)
-    Xt.data[0] = -1
-    assert_array_almost_equal(X.data, Xt.data)
-
-    # copy=False, sparse csc, axis=0 => no copy
+    # copy=False, sparse csc => no copy
     X = X_orig.copy().tocsc()
     imputer = SimpleImputer(missing_values=X.data[0], strategy="mean",
-                            copy=False, axis=0)
+                            copy=False)
     Xt = imputer.fit(X).transform(X)
     Xt.data[0] = -1
     assert_array_almost_equal(X.data, Xt.data)
 
-    # copy=False, sparse csr, axis=0 => copy
+    # copy=False, sparse csr => copy
     X = X_orig.copy()
     imputer = SimpleImputer(missing_values=X.data[0], strategy="mean",
-                            copy=False, axis=0)
+                            copy=False)
     Xt = imputer.fit(X).transform(X)
     Xt.data[0] = -1
     assert_false(np.all(X.data == Xt.data))
 
-    # copy=False, sparse csc, axis=1 => copy
-    X = X_orig.copy().tocsc()
-    imputer = SimpleImputer(missing_values=X.data[0], strategy="mean",
-                            copy=False, axis=1)
-    Xt = imputer.fit(X).transform(X)
-    Xt.data[0] = -1
-    assert_false(np.all(X.data == Xt.data))
-
-    # copy=False, sparse csr, axis=1, missing_values=0 => copy
-    X = X_orig.copy()
-    imputer = SimpleImputer(missing_values=0, strategy="mean",
-                            copy=False, axis=1)
-    Xt = imputer.fit(X).transform(X)
-    assert_false(sparse.issparse(Xt))
-
     # Note: If X is sparse and if missing_values=0, then a (dense) copy of X is
     # made, even if copy=False.
+
+
+def test_mice_rank_one():
+    rng = np.random.RandomState(0)
+    d = 100
+    A = rng.rand(d, 1)
+    B = rng.rand(1, d)
+    X = np.dot(A, B)
+    nan_mask = rng.rand(d, d) < 0.5
+    X_missing = X.copy()
+    X_missing[nan_mask] = np.nan
+
+    imputer = MICEImputer(n_imputations=5,
+                          n_burn_in=5,
+                          verbose=True,
+                          random_state=rng)
+    X_filled = imputer.fit_transform(X_missing)
+    assert_allclose(X_filled, X, atol=0.001)
+
+
+@pytest.mark.parametrize(
+    "imputation_order",
+    ['random', 'roman', 'ascending', 'descending', 'arabic']
+)
+def test_mice_imputation_order(imputation_order):
+    rng = np.random.RandomState(0)
+    n = 100
+    d = 10
+    X = sparse_random_matrix(n, d, density=0.10, random_state=rng).toarray()
+    X[:, 0] = 1  # this column should not be discarded by MICEImputer
+
+    imputer = MICEImputer(missing_values=0,
+                          n_imputations=1,
+                          n_burn_in=1,
+                          n_nearest_features=5,
+                          min_value=0,
+                          max_value=1,
+                          verbose=False,
+                          imputation_order=imputation_order,
+                          random_state=rng)
+    imputer.fit_transform(X)
+    ordered_idx = [i.feat_idx for i in imputer.imputation_sequence_]
+    if imputation_order == 'roman':
+        assert np.all(ordered_idx[:d-1] == np.arange(1, d))
+    elif imputation_order == 'arabic':
+        assert np.all(ordered_idx[:d-1] == np.arange(d-1, 0, -1))
+    elif imputation_order == 'random':
+        ordered_idx_round_1 = ordered_idx[:d-1]
+        ordered_idx_round_2 = ordered_idx[d-1:]
+        assert ordered_idx_round_1 != ordered_idx_round_2
+    elif 'ending' in imputation_order:
+        assert len(ordered_idx) == 2 * (d - 1)
+
+
+@pytest.mark.parametrize(
+    "predictor",
+    [DummyRegressor(), BayesianRidge(), ARDRegression()]
+)
+def test_mice_predictors(predictor):
+    rng = np.random.RandomState(0)
+
+    n = 100
+    d = 10
+    X = sparse_random_matrix(n, d, density=0.10, random_state=rng).toarray()
+
+    imputer = MICEImputer(missing_values=0,
+                          n_imputations=1,
+                          n_burn_in=1,
+                          predictor=predictor,
+                          random_state=rng)
+    imputer.fit_transform(X)
+
+    # check that types are correct for predictors
+    hashes = []
+    for triplet in imputer.imputation_sequence_:
+        assert triplet.predictor
+        hashes.append(id(triplet.predictor))
+
+    # check that each predictor is unique
+    assert len(set(hashes)) == len(hashes)
+
+
+def test_mice_clip():
+    rng = np.random.RandomState(0)
+    n = 100
+    d = 10
+    X = sparse_random_matrix(n, d, density=0.10,
+                             random_state=rng).toarray()
+
+    imputer = MICEImputer(missing_values=0,
+                          n_imputations=1,
+                          n_burn_in=1,
+                          min_value=0.1,
+                          max_value=0.2,
+                          random_state=rng)
+
+    Xt = imputer.fit_transform(X)
+    assert_allclose(np.min(Xt[X == 0]), 0.1)
+    assert_allclose(np.max(Xt[X == 0]), 0.2)
+    assert_allclose(Xt[X != 0], X[X != 0])
+
+
+@pytest.mark.parametrize(
+    "strategy",
+    ["mean", "median", "most_frequent"]
+)
+def test_mice_missing_at_transform(strategy):
+    rng = np.random.RandomState(0)
+    n = 100
+    d = 10
+    X_train = rng.randint(low=0, high=3, size=(n, d))
+    X_test = rng.randint(low=0, high=3, size=(n, d))
+
+    X_train[:, 0] = 1  # definitely no missing values in 0th column
+    X_test[0, 0] = 0  # definitely missing value in 0th column
+
+    mice = MICEImputer(missing_values=0,
+                       n_imputations=1,
+                       n_burn_in=1,
+                       initial_strategy=strategy,
+                       random_state=rng).fit(X_train)
+    initial_imputer = SimpleImputer(missing_values=0,
+                                    strategy=strategy).fit(X_train)
+
+    # if there were no missing values at time of fit, then mice will
+    # only use the initial imputer for that feature at transform
+    assert np.all(mice.transform(X_test)[:, 0] ==
+                  initial_imputer.transform(X_test)[:, 0])
+
+
+def test_mice_transform_stochasticity():
+    rng = np.random.RandomState(0)
+    n = 100
+    d = 10
+    X = sparse_random_matrix(n, d, density=0.10,
+                             random_state=rng).toarray()
+
+    imputer = MICEImputer(missing_values=0,
+                          n_imputations=1,
+                          n_burn_in=1,
+                          random_state=rng)
+    imputer.fit(X)
+
+    X_fitted_1 = imputer.transform(X)
+    X_fitted_2 = imputer.transform(X)
+
+    # sufficient to assert that the means are not the same
+    assert np.mean(X_fitted_1) != pytest.approx(np.mean(X_fitted_2))
+
+
+def test_mice_no_missing():
+    rng = np.random.RandomState(0)
+    X = rng.rand(100, 100)
+    X[:, 0] = np.nan
+    m1 = MICEImputer(n_imputations=10, random_state=rng)
+    m2 = MICEImputer(n_imputations=10, random_state=rng)
+    pred1 = m1.fit(X).transform(X)
+    pred2 = m2.fit_transform(X)
+    # should exclude the first column entirely
+    assert_allclose(X[:, 1:], pred1)
+    # fit and fit_transform should both be identical
+    assert_allclose(pred1, pred2)
+
+
+@pytest.mark.parametrize(
+    "rank",
+    [3, 5]
+)
+def test_mice_transform_recovery(rank):
+    rng = np.random.RandomState(0)
+    n = 100
+    d = 100
+    A = rng.rand(n, rank)
+    B = rng.rand(rank, d)
+    X_filled = np.dot(A, B)
+    # half is randomly missing
+    nan_mask = rng.rand(n, d) < 0.5
+    X_missing = X_filled.copy()
+    X_missing[nan_mask] = np.nan
+
+    # split up data in half
+    n = n // 2
+    X_train = X_missing[:n]
+    X_test_filled = X_filled[n:]
+    X_test = X_missing[n:]
+
+    imputer = MICEImputer(n_imputations=10,
+                          n_burn_in=10,
+                          verbose=True,
+                          random_state=rng).fit(X_train)
+    X_test_est = imputer.transform(X_test)
+    assert_allclose(X_test_filled, X_test_est, rtol=1e-5, atol=0.1)
+
+
+def test_mice_additive_matrix():
+    rng = np.random.RandomState(0)
+    n = 100
+    d = 10
+    A = rng.randn(n, d)
+    B = rng.randn(n, d)
+    X_filled = np.zeros(A.shape)
+    for i in range(d):
+        for j in range(d):
+            X_filled[:, (i+j) % d] += (A[:, i] + B[:, j]) / 2
+    # a quarter is randomly missing
+    nan_mask = rng.rand(n, d) < 0.25
+    X_missing = X_filled.copy()
+    X_missing[nan_mask] = np.nan
+
+    # split up data
+    n = n // 2
+    X_train = X_missing[:n]
+    X_test_filled = X_filled[n:]
+    X_test = X_missing[n:]
+
+    imputer = MICEImputer(n_imputations=25,
+                          n_burn_in=10,
+                          verbose=True,
+                          random_state=rng).fit(X_train)
+    X_test_est = imputer.transform(X_test)
+    assert_allclose(X_test_filled, X_test_est, atol=0.01)
diff --git a/sklearn/tests/test_multiclass.py b/sklearn/tests/test_multiclass.py
index f89e156bfd55..78a1fd617ccf 100644
--- a/sklearn/tests/test_multiclass.py
+++ b/sklearn/tests/test_multiclass.py
@@ -80,7 +80,7 @@ def test_ovr_fit_predict():
 
 
 def test_ovr_partial_fit():
-    # Test if partial_fit is working as intented
+    # Test if partial_fit is working as intended
     X, y = shuffle(iris.data, iris.target, random_state=0)
     ovr = OneVsRestClassifier(MultinomialNB())
     ovr.partial_fit(X[:100], y[:100], np.unique(y))
diff --git a/sklearn/tests/test_multioutput.py b/sklearn/tests/test_multioutput.py
index 23529c650ec0..717d680fa6fd 100644
--- a/sklearn/tests/test_multioutput.py
+++ b/sklearn/tests/test_multioutput.py
@@ -204,7 +204,7 @@ def test_multi_output_classification_partial_fit():
         assert_array_equal(sgd_linear_clf.predict(X), second_predictions[:, i])
 
 
-def test_mutli_output_classifiation_partial_fit_no_first_classes_exception():
+def test_multi_output_classification_partial_fit_no_first_classes_exception():
     sgd_linear_clf = SGDClassifier(loss='log', random_state=1, max_iter=5)
     multi_target_linear = MultiOutputClassifier(sgd_linear_clf)
     assert_raises_regex(ValueError, "classes must be passed on the first call "
diff --git a/sklearn/tree/_splitter.pyx b/sklearn/tree/_splitter.pyx
index 6bbf6986c8b4..3f5a176d9171 100644
--- a/sklearn/tree/_splitter.pyx
+++ b/sklearn/tree/_splitter.pyx
@@ -1253,7 +1253,7 @@ cdef class BestSparseSplitter(BaseSparseSplitter):
         cdef bint is_samples_sorted = 0  # indicate is sorted_samples is
                                          # inititialized
 
-        # We assume implicitely that end_positive = end and
+        # We assume implicitly that end_positive = end and
         # start_negative = start
         cdef SIZE_t start_positive
         cdef SIZE_t end_negative
@@ -1489,7 +1489,7 @@ cdef class RandomSparseSplitter(BaseSparseSplitter):
         cdef bint is_samples_sorted = 0  # indicate that sorted_samples is
                                          # inititialized
 
-        # We assume implicitely that end_positive = end and
+        # We assume implicitly that end_positive = end and
         # start_negative = start
         cdef SIZE_t start_positive
         cdef SIZE_t end_negative
diff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py
index 9a321e914b23..5e4c454f4b1a 100644
--- a/sklearn/utils/estimator_checks.py
+++ b/sklearn/utils/estimator_checks.py
@@ -6,10 +6,12 @@
 import traceback
 import pickle
 from copy import deepcopy
+import struct
+from functools import partial
+
 import numpy as np
 from scipy import sparse
 from scipy.stats import rankdata
-import struct
 
 from sklearn.externals.six.moves import zip
 from sklearn.externals.joblib import hash, Memory
@@ -33,6 +35,7 @@
 from sklearn.utils.testing import SkipTest
 from sklearn.utils.testing import ignore_warnings
 from sklearn.utils.testing import assert_dict_equal
+from sklearn.utils.testing import create_memmap_backed_data
 from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
 
 
@@ -73,6 +76,9 @@
                 'RANSACRegressor', 'RadiusNeighborsRegressor',
                 'RandomForestRegressor', 'Ridge', 'RidgeCV']
 
+ALLOW_NAN = ['Imputer', 'SimpleImputer', 'MICEImputer',
+             'MinMaxScaler', 'QuantileTransformer']
+
 
 def _yield_non_meta_checks(name, estimator):
     yield check_estimators_dtypes
@@ -81,6 +87,7 @@ def _yield_non_meta_checks(name, estimator):
     yield check_sample_weights_pandas_series
     yield check_sample_weights_list
     yield check_estimators_fit_returns_self
+    yield partial(check_estimators_fit_returns_self, readonly_memmap=True)
     yield check_complex_data
 
     # Check that all estimator yield informative messages when
@@ -93,7 +100,7 @@ def _yield_non_meta_checks(name, estimator):
         # cross-decomposition's "transform" returns X and Y
         yield check_pipeline_consistency
 
-    if name not in ['SimpleImputer', 'Imputer']:
+    if name not in ALLOW_NAN:
         # Test that all estimators check their input for NaN's and infs
         yield check_estimators_nan_inf
 
@@ -120,6 +127,7 @@ def _yield_classifier_checks(name, classifier):
     yield check_estimators_partial_fit_n_features
     # basic consistency testing
     yield check_classifiers_train
+    yield partial(check_classifiers_train, readonly_memmap=True)
     yield check_classifiers_regression_target
     if (name not in ["MultinomialNB", "ComplementNB", "LabelPropagation",
                      "LabelSpreading"] and
@@ -168,6 +176,7 @@ def _yield_regressor_checks(name, regressor):
     # TODO: test with multiple responses
     # basic testing
     yield check_regressors_train
+    yield partial(check_regressors_train, readonly_memmap=True)
     yield check_regressor_data_not_an_array
     yield check_estimators_partial_fit_n_features
     yield check_regressors_no_decision_function
@@ -193,6 +202,7 @@ def _yield_transformer_checks(name, transformer):
                     'FunctionTransformer', 'Normalizer']:
         # basic tests
         yield check_transformer_general
+        yield partial(check_transformer_general, readonly_memmap=True)
         yield check_transformers_unfitted
     # Dependent on external solvers and hence accessing the iter
     # param is non-trivial.
@@ -208,6 +218,7 @@ def _yield_clustering_checks(name, clusterer):
         # this is clustering on the features
         # let's not test that here.
         yield check_clustering
+        yield partial(check_clustering, readonly_memmap=True)
         yield check_estimators_partial_fit_n_features
     yield check_non_transformer_estimators_n_iter
 
@@ -220,6 +231,7 @@ def _yield_outliers_checks(name, estimator):
     # checks for estimators that can be used on a test set
     if hasattr(estimator, 'predict'):
         yield check_outliers_train
+        yield partial(check_outliers_train, readonly_memmap=True)
         # test outlier detectors can handle non-array data
         yield check_classifier_data_not_an_array
         # test if NotFittedError is raised
@@ -474,10 +486,11 @@ def check_sample_weights_pandas_series(name, estimator_orig):
     if has_fit_parameter(estimator, "sample_weight"):
         try:
             import pandas as pd
-            X = np.array([[1, 1], [1, 2], [1, 3], [2, 1], [2, 2], [2, 3]])
+            X = np.array([[1, 1], [1, 2], [1, 3], [1, 4],
+                          [2, 1], [2, 2], [2, 3], [2, 4]])
             X = pd.DataFrame(pairwise_estimator_convert_X(X, estimator_orig))
-            y = pd.Series([1, 1, 1, 2, 2, 2])
-            weights = pd.Series([1] * 6)
+            y = pd.Series([1, 1, 1, 1, 2, 2, 2, 2])
+            weights = pd.Series([1] * 8)
             try:
                 estimator.fit(X, y, sample_weight=weights)
             except ValueError:
@@ -796,7 +809,7 @@ def check_fit1d(name, estimator_orig):
 
 
 @ignore_warnings(category=(DeprecationWarning, FutureWarning))
-def check_transformer_general(name, transformer):
+def check_transformer_general(name, transformer, readonly_memmap=False):
     X, y = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],
                       random_state=0, n_features=2, cluster_std=0.1)
     X = StandardScaler().fit_transform(X)
@@ -804,6 +817,10 @@ def check_transformer_general(name, transformer):
     if name == 'PowerTransformer':
         # Box-Cox requires positive, non-zero data
         X += 1
+
+    if readonly_memmap:
+        X, y = create_memmap_backed_data([X, y])
+
     _check_transformer(name, transformer, X, y)
     _check_transformer(name, transformer, X.tolist(), y.tolist())
 
@@ -1162,11 +1179,17 @@ def check_estimators_partial_fit_n_features(name, estimator_orig):
 
 
 @ignore_warnings(category=(DeprecationWarning, FutureWarning))
-def check_clustering(name, clusterer_orig):
+def check_clustering(name, clusterer_orig, readonly_memmap=False):
     clusterer = clone(clusterer_orig)
     X, y = make_blobs(n_samples=50, random_state=1)
     X, y = shuffle(X, y, random_state=7)
     X = StandardScaler().fit_transform(X)
+    rng = np.random.RandomState(7)
+    X_noise = np.concatenate([X, rng.uniform(low=-3, high=3, size=(5, 2))])
+
+    if readonly_memmap:
+        X, y, X_noise = create_memmap_backed_data([X, y, X_noise])
+
     n_samples, n_features = X.shape
     # catch deprecation and neighbors warnings
     if hasattr(clusterer, "n_clusters"):
@@ -1198,8 +1221,6 @@ def check_clustering(name, clusterer_orig):
     assert_in(pred2.dtype, [np.dtype('int32'), np.dtype('int64')])
 
     # Add noise to X to test the possible values of the labels
-    rng = np.random.RandomState(7)
-    X_noise = np.concatenate([X, rng.uniform(low=-3, high=3, size=(5, 2))])
     labels = clusterer.fit_predict(X_noise)
 
     # There should be at least one sample in every cluster. Equivalently
@@ -1270,20 +1291,26 @@ def check_classifiers_one_label(name, classifier_orig):
 
 
 @ignore_warnings  # Warnings are raised by decision function
-def check_classifiers_train(name, classifier_orig):
+def check_classifiers_train(name, classifier_orig, readonly_memmap=False):
     X_m, y_m = make_blobs(n_samples=300, random_state=0)
     X_m, y_m = shuffle(X_m, y_m, random_state=7)
     X_m = StandardScaler().fit_transform(X_m)
     # generate binary problem from multi-class one
     y_b = y_m[y_m != 2]
     X_b = X_m[y_m != 2]
+
+    if name in ['BernoulliNB', 'MultinomialNB', 'ComplementNB']:
+        X_m -= X_m.min()
+        X_b -= X_b.min()
+
+    if readonly_memmap:
+        X_m, y_m, X_b, y_b = create_memmap_backed_data([X_m, y_m, X_b, y_b])
+
     for (X, y) in [(X_m, y_m), (X_b, y_b)]:
         classes = np.unique(y)
         n_classes = len(classes)
         n_samples, n_features = X.shape
         classifier = clone(classifier_orig)
-        if name in ['BernoulliNB', 'MultinomialNB', 'ComplementNB']:
-            X -= X.min()
         X = pairwise_estimator_convert_X(X, classifier_orig)
         set_random_state(classifier)
         # raises error on malformed input for fit
@@ -1379,9 +1406,13 @@ def check_classifiers_train(name, classifier_orig):
                 assert_array_equal(np.argsort(y_log_prob), np.argsort(y_prob))
 
 
-def check_outliers_train(name, estimator_orig):
+def check_outliers_train(name, estimator_orig, readonly_memmap=True):
     X, _ = make_blobs(n_samples=300, random_state=0)
     X = shuffle(X, random_state=7)
+
+    if readonly_memmap:
+        X = create_memmap_backed_data(X)
+
     n_samples, n_features = X.shape
     estimator = clone(estimator_orig)
     set_random_state(estimator)
@@ -1441,7 +1472,8 @@ def check_outliers_train(name, estimator_orig):
 
 
 @ignore_warnings(category=(DeprecationWarning, FutureWarning))
-def check_estimators_fit_returns_self(name, estimator_orig):
+def check_estimators_fit_returns_self(name, estimator_orig,
+                                      readonly_memmap=False):
     """Check if self is returned when calling fit"""
     X, y = make_blobs(random_state=0, n_samples=9, n_features=4)
     # some want non-negative input
@@ -1454,8 +1486,10 @@ def check_estimators_fit_returns_self(name, estimator_orig):
     estimator = clone(estimator_orig)
     y = multioutput_estimator_convert_y_2d(estimator, y)
 
-    set_random_state(estimator)
+    if readonly_memmap:
+        X, y = create_memmap_backed_data([X, y])
 
+    set_random_state(estimator)
     assert_true(estimator.fit(X, y) is estimator)
 
 
@@ -1634,14 +1668,23 @@ def check_regressors_int(name, regressor_orig):
 
 
 @ignore_warnings(category=(DeprecationWarning, FutureWarning))
-def check_regressors_train(name, regressor_orig):
+def check_regressors_train(name, regressor_orig, readonly_memmap=False):
     X, y = _boston_subset()
     X = pairwise_estimator_convert_X(X, regressor_orig)
     y = StandardScaler().fit_transform(y.reshape(-1, 1))  # X is already scaled
     y = y.ravel()
     regressor = clone(regressor_orig)
     y = multioutput_estimator_convert_y_2d(regressor, y)
-    rnd = np.random.RandomState(0)
+    if name in CROSS_DECOMPOSITION:
+        rnd = np.random.RandomState(0)
+        y_ = np.vstack([y, 2 * y + rnd.randint(2, size=len(y))])
+        y_ = y_.T
+    else:
+        y_ = y
+
+    if readonly_memmap:
+        X, y, y_ = create_memmap_backed_data([X, y, y_])
+
     if not hasattr(regressor, 'alphas') and hasattr(regressor, 'alpha'):
         # linear regressors need to set alpha, but not generalized CV ones
         regressor.alpha = 0.01
@@ -1656,11 +1699,6 @@ def check_regressors_train(name, regressor_orig):
                        "labels. Perhaps use check_X_y in fit.".format(name)):
         regressor.fit(X, y[:-1])
     # fit
-    if name in CROSS_DECOMPOSITION:
-        y_ = np.vstack([y, 2 * y + rnd.randint(2, size=len(y))])
-        y_ = y_.T
-    else:
-        y_ = y
     set_random_state(regressor)
     regressor.fit(X, y_)
     regressor.fit(X.tolist(), y_.tolist())
diff --git a/sklearn/utils/fixes.py b/sklearn/utils/fixes.py
index 551ca6cbd311..f7d9d6a29f9f 100644
--- a/sklearn/utils/fixes.py
+++ b/sklearn/utils/fixes.py
@@ -295,3 +295,42 @@ def __getstate__(self):
                                  self._fill_value)
 else:
     from numpy.ma import MaskedArray    # noqa
+
+
+if np_version < (1, 11):
+    def nanpercentile(a, q):
+        """
+        Compute the qth percentile of the data along the specified axis,
+        while ignoring nan values.
+
+        Returns the qth percentile(s) of the array elements.
+
+        Parameters
+        ----------
+        a : array_like
+            Input array or object that can be converted to an array.
+        q : float in range of [0,100] (or sequence of floats)
+            Percentile to compute, which must be between 0 and 100
+            inclusive.
+
+        Returns
+        -------
+        percentile : scalar or ndarray
+            If `q` is a single percentile and `axis=None`, then the result
+            is a scalar. If multiple percentiles are given, first axis of
+            the result corresponds to the percentiles. The other axes are
+            the axes that remain after the reduction of `a`. If the input
+            contains integers or floats smaller than ``float64``, the output
+            data-type is ``float64``. Otherwise, the output data-type is the
+            same as that of the input. If `out` is specified, that array is
+            returned instead.
+
+        """
+        data = np.compress(~np.isnan(a), a)
+        if data.size:
+            return np.percentile(data, q)
+        else:
+            size_q = 1 if np.isscalar(q) else len(q)
+            return np.array([np.nan] * size_q)
+else:
+    from numpy import nanpercentile  # noqa
diff --git a/sklearn/utils/testing.py b/sklearn/utils/testing.py
index 94972354e275..561ff2229353 100644
--- a/sklearn/utils/testing.py
+++ b/sklearn/utils/testing.py
@@ -16,6 +16,7 @@
 import warnings
 import sys
 import struct
+import functools
 
 import scipy as sp
 import scipy.io
@@ -45,7 +46,7 @@
 import sklearn
 from sklearn.base import BaseEstimator
 from sklearn.externals import joblib
-from sklearn.externals.funcsigs import signature
+from sklearn.utils.fixes import signature
 from sklearn.utils import deprecated
 
 additional_names_in_all = []
@@ -766,21 +767,29 @@ def _delete_folder(folder_path, warn=False):
 
 class TempMemmap(object):
     def __init__(self, data, mmap_mode='r'):
-        self.temp_folder = tempfile.mkdtemp(prefix='sklearn_testing_')
         self.mmap_mode = mmap_mode
         self.data = data
 
     def __enter__(self):
-        fpath = op.join(self.temp_folder, 'data.pkl')
-        joblib.dump(self.data, fpath)
-        data_read_only = joblib.load(fpath, mmap_mode=self.mmap_mode)
-        atexit.register(lambda: _delete_folder(self.temp_folder, warn=True))
+        data_read_only, self.temp_folder = create_memmap_backed_data(
+            self.data, mmap_mode=self.mmap_mode, return_folder=True)
         return data_read_only
 
     def __exit__(self, exc_type, exc_val, exc_tb):
         _delete_folder(self.temp_folder)
 
 
+def create_memmap_backed_data(data, mmap_mode='r', return_folder=False):
+    temp_folder = tempfile.mkdtemp(prefix='sklearn_testing_')
+    atexit.register(functools.partial(_delete_folder, temp_folder, warn=True))
+    filename = op.join(temp_folder, 'data.pkl')
+    joblib.dump(data, filename)
+    memmap_backed_data = joblib.load(filename, mmap_mode=mmap_mode)
+    result = (memmap_backed_data if not return_folder
+              else (memmap_backed_data, temp_folder))
+    return result
+
+
 # Utils to test docstrings
 
 
diff --git a/sklearn/utils/tests/test_class_weight.py b/sklearn/utils/tests/test_class_weight.py
index d248350faa0f..1dfedad9bcd7 100644
--- a/sklearn/utils/tests/test_class_weight.py
+++ b/sklearn/utils/tests/test_class_weight.py
@@ -124,6 +124,28 @@ def test_compute_class_weight_balanced_unordered():
     assert_array_almost_equal(cw, [2., 1., 2. / 3])
 
 
+def test_compute_class_weight_default():
+    # Test for the case where no weight is given for a present class.
+    # Current behaviour is to assign the unweighted classes a weight of 1.
+    y = np.asarray([2, 2, 2, 3, 3, 4])
+    classes = np.unique(y)
+    classes_len = len(classes)
+
+    # Test for non specified weights
+    cw = compute_class_weight(None, classes, y)
+    assert_equal(len(cw), classes_len)
+    assert_array_almost_equal(cw, np.ones(3))
+
+    # Tests for partly specified weights
+    cw = compute_class_weight({2: 1.5}, classes, y)
+    assert_equal(len(cw), classes_len)
+    assert_array_almost_equal(cw, [1.5, 1., 1.])
+
+    cw = compute_class_weight({2: 1.5, 4: 0.5}, classes, y)
+    assert_equal(len(cw), classes_len)
+    assert_array_almost_equal(cw, [1.5, 1., 0.5])
+
+
 def test_compute_sample_weight():
     # Test (and demo) compute_sample_weight.
     # Test with balanced classes
diff --git a/sklearn/utils/tests/test_fixes.py b/sklearn/utils/tests/test_fixes.py
index 7bdcfc2fc13d..8a55f74a4f6e 100644
--- a/sklearn/utils/tests/test_fixes.py
+++ b/sklearn/utils/tests/test_fixes.py
@@ -5,11 +5,16 @@
 
 import pickle
 
+import numpy as np
+import pytest
+
 from sklearn.utils.testing import assert_equal
 from sklearn.utils.testing import assert_array_equal
+from sklearn.utils.testing import assert_allclose
 
 from sklearn.utils.fixes import divide
 from sklearn.utils.fixes import MaskedArray
+from sklearn.utils.fixes import nanpercentile
 
 
 def test_divide():
@@ -24,3 +29,14 @@ def test_masked_array_obj_dtype_pickleable():
         marr_pickled = pickle.loads(pickle.dumps(marr))
         assert_array_equal(marr.data, marr_pickled.data)
         assert_array_equal(marr.mask, marr_pickled.mask)
+
+
+@pytest.mark.parametrize(
+    "a, q, expected_percentile",
+    [(np.array([1, 2, 3, np.nan]), [0, 50, 100], np.array([1., 2., 3.])),
+     (np.array([1, 2, 3, np.nan]), 50, 2.),
+     (np.array([np.nan, np.nan]), [0, 50], np.array([np.nan, np.nan]))]
+)
+def test_nanpercentile(a, q, expected_percentile):
+    percentile = nanpercentile(a, q)
+    assert_allclose(percentile, expected_percentile)
diff --git a/sklearn/utils/tests/test_random.py b/sklearn/utils/tests/test_random.py
index 159635c569d5..866a2481f919 100644
--- a/sklearn/utils/tests/test_random.py
+++ b/sklearn/utils/tests/test_random.py
@@ -106,19 +106,19 @@ def check_sample_int_distribution(sample_without_replacement):
 def test_random_choice_csc(n_samples=10000, random_state=24):
     # Explicit class probabilities
     classes = [np.array([0, 1]),  np.array([0, 1, 2])]
-    class_probabilites = [np.array([0.5, 0.5]), np.array([0.6, 0.1, 0.3])]
+    class_probabilities = [np.array([0.5, 0.5]), np.array([0.6, 0.1, 0.3])]
 
-    got = random_choice_csc(n_samples, classes, class_probabilites,
+    got = random_choice_csc(n_samples, classes, class_probabilities,
                             random_state)
     assert_true(sp.issparse(got))
 
     for k in range(len(classes)):
         p = np.bincount(got.getcol(k).toarray().ravel()) / float(n_samples)
-        assert_array_almost_equal(class_probabilites[k], p, decimal=1)
+        assert_array_almost_equal(class_probabilities[k], p, decimal=1)
 
     # Implicit class probabilities
     classes = [[0, 1],  [1, 2]]  # test for array-like support
-    class_probabilites = [np.array([0.5, 0.5]), np.array([0, 1/2, 1/2])]
+    class_probabilities = [np.array([0.5, 0.5]), np.array([0, 1/2, 1/2])]
 
     got = random_choice_csc(n_samples=n_samples,
                             classes=classes,
@@ -127,24 +127,24 @@ def test_random_choice_csc(n_samples=10000, random_state=24):
 
     for k in range(len(classes)):
         p = np.bincount(got.getcol(k).toarray().ravel()) / float(n_samples)
-        assert_array_almost_equal(class_probabilites[k], p, decimal=1)
+        assert_array_almost_equal(class_probabilities[k], p, decimal=1)
 
     # Edge case probabilities 1.0 and 0.0
     classes = [np.array([0, 1]),  np.array([0, 1, 2])]
-    class_probabilites = [np.array([1.0, 0.0]), np.array([0.0, 1.0, 0.0])]
+    class_probabilities = [np.array([1.0, 0.0]), np.array([0.0, 1.0, 0.0])]
 
-    got = random_choice_csc(n_samples, classes, class_probabilites,
+    got = random_choice_csc(n_samples, classes, class_probabilities,
                             random_state)
     assert_true(sp.issparse(got))
 
     for k in range(len(classes)):
         p = np.bincount(got.getcol(k).toarray().ravel(),
-                        minlength=len(class_probabilites[k])) / n_samples
-        assert_array_almost_equal(class_probabilites[k], p, decimal=1)
+                        minlength=len(class_probabilities[k])) / n_samples
+        assert_array_almost_equal(class_probabilities[k], p, decimal=1)
 
     # One class target data
     classes = [[1],  [0]]  # test for array-like support
-    class_probabilites = [np.array([0.0, 1.0]), np.array([1.0])]
+    class_probabilities = [np.array([0.0, 1.0]), np.array([1.0])]
 
     got = random_choice_csc(n_samples=n_samples,
                             classes=classes,
@@ -153,30 +153,30 @@ def test_random_choice_csc(n_samples=10000, random_state=24):
 
     for k in range(len(classes)):
         p = np.bincount(got.getcol(k).toarray().ravel()) / n_samples
-        assert_array_almost_equal(class_probabilites[k], p, decimal=1)
+        assert_array_almost_equal(class_probabilities[k], p, decimal=1)
 
 
 def test_random_choice_csc_errors():
-    # the length of an array in classes and class_probabilites is mismatched
+    # the length of an array in classes and class_probabilities is mismatched
     classes = [np.array([0, 1]),  np.array([0, 1, 2, 3])]
-    class_probabilites = [np.array([0.5, 0.5]), np.array([0.6, 0.1, 0.3])]
+    class_probabilities = [np.array([0.5, 0.5]), np.array([0.6, 0.1, 0.3])]
     assert_raises(ValueError, random_choice_csc, 4, classes,
-                  class_probabilites, 1)
+                  class_probabilities, 1)
 
     # the class dtype is not supported
     classes = [np.array(["a", "1"]),  np.array(["z", "1", "2"])]
-    class_probabilites = [np.array([0.5, 0.5]), np.array([0.6, 0.1, 0.3])]
+    class_probabilities = [np.array([0.5, 0.5]), np.array([0.6, 0.1, 0.3])]
     assert_raises(ValueError, random_choice_csc, 4, classes,
-                  class_probabilites, 1)
+                  class_probabilities, 1)
 
     # the class dtype is not supported
     classes = [np.array([4.2, 0.1]),  np.array([0.1, 0.2, 9.4])]
-    class_probabilites = [np.array([0.5, 0.5]), np.array([0.6, 0.1, 0.3])]
+    class_probabilities = [np.array([0.5, 0.5]), np.array([0.6, 0.1, 0.3])]
     assert_raises(ValueError, random_choice_csc, 4, classes,
-                  class_probabilites, 1)
+                  class_probabilities, 1)
 
     # Given probabilities don't sum to 1
     classes = [np.array([0, 1]),  np.array([0, 1, 2])]
-    class_probabilites = [np.array([0.5, 0.6]), np.array([0.6, 0.1, 0.3])]
+    class_probabilities = [np.array([0.5, 0.6]), np.array([0.6, 0.1, 0.3])]
     assert_raises(ValueError, random_choice_csc, 4, classes,
-                  class_probabilites, 1)
+                  class_probabilities, 1)
diff --git a/sklearn/utils/tests/test_testing.py b/sklearn/utils/tests/test_testing.py
index 0aca27861e0b..6b55431d21d7 100644
--- a/sklearn/utils/tests/test_testing.py
+++ b/sklearn/utils/tests/test_testing.py
@@ -1,13 +1,16 @@
 import warnings
 import unittest
 import sys
+import os
+import atexit
+
 import numpy as np
+
 from scipy import sparse
 
 from sklearn.utils.deprecation import deprecated
 from sklearn.utils.metaestimators import if_delegate_has_method
 from sklearn.utils.testing import (
-    assert_true,
     assert_raises,
     assert_less,
     assert_greater,
@@ -21,7 +24,10 @@
     ignore_warnings,
     check_docstring_parameters,
     assert_allclose_dense_sparse,
-    assert_raises_regex)
+    assert_raises_regex,
+    TempMemmap,
+    create_memmap_backed_data,
+    _delete_folder)
 
 from sklearn.utils.testing import SkipTest
 from sklearn.tree import DecisionTreeClassifier
@@ -478,3 +484,67 @@ def test_check_docstring_parameters():
         incorrect = check_docstring_parameters(f)
         assert len(incorrect) >= 1
         assert mess in incorrect[0], '"%s" not in "%s"' % (mess, incorrect[0])
+
+
+class RegistrationCounter(object):
+    def __init__(self):
+        self.nb_calls = 0
+
+    def __call__(self, to_register_func):
+        self.nb_calls += 1
+        assert to_register_func.func is _delete_folder
+
+
+def check_memmap(input_array, mmap_data, mmap_mode='r'):
+    assert isinstance(mmap_data, np.memmap)
+    writeable = mmap_mode != 'r'
+    assert mmap_data.flags.writeable is writeable
+    np.testing.assert_array_equal(input_array, mmap_data)
+
+
+def test_tempmemmap(monkeypatch):
+    registration_counter = RegistrationCounter()
+    monkeypatch.setattr(atexit, 'register', registration_counter)
+
+    input_array = np.ones(3)
+    with TempMemmap(input_array) as data:
+        check_memmap(input_array, data)
+        temp_folder = os.path.dirname(data.filename)
+    if os.name != 'nt':
+        assert not os.path.exists(temp_folder)
+    assert registration_counter.nb_calls == 1
+
+    mmap_mode = 'r+'
+    with TempMemmap(input_array, mmap_mode=mmap_mode) as data:
+        check_memmap(input_array, data, mmap_mode=mmap_mode)
+        temp_folder = os.path.dirname(data.filename)
+    if os.name != 'nt':
+        assert not os.path.exists(temp_folder)
+    assert registration_counter.nb_calls == 2
+
+
+def test_create_memmap_backed_data(monkeypatch):
+    registration_counter = RegistrationCounter()
+    monkeypatch.setattr(atexit, 'register', registration_counter)
+
+    input_array = np.ones(3)
+    data = create_memmap_backed_data(input_array)
+    check_memmap(input_array, data)
+    assert registration_counter.nb_calls == 1
+
+    data, folder = create_memmap_backed_data(input_array,
+                                             return_folder=True)
+    check_memmap(input_array, data)
+    assert folder == os.path.dirname(data.filename)
+    assert registration_counter.nb_calls == 2
+
+    mmap_mode = 'r+'
+    data = create_memmap_backed_data(input_array, mmap_mode=mmap_mode)
+    check_memmap(input_array, data, mmap_mode)
+    assert registration_counter.nb_calls == 3
+
+    input_list = [input_array, input_array + 1, input_array + 2]
+    mmap_data_list = create_memmap_backed_data(input_list)
+    for input_array, data in zip(input_list, mmap_data_list):
+        check_memmap(input_array, data)
+    assert registration_counter.nb_calls == 4
diff --git a/sklearn/utils/tests/test_validation.py b/sklearn/utils/tests/test_validation.py
index a3a4175d7eff..076e6d88440f 100644
--- a/sklearn/utils/tests/test_validation.py
+++ b/sklearn/utils/tests/test_validation.py
@@ -43,6 +43,7 @@
 from sklearn.exceptions import DataConversionWarning
 
 from sklearn.utils.testing import assert_raise_message
+from sklearn.utils.testing import TempMemmap
 
 
 def test_as_float_array():
@@ -690,3 +691,12 @@ def test_check_memory():
                         " have the same interface as "
                         "sklearn.externals.joblib.Memory. Got memory='{}' "
                         "instead.".format(dummy), check_memory, dummy)
+
+
+@pytest.mark.parametrize('copy', [True, False])
+def test_check_array_memmap(copy):
+    X = np.ones((4, 4))
+    with TempMemmap(X, mmap_mode='r') as X_memmap:
+        X_checked = check_array(X_memmap, copy=copy)
+        assert np.may_share_memory(X_memmap, X_checked) == (not copy)
+        assert X_checked.flags['WRITEABLE'] == copy
diff --git a/sklearn/utils/validation.py b/sklearn/utils/validation.py
index 70e968ee6d36..5fd54dc49b07 100644
--- a/sklearn/utils/validation.py
+++ b/sklearn/utils/validation.py
@@ -437,6 +437,10 @@ def check_array(array, accept_sparse=False, dtype="numeric", order=None,
             " instead.", DeprecationWarning)
         accept_sparse = False
 
+    # store reference to original array to check if copy is needed when
+    # function returns
+    array_orig = array
+
     # store whether originally we wanted numeric dtype
     dtype_numeric = isinstance(dtype, six.string_types) and dtype == "numeric"
 
@@ -487,13 +491,13 @@ def check_array(array, accept_sparse=False, dtype="numeric", order=None,
         with warnings.catch_warnings():
             try:
                 warnings.simplefilter('error', ComplexWarning)
-                array = np.array(array, dtype=dtype, order=order, copy=copy)
+                array = np.asarray(array, dtype=dtype, order=order)
             except ComplexWarning:
                 raise ValueError("Complex data not supported\n"
                                  "{}\n".format(array))
 
         # It is possible that the np.array(..) gave no warning. This happens
-        # when no dtype conversion happend, for example dtype = None. The
+        # when no dtype conversion happened, for example dtype = None. The
         # result is that np.array(..) produces an array of complex dtype
         # and we need to catch and raise exception for such cases.
         _ensure_no_complex_data(array)
@@ -513,8 +517,6 @@ def check_array(array, accept_sparse=False, dtype="numeric", order=None,
                     "Reshape your data either using array.reshape(-1, 1) if "
                     "your data has a single feature or array.reshape(1, -1) "
                     "if it contains a single sample.".format(array))
-            # To ensure that array flags are maintained
-            array = np.array(array, dtype=dtype, order=order, copy=copy)
 
         # in the future np.flexible dtypes will be handled like object dtypes
         if dtype_numeric and np.issubdtype(array.dtype, np.flexible):
@@ -556,6 +558,10 @@ def check_array(array, accept_sparse=False, dtype="numeric", order=None,
         msg = ("Data with input dtype %s was converted to %s%s."
                % (dtype_orig, array.dtype, context))
         warnings.warn(msg, DataConversionWarning)
+
+    if copy and np.may_share_memory(array, array_orig):
+        array = np.array(array, dtype=dtype, order=order)
+
     return array
 
 
