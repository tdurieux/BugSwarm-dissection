diff --git a/joblib/externals/loky/__init__.py b/joblib/externals/loky/__init__.py
index 94309f92..09356196 100644
--- a/joblib/externals/loky/__init__.py
+++ b/joblib/externals/loky/__init__.py
@@ -9,4 +9,4 @@
 
 from .backend.context import cpu_count  # noqa: F401
 
-__version__ = '2.1.1'
+__version__ = '2.1.3'
diff --git a/joblib/externals/loky/backend/context.py b/joblib/externals/loky/backend/context.py
index 513515d4..52df5589 100644
--- a/joblib/externals/loky/backend/context.py
+++ b/joblib/externals/loky/backend/context.py
@@ -205,7 +205,7 @@ def Event(self):
 class LokyInitMainContext(LokyContext):
     """Extra context with LokyProcess, which does load the main module
 
-    This context is used for compatibility in the case ``cloudpickle`` is not 
+    This context is used for compatibility in the case ``cloudpickle`` is not
     present on the running system. This permits to load functions defined in
     the ``main`` module, using proper safeguards. The declaration of the
     ``executor`` should be protected by ``if __name__ == "__main__":`` and the
diff --git a/joblib/externals/loky/backend/popen_loky_posix.py b/joblib/externals/loky/backend/popen_loky_posix.py
index a4831a05..729c7c71 100644
--- a/joblib/externals/loky/backend/popen_loky_posix.py
+++ b/joblib/externals/loky/backend/popen_loky_posix.py
@@ -142,7 +142,7 @@ def _launch(self, process_obj):
 
                 cmd_python = [sys.executable]
                 cmd_python += ['-m', self.__module__]
-                cmd_python += ['--name-process', str(process_obj.name)]
+                cmd_python += ['--process-name', str(process_obj.name)]
                 cmd_python += ['--pipe',
                                str(reduction._mk_inheritable(child_r))]
                 reduction._mk_inheritable(child_w)
@@ -180,7 +180,7 @@ def thread_is_spawning():
                         help='File handle for the pipe')
     parser.add_argument('--semaphore', type=int, required=True,
                         help='File handle name for the semaphore tracker')
-    parser.add_argument('--name-process', type=str, default=None,
+    parser.add_argument('--process-name', type=str, default=None,
                         help='Identifier for debugging purpose')
 
     args = parser.parse_args()
@@ -202,7 +202,7 @@ def thread_is_spawning():
         exitcode = process_obj._bootstrap()
     except Exception as e:
         print('\n\n' + '-' * 80)
-        print('{} failed with traceback: '.format(args.name_process))
+        print('{} failed with traceback: '.format(args.process_name))
         print('-' * 80)
         import traceback
         print(traceback.format_exc())
diff --git a/joblib/externals/loky/backend/semaphore_tracker.py b/joblib/externals/loky/backend/semaphore_tracker.py
index 79587f2e..f4942371 100644
--- a/joblib/externals/loky/backend/semaphore_tracker.py
+++ b/joblib/externals/loky/backend/semaphore_tracker.py
@@ -203,7 +203,6 @@ def main(fd):
                 try:
                     sem_unlink(name)
                     if VERBOSE:  # pragma: no cover
-                        name = name.decode('ascii')
                         sys.stderr.write("[SemaphoreTracker] unlink {}\n"
                                          .format(name))
                         sys.stderr.flush()
diff --git a/joblib/externals/loky/backend/semlock.py b/joblib/externals/loky/backend/semlock.py
index c94c4cdf..2d35f6a2 100644
--- a/joblib/externals/loky/backend/semlock.py
+++ b/joblib/externals/loky/backend/semlock.py
@@ -68,7 +68,7 @@ class FileNotFoundError(OSError):
 
 
 def sem_unlink(name):
-    if pthread.sem_unlink(name) < 0:
+    if pthread.sem_unlink(name.encode('ascii')) < 0:
         raiseFromErrno()
 
 
@@ -153,8 +153,8 @@ def __init__(self, kind, value, maxvalue, name=None, unlink_now=False):
         self.ident = 0
         self.kind = kind
         self.maxvalue = maxvalue
-        self.name = name.encode('ascii')
-        self.handle = _sem_open(self.name, value)
+        self.name = name
+        self.handle = _sem_open(self.name.encode('ascii'), value)
 
     def __del__(self):
         try:
@@ -265,7 +265,7 @@ def _rebuild(handle, kind, maxvalue, name):
         self.kind = kind
         self.maxvalue = maxvalue
         self.name = name
-        self.handle = _sem_open(name)
+        self.handle = _sem_open(name.encode('ascii'))
         return self
 
 
diff --git a/joblib/externals/loky/backend/spawn.py b/joblib/externals/loky/backend/spawn.py
index eb8700d9..a7e57b88 100644
--- a/joblib/externals/loky/backend/spawn.py
+++ b/joblib/externals/loky/backend/spawn.py
@@ -32,12 +32,12 @@ def get_command_line(pipe_handle, **kwds):
         Returns prefix of command line used for spawning a child process
         '''
         if getattr(sys, 'frozen', False):
-            return ([sys.executable, '--multiprocessing-fork'])
+            return ([sys.executable, '--multiprocessing-fork', pipe_handle])
         else:
             prog = 'from multiprocessing.forking import main; main()'
             opts = util._args_from_interpreter_flags()
-            return [_python_exe] + opts + ['-c', prog, '--multiprocessing-fork',
-                                           pipe_handle]
+            return [_python_exe] + opts + [
+                '-c', prog, '--multiprocessing-fork', pipe_handle]
 else:
     from multiprocessing.spawn import get_command_line
 
diff --git a/joblib/externals/loky/backend/synchronize.py b/joblib/externals/loky/backend/synchronize.py
index 2cdb43d3..4773b9dc 100644
--- a/joblib/externals/loky/backend/synchronize.py
+++ b/joblib/externals/loky/backend/synchronize.py
@@ -121,8 +121,7 @@ def __setstate__(self, state):
     @staticmethod
     def _make_name():
         # OSX does not support long names for semaphores
-        name = '/loky-%i-%s' % (os.getpid(), next(SemLock._rand))
-        return name
+        return '/loky-%i-%s' % (os.getpid(), next(SemLock._rand))
 
 
 #
diff --git a/joblib/externals/loky/backend/utils.py b/joblib/externals/loky/backend/utils.py
index 9f50eab4..d4cc11a4 100644
--- a/joblib/externals/loky/backend/utils.py
+++ b/joblib/externals/loky/backend/utils.py
@@ -1,7 +1,86 @@
+import os
+import sys
+import errno
+import signal
+import warnings
 import threading
+import subprocess
 
 
 def _flag_current_thread_clean_exit():
     """Put a ``_clean_exit`` flag on the current thread"""
     thread = threading.current_thread()
     thread._clean_exit = True
+
+
+def recursive_terminate(process):
+    """Terminate a process and its descendants.
+    """
+    try:
+        _recursive_terminate(process.pid)
+    except OSError as e:
+        import traceback
+        tb = traceback.format_exc()
+        warnings.warn("Failure in child introspection on this platform. You "
+                      "should report it on https://github.com/tomMoral/loky "
+                      "with the following traceback\n{}".format(tb))
+        # In case we cannot introspect the children, we fall back to the
+        # classic Process.terminate.
+        process.terminate()
+    process.join()
+
+
+def _recursive_terminate(pid):
+    """Recursively kill the descendants of a process before killing it.
+    """
+
+    if sys.platform == "win32":
+        # On windows, the taskkill function with option `/T` terminate a given
+        # process pid and its children.
+        try:
+            subprocess.check_output(
+                ["taskkill", "/F", "/T", "/PID", str(pid)],
+                stderr=None)
+        except subprocess.CalledProcessError as e:
+            # In windows, taskkill return 1 for permission denied and 128 for
+            # no process found.
+            if e.returncode not in [1, 128]:
+                raise
+            elif e.returncode == 1:
+                # Try to kill the process without its descendants if taskkill
+                # was denied permission. If this fails too, with an error
+                # different from process not found, let the top level function
+                # raise a warning and retry to kill the process.
+                try:
+                    os.kill(pid, signal.SIGTERM)
+                except OSError as e:
+                    if e.errno != errno.ESRCH:
+                        raise
+
+    else:
+        try:
+            children_pids = subprocess.check_output(
+                ["pgrep", "-P", str(pid)],
+                stderr=None
+            )
+        except subprocess.CalledProcessError as e:
+            # `ps` returns 1 when no child process has been found
+            if e.returncode == 1:
+                children_pids = b''
+            else:
+                raise
+
+        # Decode the result, split the cpid and remove the trailing line
+        children_pids = children_pids.decode().split('\n')[:-1]
+        for cpid in children_pids:
+            cpid = int(cpid)
+            _recursive_terminate(cpid)
+
+        try:
+            os.kill(pid, signal.SIGTERM)
+        except OSError as e:
+            # if OSError is raised with [Errno 3] no such process, the process
+            # is already terminated, else, raise the error and let the top
+            # level function raise a warning and retry to kill the process.
+            if e.errno != errno.ESRCH:
+                raise
diff --git a/joblib/externals/loky/process_executor.py b/joblib/externals/loky/process_executor.py
index 28b89efc..a7880240 100644
--- a/joblib/externals/loky/process_executor.py
+++ b/joblib/externals/loky/process_executor.py
@@ -1,5 +1,5 @@
 ###############################################################################
-# Re-implementation of the ProcessPoolExecutor to robustify its fault tolerance
+# Re-implementation of the ProcessPoolExecutor more robust to faults
 #
 # author: Thomas Moreau and Olivier Grisel
 #
@@ -76,6 +76,7 @@
 from .backend.compat import wait
 from .backend.context import cpu_count
 from .backend.queues import Queue, SimpleQueue, Full
+from .backend.utils import recursive_terminate
 
 try:
     from concurrent.futures.process import BrokenProcessPool as _BPPException
@@ -498,7 +499,7 @@ def shutdown_all_workers():
         executor_flags.flag_as_shutting_down()
         # Create a list to avoid RuntimeError due to concurrent modification of
         # processes. nb_children_alive is thus an upper bound. Also release the
-        # processes' safe_guard_locks to accelerate the shutdown procedure, as
+        # processes' _worker_exit_lock to accelerate the shutdown procedure, as
         # there is no need for hand-shake here.
         with processes_management_lock:
             n_children_alive = 0
@@ -594,8 +595,7 @@ def shutdown_all_workers():
                 _, p = processes.popitem()
                 mp.util.debug('terminate process {}'.format(p.name))
                 try:
-                    p.terminate()
-                    p.join()
+                    recursive_terminate(p)
                 except ProcessLookupError:  # pragma: no cover
                     pass
 
@@ -668,8 +668,7 @@ def shutdown_all_workers():
                 # locks may be in a dirty state and block forever.
                 while processes:
                     _, p = processes.popitem()
-                    p.terminate()
-                    p.join()
+                    recursive_terminate(p)
                 shutdown_all_workers()
                 return
             # Since no new work items can be added, it is safe to shutdown
diff --git a/joblib/externals/loky/reusable_executor.py b/joblib/externals/loky/reusable_executor.py
index 97b028aa..30b217fd 100644
--- a/joblib/externals/loky/reusable_executor.py
+++ b/joblib/externals/loky/reusable_executor.py
@@ -38,8 +38,9 @@ def _get_next_executor_id():
 
 
 def get_reusable_executor(max_workers=None, context=None, timeout=10,
-                          kill_workers=False, job_reducers=None,
-                          result_reducers=None, reuse="auto"):
+                          kill_workers=False, reuse="auto",
+                          job_reducers=None, result_reducers=None,
+                          initializer=None, initargs=()):
     """Return the current ReusableExectutor instance.
 
     Start a new instance if it has not been started already or if the previous
@@ -69,29 +70,45 @@ def get_reusable_executor(max_workers=None, context=None, timeout=10,
 
     The ``job_reducers`` and ``result_reducers`` are used to customize the
     pickling of tasks and results send to the executor.
+
+    When provided, the ``initializer`` is run first in newly spawned
+    processes with argument ``initargs``.
     """
     with _executor_lock:
-        global _executor, _executor_args
+        global _executor, _executor_kwargs
         executor = _executor
-        args = dict(context=context, timeout=timeout,
-                    job_reducers=job_reducers, result_reducers=result_reducers)
+
+        if max_workers is None:
+            if reuse is True and executor is not None:
+                max_workers = executor._max_workers
+            else:
+                max_workers = cpu_count()
+        elif max_workers <= 0:
+            raise ValueError(
+                "max_workers must be greater than 0, got {}."
+                .format(max_workers))
+
         if isinstance(context, STRING_TYPE):
             context = get_context(context)
         if context is not None and context.get_start_method() == "fork":
             raise ValueError("Cannot use reusable executor with the 'fork' "
                              "context")
+
+        kwargs = dict(context=context, timeout=timeout,
+                      job_reducers=job_reducers,
+                      result_reducers=result_reducers,
+                      initializer=initializer, initargs=initargs)
         if executor is None:
             mp.util.debug("Create a executor with max_workers={}."
                           .format(max_workers))
             executor_id = _get_next_executor_id()
-            _executor_args = args
+            _executor_kwargs = kwargs
             _executor = executor = _ReusablePoolExecutor(
-                _executor_lock, max_workers=max_workers, context=context,
-                timeout=timeout, executor_id=executor_id,
-                job_reducers=job_reducers, result_reducers=result_reducers)
+                _executor_lock, max_workers=max_workers,
+                executor_id=executor_id, **kwargs)
         else:
             if reuse == 'auto':
-                reuse = args == _executor_args
+                reuse = kwargs == _executor_kwargs
             if (executor._flags.broken or executor._flags.shutdown
                     or not reuse):
                 if executor._flags.broken:
@@ -105,17 +122,12 @@ def get_reusable_executor(max_workers=None, context=None, timeout=10,
                     "previous instance cannot be reused ({})."
                     .format(max_workers, reason))
                 executor.shutdown(wait=True, kill_workers=kill_workers)
-                _executor = executor = _executor_args = None
+                _executor = executor = _executor_kwargs = None
                 # Recursive call to build a new instance
                 return get_reusable_executor(max_workers=max_workers,
-                                             **args)
+                                             **kwargs)
             else:
-                if max_workers is not None and max_workers <= 0:
-                    raise ValueError(
-                        "max_workers must be greater than 0, got {}."
-                        .format(max_workers))
-
-                mp.util.debug("Reusing existing executor with max_worker={}."
+                mp.util.debug("Reusing existing executor with max_workers={}."
                               .format(executor._max_workers))
                 executor._resize(max_workers)
 
@@ -125,11 +137,11 @@ def get_reusable_executor(max_workers=None, context=None, timeout=10,
 class _ReusablePoolExecutor(ProcessPoolExecutor):
     def __init__(self, submit_resize_lock, max_workers=None, context=None,
                  timeout=None, executor_id=0, job_reducers=None,
-                 result_reducers=None):
+                 result_reducers=None, initializer=None, initargs=()):
         super(_ReusablePoolExecutor, self).__init__(
             max_workers=max_workers, context=context, timeout=timeout,
-            job_reducers=job_reducers,
-            result_reducers=result_reducers)
+            job_reducers=job_reducers, result_reducers=result_reducers,
+            initializer=initializer, initargs=initargs)
         self.executor_id = executor_id
         self._submit_resize_lock = submit_resize_lock
 
@@ -140,14 +152,17 @@ def submit(self, fn, *args, **kwargs):
 
     def _resize(self, max_workers):
         with self._submit_resize_lock:
+            if max_workers is None:
+                raise ValueError("Trying to resize with max_workers=None")
+            elif max_workers == self._max_workers:
+                return
 
             if self._queue_management_thread is None:
                 # If the queue_management_thread has not been started
                 # then no processes have been spawned and we can just
                 # update _max_workers and return
                 self._max_workers = max_workers
-            if max_workers is None or max_workers == self._max_workers:
-                return True
+                return
 
             self._wait_job_completion()
 
diff --git a/joblib/test/test_parallel.py b/joblib/test/test_parallel.py
index 756bd503..68d64861 100644
--- a/joblib/test/test_parallel.py
+++ b/joblib/test/test_parallel.py
@@ -1296,6 +1296,7 @@ def _recursive_backend_info(limit=3):
         return this_level + results[0]
 
 
+@with_multiprocessing
 @parametrize('backend', ['loky', 'threading'])
 def test_nested_parallel_limit(backend):
     with parallel_backend(backend, n_jobs=2):
