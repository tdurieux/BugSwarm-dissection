diff --git a/.travis.yml b/.travis.yml
index 33d11b60e117..788b7203d830 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -44,7 +44,7 @@ matrix:
     - env: DISTRIB="conda" PYTHON_VERSION="3.6.2" INSTALL_MKL="true"
            NUMPY_VERSION="1.14.2" SCIPY_VERSION="1.0.0" PANDAS_VERSION="0.20.3"
            CYTHON_VERSION="0.26.1" PYAMG_VERSION="3.3.2" PILLOW_VERSION="4.3.0"
-           JOBLIB_VERSION="0.12.0" COVERAGE=true
+           JOBLIB_VERSION="0.11" COVERAGE=true
            CHECK_PYTEST_SOFT_DEPENDENCY="true" TEST_DOCSTRINGS="true"
            SKLEARN_SITE_JOBLIB=1
       if: type != cron
diff --git a/doc/conftest.py b/doc/conftest.py
index 463df3f38221..11b190d8f66f 100644
--- a/doc/conftest.py
+++ b/doc/conftest.py
@@ -1,5 +1,6 @@
 from os.path import exists
 from os.path import join
+import warnings
 
 import numpy as np
 
@@ -75,6 +76,12 @@ def setup_impute():
         raise SkipTest("Skipping impute.rst, pandas not installed")
 
 
+def setup_unsupervised_learning():
+    # ignore deprecation warnings from scipy.misc.face
+    warnings.filterwarnings('ignore', 'The binary mode of fromstring',
+                            DeprecationWarning)
+
+
 def pytest_runtest_setup(item):
     fname = item.fspath.strpath
     is_index = fname.endswith('datasets/index.rst')
@@ -93,6 +100,8 @@ def pytest_runtest_setup(item):
         setup_compose()
     elif fname.endswith('modules/impute.rst'):
         setup_impute()
+    elif fname.endswith('statistical_inference/unsupervised_learning.rst'):
+        setup_unsupervised_learning()
 
 
 def pytest_runtest_teardown(item):
diff --git a/doc/modules/clustering.rst b/doc/modules/clustering.rst
index 1410b50d6518..70b38aa77027 100644
--- a/doc/modules/clustering.rst
+++ b/doc/modules/clustering.rst
@@ -1166,42 +1166,42 @@ proposed more recently and is **normalized against chance**::
   >>> labels_true = [0, 0, 0, 1, 1, 1]
   >>> labels_pred = [0, 0, 1, 1, 2, 2]
 
-  >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +ELLIPSIS
+  >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP
   0.22504...
 
 One can permute 0 and 1 in the predicted labels, rename 2 to 3 and get
 the same score::
 
   >>> labels_pred = [1, 1, 0, 0, 3, 3]
-  >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +ELLIPSIS
+  >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP
   0.22504...
 
 All, :func:`mutual_info_score`, :func:`adjusted_mutual_info_score` and
 :func:`normalized_mutual_info_score` are symmetric: swapping the argument does
 not change the score. Thus they can be used as a **consensus measure**::
 
-  >>> metrics.adjusted_mutual_info_score(labels_pred, labels_true)  # doctest: +ELLIPSIS
+  >>> metrics.adjusted_mutual_info_score(labels_pred, labels_true)  # doctest: +SKIP
   0.22504...
 
 Perfect labeling is scored 1.0::
 
   >>> labels_pred = labels_true[:]
-  >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)
+  >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP
   1.0
 
-  >>> metrics.normalized_mutual_info_score(labels_true, labels_pred)
+  >>> metrics.normalized_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP
   1.0
 
 This is not true for ``mutual_info_score``, which is therefore harder to judge::
 
-  >>> metrics.mutual_info_score(labels_true, labels_pred)  # doctest: +ELLIPSIS
+  >>> metrics.mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP
   0.69...
 
 Bad (e.g. independent labelings) have non-positive scores::
 
   >>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]
   >>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]
-  >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +ELLIPSIS
+  >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP
   -0.10526...
 
 
diff --git a/doc/modules/compose.rst b/doc/modules/compose.rst
index 8f303ed58521..8817b6d83a38 100644
--- a/doc/modules/compose.rst
+++ b/doc/modules/compose.rst
@@ -413,7 +413,7 @@ variable, but apply a :class:`feature_extraction.text.CountVectorizer
 <sklearn.feature_extraction.text.CountVectorizer>` to the ``'title'`` column.
 As we might use multiple feature extraction methods on the same column, we give
 each transformer a unique name, say ``'city_category'`` and ``'title_bow'``.
-We can ignore the remaining rating columns by setting ``remainder='drop'``::
+By default, the remaining rating columns are ignored (``remainder='drop'``)::
 
   >>> from sklearn.compose import ColumnTransformer
   >>> from sklearn.feature_extraction.text import CountVectorizer
@@ -495,7 +495,7 @@ above example would be::
   ...     ('city', CountVectorizer(analyzer=lambda x: [x])),
   ...     ('title', CountVectorizer()))
   >>> column_trans # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
-  ColumnTransformer(n_jobs=1, remainder='passthrough', transformer_weights=None,
+  ColumnTransformer(n_jobs=1, remainder='drop', transformer_weights=None,
            transformers=[('countvectorizer-1', ...)
 
 .. topic:: Examples:
diff --git a/doc/modules/ensemble.rst b/doc/modules/ensemble.rst
index 83da35eb46ca..19e46d3eab25 100644
--- a/doc/modules/ensemble.rst
+++ b/doc/modules/ensemble.rst
@@ -965,7 +965,7 @@ The following example shows how to fit the majority rule classifier::
    >>> X, y = iris.data[:, 1:3], iris.target
 
    >>> clf1 = LogisticRegression(random_state=1)
-   >>> clf2 = RandomForestClassifier(random_state=1)
+   >>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)
    >>> clf3 = GaussianNB()
 
    >>> eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')
@@ -974,7 +974,7 @@ The following example shows how to fit the majority rule classifier::
    ...     scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')
    ...     print("Accuracy: %0.2f (+/- %0.2f) [%s]" % (scores.mean(), scores.std(), label))
    Accuracy: 0.90 (+/- 0.05) [Logistic Regression]
-   Accuracy: 0.93 (+/- 0.05) [Random Forest]
+   Accuracy: 0.94 (+/- 0.04) [Random Forest]
    Accuracy: 0.91 (+/- 0.04) [naive Bayes]
    Accuracy: 0.95 (+/- 0.05) [Ensemble]
 
diff --git a/doc/modules/feature_extraction.rst b/doc/modules/feature_extraction.rst
index 611c7ecb60ee..68d4295bd658 100644
--- a/doc/modules/feature_extraction.rst
+++ b/doc/modules/feature_extraction.rst
@@ -380,6 +380,37 @@ last document::
   >>> X_2[:, feature_index]     # doctest: +ELLIPSIS
   array([0, 0, 0, 1]...)
 
+.. _stop_words:
+
+Using stop words
+................
+
+Stop words are words like "and", "the", "him", which are presumed to be
+uninformative in representing the content of a text, and which may be
+removed to avoid them being construed as signal for prediction.  Sometimes,
+however, similar words are useful for prediction, such as in classifying
+writing style or personality.
+
+There are several known issues in our provided 'english' stop word list. See
+[NQY18]_.
+
+Please take care in choosing a stop word list.
+Popular stop word lists may include words that are highly informative to
+some tasks, such as *computer*.
+
+You should also make sure that the stop word list has had the same
+preprocessing and tokenization applied as the one used in the vectorizer.
+The word *we've* is split into *we* and *ve* by CountVectorizer's default
+tokenizer, so if *we've* is in ``stop_words``, but *ve* is not, *ve* will
+be retained from *we've* in transformed text.  Our vectorizers will try to
+identify and warn about some kinds of inconsistencies.
+
+.. topic:: References
+
+    .. [NQY18] J. Nothman, H. Qin and R. Yurchak (2018).
+               `"Stop Word Lists in Free Open-source Software Packages"
+               <http://aclweb.org/anthology/W18-2502>`__.
+               In *Proc. Workshop for NLP Open Source Software*.
 
 .. _tfidf:
 
diff --git a/doc/modules/feature_selection.rst b/doc/modules/feature_selection.rst
index ae630af183cd..7c6df892e5be 100644
--- a/doc/modules/feature_selection.rst
+++ b/doc/modules/feature_selection.rst
@@ -245,7 +245,7 @@ meta-transformer)::
   >>> X, y = iris.data, iris.target
   >>> X.shape
   (150, 4)
-  >>> clf = ExtraTreesClassifier()
+  >>> clf = ExtraTreesClassifier(n_estimators=50)
   >>> clf = clf.fit(X, y)
   >>> clf.feature_importances_  # doctest: +SKIP
   array([ 0.04...,  0.05...,  0.4...,  0.4...])
diff --git a/doc/modules/preprocessing.rst b/doc/modules/preprocessing.rst
index 1d479892de82..dd1f798ccb3a 100644
--- a/doc/modules/preprocessing.rst
+++ b/doc/modules/preprocessing.rst
@@ -721,7 +721,7 @@ a transformer that applies a log transformation in a pipeline, do::
 
     >>> import numpy as np
     >>> from sklearn.preprocessing import FunctionTransformer
-    >>> transformer = FunctionTransformer(np.log1p)
+    >>> transformer = FunctionTransformer(np.log1p, validate=True)
     >>> X = np.array([[0, 1], [2, 3]])
     >>> transformer.transform(X)
     array([[0.        , 0.69314718],
diff --git a/doc/tutorial/statistical_inference/model_selection.rst b/doc/tutorial/statistical_inference/model_selection.rst
index 3feba26c6a77..2146c62b9af8 100644
--- a/doc/tutorial/statistical_inference/model_selection.rst
+++ b/doc/tutorial/statistical_inference/model_selection.rst
@@ -215,15 +215,15 @@ estimator during the construction and exposes an estimator API::
     >>> Cs = np.logspace(-6, -1, 10)
     >>> clf = GridSearchCV(estimator=svc, param_grid=dict(C=Cs),
     ...                    n_jobs=-1)
-    >>> clf.fit(X_digits[:1000], y_digits[:1000])        # doctest: +ELLIPSIS
+    >>> clf.fit(X_digits[:1000], y_digits[:1000])        # doctest: +SKIP
     GridSearchCV(cv=None,...
-    >>> clf.best_score_                                  # doctest: +ELLIPSIS
+    >>> clf.best_score_                                  # doctest: +SKIP
     0.925...
-    >>> clf.best_estimator_.C                            # doctest: +ELLIPSIS
+    >>> clf.best_estimator_.C                            # doctest: +SKIP
     0.0077...
 
     >>> # Prediction performance on test set is not as good as on train set
-    >>> clf.score(X_digits[1000:], y_digits[1000:])      # doctest: +ELLIPSIS
+    >>> clf.score(X_digits[1000:], y_digits[1000:])      # doctest: +SKIP
     0.943...
 
 
@@ -235,8 +235,7 @@ a stratified 3-fold.
 
     ::
 
-        >>> cross_val_score(clf, X_digits, y_digits)
-        ...                                               # doctest: +ELLIPSIS
+        >>> cross_val_score(clf, X_digits, y_digits) # doctest: +SKIP
         array([0.938..., 0.963..., 0.944...])
 
     Two cross-validation loops are performed in parallel: one by the
diff --git a/sklearn/compose/_column_transformer.py b/sklearn/compose/_column_transformer.py
index 7ec8f326c698..7c5cd2750a4d 100644
--- a/sklearn/compose/_column_transformer.py
+++ b/sklearn/compose/_column_transformer.py
@@ -71,14 +71,14 @@ class ColumnTransformer(_BaseComposition, TransformerMixin):
             A callable is passed the input data `X` and can return any of the
             above.
 
-    remainder : {'passthrough', 'drop'} or estimator, default 'passthrough'
-        By default, all remaining columns that were not specified in
-        `transformers` will be automatically passed through (default of
-        ``'passthrough'``). This subset of columns is concatenated with the
-        output of the transformers.
-        By using ``remainder='drop'``, only the specified columns in
-        `transformers` are transformed and combined in the output, and the
-        non-specified columns are dropped.
+    remainder : {'passthrough', 'drop'} or estimator, default 'drop'
+        By default, only the specified columns in `transformers` are
+        transformed and combined in the output, and the non-specified
+        columns are dropped. (default of ``'drop'``).
+        By specifying ``remainder='passthrough'``, all remaining columns that
+        were not specified in `transformers` will be automatically passed
+        through. This subset of columns is concatenated with the output of
+        the transformers.
         By setting ``remainder`` to be an estimator, the remaining
         non-specified columns will use the ``remainder`` estimator. The
         estimator must support `fit` and `transform`.
@@ -141,7 +141,7 @@ class ColumnTransformer(_BaseComposition, TransformerMixin):
 
     """
 
-    def __init__(self, transformers, remainder='passthrough', n_jobs=1,
+    def __init__(self, transformers, remainder='drop', n_jobs=1,
                  transformer_weights=None):
         self.transformers = transformers
         self.remainder = remainder
@@ -658,8 +658,7 @@ def make_column_transformer(*transformers, **kwargs):
     ...     (['numerical_column'], StandardScaler()),
     ...     (['categorical_column'], OneHotEncoder()))
     ...     # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
-    ColumnTransformer(n_jobs=1, remainder='passthrough',
-             transformer_weights=None,
+    ColumnTransformer(n_jobs=1, remainder='drop', transformer_weights=None,
              transformers=[('standardscaler',
                             StandardScaler(...),
                             ['numerical_column']),
@@ -669,7 +668,7 @@ def make_column_transformer(*transformers, **kwargs):
 
     """
     n_jobs = kwargs.pop('n_jobs', 1)
-    remainder = kwargs.pop('remainder', 'passthrough')
+    remainder = kwargs.pop('remainder', 'drop')
     if kwargs:
         raise TypeError('Unknown keyword arguments: "{}"'
                         .format(list(kwargs.keys())[0]))
diff --git a/sklearn/compose/tests/test_column_transformer.py b/sklearn/compose/tests/test_column_transformer.py
index 06ac7c5eed43..c1363992c0da 100644
--- a/sklearn/compose/tests/test_column_transformer.py
+++ b/sklearn/compose/tests/test_column_transformer.py
@@ -405,7 +405,7 @@ def test_column_transformer_get_set_params():
                             ('trans2', StandardScaler(), [1])])
 
     exp = {'n_jobs': 1,
-           'remainder': 'passthrough',
+           'remainder': 'drop',
            'trans1': ct.transformers[0][1],
            'trans1__copy': True,
            'trans1__with_mean': True,
@@ -424,7 +424,7 @@ def test_column_transformer_get_set_params():
 
     ct.set_params(trans1='passthrough')
     exp = {'n_jobs': 1,
-           'remainder': 'passthrough',
+           'remainder': 'drop',
            'trans1': 'passthrough',
            'trans2': ct.transformers[1][1],
            'trans2__copy': True,
@@ -492,7 +492,8 @@ def test_column_transformer_get_feature_names():
         NotImplementedError, 'get_feature_names is not yet supported',
         ct.get_feature_names)
 
-    ct = ColumnTransformer([('trans', DictVectorizer(), 0)])
+    ct = ColumnTransformer([('trans', DictVectorizer(), 0)],
+                           remainder='passthrough')
     ct.fit(X)
     assert_raise_message(
         NotImplementedError, 'get_feature_names is not yet supported',
@@ -552,23 +553,22 @@ def test_column_transformer_remainder():
     X_res_second = np.array([2, 4, 6]).reshape(-1, 1)
     X_res_both = X_array
 
-    # default passthrough
-    ct = ColumnTransformer([('trans', Trans(), [0])])
-    assert_array_equal(ct.fit_transform(X_array), X_res_both)
-    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)
+    # default drop
+    ct = ColumnTransformer([('trans1', Trans(), [0])])
+    assert_array_equal(ct.fit_transform(X_array), X_res_first)
+    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_first)
     assert len(ct.transformers_) == 2
     assert ct.transformers_[-1][0] == 'remainder'
-    assert ct.transformers_[-1][1] == 'passthrough'
+    assert ct.transformers_[-1][1] == 'drop'
     assert_array_equal(ct.transformers_[-1][2], [1])
 
-    # specify to drop remaining columns
-    ct = ColumnTransformer([('trans1', Trans(), [0])],
-                           remainder='drop')
-    assert_array_equal(ct.fit_transform(X_array), X_res_first)
-    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_first)
+    # specify passthrough
+    ct = ColumnTransformer([('trans', Trans(), [0])], remainder='passthrough')
+    assert_array_equal(ct.fit_transform(X_array), X_res_both)
+    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)
     assert len(ct.transformers_) == 2
     assert ct.transformers_[-1][0] == 'remainder'
-    assert ct.transformers_[-1][1] == 'drop'
+    assert ct.transformers_[-1][1] == 'passthrough'
     assert_array_equal(ct.transformers_[-1][2], [1])
 
     # column order is not preserved (passed through added to end)
@@ -602,6 +602,9 @@ def test_column_transformer_remainder():
         "remainder keyword needs to be one of \'drop\', \'passthrough\', "
         "or estimator.", ct.fit_transform, X_array)
 
+    # check default for make_column_transformer
+    ct = make_column_transformer(([0], Trans()))
+    assert ct.remainder == 'drop'
 
 @pytest.mark.parametrize("key", [[0], np.array([0]), slice(0, 1),
                                  np.array([True, False])])
diff --git a/sklearn/covariance/tests/test_graphical_lasso.py b/sklearn/covariance/tests/test_graphical_lasso.py
index 76bcd302a762..2c1b604e5cf7 100644
--- a/sklearn/covariance/tests/test_graphical_lasso.py
+++ b/sklearn/covariance/tests/test_graphical_lasso.py
@@ -16,6 +16,7 @@
 from sklearn.externals.six.moves import StringIO
 from sklearn.utils import check_random_state
 from sklearn import datasets
+from sklearn.utils.fixes import PY3_OR_LATER
 
 from numpy.testing import assert_equal
 
@@ -137,6 +138,8 @@ def test_graphical_lasso_cv(random_state=1):
     GraphicalLassoCV(alphas=[0.8, 0.5], tol=1e-1, n_jobs=1).fit(X)
 
 
+@pytest.mark.skipif(not PY3_OR_LATER,
+                    reason='On Python 2 DeprecationWarning is not issued for some unkown reason.')
 def test_deprecated_grid_scores(random_state=1):
     dim = 5
     n_samples = 6
diff --git a/sklearn/decomposition/dict_learning.py b/sklearn/decomposition/dict_learning.py
index f9d664e4f6a4..64315823e291 100644
--- a/sklearn/decomposition/dict_learning.py
+++ b/sklearn/decomposition/dict_learning.py
@@ -373,11 +373,13 @@ def _update_dict(dictionary, Y, code, verbose=False, return_r2=False,
     n_components = len(code)
     n_features = Y.shape[0]
     random_state = check_random_state(random_state)
-    # Residuals, computed 'in-place' for efficiency
-    R = -np.dot(dictionary, code)
-    R += Y
-    R = np.asfortranarray(R)
+    # Get BLAS functions
+    gemm, = linalg.get_blas_funcs(('gemm',), (dictionary, code, Y))
     ger, = linalg.get_blas_funcs(('ger',), (dictionary, code))
+    # Residuals, computed with BLAS for speed and efficiency
+    # R <- -1.0 * U * V^T + 1.0 * Y
+    # Outputs R as Fortran array for efficiency
+    R = gemm(-1.0, dictionary, code, 1.0, Y)
     for k in range(n_components):
         # R <- 1.0 * U_k * V_k^T + R
         R = ger(1.0, dictionary[:, k], code[k, :], a=R, overwrite_a=True)
diff --git a/sklearn/decomposition/tests/test_dict_learning.py b/sklearn/decomposition/tests/test_dict_learning.py
index 734e3913cdb3..b5852f470187 100644
--- a/sklearn/decomposition/tests/test_dict_learning.py
+++ b/sklearn/decomposition/tests/test_dict_learning.py
@@ -58,7 +58,8 @@ def test_dict_learning_overcomplete():
     assert_true(dico.components_.shape == (n_components, n_features))
 
 
-@ignore_warnings(category=DeprecationWarning)  # positive lars deprecated 0.22
+# positive lars deprecated 0.22
+@pytest.mark.filterwarnings('ignore::DeprecationWarning')
 @pytest.mark.parametrize("transform_algorithm", [
     "lasso_lars",
     "lasso_cd",
@@ -171,7 +172,8 @@ def test_dict_learning_online_shapes():
     assert_equal(np.dot(code, dictionary).shape, X.shape)
 
 
-@ignore_warnings(category=DeprecationWarning)  # positive lars deprecated 0.22
+# positive lars deprecated 0.22
+@pytest.mark.filterwarnings('ignore::DeprecationWarning')
 @pytest.mark.parametrize("transform_algorithm", [
     "lasso_lars",
     "lasso_cd",
@@ -308,7 +310,8 @@ def test_sparse_encode_shapes():
         assert_equal(code.shape, (n_samples, n_components))
 
 
-@ignore_warnings(category=DeprecationWarning)  # positive lars deprecated 0.22
+# positive lars deprecated 0.22
+@pytest.mark.filterwarnings('ignore::DeprecationWarning')
 @pytest.mark.parametrize("positive", [
     False,
     True,
diff --git a/sklearn/ensemble/tests/test_weight_boosting.py b/sklearn/ensemble/tests/test_weight_boosting.py
index 87972c67bfb1..8a0beba2207f 100755
--- a/sklearn/ensemble/tests/test_weight_boosting.py
+++ b/sklearn/ensemble/tests/test_weight_boosting.py
@@ -4,7 +4,7 @@
 import numpy as np
 
 from sklearn.utils.testing import assert_array_equal, assert_array_less
-from sklearn.utils.testing import assert_array_almost_equal, ignore_warnings
+from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_equal, assert_true, assert_greater
 from sklearn.utils.testing import assert_raises, assert_raises_regexp
 
diff --git a/sklearn/feature_extraction/tests/test_image.py b/sklearn/feature_extraction/tests/test_image.py
index bf59f7802d42..4908cbde5610 100644
--- a/sklearn/feature_extraction/tests/test_image.py
+++ b/sklearn/feature_extraction/tests/test_image.py
@@ -69,6 +69,7 @@ def test_connect_regions():
         graph = img_to_graph(face, mask)
         assert_equal(ndimage.label(mask)[1], connected_components(graph)[0])
 
+
 @ignore_warnings(category=DeprecationWarning)  # scipy deprecation inside face
 def test_connect_regions_with_grid():
     try:
diff --git a/sklearn/feature_extraction/tests/test_text.py b/sklearn/feature_extraction/tests/test_text.py
index 8bfb6537962a..db7abc0e756e 100644
--- a/sklearn/feature_extraction/tests/test_text.py
+++ b/sklearn/feature_extraction/tests/test_text.py
@@ -4,6 +4,7 @@
 import pytest
 from scipy import sparse
 
+from sklearn.externals.six import PY2
 from sklearn.feature_extraction.text import strip_tags
 from sklearn.feature_extraction.text import strip_accents_unicode
 from sklearn.feature_extraction.text import strip_accents_ascii
@@ -31,7 +32,7 @@
                                    assert_in, assert_less, assert_greater,
                                    assert_warns_message, assert_raise_message,
                                    clean_warning_registry, ignore_warnings,
-                                   SkipTest, assert_raises,
+                                   SkipTest, assert_raises, assert_no_warnings,
                                    assert_allclose_dense_sparse)
 from sklearn.utils.fixes import _Mapping as Mapping
 from collections import defaultdict
@@ -1106,3 +1107,26 @@ def test_vectorizers_invalid_ngram_range(vec):
     if isinstance(vec, HashingVectorizer):
         assert_raise_message(
             ValueError, message, vec.transform, ["good news everyone"])
+
+
+def test_vectorizer_stop_words_inconsistent():
+    if PY2:
+        lstr = "[u'and', u'll', u've']"
+    else:
+        lstr = "['and', 'll', 've']"
+    message = ('Your stop_words may be inconsistent with your '
+               'preprocessing. Tokenizing the stop words generated '
+               'tokens %s not in stop_words.' % lstr)
+    for vec in [CountVectorizer(),
+                TfidfVectorizer(), HashingVectorizer()]:
+        vec.set_params(stop_words=["you've", "you", "you'll", 'AND'])
+        assert_warns_message(UserWarning, message, vec.fit_transform,
+                             ['hello world'])
+
+    # Only one warning per stop list
+    assert_no_warnings(vec.fit_transform, ['hello world'])
+
+    # Test caching of inconsistency assessment
+    vec.set_params(stop_words=["you've", "you", "you'll", 'blah', 'AND'])
+    assert_warns_message(UserWarning, message, vec.fit_transform,
+                         ['hello world'])
diff --git a/sklearn/feature_extraction/text.py b/sklearn/feature_extraction/text.py
index 9df27af2666c..05f60d2805c7 100644
--- a/sklearn/feature_extraction/text.py
+++ b/sklearn/feature_extraction/text.py
@@ -268,6 +268,23 @@ def get_stop_words(self):
         """Build or fetch the effective stop words list"""
         return _check_stop_list(self.stop_words)
 
+    def _check_stop_words_consistency(self, stop_words, preprocess, tokenize):
+        # NB: stop_words is validated, unlike self.stop_words
+        if id(self.stop_words) != getattr(self, '_stop_words_id', None):
+            inconsistent = set()
+            for w in stop_words or ():
+                tokens = list(tokenize(preprocess(w)))
+                for token in tokens:
+                    if token not in stop_words:
+                        inconsistent.add(token)
+            self._stop_words_id = id(self.stop_words)
+
+            if inconsistent:
+                warnings.warn('Your stop_words may be inconsistent with your '
+                              'preprocessing. Tokenizing the stop words '
+                              'generated tokens %r not in stop_words.' %
+                              sorted(inconsistent))
+
     def build_analyzer(self):
         """Return a callable that handles preprocessing and tokenization"""
         if callable(self.analyzer):
@@ -285,7 +302,8 @@ def build_analyzer(self):
         elif self.analyzer == 'word':
             stop_words = self.get_stop_words()
             tokenize = self.build_tokenizer()
-
+            self._check_stop_words_consistency(stop_words, preprocess,
+                                               tokenize)
             return lambda doc: self._word_ngrams(
                 tokenize(preprocess(self.decode(doc))), stop_words)
 
@@ -427,6 +445,8 @@ class HashingVectorizer(BaseEstimator, VectorizerMixin, TransformerMixin):
 
     stop_words : string {'english'}, list, or None (default)
         If 'english', a built-in stop word list for English is used.
+        There are several known issues with 'english' and you should
+        consider an alternative (see :ref:`stop_words`).
 
         If a list, that list is assumed to contain stop words, all of which
         will be removed from the resulting tokens.
@@ -682,6 +702,8 @@ class CountVectorizer(BaseEstimator, VectorizerMixin):
 
     stop_words : string {'english'}, list, or None (default)
         If 'english', a built-in stop word list for English is used.
+        There are several known issues with 'english' and you should
+        consider an alternative (see :ref:`stop_words`).
 
         If a list, that list is assumed to contain stop words, all of which
         will be removed from the resulting tokens.
@@ -1324,6 +1346,8 @@ class TfidfVectorizer(CountVectorizer):
         If a string, it is passed to _check_stop_list and the appropriate stop
         list is returned. 'english' is currently the only supported string
         value.
+        There are several known issues with 'english' and you should
+        consider an alternative (see :ref:`stop_words`).
 
         If a list, that list is assumed to contain stop words, all of which
         will be removed from the resulting tokens.
diff --git a/sklearn/linear_model/ridge.py b/sklearn/linear_model/ridge.py
index d3f845b4442b..6ff0bd4f9300 100644
--- a/sklearn/linear_model/ridge.py
+++ b/sklearn/linear_model/ridge.py
@@ -60,7 +60,11 @@ def _mv(x):
             C = sp_linalg.LinearOperator(
                 (n_samples, n_samples), matvec=mv, dtype=X.dtype)
             # FIXME atol
-            coef, info = sp_linalg.cg(C, y_column, tol=tol, atol='legacy')
+            try:
+                coef, info = sp_linalg.cg(C, y_column, tol=tol, atol='legacy')
+            except TypeError:
+                # old scipy
+                coef, info = sp_linalg.cg(C, y_column, tol=tol)
             coefs[i] = X1.rmatvec(coef)
         else:
             # linear ridge
@@ -69,8 +73,14 @@ def _mv(x):
             C = sp_linalg.LinearOperator(
                 (n_features, n_features), matvec=mv, dtype=X.dtype)
             # FIXME atol
-            coefs[i], info = sp_linalg.cg(C, y_column, maxiter=max_iter,
-                                          tol=tol, atol='legacy')
+            try:
+                coefs[i], info = sp_linalg.cg(C, y_column, maxiter=max_iter,
+                                              tol=tol, atol='legacy')
+            except TypeError:
+                # old scipy
+                coefs[i], info = sp_linalg.cg(C, y_column, maxiter=max_iter,
+                                              tol=tol)
+
         if info < 0:
             raise ValueError("Failed with error code %d" % info)
 
diff --git a/sklearn/linear_model/tests/test_least_angle.py b/sklearn/linear_model/tests/test_least_angle.py
index f5e5b2222d24..15df84177e66 100644
--- a/sklearn/linear_model/tests/test_least_angle.py
+++ b/sklearn/linear_model/tests/test_least_angle.py
@@ -1,5 +1,7 @@
 import warnings
 
+from distutils.version import LooseVersion
+
 import numpy as np
 from scipy import linalg
 
@@ -88,18 +90,22 @@ def test_all_precomputed():
             assert_array_almost_equal(expected, got)
 
 
-@pytest.mark.filterwarnings('ignore: `rcond` parameter will change')  # numpy deprecation
+@pytest.mark.filterwarnings('ignore: `rcond` parameter will change')
+# numpy deprecation
 def test_lars_lstsq():
     # Test that Lars gives least square solution at the end
     # of the path
     X1 = 3 * diabetes.data  # use un-normalized dataset
     clf = linear_model.LassoLars(alpha=0.)
     clf.fit(X1, y)
-    coef_lstsq = np.linalg.lstsq(X1, y, rcond=None)[0]
+    # Avoid FutureWarning about default value change when numpy >= 1.14
+    rcond = None if LooseVersion(np.__version__) >= '1.14' else -1
+    coef_lstsq = np.linalg.lstsq(X1, y, rcond=rcond)[0]
     assert_array_almost_equal(clf.coef_, coef_lstsq)
 
 
-@pytest.mark.filterwarnings('ignore:`rcond` parameter will change')  # numpy deprecation
+@pytest.mark.filterwarnings('ignore:`rcond` parameter will change')
+# numpy deprecation
 def test_lasso_gives_lstsq_solution():
     # Test that Lars Lasso gives least square solution at the end
     # of the path
diff --git a/sklearn/linear_model/tests/test_randomized_l1.py b/sklearn/linear_model/tests/test_randomized_l1.py
index 077b440792d7..564fbd4e7827 100644
--- a/sklearn/linear_model/tests/test_randomized_l1.py
+++ b/sklearn/linear_model/tests/test_randomized_l1.py
@@ -1,7 +1,6 @@
 # Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>
 # License: BSD 3 clause
 
-import pytest
 from tempfile import mkdtemp
 import shutil
 
@@ -17,8 +16,8 @@
 from sklearn.utils.testing import assert_warns_message
 
 from sklearn.linear_model.randomized_l1 import(lasso_stability_path,
-                                                RandomizedLasso,
-                                                RandomizedLogisticRegression)
+                                               RandomizedLasso,
+                                               RandomizedLogisticRegression)
 
 from sklearn.datasets import load_diabetes, load_iris
 from sklearn.feature_selection import f_regression, f_classif
diff --git a/sklearn/metrics/cluster/tests/test_common.py b/sklearn/metrics/cluster/tests/test_common.py
index a83987703b2e..c186a3ad5a4c 100644
--- a/sklearn/metrics/cluster/tests/test_common.py
+++ b/sklearn/metrics/cluster/tests/test_common.py
@@ -83,8 +83,8 @@ def test_symmetric_non_symmetric_union():
             sorted(SUPERVISED_METRICS))
 
 
-@ignore_warnings(category=FutureWarning)
 # 0.22 AMI and NMI changes
+@pytest.mark.filterwarnings('ignore::FutureWarning')
 @pytest.mark.parametrize(
     'metric_name, y1, y2',
     [(name, y1, y2) for name in SYMMETRIC_METRICS]
@@ -103,8 +103,8 @@ def test_non_symmetry(metric_name, y1, y2):
     assert metric(y1, y2) != pytest.approx(metric(y2, y1))
 
 
-@ignore_warnings(category=FutureWarning)
 # 0.22 AMI and NMI changes
+@pytest.mark.filterwarnings('ignore::FutureWarning')
 @pytest.mark.parametrize("metric_name", NORMALIZED_METRICS)
 def test_normalized_output(metric_name):
     upper_bound_1 = [0, 0, 0, 1, 1, 1]
@@ -123,15 +123,15 @@ def test_normalized_output(metric_name):
     assert not (score < 0).any()
 
 
-# All clustering metrics do not change score due to permutations of labels
-# that is when 0 and 1 exchanged.
-@ignore_warnings(category=FutureWarning)
 # 0.22 AMI and NMI changes
+@pytest.mark.filterwarnings('ignore::FutureWarning')
 @pytest.mark.parametrize(
     "metric_name",
     dict(SUPERVISED_METRICS, **UNSUPERVISED_METRICS)
 )
 def test_permute_labels(metric_name):
+    # All clustering metrics do not change score due to permutations of labels
+    # that is when 0 and 1 exchanged.
     y_label = np.array([0, 0, 0, 1, 1, 0, 1])
     y_pred = np.array([1, 0, 1, 0, 1, 1, 0])
     if metric_name in SUPERVISED_METRICS:
@@ -147,13 +147,13 @@ def test_permute_labels(metric_name):
         assert_allclose(score_1, metric(X, 1 - y_pred))
 
 
-# For all clustering metrics Input parameters can be both
-@ignore_warnings(category=FutureWarning)
 # 0.22 AMI and NMI changes
+@pytest.mark.filterwarnings('ignore::FutureWarning')
 @pytest.mark.parametrize(
     "metric_name",
     dict(SUPERVISED_METRICS, **UNSUPERVISED_METRICS)
 )
+# For all clustering metrics Input parameters can be both
 # in the form of arrays lists, positive, negetive or string
 def test_format_invariance(metric_name):
     y_true = [0, 0, 0, 0, 1, 1, 1, 1]
diff --git a/sklearn/metrics/tests/test_common.py b/sklearn/metrics/tests/test_common.py
index 51ebd00f00cc..b804a97f1534 100644
--- a/sklearn/metrics/tests/test_common.py
+++ b/sklearn/metrics/tests/test_common.py
@@ -198,22 +198,20 @@ def precision_recall_curve_padded_thresholds(*args, **kwargs):
 
     "brier_score_loss": brier_score_loss,
 
-    "roc_auc_score": roc_auc_score,
+    "roc_auc_score": roc_auc_score,  # default: average="macro"
     "weighted_roc_auc": partial(roc_auc_score, average="weighted"),
     "samples_roc_auc": partial(roc_auc_score, average="samples"),
     "micro_roc_auc": partial(roc_auc_score, average="micro"),
-    "macro_roc_auc": partial(roc_auc_score, average="macro"),
     "partial_roc_auc": partial(roc_auc_score, max_fpr=0.5),
 
-    "average_precision_score": average_precision_score,
+    "average_precision_score":
+    average_precision_score,  # default: average="macro"
     "weighted_average_precision_score":
     partial(average_precision_score, average="weighted"),
     "samples_average_precision_score":
     partial(average_precision_score, average="samples"),
     "micro_average_precision_score":
     partial(average_precision_score, average="micro"),
-    "macro_average_precision_score":
-    partial(average_precision_score, average="macro"),
     "label_ranking_average_precision_score":
     label_ranking_average_precision_score,
 }
@@ -253,14 +251,12 @@ def precision_recall_curve_padded_thresholds(*args, **kwargs):
     "roc_auc_score",
     "micro_roc_auc",
     "weighted_roc_auc",
-    "macro_roc_auc",
     "samples_roc_auc",
     "partial_roc_auc",
 
     "average_precision_score",
     "weighted_average_precision_score",
     "micro_average_precision_score",
-    "macro_average_precision_score",
     "samples_average_precision_score",
 
     # with default average='binary', multiclass is prohibited
@@ -301,7 +297,6 @@ def precision_recall_curve_padded_thresholds(*args, **kwargs):
     "average_precision_score",
     "weighted_average_precision_score",
     "micro_average_precision_score",
-    "macro_average_precision_score",
     "samples_average_precision_score",
 
     # pos_label support deprecated; to be removed in 0.18:
@@ -353,11 +348,10 @@ def precision_recall_curve_padded_thresholds(*args, **kwargs):
     "unnormalized_log_loss",
 
     "roc_auc_score", "weighted_roc_auc", "samples_roc_auc",
-    "micro_roc_auc", "macro_roc_auc", "partial_roc_auc",
+    "micro_roc_auc", "partial_roc_auc",
 
     "average_precision_score", "weighted_average_precision_score",
     "samples_average_precision_score", "micro_average_precision_score",
-    "macro_average_precision_score",
 
     "coverage_error", "label_ranking_loss",
     "label_ranking_average_precision_score",
diff --git a/sklearn/neighbors/tests/test_lof.py b/sklearn/neighbors/tests/test_lof.py
index 8c54743ced51..a6fb572b6740 100644
--- a/sklearn/neighbors/tests/test_lof.py
+++ b/sklearn/neighbors/tests/test_lof.py
@@ -29,7 +29,8 @@
 iris.target = iris.target[perm]
 
 
-@ignore_warnings(category=DeprecationWarning)  # contamination changed to 'auto' 0.22
+@ignore_warnings(category=DeprecationWarning)
+# contamination changed to 'auto' 0.22
 def test_lof():
     # Toy sample (the last two samples are outliers):
     X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], [5, 3], [-4, 2]]
@@ -48,7 +49,8 @@ def test_lof():
     assert_array_equal(clf._predict(), 6 * [1] + 2 * [-1])
 
 
-@ignore_warnings(category=DeprecationWarning)  # contamination changed to 'auto' 0.22
+@ignore_warnings(category=DeprecationWarning)
+# contamination changed to 'auto' 0.22
 def test_lof_performance():
     # Generate train/test data
     rng = check_random_state(2)
@@ -70,7 +72,8 @@ def test_lof_performance():
     assert_greater(roc_auc_score(y_test, y_pred), .99)
 
 
-@ignore_warnings(category=DeprecationWarning)  # contamination changed to 'auto' 0.22
+@ignore_warnings(category=DeprecationWarning)
+# contamination changed to 'auto' 0.22
 def test_lof_values():
     # toy samples:
     X_train = [[1, 1], [1, 2], [2, 1]]
@@ -90,7 +93,8 @@ def test_lof_values():
     assert_array_almost_equal(-clf2._score_samples([[1., 1.]]), [s_1])
 
 
-@ignore_warnings(category=DeprecationWarning)  # contamination changed to 'auto' 0.22
+@ignore_warnings(category=DeprecationWarning)
+# contamination changed to 'auto' 0.22
 def test_lof_precomputed(random_state=42):
     """Tests LOF with a distance matrix."""
     # Note: smaller samples may result in spurious test success
@@ -116,7 +120,8 @@ def test_lof_precomputed(random_state=42):
     assert_array_almost_equal(pred_X_Y, pred_D_Y)
 
 
-@ignore_warnings(category=DeprecationWarning)  # contamination changed to 'auto' 0.22
+@ignore_warnings(category=DeprecationWarning)
+# contamination changed to 'auto' 0.22
 def test_n_neighbors_attribute():
     X = iris.data
     clf = neighbors.LocalOutlierFactor(n_neighbors=500).fit(X)
@@ -129,7 +134,8 @@ def test_n_neighbors_attribute():
     assert_equal(clf.n_neighbors_, X.shape[0] - 1)
 
 
-@ignore_warnings(category=DeprecationWarning)  # contamination changed to 'auto' 0.22
+@ignore_warnings(category=DeprecationWarning)
+# contamination changed to 'auto' 0.22
 def test_score_samples():
     X_train = [[1, 1], [1, 2], [2, 1]]
     clf1 = neighbors.LocalOutlierFactor(n_neighbors=2,
diff --git a/sklearn/preprocessing/tests/test_function_transformer.py b/sklearn/preprocessing/tests/test_function_transformer.py
index 966901333e7b..464581e5e9c2 100644
--- a/sklearn/preprocessing/tests/test_function_transformer.py
+++ b/sklearn/preprocessing/tests/test_function_transformer.py
@@ -26,7 +26,8 @@ def test_delegate_to_func():
     kwargs_store = {}
     X = np.arange(10).reshape((5, 2))
     assert_array_equal(
-        FunctionTransformer(_make_func(args_store, kwargs_store), validate=False).transform(X),
+        FunctionTransformer(_make_func(args_store, kwargs_store),
+                            validate=False).transform(X),
         X, 'transform should have returned X unchanged',
     )
 
diff --git a/sklearn/svm/tests/test_svm.py b/sklearn/svm/tests/test_svm.py
index eeb92e4824b9..6187a08f7b75 100644
--- a/sklearn/svm/tests/test_svm.py
+++ b/sklearn/svm/tests/test_svm.py
@@ -441,14 +441,16 @@ def test_sample_weights():
     clf.fit(X, Y, sample_weight=np.repeat(0.01, len(X)))
     assert_array_almost_equal(dual_coef_no_weight, clf.dual_coef_)
 
-@ignore_warnings(UndefinedMetricWarning)
+
+@ignore_warnings(category=UndefinedMetricWarning)
 def test_auto_weight():
     # Test class weights for imbalanced data
     from sklearn.linear_model import LogisticRegression
     # We take as dataset the two-dimensional projection of iris so
     # that it is not separable and remove half of predictors from
     # class 1.
-    # We add one to the targets as a non-regression test: class_weight="balanced"
+    # We add one to the targets as a non-regression test:
+    # class_weight="balanced"
     # used to work only when the labels where a range [0..K).
     from sklearn.utils import compute_class_weight
     X, y = iris.data[:, :2], iris.target + 1
diff --git a/sklearn/tests/test_common.py b/sklearn/tests/test_common.py
index 1f4c41ec8285..2cde1ca9a14a 100644
--- a/sklearn/tests/test_common.py
+++ b/sklearn/tests/test_common.py
@@ -110,6 +110,8 @@ def test_no_attributes_set_in_init(name, Estimator):
         check_no_attributes_set_in_init(name, estimator)
 
 
+@ignore_warnings(category=DeprecationWarning)
+# ignore deprecated open(.., 'U') in numpy distutils
 def test_configure():
     # Smoke test the 'configure' step of setup, this tests all the
     # 'configure' functions in the setup.pys in scikit-learn
diff --git a/sklearn/tests/test_docstring_parameters.py b/sklearn/tests/test_docstring_parameters.py
index de632cbd7c96..69f72d934479 100644
--- a/sklearn/tests/test_docstring_parameters.py
+++ b/sklearn/tests/test_docstring_parameters.py
@@ -60,6 +60,9 @@
 ]
 
 
+# numpydoc 0.8.0's docscrape tool raises because of collections.abc under
+# Python 3.7
+@ignore_warnings(category=DeprecationWarning)
 def test_docstring_parameters():
     # Test module docstring formatting
 
diff --git a/sklearn/tests/test_impute.py b/sklearn/tests/test_impute.py
index 978a948c824b..acd9117e9f7d 100644
--- a/sklearn/tests/test_impute.py
+++ b/sklearn/tests/test_impute.py
@@ -323,7 +323,7 @@ def test_imputation_most_frequent_pandas(dtype):
 @pytest.mark.parametrize("X_data, missing_value", [(1, 0), (1., np.nan)])
 def test_imputation_constant_error_invalid_type(X_data, missing_value):
     # Verify that exceptions are raised on invalid fill_value type
-    X = np.full((3, 5), X_data)
+    X = np.full((3, 5), X_data, dtype=float)
     X[0, 0] = missing_value
 
     with pytest.raises(ValueError, match="imputing numerical"):
diff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py
index 4bbc4270e129..953e5bb88810 100644
--- a/sklearn/utils/estimator_checks.py
+++ b/sklearn/utils/estimator_checks.py
@@ -589,7 +589,7 @@ def check_sample_weights_invariance(name, estimator_orig):
             if hasattr(estimator_orig, method):
                 X_pred1 = getattr(estimator1, method)(X)
                 X_pred2 = getattr(estimator2, method)(X)
-                assert_allclose(X_pred1, X_pred2, rtol=0.5,
+                assert_allclose(X_pred1, X_pred2,
                                 err_msg="For %s sample_weight=None is not"
                                         " equivalent to sample_weight=ones"
                                         % name)
