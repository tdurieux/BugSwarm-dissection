diff --git a/doc/whats_new/v0.20.rst b/doc/whats_new/v0.20.rst
index 703a0e7c84ce..ef73a440e149 100644
--- a/doc/whats_new/v0.20.rst
+++ b/doc/whats_new/v0.20.rst
@@ -92,7 +92,9 @@ Changelog
 
 Support for Python 3.3 has been officially dropped.
 
-.. rubric:: :mod:`sklearn.cluster`:
+
+:mod:`sklearn.cluster`
+......................
 
 - |MajorFeature| A new clustering algorithm: :class:`cluster.OPTICS`: an
   algoritm related to :class:`cluster.DBSCAN`, that has hyperparameters easier
@@ -142,7 +144,9 @@ Support for Python 3.3 has been officially dropped.
   :class:`cluster.AgglomerativeClustering`.
   :issue:`9875` by :user:`Kumar Ashutosh <thechargedneutron>`.
 
-.. rubric:: :mod:`sklearn.compose`:
+
+:mod:`sklearn.compose`
+......................
 
 - New module.
 
@@ -156,7 +160,9 @@ Support for Python 3.3 has been officially dropped.
   are mapped back to the original space via an inverse transform. :issue:`9041`
   by `Andreas Müller`_ and :user:`Guillaume Lemaitre <glemaitre>`.
 
-.. rubric:: :mod:`sklearn.covariance`:
+
+:mod:`sklearn.covariance`
+.........................
 
 - |API| The :func:`covariance.graph_lasso`,
   :class:`covariance.GraphLasso` and :class:`covariance.GraphLassoCV` have been
@@ -165,7 +171,9 @@ Support for Python 3.3 has been officially dropped.
   respectively and will be removed in version 0.22.
   :issue:`9993` by :user:`Artiem Krinitsyn <artiemq>`
 
-.. rubric:: :mod:`sklearn.datasets`:
+
+:mod:`sklearn.datasets`
+.......................
 
 - |Feature| In :func:`datasets.make_blobs`, one can now pass a list to the
   `n_samples` parameter to indicate the number of samples to generate per
@@ -190,7 +198,9 @@ Support for Python 3.3 has been officially dropped.
   data points could be generated. :issue:`10037` by :user:`Christian Braune
   <christianbraune79>`.
 
-.. rubric:: :mod:`sklearn.decomposition`:
+
+:mod:`sklearn.decomposition`
+............................
 
 - |Feature| :func:`decomposition.dict_learning` functions and models now
   support positivity constraints. This applies to the dictionary and sparse
@@ -230,13 +240,17 @@ Support for Python 3.3 has been officially dropped.
   :issue:`5956` by :user:`Vighnesh Birodkar <vighneshbirodkar>` and
   :user:`Olivier Grisel <ogrisel>`.
 
-.. rubric:: :mod:`sklearn.discriminant_analysis`:
+
+:mod:`sklearn.discriminant_analysis`
+....................................
 
 - |Efficiency| Memory usage improvement for :func:`_class_means` and
   :func:`_class_cov` in :mod:`discriminant_analysis`. :issue:`10898` by
   :user:`Nanxin Chen <bobchennan>`.`
 
-.. rubric:: :mod:`sklearn.dummy`:
+
+:mod:`sklearn.dummy`
+....................
 
 - |Feature| :class:`dummy.DummyRegressor` now has a ``return_std`` option in its
   ``predict`` method. The returned standard deviations will be zeros.
@@ -245,7 +259,9 @@ Support for Python 3.3 has been officially dropped.
   only require X to be an object with finite length or shape. :issue:`9832` by
   :user:`Vrishank Bhardwaj <vrishank97>`.
 
-.. rubric:: :mod:`sklearn.ensemble`:
+
+:mod:`sklearn.ensemble`
+.......................
 
 - |Feature| :class:`ensemble.BaggingRegressor` and
   :class:`ensemble.BaggingClassifier` can now be fit with missing/non-finite
@@ -293,7 +309,9 @@ Support for Python 3.3 has been officially dropped.
   reproduce ``fit`` result usinbg the object attributes when ``random_state``
   is set. :issue:`9723` by :user:`Guillaume Lemaitre <glemaitre>`.
 
-.. rubric:: :mod:`sklearn.feature_extraction`:
+
+:mod:`sklearn.feature_extraction`
+.................................
 
 - |Feature| Enable the call to :term:`get_feature_names` in unfitted
   :class:`feature_extraction.text.CountVectorizer` initialized with a
@@ -318,25 +336,34 @@ Support for Python 3.3 has been officially dropped.
   :issue:`10441` by :user:`Mayur Kulkarni <maykulkarni>` and
   :user:`Guillaume Lemaitre <glemaitre>`.
 
-.. rubric:: :mod:`sklearn.feature_selection`:
+
+:mod:`sklearn.feature_selection`
+................................
 
 - |Feature| Added select K best features functionality to
   :class:`feature_selection.SelectFromModel`.
   :issue:`6689` by :user:`Nihar Sheth <nsheth12>` and
   :user:`Quazi Rahman <qmaruf>`.
 
+- |Feature| Added ``min_features_to_select`` parameter to
+  :class:`feature_selection.RFECV` to bound evaluated features counts.
+  :issue:`11293` by :user:`Brent Yi <brentyi>`.
+
 - |Fix| Fixed computation of ``n_features_to_compute`` for edge case with tied
   CV scores in :class:`feature_selection.RFECV`.
   :issue:`9222` by :user:`Nick Hoh <nickypie>`.
 
-.. rubric:: :mod:`sklearn.gaussian_process`:
+:mod:`sklearn.gaussian_process`
+...............................
 
 - |Efficiency| In :class:`gaussian_process.GaussianProcessRegressor`, method
   ``predict`` is faster when using ``return_std=True`` in particular more when
   called several times in a row. :issue:`9234` by :user:`andrewww <andrewww>`
   and :user:`Minghui Liu <minghui-liu>`.
 
-.. rubric:: :mod:`sklearn.impute`:
+
+:mod:`sklearn.impute`
+.....................
 
 - New module, adopting ``preprocessing.Imputer`` as
   :class:`impute.SimpleImputer` with minor changes (see under preprocessing
@@ -352,14 +379,18 @@ Support for Python 3.3 has been officially dropped.
   data, and so does the ``'most_frequent'`` strategy now. :issue:`11211` by
   :user:`Jeremie du Boisberranger <jeremiedbb>`.
 
-.. rubric:: :mod:`sklearn.isotonic`:
+
+:mod:`sklearn.isotonic`
+.......................
 
 - |Fix| Fixed a bug in :class:`isotonic.IsotonicRegression` which incorrectly
   combined weights when fitting a model to data involving points with
   identical X values.
   :issue:`9432` by :user:`Dallas Card <dallascard>`
 
-.. rubric:: :mod:`sklearn.linear_model`:
+
+:mod:`sklearn.linear_model`
+...........................
 
 - |Feature| :class:`linear_model.SGDClassifier`,
   :class:`linear_model.SGDRegressor`,
@@ -456,7 +487,9 @@ Support for Python 3.3 has been officially dropped.
   estimators will report at most ``max_iter`` iterations even if more were
   performed. :issue:`10723` by `Joel Nothman`_.
 
-.. rubric:: :mod:`sklearn.manifold`:
+
+:mod:`sklearn.manifold`
+.......................
 
 - |Efficiency| Speed improvements for both 'exact' and 'barnes_hut' methods in
   :class:`manifold.TSNE`. :issue:`10593` and :issue:`10610` by
@@ -482,7 +515,9 @@ Support for Python 3.3 has been officially dropped.
   pairwise distances or squared distances. :issue:`9775` by
   :user:`William de Vazelhes <wdevazelhes>`.
 
-.. rubric:: :mod:`sklearn.metrics`:
+
+:mod:`sklearn.metrics`
+......................
 
 - |MajorFeature| Added the :func:`metrics.davies_bouldin_score` metric for
   evaluation of clustering models without a ground truth. :issue:`10827` by
@@ -582,7 +617,9 @@ Support for Python 3.3 has been officially dropped.
   ``working_memory`` config. See :ref:`working_memory`. :issue:`10280` by `Joel
   Nothman`_ and :user:`Aman Dalmia <dalmia>`.
 
-.. rubric:: :mod:`sklearn.mixture`:
+
+:mod:`sklearn.mixture`
+......................
 
 - |Feature| Added function :term:`fit_predict` to :class:`mixture.GaussianMixture`
   and :class:`mixture.GaussianMixture`, which is essentially equivalent to
@@ -600,7 +637,9 @@ Support for Python 3.3 has been officially dropped.
   initializations (when ``n_init > 1``), but just the lower bound of the last
   initialization. :issue:`10869` by :user:`Aurélien Géron <ageron>`.
 
-.. rubric:: :mod:`sklearn.model_selection`:
+
+:mod:`sklearn.model_selection`
+..............................
 
 - |Feature| Add `return_estimator` parameter in
   :func:`model_selection.cross_validate` to return estimators fitted on each
@@ -656,12 +695,16 @@ Support for Python 3.3 has been officially dropped.
   raises TypeError.
   :issue:`10928` by :user:`Solutus Immensus <solutusimmensus>`
 
-.. rubric:: :mod:`sklearn.multioutput`:
+
+:mod:`sklearn.multioutput`
+..........................
 
 - |MajorFeature| Added :class:`multioutput.RegressorChain` for multi-target
   regression. :issue:`9257` by :user:`Kumar Ashutosh <thechargedneutron>`.
 
-.. rubric:: :mod:`sklearn.naive_bayes`:
+
+:mod:`sklearn.naive_bayes`
+..........................
 
 - |MajorFeature| Added :class:`naive_bayes.ComplementNB`, which implements the
   Complement Naive Bayes classifier described in Rennie et al. (2003).
@@ -679,7 +722,9 @@ Support for Python 3.3 has been officially dropped.
   vector valued pseudocounts (alpha).
   :issue:`10346` by :user:`Tobias Madsen <TobiasMadsen>`
 
-.. rubric:: :mod:`sklearn.neighbors`:
+
+:mod:`sklearn.neighbors`
+........................
 
 - |Efficiency| :class:`neighbors.RadiusNeighborsRegressor` and
   :class:`neighbors.RadiusNeighborsClassifier` are now
@@ -719,7 +764,13 @@ Support for Python 3.3 has been officially dropped.
   faster construction and querying times.
   :issue:`11556` by :user:`Jake VanderPlas <jakevdp>`
 
-.. rubric:: :mod:`sklearn.neural_network`:
+- |Fix| Fixed a bug in `neighbors.KDTree` and `neighbors.BallTree` where
+  pickled tree objects would change their type to the super class `BinaryTree`.
+  :issue:`11774` by :user:`Nicolas Hug <NicolasHug>`.
+
+
+:mod:`sklearn.neural_network`
+.............................
 
 - |Feature| Add `n_iter_no_change` parameter in
   :class:`neural_network.BaseMultilayerPerceptron`,
@@ -738,14 +789,18 @@ Support for Python 3.3 has been officially dropped.
   quit unexpectedly early due to local minima or fluctuations.
   :issue:`9456` by :user:`Nicholas Nadeau <nnadeau>`
 
-.. rubric:: :mod:`sklearn.pipeline`:
+
+:mod:`sklearn.pipeline`
+.......................
 
 - |Feature| The ``predict`` method of :class:`pipeline.Pipeline` now passes
   keyword arguments on to the pipeline's last estimator, enabling the use of
   parameters such as ``return_std`` in a pipeline with caution.
   :issue:`9304` by :user:`Breno Freitas <brenolf>`.
 
-.. rubric:: :mod:`sklearn.preprocessing`:
+
+:mod:`sklearn.preprocessing`
+............................
 
 - |MajorFeature| Expanded :class:`preprocessing.OneHotEncoder` to allow to
   encode categorical string features as a numeric array using a one-hot (or
@@ -869,7 +924,9 @@ Support for Python 3.3 has been officially dropped.
   ``validate`` will be from ``True`` to ``False`` in 0.22.
   :issue:`10655` by :user:`Guillaume Lemaitre <glemaitre>`.
 
-.. rubric:: :mod:`sklearn.svm`:
+
+:mod:`sklearn.svm`
+..................
 
 - |Fix| Fixed a bug in :class:`svm.SVC` where when the argument ``kernel`` is
   unicode in Python2, the ``predict_proba`` method was raising an
@@ -886,7 +943,9 @@ Support for Python 3.3 has been officially dropped.
   version 0.22 to account better for unscaled features. :issue:`8361` by
   :user:`Gaurav Dhingra <gxyd>` and :user:`Ting Neo <neokt>`.
 
-.. rubric:: :mod:`sklearn.tree`:
+
+:mod:`sklearn.tree`
+...................
 
 - |Fix| Fixed a bug in :class:`tree.BaseDecisionTree` with `splitter="best"`
   where split threshold could become infinite when values in X were
@@ -898,7 +957,9 @@ Support for Python 3.3 has been officially dropped.
   considered all samples to be of equal weight importance.
   :issue:`11464` by :user:`John Stott <JohnStott>`.
 
-.. rubric:: :mod:`sklearn.utils`:
+
+:mod:`sklearn.utils`
+....................
 
 - |Feature| :func:`utils.check_array` and :func:`utils.check_X_y` now have
   ``accept_large_sparse`` to control whether scipy.sparse matrices with 64-bit
@@ -913,7 +974,9 @@ Support for Python 3.3 has been officially dropped.
   that arrays of bytes/strings will be interpreted as decimal numbers
   beginning in version 0.22. :issue:`10229` by :user:`Ryan Lee <rtlee9>`
 
-.. rubric:: Multiple modules
+
+Multiple modules
+................
 
 - |Feature| |API| More consistent outlier detection API:
   Add a ``score_samples`` method in :class:`svm.OneClassSVM`,
@@ -956,7 +1019,9 @@ Support for Python 3.3 has been officially dropped.
   :class:`cluster.AffinityPropagation`, and :class:`cluster.Birch`.
   :issue:`10306` by :user:`Jonathan Siebert <jotasi>`.
 
-.. rubric:: Miscellaneous
+
+Miscellaneous
+.............
 
 - |MajorFeature| A new configuration parameter, ``working_memory`` was added
   to control memory consumption limits in chunked operations, such as the new
diff --git a/sklearn/feature_selection/rfe.py b/sklearn/feature_selection/rfe.py
index 8d2ed69b105c..b6df8ad962b4 100644
--- a/sklearn/feature_selection/rfe.py
+++ b/sklearn/feature_selection/rfe.py
@@ -60,12 +60,12 @@ class RFE(BaseEstimator, MetaEstimatorMixin, SelectorMixin):
         are selected.
 
     step : int or float, optional (default=1)
-        If greater than or equal to 1, then `step` corresponds to the (integer)
-        number of features to remove at each iteration.
-        If within (0.0, 1.0), then `step` corresponds to the percentage
+        If greater than or equal to 1, then ``step`` corresponds to the
+        (integer) number of features to remove at each iteration.
+        If within (0.0, 1.0), then ``step`` corresponds to the percentage
         (rounded down) of features to remove at each iteration.
 
-    verbose : int, default=0
+    verbose : int, (default=0)
         Controls verbosity of output.
 
     Attributes
@@ -335,10 +335,18 @@ class RFECV(RFE, MetaEstimatorMixin):
         attribute or through a ``feature_importances_`` attribute.
 
     step : int or float, optional (default=1)
-        If greater than or equal to 1, then `step` corresponds to the (integer)
-        number of features to remove at each iteration.
-        If within (0.0, 1.0), then `step` corresponds to the percentage
+        If greater than or equal to 1, then ``step`` corresponds to the
+        (integer) number of features to remove at each iteration.
+        If within (0.0, 1.0), then ``step`` corresponds to the percentage
         (rounded down) of features to remove at each iteration.
+        Note that the last iteration may remove fewer than ``step`` features in
+        order to reach ``min_features_to_select``.
+
+    min_features_to_select : int, (default=1)
+        The minimum number of features to be selected. This number of features
+        will always be scored, even if the difference between the original
+        feature count and ``min_features_to_select`` isn't divisible by
+        ``step``.
 
     cv : int, cross-validation generator or an iterable, optional
         Determines the cross-validation splitting strategy.
@@ -358,20 +366,20 @@ class RFECV(RFE, MetaEstimatorMixin):
         cross-validation strategies that can be used here.
 
         .. versionchanged:: 0.20
-            ``cv`` default value if None will change from 3-fold to 5-fold
+            ``cv`` default value of None will change from 3-fold to 5-fold
             in v0.22.
 
-    scoring : string, callable or None, optional, default: None
+    scoring : string, callable or None, optional, (default=None)
         A string (see model evaluation documentation) or
         a scorer callable object / function with signature
         ``scorer(estimator, X, y)``.
 
-    verbose : int, default=0
+    verbose : int, (default=0)
         Controls verbosity of output.
 
-    n_jobs : int, default 1
+    n_jobs : int, (default=1)
         Number of cores to run in parallel while fitting across folds.
-        Defaults to 1 core. If `n_jobs=-1`, then number of jobs is set
+        Defaults to 1 core. If ``n_jobs=-1``, then number of jobs is set
         to number of cores.
 
     Attributes
@@ -399,7 +407,8 @@ class RFECV(RFE, MetaEstimatorMixin):
 
     Notes
     -----
-    The size of ``grid_scores_`` is equal to ceil((n_features - 1) / step) + 1,
+    The size of ``grid_scores_`` is equal to
+    ``ceil((n_features - min_features_to_select) / step) + 1``,
     where step is the number of features removed at each iteration.
 
     Examples
@@ -431,14 +440,15 @@ class RFECV(RFE, MetaEstimatorMixin):
            for cancer classification using support vector machines",
            Mach. Learn., 46(1-3), 389--422, 2002.
     """
-    def __init__(self, estimator, step=1, cv='warn', scoring=None, verbose=0,
-                 n_jobs=None):
+    def __init__(self, estimator, step=1, min_features_to_select=1, cv='warn',
+                 scoring=None, verbose=0, n_jobs=None):
         self.estimator = estimator
         self.step = step
         self.cv = cv
         self.scoring = scoring
         self.verbose = verbose
         self.n_jobs = n_jobs
+        self.min_features_to_select = min_features_to_select
 
     def fit(self, X, y, groups=None):
         """Fit the RFE model and automatically tune the number of selected
@@ -464,7 +474,6 @@ def fit(self, X, y, groups=None):
         cv = check_cv(self.cv, y, is_classifier(self.estimator))
         scorer = check_scoring(self.estimator, scoring=self.scoring)
         n_features = X.shape[1]
-        n_features_to_select = 1
 
         if 0.0 < self.step < 1.0:
             step = int(max(1, self.step * n_features))
@@ -473,8 +482,10 @@ def fit(self, X, y, groups=None):
         if step <= 0:
             raise ValueError("Step must be >0")
 
+        # Build an RFE object, which will evaluate and score each possible
+        # feature count, down to self.min_features_to_select
         rfe = RFE(estimator=self.estimator,
-                  n_features_to_select=n_features_to_select,
+                  n_features_to_select=self.min_features_to_select,
                   step=self.step, verbose=self.verbose)
 
         # Determine the number of subsets of features by fitting across
@@ -504,7 +515,7 @@ def fit(self, X, y, groups=None):
         argmax_idx = len(scores) - np.argmax(scores_rev) - 1
         n_features_to_select = max(
             n_features - (argmax_idx * step),
-            n_features_to_select)
+            self.min_features_to_select)
 
         # Re-execute an elimination with best_k over the whole set
         rfe = RFE(estimator=self.estimator,
diff --git a/sklearn/feature_selection/tests/test_rfe.py b/sklearn/feature_selection/tests/test_rfe.py
index 29854bb1df57..41b4a9e767c1 100644
--- a/sklearn/feature_selection/tests/test_rfe.py
+++ b/sklearn/feature_selection/tests/test_rfe.py
@@ -1,6 +1,8 @@
 """
 Testing Recursive feature elimination
 """
+from __future__ import division
+
 import pytest
 import numpy as np
 from numpy.testing import assert_array_almost_equal, assert_array_equal
@@ -229,6 +231,26 @@ def test_rfecv_verbose_output():
     assert_greater(len(verbose_output.readline()), 0)
 
 
+def test_rfecv_grid_scores_size():
+    generator = check_random_state(0)
+    iris = load_iris()
+    X = np.c_[iris.data, generator.normal(size=(len(iris.data), 6))]
+    y = list(iris.target)   # regression test: list should be supported
+
+    # Non-regression test for varying combinations of step and
+    # min_features_to_select.
+    for step, min_features_to_select in [[2, 1], [2, 2], [3, 3]]:
+        rfecv = RFECV(estimator=MockClassifier(), step=step,
+                      min_features_to_select=min_features_to_select, cv=5)
+        rfecv.fit(X, y)
+
+        score_len = np.ceil(
+            (X.shape[1] - min_features_to_select) / step) + 1
+        assert len(rfecv.grid_scores_) == score_len
+        assert len(rfecv.ranking_) == X.shape[1]
+        assert rfecv.n_features_ >= min_features_to_select
+
+
 @pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_rfe_estimator_tags():
     rfe = RFE(SVC(kernel='linear'))
diff --git a/sklearn/linear_model/coordinate_descent.py b/sklearn/linear_model/coordinate_descent.py
index bb3bbe8fcf9a..09736927bea6 100644
--- a/sklearn/linear_model/coordinate_descent.py
+++ b/sklearn/linear_model/coordinate_descent.py
@@ -1613,13 +1613,13 @@ class MultiTaskElasticNet(Lasso):
 
     The optimization objective for MultiTaskElasticNet is::
 
-        (1 / (2 * n_samples)) * ||Y - XW||^Fro_2
+        (1 / (2 * n_samples)) * ||Y - XW||_Fro^2
         + alpha * l1_ratio * ||W||_21
         + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2
 
     Where::
 
-        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}
+        ||W||_21 = sum_i sqrt(sum_j w_ij ^ 2)
 
     i.e. the sum of norm of each row.
 
diff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py
index c590d86746dd..ae6984d7a282 100644
--- a/sklearn/linear_model/logistic.py
+++ b/sklearn/linear_model/logistic.py
@@ -1192,6 +1192,7 @@ def __init__(self, penalty='l2', dual=False, tol=1e-4, C=1.0,
         self.tol = tol
         self.C = C
         self.fit_intercept = fit_intercept
+        self.intercept_scaling = intercept_scaling
         self.class_weight = class_weight
         self.random_state = random_state
         self.solver = solver
@@ -1201,16 +1202,6 @@ def __init__(self, penalty='l2', dual=False, tol=1e-4, C=1.0,
         self.warm_start = warm_start
         self.n_jobs = n_jobs
 
-        if fit_intercept and intercept_scaling == 'warn':
-            warnings.warn("liblinear regularizes the intercept."
-                          " Therefore intercept_scaling should be set "
-                          "appropriately when fit_intercept is set to True. "
-                          "Default value of 1 is used.",
-                          UserWarning)
-            self.intercept_scaling = 1.
-        else:
-            self.intercept_scaling = intercept_scaling
-
     def fit(self, X, y, sample_weight=None):
         """Fit the model according to the given training data.
 
diff --git a/sklearn/neighbors/binary_tree.pxi b/sklearn/neighbors/binary_tree.pxi
index ede29c2bb4d6..5d618ed49440 100755
--- a/sklearn/neighbors/binary_tree.pxi
+++ b/sklearn/neighbors/binary_tree.pxi
@@ -1119,7 +1119,7 @@ cdef class BinaryTree:
         """
         reduce method used for pickling
         """
-        return (newObj, (BinaryTree,), self.__getstate__())
+        return (newObj, (type(self),), self.__getstate__())
 
     def __getstate__(self):
         """
@@ -1136,7 +1136,8 @@ cdef class BinaryTree:
                 int(self.n_leaves),
                 int(self.n_splits),
                 int(self.n_calls),
-                self.dist_metric)
+                self.dist_metric,
+                self.sample_weight)
 
     def __setstate__(self, state):
         """
@@ -1162,6 +1163,7 @@ cdef class BinaryTree:
         self.dist_metric = state[11]
         self.euclidean = (self.dist_metric.__class__.__name__
                           == 'EuclideanDistance')
+        self.sample_weight = state[12]
 
     def get_tree_stats(self):
         return (self.n_trims, self.n_leaves, self.n_splits)
diff --git a/sklearn/neighbors/tests/test_ball_tree.py b/sklearn/neighbors/tests/test_ball_tree.py
index de0d166fb889..52ed63ae416b 100644
--- a/sklearn/neighbors/tests/test_ball_tree.py
+++ b/sklearn/neighbors/tests/test_ball_tree.py
@@ -228,6 +228,8 @@ def check_pickle_protocol(protocol):
         assert_array_almost_equal(ind1_pyfunc, ind2_pyfunc)
         assert_array_almost_equal(dist1_pyfunc, dist2_pyfunc)
 
+        assert isinstance(bt2, BallTree)
+
     for protocol in (0, 1, 2):
         check_pickle_protocol(protocol)
 
diff --git a/sklearn/neighbors/tests/test_kd_tree.py b/sklearn/neighbors/tests/test_kd_tree.py
index 46cddc711e76..18d213802160 100644
--- a/sklearn/neighbors/tests/test_kd_tree.py
+++ b/sklearn/neighbors/tests/test_kd_tree.py
@@ -187,6 +187,7 @@ def check_pickle_protocol(protocol):
         ind2, dist2 = kdt2.query(X)
         assert_array_almost_equal(ind1, ind2)
         assert_array_almost_equal(dist1, dist2)
+        assert isinstance(kdt2, KDTree)
 
     check_pickle_protocol(protocol)
 
diff --git a/sklearn/neighbors/tests/test_kde.py b/sklearn/neighbors/tests/test_kde.py
index 022cbce1365a..990942c9efdc 100644
--- a/sklearn/neighbors/tests/test_kde.py
+++ b/sklearn/neighbors/tests/test_kde.py
@@ -10,6 +10,7 @@
 from sklearn.datasets import make_blobs
 from sklearn.model_selection import GridSearchCV
 from sklearn.preprocessing import StandardScaler
+from sklearn.externals import joblib
 
 
 def compute_kernel_slow(Y, X, kernel, h):
@@ -202,3 +203,23 @@ def test_kde_sample_weights():
                     kde.fit(X, sample_weight=(scale_factor * weights))
                     scores_scaled_weight = kde.score_samples(test_points)
                     assert_allclose(scores_scaled_weight, scores_weight)
+
+
+def test_pickling(tmpdir):
+    # Make sure that predictions are the same before and after pickling. Used
+    # to be a bug because sample_weights wasn't pickled and the resulting tree
+    # would miss some info.
+
+    kde = KernelDensity()
+    data = np.reshape([1., 2., 3.], (-1, 1))
+    kde.fit(data)
+
+    X = np.reshape([1.1, 2.1], (-1, 1))
+    scores = kde.score_samples(X)
+
+    file_path = str(tmpdir.join('dump.pkl'))
+    joblib.dump(kde, file_path)
+    kde = joblib.load(file_path)
+    scores_pickled = kde.score_samples(X)
+
+    assert_allclose(scores, scores_pickled)
