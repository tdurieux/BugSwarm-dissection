diff --git a/cosmo_tester/resources/hostpool/service-blueprint.yaml b/cosmo_tester/resources/hostpool/service-blueprint.yaml
index 8b700c40..07aef5c2 100644
--- a/cosmo_tester/resources/hostpool/service-blueprint.yaml
+++ b/cosmo_tester/resources/hostpool/service-blueprint.yaml
@@ -2,7 +2,7 @@ tosca_definitions_version: cloudify_dsl_1_3
 
 imports:
   - http://www.getcloudify.org/spec/cloudify/4.1/types.yaml
-  - http://www.getcloudify.org/spec/fabric-plugin/1.4.2/plugin.yaml
+  - http://www.getcloudify.org/spec/fabric-plugin/1.5/plugin.yaml
   - http://www.getcloudify.org/spec/openstack-plugin/2.0.1/plugin.yaml
   - https://raw.githubusercontent.com/cloudify-cosmo/cloudify-host-pool-service/1.1/host-pool-service.yaml
 
diff --git a/cosmo_tester/test_suites/ha/__init__.py b/cosmo_tester/test_suites/ha/__init__.py
new file mode 100644
index 00000000..e69de29b
diff --git a/cosmo_tester/test_suites/ha/ha_cluster_scenarios_test.py b/cosmo_tester/test_suites/ha/ha_cluster_scenarios_test.py
new file mode 100644
index 00000000..d9d20cc5
--- /dev/null
+++ b/cosmo_tester/test_suites/ha/ha_cluster_scenarios_test.py
@@ -0,0 +1,194 @@
+########
+# Copyright (c) 2017 GigaSpaces Technologies Ltd. All rights reserved
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#        http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+#    * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#    * See the License for the specific language governing permissions and
+#    * limitations under the License.
+
+import pytest
+from cosmo_tester.framework.examples.hello_world import HelloWorldExample
+from cosmo_tester.framework.cluster import CloudifyCluster
+from .ha_helper import HighAvailabilityHelper as ha_helper
+
+
+@pytest.fixture(scope='function', params=[2, 3])
+def cluster(
+        request, cfy, ssh_key, module_tmpdir, attributes, logger):
+    """Creates a HA cluster from an image in rackspace OpenStack."""
+    logger.info('Creating HA cluster of %s managers', request.param)
+    cluster = CloudifyCluster.create_image_based(
+        cfy,
+        ssh_key,
+        module_tmpdir,
+        attributes,
+        logger,
+        number_of_managers=request.param,
+        create=False)
+
+    for manager in cluster.managers[1:]:
+        manager.upload_plugins = False
+
+    cluster.create()
+
+    try:
+        manager1 = cluster.managers[0]
+        ha_helper.delete_active_profile()
+        manager1.use()
+
+        cfy.cluster.start(timeout=600,
+                          cluster_host_ip=manager1.private_ip_address,
+                          cluster_node_name=manager1.ip_address)
+
+        for manager in cluster.managers[1:]:
+            manager.use()
+            cfy.cluster.join(manager1.ip_address,
+                             timeout=600,
+                             cluster_host_ip=manager.private_ip_address,
+                             cluster_node_name=manager.ip_address)
+
+        cfy.cluster.nodes.list()
+
+        yield cluster
+
+    finally:
+        cluster.destroy()
+
+
+@pytest.fixture(scope='function')
+def hello_world(cfy, cluster, attributes, ssh_key, tmpdir, logger):
+    hw = HelloWorldExample(
+        cfy, cluster.managers[0], attributes, ssh_key, logger, tmpdir)
+    hw.blueprint_file = 'openstack-blueprint.yaml'
+    hw.inputs.update({
+        'agent_user': attributes.centos7_username,
+        'image': attributes.centos7_image_name,
+    })
+
+    yield hw
+    if hw.cleanup_required:
+        logger.info('Hello world cleanup required..')
+        cluster.managers[0].use()
+        hw.cleanup()
+
+
+def test_data_replication(cfy, cluster, hello_world,
+                          logger):
+    manager1 = cluster.managers[0]
+    ha_helper.delete_active_profile()
+    manager1.use()
+    ha_helper.verify_nodes_status(manager1, cfy, logger)
+    hello_world.upload_blueprint()
+    hello_world.create_deployment()
+    hello_world.install()
+
+    logger.info('Manager %s resources', manager1.ip_address)
+    m1_blueprints_list = cfy.blueprints.list()
+    m1_deployments_list = cfy.deployments.list()
+    m1_plugins_list = cfy.plugins.list()
+
+    for manager in cluster.managers[1:]:
+        ha_helper.set_active(manager, cfy, logger)
+        ha_helper.delete_active_profile()
+        manager.use()
+        ha_helper.verify_nodes_status(manager, cfy, logger)
+
+        logger.info('Manager %s resources', manager.ip_address)
+        assert m1_blueprints_list == cfy.blueprints.list()
+        assert m1_deployments_list == cfy.deployments.list()
+        assert m1_plugins_list == cfy.plugins.list()
+
+    ha_helper.set_active(manager1, cfy, logger)
+    ha_helper.delete_active_profile()
+    manager1.use()
+
+
+def test_set_active(cfy, cluster,
+                    logger):
+    manager1 = cluster.managers[0]
+    ha_helper.delete_active_profile()
+    manager1.use()
+    ha_helper.verify_nodes_status(manager1, cfy, logger)
+
+    for manager in cluster.managers[1:]:
+        ha_helper.set_active(manager, cfy, logger)
+        ha_helper.delete_active_profile()
+        manager.use()
+        ha_helper.verify_nodes_status(manager, cfy, logger)
+
+
+def test_delete_manager_node(cfy, cluster, hello_world,
+                             logger):
+    ha_helper.set_active(cluster.managers[1], cfy, logger)
+    expected_master = cluster.managers[0]
+
+    for manager in cluster.managers[1:]:
+        logger.info('Deleting manager %s', manager.ip_address)
+        manager.delete()
+        ha_helper.wait_leader_election(logger)
+
+    logger.info('Expected leader %s', expected_master)
+    ha_helper.verify_nodes_status(expected_master, cfy, logger)
+    hello_world.upload_blueprint()
+
+
+def test_failover(cfy, cluster, hello_world,
+                  logger):
+    for manager in cluster.managers[:-1]:
+        logger.info('Simulating manager %s failure by stopping'
+                    ' nginx service', manager.ip_address)
+        with manager.ssh() as fabric:
+            fabric.run('sudo systemctl stop nginx')
+        ha_helper.wait_leader_election(logger)
+        cfy.cluster.nodes.list()
+
+    expected_master = cluster.managers[-1]
+    ha_helper.delete_active_profile()
+    expected_master.use()
+    ha_helper.verify_nodes_status(expected_master, cfy, logger)
+
+    with expected_master.ssh() as fabric:
+        logger.info('Simulating manager %s failure by stopping '
+                    'cloudify-mgmtworker service',
+                    expected_master.ip_address)
+        fabric.run('sudo systemctl stop cloudify-mgmtworker')
+    ha_helper.wait_leader_election(logger)
+    cfy.cluster.nodes.list()
+
+    expected_master = cluster.managers[0]
+    logger.info('Starting nginx service on manager %s',
+                expected_master.ip_address)
+    with expected_master.ssh() as fabric:
+        fabric.run('sudo systemctl start nginx')
+    ha_helper.wait_leader_election(logger)
+    ha_helper.delete_active_profile()
+    expected_master.use()
+    ha_helper.verify_nodes_status(expected_master, cfy, logger)
+    hello_world.upload_blueprint()
+
+
+def test_remove_manager_from_cluster(cfy, cluster, hello_world,
+                                     logger):
+    ha_helper.set_active(cluster.managers[1], cfy, logger)
+    ha_helper.delete_active_profile()
+
+    for manager in cluster.managers[1:]:
+        manager.use()
+        logger.info('Removing the manager %s from HA cluster',
+                    manager.ip_address)
+        cfy.cluster.nodes.remove(manager.ip_address)
+        ha_helper.wait_leader_election(logger)
+
+    expected_master = cluster.managers[0]
+    ha_helper.delete_active_profile()
+    expected_master.use()
+
+    ha_helper.verify_nodes_status(expected_master, cfy, logger)
+    hello_world.upload_blueprint()
diff --git a/cosmo_tester/test_suites/ha/ha_helper.py b/cosmo_tester/test_suites/ha/ha_helper.py
new file mode 100644
index 00000000..5fd029c8
--- /dev/null
+++ b/cosmo_tester/test_suites/ha/ha_helper.py
@@ -0,0 +1,58 @@
+########
+# Copyright (c) 2017 GigaSpaces Technologies Ltd. All rights reserved
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#        http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+#    * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#    * See the License for the specific language governing permissions and
+#    * limitations under the License.
+
+
+import time
+import os
+
+
+class HighAvailabilityHelper(object):
+    @staticmethod
+    def set_active(manager, cfy, logger):
+        try:
+            logger.info('Setting active manager %s',
+                        manager.ip_address)
+            cfy.cluster('set-active', manager.ip_address)
+        except Exception as e:
+            logger.info('Setting active manager error message: %s', e.message)
+        finally:
+            time.sleep(120)
+
+    @staticmethod
+    def wait_leader_election(logger):
+        logger.info('Waiting for a leader election...')
+        time.sleep(120)
+
+    @staticmethod
+    def verify_nodes_status(manager, cfy, logger):
+        logger.info('Verifying that manager %s is a leader '
+                    'and others are replicas', manager.ip_address)
+        cfy.cluster.nodes.list()
+        nodes = manager.client.cluster.nodes.list()
+
+        for node in nodes:
+            if node.name == str(manager.ip_address):
+                assert node.master is True
+                logger.info('Manager %s is a leader ', node.name)
+            else:
+                assert node.master is not True
+                logger.info('Manager %s is a replica ', node.name)
+
+    @staticmethod
+    def delete_active_profile():
+        active_profile_path = os.path.join(os.environ['CFY_WORKDIR'],
+                                           '.cloudify/active.profile')
+        if os.path.exists(active_profile_path):
+            os.remove(active_profile_path)
diff --git a/cosmo_tester/test_suites/ha/ha_negative_test.py b/cosmo_tester/test_suites/ha/ha_negative_test.py
new file mode 100644
index 00000000..510a78da
--- /dev/null
+++ b/cosmo_tester/test_suites/ha/ha_negative_test.py
@@ -0,0 +1,174 @@
+########
+# Copyright (c) 2017 GigaSpaces Technologies Ltd. All rights reserved
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#        http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+#    * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#    * See the License for the specific language governing permissions and
+#    * limitations under the License.
+
+import pytest
+from cosmo_tester.framework.examples.hello_world import HelloWorldExample
+from cosmo_tester.framework.cluster import CloudifyCluster
+from .ha_helper import HighAvailabilityHelper as ha_helper
+
+
+@pytest.fixture(scope='function')
+def cluster(
+        request, cfy, ssh_key, module_tmpdir, attributes, logger):
+    """Creates a HA cluster from an image in rackspace OpenStack."""
+    logger.info('Creating HA cluster of 2 managers')
+    cluster = CloudifyCluster.create_image_based(
+        cfy,
+        ssh_key,
+        module_tmpdir,
+        attributes,
+        logger,
+        number_of_managers=2,
+        create=False)
+
+    # manager2 - Cloudify latest - don't install plugins
+    cluster.managers[1].upload_plugins = False
+
+    cluster.create()
+
+    try:
+        manager1 = cluster.managers[0]
+        manager2 = cluster.managers[1]
+
+        ha_helper.delete_active_profile()
+        manager1.use()
+        cfy.cluster.start(timeout=600,
+                          cluster_host_ip=manager1.private_ip_address,
+                          cluster_node_name=manager1.ip_address)
+
+        manager2.use()
+        cfy.cluster.join(manager1.ip_address,
+                         timeout=600,
+                         cluster_host_ip=manager2.private_ip_address,
+                         cluster_node_name=manager2.ip_address)
+        cfy.cluster.nodes.list()
+
+        yield cluster
+
+    finally:
+        cluster.destroy()
+
+
+def test_nonempty_manager_join_cluster_negative(cfy,
+                                                attributes, ssh_key,
+                                                logger, tmpdir, module_tmpdir):
+    logger.info('Creating HA cluster of 2 managers')
+    cluster = CloudifyCluster.create_image_based(
+        cfy,
+        ssh_key,
+        module_tmpdir,
+        attributes,
+        logger,
+        number_of_managers=2,
+        create=False)
+
+    # manager2 - Cloudify latest - don't install plugins
+    cluster.managers[1].upload_plugins = False
+
+    cluster.create()
+
+    try:
+        manager1 = cluster.managers[0]
+        manager2 = cluster.managers[1]
+
+        ha_helper.delete_active_profile()
+        manager1.use()
+        cfy.cluster.start(timeout=600,
+                          cluster_host_ip=manager1.private_ip_address,
+                          cluster_node_name=manager1.ip_address)
+
+        cfy.cluster.nodes.list()
+
+        ha_helper.delete_active_profile()
+        manager2.use()
+        hello_world = HelloWorldExample(
+            cfy, manager2, attributes, ssh_key, logger, tmpdir)
+        hello_world.blueprint_file = 'openstack-blueprint.yaml'
+        hello_world.inputs.update({
+            'agent_user': attributes.centos7_username,
+            'image': attributes.centos7_image_name,
+        })
+
+        hello_world.upload_blueprint()
+
+        logger.info('Joining HA cluster from a non-empty manager')
+        with pytest.raises(Exception):
+            cfy.cluster.join(manager1.ip_address,
+                             timeout=600,
+                             cluster_host_ip=manager2.private_ip_address,
+                             cluster_node_name=manager2.ip_address)
+
+    finally:
+        cluster.destroy()
+
+
+def test_remove_from_cluster_and_use_negative(cfy,
+                                              cluster, logger):
+    manager1 = cluster.managers[0]
+    manager2 = cluster.managers[1]
+
+    ha_helper.delete_active_profile()
+    manager1.use()
+    logger.info('Removing the standby manager %s from the HA cluster',
+                manager2.ip_address)
+    cfy.cluster.nodes.remove(manager2.ip_address)
+    cfy.cluster.nodes.list()
+
+    logger.info('Trying to use a manager previously removed'
+                ' from HA cluster')
+    with pytest.raises(Exception):
+        ha_helper.delete_active_profile()
+        manager2.use()
+        cfy('--version')
+
+
+def test_remove_from_cluster_and_rejoin_negative(cfy,
+                                                 cluster, logger):
+    manager1 = cluster.managers[0]
+    manager2 = cluster.managers[1]
+
+    ha_helper.delete_active_profile()
+    manager1.use()
+    logger.info('Removing the standby manager %s from the HA cluster',
+                manager2.ip_address)
+    cfy.cluster.nodes.remove(manager2.ip_address)
+    cfy.cluster.nodes.list()
+
+    logger.info('Trying to rejoin HA cluster with a manager previously'
+                ' removed from cluster')
+    with pytest.raises(Exception) as exinfo:
+        cfy.cluster.join(manager1.ip_address,
+                         timeout=600,
+                         cluster_host_ip=manager2.private_ip_address,
+                         cluster_node_name=manager2.ip_address)
+    assert 'is already part of a Cloudify Manager cluster' \
+           not in str(exinfo.value)
+
+
+def test_manager_already_in_cluster_join_cluster_negative(cfy,
+                                                          cluster, logger):
+    manager1 = cluster.managers[0]
+    manager2 = cluster.managers[1]
+
+    ha_helper.set_active(manager2, cfy, logger)
+    ha_helper.delete_active_profile()
+    manager2.use()
+    logger.info('Joining HA cluster with the manager %s that is already'
+                ' a part of the cluster', manager2.ip_address)
+    with pytest.raises(Exception):
+        cfy.cluster.join(manager1.ip_address,
+                         timeout=600,
+                         cluster_host_ip=manager2.private_ip_address,
+                         cluster_node_name=manager2.ip_address)
diff --git a/cosmo_tester/test_suites/snapshots/snapshot_upgrade_test.py b/cosmo_tester/test_suites/snapshots/snapshot_upgrade_test.py
index 5b042949..40993f90 100644
--- a/cosmo_tester/test_suites/snapshots/snapshot_upgrade_test.py
+++ b/cosmo_tester/test_suites/snapshots/snapshot_upgrade_test.py
@@ -24,15 +24,13 @@
     CloudifyCluster,
     MANAGERS,
 )
-from cosmo_tester.framework.util import (
-    create_rest_client,
-    assert_snapshot_created,
-)
+from cosmo_tester.framework.util import assert_snapshot_created
 
 # CFY-6912
 from cloudify_cli.commands.executions import (
     _get_deployment_environment_creation_execution,
     )
+from cloudify_cli.constants import CLOUDIFY_TENANT_HEADER
 
 
 HELLO_WORLD_URL = 'https://github.com/cloudify-cosmo/cloudify-hello-world-example/archive/4.0.zip'  # noqa
@@ -59,8 +57,8 @@ def cluster(request, cfy, ssh_key, module_tmpdir, attributes, logger):
     if request.param.startswith('3'):
         # Install dev tools & python headers
         with cluster.managers[1].ssh() as fabric_ssh:
-            fabric_ssh.sudo('yum -y groupinstall "Development Tools"')
-            fabric_ssh.sudo('yum -y install python-devel')
+            fabric_ssh.sudo('yum -y -q groupinstall "Development Tools"')
+            fabric_ssh.sudo('yum -y -q install python-devel')
 
     yield cluster
 
@@ -100,24 +98,34 @@ def _hello_world_example(cluster, attributes, logger, tmpdir):
 
 def test_restore_snapshot_and_agents_upgrade(
         cfy, cluster, attributes, logger, tmpdir):
-    manager1 = cluster.managers[0]
-    manager2 = cluster.managers[1]
+    manager0 = cluster.managers[0]
+    manager1 = cluster.managers[1]
 
     snapshot_id = str(uuid.uuid4())
 
-    logger.info('Creating snapshot on manager1..')
-    manager1.client.snapshots.create(snapshot_id, False, False, False)
-    assert_snapshot_created(manager1, snapshot_id, attributes)
+    logger.info('Creating snapshot on old manager..')
+    manager0.client.snapshots.create(snapshot_id, False, False, False)
+    assert_snapshot_created(manager0, snapshot_id, attributes)
 
     local_snapshot_path = str(tmpdir / 'snapshot.zip')
 
     logger.info('Downloading snapshot from old manager..')
-    manager1.client.snapshots.list()
-    manager1.client.snapshots.download(snapshot_id, local_snapshot_path)
+    manager0.client.snapshots.list()
+    manager0.client.snapshots.download(snapshot_id, local_snapshot_path)
+
+    manager1.use()
+    if '3.4' in manager0.branch_name:
+        # When working with a 3.x snapshot, we need to create a new tenant
+        # into which we'll restore the snapshot
+        tenant_name = manager0.restore_tenant_name
+        manager1.client.tenants.create(tenant_name)
+
+        # Update the tenant in the manager's client and CLI
+        manager1.client._client.headers[CLOUDIFY_TENANT_HEADER] = tenant_name
+        cfy.profiles.set(['-t', tenant_name])
 
-    manager2.use()
     logger.info('Uploading snapshot to latest manager..')
-    snapshot = manager2.client.snapshots.upload(local_snapshot_path,
+    snapshot = manager1.client.snapshots.upload(local_snapshot_path,
                                                 snapshot_id)
     logger.info('Uploaded snapshot:%s%s',
                 os.linesep,
@@ -126,10 +134,7 @@ def test_restore_snapshot_and_agents_upgrade(
     cfy.snapshots.list()
 
     logger.info('Restoring snapshot on latest manager..')
-    restore_execution = manager2.client.snapshots.restore(
-        snapshot_id,
-        tenant_name=manager1.restore_tenant_name,
-        )
+    restore_execution = manager1.client.snapshots.restore(snapshot_id)
     logger.info('Snapshot restore execution:%s%s',
                 os.linesep,
                 json.dumps(restore_execution, indent=2))
@@ -137,32 +142,23 @@ def test_restore_snapshot_and_agents_upgrade(
     cfy.executions.list(['--include-system-workflows'])
 
     restore_execution = wait_for_execution(
-        manager2.client,
+        manager1.client,
         restore_execution,
         logger)
     assert restore_execution.status == 'terminated'
 
     cfy.executions.list(['--include-system-workflows'])
 
-    manager2.use(tenant=manager1.restore_tenant_name)
-    client = create_rest_client(
-        manager2.ip_address,
-        username=cluster._attributes.cloudify_username,
-        password=cluster._attributes.cloudify_password,
-        tenant=manager1.tenant_name,
-        api_version=manager2.api_version,
-        )
-
     cfy.deployments.list()
-    deployments = client.deployments.list()
+    deployments = manager1.client.deployments.list()
     assert 1 == len(deployments)
 
     logger.info('Upgrading agents..')
     cfy.agents.install()
 
     logger.info('Deleting original {version} manager..'.format(
-        version=manager1.branch_name))
-    manager1.delete()
+        version=manager0.branch_name))
+    manager0.delete()
 
     logger.info('Uninstalling deployment from latest manager..')
     cfy.executions.start.uninstall(['-d', deployment_id])
diff --git a/suites/jenkins-slave-blueprint/openstack-blueprint.yaml b/suites/jenkins-slave-blueprint/openstack-blueprint.yaml
index 26bc5dec..5bd68699 100644
--- a/suites/jenkins-slave-blueprint/openstack-blueprint.yaml
+++ b/suites/jenkins-slave-blueprint/openstack-blueprint.yaml
@@ -2,7 +2,7 @@ tosca_definitions_version: cloudify_dsl_1_2
 
 imports:
   - http://www.getcloudify.org/spec/cloudify/4.1/types.yaml
-  - http://www.getcloudify.org/spec/fabric-plugin/1.4.2/plugin.yaml
+  - http://www.getcloudify.org/spec/fabric-plugin/1.5/plugin.yaml
   - http://www.getcloudify.org/spec/openstack-plugin/2.0.1/plugin.yaml
   - types/jenkins-types.yaml
 
diff --git a/suites/jenkins-slave-blueprint/singlehost-blueprint.yaml b/suites/jenkins-slave-blueprint/singlehost-blueprint.yaml
index 6186501b..ba8afa59 100644
--- a/suites/jenkins-slave-blueprint/singlehost-blueprint.yaml
+++ b/suites/jenkins-slave-blueprint/singlehost-blueprint.yaml
@@ -2,7 +2,7 @@ tosca_definitions_version: cloudify_dsl_1_3
 
 imports:
   - http://www.getcloudify.org/spec/cloudify/4.1/types.yaml
-  - http://www.getcloudify.org/spec/fabric-plugin/1.4.2/plugin.yaml
+  - http://www.getcloudify.org/spec/fabric-plugin/1.5/plugin.yaml
   - types/jenkins-types.yaml
 
 inputs:
diff --git a/test-requirements.txt b/test-requirements.txt
index 69ee4686..631fc606 100644
--- a/test-requirements.txt
+++ b/test-requirements.txt
@@ -1,6 +1,6 @@
 git+https://github.com/cloudify-cosmo/cloudify-dsl-parser@4.1#egg=cloudify-dsl-parser==4.1
 git+https://github.com/cloudify-cosmo/cloudify-rest-client@4.1#egg=cloudify-rest-client==4.1
 git+https://github.com/cloudify-cosmo/cloudify-plugins-common@4.1#egg=cloudify-plugins-common==4.1
-git+https://github.com/cloudify-cosmo/cloudify-script-plugin@master#egg=cloudify-script-plugin==1.4
+git+https://github.com/cloudify-cosmo/cloudify-script-plugin@master#egg=cloudify-script-plugin==1.5
 git+https://github.com/cloudify-cosmo/cloudify-cli@4.1#egg=cloudify==4.1
 mock
