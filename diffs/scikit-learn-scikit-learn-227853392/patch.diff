diff --git a/.codecov.yml b/.codecov.yml
new file mode 100644
index 000000000000..db2472009c60
--- /dev/null
+++ b/.codecov.yml
@@ -0,0 +1 @@
+comment: off
diff --git a/.travis.yml b/.travis.yml
index 46364c544f03..a565efd549fb 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -12,11 +12,10 @@ cache:
 addons:
   apt:
     packages:
+      # these only required by the DISTRIB="ubuntu" builds:
+      - python-scipy
       - libatlas3gf-base
       - libatlas-dev
-      # only required by the DISTRIB="ubuntu" build:
-      - python-scipy
-
 env:
   global:
     # Directory where tests are run from
@@ -58,15 +57,8 @@ matrix:
     # This environment tests scikit-learn against numpy and scipy master
     # installed from their CI wheels in a virtualenv with the Python
     # interpreter provided by travis.
-    # Note: libatlas3gf-base is not allowed yet so we need 'sudo':
-    # https://github.com/travis-ci/apt-package-whitelist/issues/2407
-    # Once libatlas3gf-base is on the whitelist it will be possible to replace
-    # the before_install step with and addons/apt/packages declaration.
     -  python: 3.5
        env: DISTRIB="scipy-dev-wheels"
-       sudo: True
-       before_install: sudo apt-get install -yqq libatlas3gf-base libatlas-dev
-
 
 install: source build_tools/travis/install.sh
 script: bash build_tools/travis/test_script.sh
diff --git a/CONTRIBUTING.md b/CONTRIBUTING.md
index d5d99c00b8f5..f2e219861674 100644
--- a/CONTRIBUTING.md
+++ b/CONTRIBUTING.md
@@ -4,10 +4,10 @@ Contributing to scikit-learn
 
 **Note: This document is a 'getting started' summary for contributing code,
 documentation, testing, and filing issues.** Visit the [**Contributing
-page**](http://scikit-learn.org/stable/developers/index.html)
+page**](http://scikit-learn.org/stable/developers/contributing.html)
 for the full contributor's guide. Please read it carefully to help make
 the code review process go as smoothly as possible and maximize the
-likelihood of your contribution being merged.**
+likelihood of your contribution being merged.
 
 How to contribute
 -----------------
@@ -18,7 +18,8 @@ GitHub, clone, and develop on a branch. Steps:
 
 1. Fork the [project repository](https://github.com/scikit-learn/scikit-learn)
    by clicking on the 'Fork' button near the top right of the page. This creates
-   a copy of the code under your GitHub user account.
+   a copy of the code under your GitHub user account. For more details on
+   how to fork a repository see [this guide](https://help.github.com/articles/fork-a-repo/).
 
 2. Clone your fork of the scikit-learn repo from your GitHub account to your local disk:
 
@@ -48,9 +49,8 @@ GitHub, clone, and develop on a branch. Steps:
    $ git push -u origin my-feature
    ```
 
-5. Go to the GitHub web page of your fork of the scikit-learn repo.
-Click the 'Pull request' button to send your changes to the project's maintainers for
-review. This will send an email to the committers.
+5. Follow [these instructions](https://help.github.com/articles/creating-a-pull-request-from-a-fork)
+to create a pull request from your fork. This will send an email to the committers.
 
 (If any of the above seems like magic to you, please look up the
 [Git documentation](https://git-scm.com/documentation) on the web, or ask a friend or another contributor for help.)
diff --git a/ISSUE_TEMPLATE.md b/ISSUE_TEMPLATE.md
index 9d408270edec..70e9e84d48d2 100644
--- a/ISSUE_TEMPLATE.md
+++ b/ISSUE_TEMPLATE.md
@@ -13,7 +13,7 @@ For more information, see User Questions: http://scikit-learn.org/stable/support
 #### Steps/Code to Reproduce
 <!--
 Example:
-```
+```python
 from sklearn.feature_extraction.text import CountVectorizer
 from sklearn.decomposition import LatentDirichletAllocation
 
diff --git a/README.rst b/README.rst
index be5e969cfc8c..c1ed17033b3d 100644
--- a/README.rst
+++ b/README.rst
@@ -53,6 +53,8 @@ scikit-learn requires:
 - NumPy (>= 1.6.1)
 - SciPy (>= 0.9)
 
+For running the examples Matplotlib >= 1.1.1 is required.
+
 scikit-learn also uses CBLAS, the C interface to the Basic Linear Algebra
 Subprograms library. scikit-learn comes with a reference implementation, but
 the system CBLAS will be detected by the build system and used if present.
diff --git a/benchmarks/bench_isolation_forest.py b/benchmarks/bench_isolation_forest.py
index 322f2ad1b2c5..81cb4b4e52ba 100644
--- a/benchmarks/bench_isolation_forest.py
+++ b/benchmarks/bench_isolation_forest.py
@@ -2,32 +2,51 @@
 ==========================================
 IsolationForest benchmark
 ==========================================
-
 A test of IsolationForest on classical anomaly detection datasets.
-
 """
-print(__doc__)
 
 from time import time
 import numpy as np
 import matplotlib.pyplot as plt
+
 from sklearn.ensemble import IsolationForest
 from sklearn.metrics import roc_curve, auc
 from sklearn.datasets import fetch_kddcup99, fetch_covtype, fetch_mldata
-from sklearn.preprocessing import LabelBinarizer
+from sklearn.preprocessing import MultiLabelBinarizer
 from sklearn.utils import shuffle as sh
 
-np.random.seed(1)
+print(__doc__)
 
-datasets = ['http', 'smtp', 'SA', 'SF', 'shuttle', 'forestcover']
 
+def print_outlier_ratio(y):
+    """
+    Helper function to show the distinct value count of element in the target.
+    Useful indicator for the datasets used in bench_isolation_forest.py.
+    """
+    uniq, cnt = np.unique(y, return_counts=True)
+    print("----- Target count values: ")
+    for u, c in zip(uniq, cnt):
+        print("------ %s -> %d occurences" % (str(u), c))
+    print("----- Outlier ratio: %.5f" % (np.min(cnt) / len(y)))
+
+
+np.random.seed(1)
 fig_roc, ax_roc = plt.subplots(1, 1, figsize=(8, 5))
 
+# Set this to true for plotting score histograms for each dataset:
+with_decision_function_histograms = False
 
+# Removed the shuttle dataset because as of 2017-03-23 mldata.org is down:
+# datasets = ['http', 'smtp', 'SA', 'SF', 'shuttle', 'forestcover']
+datasets = ['http', 'smtp', 'SA', 'SF', 'forestcover']
+
+# Loop over all datasets for fitting and scoring the estimator:
 for dat in datasets:
-    # loading and vectorization
-    print('loading data')
-    if dat in ['http', 'smtp', 'SA', 'SF']:
+
+    # Loading and vectorizing the data:
+    print('====== %s ======' % dat)
+    print('--- Fetching data...')
+    if dat in ['http', 'smtp', 'SF', 'SA']:
         dataset = fetch_kddcup99(subset=dat, shuffle=True, percent10=True)
         X = dataset.data
         y = dataset.target
@@ -43,6 +62,7 @@
         X = X[s, :]
         y = y[s]
         y = (y != 1).astype(int)
+        print('----- ')
 
     if dat == 'forestcover':
         dataset = fetch_covtype(shuffle=True)
@@ -54,29 +74,29 @@
         X = X[s, :]
         y = y[s]
         y = (y != 2).astype(int)
+        print_outlier_ratio(y)
 
-    print('vectorizing data')
+    print('--- Vectorizing data...')
 
     if dat == 'SF':
-        lb = LabelBinarizer()
-        lb.fit(X[:, 1])
-        x1 = lb.transform(X[:, 1])
+        lb = MultiLabelBinarizer()
+        x1 = lb.fit_transform(X[:, 1])
         X = np.c_[X[:, :1], x1, X[:, 2:]]
-        y = (y != 'normal.').astype(int)
+        y = (y != b'normal.').astype(int)
+        print_outlier_ratio(y)
 
     if dat == 'SA':
-        lb = LabelBinarizer()
-        lb.fit(X[:, 1])
-        x1 = lb.transform(X[:, 1])
-        lb.fit(X[:, 2])
-        x2 = lb.transform(X[:, 2])
-        lb.fit(X[:, 3])
-        x3 = lb.transform(X[:, 3])
+        lb = MultiLabelBinarizer()
+        x1 = lb.fit_transform(X[:, 1])
+        x2 = lb.fit_transform(X[:, 2])
+        x3 = lb.fit_transform(X[:, 3])
         X = np.c_[X[:, :1], x1, x2, x3, X[:, 4:]]
-        y = (y != 'normal.').astype(int)
+        y = (y != b'normal.').astype(int)
+        print_outlier_ratio(y)
 
-    if dat == 'http' or dat == 'smtp':
-        y = (y != 'normal.').astype(int)
+    if dat in ('http', 'smtp'):
+        y = (y != b'normal.').astype(int)
+        print_outlier_ratio(y)
 
     n_samples, n_features = X.shape
     n_samples_train = n_samples // 2
@@ -87,34 +107,34 @@
     y_train = y[:n_samples_train]
     y_test = y[n_samples_train:]
 
-    print('IsolationForest processing...')
+    print('--- Fitting the IsolationForest estimator...')
     model = IsolationForest(n_jobs=-1)
     tstart = time()
     model.fit(X_train)
     fit_time = time() - tstart
     tstart = time()
 
-    scoring = - model.decision_function(X_test)  # the lower, the more normal
-
-    # Show score histograms
-    fig, ax = plt.subplots(3, sharex=True, sharey=True)
-    bins = np.linspace(-0.5, 0.5, 200)
-    ax[0].hist(scoring, bins, color='black')
-    ax[0].set_title('decision function for %s dataset' % dat)
-    ax[0].legend(loc="lower right")
-    ax[1].hist(scoring[y_test == 0], bins, color='b',
-               label='normal data')
-    ax[1].legend(loc="lower right")
-    ax[2].hist(scoring[y_test == 1], bins, color='r',
-               label='outliers')
-    ax[2].legend(loc="lower right")
+    scoring = - model.decision_function(X_test)  # the lower, the more abnormal
+
+    print("--- Preparing the plot elements...")
+    if with_decision_function_histograms:
+        fig, ax = plt.subplots(3, sharex=True, sharey=True)
+        bins = np.linspace(-0.5, 0.5, 200)
+        ax[0].hist(scoring, bins, color='black')
+        ax[0].set_title('Decision function for %s dataset' % dat)
+        ax[1].hist(scoring[y_test == 0], bins, color='b', label='normal data')
+        ax[1].legend(loc="lower right")
+        ax[2].hist(scoring[y_test == 1], bins, color='r', label='outliers')
+        ax[2].legend(loc="lower right")
 
     # Show ROC Curves
     predict_time = time() - tstart
     fpr, tpr, thresholds = roc_curve(y_test, scoring)
-    AUC = auc(fpr, tpr)
-    label = ('%s (area: %0.3f, train-time: %0.2fs, '
-             'test-time: %0.2fs)' % (dat, AUC, fit_time, predict_time))
+    auc_score = auc(fpr, tpr)
+    label = ('%s (AUC: %0.3f, train_time= %0.2fs, '
+             'test_time= %0.2fs)' % (dat, auc_score, fit_time, predict_time))
+    # Print AUC score and train/test time:
+    print(label)
     ax_roc.plot(fpr, tpr, lw=1, label=label)
 
 
diff --git a/benchmarks/bench_mnist.py b/benchmarks/bench_mnist.py
index 01e0e1bf9f4e..f84eed5f9479 100644
--- a/benchmarks/bench_mnist.py
+++ b/benchmarks/bench_mnist.py
@@ -91,14 +91,17 @@ def load_data(dtype=np.float32, order='F'):
         Nystroem(gamma=0.015, n_components=1000), LinearSVC(C=100)),
     'SampledRBF-SVM': make_pipeline(
         RBFSampler(gamma=0.015, n_components=1000), LinearSVC(C=100)),
-    'LinearRegression-SAG': LogisticRegression(solver='sag', tol=1e-1, C=1e4),
+    'LogisticRegression-SAG': LogisticRegression(solver='sag', tol=1e-1,
+                                                 C=1e4),
+    'LogisticRegression-SAGA': LogisticRegression(solver='saga', tol=1e-1,
+                                                  C=1e4),
     'MultilayerPerceptron': MLPClassifier(
         hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,
-        algorithm='sgd', learning_rate_init=0.2, momentum=0.9, verbose=1,
+        solver='sgd', learning_rate_init=0.2, momentum=0.9, verbose=1,
         tol=1e-4, random_state=1),
     'MLP-adam': MLPClassifier(
         hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,
-        algorithm='adam', learning_rate_init=0.001, verbose=1,
+        solver='adam', learning_rate_init=0.001, verbose=1,
         tol=1e-4, random_state=1)
 }
 
diff --git a/benchmarks/bench_saga.py b/benchmarks/bench_saga.py
new file mode 100644
index 000000000000..10aca379123a
--- /dev/null
+++ b/benchmarks/bench_saga.py
@@ -0,0 +1,244 @@
+"""Author: Arthur Mensch
+
+Benchmarks of sklearn SAGA vs lightning SAGA vs Liblinear. Shows the gain
+in using multinomial logistic regression in term of learning time.
+"""
+import json
+import time
+from os.path import expanduser
+
+import matplotlib.pyplot as plt
+import numpy as np
+
+from sklearn.datasets import fetch_rcv1, load_iris, load_digits, \
+    fetch_20newsgroups_vectorized
+from sklearn.externals.joblib import delayed, Parallel, Memory
+from sklearn.linear_model import LogisticRegression
+from sklearn.metrics import log_loss
+from sklearn.model_selection import train_test_split
+from sklearn.preprocessing import LabelBinarizer, LabelEncoder
+from sklearn.utils.extmath import safe_sparse_dot, softmax
+
+
+def fit_single(solver, X, y, penalty='l2', single_target=True, C=1,
+               max_iter=10, skip_slow=False):
+    if skip_slow and solver == 'lightning' and penalty == 'l1':
+        print('skip_slowping l1 logistic regression with solver lightning.')
+        return
+
+    print('Solving %s logistic regression with penalty %s, solver %s.'
+          % ('binary' if single_target else 'multinomial',
+             penalty, solver))
+
+    if solver == 'lightning':
+        from lightning.classification import SAGAClassifier
+
+    if single_target or solver not in ['sag', 'saga']:
+        multi_class = 'ovr'
+    else:
+        multi_class = 'multinomial'
+
+    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42,
+                                                        stratify=y)
+    n_samples = X_train.shape[0]
+    n_classes = np.unique(y_train).shape[0]
+    test_scores = [1]
+    train_scores = [1]
+    accuracies = [1 / n_classes]
+    times = [0]
+
+    if penalty == 'l2':
+        alpha = 1. / (C * n_samples)
+        beta = 0
+        lightning_penalty = None
+    else:
+        alpha = 0.
+        beta = 1. / (C * n_samples)
+        lightning_penalty = 'l1'
+
+    for this_max_iter in range(1, max_iter + 1, 2):
+        print('[%s, %s, %s] Max iter: %s' %
+              ('binary' if single_target else 'multinomial',
+               penalty, solver, this_max_iter))
+        if solver == 'lightning':
+            lr = SAGAClassifier(loss='log', alpha=alpha, beta=beta,
+                                penalty=lightning_penalty,
+                                tol=-1, max_iter=this_max_iter)
+        else:
+            lr = LogisticRegression(solver=solver,
+                                    multi_class=multi_class,
+                                    C=C,
+                                    penalty=penalty,
+                                    fit_intercept=False, tol=1e-24,
+                                    max_iter=this_max_iter,
+                                    random_state=42,
+                                    )
+        t0 = time.clock()
+        lr.fit(X_train, y_train)
+        train_time = time.clock() - t0
+
+        scores = []
+        for (X, y) in [(X_train, y_train), (X_test, y_test)]:
+            try:
+                y_pred = lr.predict_proba(X)
+            except NotImplementedError:
+                # Lightning predict_proba is not implemented for n_classes > 2
+                y_pred = _predict_proba(lr, X)
+            score = log_loss(y, y_pred, normalize=False) / n_samples
+            score += (0.5 * alpha * np.sum(lr.coef_ ** 2) +
+                      beta * np.sum(np.abs(lr.coef_)))
+            scores.append(score)
+        train_score, test_score = tuple(scores)
+
+        y_pred = lr.predict(X_test)
+        accuracy = np.sum(y_pred == y_test) / y_test.shape[0]
+        test_scores.append(test_score)
+        train_scores.append(train_score)
+        accuracies.append(accuracy)
+        times.append(train_time)
+    return lr, times, train_scores, test_scores, accuracies
+
+
+def _predict_proba(lr, X):
+    pred = safe_sparse_dot(X, lr.coef_.T)
+    if hasattr(lr, "intercept_"):
+        pred += lr.intercept_
+    return softmax(pred)
+
+
+def exp(solvers, penalties, single_target, n_samples=30000, max_iter=20,
+        dataset='rcv1', n_jobs=1, skip_slow=False):
+    mem = Memory(cachedir=expanduser('~/cache'), verbose=0)
+
+    if dataset == 'rcv1':
+        rcv1 = fetch_rcv1()
+
+        lbin = LabelBinarizer()
+        lbin.fit(rcv1.target_names)
+
+        X = rcv1.data
+        y = rcv1.target
+        y = lbin.inverse_transform(y)
+        le = LabelEncoder()
+        y = le.fit_transform(y)
+        if single_target:
+            y_n = y.copy()
+            y_n[y > 16] = 1
+            y_n[y <= 16] = 0
+            y = y_n
+
+    elif dataset == 'digits':
+        digits = load_digits()
+        X, y = digits.data, digits.target
+        if single_target:
+            y_n = y.copy()
+            y_n[y < 5] = 1
+            y_n[y >= 5] = 0
+            y = y_n
+    elif dataset == 'iris':
+        iris = load_iris()
+        X, y = iris.data, iris.target
+    elif dataset == '20newspaper':
+        ng = fetch_20newsgroups_vectorized()
+        X = ng.data
+        y = ng.target
+        if single_target:
+            y_n = y.copy()
+            y_n[y > 4] = 1
+            y_n[y <= 16] = 0
+            y = y_n
+
+    X = X[:n_samples]
+    y = y[:n_samples]
+
+    cached_fit = mem.cache(fit_single)
+    out = Parallel(n_jobs=n_jobs, mmap_mode=None)(
+        delayed(cached_fit)(solver, X, y,
+                            penalty=penalty, single_target=single_target,
+                            C=1, max_iter=max_iter, skip_slow=skip_slow)
+        for solver in solvers
+        for penalty in penalties)
+
+    res = []
+    idx = 0
+    for solver in solvers:
+        for penalty in penalties:
+            if not (skip_slow and solver == 'lightning' and penalty == 'l1'):
+                lr, times, train_scores, test_scores, accuracies = out[idx]
+                this_res = dict(solver=solver, penalty=penalty,
+                                single_target=single_target,
+                                times=times, train_scores=train_scores,
+                                test_scores=test_scores,
+                                accuracies=accuracies)
+                res.append(this_res)
+            idx += 1
+
+    with open('bench_saga.json', 'w+') as f:
+        json.dump(res, f)
+
+
+def plot():
+    import pandas as pd
+    with open('bench_saga.json', 'r') as f:
+        f = json.load(f)
+    res = pd.DataFrame(f)
+    res.set_index(['single_target', 'penalty'], inplace=True)
+
+    grouped = res.groupby(level=['single_target', 'penalty'])
+
+    colors = {'saga': 'blue', 'liblinear': 'orange', 'lightning': 'green'}
+
+    for idx, group in grouped:
+        single_target, penalty = idx
+        fig = plt.figure(figsize=(12, 4))
+        ax = fig.add_subplot(131)
+
+        train_scores = group['train_scores'].values
+        ref = np.min(np.concatenate(train_scores)) * 0.999
+
+        for scores, times, solver in zip(group['train_scores'], group['times'],
+                                         group['solver']):
+            scores = scores / ref - 1
+            ax.plot(times, scores, label=solver, color=colors[solver])
+        ax.set_xlabel('Time (s)')
+        ax.set_ylabel('Training objective (relative to min)')
+        ax.set_yscale('log')
+
+        ax = fig.add_subplot(132)
+
+        test_scores = group['test_scores'].values
+        ref = np.min(np.concatenate(test_scores)) * 0.999
+
+        for scores, times, solver in zip(group['test_scores'], group['times'],
+                                         group['solver']):
+            scores = scores / ref - 1
+            ax.plot(times, scores, label=solver, color=colors[solver])
+        ax.set_xlabel('Time (s)')
+        ax.set_ylabel('Test objective (relative to min)')
+        ax.set_yscale('log')
+
+        ax = fig.add_subplot(133)
+
+        for accuracy, times, solver in zip(group['accuracies'], group['times'],
+                                           group['solver']):
+            ax.plot(times, accuracy, label=solver, color=colors[solver])
+        ax.set_xlabel('Time (s)')
+        ax.set_ylabel('Test accuracy')
+        ax.legend()
+        name = 'single_target' if single_target else 'multi_target'
+        name += '_%s' % penalty
+        plt.suptitle(name)
+        name += '.png'
+        fig.tight_layout()
+        fig.subplots_adjust(top=0.9)
+        plt.savefig(name)
+        plt.close(fig)
+
+
+if __name__ == '__main__':
+    solvers = ['saga', 'liblinear', 'lightning']
+    penalties = ['l1', 'l2']
+    single_target = True
+    exp(solvers, penalties, single_target, n_samples=None, n_jobs=1,
+        dataset='20newspaper', max_iter=20)
+    plot()
diff --git a/build_tools/travis/install.sh b/build_tools/travis/install.sh
index fa6380e0451a..54de4eca74f9 100755
--- a/build_tools/travis/install.sh
+++ b/build_tools/travis/install.sh
@@ -86,11 +86,9 @@ elif [[ "$DISTRIB" == "scipy-dev-wheels" ]]; then
     source ~/testvenv/bin/activate
     pip install --upgrade pip setuptools
 
-    # We use the default Python virtualenv provided by travis
-    echo "Installing numpy master wheel"
-    pip install --pre --upgrade --no-index --timeout=60 \
-        --trusted-host travis-dev-wheels.scipy.org \
-        -f https://travis-dev-wheels.scipy.org/ numpy scipy
+    echo "Installing numpy and scipy master wheels"
+    dev_url=https://7933911d6844c6c53a7d-47bd50c35cd79bd838daf386af554a83.ssl.cf2.rackcdn.com
+    pip install --pre --upgrade --timeout=60 -f $dev_url numpy scipy
     pip install nose nose-timer cython
 fi
 
diff --git a/build_tools/travis/test_script.sh b/build_tools/travis/test_script.sh
index 58f49f4cbb70..df3add34f0c1 100755
--- a/build_tools/travis/test_script.sh
+++ b/build_tools/travis/test_script.sh
@@ -26,7 +26,7 @@ run_tests() {
     else
         TEST_CMD="nosetests --with-timer --timer-top-n 20"
     fi
-    # Get into a temp directory to run test from the installed scikit learn and
+    # Get into a temp directory to run test from the installed scikit-learn and
     # check if we do not leave artifacts
     mkdir -p $TEST_DIR
     # We need the setup.cfg for the nose settings
diff --git a/doc/developers/contributing.rst b/doc/developers/contributing.rst
index d83650345b42..0fe3513bd5ed 100644
--- a/doc/developers/contributing.rst
+++ b/doc/developers/contributing.rst
@@ -111,7 +111,8 @@ then submit a "pull request" (PR):
  2. Fork the `project repository
     <https://github.com/scikit-learn/scikit-learn>`__: click on the 'Fork'
     button near the top of the page. This creates a copy of the code under your
-    account on the GitHub server.
+    account on the GitHub server. For more details on how to fork a
+    repository see `this guide <https://help.github.com/articles/fork-a-repo/>`_.
 
  3. Clone this copy to your local disk::
 
@@ -133,10 +134,11 @@ then submit a "pull request" (PR):
 
         $ git push -u origin my-feature
 
-Finally, go to the web page of the your fork of the scikit-learn repo,
-and click 'Pull request' to send your changes to the maintainers for review.
-You may want to consider sending an email to the mailing list for more
-visibility.
+Finally, follow `these
+<https://help.github.com/articles/creating-a-pull-request-from-a-fork>`_
+instructions to create a pull request from your fork. This will send an
+email to the committers. You may want to consider sending an email to the
+mailing list for more visibility.
 
 .. note::
 
@@ -153,12 +155,22 @@ If any of the above seems like magic to you, then look up the `Git documentation
 <http://docs.scipy.org/doc/numpy/dev/gitwash/development_workflow.html>`_ on the
 web.
 
-In particular, if some conflicts arise between your branch and the master
-branch, you will need to `rebase your branch on master
-<http://docs.scipy.org/doc/numpy/dev/gitwash/development_workflow.html#rebasing-on-master>`_.
-Please avoid merging master branch into yours. If you did it anyway, you can fix
-it following `this example
-<https://github.com/scikit-learn/scikit-learn/pull/7111#issuecomment-249175383>`_.
+If some conflicts arise between your branch and the ``master`` branch, you need
+to merge ``master``. The command will be::
+
+  $ git merge master
+
+with ``master`` being synchronized with the ``upstream``.
+
+Subsequently, you need to solve the conflicts. You can refer to the `Git
+documentation related to resolving merge conflict using the command line
+<https://help.github.com/articles/resolving-a-merge-conflict-using-the-command-line/>`_.
+
+.. note::
+
+   In the past, the policy to resolve conflicts was to rebase your branch on
+   ``master``. GitHub interface deals with merging ``master`` better than in
+   the past.
 
 
 Contributing pull requests
diff --git a/doc/make.bat b/doc/make.bat
index b0411b2d55e8..fa8e7171ea7e 100644
--- a/doc/make.bat
+++ b/doc/make.bat
@@ -24,6 +24,7 @@ if "%1" == "help" (
 	echo.  changes   to make an overview over all changed/added/deprecated items
 	echo.  linkcheck to check all external links for integrity
 	echo.  doctest   to run all doctests embedded in the documentation if enabled
+	echo.  html-noplot   to make HTML files using Windows
 	goto end
 )
 
@@ -40,6 +41,12 @@ if "%1" == "html" (
 	goto end
 )
 
+if "%1" == "html-noplot" (
+	%SPHINXBUILD% -D plot_gallery=0 -b html %ALLSPHINXOPTS% %BUILDDIR%/html
+	echo.
+	echo.Build finished. The HTML pages are in %BUILDDIR%/html
+)
+
 if "%1" == "dirhtml" (
 	%SPHINXBUILD% -b dirhtml %ALLSPHINXOPTS% %BUILDDIR%/dirhtml
 	echo.
diff --git a/doc/modules/calibration.rst b/doc/modules/calibration.rst
index cd74f414fe3d..0c0af594398a 100644
--- a/doc/modules/calibration.rst
+++ b/doc/modules/calibration.rst
@@ -57,9 +57,9 @@ with different biases per method:
    ensemble away from 0. We observe this effect most strongly with random
    forests because the base-level trees trained with random forests have
    relatively high variance due to feature subseting." As a result, the
-   calibration curve shows a characteristic sigmoid shape, indicating that the
-   classifier could trust its "intuition" more and return probabilties closer
-   to 0 or 1 typically.
+   calibration curve also referred to as the reliability diagram (Wilks 1995[5]) shows a
+   characteristic sigmoid shape, indicating that the classifier could trust its
+   "intuition" more and return probabilties closer to 0 or 1 typically.
 
 .. currentmodule:: sklearn.svm
 
@@ -118,13 +118,14 @@ is evaluated with Brier score :func:`brier_score_loss`, reported in the legend
    :align: center
 
 One can observe here that logistic regression is well calibrated as its curve is
-nearly diagonal. Linear SVC's calibration curve has a sigmoid curve, which is
-typical for an under-confident classifier. In the case of LinearSVC, this is
-caused by the margin property of the hinge loss, which lets the model focus on
-hard samples that are close to the decision boundary (the support vectors). Both
-kinds of calibration can fix this issue and yield nearly identical results.
-The next figure shows the calibration curve of Gaussian naive Bayes on
-the same data, with both kinds of calibration and also without calibration.
+nearly diagonal. Linear SVC's calibration curve or reliability diagram has a
+sigmoid curve, which is typical for an under-confident classifier. In the case of
+LinearSVC, this is caused by the margin property of the hinge loss, which lets
+the model focus on hard samples that are close to the decision boundary
+(the support vectors). Both kinds of calibration can fix this issue and yield
+nearly identical results. The next figure shows the calibration curve of
+Gaussian naive Bayes on the same data, with both kinds of calibration and also
+without calibration.
 
 .. figure:: ../auto_examples/calibration/images/sphx_glr_plot_calibration_curve_001.png
    :target: ../auto_examples/calibration/plot_calibration_curve.html
@@ -200,3 +201,7 @@ a similar decrease in log-loss.
 
     .. [4] Predicting Good Probabilities with Supervised Learning,
           A. Niculescu-Mizil & R. Caruana, ICML 2005
+
+    .. [5] On the combination of forecast probabilities for
+         consecutive precipitation periods. Wea. Forecasting, 5, 640–
+         650., Wilks, D. S., 1990a
diff --git a/doc/modules/classes.rst b/doc/modules/classes.rst
index c5417e600611..c769819fa9b1 100644
--- a/doc/modules/classes.rst
+++ b/doc/modules/classes.rst
@@ -170,6 +170,8 @@ Splitter Classes
    model_selection.LeavePGroupsOut
    model_selection.LeaveOneOut
    model_selection.LeavePOut
+   model_selection.RepeatedKFold
+   model_selection.RepeatedStratifiedKFold
    model_selection.ShuffleSplit
    model_selection.GroupShuffleSplit
    model_selection.StratifiedShuffleSplit
diff --git a/doc/modules/cross_validation.rst b/doc/modules/cross_validation.rst
index 3f7cf95e5941..4b9a36e979d4 100644
--- a/doc/modules/cross_validation.rst
+++ b/doc/modules/cross_validation.rst
@@ -263,6 +263,33 @@ Thus, one can create the training/test sets using numpy indexing::
   >>> X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]
 
 
+Repeated K-Fold
+---------------
+
+:class:`RepeatedKFold` repeats K-Fold n times. It can be used when one
+requires to run :class:`KFold` n times, producing different splits in
+each repetition.
+
+Example of 2-fold K-Fold repeated 2 times::
+
+  >>> import numpy as np
+  >>> from sklearn.model_selection import RepeatedKFold
+  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
+  >>> random_state = 12883823
+  >>> rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=random_state)
+  >>> for train, test in rkf.split(X):
+  ...     print("%s %s" % (train, test))
+  ...
+  [2 3] [0 1]
+  [0 1] [2 3]
+  [0 2] [1 3]
+  [1 3] [0 2]
+
+
+Similarly, :class:`RepeatedStratifiedKFold` repeats Stratified K-Fold n times
+with different randomization in each repetition.
+
+
 Leave One Out (LOO)
 -------------------
 
@@ -409,6 +436,10 @@ two slightly unbalanced classes::
   [0 1 3 4 5 8 9] [2 6 7]
   [0 1 2 4 5 6 7] [3 8 9]
 
+:class:`RepeatedStratifiedKFold` can be used to repeat Stratified K-Fold n times
+with different randomization in each repetition.
+
+
 Stratified Shuffle Split
 ------------------------
 
diff --git a/doc/modules/feature_extraction.rst b/doc/modules/feature_extraction.rst
index 4995177705c1..32e53f0817e6 100644
--- a/doc/modules/feature_extraction.rst
+++ b/doc/modules/feature_extraction.rst
@@ -1,4 +1,4 @@
-.. _feature_extraction:
+﻿.. _feature_extraction:
 
 ==================
 Feature extraction
@@ -42,7 +42,7 @@ is a traditional numerical feature::
   >>> measurements = [
   ...     {'city': 'Dubai', 'temperature': 33.},
   ...     {'city': 'London', 'temperature': 12.},
-  ...     {'city': 'San Fransisco', 'temperature': 18.},
+  ...     {'city': 'San Francisco', 'temperature': 18.},
   ... ]
 
   >>> from sklearn.feature_extraction import DictVectorizer
@@ -54,7 +54,7 @@ is a traditional numerical feature::
          [  0.,   0.,   1.,  18.]])
 
   >>> vec.get_feature_names()
-  ['city=Dubai', 'city=London', 'city=San Fransisco', 'temperature']
+  ['city=Dubai', 'city=London', 'city=San Francisco', 'temperature']
 
 :class:`DictVectorizer` is also a useful representation transformation
 for training sequence classifiers in Natural Language Processing models
diff --git a/doc/modules/gaussian_process.rst b/doc/modules/gaussian_process.rst
index fb408c4acd71..7fae49349f34 100644
--- a/doc/modules/gaussian_process.rst
+++ b/doc/modules/gaussian_process.rst
@@ -551,7 +551,7 @@ Only the isotropic variant where :math:`l` is a scalar is supported at the momen
 The kernel is given by:
 
 .. math::
-   k(x_i, x_j) = \left(1 + \frac{d(x_i, x_j)^2}{2\alpha l^2}\right)^\alpha
+   k(x_i, x_j) = \left(1 + \frac{d(x_i, x_j)^2}{2\alpha l^2}\right)^{-\alpha}
 
 The prior and posterior of a GP resulting from an RBF kernel are shown in
 the following figure:
@@ -569,7 +569,7 @@ It is parameterized by a length-scale parameter :math:`l>0` and a periodicity pa
 The kernel is given by:
 
 .. math::
-   k(x_i, x_j) = \text{exp}\left(-2 \text{sin}(\pi / p * d(x_i, x_j)) / l\right)^2
+   k(x_i, x_j) = \text{exp}\left(-2 \left(\text{sin}(\pi / p * d(x_i, x_j)) / l\right)^2\right)
 
 The prior and posterior of a GP resulting from an ExpSineSquared kernel are shown in
 the following figure:
diff --git a/doc/modules/grid_search.rst b/doc/modules/grid_search.rst
index d557def26351..da862cd9b269 100644
--- a/doc/modules/grid_search.rst
+++ b/doc/modules/grid_search.rst
@@ -245,7 +245,7 @@ Some models can offer an information-theoretic closed-form formula of the
 optimal estimate of the regularization parameter by computing a single
 regularization path (instead of several when using cross-validation).
 
-Here is the list of models benefitting from the Aikike Information
+Here is the list of models benefitting from the Akaike Information
 Criterion (AIC) or the Bayesian Information Criterion (BIC) for automated
 model selection:
 
diff --git a/doc/modules/linear_model.rst b/doc/modules/linear_model.rst
index b6c89f1fcba9..91c09a1cb218 100644
--- a/doc/modules/linear_model.rst
+++ b/doc/modules/linear_model.rst
@@ -721,7 +721,7 @@ optimization problem
 .. math:: \underset{w, c}{min\,} \|w\|_1 + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1) .
 
 The solvers implemented in the class :class:`LogisticRegression`
-are "liblinear", "newton-cg", "lbfgs" and "sag":
+are "liblinear", "newton-cg", "lbfgs", "sag" and "saga":
 
 The solver "liblinear" uses a coordinate descent (CD) algorithm, and relies
 on the excellent C++ `LIBLINEAR library
@@ -739,25 +739,31 @@ The "lbfgs", "sag" and "newton-cg" solvers only support L2 penalization and
 are found to converge faster for some high dimensional data. Setting
 `multi_class` to "multinomial" with these solvers learns a true multinomial
 logistic regression model [5]_, which means that its probability estimates
-should be better calibrated than the default "one-vs-rest" setting. The
-"lbfgs", "sag" and "newton-cg"" solvers cannot optimize L1-penalized models,
-therefore the "multinomial" setting does not learn sparse models.
+should be better calibrated than the default "one-vs-rest" setting.
 
-The solver "sag" uses a Stochastic Average Gradient descent [6]_. It is faster
+The "sag" solver uses a Stochastic Average Gradient descent [6]_. It is faster
 than other solvers for large datasets, when both the number of samples and the
 number of features are large.
 
+The "saga" solver [7]_ is a variant of "sag" that also supports the
+non-smooth `penalty="l1"` option. This is therefore the solver of choice
+for sparse multinomial logistic regression.
+
 In a nutshell, one may choose the solver with the following rules:
 
-=================================  =============================
+=================================  =====================================
 Case                               Solver
-=================================  =============================
-Small dataset or L1 penalty        "liblinear"
-Multinomial loss or large dataset  "lbfgs", "sag" or "newton-cg"
-Very Large dataset                 "sag"
-=================================  =============================
+=================================  =====================================
+L1 penalty                         "liblinear" or "saga"
+Multinomial loss                   "lbfgs", "sag", "saga" or "newton-cg"
+Very Large dataset (`n_samples`)   "sag" or "saga"
+=================================  =====================================
+
+The "saga" solver is often the best choice. The "liblinear" solver is
+used by default for historical reasons.
 
-For large dataset, you may also consider using :class:`SGDClassifier` with 'log' loss.
+For large dataset, you may also consider using :class:`SGDClassifier`
+with 'log' loss.
 
 .. topic:: Examples:
 
@@ -767,6 +773,10 @@ For large dataset, you may also consider using :class:`SGDClassifier` with 'log'
 
   * :ref:`sphx_glr_auto_examples_linear_model_plot_logistic_multinomial.py`
 
+  * :ref:`sphx_glr_auto_examples_linear_model_plot_sparse_logistic_regression_20newsgroups.py`
+
+  * :ref:`sphx_glr_auto_examples_linear_model_plot_sparse_logistic_regression_mnist.py`
+
 .. _liblinear_differences:
 
 .. topic:: Differences from liblinear:
@@ -788,13 +798,14 @@ For large dataset, you may also consider using :class:`SGDClassifier` with 'log'
    thus be used to perform feature selection, as detailed in
    :ref:`l1_feature_selection`.
 
-:class:`LogisticRegressionCV` implements Logistic Regression with builtin
-cross-validation to find out the optimal C parameter. "newton-cg", "sag" and
-"lbfgs" solvers are found to be faster for high-dimensional dense data, due to
-warm-starting. For the multiclass case, if `multi_class` option is set to
-"ovr", an optimal C is obtained for each class and if the `multi_class` option
-is set to "multinomial", an optimal C is obtained by minimizing the cross-
-entropy loss.
+:class:`LogisticRegressionCV` implements Logistic Regression with
+builtin cross-validation to find out the optimal C parameter.
+"newton-cg", "sag", "saga" and "lbfgs" solvers are found to be faster
+for high-dimensional dense data, due to warm-starting. For the
+multiclass case, if `multi_class` option is set to "ovr", an optimal C
+is obtained for each class and if the `multi_class` option is set to
+"multinomial", an optimal C is obtained by minimizing the cross-entropy
+loss.
 
 .. topic:: References:
 
@@ -802,6 +813,8 @@ entropy loss.
 
     .. [6] Mark Schmidt, Nicolas Le Roux, and Francis Bach: `Minimizing Finite Sums with the Stochastic Average Gradient. <https://hal.inria.fr/hal-00860051/document>`_
 
+    .. [7] Aaron Defazio, Francis Bach, Simon Lacoste-Julien: `SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives. <https://arxiv.org/abs/1407.0202>`_
+
 Stochastic Gradient Descent - SGD
 =================================
 
diff --git a/doc/modules/manifold.rst b/doc/modules/manifold.rst
index ddc82fed31b1..c8c5910136db 100644
--- a/doc/modules/manifold.rst
+++ b/doc/modules/manifold.rst
@@ -538,7 +538,14 @@ entropy of the conditional probability distribution. The perplexity of a
 :math:`k`-sided die is :math:`k`, so that :math:`k` is effectively the number of
 nearest neighbors t-SNE considers when generating the conditional probabilities.
 Larger perplexities lead to more nearest neighbors and less sensitive to small
-structure. Larger datasets tend to require larger perplexities.
+structure. Conversely a lower perplexity considers a smaller number of
+neighbors, and thus ignores more global information in favour of the
+local neighborhood. As dataset sizes get larger more points will be
+required to get a reasonable sample of the local neighborhood, and hence
+larger perplexities may be required. Similarly noisier datasets will require
+larger perplexity values to encompass enough local neighbors to see beyond
+the background noise.
+
 The maximum number of iterations is usually high enough and does not need
 any tuning. The optimization consists of two phases: the early exaggeration
 phase and the final optimization. During early exaggeration the joint
@@ -554,6 +561,10 @@ is a tradeoff between performance and accuracy. Larger angles imply that we
 can approximate larger regions by a single point,leading to better speed
 but less accurate results.
 
+`"How to Use t-SNE Effectively" <http://distill.pub/2016/misread-tsne/>`_
+provides a good discussion of the effects of the various parameters, as well
+as interactive plots to explore the effects of different parameters.
+
 Barnes-Hut t-SNE
 ----------------
 
diff --git a/doc/modules/mixture.rst b/doc/modules/mixture.rst
index e8f68879f21a..2b1e403470d5 100644
--- a/doc/modules/mixture.rst
+++ b/doc/modules/mixture.rst
@@ -188,7 +188,7 @@ more uniform weights when the type of prior is 'dirichlet_distribution' while
 this is not necessarily the case for the 'dirichlet_process' type (used by
 default).
 
-.. |plot_bgmm| image:: ../auto_examples/mixture/images/sphx_glr_plot_concentration_prior_002.png
+.. |plot_bgmm| image:: ../auto_examples/mixture/images/sphx_glr_plot_concentration_prior_001.png
    :target: ../auto_examples/mixture/plot_concentration_prior.html
    :scale: 48%
 
diff --git a/doc/modules/neighbors.rst b/doc/modules/neighbors.rst
index f9e01b12a92a..1440c49403a5 100644
--- a/doc/modules/neighbors.rst
+++ b/doc/modules/neighbors.rst
@@ -620,8 +620,8 @@ is called :math:`(r, e , p1 , p2 )`-sensitive, with :math:`r, e > 0`,
 :math:`p_1 > p_2 > 0`, if for any :math:`p, q \in S`, the following conditions
 hold (:math:`D` is the distance function):
 
-* If :math:`D(p,q) <= r` then :math:`P_H[h(p) = h(q)] >= p_1`,
-* If :math:`D(p,q) > r(1 + e)` then :math:`P_H[h(p) = h(q)] <= p_2`.
+* If :math:`D(p,q) \le r` then :math:`P_H[h(p) = h(q)] \ge p_1`,
+* If :math:`D(p,q) > r(1 + e)` then :math:`P_H[h(p) = h(q)] \le p_2`.
 
 As defined, nearby points within a distance of :math:`r` to each other are
 likely to collide with probability :math:`p_1`. In contrast, distant points
@@ -665,7 +665,7 @@ There are two phases of tree traversals used in order to answer a query to find
 the :math:`m` nearest neighbors of a point :math:`q`. First, a top-down
 traversal is performed using a binary search to identify the leaf having the
 longest prefix match (maximum depth) with :math:`q`'s label after subjecting
-:math:`q` to the same hash functions. :math:`M >> m` points (total candidates)
+:math:`q` to the same hash functions. :math:`M \gg m` points (total candidates)
 are extracted from the forest, moving up from the previously found maximum 
 depth towards the root synchronously across all trees in the bottom-up
 traversal. `M` is set to  :math:`cl` where :math:`c`, the number of candidates
diff --git a/doc/modules/neural_networks_supervised.rst b/doc/modules/neural_networks_supervised.rst
index ec708e59be1a..292ed903eeff 100644
--- a/doc/modules/neural_networks_supervised.rst
+++ b/doc/modules/neural_networks_supervised.rst
@@ -173,7 +173,7 @@ which a sample can have more than one target.
 Regularization
 ==============
 
-Both :class:`MLPRegressor` and class:`MLPClassifier` use parameter ``alpha``
+Both :class:`MLPRegressor` and :class:`MLPClassifier` use parameter ``alpha``
 for regularization (L2 regularization) term which helps in avoiding overfitting
 by penalizing weights with large magnitudes. Following plot displays varying
 decision function with value of alpha.
diff --git a/doc/modules/outlier_detection.rst b/doc/modules/outlier_detection.rst
index e7441a803ae8..1d4addcb7786 100644
--- a/doc/modules/outlier_detection.rst
+++ b/doc/modules/outlier_detection.rst
@@ -107,7 +107,7 @@ points, ignoring points outside the central mode.
 
 For instance, assuming that the inlier data are Gaussian distributed, it
 will estimate the inlier location and covariance in a robust way (i.e.
-whithout being influenced by outliers). The Mahalanobis distances
+without being influenced by outliers). The Mahalanobis distances
 obtained from this estimate is used to derive a measure of outlyingness.
 This strategy is illustrated below.
 
diff --git a/doc/modules/pipeline.rst b/doc/modules/pipeline.rst
index a48164b09470..c90f35753fb0 100644
--- a/doc/modules/pipeline.rst
+++ b/doc/modules/pipeline.rst
@@ -79,6 +79,11 @@ Parameters of the estimators in the pipeline can be accessed using the
              steps=[('reduce_dim', PCA(copy=True, iterated_power='auto',...)),
                     ('clf', SVC(C=10, cache_size=200, class_weight=None,...))])
 
+Attributes of named_steps map to keys, enabling tab completion in interactive environments::
+
+    >>> pipe.named_steps.reduce_dim is pipe.named_steps['reduce_dim']
+    True
+
 This is particularly important for doing grid searches::
 
     >>> from sklearn.model_selection import GridSearchCV
diff --git a/doc/modules/sgd.rst b/doc/modules/sgd.rst
index 7c515d5459ce..e8febda201bf 100644
--- a/doc/modules/sgd.rst
+++ b/doc/modules/sgd.rst
@@ -279,7 +279,7 @@ Mathematical formulation
 ========================
 
 Given a set of training examples :math:`(x_1, y_1), \ldots, (x_n, y_n)` where
-:math:`x_i \in \mathbf{R}^n` and :math:`y_i \in \{-1,1\}`, our goal is to
+:math:`x_i \in \mathbf{R}^m` and :math:`y_i \in \{-1,1\}`, our goal is to
 learn a linear scoring function :math:`f(x) = w^T x + b` with model parameters
 :math:`w \in \mathbf{R}^m` and intercept :math:`b \in \mathbf{R}`. In order
 to make predictions, we simply look at the sign of :math:`f(x)`.
diff --git a/doc/modules/svm.rst b/doc/modules/svm.rst
index 5da3d7b468a9..044817b6f1dc 100644
--- a/doc/modules/svm.rst
+++ b/doc/modules/svm.rst
@@ -28,7 +28,8 @@ The advantages of support vector machines are:
 The disadvantages of support vector machines include:
 
     - If the number of features is much greater than the number of
-      samples, the method is likely to give poor performances.
+      samples, avoid over-fitting in choosing :ref:`svm_kernels` and regularization
+      term is crucial.
 
     - SVMs do not directly provide probability estimates, these are
       calculated using an expensive five-fold cross-validation
diff --git a/doc/related_projects.rst b/doc/related_projects.rst
index 846bc470dfdb..e978279c2703 100644
--- a/doc/related_projects.rst
+++ b/doc/related_projects.rst
@@ -64,6 +64,13 @@ enhance the functionality of scikit-learn's estimators.
 - `mlxtend <https://github.com/rasbt/mlxtend>`_ Includes model visualization
   utilities.
 
+- `scikit-plot <https://github.com/reiinakano/scikit-plot>`_ A visualization library
+  for quick and easy generation of common plots in data analysis and machine learning.
+
+- `yellowbrick <https://github.com/DistrictDataLabs/yellowbrick>`_ A suite of
+  custom matplotlib visualizers for scikit-learn estimators to support visual feature
+  analysis, model selection, evaluation, and diagnostics.
+
 
 **Model export for production**
 
@@ -174,13 +181,23 @@ and tasks.
 - `kmodes <https://github.com/nicodv/kmodes>`_ k-modes clustering algorithm for
   categorical data, and several of its variations.
 
-- `hdbscan <https://github.com/lmcinnes/hdbscan>`_ HDBSCAN and Robust Single
+- `hdbscan <https://github.com/scikit-learn-contrib/hdbscan>`_ HDBSCAN and Robust Single
   Linkage clustering algorithms for robust variable density clustering.
 
 - `spherecluster <https://github.com/clara-labs/spherecluster>`_ Spherical
   K-means and mixture of von Mises Fisher clustering routines for data on the
   unit hypersphere.
 
+**Pre-processing**
+
+- `categorical-encoding
+  <https://github.com/scikit-learn-contrib/categorical-encoding>`_ A
+  library of sklearn compatible categorical variable encoders.
+
+- `imbalanced-learn
+  <https://github.com/scikit-learn-contrib/imbalanced-learn>`_ Various
+  methods to under- and over-sample datasets.
+
 Statistical learning with Python
 --------------------------------
 Other packages useful for data analysis and machine learning.
diff --git a/doc/themes/scikit-learn/layout.html b/doc/themes/scikit-learn/layout.html
index 2446a7de3a65..d659b9ce8617 100644
--- a/doc/themes/scikit-learn/layout.html
+++ b/doc/themes/scikit-learn/layout.html
@@ -102,7 +102,7 @@
                 </div>
             </a>
             <div class="search_form">
-                <div id="cse" style="width: 100%;"></div>
+                <div class="gcse-search" id="cse" style="width: 100%;"></div>
             </div>
         </div> <!-- end navbar -->
 
@@ -346,15 +346,12 @@ <h2>Machine Learning in Python</h2>
       })();
     </script>
     {% endif %}
-
-    <script src="http://www.google.com/jsapi" type="text/javascript"></script>
-    <script type="text/javascript"> google.load('search', '1',
-        {language : 'en'}); google.setOnLoadCallback(function() {
-            var customSearchControl = new
-            google.search.CustomSearchControl('016639176250731907682:tjtqbvtvij0');
-            customSearchControl.setResultSetSize(google.search.Search.FILTERED_CSE_RESULTSET);
-            var options = new google.search.DrawOptions();
-            options.setAutoComplete(true);
-            customSearchControl.draw('cse', options); }, true);
+    <script>
+      (function() {
+        var cx = '016639176250731907682:tjtqbvtvij0';
+        var gcse = document.createElement('script'); gcse.type = 'text/javascript'; gcse.async = true;
+        gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
+        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(gcse, s);
+      })();
     </script>
 {%- endblock %}
diff --git a/doc/themes/scikit-learn/static/nature.css_t b/doc/themes/scikit-learn/static/nature.css_t
index 593cb01ce67c..8e3a3841418c 100644
--- a/doc/themes/scikit-learn/static/nature.css_t
+++ b/doc/themes/scikit-learn/static/nature.css_t
@@ -207,20 +207,40 @@ div.navbar div.nav-icon {
 }
 
 .search_form {
-    margin-top: -23px;
-    /*The min-height is added here, to prevent the element from shrinking
-    too much, while the scripts are still loading the search-bar.
-    Without it, layout glitches occur, as the element keeps dynamically
-    changing its size while its loading-contents adjusts into position.*/
+    margin-top: -40px;
     min-height: 42px;
 }
 
 #cse .gsc-clear-button {
     width: 50px;
+    position: relative;
+    top: -2px;
 }
 
 .gsc-branding {
-    display: none;
+    display: none !important;
+}
+
+form.gsc-search-box {
+    padding: 0 !important;
+}
+
+input.gsc-search-button {
+    position: relative;
+    top: -4px;
+    border-radius: 5px !important;
+    border-color: #FFFFFF !important;
+    background-color: #ff9c34 !important;
+}
+
+a.gs-title, a.gs-title > b{
+    color: blue !important;
+}
+
+.gsc-results .gsc-cursor-box .gsc-cursor-current-page {
+    border-color: white !important;
+    background-color: #ff9c34 !important;
+    color: white !important;
 }
 
 /*---------------------------------------------------------------*/
diff --git a/doc/tutorial/statistical_inference/supervised_learning.rst b/doc/tutorial/statistical_inference/supervised_learning.rst
index 6fab7e3cbb59..e5342c5cad64 100644
--- a/doc/tutorial/statistical_inference/supervised_learning.rst
+++ b/doc/tutorial/statistical_inference/supervised_learning.rst
@@ -109,21 +109,21 @@ The curse of dimensionality
 
 For an estimator to be effective, you need the distance between neighboring
 points to be less than some value :math:`d`, which depends on the problem.
-In one dimension, this requires on average :math:`n ~ 1/d` points.
+In one dimension, this requires on average :math:`n \sim 1/d` points.
 In the context of the above :math:`k`-NN example, if the data is described by
 just one feature with values ranging from 0 to 1 and with :math:`n` training
 observations, then new data will be no further away than :math:`1/n`.
 Therefore, the nearest neighbor decision rule will be efficient as soon as
 :math:`1/n` is small compared to the scale of between-class feature variations.
 
-If the number of features is :math:`p`, you now require :math:`n ~ 1/d^p`
+If the number of features is :math:`p`, you now require :math:`n \sim 1/d^p`
 points.  Let's say that we require 10 points in one dimension: now :math:`10^p`
 points are required in :math:`p` dimensions to pave the :math:`[0, 1]` space.
 As :math:`p` becomes large, the number of training points required for a good
 estimator grows exponentially.
 
 For example, if each point is just a single number (8 bytes), then an
-effective :math:`k`-NN estimator in a paltry :math:`p~20` dimensions would
+effective :math:`k`-NN estimator in a paltry :math:`p \sim 20` dimensions would
 require more training data than the current estimated size of the entire
 internet (±1000 Exabytes or so).
 
diff --git a/doc/whats_new.rst b/doc/whats_new.rst
index 7d2fa8a56288..93bd58183136 100644
--- a/doc/whats_new.rst
+++ b/doc/whats_new.rst
@@ -18,7 +18,7 @@ parameters, may produce different models from the previous version. This often
 occurs due to changes in the modelling logic (bug fixes or enhancements), or in
 random sampling procedures.
 
-* *to be listed*
+   * :class:`sklearn.ensemble.IsolationForest` (bug fix)
 
 Details are listed in the changelog below.
 
@@ -41,11 +41,22 @@ New features
      Kullback-Leibler divergence and the Itakura-Saito divergence.
      By `Tom Dupre la Tour`_.
 
+   - Added the :class:`sklearn.model_selection.RepeatedKFold` and
+     :class:`sklearn.model_selection.RepeatedStratifiedKFold`.
+     :issue:`8120` by `Neeraj Gangwar`_.
+
    - Added :func:`metrics.mean_squared_log_error`, which computes
      the mean square error of the logarithmic transformation of targets,
      particularly useful for targets with an exponential trend.
      :issue:`7655` by :user:`Karan Desai <karandesai-96>`.
 
+   - Added solver ``saga`` that implements the improved version of Stochastic
+     Average Gradient, in :class:`linear_model.LogisticRegression` and
+     :class:`linear_model.Ridge`. It allows the use of L1 penalty with
+     multinomial logistic loss, and behaves marginally better than 'sag'
+     during the first epochs of ridge and logistic regression.
+     By `Arthur Mensch`_.
+
 Enhancements
 ............
 
@@ -150,9 +161,23 @@ Enhancements
    - Add ``sample_weight`` parameter to :func:`metrics.cohen_kappa_score` by
      Victor Poughon.
 
+   - In :class:`gaussian_process.GaussianProcessRegressor`, method ``predict`` 
+     is a lot faster with ``return_std=True`` by :user:`Hadrien Bertrand <hbertrand>`.
+   - Added ability to use sparse matrices in :func:`feature_selection.f_regression`
+     with ``center=True``. :issue:`8065` by :user:`Daniel LeJeune <acadiansith>`.
+
+   - :class:`ensemble.VotingClassifier` now allow changing estimators by using
+     :meth:`ensemble.VotingClassifier.set_params`. Estimators can also be
+     removed by setting it to `None`.
+     :issue:`7674` by:user:`Yichuan Liu <yl565>`.
+
 Bug fixes
 .........
-   - Fixed a bug where :class:`sklearn.cluster.DBSCAN` gives incorrect 
+   - Fixed a bug where :class:`sklearn.ensemble.IsolationForest` uses an
+     an incorrect formula for the average path length
+     :issue:`8549` by `Peter Wang <https://github.com/PTRWang>`_.
+
+   - Fixed a bug where :class:`sklearn.cluster.DBSCAN` gives incorrect
      result when input is a precomputed sparse matrix with initial
      rows all zero.
      :issue:`8306` by :user:`Akshay Gupta <Akshay0724>`
@@ -163,13 +188,16 @@ Bug fixes
 
    - Fixed a bug where :func:`sklearn.model_selection.BaseSearchCV.inverse_transform`
      returns self.best_estimator_.transform() instead of self.best_estimator_.inverse_transform()
-     :issue:`8344` by :user:`Akshay Gupta <Akshay0724>` 
+     :issue:`8344` by :user:`Akshay Gupta <Akshay0724>`
 
    - Fixed a bug where :class:`sklearn.linear_model.RandomizedLasso` and
      :class:`sklearn.linear_model.RandomizedLogisticRegression` breaks for
      sparse input.
      :issue:`8259` by :user:`Aman Dalmia <dalmia>`.
 
+   - Fixed a bug where :func:`sklearn.linear_model.RANSACRegressor.fit` may run until
+     ``max_iter`` if finds a large inlier group early. :issue:`8251` by :user:`aivision2020`.
+
    - Fixed a bug where :func:`sklearn.datasets.make_moons` gives an
      incorrect result when ``n_samples`` is odd.
      :issue:`8198` by :user:`Josh Levy <levy5674>`.
@@ -181,6 +209,10 @@ Bug fixes
      ``download_if_missing`` keyword.  This was fixed in :issue:`7944` by
      :user:`Ralf Gommers <rgommers>`.
 
+   - Fixed a bug in :class:`sklearn.ensemble.GradientBoostingClassifier`
+     and :class:`sklearn.ensemble.GradientBoostingRegressor`
+     where a float being compared to ``0.0`` using ``==`` caused a divide by zero
+     error. This was fixed in :issue:`7970` by :user:`He Chen <chenhe95>`.
 
    - Fix a bug regarding fitting :class:`sklearn.cluster.KMeans` with a
      sparse array X and initial centroids, where X's means were unnecessarily
@@ -231,6 +263,26 @@ Bug fixes
      obstructed pickling customizations of child-classes, when used in a
      multiple inheritance context.
      :issue:`8316` by :user:`Holger Peters <HolgerPeters>`.
+   - Fix a bug in :func:`sklearn.metrics.classification._check_targets`
+     which would return ``'binary'`` if ``y_true`` and ``y_pred`` were
+     both ``'binary'`` but the union of ``y_true`` and ``y_pred`` was
+     ``'multiclass'``. :issue:`8377` by `Loic Esteve`_.
+
+   - Fix :func:`sklearn.linear_model.BayesianRidge.fit` to return 
+     ridge parameter `alpha_` and `lambda_` consistent with calculated
+     coefficients `coef_` and `intercept_`.
+     :issue:`8224` by :user:`Peter Gedeck <gedeck>`.
+
+   - Fixed a bug in :class:`manifold.TSNE` where it stored the incorrect
+     ``kl_divergence_``. :issue:`6507` by :user:`Sebastian Saeger <ssaeger>`.
+
+   - Fixed a bug in :class:`svm.OneClassSVM` where it returned floats instead of
+     integer classes. :issue:`8676` by :user:`Vathsala Achar <VathsalaAchar>`.
+
+   - Fixed a bug where :func:`sklearn.tree.export_graphviz` raised an error
+     when the length of features_names does not match n_features in the decision
+     tree.
+     :issue:`8512` by :user:`Li Li <aikinogard>`.
 
 API changes summary
 -------------------
@@ -247,6 +299,12 @@ API changes summary
      needed for the perplexity calculation. :issue:`7954` by
      :user:`Gary Foreman <garyForeman>`.
 
+   - Replace attribute ``named_steps`` ``dict`` to :class:`sklearn.utils.Bunch`
+     in :class:`sklearn.pipeline.Pipeline` to enable tab completion in interactive
+     environment. In the case conflict value on ``named_steps`` and ``dict``
+     attribute, ``dict`` behavior will be prioritized.
+     :issue:`8481` by :user:`Herilalaina Rakotoarison <herilalaina>`.
+
    - The :func:`sklearn.multioutput.MultiOutputClassifier.predict_proba`
      function used to return a 3d array (``n_samples``, ``n_classes``,
      ``n_outputs``). In the case where different target columns had different
@@ -267,6 +325,19 @@ API changes summary
       :func:`sklearn.model_selection.cross_val_predict`.
       :issue:`2879` by :user:`Stephen Hoover <stephen-hoover>`.
 
+   - Estimators with both methods ``decision_function`` and ``predict_proba``
+     are now required to have a monotonic relation between them. The
+     method ``check_decision_proba_consistency`` has been added in
+     **sklearn.utils.estimator_checks** to check their consistency.
+     :issue:`7578` by :user:`Shubham Bhardwaj <shubham0704>`
+
+   - All tree based estimators now accept a ``min_impurity_decrease``
+     parameter in lieu of the ``min_impurity_split``, which is now deprecated.
+     The ``min_impurity_decrease`` helps stop splitting the nodes in which
+     the weighted impurity decrease from splitting is no longer alteast
+     ``min_impurity_decrease``.  :issue:`8449` by `Raghav RV_`
+
+
 .. _changes_0_18_1:
 
 Version 0.18.1
@@ -5004,3 +5075,7 @@ David Huard, Dave Morrill, Ed Schofield, Travis Oliphant, Pearu Peterson.
 .. _Vincent Pham: https://github.com/vincentpham1991
 
 .. _Denis Engemann: http://denis-engemann.de
+.. _Anish Shah: https://github.com/AnishShah
+
+.. _Neeraj Gangwar: http://neerajgangwar.in
+.. _Arthur Mensch: https://amensch.fr
\ No newline at end of file
diff --git a/examples/classification/plot_classifier_comparison.py b/examples/classification/plot_classifier_comparison.py
index fbe6a4ac0297..4477f443801b 100644
--- a/examples/classification/plot_classifier_comparison.py
+++ b/examples/classification/plot_classifier_comparison.py
@@ -95,9 +95,11 @@
     if ds_cnt == 0:
         ax.set_title("Input data")
     # Plot the training points
-    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)
+    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,
+               edgecolors='k')
     # and testing points
-    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)
+    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,
+               edgecolors='k')
     ax.set_xlim(xx.min(), xx.max())
     ax.set_ylim(yy.min(), yy.max())
     ax.set_xticks(())
@@ -122,10 +124,11 @@
         ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)
 
         # Plot also the training points
-        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)
+        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,
+                   edgecolors='k')
         # and testing points
         ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,
-                   alpha=0.6)
+                   edgecolors='k', alpha=0.6)
 
         ax.set_xlim(xx.min(), xx.max())
         ax.set_ylim(yy.min(), yy.max())
diff --git a/examples/cluster/plot_cluster_comparison.py b/examples/cluster/plot_cluster_comparison.py
index ce149f19820a..f330aed91f3b 100644
--- a/examples/cluster/plot_cluster_comparison.py
+++ b/examples/cluster/plot_cluster_comparison.py
@@ -3,38 +3,44 @@
 Comparing different clustering algorithms on toy datasets
 =========================================================
 
-This example aims at showing characteristics of different
+This example shows characteristics of different
 clustering algorithms on datasets that are "interesting"
-but still in 2D. The last dataset is an example of a 'null'
-situation for clustering: the data is homogeneous, and
-there is no good clustering.
-
-While these examples give some intuition about the algorithms,
-this intuition might not apply to very high dimensional data.
-
-The results could be improved by tweaking the parameters for
-each clustering strategy, for instance setting the number of
-clusters for the methods that needs this parameter
-specified. Note that affinity propagation has a tendency to
-create many clusters. Thus in this example its two parameters
-(damping and per-point preference) were set to mitigate this
-behavior.
+but still in 2D. With the exception of the last dataset,
+the parameters of each of these dataset-algorithm pairs
+has been tuned to produce good clustering results. Some
+algorithms are more sensitive to parameter values than
+others.
+
+The last dataset is an example of a 'null' situation for
+clustering: the data is homogeneous, and there is no good
+clustering. For this example, the null dataset uses the
+same parameters as the dataset in the row above it, which
+represents a mismatch in the parameter values and the
+data structure.
+
+While these examples give some intuition about the
+algorithms, this intuition might not apply to very high
+dimensional data.
 """
 print(__doc__)
 
 import time
+import warnings
 
 import numpy as np
 import matplotlib.pyplot as plt
 
-from sklearn import cluster, datasets
+from sklearn import cluster, datasets, mixture
 from sklearn.neighbors import kneighbors_graph
 from sklearn.preprocessing import StandardScaler
+from itertools import cycle, islice
 
 np.random.seed(0)
 
+# ============
 # Generate datasets. We choose the size big enough to see the scalability
 # of the algorithms, but not too big to avoid too long running times
+# ============
 n_samples = 1500
 noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,
                                       noise=.05)
@@ -42,77 +48,131 @@
 blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)
 no_structure = np.random.rand(n_samples, 2), None
 
-colors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])
-colors = np.hstack([colors] * 20)
-
-clustering_names = [
-    'MiniBatchKMeans', 'AffinityPropagation', 'MeanShift',
-    'SpectralClustering', 'Ward', 'AgglomerativeClustering',
-    'DBSCAN', 'Birch']
-
-plt.figure(figsize=(len(clustering_names) * 2 + 3, 9.5))
+# Anisotropicly distributed data
+random_state = 170
+X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)
+transformation = [[0.6, -0.6], [-0.4, 0.8]]
+X_aniso = np.dot(X, transformation)
+aniso = (X_aniso, y)
+
+# blobs with varied variances
+varied = datasets.make_blobs(n_samples=n_samples,
+                             cluster_std=[1.0, 2.5, 0.5],
+                             random_state=random_state)
+
+# ============
+# Set up cluster parameters
+# ============
+plt.figure(figsize=(9 * 2 + 3, 12.5))
 plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,
                     hspace=.01)
 
 plot_num = 1
 
-datasets = [noisy_circles, noisy_moons, blobs, no_structure]
-for i_dataset, dataset in enumerate(datasets):
+default_base = {'quantile': .3,
+                'eps': .3,
+                'damping': .9,
+                'preference': -200,
+                'n_neighbors': 10,
+                'n_clusters': 3}
+
+datasets = [
+    (noisy_circles, {'damping': .77, 'preference': -240,
+                     'quantile': .2, 'n_clusters': 2}),
+    (noisy_moons, {'damping': .75, 'preference': -220, 'n_clusters': 2}),
+    (varied, {'eps': .18, 'n_neighbors': 2}),
+    (aniso, {'eps': .15, 'n_neighbors': 2}),
+    (blobs, {}),
+    (no_structure, {})]
+
+for i_dataset, (dataset, algo_params) in enumerate(datasets):
+    # update parameters with dataset-specific values
+    params = default_base.copy()
+    params.update(algo_params)
+
     X, y = dataset
+
     # normalize dataset for easier parameter selection
     X = StandardScaler().fit_transform(X)
 
     # estimate bandwidth for mean shift
-    bandwidth = cluster.estimate_bandwidth(X, quantile=0.3)
+    bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])
 
     # connectivity matrix for structured Ward
-    connectivity = kneighbors_graph(X, n_neighbors=10, include_self=False)
+    connectivity = kneighbors_graph(
+        X, n_neighbors=params['n_neighbors'], include_self=False)
     # make connectivity symmetric
     connectivity = 0.5 * (connectivity + connectivity.T)
 
-    # create clustering estimators
+    # ============
+    # Create cluster objects
+    # ============
     ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)
-    two_means = cluster.MiniBatchKMeans(n_clusters=2)
-    ward = cluster.AgglomerativeClustering(n_clusters=2, linkage='ward',
-                                           connectivity=connectivity)
-    spectral = cluster.SpectralClustering(n_clusters=2,
-                                          eigen_solver='arpack',
-                                          affinity="nearest_neighbors")
-    dbscan = cluster.DBSCAN(eps=.2)
-    affinity_propagation = cluster.AffinityPropagation(damping=.9,
-                                                       preference=-200)
-
-    average_linkage = cluster.AgglomerativeClustering(
-        linkage="average", affinity="cityblock", n_clusters=2,
+    two_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])
+    ward = cluster.AgglomerativeClustering(
+        n_clusters=params['n_clusters'], linkage='ward',
         connectivity=connectivity)
+    spectral = cluster.SpectralClustering(
+        n_clusters=params['n_clusters'], eigen_solver='arpack',
+        affinity="nearest_neighbors")
+    dbscan = cluster.DBSCAN(eps=params['eps'])
+    affinity_propagation = cluster.AffinityPropagation(
+        damping=params['damping'], preference=params['preference'])
+    average_linkage = cluster.AgglomerativeClustering(
+        linkage="average", affinity="cityblock",
+        n_clusters=params['n_clusters'], connectivity=connectivity)
+    birch = cluster.Birch(n_clusters=params['n_clusters'])
+    gmm = mixture.GaussianMixture(
+        n_components=params['n_clusters'], covariance_type='full')
+
+    clustering_algorithms = (
+        ('MiniBatchKMeans', two_means),
+        ('AffinityPropagation', affinity_propagation),
+        ('MeanShift', ms),
+        ('SpectralClustering', spectral),
+        ('Ward', ward),
+        ('AgglomerativeClustering', average_linkage),
+        ('DBSCAN', dbscan),
+        ('Birch', birch),
+        ('GaussianMixture', gmm)
+    )
+
+    for name, algorithm in clustering_algorithms:
+        t0 = time.time()
 
-    birch = cluster.Birch(n_clusters=2)
-    clustering_algorithms = [
-        two_means, affinity_propagation, ms, spectral, ward, average_linkage,
-        dbscan, birch]
+        # catch warnings related to kneighbors_graph
+        with warnings.catch_warnings():
+            warnings.filterwarnings(
+                "ignore",
+                message="the number of connected components of the " +
+                "connectivity matrix is [0-9]{1,2}" +
+                " > 1. Completing it to avoid stopping the tree early.",
+                category=UserWarning)
+            warnings.filterwarnings(
+                "ignore",
+                message="Graph is not fully connected, spectral embedding" +
+                " may not work as expected.",
+                category=UserWarning)
+            algorithm.fit(X)
 
-    for name, algorithm in zip(clustering_names, clustering_algorithms):
-        # predict cluster memberships
-        t0 = time.time()
-        algorithm.fit(X)
         t1 = time.time()
         if hasattr(algorithm, 'labels_'):
             y_pred = algorithm.labels_.astype(np.int)
         else:
             y_pred = algorithm.predict(X)
 
-        # plot
-        plt.subplot(4, len(clustering_algorithms), plot_num)
+        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)
         if i_dataset == 0:
             plt.title(name, size=18)
-        plt.scatter(X[:, 0], X[:, 1], color=colors[y_pred].tolist(), s=10)
-
-        if hasattr(algorithm, 'cluster_centers_'):
-            centers = algorithm.cluster_centers_
-            center_colors = colors[:len(centers)]
-            plt.scatter(centers[:, 0], centers[:, 1], s=100, c=center_colors)
-        plt.xlim(-2, 2)
-        plt.ylim(-2, 2)
+
+        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',
+                                             '#f781bf', '#a65628', '#984ea3',
+                                             '#999999', '#e41a1c', '#dede00']),
+                                      int(max(y_pred) + 1))))
+        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])
+
+        plt.xlim(-2.5, 2.5)
+        plt.ylim(-2.5, 2.5)
         plt.xticks(())
         plt.yticks(())
         plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),
diff --git a/examples/cross_decomposition/plot_compare_cross_decomposition.py b/examples/cross_decomposition/plot_compare_cross_decomposition.py
index 21702b03379b..437c08b05647 100644
--- a/examples/cross_decomposition/plot_compare_cross_decomposition.py
+++ b/examples/cross_decomposition/plot_compare_cross_decomposition.py
@@ -144,5 +144,5 @@
 
 cca = CCA(n_components=2)
 cca.fit(X_train, Y_train)
-X_train_r, Y_train_r = plsca.transform(X_train, Y_train)
-X_test_r, Y_test_r = plsca.transform(X_test, Y_test)
+X_train_r, Y_train_r = cca.transform(X_train, Y_train)
+X_test_r, Y_test_r = cca.transform(X_test, Y_test)
diff --git a/examples/decomposition/plot_pca_iris.py b/examples/decomposition/plot_pca_iris.py
index f8451915b441..23bf4ce51013 100644
--- a/examples/decomposition/plot_pca_iris.py
+++ b/examples/decomposition/plot_pca_iris.py
@@ -50,7 +50,8 @@
               bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))
 # Reorder the labels to have colors matching the cluster results
 y = np.choose(y, [1, 2, 0]).astype(np.float)
-ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.spectral)
+ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.spectral,
+           edgecolor='k')
 
 ax.w_xaxis.set_ticklabels([])
 ax.w_yaxis.set_ticklabels([])
diff --git a/examples/ensemble/plot_forest_iris.py b/examples/ensemble/plot_forest_iris.py
index 96efdab2d05a..1ce7fa052d57 100644
--- a/examples/ensemble/plot_forest_iris.py
+++ b/examples/ensemble/plot_forest_iris.py
@@ -42,6 +42,7 @@
 
 import numpy as np
 import matplotlib.pyplot as plt
+from matplotlib.colors import ListedColormap
 
 from sklearn import clone
 from sklearn.datasets import load_iris
@@ -53,7 +54,6 @@
 # Parameters
 n_classes = 3
 n_estimators = 30
-plot_colors = "ryb"
 cmap = plt.cm.RdYlBu
 plot_step = 0.02  # fine step width for decision surface contours
 plot_step_coarser = 0.5  # step widths for coarse classifier guesses
@@ -139,11 +139,8 @@
 
         # Plot the training points, these are clustered together and have a
         # black outline
-        for i, c in zip(xrange(n_classes), plot_colors):
-            idx = np.where(y == i)
-            plt.scatter(X[idx, 0], X[idx, 1], c=c, label=iris.target_names[i],
-                        cmap=cmap)
-
+        plt.scatter(X[:, 0], X[:, 1], c=y,
+                    cmap=ListedColormap(['r', 'y', 'b']))
         plot_idx += 1  # move on to the next plot in sequence
 
 plt.suptitle("Classifiers on feature subsets of the Iris dataset")
diff --git a/examples/linear_model/plot_sparse_logistic_regression_20newsgroups.py b/examples/linear_model/plot_sparse_logistic_regression_20newsgroups.py
new file mode 100644
index 000000000000..64816b9b825c
--- /dev/null
+++ b/examples/linear_model/plot_sparse_logistic_regression_20newsgroups.py
@@ -0,0 +1,118 @@
+"""
+=====================================================
+Multiclass sparse logisitic regression on newgroups20
+=====================================================
+
+Comparison of multinomial logistic L1 vs one-versus-rest L1 logistic regression
+to classify documents from the newgroups20 dataset. Multinomial logistic
+regression yields more accurate results and is faster to train on the larger
+scale dataset.
+
+Here we use the l1 sparsity that trims the weights of not informative
+features to zero. This is good if the goal is to extract the strongly
+discriminative vocabulary of each class. If the goal is to get the best
+predictive accuracy, it is better to use the non sparsity-inducing l2 penalty
+instead.
+
+A more traditional (and possibly better) way to predict on a sparse subset of
+input features would be to use univariate feature selection followed by a
+traditional (l2-penalised) logistic regression model.
+"""
+import time
+
+import matplotlib.pyplot as plt
+import numpy as np
+
+from sklearn.datasets import fetch_20newsgroups_vectorized
+from sklearn.linear_model import LogisticRegression
+from sklearn.model_selection import train_test_split
+
+print(__doc__)
+# Author: Arthur Mensch
+
+t0 = time.clock()
+
+# We use SAGA solver
+solver = 'saga'
+
+# Turn down for faster run time
+n_samples = 10000
+
+# Memorized fetch_rcv1 for faster access
+dataset = fetch_20newsgroups_vectorized('all')
+X = dataset.data
+y = dataset.target
+X = X[:n_samples]
+y = y[:n_samples]
+
+X_train, X_test, y_train, y_test = train_test_split(X, y,
+                                                    random_state=42,
+                                                    stratify=y,
+                                                    test_size=0.1)
+train_samples, n_features = X_train.shape
+n_classes = np.unique(y).shape[0]
+
+print('Dataset 20newsgroup, train_samples=%i, n_features=%i, n_classes=%i'
+      % (train_samples, n_features, n_classes))
+
+models = {'ovr': {'name': 'One versus Rest', 'iters': [1, 3]},
+          'multinomial': {'name': 'Multinomial', 'iters': [1, 3, 7]}}
+
+for model in models:
+    # Add initial chance-level values for plotting purpose
+    accuracies = [1 / n_classes]
+    times = [0]
+    densities = [1]
+
+    model_params = models[model]
+
+    # Small number of epochs for fast runtime
+    for this_max_iter in model_params['iters']:
+        print('[model=%s, solver=%s] Number of epochs: %s' %
+              (model_params['name'], solver, this_max_iter))
+        lr = LogisticRegression(solver=solver,
+                                multi_class=model,
+                                C=1,
+                                penalty='l1',
+                                fit_intercept=True,
+                                max_iter=this_max_iter,
+                                random_state=42,
+                                )
+        t1 = time.clock()
+        lr.fit(X_train, y_train)
+        train_time = time.clock() - t1
+
+        y_pred = lr.predict(X_test)
+        accuracy = np.sum(y_pred == y_test) / y_test.shape[0]
+        density = np.mean(lr.coef_ != 0, axis=1) * 100
+        accuracies.append(accuracy)
+        densities.append(density)
+        times.append(train_time)
+    models[model]['times'] = times
+    models[model]['densities'] = densities
+    models[model]['accuracies'] = accuracies
+    print('Test accuracy for model %s: %.4f' % (model, accuracies[-1]))
+    print('%% non-zero coefficients for model %s, '
+          'per class:\n %s' % (model, densities[-1]))
+    print('Run time (%i epochs) for model %s:'
+          '%.2f' % (model_params['iters'][-1], model, times[-1]))
+
+fig = plt.figure()
+ax = fig.add_subplot(111)
+
+for model in models:
+    name = models[model]['name']
+    times = models[model]['times']
+    accuracies = models[model]['accuracies']
+    ax.plot(times, accuracies, marker='o',
+            label='Model: %s' % name)
+    ax.set_xlabel('Train time (s)')
+    ax.set_ylabel('Test accuracy')
+ax.legend()
+fig.suptitle('Multinomial vs One-vs-Rest Logistic L1\n'
+             'Dataset %s' % '20newsgroups')
+fig.tight_layout()
+fig.subplots_adjust(top=0.85)
+run_time = time.clock() - t0
+print('Example run in %.3f s' % run_time)
+plt.show()
diff --git a/examples/linear_model/plot_sparse_logistic_regression_mnist.py b/examples/linear_model/plot_sparse_logistic_regression_mnist.py
new file mode 100644
index 000000000000..2b889d25013d
--- /dev/null
+++ b/examples/linear_model/plot_sparse_logistic_regression_mnist.py
@@ -0,0 +1,79 @@
+"""
+=====================================================
+MNIST classfification using multinomial logistic + L1
+=====================================================
+
+Here we fit a multinomial logistic regression with L1 penalty on a subset of
+the MNIST digits classification task. We use the SAGA algorithm for this
+purpose: this a solver that is fast when the number of samples is significantly
+larger than the number of features and is able to finely optimize non-smooth
+objective functions which is the case with the l1-penalty. Test accuracy
+reaches > 0.8, while weight vectors remains *sparse* and therefore more easily
+*interpretable*.
+
+Note that this accuracy of this l1-penalized linear model is significantly
+below what can be reached by an l2-penalized linear model or a non-linear
+multi-layer perceptron model on this dataset.
+
+"""
+import time
+import matplotlib.pyplot as plt
+import numpy as np
+
+from sklearn.datasets import fetch_mldata
+from sklearn.linear_model import LogisticRegression
+from sklearn.model_selection import train_test_split
+from sklearn.preprocessing import StandardScaler
+from sklearn.utils import check_random_state
+
+print(__doc__)
+
+# Author: Arthur Mensch <arthur.mensch@m4x.org>
+# License: BSD 3 clause
+
+# Turn down for faster convergence
+t0 = time.time()
+train_samples = 5000
+
+mnist = fetch_mldata('MNIST original')
+X = mnist.data.astype('float64')
+y = mnist.target
+random_state = check_random_state(0)
+permutation = random_state.permutation(X.shape[0])
+X = X[permutation]
+y = y[permutation]
+X = X.reshape((X.shape[0], -1))
+
+X_train, X_test, y_train, y_test = train_test_split(
+    X, y, train_size=train_samples, test_size=10000)
+
+scaler = StandardScaler()
+X_train = scaler.fit_transform(X_train)
+X_test = scaler.transform(X_test)
+
+# Turn up tolerance for faster convergence
+clf = LogisticRegression(C=50 / train_samples,
+                         multi_class='multinomial',
+                         penalty='l1', solver='saga', tol=0.1)
+clf.fit(X_train, y_train)
+sparsity = np.mean(clf.coef_ == 0) * 100
+score = clf.score(X_test, y_test)
+# print('Best C % .4f' % clf.C_)
+print("Sparsity with L1 penalty: %.2f%%" % sparsity)
+print("Test score with L1 penalty: %.4f" % score)
+
+coef = clf.coef_.copy()
+plt.figure(figsize=(10, 5))
+scale = np.abs(coef).max()
+for i in range(10):
+    l1_plot = plt.subplot(2, 5, i + 1)
+    l1_plot.imshow(coef[i].reshape(28, 28), interpolation='nearest',
+                   cmap=plt.cm.RdBu, vmin=-scale, vmax=scale)
+    l1_plot.set_xticks(())
+    l1_plot.set_yticks(())
+    l1_plot.set_xlabel('Class %i' % i)
+plt.suptitle('Classification vector for...')
+
+run_time = time.time() - t0
+print('Example run in %.3f s' % run_time)
+plt.show()
diff --git a/examples/manifold/plot_compare_methods.py b/examples/manifold/plot_compare_methods.py
index cce734922d29..34e161dfb046 100644
--- a/examples/manifold/plot_compare_methods.py
+++ b/examples/manifold/plot_compare_methods.py
@@ -43,14 +43,10 @@
 plt.suptitle("Manifold Learning with %i points, %i neighbors"
              % (1000, n_neighbors), fontsize=14)
 
-try:
-    # compatibility matplotlib < 1.0
-    ax = fig.add_subplot(251, projection='3d')
-    ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)
-    ax.view_init(4, -72)
-except:
-    ax = fig.add_subplot(251, projection='3d')
-    plt.scatter(X[:, 0], X[:, 2], c=color, cmap=plt.cm.Spectral)
+
+ax = fig.add_subplot(251, projection='3d')
+ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)
+ax.view_init(4, -72)
 
 methods = ['standard', 'ltsa', 'hessian', 'modified']
 labels = ['LLE', 'LTSA', 'Hessian LLE', 'Modified LLE']
diff --git a/examples/manifold/plot_manifold_sphere.py b/examples/manifold/plot_manifold_sphere.py
index 622a76e14bba..2b6566c4ecd9 100644
--- a/examples/manifold/plot_manifold_sphere.py
+++ b/examples/manifold/plot_manifold_sphere.py
@@ -68,11 +68,7 @@
 
 ax = fig.add_subplot(251, projection='3d')
 ax.scatter(x, y, z, c=p[indices], cmap=plt.cm.rainbow)
-try:
-    # compatibility matplotlib < 1.0
-    ax.view_init(40, -10)
-except:
-    pass
+ax.view_init(40, -10)
 
 sphere_data = np.array([x, y, z]).T
 
diff --git a/examples/manifold/plot_swissroll.py b/examples/manifold/plot_swissroll.py
index 17c318cab709..42943ba64f5a 100644
--- a/examples/manifold/plot_swissroll.py
+++ b/examples/manifold/plot_swissroll.py
@@ -33,13 +33,9 @@
 # Plot result
 
 fig = plt.figure()
-try:
-    # compatibility matplotlib < 1.0
-    ax = fig.add_subplot(211, projection='3d')
-    ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)
-except:
-    ax = fig.add_subplot(211)
-    ax.scatter(X[:, 0], X[:, 2], c=color, cmap=plt.cm.Spectral)
+
+ax = fig.add_subplot(211, projection='3d')
+ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)
 
 ax.set_title("Original data")
 ax = fig.add_subplot(212)
diff --git a/examples/mixture/plot_concentration_prior.py b/examples/mixture/plot_concentration_prior.py
index 49d9e6211b35..b51ad2280b65 100644
--- a/examples/mixture/plot_concentration_prior.py
+++ b/examples/mixture/plot_concentration_prior.py
@@ -70,7 +70,8 @@ def plot_results(ax1, ax2, estimator, X, y, title, plot_title=False):
     ax2.get_xaxis().set_tick_params(direction='out')
     ax2.yaxis.grid(True, alpha=0.7)
     for k, w in enumerate(estimator.weights_):
-        ax2.bar(k - .45, w, width=0.9, color='#56B4E9', zorder=3)
+        ax2.bar(k, w, width=0.9, color='#56B4E9', zorder=3,
+                align='center')
         ax2.text(k, w + 0.007, "%.1f%%" % (w * 100.),
                  horizontalalignment='center')
     ax2.set_xlim(-.6, 2 * n_components - .4)
diff --git a/examples/model_selection/grid_search_digits.py b/examples/model_selection/grid_search_digits.py
index 50ba4dc097f8..b1457fd98ae0 100644
--- a/examples/model_selection/grid_search_digits.py
+++ b/examples/model_selection/grid_search_digits.py
@@ -50,7 +50,7 @@
     print("# Tuning hyper-parameters for %s" % score)
     print()
 
-    clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5,
+    clf = GridSearchCV(SVC(), tuned_parameters, cv=5,
                        scoring='%s_macro' % score)
     clf.fit(X_train, y_train)
 
diff --git a/examples/model_selection/plot_nested_cross_validation_iris.py b/examples/model_selection/plot_nested_cross_validation_iris.py
index f3cb2dfb0698..917746c359d4 100644
--- a/examples/model_selection/plot_nested_cross_validation_iris.py
+++ b/examples/model_selection/plot_nested_cross_validation_iris.py
@@ -64,7 +64,7 @@
           "gamma": [.01, .1]}
 
 # We will use a Support Vector Classifier with "rbf" kernel
-svr = SVC(kernel="rbf")
+svm = SVC(kernel="rbf")
 
 # Arrays to store scores
 non_nested_scores = np.zeros(NUM_TRIALS)
@@ -80,7 +80,7 @@
     outer_cv = KFold(n_splits=4, shuffle=True, random_state=i)
 
     # Non_nested parameter search and scoring
-    clf = GridSearchCV(estimator=svr, param_grid=p_grid, cv=inner_cv)
+    clf = GridSearchCV(estimator=svm, param_grid=p_grid, cv=inner_cv)
     clf.fit(X_iris, y_iris)
     non_nested_scores[i] = clf.best_score_
 
diff --git a/examples/svm/plot_custom_kernel.py b/examples/svm/plot_custom_kernel.py
index 8065049aa0de..28641cd35f8c 100644
--- a/examples/svm/plot_custom_kernel.py
+++ b/examples/svm/plot_custom_kernel.py
@@ -50,7 +50,7 @@ def my_kernel(X, Y):
 plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)
 
 # Plot also the training points
-plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)
+plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired, edgecolors='k')
 plt.title('3-Class classification using Support Vector Machine with custom'
           ' kernel')
 plt.axis('tight')
diff --git a/examples/svm/plot_oneclass.py b/examples/svm/plot_oneclass.py
index 5720f0912565..3f04537ca1f0 100644
--- a/examples/svm/plot_oneclass.py
+++ b/examples/svm/plot_oneclass.py
@@ -46,9 +46,11 @@
 plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='palevioletred')
 
 s = 40
-b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white', s=s)
-b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='blueviolet', s=s)
-c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='gold', s=s)
+b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white', s=s, edgecolors='k')
+b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='blueviolet', s=s,
+                 edgecolors='k')
+c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='gold', s=s,
+                edgecolors='k')
 plt.axis('tight')
 plt.xlim((-5, 5))
 plt.ylim((-5, 5))
diff --git a/examples/svm/plot_rbf_parameters.py b/examples/svm/plot_rbf_parameters.py
index acec9896169b..9bbca6683ce9 100644
--- a/examples/svm/plot_rbf_parameters.py
+++ b/examples/svm/plot_rbf_parameters.py
@@ -166,7 +166,8 @@ def __call__(self, value, clip=None):
 
     # visualize parameter's effect on decision function
     plt.pcolormesh(xx, yy, -Z, cmap=plt.cm.RdBu)
-    plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_2d, cmap=plt.cm.RdBu_r)
+    plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_2d, cmap=plt.cm.RdBu_r,
+                edgecolors='k')
     plt.xticks(())
     plt.yticks(())
     plt.axis('tight')
diff --git a/examples/svm/plot_separating_hyperplane_unbalanced.py b/examples/svm/plot_separating_hyperplane_unbalanced.py
index f7278004acfc..438291dc5538 100644
--- a/examples/svm/plot_separating_hyperplane_unbalanced.py
+++ b/examples/svm/plot_separating_hyperplane_unbalanced.py
@@ -60,7 +60,7 @@
 # plot separating hyperplanes and samples
 h0 = plt.plot(xx, yy, 'k-', label='no weights')
 h1 = plt.plot(xx, wyy, 'k--', label='with weights')
-plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)
+plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='k')
 plt.legend()
 
 plt.axis('tight')
diff --git a/examples/svm/plot_svm_kernels.py b/examples/svm/plot_svm_kernels.py
index f0d06dd7968a..dbad4e0b725e 100644
--- a/examples/svm/plot_svm_kernels.py
+++ b/examples/svm/plot_svm_kernels.py
@@ -56,8 +56,9 @@
     plt.clf()
 
     plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,
-                facecolors='none', zorder=10)
-    plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired)
+                facecolors='none', zorder=10, edgecolors='k')
+    plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired,
+                edgecolors='k')
 
     plt.axis('tight')
     x_min = -3
diff --git a/examples/svm/plot_svm_margin.py b/examples/svm/plot_svm_margin.py
index 70724de1f28f..2fdc29c1b29b 100644
--- a/examples/svm/plot_svm_margin.py
+++ b/examples/svm/plot_svm_margin.py
@@ -62,8 +62,9 @@
     plt.plot(xx, yy_up, 'k--')
 
     plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,
-                facecolors='none', zorder=10)
-    plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired)
+                facecolors='none', zorder=10, edgecolors='k')
+    plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired,
+                edgecolors='k')
 
     plt.axis('tight')
     x_min = -4.8
diff --git a/examples/svm/plot_svm_nonlinear.py b/examples/svm/plot_svm_nonlinear.py
index 142c42ad1299..c453ef391a16 100644
--- a/examples/svm/plot_svm_nonlinear.py
+++ b/examples/svm/plot_svm_nonlinear.py
@@ -34,7 +34,8 @@
            origin='lower', cmap=plt.cm.PuOr_r)
 contours = plt.contour(xx, yy, Z, levels=[0], linewidths=2,
                        linetypes='--')
-plt.scatter(X[:, 0], X[:, 1], s=30, c=Y, cmap=plt.cm.Paired)
+plt.scatter(X[:, 0], X[:, 1], s=30, c=Y, cmap=plt.cm.Paired,
+            edgecolors='k')
 plt.xticks(())
 plt.yticks(())
 plt.axis([-3, 3, -3, 3])
diff --git a/examples/tree/plot_unveil_tree_structure.py b/examples/tree/plot_unveil_tree_structure.py
index 9694737613c0..161d3983530c 100644
--- a/examples/tree/plot_unveil_tree_structure.py
+++ b/examples/tree/plot_unveil_tree_structure.py
@@ -114,11 +114,11 @@
     else:
         threshold_sign = ">"
 
-    print("decision id node %s : (X[%s, %s] (= %s) %s %s)"
+    print("decision id node %s : (X_test[%s, %s] (= %s) %s %s)"
           % (node_id,
              sample_id,
              feature[node_id],
-             X_test[i, feature[node_id]],
+             X_test[sample_id, feature[node_id]],
              threshold_sign,
              threshold[node_id]))
 
diff --git a/setup.py b/setup.py
index ff7527ef04be..58885ff917db 100755
--- a/setup.py
+++ b/setup.py
@@ -33,7 +33,6 @@
 MAINTAINER_EMAIL = 'amueller@ais.uni-bonn.de'
 URL = 'http://scikit-learn.org'
 LICENSE = 'new BSD'
-DOWNLOAD_URL = 'http://sourceforge.net/projects/scikit-learn/files/'
 
 # We can actually import a restricted version of sklearn that
 # does not need the compiled code
@@ -185,7 +184,6 @@ def setup_package():
                     license=LICENSE,
                     url=URL,
                     version=VERSION,
-                    download_url=DOWNLOAD_URL,
                     long_description=LONG_DESCRIPTION,
                     classifiers=['Intended Audience :: Science/Research',
                                  'Intended Audience :: Developers',
diff --git a/sklearn/base.py b/sklearn/base.py
index 8e576382f98c..719ec21816cd 100644
--- a/sklearn/base.py
+++ b/sklearn/base.py
@@ -39,7 +39,7 @@ def clone(estimator, safe=True):
         The estimator or group of estimators to be cloned
 
     safe : boolean, optional
-        If safe is false, clone will fall back to a deepcopy on objects
+        If safe is false, clone will fall back to a deep copy on objects
         that are not estimators.
 
     """
@@ -259,7 +259,7 @@ def set_params(self, **params):
         self
         """
         if not params:
-            # Simple optimisation to gain speed (inspect is slow)
+            # Simple optimization to gain speed (inspect is slow)
             return self
         valid_params = self.get_params(deep=True)
         for key, value in six.iteritems(params):
diff --git a/sklearn/calibration.py b/sklearn/calibration.py
index 35cdab45a1b5..0d2f76cd1223 100644
--- a/sklearn/calibration.py
+++ b/sklearn/calibration.py
@@ -513,6 +513,8 @@ def predict(self, T):
 def calibration_curve(y_true, y_prob, normalize=False, n_bins=5):
     """Compute true and predicted probabilities for a calibration curve.
 
+     Calibration curves may also be referred to as reliability diagrams.
+
     Read more in the :ref:`User Guide <calibration>`.
 
     Parameters
diff --git a/sklearn/cluster/bicluster.py b/sklearn/cluster/bicluster.py
index 8f6206f9599d..6d5b6e76ee65 100644
--- a/sklearn/cluster/bicluster.py
+++ b/sklearn/cluster/bicluster.py
@@ -236,9 +236,11 @@ class SpectralCoclustering(BaseSpectral):
         (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
         are used.
 
-    random_state : int seed, RandomState instance, or None (default)
-        A pseudo random number generator used by the K-Means
-        initialization.
+    random_state : int, RandomState instance or None, optional, default: None
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     Attributes
     ----------
@@ -366,9 +368,11 @@ class SpectralBiclustering(BaseSpectral):
         (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
         are used.
 
-    random_state : int seed, RandomState instance, or None (default)
-        A pseudo random number generator used by the K-Means
-        initialization.
+    random_state : int, RandomState instance or None, optional, default: None
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     Attributes
     ----------
diff --git a/sklearn/cluster/k_means_.py b/sklearn/cluster/k_means_.py
index f33b3f65b714..680edc2672a7 100644
--- a/sklearn/cluster/k_means_.py
+++ b/sklearn/cluster/k_means_.py
@@ -230,10 +230,11 @@ def k_means(X, n_clusters, init='k-means++', precompute_distances='auto',
     verbose : boolean, optional
         Verbosity mode.
 
-    random_state : integer or numpy.RandomState, optional
-        The generator used to initialize the centers. If an integer is
-        given, it fixes the seed. Defaults to the global numpy random
-        number generator.
+    random_state : int, RandomState instance or None, optional, default: None
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     copy_x : boolean, optional
         When pre-computing distances it is more numerically accurate to center
@@ -449,10 +450,11 @@ def _kmeans_single_lloyd(X, n_clusters, max_iter=300, init='k-means++',
     precompute_distances : boolean, default: True
         Precompute distances (faster but takes more memory).
 
-    random_state : integer or numpy.RandomState, optional
-        The generator used to initialize the centers. If an integer is
-        given, it fixes the seed. Defaults to the global numpy random
-        number generator.
+    random_state : int, RandomState instance or None, optional, default: None
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     Returns
     -------
@@ -638,10 +640,11 @@ def _init_centroids(X, k, init, random_state=None, x_squared_norms=None,
     init : {'k-means++', 'random' or ndarray or callable} optional
         Method for initialization
 
-    random_state : integer or numpy.RandomState, optional
-        The generator used to initialize the centers. If an integer is
-        given, it fixes the seed. Defaults to the global numpy random
-        number generator.
+    random_state : int, RandomState instance or None, optional, default: None
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     x_squared_norms :  array, shape (n_samples,), optional
         Squared euclidean norm of each data point. Pass it if you have it at
@@ -766,10 +769,11 @@ class KMeans(BaseEstimator, ClusterMixin, TransformerMixin):
         (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
         are used.
 
-    random_state : integer or numpy.RandomState, optional
-        The generator used to initialize the centers. If an integer is
-        given, it fixes the seed. Defaults to the global numpy random
-        number generator.
+    random_state : int, RandomState instance or None, optional, default: None
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     verbose : int, default 0
         Verbosity mode.
@@ -1008,10 +1012,11 @@ def _mini_batch_step(X, x_squared_norms, centers, counts,
         the distances of each sample to its closest center.
         May not be None when random_reassign is True.
 
-    random_state : integer or numpy.RandomState, optional
-        The generator used to initialize the centers. If an integer is
-        given, it fixes the seed. Defaults to the global numpy random
-        number generator.
+    random_state : int, RandomState instance or None, optional, default: None
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     random_reassign : boolean, optional
         If True, centers with very low counts are randomly reassigned
@@ -1247,10 +1252,11 @@ class MiniBatchKMeans(KMeans):
         Compute label assignment and inertia for the complete dataset
         once the minibatch optimization has converged in fit.
 
-    random_state : integer or numpy.RandomState, optional
-        The generator used to initialize the centers. If an integer is
-        given, it fixes the seed. Defaults to the global numpy random
-        number generator.
+    random_state : int, RandomState instance or None, optional, default: None
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     reassignment_ratio : float, default: 0.01
         Control the fraction of the maximum number of counts for a
diff --git a/sklearn/cluster/mean_shift_.py b/sklearn/cluster/mean_shift_.py
index 2d554c05ff80..0830b0f7c18a 100644
--- a/sklearn/cluster/mean_shift_.py
+++ b/sklearn/cluster/mean_shift_.py
@@ -47,8 +47,11 @@ def estimate_bandwidth(X, quantile=0.3, n_samples=None, random_state=0,
     n_samples : int, optional
         The number of samples to use. If not given, all samples are used.
 
-    random_state : int or RandomState
-        Pseudo-random number generator state used for random sampling.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     n_jobs : int, optional (default = 1)
         The number of parallel jobs to run for neighbors search.
@@ -59,6 +62,8 @@ def estimate_bandwidth(X, quantile=0.3, n_samples=None, random_state=0,
     bandwidth : float
         The bandwidth parameter.
     """
+    X = check_array(X)
+
     random_state = check_random_state(random_state)
     if n_samples is not None:
         idx = random_state.permutation(X.shape[0])[:n_samples]
diff --git a/sklearn/cluster/spectral.py b/sklearn/cluster/spectral.py
index 8b64ca9a6dd1..0022659a9395 100644
--- a/sklearn/cluster/spectral.py
+++ b/sklearn/cluster/spectral.py
@@ -39,9 +39,11 @@ def discretize(vectors, copy=True, max_svd_restarts=30, n_iter_max=20,
         Maximum number of iterations to attempt in rotation and partition
         matrix search if machine precision convergence is not reached
 
-    random_state : int seed, RandomState instance, or None (default)
-        A pseudo random number generator used for the initialization of the
-        of the rotation matrix
+    random_state : int, RandomState instance or None, optional, default: None
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     Returns
     -------
@@ -194,10 +196,13 @@ def spectral_clustering(affinity, n_clusters=8, n_components=None,
         to be installed. It can be faster on very large, sparse problems,
         but may also lead to instabilities
 
-    random_state : int seed, RandomState instance, or None (default)
-        A pseudo random number generator used for the initialization
-        of the lobpcg eigen vectors decomposition when eigen_solver == 'amg'
-        and by the K-Means initialization.
+    random_state : int, RandomState instance or None, optional, default: None
+        A pseudo random number generator used for the initialization of the
+        lobpcg eigen vectors decomposition when eigen_solver == 'amg' and by
+        the K-Means initialization. If int, random_state is the seed used by
+        the random number generator; If RandomState instance, random_state is
+        the random number generator; If None, the random number generator is
+        the RandomState instance used by `np.random`.
 
     n_init : int, optional, default: 10
         Number of time the k-means algorithm will be run with different
@@ -306,9 +311,8 @@ class SpectralClustering(BaseEstimator, ClusterMixin):
         by the clustering algorithm.
 
     gamma : float, default=1.0
-        Scaling factor of RBF, polynomial, exponential chi^2 and
-        sigmoid affinity kernel. Ignored for
-        ``affinity='nearest_neighbors'``.
+        Kernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels.
+        Ignored for ``affinity='nearest_neighbors'``.
 
     degree : float, default=3
         Degree of the polynomial kernel. Ignored by other kernels.
@@ -326,10 +330,13 @@ class SpectralClustering(BaseEstimator, ClusterMixin):
         to be installed. It can be faster on very large, sparse problems,
         but may also lead to instabilities
 
-    random_state : int seed, RandomState instance, or None (default)
-        A pseudo random number generator used for the initialization
-        of the lobpcg eigen vectors decomposition when eigen_solver == 'amg'
-        and by the K-Means initialization.
+    random_state : int, RandomState instance or None, optional, default: None
+        A pseudo random number generator used for the initialization of the
+        lobpcg eigen vectors decomposition when eigen_solver == 'amg' and by
+        the K-Means initialization.  If int, random_state is the seed used by
+        the random number generator; If RandomState instance, random_state is
+        the random number generator; If None, the random number generator is
+        the RandomState instance used by `np.random`.
 
     n_init : int, optional, default: 10
         Number of time the k-means algorithm will be run with different
diff --git a/sklearn/cluster/tests/test_mean_shift.py b/sklearn/cluster/tests/test_mean_shift.py
index 74d4b91efee3..657682c9c04d 100644
--- a/sklearn/cluster/tests/test_mean_shift.py
+++ b/sklearn/cluster/tests/test_mean_shift.py
@@ -6,6 +6,8 @@
 import numpy as np
 import warnings
 
+from scipy import sparse
+
 from sklearn.utils.testing import assert_equal
 from sklearn.utils.testing import assert_false
 from sklearn.utils.testing import assert_true
@@ -47,6 +49,13 @@ def test_mean_shift():
     assert_equal(n_clusters_, n_clusters)
 
 
+def test_estimate_bandwidth_with_sparse_matrix():
+    # Test estimate_bandwidth with sparse matrix
+    X = sparse.lil_matrix((1000, 1000))
+    msg = "A sparse matrix was passed, but dense data is required."
+    assert_raise_message(TypeError, msg, estimate_bandwidth, X, 200)
+
+
 def test_parallel():
     ms1 = MeanShift(n_jobs=2)
     ms1.fit(X)
diff --git a/sklearn/covariance/robust_covariance.py b/sklearn/covariance/robust_covariance.py
index 29cbd52e183d..fdf0225dbdad 100644
--- a/sklearn/covariance/robust_covariance.py
+++ b/sklearn/covariance/robust_covariance.py
@@ -55,9 +55,11 @@ def c_step(X, n_support, remaining_iterations=30, initial_estimates=None,
     verbose : boolean, optional
         Verbose mode.
 
-    random_state : integer or numpy.RandomState, optional
-        The random generator used. If an integer is given, it fixes the
-        seed. Defaults to the global numpy random number generator.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     cov_computation_method : callable, default empirical_covariance
         The function which will be used to compute the covariance.
@@ -214,9 +216,11 @@ def select_candidates(X, n_support, n_trials, select=1, n_iter=30,
         Maximum number of iterations for the c_step procedure.
         (2 is enough to be close to the final solution. "Never" exceeds 20).
 
-    random_state : integer or numpy.RandomState, default None
-        The random generator used. If an integer is given, it fixes the
-        seed. Defaults to the global numpy random number generator.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     cov_computation_method : callable, default empirical_covariance
         The function which will be used to compute the covariance.
@@ -311,10 +315,11 @@ def fast_mcd(X, support_fraction=None,
           value of support_fraction will be used within the algorithm:
           `[n_sample + n_features + 1] / 2`.
 
-    random_state : integer or numpy.RandomState, optional
-        The generator used to randomly subsample. If an integer is
-        given, it fixes the seed. Defaults to the global numpy random
-        number generator.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     cov_computation_method : callable, default empirical_covariance
         The function which will be used to compute the covariance.
@@ -531,9 +536,11 @@ class MinCovDet(EmpiricalCovariance):
         value of support_fraction will be used within the algorithm:
         [n_sample + n_features + 1] / 2
 
-    random_state : integer or numpy.RandomState, optional
-        The random generator used. If an integer is given, it fixes the
-        seed. Defaults to the global numpy random number generator.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     Attributes
     ----------
diff --git a/sklearn/cross_validation.py b/sklearn/cross_validation.py
index 03c74b88f5f2..d56845637fc4 100644
--- a/sklearn/cross_validation.py
+++ b/sklearn/cross_validation.py
@@ -1,4 +1,3 @@
-
 """
 The :mod:`sklearn.cross_validation` module includes utilities for cross-
 validation and performance evaluation.
@@ -297,9 +296,11 @@ class KFold(_BaseKFold):
     shuffle : boolean, optional
         Whether to shuffle the data before splitting into batches.
 
-    random_state : None, int or RandomState
-        When shuffle=True, pseudo-random number generator state used for
-        shuffling. If None, use default numpy RNG for shuffling.
+    random_state : int, RandomState instance or None, optional, default=None
+        If int, random_state is the seed used by the random number
+        generator; If RandomState instance, random_state is the random number
+        generator; If None, the random number generator is the RandomState
+        instance used by `np.random`. Used when ``shuffle`` == True.
 
     Examples
     --------
@@ -499,9 +500,11 @@ class StratifiedKFold(_BaseKFold):
         Whether to shuffle each stratification of the data before splitting
         into batches.
 
-    random_state : None, int or RandomState
-        When shuffle=True, pseudo-random number generator state used for
-        shuffling. If None, use default numpy RNG for shuffling.
+    random_state : int, RandomState instance or None, optional, default=None
+        If int, random_state is the seed used by the random number
+        generator; If RandomState instance, random_state is the random number
+        generator; If None, the random number generator is the RandomState
+        instance used by `np.random`. Used when ``shuffle`` == True.
 
     Examples
     --------
@@ -822,8 +825,11 @@ class ShuffleSplit(BaseShuffleSplit):
         int, represents the absolute number of train samples. If None,
         the value is automatically set to the complement of the test size.
 
-    random_state : int or RandomState
-        Pseudo-random number generator state used for random sampling.
+    random_state : int, RandomState instance or None, optional (default None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     Examples
     --------
@@ -1031,8 +1037,11 @@ class StratifiedShuffleSplit(BaseShuffleSplit):
         int, represents the absolute number of train samples. If None,
         the value is automatically set to the complement of the test size.
 
-    random_state : int or RandomState
-        Pseudo-random number generator state used for random sampling.
+    random_state : int, RandomState instance or None, optional (default None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     Examples
     --------
@@ -1225,8 +1234,11 @@ class LabelShuffleSplit(ShuffleSplit):
         int, represents the absolute number of train labels. If None,
         the value is automatically set to the complement of the test size.
 
-    random_state : int or RandomState
-        Pseudo-random number generator state used for random sampling.
+    random_state : int, RandomState instance or None, optional (default None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     """
     def __init__(self, labels, n_iter=5, test_size=0.2, train_size=None,
@@ -1889,9 +1901,11 @@ def permutation_test_score(estimator, X, y, cv=None,
         Labels constrain the permutation among groups of samples with
         a same label.
 
-    random_state : RandomState or an int seed (0 by default)
-        A random number generator instance to define the state of the
-        random permutations generator.
+    random_state : int, RandomState instance or None, optional (default=0)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     verbose : integer, optional
         The verbosity level.
@@ -1905,11 +1919,14 @@ def permutation_test_score(estimator, X, y, cv=None,
         The scores obtained for each permutations.
 
     pvalue : float
-        The returned value equals p-value if `scoring` returns bigger
-        numbers for better scores (e.g., accuracy_score). If `scoring` is
-        rather a loss function (i.e. when lower is better such as with
-        `mean_squared_error`) then this is actually the complement of the
-        p-value:  1 - p-value.
+        The p-value, which approximates the probability that the score would
+        be obtained by chance. This is calculated as:
+
+        `(C + 1) / (n_permutations + 1)`
+
+        Where C is the number of permutations whose score >= the true score.
+
+        The best possible p-value is 1/(n_permutations + 1), the worst is 1.0.
 
     Notes
     -----
@@ -1974,8 +1991,11 @@ def train_test_split(*arrays, **options):
         int, represents the absolute number of train samples. If None,
         the value is automatically set to the complement of the test size.
 
-    random_state : int or RandomState
-        Pseudo-random number generator state used for random sampling.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     stratify : array-like or None (default is None)
         If not None, data is split in a stratified fashion, using this as
diff --git a/sklearn/datasets/base.py b/sklearn/datasets/base.py
index 2325d971428d..2ad2bdb16cbf 100644
--- a/sklearn/datasets/base.py
+++ b/sklearn/datasets/base.py
@@ -20,58 +20,13 @@
 from os.path import splitext
 from os import listdir
 from os import makedirs
+from ..utils import Bunch
 
 import numpy as np
 
 from ..utils import check_random_state
 
 
-class Bunch(dict):
-    """Container object for datasets
-
-    Dictionary-like object that exposes its keys as attributes.
-
-    >>> b = Bunch(a=1, b=2)
-    >>> b['b']
-    2
-    >>> b.b
-    2
-    >>> b.a = 3
-    >>> b['a']
-    3
-    >>> b.c = 6
-    >>> b['c']
-    6
-
-    """
-
-    def __init__(self, **kwargs):
-        super(Bunch, self).__init__(kwargs)
-
-    def __setattr__(self, key, value):
-        self[key] = value
-
-    def __dir__(self):
-        return self.keys()
-
-    def __getattr__(self, key):
-        try:
-            return self[key]
-        except KeyError:
-            raise AttributeError(key)
-
-    def __setstate__(self, state):
-        # Bunch pickles generated with scikit-learn 0.16.* have an non
-        # empty __dict__. This causes a surprising behaviour when
-        # loading these pickles scikit-learn 0.17: reading bunch.key
-        # uses __dict__ but assigning to bunch.key use __setattr__ and
-        # only changes bunch['key']. More details can be found at:
-        # https://github.com/scikit-learn/scikit-learn/issues/6196.
-        # Overriding __setstate__ to be a noop has the effect of
-        # ignoring the pickled __dict__
-        pass
-
-
 def get_data_home(data_home=None):
     """Return the path of the scikit-learn data dir.
 
diff --git a/sklearn/datasets/california_housing.py b/sklearn/datasets/california_housing.py
index 8a74ad9e60e3..a9f21510b0f0 100644
--- a/sklearn/datasets/california_housing.py
+++ b/sklearn/datasets/california_housing.py
@@ -35,7 +35,8 @@
 
 import numpy as np
 
-from .base import get_data_home, Bunch
+from .base import get_data_home
+from ..utils import Bunch
 from .base import _pkl_filepath
 from ..externals import joblib
 
@@ -57,7 +58,7 @@ def fetch_california_housing(data_home=None, download_if_missing=True):
     ----------
     data_home : optional, default: None
         Specify another download and cache folder for the datasets. By default
-        all scikit learn data is stored in '~/scikit_learn_data' subfolders.
+        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.
 
     download_if_missing : optional, True by default
         If False, raise a IOError if the data is not locally available
diff --git a/sklearn/datasets/covtype.py b/sklearn/datasets/covtype.py
index 6e0b4d2d0d21..6d34cd35754f 100644
--- a/sklearn/datasets/covtype.py
+++ b/sklearn/datasets/covtype.py
@@ -26,7 +26,7 @@
 import numpy as np
 
 from .base import get_data_home
-from .base import Bunch
+from ..utils import Bunch
 from .base import _pkl_filepath
 from ..utils.fixes import makedirs
 from ..externals import joblib
@@ -50,7 +50,7 @@ def fetch_covtype(data_home=None, download_if_missing=True,
     ----------
     data_home : string, optional
         Specify another download and cache folder for the datasets. By default
-        all scikit learn data is stored in '~/scikit_learn_data' subfolders.
+        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.
 
     download_if_missing : boolean, default=True
         If False, raise a IOError if the data is not locally available
diff --git a/sklearn/datasets/kddcup99.py b/sklearn/datasets/kddcup99.py
index c2ed39caa10a..762ca58a6355 100644
--- a/sklearn/datasets/kddcup99.py
+++ b/sklearn/datasets/kddcup99.py
@@ -23,7 +23,7 @@
 import numpy as np
 
 from .base import get_data_home
-from .base import Bunch
+from ..utils import Bunch
 from ..externals import joblib, six
 from ..utils import check_random_state
 from ..utils import shuffle as shuffle_method
@@ -228,7 +228,7 @@ def _fetch_brute_kddcup99(subset=None, data_home=None,
 
     data_home : string, optional
         Specify another download and cache folder for the datasets. By default
-        all scikit learn data is stored in '~/scikit_learn_data' subfolders.
+        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.
 
     download_if_missing : boolean, default=True
         If False, raise a IOError if the data is not locally available
diff --git a/sklearn/datasets/lfw.py b/sklearn/datasets/lfw.py
index 13aaed805b4f..50834f7705ef 100644
--- a/sklearn/datasets/lfw.py
+++ b/sklearn/datasets/lfw.py
@@ -34,7 +34,8 @@
 except ImportError:
     import urllib
 
-from .base import get_data_home, Bunch
+from .base import get_data_home
+from ..utils import Bunch
 from ..externals.joblib import Memory
 
 from ..externals.six import b
@@ -250,7 +251,7 @@ def fetch_lfw_people(data_home=None, funneled=True, resize=0.5,
     ----------
     data_home : optional, default: None
         Specify another download and cache folder for the datasets. By default
-        all scikit learn data is stored in '~/scikit_learn_data' subfolders.
+        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.
 
     funneled : boolean, optional, default: True
         Download and use the funneled variant of the dataset.
@@ -413,7 +414,7 @@ def fetch_lfw_pairs(subset='train', data_home=None, funneled=True, resize=0.5,
 
     data_home : optional, default: None
         Specify another download and cache folder for the datasets. By
-        default all scikit learn data is stored in '~/scikit_learn_data'
+        default all scikit-learn data is stored in '~/scikit_learn_data'
         subfolders.
 
     funneled : boolean, optional, default: True
diff --git a/sklearn/datasets/mldata.py b/sklearn/datasets/mldata.py
index 82ae9858e9df..9b4a8e3a1daa 100644
--- a/sklearn/datasets/mldata.py
+++ b/sklearn/datasets/mldata.py
@@ -23,7 +23,8 @@
 from scipy import io
 from shutil import copyfileobj
 
-from .base import get_data_home, Bunch
+from .base import get_data_home
+from ..utils import Bunch
 
 MLDATA_BASE_URL = "http://mldata.org/repository/data/download/matlab/%s"
 
@@ -77,7 +78,7 @@ def fetch_mldata(dataname, target_name='label', data_name='data',
 
     data_home : optional, default: None
         Specify another download and cache folder for the data sets. By default
-        all scikit learn data is stored in '~/scikit_learn_data' subfolders.
+        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.
 
     Returns
     -------
diff --git a/sklearn/datasets/olivetti_faces.py b/sklearn/datasets/olivetti_faces.py
index 5f3af040dc1a..ac80d49e937d 100644
--- a/sklearn/datasets/olivetti_faces.py
+++ b/sklearn/datasets/olivetti_faces.py
@@ -37,9 +37,9 @@
 import numpy as np
 from scipy.io.matlab import loadmat
 
-from .base import get_data_home, Bunch
+from .base import get_data_home
 from .base import _pkl_filepath
-from ..utils import check_random_state
+from ..utils import check_random_state, Bunch
 from ..externals import joblib
 
 
@@ -61,7 +61,7 @@ def fetch_olivetti_faces(data_home=None, shuffle=False, random_state=0,
     ----------
     data_home : optional, default: None
         Specify another download and cache folder for the datasets. By default
-        all scikit learn data is stored in '~/scikit_learn_data' subfolders.
+        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.
 
     shuffle : boolean, optional
         If True the order of the dataset is shuffled to avoid having
@@ -71,19 +71,23 @@ def fetch_olivetti_faces(data_home=None, shuffle=False, random_state=0,
         If False, raise a IOError if the data is not locally available
         instead of trying to download the data from the source site.
 
-    random_state : optional, integer or RandomState object
-        The seed or the random number generator used to shuffle the
-        data.
+    random_state : int, RandomState instance or None, optional (default=0)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     Returns
     -------
     An object with the following attributes:
 
     data : numpy array of shape (400, 4096)
-        Each row corresponds to a ravelled face image of original size 64 x 64 pixels.
+        Each row corresponds to a ravelled face image of original size
+        64 x 64 pixels.
 
     images : numpy array of shape (400, 64, 64)
-        Each row is a face image corresponding to one of the 40 subjects of the dataset.
+        Each row is a face image corresponding to one of the 40 subjects
+        of the dataset.
 
     target : numpy array of shape (400, )
         Labels associated to each face image. Those labels are ranging from
diff --git a/sklearn/datasets/rcv1.py b/sklearn/datasets/rcv1.py
index 83b4d223cc36..0933155c0afc 100644
--- a/sklearn/datasets/rcv1.py
+++ b/sklearn/datasets/rcv1.py
@@ -20,12 +20,12 @@
 import scipy.sparse as sp
 
 from .base import get_data_home
-from .base import Bunch
 from .base import _pkl_filepath
 from ..utils.fixes import makedirs
 from ..externals import joblib
 from .svmlight_format import load_svmlight_files
 from ..utils import shuffle as shuffle_
+from ..utils import Bunch
 
 
 URL = ('http://jmlr.csail.mit.edu/papers/volume5/lewis04a/'
@@ -57,7 +57,7 @@ def fetch_rcv1(data_home=None, subset='all', download_if_missing=True,
     ----------
     data_home : string, optional
         Specify another download and cache folder for the datasets. By default
-        all scikit learn data is stored in '~/scikit_learn_data' subfolders.
+        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.
 
     subset : string, 'train', 'test', or 'all', default='all'
         Select the dataset to load: 'train' for the training set
diff --git a/sklearn/datasets/samples_generator.py b/sklearn/datasets/samples_generator.py
index e7f61b322733..7a4543aa2068 100644
--- a/sklearn/datasets/samples_generator.py
+++ b/sklearn/datasets/samples_generator.py
@@ -1059,8 +1059,11 @@ def make_sparse_coded_signal(n_samples, n_components, n_features,
     n_nonzero_coefs : int
         number of active (non-zero) coefficients in each sample
 
-    random_state : int or RandomState instance, optional (default=None)
-        seed used by the pseudo random number generator
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     Returns
     -------
diff --git a/sklearn/datasets/species_distributions.py b/sklearn/datasets/species_distributions.py
index 330c535620b7..14b5b38d0cd3 100644
--- a/sklearn/datasets/species_distributions.py
+++ b/sklearn/datasets/species_distributions.py
@@ -50,7 +50,8 @@
 
 import numpy as np
 
-from sklearn.datasets.base import get_data_home, Bunch
+from sklearn.datasets.base import get_data_home
+from ..utils import Bunch
 from sklearn.datasets.base import _pkl_filepath
 from sklearn.externals import joblib
 
@@ -139,7 +140,7 @@ def fetch_species_distributions(data_home=None,
     ----------
     data_home : optional, default: None
         Specify another download and cache folder for the datasets. By default
-        all scikit learn data is stored in '~/scikit_learn_data' subfolders.
+        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.
 
     download_if_missing : optional, True by default
         If False, raise a IOError if the data is not locally available
diff --git a/sklearn/datasets/twenty_newsgroups.py b/sklearn/datasets/twenty_newsgroups.py
index 128610fd2830..47b543d8d2e1 100644
--- a/sklearn/datasets/twenty_newsgroups.py
+++ b/sklearn/datasets/twenty_newsgroups.py
@@ -47,10 +47,9 @@
 import scipy.sparse as sp
 
 from .base import get_data_home
-from .base import Bunch
 from .base import load_files
 from .base import _pkl_filepath
-from ..utils import check_random_state
+from ..utils import check_random_state, Bunch
 from ..feature_extraction.text import CountVectorizer
 from ..preprocessing import normalize
 from ..externals import joblib, six
diff --git a/sklearn/decomposition/dict_learning.py b/sklearn/decomposition/dict_learning.py
index baf79544dd17..154987a6279c 100644
--- a/sklearn/decomposition/dict_learning.py
+++ b/sklearn/decomposition/dict_learning.py
@@ -328,8 +328,11 @@ def _update_dict(dictionary, Y, code, verbose=False, return_r2=False,
         Whether to compute and return the residual sum of squares corresponding
         to the computed solution.
 
-    random_state : int or RandomState
-        Pseudo number generator state used for random sampling.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     Returns
     -------
@@ -434,8 +437,11 @@ def dict_learning(X, n_components, alpha, max_iter=100, tol=1e-8,
     verbose :
         Degree of output the procedure will print.
 
-    random_state : int or RandomState
-        Pseudo number generator state used for random sampling.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     return_n_iter : bool
         Whether or not to return the number of iterations.
@@ -616,8 +622,11 @@ def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,
         Number of previous iterations completed on the dictionary used for
         initialization.
 
-    random_state : int or RandomState
-        Pseudo number generator state used for random sampling.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     return_inner_stats : boolean, optional
         Return the inner statistics A (dictionary covariance) and B
@@ -1000,8 +1009,11 @@ class DictionaryLearning(BaseEstimator, SparseCodingMixin):
     verbose :
         degree of verbosity of the printed output
 
-    random_state : int or RandomState
-        Pseudo number generator state used for random sampling.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     Attributes
     ----------
@@ -1160,8 +1172,11 @@ class MiniBatchDictionaryLearning(BaseEstimator, SparseCodingMixin):
     shuffle : bool,
         whether to shuffle the samples before forming batches
 
-    random_state : int or RandomState
-        Pseudo number generator state used for random sampling.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     Attributes
     ----------
diff --git a/sklearn/decomposition/factor_analysis.py b/sklearn/decomposition/factor_analysis.py
index 16e198164a5c..3326ac197b3a 100644
--- a/sklearn/decomposition/factor_analysis.py
+++ b/sklearn/decomposition/factor_analysis.py
@@ -88,9 +88,11 @@ class FactorAnalysis(BaseEstimator, TransformerMixin):
         Number of iterations for the power method. 3 by default. Only used
         if ``svd_method`` equals 'randomized'
 
-    random_state : int or RandomState
-        Pseudo number generator state used for random sampling. Only used
-        if ``svd_method`` equals 'randomized'
+    random_state : int, RandomState instance or None, optional (default=0)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`. Only used when ``svd_method`` equals 'randomized'.
 
     Attributes
     ----------
diff --git a/sklearn/decomposition/fastica_.py b/sklearn/decomposition/fastica_.py
index fbbbbec1b713..3cca0b7d6e89 100644
--- a/sklearn/decomposition/fastica_.py
+++ b/sklearn/decomposition/fastica_.py
@@ -199,8 +199,11 @@ def my_g(x):
         Initial un-mixing array of dimension (n.comp,n.comp).
         If None (default) then an array of normal r.v.'s is used.
 
-    random_state : int or RandomState
-        Pseudo number generator state used for random sampling.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     return_X_mean : bool, optional
         If True, X_mean is returned too.
@@ -415,8 +418,11 @@ def my_g(x):
     w_init : None of an (n_components, n_components) ndarray
         The mixing matrix to be used to initialize the algorithm.
 
-    random_state : int or RandomState
-        Pseudo number generator state used for random sampling.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     Attributes
     ----------
diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py
index 1fb6b55f43aa..85dac42e2d47 100644
--- a/sklearn/decomposition/kernel_pca.py
+++ b/sklearn/decomposition/kernel_pca.py
@@ -35,7 +35,7 @@ class KernelPCA(BaseEstimator, TransformerMixin):
         Degree for poly kernels. Ignored by other kernels.
 
     gamma : float, default=1/n_features
-        Kernel coefficient for rbf and poly kernels. Ignored by other
+        Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other
         kernels.
 
     coef0 : float, default=1
@@ -74,9 +74,11 @@ class KernelPCA(BaseEstimator, TransformerMixin):
         When n_components is None, this parameter is ignored and components
         with zero eigenvalues are removed regardless.
 
-    random_state : int seed, RandomState instance, or None, default=None
-        A pseudo random number generator used for the initialization of the
-        residuals when eigen_solver == 'arpack'.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`. Used when ``eigen_solver`` == 'arpack'.
 
         .. versionadded:: 0.18
 
diff --git a/sklearn/decomposition/nmf.py b/sklearn/decomposition/nmf.py
index 63026e3ad43b..522bf150aa25 100644
--- a/sklearn/decomposition/nmf.py
+++ b/sklearn/decomposition/nmf.py
@@ -268,9 +268,11 @@ def _initialize_nmf(X, n_components, init=None, eps=1e-6,
     eps : float
         Truncate all values less then this in output to zero.
 
-    random_state : int seed, RandomState instance, or None (default)
-        Random number generator seed control, used in 'nndsvdar' and
-        'random' modes.
+    random_state : int, RandomState instance or None, optional, default: None
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`. Used when ``random`` == 'nndsvdar' or 'random'.
 
     Returns
     -------
@@ -445,8 +447,11 @@ def _fit_coordinate_descent(X, W, H, tol=1e-4, max_iter=200, l1_reg_W=0,
     shuffle : boolean, default: False
         If true, randomize the order of coordinates in the CD solver.
 
-    random_state : integer seed, RandomState instance, or None (default)
-        Random number generator seed control.
+    random_state : int, RandomState instance or None, optional, default: None
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     Returns
     -------
@@ -910,8 +915,11 @@ def non_negative_factorization(X, W=None, H=None, n_components=None,
         Select whether the regularization affects the components (H), the
         transformation (W), both or none of them.
 
-    random_state : integer seed, RandomState instance, or None (default)
-        Random number generator seed control.
+    random_state : int, RandomState instance or None, optional, default: None
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     verbose : integer, default: 0
         The verbosity level.
@@ -1097,8 +1105,11 @@ class NMF(BaseEstimator, TransformerMixin):
     max_iter : integer, default: 200
         Maximum number of iterations before timing out.
 
-    random_state : integer seed, RandomState instance, or None (default)
-        Random number generator seed control.
+    random_state : int, RandomState instance or None, optional, default: None
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     alpha : double, default: 0.
         Constant that multiplies the regularization terms. Set it to zero to
diff --git a/sklearn/decomposition/online_lda.py b/sklearn/decomposition/online_lda.py
index 8e0c5bfe6b41..d2b74a5fee85 100644
--- a/sklearn/decomposition/online_lda.py
+++ b/sklearn/decomposition/online_lda.py
@@ -219,8 +219,11 @@ class LatentDirichletAllocation(BaseEstimator, TransformerMixin):
     verbose : int, optional (default=0)
         Verbosity level.
 
-    random_state : int or RandomState instance or None, optional (default=None)
-        Pseudo-random number generator seed control.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     Attributes
     ----------
@@ -753,8 +756,8 @@ def perplexity(self, X, doc_topic_distr='deprecated', sub_sampling=False):
         Perplexity is defined as exp(-1. * log-likelihood per word)
 
         .. versionchanged:: 0.19
-           *doc_topic_distr* argument has been depricated because user no
-           longer has access to unnormalized distribution
+           *doc_topic_distr* argument has been deprecated and is ignored
+           because user no longer has access to unnormalized distribution
 
         Parameters
         ----------
@@ -763,7 +766,7 @@ def perplexity(self, X, doc_topic_distr='deprecated', sub_sampling=False):
 
         doc_topic_distr : None or array, shape=(n_samples, n_topics)
             Document topic distribution.
-            If it is None, it will be generated by applying transform on X.
+            This argument is deprecated and is currently being ignored.
 
             .. deprecated:: 0.19
 
@@ -773,8 +776,9 @@ def perplexity(self, X, doc_topic_distr='deprecated', sub_sampling=False):
             Perplexity score.
         """
         if doc_topic_distr != 'deprecated':
-            warnings.warn("Argument 'doc_topic_distr' is deprecated and will "
-                          "be ignored as of 0.19. Support for this argument "
-                          "will be removed in 0.21.", DeprecationWarning)
+            warnings.warn("Argument 'doc_topic_distr' is deprecated and is "
+                          "being ignored as of 0.19. Support for this "
+                          "argument will be removed in 0.21.",
+                          DeprecationWarning)
 
         return self._perplexity_precomp_distr(X, sub_sampling=sub_sampling)
diff --git a/sklearn/decomposition/pca.py b/sklearn/decomposition/pca.py
index a3abaa6217df..eb11d9b03210 100644
--- a/sklearn/decomposition/pca.py
+++ b/sklearn/decomposition/pca.py
@@ -183,9 +183,11 @@ class PCA(_BasePCA):
 
         .. versionadded:: 0.18.0
 
-    random_state : int or RandomState instance or None (default None)
-        Pseudo Random Number generator seed control. If None, use the
-        numpy.random singleton. Used by svd_solver == 'arpack' or 'randomized'.
+    random_state : int, RandomState instance or None, optional (default None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`. Used when ``svd_solver`` == 'arpack' or 'randomized'.
 
         .. versionadded:: 0.18.0
 
@@ -386,6 +388,9 @@ def _fit(self, X):
             return self._fit_full(X, n_components)
         elif svd_solver in ['arpack', 'randomized']:
             return self._fit_truncated(X, n_components, svd_solver)
+        else:
+            raise ValueError("Unrecognized svd_solver='{0}'"
+                             "".format(svd_solver))
 
     def _fit_full(self, X, n_components):
         """Fit the model by computing full SVD on X"""
@@ -598,9 +603,11 @@ class RandomizedPCA(BaseEstimator, TransformerMixin):
         improve the predictive accuracy of the downstream estimators by
         making their data respect some hard-wired assumptions.
 
-    random_state : int or RandomState instance or None (default)
-        Pseudo Random Number generator seed control. If None, use the
-        numpy.random singleton.
+    random_state : int, RandomState instance or None, optional, default=None
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     Attributes
     ----------
diff --git a/sklearn/decomposition/sparse_pca.py b/sklearn/decomposition/sparse_pca.py
index e6fde97ccb9d..23d1163fdc88 100644
--- a/sklearn/decomposition/sparse_pca.py
+++ b/sklearn/decomposition/sparse_pca.py
@@ -60,8 +60,11 @@ class SparsePCA(BaseEstimator, TransformerMixin):
     verbose :
         Degree of verbosity of the printed output.
 
-    random_state : int or RandomState
-        Pseudo number generator state used for random sampling.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     Attributes
     ----------
@@ -228,8 +231,11 @@ class MiniBatchSparsePCA(SparsePCA):
         Lasso solution (linear_model.Lasso). Lars will be faster if
         the estimated components are sparse.
 
-    random_state : int or RandomState
-        Pseudo number generator state used for random sampling.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     Attributes
     ----------
diff --git a/sklearn/decomposition/tests/test_pca.py b/sklearn/decomposition/tests/test_pca.py
index 5a9bcb756cbe..ea321089d719 100644
--- a/sklearn/decomposition/tests/test_pca.py
+++ b/sklearn/decomposition/tests/test_pca.py
@@ -584,3 +584,9 @@ def test_pca_spase_input():
         pca = PCA(n_components=3, svd_solver=svd_solver)
 
         assert_raises(TypeError, pca.fit, X)
+
+
+def test_pca_bad_solver():
+    X = np.random.RandomState(0).rand(5, 4)
+    pca = PCA(n_components=3, svd_solver='bad_argument')
+    assert_raises(ValueError, pca.fit, X)
diff --git a/sklearn/decomposition/truncated_svd.py b/sklearn/decomposition/truncated_svd.py
index 5d029d1205bd..7ab20926f958 100644
--- a/sklearn/decomposition/truncated_svd.py
+++ b/sklearn/decomposition/truncated_svd.py
@@ -59,9 +59,11 @@ class TruncatedSVD(BaseEstimator, TransformerMixin):
         The default is larger than the default in `randomized_svd` to handle
         sparse matrices that may have large slowly decaying spectrum.
 
-    random_state : int or RandomState, optional
-        (Seed for) pseudo-random number generator. If not given, the
-        numpy.random singleton is used.
+    random_state : int, RandomState instance or None, optional, default = None
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     tol : float, optional
         Tolerance for ARPACK. 0 means machine precision. Ignored by randomized
diff --git a/sklearn/dummy.py b/sklearn/dummy.py
index 84d42e7177a0..90a43791c81b 100644
--- a/sklearn/dummy.py
+++ b/sklearn/dummy.py
@@ -47,8 +47,11 @@ class DummyClassifier(BaseEstimator, ClassifierMixin):
              Dummy Classifier now supports prior fitting strategy using
              parameter *prior*.
 
-    random_state : int seed, RandomState instance, or None (default)
-        The seed of the pseudo random number generator to use.
+    random_state : int, RandomState instance or None, optional, default=None
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     constant : int or str or array of shape = [n_outputs]
         The explicit constant as predicted by the "constant" strategy. This
diff --git a/sklearn/ensemble/base.py b/sklearn/ensemble/base.py
index 165124d62428..5e9d6e2e1fc3 100644
--- a/sklearn/ensemble/base.py
+++ b/sklearn/ensemble/base.py
@@ -29,8 +29,11 @@ def _set_random_states(estimator, random_state=None):
         Estimator with potential randomness managed by random_state
         parameters.
 
-    random_state : numpy.RandomState or int, optional
-        Random state used to generate integer values.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     Notes
     -----
diff --git a/sklearn/ensemble/forest.py b/sklearn/ensemble/forest.py
index 5db9e2adb411..518dfc76ce59 100644
--- a/sklearn/ensemble/forest.py
+++ b/sklearn/ensemble/forest.py
@@ -374,6 +374,19 @@ def feature_importances_(self):
         return sum(all_importances) / len(self.estimators_)
 
 
+# This is a utility function for joblib's Parallel. It can't go locally in
+# ForestClassifier or ForestRegressor, because joblib complains that it cannot
+# pickle it when placed there.
+
+def accumulate_prediction(predict, X, out):
+    prediction = predict(X, check_input=False)
+    if len(out) == 1:
+        out[0] += prediction
+    else:
+        for i in range(len(out)):
+            out[i] += prediction[i]
+
+
 class ForestClassifier(six.with_metaclass(ABCMeta, BaseForest,
                                           ClassifierMixin)):
     """Base class for forest of trees-based classifiers.
@@ -565,31 +578,20 @@ class in a leaf.
         # Assign chunk of trees to jobs
         n_jobs, _, _ = _partition_estimators(self.n_estimators, self.n_jobs)
 
-        # Parallel loop
-        all_proba = Parallel(n_jobs=n_jobs, verbose=self.verbose,
-                             backend="threading")(
-            delayed(parallel_helper)(e, 'predict_proba', X,
-                                      check_input=False)
+        # avoid storing the output of every estimator by summing them here
+        all_proba = [np.zeros((X.shape[0], j), dtype=np.float64)
+                     for j in np.atleast_1d(self.n_classes_)]
+        Parallel(n_jobs=n_jobs, verbose=self.verbose, backend="threading")(
+            delayed(accumulate_prediction)(e.predict_proba, X, all_proba)
             for e in self.estimators_)
 
-        # Reduce
-        proba = all_proba[0]
-
-        if self.n_outputs_ == 1:
-            for j in range(1, len(all_proba)):
-                proba += all_proba[j]
-
+        for proba in all_proba:
             proba /= len(self.estimators_)
 
+        if len(all_proba) == 1:
+            return all_proba[0]
         else:
-            for j in range(1, len(all_proba)):
-                for k in range(self.n_outputs_):
-                    proba[k] += all_proba[j][k]
-
-            for k in range(self.n_outputs_):
-                proba[k] /= self.n_estimators
-
-        return proba
+            return all_proba
 
     def predict_log_proba(self, X):
         """Predict class log-probabilities for X.
@@ -678,14 +680,18 @@ def predict(self, X):
         # Assign chunk of trees to jobs
         n_jobs, _, _ = _partition_estimators(self.n_estimators, self.n_jobs)
 
+        # avoid storing the output of every estimator by summing them here
+        if self.n_outputs_ > 1:
+            y_hat = np.zeros((X.shape[0], self.n_outputs_), dtype=np.float64)
+        else:
+            y_hat = np.zeros((X.shape[0]), dtype=np.float64)
+
         # Parallel loop
-        all_y_hat = Parallel(n_jobs=n_jobs, verbose=self.verbose,
-                             backend="threading")(
-            delayed(parallel_helper)(e, 'predict', X, check_input=False)
+        Parallel(n_jobs=n_jobs, verbose=self.verbose, backend="threading")(
+            delayed(accumulate_prediction)(e.predict, X, [y_hat])
             for e in self.estimators_)
 
-        # Reduce
-        y_hat = sum(all_y_hat) / len(self.estimators_)
+        y_hat /= len(self.estimators_)
 
         return y_hat
 
@@ -807,11 +813,23 @@ class RandomForestClassifier(ForestClassifier):
         Best nodes are defined as relative reduction in impurity.
         If None then unlimited number of leaf nodes.
 
-    min_impurity_split : float, optional (default=1e-7)
-        Threshold for early stopping in tree growth. A node will split
-        if its impurity is above the threshold, otherwise it is a leaf.
+    min_impurity_decrease : float, optional (default=0.)
+        A node will be split if this split induces a decrease of the impurity
+        greater than or equal to this value.
 
-        .. versionadded:: 0.18
+        The weighted impurity decrease equation is the following::
+
+            N_t / N * (impurity - N_t_R / N_t * right_impurity
+                                - N_t_L / N_t * left_impurity)
+
+        where ``N`` is the total number of samples, ``N_t`` is the number of
+        samples at the current node, ``N_t_L`` is the number of samples in the
+        left child, and ``N_t_R`` is the number of samples in the right child.
+
+        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
+        if ``sample_weight`` is passed.
+
+        .. versionadded:: 0.19
 
     bootstrap : boolean, optional (default=True)
         Whether bootstrap samples are used when building trees.
@@ -891,6 +909,12 @@ class labels (multi-output problem).
 
     Notes
     -----
+    The default values for the parameters controlling the size of the trees
+    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
+    unpruned trees which can potentially be very large on some data sets. To
+    reduce memory consumption, the complexity and size of the trees should be
+    controlled by setting those parameter values.
+
     The features are always randomly permuted at each split. Therefore,
     the best found split may vary, even with the same training data,
     ``max_features=n_features`` and ``bootstrap=False``, if the improvement
@@ -916,7 +940,8 @@ def __init__(self,
                  min_weight_fraction_leaf=0.,
                  max_features="auto",
                  max_leaf_nodes=None,
-                 min_impurity_split=1e-7,
+                 min_impurity_decrease=0.,
+                 min_impurity_split=None,
                  bootstrap=True,
                  oob_score=False,
                  n_jobs=1,
@@ -929,7 +954,8 @@ def __init__(self,
             n_estimators=n_estimators,
             estimator_params=("criterion", "max_depth", "min_samples_split",
                               "min_samples_leaf", "min_weight_fraction_leaf",
-                              "max_features", "max_leaf_nodes", "min_impurity_split",
+                              "max_features", "max_leaf_nodes",
+                              "min_impurity_decrease", "min_impurity_split",
                               "random_state"),
             bootstrap=bootstrap,
             oob_score=oob_score,
@@ -946,6 +972,7 @@ def __init__(self,
         self.min_weight_fraction_leaf = min_weight_fraction_leaf
         self.max_features = max_features
         self.max_leaf_nodes = max_leaf_nodes
+        self.min_impurity_decrease = min_impurity_decrease
         self.min_impurity_split = min_impurity_split
 
 
@@ -1028,11 +1055,23 @@ class RandomForestRegressor(ForestRegressor):
         Best nodes are defined as relative reduction in impurity.
         If None then unlimited number of leaf nodes.
 
-    min_impurity_split : float, optional (default=1e-7)
-        Threshold for early stopping in tree growth. A node will split
-        if its impurity is above the threshold, otherwise it is a leaf.
+    min_impurity_decrease : float, optional (default=0.)
+        A node will be split if this split induces a decrease of the impurity
+        greater than or equal to this value.
 
-        .. versionadded:: 0.18
+        The weighted impurity decrease equation is the following::
+
+            N_t / N * (impurity - N_t_R / N_t * right_impurity
+                                - N_t_L / N_t * left_impurity)
+
+        where ``N`` is the total number of samples, ``N_t`` is the number of
+        samples at the current node, ``N_t_L`` is the number of samples in the
+        left child, and ``N_t_R`` is the number of samples in the right child.
+
+        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
+        if ``sample_weight`` is passed.
+
+        .. versionadded:: 0.19
 
     bootstrap : boolean, optional (default=True)
         Whether bootstrap samples are used when building trees.
@@ -1081,6 +1120,12 @@ class RandomForestRegressor(ForestRegressor):
 
     Notes
     -----
+    The default values for the parameters controlling the size of the trees
+    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
+    unpruned trees which can potentially be very large on some data sets. To
+    reduce memory consumption, the complexity and size of the trees should be
+    controlled by setting those parameter values.
+
     The features are always randomly permuted at each split. Therefore,
     the best found split may vary, even with the same training data,
     ``max_features=n_features`` and ``bootstrap=False``, if the improvement
@@ -1106,7 +1151,8 @@ def __init__(self,
                  min_weight_fraction_leaf=0.,
                  max_features="auto",
                  max_leaf_nodes=None,
-                 min_impurity_split=1e-7,
+                 min_impurity_decrease=0.,
+                 min_impurity_split=None,
                  bootstrap=True,
                  oob_score=False,
                  n_jobs=1,
@@ -1118,7 +1164,8 @@ def __init__(self,
             n_estimators=n_estimators,
             estimator_params=("criterion", "max_depth", "min_samples_split",
                               "min_samples_leaf", "min_weight_fraction_leaf",
-                              "max_features", "max_leaf_nodes", "min_impurity_split",
+                              "max_features", "max_leaf_nodes",
+                              "min_impurity_decrease", "min_impurity_split",
                               "random_state"),
             bootstrap=bootstrap,
             oob_score=oob_score,
@@ -1134,6 +1181,7 @@ def __init__(self,
         self.min_weight_fraction_leaf = min_weight_fraction_leaf
         self.max_features = max_features
         self.max_leaf_nodes = max_leaf_nodes
+        self.min_impurity_decrease = min_impurity_decrease
         self.min_impurity_split = min_impurity_split
 
 
@@ -1209,11 +1257,23 @@ class ExtraTreesClassifier(ForestClassifier):
         Best nodes are defined as relative reduction in impurity.
         If None then unlimited number of leaf nodes.
 
-    min_impurity_split : float, optional (default=1e-7)
-        Threshold for early stopping in tree growth. A node will split
-        if its impurity is above the threshold, otherwise it is a leaf.
+    min_impurity_decrease : float, optional (default=0.)
+        A node will be split if this split induces a decrease of the impurity
+        greater than or equal to this value.
 
-        .. versionadded:: 0.18
+        The weighted impurity decrease equation is the following::
+
+            N_t / N * (impurity - N_t_R / N_t * right_impurity
+                                - N_t_L / N_t * left_impurity)
+
+        where ``N`` is the total number of samples, ``N_t`` is the number of
+        samples at the current node, ``N_t_L`` is the number of samples in the
+        left child, and ``N_t_R`` is the number of samples in the right child.
+
+        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
+        if ``sample_weight`` is passed.
+
+        .. versionadded:: 0.19
 
     bootstrap : boolean, optional (default=False)
         Whether bootstrap samples are used when building trees.
@@ -1289,6 +1349,14 @@ class labels (multi-output problem).
         was never left out during the bootstrap. In this case,
         `oob_decision_function_` might contain NaN.
 
+    Notes
+    -----
+    The default values for the parameters controlling the size of the trees
+    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
+    unpruned trees which can potentially be very large on some data sets. To
+    reduce memory consumption, the complexity and size of the trees should be
+    controlled by setting those parameter values.
+
     References
     ----------
 
@@ -1310,7 +1378,8 @@ def __init__(self,
                  min_weight_fraction_leaf=0.,
                  max_features="auto",
                  max_leaf_nodes=None,
-                 min_impurity_split=1e-7,
+                 min_impurity_decrease=0.,
+                 min_impurity_split=None,
                  bootstrap=False,
                  oob_score=False,
                  n_jobs=1,
@@ -1323,7 +1392,8 @@ def __init__(self,
             n_estimators=n_estimators,
             estimator_params=("criterion", "max_depth", "min_samples_split",
                               "min_samples_leaf", "min_weight_fraction_leaf",
-                              "max_features", "max_leaf_nodes", "min_impurity_split",
+                              "max_features", "max_leaf_nodes",
+                              "min_impurity_decrease", "min_impurity_split",
                               "random_state"),
             bootstrap=bootstrap,
             oob_score=oob_score,
@@ -1340,6 +1410,7 @@ def __init__(self,
         self.min_weight_fraction_leaf = min_weight_fraction_leaf
         self.max_features = max_features
         self.max_leaf_nodes = max_leaf_nodes
+        self.min_impurity_decrease = min_impurity_decrease
         self.min_impurity_split = min_impurity_split
 
 
@@ -1420,11 +1491,23 @@ class ExtraTreesRegressor(ForestRegressor):
         Best nodes are defined as relative reduction in impurity.
         If None then unlimited number of leaf nodes.
 
-    min_impurity_split : float, optional (default=1e-7)
-        Threshold for early stopping in tree growth. A node will split
-        if its impurity is above the threshold, otherwise it is a leaf.
+    min_impurity_decrease : float, optional (default=0.)
+        A node will be split if this split induces a decrease of the impurity
+        greater than or equal to this value.
 
-        .. versionadded:: 0.18
+        The weighted impurity decrease equation is the following::
+
+            N_t / N * (impurity - N_t_R / N_t * right_impurity
+                                - N_t_L / N_t * left_impurity)
+
+        where ``N`` is the total number of samples, ``N_t`` is the number of
+        samples at the current node, ``N_t_L`` is the number of samples in the
+        left child, and ``N_t_R`` is the number of samples in the right child.
+
+        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
+        if ``sample_weight`` is passed.
+
+        .. versionadded:: 0.19
 
     bootstrap : boolean, optional (default=False)
         Whether bootstrap samples are used when building trees.
@@ -1470,6 +1553,14 @@ class ExtraTreesRegressor(ForestRegressor):
     oob_prediction_ : array of shape = [n_samples]
         Prediction computed with out-of-bag estimate on the training set.
 
+    Notes
+    -----
+    The default values for the parameters controlling the size of the trees
+    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
+    unpruned trees which can potentially be very large on some data sets. To
+    reduce memory consumption, the complexity and size of the trees should be
+    controlled by setting those parameter values.
+
     References
     ----------
 
@@ -1490,7 +1581,8 @@ def __init__(self,
                  min_weight_fraction_leaf=0.,
                  max_features="auto",
                  max_leaf_nodes=None,
-                 min_impurity_split=1e-7,
+                 min_impurity_decrease=0.,
+                 min_impurity_split=None,
                  bootstrap=False,
                  oob_score=False,
                  n_jobs=1,
@@ -1502,7 +1594,8 @@ def __init__(self,
             n_estimators=n_estimators,
             estimator_params=("criterion", "max_depth", "min_samples_split",
                               "min_samples_leaf", "min_weight_fraction_leaf",
-                              "max_features", "max_leaf_nodes", "min_impurity_split",
+                              "max_features", "max_leaf_nodes",
+                              "min_impurity_decrease", "min_impurity_split",
                               "random_state"),
             bootstrap=bootstrap,
             oob_score=oob_score,
@@ -1518,6 +1611,7 @@ def __init__(self,
         self.min_weight_fraction_leaf = min_weight_fraction_leaf
         self.max_features = max_features
         self.max_leaf_nodes = max_leaf_nodes
+        self.min_impurity_decrease = min_impurity_decrease
         self.min_impurity_split = min_impurity_split
 
 
@@ -1578,11 +1672,26 @@ class RandomTreesEmbedding(BaseForest):
         Best nodes are defined as relative reduction in impurity.
         If None then unlimited number of leaf nodes.
 
-    min_impurity_split : float, optional (default=1e-7)
-        Threshold for early stopping in tree growth. A node will split
-        if its impurity is above the threshold, otherwise it is a leaf.
+    min_impurity_decrease : float, optional (default=0.)
+        A node will be split if this split induces a decrease of the impurity
+        greater than or equal to this value.
 
-        .. versionadded:: 0.18
+        The weighted impurity decrease equation is the following::
+
+            N_t / N * (impurity - N_t_R / N_t * right_impurity
+                                - N_t_L / N_t * left_impurity)
+
+        where ``N`` is the total number of samples, ``N_t`` is the number of
+        samples at the current node, ``N_t_L`` is the number of samples in the
+        left child, and ``N_t_R`` is the number of samples in the right child.
+
+        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
+        if ``sample_weight`` is passed.
+
+        .. versionadded:: 0.19
+
+    bootstrap : boolean, optional (default=True)
+        Whether bootstrap samples are used when building trees.
 
     sparse_output : bool, optional (default=True)
         Whether or not to return a sparse CSR matrix, as default behavior,
@@ -1628,7 +1737,8 @@ def __init__(self,
                  min_samples_leaf=1,
                  min_weight_fraction_leaf=0.,
                  max_leaf_nodes=None,
-                 min_impurity_split=1e-7,
+                 min_impurity_decrease=0.,
+                 min_impurity_split=None,
                  sparse_output=True,
                  n_jobs=1,
                  random_state=None,
@@ -1639,7 +1749,8 @@ def __init__(self,
             n_estimators=n_estimators,
             estimator_params=("criterion", "max_depth", "min_samples_split",
                               "min_samples_leaf", "min_weight_fraction_leaf",
-                              "max_features", "max_leaf_nodes", "min_impurity_split",
+                              "max_features", "max_leaf_nodes",
+                              "min_impurity_decrease", "min_impurity_split",
                               "random_state"),
             bootstrap=False,
             oob_score=False,
@@ -1655,6 +1766,7 @@ def __init__(self,
         self.min_weight_fraction_leaf = min_weight_fraction_leaf
         self.max_features = 1
         self.max_leaf_nodes = max_leaf_nodes
+        self.min_impurity_decrease = min_impurity_decrease
         self.min_impurity_split = min_impurity_split
         self.sparse_output = sparse_output
 
diff --git a/sklearn/ensemble/gradient_boosting.py b/sklearn/ensemble/gradient_boosting.py
index 563ef3c6b102..4db93f0a5171 100644
--- a/sklearn/ensemble/gradient_boosting.py
+++ b/sklearn/ensemble/gradient_boosting.py
@@ -510,7 +510,8 @@ def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
         numerator = np.sum(sample_weight * residual)
         denominator = np.sum(sample_weight * (y - residual) * (1 - y + residual))
 
-        if denominator == 0.0:
+        # prevents overflow and division by zero
+        if abs(denominator) < 1e-150:
             tree.value[leaf, 0, 0] = 0.0
         else:
             tree.value[leaf, 0, 0] = numerator / denominator
@@ -576,7 +577,8 @@ def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
         denominator = np.sum(sample_weight * (y - residual) *
                              (1.0 - y + residual))
 
-        if denominator == 0.0:
+        # prevents overflow and division by zero
+        if abs(denominator) < 1e-150:
             tree.value[leaf, 0, 0] = 0.0
         else:
             tree.value[leaf, 0, 0] = numerator / denominator
@@ -633,7 +635,8 @@ def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
         numerator = np.sum(y_ * sample_weight * np.exp(-y_ * pred))
         denominator = np.sum(sample_weight * np.exp(-y_ * pred))
 
-        if denominator == 0.0:
+        # prevents overflow and division by zero
+        if abs(denominator) < 1e-150:
             tree.value[leaf, 0, 0] = 0.0
         else:
             tree.value[leaf, 0, 0] = numerator / denominator
@@ -720,7 +723,8 @@ class BaseGradientBoosting(six.with_metaclass(ABCMeta, BaseEnsemble)):
     @abstractmethod
     def __init__(self, loss, learning_rate, n_estimators, criterion,
                  min_samples_split, min_samples_leaf, min_weight_fraction_leaf,
-                 max_depth, min_impurity_split, init, subsample, max_features,
+                 max_depth, min_impurity_decrease, min_impurity_split,
+                 init, subsample, max_features,
                  random_state, alpha=0.9, verbose=0, max_leaf_nodes=None,
                  warm_start=False, presort='auto'):
 
@@ -734,6 +738,7 @@ def __init__(self, loss, learning_rate, n_estimators, criterion,
         self.subsample = subsample
         self.max_features = max_features
         self.max_depth = max_depth
+        self.min_impurity_decrease = min_impurity_decrease
         self.min_impurity_split = min_impurity_split
         self.init = init
         self.random_state = random_state
@@ -766,6 +771,7 @@ def _fit_stage(self, i, X, y, y_pred, sample_weight, sample_mask,
                 min_samples_split=self.min_samples_split,
                 min_samples_leaf=self.min_samples_leaf,
                 min_weight_fraction_leaf=self.min_weight_fraction_leaf,
+                min_impurity_decrease=self.min_impurity_decrease,
                 min_impurity_split=self.min_impurity_split,
                 max_features=self.max_features,
                 max_leaf_nodes=self.max_leaf_nodes,
@@ -1321,11 +1327,23 @@ class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):
         Best nodes are defined as relative reduction in impurity.
         If None then unlimited number of leaf nodes.
 
-    min_impurity_split : float, optional (default=1e-7)
-        Threshold for early stopping in tree growth. A node will split
-        if its impurity is above the threshold, otherwise it is a leaf.
+    min_impurity_decrease : float, optional (default=0.)
+        A node will be split if this split induces a decrease of the impurity
+        greater than or equal to this value.
 
-        .. versionadded:: 0.18
+        The weighted impurity decrease equation is the following::
+
+            N_t / N * (impurity - N_t_R / N_t * right_impurity
+                                - N_t_L / N_t * left_impurity)
+
+        where ``N`` is the total number of samples, ``N_t`` is the number of
+        samples at the current node, ``N_t_L`` is the number of samples in the
+        left child, and ``N_t_R`` is the number of samples in the right child.
+
+        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
+        if ``sample_weight`` is passed.
+
+        .. versionadded:: 0.19
 
     init : BaseEstimator, None, optional (default=None)
         An estimator object that is used to compute the initial
@@ -1414,7 +1432,8 @@ class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):
     def __init__(self, loss='deviance', learning_rate=0.1, n_estimators=100,
                  subsample=1.0, criterion='friedman_mse', min_samples_split=2,
                  min_samples_leaf=1, min_weight_fraction_leaf=0.,
-                 max_depth=3, min_impurity_split=1e-7, init=None,
+                 max_depth=3, min_impurity_decrease=0.,
+                 min_impurity_split=None, init=None,
                  random_state=None, max_features=None, verbose=0,
                  max_leaf_nodes=None, warm_start=False,
                  presort='auto'):
@@ -1428,6 +1447,7 @@ def __init__(self, loss='deviance', learning_rate=0.1, n_estimators=100,
             max_features=max_features,
             random_state=random_state, verbose=verbose,
             max_leaf_nodes=max_leaf_nodes,
+            min_impurity_decrease=min_impurity_decrease,
             min_impurity_split=min_impurity_split,
             warm_start=warm_start,
             presort=presort)
@@ -1712,11 +1732,23 @@ class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):
         Best nodes are defined as relative reduction in impurity.
         If None then unlimited number of leaf nodes.
 
-    min_impurity_split : float, optional (default=1e-7)
-        Threshold for early stopping in tree growth. A node will split
-        if its impurity is above the threshold, otherwise it is a leaf.
+    min_impurity_decrease : float, optional (default=0.)
+        A node will be split if this split induces a decrease of the impurity
+        greater than or equal to this value.
 
-        .. versionadded:: 0.18
+        The weighted impurity decrease equation is the following::
+
+            N_t / N * (impurity - N_t_R / N_t * right_impurity
+                                - N_t_L / N_t * left_impurity)
+
+        where ``N`` is the total number of samples, ``N_t`` is the number of
+        samples at the current node, ``N_t_L`` is the number of samples in the
+        left child, and ``N_t_R`` is the number of samples in the right child.
+
+        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
+        if ``sample_weight`` is passed.
+
+        .. versionadded:: 0.19
 
     alpha : float (default=0.9)
         The alpha-quantile of the huber loss function and the quantile
@@ -1735,8 +1767,7 @@ class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):
     warm_start : bool, default: False
         When set to ``True``, reuse the solution of the previous call to fit
         and add more estimators to the ensemble, otherwise, just erase the
-        p
-revious solution.
+        previous solution.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
@@ -1808,7 +1839,8 @@ class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):
     def __init__(self, loss='ls', learning_rate=0.1, n_estimators=100,
                  subsample=1.0, criterion='friedman_mse', min_samples_split=2,
                  min_samples_leaf=1, min_weight_fraction_leaf=0.,
-                 max_depth=3, min_impurity_split=1e-7, init=None, random_state=None,
+                 max_depth=3, min_impurity_decrease=0.,
+                 min_impurity_split=None, init=None, random_state=None,
                  max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None,
                  warm_start=False, presort='auto'):
 
@@ -1818,7 +1850,9 @@ def __init__(self, loss='ls', learning_rate=0.1, n_estimators=100,
             min_samples_leaf=min_samples_leaf,
             min_weight_fraction_leaf=min_weight_fraction_leaf,
             max_depth=max_depth, init=init, subsample=subsample,
-            max_features=max_features, min_impurity_split=min_impurity_split,
+            max_features=max_features,
+            min_impurity_decrease=min_impurity_decrease,
+            min_impurity_split=min_impurity_split,
             random_state=random_state, alpha=alpha, verbose=verbose,
             max_leaf_nodes=max_leaf_nodes, warm_start=warm_start,
             presort=presort)
diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py
index d34093c2acb8..ce379243e21d 100644
--- a/sklearn/ensemble/iforest.py
+++ b/sklearn/ensemble/iforest.py
@@ -7,6 +7,7 @@
 import numpy as np
 import scipy as sp
 from warnings import warn
+from sklearn.utils.fixes import euler_gamma
 
 from scipy.sparse import issparse
 
@@ -300,7 +301,7 @@ def _average_path_length(n_samples_leaf):
         if n_samples_leaf <= 1:
             return 1.
         else:
-            return 2. * (np.log(n_samples_leaf) + 0.5772156649) - 2. * (
+            return 2. * (np.log(n_samples_leaf - 1.) + euler_gamma) - 2. * (
                 n_samples_leaf - 1.) / n_samples_leaf
 
     else:
@@ -314,7 +315,7 @@ def _average_path_length(n_samples_leaf):
 
         average_path_length[mask] = 1.
         average_path_length[not_mask] = 2. * (
-            np.log(n_samples_leaf[not_mask]) + 0.5772156649) - 2. * (
+            np.log(n_samples_leaf[not_mask] - 1.) + euler_gamma) - 2. * (
                 n_samples_leaf[not_mask] - 1.) / n_samples_leaf[not_mask]
 
         return average_path_length.reshape(n_samples_leaf_shape)
diff --git a/sklearn/ensemble/tests/test_forest.py b/sklearn/ensemble/tests/test_forest.py
index d28780ee54e5..63f81e5b5550 100644
--- a/sklearn/ensemble/tests/test_forest.py
+++ b/sklearn/ensemble/tests/test_forest.py
@@ -28,6 +28,7 @@
 from sklearn.utils.testing import assert_greater_equal
 from sklearn.utils.testing import assert_raises
 from sklearn.utils.testing import assert_warns
+from sklearn.utils.testing import assert_warns_message
 from sklearn.utils.testing import ignore_warnings
 from sklearn.utils.testing import skip_if_32bit
 
@@ -1180,3 +1181,32 @@ def test_decision_path():
         yield check_decision_path, name
     for name in FOREST_REGRESSORS:
         yield check_decision_path, name
+
+
+def test_min_impurity_split():
+    # Test if min_impurity_split of base estimators is set
+    # Regression test for #8006
+    X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)
+    all_estimators = [RandomForestClassifier, RandomForestRegressor,
+                      ExtraTreesClassifier, ExtraTreesRegressor]
+
+    for Estimator in all_estimators:
+        est = Estimator(min_impurity_split=0.1)
+        est = assert_warns_message(DeprecationWarning, "min_impurity_decrease",
+                                   est.fit, X, y)
+        for tree in est.estimators_:
+            assert_equal(tree.min_impurity_split, 0.1)
+
+
+def test_min_impurity_decrease():
+    X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)
+    all_estimators = [RandomForestClassifier, RandomForestRegressor,
+                      ExtraTreesClassifier, ExtraTreesRegressor]
+
+    for Estimator in all_estimators:
+        est = Estimator(min_impurity_decrease=0.1)
+        est.fit(X, y)
+        for tree in est.estimators_:
+            # Simply check if the parameter is passed on correctly. Tree tests
+            # will suffice for the actual working of this param
+            assert_equal(tree.min_impurity_decrease, 0.1)
diff --git a/sklearn/ensemble/tests/test_gradient_boosting.py b/sklearn/ensemble/tests/test_gradient_boosting.py
index 6fd55f691c26..8887dba3975c 100644
--- a/sklearn/ensemble/tests/test_gradient_boosting.py
+++ b/sklearn/ensemble/tests/test_gradient_boosting.py
@@ -26,6 +26,7 @@
 from sklearn.utils.testing import assert_raises
 from sklearn.utils.testing import assert_true
 from sklearn.utils.testing import assert_warns
+from sklearn.utils.testing import assert_warns_message
 from sklearn.utils.testing import skip_if_32bit
 from sklearn.exceptions import DataConversionWarning
 from sklearn.exceptions import NotFittedError
@@ -965,15 +966,29 @@ def test_min_impurity_split():
     # Test if min_impurity_split of base estimators is set
     # Regression test for #8006
     X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)
-    all_estimators = [GradientBoostingRegressor,
-                      GradientBoostingClassifier]
+    all_estimators = [GradientBoostingRegressor, GradientBoostingClassifier]
 
     for GBEstimator in all_estimators:
-        est = GBEstimator(min_impurity_split=0.1).fit(X, y)
+        est = GBEstimator(min_impurity_split=0.1)
+        est = assert_warns_message(DeprecationWarning, "min_impurity_decrease",
+                                   est.fit, X, y)
         for tree in est.estimators_.flat:
             assert_equal(tree.min_impurity_split, 0.1)
 
 
+def test_min_impurity_decrease():
+    X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)
+    all_estimators = [GradientBoostingRegressor, GradientBoostingClassifier]
+
+    for GBEstimator in all_estimators:
+        est = GBEstimator(min_impurity_decrease=0.1)
+        est.fit(X, y)
+        for tree in est.estimators_.flat:
+            # Simply check if the parameter is passed on correctly. Tree tests
+            # will suffice for the actual working of this param
+            assert_equal(tree.min_impurity_decrease, 0.1)
+
+
 def test_warm_start_wo_nestimators_change():
     # Test if warm_start does nothing if n_estimators is not changed.
     # Regression test for #3513.
diff --git a/sklearn/ensemble/tests/test_iforest.py b/sklearn/ensemble/tests/test_iforest.py
index 767444f923f7..0ade6195c618 100644
--- a/sklearn/ensemble/tests/test_iforest.py
+++ b/sklearn/ensemble/tests/test_iforest.py
@@ -8,6 +8,8 @@
 
 import numpy as np
 
+from sklearn.utils.fixes import euler_gamma
+from sklearn.utils.testing import assert_almost_equal
 from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_raises
@@ -19,6 +21,7 @@
 
 from sklearn.model_selection import ParameterGrid
 from sklearn.ensemble import IsolationForest
+from sklearn.ensemble.iforest import _average_path_length
 from sklearn.model_selection import train_test_split
 from sklearn.datasets import load_boston, load_iris
 from sklearn.utils import check_random_state
@@ -211,3 +214,16 @@ def test_iforest_subsampled_features():
     clf = IsolationForest(max_features=0.8)
     clf.fit(X_train, y_train)
     clf.predict(X_test)
+
+
+def test_iforest_average_path_length():
+    # It tests non-regression for #8549 which used the wrong formula
+    # for average path length, strictly for the integer case
+
+    result_one = 2. * (np.log(4.) + euler_gamma) - 2. * 4. / 5.
+    result_two = 2. * (np.log(998.) + euler_gamma) - 2. * 998. / 999.
+    assert_almost_equal(_average_path_length(1), 1., decimal=10)
+    assert_almost_equal(_average_path_length(5), result_one, decimal=10)
+    assert_almost_equal(_average_path_length(999), result_two, decimal=10)
+    assert_array_almost_equal(_average_path_length(np.array([1, 5, 999])),
+                              [1., result_one, result_two], decimal=10)
diff --git a/sklearn/ensemble/tests/test_voting_classifier.py b/sklearn/ensemble/tests/test_voting_classifier.py
index 2ad007741940..d61d8bfac62b 100644
--- a/sklearn/ensemble/tests/test_voting_classifier.py
+++ b/sklearn/ensemble/tests/test_voting_classifier.py
@@ -2,7 +2,7 @@
 
 import numpy as np
 from sklearn.utils.testing import assert_almost_equal, assert_array_equal
-from sklearn.utils.testing import assert_equal
+from sklearn.utils.testing import assert_equal, assert_true, assert_false
 from sklearn.utils.testing import assert_raise_message
 from sklearn.exceptions import NotFittedError
 from sklearn.linear_model import LogisticRegression
@@ -40,6 +40,19 @@ def test_estimator_init():
            '; got 2 weights, 1 estimators')
     assert_raise_message(ValueError, msg, eclf.fit, X, y)
 
+    eclf = VotingClassifier(estimators=[('lr', clf), ('lr', clf)],
+                            weights=[1, 2])
+    msg = "Names provided are not unique: ['lr', 'lr']"
+    assert_raise_message(ValueError, msg, eclf.fit, X, y)
+
+    eclf = VotingClassifier(estimators=[('lr__', clf)])
+    msg = "Estimator names must not contain __: got ['lr__']"
+    assert_raise_message(ValueError, msg, eclf.fit, X, y)
+
+    eclf = VotingClassifier(estimators=[('estimators', clf)])
+    msg = "Estimator names conflict with constructor arguments: ['estimators']"
+    assert_raise_message(ValueError, msg, eclf.fit, X, y)
+
 
 def test_predictproba_hardvoting():
     eclf = VotingClassifier(estimators=[('lr1', LogisticRegression()),
@@ -260,6 +273,82 @@ def test_sample_weight():
     assert_raise_message(ValueError, msg, eclf3.fit, X, y, sample_weight)
 
 
+def test_set_params():
+    """set_params should be able to set estimators"""
+    clf1 = LogisticRegression(random_state=123, C=1.0)
+    clf2 = RandomForestClassifier(random_state=123, max_depth=None)
+    clf3 = GaussianNB()
+    eclf1 = VotingClassifier([('lr', clf1), ('rf', clf2)], voting='soft',
+                             weights=[1, 2])
+    eclf1.fit(X, y)
+    eclf2 = VotingClassifier([('lr', clf1), ('nb', clf3)], voting='soft',
+                             weights=[1, 2])
+    eclf2.set_params(nb=clf2).fit(X, y)
+    assert_false(hasattr(eclf2, 'nb'))
+
+    assert_array_equal(eclf1.predict(X), eclf2.predict(X))
+    assert_array_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))
+    assert_equal(eclf2.estimators[0][1].get_params(), clf1.get_params())
+    assert_equal(eclf2.estimators[1][1].get_params(), clf2.get_params())
+
+    eclf1.set_params(lr__C=10.0)
+    eclf2.set_params(nb__max_depth=5)
+
+    assert_true(eclf1.estimators[0][1].get_params()['C'] == 10.0)
+    assert_true(eclf2.estimators[1][1].get_params()['max_depth'] == 5)
+    assert_equal(eclf1.get_params()["lr__C"],
+                 eclf1.get_params()["lr"].get_params()['C'])
+
+
+def test_set_estimator_none():
+    """VotingClassifier set_params should be able to set estimators as None"""
+    # Test predict
+    clf1 = LogisticRegression(random_state=123)
+    clf2 = RandomForestClassifier(random_state=123)
+    clf3 = GaussianNB()
+    eclf1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2),
+                                         ('nb', clf3)],
+                             voting='hard', weights=[1, 0, 0.5]).fit(X, y)
+
+    eclf2 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2),
+                                         ('nb', clf3)],
+                             voting='hard', weights=[1, 1, 0.5])
+    eclf2.set_params(rf=None).fit(X, y)
+    assert_array_equal(eclf1.predict(X), eclf2.predict(X))
+
+    assert_true(dict(eclf2.estimators)["rf"] is None)
+    assert_true(len(eclf2.estimators_) == 2)
+    assert_true(all([not isinstance(est, RandomForestClassifier) for est in
+                     eclf2.estimators_]))
+    assert_true(eclf2.get_params()["rf"] is None)
+
+    eclf1.set_params(voting='soft').fit(X, y)
+    eclf2.set_params(voting='soft').fit(X, y)
+    assert_array_equal(eclf1.predict(X), eclf2.predict(X))
+    assert_array_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))
+    msg = ('All estimators are None. At least one is required'
+           ' to be a classifier!')
+    assert_raise_message(
+        ValueError, msg, eclf2.set_params(lr=None, rf=None, nb=None).fit, X, y)
+
+    # Test soft voting transform
+    X1 = np.array([[1], [2]])
+    y1 = np.array([1, 2])
+    eclf1 = VotingClassifier(estimators=[('rf', clf2), ('nb', clf3)],
+                             voting='soft', weights=[0, 0.5]).fit(X1, y1)
+
+    eclf2 = VotingClassifier(estimators=[('rf', clf2), ('nb', clf3)],
+                             voting='soft', weights=[1, 0.5])
+    eclf2.set_params(rf=None).fit(X1, y1)
+    assert_array_equal(eclf1.transform(X1), np.array([[[0.7, 0.3], [0.3, 0.7]],
+                                                      [[1., 0.], [0., 1.]]]))
+    assert_array_equal(eclf2.transform(X1), np.array([[[1., 0.], [0., 1.]]]))
+    eclf1.set_params(voting='hard')
+    eclf2.set_params(voting='hard')
+    assert_array_equal(eclf1.transform(X1), np.array([[0, 0], [1, 1]]))
+    assert_array_equal(eclf2.transform(X1), np.array([[0], [1]]))
+
+
 def test_estimator_weights_format():
     # Test estimator weights inputs as list and array
     clf1 = LogisticRegression(random_state=123)
diff --git a/sklearn/ensemble/voting_classifier.py b/sklearn/ensemble/voting_classifier.py
index cb0d6ad19c98..44cf4fe775ce 100644
--- a/sklearn/ensemble/voting_classifier.py
+++ b/sklearn/ensemble/voting_classifier.py
@@ -13,14 +13,13 @@
 
 import numpy as np
 
-from ..base import BaseEstimator
 from ..base import ClassifierMixin
 from ..base import TransformerMixin
 from ..base import clone
 from ..preprocessing import LabelEncoder
-from ..externals import six
 from ..externals.joblib import Parallel, delayed
 from ..utils.validation import has_fit_parameter, check_is_fitted
+from ..utils.metaestimators import _BaseComposition
 
 
 def _parallel_fit_estimator(estimator, X, y, sample_weight):
@@ -32,7 +31,7 @@ def _parallel_fit_estimator(estimator, X, y, sample_weight):
     return estimator
 
 
-class VotingClassifier(BaseEstimator, ClassifierMixin, TransformerMixin):
+class VotingClassifier(_BaseComposition, ClassifierMixin, TransformerMixin):
     """Soft Voting/Majority Rule classifier for unfitted estimators.
 
     .. versionadded:: 0.17
@@ -44,7 +43,8 @@ class VotingClassifier(BaseEstimator, ClassifierMixin, TransformerMixin):
     estimators : list of (string, estimator) tuples
         Invoking the ``fit`` method on the ``VotingClassifier`` will fit clones
         of those original estimators that will be stored in the class attribute
-        `self.estimators_`.
+        ``self.estimators_``. An estimator can be set to `None` using
+        ``set_params``.
 
     voting : str, {'hard', 'soft'} (default='hard')
         If 'hard', uses predicted class labels for majority rule voting.
@@ -64,7 +64,8 @@ class VotingClassifier(BaseEstimator, ClassifierMixin, TransformerMixin):
     Attributes
     ----------
     estimators_ : list of classifiers
-        The collection of fitted sub-estimators.
+        The collection of fitted sub-estimators as defined in ``estimators``
+        that are not `None`.
 
     classes_ : array-like, shape = [n_predictions]
         The classes labels.
@@ -102,11 +103,14 @@ class VotingClassifier(BaseEstimator, ClassifierMixin, TransformerMixin):
 
     def __init__(self, estimators, voting='hard', weights=None, n_jobs=1):
         self.estimators = estimators
-        self.named_estimators = dict(estimators)
         self.voting = voting
         self.weights = weights
         self.n_jobs = n_jobs
 
+    @property
+    def named_estimators(self):
+        return dict(self.estimators)
+
     def fit(self, X, y, sample_weight=None):
         """ Fit the estimators.
 
@@ -150,11 +154,16 @@ def fit(self, X, y, sample_weight=None):
         if sample_weight is not None:
             for name, step in self.estimators:
                 if not has_fit_parameter(step, 'sample_weight'):
-                    raise ValueError('Underlying estimator \'%s\' does not support'
-                                     ' sample weights.' % name)
-
-        self.le_ = LabelEncoder()
-        self.le_.fit(y)
+                    raise ValueError('Underlying estimator \'%s\' does not'
+                                     ' support sample weights.' % name)
+        names, clfs = zip(*self.estimators)
+        self._validate_names(names)
+
+        n_isnone = np.sum([clf is None for _, clf in self.estimators])
+        if n_isnone == len(self.estimators):
+            raise ValueError('All estimators are None. At least one is '
+                             'required to be a classifier!')
+        self.le_ = LabelEncoder().fit(y)
         self.classes_ = self.le_.classes_
         self.estimators_ = []
 
@@ -162,11 +171,19 @@ def fit(self, X, y, sample_weight=None):
 
         self.estimators_ = Parallel(n_jobs=self.n_jobs)(
                 delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,
-                    sample_weight)
-                    for _, clf in self.estimators)
+                                                 sample_weight)
+                for clf in clfs if clf is not None)
 
         return self
 
+    @property
+    def _weights_not_none(self):
+        """Get the weights of not `None` estimators"""
+        if self.weights is None:
+            return None
+        return [w for est, w in zip(self.estimators,
+                                    self.weights) if est[1] is not None]
+
     def predict(self, X):
         """ Predict class labels for X.
 
@@ -188,11 +205,10 @@ def predict(self, X):
 
         else:  # 'hard' voting
             predictions = self._predict(X)
-            maj = np.apply_along_axis(lambda x:
-                                      np.argmax(np.bincount(x,
-                                                weights=self.weights)),
-                                      axis=1,
-                                      arr=predictions.astype('int'))
+            maj = np.apply_along_axis(
+                lambda x: np.argmax(
+                    np.bincount(x, weights=self._weights_not_none)),
+                axis=1, arr=predictions.astype('int'))
 
         maj = self.le_.inverse_transform(maj)
 
@@ -208,7 +224,8 @@ def _predict_proba(self, X):
             raise AttributeError("predict_proba is not available when"
                                  " voting=%r" % self.voting)
         check_is_fitted(self, 'estimators_')
-        avg = np.average(self._collect_probas(X), axis=0, weights=self.weights)
+        avg = np.average(self._collect_probas(X), axis=0,
+                         weights=self._weights_not_none)
         return avg
 
     @property
@@ -252,17 +269,42 @@ def transform(self, X):
         else:
             return self._predict(X)
 
+    def set_params(self, **params):
+        """ Setting the parameters for the voting classifier
+
+        Valid parameter keys can be listed with get_params().
+
+        Parameters
+        ----------
+        params: keyword arguments
+            Specific parameters using e.g. set_params(parameter_name=new_value)
+            In addition, to setting the parameters of the ``VotingClassifier``,
+            the individual classifiers of the ``VotingClassifier`` can also be
+            set or replaced by setting them to None.
+
+        Examples
+        --------
+        # In this example, the RandomForestClassifier is removed
+        clf1 = LogisticRegression()
+        clf2 = RandomForestClassifier()
+        eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)]
+        eclf.set_params(rf=None)
+
+        """
+        super(VotingClassifier, self)._set_params('estimators', **params)
+        return self
+
     def get_params(self, deep=True):
-        """Return estimator parameter names for GridSearch support"""
-        if not deep:
-            return super(VotingClassifier, self).get_params(deep=False)
-        else:
-            out = super(VotingClassifier, self).get_params(deep=False)
-            out.update(self.named_estimators.copy())
-            for name, step in six.iteritems(self.named_estimators):
-                for key, value in six.iteritems(step.get_params(deep=True)):
-                    out['%s__%s' % (name, key)] = value
-            return out
+        """ Get the parameters of the VotingClassifier
+
+        Parameters
+        ----------
+        deep: bool
+            Setting it to True gets the various classifiers and the parameters
+            of the classifiers as well
+        """
+        return super(VotingClassifier,
+                     self)._get_params('estimators', deep=deep)
 
     def _predict(self, X):
         """Collect results from clf.predict calls. """
diff --git a/sklearn/externals/copy_joblib.sh b/sklearn/externals/copy_joblib.sh
index 6e5d2a5a3ec1..8b8de45ba42e 100755
--- a/sklearn/externals/copy_joblib.sh
+++ b/sklearn/externals/copy_joblib.sh
@@ -1,20 +1,12 @@
 #!/bin/sh
 # Script to do a local install of joblib
 export LC_ALL=C
-rm -rf tmp joblib
-PYTHON_VERSION=$(python -c 'import sys; print("{0[0]}.{0[1]}".format(sys.version_info))')
-SITE_PACKAGES="$PWD/tmp/lib/python$PYTHON_VERSION/site-packages"
+INSTALL_FOLDER=tmp/joblib_install
+rm -rf joblib $INSTALL_FOLDER
+pip install joblib --target $INSTALL_FOLDER
+cp -r $INSTALL_FOLDER/joblib .
+rm -rf $INSTALL_FOLDER
 
-mkdir -p $SITE_PACKAGES
-mkdir -p tmp/bin
-export PYTHONPATH="$SITE_PACKAGES"
-easy_install -Zeab tmp joblib
-
-cd tmp/joblib/
-python setup.py install --prefix $OLDPWD/tmp
-cd $OLDPWD
-cp -r $SITE_PACKAGES/joblib-*.egg/joblib .
-rm -rf tmp
 # Needed to rewrite the doctests
 # Note: BSD sed -i needs an argument unders OSX
 # so first renaming to .bak and then deleting backup files
@@ -25,4 +17,6 @@ find joblib -name "*.bak" | xargs rm
 # joblib is already tested on its own CI infrastructure upstream.
 rm -r joblib/test
 
-chmod -x joblib/*.py
+# Remove joblib/testing.py which is only used in tests and has a
+# pytest dependency (needed until we drop nose)
+rm joblib/testing.py
diff --git a/sklearn/externals/joblib/__init__.py b/sklearn/externals/joblib/__init__.py
index ce1957d1def5..3455b7d79b51 100644
--- a/sklearn/externals/joblib/__init__.py
+++ b/sklearn/externals/joblib/__init__.py
@@ -1,27 +1,27 @@
-""" Joblib is a set of tools to provide **lightweight pipelining in
+"""Joblib is a set of tools to provide **lightweight pipelining in
 Python**. In particular, joblib offers:
 
-  1. transparent disk-caching of the output values and lazy re-evaluation
-     (memoize pattern)
+1. transparent disk-caching of the output values and lazy re-evaluation
+   (memoize pattern)
 
-  2. easy simple parallel computing
+2. easy simple parallel computing
 
-  3. logging and tracing of the execution
+3. logging and tracing of the execution
 
 Joblib is optimized to be **fast** and **robust** in particular on large
 data and has specific optimizations for `numpy` arrays. It is
 **BSD-licensed**.
 
 
-    ============================== ============================================
-    **User documentation**:        http://pythonhosted.org/joblib
+    ========================= ================================================
+    **User documentation:**        http://pythonhosted.org/joblib
 
-    **Download packages**:         http://pypi.python.org/pypi/joblib#downloads
+    **Download packages:**         http://pypi.python.org/pypi/joblib#downloads
 
-    **Source code**:               http://github.com/joblib/joblib
+    **Source code:**               http://github.com/joblib/joblib
 
-    **Report issues**:             http://github.com/joblib/joblib/issues
-    ============================== ============================================
+    **Report issues:**             http://github.com/joblib/joblib/issues
+    ========================= ================================================
 
 
 Vision
@@ -115,8 +115,7 @@
 # Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
 # 'X.Y.dev0' is the canonical version of 'X.Y.dev'
 #
-
-__version__ = '0.10.3'
+__version__ = '0.11'
 
 
 from .memory import Memory, MemorizedResult
diff --git a/sklearn/externals/joblib/_compat.py b/sklearn/externals/joblib/_compat.py
index 6309fa528126..0c6e752478f0 100644
--- a/sklearn/externals/joblib/_compat.py
+++ b/sklearn/externals/joblib/_compat.py
@@ -4,7 +4,6 @@
 import sys
 
 PY3_OR_LATER = sys.version_info[0] >= 3
-PY26 = sys.version_info[:2] == (2, 6)
 PY27 = sys.version_info[:2] == (2, 7)
 
 try:
diff --git a/sklearn/externals/joblib/_memory_helpers.py b/sklearn/externals/joblib/_memory_helpers.py
index 08a90de96d28..857ad29d79ad 100644
--- a/sklearn/externals/joblib/_memory_helpers.py
+++ b/sklearn/externals/joblib/_memory_helpers.py
@@ -102,4 +102,4 @@ def open_py_source(filename):
         buffer.seek(0)
         text = TextIOWrapper(buffer, encoding, line_buffering=True)
         text.mode = 'r'
-        return text
\ No newline at end of file
+        return text
diff --git a/sklearn/externals/joblib/_parallel_backends.py b/sklearn/externals/joblib/_parallel_backends.py
index cc4f221d2128..8f3e768abd44 100644
--- a/sklearn/externals/joblib/_parallel_backends.py
+++ b/sklearn/externals/joblib/_parallel_backends.py
@@ -21,6 +21,8 @@
 class ParallelBackendBase(with_metaclass(ABCMeta)):
     """Helper abc which defines all methods a ParallelBackend must implement"""
 
+    supports_timeout = False
+
     @abstractmethod
     def effective_n_jobs(self, n_jobs):
         """Determine the number of jobs that can actually run in parallel
@@ -236,6 +238,8 @@ class ThreadingBackend(PoolManagerMixin, ParallelBackendBase):
     "with nogil" block or an expensive call to a library such as NumPy).
     """
 
+    supports_timeout = True
+
     def configure(self, n_jobs=1, parallel=None, **backend_args):
         """Build a process or thread pool and return the number of workers"""
         n_jobs = self.effective_n_jobs(n_jobs)
@@ -259,6 +263,8 @@ class MultiprocessingBackend(PoolManagerMixin, AutoBatchingMixin,
     # Environment variables to protect against bad situations when nesting
     JOBLIB_SPAWNED_PROCESS = "__JOBLIB_SPAWNED_PARALLEL__"
 
+    supports_timeout = True
+
     def effective_n_jobs(self, n_jobs):
         """Determine the number of jobs which are going to run in parallel.
 
@@ -277,10 +283,10 @@ def effective_n_jobs(self, n_jobs):
                     stacklevel=3)
             return 1
 
-        elif threading.current_thread().name != 'MainThread':
+        if not isinstance(threading.current_thread(), threading._MainThread):
             # Prevent posix fork inside in non-main posix threads
             warnings.warn(
-                'Multiprocessing backed parallel loops cannot be nested'
+                'Multiprocessing-backed parallel loops cannot be nested'
                 ' below threads, setting n_jobs=1',
                 stacklevel=3)
             return 1
diff --git a/sklearn/externals/joblib/backports.py b/sklearn/externals/joblib/backports.py
new file mode 100644
index 000000000000..7dd3df16f165
--- /dev/null
+++ b/sklearn/externals/joblib/backports.py
@@ -0,0 +1,80 @@
+"""
+Backports of fixes for joblib dependencies
+"""
+import os
+import time
+import ctypes
+import sys
+
+from distutils.version import LooseVersion
+
+try:
+    import numpy as np
+
+    def make_memmap(filename, dtype='uint8', mode='r+', offset=0,
+                    shape=None, order='C'):
+        """Backport of numpy memmap offset fix.
+
+        See https://github.com/numpy/numpy/pull/8443 for more details.
+
+        The numpy fix will be available in numpy 1.13.
+        """
+        mm = np.memmap(filename, dtype=dtype, mode=mode, offset=offset,
+                       shape=shape, order=order)
+        if LooseVersion(np.__version__) < '1.13':
+            mm.offset = offset
+        return mm
+except ImportError:
+    def make_memmap(filename, dtype='uint8', mode='r+', offset=0,
+                    shape=None, order='C'):
+        raise NotImplementedError(
+            "'joblib.backports.make_memmap' should not be used "
+            'if numpy is not installed.')
+
+
+if os.name == 'nt':
+    error_access_denied = 5
+    try:
+        from os import replace
+    except ImportError:
+        # Python 2.7
+        def replace(src, dst):
+            if not isinstance(src, unicode):  # noqa
+                src = unicode(src, sys.getfilesystemencoding())  # noqa
+            if not isinstance(dst, unicode):  # noqa
+                dst = unicode(dst, sys.getfilesystemencoding())  # noqa
+
+            movefile_replace_existing = 0x1
+            return_value = ctypes.windll.kernel32.MoveFileExW(
+                src, dst, movefile_replace_existing)
+            if return_value == 0:
+                raise ctypes.WinError()
+
+    def concurrency_safe_rename(src, dst):
+        """Renames ``src`` into ``dst`` overwriting ``dst`` if it exists.
+
+        On Windows os.replace (or for Python 2.7 its implementation
+        through MoveFileExW) can yield permission errors if executed by
+        two different processes.
+        """
+        max_sleep_time = 1
+        total_sleep_time = 0
+        sleep_time = 0.001
+        while total_sleep_time < max_sleep_time:
+            try:
+                replace(src, dst)
+                break
+            except Exception as exc:
+                if getattr(exc, 'winerror', None) == error_access_denied:
+                    time.sleep(sleep_time)
+                    total_sleep_time += sleep_time
+                    sleep_time *= 2
+                else:
+                    raise
+        else:
+            raise
+else:
+    try:
+        from os import replace as concurrency_safe_rename
+    except ImportError:
+        from os import rename as concurrency_safe_rename  # noqa
diff --git a/sklearn/externals/joblib/format_stack.py b/sklearn/externals/joblib/format_stack.py
index 91eabeb0e1ad..4984ebb08132 100644
--- a/sklearn/externals/joblib/format_stack.py
+++ b/sklearn/externals/joblib/format_stack.py
@@ -135,15 +135,10 @@ def _fixed_getframes(etb, context=1, tb_offset=0):
     aux = traceback.extract_tb(etb)
     assert len(records) == len(aux)
     for i, (file, lnum, _, _) in enumerate(aux):
-        maybeStart = lnum - 1 - context // 2
-        start = max(maybeStart, 0)
+        maybe_start = lnum - 1 - context // 2
+        start = max(maybe_start, 0)
         end = start + context
         lines = linecache.getlines(file)[start:end]
-        # pad with empty lines if necessary
-        if maybeStart < 0:
-            lines = (['\n'] * -maybeStart) + lines
-        if len(lines) < context:
-            lines += ['\n'] * (context - len(lines))
         buf = list(records[i])
         buf[LNUM_POS] = lnum
         buf[INDEX_POS] = lnum - 1 - start
@@ -355,13 +350,7 @@ def format_exc(etype, evalue, etb, context=5, tb_offset=0):
         pyver)
 
     # Drop topmost frames if requested
-    try:
-        records = _fixed_getframes(etb, context, tb_offset)
-    except:
-        raise
-        print('\nUnfortunately, your original traceback can not be '
-              'constructed.\n')
-        return ''
+    records = _fixed_getframes(etb, context, tb_offset)
 
     # Get (safely) a string form of the exception info
     try:
@@ -397,18 +386,13 @@ def format_outer_frames(context=5, stack_start=None, stack_end=None,
                 filename = filename[:-4] + '.py'
         if ignore_ipython:
             # Hack to avoid printing the internals of IPython
-            if (os.path.basename(filename) == 'iplib.py'
-                        and func_name in ('safe_execfile', 'runcode')):
+            if (os.path.basename(filename) in ('iplib.py', 'py3compat.py')
+                        and func_name in ('execfile', 'safe_execfile', 'runcode')):
                 break
-        maybeStart = line_no - 1 - context // 2
-        start = max(maybeStart, 0)
+        maybe_start = line_no - 1 - context // 2
+        start = max(maybe_start, 0)
         end = start + context
         lines = linecache.getlines(filename)[start:end]
-        # pad with empty lines if necessary
-        if maybeStart < 0:
-            lines = (['\n'] * -maybeStart) + lines
-        if len(lines) < context:
-            lines += ['\n'] * (context - len(lines))
         buf = list(records[i])
         buf[LNUM_POS] = line_no
         buf[INDEX_POS] = line_no - 1 - start
diff --git a/sklearn/externals/joblib/func_inspect.py b/sklearn/externals/joblib/func_inspect.py
index ad5a548d38de..30d1192b314f 100644
--- a/sklearn/externals/joblib/func_inspect.py
+++ b/sklearn/externals/joblib/func_inspect.py
@@ -190,7 +190,7 @@ def _signature_str(function_name, arg_spec):
     arg_spec_for_format = arg_spec[:7 if PY3_OR_LATER else 4]
 
     arg_spec_str = inspect.formatargspec(*arg_spec_for_format)
-    return '{0}{1}'.format(function_name, arg_spec_str)
+    return '{}{}'.format(function_name, arg_spec_str)
 
 
 def _function_called_str(function_name, args, kwargs):
@@ -316,6 +316,13 @@ def filter_args(func, ignore_lst, args=(), kwargs=dict()):
     return arg_dict
 
 
+def _format_arg(arg):
+    formatted_arg = pformat(arg, indent=2)
+    if len(formatted_arg) > 1500:
+        formatted_arg = '%s...' % formatted_arg[:700]
+    return formatted_arg
+
+
 def format_signature(func, *args, **kwargs):
     # XXX: Should this use inspect.formatargvalues/formatargspec?
     module, name = get_func_name(func)
@@ -328,14 +335,12 @@ def format_signature(func, *args, **kwargs):
     arg_str = list()
     previous_length = 0
     for arg in args:
-        arg = pformat(arg, indent=2)
-        if len(arg) > 1500:
-            arg = '%s...' % arg[:700]
+        formatted_arg = _format_arg(arg)
         if previous_length > 80:
-            arg = '\n%s' % arg
-        previous_length = len(arg)
-        arg_str.append(arg)
-    arg_str.extend(['%s=%s' % (v, pformat(i)) for v, i in kwargs.items()])
+            formatted_arg = '\n%s' % formatted_arg
+        previous_length = len(formatted_arg)
+        arg_str.append(formatted_arg)
+    arg_str.extend(['%s=%s' % (v, _format_arg(i)) for v, i in kwargs.items()])
     arg_str = ', '.join(arg_str)
 
     signature = '%s(%s)' % (name, arg_str)
diff --git a/sklearn/externals/joblib/hashing.py b/sklearn/externals/joblib/hashing.py
index ced817be17ca..88bd6cfdefea 100644
--- a/sklearn/externals/joblib/hashing.py
+++ b/sklearn/externals/joblib/hashing.py
@@ -13,6 +13,7 @@
 import types
 import struct
 import io
+import decimal
 
 from ._compat import _bytes_or_unicode, PY3_OR_LATER
 
@@ -35,7 +36,7 @@ def __init__(self, set_sequence):
             # This fails on python 3 when elements are unorderable
             # but we keep it in a try as it's faster.
             self._sequence = sorted(set_sequence)
-        except TypeError:
+        except (TypeError, decimal.InvalidOperation):
             # If elements are unorderable, sorting them using their hash.
             # This is slower but works in any case.
             self._sequence = sorted((hash(e) for e in set_sequence))
diff --git a/sklearn/externals/joblib/logger.py b/sklearn/externals/joblib/logger.py
index 41b586427dce..82a53b16500e 100644
--- a/sklearn/externals/joblib/logger.py
+++ b/sklearn/externals/joblib/logger.py
@@ -74,7 +74,7 @@ def __init__(self, depth=3):
         self.depth = depth
 
     def warn(self, msg):
-        logging.warn("[%s]: %s" % (self, msg))
+        logging.warning("[%s]: %s" % (self, msg))
 
     def debug(self, msg):
         # XXX: This conflicts with the debug flag used in children class
diff --git a/sklearn/externals/joblib/memory.py b/sklearn/externals/joblib/memory.py
index fff84ad7a2cf..14d7552535bb 100644
--- a/sklearn/externals/joblib/memory.py
+++ b/sklearn/externals/joblib/memory.py
@@ -15,11 +15,6 @@
 import time
 import pydoc
 import re
-import sys
-try:
-    import cPickle as pickle
-except ImportError:
-    import pickle
 import functools
 import traceback
 import warnings
@@ -27,19 +22,28 @@
 import json
 import weakref
 import io
+import operator
+import collections
+import datetime
+import threading
 
 # Local imports
 from . import hashing
 from .func_inspect import get_func_code, get_func_name, filter_args
-from .func_inspect import format_signature, format_call
+from .func_inspect import format_call
+from .func_inspect import format_signature
 from ._memory_helpers import open_py_source
 from .logger import Logger, format_time, pformat
 from . import numpy_pickle
-from .disk import mkdirp, rm_subdirs
+from .disk import mkdirp, rm_subdirs, memstr_to_bytes
 from ._compat import _basestring, PY3_OR_LATER
+from .backports import concurrency_safe_rename
 
 FIRST_LINE_TEXT = "# first line:"
 
+CacheItemInfo = collections.namedtuple('CacheItemInfo',
+                                       'path size last_access')
+
 # TODO: The following object should have a data store object as a sub
 # object, and the interface to persist and query should be separated in
 # the data store.
@@ -130,7 +134,82 @@ def _load_output(output_dir, func_name, timestamp=None, metadata=None,
         raise KeyError(
             "Non-existing cache value (may have been cleared).\n"
             "File %s does not exist" % filename)
-    return numpy_pickle.load(filename, mmap_mode=mmap_mode)
+    result = numpy_pickle.load(filename, mmap_mode=mmap_mode)
+
+    return result
+
+
+def _get_cache_items(root_path):
+    """Get cache information for reducing the size of the cache."""
+    cache_items = []
+
+    for dirpath, dirnames, filenames in os.walk(root_path):
+        is_cache_hash_dir = re.match('[a-f0-9]{32}', os.path.basename(dirpath))
+
+        if is_cache_hash_dir:
+            output_filename = os.path.join(dirpath, 'output.pkl')
+            try:
+                last_access = os.path.getatime(output_filename)
+            except OSError:
+                try:
+                    last_access = os.path.getatime(dirpath)
+                except OSError:
+                    # The directory has already been deleted
+                    continue
+
+            last_access = datetime.datetime.fromtimestamp(last_access)
+            try:
+                full_filenames = [os.path.join(dirpath, fn)
+                                  for fn in filenames]
+                dirsize = sum(os.path.getsize(fn)
+                              for fn in full_filenames)
+            except OSError:
+                # Either output_filename or one of the files in
+                # dirpath does not exist any more. We assume this
+                # directory is being cleaned by another process already
+                continue
+
+            cache_items.append(CacheItemInfo(dirpath, dirsize, last_access))
+
+    return cache_items
+
+
+def _get_cache_items_to_delete(root_path, bytes_limit):
+    """Get cache items to delete to keep the cache under a size limit."""
+    if isinstance(bytes_limit, _basestring):
+        bytes_limit = memstr_to_bytes(bytes_limit)
+
+    cache_items = _get_cache_items(root_path)
+    cache_size = sum(item.size for item in cache_items)
+
+    to_delete_size = cache_size - bytes_limit
+    if to_delete_size < 0:
+        return []
+
+    # We want to delete first the cache items that were accessed a
+    # long time ago
+    cache_items.sort(key=operator.attrgetter('last_access'))
+
+    cache_items_to_delete = []
+    size_so_far = 0
+
+    for item in cache_items:
+        if size_so_far > to_delete_size:
+            break
+
+        cache_items_to_delete.append(item)
+        size_so_far += item.size
+
+    return cache_items_to_delete
+
+
+def concurrency_safe_write(to_write, filename, write_func):
+    """Writes an object into a file in a concurrency-safe way."""
+    thread_id = id(threading.current_thread())
+    temporary_filename = '{}.thread-{}-pid-{}'.format(
+        filename, thread_id, os.getpid())
+    write_func(to_write, temporary_filename)
+    concurrency_safe_rename(temporary_filename, filename)
 
 
 # An in-memory store to avoid looking at the disk-based function
@@ -419,9 +498,10 @@ def _cached_call(self, args, kwargs):
         # function code has changed
         output_dir, argument_hash = self._get_output_dir(*args, **kwargs)
         metadata = None
+        output_pickle_path = os.path.join(output_dir, 'output.pkl')
         # FIXME: The statements below should be try/excepted
         if not (self._check_previous_func_code(stacklevel=4) and
-                                 os.path.exists(output_dir)):
+                os.path.isfile(output_pickle_path)):
             if self._verbose > 10:
                 _, name = get_func_name(self.func)
                 self.warn('Computing func %s, argument hash %s in '
@@ -449,11 +529,10 @@ def _cached_call(self, args, kwargs):
                     print(max(0, (80 - len(msg))) * '_' + msg)
             except Exception:
                 # XXX: Should use an exception logger
+                _, signature = format_signature(self.func, *args, **kwargs)
                 self.warn('Exception while loading results for '
-                          '(args=%s, kwargs=%s)\n %s' %
-                          (args, kwargs, traceback.format_exc()))
-
-                shutil.rmtree(output_dir, ignore_errors=True)
+                          '{}\n {}'.format(
+                              signature, traceback.format_exc()))
                 out, metadata = self.call(*args, **kwargs)
                 argument_hash = None
         return (out, argument_hash, metadata)
@@ -490,16 +569,6 @@ def __reduce__(self):
         return (self.__class__, (self.func, self.cachedir, self.ignore,
                 self.mmap_mode, self.compress, self._verbose))
 
-    def format_signature(self, *args, **kwargs):
-        warnings.warn("MemorizedFunc.format_signature will be removed in a "
-                      "future version of joblib.", DeprecationWarning)
-        return format_signature(self.func, *args, **kwargs)
-
-    def format_call(self, *args, **kwargs):
-        warnings.warn("MemorizedFunc.format_call will be removed in a "
-                      "future version of joblib.", DeprecationWarning)
-        return format_call(self.func, args, kwargs)
-
     #-------------------------------------------------------------------------
     # Private interface
     #-------------------------------------------------------------------------
@@ -688,9 +757,11 @@ def _persist_output(self, output, dir):
         """ Persist the given output tuple in the directory.
         """
         try:
-            mkdirp(dir)
             filename = os.path.join(dir, 'output.pkl')
-            numpy_pickle.dump(output, filename, compress=self.compress)
+            mkdirp(dir)
+            write_func = functools.partial(numpy_pickle.dump,
+                                           compress=self.compress)
+            concurrency_safe_write(output, filename, write_func)
             if self._verbose > 10:
                 print('Persisting in %s' % dir)
         except OSError:
@@ -724,9 +795,14 @@ def _persist_input(self, output_dir, duration, args, kwargs,
         metadata = {"duration": duration, "input_args": input_repr}
         try:
             mkdirp(output_dir)
-            with open(os.path.join(output_dir, 'metadata.json'), 'w') as f:
-                json.dump(metadata, f)
-        except:
+            filename = os.path.join(output_dir, 'metadata.json')
+
+            def write_func(output, dest_filename):
+                with open(dest_filename, 'w') as f:
+                    json.dump(output, f)
+
+            concurrency_safe_write(metadata, filename, write_func)
+        except Exception:
             pass
 
         this_duration = time.time() - start_time
@@ -750,21 +826,9 @@ def _persist_input(self, output_dir, duration, args, kwargs,
                           % this_duration, stacklevel=5)
         return metadata
 
-    def load_output(self, output_dir):
-        """ Read the results of a previous calculation from the directory
-            it was cached in.
-        """
-        warnings.warn("MemorizedFunc.load_output is deprecated and will be "
-                      "removed in a future version\n"
-                      "of joblib. A MemorizedResult provides similar features",
-                      DeprecationWarning)
-        # No metadata available here.
-        return _load_output(output_dir, _get_func_fullname(self.func),
-                            timestamp=self.timestamp,
-                            mmap_mode=self.mmap_mode, verbose=self._verbose)
-
     # XXX: Need a method to check if results are available.
 
+
     #-------------------------------------------------------------------------
     # Private `object` interface
     #-------------------------------------------------------------------------
@@ -793,7 +857,8 @@ class Memory(Logger):
     # Public interface
     #-------------------------------------------------------------------------
 
-    def __init__(self, cachedir, mmap_mode=None, compress=False, verbose=1):
+    def __init__(self, cachedir, mmap_mode=None, compress=False, verbose=1,
+                 bytes_limit=None):
         """
             Parameters
             ----------
@@ -813,6 +878,8 @@ def __init__(self, cachedir, mmap_mode=None, compress=False, verbose=1):
             verbose: int, optional
                 Verbosity flag, controls the debug messages that are issued
                 as functions are evaluated.
+            bytes_limit: int, optional
+                Limit in bytes of the size of the cache
         """
         # XXX: Bad explanation of the None value of cachedir
         Logger.__init__(self)
@@ -820,6 +887,7 @@ def __init__(self, cachedir, mmap_mode=None, compress=False, verbose=1):
         self.mmap_mode = mmap_mode
         self.timestamp = time.time()
         self.compress = compress
+        self.bytes_limit = bytes_limit
         if compress and mmap_mode is not None:
             warnings.warn('Compressed results cannot be memmapped',
                           stacklevel=2)
@@ -884,6 +952,24 @@ def clear(self, warn=True):
         if self.cachedir is not None:
             rm_subdirs(self.cachedir)
 
+    def reduce_size(self):
+        """Remove cache folders to make cache size fit in ``bytes_limit``."""
+        if self.cachedir is not None and self.bytes_limit is not None:
+            cache_items_to_delete = _get_cache_items_to_delete(
+                self.cachedir, self.bytes_limit)
+
+            for cache_item in cache_items_to_delete:
+                if self._verbose > 10:
+                    print('Deleting cache item {}'.format(cache_item))
+                try:
+                    shutil.rmtree(cache_item.path, ignore_errors=True)
+                except OSError:
+                    # Even with ignore_errors=True can shutil.rmtree
+                    # can raise OSErrror with [Errno 116] Stale file
+                    # handle if another process has deleted the folder
+                    # already.
+                    pass
+
     def eval(self, func, *args, **kwargs):
         """ Eval function func with arguments `*args` and `**kwargs`,
             in the context of the memory.
diff --git a/sklearn/externals/joblib/numpy_pickle.py b/sklearn/externals/joblib/numpy_pickle.py
index 0cf88a2bf3ab..87a1a616cd54 100644
--- a/sklearn/externals/joblib/numpy_pickle.py
+++ b/sklearn/externals/joblib/numpy_pickle.py
@@ -26,6 +26,7 @@
 # which we don't care.
 from .numpy_pickle_compat import ZNDArrayWrapper  # noqa
 from ._compat import _basestring, PY3_OR_LATER
+from .backports import make_memmap
 
 ###############################################################################
 # Utility objects for persistence.
@@ -151,12 +152,12 @@ def read_mmap(self, unpickler):
         if unpickler.mmap_mode == 'w+':
             unpickler.mmap_mode = 'r+'
 
-        marray = unpickler.np.memmap(unpickler.filename,
-                                     dtype=self.dtype,
-                                     shape=self.shape,
-                                     order=self.order,
-                                     mode=unpickler.mmap_mode,
-                                     offset=offset)
+        marray = make_memmap(unpickler.filename,
+                             dtype=self.dtype,
+                             shape=self.shape,
+                             order=self.order,
+                             mode=unpickler.mmap_mode,
+                             offset=offset)
         # update the offset so that it corresponds to the end of the read array
         unpickler.file_handle.seek(offset + marray.nbytes)
 
@@ -409,7 +410,7 @@ def dump(value, filename, compress=0, protocol=None, cache_size=None):
         if len(compress) != 2:
             raise ValueError(
                 'Compress argument tuple should contain exactly 2 elements: '
-                '(compress method, compress level), you passed {0}'
+                '(compress method, compress level), you passed {}'
                 .format(compress))
         compress_method, compress_level = compress
     else:
@@ -418,14 +419,14 @@ def dump(value, filename, compress=0, protocol=None, cache_size=None):
     if compress_level is not False and compress_level not in range(10):
         # Raising an error if a non valid compress level is given.
         raise ValueError(
-            'Non valid compress level given: "{0}". Possible values are '
-            '{1}.'.format(compress_level, list(range(10))))
+            'Non valid compress level given: "{}". Possible values are '
+            '{}.'.format(compress_level, list(range(10))))
 
     if compress_method not in _COMPRESSORS:
         # Raising an error if an unsupported compression method is given.
         raise ValueError(
-            'Non valid compression method given: "{0}". Possible values are '
-            '{1}.'.format(compress_method, _COMPRESSORS))
+            'Non valid compression method given: "{}". Possible values are '
+            '{}.'.format(compress_method, _COMPRESSORS))
 
     if not is_filename and not is_fileobj:
         # People keep inverting arguments, and the resulting error is
@@ -461,17 +462,17 @@ def dump(value, filename, compress=0, protocol=None, cache_size=None):
             compress_level = 3
 
     if not PY3_OR_LATER and compress_method in ('lzma', 'xz'):
-        raise NotImplementedError("{0} compression is only available for "
+        raise NotImplementedError("{} compression is only available for "
                                   "python version >= 3.3. You are using "
-                                  "{1}.{2}".format(compress_method,
-                                                   sys.version_info[0],
-                                                   sys.version_info[1]))
+                                  "{}.{}".format(compress_method,
+                                                 sys.version_info[0],
+                                                 sys.version_info[1]))
 
     if cache_size is not None:
         # Cache size is deprecated starting from version 0.10
         warnings.warn("Please do not set 'cache_size' in joblib.dump, "
                       "this parameter has no effect and will be removed. "
-                      "You used 'cache_size={0}'".format(cache_size),
+                      "You used 'cache_size={}'".format(cache_size),
                       DeprecationWarning, stacklevel=2)
 
     if compress_level != 0:
@@ -560,8 +561,10 @@ def load(filename, mmap_mode=None):
     if Path is not None and isinstance(filename, Path):
         filename = str(filename)
 
-    if hasattr(filename, "read") and hasattr(filename, "seek"):
-        with _read_fileobject(filename, "", mmap_mode) as fobj:
+    if hasattr(filename, "read"):
+        fobj = filename
+        filename = getattr(fobj, 'name', '')
+        with _read_fileobject(fobj, filename, mmap_mode) as fobj:
             obj = _unpickle(fobj)
     else:
         with open(filename, 'rb') as f:
diff --git a/sklearn/externals/joblib/numpy_pickle_compat.py b/sklearn/externals/joblib/numpy_pickle_compat.py
index 150d8f4e3845..ba8ab827914e 100644
--- a/sklearn/externals/joblib/numpy_pickle_compat.py
+++ b/sklearn/externals/joblib/numpy_pickle_compat.py
@@ -12,7 +12,7 @@
 
 def hex_str(an_int):
     """Convert an int to an hexadecimal string."""
-    return '{0:#x}'.format(an_int)
+    return '{:#x}'.format(an_int)
 
 if PY3_OR_LATER:
     def asbytes(s):
diff --git a/sklearn/externals/joblib/numpy_pickle_utils.py b/sklearn/externals/joblib/numpy_pickle_utils.py
index 6f471073e672..7196c0cbc85c 100644
--- a/sklearn/externals/joblib/numpy_pickle_utils.py
+++ b/sklearn/externals/joblib/numpy_pickle_utils.py
@@ -9,12 +9,11 @@
 import io
 import zlib
 import gzip
-import bz2
 import warnings
 import contextlib
 from contextlib import closing
 
-from ._compat import PY3_OR_LATER, PY26, PY27, _basestring
+from ._compat import PY3_OR_LATER, PY27, _basestring
 
 try:
     from threading import RLock
@@ -40,6 +39,16 @@
     lzma = None
 
 
+try:
+    # The python standard library can be built without bz2 so we make bz2
+    # usage optional.
+    # see https://github.com/scikit-learn/scikit-learn/issues/7526 for more
+    # details.
+    import bz2
+except ImportError:
+    bz2 = None
+
+
 # Magic numbers of supported compression file formats.        '
 _ZFILE_PREFIX = b'ZF'  # used with pickle files created before 0.9.3.
 _ZLIB_PREFIX = b'\x78'
@@ -50,7 +59,11 @@
 
 # Supported compressors
 _COMPRESSORS = ('zlib', 'bz2', 'lzma', 'xz', 'gzip')
-_COMPRESSOR_CLASSES = [gzip.GzipFile, bz2.BZ2File]
+_COMPRESSOR_CLASSES = [gzip.GzipFile]
+
+if bz2 is not None:
+    _COMPRESSOR_CLASSES.append(bz2.BZ2File)
+
 if lzma is not None:
     _COMPRESSOR_CLASSES.append(lzma.LZMAFile)
 
@@ -63,6 +76,15 @@
 _IO_BUFFER_SIZE = 1024 ** 2
 
 
+def _is_raw_file(fileobj):
+    """Check if fileobj is a raw file object, e.g created with open."""
+    if PY3_OR_LATER:
+        fileobj = getattr(fileobj, 'raw', fileobj)
+        return isinstance(fileobj, io.FileIO)
+    else:
+        return isinstance(fileobj, file)  # noqa
+
+
 ###############################################################################
 # Cache file utilities
 def _detect_compressor(fileobj):
@@ -76,10 +98,15 @@ def _detect_compressor(fileobj):
     -------
     str in {'zlib', 'gzip', 'bz2', 'lzma', 'xz', 'compat', 'not-compressed'}
     """
-    # Ensure we read the first bytes.
-    fileobj.seek(0)
-    first_bytes = fileobj.read(_MAX_PREFIX_LEN)
-    fileobj.seek(0)
+    # Read the magic number in the first bytes of the file.
+    if hasattr(fileobj, 'peek'):
+        # Peek allows to read those bytes without moving the cursor in the
+        # file whic.
+        first_bytes = fileobj.peek(_MAX_PREFIX_LEN)
+    else:
+        # Fallback to seek if the fileobject is not peekable.
+        first_bytes = fileobj.read(_MAX_PREFIX_LEN)
+        fileobj.seek(0)
 
     if first_bytes.startswith(_ZLIB_PREFIX):
         return "zlib"
@@ -99,8 +126,7 @@ def _detect_compressor(fileobj):
 
 def _buffered_read_file(fobj):
     """Return a buffered version of a read file object."""
-    if PY26 or (PY27 and isinstance(fobj, bz2.BZ2File)):
-        # Python 2.6 doesn't fully support io.BufferedReader.
+    if PY27 and bz2 is not None and isinstance(fobj, bz2.BZ2File):
         # Python 2.7 doesn't work with BZ2File through a buffer: "no
         # attribute 'readable'" error.
         return fobj
@@ -110,8 +136,7 @@ def _buffered_read_file(fobj):
 
 def _buffered_write_file(fobj):
     """Return a buffered version of a write file object."""
-    if PY26 or (PY27 and isinstance(fobj, bz2.BZ2File)):
-        # Python 2.6 doesn't fully support io.BufferedWriter.
+    if PY27 and bz2 is not None and isinstance(fobj, bz2.BZ2File):
         # Python 2.7 doesn't work with BZ2File through a buffer: no attribute
         # 'writable'.
         # BZ2File doesn't implement the file object context manager in python 2
@@ -151,8 +176,7 @@ def _read_fileobject(fileobj, filename, mmap_mode=None):
     """
     # Detect if the fileobj contains compressed data.
     compressor = _detect_compressor(fileobj)
-    if isinstance(fileobj, tuple(_COMPRESSOR_CLASSES)):
-        compressor = fileobj.__class__.__name__
+
     if compressor == 'compat':
         # Compatibility with old pickle mode: simply return the input
         # filename "as-is" and let the compatibility function be called by the
@@ -163,52 +187,53 @@ def _read_fileobject(fileobj, filename, mmap_mode=None):
                       DeprecationWarning, stacklevel=2)
         yield filename
     else:
-        # Checking if incompatible load parameters with the type of file:
-        # mmap_mode cannot be used with compressed file or in memory buffers
-        # such as io.BytesIO.
-        if ((compressor in _COMPRESSORS or
-                isinstance(fileobj, tuple(_COMPRESSOR_CLASSES))) and
-                mmap_mode is not None):
-            warnings.warn('File "%(filename)s" is compressed using '
-                          '"%(compressor)s" which is not compatible with '
-                          'mmap_mode "%(mmap_mode)s" flag passed. mmap_mode '
-                          'option will be ignored.'
-                          % locals(), stacklevel=2)
-        if isinstance(fileobj, io.BytesIO) and mmap_mode is not None:
-            warnings.warn('In memory persistence is not compatible with '
-                          'mmap_mode "%(mmap_mode)s" flag passed. mmap_mode '
-                          'option will be ignored.'
-                          % locals(), stacklevel=2)
-
-        # if the passed fileobj is in the supported list of decompressor
-        # objects (GzipFile, BZ2File, LzmaFile), we simply return it.
-        if isinstance(fileobj, tuple(_COMPRESSOR_CLASSES)):
-            yield fileobj
-        # otherwise, based on the compressor detected in the file, we open the
+        # based on the compressor detected in the file, we open the
         # correct decompressor file object, wrapped in a buffer.
-        elif compressor == 'zlib':
-            yield _buffered_read_file(BinaryZlibFile(fileobj, 'rb'))
+        if compressor == 'zlib':
+            fileobj = _buffered_read_file(BinaryZlibFile(fileobj, 'rb'))
         elif compressor == 'gzip':
-            yield _buffered_read_file(BinaryGzipFile(fileobj, 'rb'))
-        elif compressor == 'bz2':
+            fileobj = _buffered_read_file(BinaryGzipFile(fileobj, 'rb'))
+        elif compressor == 'bz2' and bz2 is not None:
             if PY3_OR_LATER:
-                yield _buffered_read_file(bz2.BZ2File(fileobj, 'rb'))
+                fileobj = _buffered_read_file(bz2.BZ2File(fileobj, 'rb'))
             else:
                 # In python 2, BZ2File doesn't support a fileobj opened in
                 # binary mode. In this case, we pass the filename.
-                yield _buffered_read_file(bz2.BZ2File(fileobj.name, 'rb'))
+                fileobj = _buffered_read_file(bz2.BZ2File(fileobj.name, 'rb'))
         elif (compressor == 'lzma' or compressor == 'xz'):
-            if lzma is not None:
-                yield _buffered_read_file(lzma.LZMAFile(fileobj, 'rb'))
+            if PY3_OR_LATER and lzma is not None:
+                # We support lzma only in python 3 because in python 2 users
+                # may have installed the pyliblzma package, which also provides
+                # the lzma module, but that unfortunately doesn't fully support
+                # the buffer interface required by joblib.
+                # See https://github.com/joblib/joblib/issues/403 for details.
+                fileobj = _buffered_read_file(lzma.LZMAFile(fileobj, 'rb'))
             else:
                 raise NotImplementedError("Lzma decompression is not "
-                                          "available for this version of "
-                                          "python ({0}.{1})"
+                                          "supported for this version of "
+                                          "python ({}.{})"
                                           .format(sys.version_info[0],
                                                   sys.version_info[1]))
-        # No compression detected => returning the input file object (open)
-        else:
-            yield fileobj
+        # Checking if incompatible load parameters with the type of file:
+        # mmap_mode cannot be used with compressed file or in memory buffers
+        # such as io.BytesIO.
+        if mmap_mode is not None:
+            if isinstance(fileobj, io.BytesIO):
+                warnings.warn('In memory persistence is not compatible with '
+                              'mmap_mode "%(mmap_mode)s" flag passed. '
+                              'mmap_mode option will be ignored.'
+                              % locals(), stacklevel=2)
+            elif compressor != 'not-compressed':
+                warnings.warn('mmap_mode "%(mmap_mode)s" is not compatible '
+                              'with compressed file %(filename)s. '
+                              '"%(mmap_mode)s" flag will be ignored.'
+                              % locals(), stacklevel=2)
+            elif not _is_raw_file(fileobj):
+                warnings.warn('"%(fileobj)r" is not a raw file, mmap_mode '
+                              '"%(mmap_mode)s" flag will be ignored.'
+                              % locals(), stacklevel=2)
+
+        yield fileobj
 
 
 def _write_fileobject(filename, compress=("zlib", 3)):
@@ -218,7 +243,7 @@ def _write_fileobject(filename, compress=("zlib", 3)):
     if compressmethod == "gzip":
         return _buffered_write_file(BinaryGzipFile(filename, 'wb',
                                     compresslevel=compresslevel))
-    elif compressmethod == "bz2":
+    elif compressmethod == "bz2" and bz2 is not None:
         return _buffered_write_file(bz2.BZ2File(filename, 'wb',
                                                 compresslevel=compresslevel))
     elif lzma is not None and compressmethod == "xz":
@@ -254,7 +279,7 @@ class BinaryZlibFile(io.BufferedIOBase):
     is returned as bytes, and data to be written should be given as bytes.
 
     This object is an adaptation of the BZ2File object and is compatible with
-    versions of python >= 2.6.
+    versions of python >= 2.7.
 
     If filename is a str or bytes object, it gives the name
     of the file to be opened. Otherwise, it should be a file object,
@@ -280,8 +305,8 @@ def __init__(self, filename, mode="rb", compresslevel=9):
         self._size = -1
 
         if not isinstance(compresslevel, int) or not (1 <= compresslevel <= 9):
-            raise ValueError("compresslevel must be between an integer "
-                             "between 1 and 9, you gave {0}"
+            raise ValueError("'compresslevel' must be an integer "
+                             "between 1 and 9. You provided 'compresslevel={}'"
                              .format(compresslevel))
 
         if mode == "rb":
@@ -300,7 +325,7 @@ def __init__(self, filename, mode="rb", compresslevel=9):
             raise ValueError("Invalid mode: %r" % (mode,))
 
         if isinstance(filename, _basestring):
-            self._fp = open(filename, mode)
+            self._fp = io.open(filename, mode)
             self._closefp = True
             self._mode = mode_code
         elif hasattr(filename, "read") or hasattr(filename, "write"):
@@ -367,7 +392,7 @@ def _check_not_closed(self):
             fname = getattr(self._fp, 'name', None)
             msg = "I/O operation on closed file"
             if fname is not None:
-                msg += " {0}".format(fname)
+                msg += " {}".format(fname)
             msg += "."
             raise ValueError(msg)
 
@@ -492,7 +517,7 @@ def write(self, data):
         with self._lock:
             self._check_can_write()
             # Convert data type if called by io.BufferedWriter.
-            if not PY26 and isinstance(data, memoryview):
+            if isinstance(data, memoryview):
                 data = data.tobytes()
 
             compressed = self._compressor.compress(data)
diff --git a/sklearn/externals/joblib/parallel.py b/sklearn/externals/joblib/parallel.py
index 74bd18c4fb25..73e681b870dd 100644
--- a/sklearn/externals/joblib/parallel.py
+++ b/sklearn/externals/joblib/parallel.py
@@ -16,9 +16,10 @@
 import itertools
 from numbers import Integral
 from contextlib import contextmanager
+import warnings
 try:
     import cPickle as pickle
-except:
+except ImportError:
     import pickle
 
 from ._multiprocessing_helpers import mp
@@ -30,7 +31,6 @@
 from ._parallel_backends import (FallbackToBackend, MultiprocessingBackend,
                                  ThreadingBackend, SequentialBackend)
 from ._compat import _basestring
-from .func_inspect import getfullargspec
 
 # Make sure that those two classes are part of the public joblib.parallel API
 # so that 3rd party backend implementers can import them from here.
@@ -282,22 +282,24 @@ class Parallel(Logger):
             is used at all, which is useful for debugging. For n_jobs below -1,
             (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all
             CPUs but one are used.
-        backend: str or None, default: 'multiprocessing'
+        backend: str, ParallelBackendBase instance or None, \
+                default: 'multiprocessing'
             Specify the parallelization backend implementation.
             Supported backends are:
-              - "multiprocessing" used by default, can induce some
-                communication and memory overhead when exchanging input and
-                output data with the worker Python processes.
-              - "threading" is a very low-overhead backend but it suffers
-                from the Python Global Interpreter Lock if the called function
-                relies a lot on Python objects. "threading" is mostly useful
-                when the execution bottleneck is a compiled extension that
-                explicitly releases the GIL (for instance a Cython loop wrapped
-                in a "with nogil" block or an expensive call to a library such
-                as NumPy).
-              - finally, you can register backends by calling
-                register_parallel_backend. This will allow you to implement
-                a backend of your liking.
+
+            - "multiprocessing" used by default, can induce some
+              communication and memory overhead when exchanging input and
+              output data with the worker Python processes.
+            - "threading" is a very low-overhead backend but it suffers
+              from the Python Global Interpreter Lock if the called function
+              relies a lot on Python objects. "threading" is mostly useful
+              when the execution bottleneck is a compiled extension that
+              explicitly releases the GIL (for instance a Cython loop wrapped
+              in a "with nogil" block or an expensive call to a library such
+              as NumPy).
+            - finally, you can register backends by calling
+              register_parallel_backend. This will allow you to implement
+              a backend of your liking.
         verbose: int, optional
             The verbosity level: if non zero, progress messages are
             printed. Above 50, the output is sent to stdout.
@@ -327,12 +329,16 @@ class Parallel(Logger):
             Folder to be used by the pool for memmaping large arrays
             for sharing memory with worker processes. If None, this will try in
             order:
-            - a folder pointed by the JOBLIB_TEMP_FOLDER environment variable,
-            - /dev/shm if the folder exists and is writable: this is a RAMdisk
-              filesystem available by default on modern Linux distributions,
-            - the default system temporary folder that can be overridden
-              with TMP, TMPDIR or TEMP environment variables, typically /tmp
-              under Unix operating systems.
+
+            - a folder pointed by the JOBLIB_TEMP_FOLDER environment
+              variable,
+            - /dev/shm if the folder exists and is writable: this is a
+              RAMdisk filesystem available by default on modern Linux
+              distributions,
+            - the default system temporary folder that can be
+              overridden with TMP, TMPDIR or TEMP environment
+              variables, typically /tmp under Unix operating systems.
+
             Only active when backend="multiprocessing".
         max_nbytes int, str, or None, optional, 1M by default
             Threshold on the size of arrays passed to the workers that
@@ -352,25 +358,25 @@ class Parallel(Logger):
         arguments. The main functionality it brings in addition to
         using the raw multiprocessing API are (see examples for details):
 
-            * More readable code, in particular since it avoids
-              constructing list of arguments.
+        * More readable code, in particular since it avoids
+          constructing list of arguments.
 
-            * Easier debugging:
-                - informative tracebacks even when the error happens on
-                  the client side
-                - using 'n_jobs=1' enables to turn off parallel computing
-                  for debugging without changing the codepath
-                - early capture of pickling errors
+        * Easier debugging:
+            - informative tracebacks even when the error happens on
+              the client side
+            - using 'n_jobs=1' enables to turn off parallel computing
+              for debugging without changing the codepath
+            - early capture of pickling errors
 
-            * An optional progress meter.
+        * An optional progress meter.
 
-            * Interruption of multiprocesses jobs with 'Ctrl-C'
+        * Interruption of multiprocesses jobs with 'Ctrl-C'
 
-            * Flexible pickling control for the communication to and from
-              the worker processes.
+        * Flexible pickling control for the communication to and from
+          the worker processes.
 
-            * Ability to use shared memory efficiently with worker
-              processes for large numpy-based datastructures.
+        * Ability to use shared memory efficiently with worker
+          processes for large numpy-based datastructures.
 
         Examples
         --------
@@ -395,76 +401,74 @@ class Parallel(Logger):
         (0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.0)
 
         The progress meter: the higher the value of `verbose`, the more
-        messages::
+        messages:
 
-            >>> from time import sleep
-            >>> from sklearn.externals.joblib import Parallel, delayed
-            >>> r = Parallel(n_jobs=2, verbose=5)(delayed(sleep)(.1) for _ in range(10)) #doctest: +SKIP
-            [Parallel(n_jobs=2)]: Done   1 out of  10 | elapsed:    0.1s remaining:    0.9s
-            [Parallel(n_jobs=2)]: Done   3 out of  10 | elapsed:    0.2s remaining:    0.5s
-            [Parallel(n_jobs=2)]: Done   6 out of  10 | elapsed:    0.3s remaining:    0.2s
-            [Parallel(n_jobs=2)]: Done   9 out of  10 | elapsed:    0.5s remaining:    0.1s
-            [Parallel(n_jobs=2)]: Done  10 out of  10 | elapsed:    0.5s finished
+        >>> from time import sleep
+        >>> from sklearn.externals.joblib import Parallel, delayed
+        >>> r = Parallel(n_jobs=2, verbose=5)(delayed(sleep)(.1) for _ in range(10)) #doctest: +SKIP
+        [Parallel(n_jobs=2)]: Done   1 out of  10 | elapsed:    0.1s remaining:    0.9s
+        [Parallel(n_jobs=2)]: Done   3 out of  10 | elapsed:    0.2s remaining:    0.5s
+        [Parallel(n_jobs=2)]: Done   6 out of  10 | elapsed:    0.3s remaining:    0.2s
+        [Parallel(n_jobs=2)]: Done   9 out of  10 | elapsed:    0.5s remaining:    0.1s
+        [Parallel(n_jobs=2)]: Done  10 out of  10 | elapsed:    0.5s finished
 
         Traceback example, note how the line of the error is indicated
         as well as the values of the parameter passed to the function that
         triggered the exception, even though the traceback happens in the
-        child process::
-
-         >>> from heapq import nlargest
-         >>> from sklearn.externals.joblib import Parallel, delayed
-         >>> Parallel(n_jobs=2)(delayed(nlargest)(2, n) for n in (range(4), 'abcde', 3)) #doctest: +SKIP
-         #...
-         ---------------------------------------------------------------------------
-         Sub-process traceback:
-         ---------------------------------------------------------------------------
-         TypeError                                          Mon Nov 12 11:37:46 2012
-         PID: 12934                                    Python 2.7.3: /usr/bin/python
-         ...........................................................................
-         /usr/lib/python2.7/heapq.pyc in nlargest(n=2, iterable=3, key=None)
-             419         if n >= size:
-             420             return sorted(iterable, key=key, reverse=True)[:n]
-             421
-             422     # When key is none, use simpler decoration
-             423     if key is None:
-         --> 424         it = izip(iterable, count(0,-1))                    # decorate
-             425         result = _nlargest(n, it)
-             426         return map(itemgetter(0), result)                   # undecorate
-             427
-             428     # General case, slowest method
+        child process:
 
+        >>> from heapq import nlargest
+        >>> from sklearn.externals.joblib import Parallel, delayed
+        >>> Parallel(n_jobs=2)(delayed(nlargest)(2, n) for n in (range(4), 'abcde', 3)) #doctest: +SKIP
+        #...
+        ---------------------------------------------------------------------------
+        Sub-process traceback:
+        ---------------------------------------------------------------------------
+        TypeError                                          Mon Nov 12 11:37:46 2012
+        PID: 12934                                    Python 2.7.3: /usr/bin/python
+        ...........................................................................
+        /usr/lib/python2.7/heapq.pyc in nlargest(n=2, iterable=3, key=None)
+            419         if n >= size:
+            420             return sorted(iterable, key=key, reverse=True)[:n]
+            421
+            422     # When key is none, use simpler decoration
+            423     if key is None:
+        --> 424         it = izip(iterable, count(0,-1))                    # decorate
+            425         result = _nlargest(n, it)
+            426         return map(itemgetter(0), result)                   # undecorate
+            427
+            428     # General case, slowest method
          TypeError: izip argument #1 must support iteration
-         ___________________________________________________________________________
+        ___________________________________________________________________________
 
 
         Using pre_dispatch in a producer/consumer situation, where the
         data is generated on the fly. Note how the producer is first
-        called a 3 times before the parallel loop is initiated, and then
+        called 3 times before the parallel loop is initiated, and then
         called to generate new data on the fly. In this case the total
-        number of iterations cannot be reported in the progress messages::
-
-         >>> from math import sqrt
-         >>> from sklearn.externals.joblib import Parallel, delayed
-
-         >>> def producer():
-         ...     for i in range(6):
-         ...         print('Produced %s' % i)
-         ...         yield i
-
-         >>> out = Parallel(n_jobs=2, verbose=100, pre_dispatch='1.5*n_jobs')(
-         ...                         delayed(sqrt)(i) for i in producer()) #doctest: +SKIP
-         Produced 0
-         Produced 1
-         Produced 2
-         [Parallel(n_jobs=2)]: Done 1 jobs     | elapsed:  0.0s
-         Produced 3
-         [Parallel(n_jobs=2)]: Done 2 jobs     | elapsed:  0.0s
-         Produced 4
-         [Parallel(n_jobs=2)]: Done 3 jobs     | elapsed:  0.0s
-         Produced 5
-         [Parallel(n_jobs=2)]: Done 4 jobs     | elapsed:  0.0s
-         [Parallel(n_jobs=2)]: Done 5 out of 6 | elapsed:  0.0s remaining: 0.0s
-         [Parallel(n_jobs=2)]: Done 6 out of 6 | elapsed:  0.0s finished
+        number of iterations cannot be reported in the progress messages:
+
+        >>> from math import sqrt
+        >>> from sklearn.externals.joblib import Parallel, delayed
+        >>> def producer():
+        ...     for i in range(6):
+        ...         print('Produced %s' % i)
+        ...         yield i
+        >>> out = Parallel(n_jobs=2, verbose=100, pre_dispatch='1.5*n_jobs')(
+        ...                delayed(sqrt)(i) for i in producer()) #doctest: +SKIP
+        Produced 0
+        Produced 1
+        Produced 2
+        [Parallel(n_jobs=2)]: Done 1 jobs     | elapsed:  0.0s
+        Produced 3
+        [Parallel(n_jobs=2)]: Done 2 jobs     | elapsed:  0.0s
+        Produced 4
+        [Parallel(n_jobs=2)]: Done 3 jobs     | elapsed:  0.0s
+        Produced 5
+        [Parallel(n_jobs=2)]: Done 4 jobs     | elapsed:  0.0s
+        [Parallel(n_jobs=2)]: Done 5 out of 6 | elapsed:  0.0s remaining: 0.0s
+        [Parallel(n_jobs=2)]: Done 6 out of 6 | elapsed:  0.0s finished
+
     '''
     def __init__(self, n_jobs=1, backend=None, verbose=0, timeout=None,
                  pre_dispatch='2 * n_jobs', batch_size='auto',
@@ -493,6 +497,9 @@ def __init__(self, n_jobs=1, backend=None, verbose=0, timeout=None,
 
         if backend is None:
             backend = active_backend
+        elif isinstance(backend, ParallelBackendBase):
+            # Use provided backend as is
+            pass
         elif hasattr(backend, 'Pool') and hasattr(backend, 'Lock'):
             # Make it possible to pass a custom multiprocessing context as
             # backend to change the start method to forkserver or spawn or
@@ -536,12 +543,22 @@ def __exit__(self, exc_type, exc_value, traceback):
     def _initialize_backend(self):
         """Build a process or thread pool and return the number of workers"""
         try:
-            return self._backend.configure(n_jobs=self.n_jobs, parallel=self,
-                                           **self._backend_args)
+            n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,
+                                             **self._backend_args)
+            if self.timeout is not None and not self._backend.supports_timeout:
+                warnings.warn(
+                    'The backend class {!r} does not support timeout. '
+                    "You have set 'timeout={}' in Parallel but "
+                    "the 'timeout' parameter will not be used.".format(
+                        self._backend.__class__.__name__,
+                        self.timeout))
+
         except FallbackToBackend as e:
             # Recursively initialize the backend in case of requested fallback.
             self._backend = e.backend
-            return self._initialize_backend()
+            n_jobs = self._initialize_backend()
+
+        return n_jobs
 
     def _effective_n_jobs(self):
         if self._backend:
@@ -676,12 +693,13 @@ def retrieve(self):
             # the use of the lock
             with self._lock:
                 job = self._jobs.pop(0)
+
             try:
-                # check if timeout supported in backend future implementation
-                if 'timeout' in getfullargspec(job.get).args:
+                if getattr(self._backend, 'supports_timeout', False):
                     self._output.extend(job.get(timeout=self.timeout))
                 else:
                     self._output.extend(job.get())
+
             except BaseException as exception:
                 # Note: we catch any BaseException instead of just Exception
                 # instances to also include KeyboardInterrupt.
@@ -689,7 +707,22 @@ def retrieve(self):
                 # Stop dispatching any new job in the async callback thread
                 self._aborting = True
 
-                if isinstance(exception, TransportableException):
+                # If the backend allows it, cancel or kill remaining running
+                # tasks without waiting for the results as we will raise
+                # the exception we got back to the caller instead of returning
+                # any result.
+                backend = self._backend
+                if (backend is not None and
+                        hasattr(backend, 'abort_everything')):
+                    # If the backend is managed externally we need to make sure
+                    # to leave it in a working state to allow for future jobs
+                    # scheduling.
+                    ensure_ready = self._managed_backend
+                    backend.abort_everything(ensure_ready=ensure_ready)
+
+                if not isinstance(exception, TransportableException):
+                    raise
+                else:
                     # Capture exception to add information on the local
                     # stack in addition to the distant stack
                     this_report = format_outer_frames(context=10,
@@ -704,19 +737,7 @@ def retrieve(self):
                     exception_type = _mk_exception(exception.etype)[0]
                     exception = exception_type(report)
 
-                # If the backends allows it, cancel or kill remaining running
-                # tasks without waiting for the results as we will raise
-                # the exception we got back to the caller instead of returning
-                # any result.
-                backend = self._backend
-                if (backend is not None and
-                        hasattr(backend, 'abort_everything')):
-                    # If the backend is managed externally we need to make sure
-                    # to leave it in a working state to allow for future jobs
-                    # scheduling.
-                    ensure_ready = self._managed_backend
-                    backend.abort_everything(ensure_ready=ensure_ready)
-                raise exception
+                    raise exception
 
     def __call__(self, iterable):
         if self._jobs:
diff --git a/sklearn/externals/joblib/pool.py b/sklearn/externals/joblib/pool.py
index e0682c182231..c53a12dfa768 100644
--- a/sklearn/externals/joblib/pool.py
+++ b/sklearn/externals/joblib/pool.py
@@ -28,7 +28,7 @@
 try:
     WindowsError
 except NameError:
-    WindowsError = None
+    WindowsError = type(None)
 
 from pickle import whichmodule
 try:
@@ -61,7 +61,7 @@
 from .numpy_pickle import load
 from .numpy_pickle import dump
 from .hashing import hash
-
+from .backports import make_memmap
 # Some system have a ramdisk mounted by default, we can use it instead of /tmp
 # as the default folder to dump big arrays to share with subprocesses
 SYSTEM_SHARED_MEM_FS = '/dev/shm'
@@ -107,13 +107,13 @@ def _strided_from_memmap(filename, dtype, mode, offset, order, shape, strides,
 
     if strides is None:
         # Simple, contiguous memmap
-        return np.memmap(filename, dtype=dtype, shape=shape, mode=mode,
-                         offset=offset, order=order)
+        return make_memmap(filename, dtype=dtype, shape=shape, mode=mode,
+                           offset=offset, order=order)
     else:
         # For non-contiguous data, memmap the total enclosing buffer and then
         # extract the non-contiguous view with the stride-tricks API
-        base = np.memmap(filename, dtype=dtype, shape=total_buffer_len,
-                         mode=mode, offset=offset, order=order)
+        base = make_memmap(filename, dtype=dtype, shape=total_buffer_len,
+                           mode=mode, offset=offset, order=order)
         return as_strided(base, shape=shape, strides=strides)
 
 
@@ -279,7 +279,7 @@ class CustomizablePickler(Pickler):
     """
 
     # We override the pure Python pickler as its the only way to be able to
-    # customize the dispatch table without side effects in Python 2.6
+    # customize the dispatch table without side effects in Python 2.7
     # to 3.2. For Python 3.3+ leverage the new dispatch_table
     # feature from http://bugs.python.org/issue14166 that makes it possible
     # to use the C implementation of the Pickler which is faster.
@@ -605,11 +605,12 @@ def terminate(self):
             try:
                 super(MemmapingPool, self).terminate()
                 break
-            except WindowsError as e:
-                # Workaround  occasional "[Error 5] Access is denied" issue
-                # when trying to terminate a process under windows.
-                sleep(0.1)
-                if i + 1 == n_retries:
-                    warnings.warn("Failed to terminate worker processes in "
-                                  " multiprocessing pool: %r" % e)
+            except OSError as e:
+                if isinstance(e, WindowsError):
+                    # Workaround  occasional "[Error 5] Access is denied" issue
+                    # when trying to terminate a process under windows.
+                    sleep(0.1)
+                    if i + 1 == n_retries:
+                        warnings.warn("Failed to terminate worker processes in"
+                                      " multiprocessing pool: %r" % e)
         delete_folder(self._temp_folder)
diff --git a/sklearn/externals/joblib/testing.py b/sklearn/externals/joblib/testing.py
deleted file mode 100644
index 94c023c3f0ae..000000000000
--- a/sklearn/externals/joblib/testing.py
+++ /dev/null
@@ -1,93 +0,0 @@
-"""
-Helper for testing.
-"""
-
-import sys
-import warnings
-import os.path
-import re
-import subprocess
-import threading
-
-from sklearn.externals.joblib._compat import PY3_OR_LATER
-
-
-def warnings_to_stdout():
-    """ Redirect all warnings to stdout.
-    """
-    showwarning_orig = warnings.showwarning
-
-    def showwarning(msg, cat, fname, lno, file=None, line=0):
-        showwarning_orig(msg, cat, os.path.basename(fname), line, sys.stdout)
-
-    warnings.showwarning = showwarning
-    #warnings.simplefilter('always')
-
-
-try:
-    from nose.tools import assert_raises_regex
-except ImportError:
-    # For Python 2.7
-    try:
-        from nose.tools import assert_raises_regexp as assert_raises_regex
-    except ImportError:
-        # for Python 2.6
-        def assert_raises_regex(expected_exception, expected_regexp,
-                                callable_obj=None, *args, **kwargs):
-            """Helper function to check for message patterns in exceptions"""
-
-            not_raised = False
-            try:
-                callable_obj(*args, **kwargs)
-                not_raised = True
-            except Exception as e:
-                error_message = str(e)
-                if not re.compile(expected_regexp).search(error_message):
-                    raise AssertionError("Error message should match pattern "
-                                         "%r. %r does not." %
-                                         (expected_regexp, error_message))
-            if not_raised:
-                raise AssertionError("Should have raised %r" %
-                                     expected_exception(expected_regexp))
-
-
-def check_subprocess_call(cmd, timeout=1, stdout_regex=None,
-                          stderr_regex=None):
-    """Runs a command in a subprocess with timeout in seconds.
-
-    Also checks returncode is zero, stdout if stdout_regex is set, and
-    stderr if stderr_regex is set.
-    """
-    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE,
-                            stderr=subprocess.PIPE)
-
-    def kill_process():
-        proc.kill()
-
-    timer = threading.Timer(timeout, kill_process)
-    try:
-        timer.start()
-        stdout, stderr = proc.communicate()
-
-        if PY3_OR_LATER:
-            stdout, stderr = stdout.decode(), stderr.decode()
-        if proc.returncode != 0:
-            message = (
-                'Non-zero return code: {0}.\nStdout:\n{1}\n'
-                'Stderr:\n{2}').format(
-                    proc.returncode, stdout, stderr)
-            raise ValueError(message)
-
-        if (stdout_regex is not None and
-                not re.search(stdout_regex, stdout)):
-            raise ValueError(
-                "Unexpected stdout: {0!r} does not match:\n{1!r}".format(
-                    stdout_regex, stdout))
-        if (stderr_regex is not None and
-                not re.search(stderr_regex, stderr)):
-            raise ValueError(
-                "Unexpected stderr: {0!r} does not match:\n{1!r}".format(
-                    stderr_regex, stderr))
-
-    finally:
-        timer.cancel()
diff --git a/sklearn/feature_extraction/image.py b/sklearn/feature_extraction/image.py
index 694c624f1111..708424cb3f84 100644
--- a/sklearn/feature_extraction/image.py
+++ b/sklearn/feature_extraction/image.py
@@ -319,9 +319,12 @@ def extract_patches_2d(image, patch_size, max_patches=None, random_state=None):
         between 0 and 1, it is taken to be a proportion of the total number
         of patches.
 
-    random_state : int or RandomState
+    random_state : int, RandomState instance or None, optional (default=None)
         Pseudo number generator state used for random sampling to use if
-        `max_patches` is not None.
+        `max_patches` is not None.  If int, random_state is the seed used by
+        the random number generator; If RandomState instance, random_state is
+        the random number generator; If None, the random number generator is
+        the RandomState instance used by `np.random`.
 
     Returns
     -------
@@ -450,8 +453,11 @@ class PatchExtractor(BaseEstimator):
         float in (0, 1), it is taken to mean a proportion of the total number
         of patches.
 
-    random_state : int or RandomState
-        Pseudo number generator state used for random sampling.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     """
     def __init__(self, patch_size=None, max_patches=None, random_state=None):
diff --git a/sklearn/feature_selection/mutual_info_.py b/sklearn/feature_selection/mutual_info_.py
index b72e884704c5..0637f784c5f9 100644
--- a/sklearn/feature_selection/mutual_info_.py
+++ b/sklearn/feature_selection/mutual_info_.py
@@ -224,9 +224,13 @@ def _estimate_mi(X, y, discrete_features='auto', discrete_target=False,
         Whether to make a copy of the given data. If set to False, the initial
         data will be overwritten.
 
-    random_state : int seed, RandomState instance or None, default None
+    random_state : int, RandomState instance or None, optional, default None
         The seed of the pseudo random number generator for adding small noise
-        to continuous variables in order to remove repeated values.
+        to continuous variables in order to remove repeated values.  If int,
+        random_state is the seed used by the random number generator; If
+        RandomState instance, random_state is the random number generator; If
+        None, the random number generator is the RandomState instance used by
+        `np.random`.
 
     Returns
     -------
@@ -327,9 +331,13 @@ def mutual_info_regression(X, y, discrete_features='auto', n_neighbors=3,
         Whether to make a copy of the given data. If set to False, the initial
         data will be overwritten.
 
-    random_state : int seed, RandomState instance or None, default None
+    random_state : int, RandomState instance or None, optional, default None
         The seed of the pseudo random number generator for adding small noise
         to continuous variables in order to remove repeated values.
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     Returns
     -------
@@ -402,9 +410,13 @@ def mutual_info_classif(X, y, discrete_features='auto', n_neighbors=3,
         Whether to make a copy of the given data. If set to False, the initial
         data will be overwritten.
 
-    random_state : int seed, RandomState instance or None, default None
+    random_state : int, RandomState instance or None, optional, default None
         The seed of the pseudo random number generator for adding small noise
-        to continuous variables in order to remove repeated values.
+        to continuous variables in order to remove repeated values.  If int,
+        random_state is the seed used by the random number generator; If
+        RandomState instance, random_state is the random number generator; If
+        None, the random number generator is the RandomState instance used by
+        `np.random`.
 
     Returns
     -------
diff --git a/sklearn/feature_selection/univariate_selection.py b/sklearn/feature_selection/univariate_selection.py
index f1d6047f0b55..73b0821bdea1 100644
--- a/sklearn/feature_selection/univariate_selection.py
+++ b/sklearn/feature_selection/univariate_selection.py
@@ -230,17 +230,18 @@ def chi2(X, y):
 def f_regression(X, y, center=True):
     """Univariate linear regression tests.
 
-    Quick linear model for testing the effect of a single regressor,
-    sequentially for many regressors.
+    Linear model for testing the individual effect of each of many regressors.
+    This is a scoring function to be used in a feature seletion procedure, not
+    a free standing feature selection procedure.
 
     This is done in 2 steps:
 
-    1. The cross correlation between each regressor and the target is computed,
+    1. The correlation between each regressor and the target is computed,
        that is, ((X[:, i] - mean(X[:, i])) * (y - mean_y)) / (std(X[:, i]) *
        std(y)).
     2. It is converted to an F score then to a p-value.
 
-    Read more in the :ref:`User Guide <univariate_feature_selection>`.
+    For more on usage see the :ref:`User Guide <univariate_feature_selection>`.
 
     Parameters
     ----------
@@ -261,10 +262,18 @@ def f_regression(X, y, center=True):
     pval : array, shape=(n_features,)
         p-values of F-scores.
 
+
     See also
     --------
+    mutual_info_regression: Mutual information for a continuous target.
     f_classif: ANOVA F-value between label/feature for classification tasks.
     chi2: Chi-squared stats of non-negative features for classification tasks.
+    SelectKBest: Select features based on the k highest scores.
+    SelectFpr: Select features based on a false positive rate test.
+    SelectFdr: Select features based on an estimated false discovery rate.
+    SelectFwe: Select features based on family-wise error rate.
+    SelectPercentile: Select features based on percentile of the highest
+        scores.
     """
     X, y = check_X_y(X, y, ['csr', 'csc', 'coo'], dtype=np.float64)
     n_samples = X.shape[0]
@@ -465,7 +474,7 @@ class SelectKBest(_BaseFilter):
     mutual_info_classif: Mutual information for a discrete target.
     chi2: Chi-squared stats of non-negative features for classification tasks.
     f_regression: F-value between label/feature for regression tasks.
-    mutual_info_regression: Mutual information for a continious target.
+    mutual_info_regression: Mutual information for a continuous target.
     SelectPercentile: Select features based on percentile of the highest scores.
     SelectFpr: Select features based on a false positive rate test.
     SelectFdr: Select features based on an estimated false discovery rate.
diff --git a/sklearn/gaussian_process/gaussian_process.py b/sklearn/gaussian_process/gaussian_process.py
index 0d1b6d4fffe7..7adac552a5c1 100644
--- a/sklearn/gaussian_process/gaussian_process.py
+++ b/sklearn/gaussian_process/gaussian_process.py
@@ -169,11 +169,12 @@ class GaussianProcess(BaseEstimator, RegressorMixin):
         exponential distribution (log-uniform on [thetaL, thetaU]).
         Default does not use random starting point (random_start = 1).
 
-    random_state : integer or numpy.RandomState, optional
+    random_state : int, RandomState instance or None, optional (default=None)
         The generator used to shuffle the sequence of coordinates of theta in
-        the Welch optimizer. If an integer is given, it fixes the seed.
-        Defaults to the global numpy random number generator.
-
+        the Welch optimizer. If int, random_state is the seed used by the
+        random number generator; If RandomState instance, random_state is the
+        random number generator; If None, the random number generator is the
+        RandomState instance used by `np.random`.
 
     Attributes
     ----------
diff --git a/sklearn/gaussian_process/gpc.py b/sklearn/gaussian_process/gpc.py
index bbb1feda98e0..6f491b376e1d 100644
--- a/sklearn/gaussian_process/gpc.py
+++ b/sklearn/gaussian_process/gpc.py
@@ -106,10 +106,11 @@ def optimizer(obj_func, initial_theta, bounds):
         which might cause predictions to change if the data is modified
         externally.
 
-    random_state : integer or numpy.RandomState, optional
-        The generator used to initialize the centers. If an integer is
-        given, it fixes the seed. Defaults to the global numpy random
-        number generator.
+    random_state : int, RandomState instance or None, optional (default: None)
+        The generator used to initialize the centers. If int, random_state is
+        the seed used by the random number generator; If RandomState instance,
+        random_state is the random number generator; If None, the random number
+        generator is the RandomState instance used by `np.random`.
 
     Attributes
     ----------
@@ -510,10 +511,12 @@ def optimizer(obj_func, initial_theta, bounds):
         which might cause predictions to change if the data is modified
         externally.
 
-    random_state : integer or numpy.RandomState, optional
-        The generator used to initialize the centers. If an integer is
-        given, it fixes the seed. Defaults to the global numpy random
-        number generator.
+    random_state : int, RandomState instance or None, optional (default: None)
+        The generator used to initialize the centers.
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     multi_class : string, default : "one_vs_rest"
         Specifies how multi-class classification problems are handled.
diff --git a/sklearn/gaussian_process/gpr.py b/sklearn/gaussian_process/gpr.py
index a0f8ff249f0c..4ee8e556c706 100644
--- a/sklearn/gaussian_process/gpr.py
+++ b/sklearn/gaussian_process/gpr.py
@@ -103,10 +103,11 @@ def optimizer(obj_func, initial_theta, bounds):
         which might cause predictions to change if the data is modified
         externally.
 
-    random_state : integer or numpy.RandomState, optional
-        The generator used to initialize the centers. If an integer is
-        given, it fixes the seed. Defaults to the global numpy random
-        number generator.
+    random_state : int, RandomState instance or None, optional (default: None)
+        The generator used to initialize the centers. If int, random_state is
+        the seed used by the random number generator; If RandomState instance,
+        random_state is the random number generator; If None, the random number
+        generator is the RandomState instance used by `np.random`.
 
     Attributes
     ----------
@@ -312,7 +313,7 @@ def predict(self, X, return_std=False, return_cov=False):
                 K_inv = L_inv.dot(L_inv.T)
                 # Compute variance of predictive distribution
                 y_var = self.kernel_.diag(X)
-                y_var -= np.einsum("ki,kj,ij->k", K_trans, K_trans, K_inv)
+                y_var -= np.einsum("ij,ij->i", np.dot(K_trans, K_inv), K_trans)
 
                 # Check if any of the variances is negative because of
                 # numerical issues. If yes: set the variance to 0.
@@ -336,8 +337,11 @@ def sample_y(self, X, n_samples=1, random_state=0):
         n_samples : int, default: 1
             The number of samples drawn from the Gaussian process
 
-        random_state : RandomState or an int seed (0 by default)
-            A random number generator instance
+        random_state : int, RandomState instance or None, optional (default=0)
+            If int, random_state is the seed used by the random number
+            generator; If RandomState instance, random_state is the
+            random number generator; If None, the random number
+            generator is the RandomState instance used by `np.random`.
 
         Returns
         -------
diff --git a/sklearn/gaussian_process/tests/test_kernels.py b/sklearn/gaussian_process/tests/test_kernels.py
index c759f813104a..a07a406a415d 100644
--- a/sklearn/gaussian_process/tests/test_kernels.py
+++ b/sklearn/gaussian_process/tests/test_kernels.py
@@ -283,8 +283,9 @@ def test_set_get_params():
         index = 0
         params = kernel.get_params()
         for hyperparameter in kernel.hyperparameters:
-            if hyperparameter.bounds == "fixed":
-                continue
+            if isinstance("string", type(hyperparameter.bounds)):
+                if hyperparameter.bounds == "fixed":
+                    continue
             size = hyperparameter.n_elements
             if size > 1:  # anisotropic kernels
                 assert_almost_equal(np.exp(kernel.theta[index:index + size]),
@@ -298,8 +299,9 @@ def test_set_get_params():
         index = 0
         value = 10  # arbitrary value
         for hyperparameter in kernel.hyperparameters:
-            if hyperparameter.bounds == "fixed":
-                continue
+            if isinstance("string", type(hyperparameter.bounds)):
+                if hyperparameter.bounds == "fixed":
+                    continue
             size = hyperparameter.n_elements
             if size > 1:  # anisotropic kernels
                 kernel.set_params(**{hyperparameter.name: [value] * size})
diff --git a/sklearn/grid_search.py b/sklearn/grid_search.py
index 5bdff14f83a7..2f432362e37e 100644
--- a/sklearn/grid_search.py
+++ b/sklearn/grid_search.py
@@ -200,9 +200,13 @@ class ParameterSampler(object):
     n_iter : integer
         Number of parameter settings that are produced.
 
-    random_state : int or RandomState
+    random_state : int, RandomState instance or None, optional (default=None)
         Pseudo random number generator state used for random uniform sampling
         from lists of possible values instead of scipy.stats distributions.
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     Returns
     -------
@@ -954,9 +958,13 @@ class RandomizedSearchCV(BaseSearchCV):
     verbose : integer
         Controls the verbosity: the higher, the more messages.
 
-    random_state : int or RandomState
+    random_state : int, RandomState instance or None, optional, default=None
         Pseudo random number generator state used for random uniform sampling
         from lists of possible values instead of scipy.stats distributions.
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     error_score : 'raise' (default) or numeric
         Value to assign to the score if an error occurs in estimator fitting.
diff --git a/sklearn/isotonic.py b/sklearn/isotonic.py
index f3e86b981342..910c23508592 100644
--- a/sklearn/isotonic.py
+++ b/sklearn/isotonic.py
@@ -57,7 +57,7 @@ def check_increasing(x, y):
     increasing_bool = rho >= 0
 
     # Run Fisher transform to get the rho CI, but handle rho=+/-1
-    if rho not in [-1.0, 1.0]:
+    if rho not in [-1.0, 1.0] and len(x) > 3:
         F = 0.5 * math.log((1. + rho) / (1. - rho))
         F_se = 1 / math.sqrt(len(x) - 3)
 
diff --git a/sklearn/kernel_approximation.py b/sklearn/kernel_approximation.py
index a47016e448c8..225a8ff67374 100644
--- a/sklearn/kernel_approximation.py
+++ b/sklearn/kernel_approximation.py
@@ -38,9 +38,11 @@ class RBFSampler(BaseEstimator, TransformerMixin):
         Number of Monte Carlo samples per original feature.
         Equals the dimensionality of the computed feature space.
 
-    random_state : {int, RandomState}, optional
+    random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
-        if RandomState instance, random_state is the random number generator.
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     Notes
     -----
@@ -124,9 +126,11 @@ class SkewedChi2Sampler(BaseEstimator, TransformerMixin):
         number of Monte Carlo samples per original feature.
         Equals the dimensionality of the computed feature space.
 
-    random_state : {int, RandomState}, optional
+    random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
-        if RandomState instance, random_state is the random number generator.
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     References
     ----------
@@ -378,8 +382,8 @@ class Nystroem(BaseEstimator, TransformerMixin):
         How many data points will be used to construct the mapping.
 
     gamma : float, default=None
-        Gamma parameter for the RBF, polynomial, exponential chi2 and
-        sigmoid kernels. Interpretation of the default value is left to
+        Gamma parameter for the RBF, laplacian, polynomial, exponential chi2
+        and sigmoid kernels. Interpretation of the default value is left to
         the kernel; see the documentation for sklearn.metrics.pairwise.
         Ignored by other kernels.
 
@@ -394,10 +398,11 @@ class Nystroem(BaseEstimator, TransformerMixin):
         Additional parameters (keyword arguments) for kernel function passed
         as callable object.
 
-    random_state : {int, RandomState}, optional
+    random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
-        if RandomState instance, random_state is the random number generator.
-
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     Attributes
     ----------
diff --git a/sklearn/linear_model/base.py b/sklearn/linear_model/base.py
index 7ac614a1cd7f..5fa412a494dc 100644
--- a/sklearn/linear_model/base.py
+++ b/sklearn/linear_model/base.py
@@ -410,14 +410,12 @@ class LinearRegression(LinearModel, RegressorMixin):
         (e.g. data is expected to be already centered).
 
     normalize : boolean, optional, default False
-        If True, the regressors X will be normalized before regression.
-        This parameter is ignored when `fit_intercept` is set to False.
-        When the regressors are normalized, note that this makes the
-        hyperparameters learnt more robust and almost independent of the number
-        of samples. The same property is not valid for standardized data.
-        However, if you wish to standardize, please use
-        `preprocessing.StandardScaler` before calling `fit` on an estimator
-        with `normalize=False`.
+        This parameter is ignored when ``fit_intercept`` is set to False.
+        If True, the regressors X will be normalized before regression by
+        subtracting the mean and dividing by the l2-norm.
+        If you wish to standardize, please use
+        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on
+        an estimator with ``normalize=False``.
 
     copy_X : boolean, optional, default True
         If True, X will be copied; else, it may be overwritten.
@@ -435,16 +433,6 @@ class LinearRegression(LinearModel, RegressorMixin):
         is a 2D array of shape (n_targets, n_features), while if only
         one target is passed, this is a 1D array of length n_features.
 
-    residues_ : array, shape (n_targets,) or (1,) or empty
-        Sum of residuals. Squared Euclidean 2-norm for each target passed
-        during the fit. If the linear regression problem is under-determined
-        (the number of linearly independent rows of the training matrix is less
-        than its number of linearly independent columns), this is an empty
-        array. If the target vector passed during the fit is 1-dimensional,
-        this is a (1,) shape array.
-
-        .. versionadded:: 0.18
-
     intercept_ : array
         Independent term in the linear model.
 
diff --git a/sklearn/linear_model/bayes.py b/sklearn/linear_model/bayes.py
index f7936f3521ac..617f79bb7aff 100644
--- a/sklearn/linear_model/bayes.py
+++ b/sklearn/linear_model/bayes.py
@@ -64,14 +64,12 @@ class BayesianRidge(LinearModel, RegressorMixin):
         Default is True.
 
     normalize : boolean, optional, default False
-        If True, the regressors X will be normalized before regression.
-        This parameter is ignored when `fit_intercept` is set to False.
-        When the regressors are normalized, note that this makes the
-        hyperparameters learnt more robust and almost independent of the number
-        of samples. The same property is not valid for standardized data.
-        However, if you wish to standardize, please use
-        `preprocessing.StandardScaler` before calling `fit` on an estimator
-        with `normalize=False`.
+        This parameter is ignored when ``fit_intercept`` is set to False.
+        If True, the regressors X will be normalized before regression by
+        subtracting the mean and dividing by the l2-norm.
+        If you wish to standardize, please use
+        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``
+        on an estimator with ``normalize=False``.
 
     copy_X : boolean, optional, default True
         If True, X will be copied; else, it may be overwritten.
@@ -201,6 +199,11 @@ def fit(self, X, y):
                     logdet_sigma_[:n_samples] += alpha_ * eigen_vals_
                     logdet_sigma_ = - np.sum(np.log(logdet_sigma_))
 
+            # Preserve the alpha and lambda values that were used to
+            # calculate the final coefficients
+            self.alpha_ = alpha_
+            self.lambda_ = lambda_
+
             # Update alpha and lambda
             rmse_ = np.sum((y - np.dot(X, coef_)) ** 2)
             gamma_ = (np.sum((alpha_ * eigen_vals_) /
@@ -229,8 +232,6 @@ def fit(self, X, y):
                 break
             coef_old_ = np.copy(coef_)
 
-        self.alpha_ = alpha_
-        self.lambda_ = lambda_
         self.coef_ = coef_
         sigma_ = np.dot(Vh.T,
                         Vh / (eigen_vals_ + lambda_ / alpha_)[:, np.newaxis])
@@ -326,14 +327,12 @@ class ARDRegression(LinearModel, RegressorMixin):
         Default is True.
 
     normalize : boolean, optional, default False
-        If True, the regressors X will be normalized before regression.
-        This parameter is ignored when `fit_intercept` is set to False.
-        When the regressors are normalized, note that this makes the
-        hyperparameters learnt more robust and almost independent of the number
-        of samples. The same property is not valid for standardized data.
-        However, if you wish to standardize, please use
-        `preprocessing.StandardScaler` before calling `fit` on an estimator
-        with `normalize=False`.
+        This parameter is ignored when ``fit_intercept`` is set to False.
+        If True, the regressors X will be normalized before regression by
+        subtracting the mean and dividing by the l2-norm.
+        If you wish to standardize, please use
+        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``
+        on an estimator with ``normalize=False``.
 
     copy_X : boolean, optional, default True.
         If True, X will be copied; else, it may be overwritten.
diff --git a/sklearn/linear_model/cd_fast.pyx b/sklearn/linear_model/cd_fast.pyx
index c56d7975fc6e..6641a8f96628 100644
--- a/sklearn/linear_model/cd_fast.pyx
+++ b/sklearn/linear_model/cd_fast.pyx
@@ -217,8 +217,8 @@ def enet_coordinate_descent(np.ndarray[floating, ndim=1] w,
     cdef floating *R_data = <floating*> R.data
     cdef floating *XtA_data = <floating*> XtA.data
 
-    if alpha == 0:
-        warnings.warn("Coordinate descent with alpha=0 may lead to unexpected"
+    if alpha == 0 and beta == 0:
+        warnings.warn("Coordinate descent with no regularization may lead to unexpected"
             " results and is discouraged.")
 
     with nogil:
diff --git a/sklearn/linear_model/coordinate_descent.py b/sklearn/linear_model/coordinate_descent.py
index 608461380cb0..a2eb3be475f8 100644
--- a/sklearn/linear_model/coordinate_descent.py
+++ b/sklearn/linear_model/coordinate_descent.py
@@ -65,14 +65,12 @@ def _alpha_grid(X, y, Xy=None, l1_ratio=1.0, fit_intercept=True,
         Whether to fit an intercept or not
 
     normalize : boolean, optional, default False
-        If ``True``, the regressors X will be normalized before regression.
-        This parameter is ignored when ``fit_intercept`` is set to ``False``.
-        When the regressors are normalized, note that this makes the
-        hyperparameters learnt more robust and almost independent of the number
-        of samples. The same property is not valid for standardized data.
-        However, if you wish to standardize, please use
-        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator
-        with ``normalize=False``.
+        This parameter is ignored when ``fit_intercept`` is set to False.
+        If True, the regressors X will be normalized before regression by
+        subtracting the mean and dividing by the l2-norm.
+        If you wish to standardize, please use
+        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``
+        on an estimator with ``normalize=False``.
 
     copy_X : boolean, optional, default True
         If ``True``, X will be copied; else, it may be overwritten.
@@ -368,7 +366,8 @@ def enet_path(X, y, l1_ratio=0.5, eps=1e-3, n_alphas=100, alphas=None,
 
     Notes
     -----
-    See examples/linear_model/plot_lasso_coordinate_descent_path.py for an example.
+    See examples/linear_model/plot_lasso_coordinate_descent_path.py for an
+    example.
 
     See also
     --------
@@ -530,8 +529,8 @@ class ElasticNet(LinearModel, RegressorMixin):
     alpha : float, optional
         Constant that multiplies the penalty terms. Defaults to 1.0.
         See the notes for the exact mathematical meaning of this
-        parameter.``alpha = 0`` is equivalent to an ordinary least square, solved
-        by the :class:`LinearRegression` object. For numerical
+        parameter.``alpha = 0`` is equivalent to an ordinary least square,
+        solved by the :class:`LinearRegression` object. For numerical
         reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.
         Given this, you should use the :class:`LinearRegression` object.
 
@@ -546,14 +545,12 @@ class ElasticNet(LinearModel, RegressorMixin):
         data is assumed to be already centered.
 
     normalize : boolean, optional, default False
-        If ``True``, the regressors X will be normalized before regression.
-        This parameter is ignored when ``fit_intercept`` is set to ``False``.
-        When the regressors are normalized, note that this makes the
-        hyperparameters learnt more robust and almost independent of the number
-        of samples. The same property is not valid for standardized data.
-        However, if you wish to standardize, please use
-        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator
-        with ``normalize=False``.
+        This parameter is ignored when ``fit_intercept`` is set to False.
+        If True, the regressors X will be normalized before regression by
+        subtracting the mean and dividing by the l2-norm.
+        If you wish to standardize, please use
+        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``
+        on an estimator with ``normalize=False``.
 
     precompute : True | False | array-like
         Whether to use a precomputed Gram matrix to speed up
@@ -585,9 +582,12 @@ class ElasticNet(LinearModel, RegressorMixin):
         (setting to 'random') often leads to significantly faster convergence
         especially when tol is higher than 1e-4.
 
-    random_state : int, RandomState instance, or None (default)
-        The seed of the pseudo random number generator that selects
-        a random feature to update. Useful only when selection is set to
+    random_state : int, RandomState instance or None, optional, default None
+        The seed of the pseudo random number generator that selects a random
+        feature to update.  If int, random_state is the seed used by the random
+        number generator; If RandomState instance, random_state is the random
+        number generator; If None, the random number generator is the
+        RandomState instance used by `np.random`. Used when ``selection`` ==
         'random'.
 
     Attributes
@@ -794,14 +794,12 @@ class Lasso(ElasticNet):
         (e.g. data is expected to be already centered).
 
     normalize : boolean, optional, default False
-        If ``True``, the regressors X will be normalized before regression.
-        This parameter is ignored when ``fit_intercept`` is set to ``False``.
-        When the regressors are normalized, note that this makes the
-        hyperparameters learnt more robust and almost independent of the number
-        of samples. The same property is not valid for standardized data.
-        However, if you wish to standardize, please use
-        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator
-        with ``normalize=False``.
+        This parameter is ignored when ``fit_intercept`` is set to False.
+        If True, the regressors X will be normalized before regression by
+        subtracting the mean and dividing by the l2-norm.
+        If you wish to standardize, please use
+        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``
+        on an estimator with ``normalize=False``.
 
     copy_X : boolean, optional, default True
         If ``True``, X will be copied; else, it may be overwritten.
@@ -834,9 +832,12 @@ class Lasso(ElasticNet):
         (setting to 'random') often leads to significantly faster convergence
         especially when tol is higher than 1e-4.
 
-    random_state : int, RandomState instance, or None (default)
-        The seed of the pseudo random number generator that selects
-        a random feature to update. Useful only when selection is set to
+    random_state : int, RandomState instance or None, optional, default None
+        The seed of the pseudo random number generator that selects a random
+        feature to update.  If int, random_state is the seed used by the random
+        number generator; If RandomState instance, random_state is the random
+        number generator; If None, the random number generator is the
+        RandomState instance used by `np.random`. Used when ``selection`` ==
         'random'.
 
     Attributes
@@ -1271,9 +1272,12 @@ class LassoCV(LinearModelCV, RegressorMixin):
         (setting to 'random') often leads to significantly faster convergence
         especially when tol is higher than 1e-4.
 
-    random_state : int, RandomState instance, or None (default)
-        The seed of the pseudo random number generator that selects
-        a random feature to update. Useful only when selection is set to
+    random_state : int, RandomState instance or None, optional, default None
+        The seed of the pseudo random number generator that selects a random
+        feature to update.  If int, random_state is the seed used by the random
+        number generator; If RandomState instance, random_state is the random
+        number generator; If None, the random number generator is the
+        RandomState instance used by `np.random`. Used when ``selection`` ==
         'random'.
 
     fit_intercept : boolean, default True
@@ -1282,14 +1286,12 @@ class LassoCV(LinearModelCV, RegressorMixin):
         (e.g. data is expected to be already centered).
 
     normalize : boolean, optional, default False
-        If ``True``, the regressors X will be normalized before regression.
-        This parameter is ignored when ``fit_intercept`` is set to ``False``.
-        When the regressors are normalized, note that this makes the
-        hyperparameters learnt more robust and almost independent of the number
-        of samples. The same property is not valid for standardized data.
-        However, if you wish to standardize, please use
-        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator
-        with ``normalize=False``.
+        This parameter is ignored when ``fit_intercept`` is set to False.
+        If True, the regressors X will be normalized before regression by
+        subtracting the mean and dividing by the l2-norm.
+        If you wish to standardize, please use
+        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``
+        on an estimator with ``normalize=False``.
 
     copy_X : boolean, optional, default True
         If ``True``, X will be copied; else, it may be overwritten.
@@ -1425,9 +1427,12 @@ class ElasticNetCV(LinearModelCV, RegressorMixin):
         (setting to 'random') often leads to significantly faster convergence
         especially when tol is higher than 1e-4.
 
-    random_state : int, RandomState instance, or None (default)
-        The seed of the pseudo random number generator that selects
-        a random feature to update. Useful only when selection is set to
+    random_state : int, RandomState instance or None, optional, default None
+        The seed of the pseudo random number generator that selects a random
+        feature to update.  If int, random_state is the seed used by the random
+        number generator; If RandomState instance, random_state is the random
+        number generator; If None, the random number generator is the
+        RandomState instance used by `np.random`. Used when ``selection`` ==
         'random'.
 
     fit_intercept : boolean
@@ -1436,14 +1441,12 @@ class ElasticNetCV(LinearModelCV, RegressorMixin):
         (e.g. data is expected to be already centered).
 
     normalize : boolean, optional, default False
-        If ``True``, the regressors X will be normalized before regression.
-        This parameter is ignored when ``fit_intercept`` is set to ``False``.
-        When the regressors are normalized, note that this makes the
-        hyperparameters learnt more robust and almost independent of the number
-        of samples. The same property is not valid for standardized data.
-        However, if you wish to standardize, please use
-        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator
-        with ``normalize=False``.
+        This parameter is ignored when ``fit_intercept`` is set to False.
+        If True, the regressors X will be normalized before regression by
+        subtracting the mean and dividing by the l2-norm.
+        If you wish to standardize, please use
+        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``
+        on an estimator with ``normalize=False``.
 
     copy_X : boolean, optional, default True
         If ``True``, X will be copied; else, it may be overwritten.
@@ -1568,14 +1571,12 @@ class MultiTaskElasticNet(Lasso):
         (e.g. data is expected to be already centered).
 
     normalize : boolean, optional, default False
-        If ``True``, the regressors X will be normalized before regression.
-        This parameter is ignored when ``fit_intercept`` is set to ``False``.
-        When the regressors are normalized, note that this makes the
-        hyperparameters learnt more robust and almost independent of the number
-        of samples. The same property is not valid for standardized data.
-        However, if you wish to standardize, please use
-        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator
-        with ``normalize=False``.
+        This parameter is ignored when ``fit_intercept`` is set to False.
+        If True, the regressors X will be normalized before regression by
+        subtracting the mean and dividing by the l2-norm.
+        If you wish to standardize, please use
+        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``
+        on an estimator with ``normalize=False``.
 
     copy_X : boolean, optional, default True
         If ``True``, X will be copied; else, it may be overwritten.
@@ -1599,9 +1600,12 @@ class MultiTaskElasticNet(Lasso):
         (setting to 'random') often leads to significantly faster convergence
         especially when tol is higher than 1e-4.
 
-    random_state : int, RandomState instance, or None (default)
-        The seed of the pseudo random number generator that selects
-        a random feature to update. Useful only when selection is set to
+    random_state : int, RandomState instance or None, optional, default None
+        The seed of the pseudo random number generator that selects a random
+        feature to update.  If int, random_state is the seed used by the random
+        number generator; If RandomState instance, random_state is the random
+        number generator; If None, the random number generator is the
+        RandomState instance used by `np.random`. Used when ``selection`` ==
         'random'.
 
     Attributes
@@ -1611,7 +1615,8 @@ class MultiTaskElasticNet(Lasso):
 
     coef_ : array, shape (n_tasks, n_features)
         Parameter vector (W in the cost function formula). If a 1D y is \
-        passed in at fit (non multi-task usage), ``coef_`` is then a 1D array
+        passed in at fit (non multi-task usage), ``coef_`` is then a 1D array.
+        Note that ``coef_`` stores the transpose of ``W``, ``W.T``.
 
     n_iter_ : int
         number of iterations run by the coordinate descent solver to reach
@@ -1753,14 +1758,12 @@ class MultiTaskLasso(MultiTaskElasticNet):
         (e.g. data is expected to be already centered).
 
     normalize : boolean, optional, default False
-        If ``True``, the regressors X will be normalized before regression.
-        This parameter is ignored when ``fit_intercept`` is set to ``False``.
-        When the regressors are normalized, note that this makes the
-        hyperparameters learnt more robust and almost independent of the number
-        of samples. The same property is not valid for standardized data.
-        However, if you wish to standardize, please use
-        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator
-        with ``normalize=False``.
+        This parameter is ignored when ``fit_intercept`` is set to False.
+        If True, the regressors X will be normalized before regression by
+        subtracting the mean and dividing by the l2-norm.
+        If you wish to standardize, please use
+        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``
+        on an estimator with ``normalize=False``.
 
     copy_X : boolean, optional, default True
         If ``True``, X will be copied; else, it may be overwritten.
@@ -1784,15 +1787,19 @@ class MultiTaskLasso(MultiTaskElasticNet):
         (setting to 'random') often leads to significantly faster convergence
         especially when tol is higher than 1e-4
 
-    random_state : int, RandomState instance, or None (default)
-        The seed of the pseudo random number generator that selects
-        a random feature to update. Useful only when selection is set to
+    random_state : int, RandomState instance or None, optional, default None
+        The seed of the pseudo random number generator that selects a random
+        feature to update.  If int, random_state is the seed used by the random
+        number generator; If RandomState instance, random_state is the random
+        number generator; If None, the random number generator is the
+        RandomState instance used by `np.random`. Used when ``selection`` ==
         'random'.
 
     Attributes
     ----------
     coef_ : array, shape (n_tasks, n_features)
-        parameter vector (W in the cost function formula)
+        Parameter vector (W in the cost function formula).
+        Note that ``coef_`` stores the transpose of ``W``, ``W.T``.
 
     intercept_ : array, shape (n_tasks,)
         independent term in decision function.
@@ -1889,14 +1896,12 @@ class MultiTaskElasticNetCV(LinearModelCV, RegressorMixin):
         (e.g. data is expected to be already centered).
 
     normalize : boolean, optional, default False
-        If ``True``, the regressors X will be normalized before regression.
-        This parameter is ignored when ``fit_intercept`` is set to ``False``.
-        When the regressors are normalized, note that this makes the
-        hyperparameters learnt more robust and almost independent of the number
-        of samples. The same property is not valid for standardized data.
-        However, if you wish to standardize, please use
-        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator
-        with ``normalize=False``.
+        This parameter is ignored when ``fit_intercept`` is set to False.
+        If True, the regressors X will be normalized before regression by
+        subtracting the mean and dividing by the l2-norm.
+        If you wish to standardize, please use
+        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``
+        on an estimator with ``normalize=False``.
 
     copy_X : boolean, optional, default True
         If ``True``, X will be copied; else, it may be overwritten.
@@ -1938,9 +1943,12 @@ class MultiTaskElasticNetCV(LinearModelCV, RegressorMixin):
         (setting to 'random') often leads to significantly faster convergence
         especially when tol is higher than 1e-4.
 
-    random_state : int, RandomState instance, or None (default)
-        The seed of the pseudo random number generator that selects
-        a random feature to update. Useful only when selection is set to
+    random_state : int, RandomState instance or None, optional, default None
+        The seed of the pseudo random number generator that selects a random
+        feature to update.  If int, random_state is the seed used by the random
+        number generator; If RandomState instance, random_state is the random
+        number generator; If None, the random number generator is the
+        RandomState instance used by `np.random`. Used when ``selection`` ==
         'random'.
 
     Attributes
@@ -1950,6 +1958,7 @@ class MultiTaskElasticNetCV(LinearModelCV, RegressorMixin):
 
     coef_ : array, shape (n_tasks, n_features)
         Parameter vector (W in the cost function formula).
+        Note that ``coef_`` stores the transpose of ``W``, ``W.T``.
 
     alpha_ : float
         The amount of penalization chosen by cross validation
@@ -2054,14 +2063,12 @@ class MultiTaskLassoCV(LinearModelCV, RegressorMixin):
         (e.g. data is expected to be already centered).
 
     normalize : boolean, optional, default False
-        If ``True``, the regressors X will be normalized before regression.
-        This parameter is ignored when ``fit_intercept`` is set to ``False``.
-        When the regressors are normalized, note that this makes the
-        hyperparameters learnt more robust and almost independent of the number
-        of samples. The same property is not valid for standardized data.
-        However, if you wish to standardize, please use
-        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator
-        with ``normalize=False``.
+        This parameter is ignored when ``fit_intercept`` is set to False.
+        If True, the regressors X will be normalized before regression by
+        subtracting the mean and dividing by the l2-norm.
+        If you wish to standardize, please use
+        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``
+        on an estimator with ``normalize=False``.
 
     copy_X : boolean, optional, default True
         If ``True``, X will be copied; else, it may be overwritten.
@@ -2103,10 +2110,13 @@ class MultiTaskLassoCV(LinearModelCV, RegressorMixin):
         (setting to 'random') often leads to significantly faster convergence
         especially when tol is higher than 1e-4.
 
-    random_state : int, RandomState instance, or None (default)
-        The seed of the pseudo random number generator that selects
-        a random feature to update. Useful only when selection is set to
-        'random'.
+    random_state : int, RandomState instance or None, optional, default None
+        The seed of the pseudo random number generator that selects a random
+        feature to update.  If int, random_state is the seed used by the random
+        number generator; If RandomState instance, random_state is the random
+        number generator; If None, the random number generator is the
+        RandomState instance used by `np.random`. Used when ``selection`` ==
+        'random'/
 
     Attributes
     ----------
@@ -2115,6 +2125,7 @@ class MultiTaskLassoCV(LinearModelCV, RegressorMixin):
 
     coef_ : array, shape (n_tasks, n_features)
         Parameter vector (W in the cost function formula).
+        Note that ``coef_`` stores the transpose of ``W``, ``W.T``.
 
     alpha_ : float
         The amount of penalization chosen by cross validation
diff --git a/sklearn/linear_model/least_angle.py b/sklearn/linear_model/least_angle.py
index c06720a25299..50b772152e58 100644
--- a/sklearn/linear_model/least_angle.py
+++ b/sklearn/linear_model/least_angle.py
@@ -513,15 +513,13 @@ class Lars(LinearModel, RegressorMixin):
     verbose : boolean or integer, optional
         Sets the verbosity amount
 
-    normalize : boolean, optional, default False
-        If True, the regressors X will be normalized before regression.
-        This parameter is ignored when `fit_intercept` is set to False.
-        When the regressors are normalized, note that this makes the
-        hyperparameters learnt more robust and almost independent of the number
-        of samples. The same property is not valid for standardized data.
-        However, if you wish to standardize, please use
-        `preprocessing.StandardScaler` before calling `fit` on an estimator
-        with `normalize=False`.
+    normalize : boolean, optional, default True
+        This parameter is ignored when ``fit_intercept`` is set to False.
+        If True, the regressors X will be normalized before regression by
+        subtracting the mean and dividing by the l2-norm.
+        If you wish to standardize, please use
+        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``
+        on an estimator with ``normalize=False``.
 
     precompute : True | False | 'auto' | array-like
         Whether to use a precomputed Gram matrix to speed up
@@ -746,15 +744,13 @@ class LassoLars(Lars):
     verbose : boolean or integer, optional
         Sets the verbosity amount
 
-    normalize : boolean, optional, default False
-        If True, the regressors X will be normalized before regression.
-        This parameter is ignored when `fit_intercept` is set to False.
-        When the regressors are normalized, note that this makes the
-        hyperparameters learnt more robust and almost independent of the number
-        of samples. The same property is not valid for standardized data.
-        However, if you wish to standardize, please use
-        `preprocessing.StandardScaler` before calling `fit` on an estimator
-        with `normalize=False`.
+    normalize : boolean, optional, default True
+        This parameter is ignored when ``fit_intercept`` is set to False.
+        If True, the regressors X will be normalized before regression by
+        subtracting the mean and dividing by the l2-norm.
+        If you wish to standardize, please use
+        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``
+        on an estimator with ``normalize=False``.
 
     copy_X : boolean, optional, default True
         If True, X will be copied; else, it may be overwritten.
@@ -903,15 +899,13 @@ def _lars_path_residues(X_train, y_train, X_test, y_test, Gram=None,
         'lasso' for expected small values of alpha in the doc of LassoLarsCV
         and LassoLarsIC.
 
-    normalize : boolean, optional, default False
-        If True, the regressors X will be normalized before regression.
-        This parameter is ignored when `fit_intercept` is set to False.
-        When the regressors are normalized, note that this makes the
-        hyperparameters learnt more robust and almost independent of the number
-        of samples. The same property is not valid for standardized data.
-        However, if you wish to standardize, please use
-        `preprocessing.StandardScaler` before calling `fit` on an estimator
-        with `normalize=False`.
+    normalize : boolean, optional, default True
+        This parameter is ignored when ``fit_intercept`` is set to False.
+        If True, the regressors X will be normalized before regression by
+        subtracting the mean and dividing by the l2-norm.
+        If you wish to standardize, please use
+        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``
+        on an estimator with ``normalize=False``.
 
     max_iter : integer, optional
         Maximum number of iterations to perform.
@@ -989,15 +983,13 @@ class LarsCV(Lars):
     verbose : boolean or integer, optional
         Sets the verbosity amount
 
-    normalize : boolean, optional, default False
-        If True, the regressors X will be normalized before regression.
-        This parameter is ignored when `fit_intercept` is set to False.
-        When the regressors are normalized, note that this makes the
-        hyperparameters learnt more robust and almost independent of the number
-        of samples. The same property is not valid for standardized data.
-        However, if you wish to standardize, please use
-        `preprocessing.StandardScaler` before calling `fit` on an estimator
-        with `normalize=False`.
+    normalize : boolean, optional, default True
+        This parameter is ignored when ``fit_intercept`` is set to False.
+        If True, the regressors X will be normalized before regression by
+        subtracting the mean and dividing by the l2-norm.
+        If you wish to standardize, please use
+        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``
+        on an estimator with ``normalize=False``.
 
     copy_X : boolean, optional, default True
         If ``True``, X will be copied; else, it may be overwritten.
@@ -1203,15 +1195,13 @@ class LassoLarsCV(LarsCV):
     verbose : boolean or integer, optional
         Sets the verbosity amount
 
-    normalize : boolean, optional, default False
-        If True, the regressors X will be normalized before regression.
-        This parameter is ignored when `fit_intercept` is set to False.
-        When the regressors are normalized, note that this makes the
-        hyperparameters learnt more robust and almost independent of the number
-        of samples. The same property is not valid for standardized data.
-        However, if you wish to standardize, please use
-        `preprocessing.StandardScaler` before calling `fit` on an estimator
-        with `normalize=False`.
+    normalize : boolean, optional, default True
+        This parameter is ignored when ``fit_intercept`` is set to False.
+        If True, the regressors X will be normalized before regression by
+        subtracting the mean and dividing by the l2-norm.
+        If you wish to standardize, please use
+        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``
+        on an estimator with ``normalize=False``.
 
     precompute : True | False | 'auto' | array-like
         Whether to use a precomputed Gram matrix to speed up
@@ -1338,15 +1328,13 @@ class LassoLarsIC(LassoLars):
     verbose : boolean or integer, optional
         Sets the verbosity amount
 
-    normalize : boolean, optional, default False
-        If True, the regressors X will be normalized before regression.
-        This parameter is ignored when `fit_intercept` is set to False.
-        When the regressors are normalized, note that this makes the
-        hyperparameters learnt more robust and almost independent of the number
-        of samples. The same property is not valid for standardized data.
-        However, if you wish to standardize, please use
-        `preprocessing.StandardScaler` before calling `fit` on an estimator
-        with `normalize=False`.
+    normalize : boolean, optional, default True
+        This parameter is ignored when ``fit_intercept`` is set to False.
+        If True, the regressors X will be normalized before regression by
+        subtracting the mean and dividing by the l2-norm.
+        If you wish to standardize, please use
+        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``
+        on an estimator with ``normalize=False``.
 
     copy_X : boolean, optional, default True
         If True, X will be copied; else, it may be overwritten.
diff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py
index 7b4bb4fe0fea..fa7bc7f32e4b 100644
--- a/sklearn/linear_model/logistic.py
+++ b/sklearn/linear_model/logistic.py
@@ -8,6 +8,7 @@
 #         Manoj Kumar <manojkumarsivaraj334@gmail.com>
 #         Lars Buitinck
 #         Simon Wu <s8wu@uwaterloo.ca>
+#         Arthur Mensch <arthur.mensch@m4x.org
 
 import numbers
 import warnings
@@ -421,7 +422,7 @@ def hessp(v):
 
 
 def _check_solver_option(solver, multi_class, penalty, dual):
-    if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag']:
+    if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:
         raise ValueError("Logistic Regression supports only liblinear,"
                          " newton-cg, lbfgs and sag solvers, got %s" % solver)
 
@@ -433,10 +434,11 @@ def _check_solver_option(solver, multi_class, penalty, dual):
         raise ValueError("Solver %s does not support "
                          "a multinomial backend." % solver)
 
-    if solver != 'liblinear':
+    if solver not in ['liblinear', 'saga']:
         if penalty != 'l2':
             raise ValueError("Solver %s supports only l2 penalties, "
                              "got %s penalty." % (solver, penalty))
+    if solver != 'liblinear':
         if dual:
             raise ValueError("Solver %s supports only "
                              "dual=False, got dual=%s" % (solver, dual))
@@ -494,7 +496,7 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
         For the liblinear and lbfgs solvers set verbose to any positive
         number for verbosity.
 
-    solver : {'lbfgs', 'newton-cg', 'liblinear', 'sag'}
+    solver : {'lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga'}
         Numerical solver to use.
 
     coef : array-like, shape (n_features,), default None
@@ -541,9 +543,13 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
         the entire probability distribution. Works only for the 'lbfgs' and
         'newton-cg' solvers.
 
-    random_state : int seed, RandomState instance, or None (default)
-        The seed of the pseudo random number generator to use when
-        shuffling the data. Used only in solvers 'sag' and 'liblinear'.
+    random_state : int, RandomState instance or None, optional, default None
+        The seed of the pseudo random number generator to use when shuffling
+        the data.  If int, random_state is the seed used by the random number
+        generator; If RandomState instance, random_state is the random number
+        generator; If None, the random number generator is the RandomState
+        instance used by `np.random`. Used when ``solver`` == 'sag' or
+        'liblinear'.
 
     check_input : bool, default True
         If False, the input arrays X and y will not be checked.
@@ -631,7 +637,7 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
             sample_weight *= class_weight_[le.fit_transform(y_bin)]
 
     else:
-        if solver != 'sag':
+        if solver not in ['sag', 'saga']:
             lbin = LabelBinarizer()
             Y_multi = lbin.fit_transform(y)
             if Y_multi.shape[1] == 1:
@@ -726,16 +732,23 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
             else:
                 w0 = coef_.ravel()
 
-        elif solver == 'sag':
+        elif solver in ['sag', 'saga']:
             if multi_class == 'multinomial':
                 target = target.astype(np.float64)
                 loss = 'multinomial'
             else:
                 loss = 'log'
-
+            if penalty == 'l1':
+                alpha = 0.
+                beta = 1. / C
+            else:
+                alpha = 1. / C
+                beta = 0.
             w0, n_iter_i, warm_start_sag = sag_solver(
-                X, target, sample_weight, loss, 1. / C, max_iter, tol,
-                verbose, random_state, False, max_squared_sum, warm_start_sag)
+                X, target, sample_weight, loss, alpha,
+                beta, max_iter, tol,
+                verbose, random_state, False, max_squared_sum, warm_start_sag,
+                is_saga=(solver == 'saga'))
 
         else:
             raise ValueError("solver must be one of {'liblinear', 'lbfgs', "
@@ -820,7 +833,7 @@ def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,
         For the liblinear and lbfgs solvers set verbose to any positive
         number for verbosity.
 
-    solver : {'lbfgs', 'newton-cg', 'liblinear', 'sag'}
+    solver : {'lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga'}
         Decides which solver to use.
 
     penalty : str, 'l1' or 'l2'
@@ -848,12 +861,16 @@ def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,
         Multiclass option can be either 'ovr' or 'multinomial'. If the option
         chosen is 'ovr', then a binary problem is fit for each label. Else
         the loss minimised is the multinomial loss fit across
-        the entire probability distribution. Works only for the 'lbfgs' and
-        'newton-cg' solver.
+        the entire probability distribution. Does not work for
+        liblinear solver.
 
-    random_state : int seed, RandomState instance, or None (default)
-        The seed of the pseudo random number generator to use when
-        shuffling the data. Used only in solvers 'sag' and 'liblinear'.
+    random_state : int, RandomState instance or None, optional, default None
+        The seed of the pseudo random number generator to use when shuffling
+        the data.  If int, random_state is the seed used by the random number
+        generator; If RandomState instance, random_state is the random number
+        generator; If None, the random number generator is the RandomState
+        instance used by `np.random`. Used when ``solver`` == 'sag' and
+        'liblinear'.
 
     max_squared_sum : float, default None
         Maximum squared sum of X over samples. Used only in SAG solver.
@@ -967,6 +984,9 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,
         Used to specify the norm used in the penalization. The 'newton-cg',
         'sag' and 'lbfgs' solvers support only l2 penalties.
 
+        .. versionadded:: 0.19
+           l1 penalty with SAGA solver (allowing 'multinomial' + L1)
+
     dual : bool, default: False
         Dual or primal formulation. Dual formulation is only implemented for
         l2 penalty with liblinear solver. Prefer dual=False when
@@ -1012,26 +1032,34 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,
         Useful only for the newton-cg, sag and lbfgs solvers.
         Maximum number of iterations taken for the solvers to converge.
 
-    random_state : int seed, RandomState instance, default: None
-        The seed of the pseudo random number generator to use when
-        shuffling the data. Used only in solvers 'sag' and 'liblinear'.
+    random_state : int, RandomState instance or None, optional, default: None
+        The seed of the pseudo random number generator to use when shuffling
+        the data.  If int, random_state is the seed used by the random number
+        generator; If RandomState instance, random_state is the random number
+        generator; If None, the random number generator is the RandomState
+        instance used by `np.random`. Used when ``solver`` == 'sag' or
+        'liblinear'.
 
-    solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag'}, default: 'liblinear'
+    solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},
+        default: 'liblinear'
         Algorithm to use in the optimization problem.
 
-        - For small datasets, 'liblinear' is a good choice, whereas 'sag' is
-            faster for large ones.
-        - For multiclass problems, only 'newton-cg', 'sag' and 'lbfgs' handle
-            multinomial loss; 'liblinear' is limited to one-versus-rest
+        - For small datasets, 'liblinear' is a good choice, whereas 'sag' and
+            'saga' are faster for large ones.
+        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'
+            handle multinomial loss; 'liblinear' is limited to one-versus-rest
             schemes.
-        - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty.
+        - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas
+            'liblinear' and 'saga' handle L1 penalty.
 
-        Note that 'sag' fast convergence is only guaranteed on features with
-        approximately the same scale. You can preprocess the data with a
-        scaler from sklearn.preprocessing.
+        Note that 'sag' and 'saga' fast convergence is only guaranteed on
+        features with approximately the same scale. You can
+        preprocess the data with a scaler from sklearn.preprocessing.
 
         .. versionadded:: 0.17
            Stochastic Average Gradient descent solver.
+        .. versionadded:: 0.19
+           SAGA solver.
 
     tol : float, default: 1e-4
         Tolerance for stopping criteria.
@@ -1040,8 +1068,8 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,
         Multiclass option can be either 'ovr' or 'multinomial'. If the option
         chosen is 'ovr', then a binary problem is fit for each label. Else
         the loss minimised is the multinomial loss fit across
-        the entire probability distribution. Works only for the 'newton-cg',
-        'sag' and 'lbfgs' solver.
+        the entire probability distribution. Does not work for liblinear
+        solver.
 
         .. versionadded:: 0.18
            Stochastic Average Gradient descent solver for 'multinomial' case.
@@ -1056,7 +1084,7 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,
         Useless for liblinear solver.
 
         .. versionadded:: 0.17
-           *warm_start* to support *lbfgs*, *newton-cg*, *sag* solvers.
+           *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.
 
     n_jobs : int, default: 1
         Number of CPU cores used when parallelizing over classes
@@ -1110,6 +1138,11 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,
         Minimizing Finite Sums with the Stochastic Average Gradient
         https://hal.inria.fr/hal-00860051/document
 
+    SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).
+        SAGA: A Fast Incremental Gradient Method With Support
+        for Non-Strongly Convex Composite Objectives
+        https://arxiv.org/abs/1407.0202
+
     Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent
         methods for logistic regression and maximum entropy models.
         Machine Learning 85(1-2):41-75.
@@ -1188,7 +1221,7 @@ def fit(self, X, y, sample_weight=None):
             self.n_iter_ = np.array([n_iter_])
             return self
 
-        if self.solver == 'sag':
+        if self.solver in ['sag', 'saga']:
             max_squared_sum = row_norms(X, squared=True).max()
         else:
             max_squared_sum = None
@@ -1220,7 +1253,6 @@ def fit(self, X, y, sample_weight=None):
         if self.multi_class == 'multinomial':
             classes_ = [None]
             warm_start_coef = [warm_start_coef]
-
         if warm_start_coef is None:
             warm_start_coef = [None] * n_classes
 
@@ -1228,7 +1260,10 @@ def fit(self, X, y, sample_weight=None):
 
         # The SAG solver releases the GIL so it's more efficient to use
         # threads for this solver.
-        backend = 'threading' if self.solver == 'sag' else 'multiprocessing'
+        if self.solver in ['sag', 'saga']:
+            backend = 'threading'
+        else:
+            backend = 'multiprocessing'
         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
                                backend=backend)(
             path_func(X, y, pos_class=class_, Cs=[self.C],
@@ -1237,9 +1272,10 @@ def fit(self, X, y, sample_weight=None):
                       multi_class=self.multi_class, max_iter=self.max_iter,
                       class_weight=self.class_weight, check_input=False,
                       random_state=self.random_state, coef=warm_start_coef_,
+                      penalty=self.penalty,
                       max_squared_sum=max_squared_sum,
                       sample_weight=sample_weight)
-            for (class_, warm_start_coef_) in zip(classes_, warm_start_coef))
+            for class_, warm_start_coef_ in zip(classes_, warm_start_coef))
 
         fold_coefs_, _, n_iter_ = zip(*fold_coefs_)
         self.n_iter_ = np.asarray(n_iter_, dtype=np.int32)[:, 0]
@@ -1379,25 +1415,28 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,
         that can be used, look at :mod:`sklearn.metrics`. The
         default scoring option used is 'accuracy'.
 
-
-    solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag'}
+    solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},
+        default: 'liblinear'
         Algorithm to use in the optimization problem.
 
-        - For small datasets, 'liblinear' is a good choice, whereas 'sag' is
-            faster for large ones.
-        - For multiclass problems, only 'newton-cg', 'sag' and 'lbfgs' handle
-            multinomial loss; 'liblinear' is limited to one-versus-rest
+        - For small datasets, 'liblinear' is a good choice, whereas 'sag' and
+            'saga' are faster for large ones.
+        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'
+            handle multinomial loss; 'liblinear' is limited to one-versus-rest
             schemes.
-        - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty.
+        - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas
+            'liblinear' and 'saga' handle L1 penalty.
         - 'liblinear' might be slower in LogisticRegressionCV because it does
             not handle warm-starting.
 
-        Note that 'sag' fast convergence is only guaranteed on features with
-        approximately the same scale. You can preprocess the data with a
-        scaler from sklearn.preprocessing.
+        Note that 'sag' and 'saga' fast convergence is only guaranteed on
+        features with approximately the same scale. You can preprocess the data
+        with a scaler from sklearn.preprocessing.
 
         .. versionadded:: 0.17
            Stochastic Average Gradient descent solver.
+        .. versionadded:: 0.19
+           SAGA solver.
 
     tol : float, optional
         Tolerance for stopping criteria.
@@ -1443,9 +1482,11 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,
         To lessen the effect of regularization on synthetic feature weight
         (and therefore on the intercept) intercept_scaling has to be increased.
 
-    random_state : int seed, RandomState instance, or None (default)
-        The seed of the pseudo random number generator to use when
-        shuffling the data.
+    random_state : int, RandomState instance or None, optional, default None
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     Attributes
     ----------
@@ -1569,7 +1610,7 @@ def fit(self, X, y, sample_weight=None):
         classes = self.classes_ = label_encoder.classes_
         encoded_labels = label_encoder.transform(label_encoder.classes_)
 
-        if self.solver == 'sag':
+        if self.solver in ['sag', 'saga']:
             max_squared_sum = row_norms(X, squared=True).max()
         else:
             max_squared_sum = None
@@ -1612,7 +1653,10 @@ def fit(self, X, y, sample_weight=None):
 
         # The SAG solver releases the GIL so it's more efficient to use
         # threads for this solver.
-        backend = 'threading' if self.solver == 'sag' else 'multiprocessing'
+        if self.solver in ['sag', 'saga']:
+            backend = 'threading'
+        else:
+            backend = 'multiprocessing'
         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
                                backend=backend)(
             path_func(X, y, train, test, pos_class=label, Cs=self.Cs,
diff --git a/sklearn/linear_model/omp.py b/sklearn/linear_model/omp.py
index 5328a2ed8170..8cf73754538c 100644
--- a/sklearn/linear_model/omp.py
+++ b/sklearn/linear_model/omp.py
@@ -557,15 +557,13 @@ class OrthogonalMatchingPursuit(LinearModel, RegressorMixin):
         to false, no intercept will be used in calculations
         (e.g. data is expected to be already centered).
 
-    normalize : boolean, optional, default False
-        If True, the regressors X will be normalized before regression.
-        This parameter is ignored when `fit_intercept` is set to `False`.
-        When the regressors are normalized, note that this makes the
-        hyperparameters learnt more robust and almost independent of the number
-        of samples. The same property is not valid for standardized data.
-        However, if you wish to standardize, please use
-        `preprocessing.StandardScaler` before calling `fit` on an estimator
-        with `normalize=False`.
+    normalize : boolean, optional, default True
+        This parameter is ignored when ``fit_intercept`` is set to False.
+        If True, the regressors X will be normalized before regression by
+        subtracting the mean and dividing by the l2-norm.
+        If you wish to standardize, please use
+        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``
+        on an estimator with ``normalize=False``.
 
     precompute : {True, False, 'auto'}, default 'auto'
         Whether to use a precomputed Gram and Xy matrix to speed up
@@ -695,15 +693,13 @@ def _omp_path_residues(X_train, y_train, X_test, y_test, copy=True,
         to false, no intercept will be used in calculations
         (e.g. data is expected to be already centered).
 
-    normalize : boolean, optional, default False
-        If True, the regressors X will be normalized before regression.
-        This parameter is ignored when `fit_intercept` is set to `False`.
-        When the regressors are normalized, note that this makes the
-        hyperparameters learnt more robust and almost independent of the number
-        of samples. The same property is not valid for standardized data.
-        However, if you wish to standardize, please use
-        `preprocessing.StandardScaler` before calling `fit` on an estimator
-        with `normalize=False`.
+    normalize : boolean, optional, default True
+        This parameter is ignored when ``fit_intercept`` is set to False.
+        If True, the regressors X will be normalized before regression by
+        subtracting the mean and dividing by the l2-norm.
+        If you wish to standardize, please use
+        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``
+        on an estimator with ``normalize=False``.
 
     max_iter : integer, optional
         Maximum numbers of iterations to perform, therefore maximum features
@@ -762,15 +758,13 @@ class OrthogonalMatchingPursuitCV(LinearModel, RegressorMixin):
         to false, no intercept will be used in calculations
         (e.g. data is expected to be already centered).
 
-    normalize : boolean, optional, default False
-        If True, the regressors X will be normalized before regression.
-        This parameter is ignored when `fit_intercept` is set to `False`.
-        When the regressors are normalized, note that this makes the
-        hyperparameters learnt more robust and almost independent of the number
-        of samples. The same property is not valid for standardized data.
-        However, if you wish to standardize, please use
-        `preprocessing.StandardScaler` before calling `fit` on an estimator
-        with `normalize=False`.
+    normalize : boolean, optional, default True
+        This parameter is ignored when ``fit_intercept`` is set to False.
+        If True, the regressors X will be normalized before regression by
+        subtracting the mean and dividing by the l2-norm.
+        If you wish to standardize, please use
+        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``
+        on an estimator with ``normalize=False``.
 
     max_iter : integer, optional
         Maximum numbers of iterations to perform, therefore maximum features
diff --git a/sklearn/linear_model/passive_aggressive.py b/sklearn/linear_model/passive_aggressive.py
index 376ca92e9342..941f398bd6e1 100644
--- a/sklearn/linear_model/passive_aggressive.py
+++ b/sklearn/linear_model/passive_aggressive.py
@@ -28,9 +28,12 @@ class PassiveAggressiveClassifier(BaseSGDClassifier):
     shuffle : bool, default=True
         Whether or not the training data should be shuffled after each epoch.
 
-    random_state : int seed, RandomState instance, or None (default)
-        The seed of the pseudo random number generator to use when
-        shuffling the data.
+    random_state : int, RandomState instance or None, optional, default=None
+        The seed of the pseudo random number generator to use when shuffling
+        the data.  If int, random_state is the seed used by the random number
+        generator; If RandomState instance, random_state is the random number
+        generator; If None, the random number generator is the RandomState
+        instance used by `np.random`.
 
     verbose : integer, optional
         The verbosity level
@@ -204,9 +207,12 @@ class PassiveAggressiveRegressor(BaseSGDRegressor):
     shuffle : bool, default=True
         Whether or not the training data should be shuffled after each epoch.
 
-    random_state : int seed, RandomState instance, or None (default)
-        The seed of the pseudo random number generator to use when
-        shuffling the data.
+    random_state : int, RandomState instance or None, optional, default=None
+        The seed of the pseudo random number generator to use when shuffling
+        the data.  If int, random_state is the seed used by the random number
+        generator; If RandomState instance, random_state is the random number
+        generator; If None, the random number generator is the RandomState
+        instance used by `np.random`.
 
     verbose : integer, optional
         The verbosity level
diff --git a/sklearn/linear_model/perceptron.py b/sklearn/linear_model/perceptron.py
index d59718176502..0b11049fc3b3 100644
--- a/sklearn/linear_model/perceptron.py
+++ b/sklearn/linear_model/perceptron.py
@@ -30,9 +30,12 @@ class Perceptron(BaseSGDClassifier):
     shuffle : bool, optional, default True
         Whether or not the training data should be shuffled after each epoch.
 
-    random_state : int seed, RandomState instance, or None (default)
-        The seed of the pseudo random number generator to use when
-        shuffling the data.
+    random_state : int, RandomState instance or None, optional, default None
+        The seed of the pseudo random number generator to use when shuffling
+        the data.  If int, random_state is the seed used by the random number
+        generator; If RandomState instance, random_state is the random number
+        generator; If None, the random number generator is the RandomState
+        instance used by `np.random`.
 
     verbose : integer, optional
         The verbosity level
diff --git a/sklearn/linear_model/randomized_l1.py b/sklearn/linear_model/randomized_l1.py
index 2680c3b70336..6ebf95d2533f 100644
--- a/sklearn/linear_model/randomized_l1.py
+++ b/sklearn/linear_model/randomized_l1.py
@@ -416,7 +416,7 @@ class RandomizedLogisticRegression(BaseRandomizedLinearModel):
     verbose : boolean or integer, optional
         Sets the verbosity amount
 
-    normalize : boolean, optional, default False
+    normalize : boolean, optional, default True
         If True, the regressors X will be normalized before regression.
         This parameter is ignored when `fit_intercept` is set to False.
         When the regressors are normalized, note that this makes the
@@ -575,8 +575,11 @@ def lasso_stability_path(X, y, scaling=0.5, random_state=None,
         The alpha parameter in the stability selection article used to
         randomly scale the features. Should be between 0 and 1.
 
-    random_state : integer or numpy.random.RandomState, optional
-        The generator used to randomize the design.
+    random_state : int, RandomState instance or None, optional, default=None
+        The generator used to randomize the design.  If int, random_state is
+        the seed used by the random number generator; If RandomState instance,
+        random_state is the random number generator; If None, the random number
+        generator is the RandomState instance used by `np.random`.
 
     n_resampling : int, optional, default=200
         Number of randomized models.
@@ -611,6 +614,7 @@ def lasso_stability_path(X, y, scaling=0.5, random_state=None,
     -----
     See examples/linear_model/plot_sparse_recovery.py for an example.
     """
+    X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'])
     rng = check_random_state(random_state)
 
     if not (0 < scaling < 1):
diff --git a/sklearn/linear_model/ransac.py b/sklearn/linear_model/ransac.py
index e4e391cb101c..33bcebc9ad51 100644
--- a/sklearn/linear_model/ransac.py
+++ b/sklearn/linear_model/ransac.py
@@ -158,10 +158,11 @@ class RANSACRegressor(BaseEstimator, MetaEstimatorMixin, RegressorMixin):
         If the loss on a sample is greater than the ``residual_threshold``, then
         this sample is classified as an outlier.
 
-    random_state : integer or numpy.RandomState, optional
-        The generator used to initialize the centers. If an integer is
-        given, it fixes the seed. Defaults to the global numpy random
-        number generator.
+    random_state : int, RandomState instance or None, optional, default None
+        The generator used to initialize the centers.  If int, random_state is
+        the seed used by the random number generator; If RandomState instance,
+        random_state is the random number generator; If None, the random number
+        generator is the RandomState instance used by `np.random`.
 
     Attributes
     ----------
@@ -341,7 +342,10 @@ def fit(self, X, y, sample_weight=None):
 
         n_samples, _ = X.shape
 
-        for self.n_trials_ in range(1, self.max_trials + 1):
+        self.n_trials_ = 0
+        max_trials = self.max_trials
+        while self.n_trials_ < max_trials:
+            self.n_trials_ += 1
 
             if (self.n_skips_no_inliers_ + self.n_skips_invalid_data_ +
                     self.n_skips_invalid_model_) > self.max_skips:
@@ -415,13 +419,14 @@ def fit(self, X, y, sample_weight=None):
             X_inlier_best = X_inlier_subset
             y_inlier_best = y_inlier_subset
 
+            max_trials = min(
+                max_trials,
+                _dynamic_max_trials(n_inliers_best, n_samples,
+                                    min_samples, self.stop_probability))
+
             # break if sufficient number of inliers or score is reached
-            if (n_inliers_best >= self.stop_n_inliers
-                    or score_best >= self.stop_score
-                    or self.n_trials_
-                       >= _dynamic_max_trials(n_inliers_best, n_samples,
-                                              min_samples,
-                                              self.stop_probability)):
+            if n_inliers_best >= self.stop_n_inliers or \
+                            score_best >= self.stop_score:
                 break
 
         # if none of the iterations met the required criteria
diff --git a/sklearn/linear_model/ridge.py b/sklearn/linear_model/ridge.py
index 3b89434ac611..398016b886bd 100644
--- a/sklearn/linear_model/ridge.py
+++ b/sklearn/linear_model/ridge.py
@@ -229,7 +229,7 @@ def ridge_regression(X, y, alpha, sample_weight=None, solver='auto',
 
         .. versionadded:: 0.17
 
-    solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg'}
+    solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'}
         Solver to use in the computational routines:
 
         - 'auto' chooses the solver automatically based on the type of data.
@@ -251,18 +251,22 @@ def ridge_regression(X, y, alpha, sample_weight=None, solver='auto',
           scipy.sparse.linalg.lsqr. It is the fastest but may not be available
           in old scipy versions. It also uses an iterative procedure.
 
-        - 'sag' uses a Stochastic Average Gradient descent. It also uses an
-          iterative procedure, and is often faster than other solvers when
-          both n_samples and n_features are large. Note that 'sag' fast
-          convergence is only guaranteed on features with approximately the
-          same scale. You can preprocess the data with a scaler from
-          sklearn.preprocessing.
+        - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses
+          its improved, unbiased version named SAGA. Both methods also use an
+          iterative procedure, and are often faster than other solvers when
+          both n_samples and n_features are large. Note that 'sag' and
+          'saga' fast convergence is only guaranteed on features with
+          approximately the same scale. You can preprocess the data with a
+          scaler from sklearn.preprocessing.
 
-        All last four solvers support both dense and sparse data. However,
-        only 'sag' supports sparse input when `fit_intercept` is True.
+
+        All last five solvers support both dense and sparse data. However, only
+        'sag' and 'saga' supports sparse input when`fit_intercept` is True.
 
         .. versionadded:: 0.17
            Stochastic Average Gradient descent solver.
+        .. versionadded:: 0.19
+           SAGA solver.
 
     tol : float
         Precision of the solution.
@@ -271,9 +275,12 @@ def ridge_regression(X, y, alpha, sample_weight=None, solver='auto',
         Verbosity level. Setting verbose > 0 will display additional
         information depending on the solver used.
 
-    random_state : int seed, RandomState instance, or None (default)
-        The seed of the pseudo random number generator to use when
-        shuffling the data. Used only in 'sag' solver.
+    random_state : int, RandomState instance or None, optional, default None
+        The seed of the pseudo random number generator to use when shuffling
+        the data.  If int, random_state is the seed used by the random number
+        generator; If RandomState instance, random_state is the random number
+        generator; If None, the random number generator is the RandomState
+        instance used by `np.random`. Used when ``solver`` == 'sag'.
 
     return_n_iter : boolean, default False
         If True, the method also returns `n_iter`, the actual number of
@@ -314,7 +321,7 @@ def ridge_regression(X, y, alpha, sample_weight=None, solver='auto',
         solver = 'sag'
 
     # SAG needs X and y columns to be C-contiguous and np.float64
-    if solver == 'sag':
+    if solver in ['sag', 'saga']:
         X = check_array(X, accept_sparse=['csr'],
                         dtype=np.float64, order='C')
         y = check_array(y, dtype=np.float64, ensure_2d=False, order='F')
@@ -358,7 +365,7 @@ def ridge_regression(X, y, alpha, sample_weight=None, solver='auto',
         if np.atleast_1d(sample_weight).ndim > 1:
             raise ValueError("Sample weights must be 1D array or scalar")
 
-        if solver != 'sag':
+        if solver not in ['sag', 'saga']:
             # SAG supports sample_weight directly. For other solvers,
             # we implement sample_weight via a simple rescaling.
             X, y = _rescale_data(X, y, sample_weight)
@@ -373,7 +380,7 @@ def ridge_regression(X, y, alpha, sample_weight=None, solver='auto',
     if alpha.size == 1 and n_targets > 1:
         alpha = np.repeat(alpha, n_targets)
 
-    if solver not in ('sparse_cg', 'cholesky', 'svd', 'lsqr', 'sag'):
+    if solver not in ('sparse_cg', 'cholesky', 'svd', 'lsqr', 'sag', 'saga'):
         raise ValueError('Solver %s not understood' % solver)
 
     n_iter = None
@@ -401,7 +408,7 @@ def ridge_regression(X, y, alpha, sample_weight=None, solver='auto',
                 # use SVD solver if matrix is singular
                 solver = 'svd'
 
-    elif solver == 'sag':
+    elif solver in ['sag', 'saga']:
         # precompute max_squared_sum for all targets
         max_squared_sum = row_norms(X, squared=True).max()
 
@@ -411,9 +418,10 @@ def ridge_regression(X, y, alpha, sample_weight=None, solver='auto',
         for i, (alpha_i, target) in enumerate(zip(alpha, y.T)):
             init = {'coef': np.zeros((n_features + int(return_intercept), 1))}
             coef_, n_iter_, _ = sag_solver(
-                X, target.ravel(), sample_weight, 'squared', alpha_i,
+                X, target.ravel(), sample_weight, 'squared', alpha_i, 0,
                 max_iter, tol, verbose, random_state, False, max_squared_sum,
-                init)
+                init,
+                is_saga=solver == 'saga')
             if return_intercept:
                 coef[i] = coef_[:-1]
                 intercept[i] = coef_[-1]
@@ -527,16 +535,14 @@ class Ridge(_BaseRidge, RegressorMixin):
         by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.
 
     normalize : boolean, optional, default False
-        If True, the regressors X will be normalized before regression.
-        This parameter is ignored when `fit_intercept` is set to False.
-        When the regressors are normalized, note that this makes the
-        hyperparameters learnt more robust and almost independent of the number
-        of samples. The same property is not valid for standardized data.
-        However, if you wish to standardize, please use
-        `preprocessing.StandardScaler` before calling `fit` on an estimator
-        with `normalize=False`.
-
-    solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag'}
+        This parameter is ignored when ``fit_intercept`` is set to False.
+        If True, the regressors X will be normalized before regression by
+        subtracting the mean and dividing by the l2-norm.
+        If you wish to standardize, please use
+        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``
+        on an estimator with ``normalize=False``.
+
+    solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'}
         Solver to use in the computational routines:
 
         - 'auto' chooses the solver automatically based on the type of data.
@@ -557,25 +563,32 @@ class Ridge(_BaseRidge, RegressorMixin):
           scipy.sparse.linalg.lsqr. It is the fastest but may not be available
           in old scipy versions. It also uses an iterative procedure.
 
-        - 'sag' uses a Stochastic Average Gradient descent. It also uses an
-          iterative procedure, and is often faster than other solvers when
-          both n_samples and n_features are large. Note that 'sag' fast
-          convergence is only guaranteed on features with approximately the
-          same scale. You can preprocess the data with a scaler from
-          sklearn.preprocessing.
+        - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses
+          its improved, unbiased version named SAGA. Both methods also use an
+          iterative procedure, and are often faster than other solvers when
+          both n_samples and n_features are large. Note that 'sag' and
+          'saga' fast convergence is only guaranteed on features with
+          approximately the same scale. You can preprocess the data with a
+          scaler from sklearn.preprocessing.
 
-        All last four solvers support both dense and sparse data. However,
-        only 'sag' supports sparse input when `fit_intercept` is True.
+        All last five solvers support both dense and sparse data. However,
+        only 'sag' and 'saga' supports sparse input when `fit_intercept` is
+        True.
 
         .. versionadded:: 0.17
            Stochastic Average Gradient descent solver.
+        .. versionadded:: 0.19
+           SAGA solver.
 
     tol : float
         Precision of the solution.
 
-    random_state : int seed, RandomState instance, or None (default)
-        The seed of the pseudo random number generator to use when
-        shuffling the data. Used only in 'sag' solver.
+    random_state : int, RandomState instance or None, optional, default None
+        The seed of the pseudo random number generator to use when shuffling
+        the data.  If int, random_state is the seed used by the random number
+        generator; If RandomState instance, random_state is the random number
+        generator; If None, the random number generator is the RandomState
+        instance used by `np.random`. Used when ``solver`` == 'sag'.
 
         .. versionadded:: 0.17
            *random_state* to support Stochastic Average Gradient.
@@ -677,16 +690,14 @@ class RidgeClassifier(LinearClassifierMixin, _BaseRidge):
         The default value is determined by scipy.sparse.linalg.
 
     normalize : boolean, optional, default False
-        If True, the regressors X will be normalized before regression.
-        This parameter is ignored when `fit_intercept` is set to False.
-        When the regressors are normalized, note that this makes the
-        hyperparameters learnt more robust and almost independent of the number
-        of samples. The same property is not valid for standardized data.
-        However, if you wish to standardize, please use
-        `preprocessing.StandardScaler` before calling `fit` on an estimator
-        with `normalize=False`.
-
-    solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag'}
+        This parameter is ignored when ``fit_intercept`` is set to False.
+        If True, the regressors X will be normalized before regression by
+        subtracting the mean and dividing by the l2-norm.
+        If you wish to standardize, please use
+        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``
+        on an estimator with ``normalize=False``.
+
+    solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'}
         Solver to use in the computational routines:
 
         - 'auto' chooses the solver automatically based on the type of data.
@@ -707,19 +718,28 @@ class RidgeClassifier(LinearClassifierMixin, _BaseRidge):
           scipy.sparse.linalg.lsqr. It is the fastest but may not be available
           in old scipy versions. It also uses an iterative procedure.
 
-        - 'sag' uses a Stochastic Average Gradient descent. It also uses an
-          iterative procedure, and is faster than other solvers when both
-          n_samples and n_features are large.
+        - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses
+          its unbiased and more flexible version named SAGA. Both methods
+          use an iterative procedure, and are often faster than other solvers
+          when both n_samples and n_features are large. Note that 'sag' and
+          'saga' fast convergence is only guaranteed on features with
+          approximately the same scale. You can preprocess the data with a
+          scaler from sklearn.preprocessing.
 
           .. versionadded:: 0.17
              Stochastic Average Gradient descent solver.
+          .. versionadded:: 0.19
+           SAGA solver.
 
     tol : float
         Precision of the solution.
 
-    random_state : int seed, RandomState instance, or None (default)
-        The seed of the pseudo random number generator to use when
-        shuffling the data. Used in 'sag' solver.
+    random_state : int, RandomState instance or None, optional, default None
+        The seed of the pseudo random number generator to use when shuffling
+        the data.  If int, random_state is the seed used by the random number
+        generator; If RandomState instance, random_state is the random number
+        generator; If None, the random number generator is the RandomState
+        instance used by `np.random`. Used when ``solver`` == 'sag'.
 
     Attributes
     ----------
@@ -1126,14 +1146,12 @@ class RidgeCV(_BaseRidgeCV, RegressorMixin):
         (e.g. data is expected to be already centered).
 
     normalize : boolean, optional, default False
-        If True, the regressors X will be normalized before regression.
-        This parameter is ignored when `fit_intercept` is set to False.
-        When the regressors are normalized, note that this makes the
-        hyperparameters learnt more robust and almost independent of the number
-        of samples. The same property is not valid for standardized data.
-        However, if you wish to standardize, please use
-        `preprocessing.StandardScaler` before calling `fit` on an estimator
-        with `normalize=False`.
+        This parameter is ignored when ``fit_intercept`` is set to False.
+        If True, the regressors X will be normalized before regression by
+        subtracting the mean and dividing by the l2-norm.
+        If you wish to standardize, please use
+        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``
+        on an estimator with ``normalize=False``.
 
     scoring : string, callable or None, optional, default: None
         A string (see model evaluation documentation) or
@@ -1229,14 +1247,12 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):
         (e.g. data is expected to be already centered).
 
     normalize : boolean, optional, default False
-        If True, the regressors X will be normalized before regression.
-        This parameter is ignored when `fit_intercept` is set to False.
-        When the regressors are normalized, note that this makes the
-        hyperparameters learnt more robust and almost independent of the number
-        of samples. The same property is not valid for standardized data.
-        However, if you wish to standardize, please use
-        `preprocessing.StandardScaler` before calling `fit` on an estimator
-        with `normalize=False`.
+        This parameter is ignored when ``fit_intercept`` is set to False.
+        If True, the regressors X will be normalized before regression by
+        subtracting the mean and dividing by the l2-norm.
+        If you wish to standardize, please use
+        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``
+        on an estimator with ``normalize=False``.
 
     scoring : string, callable or None, optional, default: None
         A string (see model evaluation documentation) or
diff --git a/sklearn/linear_model/sag.py b/sklearn/linear_model/sag.py
index d09d7ecaaa55..9bf807a18238 100644
--- a/sklearn/linear_model/sag.py
+++ b/sklearn/linear_model/sag.py
@@ -4,17 +4,20 @@
 #
 # License: BSD 3 clause
 
-import numpy as np
 import warnings
 
+import numpy as np
+
+from .base import make_dataset
+from .sag_fast import sag
 from ..exceptions import ConvergenceWarning
 from ..utils import check_array
 from ..utils.extmath import row_norms
-from .base import make_dataset
-from .sag_fast import sag
 
 
-def get_auto_step_size(max_squared_sum, alpha_scaled, loss, fit_intercept):
+def get_auto_step_size(max_squared_sum, alpha_scaled, loss, fit_intercept,
+                       n_samples=None,
+                       is_saga=False):
     """Compute automatic step size for SAG solver
 
     The step size is set to 1 / (alpha_scaled + L + fit_intercept) where L is
@@ -36,6 +39,13 @@ def get_auto_step_size(max_squared_sum, alpha_scaled, loss, fit_intercept):
         Specifies if a constant (a.k.a. bias or intercept) will be
         added to the decision function.
 
+    n_samples : int, optional
+        Number of rows in X. Useful if is_saga=True.
+
+    is_saga : boolean, optional
+        Whether to return step size for the SAGA algorithm or the SAG
+        algorithm.
+
     Returns
     -------
     step_size : float
@@ -46,23 +56,38 @@ def get_auto_step_size(max_squared_sum, alpha_scaled, loss, fit_intercept):
     Schmidt, M., Roux, N. L., & Bach, F. (2013).
     Minimizing finite sums with the stochastic average gradient
     https://hal.inria.fr/hal-00860051/document
+
+    Defazio, A., Bach F. & Lacoste-Julien S. (2014).
+    SAGA: A Fast Incremental Gradient Method With Support
+    for Non-Strongly Convex Composite Objectives
+    https://arxiv.org/abs/1407.0202
     """
     if loss in ('log', 'multinomial'):
-        # inverse Lipschitz constant for log loss
-        return 4.0 / (max_squared_sum + int(fit_intercept)
-                      + 4.0 * alpha_scaled)
+        L = (0.25 * (max_squared_sum + int(fit_intercept)) + alpha_scaled)
     elif loss == 'squared':
         # inverse Lipschitz constant for squared loss
-        return 1.0 / (max_squared_sum + int(fit_intercept) + alpha_scaled)
+        L = max_squared_sum + int(fit_intercept) + alpha_scaled
     else:
         raise ValueError("Unknown loss function for SAG solver, got %s "
                          "instead of 'log' or 'squared'" % loss)
+    if is_saga:
+        # SAGA theoretical step size is 1/3L or 1 / (2 * (L + mu n))
+        # See Defazio et al. 2014
+        mun = min(2 * n_samples * alpha_scaled, L)
+        step = 1. / (2 * L + mun)
+    else:
+        # SAG theoretical step size is 1/16L but it is recommended to use 1 / L
+        # see http://www.birs.ca//workshops//2014/14w5003/files/schmidt.pdf,
+        # slide 65
+        step = 1. / L
+    return step
 
 
-def sag_solver(X, y, sample_weight=None, loss='log', alpha=1.,
+def sag_solver(X, y, sample_weight=None, loss='log', alpha=1., beta=0.,
                max_iter=1000, tol=0.001, verbose=0, random_state=None,
                check_input=True, max_squared_sum=None,
-               warm_start_mem=None):
+               warm_start_mem=None,
+               is_saga=False):
     """SAG solver for Ridge and LogisticRegression
 
     SAG stands for Stochastic Average Gradient: the gradient of the loss is
@@ -119,9 +144,12 @@ def sag_solver(X, y, sample_weight=None, loss='log', alpha=1.,
     verbose : integer, optional
         The verbosity level.
 
-    random_state : int seed, RandomState instance, or None (default)
-        The seed of the pseudo random number generator to use when
-        shuffling the data.
+    random_state : int, RandomState instance or None, optional, default None
+        The seed of the pseudo random number generator to use when shuffling
+        the data.  If int, random_state is the seed used by the random number
+        generator; If RandomState instance, random_state is the random number
+        generator; If None, the random number generator is the RandomState
+        instance used by `np.random`.
 
     check_input : bool, default True
         If False, the input arrays X and y will not be checked.
@@ -145,6 +173,10 @@ def sag_solver(X, y, sample_weight=None, loss='log', alpha=1.,
             - 'seen': array of boolean describing the seen samples.
             - 'num_seen': the number of seen samples.
 
+    is_saga : boolean, optional
+        Whether to use the SAGA algorithm or the SAG algorithm. SAGA behaves
+        better in the first epochs, and allow for l1 regularisation.
+
     Returns
     -------
     coef_ : array, shape (n_features)
@@ -188,6 +220,11 @@ def sag_solver(X, y, sample_weight=None, loss='log', alpha=1.,
     Minimizing finite sums with the stochastic average gradient
     https://hal.inria.fr/hal-00860051/document
 
+    Defazio, A., Bach F. & Lacoste-Julien S. (2014).
+    SAGA: A Fast Incremental Gradient Method With Support
+    for Non-Strongly Convex Composite Objectives
+    https://arxiv.org/abs/1407.0202
+
     See also
     --------
     Ridge, SGDRegressor, ElasticNet, Lasso, SVR, and
@@ -206,6 +243,7 @@ def sag_solver(X, y, sample_weight=None, loss='log', alpha=1.,
     n_samples, n_features = X.shape[0], X.shape[1]
     # As in SGD, the alpha is scaled by n_samples.
     alpha_scaled = float(alpha) / n_samples
+    beta_scaled = float(beta) / n_samples
 
     # if loss == 'multinomial', y should be label encoded.
     n_classes = int(y.max()) + 1 if loss == 'multinomial' else 1
@@ -261,8 +299,8 @@ def sag_solver(X, y, sample_weight=None, loss='log', alpha=1.,
     if max_squared_sum is None:
         max_squared_sum = row_norms(X, squared=True).max()
     step_size = get_auto_step_size(max_squared_sum, alpha_scaled, loss,
-                                   fit_intercept)
-
+                                   fit_intercept, n_samples=n_samples,
+                                   is_saga=is_saga)
     if step_size * alpha_scaled == 1:
         raise ZeroDivisionError("Current sag implementation does not handle "
                                 "the case step_size * alpha_scaled == 1")
@@ -273,6 +311,7 @@ def sag_solver(X, y, sample_weight=None, loss='log', alpha=1.,
                             max_iter,
                             loss,
                             step_size, alpha_scaled,
+                            beta_scaled,
                             sum_gradient_init,
                             gradient_memory_init,
                             seen_init,
@@ -280,6 +319,7 @@ def sag_solver(X, y, sample_weight=None, loss='log', alpha=1.,
                             fit_intercept,
                             intercept_sum_gradient,
                             intercept_decay,
+                            is_saga,
                             verbose)
     if n_iter_ == max_iter:
         warnings.warn("The max_iter was reached which means "
diff --git a/sklearn/linear_model/sag_fast.pyx b/sklearn/linear_model/sag_fast.pyx
index f4dce9c74916..8c370db7e3b1 100644
--- a/sklearn/linear_model/sag_fast.pyx
+++ b/sklearn/linear_model/sag_fast.pyx
@@ -4,18 +4,19 @@
 #
 # Authors: Danny Sullivan <dbsullivan23@gmail.com>
 #          Tom Dupre la Tour <tom.dupre-la-tour@m4x.org>
+#          Arthur Mensch <arthur.mensch@m4x.org
 #
 # License: BSD 3 clause
-import numpy as np
 cimport numpy as np
-import scipy.sparse as sp
+import numpy as np
 from libc.math cimport fabs, exp, log
 from libc.time cimport time, time_t
 
-from ..utils.seq_dataset cimport SequentialDataset
 from .sgd_fast cimport LossFunction
 from .sgd_fast cimport Log, SquaredLoss
+from ..utils.seq_dataset cimport SequentialDataset
 
+from libc.stdio cimport printf
 
 cdef extern from "sgd_fast_helpers.h":
     bint skl_isfinite(double) nogil
@@ -214,6 +215,10 @@ def _multinomial_grad_loss_all_samples(
     return sum_loss, sum_gradient_array
 
 
+cdef inline double _soft_thresholding(double x, double shrinkage) nogil:
+    return fmax(x - shrinkage, 0) - fmax(- x - shrinkage, 0)
+
+
 def sag(SequentialDataset dataset,
         np.ndarray[double, ndim=2, mode='c'] weights_array,
         np.ndarray[double, ndim=1, mode='c'] intercept_array,
@@ -225,6 +230,7 @@ def sag(SequentialDataset dataset,
         str loss_function,
         double step_size,
         double alpha,
+        double beta,
         np.ndarray[double, ndim=2, mode='c'] sum_gradient_init,
         np.ndarray[double, ndim=2, mode='c'] gradient_memory_init,
         np.ndarray[bint, ndim=1, mode='c'] seen_init,
@@ -232,8 +238,9 @@ def sag(SequentialDataset dataset,
         bint fit_intercept,
         np.ndarray[double, ndim=1, mode='c'] intercept_sum_gradient_init,
         double intercept_decay,
+        bint saga,
         bint verbose):
-    """Stochastic Average Gradient (SAG) solver.
+    """Stochastic Average Gradient (SAG) and SAGA solvers.
 
     Used in Ridge and LogisticRegression.
 
@@ -243,6 +250,12 @@ def sag(SequentialDataset dataset,
     Minimizing finite sums with the stochastic average gradient
     https://hal.inria.fr/hal-00860051/PDF/sag_journal.pdf
     (section 4.3)
+
+    Defazio, A., Bach, F., Lacoste-Julien, S. (2014),
+    SAGA: A Fast Incremental Gradient Method With Support
+    for Non-Strongly Convex Composite Objectives
+    https://arxiv.org/abs/1407.0202
+
     """
     # the data pointer for x, the current sample
     cdef double *x_data_ptr = NULL
@@ -320,12 +333,21 @@ def sag(SequentialDataset dataset,
         np.zeros(n_classes, dtype=np.double, order="c")
     cdef double* gradient = <double*> gradient_array.data
 
+    # Bias correction term in saga
+    cdef double gradient_correction
+
     # the scalar used for multiplying z
     cdef double wscale = 1.0
 
     # the cumulative sums for each iteration for the sparse implementation
     cumulative_sums[0] = 0.0
 
+    # the multipliative scale needed for JIT params
+    cdef np.ndarray[double, ndim=1] cumulative_sums_prox_array
+    cdef double* cumulative_sums_prox
+
+    cdef bint prox = beta > 0 and saga
+
     # Loss function to optimize
     cdef LossFunction loss
     # Wether the loss function is multinomial
@@ -345,11 +367,17 @@ def sag(SequentialDataset dataset,
                          "one of ('log', 'squared', 'multinomial')"
                          % loss_function)
 
+    if prox:
+        cumulative_sums_prox_array = np.empty(n_samples,
+                                              dtype=np.double, order="c")
+        cumulative_sums_prox = <double*> cumulative_sums_prox_array.data
+    else:
+        cumulative_sums_prox = NULL
+
     with nogil:
         start_time = time(NULL)
         for n_iter in range(max_iter):
             for sample_itr in range(n_samples):
-
                 # extract a random sample
                 sample_ind = dataset.random(&x_data_ptr, &x_ind_ptr, &xnnz,
                                               &y, &sample_weight)
@@ -364,26 +392,16 @@ def sag(SequentialDataset dataset,
 
                 # make the weight updates
                 if sample_itr > 0:
-                    for j in range(xnnz):
-                        feature_ind = x_ind_ptr[j]
-                        # cached index for sum_gradient and weights
-                        f_idx = feature_ind * n_classes
-
-                        cum_sum = cumulative_sums[sample_itr - 1]
-                        if feature_hist[feature_ind] != 0:
-                            cum_sum -= \
-                                cumulative_sums[feature_hist[feature_ind] - 1]
-
-                        for class_ind in range(n_classes):
-                            weights[f_idx + class_ind] -= \
-                                cum_sum * sum_gradient[f_idx + class_ind]
-
-                            # check to see that the weight is not inf or NaN
-                            if not skl_isfinite(weights[f_idx + class_ind]):
-                                with gil:
-                                    raise_infinite_error(n_iter)
-
-                        feature_hist[feature_ind] = sample_itr
+                   lagged_update(weights, wscale, xnnz,
+                          n_samples, n_classes, sample_itr,
+                          cumulative_sums,
+                          cumulative_sums_prox,
+                          feature_hist,
+                          prox,
+                          sum_gradient,
+                          x_ind_ptr,
+                          False,
+                          n_iter)
 
                 # find the current prediction
                 predict_sample(x_data_ptr, x_ind_ptr, xnnz, weights, wscale,
@@ -396,25 +414,39 @@ def sag(SequentialDataset dataset,
                 else:
                     gradient[0] = loss._dloss(prediction[0], y) * sample_weight
 
+                # L2 regularization by simply rescaling the weights
+                wscale *= wscale_update
+
                 # make the updates to the sum of gradients
                 for j in range(xnnz):
                     feature_ind = x_ind_ptr[j]
                     val = x_data_ptr[j]
                     f_idx = feature_ind * n_classes
                     for class_ind in range(n_classes):
-                        sum_gradient[f_idx + class_ind] += \
+                        gradient_correction = \
                             val * (gradient[class_ind] -
                                    gradient_memory[s_idx + class_ind])
+                        if saga:
+                            weights[f_idx + class_ind] -= \
+                                (gradient_correction * step_size
+                                 * (1 - 1. / num_seen) / wscale)
+                        sum_gradient[f_idx + class_ind] += gradient_correction
 
                 # fit the intercept
                 if fit_intercept:
                     for class_ind in range(n_classes):
-                        intercept_sum_gradient[class_ind] += \
-                            (gradient[class_ind] -
-                             gradient_memory[s_idx + class_ind])
-                        intercept[class_ind] -= \
-                            (step_size * intercept_sum_gradient[class_ind] /
-                             num_seen * intercept_decay)
+                        gradient_correction = (gradient[class_ind] -
+                                               gradient_memory[s_idx + class_ind])
+                        intercept_sum_gradient[class_ind] += gradient_correction
+                        gradient_correction *= step_size * (1. - 1. / num_seen)
+                        if saga:
+                            intercept[class_ind] -= \
+                                (step_size * intercept_sum_gradient[class_ind] /
+                                 num_seen * intercept_decay) + gradient_correction
+                        else:
+                            intercept[class_ind] -= \
+                                (step_size * intercept_sum_gradient[class_ind] /
+                                 num_seen * intercept_decay)
 
                         # check to see that the intercept is not inf or NaN
                         if not skl_isfinite(intercept[class_ind]):
@@ -425,16 +457,18 @@ def sag(SequentialDataset dataset,
                 for class_ind in range(n_classes):
                     gradient_memory[s_idx + class_ind] = gradient[class_ind]
 
-                # L2 regularization by simply rescaling the weights
-                wscale *= wscale_update
-
                 if sample_itr == 0:
                     cumulative_sums[0] = step_size / (wscale * num_seen)
+                    if prox:
+                        cumulative_sums_prox[0] = step_size * beta / wscale
                 else:
                     cumulative_sums[sample_itr] = \
                         (cumulative_sums[sample_itr - 1] +
                          step_size / (wscale * num_seen))
-
+                    if prox:
+                        cumulative_sums_prox[sample_itr] = \
+                        (cumulative_sums_prox[sample_itr - 1] +
+                             step_size * beta / wscale)
                 # If wscale gets too small, we need to reset the scale.
                 if wscale < 1e-9:
                     if verbose:
@@ -442,31 +476,39 @@ def sag(SequentialDataset dataset,
                             print("rescaling...")
                     wscale = scale_weights(
                         weights, wscale, n_features, n_samples, n_classes,
-                        sample_itr, cumulative_sums, feature_hist, sum_gradient)
+                        sample_itr, cumulative_sums,
+                        cumulative_sums_prox,
+                        feature_hist,
+                        prox, sum_gradient, n_iter)
 
             # we scale the weights every n_samples iterations and reset the
             # just-in-time update system for numerical stability.
             wscale = scale_weights(weights, wscale, n_features, n_samples,
                                    n_classes, n_samples - 1, cumulative_sums,
-                                   feature_hist, sum_gradient)
+                                   cumulative_sums_prox,
+                                   feature_hist,
+                                   prox, sum_gradient, n_iter)
 
             # check if the stopping criteria is reached
             max_change = 0.0
             max_weight = 0.0
-            for feature_ind in range(n_features):
-                max_weight = fmax(max_weight, fabs(weights[feature_ind]))
+            for idx in range(n_features * n_classes):
+                max_weight = fmax(max_weight, fabs(weights[idx]))
                 max_change = fmax(max_change,
-                                  fabs(weights[feature_ind] -
-                                       previous_weights[feature_ind]))
-                previous_weights[feature_ind] = weights[feature_ind]
-
-            if max_change / max_weight <= tol:
+                                  fabs(weights[idx] -
+                                       previous_weights[idx]))
+                previous_weights[idx] = weights[idx]
+            if ((max_weight != 0 and max_change / max_weight <= tol)
+                or max_weight == 0 and max_change == 0):
                 if verbose:
                     end_time = time(NULL)
                     with gil:
                         print("convergence after %d epochs took %d seconds" %
                               (n_iter + 1, end_time - start_time))
                 break
+            elif verbose:
+                printf('Epoch %d, change: %.8f\n', n_iter + 1,
+                                                  max_change / max_weight)
     n_iter += 1
 
     if verbose and n_iter >= max_iter:
@@ -486,8 +528,12 @@ cdef void raise_infinite_error(int n_iter):
 
 cdef double scale_weights(double* weights, double wscale, int n_features,
                           int n_samples, int n_classes, int sample_itr,
-                          double* cumulative_sums, int* feature_hist,
-                          double* sum_gradient) nogil:
+                          double* cumulative_sums,
+                          double* cumulative_sums_prox,
+                          int* feature_hist,
+                          bint prox,
+                          double* sum_gradient,
+                          int n_iter) nogil:
     """Scale the weights with wscale for numerical stability.
 
     wscale = (1 - step_size * alpha) ** (n_iter * n_samples + sample_itr)
@@ -496,27 +542,103 @@ cdef double scale_weights(double* weights, double wscale, int n_features,
     coefficients and reset the just-in-time update system.
     This also limits the size of `cumulative_sums`.
     """
-    cdef int feature_ind, class_ind, idx
-    cdef double cum_sum
-    idx = -1
-    for feature_ind in range(n_features):
-        cum_sum = cumulative_sums[sample_itr]
-        if feature_hist[feature_ind] != 0:
-            cum_sum -= cumulative_sums[feature_hist[feature_ind] - 1]
-
-        for class_ind in range(n_classes):
-            idx += 1  # idx = feature_ind * n_classes + class_ind
-            weights[idx] -= cum_sum * sum_gradient[idx]
-            weights[idx] *= wscale
-
-        feature_hist[feature_ind] = (sample_itr + 1) % n_samples
-
-    cumulative_sums[sample_itr % n_samples] = 0.0
 
+    lagged_update(weights, wscale, n_features,
+                          n_samples, n_classes, sample_itr + 1,
+                          cumulative_sums,
+                          cumulative_sums_prox,
+                          feature_hist,
+                          prox,
+                          sum_gradient,
+                          NULL,
+                          True,
+                          n_iter)
     # reset wscale to 1.0
     return 1.0
 
 
+cdef void lagged_update(double* weights, double wscale, int xnnz,
+                          int n_samples, int n_classes, int sample_itr,
+                          double* cumulative_sums,
+                          double* cumulative_sums_prox,
+                          int* feature_hist,
+                          bint prox,
+                          double* sum_gradient,
+                          int* x_ind_ptr,
+                          bint reset,
+                          int n_iter) nogil:
+    """Hard perform the JIT updates for non-zero features of present sample.
+     
+    The updates that awaits are kept in memory using cumulative_sums,
+    cumulative_sums_prox, wscale and feature_hist. See original SAGA paper
+    (Defazio et al. 2014) for details. If reset=True, we also reset wscale to
+    1 (this is done at the end of each epoch).
+    """
+    cdef int feature_ind, class_ind, idx, f_idx, lagged_ind, last_update_ind
+    cdef double cum_sum, grad_step, prox_step
+    for feature_ind in range(xnnz):
+        if not reset:
+            feature_ind = x_ind_ptr[feature_ind]
+        f_idx = feature_ind * n_classes
+
+        cum_sum = cumulative_sums[sample_itr - 1]
+        if prox:
+            cum_sum_prox = cumulative_sums_prox[sample_itr - 1]
+        if feature_hist[feature_ind] != 0:
+            cum_sum -= cumulative_sums[feature_hist[feature_ind] - 1]
+            if prox:
+                cum_sum_prox -= cumulative_sums_prox[feature_hist[feature_ind] - 1]
+        if not prox:
+            for class_ind in range(n_classes):
+                idx = f_idx + class_ind
+                weights[idx] -= cum_sum * sum_gradient[idx]
+                if reset:
+                    weights[idx] *= wscale
+                    if not skl_isfinite(weights[idx]):
+                        with gil:
+                            raise_infinite_error(n_iter)
+        else:
+            for class_ind in range(n_classes):
+                idx = f_idx + class_ind
+                if fabs(sum_gradient[idx] * cum_sum) < cum_sum_prox:
+                    # In this case, we can perform all the gradient steps and
+                    # all the proximal steps in this order, which is more
+                    # efficient than unrolling all the lagged updates.
+                    # Idea taken from scikit-learn-contrib/lightning.
+                    weights[idx] -= cum_sum * sum_gradient[idx]
+                    weights[idx] = _soft_thresholding(weights[idx],
+                                                      cum_sum_prox)
+                else:
+                    last_update_ind = feature_hist[feature_ind] - 1
+                    if last_update_ind == -1:
+                        last_update_ind = sample_itr - 1
+                    for lagged_ind in range(sample_itr - 1,
+                                   last_update_ind - 1, -1):
+                        grad_step = (cumulative_sums[lagged_ind]
+                           - cumulative_sums[lagged_ind - 1])
+                        prox_step = (cumulative_sums_prox[lagged_ind]
+                           - cumulative_sums_prox[lagged_ind - 1])
+                        weights[idx] -= sum_gradient[idx] * grad_step
+                        weights[idx] = _soft_thresholding(weights[idx],
+                                                          prox_step)
+
+                if reset:
+                    weights[idx] *= wscale
+                    # check to see that the weight is not inf or NaN
+                    if not skl_isfinite(weights[idx]):
+                        with gil:
+                            raise_infinite_error(n_iter)
+        if reset:
+            feature_hist[feature_ind] = sample_itr % n_samples
+        else:
+            feature_hist[feature_ind] = sample_itr
+
+    if reset:
+        cumulative_sums[sample_itr - 1] = 0.0
+        if prox:
+            cumulative_sums_prox[sample_itr - 1] = 0.0
+
+
 cdef void predict_sample(double* x_data_ptr, int* x_ind_ptr, int xnnz,
                          double* w_data_ptr, double wscale, double* intercept,
                          double* prediction, int n_classes) nogil:
diff --git a/sklearn/linear_model/stochastic_gradient.py b/sklearn/linear_model/stochastic_gradient.py
index c234b8eb94f0..b3c61408470c 100644
--- a/sklearn/linear_model/stochastic_gradient.py
+++ b/sklearn/linear_model/stochastic_gradient.py
@@ -608,9 +608,12 @@ class SGDClassifier(BaseSGDClassifier):
         Whether or not the training data should be shuffled after each epoch.
         Defaults to True.
 
-    random_state : int seed, RandomState instance, or None (default)
-        The seed of the pseudo random number generator to use when
-        shuffling the data.
+    random_state : int, RandomState instance or None, optional (default=None)
+        The seed of the pseudo random number generator to use when shuffling
+        the data.  If int, random_state is the seed used by the random number
+        generator; If RandomState instance, random_state is the random number
+        generator; If None, the random number generator is the RandomState
+        instance used by `np.random`.
 
     verbose : integer, optional
         The verbosity level
@@ -1134,9 +1137,12 @@ class SGDRegressor(BaseSGDRegressor):
         Whether or not the training data should be shuffled after each epoch.
         Defaults to True.
 
-    random_state : int seed, RandomState instance, or None (default)
-        The seed of the pseudo random number generator to use when
-        shuffling the data.
+    random_state : int, RandomState instance or None, optional (default=None)
+        The seed of the pseudo random number generator to use when shuffling
+        the data.  If int, random_state is the seed used by the random number
+        generator; If RandomState instance, random_state is the random number
+        generator; If None, the random number generator is the RandomState
+        instance used by `np.random`.
 
     verbose : integer, optional
         The verbosity level.
diff --git a/sklearn/linear_model/tests/test_bayes.py b/sklearn/linear_model/tests/test_bayes.py
index 9b0e0db26c7a..48eeef5e192c 100644
--- a/sklearn/linear_model/tests/test_bayes.py
+++ b/sklearn/linear_model/tests/test_bayes.py
@@ -6,12 +6,13 @@
 import numpy as np
 
 from sklearn.utils.testing import assert_array_equal
+from sklearn.utils.testing import assert_array_almost_equal
+from sklearn.utils.testing import assert_almost_equal
 from sklearn.utils.testing import SkipTest
 from sklearn.linear_model.bayes import BayesianRidge, ARDRegression
+from sklearn.linear_model import Ridge
 from sklearn import datasets
 
-from sklearn.utils.testing import assert_array_almost_equal
-
 
 def test_bayesian_on_diabetes():
     # Test BayesianRidge on diabetes
@@ -34,6 +35,19 @@ def test_bayesian_on_diabetes():
     assert_array_equal(np.diff(clf.scores_) > 0, True)
 
 
+def test_bayesian_ridge_parameter():
+    # Test correctness of lambda_ and alpha_ parameters (Github issue #8224)
+    X = np.array([[1, 1], [3, 4], [5, 7], [4, 1], [2, 6], [3, 10], [3, 2]])
+    y = np.array([1, 2, 3, 2, 0, 4, 5]).T
+
+    # A Ridge regression model using an alpha value equal to the ratio of
+    # lambda_ and alpha_ from the Bayesian Ridge model must be identical
+    br_model = BayesianRidge(compute_score=True).fit(X, y)
+    rr_model = Ridge(alpha=br_model.lambda_ / br_model.alpha_).fit(X, y)
+    assert_array_almost_equal(rr_model.coef_, br_model.coef_)
+    assert_almost_equal(rr_model.intercept_, br_model.intercept_)
+
+
 def test_toy_bayesian_ridge_object():
     # Test BayesianRidge on toy
     X = np.array([[1], [2], [6], [8], [10]])
@@ -64,7 +78,7 @@ def f(X):
         return np.dot(X, w) + b
 
     def f_noise(X, noise_mult):
-        return f(X) + np.random.randn(X.shape[0])*noise_mult
+        return f(X) + np.random.randn(X.shape[0]) * noise_mult
 
     d = 5
     n_train = 50
diff --git a/sklearn/linear_model/tests/test_logistic.py b/sklearn/linear_model/tests/test_logistic.py
index ec2be517bf38..01f470a0e44c 100644
--- a/sklearn/linear_model/tests/test_logistic.py
+++ b/sklearn/linear_model/tests/test_logistic.py
@@ -1,32 +1,31 @@
 import numpy as np
 import scipy.sparse as sp
 from scipy import linalg, optimize, sparse
-
+from sklearn.datasets import load_iris, make_classification
+from sklearn.metrics import log_loss
+from sklearn.model_selection import StratifiedKFold
+from sklearn.preprocessing import LabelEncoder
+from sklearn.utils import compute_class_weight
+from sklearn.utils.fixes import sp_version
 from sklearn.utils.testing import assert_almost_equal
-from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_array_almost_equal
+from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_equal
 from sklearn.utils.testing import assert_greater
+from sklearn.utils.testing import assert_raise_message
 from sklearn.utils.testing import assert_raises
 from sklearn.utils.testing import assert_true
 from sklearn.utils.testing import assert_warns
-from sklearn.utils.testing import raises
 from sklearn.utils.testing import ignore_warnings
-from sklearn.utils.testing import assert_raise_message
-from sklearn.exceptions import ConvergenceWarning
-from sklearn.utils import compute_class_weight
-from sklearn.utils.fixes import sp_version
+from sklearn.utils.testing import raises
 
+from sklearn.exceptions import ConvergenceWarning
 from sklearn.linear_model.logistic import (
     LogisticRegression,
     logistic_regression_path, LogisticRegressionCV,
     _logistic_loss_and_grad, _logistic_grad_hess,
     _multinomial_grad_hess, _logistic_loss,
-    )
-from sklearn.model_selection import StratifiedKFold
-from sklearn.datasets import load_iris, make_classification
-from sklearn.metrics import log_loss
-from sklearn.preprocessing import LabelEncoder
+)
 
 X = [[-1, 0], [0, 1], [1, 1]]
 X_sp = sp.csr_matrix(X)
@@ -106,7 +105,10 @@ def test_predict_iris():
                 LogisticRegression(C=len(iris.data), solver='newton-cg',
                                    multi_class='multinomial'),
                 LogisticRegression(C=len(iris.data), solver='sag', tol=1e-2,
-                                   multi_class='ovr', random_state=42)]:
+                                   multi_class='ovr', random_state=42),
+                LogisticRegression(C=len(iris.data), solver='saga', tol=1e-2,
+                                   multi_class='ovr', random_state=42)
+                ]:
         clf.fit(iris.data, target)
         assert_array_equal(np.unique(target), clf.classes_)
 
@@ -122,7 +124,7 @@ def test_predict_iris():
 
 
 def test_multinomial_validation():
-    for solver in ['lbfgs', 'newton-cg', 'sag']:
+    for solver in ['lbfgs', 'newton-cg', 'sag', 'saga']:
         lr = LogisticRegression(C=-1, solver=solver, multi_class='multinomial')
         assert_raises(ValueError, lr.fit, [[0, 1], [1, 0]], [0, 1])
 
@@ -151,7 +153,7 @@ def test_check_solver_option():
                    solver)
             lr = LR(solver=solver, penalty='l1')
             assert_raise_message(ValueError, msg, lr.fit, X, y)
-
+        for solver in ['newton-cg', 'lbfgs', 'sag', 'saga']:
             msg = ("Solver %s supports only dual=False, got dual=True" %
                    solver)
             lr = LR(solver=solver, dual=True)
@@ -163,7 +165,7 @@ def test_multinomial_binary():
     target = (iris.target > 0).astype(np.intp)
     target = np.array(["setosa", "not-setosa"])[target]
 
-    for solver in ['lbfgs', 'newton-cg', 'sag']:
+    for solver in ['lbfgs', 'newton-cg', 'sag', 'saga']:
         clf = LogisticRegression(solver=solver, multi_class='multinomial',
                                  random_state=42, max_iter=2000)
         clf.fit(iris.data, target)
@@ -249,12 +251,14 @@ def test_consistency_path():
     f = ignore_warnings
     # can't test with fit_intercept=True since LIBLINEAR
     # penalizes the intercept
-    for solver in ('lbfgs', 'newton-cg', 'liblinear', 'sag'):
+    for solver in ['sag', 'saga']:
         coefs, Cs, _ = f(logistic_regression_path)(
             X, y, Cs=Cs, fit_intercept=False, tol=1e-5, solver=solver,
+            max_iter=1000,
             random_state=0)
         for i, C in enumerate(Cs):
             lr = LogisticRegression(C=C, fit_intercept=False, tol=1e-5,
+                                    solver=solver,
                                     random_state=0)
             lr.fit(X, y)
             lr_coef = lr.coef_.ravel()
@@ -262,7 +266,7 @@ def test_consistency_path():
                                       err_msg="with solver = %s" % solver)
 
     # test for fit_intercept=True
-    for solver in ('lbfgs', 'newton-cg', 'liblinear', 'sag'):
+    for solver in ('lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga'):
         Cs = [1e3]
         coefs, Cs, _ = f(logistic_regression_path)(
             X, y, Cs=Cs, fit_intercept=True, tol=1e-6, solver=solver,
@@ -307,19 +311,19 @@ def test_logistic_loss_and_grad():
         loss, grad = _logistic_loss_and_grad(w, X, y, alpha=1.)
         approx_grad = optimize.approx_fprime(
             w, lambda w: _logistic_loss_and_grad(w, X, y, alpha=1.)[0], 1e-3
-            )
+        )
         assert_array_almost_equal(grad, approx_grad, decimal=2)
 
         # Second check that our intercept implementation is good
         w = np.zeros(n_features + 1)
         loss_interp, grad_interp = _logistic_loss_and_grad(
             w, X, y, alpha=1.
-            )
+        )
         assert_array_almost_equal(loss, loss_interp)
 
         approx_grad = optimize.approx_fprime(
             w, lambda w: _logistic_loss_and_grad(w, X, y, alpha=1.)[0], 1e-3
-            )
+        )
         assert_array_almost_equal(grad_interp, approx_grad, decimal=2)
 
 
@@ -356,7 +360,7 @@ def test_logistic_grad_hess():
         d_grad = np.array([
             _logistic_loss_and_grad(w + t * vector, X, y, alpha=1.)[1]
             for t in d_x
-            ])
+        ])
 
         d_grad -= d_grad.mean(axis=0)
         approx_hess_col = linalg.lstsq(d_x[:, np.newaxis], d_grad)[0].ravel()
@@ -393,7 +397,7 @@ def test_logistic_cv():
 
     coefs_paths = np.asarray(list(lr_cv.coefs_paths_.values()))
     assert_array_equal(coefs_paths.shape, (1, 3, 1, n_features))
-    assert_array_equal(lr_cv.Cs_.shape, (1, ))
+    assert_array_equal(lr_cv.Cs_.shape, (1,))
     scores = np.asarray(list(lr_cv.scores_.values()))
     assert_array_equal(scores.shape, (1, 3, 1))
 
@@ -518,16 +522,17 @@ def test_ovr_multinomial_iris():
     assert_array_equal(clf.classes_, [0, 1, 2])
     coefs_paths = np.asarray(list(clf.coefs_paths_.values()))
     assert_array_almost_equal(coefs_paths.shape, (3, n_cv, 10, n_features + 1))
-    assert_equal(clf.Cs_.shape, (10, ))
+    assert_equal(clf.Cs_.shape, (10,))
     scores = np.asarray(list(clf.scores_.values()))
     assert_equal(scores.shape, (3, n_cv, 10))
 
     # Test that for the iris data multinomial gives a better accuracy than OvR
-    for solver in ['lbfgs', 'newton-cg', 'sag']:
-        max_iter = 100 if solver == 'sag' else 15
+    for solver in ['lbfgs', 'newton-cg', 'sag', 'saga']:
+        max_iter = 2000 if solver in ['sag', 'saga'] else 15
         clf_multi = LogisticRegressionCV(
             solver=solver, multi_class='multinomial', max_iter=max_iter,
-            random_state=42, tol=1e-2, cv=2)
+            random_state=42, tol=1e-5 if solver in ['sag', 'saga'] else 1e-2,
+            cv=2)
         clf_multi.fit(train, target)
         multi_score = clf_multi.score(train, target)
         ovr_score = clf.score(train, target)
@@ -539,7 +544,7 @@ def test_ovr_multinomial_iris():
         coefs_paths = np.asarray(list(clf_multi.coefs_paths_.values()))
         assert_array_almost_equal(coefs_paths.shape, (3, n_cv, 10,
                                                       n_features + 1))
-        assert_equal(clf_multi.Cs_.shape, (10, ))
+        assert_equal(clf_multi.Cs_.shape, (10,))
         scores = np.asarray(list(clf_multi.scores_.values()))
         assert_equal(scores.shape, (3, n_cv, 10))
 
@@ -552,9 +557,12 @@ def test_logistic_regression_solvers():
     lib = LogisticRegression(fit_intercept=False)
     sag = LogisticRegression(solver='sag', fit_intercept=False,
                              random_state=42)
+    saga = LogisticRegression(solver='saga', fit_intercept=False,
+                              random_state=42)
     ncg.fit(X, y)
     lbf.fit(X, y)
     sag.fit(X, y)
+    saga.fit(X, y)
     lib.fit(X, y)
     assert_array_almost_equal(ncg.coef_, lib.coef_, decimal=3)
     assert_array_almost_equal(lib.coef_, lbf.coef_, decimal=3)
@@ -562,20 +570,27 @@ def test_logistic_regression_solvers():
     assert_array_almost_equal(sag.coef_, lib.coef_, decimal=3)
     assert_array_almost_equal(sag.coef_, ncg.coef_, decimal=3)
     assert_array_almost_equal(sag.coef_, lbf.coef_, decimal=3)
+    assert_array_almost_equal(saga.coef_, sag.coef_, decimal=3)
+    assert_array_almost_equal(saga.coef_, lbf.coef_, decimal=3)
+    assert_array_almost_equal(saga.coef_, ncg.coef_, decimal=3)
+    assert_array_almost_equal(saga.coef_, lib.coef_, decimal=3)
 
 
 def test_logistic_regression_solvers_multiclass():
     X, y = make_classification(n_samples=20, n_features=20, n_informative=10,
                                n_classes=3, random_state=0)
-    tol = 1e-6
+    tol = 1e-7
     ncg = LogisticRegression(solver='newton-cg', fit_intercept=False, tol=tol)
     lbf = LogisticRegression(solver='lbfgs', fit_intercept=False, tol=tol)
     lib = LogisticRegression(fit_intercept=False, tol=tol)
     sag = LogisticRegression(solver='sag', fit_intercept=False, tol=tol,
                              max_iter=1000, random_state=42)
+    saga = LogisticRegression(solver='saga', fit_intercept=False, tol=tol,
+                              max_iter=10000, random_state=42)
     ncg.fit(X, y)
     lbf.fit(X, y)
     sag.fit(X, y)
+    saga.fit(X, y)
     lib.fit(X, y)
     assert_array_almost_equal(ncg.coef_, lib.coef_, decimal=4)
     assert_array_almost_equal(lib.coef_, lbf.coef_, decimal=4)
@@ -583,6 +598,10 @@ def test_logistic_regression_solvers_multiclass():
     assert_array_almost_equal(sag.coef_, lib.coef_, decimal=4)
     assert_array_almost_equal(sag.coef_, ncg.coef_, decimal=4)
     assert_array_almost_equal(sag.coef_, lbf.coef_, decimal=4)
+    assert_array_almost_equal(saga.coef_, sag.coef_, decimal=4)
+    assert_array_almost_equal(saga.coef_, lbf.coef_, decimal=4)
+    assert_array_almost_equal(saga.coef_, ncg.coef_, decimal=4)
+    assert_array_almost_equal(saga.coef_, lib.coef_, decimal=4)
 
 
 def test_logistic_regressioncv_class_weights():
@@ -608,13 +627,20 @@ def test_logistic_regressioncv_class_weights():
                                            class_weight=class_weight,
                                            tol=1e-5, max_iter=10000,
                                            random_state=0)
+            clf_saga = LogisticRegressionCV(solver='saga', Cs=1,
+                                            fit_intercept=False,
+                                            class_weight=class_weight,
+                                            tol=1e-5, max_iter=10000,
+                                            random_state=0)
             clf_lbf.fit(X, y)
             clf_ncg.fit(X, y)
             clf_lib.fit(X, y)
             clf_sag.fit(X, y)
+            clf_saga.fit(X, y)
             assert_array_almost_equal(clf_lib.coef_, clf_lbf.coef_, decimal=4)
             assert_array_almost_equal(clf_ncg.coef_, clf_lbf.coef_, decimal=4)
             assert_array_almost_equal(clf_sag.coef_, clf_lbf.coef_, decimal=4)
+            assert_array_almost_equal(clf_saga.coef_, clf_lbf.coef_, decimal=4)
 
 
 def test_logistic_regression_sample_weights():
@@ -760,11 +786,12 @@ def test_logistic_regression_multinomial():
     ref_w.fit(X, y)
     assert_array_equal(ref_i.coef_.shape, (n_classes, n_features))
     assert_array_equal(ref_w.coef_.shape, (n_classes, n_features))
-    for solver in ['sag', 'newton-cg']:
+    for solver in ['sag', 'saga', 'newton-cg']:
         clf_i = LogisticRegression(solver=solver, multi_class='multinomial',
-                                   random_state=42, max_iter=1000, tol=1e-6)
+                                   random_state=42, max_iter=2000, tol=1e-7,
+                                   )
         clf_w = LogisticRegression(solver=solver, multi_class='multinomial',
-                                   random_state=42, max_iter=1000, tol=1e-6,
+                                   random_state=42, max_iter=2000, tol=1e-7,
                                    fit_intercept=False)
         clf_i.fit(X, y)
         clf_w.fit(X, y)
@@ -779,7 +806,7 @@ def test_logistic_regression_multinomial():
     # Test that the path give almost the same results. However since in this
     # case we take the average of the coefs after fitting across all the
     # folds, it need not be exactly the same.
-    for solver in ['lbfgs', 'newton-cg', 'sag']:
+    for solver in ['lbfgs', 'newton-cg', 'sag', 'saga']:
         clf_path = LogisticRegressionCV(solver=solver, max_iter=2000, tol=1e-6,
                                         multi_class='multinomial', Cs=[1.])
         clf_path.fit(X, y)
@@ -812,7 +839,7 @@ def test_multinomial_grad_hess():
         _multinomial_grad_hess(w + t * vec, X, Y, alpha=1.,
                                sample_weight=sample_weights)[0]
         for t in d_x
-        ])
+    ])
     d_grad -= d_grad.mean(axis=0)
     approx_hess_col = linalg.lstsq(d_x[:, np.newaxis], d_grad)[0].ravel()
     assert_array_almost_equal(hess_col, approx_hess_col)
@@ -841,6 +868,14 @@ def test_liblinear_logregcv_sparse():
     clf.fit(sparse.csr_matrix(X), y)
 
 
+def test_saga_sparse():
+    # Test LogRegCV with solver='liblinear' works for sparse matrices
+
+    X, y = make_classification(n_samples=10, n_features=5, random_state=0)
+    clf = LogisticRegressionCV(solver='saga')
+    clf.fit(sparse.csr_matrix(X), y)
+
+
 def test_logreg_intercept_scaling():
     # Test that the right error message is thrown when intercept_scaling <= 0
 
@@ -860,6 +895,71 @@ def test_logreg_intercept_scaling_zero():
     assert_equal(clf.intercept_, 0.)
 
 
+def test_logreg_l1():
+    # Because liblinear penalizes the intercept and saga does not, we do not
+    # fit the intercept to make it possible to compare the coefficients of
+    # the two models at convergence.
+    rng = np.random.RandomState(42)
+    n_samples = 50
+    X, y = make_classification(n_samples=n_samples, n_features=20,
+                               random_state=0)
+    X_noise = rng.normal(size=(n_samples, 3))
+    X_constant = np.ones(shape=(n_samples, 2))
+    X = np.concatenate((X, X_noise, X_constant), axis=1)
+    lr_liblinear = LogisticRegression(penalty="l1", C=1.0, solver='liblinear',
+                                      fit_intercept=False,
+                                      tol=1e-10)
+    lr_liblinear.fit(X, y)
+
+    lr_saga = LogisticRegression(penalty="l1", C=1.0, solver='saga',
+                                 fit_intercept=False,
+                                 max_iter=1000, tol=1e-10)
+    lr_saga.fit(X, y)
+    assert_array_almost_equal(lr_saga.coef_, lr_liblinear.coef_)
+
+    # Noise and constant features should be regularized to zero by the l1
+    # penalty
+    assert_array_almost_equal(lr_liblinear.coef_[0, -5:], np.zeros(5))
+    assert_array_almost_equal(lr_saga.coef_[0, -5:], np.zeros(5))
+
+
+def test_logreg_l1_sparse_data():
+    # Because liblinear penalizes the intercept and saga does not, we do not
+    # fit the intercept to make it possible to compare the coefficients of
+    # the two models at convergence.
+    rng = np.random.RandomState(42)
+    n_samples = 50
+    X, y = make_classification(n_samples=n_samples, n_features=20,
+                               random_state=0)
+    X_noise = rng.normal(scale=0.1, size=(n_samples, 3))
+    X_constant = np.zeros(shape=(n_samples, 2))
+    X = np.concatenate((X, X_noise, X_constant), axis=1)
+    X[X < 1] = 0
+    X = sparse.csr_matrix(X)
+
+    lr_liblinear = LogisticRegression(penalty="l1", C=1.0, solver='liblinear',
+                                      fit_intercept=False,
+                                      tol=1e-10)
+    lr_liblinear.fit(X, y)
+
+    lr_saga = LogisticRegression(penalty="l1", C=1.0, solver='saga',
+                                 fit_intercept=False,
+                                 max_iter=1000, tol=1e-10)
+    lr_saga.fit(X, y)
+    assert_array_almost_equal(lr_saga.coef_, lr_liblinear.coef_)
+    # Noise and constant features should be regularized to zero by the l1
+    # penalty
+    assert_array_almost_equal(lr_liblinear.coef_[0, -5:], np.zeros(5))
+    assert_array_almost_equal(lr_saga.coef_[0, -5:], np.zeros(5))
+
+    # Check that solving on the sparse and dense data yield the same results
+    lr_saga_dense = LogisticRegression(penalty="l1", C=1.0, solver='saga',
+                                       fit_intercept=False,
+                                       max_iter=1000, tol=1e-10)
+    lr_saga_dense.fit(X.toarray(), y)
+    assert_array_almost_equal(lr_saga.coef_, lr_saga_dense.coef_)
+
+
 def test_logreg_cv_penalty():
     # Test that the correct penalty is passed to the final fit.
     X, y = make_classification(n_samples=50, n_features=20, random_state=0)
@@ -897,7 +997,7 @@ def test_max_iter():
     X, y_bin = iris.data, iris.target.copy()
     y_bin[y_bin == 2] = 0
 
-    solvers = ['newton-cg', 'liblinear', 'sag']
+    solvers = ['newton-cg', 'liblinear', 'sag', 'saga']
     # old scipy doesn't have maxiter
     if sp_version >= (0, 12):
         solvers.append('lbfgs')
@@ -923,7 +1023,7 @@ def test_n_iter():
     n_Cs = 4
     n_cv_fold = 2
 
-    for solver in ['newton-cg', 'liblinear', 'sag', 'lbfgs']:
+    for solver in ['newton-cg', 'liblinear', 'sag', 'saga', 'lbfgs']:
         # OvR case
         n_classes = 1 if solver == 'liblinear' else np.unique(y).shape[0]
         clf = LogisticRegression(tol=1e-2, multi_class='ovr',
@@ -943,7 +1043,7 @@ def test_n_iter():
 
         # multinomial case
         n_classes = 1
-        if solver in ('liblinear', 'sag'):
+        if solver in ('liblinear', 'sag', 'saga'):
             break
 
         clf = LogisticRegression(tol=1e-2, multi_class='multinomial',
@@ -967,7 +1067,7 @@ def test_warm_start():
     # Warm starting does not work with liblinear solver.
     X, y = iris.data, iris.target
 
-    solvers = ['newton-cg', 'sag']
+    solvers = ['newton-cg', 'sag', 'saga']
     # old scipy doesn't have maxiter
     if sp_version >= (0, 12):
         solvers.append('lbfgs')
@@ -996,3 +1096,43 @@ def test_warm_start():
                         assert_greater(2.0, cum_diff, msg)
                     else:
                         assert_greater(cum_diff, 2.0, msg)
+
+
+def test_saga_vs_liblinear():
+    iris = load_iris()
+    X, y = iris.data, iris.target
+    X = np.concatenate([X] * 10)
+    y = np.concatenate([y] * 10)
+
+    X_bin = X[y <= 1]
+    y_bin = y[y <= 1] * 2 - 1
+
+    X_sparse, y_sparse = make_classification(n_samples=50, n_features=20,
+                                             random_state=0)
+    X_sparse = sparse.csr_matrix(X_sparse)
+
+    for (X, y) in ((X_bin, y_bin), (X_sparse, y_sparse)):
+        for penalty in ['l1', 'l2']:
+            n_samples = X.shape[0]
+            # alpha=1e-3 is time consuming
+            for alpha in np.logspace(-1, 1, 3):
+                saga = LogisticRegression(
+                    C=1. / (n_samples * alpha),
+                    solver='saga',
+                    multi_class='ovr',
+                    max_iter=200,
+                    fit_intercept=False,
+                    penalty=penalty, random_state=0, tol=1e-24)
+
+                liblinear = LogisticRegression(
+                    C=1. / (n_samples * alpha),
+                    solver='liblinear',
+                    multi_class='ovr',
+                    max_iter=200,
+                    fit_intercept=False,
+                    penalty=penalty, random_state=0, tol=1e-24)
+
+                saga.fit(X, y)
+                liblinear.fit(X, y)
+                # Convergence for alpha=1e-3 is very slow
+                assert_array_almost_equal(saga.coef_, liblinear.coef_, 3)
diff --git a/sklearn/linear_model/tests/test_ransac.py b/sklearn/linear_model/tests/test_ransac.py
index b19ee0aa2589..41255f0c45fa 100644
--- a/sklearn/linear_model/tests/test_ransac.py
+++ b/sklearn/linear_model/tests/test_ransac.py
@@ -22,10 +22,9 @@
 data = np.column_stack([X, y])
 
 # Add some faulty data
-outliers = np.array((10, 30, 200))
-data[outliers[0], :] = (1000, 1000)
-data[outliers[1], :] = (-1000, -1000)
-data[outliers[2], :] = (-100, -50)
+rng = np.random.RandomState(1000)
+outliers = np.unique(rng.randint(len(X), size=200))
+data[outliers, :] += 50 + rng.rand(len(outliers), 2) * 10
 
 X = data[:, 0][:, np.newaxis]
 y = data[:, 1]
@@ -90,13 +89,16 @@ def test_ransac_max_trials():
                                        random_state=0)
     assert_raises(ValueError, ransac_estimator.fit, X, y)
 
-    ransac_estimator = RANSACRegressor(base_estimator, min_samples=2,
-                                       residual_threshold=5, max_trials=11,
-                                       random_state=0)
-    assert getattr(ransac_estimator, 'n_trials_', None) is None
-    ransac_estimator.fit(X, y)
-    assert_equal(ransac_estimator.n_trials_, 2)
-
+    # there is a 1e-9 chance it will take these many trials. No good reason
+    # 1e-2 isn't enough, can still happen
+    # 2 is the what ransac defines  as min_samples = X.shape[1] + 1
+    max_trials = _dynamic_max_trials(
+        len(X) - len(outliers), X.shape[0], 2, 1 - 1e-9)
+    ransac_estimator = RANSACRegressor(base_estimator, min_samples=2)
+    for i in range(50):
+        ransac_estimator.set_params(min_samples=2, random_state=i)
+        ransac_estimator.fit(X, y)
+        assert_less(ransac_estimator.n_trials_, max_trials + 1)
 
 def test_ransac_stop_n_inliers():
     base_estimator = LinearRegression()
@@ -383,6 +385,7 @@ def test_ransac_residual_metric():
     assert_array_almost_equal(ransac_estimator0.predict(X),
                               ransac_estimator2.predict(X))
 
+
 def test_ransac_residual_loss():
     loss_multi1 = lambda y_true, y_pred: np.sum(np.abs(y_true - y_pred), axis=1)
     loss_multi2 = lambda y_true, y_pred: np.sum((y_true - y_pred) ** 2, axis=1)
diff --git a/sklearn/linear_model/tests/test_ridge.py b/sklearn/linear_model/tests/test_ridge.py
index 433801e45a8c..bac5a146d90f 100644
--- a/sklearn/linear_model/tests/test_ridge.py
+++ b/sklearn/linear_model/tests/test_ridge.py
@@ -282,7 +282,7 @@ def test_ridge_individual_penalties():
 
     coefs_indiv_pen = [
         Ridge(alpha=penalties, solver=solver, tol=1e-8).fit(X, y).coef_
-        for solver in ['svd', 'sparse_cg', 'lsqr', 'cholesky', 'sag']]
+        for solver in ['svd', 'sparse_cg', 'lsqr', 'cholesky', 'sag', 'saga']]
     for coef_indiv_pen in coefs_indiv_pen:
         assert_array_almost_equal(coef_cholesky, coef_indiv_pen)
 
@@ -712,7 +712,7 @@ def test_n_iter():
     y_n = np.tile(y, (n_targets, 1)).T
 
     for max_iter in range(1, 4):
-        for solver in ('sag', 'lsqr'):
+        for solver in ('sag', 'saga', 'lsqr'):
             reg = Ridge(solver=solver, max_iter=max_iter, tol=1e-12)
             reg.fit(X, y_n)
             assert_array_equal(reg.n_iter_, np.tile(max_iter, n_targets))
@@ -728,12 +728,13 @@ def test_ridge_fit_intercept_sparse():
                            bias=10., random_state=42)
     X_csr = sp.csr_matrix(X)
 
-    dense = Ridge(alpha=1., tol=1.e-15, solver='sag', fit_intercept=True)
-    sparse = Ridge(alpha=1., tol=1.e-15, solver='sag', fit_intercept=True)
-    dense.fit(X, y)
-    sparse.fit(X_csr, y)
-    assert_almost_equal(dense.intercept_, sparse.intercept_)
-    assert_array_almost_equal(dense.coef_, sparse.coef_)
+    for solver in ['saga', 'sag']:
+        dense = Ridge(alpha=1., tol=1.e-15, solver=solver, fit_intercept=True)
+        sparse = Ridge(alpha=1., tol=1.e-15, solver=solver, fit_intercept=True)
+        dense.fit(X, y)
+        sparse.fit(X_csr, y)
+        assert_almost_equal(dense.intercept_, sparse.intercept_)
+        assert_array_almost_equal(dense.coef_, sparse.coef_)
 
     # test the solver switch and the corresponding warning
     sparse = Ridge(alpha=1., tol=1.e-15, solver='lsqr', fit_intercept=True)
diff --git a/sklearn/linear_model/tests/test_sag.py b/sklearn/linear_model/tests/test_sag.py
index a993fb9f5919..e21cd329c55f 100644
--- a/sklearn/linear_model/tests/test_sag.py
+++ b/sklearn/linear_model/tests/test_sag.py
@@ -63,7 +63,7 @@ def get_pobj(w, alpha, myX, myy, loss):
 
 
 def sag(X, y, step_size, alpha, n_iter=1, dloss=None, sparse=False,
-        sample_weight=None, fit_intercept=True):
+        sample_weight=None, fit_intercept=True, saga=False):
     n_samples, n_features = X.shape[0], X.shape[1]
 
     weights = np.zeros(X.shape[1])
@@ -93,15 +93,25 @@ def sag(X, y, step_size, alpha, n_iter=1, dloss=None, sparse=False,
             if sample_weight is not None:
                 gradient *= sample_weight[idx]
             update = entry * gradient + alpha * weights
-            sum_gradient += update - gradient_memory[idx]
+            gradient_correction = update - gradient_memory[idx]
+            sum_gradient += gradient_correction
             gradient_memory[idx] = update
+            if saga:
+                weights -= (gradient_correction *
+                            step_size * (1 - 1. / len(seen)))
 
             if fit_intercept:
-                intercept_sum_gradient += (gradient -
-                                           intercept_gradient_memory[idx])
+                gradient_correction = (gradient -
+                                       intercept_gradient_memory[idx])
                 intercept_gradient_memory[idx] = gradient
-                intercept -= (step_size * intercept_sum_gradient
-                              / len(seen) * decay)
+                intercept_sum_gradient += gradient_correction
+                gradient_correction *= step_size * (1. - 1. / len(seen))
+                if saga:
+                    intercept -= (step_size * intercept_sum_gradient /
+                                  len(seen) * decay) + gradient_correction
+                else:
+                    intercept -= (step_size * intercept_sum_gradient /
+                                  len(seen) * decay)
 
             weights -= step_size * sum_gradient / len(seen)
 
@@ -110,7 +120,7 @@ def sag(X, y, step_size, alpha, n_iter=1, dloss=None, sparse=False,
 
 def sag_sparse(X, y, step_size, alpha, n_iter=1,
                dloss=None, sample_weight=None, sparse=False,
-               fit_intercept=True):
+               fit_intercept=True, saga=False):
     if step_size * alpha == 1.:
         raise ZeroDivisionError("Sparse sag does not handle the case "
                                 "step_size * alpha == 1")
@@ -158,12 +168,24 @@ def sag_sparse(X, y, step_size, alpha, n_iter=1,
                 gradient *= sample_weight[idx]
 
             update = entry * gradient
-            sum_gradient += update - (gradient_memory[idx] * entry)
+            gradient_correction = update - (gradient_memory[idx] * entry)
+            sum_gradient += gradient_correction
+            if saga:
+                for j in range(n_features):
+                    weights[j] -= (gradient_correction[j] * step_size *
+                                   (1 - 1. / len(seen)) / wscale)
 
             if fit_intercept:
-                intercept_sum_gradient += gradient - gradient_memory[idx]
-                intercept -= (step_size * intercept_sum_gradient
-                              / len(seen) * decay)
+                gradient_correction = gradient - gradient_memory[idx]
+                intercept_sum_gradient += gradient_correction
+                gradient_correction *= step_size * (1. - 1. / len(seen))
+                if saga:
+                    intercept -= ((step_size * intercept_sum_gradient /
+                                   len(seen) * decay) +
+                                  gradient_correction)
+                else:
+                    intercept -= (step_size * intercept_sum_gradient /
+                                  len(seen) * decay)
 
             gradient_memory[idx] = gradient
 
@@ -202,8 +224,8 @@ def sag_sparse(X, y, step_size, alpha, n_iter=1,
 
 def get_step_size(X, alpha, fit_intercept, classification=True):
     if classification:
-        return (4.0 / (np.max(np.sum(X * X, axis=1))
-                + fit_intercept + 4.0 * alpha))
+        return (4.0 / (np.max(np.sum(X * X, axis=1)) +
+                       fit_intercept + 4.0 * alpha))
     else:
         return 1.0 / (np.max(np.sum(X * X, axis=1)) + fit_intercept + alpha)
 
@@ -215,29 +237,36 @@ def test_classifier_matching():
                       cluster_std=0.1)
     y[y == 0] = -1
     alpha = 1.1
-    n_iter = 80
     fit_intercept = True
     step_size = get_step_size(X, alpha, fit_intercept)
-    clf = LogisticRegression(solver="sag", fit_intercept=fit_intercept,
-                             tol=1e-11, C=1. / alpha / n_samples,
-                             max_iter=n_iter, random_state=10)
-    clf.fit(X, y)
-
-    weights, intercept = sag_sparse(X, y, step_size, alpha, n_iter=n_iter,
-                                    dloss=log_dloss,
-                                    fit_intercept=fit_intercept)
-    weights2, intercept2 = sag(X, y, step_size, alpha, n_iter=n_iter,
-                               dloss=log_dloss,
-                               fit_intercept=fit_intercept)
-    weights = np.atleast_2d(weights)
-    intercept = np.atleast_1d(intercept)
-    weights2 = np.atleast_2d(weights2)
-    intercept2 = np.atleast_1d(intercept2)
-
-    assert_array_almost_equal(weights, clf.coef_, decimal=10)
-    assert_array_almost_equal(intercept, clf.intercept_, decimal=10)
-    assert_array_almost_equal(weights2, clf.coef_, decimal=10)
-    assert_array_almost_equal(intercept2, clf.intercept_, decimal=10)
+    for solver in ['sag', 'saga']:
+        if solver == 'sag':
+            n_iter = 80
+        else:
+            # SAGA variance w.r.t. stream order is higher
+            n_iter = 300
+        clf = LogisticRegression(solver=solver, fit_intercept=fit_intercept,
+                                 tol=1e-11, C=1. / alpha / n_samples,
+                                 max_iter=n_iter, random_state=10)
+        clf.fit(X, y)
+
+        weights, intercept = sag_sparse(X, y, step_size, alpha, n_iter=n_iter,
+                                        dloss=log_dloss,
+                                        fit_intercept=fit_intercept,
+                                        saga=solver == 'saga')
+        weights2, intercept2 = sag(X, y, step_size, alpha, n_iter=n_iter,
+                                   dloss=log_dloss,
+                                   fit_intercept=fit_intercept,
+                                   saga=solver == 'saga')
+        weights = np.atleast_2d(weights)
+        intercept = np.atleast_1d(intercept)
+        weights2 = np.atleast_2d(weights2)
+        intercept2 = np.atleast_1d(intercept2)
+
+        assert_array_almost_equal(weights, clf.coef_, decimal=9)
+        assert_array_almost_equal(intercept, clf.intercept_, decimal=9)
+        assert_array_almost_equal(weights2, clf.coef_, decimal=9)
+        assert_array_almost_equal(intercept2, clf.intercept_, decimal=9)
 
 
 @ignore_warnings
@@ -372,10 +401,10 @@ def test_sag_regressor_computed_correctly():
     assert_almost_equal(clf1.intercept_, spintercept1, decimal=1)
 
     # TODO: uncomment when sparse Ridge with intercept will be fixed (#4710)
-    #assert_array_almost_equal(clf2.coef_.ravel(),
+    # assert_array_almost_equal(clf2.coef_.ravel(),
     #                          spweights2.ravel(),
     #                          decimal=3)
-    #assert_almost_equal(clf2.intercept_, spintercept2, decimal=1)'''
+    # assert_almost_equal(clf2.intercept_, spintercept2, decimal=1)'''
 
 
 @ignore_warnings
@@ -386,20 +415,37 @@ def test_get_auto_step_size():
     # sum the squares of the second sample because that's the largest
     max_squared_sum = 4 + 9 + 16
     max_squared_sum_ = row_norms(X, squared=True).max()
+    n_samples = X.shape[0]
     assert_almost_equal(max_squared_sum, max_squared_sum_, decimal=4)
 
-    for fit_intercept in (True, False):
-        step_size_sqr = 1.0 / (max_squared_sum + alpha + int(fit_intercept))
-        step_size_log = 4.0 / (max_squared_sum + 4.0 * alpha +
-                               int(fit_intercept))
-
-        step_size_sqr_ = get_auto_step_size(max_squared_sum_, alpha, "squared",
-                                            fit_intercept)
-        step_size_log_ = get_auto_step_size(max_squared_sum_, alpha, "log",
-                                            fit_intercept)
-
-        assert_almost_equal(step_size_sqr, step_size_sqr_, decimal=4)
-        assert_almost_equal(step_size_log, step_size_log_, decimal=4)
+    for saga in [True, False]:
+        for fit_intercept in (True, False):
+            if saga:
+                L_sqr = (max_squared_sum + alpha + int(fit_intercept))
+                L_log = (max_squared_sum + 4.0 * alpha +
+                         int(fit_intercept)) / 4.0
+                mun_sqr = min(2 * n_samples * alpha, L_sqr)
+                mun_log = min(2 * n_samples * alpha, L_log)
+                step_size_sqr = 1 / (2 * L_sqr + mun_sqr)
+                step_size_log = 1 / (2 * L_log + mun_log)
+            else:
+                step_size_sqr = 1.0 / (max_squared_sum +
+                                       alpha + int(fit_intercept))
+                step_size_log = 4.0 / (max_squared_sum + 4.0 * alpha +
+                                       int(fit_intercept))
+
+            step_size_sqr_ = get_auto_step_size(max_squared_sum_, alpha,
+                                                "squared",
+                                                fit_intercept,
+                                                n_samples=n_samples,
+                                                is_saga=saga)
+            step_size_log_ = get_auto_step_size(max_squared_sum_, alpha, "log",
+                                                fit_intercept,
+                                                n_samples=n_samples,
+                                                is_saga=saga)
+
+            assert_almost_equal(step_size_sqr, step_size_sqr_, decimal=4)
+            assert_almost_equal(step_size_log, step_size_log_, decimal=4)
 
     msg = 'Unknown loss function for SAG solver, got wrong instead of'
     assert_raise_message(ValueError, msg, get_auto_step_size,
diff --git a/sklearn/linear_model/theil_sen.py b/sklearn/linear_model/theil_sen.py
index 23b3c106c9bd..b51f7d6dd3c3 100644
--- a/sklearn/linear_model/theil_sen.py
+++ b/sklearn/linear_model/theil_sen.py
@@ -243,9 +243,12 @@ class TheilSenRegressor(LinearModel, RegressorMixin):
     tol : float, optional, default 1.e-3
         Tolerance when calculating spatial median.
 
-    random_state : RandomState or an int seed, optional, default None
-        A random number generator instance to define the state of the
-        random permutations generator.
+    random_state : int, RandomState instance or None, optional, default None
+        A random number generator instance to define the state of the random
+        permutations generator.  If int, random_state is the seed used by the
+        random number generator; If RandomState instance, random_state is the
+        random number generator; If None, the random number generator is the
+        RandomState instance used by `np.random`.
 
     n_jobs : integer, optional, default 1
         Number of CPUs to use during the cross validation. If ``-1``, use
diff --git a/sklearn/manifold/locally_linear.py b/sklearn/manifold/locally_linear.py
index 367710edc667..82c4b6125436 100644
--- a/sklearn/manifold/locally_linear.py
+++ b/sklearn/manifold/locally_linear.py
@@ -140,9 +140,11 @@ def null_space(M, k, k_skip=1, eigen_solver='arpack', tol=1E-6, max_iter=100,
     max_iter : maximum number of iterations for 'arpack' method
         not used if eigen_solver=='dense'
 
-    random_state : numpy.RandomState or int, optional
-        The generator or seed used to determine the starting vector for arpack
-        iterations.  Defaults to numpy.random.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`. Used when ``solver`` == 'arpack'.
 
     """
     if eigen_solver == 'auto':
@@ -245,9 +247,11 @@ def locally_linear_embedding(
         Tolerance for modified LLE method.
         Only used if method == 'modified'
 
-    random_state : numpy.RandomState or int, optional
-        The generator or seed used to determine the starting vector for arpack
-        iterations.  Defaults to numpy.random.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`. Used when ``solver`` == 'arpack'.
 
     n_jobs : int, optional (default = 1)
         The number of parallel jobs to run for neighbors search.
@@ -568,9 +572,11 @@ class LocallyLinearEmbedding(BaseEstimator, TransformerMixin):
         algorithm to use for nearest neighbors search,
         passed to neighbors.NearestNeighbors instance
 
-    random_state : numpy.RandomState or int, optional
-        The generator or seed used to determine the starting vector for arpack
-        iterations.  Defaults to numpy.random.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`. Used when ``eigen_solver`` == 'arpack'.
 
     n_jobs : int, optional (default = 1)
         The number of parallel jobs to run.
diff --git a/sklearn/manifold/mds.py b/sklearn/manifold/mds.py
index b2fe62040bb9..5f7327ef4dc8 100644
--- a/sklearn/manifold/mds.py
+++ b/sklearn/manifold/mds.py
@@ -19,8 +19,7 @@
 
 def _smacof_single(dissimilarities, metric=True, n_components=2, init=None,
                    max_iter=300, verbose=0, eps=1e-3, random_state=None):
-    """
-    Computes multidimensional scaling using SMACOF algorithm
+    """Computes multidimensional scaling using SMACOF algorithm
 
     Parameters
     ----------
@@ -50,10 +49,11 @@ def _smacof_single(dissimilarities, metric=True, n_components=2, init=None,
         Relative tolerance with respect to stress at which to declare
         convergence.
 
-    random_state : integer or numpy.RandomState, optional
-        The generator used to initialize the centers. If an integer is
-        given, it fixes the seed. Defaults to the global numpy random
-        number generator.
+    random_state : int, RandomState instance or None, optional, default: None
+        The generator used to initialize the centers.  If int, random_state is
+        the seed used by the random number generator; If RandomState instance,
+        random_state is the random number generator; If None, the random number
+        generator is the RandomState instance used by `np.random`.
 
     Returns
     -------
@@ -134,8 +134,7 @@ def _smacof_single(dissimilarities, metric=True, n_components=2, init=None,
 def smacof(dissimilarities, metric=True, n_components=2, init=None, n_init=8,
            n_jobs=1, max_iter=300, verbose=0, eps=1e-3, random_state=None,
            return_n_iter=False):
-    """
-    Computes multidimensional scaling using the SMACOF algorithm.
+    """Computes multidimensional scaling using the SMACOF algorithm.
 
     The SMACOF (Scaling by MAjorizing a COmplicated Function) algorithm is a
     multidimensional scaling algorithm which minimizes an objective function
@@ -198,10 +197,11 @@ def smacof(dissimilarities, metric=True, n_components=2, init=None, n_init=8,
         Relative tolerance with respect to stress at which to declare
         convergence.
 
-    random_state : integer or numpy.RandomState, optional, default: None
-        The generator used to initialize the centers. If an integer is given,
-        it fixes the seed. Defaults to the global numpy random number
-        generator.
+    random_state : int, RandomState instance or None, optional, default: None
+        The generator used to initialize the centers.  If int, random_state is
+        the seed used by the random number generator; If RandomState instance,
+        random_state is the random number generator; If None, the random number
+        generator is the RandomState instance used by `np.random`.
 
     return_n_iter : bool, optional, default: False
         Whether or not to return the number of iterations.
@@ -314,10 +314,11 @@ class MDS(BaseEstimator):
         (``n_cpus + 1 + n_jobs``) are used. Thus for ``n_jobs = -2``, all CPUs
         but one are used.
 
-    random_state : integer or numpy.RandomState, optional, default: None
-        The generator used to initialize the centers. If an integer is given,
-        it fixes the seed. Defaults to the global numpy random number
-        generator.
+    random_state : int, RandomState instance or None, optional, default: None
+        The generator used to initialize the centers.  If int, random_state is
+        the seed used by the random number generator; If RandomState instance,
+        random_state is the random number generator; If None, the random number
+        generator is the RandomState instance used by `np.random`.
 
     dissimilarity : 'euclidean' | 'precomputed', optional, default: 'euclidean'
         Dissimilarity measure to use:
diff --git a/sklearn/manifold/spectral_embedding_.py b/sklearn/manifold/spectral_embedding_.py
index 39a7355cc8f5..31c90aa8b30a 100644
--- a/sklearn/manifold/spectral_embedding_.py
+++ b/sklearn/manifold/spectral_embedding_.py
@@ -166,10 +166,13 @@ def spectral_embedding(adjacency, n_components=8, eigen_solver=None,
         to be installed. It can be faster on very large, sparse problems,
         but may also lead to instabilities.
 
-    random_state : int seed, RandomState instance, or None (default)
+    random_state : int, RandomState instance or None, optional, default: None
         A pseudo random number generator used for the initialization of the
-        lobpcg eigenvectors decomposition when eigen_solver == 'amg'.
-        By default, arpack is used.
+        lobpcg eigenvectors decomposition.  If int, random_state is the seed
+        used by the random number generator; If RandomState instance,
+        random_state is the random number generator; If None, the random number
+        generator is the RandomState instance used by `np.random`. Used when
+        ``solver`` == 'amg'.
 
     eigen_tol : float, optional, default=0.0
         Stopping criterion for eigendecomposition of the Laplacian matrix
@@ -345,9 +348,13 @@ class SpectralEmbedding(BaseEstimator):
         to be installed. It can be faster on very large, sparse problems,
         but may also lead to instabilities.
 
-    random_state : int seed, RandomState instance, or None, default : None
+    random_state : int, RandomState instance or None, optional, default: None
         A pseudo random number generator used for the initialization of the
-        lobpcg eigenvectors decomposition when eigen_solver == 'amg'.
+        lobpcg eigenvectors.  If int, random_state is the seed used by the
+        random number generator; If RandomState instance, random_state is the
+        random number generator; If None, the random number generator is the
+        RandomState instance used by `np.random`. Used when ``solver`` ==
+        'amg'.
 
     affinity : string or callable, default : "nearest_neighbors"
         How to construct the affinity matrix.
@@ -384,7 +391,7 @@ class SpectralEmbedding(BaseEstimator):
       Ulrike von Luxburg
       http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323
 
-    - On Spectral Clustering: Analysis and an algorithm, 2011
+    - On Spectral Clustering: Analysis and an algorithm, 2001
       Andrew Y. Ng, Michael I. Jordan, Yair Weiss
       http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.19.8100
 
diff --git a/sklearn/manifold/t_sne.py b/sklearn/manifold/t_sne.py
index 1ce77f163896..b31f34d9eef1 100644
--- a/sklearn/manifold/t_sne.py
+++ b/sklearn/manifold/t_sne.py
@@ -441,7 +441,7 @@ def trustworthiness(X, X_embedded, n_neighbors=5, precomputed=False):
     .. math::
 
         T(k) = 1 - \frac{2}{nk (2n - 3k - 1)} \sum^n_{i=1}
-            \sum_{j \in U^{(k)}_i (r(i, j) - k)}
+            \sum_{j \in U^{(k)}_i} (r(i, j) - k)
 
     where :math:`r(i, j)` is the rank of the embedded datapoint j
     according to the pairwise distances between the embedded datapoints,
@@ -582,10 +582,12 @@ class TSNE(BaseEstimator):
     verbose : int, optional (default: 0)
         Verbosity level.
 
-    random_state : int or RandomState instance or None (default)
-        Pseudo Random Number generator seed control. If None, use the
-        numpy.random singleton. Note that different initializations
-        might result in different local minima of the cost function.
+    random_state : int, RandomState instance or None, optional (default: None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.  Note that different initializations might result in
+        different local minima of the cost function.
 
     method : string (default: 'barnes_hut')
         By default the gradient calculation algorithm uses Barnes-Hut
@@ -864,7 +866,8 @@ def _tsne(self, P, degrees_of_freedom, n_samples, random_state,
         P /= self.early_exaggeration
         opt_args['n_iter'] = self.n_iter
         opt_args['it'] = it + 1
-        params, error, it = _gradient_descent(obj_func, params, **opt_args)
+        params, kl_divergence, it = _gradient_descent(obj_func, params,
+                                                      **opt_args)
 
         if self.verbose:
             print("[t-SNE] Error after %d iterations: %f"
diff --git a/sklearn/manifold/tests/test_t_sne.py b/sklearn/manifold/tests/test_t_sne.py
index 3be02f359c16..ea9037776d71 100644
--- a/sklearn/manifold/tests/test_t_sne.py
+++ b/sklearn/manifold/tests/test_t_sne.py
@@ -625,3 +625,30 @@ def test_min_grad_norm():
     # The gradient norm can be smaller than min_grad_norm at most once,
     # because in the moment it becomes smaller the optimization stops
     assert_less_equal(n_smaller_gradient_norms, 1)
+
+
+def test_accessible_kl_divergence():
+    # Ensures that the accessible kl_divergence matches the computed value
+    random_state = check_random_state(0)
+    X = random_state.randn(100, 2)
+    tsne = TSNE(n_iter_without_progress=2, verbose=2,
+                random_state=0, method='exact')
+
+    old_stdout = sys.stdout
+    sys.stdout = StringIO()
+    try:
+        tsne.fit_transform(X)
+    finally:
+        out = sys.stdout.getvalue()
+        sys.stdout.close()
+        sys.stdout = old_stdout
+
+    # The output needs to contain the accessible kl_divergence as the error at
+    # the last iteration
+    for line in out.split('\n')[::-1]:
+        if 'Iteration' in line:
+            _, _, error = line.partition('error = ')
+            if error:
+                error, _, _ = error.partition(',')
+                break
+    assert_almost_equal(tsne.kl_divergence_, float(error), decimal=5)
diff --git a/sklearn/metrics/classification.py b/sklearn/metrics/classification.py
index 798d4ae12441..4884b4094e51 100644
--- a/sklearn/metrics/classification.py
+++ b/sklearn/metrics/classification.py
@@ -91,6 +91,10 @@ def _check_targets(y_true, y_pred):
     if y_type in ["binary", "multiclass"]:
         y_true = column_or_1d(y_true)
         y_pred = column_or_1d(y_pred)
+        if y_type == "binary":
+            unique_values = np.union1d(y_true, y_pred)
+            if len(unique_values) > 2:
+                y_type = "multiclass"
 
     if y_type.startswith('multilabel'):
         y_true = csr_matrix(y_true)
@@ -236,6 +240,12 @@ def confusion_matrix(y_true, y_pred, labels=None, sample_weight=None):
            [0, 0, 1],
            [1, 0, 2]])
 
+    In the binary case, we can extract true positives, etc as follows:
+
+    >>> tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()
+    >>> (tn, fp, fn, tp)
+    (0, 2, 1, 1)
+
     """
     y_type, y_true, y_pred = _check_targets(y_true, y_pred)
     if y_type not in ("binary", "multiclass"):
diff --git a/sklearn/metrics/cluster/unsupervised.py b/sklearn/metrics/cluster/unsupervised.py
index 606ffcddf849..3be683ae08d9 100644
--- a/sklearn/metrics/cluster/unsupervised.py
+++ b/sklearn/metrics/cluster/unsupervised.py
@@ -62,10 +62,12 @@ def silhouette_score(X, labels, metric='euclidean', sample_size=None,
         on a random subset of the data.
         If ``sample_size is None``, no sampling is used.
 
-    random_state : integer or numpy.RandomState, optional
-        The generator used to randomly select a subset of samples if
-        ``sample_size is not None``. If an integer is given, it fixes the seed.
-        Defaults to the global numpy random number generator.
+    random_state : int, RandomState instance or None, optional (default=None)
+        The generator used to randomly select a subset of samples.  If int,
+        random_state is the seed used by the random number generator; If
+        RandomState instance, random_state is the random number generator; If
+        None, the random number generator is the RandomState instance used by
+        `np.random`. Used when ``sample_size is not None``.
 
     **kwds : optional keyword parameters
         Any further parameters are passed directly to the distance function.
diff --git a/sklearn/metrics/tests/test_classification.py b/sklearn/metrics/tests/test_classification.py
index e9616e933b70..2e6e24051507 100644
--- a/sklearn/metrics/tests/test_classification.py
+++ b/sklearn/metrics/tests/test_classification.py
@@ -366,7 +366,8 @@ def test_matthews_corrcoef():
     y_true_inv = ["b" if i == "a" else "a" for i in y_true]
 
     assert_almost_equal(matthews_corrcoef(y_true, y_true_inv), -1)
-    y_true_inv2 = label_binarize(y_true, ["a", "b"]) * -1
+    y_true_inv2 = label_binarize(y_true, ["a", "b"])
+    y_true_inv2 = np.where(y_true_inv2, 'a', 'b')
     assert_almost_equal(matthews_corrcoef(y_true, y_true_inv2), -1)
 
     # For the zero vector case, the corrcoef cannot be calculated and should
@@ -379,8 +380,7 @@ def test_matthews_corrcoef():
 
     # And also for any other vector with 0 variance
     mcc = assert_warns_message(RuntimeWarning, 'invalid value encountered',
-                               matthews_corrcoef, y_true,
-                               rng.randint(-100, 100) * np.ones(20, dtype=int))
+                               matthews_corrcoef, y_true, ['a'] * len(y_true))
 
     # But will output 0
     assert_almost_equal(mcc, 0.)
@@ -1267,6 +1267,13 @@ def test__check_targets():
     assert_raise_message(ValueError, msg, _check_targets, y1, y2)
 
 
+def test__check_targets_multiclass_with_both_y_true_and_y_pred_binary():
+    # https://github.com/scikit-learn/scikit-learn/issues/8098
+    y_true = [0, 1]
+    y_pred = [0, -1]
+    assert_equal(_check_targets(y_true, y_pred)[0], 'multiclass')
+
+
 def test_hinge_loss_binary():
     y_true = np.array([-1, 1, 1, -1])
     pred_decision = np.array([-8.5, 0.5, 1.5, -0.3])
diff --git a/sklearn/metrics/tests/test_common.py b/sklearn/metrics/tests/test_common.py
index a91fc57c74a7..00db32e1ef38 100644
--- a/sklearn/metrics/tests/test_common.py
+++ b/sklearn/metrics/tests/test_common.py
@@ -1013,6 +1013,17 @@ def check_sample_weight_invariance(name, metric, y1, y2):
 
 def test_sample_weight_invariance(n_samples=50):
     random_state = check_random_state(0)
+    # regression
+    y_true = random_state.random_sample(size=(n_samples,))
+    y_pred = random_state.random_sample(size=(n_samples,))
+    for name in ALL_METRICS:
+        if name not in REGRESSION_METRICS:
+            continue
+        if name in METRICS_WITHOUT_SAMPLE_WEIGHT:
+            continue
+        metric = ALL_METRICS[name]
+        yield _named_check(check_sample_weight_invariance, name), name,\
+            metric, y_true, y_pred
 
     # binary
     random_state = check_random_state(0)
@@ -1020,6 +1031,8 @@ def test_sample_weight_invariance(n_samples=50):
     y_pred = random_state.randint(0, 2, size=(n_samples, ))
     y_score = random_state.random_sample(size=(n_samples,))
     for name in ALL_METRICS:
+        if name in REGRESSION_METRICS:
+            continue
         if (name in METRICS_WITHOUT_SAMPLE_WEIGHT or
                 name in METRIC_UNDEFINED_BINARY):
             continue
@@ -1037,6 +1050,8 @@ def test_sample_weight_invariance(n_samples=50):
     y_pred = random_state.randint(0, 5, size=(n_samples, ))
     y_score = random_state.random_sample(size=(n_samples, 5))
     for name in ALL_METRICS:
+        if name in REGRESSION_METRICS:
+            continue
         if (name in METRICS_WITHOUT_SAMPLE_WEIGHT or
                 name in METRIC_UNDEFINED_BINARY_MULTICLASS):
             continue
diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py
index ca48ea0c5c17..be0711b0f78c 100644
--- a/sklearn/mixture/base.py
+++ b/sklearn/mixture/base.py
@@ -230,7 +230,7 @@ def fit(self, X, y=None):
                 best_n_iter = n_iter
 
         if not self.converged_:
-            warnings.warn('Initialization %d did not converged. '
+            warnings.warn('Initialization %d did not converge. '
                           'Try different init parameters, '
                           'or increase max_iter, tol '
                           'or check for degenerate data.'
diff --git a/sklearn/mixture/bayesian_mixture.py b/sklearn/mixture/bayesian_mixture.py
index 497b339a4f80..24c0ae62e4ef 100644
--- a/sklearn/mixture/bayesian_mixture.py
+++ b/sklearn/mixture/bayesian_mixture.py
@@ -163,8 +163,11 @@ class BayesianGaussianMixture(BaseMixture):
                 (n_features)             if 'diag',
                 float                    if 'spherical'
 
-    random_state : RandomState or an int seed, defaults to None.
-        A random number generator instance.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     warm_start : bool, default to False.
         If 'warm_start' is True, the solution of the last fitting is used as
diff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py
index edbfc08c4e07..eced54072494 100644
--- a/sklearn/mixture/gaussian_mixture.py
+++ b/sklearn/mixture/gaussian_mixture.py
@@ -500,8 +500,11 @@ class GaussianMixture(BaseMixture):
             (n_components, n_features)             if 'diag',
             (n_components, n_features, n_features) if 'full'
 
-    random_state : RandomState or an int seed, defaults to None.
-        A random number generator instance.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     warm_start : bool, default to False.
         If 'warm_start' is True, the solution of the last fitting is used as
diff --git a/sklearn/mixture/gmm.py b/sklearn/mixture/gmm.py
index 024981bda8fa..5b2dece572c3 100644
--- a/sklearn/mixture/gmm.py
+++ b/sklearn/mixture/gmm.py
@@ -152,8 +152,11 @@ class _GMMBase(BaseEstimator):
         use.  Must be one of 'spherical', 'tied', 'diag', 'full'.
         Defaults to 'diag'.
 
-    random_state : RandomState or an int seed (None by default)
-        A random number generator instance
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     min_covar : float, optional
         Floor on the diagonal of the covariance matrix to prevent
diff --git a/sklearn/mixture/tests/test_gaussian_mixture.py b/sklearn/mixture/tests/test_gaussian_mixture.py
index 3aba57494cb9..ae1df9636aca 100644
--- a/sklearn/mixture/tests/test_gaussian_mixture.py
+++ b/sklearn/mixture/tests/test_gaussian_mixture.py
@@ -649,7 +649,7 @@ def test_gaussian_mixture_fit_convergence_warning():
                             max_iter=max_iter, reg_covar=0, random_state=rng,
                             covariance_type=covar_type)
         assert_warns_message(ConvergenceWarning,
-                             'Initialization %d did not converged. '
+                             'Initialization %d did not converge. '
                              'Try different init parameters, '
                              'or increase max_iter, tol '
                              'or check for degenerate data.'
diff --git a/sklearn/model_selection/__init__.py b/sklearn/model_selection/__init__.py
index f5ab0d7526cc..73c842e706df 100644
--- a/sklearn/model_selection/__init__.py
+++ b/sklearn/model_selection/__init__.py
@@ -7,6 +7,8 @@
 from ._split import LeaveOneOut
 from ._split import LeavePGroupsOut
 from ._split import LeavePOut
+from ._split import RepeatedKFold
+from ._split import RepeatedStratifiedKFold
 from ._split import ShuffleSplit
 from ._split import GroupShuffleSplit
 from ._split import StratifiedShuffleSplit
@@ -36,6 +38,8 @@
            'LeaveOneOut',
            'LeavePGroupsOut',
            'LeavePOut',
+           'RepeatedKFold',
+           'RepeatedStratifiedKFold',
            'ParameterGrid',
            'ParameterSampler',
            'PredefinedSplit',
diff --git a/sklearn/model_selection/_search.py b/sklearn/model_selection/_search.py
index 3b8a0ed882cf..32f0ee89465b 100644
--- a/sklearn/model_selection/_search.py
+++ b/sklearn/model_selection/_search.py
@@ -192,9 +192,13 @@ class ParameterSampler(object):
     n_iter : integer
         Number of parameter settings that are produced.
 
-    random_state : int or RandomState
+    random_state : int, RandomState instance or None, optional (default=None)
         Pseudo random number generator state used for random uniform sampling
         from lists of possible values instead of scipy.stats distributions.
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     Returns
     -------
@@ -814,8 +818,8 @@ class GridSearchCV(BaseSearchCV):
     >>> from sklearn.model_selection import GridSearchCV
     >>> iris = datasets.load_iris()
     >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
-    >>> svr = svm.SVC()
-    >>> clf = GridSearchCV(svr, parameters)
+    >>> svc = svm.SVC()
+    >>> clf = GridSearchCV(svc, parameters)
     >>> clf.fit(iris.data, iris.target)
     ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
     GridSearchCV(cv=None, error_score=...,
@@ -1054,9 +1058,13 @@ class RandomizedSearchCV(BaseSearchCV):
     verbose : integer
         Controls the verbosity: the higher, the more messages.
 
-    random_state : int or RandomState
+    random_state : int, RandomState instance or None, optional, default=None
         Pseudo random number generator state used for random uniform sampling
         from lists of possible values instead of scipy.stats distributions.
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     error_score : 'raise' (default) or numeric
         Value to assign to the score if an error occurs in estimator fitting.
diff --git a/sklearn/model_selection/_split.py b/sklearn/model_selection/_split.py
index b2ed060e3171..151bbafd6222 100644
--- a/sklearn/model_selection/_split.py
+++ b/sklearn/model_selection/_split.py
@@ -41,6 +41,8 @@
            'LeaveOneOut',
            'LeavePGroupsOut',
            'LeavePOut',
+           'RepeatedStratifiedKFold',
+           'RepeatedKFold',
            'ShuffleSplit',
            'GroupShuffleSplit',
            'StratifiedKFold',
@@ -362,9 +364,11 @@ class KFold(_BaseKFold):
     shuffle : boolean, optional
         Whether to shuffle the data before splitting into batches.
 
-    random_state : None, int or RandomState
-        When shuffle=True, pseudo-random number generator state used for
-        shuffling. If None, use default numpy RNG for shuffling.
+    random_state : int, RandomState instance or None, optional, default=None
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`. Used when ``shuffle`` == True.
 
     Examples
     --------
@@ -397,6 +401,8 @@ class KFold(_BaseKFold):
         classification tasks).
 
     GroupKFold: K-fold iterator variant with non-overlapping groups.
+
+    RepeatedKFold: Repeats K-Fold n times.
     """
 
     def __init__(self, n_splits=3, shuffle=False,
@@ -527,9 +533,11 @@ class StratifiedKFold(_BaseKFold):
         Whether to shuffle each stratification of the data before splitting
         into batches.
 
-    random_state : None, int or RandomState
-        When shuffle=True, pseudo-random number generator state used for
-        shuffling. If None, use default numpy RNG for shuffling.
+    random_state : int, RandomState instance or None, optional, default=None
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`. Used when ``shuffle`` == True.
 
     Examples
     --------
@@ -553,6 +561,9 @@ class StratifiedKFold(_BaseKFold):
     All the folds have size ``trunc(n_samples / n_splits)``, the last one has
     the complementary.
 
+    See also
+    --------
+    RepeatedStratifiedKFold: Repeats Stratified K-Fold n times.
     """
 
     def __init__(self, n_splits=3, shuffle=False, random_state=None):
@@ -913,6 +924,201 @@ def get_n_splits(self, X, y, groups):
         return int(comb(len(np.unique(groups)), self.n_groups, exact=True))
 
 
+class _RepeatedSplits(with_metaclass(ABCMeta)):
+    """Repeated splits for an arbitrary randomized CV splitter.
+
+    Repeats splits for cross-validators n times with different randomization
+    in each repetition.
+
+    Parameters
+    ----------
+    cv : callable
+        Cross-validator class.
+
+    n_repeats : int, default=10
+        Number of times cross-validator needs to be repeated.
+
+    random_state : int, RandomState instance or None, optional, default=None
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
+
+    **cvargs : additional params
+        Constructor parameters for cv. Must not contain random_state
+        and shuffle.
+    """
+    def __init__(self, cv, n_repeats=10, random_state=None, **cvargs):
+        if not isinstance(n_repeats, (np.integer, numbers.Integral)):
+            raise ValueError("Number of repetitions must be of Integral type.")
+
+        if n_repeats <= 1:
+            raise ValueError("Number of repetitions must be greater than 1.")
+
+        if any(key in cvargs for key in ('random_state', 'shuffle')):
+            raise ValueError(
+                "cvargs must not contain random_state or shuffle.")
+
+        self.cv = cv
+        self.n_repeats = n_repeats
+        self.random_state = random_state
+        self.cvargs = cvargs
+
+    def split(self, X, y=None, groups=None):
+        """Generates indices to split data into training and test set.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+            Training data, where n_samples is the number of samples
+            and n_features is the number of features.
+
+        y : array-like, of length n_samples
+            The target variable for supervised learning problems.
+
+        groups : array-like, with shape (n_samples,), optional
+            Group labels for the samples used while splitting the dataset into
+            train/test set.
+
+        Returns
+        -------
+        train : ndarray
+            The training set indices for that split.
+
+        test : ndarray
+            The testing set indices for that split.
+        """
+        n_repeats = self.n_repeats
+        rng = check_random_state(self.random_state)
+
+        for idx in range(n_repeats):
+            cv = self.cv(random_state=rng, shuffle=True,
+                         **self.cvargs)
+            for train_index, test_index in cv.split(X, y, groups):
+                yield train_index, test_index
+
+    def get_n_splits(self, X=None, y=None, groups=None):
+        """Returns the number of splitting iterations in the cross-validator
+
+        Parameters
+        ----------
+        X : object
+            Always ignored, exists for compatibility.
+            ``np.zeros(n_samples)`` may be used as a placeholder.
+
+        y : object
+            Always ignored, exists for compatibility.
+            ``np.zeros(n_samples)`` may be used as a placeholder.
+
+        groups : array-like, with shape (n_samples,), optional
+            Group labels for the samples used while splitting the dataset into
+            train/test set.
+
+        Returns
+        -------
+        n_splits : int
+            Returns the number of splitting iterations in the cross-validator.
+        """
+        rng = check_random_state(self.random_state)
+        cv = self.cv(random_state=rng, shuffle=True,
+                     **self.cvargs)
+        return cv.get_n_splits(X, y, groups) * self.n_repeats
+
+
+class RepeatedKFold(_RepeatedSplits):
+    """Repeated K-Fold cross validator.
+
+    Repeats K-Fold n times with different randomization in each repetition.
+
+    Read more in the :ref:`User Guide <cross_validation>`.
+
+    Parameters
+    ----------
+    n_splits : int, default=5
+        Number of folds. Must be at least 2.
+
+    n_repeats : int, default=10
+        Number of times cross-validator needs to be repeated.
+
+    random_state : int, RandomState instance or None, optional, default=None
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
+
+    Examples
+    --------
+    >>> from sklearn.model_selection import RepeatedKFold
+    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
+    >>> y = np.array([0, 0, 1, 1])
+    >>> rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=2652124)
+    >>> for train_index, test_index in rkf.split(X):
+    ...     print("TRAIN:", train_index, "TEST:", test_index)
+    ...     X_train, X_test = X[train_index], X[test_index]
+    ...     y_train, y_test = y[train_index], y[test_index]
+    ...
+    TRAIN: [0 1] TEST: [2 3]
+    TRAIN: [2 3] TEST: [0 1]
+    TRAIN: [1 2] TEST: [0 3]
+    TRAIN: [0 3] TEST: [1 2]
+
+
+    See also
+    --------
+    RepeatedStratifiedKFold: Repeates Stratified K-Fold n times.
+    """
+    def __init__(self, n_splits=5, n_repeats=10, random_state=None):
+        super(RepeatedKFold, self).__init__(
+            KFold, n_repeats, random_state, n_splits=n_splits)
+
+
+class RepeatedStratifiedKFold(_RepeatedSplits):
+    """Repeated Stratified K-Fold cross validator.
+
+    Repeats Stratified K-Fold n times with different randomization in each
+    repetition.
+
+    Read more in the :ref:`User Guide <cross_validation>`.
+
+    Parameters
+    ----------
+    n_splits : int, default=5
+        Number of folds. Must be at least 2.
+
+    n_repeats : int, default=10
+        Number of times cross-validator needs to be repeated.
+
+    random_state : None, int or RandomState, default=None
+        Random state to be used to generate random state for each
+        repetition.
+
+    Examples
+    --------
+    >>> from sklearn.model_selection import RepeatedStratifiedKFold
+    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
+    >>> y = np.array([0, 0, 1, 1])
+    >>> rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=2,
+    ...     random_state=36851234)
+    >>> for train_index, test_index in rskf.split(X, y):
+    ...     print("TRAIN:", train_index, "TEST:", test_index)
+    ...     X_train, X_test = X[train_index], X[test_index]
+    ...     y_train, y_test = y[train_index], y[test_index]
+    ...
+    TRAIN: [1 2] TEST: [0 3]
+    TRAIN: [0 3] TEST: [1 2]
+    TRAIN: [1 3] TEST: [0 2]
+    TRAIN: [0 2] TEST: [1 3]
+
+
+    See also
+    --------
+    RepeatedKFold: Repeats K-Fold n times.
+    """
+    def __init__(self, n_splits=5, n_repeats=10, random_state=None):
+        super(RepeatedStratifiedKFold, self).__init__(
+            StratifiedKFold, n_repeats, random_state, n_splits=n_splits)
+
+
 class BaseShuffleSplit(with_metaclass(ABCMeta)):
     """Base class for ShuffleSplit and StratifiedShuffleSplit"""
 
@@ -1009,8 +1215,11 @@ class ShuffleSplit(BaseShuffleSplit):
         int, represents the absolute number of train samples. If None,
         the value is automatically set to the complement of the test size.
 
-    random_state : int or RandomState
-        Pseudo-random number generator state used for random sampling.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     Examples
     --------
@@ -1091,8 +1300,12 @@ class GroupShuffleSplit(ShuffleSplit):
         int, represents the absolute number of train groups. If None,
         the value is automatically set to the complement of the test size.
 
-    random_state : int or RandomState
-        Pseudo-random number generator state used for random sampling.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
+
     '''
 
     def __init__(self, n_splits=5, test_size=0.2, train_size=None,
@@ -1218,8 +1431,12 @@ class StratifiedShuffleSplit(BaseShuffleSplit):
         int, represents the absolute number of train samples. If None,
         the value is automatically set to the complement of the test size.
 
-    random_state : int or RandomState
-        Pseudo-random number generator state used for random sampling.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
+
 
     Examples
     --------
@@ -1624,8 +1841,11 @@ def train_test_split(*arrays, **options):
         int, represents the absolute number of train samples. If None,
         the value is automatically set to the complement of the test size.
 
-    random_state : int or RandomState
-        Pseudo-random number generator state used for random sampling.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     stratify : array-like or None (default is None)
         If not None, data is split in a stratified fashion, using this as
diff --git a/sklearn/model_selection/_validation.py b/sklearn/model_selection/_validation.py
index ab18d9035b4d..e105f0d0b122 100644
--- a/sklearn/model_selection/_validation.py
+++ b/sklearn/model_selection/_validation.py
@@ -580,9 +580,11 @@ def permutation_test_score(estimator, X, y, groups=None, cv=None,
         The number of CPUs to use to do the computation. -1 means
         'all CPUs'.
 
-    random_state : RandomState or an int seed (0 by default)
-        A random number generator instance to define the state of the
-        random permutations generator.
+    random_state : int, RandomState instance or None, optional (default=0)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     verbose : integer, optional
         The verbosity level.
@@ -596,11 +598,14 @@ def permutation_test_score(estimator, X, y, groups=None, cv=None,
         The scores obtained for each permutations.
 
     pvalue : float
-        The returned value equals p-value if `scoring` returns bigger
-        numbers for better scores (e.g., accuracy_score). If `scoring` is
-        rather a loss function (i.e. when lower is better such as with
-        `mean_squared_error`) then this is actually the complement of the
-        p-value:  1 - p-value.
+        The p-value, which approximates the probability that the score would
+        be obtained by chance. This is calculated as:
+
+        `(C + 1) / (n_permutations + 1)`
+
+        Where C is the number of permutations whose score >= the true score.
+
+        The best possible p-value is 1/(n_permutations + 1), the worst is 1.0.
 
     Notes
     -----
@@ -740,9 +745,11 @@ def learning_curve(estimator, X, y, groups=None,
         Whether to shuffle training data before taking prefixes of it
         based on``train_sizes``.
 
-    random_state : None, int or RandomState
-        When shuffle=True, pseudo-random number generator state used for
-        shuffling. If None, use default numpy RNG for shuffling.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`. Used when ``shuffle`` == 'True'.
 
     -------
     train_sizes_abs : array, shape = (n_unique_ticks,), dtype int
diff --git a/sklearn/model_selection/tests/test_split.py b/sklearn/model_selection/tests/test_split.py
index 601e9b259c53..fcd0160ca74e 100644
--- a/sklearn/model_selection/tests/test_split.py
+++ b/sklearn/model_selection/tests/test_split.py
@@ -42,6 +42,8 @@
 from sklearn.model_selection import check_cv
 from sklearn.model_selection import train_test_split
 from sklearn.model_selection import GridSearchCV
+from sklearn.model_selection import RepeatedKFold
+from sklearn.model_selection import RepeatedStratifiedKFold
 
 from sklearn.linear_model import Ridge
 
@@ -804,6 +806,92 @@ def test_leave_one_p_group_out_error_on_fewer_number_of_groups():
                          LeavePGroupsOut(n_groups=3).split(X, y, groups))
 
 
+def test_repeated_cv_value_errors():
+    # n_repeats is not integer or <= 1
+    for cv in (RepeatedKFold, RepeatedStratifiedKFold):
+        assert_raises(ValueError, cv, n_repeats=1)
+        assert_raises(ValueError, cv, n_repeats=1.5)
+
+
+def test_repeated_kfold_determinstic_split():
+    X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]
+    random_state = 258173307
+    rkf = RepeatedKFold(
+        n_splits=2,
+        n_repeats=2,
+        random_state=random_state)
+
+    # split should produce same and deterministic splits on
+    # each call
+    for _ in range(3):
+        splits = rkf.split(X)
+        train, test = next(splits)
+        assert_array_equal(train, [2, 4])
+        assert_array_equal(test, [0, 1, 3])
+
+        train, test = next(splits)
+        assert_array_equal(train, [0, 1, 3])
+        assert_array_equal(test, [2, 4])
+
+        train, test = next(splits)
+        assert_array_equal(train, [0, 1])
+        assert_array_equal(test, [2, 3, 4])
+
+        train, test = next(splits)
+        assert_array_equal(train, [2, 3, 4])
+        assert_array_equal(test, [0, 1])
+
+        assert_raises(StopIteration, next, splits)
+
+
+def test_get_n_splits_for_repeated_kfold():
+    n_splits = 3
+    n_repeats = 4
+    rkf = RepeatedKFold(n_splits, n_repeats)
+    expected_n_splits = n_splits * n_repeats
+    assert_equal(expected_n_splits, rkf.get_n_splits())
+
+
+def test_get_n_splits_for_repeated_stratified_kfold():
+    n_splits = 3
+    n_repeats = 4
+    rskf = RepeatedStratifiedKFold(n_splits, n_repeats)
+    expected_n_splits = n_splits * n_repeats
+    assert_equal(expected_n_splits, rskf.get_n_splits())
+
+
+def test_repeated_stratified_kfold_determinstic_split():
+    X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]
+    y = [1, 1, 1, 0, 0]
+    random_state = 1944695409
+    rskf = RepeatedStratifiedKFold(
+        n_splits=2,
+        n_repeats=2,
+        random_state=random_state)
+
+    # split should produce same and deterministic splits on
+    # each call
+    for _ in range(3):
+        splits = rskf.split(X, y)
+        train, test = next(splits)
+        assert_array_equal(train, [1, 4])
+        assert_array_equal(test, [0, 2, 3])
+
+        train, test = next(splits)
+        assert_array_equal(train, [0, 2, 3])
+        assert_array_equal(test, [1, 4])
+
+        train, test = next(splits)
+        assert_array_equal(train, [2, 3])
+        assert_array_equal(test, [0, 1, 4])
+
+        train, test = next(splits)
+        assert_array_equal(train, [0, 1, 4])
+        assert_array_equal(test, [2, 3])
+
+        assert_raises(StopIteration, next, splits)
+
+
 def test_train_test_split_errors():
     assert_raises(ValueError, train_test_split)
     assert_raises(ValueError, train_test_split, range(3), train_size=1.1)
diff --git a/sklearn/multiclass.py b/sklearn/multiclass.py
index 3de5ee319c71..8f9788e6a425 100644
--- a/sklearn/multiclass.py
+++ b/sklearn/multiclass.py
@@ -640,9 +640,11 @@ class OutputCodeClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):
         one-vs-the-rest. A number greater than 1 will require more classifiers
         than one-vs-the-rest.
 
-    random_state : numpy.RandomState, optional
-        The generator used to initialize the codebook. Defaults to
-        numpy.random.
+    random_state : int, RandomState instance or None, optional, default: None
+        The generator used to initialize the codebook.  If int, random_state is
+        the seed used by the random number generator; If RandomState instance,
+        random_state is the random number generator; If None, the random number
+        generator is the RandomState instance used by `np.random`.
 
     n_jobs : int, optional, default: 1
         The number of jobs to use for the computation. If -1 all CPUs are used.
diff --git a/sklearn/neighbors/approximate.py b/sklearn/neighbors/approximate.py
index c6f602979ea1..7f7321abdfb1 100644
--- a/sklearn/neighbors/approximate.py
+++ b/sklearn/neighbors/approximate.py
@@ -407,7 +407,7 @@ def kneighbors(self, X, n_neighbors=None, return_distance=True):
             List of n_features-dimensional data points.  Each row
             corresponds to a single query.
 
-        n_neighbors : int, opitonal (default = None)
+        n_neighbors : int, optional (default = None)
             Number of neighbors required. If not provided, this will
             return the number specified at the initialization.
 
diff --git a/sklearn/neighbors/kde.py b/sklearn/neighbors/kde.py
index dfb349a8dc42..3cfdbc63042b 100644
--- a/sklearn/neighbors/kde.py
+++ b/sklearn/neighbors/kde.py
@@ -184,8 +184,11 @@ def sample(self, n_samples=1, random_state=None):
         n_samples : int, optional
             Number of samples to generate. Defaults to 1.
 
-        random_state : RandomState or an int seed (0 by default)
-            A random number generator instance.
+        random_state : int, RandomState instance or None. default to None
+            If int, random_state is the seed used by the random number
+            generator; If RandomState instance, random_state is the random
+            number generator; If None, the random number generator is the
+            RandomState instance used by `np.random`.
 
         Returns
         -------
diff --git a/sklearn/neural_network/multilayer_perceptron.py b/sklearn/neural_network/multilayer_perceptron.py
index 720a3fef21d8..1d329f8074c2 100644
--- a/sklearn/neural_network/multilayer_perceptron.py
+++ b/sklearn/neural_network/multilayer_perceptron.py
@@ -755,8 +755,11 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
         Maximum number of iterations. The solver iterates until convergence
         (determined by 'tol') or this number of iterations.
 
-    random_state : int or RandomState, optional, default None
-        State or seed for random number generator.
+    random_state : int, RandomState instance or None, optional, default None
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     shuffle : bool, optional, default True
         Whether to shuffle samples in each iteration. Only used when
@@ -1126,8 +1129,11 @@ class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):
         Maximum number of iterations. The solver iterates until convergence
         (determined by 'tol') or this number of iterations.
 
-    random_state : int or RandomState, optional, default None
-        State or seed for random number generator.
+    random_state : int, RandomState instance or None, optional, default None
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     shuffle : bool, optional, default True
         Whether to shuffle samples in each iteration. Only used when
diff --git a/sklearn/pipeline.py b/sklearn/pipeline.py
index 61d7b12b7564..9377c8e2fd7a 100644
--- a/sklearn/pipeline.py
+++ b/sklearn/pipeline.py
@@ -10,6 +10,7 @@
 # License: BSD
 
 from collections import defaultdict
+
 from abc import ABCMeta, abstractmethod
 
 import numpy as np
@@ -20,69 +21,14 @@
 from .externals import six
 from .utils import tosequence
 from .utils.metaestimators import if_delegate_has_method
+from .utils import Bunch
 
-__all__ = ['Pipeline', 'FeatureUnion']
-
+from .utils.metaestimators import _BaseComposition
 
-class _BasePipeline(six.with_metaclass(ABCMeta, BaseEstimator)):
-    """Handles parameter management for classifiers composed of named steps.
-    """
+__all__ = ['Pipeline', 'FeatureUnion']
 
-    @abstractmethod
-    def __init__(self):
-        pass
-
-    def _replace_step(self, steps_attr, name, new_val):
-        # assumes `name` is a valid step name
-        new_steps = getattr(self, steps_attr)[:]
-        for i, (step_name, _) in enumerate(new_steps):
-            if step_name == name:
-                new_steps[i] = (name, new_val)
-                break
-        setattr(self, steps_attr, new_steps)
-
-    def _get_params(self, steps_attr, deep=True):
-        out = super(_BasePipeline, self).get_params(deep=False)
-        if not deep:
-            return out
-        steps = getattr(self, steps_attr)
-        out.update(steps)
-        for name, estimator in steps:
-            if estimator is None:
-                continue
-            for key, value in six.iteritems(estimator.get_params(deep=True)):
-                out['%s__%s' % (name, key)] = value
-        return out
-
-    def _set_params(self, steps_attr, **params):
-        # Ensure strict ordering of parameter setting:
-        # 1. All steps
-        if steps_attr in params:
-            setattr(self, steps_attr, params.pop(steps_attr))
-        # 2. Step replacement
-        step_names, _ = zip(*getattr(self, steps_attr))
-        for name in list(six.iterkeys(params)):
-            if '__' not in name and name in step_names:
-                self._replace_step(steps_attr, name, params.pop(name))
-        # 3. Step parameters and other initilisation arguments
-        super(_BasePipeline, self).set_params(**params)
-        return self
 
-    def _validate_names(self, names):
-        if len(set(names)) != len(names):
-            raise ValueError('Names provided are not unique: '
-                             '{0!r}'.format(list(names)))
-        invalid_names = set(names).intersection(self.get_params(deep=False))
-        if invalid_names:
-            raise ValueError('Step names conflict with constructor arguments: '
-                             '{0!r}'.format(sorted(invalid_names)))
-        invalid_names = [name for name in names if '__' in name]
-        if invalid_names:
-            raise ValueError('Step names must not contain __: got '
-                             '{0!r}'.format(invalid_names))
-
-
-class Pipeline(_BasePipeline):
+class Pipeline(_BaseComposition):
     """Pipeline of transforms with a final estimator.
 
     Sequentially apply a list of transforms and a final estimator.
@@ -109,20 +55,18 @@ class Pipeline(_BasePipeline):
         an estimator.
 
     memory : Instance of joblib.Memory or string, optional (default=None)
-        Used to caching the fitted transformers of the transformer of the
-        pipeline. By default, no cache is performed.
-        If a string is given, it is the path to the caching directory.
-        Enabling caching triggers a clone of the transformers before fitting.
-        Therefore, the transformer instance given to the pipeline cannot be
-        inspected directly. Use the attribute ``named_steps`` or ``steps``
-        to inspect estimators within the pipeline.
-        Caching the transformers is advantageous when fitting is time
-        consuming.
-
+        Used to cache the fitted transformers of the pipeline. By default,
+        no caching is performed. If a string is given, it is the path to
+        the caching directory. Enabling caching triggers a clone of
+        the transformers before fitting. Therefore, the transformer
+        instance given to the pipeline cannot be inspected
+        directly. Use the attribute ``named_steps`` or ``steps`` to
+        inspect estimators within the pipeline. Caching the
+        transformers is advantageous when fitting is time consuming.
 
     Attributes
     ----------
-    named_steps : dict
+    named_steps : bunch object, a dictionary with attribute access
         Read-only attribute to access any step parameter by user given name.
         Keys are step names and values are steps parameters.
 
@@ -157,7 +101,12 @@ class Pipeline(_BasePipeline):
     array([False, False,  True,  True, False, False, True,  True, False,
            True,  False,  True,  True, False, True,  False, True, True,
            False, False], dtype=bool)
-
+    >>> # Another way to get selected features chosen by anova_filter
+    >>> anova_svm.named_steps.anova.get_support()
+    ... # doctest: +NORMALIZE_WHITESPACE
+    array([False, False,  True,  True, False, False, True,  True, False,
+           True,  False,  True,  True, False, True,  False, True, True,
+           False, False], dtype=bool)
     """
 
     # BaseEstimator interface
@@ -227,7 +176,8 @@ def _estimator_type(self):
 
     @property
     def named_steps(self):
-        return dict(self.steps)
+        # Use Bunch object to improve autocomplete
+        return Bunch(**dict(self.steps))
 
     @property
     def _final_estimator(self):
@@ -626,7 +576,7 @@ def _fit_transform_one(transformer, weight, X, y,
     return res * weight, transformer
 
 
-class FeatureUnion(_BasePipeline, TransformerMixin):
+class FeatureUnion(_BaseComposition, TransformerMixin):
     """Concatenates results of multiple transformer objects.
 
     This estimator applies a list of transformer objects in parallel to the
diff --git a/sklearn/preprocessing/data.py b/sklearn/preprocessing/data.py
index fbcb22e46432..5fe724a40527 100644
--- a/sklearn/preprocessing/data.py
+++ b/sklearn/preprocessing/data.py
@@ -1281,7 +1281,7 @@ def normalize(X, norm='l2', axis=1, copy=True, return_norm=False):
     else:
         raise ValueError("'%d' is not a supported axis" % axis)
 
-    X = check_array(X, sparse_format, copy=copy, warn_on_dtype=True,
+    X = check_array(X, sparse_format, copy=copy,
                     estimator='the normalize function', dtype=FLOAT_DTYPES)
     if axis == 0:
         X = X.T
diff --git a/sklearn/preprocessing/discretization.py b/sklearn/preprocessing/discretization.py
index 6058d5ae55d6..e2caa60e7307 100644
--- a/sklearn/preprocessing/discretization.py
+++ b/sklearn/preprocessing/discretization.py
@@ -8,6 +8,7 @@
 
 from sklearn.base import BaseEstimator, TransformerMixin
 from sklearn.preprocessing.data import _transform_selected
+from sklearn.utils.fixes import isclose
 from sklearn.utils.validation import (
     check_array,
     check_is_fitted,
@@ -80,13 +81,13 @@ class KBinsDiscretizer(BaseEstimator, TransformerMixin):
 
     Notes
     -----
-    Bin edges for feature `i` are defined as
+    Bin edges for feature ``i`` are defined as::
 
-    ```
-    np.concatenate([
-        -np.inf, offset_[i] + bin_width_[i] * np.arange(1, n_bins_[i]), np.inf
-    ])
-    ```
+      np.concatenate([
+        -np.inf,
+        offset_[i] + bin_width_[i] * np.arange(1, n_bins_[i]),
+        np.inf
+      ])
     """
 
     def __init__(self, n_bins=2, ignored_features=None):
@@ -114,9 +115,9 @@ def fit(self, X, y=None):
                                                n_features)
         self.transformed_features_ = np.delete(np.arange(n_features), ignored)
 
-        min = np.min(X, axis=0)
-        min[ignored] = 0
-        self.offset_ = min
+        offset = np.min(X, axis=0)
+        offset[ignored] = 0
+        self.offset_ = offset
 
         n_bins = self._check_n_bins(self.n_bins, n_features, ignored)
         n_bins[ignored] = 0
@@ -208,7 +209,7 @@ def _transform(self, X):
         # numeric instability. For these values, after normalizing into
         # [-1, n_bins] range, add 0.5 so they are binned correctly.
         with np.errstate(divide='ignore', invalid='ignore'):
-            needs_correction = np.isclose(np.mod(X, bin_width), bin_width)
+            needs_correction = isclose(np.mod(X, bin_width), bin_width)
             X /= bin_width
         X[needs_correction] += 0.5
         np.floor(X, out=X)
diff --git a/sklearn/random_projection.py b/sklearn/random_projection.py
index 0d47b2886ff0..1ec4d0d21e67 100644
--- a/sklearn/random_projection.py
+++ b/sklearn/random_projection.py
@@ -154,7 +154,7 @@ def _check_input_size(n_components, n_features):
 
 
 def gaussian_random_matrix(n_components, n_features, random_state=None):
-    """ Generate a dense Gaussian random matrix.
+    """Generate a dense Gaussian random matrix.
 
     The components of the random matrix are drawn from
 
@@ -170,9 +170,12 @@ def gaussian_random_matrix(n_components, n_features, random_state=None):
     n_features : int,
         Dimensionality of the original source space.
 
-    random_state : int, RandomState instance or None (default=None)
-        Control the pseudo random number generator used to generate the
-        matrix at fit time.
+    random_state : int, RandomState instance or None, optional (default=None)
+        Control the pseudo random number generator used to generate the matrix
+        at fit time.  If int, random_state is the seed used by the random
+        number generator; If RandomState instance, random_state is the random
+        number generator; If None, the random number generator is the
+        RandomState instance used by `np.random`.
 
     Returns
     -------
@@ -226,9 +229,12 @@ def sparse_random_matrix(n_components, n_features, density='auto',
         Use density = 1 / 3.0 if you want to reproduce the results from
         Achlioptas, 2001.
 
-    random_state : integer, RandomState instance or None (default=None)
-        Control the pseudo random number generator used to generate the
-        matrix at fit time.
+    random_state : int, RandomState instance or None, optional (default=None)
+        Control the pseudo random number generator used to generate the matrix
+        at fit time.  If int, random_state is the seed used by the random
+        number generator; If RandomState instance, random_state is the random
+        number generator; If None, the random number generator is the
+        RandomState instance used by `np.random`.
 
     Returns
     -------
@@ -446,9 +452,12 @@ class GaussianRandomProjection(BaseRandomProjection):
         Smaller values lead to better embedding and higher number of
         dimensions (n_components) in the target projection space.
 
-    random_state : integer, RandomState instance or None (default=None)
-        Control the pseudo random number generator used to generate the
-        matrix at fit time.
+    random_state : int, RandomState instance or None, optional (default=None)
+        Control the pseudo random number generator used to generate the matrix
+        at fit time.  If int, random_state is the seed used by the random
+        number generator; If RandomState instance, random_state is the random
+        number generator; If None, the random number generator is the
+        RandomState instance used by `np.random`.
 
     Attributes
     ----------
@@ -552,9 +561,12 @@ class SparseRandomProjection(BaseRandomProjection):
         If False, the projected data uses a sparse representation if
         the input is sparse.
 
-    random_state : integer, RandomState instance or None (default=None)
-        Control the pseudo random number generator used to generate the
-        matrix at fit time.
+    random_state : int, RandomState instance or None, optional (default=None)
+        Control the pseudo random number generator used to generate the matrix
+        at fit time.  If int, random_state is the seed used by the random
+        number generator; If RandomState instance, random_state is the random
+        number generator; If None, the random number generator is the
+        RandomState instance used by `np.random`.
 
     Attributes
     ----------
diff --git a/sklearn/svm/base.py b/sklearn/svm/base.py
index cff4c35a58b4..208a69f3720c 100644
--- a/sklearn/svm/base.py
+++ b/sklearn/svm/base.py
@@ -804,9 +804,13 @@ def _fit_liblinear(X, y, C, fit_intercept, intercept_scaling, class_weight,
     tol : float
         Stopping condition.
 
-    random_state : int seed, RandomState instance, or None (default)
-        The seed of the pseudo random number generator to use when
-        shuffling the data.
+    random_state : int, RandomState instance or None, optional (default=None)
+        The seed of the pseudo random number generator to use when shuffling
+        the data.  If int, random_state is the seed used by the random number
+        generator; If RandomState instance, random_state is the random number
+        generator; If None, the random number generator is the RandomState
+        instance used by `np.random`.
+
 
     multi_class : str, {'ovr', 'crammer_singer'}
         `ovr` trains n_classes one-vs-rest classifiers, while `crammer_singer`
diff --git a/sklearn/svm/classes.py b/sklearn/svm/classes.py
index 7e920011d002..2420f8d93cd5 100644
--- a/sklearn/svm/classes.py
+++ b/sklearn/svm/classes.py
@@ -86,9 +86,12 @@ class LinearSVC(BaseEstimator, LinearClassifierMixin,
         per-process runtime setting in liblinear that, if enabled, may not work
         properly in a multithreaded context.
 
-    random_state : int seed, RandomState instance, or None (default=None)
-        The seed of the pseudo random number generator to use when
-        shuffling the data.
+    random_state : int, RandomState instance or None, optional (default=None)
+        The seed of the pseudo random number generator to use when shuffling
+        the data.  If int, random_state is the seed used by the random number
+        generator; If RandomState instance, random_state is the random number
+        generator; If None, the random number generator is the RandomState
+        instance used by `np.random`.
 
     max_iter : int, (default=1000)
         The maximum number of iterations to be run.
@@ -277,9 +280,12 @@ class LinearSVR(LinearModel, RegressorMixin):
         per-process runtime setting in liblinear that, if enabled, may not work
         properly in a multithreaded context.
 
-    random_state : int seed, RandomState instance, or None (default=None)
-        The seed of the pseudo random number generator to use when
-        shuffling the data.
+    random_state : int, RandomState instance or None, optional (default=None)
+        The seed of the pseudo random number generator to use when shuffling
+        the data.  If int, random_state is the seed used by the random number
+        generator; If RandomState instance, random_state is the random number
+        generator; If None, the random number generator is the RandomState
+        instance used by `np.random`.
 
     max_iter : int, (default=1000)
         The maximum number of iterations to be run.
@@ -468,9 +474,12 @@ class SVC(BaseSVC):
         .. versionchanged:: 0.17
            Deprecated *decision_function_shape='ovo' and None*.
 
-    random_state : int seed, RandomState instance, or None (default)
-        The seed of the pseudo random number generator to use when
-        shuffling the data for probability estimation.
+    random_state : int, RandomState instance or None, optional (default=None)
+        The seed of the pseudo random number generator to use when shuffling
+        the data.  If int, random_state is the seed used by the random number
+        generator; If RandomState instance, random_state is the random number
+        generator; If None, the random number generator is the RandomState
+        instance used by `np.random`.
 
     Attributes
     ----------
@@ -621,9 +630,12 @@ class NuSVC(BaseSVC):
         .. versionchanged:: 0.17
            Deprecated *decision_function_shape='ovo' and None*.
 
-    random_state : int seed, RandomState instance, or None (default)
-        The seed of the pseudo random number generator to use when
-        shuffling the data for probability estimation.
+    random_state : int, RandomState instance or None, optional (default=None)
+        The seed of the pseudo random number generator to use when shuffling
+        the data.  If int, random_state is the seed used by the random number
+        generator; If RandomState instance, random_state is the random number
+        generator; If None, the random number generator is the RandomState
+        instance used by `np.random`.
 
     Attributes
     ----------
@@ -972,9 +984,12 @@ class OneClassSVM(BaseLibSVM):
     max_iter : int, optional (default=-1)
         Hard limit on iterations within solver, or -1 for no limit.
 
-    random_state : int seed, RandomState instance, or None (default)
-        The seed of the pseudo random number generator to use when
-        shuffling the data for probability estimation.
+    random_state : int, RandomState instance or None, optional (default=None)
+        The seed of the pseudo random number generator to use when shuffling
+        the data.  If int, random_state is the seed used by the random number
+        generator; If RandomState instance, random_state is the random number
+        generator; If None, the random number generator is the RandomState
+        instance used by `np.random`.
 
     Attributes
     ----------
@@ -1049,3 +1064,23 @@ def decision_function(self, X):
         """
         dec = self._decision_function(X)
         return dec
+
+    def predict(self, X):
+        """
+        Perform classification on samples in X.
+
+        For an one-class model, +1 or -1 is returned.
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
+            For kernel="precomputed", the expected shape of X is
+            [n_samples_test, n_samples_train]
+
+        Returns
+        -------
+        y_pred : array, shape (n_samples,)
+            Class labels for samples in X.
+        """
+        y = super(OneClassSVM, self).predict(X)
+        return np.asarray(y, dtype=np.intp)
diff --git a/sklearn/svm/libsvm.pyx b/sklearn/svm/libsvm.pyx
index 8607e74a7e92..b564380d4743 100644
--- a/sklearn/svm/libsvm.pyx
+++ b/sklearn/svm/libsvm.pyx
@@ -88,8 +88,8 @@ def fit(
         set to polynomial), 3 by default.
 
     gamma : float64, optional
-        Gamma parameter in RBF kernel (only relevant if kernel is set
-        to RBF). 0.1 by default.
+        Gamma parameter in rbf, poly and sigmoid kernels. Ignored by other
+        kernels. 0.1 by default.
 
     coef0 : float64, optional
         Independent parameter in poly/sigmoid kernel. 0 by default.
@@ -295,7 +295,8 @@ def predict(np.ndarray[np.float64_t, ndim=2, mode='c'] X,
     degree : int
         Degree of the polynomial kernel.
     gamma : float
-        Gamma parameter in RBF kernel.
+        Gamma parameter in rbf, poly and sigmoid kernels. Ignored by other
+        kernels. 0.1 by default.
     coef0 : float
         Independent parameter in poly/sigmoid kernel.
 
@@ -494,8 +495,8 @@ def cross_validation(
         set to polynomial)
 
     gamma : float
-        Gamma parameter in RBF kernel (only relevant if kernel is set
-        to RBF)
+        Gamma parameter in rbf, poly and sigmoid kernels. Ignored by other
+        kernels. 0.1 by default.
 
     coef0 : float
         Independent parameter in poly/sigmoid kernel.
diff --git a/sklearn/svm/tests/test_svm.py b/sklearn/svm/tests/test_svm.py
index 0f85be117a9a..daf35f82a39e 100644
--- a/sklearn/svm/tests/test_svm.py
+++ b/sklearn/svm/tests/test_svm.py
@@ -241,7 +241,8 @@ def test_oneclass():
     clf.fit(X)
     pred = clf.predict(T)
 
-    assert_array_almost_equal(pred, [-1, -1, -1])
+    assert_array_equal(pred, [-1, -1, -1])
+    assert_equal(pred.dtype, np.dtype('intp'))
     assert_array_almost_equal(clf.intercept_, [-1.008], decimal=3)
     assert_array_almost_equal(clf.dual_coef_,
                               [[0.632, 0.233, 0.633, 0.234, 0.632, 0.633]],
diff --git a/sklearn/tests/test_base.py b/sklearn/tests/test_base.py
index 740e83105c99..8112e7fd8196 100644
--- a/sklearn/tests/test_base.py
+++ b/sklearn/tests/test_base.py
@@ -14,6 +14,7 @@
 from sklearn.utils.testing import assert_no_warnings
 from sklearn.utils.testing import assert_warns_message
 from sklearn.utils.testing import assert_dict_equal
+from sklearn.utils.testing import ignore_warnings
 
 from sklearn.base import BaseEstimator, clone, is_classifier
 from sklearn.svm import SVC
@@ -440,6 +441,7 @@ def __getstate__(self):
         return data
 
 
+@ignore_warnings(category=(UserWarning))
 def test_pickling_works_when_getstate_is_overwritten_in_the_child_class():
     estimator = SingleInheritanceEstimator()
     estimator._attribute_not_pickled = "this attribute should not be pickled"
diff --git a/sklearn/tests/test_isotonic.py b/sklearn/tests/test_isotonic.py
index 7abadaf562d9..d5d0715a0fb7 100644
--- a/sklearn/tests/test_isotonic.py
+++ b/sklearn/tests/test_isotonic.py
@@ -27,6 +27,14 @@ def test_permutation_invariance():
     assert_array_equal(y_transformed, y_transformed_s)
 
 
+def test_check_increasing_small_number_of_samples():
+    x = [0, 1, 2]
+    y = [1, 1.1, 1.05]
+
+    is_increasing = assert_no_warnings(check_increasing, x, y)
+    assert_true(is_increasing)
+
+
 def test_check_increasing_up():
     x = [0, 1, 2, 3, 4, 5]
     y = [0, 1.5, 2.77, 8.99, 8.99, 50]
diff --git a/sklearn/tests/test_pipeline.py b/sklearn/tests/test_pipeline.py
index 33e3128931af..a7c8e4593420 100644
--- a/sklearn/tests/test_pipeline.py
+++ b/sklearn/tests/test_pipeline.py
@@ -509,6 +509,23 @@ def test_set_pipeline_steps():
     assert_raises(TypeError, pipeline.fit_transform, [[1]], [1])
 
 
+def test_pipeline_named_steps():
+    transf = Transf()
+    mult2 = Mult(mult=2)
+    pipeline = Pipeline([('mock', transf), ("mult", mult2)])
+
+    # Test access via named_steps bunch object
+    assert_true('mock' in pipeline.named_steps)
+    assert_true('mock2' not in pipeline.named_steps)
+    assert_true(pipeline.named_steps.mock is transf)
+    assert_true(pipeline.named_steps.mult is mult2)
+
+    # Test bunch with conflict attribute of dict
+    pipeline = Pipeline([('values', transf), ("mult", mult2)])
+    assert_true(pipeline.named_steps.values is not transf)
+    assert_true(pipeline.named_steps.mult is mult2)
+
+
 def test_set_pipeline_step_none():
     # Test setting Pipeline steps to None
     X = np.array([[1]])
@@ -794,9 +811,9 @@ def test_step_name_validation():
         # we validate in construction (despite scikit-learn convention)
         bad_steps3 = [('a', Mult(2)), (param, Mult(3))]
         for bad_steps, message in [
-            (bad_steps1, "Step names must not contain __: got ['a__q']"),
+            (bad_steps1, "Estimator names must not contain __: got ['a__q']"),
             (bad_steps2, "Names provided are not unique: ['a', 'a']"),
-            (bad_steps3, "Step names conflict with constructor "
+            (bad_steps3, "Estimator names conflict with constructor "
                          "arguments: ['%s']" % param),
         ]:
             # three ways to make invalid:
diff --git a/sklearn/tree/_tree.pxd b/sklearn/tree/_tree.pxd
index 4f9f35972564..14b03103deff 100644
--- a/sklearn/tree/_tree.pxd
+++ b/sklearn/tree/_tree.pxd
@@ -90,13 +90,14 @@ cdef class TreeBuilder:
     # This class controls the various stopping criteria and the node splitting
     # evaluation order, e.g. depth-first or best-first.
 
-    cdef Splitter splitter          # Splitting algorithm
-
-    cdef SIZE_t min_samples_split   # Minimum number of samples in an internal node
-    cdef SIZE_t min_samples_leaf    # Minimum number of samples in a leaf
-    cdef double min_weight_leaf     # Minimum weight in a leaf
-    cdef SIZE_t max_depth           # Maximal tree depth
-    cdef double min_impurity_split  # Impurity threshold for early stopping
+    cdef Splitter splitter              # Splitting algorithm
+
+    cdef SIZE_t min_samples_split       # Minimum number of samples in an internal node
+    cdef SIZE_t min_samples_leaf        # Minimum number of samples in a leaf
+    cdef double min_weight_leaf         # Minimum weight in a leaf
+    cdef SIZE_t max_depth               # Maximal tree depth
+    cdef double min_impurity_split
+    cdef double min_impurity_decrease   # Impurity threshold for early stopping
 
     cpdef build(self, Tree tree, object X, np.ndarray y,
                 np.ndarray sample_weight=*,
diff --git a/sklearn/tree/_tree.pyx b/sklearn/tree/_tree.pyx
index 69ab8572d2ae..33aece77c91c 100644
--- a/sklearn/tree/_tree.pyx
+++ b/sklearn/tree/_tree.pyx
@@ -19,6 +19,7 @@
 from cpython cimport Py_INCREF, PyObject
 
 from libc.stdlib cimport free
+from libc.math cimport fabs
 from libc.string cimport memcpy
 from libc.string cimport memset
 
@@ -51,6 +52,7 @@ from numpy import float32 as DTYPE
 from numpy import float64 as DOUBLE
 
 cdef double INFINITY = np.inf
+cdef double EPSILON = np.finfo('double').eps
 
 # Some handy constants (BestFirstTreeBuilder)
 cdef int IS_FIRST = 1
@@ -130,12 +132,14 @@ cdef class DepthFirstTreeBuilder(TreeBuilder):
 
     def __cinit__(self, Splitter splitter, SIZE_t min_samples_split,
                   SIZE_t min_samples_leaf, double min_weight_leaf,
-                  SIZE_t max_depth, double min_impurity_split):
+                  SIZE_t max_depth, double min_impurity_decrease,
+                  double min_impurity_split):
         self.splitter = splitter
         self.min_samples_split = min_samples_split
         self.min_samples_leaf = min_samples_leaf
         self.min_weight_leaf = min_weight_leaf
         self.max_depth = max_depth
+        self.min_impurity_decrease = min_impurity_decrease
         self.min_impurity_split = min_impurity_split
 
     cpdef build(self, Tree tree, object X, np.ndarray y,
@@ -166,6 +170,7 @@ cdef class DepthFirstTreeBuilder(TreeBuilder):
         cdef SIZE_t min_samples_leaf = self.min_samples_leaf
         cdef double min_weight_leaf = self.min_weight_leaf
         cdef SIZE_t min_samples_split = self.min_samples_split
+        cdef double min_impurity_decrease = self.min_impurity_decrease
         cdef double min_impurity_split = self.min_impurity_split
 
         # Recursive partition (without actual recursion)
@@ -229,7 +234,12 @@ cdef class DepthFirstTreeBuilder(TreeBuilder):
 
                 if not is_leaf:
                     splitter.node_split(impurity, &split, &n_constant_features)
-                    is_leaf = is_leaf or (split.pos >= end)
+                    # If EPSILON=0 in the below comparison, float precision
+                    # issues stop splitting, producing trees that are
+                    # dissimilar to v0.18
+                    is_leaf = (is_leaf or split.pos >= end or
+                               (split.improvement + EPSILON <
+                                min_impurity_decrease))
 
                 node_id = tree._add_node(parent, is_left, is_leaf, split.feature,
                                          split.threshold, impurity, n_node_samples,
@@ -293,13 +303,14 @@ cdef class BestFirstTreeBuilder(TreeBuilder):
     def __cinit__(self, Splitter splitter, SIZE_t min_samples_split,
                   SIZE_t min_samples_leaf,  min_weight_leaf,
                   SIZE_t max_depth, SIZE_t max_leaf_nodes,
-                  double min_impurity_split):
+                  double min_impurity_decrease, double min_impurity_split):
         self.splitter = splitter
         self.min_samples_split = min_samples_split
         self.min_samples_leaf = min_samples_leaf
         self.min_weight_leaf = min_weight_leaf
         self.max_depth = max_depth
         self.max_leaf_nodes = max_leaf_nodes
+        self.min_impurity_decrease = min_impurity_decrease
         self.min_impurity_split = min_impurity_split
 
     cpdef build(self, Tree tree, object X, np.ndarray y,
@@ -426,6 +437,7 @@ cdef class BestFirstTreeBuilder(TreeBuilder):
         cdef SIZE_t n_node_samples
         cdef SIZE_t n_constant_features = 0
         cdef double weighted_n_samples = splitter.weighted_n_samples
+        cdef double min_impurity_decrease = self.min_impurity_decrease
         cdef double min_impurity_split = self.min_impurity_split
         cdef double weighted_n_node_samples
         cdef bint is_leaf
@@ -446,7 +458,10 @@ cdef class BestFirstTreeBuilder(TreeBuilder):
 
         if not is_leaf:
             splitter.node_split(impurity, &split, &n_constant_features)
-            is_leaf = is_leaf or (split.pos >= end)
+            # If EPSILON=0 in the below comparison, float precision issues stop
+            # splitting early, producing trees that are dissimilar to v0.18
+            is_leaf = (is_leaf or split.pos >= end or
+                       split.improvement + EPSILON < min_impurity_decrease)
 
         node_id = tree._add_node(parent - tree.nodes
                                  if parent != NULL
diff --git a/sklearn/tree/_utils.pyx b/sklearn/tree/_utils.pyx
index faf2e5b77744..80f3000c74dd 100644
--- a/sklearn/tree/_utils.pyx
+++ b/sklearn/tree/_utils.pyx
@@ -71,7 +71,7 @@ cdef inline np.ndarray sizet_ptr_to_ndarray(SIZE_t* data, SIZE_t size):
 
 cdef inline SIZE_t rand_int(SIZE_t low, SIZE_t high,
                             UINT32_t* random_state) nogil:
-    """Generate a random integer in [0; end)."""
+    """Generate a random integer in [low; end)."""
     return low + our_rand_r(random_state) % (high - low)
 
 
diff --git a/sklearn/tree/export.py b/sklearn/tree/export.py
index 43e8aa11b961..f526c771af04 100644
--- a/sklearn/tree/export.py
+++ b/sklearn/tree/export.py
@@ -8,12 +8,16 @@
 #          Noel Dawe <noel@dawe.me>
 #          Satrajit Gosh <satrajit.ghosh@gmail.com>
 #          Trevor Stephens <trev.stephens@gmail.com>
+#          Li Li <aiki.nogard@gmail.com>
 # License: BSD 3 clause
 
+from numbers import Integral
+
 import numpy as np
 import warnings
 
 from ..externals import six
+from ..utils.validation import check_is_fitted
 
 from . import _criterion
 from . import _tree
@@ -71,7 +75,7 @@ def export_graphviz(decision_tree, out_file=SENTINEL, max_depth=None,
                     feature_names=None, class_names=None, label='all',
                     filled=False, leaves_parallel=False, impurity=True,
                     node_ids=False, proportion=False, rotate=False,
-                    rounded=False, special_characters=False):
+                    rounded=False, special_characters=False, precision=3):
     """Export a decision tree in DOT format.
 
     This function generates a GraphViz representation of the decision tree,
@@ -141,6 +145,10 @@ def export_graphviz(decision_tree, out_file=SENTINEL, max_depth=None,
         When set to ``False``, ignore special characters for PostScript
         compatibility.
 
+    precision : int, optional (default=3)
+        Number of digits of precision for floating point in the values of
+        impurity, threshold and value attributes of each node.
+
     Returns
     -------
     dot_data : string
@@ -160,6 +168,7 @@ def export_graphviz(decision_tree, out_file=SENTINEL, max_depth=None,
     >>> clf = clf.fit(iris.data, iris.target)
     >>> tree.export_graphviz(clf,
     ...     out_file='tree.dot')                # doctest: +SKIP
+
     """
 
     def get_color(value):
@@ -171,7 +180,8 @@ def get_color(value):
             if len(sorted_values) == 1:
                 alpha = 0
             else:
-                alpha = int(np.round(255 * (sorted_values[0] - sorted_values[1]) /
+                alpha = int(np.round(255 * (sorted_values[0] -
+                                            sorted_values[1]) /
                                            (1 - sorted_values[1]), 0))
         else:
             # Regression tree or multi-output
@@ -223,7 +233,8 @@ def node_to_str(tree, node_id, criterion):
                                        characters[2])
             node_string += '%s %s %s%s' % (feature,
                                            characters[3],
-                                           round(tree.threshold[node_id], 4),
+                                           round(tree.threshold[node_id],
+                                                 precision),
                                            characters[4])
 
         # Write impurity
@@ -234,7 +245,7 @@ def node_to_str(tree, node_id, criterion):
                 criterion = "impurity"
             if labels:
                 node_string += '%s = ' % criterion
-            node_string += (str(round(tree.impurity[node_id], 4)) +
+            node_string += (str(round(tree.impurity[node_id], precision)) +
                             characters[4])
 
         # Write node sample count
@@ -257,16 +268,16 @@ def node_to_str(tree, node_id, criterion):
             node_string += 'value = '
         if tree.n_classes[0] == 1:
             # Regression
-            value_text = np.around(value, 4)
+            value_text = np.around(value, precision)
         elif proportion:
             # Classification
-            value_text = np.around(value, 2)
+            value_text = np.around(value, precision)
         elif np.all(np.equal(np.mod(value, 1), 0)):
             # Classification without floating-point weights
             value_text = value.astype(int)
         else:
             # Classification with floating-point weights
-            value_text = np.around(value, 4)
+            value_text = np.around(value, precision)
         # Strip whitespace
         value_text = str(value_text.astype('S32')).replace("b'", "'")
         value_text = value_text.replace("' '", ", ").replace("'", "")
@@ -329,7 +340,8 @@ def recurse(tree, node_id, criterion, parent=None, depth=0):
                         # Find max and min impurities for multi-output
                         colors['bounds'] = (np.min(-tree.impurity),
                                             np.max(-tree.impurity))
-                    elif tree.n_classes[0] == 1 and len(np.unique(tree.value)) != 1:
+                    elif (tree.n_classes[0] == 1 and
+                          len(np.unique(tree.value)) != 1):
                         # Find max and min values in leaf nodes for regression
                         colors['bounds'] = (np.min(tree.value),
                                             np.max(tree.value))
@@ -377,6 +389,7 @@ def recurse(tree, node_id, criterion, parent=None, depth=0):
                 # Add edge to parent
                 out_file.write('%d -> %d ;\n' % (parent, node_id))
 
+    check_is_fitted(decision_tree, 'tree_')
     own_file = False
     return_string = False
     try:
@@ -397,6 +410,24 @@ def recurse(tree, node_id, criterion, parent=None, depth=0):
             return_string = True
             out_file = six.StringIO()
 
+        if isinstance(precision, Integral):
+            if precision < 0:
+                raise ValueError("'precision' should be greater or equal to 0."
+                                 " Got {} instead.".format(precision))
+        else:
+            raise ValueError("'precision' should be an integer. Got {}"
+                             " instead.".format(type(precision)))
+
+        # Check length of feature_names before getting into the tree node
+        # Raise error if length of feature_names does not match
+        # n_features_ in the decision_tree
+        if feature_names is not None:
+            if len(feature_names) != decision_tree.n_features_:
+                raise ValueError("Length of feature_names, %d "
+                                 "does not match number of features, %d"
+                                 % (len(feature_names),
+                                    decision_tree.n_features_))
+
         # The depth of each node for plotting with 'leaf' option
         ranks = {'leaves': []}
         # The colors to render each node with
diff --git a/sklearn/tree/tests/test_export.py b/sklearn/tree/tests/test_export.py
index 1379a7703f31..0bf70073d34c 100644
--- a/sklearn/tree/tests/test_export.py
+++ b/sklearn/tree/tests/test_export.py
@@ -2,13 +2,19 @@
 Testing for export functions of decision trees (sklearn.tree.export).
 """
 
-from re import finditer
+from re import finditer, search
 
+from numpy.random import RandomState
+
+from sklearn.base import ClassifierMixin
 from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
 from sklearn.ensemble import GradientBoostingClassifier
 from sklearn.tree import export_graphviz
 from sklearn.externals.six import StringIO
-from sklearn.utils.testing import assert_in, assert_equal, assert_raises
+from sklearn.utils.testing import (assert_in, assert_equal, assert_raises,
+                                   assert_less_equal, assert_raises_regex,
+                                   assert_raise_message)
+from sklearn.exceptions import NotFittedError
 
 # toy sample
 X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]
@@ -210,16 +216,36 @@ def test_graphviz_toy():
 def test_graphviz_errors():
     # Check for errors of export_graphviz
     clf = DecisionTreeClassifier(max_depth=3, min_samples_split=2)
-    clf.fit(X, y)
 
-    # Check feature_names error
+    # Check not-fitted decision tree error
     out = StringIO()
-    assert_raises(IndexError, export_graphviz, clf, out, feature_names=[])
+    assert_raises(NotFittedError, export_graphviz, clf, out)
+
+    clf.fit(X, y)
+
+    # Check if it errors when length of feature_names
+    # mismatches with number of features
+    message = ("Length of feature_names, "
+               "1 does not match number of features, 2")
+    assert_raise_message(ValueError, message, export_graphviz, clf, None,
+                         feature_names=["a"])
+
+    message = ("Length of feature_names, "
+               "3 does not match number of features, 2")
+    assert_raise_message(ValueError, message, export_graphviz, clf, None,
+                         feature_names=["a", "b", "c"])
 
     # Check class_names error
     out = StringIO()
     assert_raises(IndexError, export_graphviz, clf, out, class_names=[])
 
+    # Check precision error
+    out = StringIO()
+    assert_raises_regex(ValueError, "should be greater or equal",
+                        export_graphviz, clf, out, precision=-1)
+    assert_raises_regex(ValueError, "should be an integer",
+                        export_graphviz, clf, out, precision="1")
+
 
 def test_friedman_mse_in_graphviz():
     clf = DecisionTreeRegressor(criterion="friedman_mse", random_state=0)
@@ -234,3 +260,48 @@ def test_friedman_mse_in_graphviz():
 
     for finding in finditer("\[.*?samples.*?\]", dot_data.getvalue()):
         assert_in("friedman_mse", finding.group())
+
+
+def test_precision():
+
+    rng_reg = RandomState(2)
+    rng_clf = RandomState(8)
+    for X, y, clf in zip(
+            (rng_reg.random_sample((5, 2)),
+             rng_clf.random_sample((1000, 4))),
+            (rng_reg.random_sample((5, )),
+             rng_clf.randint(2, size=(1000, ))),
+            (DecisionTreeRegressor(criterion="friedman_mse", random_state=0,
+                                   max_depth=1),
+             DecisionTreeClassifier(max_depth=1, random_state=0))):
+
+        clf.fit(X, y)
+        for precision in (4, 3):
+            dot_data = export_graphviz(clf, out_file=None, precision=precision,
+                                       proportion=True)
+
+            # With the current random state, the impurity and the threshold
+            # will have the number of precision set in the export_graphviz
+            # function. We will check the number of precision with a strict
+            # equality. The value reported will have only 2 precision and
+            # therefore, only a less equal comparison will be done.
+
+            # check value
+            for finding in finditer("value = \d+\.\d+", dot_data):
+                assert_less_equal(
+                    len(search("\.\d+", finding.group()).group()),
+                    precision + 1)
+            # check impurity
+            if isinstance(clf, ClassifierMixin):
+                pattern = "gini = \d+\.\d+"
+            else:
+                pattern = "friedman_mse = \d+\.\d+"
+
+            # check impurity
+            for finding in finditer(pattern, dot_data):
+                assert_equal(len(search("\.\d+", finding.group()).group()),
+                             precision + 1)
+            # check threshold
+            for finding in finditer("<= \d+\.\d+", dot_data):
+                assert_equal(len(search("\.\d+", finding.group()).group()),
+                             precision + 1)
diff --git a/sklearn/tree/tests/test_tree.py b/sklearn/tree/tests/test_tree.py
index ff662e9af414..31464072d264 100644
--- a/sklearn/tree/tests/test_tree.py
+++ b/sklearn/tree/tests/test_tree.py
@@ -28,6 +28,8 @@
 from sklearn.utils.testing import assert_less
 from sklearn.utils.testing import assert_less_equal
 from sklearn.utils.testing import assert_true
+from sklearn.utils.testing import assert_warns
+from sklearn.utils.testing import assert_warns_message
 from sklearn.utils.testing import raises
 from sklearn.utils.testing import ignore_warnings
 
@@ -334,7 +336,7 @@ def test_pure_set():
     for name, TreeRegressor in REG_TREES.items():
         reg = TreeRegressor(random_state=0)
         reg.fit(X, y)
-        assert_almost_equal(clf.predict(X), y,
+        assert_almost_equal(reg.predict(X), y,
                             err_msg="Failed with {0}".format(name))
 
 
@@ -525,6 +527,8 @@ def test_error():
         assert_raises(ValueError, TreeEstimator(max_features=42).fit, X, y)
         assert_raises(ValueError, TreeEstimator(min_impurity_split=-1.0).fit,
                       X, y)
+        assert_raises(ValueError,
+                      TreeEstimator(min_impurity_decrease=-1.0).fit, X, y)
 
         # Wrong dimensions
         est = TreeEstimator()
@@ -798,27 +802,33 @@ def test_min_impurity_split():
         # impurity 1e-7
         est = TreeEstimator(max_leaf_nodes=max_leaf_nodes,
                             random_state=0)
-        assert_less_equal(est.min_impurity_split, 1e-7,
-                     "Failed, min_impurity_split = {0} > 1e-7".format(
-                         est.min_impurity_split))
-        est.fit(X, y)
+        assert_true(est.min_impurity_split is None,
+                    "Failed, min_impurity_split = {0} > 1e-7".format(
+                        est.min_impurity_split))
+        try:
+            assert_warns(DeprecationWarning, est.fit, X, y)
+        except AssertionError:
+            pass
         for node in range(est.tree_.node_count):
             if (est.tree_.children_left[node] == TREE_LEAF or
-                est.tree_.children_right[node] == TREE_LEAF):
+                    est.tree_.children_right[node] == TREE_LEAF):
                 assert_equal(est.tree_.impurity[node], 0.,
                              "Failed with {0} "
                              "min_impurity_split={1}".format(
                                  est.tree_.impurity[node],
                                  est.min_impurity_split))
 
-        # verify leaf nodes have impurity [0,min_impurity_split] when using min_impurity_split
+        # verify leaf nodes have impurity [0,min_impurity_split] when using
+        # min_impurity_split
         est = TreeEstimator(max_leaf_nodes=max_leaf_nodes,
                             min_impurity_split=min_impurity_split,
                             random_state=0)
-        est.fit(X, y)
+        assert_warns_message(DeprecationWarning,
+                             "Use the min_impurity_decrease",
+                             est.fit, X, y)
         for node in range(est.tree_.node_count):
             if (est.tree_.children_left[node] == TREE_LEAF or
-                est.tree_.children_right[node] == TREE_LEAF):
+                    est.tree_.children_right[node] == TREE_LEAF):
                 assert_greater_equal(est.tree_.impurity[node], 0,
                                      "Failed with {0}, "
                                      "min_impurity_split={1}".format(
@@ -831,7 +841,66 @@ def test_min_impurity_split():
                                       est.min_impurity_split))
 
 
-def test_pickle():
+def test_min_impurity_decrease():
+    # test if min_impurity_decrease ensure that a split is made only if
+    # if the impurity decrease is atleast that value
+    X, y = datasets.make_classification(n_samples=10000, random_state=42)
+
+    # test both DepthFirstTreeBuilder and BestFirstTreeBuilder
+    # by setting max_leaf_nodes
+    for max_leaf_nodes, name in product((None, 1000), ALL_TREES.keys()):
+        TreeEstimator = ALL_TREES[name]
+
+        # Check default value of min_impurity_decrease, 1e-7
+        est1 = TreeEstimator(max_leaf_nodes=max_leaf_nodes, random_state=0)
+        # Check with explicit value of 0.05
+        est2 = TreeEstimator(max_leaf_nodes=max_leaf_nodes,
+                             min_impurity_decrease=0.05, random_state=0)
+        # Check with a much lower value of 0.0001
+        est3 = TreeEstimator(max_leaf_nodes=max_leaf_nodes,
+                             min_impurity_decrease=0.0001, random_state=0)
+        # Check with a much lower value of 0.1
+        est4 = TreeEstimator(max_leaf_nodes=max_leaf_nodes,
+                             min_impurity_decrease=0.1, random_state=0)
+
+        for est, expected_decrease in ((est1, 1e-7), (est2, 0.05),
+                                       (est3, 0.0001), (est4, 0.1)):
+            assert_less_equal(est.min_impurity_decrease, expected_decrease,
+                              "Failed, min_impurity_decrease = {0} > {1}"
+                              .format(est.min_impurity_decrease,
+                                      expected_decrease))
+            est.fit(X, y)
+            for node in range(est.tree_.node_count):
+                # If current node is a not leaf node, check if the split was
+                # justified w.r.t the min_impurity_decrease
+                if est.tree_.children_left[node] != TREE_LEAF:
+                    imp_parent = est.tree_.impurity[node]
+                    wtd_n_node = est.tree_.weighted_n_node_samples[node]
+
+                    left = est.tree_.children_left[node]
+                    wtd_n_left = est.tree_.weighted_n_node_samples[left]
+                    imp_left = est.tree_.impurity[left]
+                    wtd_imp_left = wtd_n_left * imp_left
+
+                    right = est.tree_.children_right[node]
+                    wtd_n_right = est.tree_.weighted_n_node_samples[right]
+                    imp_right = est.tree_.impurity[right]
+                    wtd_imp_right = wtd_n_right * imp_right
+
+                    wtd_avg_left_right_imp = wtd_imp_right + wtd_imp_left
+                    wtd_avg_left_right_imp /= wtd_n_node
+
+                    fractional_node_weight = (
+                        est.tree_.weighted_n_node_samples[node] / X.shape[0])
+
+                    actual_decrease = fractional_node_weight * (
+                        imp_parent - wtd_avg_left_right_imp)
+
+                    assert_greater_equal(actual_decrease, expected_decrease,
+                                         "Failed with {0} "
+                                         "expected min_impurity_decrease={1}"
+                                         .format(actual_decrease,
+                                                 expected_decrease))
 
     for name, TreeEstimator in ALL_TREES.items():
         if "Classifier" in name:
@@ -1186,6 +1255,19 @@ def test_only_constant_features():
         assert_equal(est.tree_.max_depth, 0)
 
 
+def test_behaviour_constant_feature_after_splits():
+    X = np.transpose(np.vstack(([[0, 0, 0, 0, 0, 1, 2, 4, 5, 6, 7]],
+                               np.zeros((4, 11)))))
+    y = [0, 0, 0, 1, 1, 2, 2, 2, 3, 3, 3]
+    for name, TreeEstimator in ALL_TREES.items():
+        # do not check extra random trees
+        if "ExtraTree" not in name:
+            est = TreeEstimator(random_state=0, max_features=1)
+            est.fit(X, y)
+            assert_equal(est.tree_.max_depth, 2)
+            assert_equal(est.tree_.node_count, 5)
+
+
 def test_with_only_one_non_constant_features():
     X = np.hstack([np.array([[1.], [1.], [0.], [0.]]),
                    np.zeros((4, 1000))])
@@ -1600,11 +1682,12 @@ def test_mae():
     # on small toy dataset
     dt_mae = DecisionTreeRegressor(random_state=0, criterion="mae",
                                    max_leaf_nodes=2)
-    dt_mae.fit([[3],[5],[3],[8],[5]],[6,7,3,4,3])
+    dt_mae.fit([[3], [5], [3], [8], [5]], [6, 7, 3, 4, 3])
     assert_array_equal(dt_mae.tree_.impurity, [1.4, 1.5, 4.0/3.0])
     assert_array_equal(dt_mae.tree_.value.flat, [4, 4.5, 4.0])
 
-    dt_mae.fit([[3],[5],[3],[8],[5]],[6,7,3,4,3], [0.6,0.3,0.1,1.0,0.3])
+    dt_mae.fit([[3], [5], [3], [8], [5]], [6, 7, 3, 4, 3],
+               [0.6, 0.3, 0.1, 1.0, 0.3])
     assert_array_equal(dt_mae.tree_.impurity, [7.0/2.3, 3.0/0.7, 4.0/1.6])
     assert_array_equal(dt_mae.tree_.value.flat, [4.0, 6.0, 4.0])
 
diff --git a/sklearn/tree/tree.py b/sklearn/tree/tree.py
index adda8ad8af49..147c5b4bb8ac 100644
--- a/sklearn/tree/tree.py
+++ b/sklearn/tree/tree.py
@@ -18,6 +18,7 @@
 
 
 import numbers
+import warnings
 from abc import ABCMeta
 from abc import abstractmethod
 from math import ceil
@@ -89,6 +90,7 @@ def __init__(self,
                  max_features,
                  max_leaf_nodes,
                  random_state,
+                 min_impurity_decrease,
                  min_impurity_split,
                  class_weight=None,
                  presort=False):
@@ -101,6 +103,7 @@ def __init__(self,
         self.max_features = max_features
         self.random_state = random_state
         self.max_leaf_nodes = max_leaf_nodes
+        self.min_impurity_decrease = min_impurity_decrease
         self.min_impurity_split = min_impurity_split
         self.class_weight = class_weight
         self.presort = presort
@@ -272,10 +275,23 @@ def fit(self, X, y, sample_weight=None, check_input=True,
             min_weight_leaf = (self.min_weight_fraction_leaf *
                                np.sum(sample_weight))
 
-        if self.min_impurity_split < 0.:
+        if self.min_impurity_split is not None:
+            warnings.warn("The min_impurity_split parameter is deprecated and"
+                          " will be removed in version 0.21. "
+                          "Use the min_impurity_decrease parameter instead.",
+                          DeprecationWarning)
+            min_impurity_split = self.min_impurity_split
+        else:
+            min_impurity_split = 1e-7
+
+        if min_impurity_split < 0.:
             raise ValueError("min_impurity_split must be greater than "
                              "or equal to 0")
 
+        if self.min_impurity_decrease < 0.:
+            raise ValueError("min_impurity_decrease must be greater than "
+                             "or equal to 0")
+
         presort = self.presort
         # Allow presort to be 'auto', which means True if the dataset is dense,
         # otherwise it will be False.
@@ -331,14 +347,17 @@ def fit(self, X, y, sample_weight=None, check_input=True,
             builder = DepthFirstTreeBuilder(splitter, min_samples_split,
                                             min_samples_leaf,
                                             min_weight_leaf,
-                                            max_depth, self.min_impurity_split)
+                                            max_depth,
+                                            self.min_impurity_decrease,
+                                            min_impurity_split)
         else:
             builder = BestFirstTreeBuilder(splitter, min_samples_split,
                                            min_samples_leaf,
                                            min_weight_leaf,
                                            max_depth,
                                            max_leaf_nodes,
-                                           self.min_impurity_split)
+                                           self.min_impurity_decrease,
+                                           min_impurity_split)
 
         builder.build(self.tree_, X, y, sample_weight, X_idx_sorted)
 
@@ -350,10 +369,6 @@ def fit(self, X, y, sample_weight=None, check_input=True,
 
     def _validate_X_predict(self, X, check_input):
         """Validate X whenever one tries to predict, apply, predict_proba"""
-        if self.tree_ is None:
-            raise NotFittedError("Estimator not fitted, "
-                                 "call `fit` before exploiting the model.")
-
         if check_input:
             X = check_array(X, dtype=DTYPE, accept_sparse="csr")
             if issparse(X) and (X.indices.dtype != np.intc or
@@ -587,11 +602,23 @@ class DecisionTreeClassifier(BaseDecisionTree, ClassifierMixin):
         If None, the random number generator is the RandomState instance used
         by `np.random`.
 
-    min_impurity_split : float, optional (default=1e-7)
-        Threshold for early stopping in tree growth. A node will split
-        if its impurity is above the threshold, otherwise it is a leaf.
+    min_impurity_decrease : float, optional (default=0.)
+        A node will be split if this split induces a decrease of the impurity
+        greater than or equal to this value.
 
-        .. versionadded:: 0.18
+        The weighted impurity decrease equation is the following::
+
+            N_t / N * (impurity - N_t_R / N_t * right_impurity
+                                - N_t_L / N_t * left_impurity)
+
+        where ``N`` is the total number of samples, ``N_t`` is the number of
+        samples at the current node, ``N_t_L`` is the number of samples in the
+        left child, and ``N_t_R`` is the number of samples in the right child.
+
+        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
+        if ``sample_weight`` is passed.
+
+        .. versionadded:: 0.19
 
     presort : bool, optional (default=False)
         Whether to presort the data to speed up the finding of best splits in
@@ -631,6 +658,12 @@ class DecisionTreeClassifier(BaseDecisionTree, ClassifierMixin):
 
     Notes
     -----
+    The default values for the parameters controlling the size of the trees
+    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
+    unpruned trees which can potentially be very large on some data sets. To
+    reduce memory consumption, the complexity and size of the trees should be
+    controlled by setting those parameter values.
+
     The features are always randomly permuted at each split. Therefore,
     the best found split may vary, even with the same training data and
     ``max_features=n_features``, if the improvement of the criterion is
@@ -679,7 +712,8 @@ def __init__(self,
                  max_features=None,
                  random_state=None,
                  max_leaf_nodes=None,
-                 min_impurity_split=1e-7,
+                 min_impurity_decrease=0.,
+                 min_impurity_split=None,
                  class_weight=None,
                  presort=False):
         super(DecisionTreeClassifier, self).__init__(
@@ -693,6 +727,7 @@ def __init__(self,
             max_leaf_nodes=max_leaf_nodes,
             class_weight=class_weight,
             random_state=random_state,
+            min_impurity_decrease=min_impurity_decrease,
             min_impurity_split=min_impurity_split,
             presort=presort)
 
@@ -897,11 +932,23 @@ class DecisionTreeRegressor(BaseDecisionTree, RegressorMixin):
         If None, the random number generator is the RandomState instance used
         by `np.random`.
 
-    min_impurity_split : float, optional (default=1e-7)
-        Threshold for early stopping in tree growth. If the impurity
-        of a node is below the threshold, the node is a leaf.
+    min_impurity_decrease : float, optional (default=0.)
+        A node will be split if this split induces a decrease of the impurity
+        greater than or equal to this value.
 
-        .. versionadded:: 0.18
+        The weighted impurity decrease equation is the following::
+
+            N_t / N * (impurity - N_t_R / N_t * right_impurity
+                                - N_t_L / N_t * left_impurity)
+
+        where ``N`` is the total number of samples, ``N_t`` is the number of
+        samples at the current node, ``N_t_L`` is the number of samples in the
+        left child, and ``N_t_R`` is the number of samples in the right child.
+
+        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
+        if ``sample_weight`` is passed.
+
+        .. versionadded:: 0.19
 
     presort : bool, optional (default=False)
         Whether to presort the data to speed up the finding of best splits in
@@ -933,6 +980,12 @@ class DecisionTreeRegressor(BaseDecisionTree, RegressorMixin):
 
     Notes
     -----
+    The default values for the parameters controlling the size of the trees
+    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
+    unpruned trees which can potentially be very large on some data sets. To
+    reduce memory consumption, the complexity and size of the trees should be
+    controlled by setting those parameter values.
+
     The features are always randomly permuted at each split. Therefore,
     the best found split may vary, even with the same training data and
     ``max_features=n_features``, if the improvement of the criterion is
@@ -981,7 +1034,8 @@ def __init__(self,
                  max_features=None,
                  random_state=None,
                  max_leaf_nodes=None,
-                 min_impurity_split=1e-7,
+                 min_impurity_decrease=0.,
+                 min_impurity_split=None,
                  presort=False):
         super(DecisionTreeRegressor, self).__init__(
             criterion=criterion,
@@ -993,6 +1047,7 @@ def __init__(self,
             max_features=max_features,
             max_leaf_nodes=max_leaf_nodes,
             random_state=random_state,
+            min_impurity_decrease=min_impurity_decrease,
             min_impurity_split=min_impurity_split,
             presort=presort)
 
@@ -1058,6 +1113,14 @@ class ExtraTreeClassifier(DecisionTreeClassifier):
     --------
     ExtraTreeRegressor, ExtraTreesClassifier, ExtraTreesRegressor
 
+    Notes
+    -----
+    The default values for the parameters controlling the size of the trees
+    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
+    unpruned trees which can potentially be very large on some data sets. To
+    reduce memory consumption, the complexity and size of the trees should be
+    controlled by setting those parameter values.
+
     References
     ----------
 
@@ -1074,7 +1137,8 @@ def __init__(self,
                  max_features="auto",
                  random_state=None,
                  max_leaf_nodes=None,
-                 min_impurity_split=1e-7,
+                 min_impurity_decrease=0.,
+                 min_impurity_split=None,
                  class_weight=None):
         super(ExtraTreeClassifier, self).__init__(
             criterion=criterion,
@@ -1086,6 +1150,7 @@ def __init__(self,
             max_features=max_features,
             max_leaf_nodes=max_leaf_nodes,
             class_weight=class_weight,
+            min_impurity_decrease=min_impurity_decrease,
             min_impurity_split=min_impurity_split,
             random_state=random_state)
 
@@ -1108,6 +1173,14 @@ class ExtraTreeRegressor(DecisionTreeRegressor):
     --------
     ExtraTreeClassifier, ExtraTreesClassifier, ExtraTreesRegressor
 
+    Notes
+    -----
+    The default values for the parameters controlling the size of the trees
+    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
+    unpruned trees which can potentially be very large on some data sets. To
+    reduce memory consumption, the complexity and size of the trees should be
+    controlled by setting those parameter values.
+
     References
     ----------
 
@@ -1123,7 +1196,8 @@ def __init__(self,
                  min_weight_fraction_leaf=0.,
                  max_features="auto",
                  random_state=None,
-                 min_impurity_split=1e-7,
+                 min_impurity_decrease=0.,
+                 min_impurity_split=None,
                  max_leaf_nodes=None):
         super(ExtraTreeRegressor, self).__init__(
             criterion=criterion,
@@ -1134,5 +1208,6 @@ def __init__(self,
             min_weight_fraction_leaf=min_weight_fraction_leaf,
             max_features=max_features,
             max_leaf_nodes=max_leaf_nodes,
+            min_impurity_decrease=min_impurity_decrease,
             min_impurity_split=min_impurity_split,
             random_state=random_state)
diff --git a/sklearn/utils/__init__.py b/sklearn/utils/__init__.py
index a4e5b6a4f3ea..b3e41e1c130f 100644
--- a/sklearn/utils/__init__.py
+++ b/sklearn/utils/__init__.py
@@ -28,6 +28,52 @@
            "check_symmetric", "indices_to_mask", "deprecated"]
 
 
+class Bunch(dict):
+    """Container object for datasets
+
+    Dictionary-like object that exposes its keys as attributes.
+
+    >>> b = Bunch(a=1, b=2)
+    >>> b['b']
+    2
+    >>> b.b
+    2
+    >>> b.a = 3
+    >>> b['a']
+    3
+    >>> b.c = 6
+    >>> b['c']
+    6
+
+    """
+
+    def __init__(self, **kwargs):
+        super(Bunch, self).__init__(kwargs)
+
+    def __setattr__(self, key, value):
+        self[key] = value
+
+    def __dir__(self):
+        return self.keys()
+
+    def __getattr__(self, key):
+        try:
+            return self[key]
+        except KeyError:
+            raise AttributeError(key)
+
+    def __setstate__(self, state):
+        # Bunch pickles generated with scikit-learn 0.16.* have an non
+        # empty __dict__. This causes a surprising behaviour when
+        # loading these pickles scikit-learn 0.17: reading bunch.key
+        # uses __dict__ but assigning to bunch.key use __setattr__ and
+        # only changes bunch['key']. More details can be found at:
+        # https://github.com/scikit-learn/scikit-learn/issues/6196.
+        # Overriding __setstate__ to be a noop has the effect of
+        # ignoring the pickled __dict__
+        pass
+
+
 def safe_mask(X, mask):
     """Return a mask which is safe to use on X.
 
@@ -129,8 +175,12 @@ def resample(*arrays, **options):
         If replace is False it should not be larger than the length of
         arrays.
 
-    random_state : int or RandomState instance
-        Control the shuffling for reproducible behavior.
+    random_state : int, RandomState instance or None, optional (default=None)
+        The seed of the pseudo random number generator to use when shuffling
+        the data.  If int, random_state is the seed used by the random number
+        generator; If RandomState instance, random_state is the random number
+        generator; If None, the random number generator is the RandomState
+        instance used by `np.random`.
 
     Returns
     -------
@@ -225,8 +275,12 @@ def shuffle(*arrays, **options):
         Indexable data-structures can be arrays, lists, dataframes or scipy
         sparse matrices with consistent first dimension.
 
-    random_state : int or RandomState instance
-        Control the shuffling for reproducible behavior.
+    random_state : int, RandomState instance or None, optional (default=None)
+        The seed of the pseudo random number generator to use when shuffling
+        the data.  If int, random_state is the seed used by the random number
+        generator; If RandomState instance, random_state is the random number
+        generator; If None, the random number generator is the RandomState
+        instance used by `np.random`.
 
     n_samples : int, None by default
         Number of samples to generate. If left to None this is
diff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py
index cb23e0ba8a31..5c790e4f6522 100644
--- a/sklearn/utils/estimator_checks.py
+++ b/sklearn/utils/estimator_checks.py
@@ -6,9 +6,9 @@
 import traceback
 import pickle
 from copy import deepcopy
-
 import numpy as np
 from scipy import sparse
+from scipy.stats import rankdata
 import struct
 
 from sklearn.externals.six.moves import zip
@@ -113,10 +113,10 @@ def _yield_classifier_checks(name, Classifier):
     # basic consistency testing
     yield check_classifiers_train
     yield check_classifiers_regression_target
-    if (name not in ["MultinomialNB", "LabelPropagation", "LabelSpreading"]
+    if (name not in
+        ["MultinomialNB", "LabelPropagation", "LabelSpreading"] and
         # TODO some complication with -1 label
-            and name not in ["DecisionTreeClassifier",
-                             "ExtraTreeClassifier"]):
+       name not in ["DecisionTreeClassifier", "ExtraTreeClassifier"]):
             # We don't raise a warning in these classifiers, as
             # the column y interface is used by the forests.
 
@@ -127,6 +127,8 @@ def _yield_classifier_checks(name, Classifier):
         yield check_class_weight_classifiers
 
     yield check_non_transformer_estimators_n_iter
+    # test if predict_proba is a monotonic transformation of decision_function
+    yield check_decision_proba_consistency
 
 
 @ignore_warnings(category=DeprecationWarning)
@@ -269,8 +271,7 @@ def set_testing_parameters(estimator):
     # set parameters to speed up some estimators and
     # avoid deprecated behaviour
     params = estimator.get_params()
-    if ("n_iter" in params
-            and estimator.__class__.__name__ != "TSNE"):
+    if ("n_iter" in params and estimator.__class__.__name__ != "TSNE"):
         estimator.set_params(n_iter=5)
     if "max_iter" in params:
         warnings.simplefilter("ignore", ConvergenceWarning)
@@ -1112,8 +1113,7 @@ def check_classifiers_train(name, Classifier):
                     assert_equal(decision.shape, (n_samples,))
                     dec_pred = (decision.ravel() > 0).astype(np.int)
                     assert_array_equal(dec_pred, y_pred)
-                if (n_classes is 3
-                        and not isinstance(classifier, BaseLibSVM)):
+                if (n_classes is 3 and not isinstance(classifier, BaseLibSVM)):
                     # 1on1 of LibSVM works differently
                     assert_equal(decision.shape, (n_samples, n_classes))
                     assert_array_equal(np.argmax(decision, axis=1), y_pred)
@@ -1574,9 +1574,9 @@ def check_parameters_default_constructible(name, Estimator):
         try:
             def param_filter(p):
                 """Identify hyper parameters of an estimator"""
-                return (p.name != 'self'
-                        and p.kind != p.VAR_KEYWORD
-                        and p.kind != p.VAR_POSITIONAL)
+                return (p.name != 'self' and
+                        p.kind != p.VAR_KEYWORD and
+                        p.kind != p.VAR_POSITIONAL)
 
             init_params = [p for p in signature(init).parameters.values()
                            if param_filter(p)]
@@ -1721,3 +1721,25 @@ def check_classifiers_regression_target(name, Estimator):
     e = Estimator()
     msg = 'Unknown label type: '
     assert_raises_regex(ValueError, msg, e.fit, X, y)
+
+
+@ignore_warnings(category=DeprecationWarning)
+def check_decision_proba_consistency(name, Estimator):
+    # Check whether an estimator having both decision_function and
+    # predict_proba methods has outputs with perfect rank correlation.
+
+    centers = [(2, 2), (4, 4)]
+    X, y = make_blobs(n_samples=100, random_state=0, n_features=4,
+                      centers=centers, cluster_std=1.0, shuffle=True)
+    X_test = np.random.randn(20, 2) + 4
+    estimator = Estimator()
+
+    set_testing_parameters(estimator)
+
+    if (hasattr(estimator, "decision_function") and
+            hasattr(estimator, "predict_proba")):
+
+        estimator.fit(X, y)
+        a = estimator.predict_proba(X_test)[:, 1]
+        b = estimator.decision_function(X_test)
+        assert_array_equal(rankdata(a), rankdata(b))
diff --git a/sklearn/utils/extmath.py b/sklearn/utils/extmath.py
index df1f56dbcb89..d797950ba8ef 100644
--- a/sklearn/utils/extmath.py
+++ b/sklearn/utils/extmath.py
@@ -215,8 +215,12 @@ def randomized_range_finder(A, size, n_iter,
 
         .. versionadded:: 0.18
 
-    random_state : RandomState or an int seed (0 by default)
-        A random number generator instance
+    random_state : int, RandomState instance or None, optional (default=None)
+        The seed of the pseudo random number generator to use when shuffling
+        the data.  If int, random_state is the seed used by the random number
+        generator; If RandomState instance, random_state is the random number
+        generator; If None, the random number generator is the RandomState
+        instance used by `np.random`.
 
     Returns
     -------
@@ -320,8 +324,12 @@ def randomized_svd(M, n_components, n_oversamples=10, n_iter='auto',
         set to `True`, the sign ambiguity is resolved by making the largest
         loadings for each component in the left singular vectors positive.
 
-    random_state : RandomState or an int seed (0 by default)
-        A random number generator instance to make behavior
+    random_state : int, RandomState instance or None, optional (default=None)
+        The seed of the pseudo random number generator to use when shuffling
+        the data.  If int, random_state is the seed used by the random number
+        generator; If RandomState instance, random_state is the random number
+        generator; If None, the random number generator is the RandomState
+        instance used by `np.random`.
 
     Notes
     -----
diff --git a/sklearn/utils/fixes.py b/sklearn/utils/fixes.py
index 7f1fe8eb964a..0afb52ef032a 100644
--- a/sklearn/utils/fixes.py
+++ b/sklearn/utils/fixes.py
@@ -36,6 +36,8 @@ def _parse_version(version_string):
             version.append(x)
     return tuple(version)
 
+euler_gamma = getattr(np, 'euler_gamma',
+                      0.577215664901532860606512090082402431)
 
 np_version = _parse_version(np.__version__)
 sp_version = _parse_version(scipy.__version__)
@@ -205,6 +207,50 @@ def partition(a, kth, axis=-1, kind='introselect', order=None):
 
 
 if np_version < (1, 7):
+    # numpy.isclose was introduced in v 1.7.0
+    def isclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):
+        def within_tol(x, y, atol, rtol):
+            with np.errstate(invalid='ignore'):
+                result = np.less_equal(abs(x-y), atol + rtol * abs(y))
+            if np.isscalar(a) and np.isscalar(b):
+                result = bool(result)
+            return result
+
+        x = np.array(a, copy=False, subok=True, ndmin=1)
+        y = np.array(b, copy=False, subok=True, ndmin=1)
+
+        # Make sure y is an inexact type to avoid bad behavior on abs(MIN_INT).
+        # This will cause casting of x later. Also, make sure to allow
+        # subclasses (e.g., for numpy.ma).
+        dt = np.core.multiarray.result_type(y, 1.)
+        y = np.array(y, dtype=dt, copy=False, subok=True)
+
+        xfin = np.isfinite(x)
+        yfin = np.isfinite(y)
+        if xfin.all() and yfin.all():
+            return within_tol(x, y, atol, rtol)
+        else:
+            finite = xfin & yfin
+            cond = np.zeros_like(finite, subok=True)
+            # Because we're using boolean indexing, x & y must be the same
+            # shape. Ideally, we'd just do x, y = broadcast_arrays(x, y). It's
+            # in lib.stride_tricks, though, so we can't import it here.
+            x = x * np.ones_like(cond)
+            y = y * np.ones_like(cond)
+            # Avoid subtraction with infinite/nan values...
+            cond[finite] = within_tol(x[finite], y[finite], atol, rtol)
+            # Check for equality of infinite values...
+            cond[~finite] = (x[~finite] == y[~finite])
+            if equal_nan:
+                # Make NaN == NaN
+                both_nan = np.isnan(x) & np.isnan(y)
+                cond[both_nan] = both_nan[both_nan]
+
+            if np.isscalar(a) and np.isscalar(b):
+                return bool(cond)
+            else:
+                return cond
+
     # Prior to 1.7.0, np.frombuffer wouldn't work for empty first arg.
     def frombuffer_empty(buf, dtype):
         if len(buf) == 0:
@@ -213,6 +259,7 @@ def frombuffer_empty(buf, dtype):
             return np.frombuffer(buf, dtype=dtype)
 else:
     frombuffer_empty = np.frombuffer
+    isclose = np.isclose
 
 
 if np_version < (1, 8):
diff --git a/sklearn/utils/metaestimators.py b/sklearn/utils/metaestimators.py
index 3123bb1778ce..fcbbbf894b76 100644
--- a/sklearn/utils/metaestimators.py
+++ b/sklearn/utils/metaestimators.py
@@ -3,14 +3,75 @@
 #         Andreas Mueller
 # License: BSD
 
+from abc import ABCMeta, abstractmethod
 from operator import attrgetter
 from functools import update_wrapper
 import numpy as np
+
 from ..utils import safe_indexing
+from ..externals import six
+from ..base import BaseEstimator
 
 __all__ = ['if_delegate_has_method']
 
 
+class _BaseComposition(six.with_metaclass(ABCMeta, BaseEstimator)):
+    """Handles parameter management for classifiers composed of named estimators.
+    """
+    @abstractmethod
+    def __init__(self):
+        pass
+
+    def _get_params(self, attr, deep=True):
+        out = super(_BaseComposition, self).get_params(deep=False)
+        if not deep:
+            return out
+        estimators = getattr(self, attr)
+        out.update(estimators)
+        for name, estimator in estimators:
+            if estimator is None:
+                continue
+            for key, value in six.iteritems(estimator.get_params(deep=True)):
+                out['%s__%s' % (name, key)] = value
+        return out
+
+    def _set_params(self, attr, **params):
+        # Ensure strict ordering of parameter setting:
+        # 1. All steps
+        if attr in params:
+            setattr(self, attr, params.pop(attr))
+        # 2. Step replacement
+        names, _ = zip(*getattr(self, attr))
+        for name in list(six.iterkeys(params)):
+            if '__' not in name and name in names:
+                self._replace_estimator(attr, name, params.pop(name))
+        # 3. Step parameters and other initilisation arguments
+        super(_BaseComposition, self).set_params(**params)
+        return self
+
+    def _replace_estimator(self, attr, name, new_val):
+        # assumes `name` is a valid estimator name
+        new_estimators = getattr(self, attr)[:]
+        for i, (estimator_name, _) in enumerate(new_estimators):
+            if estimator_name == name:
+                new_estimators[i] = (name, new_val)
+                break
+        setattr(self, attr, new_estimators)
+
+    def _validate_names(self, names):
+        if len(set(names)) != len(names):
+            raise ValueError('Names provided are not unique: '
+                             '{0!r}'.format(list(names)))
+        invalid_names = set(names).intersection(self.get_params(deep=False))
+        if invalid_names:
+            raise ValueError('Estimator names conflict with constructor '
+                             'arguments: {0!r}'.format(sorted(invalid_names)))
+        invalid_names = [name for name in names if '__' in name]
+        if invalid_names:
+            raise ValueError('Estimator names must not contain __: got '
+                             '{0!r}'.format(invalid_names))
+
+
 class _IffHasAttrDescriptor(object):
     """Implements a conditional property using the descriptor protocol.
 
diff --git a/sklearn/utils/tests/test_validation.py b/sklearn/utils/tests/test_validation.py
index 49d867a1b0be..402782668632 100644
--- a/sklearn/utils/tests/test_validation.py
+++ b/sklearn/utils/tests/test_validation.py
@@ -42,16 +42,28 @@ def test_as_float_array():
     # Test function for as_float_array
     X = np.ones((3, 10), dtype=np.int32)
     X = X + np.arange(10, dtype=np.int32)
-    # Checks that the return type is ok
     X2 = as_float_array(X, copy=False)
-    np.testing.assert_equal(X2.dtype, np.float32)
+    assert_equal(X2.dtype, np.float32)
     # Another test
     X = X.astype(np.int64)
     X2 = as_float_array(X, copy=True)
     # Checking that the array wasn't overwritten
     assert_true(as_float_array(X, False) is not X)
-    # Checking that the new type is ok
-    np.testing.assert_equal(X2.dtype, np.float64)
+    assert_equal(X2.dtype, np.float64)
+    # Test int dtypes <= 32bit
+    tested_dtypes = [np.bool,
+                     np.int8, np.int16, np.int32,
+                     np.uint8, np.uint16, np.uint32]
+    for dtype in tested_dtypes:
+        X = X.astype(dtype)
+        X2 = as_float_array(X)
+        assert_equal(X2.dtype, np.float32)
+
+    # Test object dtype
+    X = X.astype(object)
+    X2 = as_float_array(X, copy=True)
+    assert_equal(X2.dtype, np.float64)
+
     # Here, X is of the right type, it shouldn't be modified
     X = np.ones((3, 2), dtype=np.float32)
     assert_true(as_float_array(X, copy=False) is X)
diff --git a/sklearn/utils/validation.py b/sklearn/utils/validation.py
index aa7b3cc78f80..02a60786c586 100644
--- a/sklearn/utils/validation.py
+++ b/sklearn/utils/validation.py
@@ -80,7 +80,11 @@ def as_float_array(X, copy=True, force_all_finite=True):
     elif X.dtype in [np.float32, np.float64]:  # is numpy array
         return X.copy('F' if X.flags['F_CONTIGUOUS'] else 'C') if copy else X
     else:
-        return X.astype(np.float32 if X.dtype == np.int32 else np.float64)
+        if X.dtype.kind in 'uib' and X.dtype.itemsize <= 4:
+            return_dtype = np.float32
+        else:
+            return_dtype = np.float64
+        return X.astype(return_dtype)
 
 
 def _is_arraylike(x):
