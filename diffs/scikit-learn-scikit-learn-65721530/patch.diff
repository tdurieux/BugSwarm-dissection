diff --git a/doc/modules/model_persistence.rst b/doc/modules/model_persistence.rst
index e1c9e92f70b5..871dba6c0f01 100644
--- a/doc/modules/model_persistence.rst
+++ b/doc/modules/model_persistence.rst
@@ -22,9 +22,9 @@ persistence model, namely `pickle <http://docs.python.org/library/pickle.html>`_
   >>> iris = datasets.load_iris()
   >>> X, y = iris.data, iris.target
   >>> clf.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE
-  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
-    kernel='rbf', max_iter=-1, probability=False, random_state=None,
-    shrinking=True, tol=0.001, verbose=False)
+  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, 
+    gamma='auto', kernel='rbf', max_iter=-1, probability=False, 
+    random_state=None, shrinking=True, tol=0.001, verbose=False)
 
   >>> import pickle
   >>> s = pickle.dumps(clf)
diff --git a/doc/modules/pipeline.rst b/doc/modules/pipeline.rst
index 61a0e318da5b..6f7146f1afb0 100644
--- a/doc/modules/pipeline.rst
+++ b/doc/modules/pipeline.rst
@@ -42,7 +42,7 @@ is an estimator object::
     >>> clf # doctest: +NORMALIZE_WHITESPACE
     Pipeline(steps=[('reduce_dim', PCA(copy=True, n_components=None,
         whiten=False)), ('svm', SVC(C=1.0, cache_size=200, class_weight=None,
-        coef0=0.0, degree=3, gamma=0.0, kernel='rbf', max_iter=-1,
+        coef0=0.0, degree=3, gamma='auto', kernel='rbf', max_iter=-1,
         probability=False, random_state=None, shrinking=True, tol=0.001,
         verbose=False))])
 
@@ -76,7 +76,7 @@ Parameters of the estimators in the pipeline can be accessed using the
     >>> clf.set_params(svm__C=10) # doctest: +NORMALIZE_WHITESPACE
     Pipeline(steps=[('reduce_dim', PCA(copy=True, n_components=None,
         whiten=False)), ('svm', SVC(C=10, cache_size=200, class_weight=None,
-        coef0=0.0, degree=3, gamma=0.0, kernel='rbf', max_iter=-1,
+        coef0=0.0, degree=3, gamma='auto', kernel='rbf', max_iter=-1,
         probability=False, random_state=None, shrinking=True, tol=0.001,
         verbose=False))])
 
diff --git a/doc/modules/svm.rst b/doc/modules/svm.rst
index 405f88c56895..1041ac6bf066 100644
--- a/doc/modules/svm.rst
+++ b/doc/modules/svm.rst
@@ -77,8 +77,8 @@ n_features]`` holding the training samples, and an array y of class labels
     >>> clf = svm.SVC()
     >>> clf.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE
     SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
-    gamma=0.0, kernel='rbf', max_iter=-1, probability=False, random_state=None,
-    shrinking=True, tol=0.001, verbose=False)
+    gamma='auto', kernel='rbf', max_iter=-1, probability=False, 
+    random_state=None, shrinking=True, tol=0.001, verbose=False)
 
 After being fitted, the model can then be used to predict new values::
 
@@ -116,8 +116,8 @@ classifiers are constructed and each one trains data from two classes::
     >>> clf = svm.SVC()
     >>> clf.fit(X, Y) # doctest: +NORMALIZE_WHITESPACE
     SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
-    gamma=0.0, kernel='rbf', max_iter=-1, probability=False, random_state=None,
-    shrinking=True, tol=0.001, verbose=False)
+    gamma='auto', kernel='rbf', max_iter=-1, probability=False, 
+    random_state=None, shrinking=True, tol=0.001, verbose=False)
     >>> dec = clf.decision_function([[1]])
     >>> dec.shape[1] # 4 classes: 4*3/2 = 6
     6
@@ -304,7 +304,7 @@ floating point values instead of integer values::
     >>> y = [0.5, 2.5]
     >>> clf = svm.SVR()
     >>> clf.fit(X, y) # doctest: +NORMALIZE_WHITESPACE
-    SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.0,
+    SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',
         kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)
     >>> clf.predict([[1, 1]])
     array([ 1.5])
@@ -504,7 +504,7 @@ test vectors must be provided.
     >>> gram = np.dot(X, X.T)
     >>> clf.fit(gram, y) # doctest: +NORMALIZE_WHITESPACE
     SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
-    gamma=0.0, kernel='precomputed', max_iter=-1, probability=False,
+    gamma='auto', kernel='precomputed', max_iter=-1, probability=False,
     random_state=None, shrinking=True, tol=0.001, verbose=False)
     >>> # predict on training examples
     >>> clf.predict(gram)
diff --git a/doc/tutorial/basic/tutorial.rst b/doc/tutorial/basic/tutorial.rst
index bd50dca76ea9..cae019ec9407 100644
--- a/doc/tutorial/basic/tutorial.rst
+++ b/doc/tutorial/basic/tutorial.rst
@@ -214,9 +214,9 @@ persistence model, namely `pickle <http://docs.python.org/library/pickle.html>`_
   >>> iris = datasets.load_iris()
   >>> X, y = iris.data, iris.target
   >>> clf.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE
-  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
-    kernel='rbf', max_iter=-1, probability=False, random_state=None,
-    shrinking=True, tol=0.001, verbose=False)
+  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
+    gamma='auto', kernel='rbf', max_iter=-1, probability=False,
+    random_state=None, shrinking=True, tol=0.001, verbose=False)
 
   >>> import pickle
   >>> s = pickle.dumps(clf)
@@ -287,17 +287,17 @@ maintained::
     >>> iris = datasets.load_iris()
     >>> clf = SVC()
     >>> clf.fit(iris.data, iris.target)
-    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
-      kernel='rbf', max_iter=-1, probability=False, random_state=None,
-      shrinking=True, tol=0.001, verbose=False)
+    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
+      gamma='auto', kernel='rbf', max_iter=-1, probability=False,
+      random_state=None, shrinking=True, tol=0.001, verbose=False)
 
     >>> list(clf.predict(iris.data[:3]))
     [0, 0, 0]
 
     >>> clf.fit(iris.data, iris.target_names[iris.target])
-    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
-      kernel='rbf', max_iter=-1, probability=False, random_state=None,
-      shrinking=True, tol=0.001, verbose=False)
+    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
+      gamma='auto', kernel='rbf', max_iter=-1, probability=False,
+      random_state=None, shrinking=True, tol=0.001, verbose=False)
 
     >>> list(clf.predict(iris.data[:3]))  # doctest: +NORMALIZE_WHITESPACE
     ['setosa', 'setosa', 'setosa']
@@ -324,16 +324,16 @@ more than once will overwrite what was learned by any previous ``fit()``::
 
   >>> clf = SVC()
   >>> clf.set_params(kernel='linear').fit(X, y)
-  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
-    kernel='linear', max_iter=-1, probability=False, random_state=None,
-    shrinking=True, tol=0.001, verbose=False)
+  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
+    gamma='auto', kernel='linear', max_iter=-1, probability=False,
+    random_state=None, shrinking=True, tol=0.001, verbose=False)
   >>> clf.predict(X_test)
   array([1, 0, 1, 1, 0])
 
   >>> clf.set_params(kernel='rbf').fit(X, y)
-  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
-    kernel='rbf', max_iter=-1, probability=False, random_state=None,
-    shrinking=True, tol=0.001, verbose=False)
+  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
+    gamma='auto', kernel='rbf', max_iter=-1, probability=False,
+    random_state=None, shrinking=True, tol=0.001, verbose=False)
   >>> clf.predict(X_test)
   array([0, 0, 0, 1, 0])
 
diff --git a/doc/tutorial/statistical_inference/supervised_learning.rst b/doc/tutorial/statistical_inference/supervised_learning.rst
index 901d0409fa6b..5ef8391d9189 100644
--- a/doc/tutorial/statistical_inference/supervised_learning.rst
+++ b/doc/tutorial/statistical_inference/supervised_learning.rst
@@ -453,9 +453,9 @@ classification --:class:`SVC` (Support Vector Classification).
     >>> from sklearn import svm
     >>> svc = svm.SVC(kernel='linear')
     >>> svc.fit(iris_X_train, iris_y_train)    # doctest: +NORMALIZE_WHITESPACE
-    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
-      kernel='linear', max_iter=-1, probability=False, random_state=None,
-      shrinking=True, tol=0.001, verbose=False)
+    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, 
+      gamma='auto', kernel='linear', max_iter=-1, probability=False, 
+      random_state=None, shrinking=True, tol=0.001, verbose=False)
 
 
 .. warning:: **Normalizing data**
diff --git a/sklearn/mixture/gmm.py b/sklearn/mixture/gmm.py
index 298e54dcacae..f064b2b9fd75 100644
--- a/sklearn/mixture/gmm.py
+++ b/sklearn/mixture/gmm.py
@@ -211,7 +211,7 @@ class GaussianMixtureModel(DensityMixin, BaseEstimator):
     ...                       10 + np.random.randn(300, 1)))
     >>> g.fit(obs) # doctest: +NORMALIZE_WHITESPACE
     GaussianMixtureModel(covariance_type='diag', init_params='wmc',
-        min_covar=0.001, n_components=2, n_init=1, n_iter=100, params='wmc',
+        min_covar=0.001, n_components=2, n_init=1, n_iter=100, params=None,
         random_state=None, thresh=None, tol=0.001, verbose=0)
     >>> np.round(g.weights_, 2)
     array([ 0.75,  0.25])
@@ -229,7 +229,7 @@ class GaussianMixtureModel(DensityMixin, BaseEstimator):
     >>> # same), this time with an even split between the two modes.
     >>> g.fit(20 * [[0]] +  20 * [[10]]) # doctest: +NORMALIZE_WHITESPACE
     GaussianMixtureModel(covariance_type='diag', init_params='wmc',
-        min_covar=0.001, n_components=2, n_init=1, n_iter=100, params='wmc',
+        min_covar=0.001, n_components=2, n_init=1, n_iter=100, params=None,
         random_state=None, thresh=None, tol=0.001, verbose=0)
     >>> np.round(g.weights_, 2)
     array([ 0.5,  0.5])
@@ -238,7 +238,7 @@ class GaussianMixtureModel(DensityMixin, BaseEstimator):
 
     def __init__(self, n_components=1, covariance_type='diag',
                  random_state=None, thresh=None, tol=1e-3, min_covar=1e-3,
-                 n_iter=100, n_init=1, params='wmc', init_params='wmc',
+                 n_iter=100, n_init=1, params=None, init_params='wmc',
                  verbose=0):
         if thresh is not None:
             warnings.warn("'thresh' has been replaced by 'tol' in 0.16 "
@@ -261,7 +261,12 @@ def __init__(self, n_components=1, covariance_type='diag',
                              covariance_type)
 
         if n_init < 1:
-            raise ValueError('GMM estimation requires at least one run')
+            raise ValueError('The estimation requires at least one run')
+
+        if params is not None:
+            warnings.warn("The 'params' has been deprecated in 0.17 "
+                          "and will be removed in 0.19.",
+                          DeprecationWarning)
 
         self.weights_ = np.ones(self.n_components) / self.n_components
 
@@ -598,11 +603,11 @@ def _do_mstep(self, X, responsibilities, params, min_covar=0):
         weighted_X_sum = np.dot(responsibilities.T, X)
         inverse_weights = 1.0 / (weights[:, np.newaxis] + 10 * EPS)
 
-        if 'w' in params:
+        if params is None or 'w' in params:
             self.weights_ = (weights / (weights.sum() + 10 * EPS) + EPS)
-        if 'm' in params:
+        if params is None or 'm' in params:
             self.means_ = weighted_X_sum * inverse_weights
-        if 'c' in params:
+        if params is None or 'c' in params:
             covar_mstep_func = _covar_mstep_funcs[self.covariance_type]
             self.covars_ = covar_mstep_func(
                 self, X, responsibilities, weighted_X_sum, inverse_weights,
@@ -916,7 +921,7 @@ class GMM(GaussianMixtureModel):
     ...                       10 + np.random.randn(300, 1)))
     >>> g.fit(obs) # doctest: +NORMALIZE_WHITESPACE
     GMM(covariance_type='diag', init_params='wmc', min_covar=0.001,
-            n_components=2, n_init=1, n_iter=100, params='wmc',
+            n_components=2, n_init=1, n_iter=100, params=None,
             random_state=None, thresh=None, tol=0.001, verbose=0)
     >>> np.round(g.weights_, 2)
     array([ 0.75,  0.25])
@@ -934,7 +939,7 @@ class GMM(GaussianMixtureModel):
     >>> # same), this time with an even split between the two modes.
     >>> g.fit(20 * [[0]] +  20 * [[10]]) # doctest: +NORMALIZE_WHITESPACE
     GMM(covariance_type='diag', init_params='wmc', min_covar=0.001,
-            n_components=2, n_init=1, n_iter=100, params='wmc',
+            n_components=2, n_init=1, n_iter=100, params=None,
             random_state=None, thresh=None, tol=0.001, verbose=0)
     >>> np.round(g.weights_, 2)
     array([ 0.5,  0.5])
@@ -943,17 +948,13 @@ class GMM(GaussianMixtureModel):
 
     def __init__(self, n_components=1, covariance_type='diag',
                  random_state=None, thresh=None, tol=1e-3, min_covar=1e-3,
-                 n_iter=100, n_init=1, params='wmc', init_params='wmc',
+                 n_iter=100, n_init=1, params=None, init_params='wmc',
                  verbose=0):
         warnings.warn("The 'GMM' class has been renamed to "
                       "'GaussianMixtureModel' in 0.17 and "
                       "will be removed in 0.19. "
                       "Use the 'GaussianMixtureModel' class instead.",
                       DeprecationWarning)
-        if params is not 'wmc':
-            warnings.warn("'params' has been deprecated in 0.17 "
-                          "and will be removed in 0.19.",
-                          DeprecationWarning)
         super(GMM, self).__init__(n_components, covariance_type,
                                   random_state, thresh, tol, min_covar,
                                   n_iter, n_init, params, init_params, verbose)
diff --git a/sklearn/mixture/tests/test_gmm.py b/sklearn/mixture/tests/test_gmm.py
index a91a3579ad9e..409ebb7de9a6 100644
--- a/sklearn/mixture/tests/test_gmm.py
+++ b/sklearn/mixture/tests/test_gmm.py
@@ -479,7 +479,7 @@ def test_GMM_deprecation_warnings():
                          "Use the 'GaussianMixtureModel' class instead.",
                          mixture.GMM)
     assert_warns_message(DeprecationWarning,
-                         "'params' has been deprecated in 0.17 "
+                         "The 'params' has been deprecated in 0.17 "
                          "and will be removed in 0.19.", mixture.GMM,
                          params='wm')
     g = mixture.GMM()
@@ -502,5 +502,6 @@ def test_GMM_deprecation_warnings():
                          "'log_probability' of each sample. "
                          "Use the 'predict_proba' method "
                          "in the 'GaussianMixtureModel' class to get the "
-                         "'responsibility' of each sample.", g.score_samples, X)
+                         "'responsibility' of each sample.",
+                         g.score_samples, X)
 
diff --git a/sklearn/svm/base.py b/sklearn/svm/base.py
index eb4ef212b8e0..44d18a5cb57a 100644
--- a/sklearn/svm/base.py
+++ b/sklearn/svm/base.py
@@ -72,6 +72,15 @@ def __init__(self, impl, kernel, degree, gamma, coef0,
         if impl not in LIBSVM_IMPL:  # pragma: no cover
             raise ValueError("impl should be one of %s, %s was given" % (
                 LIBSVM_IMPL, impl))
+       
+        # FIXME Remove gamma=0.0 support in 0.18 
+        if gamma == 0:
+            msg = ("gamma=%s has been deprecated in favor of "
+                   "gamma='%s' as of 0.17. Backward compatibility"
+                   " for gamma=%s will be removed in %s")
+            invalid_gamma = 0.0
+            warnings.warn(msg % (invalid_gamma, "auto", 
+                invalid_gamma, "0.18"), DeprecationWarning)
 
         self._impl = impl
         self.kernel = kernel
@@ -159,10 +168,14 @@ def fit(self, X, y, sample_weight=None):
                              "Note: Sparse matrices cannot be indexed w/"
                              "boolean masks (use `indices=True` in CV)."
                              % (sample_weight.shape, X.shape))
-
-        if (self.kernel in ['poly', 'rbf']) and (self.gamma == 0):
+        
+        # FIXME remove (self.gamma == 0) in 0.18
+        if (self.kernel in ['poly', 'rbf']) and ((self.gamma == 0) 
+                or (self.gamma == 'auto')):
             # if custom gamma is not provided ...
             self._gamma = 1.0 / X.shape[1]
+        elif self.gamma == 'auto':
+            self._gamma = 0.0
         else:
             self._gamma = self.gamma
 
diff --git a/sklearn/svm/classes.py b/sklearn/svm/classes.py
index bc8e4adb5f14..b64f68e2a9af 100644
--- a/sklearn/svm/classes.py
+++ b/sklearn/svm/classes.py
@@ -411,9 +411,9 @@ class SVC(BaseSVC):
         Degree of the polynomial kernel function ('poly').
         Ignored by all other kernels.
 
-    gamma : float, optional (default=0.0)
+    gamma : float, optional (default='auto')
         Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.
-        If gamma is 0.0 then 1/n_features will be used instead.
+        If gamma is 'auto' then 1/n_features will be used instead.
 
     coef0 : float, optional (default=0.0)
         Independent term in kernel function.
@@ -489,7 +489,7 @@ class SVC(BaseSVC):
     >>> clf = SVC()
     >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE
     SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
-        gamma=0.0, kernel='rbf', max_iter=-1, probability=False,
+        gamma='auto', kernel='rbf', max_iter=-1, probability=False,
         random_state=None, shrinking=True, tol=0.001, verbose=False)
     >>> print(clf.predict([[-0.8, -1]]))
     [1]
@@ -506,7 +506,7 @@ class SVC(BaseSVC):
 
     """
 
-    def __init__(self, C=1.0, kernel='rbf', degree=3, gamma=0.0,
+    def __init__(self, C=1.0, kernel='rbf', degree=3, gamma='auto',
                  coef0=0.0, shrinking=True, probability=False,
                  tol=1e-3, cache_size=200, class_weight=None,
                  verbose=False, max_iter=-1, random_state=None):
@@ -545,9 +545,9 @@ class NuSVC(BaseSVC):
         Degree of the polynomial kernel function ('poly').
         Ignored by all other kernels.
 
-    gamma : float, optional (default=0.0)
+    gamma : float, optional (default='auto')
         Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.
-        If gamma is 0.0 then 1/n_features will be used instead.
+        If gamma is 'auto' then 1/n_features will be used instead.
 
     coef0 : float, optional (default=0.0)
         Independent term in kernel function.
@@ -614,7 +614,7 @@ class NuSVC(BaseSVC):
     >>> from sklearn.svm import NuSVC
     >>> clf = NuSVC()
     >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE
-    NuSVC(cache_size=200, coef0=0.0, degree=3, gamma=0.0, kernel='rbf',
+    NuSVC(cache_size=200, coef0=0.0, degree=3, gamma='auto', kernel='rbf',
           max_iter=-1, nu=0.5, probability=False, random_state=None,
           shrinking=True, tol=0.001, verbose=False)
     >>> print(clf.predict([[-0.8, -1]]))
@@ -630,7 +630,7 @@ class NuSVC(BaseSVC):
         liblinear.
     """
 
-    def __init__(self, nu=0.5, kernel='rbf', degree=3, gamma=0.0,
+    def __init__(self, nu=0.5, kernel='rbf', degree=3, gamma='auto',
                  coef0=0.0, shrinking=True, probability=False,
                  tol=1e-3, cache_size=200, verbose=False, max_iter=-1,
                  random_state=None):
@@ -671,9 +671,9 @@ class SVR(BaseLibSVM, RegressorMixin):
         Degree of the polynomial kernel function ('poly').
         Ignored by all other kernels.
 
-    gamma : float, optional (default=0.0)
+    gamma : float, optional (default='auto')
         Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.
-        If gamma is 0.0 then 1/n_features will be used instead.
+        If gamma is 'auto' then 1/n_features will be used instead.
 
     coef0 : float, optional (default=0.0)
         Independent term in kernel function.
@@ -727,7 +727,7 @@ class SVR(BaseLibSVM, RegressorMixin):
     >>> X = np.random.randn(n_samples, n_features)
     >>> clf = SVR(C=1.0, epsilon=0.2)
     >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE
-    SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.2, gamma=0.0,
+    SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.2, gamma='auto',
         kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)
 
     See also
@@ -740,9 +740,9 @@ class SVR(BaseLibSVM, RegressorMixin):
         Scalable Linear Support Vector Machine for regression
         implemented using liblinear.
     """
-    def __init__(self, kernel='rbf', degree=3, gamma=0.0, coef0=0.0, tol=1e-3,
-                 C=1.0, epsilon=0.1, shrinking=True, cache_size=200,
-                 verbose=False, max_iter=-1):
+    def __init__(self, kernel='rbf', degree=3, gamma='auto', coef0=0.0, 
+                 tol=1e-3, C=1.0, epsilon=0.1, shrinking=True, 
+                 cache_size=200, verbose=False, max_iter=-1):
 
         super(SVR, self).__init__(
             'epsilon_svr', kernel=kernel, degree=degree, gamma=gamma,
@@ -783,9 +783,9 @@ class NuSVR(BaseLibSVM, RegressorMixin):
         Degree of the polynomial kernel function ('poly').
         Ignored by all other kernels.
 
-    gamma : float, optional (default=0.0)
+    gamma : float, optional (default='auto')
         Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.
-        If gamma is 0.0 then 1/n_features will be used instead.
+        If gamma is 'auto' then 1/n_features will be used instead.
 
     coef0 : float, optional (default=0.0)
         Independent term in kernel function.
@@ -839,8 +839,9 @@ class NuSVR(BaseLibSVM, RegressorMixin):
     >>> X = np.random.randn(n_samples, n_features)
     >>> clf = NuSVR(C=1.0, nu=0.1)
     >>> clf.fit(X, y)  #doctest: +NORMALIZE_WHITESPACE
-    NuSVR(C=1.0, cache_size=200, coef0=0.0, degree=3, gamma=0.0, kernel='rbf',
-          max_iter=-1, nu=0.1, shrinking=True, tol=0.001, verbose=False)
+    NuSVR(C=1.0, cache_size=200, coef0=0.0, degree=3, gamma='auto', 
+          kernel='rbf', max_iter=-1, nu=0.1, shrinking=True, tol=0.001, 
+          verbose=False)
 
     See also
     --------
@@ -853,7 +854,7 @@ class NuSVR(BaseLibSVM, RegressorMixin):
     """
 
     def __init__(self, nu=0.5, C=1.0, kernel='rbf', degree=3,
-                 gamma=0.0, coef0=0.0, shrinking=True, tol=1e-3,
+                 gamma='auto', coef0=0.0, shrinking=True, tol=1e-3,
                  cache_size=200, verbose=False, max_iter=-1):
 
         super(NuSVR, self).__init__(
@@ -891,9 +892,9 @@ class OneClassSVM(BaseLibSVM):
         Degree of the polynomial kernel function ('poly').
         Ignored by all other kernels.
 
-    gamma : float, optional (default=0.0)
+    gamma : float, optional (default='auto')
         Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.
-        If gamma is 0.0 then 1/n_features will be used instead.
+        If gamma is 'auto' then 1/n_features will be used instead.
 
     coef0 : float, optional (default=0.0)
         Independent term in kernel function.
@@ -942,9 +943,9 @@ class OneClassSVM(BaseLibSVM):
         Constants in decision function.
 
     """
-    def __init__(self, kernel='rbf', degree=3, gamma=0.0, coef0=0.0, tol=1e-3,
-                 nu=0.5, shrinking=True, cache_size=200, verbose=False,
-                 max_iter=-1, random_state=None):
+    def __init__(self, kernel='rbf', degree=3, gamma='auto', coef0=0.0, 
+                 tol=1e-3, nu=0.5, shrinking=True, cache_size=200, 
+                 verbose=False, max_iter=-1, random_state=None):
 
         super(OneClassSVM, self).__init__(
             'one_class', kernel, degree, gamma, coef0, tol, 0., nu, 0.,
diff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py
index 4dea7e003721..512e4fcace7a 100644
--- a/sklearn/utils/estimator_checks.py
+++ b/sklearn/utils/estimator_checks.py
@@ -877,6 +877,7 @@ def check_classifiers_classes(name, Classifier):
         if name == 'BernoulliNB':
             classifier.set_params(binarize=X.mean())
         set_fast_parameters(classifier)
+        set_random_state(classifier)
         # fit
         classifier.fit(X, y_)
 
