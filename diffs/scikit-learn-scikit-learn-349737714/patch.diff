diff --git a/CONTRIBUTING.md b/CONTRIBUTING.md
index 7da8c87ac6b2..ece0ef947ef2 100644
--- a/CONTRIBUTING.md
+++ b/CONTRIBUTING.md
@@ -143,25 +143,11 @@ tools:
   $ pytest --cov sklearn path/to/tests_for_package
   ```
 
--  No pyflakes warnings, check with:
+-  No flake8 warnings, check with:
 
   ```bash
-  $ pip install pyflakes
-  $ pyflakes path/to/module.py
-  ```
-
--  No PEP8 warnings, check with:
-
-  ```bash
-  $ pip install pep8
-  $ pep8 path/to/module.py
-  ```
-
--  AutoPEP8 can help you fix some of the easy redundant errors:
-
-  ```bash
-  $ pip install autopep8
-  $ autopep8 path/to/pep8.py
+  $ pip install flake8
+  $ flake8 path/to/module.py
   ```
 
 Bonus points for contributions that include a performance analysis with
@@ -231,7 +217,7 @@ You can edit the documentation using any text editor and then generate
 the HTML output by typing ``make html`` from the doc/ directory.
 Alternatively, ``make`` can be used to quickly generate the
 documentation without the example gallery. The resulting HTML files will
-be placed in ``_build/html/`` and are viewable in a web browser. See the
+be placed in ``_build/html/stable`` and are viewable in a web browser. See the
 ``README`` file in the ``doc/`` directory for more information.
 
 For building the documentation, you will need
diff --git a/appveyor.yml b/appveyor.yml
index dac1417cafd4..e0989fda8b68 100644
--- a/appveyor.yml
+++ b/appveyor.yml
@@ -25,12 +25,12 @@ environment:
       PYTHON_VERSION: "2.7.8"
       PYTHON_ARCH: "64"
 
-    - PYTHON: "C:\\Python35"
-      PYTHON_VERSION: "3.5.0"
+    - PYTHON: "C:\\Python36"
+      PYTHON_VERSION: "3.6.1"
       PYTHON_ARCH: "32"
 
-    - PYTHON: "C:\\Python35-x64"
-      PYTHON_VERSION: "3.5.0"
+    - PYTHON: "C:\\Python36-x64"
+      PYTHON_VERSION: "3.6.1"
       PYTHON_ARCH: "64"
 
 
diff --git a/build_tools/appveyor/requirements.txt b/build_tools/appveyor/requirements.txt
index a14f4032d770..35c772b52d32 100644
--- a/build_tools/appveyor/requirements.txt
+++ b/build_tools/appveyor/requirements.txt
@@ -7,8 +7,8 @@
 # fix the versions of numpy to force the use of numpy and scipy to use the whl
 # of the rackspace folder instead of trying to install from more recent
 # source tarball published on PyPI
-numpy==1.9.3
-scipy==0.16.0
+numpy==1.13.0
+scipy==0.19.0
 cython
 pytest
 wheel
diff --git a/doc/developers/contributing.rst b/doc/developers/contributing.rst
index 498352a2d2d1..7117d9ade434 100644
--- a/doc/developers/contributing.rst
+++ b/doc/developers/contributing.rst
@@ -285,20 +285,10 @@ You can also check for common programming errors with the following tools:
 
   see also :ref:`testing_coverage`
 
-* No pyflakes warnings, check with::
+* No flake8 warnings, check with::
 
-    $ pip install pyflakes
-    $ pyflakes path/to/module.py
-
-* No PEP8 warnings, check with::
-
-    $ pip install pep8
-    $ pep8 path/to/module.py
-
-* AutoPEP8 can help you fix some of the easy redundant errors::
-
-    $ pip install autopep8
-    $ autopep8 path/to/pep8.py
+    $ pip install flake8
+    $ flake8 path/to/module.py
 
 Bonus points for contributions that include a performance analysis with
 a benchmark script and profiling output (please report on the mailing
@@ -432,13 +422,18 @@ documents live in the source code repository under the ``doc/`` directory.
 You can edit the documentation using any text editor, and then generate the
 HTML output by building the documentation website.
 
-**Building the documentation**
+Building the documentation
+^^^^^^^^^^^^^^^^^^^^^^^^^^
 
 Building the documentation requires the ``sphinx``, ``sphinx-gallery``,
 ``numpydoc``, ``matplotlib``, and ``Pillow`` packages::
 
     pip install sphinx sphinx-gallery numpydoc matplotlib Pillow
 
+To build the documentation, you need to be in the ``doc`` folder::
+
+    cd doc
+
 It also requires having the version of scikit-learn installed that corresponds
 to the documentation, e.g.::
 
@@ -462,9 +457,20 @@ To build the PDF manual, run::
 
     make latexpdf
 
-**When you are writing documentation**, it is important to keep a good
-compromise between mathematical and algorithmic details, and give
-intuition to the reader on what the algorithm does.
+.. warning:: **Sphinx version**
+
+   While we do our best to have the documentation build under as many
+   versions of Sphinx as possible, the different versions tend to
+   behave slightly differently. To get the best results, you should
+   use the same version as the one we used on CircleCI. Look at this
+   `github search <https://github.com/search?utf8=%E2%9C%93&q=sphinx+repo%3Ascikit-learn%2Fscikit-learn+extension%3Ash+path%3Abuild_tools%2Fcircle&type=Code>`_
+   to know the exact version.
+
+Guidelines for writing documentation
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+It is important to keep a good compromise between mathematical and algorithmic
+details, and give intuition to the reader on what the algorithm does.
 
 Basically, to elaborate on the above, it is best to always
 start with a small paragraph with a hand-waving explanation of what the
@@ -489,24 +495,6 @@ documentation with the maths makes it more friendly towards
 users that are just interested in what the feature will do, as
 opposed to how it works "under the hood".
 
-When you change the documentation in a pull request, CircleCI automatically
-builds it. To view the documentation generated by CircleCI:
-
-* navigate to the bottom of your pull request page to see the CI
-  statuses. You may need to click on "Show all checks" to see all the CI
-  statuses.
-* click on the CircleCI status with "python3" in the title.
-* add ``#artifacts`` at the end of the URL. Note: you need to wait for the
-  CircleCI build to finish before being able to look at the artifacts.
-* once the artifacts are visible, navigate to ``doc/_changed.html`` to see a
-  list of documentation pages that are likely to be affected by your pull
-  request. Navigate to ``doc/index.html`` to see the full generated html
-  documentation.
-
-If you often need to look at the documentation generated by CircleCI, e.g. when
-reviewing pull requests, you may find :ref:`this tip
-<viewing_rendered_html_documentation>` very handy.
-
 Finally, follow the formatting rules below to make it consistently good:
 
 * Add "See also" in docstrings for related classes/functions.
@@ -524,15 +512,26 @@ Finally, follow the formatting rules below to make it consistently good:
     * For "References" in docstrings, see the Silhouette Coefficient
       (:func:`sklearn.metrics.silhouette_score`).
 
-.. warning:: **Sphinx version**
+Generated documentation on CircleCI
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
-   While we do our best to have the documentation build under as many
-   versions of Sphinx as possible, the different versions tend to
-   behave slightly differently. To get the best results, you should
-   use the same version as the one we used on CircleCI. Look at this
-   `github search <https://github.com/search?utf8=%E2%9C%93&q=sphinx+repo%3Ascikit-learn%2Fscikit-learn+extension%3Ash+path%3Abuild_tools%2Fcircle&type=Code>`_
-   to know the exact version.
+When you change the documentation in a pull request, CircleCI automatically
+builds it. To view the documentation generated by CircleCI:
 
+* navigate to the bottom of your pull request page to see the CI
+  statuses. You may need to click on "Show all checks" to see all the CI
+  statuses.
+* click on the CircleCI status with "python3" in the title.
+* add ``#artifacts`` at the end of the URL. Note: you need to wait for the
+  CircleCI build to finish before being able to look at the artifacts.
+* once the artifacts are visible, navigate to ``doc/_changed.html`` to see a
+  list of documentation pages that are likely to be affected by your pull
+  request. Navigate to ``doc/index.html`` to see the full generated html
+  documentation.
+
+If you often need to look at the documentation generated by CircleCI, e.g. when
+reviewing pull requests, you may find :ref:`this tip
+<viewing_rendered_html_documentation>` very handy.
 
 .. _testing_coverage:
 
diff --git a/doc/developers/utilities.rst b/doc/developers/utilities.rst
index 39c0925de0d4..6f0d02f2aed4 100644
--- a/doc/developers/utilities.rst
+++ b/doc/developers/utilities.rst
@@ -97,7 +97,7 @@ Efficient Linear Algebra & Array Operations
   number of components.
 
 - :func:`arrayfuncs.cholesky_delete`:
-  (used in :func:`sklearn.linear_model.least_angle.lars_path`)  Remove an
+  (used in :func:`sklearn.linear_model.lars_path`)  Remove an
   item from a cholesky factorization.
 
 - :func:`arrayfuncs.min_pos`: (used in ``sklearn.linear_model.least_angle``)
@@ -120,7 +120,7 @@ Efficient Linear Algebra & Array Operations
   used in :func:`shuffle`, below.
 
 - :func:`shuffle`: Shuffle arrays or sparse matrices in a consistent way.
-  Used in ``sklearn.cluster.k_means``.
+  Used in :func:`sklearn.cluster.k_means`.
 
 
 Efficient Random Sampling
@@ -140,10 +140,10 @@ efficiently process ``scipy.sparse`` data.
 - :func:`sparsefuncs.mean_variance_axis`: compute the means and
   variances along a specified axis of a CSR matrix.
   Used for normalizing the tolerance stopping criterion in
-  :class:`sklearn.cluster.k_means_.KMeans`.
+  :class:`sklearn.cluster.KMeans`.
 
-- :func:`sparsefuncs.inplace_csr_row_normalize_l1` and
-  :func:`sparsefuncs.inplace_csr_row_normalize_l2`: can be used to normalize
+- :func:`sparsefuncs_fast.inplace_csr_row_normalize_l1` and
+  :func:`sparsefuncs_fast.inplace_csr_row_normalize_l2`: can be used to normalize
   individual sparse samples to unit L1 or L2 norm as done in
   :class:`sklearn.preprocessing.Normalizer`.
 
@@ -200,9 +200,6 @@ Multiclass and multilabel utility function
 - :func:`multiclass.is_multilabel`: Helper function to check if the task
   is a multi-label classification one.
 
-- :func:`multiclass.is_label_indicator_matrix`: Helper function to check if
-  a classification output is in label indicator matrix format.
-
 - :func:`multiclass.unique_labels`: Helper function to extract an ordered
   array of unique labels from different formats of target.
 
@@ -211,8 +208,8 @@ Helper Functions
 ================
 
 - :class:`gen_even_slices`: generator to create ``n``-packs of slices going up
-  to ``n``.  Used in ``sklearn.decomposition.dict_learning`` and
-  ``sklearn.cluster.k_means``.
+  to ``n``.  Used in :func:`sklearn.decomposition.dict_learning` and
+  :func:`sklearn.cluster.k_means`.
 
 - :func:`safe_mask`: Helper function to convert a mask to the format expected
   by the numpy array or scipy sparse matrix on which to use it (sparse
@@ -250,4 +247,4 @@ Warnings and Exceptions
 - :class:`deprecated`: Decorator to mark a function or class as deprecated.
 
 - :class:`sklearn.exceptions.ConvergenceWarning`: Custom warning to catch
-  convergence problems. Used in ``sklearn.covariance.graph_lasso``.
+  convergence problems. Used in ``sklearn.covariance.graphical_lasso``.
diff --git a/doc/glossary.rst b/doc/glossary.rst
index 0c883a0291c4..9105251f5980 100644
--- a/doc/glossary.rst
+++ b/doc/glossary.rst
@@ -1547,9 +1547,15 @@ functions or non-estimator constructors.
         their number.
 
         :term:`partial_fit` also retains the model between calls, but differs:
-        with ``warm_start`` the parameters change and the data is constant
-        across calls to ``fit``; with ``partial_fit``, the mini-batch of data
-        changes and model parameters stay fixed.
+        with ``warm_start`` the parameters change and the data is
+        (more-or-less) constant across calls to ``fit``; with ``partial_fit``,
+        the mini-batch of data changes and model parameters stay fixed.
+
+        There are cases where you want to use ``warm_start`` to fit on
+        different, but closely related data. For example, one may initially fit
+        to a subset of the data, then fine-tune the parameter search on the
+        full dataset. For classification, all data in a sequence of
+        ``warm_start`` calls to ``fit`` must include samples from each class.
 
 .. _glossary_attributes:
 
diff --git a/doc/modules/classes.rst b/doc/modules/classes.rst
index 73b1777e4311..a6174d282a36 100644
--- a/doc/modules/classes.rst
+++ b/doc/modules/classes.rst
@@ -159,8 +159,8 @@ Classes
 
    covariance.EmpiricalCovariance
    covariance.EllipticEnvelope
-   covariance.GraphLasso
-   covariance.GraphLassoCV
+   covariance.GraphicalLasso
+   covariance.GraphicalLassoCV
    covariance.LedoitWolf
    covariance.MinCovDet
    covariance.OAS
@@ -171,7 +171,7 @@ Classes
    :template: function.rst
 
    covariance.empirical_covariance
-   covariance.graph_lasso
+   covariance.graphical_lasso
    covariance.ledoit_wolf
    covariance.oas
    covariance.shrunk_covariance
@@ -1211,7 +1211,6 @@ Model validation
 
    preprocessing.Binarizer
    preprocessing.FunctionTransformer
-   preprocessing.Imputer
    preprocessing.KernelCenterer
    preprocessing.LabelBinarizer
    preprocessing.LabelEncoder
@@ -1382,24 +1381,47 @@ Low-level methods
 
 .. currentmodule:: sklearn
 
+.. autosummary::
+   :toctree: generated/
+   :template: class.rst
+
+   utils.testing.mock_mldata_urlopen
+
 .. autosummary::
    :toctree: generated/
    :template: function.rst
 
+   utils.arrayfuncs.cholesky_delete
+   utils.arrayfuncs.min_pos
    utils.as_float_array
    utils.assert_all_finite
+   utils.bench.total_seconds
    utils.check_X_y
    utils.check_array
    utils.check_consistent_length
    utils.check_random_state
    utils.class_weight.compute_class_weight
    utils.class_weight.compute_sample_weight
+   utils.deprecated
    utils.estimator_checks.check_estimator
    utils.extmath.safe_sparse_dot
+   utils.extmath.randomized_range_finder
+   utils.extmath.randomized_svd
+   utils.extmath.fast_logdet
+   utils.extmath.density
+   utils.extmath.weighted_mode
+   utils.gen_even_slices
+   utils.graph.single_source_shortest_path_length
+   utils.graph_shortest_path.graph_shortest_path
    utils.indexable
    utils.multiclass.type_of_target
+   utils.multiclass.is_multilabel
+   utils.multiclass.unique_labels
+   utils.murmurhash3_32
    utils.resample
    utils.safe_indexing
+   utils.safe_mask
+   utils.safe_sqr
    utils.shuffle
    utils.sparsefuncs.incr_mean_variance_axis
    utils.sparsefuncs.inplace_column_scale
@@ -1407,11 +1429,19 @@ Low-level methods
    utils.sparsefuncs.inplace_swap_row
    utils.sparsefuncs.inplace_swap_column
    utils.sparsefuncs.mean_variance_axis
+   utils.sparsefuncs.inplace_csr_column_scale
+   utils.sparsefuncs_fast.inplace_csr_row_normalize_l1
+   utils.sparsefuncs_fast.inplace_csr_row_normalize_l2
+   utils.random.sample_without_replacement
    utils.validation.check_is_fitted
    utils.validation.check_memory
    utils.validation.check_symmetric
    utils.validation.column_or_1d
    utils.validation.has_fit_parameter
+   utils.testing.assert_in
+   utils.testing.assert_not_in
+   utils.testing.assert_raise_message
+   utils.testing.all_estimators
 
 Recently deprecated
 ===================
@@ -1424,8 +1454,16 @@ To be removed in 0.22
    :toctree: generated/
    :template: deprecated_class.rst
 
+   covariance.GraphLasso
+   covariance.GraphLassoCV
    preprocessing.Imputer
 
+.. autosummary::
+   :toctree: generated/
+   :template: deprecated_function.rst
+
+   covariance.graph_lasso
+
 
 To be removed in 0.21
 ---------------------
diff --git a/doc/modules/clustering.rst b/doc/modules/clustering.rst
index 2a3d93e26300..e6c5342fb14e 100644
--- a/doc/modules/clustering.rst
+++ b/doc/modules/clustering.rst
@@ -134,7 +134,7 @@ although they live in the same space.
 The K-means algorithm aims to choose centroids
 that minimise the *inertia*, or within-cluster sum of squared criterion:
 
-.. math:: \sum_{i=0}^{n}\min_{\mu_j \in C}(||x_j - \mu_i||^2)
+.. math:: \sum_{i=0}^{n}\min_{\mu_j \in C}(||x_i - \mu_j||^2)
 
 Inertia, or the within-cluster sum of squares criterion,
 can be recognized as a measure of how internally coherent clusters are.
@@ -469,15 +469,15 @@ function of the gradient of the image.
  * :ref:`sphx_glr_auto_examples_cluster_plot_segmentation_toy.py`: Segmenting objects
    from a noisy background using spectral clustering.
 
- * :ref:`sphx_glr_auto_examples_cluster_plot_face_segmentation.py`: Spectral clustering
-   to split the image of the raccoon face in regions.
+ * :ref:`sphx_glr_auto_examples_cluster_plot_coin_segmentation.py`: Spectral clustering
+   to split the image of coins in regions.
 
-.. |face_kmeans| image:: ../auto_examples/cluster/images/sphx_glr_plot_face_segmentation_001.png
-    :target: ../auto_examples/cluster/plot_face_segmentation.html
+.. |coin_kmeans| image:: ../auto_examples/cluster/images/sphx_glr_plot_coin_segmentation_001.png
+    :target: ../auto_examples/cluster/plot_coin_segmentation.html
     :scale: 65
 
-.. |face_discretize| image:: ../auto_examples/cluster/images/sphx_glr_plot_face_segmentation_002.png
-    :target: ../auto_examples/cluster/plot_face_segmentation.html
+.. |coin_discretize| image:: ../auto_examples/cluster/images/sphx_glr_plot_coin_segmentation_002.png
+    :target: ../auto_examples/cluster/plot_coin_segmentation.html
     :scale: 65
 
 Different label assignment strategies
@@ -495,7 +495,7 @@ geometrical shape.
 =====================================  =====================================
  ``assign_labels="kmeans"``              ``assign_labels="discretize"``
 =====================================  =====================================
-|face_kmeans|                          |face_discretize|
+|coin_kmeans|                          |coin_discretize|
 =====================================  =====================================
 
 Spectral Clustering Graphs
@@ -631,12 +631,12 @@ merging to nearest neighbors as in :ref:`this example
 <sphx_glr_auto_examples_cluster_plot_agglomerative_clustering.py>`, or
 using :func:`sklearn.feature_extraction.image.grid_to_graph` to
 enable only merging of neighboring pixels on an image, as in the
-:ref:`raccoon face <sphx_glr_auto_examples_cluster_plot_face_ward_segmentation.py>` example.
+:ref:`coin <sphx_glr_auto_examples_cluster_plot_coin_ward_segmentation.py>` example.
 
 .. topic:: Examples:
 
- * :ref:`sphx_glr_auto_examples_cluster_plot_face_ward_segmentation.py`: Ward clustering
-   to split the image of a raccoon face in regions.
+ * :ref:`sphx_glr_auto_examples_cluster_plot_coin_ward_segmentation.py`: Ward clustering
+   to split the image of coins in regions.
 
  * :ref:`sphx_glr_auto_examples_cluster_plot_ward_structured_vs_unstructured.py`: Example of
    Ward algorithm on a swiss-roll, comparison of structured approaches
diff --git a/doc/modules/covariance.rst b/doc/modules/covariance.rst
index 5d2cb249e708..8325720fe565 100644
--- a/doc/modules/covariance.rst
+++ b/doc/modules/covariance.rst
@@ -202,9 +202,9 @@ situation, or for very correlated data, they can be numerically unstable.
 In addition, unlike shrinkage estimators, sparse estimators are able to
 recover off-diagonal structure.
 
-The :class:`GraphLasso` estimator uses an l1 penalty to enforce sparsity on
+The :class:`GraphicalLasso` estimator uses an l1 penalty to enforce sparsity on
 the precision matrix: the higher its ``alpha`` parameter, the more sparse
-the precision matrix. The corresponding :class:`GraphLassoCV` object uses
+the precision matrix. The corresponding :class:`GraphicalLassoCV` object uses
 cross-validation to automatically set the ``alpha`` parameter.
 
 .. figure:: ../auto_examples/covariance/images/sphx_glr_plot_sparse_cov_001.png
@@ -223,7 +223,7 @@ cross-validation to automatically set the ``alpha`` parameter.
    that:
 
    * Recovery is easier from a correlation matrix than a covariance
-     matrix: standardize your observations before running :class:`GraphLasso`
+     matrix: standardize your observations before running :class:`GraphicalLasso`
 
    * If the underlying graph has nodes with much more connections than
      the average node, the algorithm will miss some of these connections.
@@ -233,7 +233,7 @@ cross-validation to automatically set the ``alpha`` parameter.
 
    * Even if you are in favorable recovery conditions, the alpha
      parameter chosen by cross-validation (e.g. using the
-     :class:`GraphLassoCV` object) will lead to selecting too many edges.
+     :class:`GraphicalLassoCV` object) will lead to selecting too many edges.
      However, the relevant edges will have heavier weights than the
      irrelevant ones.
 
diff --git a/doc/modules/cross_validation.rst b/doc/modules/cross_validation.rst
index 591dbbd43964..ca7d570ab600 100644
--- a/doc/modules/cross_validation.rst
+++ b/doc/modules/cross_validation.rst
@@ -182,7 +182,8 @@ The ``cross_validate`` function differs from ``cross_val_score`` in two ways -
 
 - It allows specifying multiple metrics for evaluation.
 
-- It returns a dict containing training scores, fit-times and score-times in
+- It returns a dict containing fit-times, score-times
+  (and optionally training scores as well as fitted estimators) in
   addition to the test score.
 
 For single metric evaluation, where the scoring parameter is a string,
@@ -196,6 +197,9 @@ following keys -
 for all the scorers. If train scores are not needed, this should be set to
 ``False`` explicitly.
 
+You may also retain the estimator fitted on each training set by setting
+``return_estimator=True``.
+
 The multiple metrics can be specified either as a list, tuple or set of
 predefined scorer names::
 
@@ -226,9 +230,10 @@ Or as a dict mapping scorer name to a predefined or custom scoring function::
 Here is an example of ``cross_validate`` using a single metric::
 
     >>> scores = cross_validate(clf, iris.data, iris.target,
-    ...                         scoring='precision_macro')
+    ...                         scoring='precision_macro',
+    ...                         return_estimator=True)
     >>> sorted(scores.keys())
-    ['fit_time', 'score_time', 'test_score', 'train_score']
+    ['estimator', 'fit_time', 'score_time', 'test_score', 'train_score']
 
 
 Obtaining predictions by cross-validation
@@ -260,7 +265,7 @@ section.
     * :ref:`sphx_glr_auto_examples_feature_selection_plot_rfe_with_cross_validation.py`,
     * :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_digits.py`,
     * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`,
-    * :ref:`sphx_glr_auto_examples_plot_cv_predict.py`,
+    * :ref:`sphx_glr_auto_examples_model_selection_plot_cv_predict.py`,
     * :ref:`sphx_glr_auto_examples_model_selection_plot_nested_cross_validation_iris.py`.
 
 Cross validation iterators
diff --git a/doc/modules/feature_extraction.rst b/doc/modules/feature_extraction.rst
index 1bd1873c4b05..eb8b65fbaa84 100644
--- a/doc/modules/feature_extraction.rst
+++ b/doc/modules/feature_extraction.rst
@@ -1011,8 +1011,8 @@ features or samples. For instance Ward clustering
 (:ref:`hierarchical_clustering`) can cluster together only neighboring pixels
 of an image, thus forming contiguous patches:
 
-.. figure:: ../auto_examples/cluster/images/sphx_glr_plot_face_ward_segmentation_001.png
-   :target: ../auto_examples/cluster/plot_face_ward_segmentation.html
+.. figure:: ../auto_examples/cluster/images/sphx_glr_plot_coin_ward_segmentation_001.png
+   :target: ../auto_examples/cluster/plot_coin_ward_segmentation.html
    :align: center
    :scale: 40
 
@@ -1030,7 +1030,7 @@ or similarity matrices.
 
 .. note:: **Examples**
 
-   * :ref:`sphx_glr_auto_examples_cluster_plot_face_ward_segmentation.py`
+   * :ref:`sphx_glr_auto_examples_cluster_plot_coin_ward_segmentation.py`
 
    * :ref:`sphx_glr_auto_examples_cluster_plot_segmentation_toy.py`
 
diff --git a/doc/modules/model_evaluation.rst b/doc/modules/model_evaluation.rst
index 37246e6bb7ea..03aacf194df7 100644
--- a/doc/modules/model_evaluation.rst
+++ b/doc/modules/model_evaluation.rst
@@ -174,24 +174,23 @@ Here is an example of building custom scorers, and of using the
 ``greater_is_better`` parameter::
 
     >>> import numpy as np
-    >>> def my_custom_loss_func(ground_truth, predictions):
-    ...     diff = np.abs(ground_truth - predictions).max()
+    >>> def my_custom_loss_func(y_true, y_pred):
+    ...     diff = np.abs(y_true - y_pred).max()
     ...     return np.log(1 + diff)
     ...
-    >>> # loss_func will negate the return value of my_custom_loss_func,
-    >>> #  which will be np.log(2), 0.693, given the values for ground_truth
-    >>> #  and predictions defined below.
-    >>> loss  = make_scorer(my_custom_loss_func, greater_is_better=False)
-    >>> score = make_scorer(my_custom_loss_func, greater_is_better=True)
-    >>> ground_truth = [[1], [1]]
-    >>> predictions  = [0, 1]
+    >>> # score will negate the return value of my_custom_loss_func,
+    >>> # which will be np.log(2), 0.693, given the values for X
+    >>> # and y defined below.
+    >>> score = make_scorer(my_custom_loss_func, greater_is_better=False)
+    >>> X = [[1], [1]]
+    >>> y  = [0, 1]
     >>> from sklearn.dummy import DummyClassifier
     >>> clf = DummyClassifier(strategy='most_frequent', random_state=0)
-    >>> clf = clf.fit(ground_truth, predictions)
-    >>> loss(clf,ground_truth, predictions) # doctest: +ELLIPSIS
-    -0.69...
-    >>> score(clf,ground_truth, predictions) # doctest: +ELLIPSIS
+    >>> clf = clf.fit(X, y)
+    >>> my_custom_loss_func(clf.predict(X), y) # doctest: +ELLIPSIS
     0.69...
+    >>> score(clf, X, y) # doctest: +ELLIPSIS
+    -0.69...
 
 
 .. _diy_scoring:
@@ -1173,6 +1172,10 @@ F1 score, ROC doesn't require optimizing a threshold for each label. The
 :func:`roc_auc_score` function can also be used in multi-class classification,
 if the predicted outputs have been binarized.
 
+In applications where a high false positive rate is not tolerable the parameter
+``max_fpr`` of :func:`roc_auc_score` can be used to summarize the ROC curve up
+to the given limit.
+
 
 .. image:: ../auto_examples/model_selection/images/sphx_glr_plot_roc_002.png
    :target: ../auto_examples/model_selection/plot_roc.html
diff --git a/doc/modules/tree.rst b/doc/modules/tree.rst
index 3f577795e24b..c6a5a74eb17f 100644
--- a/doc/modules/tree.rst
+++ b/doc/modules/tree.rst
@@ -490,17 +490,17 @@ Mean Squared Error:
 
 .. math::
 
-    c_m = \frac{1}{N_m} \sum_{i \in N_m} y_i
+    \bar{y}_m = \frac{1}{N_m} \sum_{i \in N_m} y_i
 
-    H(X_m) = \frac{1}{N_m} \sum_{i \in N_m} (y_i - c_m)^2
+    H(X_m) = \frac{1}{N_m} \sum_{i \in N_m} (y_i - \bar{y}_m)^2
 
 Mean Absolute Error:
 
 .. math::
 
-    \bar{y_m} = \frac{1}{N_m} \sum_{i \in N_m} y_i
+    \bar{y}_m = \frac{1}{N_m} \sum_{i \in N_m} y_i
 
-    H(X_m) = \frac{1}{N_m} \sum_{i \in N_m} |y_i - \bar{y_m}|
+    H(X_m) = \frac{1}{N_m} \sum_{i \in N_m} |y_i - \bar{y}_m|
 
 where :math:`X_m` is the training data in node :math:`m`
 
diff --git a/doc/themes/scikit-learn/layout.html b/doc/themes/scikit-learn/layout.html
index 1014a97b7cf5..79ddd0809301 100644
--- a/doc/themes/scikit-learn/layout.html
+++ b/doc/themes/scikit-learn/layout.html
@@ -180,8 +180,8 @@
 		      <img src="_images/sphx_glr_plot_gmm_pdf_thumb.png"></a>
 		  </div>
 		  <div class="item">
-		    <a href="{{ pathto('auto_examples/cluster/plot_face_ward_segmentation') }}">
-		      <img src="_images/sphx_glr_plot_face_ward_segmentation_thumb.png"></a>
+		    <a href="{{ pathto('auto_examples/cluster/plot_coin_ward_segmentation') }}">
+		      <img src="_images/sphx_glr_plot_coin_ward_segmentation_thumb.png"></a>
 		  </div>
 		</div>
 		<!-- Carousel nav -->
diff --git a/doc/tutorial/basic/tutorial.rst b/doc/tutorial/basic/tutorial.rst
index bc5df4d74f66..3ea0e5fd899d 100644
--- a/doc/tutorial/basic/tutorial.rst
+++ b/doc/tutorial/basic/tutorial.rst
@@ -21,7 +21,7 @@ more than a single number and, for instance, a multi-dimensional entry
 (aka `multivariate <https://en.wikipedia.org/wiki/Multivariate_random_variable>`_
 data), it is said to have several attributes or **features**.
 
-We can separate learning problems in a few large categories:
+Learning problems fall into a few categories:
 
  * `supervised learning <https://en.wikipedia.org/wiki/Supervised_learning>`_,
    in which the data comes with additional attributes that we want to predict
@@ -33,8 +33,8 @@ We can separate learning problems in a few large categories:
       <https://en.wikipedia.org/wiki/Classification_in_machine_learning>`_:
       samples belong to two or more classes and we
       want to learn from already labeled data how to predict the class
-      of unlabeled data. An example of classification problem would
-      be the handwritten digit recognition example, in which the aim is
+      of unlabeled data. An example of a classification problem would
+      be handwritten digit recognition, in which the aim is
       to assign each input vector to one of a finite number of discrete
       categories.  Another way to think of classification is as a discrete
       (as opposed to continuous) form of supervised learning where one has a
@@ -62,11 +62,12 @@ We can separate learning problems in a few large categories:
 .. topic:: Training set and testing set
 
     Machine learning is about learning some properties of a data set
-    and applying them to new data. This is why a common practice in
-    machine learning to evaluate an algorithm is to split the data
-    at hand into two sets, one that we call the **training set** on which
-    we learn data properties and one that we call the **testing set**
-    on which we test these properties.
+    and then testing those properties against another data set. A common
+    practice in machine learning is to evaluate an algorithm by splitting a data
+    set into two. We call one of those sets the **training set**, on which we
+    learn some properties; we call the other set the **testing set**, on which
+    we test the learned properties.
+
 
 .. _loading_example_dataset:
 
@@ -153,30 +154,30 @@ the classes to which unseen samples belong.
 In scikit-learn, an estimator for classification is a Python object that
 implements the methods ``fit(X, y)`` and ``predict(T)``.
 
-An example of an estimator is the class ``sklearn.svm.SVC`` that
+An example of an estimator is the class ``sklearn.svm.SVC``, which
 implements `support vector classification
 <https://en.wikipedia.org/wiki/Support_vector_machine>`_. The
-constructor of an estimator takes as arguments the parameters of the
-model, but for the time being, we will consider the estimator as a black
-box::
+estimator's constructor takes as arguments the model's parameters.
+
+For now, we will consider the estimator as a black box::
 
   >>> from sklearn import svm
   >>> clf = svm.SVC(gamma=0.001, C=100.)
 
 .. topic:: Choosing the parameters of the model
 
-  In this example we set the value of ``gamma`` manually. It is possible
-  to automatically find good values for the parameters by using tools
+  In this example, we set the value of ``gamma`` manually.
+  To find good values for these parameters, we can use tools
   such as :ref:`grid search <grid_search>` and :ref:`cross validation
   <cross_validation>`.
 
-We call our estimator instance ``clf``, as it is a classifier. It now must
-be fitted to the model, that is, it must *learn* from the model. This is
-done by passing our training set to the ``fit`` method. As a training
-set, let us use all the images of our dataset apart from the last
-one. We select this training set with the ``[:-1]`` Python syntax,
-which produces a new array that contains all but
-the last entry of ``digits.data``::
+The ``clf`` (for classifier) estimator instance is first
+fitted to the model; that is, it must *learn* from the model. This is
+done by passing our training set to the ``fit`` method. For the training
+set, we'll use all the images from our dataset, except for the last
+image, which we'll reserve for our predicting. We select the training set with
+the ``[:-1]`` Python syntax, which produces a new array that contains all but
+the last item from ``digits.data``::
 
   >>> clf.fit(digits.data[:-1], digits.target[:-1])  # doctest: +NORMALIZE_WHITESPACE
   SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0,
@@ -184,21 +185,22 @@ the last entry of ``digits.data``::
     max_iter=-1, probability=False, random_state=None, shrinking=True,
     tol=0.001, verbose=False)
 
-Now you can predict new values, in particular, we can ask to the
-classifier what is the digit of our last image in the ``digits`` dataset,
-which we have not used to train the classifier::
+Now you can *predict* new values. In this case, you'll predict using the last
+image from ``digits.data``. By predicting, you'll determine the image from the 
+training set that best matches the last image.
+
 
   >>> clf.predict(digits.data[-1:])
   array([8])
 
-The corresponding image is the following:
+The corresponding image is:
 
 .. image:: /auto_examples/datasets/images/sphx_glr_plot_digits_last_image_001.png
     :target: ../../auto_examples/datasets/plot_digits_last_image.html
     :align: center
     :scale: 50
 
-As you can see, it is a challenging task: the images are of poor
+As you can see, it is a challenging task: after all, the images are of poor
 resolution. Do you agree with the classifier?
 
 A complete example of this classification problem is available as an
@@ -210,7 +212,7 @@ Model persistence
 -----------------
 
 It is possible to save a model in scikit-learn by using Python's built-in
-persistence model, namely `pickle <https://docs.python.org/2/library/pickle.html>`_::
+persistence model, `pickle <https://docs.python.org/2/library/pickle.html>`_::
 
   >>> from sklearn import svm
   >>> from sklearn import datasets
@@ -232,14 +234,14 @@ persistence model, namely `pickle <https://docs.python.org/2/library/pickle.html
   0
 
 In the specific case of scikit-learn, it may be more interesting to use
-joblib's replacement of pickle (``joblib.dump`` & ``joblib.load``),
-which is more efficient on big data, but can only pickle to the disk
+joblib's replacement for pickle (``joblib.dump`` & ``joblib.load``),
+which is more efficient on big data but it can only pickle to the disk
 and not to a string::
 
   >>> from sklearn.externals import joblib
   >>> joblib.dump(clf, 'filename.pkl') # doctest: +SKIP
 
-Later you can load back the pickled model (possibly in another Python process)
+Later, you can reload the pickled model (possibly in another Python process)
 with::
 
   >>> clf = joblib.load('filename.pkl') # doctest:+SKIP
@@ -283,7 +285,7 @@ Unless otherwise specified, input will be cast to ``float64``::
 In this example, ``X`` is ``float32``, which is cast to ``float64`` by
 ``fit_transform(X)``.
 
-Regression targets are cast to ``float64``, classification targets are
+Regression targets are cast to ``float64`` and classification targets are
 maintained::
 
     >>> from sklearn import datasets
diff --git a/doc/tutorial/statistical_inference/unsupervised_learning.rst b/doc/tutorial/statistical_inference/unsupervised_learning.rst
index cef8fbe7809d..d771daa4f00c 100644
--- a/doc/tutorial/statistical_inference/unsupervised_learning.rst
+++ b/doc/tutorial/statistical_inference/unsupervised_learning.rst
@@ -177,12 +177,12 @@ This can be useful, for instance, to retrieve connected regions (sometimes
 also referred to as connected components) when
 clustering an image:
 
-.. image:: /auto_examples/cluster/images/sphx_glr_plot_face_ward_segmentation_001.png
-    :target: ../../auto_examples/cluster/plot_face_ward_segmentation.html
+.. image:: /auto_examples/cluster/images/sphx_glr_plot_coin_ward_segmentation_001.png
+    :target: ../../auto_examples/cluster/plot_coin_ward_segmentation.html
     :scale: 40
     :align: right
 
-.. literalinclude:: ../../auto_examples/cluster/plot_face_ward_segmentation.py
+.. literalinclude:: ../../auto_examples/cluster/plot_coin_ward_segmentation.py
     :lines: 21-45
 
 ..
diff --git a/doc/tutorial/text_analytics/working_with_text_data.rst b/doc/tutorial/text_analytics/working_with_text_data.rst
index 8077a10ad7a0..cd222386d79c 100644
--- a/doc/tutorial/text_analytics/working_with_text_data.rst
+++ b/doc/tutorial/text_analytics/working_with_text_data.rst
@@ -5,7 +5,7 @@ Working With Text Data
 ======================
 
 The goal of this guide is to explore some of the main ``scikit-learn``
-tools on a single practical task: analysing a collection of text
+tools on a single practical task: analyzing a collection of text
 documents (newsgroups posts) on twenty different topics.
 
 In this section we will see how to:
@@ -20,22 +20,23 @@ In this section we will see how to:
     the feature extraction components and the classifier
 
 
-
 Tutorial setup
 --------------
 
-To get started with this tutorial, you firstly must have the
-*scikit-learn* and all of its required dependencies installed.
+To get started with this tutorial, you must first install
+*scikit-learn* and all of its required dependencies.
 
 Please refer to the :ref:`installation instructions <installation-instructions>`
-page for more information and for per-system instructions.
+page for more information and for system-specific instructions.
 
-The source of this tutorial can be found within your
-scikit-learn folder::
+The source of this tutorial can be found within your scikit-learn folder::
 
     scikit-learn/doc/tutorial/text_analytics/
 
-The tutorial folder, should contain the following folders:
+The source can also be found `on Github
+<https://github.com/scikit-learn/scikit-learn/tree/master/doc/tutorial/text_analytics>`_.
+
+The tutorial folder should contain the following sub-folders:
 
   * ``*.rst files`` - the source of the tutorial document written with sphinx
 
@@ -53,7 +54,7 @@ the original skeletons intact::
 
     % cp -r skeletons work_directory/sklearn_tut_workspace
 
-Machine Learning algorithms need data. Go to each ``$TUTORIAL_HOME/data``
+Machine learning algorithms need data. Go to each ``$TUTORIAL_HOME/data``
 sub-folder and run the ``fetch_data.py`` script from there (after
 having read them first).
 
@@ -82,8 +83,8 @@ description, quoted from the `website
 
 In the following we will use the built-in dataset loader for 20 newsgroups
 from scikit-learn. Alternatively, it is possible to download the dataset
-manually from the web-site and use the :func:`sklearn.datasets.load_files`
-function by pointing it to the ``20news-bydate-train`` subfolder of the
+manually from the website and use the :func:`sklearn.datasets.load_files`
+function by pointing it to the ``20news-bydate-train`` sub-folder of the
 uncompressed archive folder.
 
 In order to get faster execution times for this first example we will
@@ -154,10 +155,10 @@ It is possible to get back the category names as follows::
   sci.med
   sci.med
 
-You can notice that the samples have been shuffled randomly (with
-a fixed RNG seed): this is useful if you select only the first
-samples to quickly train a model and get a first idea of the results
-before re-training on the complete dataset later.
+You might have noticed that the samples were shuffled randomly when we called
+``fetch_20newsgroups(..., shuffle=True, random_state=42)``: this is useful if 
+you wish to select only a subset of samples to quickly train a model and get a 
+first idea of the results before re-training on the complete dataset later.
 
 
 Extracting features from text files
@@ -172,26 +173,26 @@ turn the text content into numerical feature vectors.
 Bags of words
 ~~~~~~~~~~~~~
 
-The most intuitive way to do so is the bags of words representation:
+The most intuitive way to do so is to use a bags of words representation:
 
-  1. assign a fixed integer id to each word occurring in any document
+  1. Assign a fixed integer id to each word occurring in any document
      of the training set (for instance by building a dictionary
      from words to integer indices).
 
-  2. for each document ``#i``, count the number of occurrences of each
+  2. For each document ``#i``, count the number of occurrences of each
      word ``w`` and store it in ``X[i, j]`` as the value of feature
-     ``#j`` where ``j`` is the index of word ``w`` in the dictionary
+     ``#j`` where ``j`` is the index of word ``w`` in the dictionary.
 
 The bags of words representation implies that ``n_features`` is
 the number of distinct words in the corpus: this number is typically
 larger than 100,000.
 
-If ``n_samples == 10000``, storing ``X`` as a numpy array of type
+If ``n_samples == 10000``, storing ``X`` as a NumPy array of type
 float32 would require 10000 x 100000 x 4 bytes = **4GB in RAM** which
 is barely manageable on today's computers.
 
 Fortunately, **most values in X will be zeros** since for a given
-document less than a couple thousands of distinct words will be
+document less than a few thousand distinct words will be
 used. For this reason we say that bags of words are typically
 **high-dimensional sparse datasets**. We can save a lot of memory by
 only storing the non-zero parts of the feature vectors in memory.
@@ -203,8 +204,9 @@ and ``scikit-learn`` has built-in support for these structures.
 Tokenizing text with ``scikit-learn``
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
-Text preprocessing, tokenizing and filtering of stopwords are included in a high level component that is able to build a
-dictionary of features and transform documents to feature vectors::
+Text preprocessing, tokenizing and filtering of stopwords are all included
+in :class:`CountVectorizer`, which builds a dictionary of features and 
+transforms documents to feature vectors::
 
   >>> from sklearn.feature_extraction.text import CountVectorizer
   >>> count_vect = CountVectorizer()
@@ -212,8 +214,9 @@ dictionary of features and transform documents to feature vectors::
   >>> X_train_counts.shape
   (2257, 35788)
 
-:class:`CountVectorizer` supports counts of N-grams of words or consecutive characters.
-Once fitted, the vectorizer has built a dictionary of feature indices::
+:class:`CountVectorizer` supports counts of N-grams of words or consecutive 
+characters. Once fitted, the vectorizer has built a dictionary of feature 
+indices::
 
   >>> count_vect.vocabulary_.get(u'algorithm')
   4690
@@ -254,7 +257,8 @@ Inverse Document Frequency".
 .. _`tf–idf`: https://en.wikipedia.org/wiki/Tf-idf
 
 
-Both **tf** and **tf–idf** can be computed as follows::
+Both **tf** and **tf–idf** can be computed as follows using
+:class:`TfidfTransformer`::
 
   >>> from sklearn.feature_extraction.text import TfidfTransformer
   >>> tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)
@@ -311,7 +315,7 @@ Building a pipeline
 -------------------
 
 In order to make the vectorizer => transformer => classifier easier
-to work with, ``scikit-learn`` provides a ``Pipeline`` class that behaves
+to work with, ``scikit-learn`` provides a :class:`~sklearn.pipeline.Pipeline` class that behaves
 like a compound classifier::
 
   >>> from sklearn.pipeline import Pipeline
@@ -321,7 +325,7 @@ like a compound classifier::
   ... ])
 
 The names ``vect``, ``tfidf`` and ``clf`` (classifier) are arbitrary.
-We shall see their use in the section on grid search, below.
+We will use them to perform grid search for suitable hyperparameters below. 
 We can now train the model with a single command::
 
   >>> text_clf.fit(twenty_train.data, twenty_train.target)  # doctest: +ELLIPSIS
@@ -339,13 +343,13 @@ Evaluating the predictive accuracy of the model is equally easy::
   >>> docs_test = twenty_test.data
   >>> predicted = text_clf.predict(docs_test)
   >>> np.mean(predicted == twenty_test.target)            # doctest: +ELLIPSIS
-  0.834...
+  0.8348...
 
-I.e., we achieved 83.4% accuracy. Let's see if we can do better with a
+We achieved 83.5% accuracy. Let's see if we can do better with a
 linear :ref:`support vector machine (SVM) <svm>`,
 which is widely regarded as one of
 the best text classification algorithms (although it's also a bit slower
-than naïve Bayes). We can change the learner by just plugging a different
+than naïve Bayes). We can change the learner by simply plugging a different
 classifier object into our pipeline::
 
   >>> from sklearn.linear_model import SGDClassifier
@@ -359,10 +363,10 @@ classifier object into our pipeline::
   Pipeline(...)
   >>> predicted = text_clf.predict(docs_test)
   >>> np.mean(predicted == twenty_test.target)            # doctest: +ELLIPSIS
-  0.912...
+  0.9127...
 
-``scikit-learn`` further provides utilities for more detailed performance
-analysis of the results::
+We achieved 91.3% accuracy using the SVM. ``scikit-learn`` provides further 
+utilities for more detailed performance analysis of the results::
 
   >>> from sklearn import metrics
   >>> print(metrics.classification_report(twenty_test.target, predicted,
@@ -386,7 +390,7 @@ analysis of the results::
 
 
 As expected the confusion matrix shows that posts from the newsgroups
-on atheism and christian are more often confused for one another than
+on atheism and Christianity are more often confused for one another than
 with computer graphics.
 
 .. note:
@@ -415,7 +419,7 @@ We've already encountered some parameters such as ``use_idf`` in the
 e.g., ``MultinomialNB`` includes a smoothing parameter ``alpha`` and
 ``SGDClassifier`` has a penalty parameter ``alpha`` and configurable loss
 and penalty terms in the objective function (see the module documentation,
-or use the Python ``help`` function, to get a description of these).
+or use the Python ``help`` function to get a description of these).
 
 Instead of tweaking the parameters of the various components of the
 chain, it is possible to run an exhaustive search of the best
@@ -433,7 +437,7 @@ Obviously, such an exhaustive search can be expensive. If we have multiple
 CPU cores at our disposal, we can tell the grid searcher to try these eight
 parameter combinations in parallel with the ``n_jobs`` parameter. If we give
 this parameter a value of ``-1``, grid search will detect how many cores
-are installed and uses them all::
+are installed and use them all::
 
   >>> gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)
 
@@ -481,7 +485,7 @@ a new folder named 'workspace'::
 
   % cp -r skeletons workspace
 
-You can then edit the content of the workspace without fear of loosing
+You can then edit the content of the workspace without fear of losing
 the original exercise instructions.
 
 Then fire an ipython shell and run the work-in-progress script with::
@@ -547,14 +551,14 @@ upon the completion of this tutorial:
 
 
 * Try playing around with the ``analyzer`` and ``token normalisation`` under
-  :class:`CountVectorizer`
+  :class:`CountVectorizer`.
 
 * If you don't have labels, try using
   :ref:`Clustering <sphx_glr_auto_examples_text_document_clustering.py>`
   on your problem.
 
 * If you have multiple labels per document, e.g categories, have a look
-  at the :ref:`Multiclass and multilabel section <multiclass>`
+  at the :ref:`Multiclass and multilabel section <multiclass>`.
 
 * Try using :ref:`Truncated SVD <LSA>` for
   `latent semantic analysis <https://en.wikipedia.org/wiki/Latent_semantic_analysis>`_.
diff --git a/doc/whats_new.rst b/doc/whats_new.rst
index a43f731d3a31..63b1b309b844 100644
--- a/doc/whats_new.rst
+++ b/doc/whats_new.rst
@@ -1,13 +1,18 @@
 .. currentmodule:: sklearn
 .. include:: includes/big_toc_css.rst
 .. include:: whats_new/_contributors.rst
-===============
+
 Release History
 ===============
+
+Release notes for current and recent releases are detailed on this page, with
+:ref:`previous releases <previous_releases_whats_new>` linked below.
+
 .. include:: whats_new/v0.20.rst
 .. include:: whats_new/v0.19.rst
 
-=================
+.. _previous_releases_whats_new:
+
 Previous Releases
 =================
 .. toctree::
diff --git a/doc/whats_new/v0.20.rst b/doc/whats_new/v0.20.rst
index 46898954944d..dd9895f7e857 100644
--- a/doc/whats_new/v0.20.rst
+++ b/doc/whats_new/v0.20.rst
@@ -50,10 +50,16 @@ Classifiers and regressors
   via ``n_iter_no_change``, ``validation_fraction`` and ``tol``. :issue:`7071`
   by `Raghav RV`_
 
+- :class:`dummy.DummyRegressor` now has a ``return_std`` option in its 
+  ``predict`` method. The returned standard deviations will be zeros.
+
 - Added :class:`naive_bayes.ComplementNB`, which implements the Complement
   Naive Bayes classifier described in Rennie et al. (2003).
   :issue:`8190` by :user:`Michael A. Alcorn <airalcorn2>`.
 
+- Added :class:`multioutput.RegressorChain` for multi-target
+  regression. :issue:`9257` by :user:`Kumar Ashutosh <thechargedneutron>`.
+
 Preprocessing
 
 - Added :class:`preprocessing.CategoricalEncoder`, which allows to encode
@@ -72,14 +78,16 @@ Preprocessing
   :issue:`10210` by :user:`Eric Chang <ericchang00>` and
   :user:`Maniteja Nandana <maniteja123>`.
 
+- Added the :class:`preprocessing.TransformedTargetRegressor` which transforms
+  the target y before fitting a regression model. The predictions are mapped
+  back to the original space via an inverse transform. :issue:`9041` by
+  `Andreas Müller`_ and :user:`Guillaume Lemaitre <glemaitre>`.
 
 Model evaluation
 
 - Added the :func:`metrics.balanced_accuracy_score` metric and a corresponding
   ``'balanced_accuracy'`` scorer for binary classification.
   :issue:`8066` by :user:`xyguo` and :user:`Aman Dalmia <dalmia>`.
-- Added :class:`multioutput.RegressorChain` for multi-target
-  regression. :issue:`9257` by :user:`Kumar Ashutosh <thechargedneutron>`.
 
 Decomposition, manifold learning and clustering
 
@@ -87,10 +95,11 @@ Decomposition, manifold learning and clustering
   clustering via ``linkage='single'``. :issue:`9372` by
   :user:`Leland McInnes <lmcinnes>` and :user:`Steve Astels <sastels>`.
 
-- Speed improvements for both 'exact' and 'barnes_hut' methods in
-  :class:`manifold.TSNE`. :issue:`10593` and :issue:`10610` by
-  `Tom Dupre la Tour`_.
+Metrics
 
+- Partial AUC is available via ``max_fpr`` parameter in
+  :func:`metrics.roc_auc_score`. :issue:`3273` by
+  :user:`Alexander Niederbühl <Alexander-N>`.
 
 Enhancements
 ............
@@ -163,6 +172,21 @@ Model evaluation and meta-estimators
   group-based CV strategies. :issue:`9085` by :user:`Laurent Direr <ldirer>`
   and `Andreas Müller`_.
 
+- The ``predict`` method of :class:`pipeline.Pipeline` now passes keyword
+  arguments on to the pipeline's last estimator, enabling the use of parameters
+  such as ``return_std`` in a pipeline with caution.
+  :issue:`9304` by :user:`Breno Freitas <brenolf>`.
+
+- Add `return_estimator` parameter in :func:`model_selection.cross_validate` to
+  return estimators fitted on each split. :issue:`9686` by :user:`Aurélien Bellet
+  <bellet>`.
+
+Decomposition and manifold learning
+
+- Speed improvements for both 'exact' and 'barnes_hut' methods in
+  :class:`manifold.TSNE`. :issue:`10593` and :issue:`10610` by
+  `Tom Dupre la Tour`_.
+
 Metrics
 
 - :func:`metrics.roc_auc_score` now supports binary ``y_true`` other than
@@ -246,6 +270,10 @@ Classifiers and regressors
   overridden when using parameter ``copy_X=True`` and ``check_input=False``.
   :issue:`10581` by :user:`Yacine Mazari <ymazari>`.
 
+- Fixed a bug in :class:`sklearn.linear_model.Lasso`
+  where the coefficient had wrong shape when ``fit_intercept=False``.
+  :issue:`10687` by :user:`Martin Hahn <martin-hahn>`.
+
 Decomposition, manifold learning and clustering
 
 - Fix for uninformative error in :class:`decomposition.IncrementalPCA`:
@@ -335,6 +363,12 @@ Feature Extraction
   (words or n-grams). :issue:`9147` by :user:`Claes-Fredrik Mannby <mannby>`
   and `Roman Yurchak`_.
 
+Utils
+
+- :func:`utils.validation.check_array` yield a ``FutureWarning`` indicating
+  that arrays of bytes/strings will be interpreted as decimal numbers
+  beginning in version 0.22. :issue:`10229` by :user:`Ryan Lee <rtlee9>`
+
 Preprocessing
 
 - Fixed bugs in :class:`preprocessing.LabelEncoder` which would sometimes throw
@@ -358,6 +392,13 @@ Linear, kernelized and related models
   underlying implementation is broken. Use :class:`linear_model.Lasso` instead.
   :issue:`9837` by `Alexandre Gramfort`_.
 
+- ``n_iter_`` may vary from previous releases in
+  :class:`linear_model.LogisticRegression` with ``solver='lbfgs'`` and
+  :class:`linear_model.HuberRegressor`.  For Scipy <= 1.0.0, the optimizer could
+  perform more than the requested maximum number of iterations. Now both
+  estimators will report at most ``max_iter`` iterations even if more were
+  performed. :issue:`10723` by `Joel Nothman`_.
+
 Metrics
 
 - Deprecate ``reorder`` parameter in :func:`metrics.auc` as it's no longer required
@@ -395,6 +436,14 @@ Outlier Detection models
   ``raw_values`` parameter is deprecated as the shifted Mahalanobis distance
   will be always returned in 0.22. :issue:`9015` by `Nicolas Goix`_.
 
+Covariance
+
+- The :func:`covariance.graph_lasso`, :class:`covariance.GraphLasso` and 
+  :class:`covariance.GraphLassoCV` have been renamed to 
+  :func:`covariance.graphical_lasso`, :class:`covariance.GraphicalLasso` and
+  :class:`covariance.GraphicalLassoCV` respectively and will be removed in version 0.22.
+  :issue:`9993` by :user:`Artiem Krinitsyn <artiemq>`
+
 Misc
 
 - Changed warning type from UserWarning to ConvergenceWarning for failing
diff --git a/examples/applications/plot_stock_market.py b/examples/applications/plot_stock_market.py
index 75273d1ea7ec..25b7f92f92fd 100644
--- a/examples/applications/plot_stock_market.py
+++ b/examples/applications/plot_stock_market.py
@@ -233,7 +233,7 @@ def quotes_historical_google(symbol, start_date, end_date):
 
 # #############################################################################
 # Learn a graphical structure from the correlations
-edge_model = covariance.GraphLassoCV()
+edge_model = covariance.GraphicalLassoCV()
 
 # standardize the time series: using correlations rather than covariance
 # is more efficient for structure recovery
diff --git a/examples/cluster/plot_face_segmentation.py b/examples/cluster/plot_coin_segmentation.py
similarity index 73%
rename from examples/cluster/plot_face_segmentation.py
rename to examples/cluster/plot_coin_segmentation.py
index 1f8d6aab9d52..4cb672bb06d3 100644
--- a/examples/cluster/plot_face_segmentation.py
+++ b/examples/cluster/plot_coin_segmentation.py
@@ -1,7 +1,7 @@
 """
-===================================================
-Segmenting the picture of a raccoon face in regions
-===================================================
+================================================
+Segmenting the picture of greek coins in regions
+================================================
 
 This example uses :ref:`spectral_clustering` on a graph created from
 voxel-to-voxel difference on an image to break this image into multiple
@@ -25,37 +25,32 @@
 import time
 
 import numpy as np
-import scipy as sp
 from scipy.ndimage.filters import gaussian_filter
 import matplotlib.pyplot as plt
-from skimage import img_as_float
+from skimage.data import coins
 from skimage.transform import rescale
 
 from sklearn.feature_extraction import image
 from sklearn.cluster import spectral_clustering
 
 
-# load the raccoon face as a numpy array
-try:  # SciPy >= 0.16 have face in misc
-    from scipy.misc import face
-    orig_face = img_as_float(face(gray=True))
-except ImportError:
-    orig_face = img_as_float(sp.face(gray=True))
+# load the coins as a numpy array
+orig_coins = coins()
 
-# Resize it to 10% of the original size to speed up the processing
+# Resize it to 20% of the original size to speed up the processing
 # Applying a Gaussian filter for smoothing prior to down-scaling
 # reduces aliasing artifacts.
-smoothened_face = gaussian_filter(orig_face, sigma=4.5)
-rescaled_face = rescale(smoothened_face, 0.1, mode="reflect")
+smoothened_coins = gaussian_filter(orig_coins, sigma=2)
+rescaled_coins = rescale(smoothened_coins, 0.2, mode="reflect")
 
 # Convert the image into a graph with the value of the gradient on the
 # edges.
-graph = image.img_to_graph(rescaled_face)
+graph = image.img_to_graph(rescaled_coins)
 
 # Take a decreasing function of the gradient: an exponential
 # The smaller beta is, the more independent the segmentation is of the
 # actual image. For beta=1, the segmentation is close to a voronoi
-beta = 5
+beta = 10
 eps = 1e-6
 graph.data = np.exp(-beta * graph.data / graph.data.std()) + eps
 
@@ -71,10 +66,10 @@
     labels = spectral_clustering(graph, n_clusters=N_REGIONS,
                                  assign_labels=assign_labels, random_state=42)
     t1 = time.time()
-    labels = labels.reshape(rescaled_face.shape)
+    labels = labels.reshape(rescaled_coins.shape)
 
     plt.figure(figsize=(5, 5))
-    plt.imshow(rescaled_face, cmap=plt.cm.gray)
+    plt.imshow(rescaled_coins, cmap=plt.cm.gray)
     for l in range(N_REGIONS):
         plt.contour(labels == l,
                     colors=[plt.cm.spectral(l / float(N_REGIONS))])
diff --git a/examples/cluster/plot_face_ward_segmentation.py b/examples/cluster/plot_coin_ward_segmentation.py
similarity index 70%
rename from examples/cluster/plot_face_ward_segmentation.py
rename to examples/cluster/plot_coin_ward_segmentation.py
index 50f04ff7b27d..6e28bc0270e9 100644
--- a/examples/cluster/plot_face_ward_segmentation.py
+++ b/examples/cluster/plot_coin_ward_segmentation.py
@@ -1,7 +1,7 @@
 """
-=========================================================================
-A demo of structured Ward hierarchical clustering on a raccoon face image
-=========================================================================
+======================================================================
+A demo of structured Ward hierarchical clustering on an image of coins
+======================================================================
 
 Compute the segmentation of a 2D image with Ward hierarchical
 clustering. The clustering is spatially constrained in order
@@ -17,12 +17,11 @@
 import time as time
 
 import numpy as np
-import scipy as sp
 from scipy.ndimage.filters import gaussian_filter
 
 import matplotlib.pyplot as plt
 
-from skimage import img_as_float
+from skimage.data import coins
 from skimage.transform import rescale
 
 from sklearn.feature_extraction.image import grid_to_graph
@@ -31,33 +30,29 @@
 
 # #############################################################################
 # Generate data
-try:  # SciPy >= 0.16 have face in misc
-    from scipy.misc import face
-    orig_face = img_as_float(face(gray=True))
-except ImportError:
-    orig_face = img_as_float(sp.face(gray=True))
+orig_coins = coins()
 
-# Resize it to 10% of the original size to speed up the processing
+# Resize it to 20% of the original size to speed up the processing
 # Applying a Gaussian filter for smoothing prior to down-scaling
 # reduces aliasing artifacts.
-smoothened_face = gaussian_filter(orig_face, sigma=4.5)
-rescaled_face = rescale(smoothened_face, 0.1, mode="reflect")
+smoothened_coins = gaussian_filter(orig_coins, sigma=2)
+rescaled_coins = rescale(smoothened_coins, 0.2, mode="reflect")
 
-X = np.reshape(rescaled_face, (-1, 1))
+X = np.reshape(rescaled_coins, (-1, 1))
 
 # #############################################################################
 # Define the structure A of the data. Pixels connected to their neighbors.
-connectivity = grid_to_graph(*rescaled_face.shape)
+connectivity = grid_to_graph(*rescaled_coins.shape)
 
 # #############################################################################
 # Compute clustering
 print("Compute structured hierarchical clustering...")
 st = time.time()
-n_clusters = 15  # number of regions
+n_clusters = 27  # number of regions
 ward = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward',
                                connectivity=connectivity)
 ward.fit(X)
-label = np.reshape(ward.labels_, rescaled_face.shape)
+label = np.reshape(ward.labels_, rescaled_coins.shape)
 print("Elapsed time: ", time.time() - st)
 print("Number of pixels: ", label.size)
 print("Number of clusters: ", np.unique(label).size)
@@ -65,7 +60,7 @@
 # #############################################################################
 # Plot the results on an image
 plt.figure(figsize=(5, 5))
-plt.imshow(rescaled_face, cmap=plt.cm.gray)
+plt.imshow(rescaled_coins, cmap=plt.cm.gray)
 for l in range(n_clusters):
     plt.contour(label == l,
                 colors=[plt.cm.spectral(l / float(n_clusters)), ])
diff --git a/examples/covariance/plot_sparse_cov.py b/examples/covariance/plot_sparse_cov.py
index 1d6782cb43ef..55ec9bde5249 100644
--- a/examples/covariance/plot_sparse_cov.py
+++ b/examples/covariance/plot_sparse_cov.py
@@ -3,7 +3,7 @@
 Sparse inverse covariance estimation
 ======================================
 
-Using the GraphLasso estimator to learn a covariance and sparse precision
+Using the GraphicalLasso estimator to learn a covariance and sparse precision
 from a small number of samples.
 
 To estimate a probabilistic model (e.g. a Gaussian model), estimating the
@@ -43,8 +43,8 @@
 improve readability of the figure. The full range of values of the
 empirical precision is not displayed.
 
-The alpha parameter of the GraphLasso setting the sparsity of the model is
-set by internal cross-validation in the GraphLassoCV. As can be
+The alpha parameter of the GraphicalLasso setting the sparsity of the model is
+set by internal cross-validation in the GraphicalLassoCV. As can be
 seen on figure 2, the grid to compute the cross-validation score is
 iteratively refined in the neighborhood of the maximum.
 """
@@ -56,7 +56,7 @@
 import numpy as np
 from scipy import linalg
 from sklearn.datasets import make_sparse_spd_matrix
-from sklearn.covariance import GraphLassoCV, ledoit_wolf
+from sklearn.covariance import GraphicalLassoCV, ledoit_wolf
 import matplotlib.pyplot as plt
 
 # #############################################################################
@@ -83,7 +83,7 @@
 # Estimate the covariance
 emp_cov = np.dot(X.T, X) / n_samples
 
-model = GraphLassoCV()
+model = GraphicalLassoCV()
 model.fit(X)
 cov_ = model.covariance_
 prec_ = model.precision_
@@ -98,7 +98,7 @@
 
 # plot the covariances
 covs = [('Empirical', emp_cov), ('Ledoit-Wolf', lw_cov_),
-        ('GraphLasso', cov_), ('True', cov)]
+        ('GraphicalLassoCV', cov_), ('True', cov)]
 vmax = cov_.max()
 for i, (name, this_cov) in enumerate(covs):
     plt.subplot(2, 4, i + 1)
@@ -111,7 +111,7 @@
 
 # plot the precisions
 precs = [('Empirical', linalg.inv(emp_cov)), ('Ledoit-Wolf', lw_prec_),
-         ('GraphLasso', prec_), ('True', prec)]
+         ('GraphicalLasso', prec_), ('True', prec)]
 vmax = .9 * prec_.max()
 for i, (name, this_prec) in enumerate(precs):
     ax = plt.subplot(2, 4, i + 5)
diff --git a/examples/plot_cv_predict.py b/examples/model_selection/plot_cv_predict.py
similarity index 100%
rename from examples/plot_cv_predict.py
rename to examples/model_selection/plot_cv_predict.py
diff --git a/examples/model_selection/plot_learning_curve.py b/examples/model_selection/plot_learning_curve.py
index cb8cd87a7803..6e022ebe2718 100644
--- a/examples/model_selection/plot_learning_curve.py
+++ b/examples/model_selection/plot_learning_curve.py
@@ -65,6 +65,16 @@ def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,
 
     n_jobs : integer, optional
         Number of jobs to run in parallel (default 1).
+
+    train_sizes : array-like, shape (n_ticks,), dtype float or int
+        Relative or absolute numbers of training examples that will be used to
+        generate the learning curve. If the dtype is float, it is regarded as a
+        fraction of the maximum size of the training set (that is determined
+        by the selected validation method), i.e. it has to be within (0, 1].
+        Otherwise it is interpreted as absolute sizes of the training sets.
+        Note that for classification the number of samples usually have to
+        be big enough to contain at least one sample from each class.
+        (default: np.linspace(0.1, 1.0, 5))
     """
     plt.figure()
     plt.title(title)
diff --git a/sklearn/base.py b/sklearn/base.py
index 6f59cea3c7ab..f62c0308fb56 100644
--- a/sklearn/base.py
+++ b/sklearn/base.py
@@ -524,6 +524,29 @@ def score(self, X, y=None):
         pass
 
 
+class OutlierMixin(object):
+    """Mixin class for all outlier detection estimators in scikit-learn."""
+    _estimator_type = "outlier_detector"
+
+    def fit_predict(self, X, y=None):
+        """Performs outlier detection on X.
+
+        Returns -1 for outliers and 1 for inliers.
+
+        Parameters
+        ----------
+        X : ndarray, shape (n_samples, n_features)
+            Input data.
+
+        Returns
+        -------
+        y : ndarray, shape (n_samples,)
+            1 for inliers, -1 for outliers.
+        """
+        # override for transductive outlier detectors like LocalOulierFactor
+        return self.fit(X).predict(X)
+
+
 ###############################################################################
 class MetaEstimatorMixin(object):
     """Mixin class for all meta estimators in scikit-learn."""
@@ -562,3 +585,19 @@ def is_regressor(estimator):
         True if estimator is a regressor and False otherwise.
     """
     return getattr(estimator, "_estimator_type", None) == "regressor"
+
+
+def is_outlier_detector(estimator):
+    """Returns True if the given estimator is (probably) an outlier detector.
+
+    Parameters
+    ----------
+    estimator : object
+        Estimator object to test.
+
+    Returns
+    -------
+    out : bool
+        True if estimator is an outlier detector and False otherwise.
+    """
+    return getattr(estimator, "_estimator_type", None) == "outlier_detector"
diff --git a/sklearn/cluster/dbscan_.py b/sklearn/cluster/dbscan_.py
index 45bedb26e76b..45c63e5b30cf 100644
--- a/sklearn/cluster/dbscan_.py
+++ b/sklearn/cluster/dbscan_.py
@@ -43,8 +43,8 @@ def dbscan(X, eps=0.5, min_samples=5, metric='minkowski', metric_params=None,
     metric : string, or callable
         The metric to use when calculating distance between instances in a
         feature array. If metric is a string or callable, it must be one of
-        the options allowed by metrics.pairwise.pairwise_distances for its
-        metric parameter.
+        the options allowed by :func:`sklearn.metrics.pairwise_distances` for
+        its metric parameter.
         If metric is "precomputed", X is assumed to be a distance matrix and
         must be square. X may be a sparse matrix, in which case only "nonzero"
         elements may be considered neighbors for DBSCAN.
@@ -182,8 +182,8 @@ class DBSCAN(BaseEstimator, ClusterMixin):
     metric : string, or callable
         The metric to use when calculating distance between instances in a
         feature array. If metric is a string or callable, it must be one of
-        the options allowed by metrics.pairwise.calculate_distance for its
-        metric parameter.
+        the options allowed by :func:`sklearn.metrics.pairwise_distances` for
+        its metric parameter.
         If metric is "precomputed", X is assumed to be a distance matrix and
         must be square. X may be a sparse matrix, in which case only "nonzero"
         elements may be considered neighbors for DBSCAN.
diff --git a/sklearn/covariance/__init__.py b/sklearn/covariance/__init__.py
index 6ce619487648..6043268675f8 100644
--- a/sklearn/covariance/__init__.py
+++ b/sklearn/covariance/__init__.py
@@ -12,7 +12,8 @@
     ledoit_wolf, ledoit_wolf_shrinkage, \
     LedoitWolf, oas, OAS
 from .robust_covariance import fast_mcd, MinCovDet
-from .graph_lasso_ import graph_lasso, GraphLasso, GraphLassoCV
+from .graph_lasso_ import graph_lasso, GraphLasso, GraphLassoCV,\
+    graphical_lasso, GraphicalLasso, GraphicalLassoCV
 from .elliptic_envelope import EllipticEnvelope
 
 
@@ -20,6 +21,8 @@
            'EmpiricalCovariance',
            'GraphLasso',
            'GraphLassoCV',
+           'GraphicalLasso',
+           'GraphicalLassoCV',
            'LedoitWolf',
            'MinCovDet',
            'OAS',
@@ -27,6 +30,7 @@
            'empirical_covariance',
            'fast_mcd',
            'graph_lasso',
+           'graphical_lasso',
            'ledoit_wolf',
            'ledoit_wolf_shrinkage',
            'log_likelihood',
diff --git a/sklearn/covariance/elliptic_envelope.py b/sklearn/covariance/elliptic_envelope.py
index 1d712207f066..633f102fc006 100644
--- a/sklearn/covariance/elliptic_envelope.py
+++ b/sklearn/covariance/elliptic_envelope.py
@@ -8,9 +8,10 @@
 from . import MinCovDet
 from ..utils.validation import check_is_fitted, check_array
 from ..metrics import accuracy_score
+from ..base import OutlierMixin
 
 
-class EllipticEnvelope(MinCovDet):
+class EllipticEnvelope(MinCovDet, OutlierMixin):
     """An object for detecting outliers in a Gaussian distributed dataset.
 
     Read more in the :ref:`User Guide <outlier_detection>`.
diff --git a/sklearn/covariance/graph_lasso_.py b/sklearn/covariance/graph_lasso_.py
index 0f53aebf7cf9..92fb0c9a6040 100644
--- a/sklearn/covariance/graph_lasso_.py
+++ b/sklearn/covariance/graph_lasso_.py
@@ -1,4 +1,4 @@
-"""GraphLasso: sparse inverse covariance estimation with an l1-penalized
+"""GraphicalLasso: sparse inverse covariance estimation with an l1-penalized
 estimator.
 """
 
@@ -29,7 +29,7 @@
 # Helper functions to compute the objective and dual objective functions
 # of the l1-penalized estimator
 def _objective(mle, precision_, alpha):
-    """Evaluation of the graph-lasso objective function
+    """Evaluation of the graphical-lasso objective function
 
     the objective function is made of a shifted scaled version of the
     normalized log-likelihood (i.e. its empirical mean over the samples) and a
@@ -67,7 +67,7 @@ def alpha_max(emp_cov):
     -----
 
     This results from the bound for the all the Lasso that are solved
-    in GraphLasso: each time, the row of cov corresponds to Xy. As the
+    in GraphicalLasso: each time, the row of cov corresponds to Xy. As the
     bound for alpha is given by `max(abs(Xy))`, the result follows.
 
     """
@@ -78,10 +78,10 @@ def alpha_max(emp_cov):
 
 # The g-lasso algorithm
 
-def graph_lasso(emp_cov, alpha, cov_init=None, mode='cd', tol=1e-4,
-                enet_tol=1e-4, max_iter=100, verbose=False,
-                return_costs=False, eps=np.finfo(np.float64).eps,
-                return_n_iter=False):
+def graphical_lasso(emp_cov, alpha, cov_init=None, mode='cd', tol=1e-4,
+                    enet_tol=1e-4, max_iter=100, verbose=False,
+                    return_costs=False, eps=np.finfo(np.float64).eps,
+                    return_n_iter=False):
     """l1-penalized covariance estimator
 
     Read more in the :ref:`User Guide <sparse_inverse_covariance>`.
@@ -149,7 +149,7 @@ def graph_lasso(emp_cov, alpha, cov_init=None, mode='cd', tol=1e-4,
 
     See Also
     --------
-    GraphLasso, GraphLassoCV
+    GraphicalLasso, GraphicalLassoCV
 
     Notes
     -----
@@ -223,8 +223,9 @@ def graph_lasso(emp_cov, alpha, cov_init=None, mode='cd', tol=1e-4,
                         coefs = -(precision_[indices != idx, idx]
                                   / (precision_[idx, idx] + 1000 * eps))
                         coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(
-                            coefs, alpha, 0, sub_covariance, row, row,
-                            max_iter, enet_tol, check_random_state(None), False)
+                            coefs, alpha, 0, sub_covariance,
+                            row, row, max_iter, enet_tol,
+                            check_random_state(None), False)
                     else:
                         # Use LARS
                         _, _, coefs = lars_path(
@@ -245,9 +246,9 @@ def graph_lasso(emp_cov, alpha, cov_init=None, mode='cd', tol=1e-4,
             d_gap = _dual_gap(emp_cov, precision_, alpha)
             cost = _objective(emp_cov, precision_, alpha)
             if verbose:
-                print(
-                    '[graph_lasso] Iteration % 3i, cost % 3.2e, dual gap %.3e'
-                    % (i, cost, d_gap))
+                print('[graphical_lasso] Iteration '
+                      '% 3i, cost % 3.2e, dual gap %.3e'
+                      % (i, cost, d_gap))
             if return_costs:
                 costs.append((cost, d_gap))
             if np.abs(d_gap) < tol:
@@ -256,9 +257,9 @@ def graph_lasso(emp_cov, alpha, cov_init=None, mode='cd', tol=1e-4,
                 raise FloatingPointError('Non SPD result: the system is '
                                          'too ill-conditioned for this solver')
         else:
-            warnings.warn('graph_lasso: did not converge after %i iteration:'
-                          ' dual gap: %.3e' % (max_iter, d_gap),
-                          ConvergenceWarning)
+            warnings.warn('graphical_lasso: did not converge after '
+                          '%i iteration: dual gap: %.3e'
+                          % (max_iter, d_gap), ConvergenceWarning)
     except FloatingPointError as e:
         e.args = (e.args[0]
                   + '. The system is too ill-conditioned for this solver',)
@@ -276,7 +277,7 @@ def graph_lasso(emp_cov, alpha, cov_init=None, mode='cd', tol=1e-4,
             return covariance_, precision_
 
 
-class GraphLasso(EmpiricalCovariance):
+class GraphicalLasso(EmpiricalCovariance):
     """Sparse inverse covariance estimation with an l1-penalized estimator.
 
     Read more in the :ref:`User Guide <sparse_inverse_covariance>`.
@@ -328,12 +329,12 @@ class GraphLasso(EmpiricalCovariance):
 
     See Also
     --------
-    graph_lasso, GraphLassoCV
+    graphical_lasso, GraphicalLassoCV
     """
 
     def __init__(self, alpha=.01, mode='cd', tol=1e-4, enet_tol=1e-4,
                  max_iter=100, verbose=False, assume_centered=False):
-        super(GraphLasso, self).__init__(assume_centered=assume_centered)
+        super(GraphicalLasso, self).__init__(assume_centered=assume_centered)
         self.alpha = alpha
         self.mode = mode
         self.tol = tol
@@ -342,7 +343,7 @@ def __init__(self, alpha=.01, mode='cd', tol=1e-4, enet_tol=1e-4,
         self.verbose = verbose
 
     def fit(self, X, y=None):
-        """Fits the GraphLasso model to X.
+        """Fits the GraphicalLasso model to X.
 
         Parameters
         ----------
@@ -360,16 +361,16 @@ def fit(self, X, y=None):
             self.location_ = X.mean(0)
         emp_cov = empirical_covariance(
             X, assume_centered=self.assume_centered)
-        self.covariance_, self.precision_, self.n_iter_ = graph_lasso(
+        self.covariance_, self.precision_, self.n_iter_ = graphical_lasso(
             emp_cov, alpha=self.alpha, mode=self.mode, tol=self.tol,
             enet_tol=self.enet_tol, max_iter=self.max_iter,
             verbose=self.verbose, return_n_iter=True)
         return self
 
 
-# Cross-validation with GraphLasso
-def graph_lasso_path(X, alphas, cov_init=None, X_test=None, mode='cd',
-                     tol=1e-4, enet_tol=1e-4, max_iter=100, verbose=False):
+# Cross-validation with GraphicalLasso
+def graphical_lasso_path(X, alphas, cov_init=None, X_test=None, mode='cd',
+                         tol=1e-4, enet_tol=1e-4, max_iter=100, verbose=False):
     """l1-penalized covariance estimator along a path of decreasing alphas
 
     Read more in the :ref:`User Guide <sparse_inverse_covariance>`.
@@ -434,7 +435,7 @@ def graph_lasso_path(X, alphas, cov_init=None, X_test=None, mode='cd',
     for alpha in alphas:
         try:
             # Capture the errors, and move on
-            covariance_, precision_ = graph_lasso(
+            covariance_, precision_ = graphical_lasso(
                 emp_cov, alpha=alpha, cov_init=covariance_, mode=mode, tol=tol,
                 enet_tol=enet_tol, max_iter=max_iter, verbose=inner_verbose)
             covariances_.append(covariance_)
@@ -453,16 +454,16 @@ def graph_lasso_path(X, alphas, cov_init=None, X_test=None, mode='cd',
             sys.stderr.write('.')
         elif verbose > 1:
             if X_test is not None:
-                print('[graph_lasso_path] alpha: %.2e, score: %.2e'
+                print('[graphical_lasso_path] alpha: %.2e, score: %.2e'
                       % (alpha, this_score))
             else:
-                print('[graph_lasso_path] alpha: %.2e' % alpha)
+                print('[graphical_lasso_path] alpha: %.2e' % alpha)
     if X_test is not None:
         return covariances_, precisions_, scores_
     return covariances_, precisions_
 
 
-class GraphLassoCV(GraphLasso):
+class GraphicalLassoCV(GraphicalLasso):
     """Sparse inverse covariance w/ cross-validated choice of the l1 penalty
 
     Read more in the :ref:`User Guide <sparse_inverse_covariance>`.
@@ -547,7 +548,7 @@ class GraphLassoCV(GraphLasso):
 
     See Also
     --------
-    graph_lasso, GraphLasso
+    graphical_lasso, GraphicalLasso
 
     Notes
     -----
@@ -565,7 +566,7 @@ class GraphLassoCV(GraphLasso):
     def __init__(self, alphas=4, n_refinements=4, cv=None, tol=1e-4,
                  enet_tol=1e-4, max_iter=100, mode='cd', n_jobs=1,
                  verbose=False, assume_centered=False):
-        super(GraphLassoCV, self).__init__(
+        super(GraphicalLassoCV, self).__init__(
             mode=mode, tol=tol, verbose=verbose, enet_tol=enet_tol,
             max_iter=max_iter, assume_centered=assume_centered)
         self.alphas = alphas
@@ -580,7 +581,7 @@ def grid_scores(self):
         return self.grid_scores_
 
     def fit(self, X, y=None):
-        """Fits the GraphLasso covariance model to X.
+        """Fits the GraphicalLasso covariance model to X.
 
         Parameters
         ----------
@@ -623,18 +624,19 @@ def fit(self, X, y=None):
                 warnings.simplefilter('ignore', ConvergenceWarning)
                 # Compute the cross-validated loss on the current grid
 
-                # NOTE: Warm-restarting graph_lasso_path has been tried, and
-                # this did not allow to gain anything (same execution time with
-                # or without).
+                # NOTE: Warm-restarting graphical_lasso_path has been tried,
+                # and this did not allow to gain anything
+                # (same execution time with or without).
                 this_path = Parallel(
                     n_jobs=self.n_jobs,
                     verbose=self.verbose
-                )(delayed(graph_lasso_path)(X[train], alphas=alphas,
-                                            X_test=X[test], mode=self.mode,
-                                            tol=self.tol,
-                                            enet_tol=self.enet_tol,
-                                            max_iter=int(.1 * self.max_iter),
-                                            verbose=inner_verbose)
+                )(delayed(graphical_lasso_path)(X[train], alphas=alphas,
+                                                X_test=X[test], mode=self.mode,
+                                                tol=self.tol,
+                                                enet_tol=self.enet_tol,
+                                                max_iter=int(.1 *
+                                                             self.max_iter),
+                                                verbose=inner_verbose)
                   for train, test in cv.split(X, y))
 
             # Little danse to transform the list in what we need
@@ -685,8 +687,8 @@ def fit(self, X, y=None):
                 alphas = alphas[1:-1]
 
             if self.verbose and n_refinements > 1:
-                print('[GraphLassoCV] Done refinement % 2i out of %i: % 3is'
-                      % (i + 1, n_refinements, time.time() - t0))
+                print('[GraphicalLassoCV] Done refinement % 2i out of'
+                      ' %i: % 3is' % (i + 1, n_refinements, time.time() - t0))
 
         path = list(zip(*path))
         grid_scores = list(path[1])
@@ -702,8 +704,264 @@ def fit(self, X, y=None):
         self.cv_alphas_ = alphas
 
         # Finally fit the model with the selected alpha
-        self.covariance_, self.precision_, self.n_iter_ = graph_lasso(
+        self.covariance_, self.precision_, self.n_iter_ = graphical_lasso(
             emp_cov, alpha=best_alpha, mode=self.mode, tol=self.tol,
             enet_tol=self.enet_tol, max_iter=self.max_iter,
             verbose=inner_verbose, return_n_iter=True)
         return self
+
+
+# The g-lasso algorithm
+@deprecated("The 'graph_lasso' was renamed to 'graphical_lasso' "
+            "in version 0.20 and will be removed in 0.22.")
+def graph_lasso(emp_cov, alpha, cov_init=None, mode='cd', tol=1e-4,
+                enet_tol=1e-4, max_iter=100, verbose=False,
+                return_costs=False, eps=np.finfo(np.float64).eps,
+                return_n_iter=False):
+    """l1-penalized covariance estimator
+
+    Read more in the :ref:`User Guide <sparse_inverse_covariance>`.
+
+    Parameters
+    ----------
+    emp_cov : 2D ndarray, shape (n_features, n_features)
+        Empirical covariance from which to compute the covariance estimate.
+
+    alpha : positive float
+        The regularization parameter: the higher alpha, the more
+        regularization, the sparser the inverse covariance.
+
+    cov_init : 2D array (n_features, n_features), optional
+        The initial guess for the covariance.
+
+    mode : {'cd', 'lars'}
+        The Lasso solver to use: coordinate descent or LARS. Use LARS for
+        very sparse underlying graphs, where p > n. Elsewhere prefer cd
+        which is more numerically stable.
+
+    tol : positive float, optional
+        The tolerance to declare convergence: if the dual gap goes below
+        this value, iterations are stopped.
+
+    enet_tol : positive float, optional
+        The tolerance for the elastic net solver used to calculate the descent
+        direction. This parameter controls the accuracy of the search direction
+        for a given column update, not of the overall parameter estimate. Only
+        used for mode='cd'.
+
+    max_iter : integer, optional
+        The maximum number of iterations.
+
+    verbose : boolean, optional
+        If verbose is True, the objective function and dual gap are
+        printed at each iteration.
+
+    return_costs : boolean, optional
+        If return_costs is True, the objective function and dual gap
+        at each iteration are returned.
+
+    eps : float, optional
+        The machine-precision regularization in the computation of the
+        Cholesky diagonal factors. Increase this for very ill-conditioned
+        systems.
+
+    return_n_iter : bool, optional
+        Whether or not to return the number of iterations.
+
+    Returns
+    -------
+    covariance : 2D ndarray, shape (n_features, n_features)
+        The estimated covariance matrix.
+
+    precision : 2D ndarray, shape (n_features, n_features)
+        The estimated (sparse) precision matrix.
+
+    costs : list of (objective, dual_gap) pairs
+        The list of values of the objective function and the dual gap at
+        each iteration. Returned only if return_costs is True.
+
+    n_iter : int
+        Number of iterations. Returned only if `return_n_iter` is set to True.
+
+    See Also
+    --------
+    GraphLasso, GraphLassoCV
+
+    Notes
+    -----
+    The algorithm employed to solve this problem is the GLasso algorithm,
+    from the Friedman 2008 Biostatistics paper. It is the same algorithm
+    as in the R `glasso` package.
+
+    One possible difference with the `glasso` R package is that the
+    diagonal coefficients are not penalized.
+
+    """
+    return graphical_lasso(emp_cov, alpha, cov_init, mode, tol,
+                           enet_tol, max_iter, verbose, return_costs,
+                           eps, return_n_iter)
+
+
+@deprecated("The 'GraphLasso' was renamed to 'GraphicalLasso' "
+            "in version 0.20 and will be removed in 0.22.")
+class GraphLasso(GraphicalLasso):
+    """Sparse inverse covariance estimation with an l1-penalized estimator.
+
+    This class implements the Graphical Lasso algorithm.
+
+    Read more in the :ref:`User Guide <sparse_inverse_covariance>`.
+
+    Parameters
+    ----------
+    alpha : positive float, default 0.01
+        The regularization parameter: the higher alpha, the more
+        regularization, the sparser the inverse covariance.
+
+    mode : {'cd', 'lars'}, default 'cd'
+        The Lasso solver to use: coordinate descent or LARS. Use LARS for
+        very sparse underlying graphs, where p > n. Elsewhere prefer cd
+        which is more numerically stable.
+
+    tol : positive float, default 1e-4
+        The tolerance to declare convergence: if the dual gap goes below
+        this value, iterations are stopped.
+
+    enet_tol : positive float, optional
+        The tolerance for the elastic net solver used to calculate the descent
+        direction. This parameter controls the accuracy of the search direction
+        for a given column update, not of the overall parameter estimate. Only
+        used for mode='cd'.
+
+    max_iter : integer, default 100
+        The maximum number of iterations.
+
+    verbose : boolean, default False
+        If verbose is True, the objective function and dual gap are
+        plotted at each iteration.
+
+    assume_centered : boolean, default False
+        If True, data are not centered before computation.
+        Useful when working with data whose mean is almost, but not exactly
+        zero.
+        If False, data are centered before computation.
+
+    Attributes
+    ----------
+    covariance_ : array-like, shape (n_features, n_features)
+        Estimated covariance matrix
+
+    precision_ : array-like, shape (n_features, n_features)
+        Estimated pseudo inverse matrix.
+
+    n_iter_ : int
+        Number of iterations run.
+
+    See Also
+    --------
+    graph_lasso, GraphLassoCV
+    """
+
+
+@deprecated("The 'GraphLassoCV' was renamed to 'GraphicalLassoCV' "
+            "in version 0.20 and will be removed in 0.22.")
+class GraphLassoCV(GraphicalLassoCV):
+    """Sparse inverse covariance w/ cross-validated choice of the l1 penalty
+
+    This class implements the Graphical Lasso algorithm.
+
+    Read more in the :ref:`User Guide <sparse_inverse_covariance>`.
+
+    Parameters
+    ----------
+    alphas : integer, or list positive float, optional
+        If an integer is given, it fixes the number of points on the
+        grids of alpha to be used. If a list is given, it gives the
+        grid to be used. See the notes in the class docstring for
+        more details.
+
+    n_refinements : strictly positive integer
+        The number of times the grid is refined. Not used if explicit
+        values of alphas are passed.
+
+    cv : int, cross-validation generator or an iterable, optional
+        Determines the cross-validation splitting strategy.
+        Possible inputs for cv are:
+
+        - None, to use the default 3-fold cross-validation,
+        - integer, to specify the number of folds.
+        - An object to be used as a cross-validation generator.
+        - An iterable yielding train/test splits.
+
+        For integer/None inputs :class:`KFold` is used.
+
+        Refer :ref:`User Guide <cross_validation>` for the various
+        cross-validation strategies that can be used here.
+
+    tol : positive float, optional
+        The tolerance to declare convergence: if the dual gap goes below
+        this value, iterations are stopped.
+
+    enet_tol : positive float, optional
+        The tolerance for the elastic net solver used to calculate the descent
+        direction. This parameter controls the accuracy of the search direction
+        for a given column update, not of the overall parameter estimate. Only
+        used for mode='cd'.
+
+    max_iter : integer, optional
+        Maximum number of iterations.
+
+    mode : {'cd', 'lars'}
+        The Lasso solver to use: coordinate descent or LARS. Use LARS for
+        very sparse underlying graphs, where number of features is greater
+        than number of samples. Elsewhere prefer cd which is more numerically
+        stable.
+
+    n_jobs : int, optional
+        number of jobs to run in parallel (default 1).
+
+    verbose : boolean, optional
+        If verbose is True, the objective function and duality gap are
+        printed at each iteration.
+
+    assume_centered : Boolean
+        If True, data are not centered before computation.
+        Useful when working with data whose mean is almost, but not exactly
+        zero.
+        If False, data are centered before computation.
+
+    Attributes
+    ----------
+    covariance_ : numpy.ndarray, shape (n_features, n_features)
+        Estimated covariance matrix.
+
+    precision_ : numpy.ndarray, shape (n_features, n_features)
+        Estimated precision matrix (inverse covariance).
+
+    alpha_ : float
+        Penalization parameter selected.
+
+    cv_alphas_ : list of float
+        All penalization parameters explored.
+
+    grid_scores_ : 2D numpy.ndarray (n_alphas, n_folds)
+        Log-likelihood score on left-out data across folds.
+
+    n_iter_ : int
+        Number of iterations run for the optimal alpha.
+
+    See Also
+    --------
+    graph_lasso, GraphLasso
+
+    Notes
+    -----
+    The search for the optimal penalization parameter (alpha) is done on an
+    iteratively refined grid: first the cross-validated scores on a grid are
+    computed, then a new refined grid is centered around the maximum, and so
+    on.
+
+    One of the challenges which is faced here is that the solvers can
+    fail to converge to a well-conditioned estimate. The corresponding
+    values of alpha then come out as missing values, but the optimum may
+    be close to these missing values.
+    """
diff --git a/sklearn/covariance/shrunk_covariance_.py b/sklearn/covariance/shrunk_covariance_.py
index f45fb9c966a4..5a61759d665e 100644
--- a/sklearn/covariance/shrunk_covariance_.py
+++ b/sklearn/covariance/shrunk_covariance_.py
@@ -45,10 +45,9 @@ def shrunk_covariance(emp_cov, shrinkage=0.1):
 
     Notes
     -----
-    The regularized (shrunk) covariance is given by
+    The regularized (shrunk) covariance is given by:
 
-    (1 - shrinkage)*cov
-      + shrinkage*mu*np.identity(n_features)
+    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)
 
     where mu = trace(cov) / n_features
 
@@ -98,10 +97,9 @@ class ShrunkCovariance(EmpiricalCovariance):
 
     Notes
     -----
-    The regularized covariance is given by
+    The regularized covariance is given by:
 
-    (1 - shrinkage)*cov
-      + shrinkage*mu*np.identity(n_features)
+    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)
 
     where mu = trace(cov) / n_features
 
@@ -176,8 +174,7 @@ def ledoit_wolf_shrinkage(X, assume_centered=False, block_size=1000):
     -----
     The regularized (shrunk) covariance is:
 
-    (1 - shrinkage)*cov
-      + shrinkage * mu * np.identity(n_features)
+    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)
 
     where mu = trace(cov) / n_features
 
@@ -276,8 +273,7 @@ def ledoit_wolf(X, assume_centered=False, block_size=1000):
     -----
     The regularized (shrunk) covariance is:
 
-    (1 - shrinkage)*cov
-      + shrinkage * mu * np.identity(n_features)
+    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)
 
     where mu = trace(cov) / n_features
 
@@ -350,10 +346,9 @@ class LedoitWolf(EmpiricalCovariance):
 
     Notes
     -----
-    The regularised covariance is::
+    The regularised covariance is:
 
-        (1 - shrinkage)*cov
-                + shrinkage*mu*np.identity(n_features)
+    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)
 
     where mu = trace(cov) / n_features
     and shrinkage is given by the Ledoit and Wolf formula (see References)
@@ -433,8 +428,7 @@ def oas(X, assume_centered=False):
     -----
     The regularised (shrunk) covariance is:
 
-    (1 - shrinkage)*cov
-      + shrinkage * mu * np.identity(n_features)
+    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)
 
     where mu = trace(cov) / n_features
 
@@ -513,10 +507,9 @@ class OAS(EmpiricalCovariance):
 
     Notes
     -----
-    The regularised covariance is::
+    The regularised covariance is:
 
-        (1 - shrinkage)*cov
-                + shrinkage*mu*np.identity(n_features)
+    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)
 
     where mu = trace(cov) / n_features
     and shrinkage is given by the OAS formula (see References)
diff --git a/sklearn/covariance/tests/test_graph_lasso.py b/sklearn/covariance/tests/test_graph_lasso.py
index b87e67e910d6..60978fa25978 100644
--- a/sklearn/covariance/tests/test_graph_lasso.py
+++ b/sklearn/covariance/tests/test_graph_lasso.py
@@ -8,6 +8,7 @@
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_array_less
 from sklearn.utils.testing import assert_warns_message
+from sklearn.utils.testing import ignore_warnings
 
 from sklearn.covariance import (graph_lasso, GraphLasso, GraphLassoCV,
                                 empirical_covariance)
@@ -19,6 +20,7 @@
 from numpy.testing import assert_equal
 
 
+@ignore_warnings(category=DeprecationWarning)
 def test_graph_lasso(random_state=0):
     # Sample data from a sparse multivariate normal
     dim = 20
@@ -62,6 +64,7 @@ def test_graph_lasso(random_state=0):
     assert_array_almost_equal(precs[0], precs[1])
 
 
+@ignore_warnings(category=DeprecationWarning)
 def test_graph_lasso_iris():
     # Hard-coded solution from R glasso package for alpha=1.0
     # The iris datasets in R and scikit-learn do not match in a few places,
@@ -87,6 +90,7 @@ def test_graph_lasso_iris():
         assert_array_almost_equal(icov, icov_R)
 
 
+@ignore_warnings(category=DeprecationWarning)
 def test_graph_lasso_iris_singular():
     # Small subset of rows to test the rank-deficient case
     # Need to choose samples such that none of the variances are zero
@@ -114,6 +118,7 @@ def test_graph_lasso_iris_singular():
         assert_array_almost_equal(icov, icov_R, decimal=5)
 
 
+@ignore_warnings(category=DeprecationWarning)
 def test_graph_lasso_cv(random_state=1):
     # Sample data from a sparse multivariate normal
     dim = 5
@@ -136,6 +141,7 @@ def test_graph_lasso_cv(random_state=1):
     GraphLassoCV(alphas=[0.8, 0.5], tol=1e-1, n_jobs=1).fit(X)
 
 
+@ignore_warnings(category=DeprecationWarning)
 def test_deprecated_grid_scores(random_state=1):
     dim = 5
     n_samples = 6
diff --git a/sklearn/covariance/tests/test_graphical_lasso.py b/sklearn/covariance/tests/test_graphical_lasso.py
new file mode 100644
index 000000000000..42c4c88f01aa
--- /dev/null
+++ b/sklearn/covariance/tests/test_graphical_lasso.py
@@ -0,0 +1,157 @@
+""" Test the graphical_lasso module.
+"""
+import sys
+
+import numpy as np
+from scipy import linalg
+
+from sklearn.utils.testing import assert_array_almost_equal
+from sklearn.utils.testing import assert_array_less
+from sklearn.utils.testing import assert_warns_message
+
+from sklearn.covariance import (graphical_lasso, GraphicalLasso,
+                                GraphicalLassoCV, empirical_covariance)
+from sklearn.datasets.samples_generator import make_sparse_spd_matrix
+from sklearn.externals.six.moves import StringIO
+from sklearn.utils import check_random_state
+from sklearn import datasets
+
+from numpy.testing import assert_equal
+
+
+def test_graphical_lasso(random_state=0):
+    # Sample data from a sparse multivariate normal
+    dim = 20
+    n_samples = 100
+    random_state = check_random_state(random_state)
+    prec = make_sparse_spd_matrix(dim, alpha=.95,
+                                  random_state=random_state)
+    cov = linalg.inv(prec)
+    X = random_state.multivariate_normal(np.zeros(dim), cov, size=n_samples)
+    emp_cov = empirical_covariance(X)
+
+    for alpha in (0., .1, .25):
+        covs = dict()
+        icovs = dict()
+        for method in ('cd', 'lars'):
+            cov_, icov_, costs = graphical_lasso(emp_cov, return_costs=True,
+                                                 alpha=alpha, mode=method)
+            covs[method] = cov_
+            icovs[method] = icov_
+            costs, dual_gap = np.array(costs).T
+            # Check that the costs always decrease (doesn't hold if alpha == 0)
+            if not alpha == 0:
+                assert_array_less(np.diff(costs), 0)
+        # Check that the 2 approaches give similar results
+        assert_array_almost_equal(covs['cd'], covs['lars'], decimal=4)
+        assert_array_almost_equal(icovs['cd'], icovs['lars'], decimal=4)
+
+    # Smoke test the estimator
+    model = GraphicalLasso(alpha=.25).fit(X)
+    model.score(X)
+    assert_array_almost_equal(model.covariance_, covs['cd'], decimal=4)
+    assert_array_almost_equal(model.covariance_, covs['lars'], decimal=4)
+
+    # For a centered matrix, assume_centered could be chosen True or False
+    # Check that this returns indeed the same result for centered data
+    Z = X - X.mean(0)
+    precs = list()
+    for assume_centered in (False, True):
+        prec_ = GraphicalLasso(
+            assume_centered=assume_centered).fit(Z).precision_
+        precs.append(prec_)
+    assert_array_almost_equal(precs[0], precs[1])
+
+
+def test_graphical_lasso_iris():
+    # Hard-coded solution from R glasso package for alpha=1.0
+    # The iris datasets in R and scikit-learn do not match in a few places,
+    # these values are for the scikit-learn version.
+    cov_R = np.array([
+        [0.68112222, 0.0, 0.2651911, 0.02467558],
+        [0.00, 0.1867507, 0.0, 0.00],
+        [0.26519111, 0.0, 3.0924249, 0.28774489],
+        [0.02467558, 0.0, 0.2877449, 0.57853156]
+    ])
+    icov_R = np.array([
+        [1.5188780, 0.0, -0.1302515, 0.0],
+        [0.0, 5.354733, 0.0, 0.0],
+        [-0.1302515, 0.0, 0.3502322, -0.1686399],
+        [0.0, 0.0, -0.1686399, 1.8123908]
+    ])
+    X = datasets.load_iris().data
+    emp_cov = empirical_covariance(X)
+    for method in ('cd', 'lars'):
+        cov, icov = graphical_lasso(emp_cov, alpha=1.0, return_costs=False,
+                                    mode=method)
+        assert_array_almost_equal(cov, cov_R)
+        assert_array_almost_equal(icov, icov_R)
+
+
+def test_graphical_lasso_iris_singular():
+    # Small subset of rows to test the rank-deficient case
+    # Need to choose samples such that none of the variances are zero
+    indices = np.arange(10, 13)
+
+    # Hard-coded solution from R glasso package for alpha=0.01
+    cov_R = np.array([
+        [0.08, 0.056666662595, 0.00229729713223, 0.00153153142149],
+        [0.056666662595, 0.082222222222, 0.00333333333333, 0.00222222222222],
+        [0.002297297132, 0.003333333333, 0.00666666666667, 0.00009009009009],
+        [0.001531531421, 0.002222222222, 0.00009009009009, 0.00222222222222]
+    ])
+    icov_R = np.array([
+        [24.42244057, -16.831679593, 0.0, 0.0],
+        [-16.83168201, 24.351841681, -6.206896552, -12.5],
+        [0.0, -6.206896171, 153.103448276, 0.0],
+        [0.0, -12.499999143, 0.0, 462.5]
+    ])
+    X = datasets.load_iris().data[indices, :]
+    emp_cov = empirical_covariance(X)
+    for method in ('cd', 'lars'):
+        cov, icov = graphical_lasso(emp_cov, alpha=0.01, return_costs=False,
+                                    mode=method)
+        assert_array_almost_equal(cov, cov_R, decimal=5)
+        assert_array_almost_equal(icov, icov_R, decimal=5)
+
+
+def test_graphical_lasso_cv(random_state=1):
+    # Sample data from a sparse multivariate normal
+    dim = 5
+    n_samples = 6
+    random_state = check_random_state(random_state)
+    prec = make_sparse_spd_matrix(dim, alpha=.96,
+                                  random_state=random_state)
+    cov = linalg.inv(prec)
+    X = random_state.multivariate_normal(np.zeros(dim), cov, size=n_samples)
+    # Capture stdout, to smoke test the verbose mode
+    orig_stdout = sys.stdout
+    try:
+        sys.stdout = StringIO()
+        # We need verbose very high so that Parallel prints on stdout
+        GraphicalLassoCV(verbose=100, alphas=5, tol=1e-1).fit(X)
+    finally:
+        sys.stdout = orig_stdout
+
+    # Smoke test with specified alphas
+    GraphicalLassoCV(alphas=[0.8, 0.5], tol=1e-1, n_jobs=1).fit(X)
+
+
+def test_deprecated_grid_scores(random_state=1):
+    dim = 5
+    n_samples = 6
+    random_state = check_random_state(random_state)
+    prec = make_sparse_spd_matrix(dim, alpha=.96,
+                                  random_state=random_state)
+    cov = linalg.inv(prec)
+    X = random_state.multivariate_normal(np.zeros(dim), cov, size=n_samples)
+    graphical_lasso = GraphicalLassoCV(alphas=[0.8, 0.5], tol=1e-1, n_jobs=1)
+    graphical_lasso.fit(X)
+
+    depr_message = ("Attribute grid_scores was deprecated in version "
+                    "0.19 and will be removed in 0.21. Use "
+                    "``grid_scores_`` instead")
+
+    assert_warns_message(DeprecationWarning, depr_message,
+                         lambda: graphical_lasso.grid_scores)
+    assert_equal(graphical_lasso.grid_scores, graphical_lasso.grid_scores_)
diff --git a/sklearn/datasets/samples_generator.py b/sklearn/datasets/samples_generator.py
index 375ea10398cb..00f15c96446c 100644
--- a/sklearn/datasets/samples_generator.py
+++ b/sklearn/datasets/samples_generator.py
@@ -43,14 +43,18 @@ def make_classification(n_samples=100, n_features=20, n_informative=2,
     """Generate a random n-class classification problem.
 
     This initially creates clusters of points normally distributed (std=1)
-    about vertices of an `n_informative`-dimensional hypercube with sides of
-    length `2*class_sep` and assigns an equal number of clusters to each
+    about vertices of an ``n_informative``-dimensional hypercube with sides of
+    length ``2*class_sep`` and assigns an equal number of clusters to each
     class. It introduces interdependence between these features and adds
     various types of further noise to the data.
 
-    Prior to shuffling, `X` stacks a number of these primary "informative"
-    features, "redundant" linear combinations of these, "repeated" duplicates
-    of sampled features, and arbitrary noise for and remaining features.
+    Without shuffling, ``X`` horizontally stacks features in the following
+    order: the primary ``n_informative`` features, followed by ``n_redundant``
+    linear combinations of the informative features, followed by ``n_repeated``
+    duplicates, drawn randomly with replacement from the informative and
+    redundant features. The remaining features are filled with random noise.
+    Thus, without shuffling, all useful features are contained in the columns
+    ``X[:, :n_informative + n_redundant + n_repeated]``.
 
     Read more in the :ref:`User Guide <sample_generators>`.
 
@@ -60,15 +64,16 @@ def make_classification(n_samples=100, n_features=20, n_informative=2,
         The number of samples.
 
     n_features : int, optional (default=20)
-        The total number of features. These comprise `n_informative`
-        informative features, `n_redundant` redundant features, `n_repeated`
-        duplicated features and `n_features-n_informative-n_redundant-
-        n_repeated` useless features drawn at random.
+        The total number of features. These comprise ``n_informative``
+        informative features, ``n_redundant`` redundant features,
+        ``n_repeated`` duplicated features and
+        ``n_features-n_informative-n_redundant-n_repeated`` useless features
+        drawn at random.
 
     n_informative : int, optional (default=2)
         The number of informative features. Each class is composed of a number
         of gaussian clusters each located around the vertices of a hypercube
-        in a subspace of dimension `n_informative`. For each cluster,
+        in a subspace of dimension ``n_informative``. For each cluster,
         informative features are drawn independently from  N(0, 1) and then
         randomly linearly combined within each cluster in order to add
         covariance. The clusters are then placed on the vertices of the
@@ -90,10 +95,10 @@ def make_classification(n_samples=100, n_features=20, n_informative=2,
 
     weights : list of floats or None (default=None)
         The proportions of samples assigned to each class. If None, then
-        classes are balanced. Note that if `len(weights) == n_classes - 1`,
+        classes are balanced. Note that if ``len(weights) == n_classes - 1``,
         then the last class weight is automatically inferred.
-        More than `n_samples` samples may be returned if the sum of `weights`
-        exceeds 1.
+        More than ``n_samples`` samples may be returned if the sum of
+        ``weights`` exceeds 1.
 
     flip_y : float, optional (default=0.01)
         The fraction of samples whose class are randomly exchanged. Larger
@@ -124,7 +129,7 @@ def make_classification(n_samples=100, n_features=20, n_informative=2,
         If int, random_state is the seed used by the random number generator;
         If RandomState instance, random_state is the random number generator;
         If None, the random number generator is the RandomState instance used
-        by `np.random`.
+        by ``np.random``.
 
     Returns
     -------
diff --git a/sklearn/dummy.py b/sklearn/dummy.py
index 8b0ce6713774..f9a4762806f1 100644
--- a/sklearn/dummy.py
+++ b/sklearn/dummy.py
@@ -445,7 +445,7 @@ def fit(self, X, y, sample_weight=None):
         self.constant_ = np.reshape(self.constant_, (1, -1))
         return self
 
-    def predict(self, X):
+    def predict(self, X, return_std=False):
         """
         Perform classification on test vectors X.
 
@@ -454,17 +454,26 @@ def predict(self, X):
         X : {array-like, object with finite length or shape}
             Training data, requires length = n_samples
 
+        return_std : boolean, optional
+            Whether to return the standard deviation of posterior prediction.
+            All zeros in this case.
+
         Returns
         -------
         y : array, shape = [n_samples]  or [n_samples, n_outputs]
             Predicted target values for X.
+
+        y_std : array, shape = [n_samples]  or [n_samples, n_outputs]
+            Standard deviation of predictive distribution of query points.
         """
         check_is_fitted(self, "constant_")
         n_samples = _num_samples(X)
 
-        y = np.ones((n_samples, 1)) * self.constant_
+        y = np.ones((n_samples, self.n_outputs_)) * self.constant_
+        y_std = np.zeros((n_samples, self.n_outputs_))
 
         if self.n_outputs_ == 1 and not self.output_2d_:
             y = np.ravel(y)
+            y_std = np.ravel(y_std)
 
-        return y
+        return (y, y_std) if return_std else y
diff --git a/sklearn/ensemble/bagging.py b/sklearn/ensemble/bagging.py
index b1e90fba846c..b8952484023f 100644
--- a/sklearn/ensemble/bagging.py
+++ b/sklearn/ensemble/bagging.py
@@ -480,7 +480,7 @@ class BaggingClassifier(BaseBagging, ClassifierMixin):
     warm_start : bool, optional (default=False)
         When set to True, reuse the solution of the previous call to fit
         and add more estimators to the ensemble, otherwise, just fit
-        a whole new ensemble.
+        a whole new ensemble. See :term:`the Glossary <warm_start>`.
 
         .. versionadded:: 0.17
            *warm_start* constructor parameter.
@@ -850,7 +850,7 @@ class BaggingRegressor(BaseBagging, RegressorMixin):
     warm_start : bool, optional (default=False)
         When set to True, reuse the solution of the previous call to fit
         and add more estimators to the ensemble, otherwise, just fit
-        a whole new ensemble.
+        a whole new ensemble. See :term:`the Glossary <warm_start>`.
 
     n_jobs : int, optional (default=1)
         The number of jobs to run in parallel for both `fit` and `predict`.
diff --git a/sklearn/ensemble/forest.py b/sklearn/ensemble/forest.py
index b80503d093b7..6d354ce5b0e5 100644
--- a/sklearn/ensemble/forest.py
+++ b/sklearn/ensemble/forest.py
@@ -767,7 +767,7 @@ class RandomForestClassifier(ForestClassifier):
         The number of features to consider when looking for the best split:
 
         - If int, then consider `max_features` features at each split.
-        - If float, then `max_features` is a percentage and
+        - If float, then `max_features` is a fraction and
           `int(max_features * n_features)` features are considered at each
           split.
         - If "auto", then `max_features=sqrt(n_features)`.
@@ -788,23 +788,23 @@ class RandomForestClassifier(ForestClassifier):
         The minimum number of samples required to split an internal node:
 
         - If int, then consider `min_samples_split` as the minimum number.
-        - If float, then `min_samples_split` is a percentage and
+        - If float, then `min_samples_split` is a fraction and
           `ceil(min_samples_split * n_samples)` are the minimum
           number of samples for each split.
 
         .. versionchanged:: 0.18
-           Added float values for percentages.
+           Added float values for fractions.
 
     min_samples_leaf : int, float, optional (default=1)
         The minimum number of samples required to be at a leaf node:
 
         - If int, then consider `min_samples_leaf` as the minimum number.
-        - If float, then `min_samples_leaf` is a percentage and
+        - If float, then `min_samples_leaf` is a fraction and
           `ceil(min_samples_leaf * n_samples)` are the minimum
           number of samples for each node.
 
         .. versionchanged:: 0.18
-           Added float values for percentages.
+           Added float values for fractions.
 
     min_weight_fraction_leaf : float, optional (default=0.)
         The minimum weighted fraction of the sum total of weights (of all
@@ -866,7 +866,7 @@ class RandomForestClassifier(ForestClassifier):
     warm_start : bool, optional (default=False)
         When set to ``True``, reuse the solution of the previous call to fit
         and add more estimators to the ensemble, otherwise, just fit a whole
-        new forest.
+        new forest. See :term:`the Glossary <warm_start>`.
 
     class_weight : dict, list of dicts, "balanced",
         "balanced_subsample" or None, optional (default=None)
@@ -1045,7 +1045,7 @@ class RandomForestRegressor(ForestRegressor):
         The number of features to consider when looking for the best split:
 
         - If int, then consider `max_features` features at each split.
-        - If float, then `max_features` is a percentage and
+        - If float, then `max_features` is a fraction and
           `int(max_features * n_features)` features are considered at each
           split.
         - If "auto", then `max_features=n_features`.
@@ -1066,23 +1066,23 @@ class RandomForestRegressor(ForestRegressor):
         The minimum number of samples required to split an internal node:
 
         - If int, then consider `min_samples_split` as the minimum number.
-        - If float, then `min_samples_split` is a percentage and
+        - If float, then `min_samples_split` is a fraction and
           `ceil(min_samples_split * n_samples)` are the minimum
           number of samples for each split.
 
         .. versionchanged:: 0.18
-           Added float values for percentages.
+           Added float values for fractions.
 
     min_samples_leaf : int, float, optional (default=1)
         The minimum number of samples required to be at a leaf node:
 
         - If int, then consider `min_samples_leaf` as the minimum number.
-        - If float, then `min_samples_leaf` is a percentage and
+        - If float, then `min_samples_leaf` is a fraction and
           `ceil(min_samples_leaf * n_samples)` are the minimum
           number of samples for each node.
 
         .. versionchanged:: 0.18
-           Added float values for percentages.
+           Added float values for fractions.
 
     min_weight_fraction_leaf : float, optional (default=0.)
         The minimum weighted fraction of the sum total of weights (of all
@@ -1144,7 +1144,7 @@ class RandomForestRegressor(ForestRegressor):
     warm_start : bool, optional (default=False)
         When set to ``True``, reuse the solution of the previous call to fit
         and add more estimators to the ensemble, otherwise, just fit a whole
-        new forest.
+        new forest. See :term:`the Glossary <warm_start>`.
 
     Attributes
     ----------
@@ -1276,7 +1276,7 @@ class ExtraTreesClassifier(ForestClassifier):
         The number of features to consider when looking for the best split:
 
         - If int, then consider `max_features` features at each split.
-        - If float, then `max_features` is a percentage and
+        - If float, then `max_features` is a fraction and
           `int(max_features * n_features)` features are considered at each
           split.
         - If "auto", then `max_features=sqrt(n_features)`.
@@ -1297,23 +1297,23 @@ class ExtraTreesClassifier(ForestClassifier):
         The minimum number of samples required to split an internal node:
 
         - If int, then consider `min_samples_split` as the minimum number.
-        - If float, then `min_samples_split` is a percentage and
+        - If float, then `min_samples_split` is a fraction and
           `ceil(min_samples_split * n_samples)` are the minimum
           number of samples for each split.
 
         .. versionchanged:: 0.18
-           Added float values for percentages.
+           Added float values for fractions.
 
     min_samples_leaf : int, float, optional (default=1)
         The minimum number of samples required to be at a leaf node:
 
         - If int, then consider `min_samples_leaf` as the minimum number.
-        - If float, then `min_samples_leaf` is a percentage and
+        - If float, then `min_samples_leaf` is a fraction and
           `ceil(min_samples_leaf * n_samples)` are the minimum
           number of samples for each node.
 
         .. versionchanged:: 0.18
-           Added float values for percentages.
+           Added float values for fractions.
 
     min_weight_fraction_leaf : float, optional (default=0.)
         The minimum weighted fraction of the sum total of weights (of all
@@ -1375,7 +1375,7 @@ class ExtraTreesClassifier(ForestClassifier):
     warm_start : bool, optional (default=False)
         When set to ``True``, reuse the solution of the previous call to fit
         and add more estimators to the ensemble, otherwise, just fit a whole
-        new forest.
+        new forest. See :term:`the Glossary <warm_start>`.
 
     class_weight : dict, list of dicts, "balanced", "balanced_subsample" or None, optional (default=None)
         Weights associated with classes in the form ``{class_label: weight}``.
@@ -1525,7 +1525,7 @@ class ExtraTreesRegressor(ForestRegressor):
         The number of features to consider when looking for the best split:
 
         - If int, then consider `max_features` features at each split.
-        - If float, then `max_features` is a percentage and
+        - If float, then `max_features` is a fraction and
           `int(max_features * n_features)` features are considered at each
           split.
         - If "auto", then `max_features=n_features`.
@@ -1546,23 +1546,23 @@ class ExtraTreesRegressor(ForestRegressor):
         The minimum number of samples required to split an internal node:
 
         - If int, then consider `min_samples_split` as the minimum number.
-        - If float, then `min_samples_split` is a percentage and
+        - If float, then `min_samples_split` is a fraction and
           `ceil(min_samples_split * n_samples)` are the minimum
           number of samples for each split.
 
         .. versionchanged:: 0.18
-           Added float values for percentages.
+           Added float values for fractions.
 
     min_samples_leaf : int, float, optional (default=1)
         The minimum number of samples required to be at a leaf node:
 
         - If int, then consider `min_samples_leaf` as the minimum number.
-        - If float, then `min_samples_leaf` is a percentage and
+        - If float, then `min_samples_leaf` is a fraction and
           `ceil(min_samples_leaf * n_samples)` are the minimum
           number of samples for each node.
 
         .. versionchanged:: 0.18
-           Added float values for percentages.
+           Added float values for fractions.
 
     min_weight_fraction_leaf : float, optional (default=0.)
         The minimum weighted fraction of the sum total of weights (of all
@@ -1623,7 +1623,7 @@ class ExtraTreesRegressor(ForestRegressor):
     warm_start : bool, optional (default=False)
         When set to ``True``, reuse the solution of the previous call to fit
         and add more estimators to the ensemble, otherwise, just fit a whole
-        new forest.
+        new forest. See :term:`the Glossary <warm_start>`.
 
     Attributes
     ----------
@@ -1736,23 +1736,23 @@ class RandomTreesEmbedding(BaseForest):
         The minimum number of samples required to split an internal node:
 
         - If int, then consider `min_samples_split` as the minimum number.
-        - If float, then `min_samples_split` is a percentage and
+        - If float, then `min_samples_split` is a fraction and
           `ceil(min_samples_split * n_samples)` is the minimum
           number of samples for each split.
 
         .. versionchanged:: 0.18
-           Added float values for percentages.
+           Added float values for fractions.
 
     min_samples_leaf : int, float, optional (default=1)
         The minimum number of samples required to be at a leaf node:
 
         - If int, then consider `min_samples_leaf` as the minimum number.
-        - If float, then `min_samples_leaf` is a percentage and
+        - If float, then `min_samples_leaf` is a fraction and
           `ceil(min_samples_leaf * n_samples)` is the minimum
           number of samples for each node.
 
         .. versionchanged:: 0.18
-           Added float values for percentages.
+           Added float values for fractions.
 
     min_weight_fraction_leaf : float, optional (default=0.)
         The minimum weighted fraction of the sum total of weights (of all
@@ -1814,7 +1814,7 @@ class RandomTreesEmbedding(BaseForest):
     warm_start : bool, optional (default=False)
         When set to ``True``, reuse the solution of the previous call to fit
         and add more estimators to the ensemble, otherwise, just fit a whole
-        new forest.
+        new forest. See :term:`the Glossary <warm_start>`.
 
     Attributes
     ----------
diff --git a/sklearn/ensemble/gradient_boosting.py b/sklearn/ensemble/gradient_boosting.py
index b252319e36b1..2043c1fa7ae6 100644
--- a/sklearn/ensemble/gradient_boosting.py
+++ b/sklearn/ensemble/gradient_boosting.py
@@ -1330,23 +1330,23 @@ class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):
         The minimum number of samples required to split an internal node:
 
         - If int, then consider `min_samples_split` as the minimum number.
-        - If float, then `min_samples_split` is a percentage and
+        - If float, then `min_samples_split` is a fraction and
           `ceil(min_samples_split * n_samples)` are the minimum
           number of samples for each split.
 
         .. versionchanged:: 0.18
-           Added float values for percentages.
+           Added float values for fractions.
 
     min_samples_leaf : int, float, optional (default=1)
         The minimum number of samples required to be at a leaf node:
 
         - If int, then consider `min_samples_leaf` as the minimum number.
-        - If float, then `min_samples_leaf` is a percentage and
+        - If float, then `min_samples_leaf` is a fraction and
           `ceil(min_samples_leaf * n_samples)` are the minimum
           number of samples for each node.
 
         .. versionchanged:: 0.18
-           Added float values for percentages.
+           Added float values for fractions.
 
     min_weight_fraction_leaf : float, optional (default=0.)
         The minimum weighted fraction of the sum total of weights (of all
@@ -1364,7 +1364,7 @@ class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):
         The number of features to consider when looking for the best split:
 
         - If int, then consider `max_features` features at each split.
-        - If float, then `max_features` is a percentage and
+        - If float, then `max_features` is a fraction and
           `int(max_features * n_features)` features are considered at each
           split.
         - If "auto", then `max_features=sqrt(n_features)`.
@@ -1424,7 +1424,7 @@ class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):
     warm_start : bool, default: False
         When set to ``True``, reuse the solution of the previous call to fit
         and add more estimators to the ensemble, otherwise, just erase the
-        previous solution.
+        previous solution. See :term:`the Glossary <warm_start>`.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
@@ -1784,23 +1784,23 @@ class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):
         The minimum number of samples required to split an internal node:
 
         - If int, then consider `min_samples_split` as the minimum number.
-        - If float, then `min_samples_split` is a percentage and
+        - If float, then `min_samples_split` is a fraction and
           `ceil(min_samples_split * n_samples)` are the minimum
           number of samples for each split.
 
         .. versionchanged:: 0.18
-           Added float values for percentages.
+           Added float values for fractions.
 
     min_samples_leaf : int, float, optional (default=1)
         The minimum number of samples required to be at a leaf node:
 
         - If int, then consider `min_samples_leaf` as the minimum number.
-        - If float, then `min_samples_leaf` is a percentage and
+        - If float, then `min_samples_leaf` is a fraction and
           `ceil(min_samples_leaf * n_samples)` are the minimum
           number of samples for each node.
 
         .. versionchanged:: 0.18
-           Added float values for percentages.
+           Added float values for fractions.
 
     min_weight_fraction_leaf : float, optional (default=0.)
         The minimum weighted fraction of the sum total of weights (of all
@@ -1818,7 +1818,7 @@ class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):
         The number of features to consider when looking for the best split:
 
         - If int, then consider `max_features` features at each split.
-        - If float, then `max_features` is a percentage and
+        - If float, then `max_features` is a fraction and
           `int(max_features * n_features)` features are considered at each
           split.
         - If "auto", then `max_features=n_features`.
@@ -1882,7 +1882,7 @@ class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):
     warm_start : bool, default: False
         When set to ``True``, reuse the solution of the previous call to fit
         and add more estimators to the ensemble, otherwise, just erase the
-        previous solution.
+        previous solution. See :term:`the Glossary <warm_start>`.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py
index fccb075381ab..636640b19511 100644
--- a/sklearn/ensemble/iforest.py
+++ b/sklearn/ensemble/iforest.py
@@ -17,6 +17,7 @@
 from ..tree import ExtraTreeRegressor
 from ..utils import check_random_state, check_array
 from ..utils.validation import check_is_fitted
+from ..base import OutlierMixin
 
 from .bagging import BaseBagging
 
@@ -25,7 +26,7 @@
 INTEGER_TYPES = (numbers.Integral, np.integer)
 
 
-class IsolationForest(BaseBagging):
+class IsolationForest(BaseBagging, OutlierMixin):
     """Isolation Forest Algorithm
 
     Return the anomaly score of each sample using the IsolationForest algorithm
diff --git a/sklearn/ensemble/tests/test_voting_classifier.py b/sklearn/ensemble/tests/test_voting_classifier.py
index 70d92132125a..ef6c0c739763 100644
--- a/sklearn/ensemble/tests/test_voting_classifier.py
+++ b/sklearn/ensemble/tests/test_voting_classifier.py
@@ -414,7 +414,7 @@ def test_transform():
         flatten_transform=False).fit(X, y)
 
     warn_msg = ("'flatten_transform' default value will be "
-                "changed to True in 0.21."
+                "changed to True in 0.21. "
                 "To silence this warning you may"
                 " explicitly set flatten_transform=False.")
     res = assert_warns_message(DeprecationWarning, warn_msg,
diff --git a/sklearn/ensemble/voting_classifier.py b/sklearn/ensemble/voting_classifier.py
index 00f72f009416..81db37de290d 100644
--- a/sklearn/ensemble/voting_classifier.py
+++ b/sklearn/ensemble/voting_classifier.py
@@ -296,7 +296,7 @@ def transform(self, X):
             probas = self._collect_probas(X)
             if self.flatten_transform is None:
                 warnings.warn("'flatten_transform' default value will be "
-                              "changed to True in 0.21."
+                              "changed to True in 0.21. "
                               "To silence this warning you may"
                               " explicitly set flatten_transform=False.",
                               DeprecationWarning)
diff --git a/sklearn/gaussian_process/gpc.py b/sklearn/gaussian_process/gpc.py
index a3b698241b2d..e1f37617b6d1 100644
--- a/sklearn/gaussian_process/gpc.py
+++ b/sklearn/gaussian_process/gpc.py
@@ -99,7 +99,8 @@ def optimizer(obj_func, initial_theta, bounds):
         on the Laplace approximation of the posterior mode is used as
         initialization for the next call of _posterior_mode(). This can speed
         up convergence when _posterior_mode is called several times on similar
-        problems as in hyperparameter optimization.
+        problems as in hyperparameter optimization. See :term:`the Glossary
+        <warm_start>`.
 
     copy_X_train : bool, optional (default: True)
         If True, a persistent copy of the training data is stored in the
@@ -506,7 +507,8 @@ def optimizer(obj_func, initial_theta, bounds):
         on the Laplace approximation of the posterior mode is used as
         initialization for the next call of _posterior_mode(). This can speed
         up convergence when _posterior_mode is called several times on similar
-        problems as in hyperparameter optimization.
+        problems as in hyperparameter optimization. See :term:`the Glossary
+        <warm_start>`.
 
     copy_X_train : bool, optional (default: True)
         If True, a persistent copy of the training data is stored in the
diff --git a/sklearn/impute.py b/sklearn/impute.py
index c177c11673ea..69fba61f3d8f 100644
--- a/sklearn/impute.py
+++ b/sklearn/impute.py
@@ -151,8 +151,10 @@ def fit(self, X, y=None):
         # transform(X), the imputation data will be computed in transform()
         # when the imputation is done per sample (i.e., when axis=1).
         if self.axis == 0:
-            X = check_array(X, accept_sparse='csc', dtype=np.float64,
-                            force_all_finite=False)
+            X = check_array(X, accept_sparse='csc', dtype=FLOAT_DTYPES,
+                            force_all_finite='allow-nan'
+                            if self.missing_values == 'NaN'
+                            or np.isnan(self.missing_values) else True)
 
             if sparse.issparse(X):
                 self.statistics_ = self._sparse_fit(X,
@@ -249,7 +251,9 @@ def _sparse_fit(self, X, strategy, missing_values, axis):
 
     def _dense_fit(self, X, strategy, missing_values, axis):
         """Fit the transformer on dense data."""
-        X = check_array(X, force_all_finite=False)
+        X = check_array(X, force_all_finite='allow-nan'
+                        if self.missing_values == 'NaN'
+                        or np.isnan(self.missing_values) else True)
         mask = _get_mask(X, missing_values)
         masked_X = ma.masked_array(X, mask=mask)
 
@@ -264,12 +268,6 @@ def _dense_fit(self, X, strategy, missing_values, axis):
 
         # Median
         elif strategy == "median":
-            if tuple(int(v) for v in np.__version__.split('.')[:2]) < (1, 5):
-                # In old versions of numpy, calling a median on an array
-                # containing nans returns nan. This is different is
-                # recent versions of numpy, which we want to mimic
-                masked_X.mask = np.logical_or(masked_X.mask,
-                                              np.isnan(X))
             median_masked = np.ma.median(masked_X, axis=axis)
             # Avoid the warning "Warning: converting a masked element to nan."
             median = np.ma.getdata(median_masked)
@@ -309,7 +307,10 @@ def transform(self, X):
         if self.axis == 0:
             check_is_fitted(self, 'statistics_')
             X = check_array(X, accept_sparse='csc', dtype=FLOAT_DTYPES,
-                            force_all_finite=False, copy=self.copy)
+                            force_all_finite='allow-nan'
+                            if self.missing_values == 'NaN'
+                            or np.isnan(self.missing_values) else True,
+                            copy=self.copy)
             statistics = self.statistics_
             if X.shape[1] != statistics.shape[0]:
                 raise ValueError("X has %d features per sample, expected %d"
@@ -320,7 +321,10 @@ def transform(self, X):
         # when the imputation is done per sample
         else:
             X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES,
-                            force_all_finite=False, copy=self.copy)
+                            force_all_finite='allow-nan'
+                            if self.missing_values == 'NaN'
+                            or np.isnan(self.missing_values) else True,
+                            copy=self.copy)
 
             if sparse.issparse(X):
                 statistics = self._sparse_fit(X,
@@ -338,7 +342,7 @@ def transform(self, X):
         invalid_mask = np.isnan(statistics)
         valid_mask = np.logical_not(invalid_mask)
         valid_statistics = statistics[valid_mask]
-        valid_statistics_indexes = np.where(valid_mask)[0]
+        valid_statistics_indexes = np.flatnonzero(valid_mask)
         missing = np.arange(X.shape[not self.axis])[invalid_mask]
 
         if self.axis == 0 and invalid_mask.any():
diff --git a/sklearn/linear_model/coordinate_descent.py b/sklearn/linear_model/coordinate_descent.py
index 32de16e2f8f2..c2c3afead51e 100644
--- a/sklearn/linear_model/coordinate_descent.py
+++ b/sklearn/linear_model/coordinate_descent.py
@@ -580,6 +580,7 @@ class ElasticNet(LinearModel, RegressorMixin):
     warm_start : bool, optional
         When set to ``True``, reuse the solution of the previous call to fit as
         initialization, otherwise, just erase the previous solution.
+        See :term:`the Glossary <warm_start>`.
 
     positive : bool, optional
         When set to ``True``, forces the coefficients to be positive.
@@ -762,8 +763,12 @@ def fit(self, X, y, check_input=True):
 
         if n_targets == 1:
             self.n_iter_ = self.n_iter_[0]
+            self.coef_ = coef_[0]
+            self.dual_gap_ = dual_gaps_[0]
+        else:
+            self.coef_ = coef_
+            self.dual_gap_ = dual_gaps_
 
-        self.coef_, self.dual_gap_ = map(np.squeeze, [coef_, dual_gaps_])
         self._set_intercept(X_offset, y_offset, X_scale)
 
         # workaround since _set_intercept will cast self.coef_ into X.dtype
@@ -855,6 +860,7 @@ class Lasso(ElasticNet):
     warm_start : bool, optional
         When set to True, reuse the solution of the previous call to fit as
         initialization, otherwise, just erase the previous solution.
+        See :term:`the Glossary <warm_start>`.
 
     positive : bool, optional
         When set to ``True``, forces the coefficients to be positive.
@@ -1648,6 +1654,7 @@ class MultiTaskElasticNet(Lasso):
     warm_start : bool, optional
         When set to ``True``, reuse the solution of the previous call to fit as
         initialization, otherwise, just erase the previous solution.
+        See :term:`the Glossary <warm_start>`.
 
     random_state : int, RandomState instance or None, optional, default None
         The seed of the pseudo random number generator that selects a random
@@ -1838,6 +1845,7 @@ class MultiTaskLasso(MultiTaskElasticNet):
     warm_start : bool, optional
         When set to ``True``, reuse the solution of the previous call to fit as
         initialization, otherwise, just erase the previous solution.
+        See :term:`the Glossary <warm_start>`.
 
     random_state : int, RandomState instance or None, optional, default None
         The seed of the pseudo random number generator that selects a random
diff --git a/sklearn/linear_model/huber.py b/sklearn/linear_model/huber.py
index f11fff115566..b6f4658ea573 100644
--- a/sklearn/linear_model/huber.py
+++ b/sklearn/linear_model/huber.py
@@ -158,6 +158,7 @@ class HuberRegressor(LinearModel, RegressorMixin, BaseEstimator):
         This is useful if the stored attributes of a previously used model
         has to be reused. If set to False, then the coefficients will
         be rewritten for every call to fit.
+        See :term:`the Glossary <warm_start>`.
 
     fit_intercept : bool, default True
         Whether or not to fit the intercept. This can be set to False
@@ -181,7 +182,11 @@ class HuberRegressor(LinearModel, RegressorMixin, BaseEstimator):
 
     n_iter_ : int
         Number of iterations that fmin_l_bfgs_b has run for.
-        Not available if SciPy version is 0.9 and below.
+
+        .. versionchanged:: 0.20
+
+            In SciPy <= 1.0.0 the number of lbfgs iterations may exceed
+            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.
 
     outliers_ : array, shape (n_samples,)
         A boolean mask which is set to True where the samples are identified
@@ -254,24 +259,18 @@ def fit(self, X, y, sample_weight=None):
         bounds = np.tile([-np.inf, np.inf], (parameters.shape[0], 1))
         bounds[-1][0] = np.finfo(np.float64).eps * 10
 
-        # Type Error caused in old versions of SciPy because of no
-        # maxiter argument ( <= 0.9).
-        try:
-            parameters, f, dict_ = optimize.fmin_l_bfgs_b(
-                _huber_loss_and_gradient, parameters,
-                args=(X, y, self.epsilon, self.alpha, sample_weight),
-                maxiter=self.max_iter, pgtol=self.tol, bounds=bounds,
-                iprint=0)
-        except TypeError:
-            parameters, f, dict_ = optimize.fmin_l_bfgs_b(
-                _huber_loss_and_gradient, parameters,
-                args=(X, y, self.epsilon, self.alpha, sample_weight),
-                bounds=bounds)
+        parameters, f, dict_ = optimize.fmin_l_bfgs_b(
+            _huber_loss_and_gradient, parameters,
+            args=(X, y, self.epsilon, self.alpha, sample_weight),
+            maxiter=self.max_iter, pgtol=self.tol, bounds=bounds,
+            iprint=0)
         if dict_['warnflag'] == 2:
             raise ValueError("HuberRegressor convergence failed:"
                              " l-BFGS-b solver terminated with %s"
                              % dict_['task'].decode('ascii'))
-        self.n_iter_ = dict_.get('nit', None)
+        # In scipy <= 1.0.0, nit may exceed maxiter.
+        # See https://github.com/scipy/scipy/issues/7854.
+        self.n_iter_ = min(dict_['nit'], self.max_iter)
         self.scale_ = parameters[-1]
         if self.fit_intercept:
             self.intercept_ = parameters[-2]
diff --git a/sklearn/linear_model/least_angle.py b/sklearn/linear_model/least_angle.py
index 1d53802deafb..929e6ab6d08b 100644
--- a/sklearn/linear_model/least_angle.py
+++ b/sklearn/linear_model/least_angle.py
@@ -169,13 +169,7 @@ def lars_path(X, y, Xy=None, Gram=None, max_iter=500,
 
     # will hold the cholesky factorization. Only lower part is
     # referenced.
-    # We are initializing this to "zeros" and not empty, because
-    # it is passed to scipy linalg functions and thus if it has NaNs,
-    # even if they are in the upper part that it not used, we
-    # get errors raised.
-    # Once we support only scipy > 0.12 we can use check_finite=False and
-    # go back to "empty"
-    L = np.zeros((max_features, max_features), dtype=X.dtype)
+    L = np.empty((max_features, max_features), dtype=X.dtype)
     swap, nrm2 = linalg.get_blas_funcs(('swap', 'nrm2'), (X,))
     solve_cholesky, = get_lapack_funcs(('potrs',), (X,))
 
diff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py
index 0bc0f0abefb0..26eaeaa029f1 100644
--- a/sklearn/linear_model/logistic.py
+++ b/sklearn/linear_model/logistic.py
@@ -703,24 +703,16 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
     n_iter = np.zeros(len(Cs), dtype=np.int32)
     for i, C in enumerate(Cs):
         if solver == 'lbfgs':
-            try:
-                w0, loss, info = optimize.fmin_l_bfgs_b(
-                    func, w0, fprime=None,
-                    args=(X, target, 1. / C, sample_weight),
-                    iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
-            except TypeError:
-                # old scipy doesn't have maxiter
-                w0, loss, info = optimize.fmin_l_bfgs_b(
-                    func, w0, fprime=None,
-                    args=(X, target, 1. / C, sample_weight),
-                    iprint=(verbose > 0) - 1, pgtol=tol)
+            w0, loss, info = optimize.fmin_l_bfgs_b(
+                func, w0, fprime=None,
+                args=(X, target, 1. / C, sample_weight),
+                iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
             if info["warnflag"] == 1 and verbose > 0:
                 warnings.warn("lbfgs failed to converge. Increase the number "
                               "of iterations.", ConvergenceWarning)
-            try:
-                n_iter_i = info['nit'] - 1
-            except:
-                n_iter_i = info['funcalls'] - 1
+            # In scipy <= 1.0.0, nit may exceed maxiter.
+            # See https://github.com/scipy/scipy/issues/7854.
+            n_iter_i = min(info['nit'], max_iter)
         elif solver == 'newton-cg':
             args = (X, target, 1. / C, sample_weight)
             w0, n_iter_i = newton_cg(hess, func, grad, w0, args=args,
@@ -1084,7 +1076,7 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,
     warm_start : bool, default: False
         When set to True, reuse the solution of the previous call to fit as
         initialization, otherwise, just erase the previous solution.
-        Useless for liblinear solver.
+        Useless for liblinear solver. See :term:`the Glossary <warm_start>`.
 
         .. versionadded:: 0.17
            *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.
@@ -1119,6 +1111,11 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,
         it returns only 1 element. For liblinear solver, only the maximum
         number of iteration across all classes is given.
 
+        .. versionchanged:: 0.20
+
+            In SciPy <= 1.0.0 the number of lbfgs iterations may exceed
+            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.
+
     See also
     --------
     SGDClassifier : incrementally trained logistic regression (when given
diff --git a/sklearn/linear_model/omp.py b/sklearn/linear_model/omp.py
index f1f7542ee3e6..298a1fa4259d 100644
--- a/sklearn/linear_model/omp.py
+++ b/sklearn/linear_model/omp.py
@@ -18,8 +18,6 @@
 from ..model_selection import check_cv
 from ..externals.joblib import Parallel, delayed
 
-solve_triangular_args = {'check_finite': False}
-
 premature = """ Orthogonal matching pursuit ended prematurely due to linear
 dependence in the dictionary. The requested precision might not have been met.
 """
@@ -85,12 +83,8 @@ def _cholesky_omp(X, y, n_nonzero_coefs, tol=None, copy_X=True,
     indices = np.arange(X.shape[1])  # keeping track of swapping
 
     max_features = X.shape[1] if tol is not None else n_nonzero_coefs
-    if solve_triangular_args:
-        # new scipy, don't need to initialize because check_finite=False
-        L = np.empty((max_features, max_features), dtype=X.dtype)
-    else:
-        # old scipy, we need the garbage upper triangle to be non-Inf
-        L = np.zeros((max_features, max_features), dtype=X.dtype)
+
+    L = np.empty((max_features, max_features), dtype=X.dtype)
 
     if return_path:
         coefs = np.empty_like(L)
@@ -109,7 +103,7 @@ def _cholesky_omp(X, y, n_nonzero_coefs, tol=None, copy_X=True,
                                     L[n_active, :n_active],
                                     trans=0, lower=1,
                                     overwrite_b=True,
-                                    **solve_triangular_args)
+                                    check_finite=False)
             v = nrm2(L[n_active, :n_active]) ** 2
             Lkk = linalg.norm(X[:, lam]) ** 2 - v
             if Lkk <= min_float:  # selected atoms are dependent
@@ -212,12 +206,9 @@ def _gram_omp(Gram, Xy, n_nonzero_coefs, tol_0=None, tol=None,
     n_active = 0
 
     max_features = len(Gram) if tol is not None else n_nonzero_coefs
-    if solve_triangular_args:
-        # new scipy, don't need to initialize because check_finite=False
-        L = np.empty((max_features, max_features), dtype=Gram.dtype)
-    else:
-        # old scipy, we need the garbage upper triangle to be non-Inf
-        L = np.zeros((max_features, max_features), dtype=Gram.dtype)
+
+    L = np.empty((max_features, max_features), dtype=Gram.dtype)
+
     L[0, 0] = 1.
     if return_path:
         coefs = np.empty_like(L)
@@ -234,7 +225,7 @@ def _gram_omp(Gram, Xy, n_nonzero_coefs, tol_0=None, tol=None,
                                     L[n_active, :n_active],
                                     trans=0, lower=1,
                                     overwrite_b=True,
-                                    **solve_triangular_args)
+                                    check_finite=False)
             v = nrm2(L[n_active, :n_active]) ** 2
             Lkk = Gram[lam, lam] - v
             if Lkk <= min_float:  # selected atoms are dependent
diff --git a/sklearn/linear_model/passive_aggressive.py b/sklearn/linear_model/passive_aggressive.py
index 37eb5b2ca58c..91bb4d7b6ac0 100644
--- a/sklearn/linear_model/passive_aggressive.py
+++ b/sklearn/linear_model/passive_aggressive.py
@@ -62,6 +62,7 @@ class PassiveAggressiveClassifier(BaseSGDClassifier):
     warm_start : bool, optional
         When set to True, reuse the solution of the previous call to fit as
         initialization, otherwise, just erase the previous solution.
+        See :term:`the Glossary <warm_start>`.
 
         Repeatedly calling fit or partial_fit when warm_start is True can
         result in a different solution than when calling fit a single time
@@ -285,6 +286,7 @@ class PassiveAggressiveRegressor(BaseSGDRegressor):
     warm_start : bool, optional
         When set to True, reuse the solution of the previous call to fit as
         initialization, otherwise, just erase the previous solution.
+        See :term:`the Glossary <warm_start>`.
 
         Repeatedly calling fit or partial_fit when warm_start is True can
         result in a different solution than when calling fit a single time
diff --git a/sklearn/linear_model/perceptron.py b/sklearn/linear_model/perceptron.py
index 28cb4561521f..2ecb5af96b90 100644
--- a/sklearn/linear_model/perceptron.py
+++ b/sklearn/linear_model/perceptron.py
@@ -71,7 +71,8 @@ class Perceptron(BaseSGDClassifier):
 
     warm_start : bool, optional
         When set to True, reuse the solution of the previous call to fit as
-        initialization, otherwise, just erase the previous solution.
+        initialization, otherwise, just erase the previous solution. See
+        :term:`the Glossary <warm_start>`.
 
     n_iter : int, optional
         The number of passes over the training data (aka epochs).
diff --git a/sklearn/linear_model/ridge.py b/sklearn/linear_model/ridge.py
index a43367c6e7fd..ca3d26bd0b6b 100644
--- a/sklearn/linear_model/ridge.py
+++ b/sklearn/linear_model/ridge.py
@@ -244,8 +244,8 @@ def ridge_regression(X, y, alpha, sample_weight=None, solver='auto',
           (possibility to set `tol` and `max_iter`).
 
         - 'lsqr' uses the dedicated regularized least-squares routine
-          scipy.sparse.linalg.lsqr. It is the fastest but may not be available
-          in old scipy versions. It also uses an iterative procedure.
+          scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative
+          procedure.
 
         - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses
           its improved, unbiased version named SAGA. Both methods also use an
@@ -360,11 +360,6 @@ def ridge_regression(X, y, alpha, sample_weight=None, solver='auto',
         else:
             solver = 'sparse_cg'
 
-    elif solver == 'lsqr' and not hasattr(sp_linalg, 'lsqr'):
-        warnings.warn("""lsqr not available on this machine, falling back
-                      to sparse_cg.""")
-        solver = 'sparse_cg'
-
     if has_sw:
         if np.atleast_1d(sample_weight).ndim > 1:
             raise ValueError("Sample weights must be 1D array or scalar")
@@ -578,8 +573,8 @@ class Ridge(_BaseRidge, RegressorMixin):
           (possibility to set `tol` and `max_iter`).
 
         - 'lsqr' uses the dedicated regularized least-squares routine
-          scipy.sparse.linalg.lsqr. It is the fastest but may not be available
-          in old scipy versions. It also uses an iterative procedure.
+          scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative
+          procedure.
 
         - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses
           its improved, unbiased version named SAGA. Both methods also use an
@@ -736,8 +731,8 @@ class RidgeClassifier(LinearClassifierMixin, _BaseRidge):
           (possibility to set `tol` and `max_iter`).
 
         - 'lsqr' uses the dedicated regularized least-squares routine
-          scipy.sparse.linalg.lsqr. It is the fastest but may not be available
-          in old scipy versions. It also uses an iterative procedure.
+          scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative
+          procedure.
 
         - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses
           its unbiased and more flexible version named SAGA. Both methods
diff --git a/sklearn/linear_model/stochastic_gradient.py b/sklearn/linear_model/stochastic_gradient.py
index 9aec1c60daa6..b3f8a34fe6a8 100644
--- a/sklearn/linear_model/stochastic_gradient.py
+++ b/sklearn/linear_model/stochastic_gradient.py
@@ -724,6 +724,7 @@ class SGDClassifier(BaseSGDClassifier):
     warm_start : bool, optional
         When set to True, reuse the solution of the previous call to fit as
         initialization, otherwise, just erase the previous solution.
+        See :term:`the Glossary <warm_start>`.
 
         Repeatedly calling fit or partial_fit when warm_start is True can
         result in a different solution than when calling fit a single time
@@ -1281,6 +1282,7 @@ class SGDRegressor(BaseSGDRegressor):
     warm_start : bool, optional
         When set to True, reuse the solution of the previous call to fit as
         initialization, otherwise, just erase the previous solution.
+        See :term:`the Glossary <warm_start>`.
 
         Repeatedly calling fit or partial_fit when warm_start is True can
         result in a different solution than when calling fit a single time
diff --git a/sklearn/linear_model/tests/test_coordinate_descent.py b/sklearn/linear_model/tests/test_coordinate_descent.py
index aeeb5d158228..a3b35f40a88d 100644
--- a/sklearn/linear_model/tests/test_coordinate_descent.py
+++ b/sklearn/linear_model/tests/test_coordinate_descent.py
@@ -803,3 +803,9 @@ def test_enet_l1_ratio():
         est.fit(X, y[:, None])
         est_desired.fit(X, y[:, None])
     assert_array_almost_equal(est.coef_, est_desired.coef_, decimal=5)
+
+
+def test_coef_shape_not_zero():
+    est_no_intercept = Lasso(fit_intercept=False)
+    est_no_intercept.fit(np.c_[np.ones(3)], np.ones(3))
+    assert est_no_intercept.coef_.shape == (1,)
diff --git a/sklearn/linear_model/tests/test_huber.py b/sklearn/linear_model/tests/test_huber.py
index 08f4fdf281b3..d7658396b3f2 100644
--- a/sklearn/linear_model/tests/test_huber.py
+++ b/sklearn/linear_model/tests/test_huber.py
@@ -42,6 +42,13 @@ def test_huber_equals_lr_for_high_epsilon():
     assert_almost_equal(huber.intercept_, lr.intercept_, 2)
 
 
+def test_huber_max_iter():
+    X, y = make_regression_with_outliers()
+    huber = HuberRegressor(max_iter=1)
+    huber.fit(X, y)
+    assert huber.n_iter_ == huber.max_iter
+
+
 def test_huber_gradient():
     # Test that the gradient calculated by _huber_loss_and_gradient is correct
     rng = np.random.RandomState(1)
@@ -168,9 +175,7 @@ def test_huber_warm_start():
     # these would be almost same but not equal.
     assert_array_almost_equal(huber_warm.coef_, huber_warm_coef, 1)
 
-    # No n_iter_ in old SciPy (<=0.9)
-    if huber_warm.n_iter_ is not None:
-        assert_equal(0, huber_warm.n_iter_)
+    assert huber_warm.n_iter_ == 0
 
 
 def test_huber_better_r2_score():
diff --git a/sklearn/metrics/ranking.py b/sklearn/metrics/ranking.py
index 1d8d37954b99..e78bbc721f72 100644
--- a/sklearn/metrics/ranking.py
+++ b/sklearn/metrics/ranking.py
@@ -202,6 +202,11 @@ def average_precision_score(y_true, y_score, average="macro",
     >>> average_precision_score(y_true, y_scores)  # doctest: +ELLIPSIS
     0.83...
 
+    Notes
+    -----
+    .. versionchanged:: 0.19
+      Instead of linearly interpolating between operating points, precisions
+      are weighted by the change in recall since the last operating point.
     """
     def _binary_uninterpolated_average_precision(
             y_true, y_score, sample_weight=None):
@@ -217,7 +222,8 @@ def _binary_uninterpolated_average_precision(
                                  sample_weight=sample_weight)
 
 
-def roc_auc_score(y_true, y_score, average="macro", sample_weight=None):
+def roc_auc_score(y_true, y_score, average="macro", sample_weight=None,
+                  max_fpr=None):
     """Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC)
     from prediction scores.
 
@@ -257,6 +263,10 @@ def roc_auc_score(y_true, y_score, average="macro", sample_weight=None):
     sample_weight : array-like of shape = [n_samples], optional
         Sample weights.
 
+    max_fpr : float > 0 and <= 1, optional
+        If not ``None``, the standardized partial AUC [3]_ over the range
+        [0, max_fpr] is returned.
+
     Returns
     -------
     auc : float
@@ -269,6 +279,9 @@ def roc_auc_score(y_true, y_score, average="macro", sample_weight=None):
     .. [2] Fawcett T. An introduction to ROC analysis[J]. Pattern Recognition
            Letters, 2006, 27(8):861-874.
 
+    .. [3] `Analyzing a portion of the ROC curve. McClish, 1989
+            <http://www.ncbi.nlm.nih.gov/pubmed/2668680>`_
+
     See also
     --------
     average_precision_score : Area under the precision-recall curve
@@ -292,7 +305,25 @@ def _binary_roc_auc_score(y_true, y_score, sample_weight=None):
 
         fpr, tpr, tresholds = roc_curve(y_true, y_score,
                                         sample_weight=sample_weight)
-        return auc(fpr, tpr)
+        if max_fpr is None or max_fpr == 1:
+            return auc(fpr, tpr)
+        if max_fpr <= 0 or max_fpr > 1:
+            raise ValueError("Expected max_frp in range ]0, 1], got: %r"
+                             % max_fpr)
+
+        # Add a single point at max_fpr by linear interpolation
+        stop = np.searchsorted(fpr, max_fpr, 'right')
+        x_interp = [fpr[stop - 1], fpr[stop]]
+        y_interp = [tpr[stop - 1], tpr[stop]]
+        tpr = np.append(tpr[:stop], np.interp(max_fpr, x_interp, y_interp))
+        fpr = np.append(fpr[:stop], max_fpr)
+        partial_auc = auc(fpr, tpr)
+
+        # McClish correction: standardize result to be 0.5 if non-discriminant
+        # and 1 if maximal
+        min_area = 0.5 * max_fpr**2
+        max_area = max_fpr
+        return 0.5 * (1 + (partial_auc - min_area) / (max_area - min_area))
 
     y_type = type_of_target(y_true)
     if y_type == "binary":
diff --git a/sklearn/metrics/tests/test_common.py b/sklearn/metrics/tests/test_common.py
index ad323a548362..84d5ebe3fbf5 100644
--- a/sklearn/metrics/tests/test_common.py
+++ b/sklearn/metrics/tests/test_common.py
@@ -163,6 +163,7 @@
     "samples_roc_auc": partial(roc_auc_score, average="samples"),
     "micro_roc_auc": partial(roc_auc_score, average="micro"),
     "macro_roc_auc": partial(roc_auc_score, average="macro"),
+    "partial_roc_auc": partial(roc_auc_score, max_fpr=0.5),
 
     "average_precision_score": average_precision_score,
     "weighted_average_precision_score":
@@ -220,6 +221,7 @@
     "weighted_roc_auc",
     "macro_roc_auc",
     "samples_roc_auc",
+    "partial_roc_auc",
 
     # with default average='binary', multiclass is prohibited
     "precision_score",
@@ -240,7 +242,7 @@
 
 # Threshold-based metrics with an "average" argument
 THRESHOLDED_METRICS_WITH_AVERAGING = [
-    "roc_auc_score", "average_precision_score",
+    "roc_auc_score", "average_precision_score", "partial_roc_auc",
 ]
 
 # Metrics with a "pos_label" argument
@@ -297,7 +299,7 @@
     "unnormalized_log_loss",
 
     "roc_auc_score", "weighted_roc_auc", "samples_roc_auc",
-    "micro_roc_auc", "macro_roc_auc",
+    "micro_roc_auc", "macro_roc_auc", "partial_roc_auc",
 
     "average_precision_score", "weighted_average_precision_score",
     "samples_average_precision_score", "micro_average_precision_score",
diff --git a/sklearn/metrics/tests/test_ranking.py b/sklearn/metrics/tests/test_ranking.py
index a17935ae7de1..07c35c609358 100644
--- a/sklearn/metrics/tests/test_ranking.py
+++ b/sklearn/metrics/tests/test_ranking.py
@@ -1,5 +1,6 @@
 from __future__ import division, print_function
 
+import pytest
 import numpy as np
 from itertools import product
 import warnings
@@ -148,6 +149,34 @@ def _average_precision_slow(y_true, y_score):
     return average_precision
 
 
+def _partial_roc_auc_score(y_true, y_predict, max_fpr):
+    """Alternative implementation to check for correctness of `roc_auc_score`
+    with `max_fpr` set.
+    """
+
+    def _partial_roc(y_true, y_predict, max_fpr):
+        fpr, tpr, _ = roc_curve(y_true, y_predict)
+        new_fpr = fpr[fpr <= max_fpr]
+        new_fpr = np.append(new_fpr, max_fpr)
+        new_tpr = tpr[fpr <= max_fpr]
+        idx_out = np.argmax(fpr > max_fpr)
+        idx_in = idx_out - 1
+        x_interp = [fpr[idx_in], fpr[idx_out]]
+        y_interp = [tpr[idx_in], tpr[idx_out]]
+        new_tpr = np.append(new_tpr, np.interp(max_fpr, x_interp, y_interp))
+        return (new_fpr, new_tpr)
+
+    new_fpr, new_tpr = _partial_roc(y_true, y_predict, max_fpr)
+    partial_auc = auc(new_fpr, new_tpr)
+
+    # Formula (5) from McClish 1989
+    fpr1 = 0
+    fpr2 = max_fpr
+    min_area = 0.5 * (fpr2 - fpr1) * (fpr2 + fpr1)
+    max_area = fpr2 - fpr1
+    return 0.5 * (1 + (partial_auc - min_area) / (max_area - min_area))
+
+
 def test_roc_curve():
     # Test Area under Receiver Operating Characteristic (ROC) curve
     y_true, _, probas_pred = make_prediction(binary=True)
@@ -1052,3 +1081,28 @@ def test_ranking_loss_ties_handling():
     assert_almost_equal(label_ranking_loss([[1, 0, 0]], [[0.25, 0.5, 0.5]]), 1)
     assert_almost_equal(label_ranking_loss([[1, 0, 1]], [[0.25, 0.5, 0.5]]), 1)
     assert_almost_equal(label_ranking_loss([[1, 1, 0]], [[0.25, 0.5, 0.5]]), 1)
+
+
+def test_partial_roc_auc_score():
+    # Check `roc_auc_score` for max_fpr != `None`
+    y_true = np.array([0, 0, 1, 1])
+    assert roc_auc_score(y_true, y_true, max_fpr=1) == 1
+    assert roc_auc_score(y_true, y_true, max_fpr=0.001) == 1
+    with pytest.raises(ValueError):
+        assert roc_auc_score(y_true, y_true, max_fpr=-0.1)
+    with pytest.raises(ValueError):
+        assert roc_auc_score(y_true, y_true, max_fpr=1.1)
+    with pytest.raises(ValueError):
+        assert roc_auc_score(y_true, y_true, max_fpr=0)
+
+    y_scores = np.array([0.1,  0,  0.1, 0.01])
+    roc_auc_with_max_fpr_one = roc_auc_score(y_true, y_scores, max_fpr=1)
+    unconstrained_roc_auc = roc_auc_score(y_true, y_scores)
+    assert roc_auc_with_max_fpr_one == unconstrained_roc_auc
+    assert roc_auc_score(y_true, y_scores, max_fpr=0.3) == 0.5
+
+    y_true, y_pred, _ = make_prediction(binary=True)
+    for max_fpr in np.linspace(1e-4, 1, 5):
+        assert_almost_equal(
+            roc_auc_score(y_true, y_pred, max_fpr=max_fpr),
+            _partial_roc_auc_score(y_true, y_pred, max_fpr))
diff --git a/sklearn/mixture/bayesian_mixture.py b/sklearn/mixture/bayesian_mixture.py
index 642c0aade30d..aef6828fa795 100644
--- a/sklearn/mixture/bayesian_mixture.py
+++ b/sklearn/mixture/bayesian_mixture.py
@@ -172,7 +172,8 @@ class BayesianGaussianMixture(BaseMixture):
     warm_start : bool, default to False.
         If 'warm_start' is True, the solution of the last fitting is used as
         initialization for the next call of fit(). This can speed up
-        convergence when fit is called several time on similar problems.
+        convergence when fit is called several times on similar problems.
+        See :term:`the Glossary <warm_start>`.
 
     verbose : int, default to 0.
         Enable verbose output. If 1 then it prints the current
diff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py
index ac18159a19f0..d58a9e326c69 100644
--- a/sklearn/mixture/gaussian_mixture.py
+++ b/sklearn/mixture/gaussian_mixture.py
@@ -508,7 +508,8 @@ class GaussianMixture(BaseMixture):
     warm_start : bool, default to False.
         If 'warm_start' is True, the solution of the last fitting is used as
         initialization for the next call of fit(). This can speed up
-        convergence when fit is called several time on similar problems.
+        convergence when fit is called several times on similar problems.
+        See :term:`the Glossary <warm_start>`.
 
     verbose : int, default to 0.
         Enable verbose output. If 1 then it prints the current
diff --git a/sklearn/model_selection/_search.py b/sklearn/model_selection/_search.py
index 8a4c13c640bc..4af8a03c3e19 100644
--- a/sklearn/model_selection/_search.py
+++ b/sklearn/model_selection/_search.py
@@ -273,7 +273,7 @@ def __len__(self):
 
 
 def fit_grid_point(X, y, estimator, parameters, train, test, scorer,
-                   verbose, error_score='raise', **fit_params):
+                   verbose, error_score='raise-deprecating', **fit_params):
     """Run fit on one set of parameters.
 
     Parameters
@@ -311,11 +311,12 @@ def fit_grid_point(X, y, estimator, parameters, train, test, scorer,
     **fit_params : kwargs
         Additional parameter passed to the fit function of the estimator.
 
-    error_score : 'raise' (default) or numeric
+    error_score : 'raise' or numeric
         Value to assign to the score if an error occurs in estimator fitting.
         If set to 'raise', the error is raised. If a numeric value is given,
         FitFailedWarning is raised. This parameter does not affect the refit
-        step, which will always raise the error.
+        step, which will always raise the error. Default is 'raise' but from
+        version 0.22 it will change to np.nan.
 
     Returns
     -------
@@ -391,7 +392,7 @@ class BaseSearchCV(six.with_metaclass(ABCMeta, BaseEstimator,
     def __init__(self, estimator, scoring=None,
                  fit_params=None, n_jobs=1, iid='warn',
                  refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs',
-                 error_score='raise', return_train_score=True):
+                 error_score='raise-deprecating', return_train_score=True):
 
         self.scoring = scoring
         self.estimator = estimator
@@ -909,11 +910,12 @@ class GridSearchCV(BaseSearchCV):
     verbose : integer
         Controls the verbosity: the higher, the more messages.
 
-    error_score : 'raise' (default) or numeric
+    error_score : 'raise' or numeric
         Value to assign to the score if an error occurs in estimator fitting.
         If set to 'raise', the error is raised. If a numeric value is given,
         FitFailedWarning is raised. This parameter does not affect the refit
-        step, which will always raise the error.
+        step, which will always raise the error. Default is 'raise' but from
+        version 0.22 it will change to np.nan.
 
     return_train_score : boolean, optional
         If ``False``, the ``cv_results_`` attribute will not include training
@@ -1085,7 +1087,7 @@ class GridSearchCV(BaseSearchCV):
 
     def __init__(self, estimator, param_grid, scoring=None, fit_params=None,
                  n_jobs=1, iid='warn', refit=True, cv=None, verbose=0,
-                 pre_dispatch='2*n_jobs', error_score='raise',
+                 pre_dispatch='2*n_jobs', error_score='raise-deprecating',
                  return_train_score="warn"):
         super(GridSearchCV, self).__init__(
             estimator=estimator, scoring=scoring, fit_params=fit_params,
@@ -1244,11 +1246,12 @@ class RandomizedSearchCV(BaseSearchCV):
         If None, the random number generator is the RandomState instance used
         by `np.random`.
 
-    error_score : 'raise' (default) or numeric
+    error_score : 'raise' or numeric
         Value to assign to the score if an error occurs in estimator fitting.
         If set to 'raise', the error is raised. If a numeric value is given,
         FitFailedWarning is raised. This parameter does not affect the refit
-        step, which will always raise the error.
+        step, which will always raise the error. Default is 'raise' but from
+        version 0.22 it will change to np.nan.
 
     return_train_score : boolean, optional
         If ``False``, the ``cv_results_`` attribute will not include training
@@ -1386,7 +1389,7 @@ class RandomizedSearchCV(BaseSearchCV):
     def __init__(self, estimator, param_distributions, n_iter=10, scoring=None,
                  fit_params=None, n_jobs=1, iid='warn', refit=True, cv=None,
                  verbose=0, pre_dispatch='2*n_jobs', random_state=None,
-                 error_score='raise', return_train_score="warn"):
+                 error_score='raise-deprecating', return_train_score="warn"):
         self.param_distributions = param_distributions
         self.n_iter = n_iter
         self.random_state = random_state
diff --git a/sklearn/model_selection/_split.py b/sklearn/model_selection/_split.py
index 616b105cb7da..fce5fc48c9cf 100644
--- a/sklearn/model_selection/_split.py
+++ b/sklearn/model_selection/_split.py
@@ -83,12 +83,6 @@ def split(self, X, y=None, groups=None):
 
         test : ndarray
             The testing set indices for that split.
-
-        Notes
-        -----
-        Randomized CV splitters may return different results for each call of
-        split. You can make the results identical by setting ``random_state``
-        to an integer.
         """
         X, y, groups = indexable(X, y, groups)
         indices = np.arange(_num_samples(X))
@@ -314,12 +308,6 @@ def split(self, X, y=None, groups=None):
 
         test : ndarray
             The testing set indices for that split.
-
-        Notes
-        -----
-        Randomized CV splitters may return different results for each call of
-        split. You can make the results identical by setting ``random_state``
-        to an integer.
         """
         X, y, groups = indexable(X, y, groups)
         n_samples = _num_samples(X)
@@ -402,6 +390,10 @@ class KFold(_BaseKFold):
     ``n_samples // n_splits + 1``, other folds have size
     ``n_samples // n_splits``, where ``n_samples`` is the number of samples.
 
+    Randomized CV splitters may return different results for each call of
+    split. You can make the results identical by setting ``random_state``
+    to an integer.
+
     See also
     --------
     StratifiedKFold
@@ -749,12 +741,6 @@ def split(self, X, y=None, groups=None):
 
         test : ndarray
             The testing set indices for that split.
-
-        Notes
-        -----
-        Randomized CV splitters may return different results for each call of
-        split. You can make the results identical by setting ``random_state``
-        to an integer.
         """
         X, y, groups = indexable(X, y, groups)
         n_samples = _num_samples(X)
@@ -1102,6 +1088,11 @@ class RepeatedKFold(_RepeatedSplits):
     TRAIN: [1 2] TEST: [0 3]
     TRAIN: [0 3] TEST: [1 2]
 
+    Notes
+    -----
+    Randomized CV splitters may return different results for each call of
+    split. You can make the results identical by setting ``random_state``
+    to an integer.
 
     See also
     --------
@@ -1149,6 +1140,11 @@ class RepeatedStratifiedKFold(_RepeatedSplits):
     TRAIN: [1 3] TEST: [0 2]
     TRAIN: [0 2] TEST: [1 3]
 
+    Notes
+    -----
+    Randomized CV splitters may return different results for each call of
+    split. You can make the results identical by setting ``random_state``
+    to an integer.
 
     See also
     --------
diff --git a/sklearn/model_selection/_validation.py b/sklearn/model_selection/_validation.py
index d6d4c0924b35..03bf0c92c8b0 100644
--- a/sklearn/model_selection/_validation.py
+++ b/sklearn/model_selection/_validation.py
@@ -39,7 +39,8 @@
 
 def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv=None,
                    n_jobs=1, verbose=0, fit_params=None,
-                   pre_dispatch='2*n_jobs', return_train_score="warn"):
+                   pre_dispatch='2*n_jobs', return_train_score="warn",
+                   return_estimator=False):
     """Evaluate metric(s) by cross-validation and also record fit/score times.
 
     Read more in the :ref:`User Guide <multimetric_cross_validation>`.
@@ -129,6 +130,9 @@ def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv=None,
         expensive and is not strictly required to select the parameters that
         yield the best generalization performance.
 
+    return_estimator : boolean, default False
+        Whether to return the estimators fitted on each split.
+
     Returns
     -------
     scores : dict of float arrays of shape=(n_splits,)
@@ -150,6 +154,10 @@ def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv=None,
                 The time for scoring the estimator on the test set for each
                 cv split. (Note time for scoring on the train set is not
                 included even if ``return_train_score`` is set to ``True``
+            ``estimator``
+                The estimator objects for each cv split.
+                This is available only if ``return_estimator`` parameter
+                is set to ``True``.
 
     Examples
     --------
@@ -203,14 +211,16 @@ def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv=None,
         delayed(_fit_and_score)(
             clone(estimator), X, y, scorers, train, test, verbose, None,
             fit_params, return_train_score=return_train_score,
-            return_times=True)
+            return_times=True, return_estimator=return_estimator)
         for train, test in cv.split(X, y, groups))
 
+    zipped_scores = list(zip(*scores))
     if return_train_score:
-        train_scores, test_scores, fit_times, score_times = zip(*scores)
+        train_scores = zipped_scores.pop(0)
         train_scores = _aggregate_score_dicts(train_scores)
-    else:
-        test_scores, fit_times, score_times = zip(*scores)
+    if return_estimator:
+        fitted_estimators = zipped_scores.pop()
+    test_scores, fit_times, score_times = zipped_scores
     test_scores = _aggregate_score_dicts(test_scores)
 
     # TODO: replace by a dict in 0.21
@@ -218,6 +228,9 @@ def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv=None,
     ret['fit_time'] = np.array(fit_times)
     ret['score_time'] = np.array(score_times)
 
+    if return_estimator:
+        ret['estimator'] = fitted_estimators
+
     for name in scorers:
         ret['test_%s' % name] = np.array(test_scores[name])
         if return_train_score:
@@ -347,7 +360,8 @@ def cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv=None,
 def _fit_and_score(estimator, X, y, scorer, train, test, verbose,
                    parameters, fit_params, return_train_score=False,
                    return_parameters=False, return_n_test_samples=False,
-                   return_times=False, error_score='raise'):
+                   return_times=False, return_estimator=False,
+                   error_score='raise-deprecating'):
     """Fit estimator and compute scores for a given dataset split.
 
     Parameters
@@ -381,11 +395,12 @@ def _fit_and_score(estimator, X, y, scorer, train, test, verbose,
     verbose : integer
         The verbosity level.
 
-    error_score : 'raise' (default) or numeric
+    error_score : 'raise' or numeric
         Value to assign to the score if an error occurs in estimator fitting.
         If set to 'raise', the error is raised. If a numeric value is given,
         FitFailedWarning is raised. This parameter does not affect the refit
-        step, which will always raise the error.
+        step, which will always raise the error. Default is 'raise' but from
+        version 0.22 it will change to np.nan.
 
     parameters : dict or None
         Parameters to be set on the estimator.
@@ -405,6 +420,9 @@ def _fit_and_score(estimator, X, y, scorer, train, test, verbose,
     return_times : boolean, optional, default: False
         Whether to return the fit/score times.
 
+    return_estimator : boolean, optional, default: False
+        Whether to return the fitted estimator.
+
     Returns
     -------
     train_scores : dict of scorer name -> float, optional
@@ -425,6 +443,9 @@ def _fit_and_score(estimator, X, y, scorer, train, test, verbose,
 
     parameters : dict or None, optional
         The parameters that have been evaluated.
+
+    estimator : estimator object
+        The fitted estimator
     """
     if verbose > 1:
         if parameters is None:
@@ -439,7 +460,6 @@ def _fit_and_score(estimator, X, y, scorer, train, test, verbose,
     fit_params = dict([(k, _index_param_value(X, v, train))
                       for k, v in fit_params.items()])
 
-    test_scores = {}
     train_scores = {}
     if parameters is not None:
         estimator.set_params(**parameters)
@@ -464,6 +484,14 @@ def _fit_and_score(estimator, X, y, scorer, train, test, verbose,
         score_time = 0.0
         if error_score == 'raise':
             raise
+        elif error_score == 'raise-deprecating':
+            warnings.warn("From version 0.22, errors during fit will result "
+                          "in a cross validation score of NaN by default. Use "
+                          "error_score='raise' if you want an exception "
+                          "raised or error_score=np.nan to adopt the "
+                          "behavior from version 0.22.",
+                          FutureWarning)
+            raise
         elif isinstance(error_score, numbers.Number):
             if is_multimetric:
                 test_scores = dict(zip(scorer.keys(),
@@ -513,6 +541,8 @@ def _fit_and_score(estimator, X, y, scorer, train, test, verbose,
         ret.extend([fit_time, score_time])
     if return_parameters:
         ret.append(parameters)
+    if return_estimator:
+        ret.append(estimator)
     return ret
 
 
diff --git a/sklearn/model_selection/tests/test_search.py b/sklearn/model_selection/tests/test_search.py
index 91081c9d3025..6412a14c036e 100644
--- a/sklearn/model_selection/tests/test_search.py
+++ b/sklearn/model_selection/tests/test_search.py
@@ -1353,6 +1353,14 @@ def get_cand_scores(i):
                if gs.cv_results_['param_parameter'][cand_i] ==
                FailingClassifier.FAILING_PARAMETER)
 
+    ranks = gs.cv_results_['rank_test_score']
+
+    # Check that succeeded estimators have lower ranks
+    assert ranks[0] <= 2 and ranks[1] <= 2
+    # Check that failed estimator has the highest rank
+    assert ranks[clf.FAILING_PARAMETER] == 3
+    assert gs.best_index_ != clf.FAILING_PARAMETER
+
 
 def test_grid_search_failing_classifier_raise():
     # GridSearchCV with on_error == 'raise' raises the error
diff --git a/sklearn/model_selection/tests/test_validation.py b/sklearn/model_selection/tests/test_validation.py
index 19b8b6510ca2..23d127631ad7 100644
--- a/sklearn/model_selection/tests/test_validation.py
+++ b/sklearn/model_selection/tests/test_validation.py
@@ -7,6 +7,7 @@
 import os
 from time import sleep
 
+import pytest
 import numpy as np
 from scipy.sparse import coo_matrix, csr_matrix
 from sklearn.exceptions import FitFailedWarning
@@ -368,20 +369,23 @@ def test_cross_validate():
         test_mse_scores = []
         train_r2_scores = []
         test_r2_scores = []
+        fitted_estimators = []
         for train, test in cv.split(X, y):
             est = clone(reg).fit(X[train], y[train])
             train_mse_scores.append(mse_scorer(est, X[train], y[train]))
             train_r2_scores.append(r2_scorer(est, X[train], y[train]))
             test_mse_scores.append(mse_scorer(est, X[test], y[test]))
             test_r2_scores.append(r2_scorer(est, X[test], y[test]))
+            fitted_estimators.append(est)
 
         train_mse_scores = np.array(train_mse_scores)
         test_mse_scores = np.array(test_mse_scores)
         train_r2_scores = np.array(train_r2_scores)
         test_r2_scores = np.array(test_r2_scores)
+        fitted_estimators = np.array(fitted_estimators)
 
         scores = (train_mse_scores, test_mse_scores, train_r2_scores,
-                  test_r2_scores)
+                  test_r2_scores, fitted_estimators)
 
         yield check_cross_validate_single_metric, est, X, y, scores
         yield check_cross_validate_multi_metric, est, X, y, scores
@@ -411,7 +415,7 @@ def test_cross_validate_return_train_score_warn():
 
 def check_cross_validate_single_metric(clf, X, y, scores):
     (train_mse_scores, test_mse_scores, train_r2_scores,
-     test_r2_scores) = scores
+     test_r2_scores, fitted_estimators) = scores
     # Test single metric evaluation when scoring is string or singleton list
     for (return_train_score, dict_len) in ((True, 4), (False, 3)):
         # Single metric passed as a string
@@ -443,11 +447,19 @@ def check_cross_validate_single_metric(clf, X, y, scores):
         assert_equal(len(r2_scores_dict), dict_len)
         assert_array_almost_equal(r2_scores_dict['test_r2'], test_r2_scores)
 
+    # Test return_estimator option
+    mse_scores_dict = cross_validate(clf, X, y, cv=5,
+                                     scoring='neg_mean_squared_error',
+                                     return_estimator=True)
+    for k, est in enumerate(mse_scores_dict['estimator']):
+        assert_almost_equal(est.coef_, fitted_estimators[k].coef_)
+        assert_almost_equal(est.intercept_, fitted_estimators[k].intercept_)
+
 
 def check_cross_validate_multi_metric(clf, X, y, scores):
     # Test multimetric evaluation when scoring is a list / dict
     (train_mse_scores, test_mse_scores, train_r2_scores,
-     test_r2_scores) = scores
+     test_r2_scores, fitted_estimators) = scores
     all_scoring = (('r2', 'neg_mean_squared_error'),
                    {'r2': make_scorer(r2_score),
                     'neg_mean_squared_error': 'neg_mean_squared_error'})
@@ -1450,3 +1462,23 @@ def test_fit_and_score():
     # check if the same warning is triggered
     assert_warns_message(FitFailedWarning, warning_message, _fit_and_score,
                          *fit_and_score_args, **fit_and_score_kwargs)
+
+    # check if exception is raised, with default error_score argument
+    assert_raise_message(ValueError, "Failing classifier failed as required",
+                         _fit_and_score, *fit_and_score_args)
+
+    # check if warning was raised, with default error_score argument
+    warning_message = ("From version 0.22, errors during fit will result "
+                       "in a cross validation score of NaN by default. Use "
+                       "error_score='raise' if you want an exception "
+                       "raised or error_score=np.nan to adopt the "
+                       "behavior from version 0.22.")
+    with pytest.raises(ValueError):
+        assert_warns_message(FutureWarning, warning_message, _fit_and_score,
+                             *fit_and_score_args)
+
+    fit_and_score_kwargs = {'error_score': 'raise'}
+    # check if exception was raised, with default error_score='raise'
+    assert_raise_message(ValueError, "Failing classifier failed as required",
+                         _fit_and_score, *fit_and_score_args,
+                         **fit_and_score_kwargs)
diff --git a/sklearn/neighbors/lof.py b/sklearn/neighbors/lof.py
index 41e156b42ea8..f7f1a16ebeb2 100644
--- a/sklearn/neighbors/lof.py
+++ b/sklearn/neighbors/lof.py
@@ -9,6 +9,7 @@
 from .base import NeighborsBase
 from .base import KNeighborsMixin
 from .base import UnsupervisedMixin
+from ..base import OutlierMixin
 
 from ..utils.validation import check_is_fitted
 from ..utils import check_array
@@ -16,7 +17,8 @@
 __all__ = ["LocalOutlierFactor"]
 
 
-class LocalOutlierFactor(NeighborsBase, KNeighborsMixin, UnsupervisedMixin):
+class LocalOutlierFactor(NeighborsBase, KNeighborsMixin, UnsupervisedMixin,
+                         OutlierMixin):
     """Unsupervised Outlier Detection using Local Outlier Factor (LOF)
 
     The anomaly score of each sample is called Local Outlier Factor.
diff --git a/sklearn/neighbors/tests/test_approximate.py b/sklearn/neighbors/tests/test_approximate.py
index 803e69b1f57a..153627189762 100644
--- a/sklearn/neighbors/tests/test_approximate.py
+++ b/sklearn/neighbors/tests/test_approximate.py
@@ -474,10 +474,8 @@ def test_graphs():
 
 
 def test_sparse_input():
-    # note: Fixed random state in sp.rand is not supported in older scipy.
-    #       The test should succeed regardless.
-    X1 = sp.rand(50, 100)
-    X2 = sp.rand(10, 100)
+    X1 = sp.rand(50, 100, random_state=0)
+    X2 = sp.rand(10, 100, random_state=1)
     forest_sparse = ignore_warnings(LSHForest, category=DeprecationWarning)(
         radius=1, random_state=0).fit(X1)
     forest_dense = ignore_warnings(LSHForest, category=DeprecationWarning)(
diff --git a/sklearn/neighbors/tests/test_ball_tree.py b/sklearn/neighbors/tests/test_ball_tree.py
index 6d4028386441..a91e4ac4edd2 100644
--- a/sklearn/neighbors/tests/test_ball_tree.py
+++ b/sklearn/neighbors/tests/test_ball_tree.py
@@ -192,11 +192,7 @@ def test_gaussian_kde(n_samples=1000):
 
     for h in [0.01, 0.1, 1]:
         bt = BallTree(x_in[:, None])
-        try:
-            gkde = gaussian_kde(x_in, bw_method=h / np.std(x_in))
-        except TypeError:
-            raise SkipTest("Old version of scipy, doesn't accept "
-                           "explicit bandwidth.")
+        gkde = gaussian_kde(x_in, bw_method=h / np.std(x_in))
 
         dens_bt = bt.kernel_density(x_out[:, None], h) / n_samples
         dens_gkde = gkde.evaluate(x_out)
diff --git a/sklearn/neighbors/tests/test_kd_tree.py b/sklearn/neighbors/tests/test_kd_tree.py
index af42e46143fe..e1b7cb196598 100644
--- a/sklearn/neighbors/tests/test_kd_tree.py
+++ b/sklearn/neighbors/tests/test_kd_tree.py
@@ -145,10 +145,7 @@ def test_gaussian_kde(n_samples=1000):
 
     for h in [0.01, 0.1, 1]:
         kdt = KDTree(x_in[:, None])
-        try:
-            gkde = gaussian_kde(x_in, bw_method=h / np.std(x_in))
-        except TypeError:
-            raise SkipTest("Old scipy, does not accept explicit bandwidth.")
+        gkde = gaussian_kde(x_in, bw_method=h / np.std(x_in))
 
         dens_kdt = kdt.kernel_density(x_out[:, None], h) / n_samples
         dens_gkde = gkde.evaluate(x_out)
diff --git a/sklearn/neural_network/multilayer_perceptron.py b/sklearn/neural_network/multilayer_perceptron.py
index c693c1161470..de559dc67e18 100644
--- a/sklearn/neural_network/multilayer_perceptron.py
+++ b/sklearn/neural_network/multilayer_perceptron.py
@@ -94,11 +94,6 @@ def _forward_pass(self, activations):
         ----------
         activations : list, length = n_layers - 1
             The ith element of the list holds the values of the ith layer.
-
-        with_output_activation : bool, default True
-            If True, the output passes through the output activation
-            function, which is either the softmax function or the
-            logistic function
         """
         hidden_activation = ACTIVATIONS[self.activation]
         # Iterate over the hidden layers
@@ -143,7 +138,7 @@ def _loss_grad_lbfgs(self, packed_coef_inter, X, y, activations, deltas,
 
         Parameters
         ----------
-        packed_parameters : array-like
+        packed_coef_inter : array-like
             A vector comprising the flattened coefficients and intercepts.
 
         X : {array-like, sparse matrix}, shape (n_samples, n_features)
@@ -162,7 +157,7 @@ def _loss_grad_lbfgs(self, packed_coef_inter, X, y, activations, deltas,
             in each layer, where z = wx + b is the value of a particular layer
             before passing through the activation function
 
-        coef_grad : list, length = n_layers - 1
+        coef_grads : list, length = n_layers - 1
             The ith element contains the amount of change used to update the
             coefficient parameters of the ith layer in an iteration.
 
@@ -174,7 +169,6 @@ def _loss_grad_lbfgs(self, packed_coef_inter, X, y, activations, deltas,
         -------
         loss : float
         grad : array-like, shape (number of nodes of all layers,)
-
         """
         self._unpack(packed_coef_inter)
         loss, coef_grads, intercept_grads = self._backprop(
@@ -206,7 +200,7 @@ def _backprop(self, X, y, activations, deltas, coef_grads,
             in each layer, where z = wx + b is the value of a particular layer
             before passing through the activation function
 
-        coef_grad : list, length = n_layers - 1
+        coef_grads : list, length = n_layers - 1
             The ith element contains the amount of change used to update the
             coefficient parameters of the ith layer in an iteration.
 
@@ -300,17 +294,14 @@ def _initialize(self, y, layer_units):
                 self.best_loss_ = np.inf
 
     def _init_coef(self, fan_in, fan_out):
+        # Use the initialization method recommended by
+        # Glorot et al.
+        factor = 6.
         if self.activation == 'logistic':
-            # Use the initialization method recommended by
-            # Glorot et al.
-            init_bound = np.sqrt(2. / (fan_in + fan_out))
-        elif self.activation in ('identity', 'tanh', 'relu'):
-            init_bound = np.sqrt(6. / (fan_in + fan_out))
-        else:
-            # this was caught earlier, just to make sure
-            raise ValueError("Unknown activation function %s" %
-                             self.activation)
+            factor = 2.
+        init_bound = np.sqrt(factor / (fan_in + fan_out))
 
+        # Generate weights and bias:
         coef_init = self._random_state.uniform(-init_bound, init_bound,
                                                (fan_in, fan_out))
         intercept_init = self._random_state.uniform(-init_bound, init_bound,
@@ -797,7 +788,7 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
     warm_start : bool, optional, default False
         When set to True, reuse the solution of the previous
         call to fit as initialization, otherwise, just erase the
-        previous solution.
+        previous solution. See :term:`the Glossary <warm_start>`.
 
     momentum : float, default 0.9
         Momentum for gradient descent update. Should be between 0 and 1. Only
@@ -893,7 +884,6 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
 
     Kingma, Diederik, and Jimmy Ba. "Adam: A method for stochastic
         optimization." arXiv preprint arXiv:1412.6980 (2014).
-
     """
     def __init__(self, hidden_layer_sizes=(100,), activation="relu",
                  solver='adam', alpha=0.0001,
@@ -938,7 +928,7 @@ def _validate_input(self, X, y, incremental):
                                  (self.classes_, classes))
         else:
             classes = unique_labels(y)
-            if np.setdiff1d(classes, self.classes_, assume_unique=True):
+            if len(np.setdiff1d(classes, self.classes_, assume_unique=True)):
                 raise ValueError("`y` has classes not in `self.classes_`."
                                  " `self.classes_` has %s. 'y' has %s." %
                                  (self.classes_, classes))
@@ -998,7 +988,7 @@ def partial_fit(self):
         y : array-like, shape (n_samples,)
             The target values.
 
-        classes : array, shape (n_classes)
+        classes : array, shape (n_classes), default None
             Classes across all calls to partial_fit.
             Can be obtained via `np.unique(y_all)`, where y_all is the
             target vector of the entire dataset.
@@ -1181,7 +1171,7 @@ class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):
     warm_start : bool, optional, default False
         When set to True, reuse the solution of the previous
         call to fit as initialization, otherwise, just erase the
-        previous solution.
+        previous solution. See :term:`the Glossary <warm_start>`.
 
     momentum : float, default 0.9
         Momentum for gradient descent update.  Should be between 0 and 1. Only
@@ -1274,7 +1264,6 @@ class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):
 
     Kingma, Diederik, and Jimmy Ba. "Adam: A method for stochastic
         optimization." arXiv preprint arXiv:1412.6980 (2014).
-
     """
     def __init__(self, hidden_layer_sizes=(100,), activation="relu",
                  solver='adam', alpha=0.0001,
diff --git a/sklearn/pipeline.py b/sklearn/pipeline.py
index 796de4d5543b..67649d76d428 100644
--- a/sklearn/pipeline.py
+++ b/sklearn/pipeline.py
@@ -287,7 +287,7 @@ def fit_transform(self, X, y=None, **fit_params):
             return last_step.fit(Xt, y, **fit_params).transform(Xt)
 
     @if_delegate_has_method(delegate='_final_estimator')
-    def predict(self, X):
+    def predict(self, X, **predict_params):
         """Apply transforms to the data, and predict with the final estimator
 
         Parameters
@@ -296,6 +296,14 @@ def predict(self, X):
             Data to predict on. Must fulfill input requirements of first step
             of the pipeline.
 
+        **predict_params : dict of string -> object
+            Parameters to the ``predict`` called at the end of all
+            transformations in the pipeline. Note that while this may be
+            used to return uncertainties from some models with return_std
+            or return_cov, uncertainties that are generated by the
+            transformations in the pipeline are not propagated to the
+            final estimator.
+
         Returns
         -------
         y_pred : array-like
@@ -304,7 +312,7 @@ def predict(self, X):
         for name, transform in self.steps[:-1]:
             if transform is not None:
                 Xt = transform.transform(Xt)
-        return self.steps[-1][-1].predict(Xt)
+        return self.steps[-1][-1].predict(Xt, **predict_params)
 
     @if_delegate_has_method(delegate='_final_estimator')
     def fit_predict(self, X, y=None, **fit_params):
diff --git a/sklearn/preprocessing/data.py b/sklearn/preprocessing/data.py
index bbd2fae10c0e..9e5f6d30293f 100644
--- a/sklearn/preprocessing/data.py
+++ b/sklearn/preprocessing/data.py
@@ -13,6 +13,7 @@
 import numbers
 import warnings
 from itertools import combinations_with_replacement as combinations_w_r
+from distutils.version import LooseVersion
 
 import numpy as np
 from scipy import sparse
@@ -2222,9 +2223,11 @@ def _dense_fit(self, X, random_state):
                           " sparse matrix. This parameter has no effect.")
 
         n_samples, n_features = X.shape
-        # for compatibility issue with numpy<=1.8.X, references
-        # need to be a list scaled between 0 and 100
-        references = (self.references_ * 100).tolist()
+        references = self.references_ * 100
+        # numpy < 1.9 bug: np.percentile 2nd argument needs to be a list
+        if LooseVersion(np.__version__) < '1.9':
+            references = references.tolist()
+
         self.quantiles_ = []
         for col in X.T:
             if self.subsample < n_samples:
@@ -2245,10 +2248,11 @@ def _sparse_fit(self, X, random_state):
             needs to be nonnegative.
         """
         n_samples, n_features = X.shape
+        references = self.references_ * 100
+        # numpy < 1.9 bug: np.percentile 2nd argument needs to be a list
+        if LooseVersion(np.__version__) < '1.9':
+            references = references.tolist()
 
-        # for compatibility issue with numpy<=1.8.X, references
-        # need to be a list scaled between 0 and 100
-        references = list(map(lambda x: x * 100, self.references_))
         self.quantiles_ = []
         for feature_idx in range(n_features):
             column_nnz_data = X.data[X.indptr[feature_idx]:
@@ -2333,8 +2337,6 @@ def _transform_col(self, X_col, quantiles, inverse):
             output_distribution = self.output_distribution
         output_distribution = getattr(stats, output_distribution)
 
-        # older version of scipy do not handle tuple as fill_value
-        # clipping the value before transform solve the issue
         if not inverse:
             lower_bound_x = quantiles[0]
             upper_bound_x = quantiles[-1]
diff --git a/sklearn/preprocessing/imputation.py b/sklearn/preprocessing/imputation.py
index bcb2efa8072a..4318122d4be6 100644
--- a/sklearn/preprocessing/imputation.py
+++ b/sklearn/preprocessing/imputation.py
@@ -267,12 +267,6 @@ def _dense_fit(self, X, strategy, missing_values, axis):
 
         # Median
         elif strategy == "median":
-            if tuple(int(v) for v in np.__version__.split('.')[:2]) < (1, 5):
-                # In old versions of numpy, calling a median on an array
-                # containing nans returns nan. This is different is
-                # recent versions of numpy, which we want to mimic
-                masked_X.mask = np.logical_or(masked_X.mask,
-                                              np.isnan(X))
             median_masked = np.ma.median(masked_X, axis=axis)
             # Avoid the warning "Warning: converting a masked element to nan."
             median = np.ma.getdata(median_masked)
diff --git a/sklearn/preprocessing/tests/test_data.py b/sklearn/preprocessing/tests/test_data.py
index f4c3d3d57177..03947d2f923e 100644
--- a/sklearn/preprocessing/tests/test_data.py
+++ b/sklearn/preprocessing/tests/test_data.py
@@ -1094,9 +1094,7 @@ def test_quantile_transform_subsampling():
 
     # sparse support
 
-    # TODO: rng should be seeded once we drop support for older versions of
-    # scipy (< 0.13) that don't support seeding.
-    X = sparse.rand(n_samples, 1, density=.99, format='csc')
+    X = sparse.rand(n_samples, 1, density=.99, format='csc', random_state=0)
     inf_norm_arr = []
     for random_state in range(ROUND):
         transformer = QuantileTransformer(random_state=random_state,
diff --git a/sklearn/svm/classes.py b/sklearn/svm/classes.py
index 2f2bd49be259..7a7955d30d29 100644
--- a/sklearn/svm/classes.py
+++ b/sklearn/svm/classes.py
@@ -2,7 +2,7 @@
 import numpy as np
 
 from .base import _fit_liblinear, BaseSVC, BaseLibSVM
-from ..base import BaseEstimator, RegressorMixin
+from ..base import BaseEstimator, RegressorMixin, OutlierMixin
 from ..linear_model.base import LinearClassifierMixin, SparseCoefMixin, \
     LinearModel
 from ..utils import check_X_y
@@ -975,7 +975,7 @@ def __init__(self, nu=0.5, C=1.0, kernel='rbf', degree=3,
             verbose=verbose, max_iter=max_iter, random_state=None)
 
 
-class OneClassSVM(BaseLibSVM):
+class OneClassSVM(BaseLibSVM, OutlierMixin):
     """Unsupervised Outlier Detection.
 
     Estimate the support of a high-dimensional distribution.
diff --git a/sklearn/tests/test_dummy.py b/sklearn/tests/test_dummy.py
index 4ad877146c30..5d955f51017a 100644
--- a/sklearn/tests/test_dummy.py
+++ b/sklearn/tests/test_dummy.py
@@ -620,3 +620,16 @@ def test_dummy_classifier_on_3D_array():
     y_pred_proba = cls.predict_proba(X)
     assert_array_equal(y_pred, y_expected)
     assert_array_equal(y_pred_proba, y_proba_expected)
+
+
+def test_dummy_regressor_return_std():
+    X = [[0]] * 3  # ignored
+    y = np.array([2, 2, 2])
+    y_std_expected = np.array([0, 0, 0])
+    cls = DummyRegressor()
+    cls.fit(X, y)
+    y_pred_list = cls.predict(X, return_std=True)
+    # there should be two elements when return_std is True
+    assert_equal(len(y_pred_list), 2)
+    # the second element should be all zeros
+    assert_array_equal(y_pred_list[1], y_std_expected)
diff --git a/sklearn/tests/test_impute.py b/sklearn/tests/test_impute.py
index 9c73770062cc..802ab82e406e 100644
--- a/sklearn/tests/test_impute.py
+++ b/sklearn/tests/test_impute.py
@@ -92,40 +92,6 @@ def test_imputation_shape():
         assert_equal(X_imputed.shape, (10, 2))
 
 
-def test_imputation_mean_median_only_zero():
-    # Test imputation using the mean and median strategies, when
-    # missing_values == 0.
-    X = np.array([
-        [np.nan, 0, 0, 0, 5],
-        [np.nan, 1, 0, np.nan, 3],
-        [np.nan, 2, 0, 0, 0],
-        [np.nan, 6, 0, 5, 13],
-    ])
-
-    X_imputed_mean = np.array([
-        [3, 5],
-        [1, 3],
-        [2, 7],
-        [6, 13],
-    ])
-    statistics_mean = [np.nan, 3, np.nan, np.nan, 7]
-
-    # Behaviour of median with NaN is undefined, e.g. different results in
-    # np.median and np.ma.median
-    X_for_median = X[:, [0, 1, 2, 4]]
-    X_imputed_median = np.array([
-        [2, 5],
-        [1, 3],
-        [2, 5],
-        [6, 13],
-    ])
-    statistics_median = [np.nan, 2, np.nan, 5]
-
-    _check_statistics(X, X_imputed_mean, "mean", statistics_mean, 0)
-    _check_statistics(X_for_median, X_imputed_median, "median",
-                      statistics_median, 0)
-
-
 def safe_median(arr, *args, **kwargs):
     # np.median([]) raises a TypeError for numpy >= 1.10.1
     length = arr.size if hasattr(arr, 'size') else len(arr)
@@ -276,26 +242,6 @@ def test_imputation_pipeline_grid_search():
     gs.fit(X, Y)
 
 
-def test_imputation_pickle():
-    # Test for pickling imputers.
-    import pickle
-
-    X = sparse_random_matrix(100, 100, density=0.10)
-
-    for strategy in ["mean", "median", "most_frequent"]:
-        imputer = SimpleImputer(missing_values=0, strategy=strategy)
-        imputer.fit(X)
-
-        imputer_pickled = pickle.loads(pickle.dumps(imputer))
-
-        assert_array_almost_equal(
-            imputer.transform(X.copy()),
-            imputer_pickled.transform(X.copy()),
-            err_msg="Fail to transform the data after pickling "
-            "(strategy = %s)" % (strategy)
-        )
-
-
 def test_imputation_copy():
     # Test imputation with copy
     X_orig = sparse_random_matrix(5, 5, density=0.75, random_state=0)
diff --git a/sklearn/tests/test_kernel_ridge.py b/sklearn/tests/test_kernel_ridge.py
index a185d58a5ce5..21f8e162deed 100644
--- a/sklearn/tests/test_kernel_ridge.py
+++ b/sklearn/tests/test_kernel_ridge.py
@@ -18,6 +18,7 @@
 from sklearn.kernel_ridge import KernelRidgeClassifier
 from sklearn.linear_model import Ridge
 from sklearn.kernel_ridge import KernelRidge
+from sklearn.preprocessing import label_binarize
 
 # toy sample
 X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]
@@ -129,3 +130,14 @@ def test_kernel_ridge_multi_output():
                         alpha=1).fit(X_reg, y_reg).predict(X_reg)
     pred3 = np.array([pred3, pred3]).T
     assert_array_almost_equal(pred2, pred3)
+
+
+def test_ridge_regression_classifier():
+    # This test asserts that KernelRidge and KernelRidgeClassifier
+    # have the same dual_coef_ when classification target are
+    # specified as regressions
+    krc = KernelRidgeClassifier(kernel='linear', alpha=1).fit(X, Y)
+    Y_krr = label_binarize(Y, classes=np.array([1, 2]),
+                           neg_label=-1, pos_label=1).ravel()
+    krr = KernelRidge(kernel='linear', alpha=1).fit(X, Y_krr)
+    assert_array_equal(krc.dual_coef_, krr.dual_coef_)
diff --git a/sklearn/tests/test_pipeline.py b/sklearn/tests/test_pipeline.py
index ab2108ed690f..95172b103be7 100644
--- a/sklearn/tests/test_pipeline.py
+++ b/sklearn/tests/test_pipeline.py
@@ -144,6 +144,17 @@ def fit(self, X, y):
         return self
 
 
+class DummyEstimatorParams(BaseEstimator):
+    """Mock classifier that takes params on predict"""
+
+    def fit(self, X, y):
+        return self
+
+    def predict(self, X, got_attribute=False):
+        self.got_attribute = got_attribute
+        return self
+
+
 def test_pipeline_init():
     # Test the various init parameters of the pipeline.
     assert_raises(TypeError, Pipeline)
@@ -398,6 +409,16 @@ def test_fit_predict_with_intermediate_fit_params():
     assert_false('should_succeed' in pipe.named_steps['transf'].fit_params)
 
 
+def test_predict_with_predict_params():
+    # tests that Pipeline passes predict_params to the final estimator
+    # when predict is invoked
+    pipe = Pipeline([('transf', Transf()), ('clf', DummyEstimatorParams())])
+    pipe.fit(None, None)
+    pipe.predict(X=None, got_attribute=True)
+
+    assert_true(pipe.named_steps['clf'].got_attribute)
+
+
 def test_feature_union():
     # basic sanity check for feature union
     iris = load_iris()
diff --git a/sklearn/tree/tree.py b/sklearn/tree/tree.py
index 6078d0ccdd14..dc52eee06814 100644
--- a/sklearn/tree/tree.py
+++ b/sklearn/tree/tree.py
@@ -540,23 +540,23 @@ class DecisionTreeClassifier(BaseDecisionTree, ClassifierMixin):
         The minimum number of samples required to split an internal node:
 
         - If int, then consider `min_samples_split` as the minimum number.
-        - If float, then `min_samples_split` is a percentage and
+        - If float, then `min_samples_split` is a fraction and
           `ceil(min_samples_split * n_samples)` are the minimum
           number of samples for each split.
 
         .. versionchanged:: 0.18
-           Added float values for percentages.
+           Added float values for fractions.
 
     min_samples_leaf : int, float, optional (default=1)
         The minimum number of samples required to be at a leaf node:
 
         - If int, then consider `min_samples_leaf` as the minimum number.
-        - If float, then `min_samples_leaf` is a percentage and
+        - If float, then `min_samples_leaf` is a fraction and
           `ceil(min_samples_leaf * n_samples)` are the minimum
           number of samples for each node.
 
         .. versionchanged:: 0.18
-           Added float values for percentages.
+           Added float values for fractions.
 
     min_weight_fraction_leaf : float, optional (default=0.)
         The minimum weighted fraction of the sum total of weights (of all
@@ -567,7 +567,7 @@ class DecisionTreeClassifier(BaseDecisionTree, ClassifierMixin):
         The number of features to consider when looking for the best split:
 
             - If int, then consider `max_features` features at each split.
-            - If float, then `max_features` is a percentage and
+            - If float, then `max_features` is a fraction and
               `int(max_features * n_features)` features are considered at each
               split.
             - If "auto", then `max_features=sqrt(n_features)`.
@@ -905,23 +905,23 @@ class DecisionTreeRegressor(BaseDecisionTree, RegressorMixin):
         The minimum number of samples required to split an internal node:
 
         - If int, then consider `min_samples_split` as the minimum number.
-        - If float, then `min_samples_split` is a percentage and
+        - If float, then `min_samples_split` is a fraction and
           `ceil(min_samples_split * n_samples)` are the minimum
           number of samples for each split.
 
         .. versionchanged:: 0.18
-           Added float values for percentages.
+           Added float values for fractions.
 
     min_samples_leaf : int, float, optional (default=1)
         The minimum number of samples required to be at a leaf node:
 
         - If int, then consider `min_samples_leaf` as the minimum number.
-        - If float, then `min_samples_leaf` is a percentage and
+        - If float, then `min_samples_leaf` is a fraction and
           `ceil(min_samples_leaf * n_samples)` are the minimum
           number of samples for each node.
 
         .. versionchanged:: 0.18
-           Added float values for percentages.
+           Added float values for fractions.
 
     min_weight_fraction_leaf : float, optional (default=0.)
         The minimum weighted fraction of the sum total of weights (of all
@@ -932,7 +932,7 @@ class DecisionTreeRegressor(BaseDecisionTree, RegressorMixin):
         The number of features to consider when looking for the best split:
 
         - If int, then consider `max_features` features at each split.
-        - If float, then `max_features` is a percentage and
+        - If float, then `max_features` is a fraction and
           `int(max_features * n_features)` features are considered at each
           split.
         - If "auto", then `max_features=n_features`.
@@ -1160,23 +1160,23 @@ class ExtraTreeClassifier(DecisionTreeClassifier):
         The minimum number of samples required to split an internal node:
 
         - If int, then consider `min_samples_split` as the minimum number.
-        - If float, then `min_samples_split` is a percentage and
+        - If float, then `min_samples_split` is a fraction and
           `ceil(min_samples_split * n_samples)` are the minimum
           number of samples for each split.
 
         .. versionchanged:: 0.18
-           Added float values for percentages.
+           Added float values for fractions.
 
     min_samples_leaf : int, float, optional (default=1)
         The minimum number of samples required to be at a leaf node:
 
         - If int, then consider `min_samples_leaf` as the minimum number.
-        - If float, then `min_samples_leaf` is a percentage and
+        - If float, then `min_samples_leaf` is a fraction and
           `ceil(min_samples_leaf * n_samples)` are the minimum
           number of samples for each node.
 
         .. versionchanged:: 0.18
-           Added float values for percentages.
+           Added float values for fractions.
 
     min_weight_fraction_leaf : float, optional (default=0.)
         The minimum weighted fraction of the sum total of weights (of all
@@ -1187,7 +1187,7 @@ class ExtraTreeClassifier(DecisionTreeClassifier):
         The number of features to consider when looking for the best split:
 
             - If int, then consider `max_features` features at each split.
-            - If float, then `max_features` is a percentage and
+            - If float, then `max_features` is a fraction and
               `int(max_features * n_features)` features are considered at each
               split.
             - If "auto", then `max_features=sqrt(n_features)`.
@@ -1343,23 +1343,23 @@ class ExtraTreeRegressor(DecisionTreeRegressor):
         The minimum number of samples required to split an internal node:
 
         - If int, then consider `min_samples_split` as the minimum number.
-        - If float, then `min_samples_split` is a percentage and
+        - If float, then `min_samples_split` is a fraction and
           `ceil(min_samples_split * n_samples)` are the minimum
           number of samples for each split.
 
         .. versionchanged:: 0.18
-           Added float values for percentages.
+           Added float values for fractions.
 
     min_samples_leaf : int, float, optional (default=1)
         The minimum number of samples required to be at a leaf node:
 
         - If int, then consider `min_samples_leaf` as the minimum number.
-        - If float, then `min_samples_leaf` is a percentage and
+        - If float, then `min_samples_leaf` is a fraction and
           `ceil(min_samples_leaf * n_samples)` are the minimum
           number of samples for each node.
 
         .. versionchanged:: 0.18
-           Added float values for percentages.
+           Added float values for fractions.
 
     min_weight_fraction_leaf : float, optional (default=0.)
         The minimum weighted fraction of the sum total of weights (of all
@@ -1370,7 +1370,7 @@ class ExtraTreeRegressor(DecisionTreeRegressor):
         The number of features to consider when looking for the best split:
 
         - If int, then consider `max_features` features at each split.
-        - If float, then `max_features` is a percentage and
+        - If float, then `max_features` is a fraction and
           `int(max_features * n_features)` features are considered at each
           split.
         - If "auto", then `max_features=n_features`.
diff --git a/sklearn/utils/__init__.py b/sklearn/utils/__init__.py
index be0048b60b0e..f0a00d8b757e 100644
--- a/sklearn/utils/__init__.py
+++ b/sklearn/utils/__init__.py
@@ -493,11 +493,19 @@ def indices_to_mask(indices, mask_length):
         List of integers treated as indices.
     mask_length : int
         Length of boolean mask to be generated.
+        This parameter must be greater than max(indices)
 
     Returns
     -------
     mask : 1d boolean nd-array
         Boolean array that is True where indices are present, else False.
+
+    Examples
+    --------
+    >>> from sklearn.utils import indices_to_mask
+    >>> indices = [1, 2 , 3, 4]
+    >>> indices_to_mask(indices, 5)
+    array([False,  True,  True,  True,  True], dtype=bool)
     """
     if mask_length <= np.max(indices):
         raise ValueError("mask_length must be greater than max(indices)")
diff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py
index 6e9b6f4b9250..483184a17632 100644
--- a/sklearn/utils/estimator_checks.py
+++ b/sklearn/utils/estimator_checks.py
@@ -18,6 +18,7 @@
 from sklearn.utils.testing import assert_raise_message
 from sklearn.utils.testing import assert_equal
 from sklearn.utils.testing import assert_not_equal
+from sklearn.utils.testing import assert_almost_equal
 from sklearn.utils.testing import assert_true
 from sklearn.utils.testing import assert_false
 from sklearn.utils.testing import assert_in
@@ -36,7 +37,8 @@
 
 
 from sklearn.base import (clone, TransformerMixin, ClusterMixin,
-                          BaseEstimator, is_classifier, is_regressor)
+                          BaseEstimator, is_classifier, is_regressor,
+                          is_outlier_detector)
 
 from sklearn.metrics import accuracy_score, adjusted_rand_score, f1_score
 
@@ -211,6 +213,20 @@ def _yield_clustering_checks(name, clusterer):
     yield check_non_transformer_estimators_n_iter
 
 
+def _yield_outliers_checks(name, estimator):
+
+    # checks for all outlier detectors
+    yield check_outliers_fit_predict
+
+    # checks for estimators that can be used on a test set
+    if hasattr(estimator, 'predict'):
+        yield check_outliers_train
+        # test outlier detectors can handle non-array data
+        yield check_classifier_data_not_an_array
+        # test if NotFittedError is raised
+        yield check_estimators_unfitted
+
+
 def _yield_all_checks(name, estimator):
     for check in _yield_non_meta_checks(name, estimator):
         yield check
@@ -226,6 +242,9 @@ def _yield_all_checks(name, estimator):
     if isinstance(estimator, ClusterMixin):
         for check in _yield_clustering_checks(name, estimator):
             yield check
+    if is_outlier_detector(estimator):
+        for check in _yield_outliers_checks(name, estimator):
+            yield check
     yield check_fit2d_predict1d
     yield check_methods_subset_invariance
     if name != 'GaussianProcess':  # FIXME
@@ -1269,7 +1288,7 @@ def check_classifiers_train(name, classifier_orig):
         X = pairwise_estimator_convert_X(X, classifier_orig)
         set_random_state(classifier)
         # raises error on malformed input for fit
-        with assert_raises(ValueError, msg="The classifer {} does not"
+        with assert_raises(ValueError, msg="The classifier {} does not"
                            " raise an error when incorrect/malformed input "
                            "data for fit is passed. The number of training "
                            "examples is not the same as the number of labels."
@@ -1361,6 +1380,67 @@ def check_classifiers_train(name, classifier_orig):
                 assert_array_equal(np.argsort(y_log_prob), np.argsort(y_prob))
 
 
+def check_outliers_train(name, estimator_orig):
+    X, _ = make_blobs(n_samples=300, random_state=0)
+    X = shuffle(X, random_state=7)
+    n_samples, n_features = X.shape
+    estimator = clone(estimator_orig)
+    set_random_state(estimator)
+
+    # fit
+    estimator.fit(X)
+    # with lists
+    estimator.fit(X.tolist())
+
+    y_pred = estimator.predict(X)
+    assert y_pred.shape == (n_samples,)
+    assert y_pred.dtype.kind == 'i'
+    assert_array_equal(np.unique(y_pred), np.array([-1, 1]))
+
+    decision = estimator.decision_function(X)
+    assert decision.dtype == np.dtype('float')
+
+    score = estimator.score_samples(X)
+    assert score.dtype == np.dtype('float')
+
+    # raises error on malformed input for predict
+    assert_raises(ValueError, estimator.predict, X.T)
+
+    # decision_function agrees with predict
+    decision = estimator.decision_function(X)
+    assert decision.shape == (n_samples,)
+    dec_pred = (decision >= 0).astype(np.int)
+    dec_pred[dec_pred == 0] = -1
+    assert_array_equal(dec_pred, y_pred)
+
+    # raises error on malformed input for decision_function
+    assert_raises(ValueError, estimator.decision_function, X.T)
+
+    # decision_function is a translation of score_samples
+    y_scores = estimator.score_samples(X)
+    assert y_scores.shape == (n_samples,)
+    y_dec = y_scores - estimator.offset_
+    assert_array_equal(y_dec, decision)
+
+    # raises error on malformed input for score_samples
+    assert_raises(ValueError, estimator.score_samples, X.T)
+
+    # contamination parameter (not for OneClassSVM which has the nu parameter)
+    if hasattr(estimator, "contamination"):
+        # proportion of outliers equal to contamination parameter when not
+        # set to 'auto'
+        contamination = 0.1
+        estimator.set_params(contamination=contamination)
+        estimator.fit(X)
+        y_pred = estimator.predict(X)
+        assert_almost_equal(np.mean(y_pred != 1), contamination)
+
+        # raises error when contamination is a scalar and not in [0,1]
+        for contamination in [-0.5, 2.3]:
+            estimator.set_params(contamination=contamination)
+            assert_raises(ValueError, estimator.fit, X)
+
+
 @ignore_warnings(category=(DeprecationWarning, FutureWarning))
 def check_estimators_fit_returns_self(name, estimator_orig):
     """Check if self is returned when calling fit"""
@@ -1389,7 +1469,7 @@ def check_estimators_unfitted(name, estimator_orig):
     therefore be adequately raised for that purpose.
     """
 
-    # Common test for Regressors as well as Classifiers
+    # Common test for Regressors, Classifiers and Outlier detection estimators
     X, y = _boston_subset()
 
     est = clone(estimator_orig)
@@ -1444,41 +1524,86 @@ def check_supervised_y_2d(name, estimator_orig):
     assert_allclose(y_pred.ravel(), y_pred_2d.ravel())
 
 
-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
+@ignore_warnings
+def check_classifiers_predictions(X, y, name, classifier_orig):
+    classes = np.unique(y)
+    classifier = clone(classifier_orig)
+    if name == 'BernoulliNB':
+        X = X > X.mean()
+    set_random_state(classifier)
+
+    classifier.fit(X, y)
+    y_pred = classifier.predict(X)
+
+    if hasattr(classifier, "decision_function"):
+        decision = classifier.decision_function(X)
+        n_samples, n_features = X.shape
+        assert isinstance(decision, np.ndarray)
+        if len(classes) == 2:
+            dec_pred = (decision.ravel() > 0).astype(np.int)
+            dec_exp = classifier.classes_[dec_pred]
+            assert_array_equal(dec_exp, y_pred,
+                               err_msg="decision_function does not match "
+                               "classifier for %r: expected '%s', got '%s'" %
+                               (classifier, ", ".join(map(str, dec_exp)),
+                                ", ".join(map(str, y_pred))))
+        elif getattr(classifier, 'decision_function_shape', 'ovr') == 'ovr':
+            decision_y = np.argmax(decision, axis=1).astype(int)
+            y_exp = classifier.classes_[decision_y]
+            assert_array_equal(y_exp, y_pred,
+                               err_msg="decision_function does not match "
+                               "classifier for %r: expected '%s', got '%s'" %
+                               (classifier, ", ".join(map(str, y_exp)),
+                                ", ".join(map(str, y_pred))))
+
+    # training set performance
+    if name != "ComplementNB":
+        # This is a pathological data set for ComplementNB.
+        # For some specific cases 'ComplementNB' predicts less classes
+        # than expected
+        assert_array_equal(np.unique(y), np.unique(y_pred))
+    assert_array_equal(classes, classifier.classes_,
+                       err_msg="Unexpected classes_ attribute for %r: "
+                       "expected '%s', got '%s'" %
+                       (classifier, ", ".join(map(str, classes)),
+                        ", ".join(map(str, classifier.classes_))))
+
+
+def choose_check_classifiers_labels(name, y, y_names):
+    return y if name in ["LabelPropagation", "LabelSpreading"] else y_names
+
 def check_classifiers_classes(name, classifier_orig):
-    X, y = make_blobs(n_samples=30, random_state=0, cluster_std=0.1)
-    X, y = shuffle(X, y, random_state=7)
-    X = StandardScaler().fit_transform(X)
+    X_multiclass, y_multiclass = make_blobs(n_samples=30, random_state=0,
+                                            cluster_std=0.1)
+    X_multiclass, y_multiclass = shuffle(X_multiclass, y_multiclass,
+                                         random_state=7)
+    X_multiclass = StandardScaler().fit_transform(X_multiclass)
     # We need to make sure that we have non negative data, for things
     # like NMF
-    X -= X.min() - .1
-    X = pairwise_estimator_convert_X(X, classifier_orig)
-    y_names = np.array(["one", "two", "three"])[y]
+    X_multiclass -= X_multiclass.min() - .1
 
-    for y_names in [y_names, y_names.astype('O')]:
-        if name in ["LabelPropagation", "LabelSpreading"]:
-            # TODO some complication with -1 label
-            y_ = y
-        else:
-            y_ = y_names
+    X_binary = X_multiclass[y_multiclass != 2]
+    y_binary = y_multiclass[y_multiclass != 2]
 
-        classes = np.unique(y_)
-        classifier = clone(classifier_orig)
-        if name == 'BernoulliNB':
-            X = X > X.mean()
-        set_random_state(classifier)
-        # fit
-        classifier.fit(X, y_)
+    X_multiclass = pairwise_estimator_convert_X(X_multiclass, classifier_orig)
+    X_binary = pairwise_estimator_convert_X(X_binary, classifier_orig)
 
-        y_pred = classifier.predict(X)
-        # training set performance
-        if name != "ComplementNB":
-            # This is a pathological data set for ComplementNB.
-            assert_array_equal(np.unique(y_), np.unique(y_pred))
-        if np.any(classifier.classes_ != classes):
-            print("Unexpected classes_ attribute for %r: "
-                  "expected %s, got %s" %
-                  (classifier, classes, classifier.classes_))
+    labels_multiclass = ["one", "two", "three"]
+    labels_binary = ["one", "two"]
+
+    y_names_multiclass = np.take(labels_multiclass, y_multiclass)
+    y_names_binary = np.take(labels_binary, y_binary)
+
+    for X, y, y_names in [(X_multiclass, y_multiclass, y_names_multiclass),
+                          (X_binary, y_binary, y_names_binary)]:
+        for y_names_i in [y_names, y_names.astype('O')]:
+            y_ = choose_check_classifiers_labels(name, y, y_names_i)
+            check_classifiers_predictions(X, y_, name, classifier_orig)
+
+    labels_binary = [-1, 1]
+    y_names_binary = np.take(labels_binary, y_binary)
+    y_binary = choose_check_classifiers_labels(name, y_binary, y_names_binary)
+    check_classifiers_predictions(X_binary, y_binary, name, classifier_orig)
 
 
 @ignore_warnings(category=(DeprecationWarning, FutureWarning))
@@ -1525,7 +1650,7 @@ def check_regressors_train(name, regressor_orig):
         regressor.C = 0.01
 
     # raises error on malformed input for fit
-    with assert_raises(ValueError, msg="The classifer {} does not"
+    with assert_raises(ValueError, msg="The classifier {} does not"
                        " raise an error when incorrect/malformed input "
                        "data for fit is passed. The number of training "
                        "examples is not the same as the number of "
@@ -1912,10 +2037,7 @@ def check_non_transformer_estimators_n_iter(name, estimator_orig):
         else:
             estimator.fit(X, y_)
 
-        # HuberRegressor depends on scipy.optimize.fmin_l_bfgs_b
-        # which doesn't return a n_iter for old versions of SciPy.
-        if not (name == 'HuberRegressor' and estimator.n_iter_ is None):
-            assert_greater_equal(estimator.n_iter_, 1)
+        assert estimator.n_iter_ >= 1
 
 
 @ignore_warnings(category=(DeprecationWarning, FutureWarning))
@@ -1998,3 +2120,37 @@ def check_decision_proba_consistency(name, estimator_orig):
         a = estimator.predict_proba(X_test)[:, 1]
         b = estimator.decision_function(X_test)
         assert_array_equal(rankdata(a), rankdata(b))
+
+
+def check_outliers_fit_predict(name, estimator_orig):
+    # Check fit_predict for outlier detectors.
+
+    X, _ = make_blobs(n_samples=300, random_state=0)
+    X = shuffle(X, random_state=7)
+    n_samples, n_features = X.shape
+    estimator = clone(estimator_orig)
+
+    set_random_state(estimator)
+
+    y_pred = estimator.fit_predict(X)
+    assert y_pred.shape == (n_samples,)
+    assert y_pred.dtype.kind == 'i'
+    assert_array_equal(np.unique(y_pred), np.array([-1, 1]))
+
+    # check fit_predict = fit.predict when possible
+    if hasattr(estimator, 'predict'):
+        y_pred_2 = estimator.fit(X).predict(X)
+        assert_array_equal(y_pred, y_pred_2)
+
+    if hasattr(estimator, "contamination"):
+        # proportion of outliers equal to contamination parameter when not
+        # set to 'auto'
+        contamination = 0.1
+        estimator.set_params(contamination=contamination)
+        y_pred = estimator.fit_predict(X)
+        assert_almost_equal(np.mean(y_pred != 1), contamination)
+
+        # raises error when contamination is a scalar and not in [0,1]
+        for contamination in [-0.5, 2.3]:
+            estimator.set_params(contamination=contamination)
+            assert_raises(ValueError, estimator.fit_predict, X)
diff --git a/sklearn/utils/fixes.py b/sklearn/utils/fixes.py
index 3c81a2f86d35..551ca6cbd311 100644
--- a/sklearn/utils/fixes.py
+++ b/sklearn/utils/fixes.py
@@ -78,7 +78,7 @@ def divide(x1, x2, out=None, dtype=None):
         warnings.simplefilter('always')
         sp.csr_matrix([1.0, 2.0, 3.0]).max(axis=0)
 except (TypeError, AttributeError):
-    # in scipy < 14.0, sparse matrix min/max doesn't accept an `axis` argument
+    # in scipy < 0.14.0, sparse matrix min/max doesn't accept `axis` argument
     # the following code is taken from the scipy 0.14 codebase
 
     def _minor_reduce(X, ufunc):
diff --git a/sklearn/utils/testing.py b/sklearn/utils/testing.py
index ec12d2d06b74..94972354e275 100644
--- a/sklearn/utils/testing.py
+++ b/sklearn/utils/testing.py
@@ -484,20 +484,22 @@ def fake_mldata(columns_dict, dataname, matfile, ordering=None):
 
 
 class mock_mldata_urlopen(object):
+    """Object that mocks the urlopen function to fake requests to mldata.
 
+    When requesting a dataset with a name that is in mock_datasets, this object
+    creates a fake dataset in a StringIO object and returns it. Otherwise, it
+    raises an HTTPError.
+
+    Parameters
+    ----------
+    mock_datasets : dict
+        A dictionary of {dataset_name: data_dict}, or
+        {dataset_name: (data_dict, ordering). `data_dict` itself is a
+        dictionary of {column_name: data_array}, and `ordering` is a list of
+        column_names to determine the ordering in the data set (see
+        :func:`fake_mldata` for details).
+    """
     def __init__(self, mock_datasets):
-        """Object that mocks the urlopen function to fake requests to mldata.
-
-        `mock_datasets` is a dictionary of {dataset_name: data_dict}, or
-        {dataset_name: (data_dict, ordering).
-        `data_dict` itself is a dictionary of {column_name: data_array},
-        and `ordering` is a list of column_names to determine the ordering
-        in the data set (see `fake_mldata` for details).
-
-        When requesting a dataset with a name that is in mock_datasets,
-        this object creates a fake dataset in a StringIO object and
-        returns it. Otherwise, it raises an HTTPError.
-        """
         self.mock_datasets = mock_datasets
 
     def __call__(self, urlname):
@@ -542,8 +544,8 @@ def uninstall_mldata_mock():
 OTHER = ["Pipeline", "FeatureUnion", "GridSearchCV", "RandomizedSearchCV",
          "SelectFromModel"]
 
-# some trange ones
-DONT_TEST = ['SparseCoder', 'EllipticEnvelope', 'DictVectorizer',
+# some strange ones
+DONT_TEST = ['SparseCoder', 'DictVectorizer',
              'LabelBinarizer', 'LabelEncoder',
              'MultiLabelBinarizer', 'TfidfTransformer',
              'TfidfVectorizer', 'IsotonicRegression',
diff --git a/sklearn/utils/tests/test_validation.py b/sklearn/utils/tests/test_validation.py
index 2f134d33c3e4..95f5841ac80b 100644
--- a/sklearn/utils/tests/test_validation.py
+++ b/sklearn/utils/tests/test_validation.py
@@ -285,6 +285,42 @@ def test_check_array():
     result = check_array(X_no_array)
     assert_true(isinstance(result, np.ndarray))
 
+    # deprecation warning if string-like array with dtype="numeric"
+    X_str = [['a', 'b'], ['c', 'd']]
+    assert_warns_message(
+        FutureWarning,
+        "arrays of strings will be interpreted as decimal numbers if "
+        "parameter 'dtype' is 'numeric'. It is recommended that you convert "
+        "the array to type np.float64 before passing it to check_array.",
+        check_array, X_str, "numeric")
+    assert_warns_message(
+        FutureWarning,
+        "arrays of strings will be interpreted as decimal numbers if "
+        "parameter 'dtype' is 'numeric'. It is recommended that you convert "
+        "the array to type np.float64 before passing it to check_array.",
+        check_array, np.array(X_str, dtype='U'), "numeric")
+    assert_warns_message(
+        FutureWarning,
+        "arrays of strings will be interpreted as decimal numbers if "
+        "parameter 'dtype' is 'numeric'. It is recommended that you convert "
+        "the array to type np.float64 before passing it to check_array.",
+        check_array, np.array(X_str, dtype='S'), "numeric")
+
+    # deprecation warning if byte-like array with dtype="numeric"
+    X_bytes = [[b'a', b'b'], [b'c', b'd']]
+    assert_warns_message(
+        FutureWarning,
+        "arrays of strings will be interpreted as decimal numbers if "
+        "parameter 'dtype' is 'numeric'. It is recommended that you convert "
+        "the array to type np.float64 before passing it to check_array.",
+        check_array, X_bytes, "numeric")
+    assert_warns_message(
+        FutureWarning,
+        "arrays of strings will be interpreted as decimal numbers if "
+        "parameter 'dtype' is 'numeric'. It is recommended that you convert "
+        "the array to type np.float64 before passing it to check_array.",
+        check_array, np.array(X_bytes, dtype='V1'), "numeric")
+
 
 def test_check_array_pandas_dtype_object_conversion():
     # test that data-frame like objects with dtype object
diff --git a/sklearn/utils/validation.py b/sklearn/utils/validation.py
index d47c61202332..70e968ee6d36 100644
--- a/sklearn/utils/validation.py
+++ b/sklearn/utils/validation.py
@@ -516,6 +516,15 @@ def check_array(array, accept_sparse=False, dtype="numeric", order=None,
             # To ensure that array flags are maintained
             array = np.array(array, dtype=dtype, order=order, copy=copy)
 
+        # in the future np.flexible dtypes will be handled like object dtypes
+        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):
+            warnings.warn(
+                "Beginning in version 0.22, arrays of strings will be "
+                "interpreted as decimal numbers if parameter 'dtype' is "
+                "'numeric'. It is recommended that you convert the array to "
+                "type np.float64 before passing it to check_array.",
+                FutureWarning)
+
         # make sure we actually converted to numeric:
         if dtype_numeric and array.dtype.kind == "O":
             array = array.astype(np.float64)
