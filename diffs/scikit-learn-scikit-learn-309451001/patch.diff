diff --git a/.travis.yml b/.travis.yml
index 4737365c7bd0..aea19fc6b36b 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -22,6 +22,7 @@ matrix:
   include:
     # This environment tests that scikit-learn can be built against
     # versions of numpy, scipy with ATLAS that comes with Ubuntu Trusty 14.04
+    # i.e. numpy 1.8.2 and scipy 0.13.3
     - env: DISTRIB="ubuntu" PYTHON_VERSION="2.7" CYTHON_VERSION="0.23.5"
            COVERAGE=true
       if: type != cron
@@ -30,11 +31,11 @@ matrix:
           packages:
             # these only required by the DISTRIB="ubuntu" builds:
             - python-scipy
-            - libatlas3gf-base
+            - libatlas3-base
             - libatlas-dev
-    # This environment tests the oldest supported anaconda env
-    - env: DISTRIB="conda" PYTHON_VERSION="2.7" INSTALL_MKL="false"
-           NUMPY_VERSION="1.8.2" SCIPY_VERSION="0.13.3" CYTHON_VERSION="0.23.5"
+    # Python 3.4 build
+    - env: DISTRIB="conda" PYTHON_VERSION="3.4" INSTALL_MKL="false"
+           NUMPY_VERSION="1.10.4" SCIPY_VERSION="0.16.1" CYTHON_VERSION="0.25.2"
            COVERAGE=true
       if: type != cron
     # This environment tests the newest supported Anaconda release (5.0.0)
diff --git a/README.rst b/README.rst
index a1ec4c06f535..7b9d1101518d 100644
--- a/README.rst
+++ b/README.rst
@@ -49,7 +49,7 @@ Dependencies
 
 scikit-learn requires:
 
-- Python (>= 2.7 or >= 3.3)
+- Python (>= 2.7 or >= 3.4)
 - NumPy (>= 1.8.2)
 - SciPy (>= 0.13.3)
 
@@ -114,7 +114,7 @@ source directory (you will need to have the ``pytest`` package installed)::
 
     pytest sklearn
 
-See the web page http://scikit-learn.org/stable/developers/advanced_installation.html#testing
+See the web page http://scikit-learn.org/dev/developers/advanced_installation.html#testing
 for more information.
 
     Random number generation can be controlled during testing by setting
diff --git a/doc/developers/advanced_installation.rst b/doc/developers/advanced_installation.rst
index 1566fa5302ba..ee5c83e34965 100644
--- a/doc/developers/advanced_installation.rst
+++ b/doc/developers/advanced_installation.rst
@@ -35,7 +35,7 @@ Installing an official release
 
 Scikit-learn requires:
 
-- Python (>= 2.7 or >= 3.3),
+- Python (>= 2.7 or >= 3.4),
 - NumPy (>= 1.8.2),
 - SciPy (>= 0.13.3).
 
@@ -67,15 +67,15 @@ Python 2 you can install all these requirements by issuing::
 
     sudo apt-get install build-essential python-dev python-setuptools \
                          python-numpy python-scipy \
-                         libatlas-dev libatlas3gf-base
+                         libatlas-dev libatlas3-base
 
 If you have Python 3::
 
     sudo apt-get install build-essential python3-dev python3-setuptools \
                          python3-numpy python3-scipy \
-                         libatlas-dev libatlas3gf-base
+                         libatlas-dev libatlas3-base
 
-On recent Debian and Ubuntu (e.g. Ubuntu 13.04 or later) make sure that ATLAS
+On recent Debian and Ubuntu (e.g. Ubuntu 14.04 or later) make sure that ATLAS
 is used to provide the implementation of the BLAS and LAPACK linear algebra
 routines::
 
diff --git a/doc/install.rst b/doc/install.rst
index e4240b41d857..6fecec888c24 100644
--- a/doc/install.rst
+++ b/doc/install.rst
@@ -15,7 +15,7 @@ Installing the latest release
 
 Scikit-learn requires:
 
-- Python (>= 2.7 or >= 3.3),
+- Python (>= 2.7 or >= 3.4),
 - NumPy (>= 1.8.2),
 - SciPy (>= 0.13.3).
 
diff --git a/doc/modules/preprocessing.rst b/doc/modules/preprocessing.rst
index 0c08063331c6..969a2921b406 100644
--- a/doc/modules/preprocessing.rst
+++ b/doc/modules/preprocessing.rst
@@ -261,11 +261,11 @@ defined by :math:`phi` followed by removal of the mean in that space.
 Non-linear transformation
 =========================
 
-Like scalers, :class:`QuantileTransformer` puts each feature into the same
-range or distribution. However, by performing a rank transformation, it smooths
-out unusual distributions and is less influenced by outliers than scaling
-methods. It does, however, distort correlations and distances within and across
-features.
+Like scalers, :class:`QuantileTransformer` puts all features into the same,
+known range or distribution. However, by performing a rank transformation, it
+smooths out unusual distributions and is less influenced by outliers than
+scaling methods. It does, however, distort correlations and distances within
+and across features.
 
 :class:`QuantileTransformer` and :func:`quantile_transform` provide a
 non-parametric transformation based on the quantile function to map the data to
diff --git a/doc/whats_new/_contributors.rst b/doc/whats_new/_contributors.rst
index fd12247043e0..3dbcac534058 100644
--- a/doc/whats_new/_contributors.rst
+++ b/doc/whats_new/_contributors.rst
@@ -151,3 +151,5 @@
 .. _Arthur Mensch: https://amensch.fr
 
 .. _Joris Van den Bossche: https://github.com/jorisvandenbossche
+
+.. _Roman Yurchak: https://github.com/rth
diff --git a/doc/whats_new/v0.20.rst b/doc/whats_new/v0.20.rst
index d37a86d4fa2f..3f943772f54c 100644
--- a/doc/whats_new/v0.20.rst
+++ b/doc/whats_new/v0.20.rst
@@ -33,6 +33,8 @@ cannot assure that this list is complete.)
 Changelog
 ---------
 
+Support for Python 3.3 has been officially dropped.
+
 New features
 ............
 
@@ -209,6 +211,22 @@ Neighbors
   warning when no neighbors are found for samples.  :issue:`9655` by
   :user:`Andreas Bjerre-Nielsen <abjer>`.
 
+Feature Extraction
+
+- Fixed a bug in :func:`feature_extraction.image.extract_patches_2d` which would
+  throw an exception if ``max_patches`` was greater than or equal to the number
+  of all possible patches rather than simply returning the number of possible
+  patches. :issue:`10100` by :user:`Varun Agrawal <varunagrawal>`
+  
+- Fixed a bug in :class:`feature_extraction.text.CountVectorizer`,
+  :class:`feature_extraction.text.TfidfVectorizer`,
+  :class:`feature_extraction.text.HashingVectorizer` to support 64 bit sparse
+  array indexing necessary to process large datasets with more than 2·10⁹ tokens
+  (words or n-grams). :issue:`9147` by :user:`Claes-Fredrik Mannby <mannby>`
+  and `Roman Yurchak`_.
+
+	
+
 API changes summary
 -------------------
 
diff --git a/sklearn/ensemble/_gradient_boosting.pyx b/sklearn/ensemble/_gradient_boosting.pyx
index 9bf1e9290137..cd749eae8e9b 100644
--- a/sklearn/ensemble/_gradient_boosting.pyx
+++ b/sklearn/ensemble/_gradient_boosting.pyx
@@ -208,7 +208,7 @@ def predict_stages(np.ndarray[object, ndim=2] estimators,
                              " expected, got {!r}".format(type(X)))
         _predict_regression_tree_stages_sparse(estimators, X, scale, out)
     else:
-        if not isinstance(X, np.ndarray) and np.isfortran(X):
+        if not isinstance(X, np.ndarray) or np.isfortran(X):
             raise ValueError("X should be C-ordered np.ndarray,"
                              " got {}".format(type(X)))
 
diff --git a/sklearn/ensemble/gradient_boosting.py b/sklearn/ensemble/gradient_boosting.py
index 6b3bec33c317..c82d47c9bb34 100644
--- a/sklearn/ensemble/gradient_boosting.py
+++ b/sklearn/ensemble/gradient_boosting.py
@@ -887,6 +887,11 @@ def _check_params(self):
                              "integer. %r was passed"
                              % self.n_iter_no_change)
 
+        allowed_presort = ('auto', True, False)
+        if self.presort not in allowed_presort:
+            raise ValueError("'presort' should be in {}. Got {!r} instead."
+                             .format(allowed_presort, self.presort))
+
     def _init_state(self):
         """Initialize model state and allocate model state data structures. """
 
@@ -1041,21 +1046,20 @@ def fit(self, X, y, sample_weight=None, monitor=None):
             y_pred = self._decision_function(X)
             self._resize_state()
 
-        X_idx_sorted = None
+        if self.presort is True and issparse(X):
+            raise ValueError(
+                "Presorting is not supported for sparse matrices.")
+
         presort = self.presort
         # Allow presort to be 'auto', which means True if the dataset is dense,
         # otherwise it will be False.
-        if presort == 'auto' and issparse(X):
-            presort = False
-        elif presort == 'auto':
-            presort = True
-
-        if presort == True:
-            if issparse(X):
-                raise ValueError("Presorting is not supported for sparse matrices.")
-            else:
-                X_idx_sorted = np.asfortranarray(np.argsort(X, axis=0),
-                                                 dtype=np.int32)
+        if presort == 'auto':
+            presort = not issparse(X)
+
+        X_idx_sorted = None
+        if presort:
+            X_idx_sorted = np.asfortranarray(np.argsort(X, axis=0),
+                                             dtype=np.int32)
 
         # fit the boosting stages
         n_stages = self._fit_stages(X, y, y_pred, sample_weight, self._rng,
diff --git a/sklearn/ensemble/tests/test_gradient_boosting.py b/sklearn/ensemble/tests/test_gradient_boosting.py
index 64682ec063a1..4c59b33e675e 100644
--- a/sklearn/ensemble/tests/test_gradient_boosting.py
+++ b/sklearn/ensemble/tests/test_gradient_boosting.py
@@ -83,7 +83,6 @@ def test_classification_toy():
 
 def test_classifier_parameter_checks():
     # Check input parameter validation for GradientBoostingClassifier.
-
     assert_raises(ValueError,
                   GradientBoostingClassifier(n_estimators=0).fit, X, y)
     assert_raises(ValueError,
@@ -141,6 +140,13 @@ def test_classifier_parameter_checks():
                       loss='deviance').fit(X, y),
                   X, [0, 0, 0, 0])
 
+    allowed_presort = ('auto', True, False)
+    assert_raise_message(ValueError,
+                         "'presort' should be in {}. "
+                         "Got 'invalid' instead.".format(allowed_presort),
+                         GradientBoostingClassifier(presort='invalid')
+                         .fit, X, y)
+
 
 def test_regressor_parameter_checks():
     # Check input parameter validation for GradientBoostingRegressor
@@ -159,6 +165,12 @@ def test_regressor_parameter_checks():
                          " or an integer. 'invalid' was passed",
                          GradientBoostingRegressor(n_iter_no_change='invalid')
                          .fit, X, y)
+    allowed_presort = ('auto', True, False)
+    assert_raise_message(ValueError,
+                         "'presort' should be in {}. "
+                         "Got 'invalid' instead.".format(allowed_presort),
+                         GradientBoostingRegressor(presort='invalid')
+                         .fit, X, y)
 
 
 def test_loss_function():
@@ -389,9 +401,14 @@ def test_check_inputs_predict_stages():
     clf.fit(x, y)
     score = np.zeros((y.shape)).reshape(-1, 1)
     assert_raise_message(ValueError,
-                         "X should be in np.ndarray or csr_matrix format",
+                         "When X is a sparse matrix, a CSR format is expected",
                          predict_stages, clf.estimators_, x_sparse_csc,
                          clf.learning_rate, score)
+    x_fortran = np.asfortranarray(x)
+    assert_raise_message(ValueError,
+                         "X should be C-ordered np.ndarray",
+                         predict_stages, clf.estimators_, x_fortran,
+                         clf.learning_rate, score)
 
 
 def test_check_max_features():
diff --git a/sklearn/feature_extraction/_hashing.pyx b/sklearn/feature_extraction/_hashing.pyx
index e39aeafa0868..c462dd8a2471 100644
--- a/sklearn/feature_extraction/_hashing.pyx
+++ b/sklearn/feature_extraction/_hashing.pyx
@@ -1,6 +1,7 @@
 # Author: Lars Buitinck
 # License: BSD 3 clause
 
+import sys
 import array
 from cpython cimport array
 cimport cython
@@ -9,6 +10,7 @@ cimport numpy as np
 import numpy as np
 
 from sklearn.utils.murmurhash cimport murmurhash3_bytes_s32
+from sklearn.utils.fixes import sp_version
 
 np.import_array()
 
@@ -33,12 +35,20 @@ def transform(raw_X, Py_ssize_t n_features, dtype, bint alternate_sign=1):
     cdef array.array indices
     cdef array.array indptr
     indices = array.array("i")
-    indptr = array.array("i", [0])
+    if sys.version_info >= (3, 3):
+        indices_array_dtype = "q"
+        indices_np_dtype = np.longlong
+    else:
+        # On Windows with PY2.7 long int would still correspond to 32 bit. 
+        indices_array_dtype = "l"
+        indices_np_dtype = np.int_
+
+    indptr = array.array(indices_array_dtype, [0])
 
     # Since Python array does not understand Numpy dtypes, we grow the indices
     # and values arrays ourselves. Use a Py_ssize_t capacity for safety.
     cdef Py_ssize_t capacity = 8192     # arbitrary
-    cdef np.int32_t size = 0
+    cdef np.int64_t size = 0
     cdef np.ndarray values = np.empty(capacity, dtype=dtype)
 
     for x in raw_X:
@@ -79,4 +89,18 @@ def transform(raw_X, Py_ssize_t n_features, dtype, bint alternate_sign=1):
         indptr[len(indptr) - 1] = size
 
     indices_a = np.frombuffer(indices, dtype=np.int32)
-    return (indices_a, np.frombuffer(indptr, dtype=np.int32), values[:size])
+    indptr_a = np.frombuffer(indptr, dtype=indices_np_dtype)
+
+    if indptr[-1] > 2147483648:  # = 2**31
+        if sp_version < (0, 14):
+            raise ValueError(('sparse CSR array has {} non-zero '
+                              'elements and requires 64 bit indexing, '
+                              ' which is unsupported with scipy {}. '
+                              'Please upgrade to scipy >=0.14')
+                             .format(indptr[-1], '.'.join(sp_version)))
+        # both indices and indptr have the same dtype in CSR arrays
+        indices_a = indices_a.astype(np.int64)
+    else:
+        indptr_a = indptr_a.astype(np.int32)
+
+    return (indices_a, indptr_a, values[:size])
diff --git a/sklearn/feature_extraction/image.py b/sklearn/feature_extraction/image.py
index 37e1a7e3465e..8c4f5b268945 100644
--- a/sklearn/feature_extraction/image.py
+++ b/sklearn/feature_extraction/image.py
@@ -229,6 +229,9 @@ def _compute_n_patches(i_h, i_w, p_h, p_w, max_patches=None):
         if (isinstance(max_patches, (numbers.Integral))
                 and max_patches < all_patches):
             return max_patches
+        elif (isinstance(max_patches, (numbers.Integral))
+              and max_patches >= all_patches):
+            return all_patches
         elif (isinstance(max_patches, (numbers.Real))
                 and 0 < max_patches < 1):
             return int(max_patches * all_patches)
diff --git a/sklearn/feature_extraction/tests/test_image.py b/sklearn/feature_extraction/tests/test_image.py
index 5e1b53040f43..dc9367980600 100644
--- a/sklearn/feature_extraction/tests/test_image.py
+++ b/sklearn/feature_extraction/tests/test_image.py
@@ -2,6 +2,7 @@
 #          Gael Varoquaux <gael.varoquaux@normalesup.org>
 # License: BSD 3 clause
 
+from __future__ import division
 import numpy as np
 import scipy as sp
 from scipy import ndimage
@@ -170,6 +171,25 @@ def test_extract_patches_max_patches():
                   max_patches=-1.0)
 
 
+def test_extract_patch_same_size_image():
+    face = downsampled_face
+    # Request patches of the same size as image
+    # Should return just the single patch a.k.a. the image
+    patches = extract_patches_2d(face, face.shape, max_patches=2)
+    assert_equal(patches.shape[0], 1)
+
+
+def test_extract_patches_less_than_max_patches():
+    face = downsampled_face
+    i_h, i_w = face.shape
+    p_h, p_w = 3 * i_h // 4, 3 * i_w // 4
+    # this is 3185
+    expected_n_patches = (i_h - p_h + 1) * (i_w - p_w + 1)
+
+    patches = extract_patches_2d(face, (p_h, p_w), max_patches=4000)
+    assert_equal(patches.shape, (expected_n_patches, p_h, p_w))
+
+
 def test_reconstruct_patches_perfect():
     face = downsampled_face
     p_h, p_w = 16, 16
diff --git a/sklearn/feature_extraction/text.py b/sklearn/feature_extraction/text.py
index 417aeef2f8bc..a1e0845abe9a 100644
--- a/sklearn/feature_extraction/text.py
+++ b/sklearn/feature_extraction/text.py
@@ -30,6 +30,7 @@
 from .hashing import FeatureHasher
 from .stop_words import ENGLISH_STOP_WORDS
 from ..utils.validation import check_is_fitted
+from ..utils.fixes import sp_version
 
 __all__ = ['CountVectorizer',
            'ENGLISH_STOP_WORDS',
@@ -784,7 +785,8 @@ def _count_vocab(self, raw_documents, fixed_vocab):
 
         analyze = self.build_analyzer()
         j_indices = []
-        indptr = _make_int_array()
+        indptr = []
+
         values = _make_int_array()
         indptr.append(0)
         for doc in raw_documents:
@@ -811,8 +813,20 @@ def _count_vocab(self, raw_documents, fixed_vocab):
                 raise ValueError("empty vocabulary; perhaps the documents only"
                                  " contain stop words")
 
-        j_indices = np.asarray(j_indices, dtype=np.intc)
-        indptr = np.frombuffer(indptr, dtype=np.intc)
+        if indptr[-1] > 2147483648:  # = 2**31 - 1
+            if sp_version >= (0, 14):
+                indices_dtype = np.int64
+            else:
+                raise ValueError(('sparse CSR array has {} non-zero '
+                                  'elements and requires 64 bit indexing, '
+                                  ' which is unsupported with scipy {}. '
+                                  'Please upgrade to scipy >=0.14')
+                                 .format(indptr[-1], '.'.join(sp_version)))
+
+        else:
+            indices_dtype = np.int32
+        j_indices = np.asarray(j_indices, dtype=indices_dtype)
+        indptr = np.asarray(indptr, dtype=indices_dtype)
         values = np.frombuffer(values, dtype=np.intc)
 
         X = sp.csr_matrix((values, j_indices, indptr),
diff --git a/sklearn/linear_model/passive_aggressive.py b/sklearn/linear_model/passive_aggressive.py
index 9c8d111371f7..37eb5b2ca58c 100644
--- a/sklearn/linear_model/passive_aggressive.py
+++ b/sklearn/linear_model/passive_aggressive.py
@@ -63,6 +63,10 @@ class PassiveAggressiveClassifier(BaseSGDClassifier):
         When set to True, reuse the solution of the previous call to fit as
         initialization, otherwise, just erase the previous solution.
 
+        Repeatedly calling fit or partial_fit when warm_start is True can
+        result in a different solution than when calling fit a single time
+        because of the way the data is shuffled.
+
     class_weight : dict, {class_label: weight} or "balanced" or None, optional
         Preset for the class_weight fit parameter.
 
@@ -282,6 +286,10 @@ class PassiveAggressiveRegressor(BaseSGDRegressor):
         When set to True, reuse the solution of the previous call to fit as
         initialization, otherwise, just erase the previous solution.
 
+        Repeatedly calling fit or partial_fit when warm_start is True can
+        result in a different solution than when calling fit a single time
+        because of the way the data is shuffled.
+
     average : bool or int, optional
         When set to True, computes the averaged SGD weights and stores the
         result in the ``coef_`` attribute. If set to an int greater than 1,
diff --git a/sklearn/linear_model/stochastic_gradient.py b/sklearn/linear_model/stochastic_gradient.py
index 1cd37b953060..3fdf17763a5c 100644
--- a/sklearn/linear_model/stochastic_gradient.py
+++ b/sklearn/linear_model/stochastic_gradient.py
@@ -724,6 +724,14 @@ class SGDClassifier(BaseSGDClassifier):
         When set to True, reuse the solution of the previous call to fit as
         initialization, otherwise, just erase the previous solution.
 
+        Repeatedly calling fit or partial_fit when warm_start is True can
+        result in a different solution than when calling fit a single time
+        because of the way the data is shuffled.
+        If a dynamic learning rate is used, the learning rate is adapted
+        depending on the number of samples already seen. Calling ``fit`` resets
+        this counter, while ``partial_fit`` will result in increasing the
+        existing counter.
+
     average : bool or int, optional
         When set to True, computes the averaged SGD weights and stores the
         result in the ``coef_`` attribute. If set to an int greater than 1,
@@ -1273,6 +1281,14 @@ class SGDRegressor(BaseSGDRegressor):
         When set to True, reuse the solution of the previous call to fit as
         initialization, otherwise, just erase the previous solution.
 
+        Repeatedly calling fit or partial_fit when warm_start is True can
+        result in a different solution than when calling fit a single time
+        because of the way the data is shuffled.
+        If a dynamic learning rate is used, the learning rate is adapted
+        depending on the number of samples already seen. Calling ``fit`` resets
+        this counter, while ``partial_fit``  will result in increasing the
+        existing counter.
+
     average : bool or int, optional
         When set to True, computes the averaged SGD weights and stores the
         result in the ``coef_`` attribute. If set to an int greater than 1,
diff --git a/sklearn/tree/tests/test_tree.py b/sklearn/tree/tests/test_tree.py
index c209c93ae3c9..0e49dfae3ba0 100644
--- a/sklearn/tree/tests/test_tree.py
+++ b/sklearn/tree/tests/test_tree.py
@@ -31,6 +31,7 @@
 from sklearn.utils.testing import assert_warns
 from sklearn.utils.testing import assert_warns_message
 from sklearn.utils.testing import ignore_warnings
+from sklearn.utils.testing import assert_raise_message
 
 from sklearn.utils.validation import check_random_state
 
@@ -502,7 +503,6 @@ def test_error():
         assert_raises(ValueError, est.predict_proba, X2)
 
     for name, TreeEstimator in ALL_TREES.items():
-        # Invalid values for parameters
         assert_raises(ValueError, TreeEstimator(min_samples_leaf=-1).fit, X, y)
         assert_raises(ValueError, TreeEstimator(min_samples_leaf=.6).fit, X, y)
         assert_raises(ValueError, TreeEstimator(min_samples_leaf=0.).fit, X, y)
@@ -523,8 +523,10 @@ def test_error():
                       X, y)
         assert_raises(ValueError, TreeEstimator(max_depth=-1).fit, X, y)
         assert_raises(ValueError, TreeEstimator(max_features=42).fit, X, y)
-        assert_raises(ValueError, TreeEstimator(min_impurity_split=-1.0).fit,
-                      X, y)
+        # min_impurity_split warning
+        with ignore_warnings(category=DeprecationWarning):
+            assert_raises(ValueError,
+                          TreeEstimator(min_impurity_split=-1.0).fit, X, y)
         assert_raises(ValueError,
                       TreeEstimator(min_impurity_decrease=-1.0).fit, X, y)
 
@@ -1237,7 +1239,8 @@ def test_arrays_persist():
     # non-regression for #2726
     for attr in ['n_classes', 'value', 'children_left', 'children_right',
                  'threshold', 'impurity', 'feature', 'n_node_samples']:
-        value = getattr(DecisionTreeClassifier().fit([[0], [1]], [0, 1]).tree_, attr)
+        value = getattr(DecisionTreeClassifier().fit([[0], [1]],
+                                                     [0, 1]).tree_, attr)
         # if pointing to freed memory, contents may be arbitrary
         assert_true(-3 <= value.flat[0] < 3,
                     'Array points to arbitrary memory')
@@ -1621,6 +1624,18 @@ def test_presort_sparse():
         yield check_presort_sparse, est, sparse_matrix(X), y
 
 
+def test_invalid_presort():
+    classes = (DecisionTreeRegressor, DecisionTreeClassifier)
+    allowed_presort = ('auto', True, False)
+    invalid_presort = 'invalid'
+    msg = ("'presort' should be in {}. "
+           "Got {!r} instead.".format(allowed_presort, invalid_presort))
+    for cls in classes:
+        est = cls(presort=invalid_presort)
+        assert_raise_message(ValueError, msg,
+                             est.fit, X, y)
+
+
 def test_decision_path_hardcoded():
     X = iris.data
     y = iris.target
diff --git a/sklearn/tree/tree.py b/sklearn/tree/tree.py
index 789ffb8b61ca..50b81dfdfda0 100644
--- a/sklearn/tree/tree.py
+++ b/sklearn/tree/tree.py
@@ -292,18 +292,21 @@ def fit(self, X, y, sample_weight=None, check_input=True,
             raise ValueError("min_impurity_decrease must be greater than "
                              "or equal to 0")
 
-        presort = self.presort
-        # Allow presort to be 'auto', which means True if the dataset is dense,
-        # otherwise it will be False.
-        if self.presort == 'auto' and issparse(X):
-            presort = False
-        elif self.presort == 'auto':
-            presort = True
+        allowed_presort = ('auto', True, False)
+        if self.presort not in allowed_presort:
+            raise ValueError("'presort' should be in {}. Got {!r} instead."
+                             .format(allowed_presort, self.presort))
 
-        if presort is True and issparse(X):
+        if self.presort is True and issparse(X):
             raise ValueError("Presorting is not supported for sparse "
                              "matrices.")
 
+        presort = self.presort
+        # Allow presort to be 'auto', which means True if the dataset is dense,
+        # otherwise it will be False.
+        if self.presort == 'auto':
+            presort = not issparse(X)
+
         # If multiple trees are built on the same dataset, we only want to
         # presort once. Splitters now can accept presorted indices if desired,
         # but do not handle any presorting themselves. Ensemble algorithms
