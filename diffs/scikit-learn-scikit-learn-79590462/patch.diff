diff --git a/sklearn/datasets/svmlight_format.py b/sklearn/datasets/svmlight_format.py
index f453e75652ec..892f139ff10e 100644
--- a/sklearn/datasets/svmlight_format.py
+++ b/sklearn/datasets/svmlight_format.py
@@ -341,8 +341,9 @@ def dump_svmlight_file(X, y, f,  zero_based=True, comment=None, query_id=None,
         Training vectors, where n_samples is the number of samples and
         n_features is the number of features.
 
-    y : array-like, shape = [n_samples]
-        Target values.
+    y : array-like, shape = [n_samples] or [n_samples, n_labels]
+        Target values. Class labels must be an integer or float, or array-like 
+        objects of integer or float for multilabel classifications.
 
     f : string or file-like in binary mode
         If string, specifies the path that will contain the data.
diff --git a/sklearn/ensemble/tests/test_forest.py b/sklearn/ensemble/tests/test_forest.py
index 2a1463292959..665c92612477 100644
--- a/sklearn/ensemble/tests/test_forest.py
+++ b/sklearn/ensemble/tests/test_forest.py
@@ -15,6 +15,7 @@
 import numpy as np
 from scipy.sparse import csr_matrix, csc_matrix, coo_matrix
 
+from sklearn.utils import warnings
 from sklearn.utils.testing import assert_almost_equal
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_array_equal
@@ -24,6 +25,7 @@
 from sklearn.utils.testing import assert_greater_equal
 from sklearn.utils.testing import assert_raises
 from sklearn.utils.testing import assert_warns
+from sklearn.utils.testing import clean_warning_registry
 from sklearn.utils.testing import ignore_warnings
 
 from sklearn import datasets
@@ -198,8 +200,10 @@ def check_importances(name, X, y):
         assert_equal(importances.shape[0], 10)
         assert_equal(n_important, 3)
 
-        X_new = clf.transform(X, threshold="mean")
-        assert_less(0 < X_new.shape[1], X.shape[1])
+        clean_warning_registry()
+        with warnings.catch_warnings(record=True) as record:
+            X_new = clf.transform(X, threshold="mean")
+            assert_less(0 < X_new.shape[1], X.shape[1])
 
         # Check with sample weights
         sample_weight = np.ones(y.shape)
diff --git a/sklearn/ensemble/tests/test_gradient_boosting.py b/sklearn/ensemble/tests/test_gradient_boosting.py
index 29a1e4d3c33d..443e8eeb38b0 100644
--- a/sklearn/ensemble/tests/test_gradient_boosting.py
+++ b/sklearn/ensemble/tests/test_gradient_boosting.py
@@ -10,7 +10,7 @@
 from sklearn.ensemble import GradientBoostingRegressor
 from sklearn.ensemble.gradient_boosting import ZeroEstimator
 from sklearn.metrics import mean_squared_error
-from sklearn.utils import check_random_state, tosequence
+from sklearn.utils import check_random_state, tosequence, warnings
 from sklearn.utils.testing import assert_almost_equal
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_array_equal
@@ -20,6 +20,8 @@
 from sklearn.utils.testing import assert_raises
 from sklearn.utils.testing import assert_true
 from sklearn.utils.testing import assert_warns
+from sklearn.utils.testing import clean_warning_registry
+from sklearn.utils.testing import ignore_warnings
 from sklearn.utils.validation import DataConversionWarning
 from sklearn.utils.validation import NotFittedError
 
@@ -255,8 +257,10 @@ def test_feature_importances():
     #feature_importances = clf.feature_importances_
     assert_true(hasattr(clf, 'feature_importances_'))
 
-    X_new = clf.transform(X, threshold="mean")
-    assert_less(X_new.shape[1], X.shape[1])
+    clean_warning_registry()
+    with warnings.catch_warnings(record=True) as record:
+        X_new = clf.transform(X, threshold="mean")
+        assert_less(X_new.shape[1], X.shape[1])
 
     feature_mask = clf.feature_importances_ > clf.feature_importances_.mean()
     assert_array_almost_equal(X_new, X[:, feature_mask])
diff --git a/sklearn/tree/tests/test_tree.py b/sklearn/tree/tests/test_tree.py
index 12c150decbe2..77645c79488c 100644
--- a/sklearn/tree/tests/test_tree.py
+++ b/sklearn/tree/tests/test_tree.py
@@ -16,6 +16,7 @@
 from sklearn.metrics import accuracy_score
 from sklearn.metrics import mean_squared_error
 
+from sklearn.utils import warnings
 from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_almost_equal
@@ -26,7 +27,9 @@
 from sklearn.utils.testing import assert_greater_equal
 from sklearn.utils.testing import assert_less
 from sklearn.utils.testing import assert_true
+from sklearn.utils.testing import clean_warning_registry
 from sklearn.utils.testing import raises
+
 from sklearn.utils.validation import check_random_state
 from sklearn.utils.validation import NotFittedError
 from sklearn.utils.testing import ignore_warnings
@@ -377,9 +380,12 @@ def test_importances():
         assert_equal(importances.shape[0], 10, "Failed with {0}".format(name))
         assert_equal(n_important, 3, "Failed with {0}".format(name))
 
-        X_new = clf.transform(X, threshold="mean")
-        assert_less(0, X_new.shape[1], "Failed with {0}".format(name))
-        assert_less(X_new.shape[1], X.shape[1], "Failed with {0}".format(name))
+
+        clean_warning_registry()
+        with warnings.catch_warnings(record=True) as record:
+            X_new = clf.transform(X, threshold="mean")
+            assert_less(0, X_new.shape[1], "Failed with {0}".format(name))
+            assert_less(X_new.shape[1], X.shape[1], "Failed with {0}".format(name))
 
     # Check on iris that importances are the same for all builders
     clf = DecisionTreeClassifier(random_state=0)
