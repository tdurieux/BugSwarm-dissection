diff --git a/build_tools/circle/push_doc.sh b/build_tools/circle/push_doc.sh
index 2ab1cff4513b..cb87a84548b8 100755
--- a/build_tools/circle/push_doc.sh
+++ b/build_tools/circle/push_doc.sh
@@ -4,7 +4,7 @@
 # The behavior of the script is controlled by environment variable defined
 # in the circle.yml in the top level folder of the project.
 
-set -e
+set -ex
 
 if [ -z $CIRCLE_PROJECT_USERNAME ];
 then USERNAME="sklearn-ci";
@@ -38,11 +38,23 @@ if [ ! -d $DOC_REPO ];
 then git clone --depth 1 --no-checkout "git@github.com:scikit-learn/"$DOC_REPO".git";
 fi
 cd $DOC_REPO
-git config core.sparseCheckout true
+
+# check if it's a new branch
+
 echo $dir > .git/info/sparse-checkout
-git checkout $CIRCLE_BRANCH
-git reset --hard origin/$CIRCLE_BRANCH
-git rm -rf $dir/ && rm -rf $dir/
+if ! git show HEAD:$dir >/dev/null
+then
+	# directory does not exist. Need to make it so sparse checkout works
+	mkdir $dir
+	touch $dir/index.html
+	git add $dir
+fi
+git checkout master
+git reset --hard origin/master
+if [ -d $dir ]
+then
+	git rm -rf $dir/ && rm -rf $dir/
+fi
 cp -R $GENERATED_DOC_DIR $dir
 git config user.email "olivier.grisel+sklearn-ci@gmail.com"
 git config user.name $USERNAME
diff --git a/doc/index.rst b/doc/index.rst
index 0de085b1d721..d97d28f7011f 100644
--- a/doc/index.rst
+++ b/doc/index.rst
@@ -209,12 +209,10 @@
                     </li>
                     <li><strong>Scikit-learn 0.21 will drop support for Python 2.7 and Python 3.4.</strong>
                     </li>
-                    <li><em>July 2018.</em> scikit-learn 0.20 is available for download (<a href="whats_new.html#version-0-20">Changelog</a>).
+                    <li><em>September 2018.</em> scikit-learn 0.20 is available for download (<a href="whats_new.html#version-0-20">Changelog</a>).
                     </li>
                     <li><em>July 2018.</em> scikit-learn 0.19.2 is available for download (<a href="whats_new.html#version-0-19">Changelog</a>).
                     </li>
-                    <li><em>October 2017.</em> scikit-learn 0.19.1 is available for download (<a href="whats_new.html#version-0-19">Changelog</a>).
-                    </li>
                     <li><em>July 2017.</em> scikit-learn 0.19.0 is available for download (<a href="whats_new/v0.19.html#version-0-19">Changelog</a>).
                     </li>
                     <li><em>June 2017.</em> scikit-learn 0.18.2 is available for download (<a href="whats_new/v0.18.html#version-0-18-2">Changelog</a>).
diff --git a/doc/install.rst b/doc/install.rst
index 7dbb2287c406..bb6b67af3e3c 100644
--- a/doc/install.rst
+++ b/doc/install.rst
@@ -78,7 +78,7 @@ Canopy and Anaconda for all supported platforms
 
 `Canopy
 <https://www.enthought.com/products/canopy>`_ and `Anaconda
-<https://www.continuum.io/downloads>`_ both ship a recent
+<https://www.anaconda.com/download>`_ both ship a recent
 version of scikit-learn, in addition to a large set of scientific python
 library for Windows, Mac OSX and Linux.
 
diff --git a/doc/modules/clustering.rst b/doc/modules/clustering.rst
index 968a66e67fdc..1f8210f35ffb 100644
--- a/doc/modules/clustering.rst
+++ b/doc/modules/clustering.rst
@@ -838,9 +838,9 @@ algorithm builds a *reachability* graph, which assigns each sample both a
 ``reachability_`` distance, and a spot within the cluster ``ordering_``
 attribute; these two attributes are assigned when the model is fitted, and are
 used to determine cluster membership. If OPTICS is run with the default value
-of *inf* set for ``max_bound``, then DBSCAN style cluster extraction can be
+of *inf* set for ``max_eps``, then DBSCAN style cluster extraction can be
 performed in linear time for any given ``eps`` value using the
-``extract_dbscan`` method. Setting ``max_bound`` to a lower value will result
+``extract_dbscan`` method. Setting ``max_eps`` to a lower value will result
 in shorter run times, and can be thought of as the maximum cluster object size
 (in diameter) that OPTICS will be able to extract.
 
@@ -892,10 +892,10 @@ larger parent cluster.
     shorter run time than OPTICS; however, for repeated runs at varying ``eps``
     values, a single run of OPTICS may require less cumulative runtime than
     DBSCAN. It is also important to note that OPTICS output can be unstable at
-    ``eps`` values very close to the initial ``max_bound`` value. OPTICS seems
+    ``eps`` values very close to the initial ``max_eps`` value. OPTICS seems
     to produce near identical results to DBSCAN provided that ``eps`` passed to
     ``extract_dbscan`` is a half order of magnitude less than the inital
-    ``max_bound`` that was used to fit; using a value close to ``max_bound``
+    ``max_eps`` that was used to fit; using a value close to ``max_eps``
     will throw a warning, and using a value larger will result in an exception. 
 
 .. topic:: Computational Complexity
@@ -909,7 +909,7 @@ larger parent cluster.
     multithreaded, and has better algorithmic runtime complexity than OPTICS--
     at the cost of worse memory scaling. For extremely large datasets that
     exhaust system memory using HDBSCAN, OPTICS will maintain *n* (as opposed
-    to *n^2* memory scaling); however, tuning of the ``max_bound`` parameter
+    to *n^2* memory scaling); however, tuning of the ``max_eps`` parameter
     will likely need to be used to give a solution in a reasonable amount of
     wall time.
 
diff --git a/doc/modules/model_persistence.rst b/doc/modules/model_persistence.rst
index f5173e5d9f3f..ccf5755c1c7e 100644
--- a/doc/modules/model_persistence.rst
+++ b/doc/modules/model_persistence.rst
@@ -35,7 +35,7 @@ persistence model, namely `pickle <https://docs.python.org/2/library/pickle.html
   >>> y[0]
   0
 
-In the specific case of scikit-learn, it may be more interesting to use
+In the specific case of scikit-learn, it may be better to use
 joblib's replacement of pickle (``joblib.dump`` & ``joblib.load``),
 which is more efficient on objects that carry large numpy arrays internally as
 is often the case for fitted scikit-learn estimators, but can only pickle to the
diff --git a/doc/modules/outlier_detection.rst b/doc/modules/outlier_detection.rst
index 9dbe013bef5d..3482d4246cda 100644
--- a/doc/modules/outlier_detection.rst
+++ b/doc/modules/outlier_detection.rst
@@ -8,9 +8,9 @@ Novelty and Outlier Detection
 
 Many applications require being able to decide whether a new observation
 belongs to the same distribution as existing observations (it is an
-`inlier`), or should be considered as different (it is an outlier).
+*inlier*), or should be considered as different (it is an *outlier*).
 Often, this ability is used to clean real data sets. Two important
-distinction must be made:
+distinctions must be made:
 
 :outlier detection:
   The training data contains outliers which are defined as observations that
@@ -35,7 +35,7 @@ a low density region of the training data, considered as normal in this
 context.
 
 The scikit-learn project provides a set of machine learning tools that
-can be used both for novelty or outliers detection. This strategy is
+can be used both for novelty or outlier detection. This strategy is
 implemented with objects learning in an unsupervised way from the data::
 
     estimator.fit(X_train)
@@ -77,6 +77,18 @@ not available.
   The scores of abnormality of the training samples are always accessible
   through the ``negative_outlier_factor_`` attribute.
 
+The behavior of :class:`neighbors.LocalOutlierFactor` is summarized in the
+following table.
+
+===================== ================================ =====================
+Method                Outlier detection                Novelty detection
+===================== ================================ =====================
+``fit_predict``       OK                               Not available
+``predict``           Not available                    Use only on new data
+``decision_function`` Not available                    Use only on new data
+``score_samples``     Use ``negative_outlier_factor_`` Use only on new data
+===================== ================================ =====================
+
 
 Overview of outlier detection methods
 =====================================
@@ -162,7 +174,7 @@ Outlier Detection
 
 Outlier detection is similar to novelty detection in the sense that
 the goal is to separate a core of regular observations from some
-polluting ones, called "outliers". Yet, in the case of outlier
+polluting ones, called *outliers*. Yet, in the case of outlier
 detection, we don't have a clean data set representing the population
 of regular observations that can be used to train any tool.
 
@@ -341,19 +353,7 @@ Note that ``fit_predict`` is not available in this case.
   The scores of abnormality of the training samples are always accessible
   through the ``negative_outlier_factor_`` attribute.
 
-The behavior of LOF is summarized in the following table.
-
-====================  ================================  =====================
-Method                Outlier detection                 Novelty detection
-====================  ================================  =====================
-`fit_predict`         OK                                Not available
-`predict`             Not available                     Use only on test data
-`decision_function`   Not available                     Use only on test data
-`score_samples`       Use `negative_outlier_factor_`    Use only on test data
-====================  ================================  =====================
-
-
-This strategy is illustrated below.
+Novelty detection with Local Outlier Factor is illustrated below.
 
   .. figure:: ../auto_examples/neighbors/images/sphx_glr_plot_lof_novelty_detection_001.png
      :target: ../auto_examples/neighbors/sphx_glr_plot_lof_novelty_detection.html
diff --git a/doc/related_projects.rst b/doc/related_projects.rst
index 9e5d5a32c057..ce5f5c24dbf3 100644
--- a/doc/related_projects.rst
+++ b/doc/related_projects.rst
@@ -183,7 +183,10 @@ and tasks.
 
 - `multiisotonic <https://github.com/alexfields/multiisotonic>`_ Isotonic
   regression on multidimensional features.
-  
+
+- `scikit-multilearn <https://scikit.ml>`_ Multi-label classification with 
+  focus on label space manipulation.
+
 - `seglearn <https://github.com/dmbee/seglearn>`_ Time series and sequence 
   learning using sliding window segmentation.
 
diff --git a/doc/whats_new.rst b/doc/whats_new.rst
index 0e7345836f48..03cbcd3ed34b 100644
--- a/doc/whats_new.rst
+++ b/doc/whats_new.rst
@@ -11,8 +11,8 @@ Release notes for current and recent releases are detailed on this page, with
 **Tip:** `Subscribe to scikit-learn releases <https://libraries.io/pypi/scikit-learn>`__
 on libraries.io to be notified when new versions are released.
 
+.. include:: whats_new/v0.21.rst
 .. include:: whats_new/v0.20.rst
-.. include:: whats_new/v0.19.rst
 
 .. _previous_releases_whats_new:
 
@@ -21,6 +21,7 @@ Previous Releases
 .. toctree::
     :maxdepth: 1
 
+    Version 0.19 <whats_new/v0.19.rst>
     Version 0.18 <whats_new/v0.18.rst>
     Version 0.17 <whats_new/v0.17.rst>
     Version 0.16 <whats_new/v0.16.rst>
diff --git a/doc/whats_new/v0.20.rst b/doc/whats_new/v0.20.rst
index aae4a3e1db22..2ed336b78217 100644
--- a/doc/whats_new/v0.20.rst
+++ b/doc/whats_new/v0.20.rst
@@ -4,8 +4,10 @@
 
 .. _changes_0_20:
 
-Version 0.20 (under development)
-================================
+Version 0.20.0
+==============
+
+**September, 2018**
 
 This release packs in a mountain of bug fixes, features and enhancements for
 the Scikit-learn library, and improvements to the documentation and examples.
@@ -92,7 +94,7 @@ Known Major Bugs
 
 * :issue:`11924`: :class:`LogisticRegressionCV` with `solver='lbfgs'` and
   `multi_class='multinomial'` may be non-deterministic or otherwise broken on
-  MacOS. This appears to be the case on Travis CI servers, but has not been
+  macOS. This appears to be the case on Travis CI servers, but has not been
   confirmed on personal MacBooks! This issue has been present in previous
   releases.
 
@@ -107,7 +109,7 @@ Support for Python 3.3 has been officially dropped.
 
 - |MajorFeature| A new clustering algorithm: :class:`cluster.OPTICS`: an
   algoritm related to :class:`cluster.DBSCAN`, that has hyperparameters easier
-  to set and tat scales better, by :user:`Shane <espg>`.
+  to set and that scales better, by :user:`Shane <espg>`.
 
 - |MajorFeature| :class:`cluster.AgglomerativeClustering` now supports Single
   Linkage clustering via ``linkage='single'``. :issue:`9372` by :user:`Leland
@@ -126,7 +128,7 @@ Support for Python 3.3 has been officially dropped.
   regardless of ``algorithm``.
   :issue:`8003` by :user:`Joël Billaud <recamshak>`.
 
-- |Enhancement| :class:`cluster.KMeans` now gives a warning, if the number of
+- |Enhancement| :class:`cluster.KMeans` now gives a warning if the number of
   distinct clusters found is smaller than ``n_clusters``. This may occur when
   the number of distinct points in the data set is actually smaller than the
   number of cluster one is looking for.
@@ -145,8 +147,8 @@ Support for Python 3.3 has been officially dropped.
   and :user:`Devansh D. <devanshdalal>`.
 
 - |Fix| Fixed a bug in :func:`cluster.k_means_elkan` where the returned
-  `iteration` was 1 less than the correct value. Also added the missing
-  `n_iter_` attribute in the docstring of :class:`cluster.KMeans`.
+  ``iteration`` was 1 less than the correct value. Also added the missing
+  ``n_iter_`` attribute in the docstring of :class:`cluster.KMeans`.
   :issue:`11353` by :user:`Jeremie du Boisberranger <jeremiedbb>`.
 
 - |API| Deprecate ``pooling_func`` unused parameter in
@@ -189,12 +191,12 @@ Support for Python 3.3 has been officially dropped.
 .......................
 
 - |MajorFeature| Added :func:`datasets.fetch_openml` to fetch datasets from
-  `OpenML <http://openml.org>`. OpenML is a free, open data sharing platform
+  `OpenML <http://openml.org>`_. OpenML is a free, open data sharing platform
   and will be used instead of mldata as it provides better service availability.
   :issue:`9908` by `Andreas Müller`_ and :user:`Jan N. van Rijn <janvanrijn>`.
 
 - |Feature| In :func:`datasets.make_blobs`, one can now pass a list to the
-  `n_samples` parameter to indicate the number of samples to generate per
+  ``n_samples`` parameter to indicate the number of samples to generate per
   cluster. :issue:`8617` by :user:`Maskani Filali Mohamed <maskani-moh>` and
   :user:`Konstantinos Katrioplas <kkatrio>`.
 
@@ -253,8 +255,8 @@ Support for Python 3.3 has been officially dropped.
 
 - |Fix| In :class:`decomposition.PCA` selecting a n_components parameter greater
   than the number of samples now raises an error. Similarly, the
-  ``n_components=None`` case now selects the minimum of n_samples and
-  n_features.
+  ``n_components=None`` case now selects the minimum of ``n_samples`` and
+  ``n_features``.
   :issue:`8484` by :user:`Wally Gauze <wallygauze>`.
 
 - |Fix| Fixed a bug in :class:`decomposition.PCA` where users will get
@@ -266,7 +268,7 @@ Support for Python 3.3 has been officially dropped.
   :class:`decomposition.NMF` :issue:`10142` by `Tom Dupre la Tour`_.
 
 - |Fix| Fixed a bug in :class:`decomposition.SparseCoder` when running OMP
-  sparse coding in parallel using readonly memory mapped datastructures.
+  sparse coding in parallel using read-only memory mapped datastructures.
   :issue:`5956` by :user:`Vighnesh Birodkar <vighneshbirodkar>` and
   :user:`Olivier Grisel <ogrisel>`.
 
@@ -276,7 +278,7 @@ Support for Python 3.3 has been officially dropped.
 
 - |Efficiency| Memory usage improvement for :func:`_class_means` and
   :func:`_class_cov` in :mod:`discriminant_analysis`. :issue:`10898` by
-  :user:`Nanxin Chen <bobchennan>`.`
+  :user:`Nanxin Chen <bobchennan>`.
 
 
 :mod:`sklearn.dummy`
@@ -303,7 +305,7 @@ Support for Python 3.3 has been officially dropped.
   via ``n_iter_no_change``, ``validation_fraction`` and ``tol``. :issue:`7071`
   by `Raghav RV`_
 
-- |Feature| Add `named_estimators_` parameter in
+- |Feature| Added ``named_estimators_`` parameter in
   :class:`ensemble.VotingClassifier` to access fitted estimators.
   :issue:`9157` by :user:`Herilalaina Rakotoarison <herilalaina>`.
 
@@ -468,7 +470,7 @@ Support for Python 3.3 has been officially dropped.
 
 - |Fix| Fixed a bug in :class:`linear_model.LogisticRegressionCV` where the
   'ovr' strategy was always used to compute cross-validation scores in the
-  multiclass setting, even if 'multinomial' was set.
+  multiclass setting, even if ``'multinomial'`` was set.
   :issue:`8720` by :user:`William de Vazelhes <wdevazelhes>`.
 
 - |Fix| Fixed a bug in :class:`linear_model.OrthogonalMatchingPursuit` that was
@@ -499,7 +501,7 @@ Support for Python 3.3 has been officially dropped.
   :issue:`10687` by :user:`Martin Hahn <martin-hahn>`.
 
 - |Fix| Fixed a bug in :func:`sklearn.linear_model.LogisticRegression` where the
-  multi_class='multinomial' with binary output with warm_start = True
+  ``multi_class='multinomial'`` with binary output ``with warm_start=True``
   :issue:`10836` by :user:`Aishwarya Srinivasan <aishgrt1>`.
 
 - |Fix| Fixed a bug in :class:`linear_model.RidgeCV` where using integer
@@ -515,7 +517,7 @@ Support for Python 3.3 has been officially dropped.
   :class:`linear_model.PassiveAggressiveClassifier`,
   :class:`linear_model.PassiveAggressiveRegressor` and
   :class:`linear_model.Perceptron`, where the stopping criterion was stopping
-  the algorithm before convergence. A parameter `n_iter_no_change` was added
+  the algorithm before convergence. A parameter ``n_iter_no_change`` was added
   and set by default to 5. Previous behavior is equivalent to setting the
   parameter to 1. :issue:`9043` by `Tom Dupre la Tour`_.
 
@@ -800,7 +802,7 @@ Support for Python 3.3 has been officially dropped.
   memory efficient when ``algorithm='brute'``.
   :issue:`11136` by `Joel Nothman`_ and :user:`Aman Dalmia <dalmia>`.
 
-- |Feature| Add `sample_weight` parameter to the fit method of
+- |Feature| Add ``sample_weight`` parameter to the fit method of
   :class:`neighbors.KernelDensity` to enable weighting in kernel density
   estimation.
   :issue:`4394` by :user:`Samuel O. Ronsin <samronsin>`.
@@ -829,8 +831,8 @@ Support for Python 3.3 has been officially dropped.
   faster construction and querying times.
   :issue:`11556` by :user:`Jake VanderPlas <jakevdp>`
 
-- |Fix| Fixed a bug in `neighbors.KDTree` and `neighbors.BallTree` where
-  pickled tree objects would change their type to the super class `BinaryTree`.
+- |Fix| Fixed a bug in :class:`neighbors.KDTree` and :class:`neighbors.BallTree` where
+  pickled tree objects would change their type to the super class :class:`BinaryTree`.
   :issue:`11774` by :user:`Nicolas Hug <NicolasHug>`.
 
 
@@ -947,7 +949,7 @@ Support for Python 3.3 has been officially dropped.
   :issue:`11042` by :user:`Daniel Morales <DanielMorales9>`.
 
 - |Fix| Fix ``fit`` and ``partial_fit`` in
-  :class:`preprocessing.StandardScaler` in the rare case when `with_mean=False`
+  :class:`preprocessing.StandardScaler` in the rare case when ``with_mean=False``
   and `with_std=False` which was crashing by calling ``fit`` more than once and
   giving inconsistent results for ``mean_`` whether the input was a sparse or a
   dense matrix. ``mean_`` will be set to ``None`` with both sparse and dense
diff --git a/doc/whats_new/v0.21.rst b/doc/whats_new/v0.21.rst
new file mode 100644
index 000000000000..202972f0575c
--- /dev/null
+++ b/doc/whats_new/v0.21.rst
@@ -0,0 +1,49 @@
+.. include:: _contributors.rst
+
+.. currentmodule:: sklearn
+
+.. _changes_0_21:
+
+Version 0.21.0
+==============
+
+**In development**
+
+Changed models
+--------------
+
+The following estimators and functions, when fit with the same data and
+parameters, may produce different models from the previous version. This often
+occurs due to changes in the modelling logic (bug fixes or enhancements), or in
+random sampling procedures.
+
+- please add class and reason here (see version 0.20 what's new)
+
+Details are listed in the changelog below.
+
+(While we are trying to better inform users by providing this information, we
+cannot assure that this list is complete.)
+
+Changelog
+---------
+
+Support for Python 3.4 and below has been officially dropped.
+
+..
+    See version doc/whats_new/v0.20.rst for structure. Entries should be
+    prefixed with one of the labels: |MajorFeature|, |Feature|, |Efficiency|,
+    |Enhancement|, |Fix| or |API|. They should be under a heading for the
+    relevant module (or *Multiple Modules* or *Miscellaneous*), and within each
+    section should be ordered according to the label ordering above. Entries
+    should end with: :issue:`123456` by :user:`Joe Bloggs <joeongithub>`.
+
+- An entry goes here
+- An entry goes here
+
+Multiple modules
+................
+
+Changes to estimator checks
+---------------------------
+
+These changes mostly affect library developers.
diff --git a/examples/cluster/plot_optics.py b/examples/cluster/plot_optics.py
index 19fd683dddc3..ed4a742b99da 100755
--- a/examples/cluster/plot_optics.py
+++ b/examples/cluster/plot_optics.py
@@ -68,8 +68,8 @@
     Rk = reachability[labels == k]
     ax1.plot(Xk, Rk, c, alpha=0.3)
 ax1.plot(space[labels == -1], reachability[labels == -1], 'k.', alpha=0.3)
-ax1.plot(space, np.full_like(space, 0.75), 'k-', alpha=0.5)
-ax1.plot(space, np.full_like(space, 0.25), 'k-.', alpha=0.5)
+ax1.plot(space, np.full_like(space, 0.75, dtype=float), 'k-', alpha=0.5)
+ax1.plot(space, np.full_like(space, 0.25, dtype=float), 'k-.', alpha=0.5)
 ax1.set_ylabel('Reachability (epsilon distance)')
 ax1.set_title('Reachability Plot')
 
diff --git a/examples/compose/plot_column_transformer_mixed_types.py b/examples/compose/plot_column_transformer_mixed_types.py
index 64f1a3c88d3d..73ee27f83a90 100644
--- a/examples/compose/plot_column_transformer_mixed_types.py
+++ b/examples/compose/plot_column_transformer_mixed_types.py
@@ -61,13 +61,12 @@
 categorical_features = ['embarked', 'sex', 'pclass']
 categorical_transformer = Pipeline(steps=[
     ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
-    ('onehot', OneHotEncoder(sparse=False, handle_unknown='ignore'))])
+    ('onehot', OneHotEncoder(handle_unknown='ignore'))])
 
 preprocessor = ColumnTransformer(
     transformers=[
         ('num', numeric_transformer, numeric_features),
-        ('cat', categorical_transformer, categorical_features)],
-    remainder='drop')
+        ('cat', categorical_transformer, categorical_features)])
 
 # Append classifier to preprocessing pipeline.
 # Now we have a full prediction pipeline.
@@ -77,8 +76,7 @@
 X = data.drop('survived', axis=1)
 y = data['survived']
 
-X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
-                                                    shuffle=True)
+X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
 
 clf.fit(X_train, y_train)
 print("model score: %.3f" % clf.score(X_test, y_test))
diff --git a/examples/neural_networks/plot_mnist_filters.py b/examples/neural_networks/plot_mnist_filters.py
index ab50d4e59a81..408e555443c5 100644
--- a/examples/neural_networks/plot_mnist_filters.py
+++ b/examples/neural_networks/plot_mnist_filters.py
@@ -28,6 +28,7 @@
 
 # Load data from https://www.openml.org/d/554
 X, y = fetch_openml('mnist_784', version=1, return_X_y=True)
+X = X / 255.
 
 # rescale the data, use the traditional train/test split
 X_train, X_test = X[:60000], X[60000:]
diff --git a/sklearn/__init__.py b/sklearn/__init__.py
index 1d7cd2ef9200..aafc8a34b2a1 100644
--- a/sklearn/__init__.py
+++ b/sklearn/__init__.py
@@ -44,7 +44,7 @@
 # Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
 # 'X.Y.dev0' is the canonical version of 'X.Y.dev'
 #
-__version__ = '0.20.dev0'
+__version__ = '0.21.dev0'
 
 
 try:
diff --git a/sklearn/base.py b/sklearn/base.py
index d75adb06d61b..56ffb18bf8b6 100644
--- a/sklearn/base.py
+++ b/sklearn/base.py
@@ -342,9 +342,12 @@ def fit_predict(self, X, y=None):
         X : ndarray, shape (n_samples, n_features)
             Input data.
 
+        y : Ignored
+            not used, present for API consistency by convention.
+
         Returns
         -------
-        y : ndarray, shape (n_samples,)
+        labels : ndarray, shape (n_samples,)
             cluster labels
         """
         # non-optimized default implementation; override when a better
@@ -494,6 +497,9 @@ def fit_predict(self, X, y=None):
         X : ndarray, shape (n_samples, n_features)
             Input data.
 
+        y : Ignored
+            not used, present for API consistency by convention.
+
         Returns
         -------
         y : ndarray, shape (n_samples,)
diff --git a/sklearn/cluster/dbscan_.py b/sklearn/cluster/dbscan_.py
index f10890e10f2c..c1239b1388dc 100644
--- a/sklearn/cluster/dbscan_.py
+++ b/sklearn/cluster/dbscan_.py
@@ -233,7 +233,7 @@ class DBSCAN(BaseEstimator, ClusterMixin):
 
     n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run.
-       ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
         for more details.
 
diff --git a/sklearn/cluster/k_means_.py b/sklearn/cluster/k_means_.py
index c402bf6c8b61..5fbe8810e56e 100644
--- a/sklearn/cluster/k_means_.py
+++ b/sklearn/cluster/k_means_.py
@@ -948,6 +948,7 @@ def fit(self, X, y=None, sample_weight=None):
             copy if the given data is not C-contiguous.
 
         y : Ignored
+            not used, present here for API consistency by convention.
 
         sample_weight : array-like, shape (n_samples,), optional
             The weights for each observation in X. If None, all observations
@@ -979,6 +980,7 @@ def fit_predict(self, X, y=None, sample_weight=None):
             New data to transform.
 
         y : Ignored
+            not used, present here for API consistency by convention.
 
         sample_weight : array-like, shape (n_samples,), optional
             The weights for each observation in X. If None, all observations
@@ -1002,6 +1004,7 @@ def fit_transform(self, X, y=None, sample_weight=None):
             New data to transform.
 
         y : Ignored
+            not used, present here for API consistency by convention.
 
         sample_weight : array-like, shape (n_samples,), optional
             The weights for each observation in X. If None, all observations
@@ -1081,6 +1084,7 @@ def score(self, X, y=None, sample_weight=None):
             New data.
 
         y : Ignored
+            not used, present here for API consistency by convention.
 
         sample_weight : array-like, shape (n_samples,), optional
             The weights for each observation in X. If None, all observations
@@ -1473,6 +1477,7 @@ def fit(self, X, y=None, sample_weight=None):
             if the given data is not C-contiguous.
 
         y : Ignored
+            not used, present here for API consistency by convention.
 
         sample_weight : array-like, shape (n_samples,), optional
             The weights for each observation in X. If None, all observations
@@ -1654,6 +1659,7 @@ def partial_fit(self, X, y=None, sample_weight=None):
             X will be copied if it is not C-contiguous.
 
         y : Ignored
+            not used, present here for API consistency by convention.
 
         sample_weight : array-like, shape (n_samples,), optional
             The weights for each observation in X. If None, all observations
diff --git a/sklearn/cluster/optics_.py b/sklearn/cluster/optics_.py
index e10a92a7590e..5c20ddb42184 100755
--- a/sklearn/cluster/optics_.py
+++ b/sklearn/cluster/optics_.py
@@ -21,7 +21,7 @@
 from ._optics_inner import quick_scan
 
 
-def optics(X, min_samples=5, max_bound=np.inf, metric='euclidean',
+def optics(X, min_samples=5, max_eps=np.inf, metric='euclidean',
            p=2, metric_params=None, maxima_ratio=.75,
            rejection_ratio=.7, similarity_threshold=0.4,
            significant_min=.003, min_cluster_size_ratio=.005,
@@ -41,20 +41,19 @@ def optics(X, min_samples=5, max_bound=np.inf, metric='euclidean',
     X : array, shape (n_samples, n_features)
         The data.
 
-    min_samples : int
+    min_samples : int (default=5)
         The number of samples in a neighborhood for a point to be considered
         as a core point.
 
-    max_bound : float, optional
+    max_eps : float, optional (default=np.inf)
         The maximum distance between two samples for them to be considered
-        as in the same neighborhood. This is also the largest object size
-        expected within the dataset. Default value of "np.inf" will identify
-        clusters across all scales; reducing `max_bound` will result in
+        as in the same neighborhood. Default value of "np.inf" will identify
+        clusters across all scales; reducing `max_eps` will result in
         shorter run times.
 
-    metric : string or callable, optional
+    metric : string or callable, optional (default='euclidean')
         The distance metric to use for neighborhood lookups. Default is
-        "minkowski". Other options include "euclidean", "manhattan",
+        "euclidean". Other options include "minkowski", "manhattan",
         "chebyshev", "haversine", "seuclidean", "hamming", "canberra",
         and "braycurtis". The "wminkowski" and "mahalanobis" metrics are
         also valid with an additional argument.
@@ -68,20 +67,20 @@ def optics(X, min_samples=5, max_bound=np.inf, metric='euclidean',
     metric_params : dict, optional (default=None)
         Additional keyword arguments for the metric function.
 
-    maxima_ratio : float, optional
+    maxima_ratio : float, optional (default=.75)
         The maximum ratio we allow of average height of clusters on the
         right and left to the local maxima in question. The higher the
         ratio, the more generous the algorithm is to preserving local
         minima, and the more cuts the resulting tree will have.
 
-    rejection_ratio : float, optional
+    rejection_ratio : float, optional (default=.7)
         Adjusts the fitness of the clustering. When the maxima_ratio is
         exceeded, determine which of the clusters to the left and right to
         reject based on rejection_ratio. Higher values will result in points
         being more readily classified as noise; conversely, lower values will
         result in more points being clustered.
 
-    similarity_threshold : float, optional
+    similarity_threshold : float, optional (default=.4)
         Used to check if nodes can be moved up one level, that is, if the
         new cluster created is too "similar" to its parent, given the
         similarity threshold. Similarity can be determined by 1) the size
@@ -91,19 +90,21 @@ def optics(X, min_samples=5, max_bound=np.inf, metric='euclidean',
         node. A lower value for the similarity threshold means less levels
         in the tree.
 
-    significant_min : float, optional
+    significant_min : float, optional (default=.003)
         Sets a lower threshold on how small a significant maxima can be.
 
-    min_cluster_size_ratio : float, optional
+    min_cluster_size_ratio : float, optional (default=.005)
         Minimum percentage of dataset expected for cluster membership.
 
-    min_maxima_ratio : float, optional
+    min_maxima_ratio : float, optional (default=.001)
         Used to determine neighborhood size for minimum cluster membership.
+        Each local maxima should be a largest value in a neighborhood
+        of the `size min_maxima_ratio * len(X)` from left and right.
 
     algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional
         Algorithm used to compute the nearest neighbors:
 
-        - 'ball_tree' will use :class:`BallTree`
+        - 'ball_tree' will use :class:`BallTree` (default)
         - 'kd_tree' will use :class:`KDTree`
         - 'brute' will use a brute-force search.
         - 'auto' will attempt to decide the most appropriate algorithm
@@ -147,7 +148,7 @@ def optics(X, min_samples=5, max_bound=np.inf, metric='euclidean',
     Record 28, no. 2 (1999): 49-60.
     """
 
-    clust = OPTICS(min_samples, max_bound, metric, p, metric_params,
+    clust = OPTICS(min_samples, max_eps, metric, p, metric_params,
                    maxima_ratio, rejection_ratio,
                    similarity_threshold, significant_min,
                    min_cluster_size_ratio, min_maxima_ratio,
@@ -168,20 +169,19 @@ class OPTICS(BaseEstimator, ClusterMixin):
 
     Parameters
     ----------
-    min_samples : int
+    min_samples : int (default=5)
         The number of samples in a neighborhood for a point to be considered
         as a core point.
 
-    max_bound : float, optional
+    max_eps : float, optional (default=np.inf)
         The maximum distance between two samples for them to be considered
-        as in the same neighborhood. This is also the largest object size
-        expected within the dataset. Default value of "np.inf" will identify
-        clusters across all scales; reducing `max_bound` will result in
+        as in the same neighborhood. Default value of "np.inf" will identify
+        clusters across all scales; reducing `max_eps` will result in
         shorter run times.
 
-    metric : string or callable, optional
+    metric : string or callable, optional (default='euclidean')
         The distance metric to use for neighborhood lookups. Default is
-        "minkowski". Other options include "euclidean", "manhattan",
+        "euclidean". Other options include "minkowski", "manhattan",
         "chebyshev", "haversine", "seuclidean", "hamming", "canberra",
         and "braycurtis". The "wminkowski" and "mahalanobis" metrics are
         also valid with an additional argument.
@@ -195,20 +195,20 @@ class OPTICS(BaseEstimator, ClusterMixin):
     metric_params : dict, optional (default=None)
         Additional keyword arguments for the metric function.
 
-    maxima_ratio : float, optional
+    maxima_ratio : float, optional (default=.75)
         The maximum ratio we allow of average height of clusters on the
         right and left to the local maxima in question. The higher the
         ratio, the more generous the algorithm is to preserving local
         minima, and the more cuts the resulting tree will have.
 
-    rejection_ratio : float, optional
+    rejection_ratio : float, optional (default=.7)
         Adjusts the fitness of the clustering. When the maxima_ratio is
         exceeded, determine which of the clusters to the left and right to
         reject based on rejection_ratio. Higher values will result in points
         being more readily classified as noise; conversely, lower values will
         result in more points being clustered.
 
-    similarity_threshold : float, optional
+    similarity_threshold : float, optional (default=.4)
         Used to check if nodes can be moved up one level, that is, if the
         new cluster created is too "similar" to its parent, given the
         similarity threshold. Similarity can be determined by 1) the size
@@ -218,19 +218,21 @@ class OPTICS(BaseEstimator, ClusterMixin):
         node. A lower value for the similarity threshold means less levels
         in the tree.
 
-    significant_min : float, optional
+    significant_min : float, optional (default=.003)
         Sets a lower threshold on how small a significant maxima can be.
 
-    min_cluster_size_ratio : float, optional
+    min_cluster_size_ratio : float, optional (default=.005)
         Minimum percentage of dataset expected for cluster membership.
 
-    min_maxima_ratio : float, optional
+    min_maxima_ratio : float, optional (default=.001)
         Used to determine neighborhood size for minimum cluster membership.
+        Each local maxima should be a largest value in a neighborhood
+        of the `size min_maxima_ratio * len(X)` from left and right.
 
     algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional
         Algorithm used to compute the nearest neighbors:
 
-        - 'ball_tree' will use :class:`BallTree`
+        - 'ball_tree' will use :class:`BallTree` (default)
         - 'kd_tree' will use :class:`KDTree`
         - 'brute' will use a brute-force search.
         - 'auto' will attempt to decide the most appropriate algorithm
@@ -284,14 +286,14 @@ class OPTICS(BaseEstimator, ClusterMixin):
     Record 28, no. 2 (1999): 49-60.
     """
 
-    def __init__(self, min_samples=5, max_bound=np.inf, metric='euclidean',
+    def __init__(self, min_samples=5, max_eps=np.inf, metric='euclidean',
                  p=2, metric_params=None, maxima_ratio=.75,
                  rejection_ratio=.7, similarity_threshold=0.4,
                  significant_min=.003, min_cluster_size_ratio=.005,
                  min_maxima_ratio=0.001, algorithm='ball_tree',
                  leaf_size=30, n_jobs=None):
 
-        self.max_bound = max_bound
+        self.max_eps = max_eps
         self.min_samples = min_samples
         self.maxima_ratio = maxima_ratio
         self.rejection_ratio = rejection_ratio
@@ -310,7 +312,7 @@ def fit(self, X, y=None):
         """Perform OPTICS clustering
 
         Extracts an ordered list of points and reachability distances, and
-        performs initial clustering using `max_bound` distance specified at
+        performs initial clustering using `max_eps` distance specified at
         OPTICS object instantiation.
 
         Parameters
@@ -370,7 +372,6 @@ def fit(self, X, y=None):
                                                  self.min_cluster_size_ratio,
                                                  self.min_maxima_ratio)
         self.core_sample_indices_ = indices_
-        self.n_clusters_ = np.max(self.labels_)
         return self
 
     # OPTICS helper functions; these should not be public #
@@ -378,7 +379,7 @@ def fit(self, X, y=None):
     def _expand_cluster_order(self, point, X, nbrs):
         # As above, not parallelizable. Parallelizing would allow items in
         # the 'unprocessed' list to switch to 'processed'
-        if self.core_distances_[point] <= self.max_bound:
+        if self.core_distances_[point] <= self.max_eps:
             while not self._processed[point]:
                 self._processed[point] = True
                 self.ordering_.append(point)
@@ -389,7 +390,7 @@ def _expand_cluster_order(self, point, X, nbrs):
 
     def _set_reach_dist(self, point_index, X, nbrs):
         P = np.array(X[point_index]).reshape(1, -1)
-        indices = nbrs.radius_neighbors(P, radius=self.max_bound,
+        indices = nbrs.radius_neighbors(P, radius=self.max_eps,
                                         return_distance=False)[0]
 
         # Getting indices of neighbors that have not been processed
@@ -416,17 +417,17 @@ def _set_reach_dist(self, point_index, X, nbrs):
     def extract_dbscan(self, eps):
         """Performs DBSCAN extraction for an arbitrary epsilon.
 
-        Extraction runs in linear time. Note that if the `max_bound` OPTICS
+        Extraction runs in linear time. Note that if the `max_eps` OPTICS
         parameter was set to < inf for extracting reachability and ordering
         arrays, DBSCAN extractions will be unstable for `eps` values close to
-        `max_bound`. Setting `eps` < (`max_bound` / 5.0) will guarantee
+        `max_eps`. Setting `eps` < (`max_eps` / 5.0) will guarantee
         extraction parity with DBSCAN.
 
         Parameters
         ----------
         eps : float or int, required
-            DBSCAN `eps` parameter. Must be set to < `max_bound`. Equivalence
-            with DBSCAN algorithm is achieved if `eps` is < (`max_bound` / 5)
+            DBSCAN `eps` parameter. Must be set to < `max_eps`. Equivalence
+            with DBSCAN algorithm is achieved if `eps` is < (`max_eps` / 5)
 
         Returns
         -------
@@ -438,14 +439,14 @@ def extract_dbscan(self, eps):
         """
         check_is_fitted(self, 'reachability_')
 
-        if eps > self.max_bound:
+        if eps > self.max_eps:
             raise ValueError('Specify an epsilon smaller than %s. Got %s.'
-                             % (self.max_bound, eps))
+                             % (self.max_eps, eps))
 
-        if eps * 5.0 > (self.max_bound * 1.05):
+        if eps * 5.0 > (self.max_eps * 1.05):
             warnings.warn(
-                "Warning, max_bound (%s) is close to eps (%s): "
-                "Output may be unstable." % (self.max_bound, eps),
+                "Warning, max_eps (%s) is close to eps (%s): "
+                "Output may be unstable." % (self.max_eps, eps),
                 RuntimeWarning, stacklevel=2)
         # Stability warning is documented in _extract_dbscan method...
 
@@ -612,12 +613,6 @@ def __init__(self, points, start, end, parent_node):
         self.children = []
         self.split_point = -1
 
-    def assign_split_point(self, split_point):
-        self.split_point = split_point
-
-    def add_child(self, child):
-        self.children.append(child)
-
 
 def _is_local_maxima(index, reachability_plot, neighborhood_size):
     right_idx = slice(index + 1, index + neighborhood_size + 1)
@@ -661,7 +656,7 @@ def _cluster_tree(node, parent_node, local_maxima_points,
 
     # take largest local maximum as possible separation between clusters
     s = local_maxima_points[0]
-    node.assign_split_point(s)
+    node.split_point = s
     local_maxima_points = local_maxima_points[1:]
 
     # create two new nodes and add to list of nodes
@@ -683,7 +678,7 @@ def _cluster_tree(node, parent_node, local_maxima_points,
     node_list.append((node_2, local_max_2))
 
     if reachability_plot[s] < significant_min:
-        node.assign_split_point(-1)
+        node.split_point = -1
         # if split_point is not significant, ignore this split and continue
         _cluster_tree(node, parent_node, local_maxima_points,
                       reachability_plot, reachability_ordering,
@@ -715,7 +710,7 @@ def _cluster_tree(node, parent_node, local_maxima_points,
                 (avg_reach2 / reachability_plot[s]) >= rejection_ratio):
             # since split_point is not significant,
             # ignore this split and continue (reject both child nodes)
-            node.assign_split_point(-1)
+            node.split_point = -1
             _cluster_tree(node, parent_node, local_maxima_points,
                           reachability_plot, reachability_ordering,
                           min_cluster_size, maxima_ratio, rejection_ratio,
@@ -733,7 +728,7 @@ def _cluster_tree(node, parent_node, local_maxima_points,
         node_list.remove((node_2, local_max_2))
     if not node_list:
         # parent_node will be a leaf
-        node.assign_split_point(-1)
+        node.split_point = -1
         return
 
     # Check if nodes can be moved up one level - the new cluster created
@@ -748,13 +743,13 @@ def _cluster_tree(node, parent_node, local_maxima_points,
 
     for nl in node_list:
         if bypass_node == 1:
-            parent_node.add_child(nl[0])
+            parent_node.children.append(nl[0])
             _cluster_tree(nl[0], parent_node, nl[1],
                           reachability_plot, reachability_ordering,
                           min_cluster_size, maxima_ratio, rejection_ratio,
                           similarity_threshold, significant_min)
         else:
-            node.add_child(nl[0])
+            node.children.append(nl[0])
             _cluster_tree(nl[0], node, nl[1], reachability_plot,
                           reachability_ordering, min_cluster_size,
                           maxima_ratio, rejection_ratio,
diff --git a/sklearn/cluster/tests/test_optics.py b/sklearn/cluster/tests/test_optics.py
index b5b4acfae12c..5a89cb7a0c43 100755
--- a/sklearn/cluster/tests/test_optics.py
+++ b/sklearn/cluster/tests/test_optics.py
@@ -15,6 +15,7 @@
 from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_raise_message
 from sklearn.utils.testing import assert_allclose
+from sklearn.utils import _IS_32BIT
 
 from sklearn.cluster.tests.common import generate_clustered_data
 
@@ -26,7 +27,7 @@ def test_correct_number_of_clusters():
     X = generate_clustered_data(n_clusters=n_clusters)
     # Parameters chosen specifically for this task.
     # Compute OPTICS
-    clust = OPTICS(max_bound=5.0 * 6.0, min_samples=4, metric='euclidean')
+    clust = OPTICS(max_eps=5.0 * 6.0, min_samples=4, metric='euclidean')
     clust.fit(X)
     # number of clusters, ignoring noise if present
     n_clusters_1 = len(set(clust.labels_)) - int(-1 in clust.labels_)
@@ -40,7 +41,7 @@ def test_minimum_number_of_sample_check():
 
     # Compute OPTICS
     X = [[1, 1]]
-    clust = OPTICS(max_bound=5.0 * 0.3, min_samples=10)
+    clust = OPTICS(max_eps=5.0 * 0.3, min_samples=10)
 
     # Run the fit
     assert_raise_message(ValueError, msg, clust.fit, X)
@@ -50,7 +51,7 @@ def test_empty_extract():
     # Test extract where fit() has not yet been run.
     msg = ("This OPTICS instance is not fitted yet. Call 'fit' with "
            "appropriate arguments before using this method.")
-    clust = OPTICS(max_bound=5.0 * 0.3, min_samples=10)
+    clust = OPTICS(max_eps=5.0 * 0.3, min_samples=10)
     assert_raise_message(ValueError, msg, clust.extract_dbscan, 0.01)
 
 
@@ -62,7 +63,7 @@ def test_bad_extract():
                                 cluster_std=0.4, random_state=0)
 
     # Compute OPTICS
-    clust = OPTICS(max_bound=5.0 * 0.003, min_samples=10)
+    clust = OPTICS(max_eps=5.0 * 0.003, min_samples=10)
     clust2 = clust.fit(X)
     assert_raise_message(ValueError, msg, clust2.extract_dbscan, 0.3)
 
@@ -75,7 +76,7 @@ def test_close_extract():
                                 cluster_std=0.4, random_state=0)
 
     # Compute OPTICS
-    clust = OPTICS(max_bound=1.0, min_samples=10)
+    clust = OPTICS(max_eps=1.0, min_samples=10)
     clust3 = clust.fit(X)
     # check warning when centers are passed
     assert_warns(RuntimeWarning, clust3.extract_dbscan, .3)
diff --git a/sklearn/covariance/elliptic_envelope.py b/sklearn/covariance/elliptic_envelope.py
index a150c032ed43..90e5a0f6d6b6 100644
--- a/sklearn/covariance/elliptic_envelope.py
+++ b/sklearn/covariance/elliptic_envelope.py
@@ -103,7 +103,10 @@ def fit(self, X, y=None):
         ----------
         X : numpy array or sparse matrix, shape (n_samples, n_features).
             Training data
-        y : (ignored)
+
+        y : Ignored
+            not used, present for API consistency by convention.
+
         """
         super(EllipticEnvelope, self).fit(X)
         self.offset_ = np.percentile(-self.dist_, 100. * self.contamination)
diff --git a/sklearn/datasets/openml.py b/sklearn/datasets/openml.py
index a35fa5130799..a58aa7482cda 100644
--- a/sklearn/datasets/openml.py
+++ b/sklearn/datasets/openml.py
@@ -7,10 +7,10 @@
 
 try:
     # Python 3+
-    from urllib.request import urlopen
+    from urllib.request import urlopen, Request
 except ImportError:
     # Python 2
-    from urllib2 import urlopen
+    from urllib2 import urlopen, Request
 
 
 import numpy as np
@@ -18,7 +18,7 @@
 
 from sklearn.externals import _arff
 from .base import get_data_home
-from ..externals.six import string_types, PY2
+from ..externals.six import string_types, PY2, BytesIO
 from ..externals.six.moves.urllib.error import HTTPError
 from ..utils import Bunch
 
@@ -50,8 +50,18 @@ def _open_openml_url(openml_path, data_home):
     result : stream
         A stream to the OpenML resource
     """
+    req = Request(_OPENML_PREFIX + openml_path)
+    req.add_header('Accept-encoding', 'gzip')
+    fsrc = urlopen(req)
+    is_gzip = fsrc.info().get('Content-Encoding', '') == 'gzip'
+
     if data_home is None:
-        return urlopen(_OPENML_PREFIX + openml_path)
+        if is_gzip:
+            if PY2:
+                fsrc = BytesIO(fsrc.read())
+            return gzip.GzipFile(fileobj=fsrc, mode='rb')
+        return fsrc
+
     local_path = os.path.join(data_home, 'openml.org', openml_path + ".gz")
     if not os.path.exists(local_path):
         try:
@@ -61,15 +71,16 @@ def _open_openml_url(openml_path, data_home):
             pass
 
         try:
-            with gzip.GzipFile(local_path, 'wb') as fdst:
-                fsrc = urlopen(_OPENML_PREFIX + openml_path)
+            with open(local_path, 'wb') as fdst:
                 shutil.copyfileobj(fsrc, fdst)
                 fsrc.close()
         except Exception:
             os.unlink(local_path)
             raise
     # XXX: unnecessary decompression on first access
-    return gzip.GzipFile(local_path, 'rb')
+    if is_gzip:
+        return gzip.GzipFile(local_path, 'rb')
+    return fsrc
 
 
 def _get_json_content_from_openml_api(url, error_message, raise_if_error,
@@ -308,7 +319,7 @@ def _download_data_arff(file_id, sparse, data_home, encode_nominal=True):
         return_type = _arff.DENSE
 
     if PY2:
-        arff_file = _arff.load(response, encode_nominal=encode_nominal,
+        arff_file = _arff.load(response.read(), encode_nominal=encode_nominal,
                                return_type=return_type, )
     else:
         arff_file = _arff.loads(response.read().decode('utf-8'),
diff --git a/sklearn/datasets/tests/test_openml.py b/sklearn/datasets/tests/test_openml.py
index c5be1b41607b..3f5716cb9678 100644
--- a/sklearn/datasets/tests/test_openml.py
+++ b/sklearn/datasets/tests/test_openml.py
@@ -7,6 +7,7 @@
 import re
 import scipy.sparse
 import sklearn
+import pytest
 
 from sklearn.datasets import fetch_openml
 from sklearn.datasets.openml import (_open_openml_url,
@@ -135,7 +136,10 @@ def _fetch_dataset_from_openml(data_id, data_name, data_version,
     return data_by_id
 
 
-def _monkey_patch_webbased_functions(context, data_id, gziped_files):
+def _monkey_patch_webbased_functions(context,
+                                     data_id,
+                                     gziped_files,
+                                     gzip_response):
     url_prefix_data_description = "https://openml.org/api/v1/json/data/"
     url_prefix_data_features = "https://openml.org/api/v1/json/data/features/"
     url_prefix_download_data = "https://openml.org/data/v1/"
@@ -147,32 +151,70 @@ def _monkey_patch_webbased_functions(context, data_id, gziped_files):
         path_suffix = '.gz'
         read_fn = gzip.open
 
+    class MockHTTPResponse(object):
+        def __init__(self, data, is_gzip):
+            self.data = data
+            self.is_gzip = is_gzip
+
+        def read(self, amt=-1):
+            return self.data.read(amt)
+
+        def tell(self):
+            return self.data.tell()
+
+        def seek(self, pos, whence=0):
+            return self.data.seek(pos, whence)
+
+        def close(self):
+            self.data.close()
+
+        def info(self):
+            if self.is_gzip:
+                return {'Content-Encoding': 'gzip'}
+            return {}
+
     def _file_name(url, suffix):
         return (re.sub(r'\W', '-', url[len("https://openml.org/"):])
                 + suffix + path_suffix)
 
-    def _mock_urlopen_data_description(url):
+    def _mock_urlopen_data_description(url, has_gzip_header):
         assert url.startswith(url_prefix_data_description)
 
         path = os.path.join(currdir, 'data', 'openml', str(data_id),
                             _file_name(url, '.json'))
-        return read_fn(path, 'rb')
 
-    def _mock_urlopen_data_features(url):
-        assert url.startswith(url_prefix_data_features)
+        if has_gzip_header and gzip_response:
+            fp = open(path, 'rb')
+            return MockHTTPResponse(fp, True)
+        else:
+            fp = read_fn(path, 'rb')
+            return MockHTTPResponse(fp, False)
 
+    def _mock_urlopen_data_features(url, has_gzip_header):
+        assert url.startswith(url_prefix_data_features)
         path = os.path.join(currdir, 'data', 'openml', str(data_id),
                             _file_name(url, '.json'))
-        return read_fn(path, 'rb')
+        if has_gzip_header and gzip_response:
+            fp = open(path, 'rb')
+            return MockHTTPResponse(fp, True)
+        else:
+            fp = read_fn(path, 'rb')
+            return MockHTTPResponse(fp, False)
 
-    def _mock_urlopen_download_data(url):
+    def _mock_urlopen_download_data(url, has_gzip_header):
         assert (url.startswith(url_prefix_download_data))
 
         path = os.path.join(currdir, 'data', 'openml', str(data_id),
                             _file_name(url, '.arff'))
-        return read_fn(path, 'rb')
 
-    def _mock_urlopen_data_list(url):
+        if has_gzip_header and gzip_response:
+            fp = open(path, 'rb')
+            return MockHTTPResponse(fp, True)
+        else:
+            fp = read_fn(path, 'rb')
+            return MockHTTPResponse(fp, False)
+
+    def _mock_urlopen_data_list(url, has_gzip_header):
         assert url.startswith(url_prefix_data_list)
 
         json_file_path = os.path.join(currdir, 'data', 'openml',
@@ -184,17 +226,25 @@ def _mock_urlopen_data_list(url):
             raise HTTPError(url=None, code=412,
                             msg='Simulated mock error',
                             hdrs=None, fp=None)
-        return read_fn(json_file_path, 'rb')
 
-    def _mock_urlopen(url):
+        if has_gzip_header:
+            fp = open(json_file_path, 'rb')
+            return MockHTTPResponse(fp, True)
+        else:
+            fp = read_fn(json_file_path, 'rb')
+            return MockHTTPResponse(fp, False)
+
+    def _mock_urlopen(request):
+        url = request.get_full_url()
+        has_gzip_header = request.get_header('Accept-encoding') == "gzip"
         if url.startswith(url_prefix_data_list):
-            return _mock_urlopen_data_list(url)
+            return _mock_urlopen_data_list(url, has_gzip_header)
         elif url.startswith(url_prefix_data_features):
-            return _mock_urlopen_data_features(url)
+            return _mock_urlopen_data_features(url, has_gzip_header)
         elif url.startswith(url_prefix_download_data):
-            return _mock_urlopen_download_data(url)
+            return _mock_urlopen_download_data(url, has_gzip_header)
         elif url.startswith(url_prefix_data_description):
-            return _mock_urlopen_data_description(url)
+            return _mock_urlopen_data_description(url, has_gzip_header)
         else:
             raise ValueError('Unknown mocking URL pattern: %s' % url)
 
@@ -203,7 +253,8 @@ def _mock_urlopen(url):
         context.setattr(sklearn.datasets.openml, 'urlopen', _mock_urlopen)
 
 
-def test_fetch_openml_iris(monkeypatch):
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_fetch_openml_iris(monkeypatch, gzip_response):
     # classification dataset with numeric only columns
     data_id = 61
     data_name = 'iris'
@@ -213,7 +264,8 @@ def test_fetch_openml_iris(monkeypatch):
     expected_features = 4
     expected_missing = 0
 
-    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    _monkey_patch_webbased_functions(
+        monkeypatch, data_id, test_gzip, gzip_response)
     assert_warns_message(
         UserWarning,
         "Multiple active versions of the dataset matching the name"
@@ -238,7 +290,8 @@ def test_decode_iris():
     _test_features_list(data_id)
 
 
-def test_fetch_openml_iris_multitarget(monkeypatch):
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_fetch_openml_iris_multitarget(monkeypatch, gzip_response):
     # classification dataset with numeric only columns
     data_id = 61
     data_name = 'iris'
@@ -248,7 +301,8 @@ def test_fetch_openml_iris_multitarget(monkeypatch):
     expected_features = 3
     expected_missing = 0
 
-    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    _monkey_patch_webbased_functions(
+        monkeypatch, data_id, test_gzip, gzip_response)
     _fetch_dataset_from_openml(data_id, data_name, data_version, target_column,
                                expected_observations, expected_features,
                                expected_missing,
@@ -256,7 +310,8 @@ def test_fetch_openml_iris_multitarget(monkeypatch):
                                compare_default_target=False)
 
 
-def test_fetch_openml_anneal(monkeypatch):
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_fetch_openml_anneal(monkeypatch, gzip_response):
     # classification dataset with numeric and categorical columns
     data_id = 2
     data_name = 'anneal'
@@ -266,7 +321,8 @@ def test_fetch_openml_anneal(monkeypatch):
     expected_observations = 11
     expected_features = 38
     expected_missing = 267
-    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    _monkey_patch_webbased_functions(
+        monkeypatch, data_id, test_gzip, gzip_response)
     _fetch_dataset_from_openml(data_id, data_name, data_version, target_column,
                                expected_observations, expected_features,
                                expected_missing,
@@ -279,7 +335,8 @@ def test_decode_anneal():
     _test_features_list(data_id)
 
 
-def test_fetch_openml_anneal_multitarget(monkeypatch):
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_fetch_openml_anneal_multitarget(monkeypatch, gzip_response):
     # classification dataset with numeric and categorical columns
     data_id = 2
     data_name = 'anneal'
@@ -289,7 +346,8 @@ def test_fetch_openml_anneal_multitarget(monkeypatch):
     expected_observations = 11
     expected_features = 36
     expected_missing = 267
-    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    _monkey_patch_webbased_functions(
+        monkeypatch, data_id, test_gzip, gzip_response)
     _fetch_dataset_from_openml(data_id, data_name, data_version, target_column,
                                expected_observations, expected_features,
                                expected_missing,
@@ -297,7 +355,8 @@ def test_fetch_openml_anneal_multitarget(monkeypatch):
                                compare_default_target=False)
 
 
-def test_fetch_openml_cpu(monkeypatch):
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_fetch_openml_cpu(monkeypatch, gzip_response):
     # regression dataset with numeric and categorical columns
     data_id = 561
     data_name = 'cpu'
@@ -306,7 +365,8 @@ def test_fetch_openml_cpu(monkeypatch):
     expected_observations = 209
     expected_features = 7
     expected_missing = 0
-    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    _monkey_patch_webbased_functions(
+        monkeypatch, data_id, test_gzip, gzip_response)
     _fetch_dataset_from_openml(data_id, data_name, data_version, target_column,
                                expected_observations, expected_features,
                                expected_missing,
@@ -319,7 +379,8 @@ def test_decode_cpu():
     _test_features_list(data_id)
 
 
-def test_fetch_openml_australian(monkeypatch):
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_fetch_openml_australian(monkeypatch, gzip_response):
     # sparse dataset
     # Australian is the only sparse dataset that is reasonably small
     # as it is inactive, we need to catch the warning. Due to mocking
@@ -332,7 +393,8 @@ def test_fetch_openml_australian(monkeypatch):
     expected_observations = 85
     expected_features = 14
     expected_missing = 0
-    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    _monkey_patch_webbased_functions(
+        monkeypatch, data_id, test_gzip, gzip_response)
     assert_warns_message(
         UserWarning,
         "Version 1 of dataset Australian is inactive,",
@@ -350,7 +412,8 @@ def test_fetch_openml_australian(monkeypatch):
     )
 
 
-def test_fetch_openml_miceprotein(monkeypatch):
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_fetch_openml_miceprotein(monkeypatch, gzip_response):
     # JvR: very important check, as this dataset defined several row ids
     # and ignore attributes. Note that data_features json has 82 attributes,
     # and row id (1), ignore attributes (3) have been removed (and target is
@@ -363,7 +426,8 @@ def test_fetch_openml_miceprotein(monkeypatch):
     expected_observations = 7
     expected_features = 77
     expected_missing = 7
-    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    _monkey_patch_webbased_functions(
+        monkeypatch, data_id, test_gzip, gzip_response)
     _fetch_dataset_from_openml(data_id, data_name, data_version, target_column,
                                expected_observations, expected_features,
                                expected_missing,
@@ -371,7 +435,8 @@ def test_fetch_openml_miceprotein(monkeypatch):
                                compare_default_target=True)
 
 
-def test_fetch_openml_emotions(monkeypatch):
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_fetch_openml_emotions(monkeypatch, gzip_response):
     # classification dataset with multiple targets (natively)
     data_id = 40589
     data_name = 'emotions'
@@ -381,7 +446,8 @@ def test_fetch_openml_emotions(monkeypatch):
     expected_observations = 13
     expected_features = 72
     expected_missing = 0
-    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    _monkey_patch_webbased_functions(
+        monkeypatch, data_id, test_gzip, gzip_response)
 
     _fetch_dataset_from_openml(data_id, data_name, data_version, target_column,
                                expected_observations, expected_features,
@@ -395,10 +461,12 @@ def test_decode_emotions():
     _test_features_list(data_id)
 
 
-def test_open_openml_url_cache(monkeypatch):
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_open_openml_url_cache(monkeypatch, gzip_response):
     data_id = 61
 
-    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    _monkey_patch_webbased_functions(
+        monkeypatch, data_id, test_gzip, gzip_response)
     openml_path = sklearn.datasets.openml._DATA_FILE.format(data_id)
     test_directory = os.path.join(os.path.expanduser('~'), 'scikit_learn_data')
     # first fill the cache
@@ -411,23 +479,27 @@ def test_open_openml_url_cache(monkeypatch):
     assert response1.read() == response2.read()
 
 
-def test_fetch_openml_notarget(monkeypatch):
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_fetch_openml_notarget(monkeypatch, gzip_response):
     data_id = 61
     target_column = None
     expected_observations = 150
     expected_features = 5
 
-    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    _monkey_patch_webbased_functions(
+        monkeypatch, data_id, test_gzip, gzip_response)
     data = fetch_openml(data_id=data_id, target_column=target_column,
                         cache=False)
     assert data.data.shape == (expected_observations, expected_features)
     assert data.target is None
 
 
-def test_fetch_openml_inactive(monkeypatch):
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_fetch_openml_inactive(monkeypatch, gzip_response):
     # fetch inactive dataset by id
     data_id = 40675
-    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    _monkey_patch_webbased_functions(
+        monkeypatch, data_id, test_gzip, gzip_response)
     glas2 = assert_warns_message(
         UserWarning, "Version 1 of dataset glass2 is inactive,", fetch_openml,
         data_id=data_id, cache=False)
@@ -439,19 +511,23 @@ def test_fetch_openml_inactive(monkeypatch):
     assert int(glas2_by_version.details['id']) == data_id
 
 
-def test_fetch_nonexiting(monkeypatch):
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_fetch_nonexiting(monkeypatch, gzip_response):
     # there is no active version of glass2
     data_id = 40675
-    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    _monkey_patch_webbased_functions(
+        monkeypatch, data_id, test_gzip, gzip_response)
     # Note that we only want to search by name (not data id)
     assert_raise_message(ValueError, "No active dataset glass2 found",
                          fetch_openml, name='glass2', cache=False)
 
 
-def test_raises_illegal_multitarget(monkeypatch):
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_raises_illegal_multitarget(monkeypatch, gzip_response):
     data_id = 61
     targets = ['sepalwidth', 'class']
-    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    _monkey_patch_webbased_functions(
+        monkeypatch, data_id, test_gzip, gzip_response)
     # Note that we only want to search by name (not data id)
     assert_raise_message(ValueError,
                          "Can only handle homogeneous multi-target datasets,",
@@ -459,11 +535,13 @@ def test_raises_illegal_multitarget(monkeypatch):
                          target_column=targets, cache=False)
 
 
-def test_warn_ignore_attribute(monkeypatch):
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_warn_ignore_attribute(monkeypatch, gzip_response):
     data_id = 40966
     expected_row_id_msg = "target_column={} has flag is_row_identifier."
     expected_ignore_msg = "target_column={} has flag is_ignore."
-    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    _monkey_patch_webbased_functions(
+        monkeypatch, data_id, test_gzip, gzip_response)
     # single column test
     assert_warns_message(UserWarning, expected_row_id_msg.format('MouseID'),
                          fetch_openml, data_id=data_id,
@@ -484,18 +562,22 @@ def test_warn_ignore_attribute(monkeypatch):
                          cache=False)
 
 
-def test_string_attribute(monkeypatch):
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_string_attribute(monkeypatch, gzip_response):
     data_id = 40945
-    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    _monkey_patch_webbased_functions(
+        monkeypatch, data_id, test_gzip, gzip_response)
     # single column test
     assert_raise_message(ValueError,
                          'STRING attributes are not yet supported',
                          fetch_openml, data_id=data_id, cache=False)
 
 
-def test_illegal_column(monkeypatch):
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_illegal_column(monkeypatch, gzip_response):
     data_id = 61
-    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    _monkey_patch_webbased_functions(
+        monkeypatch, data_id, test_gzip, gzip_response)
     assert_raise_message(KeyError, "Could not find target_column=",
                          fetch_openml, data_id=data_id,
                          target_column='undefined', cache=False)
@@ -506,9 +588,11 @@ def test_illegal_column(monkeypatch):
                          cache=False)
 
 
-def test_fetch_openml_raises_missing_values_target(monkeypatch):
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_fetch_openml_raises_missing_values_target(monkeypatch, gzip_response):
     data_id = 2
-    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    _monkey_patch_webbased_functions(
+        monkeypatch, data_id, test_gzip, gzip_response)
     assert_raise_message(ValueError, "Target column ",
                          fetch_openml, data_id=data_id, target_column='family')
 
diff --git a/sklearn/decomposition/dict_learning.py b/sklearn/decomposition/dict_learning.py
index 17054dd0a4a7..d050949f131f 100644
--- a/sklearn/decomposition/dict_learning.py
+++ b/sklearn/decomposition/dict_learning.py
@@ -411,8 +411,7 @@ def _update_dict(dictionary, Y, code, verbose=False, return_r2=False,
             # R <- -1.0 * U_k * V_k^T + R
             R = ger(-1.0, dictionary[:, k], code[k, :], a=R, overwrite_a=True)
     if return_r2:
-        R **= 2
-        R = R.sum()
+        R = nrm2(R) ** 2.0
         return dictionary, R
     return dictionary
 
diff --git a/sklearn/ensemble/gradient_boosting.py b/sklearn/ensemble/gradient_boosting.py
index 6e9cd843d59b..ec2800ac669d 100644
--- a/sklearn/ensemble/gradient_boosting.py
+++ b/sklearn/ensemble/gradient_boosting.py
@@ -1190,22 +1190,14 @@ def _fit_stage(self, i, X, y, y_pred, sample_weight, sample_mask,
                 # no inplace multiplication!
                 sample_weight = sample_weight * sample_mask.astype(np.float64)
 
-            if X_csc is not None:
-                tree.fit(X_csc, residual, sample_weight=sample_weight,
-                         check_input=False, X_idx_sorted=X_idx_sorted)
-            else:
-                tree.fit(X, residual, sample_weight=sample_weight,
-                         check_input=False, X_idx_sorted=X_idx_sorted)
+            X = X_csr if X_csr is not None else X
+            tree.fit(X, residual, sample_weight=sample_weight,
+                     check_input=False, X_idx_sorted=X_idx_sorted)
 
             # update tree leaves
-            if X_csr is not None:
-                loss.update_terminal_regions(tree.tree_, X_csr, y, residual, y_pred,
-                                             sample_weight, sample_mask,
-                                             self.learning_rate, k=k)
-            else:
-                loss.update_terminal_regions(tree.tree_, X, y, residual, y_pred,
-                                             sample_weight, sample_mask,
-                                             self.learning_rate, k=k)
+            loss.update_terminal_regions(tree.tree_, X, y, residual, y_pred,
+                                         sample_weight, sample_mask,
+                                         self.learning_rate, k=k)
 
             # add tree to ensemble
             self.estimators_[i, k] = tree
diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py
index 4ab267fc737a..72d1d206f478 100644
--- a/sklearn/ensemble/iforest.py
+++ b/sklearn/ensemble/iforest.py
@@ -200,6 +200,9 @@ def fit(self, X, y=None, sample_weight=None):
         sample_weight : array-like, shape = [n_samples] or None
             Sample weights. If None, then samples are equally weighted.
 
+        y : Ignored
+            not used, present for API consistency by convention.
+
         Returns
         -------
         self : object
diff --git a/sklearn/externals/copy_joblib.sh b/sklearn/externals/copy_joblib.sh
index 878413297759..f2c4ab3ed359 100755
--- a/sklearn/externals/copy_joblib.sh
+++ b/sklearn/externals/copy_joblib.sh
@@ -11,7 +11,7 @@ else
         JOBLIB=$1
 fi
 
-pip install $JOBLIB --target $INSTALL_FOLDER
+pip install --no-cache $JOBLIB --target $INSTALL_FOLDER
 cp -r $INSTALL_FOLDER/joblib joblib
 rm -rf $INSTALL_FOLDER
 
diff --git a/sklearn/externals/joblib/__init__.py b/sklearn/externals/joblib/__init__.py
index 1b5938350ee3..a42646eb4c75 100644
--- a/sklearn/externals/joblib/__init__.py
+++ b/sklearn/externals/joblib/__init__.py
@@ -12,7 +12,7 @@
 
 
     ==================== ===============================================
-    **Documentation:**       http://pythonhosted.org/joblib
+    **Documentation:**       https://joblib.readthedocs.io
 
     **Download:**            http://pypi.python.org/pypi/joblib#downloads
 
@@ -106,7 +106,7 @@
 # Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
 # 'X.Y.dev0' is the canonical version of 'X.Y.dev'
 #
-__version__ = '0.12.2'
+__version__ = '0.12.4'
 
 
 from .memory import Memory, MemorizedResult, register_store_backend
diff --git a/sklearn/externals/joblib/_multiprocessing_helpers.py b/sklearn/externals/joblib/_multiprocessing_helpers.py
index 4111a26f229c..be642b869feb 100644
--- a/sklearn/externals/joblib/_multiprocessing_helpers.py
+++ b/sklearn/externals/joblib/_multiprocessing_helpers.py
@@ -4,6 +4,7 @@
 circular dependencies (for instance for the assert_spawning name).
 """
 import os
+import sys
 import warnings
 
 
@@ -21,7 +22,16 @@
 #            issue a warning if not
 if mp is not None:
     try:
-        _sem = mp.Semaphore()
+        # Use the spawn context
+        if sys.version_info < (3, 3):
+            Semaphore = mp.Semaphore
+        else:
+            # Using mp.Semaphore has a border effect and set the default
+            # backend for multiprocessing. To avoid that, we use the 'spawn'
+            # context which is available on all supported platforms.
+            ctx = mp.get_context('spawn')
+            Semaphore = ctx.Semaphore
+        _sem = Semaphore()
         del _sem  # cleanup
     except (ImportError, OSError) as e:
         mp = None
diff --git a/sklearn/externals/joblib/_parallel_backends.py b/sklearn/externals/joblib/_parallel_backends.py
index 85312abec6aa..c78750667edb 100644
--- a/sklearn/externals/joblib/_parallel_backends.py
+++ b/sklearn/externals/joblib/_parallel_backends.py
@@ -130,7 +130,6 @@ def get_nested_backend(self):
         else:
             return ThreadingBackend(nesting_level=nesting_level)
 
-
     @contextlib.contextmanager
     def retrieval_context(self):
         """Context manager to manage an execution context.
@@ -348,7 +347,8 @@ def configure(self, n_jobs=1, parallel=None, **backend_args):
         n_jobs = self.effective_n_jobs(n_jobs)
         if n_jobs == 1:
             # Avoid unnecessary overhead and use sequential backend instead.
-            raise FallbackToBackend(SequentialBackend())
+            raise FallbackToBackend(
+                SequentialBackend(nesting_level=self.nesting_level))
         self.parallel = parallel
         self._n_jobs = n_jobs
         return n_jobs
@@ -421,7 +421,8 @@ def configure(self, n_jobs=1, parallel=None, prefer=None, require=None,
         """Build a process or thread pool and return the number of workers"""
         n_jobs = self.effective_n_jobs(n_jobs)
         if n_jobs == 1:
-            raise FallbackToBackend(SequentialBackend())
+            raise FallbackToBackend(
+                SequentialBackend(nesting_level=self.nesting_level))
 
         already_forked = int(os.environ.get(self.JOBLIB_SPAWNED_PROCESS, 0))
         if already_forked:
@@ -462,7 +463,8 @@ def configure(self, n_jobs=1, parallel=None, prefer=None, require=None,
         """Build a process executor and return the number of workers"""
         n_jobs = self.effective_n_jobs(n_jobs)
         if n_jobs == 1:
-            raise FallbackToBackend(SequentialBackend())
+            raise FallbackToBackend(
+                SequentialBackend(nesting_level=self.nesting_level))
 
         self._workers = get_memmapping_executor(
             n_jobs, timeout=idle_worker_timeout,
diff --git a/sklearn/externals/joblib/_store_backends.py b/sklearn/externals/joblib/_store_backends.py
index 027fb9f9f7fb..9196f0a7746a 100644
--- a/sklearn/externals/joblib/_store_backends.py
+++ b/sklearn/externals/joblib/_store_backends.py
@@ -35,6 +35,8 @@ class StoreBackendBase(with_metaclass(ABCMeta)):
     """Helper Abstract Base Class which defines all methods that
        a StorageBackend must implement."""
 
+    location = None
+
     @abstractmethod
     def _open_item(self, f, mode):
         """Opens an item on the store and return a file-like object.
@@ -327,7 +329,8 @@ def _concurrency_safe_write(self, to_write, filename, write_func):
 
     def __repr__(self):
         """Printable representation of the store location."""
-        return self.location
+        return '{class_name}(location="{location}")'.format(
+            class_name=self.__class__.__name__, location=self.location)
 
 
 class FileSystemStoreBackend(StoreBackendBase, StoreBackendMixin):
@@ -384,11 +387,13 @@ def get_items(self):
 
         return items
 
-    def configure(self, location, verbose=1, backend_options={}):
+    def configure(self, location, verbose=1, backend_options=None):
         """Configure the store backend.
 
         For this backend, valid store options are 'compress' and 'mmap_mode'
         """
+        if backend_options is None:
+            backend_options = {}
 
         # setup location directory
         self.location = location
@@ -396,17 +401,15 @@ def configure(self, location, verbose=1, backend_options={}):
             mkdirp(self.location)
 
         # item can be stored compressed for faster I/O
-        self.compress = backend_options['compress']
+        self.compress = backend_options.get('compress', False)
 
         # FileSystemStoreBackend can be used with mmap_mode options under
         # certain conditions.
-        mmap_mode = None
-        if 'mmap_mode' in backend_options:
-            mmap_mode = backend_options['mmap_mode']
-            if self.compress and mmap_mode is not None:
-                warnings.warn('Compressed items cannot be memmapped in a '
-                              'filesystem store. Option will be ignored.',
-                              stacklevel=2)
+        mmap_mode = backend_options.get('mmap_mode')
+        if self.compress and mmap_mode is not None:
+            warnings.warn('Compressed items cannot be memmapped in a '
+                          'filesystem store. Option will be ignored.',
+                          stacklevel=2)
 
         self.mmap_mode = mmap_mode
         self.verbose = verbose
diff --git a/sklearn/externals/joblib/externals/cloudpickle/__init__.py b/sklearn/externals/joblib/externals/cloudpickle/__init__.py
index c8c8fa208a16..df671a0f1569 100644
--- a/sklearn/externals/joblib/externals/cloudpickle/__init__.py
+++ b/sklearn/externals/joblib/externals/cloudpickle/__init__.py
@@ -2,4 +2,4 @@
 
 from .cloudpickle import *
 
-__version__ = '0.5.2'
+__version__ = '0.5.5'
diff --git a/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py b/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py
index e5aab0591f57..b1107ba92c1d 100644
--- a/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py
+++ b/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py
@@ -163,7 +163,7 @@ def cell_set(cell, value):
     )(value)
 
 
-#relevant opcodes
+# relevant opcodes
 STORE_GLOBAL = opcode.opmap['STORE_GLOBAL']
 DELETE_GLOBAL = opcode.opmap['DELETE_GLOBAL']
 LOAD_GLOBAL = opcode.opmap['LOAD_GLOBAL']
@@ -173,7 +173,7 @@ def cell_set(cell, value):
 
 
 def islambda(func):
-    return getattr(func,'__name__') == '<lambda>'
+    return getattr(func, '__name__') == '<lambda>'
 
 
 _BUILTIN_TYPE_NAMES = {}
@@ -270,24 +270,19 @@ def dump(self, obj):
             if 'recursion' in e.args[0]:
                 msg = """Could not pickle object as excessively deep recursion required."""
                 raise pickle.PicklingError(msg)
+            else:
+                raise
 
     def save_memoryview(self, obj):
         self.save(obj.tobytes())
+
     dispatch[memoryview] = save_memoryview
 
     if not PY3:
         def save_buffer(self, obj):
             self.save(str(obj))
-        dispatch[buffer] = save_buffer
 
-    def save_unsupported(self, obj):
-        raise pickle.PicklingError("Cannot pickle objects of type %s" % type(obj))
-    dispatch[types.GeneratorType] = save_unsupported
-
-    # itertools objects do not pickle!
-    for v in itertools.__dict__.values():
-        if type(v) is type:
-            dispatch[v] = save_unsupported
+        dispatch[buffer] = save_buffer  # noqa: F821 'buffer' was removed in Python 3
 
     def save_module(self, obj):
         """
@@ -309,6 +304,7 @@ def save_module(self, obj):
             self.save_reduce(dynamic_subimport, (obj.__name__, vars(obj)), obj=obj)
         else:
             self.save_reduce(subimport, (obj.__name__,), obj=obj)
+
     dispatch[types.ModuleType] = save_module
 
     def save_codeobject(self, obj):
@@ -329,6 +325,7 @@ def save_codeobject(self, obj):
                 obj.co_firstlineno, obj.co_lnotab, obj.co_freevars, obj.co_cellvars
             )
         self.save_reduce(types.CodeType, args, obj=obj)
+
     dispatch[types.CodeType] = save_codeobject
 
     def save_function(self, obj, name=None):
@@ -337,7 +334,13 @@ def save_function(self, obj, name=None):
         Determines what kind of function obj is (e.g. lambda, defined at
         interactive prompt, etc) and handles the pickling appropriately.
         """
-        if obj in _BUILTIN_TYPE_CONSTRUCTORS:
+        try:
+            should_special_case = obj in _BUILTIN_TYPE_CONSTRUCTORS
+        except TypeError:
+            # Methods of builtin types aren't hashable in python 2.
+            should_special_case = False
+
+        if should_special_case:
             # We keep a special-cased cache of built-in type constructors at
             # global scope, because these functions are structured very
             # differently in different python versions and implementations (for
@@ -420,6 +423,7 @@ def save_function(self, obj, name=None):
         else:
             write(pickle.GLOBAL + modname + '\n' + name + '\n')
             self.memoize(obj)
+
     dispatch[types.FunctionType] = save_function
 
     def _save_subimports(self, code, top_level_dependencies):
@@ -427,19 +431,22 @@ def _save_subimports(self, code, top_level_dependencies):
         Ensure de-pickler imports any package child-modules that
         are needed by the function
         """
+
         # check if any known dependency is an imported package
         for x in top_level_dependencies:
             if isinstance(x, types.ModuleType) and hasattr(x, '__package__') and x.__package__:
                 # check if the package has any currently loaded sub-imports
                 prefix = x.__name__ + '.'
-                for name, module in sys.modules.items():
+                # A concurrent thread could mutate sys.modules,
+                # make sure we iterate over a copy to avoid exceptions
+                for name in list(sys.modules):
                     # Older versions of pytest will add a "None" module to sys.modules.
                     if name is not None and name.startswith(prefix):
                         # check whether the function can address the sub-module
                         tokens = set(name[len(prefix):].split('.'))
                         if not tokens - set(code.co_names):
                             # ensure unpickler executes this import
-                            self.save(module)
+                            self.save(sys.modules[name])
                             # then discards the reference to it
                             self.write(pickle.POP)
 
@@ -454,6 +461,15 @@ def save_dynamic_class(self, obj):
         clsdict = dict(obj.__dict__)  # copy dict proxy to a dict
         clsdict.pop('__weakref__', None)
 
+        # For ABCMeta in python3.7+, remove _abc_impl as it is not picklable.
+        # This is a fix which breaks the cache but this only makes the first
+        # calls to issubclass slower.
+        if "_abc_impl" in clsdict:
+            import abc
+            (registry, _, _, _) = abc._get_dump(obj)
+            clsdict["_abc_impl"] = [subclass_weakref()
+                                    for subclass_weakref in registry]
+
         # On PyPy, __doc__ is a readonly attribute, so we need to include it in
         # the initial skeleton class.  This is safe because we know that the
         # doc can't participate in a cycle with the original class.
@@ -545,9 +561,13 @@ def save_function_tuple(self, func):
             'globals': f_globals,
             'defaults': defaults,
             'dict': dct,
-            'module': func.__module__,
             'closure_values': closure_values,
+            'module': func.__module__,
+            'name': func.__name__,
+            'doc': func.__doc__,
         }
+        if hasattr(func, '__annotations__'):
+            state['annotations'] = func.__annotations__
         if hasattr(func, '__qualname__'):
             state['qualname'] = func.__qualname__
         save(state)
@@ -572,8 +592,7 @@ def extract_code_globals(cls, co):
                 # PyPy "builtin-code" object
                 out_names = set()
             else:
-                out_names = set(names[oparg]
-                                for op, oparg in _walk_global_ops(co))
+                out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}
 
                 # see if nested function have any global refs
                 if co.co_consts:
@@ -614,7 +633,15 @@ def extract_func_data(self, func):
         # save the dict
         dct = func.__dict__
 
-        base_globals = self.globals_ref.get(id(func.__globals__), {})
+        base_globals = self.globals_ref.get(id(func.__globals__), None)
+        if base_globals is None:
+            # For functions defined in __main__, use vars(__main__) for
+            # base_global. This is necessary to share the global variables
+            # across multiple functions in this module.
+            if func.__module__ == "__main__":
+                base_globals = "__main__"
+            else:
+                base_globals = {}
         self.globals_ref[id(func.__globals__)] = base_globals
 
         return (code, f_globals, defaults, closure, dct, base_globals)
@@ -623,6 +650,7 @@ def save_builtin_function(self, obj):
         if obj.__module__ == "__builtin__":
             return self.save_global(obj)
         return self.save_function(obj)
+
     dispatch[types.BuiltinFunctionType] = save_builtin_function
 
     def save_global(self, obj, name=None, pack=struct.pack):
@@ -661,7 +689,8 @@ def save_instancemethod(self, obj):
                 self.save_reduce(types.MethodType, (obj.__func__, obj.__self__), obj=obj)
             else:
                 self.save_reduce(types.MethodType, (obj.__func__, obj.__self__, obj.__self__.__class__),
-                         obj=obj)
+                                 obj=obj)
+
     dispatch[types.MethodType] = save_instancemethod
 
     def save_inst(self, obj):
@@ -715,11 +744,13 @@ def save_inst(self, obj):
     def save_property(self, obj):
         # properties not correctly saved in python
         self.save_reduce(property, (obj.fget, obj.fset, obj.fdel, obj.__doc__), obj=obj)
+
     dispatch[property] = save_property
 
     def save_classmethod(self, obj):
         orig_func = obj.__func__
         self.save_reduce(type(obj), (orig_func,), obj=obj)
+
     dispatch[classmethod] = save_classmethod
     dispatch[staticmethod] = save_classmethod
 
@@ -730,7 +761,7 @@ def __getitem__(self, item):
                 return item
         items = obj(Dummy())
         if not isinstance(items, tuple):
-            items = (items, )
+            items = (items,)
         return self.save_reduce(operator.itemgetter, items)
 
     if type(operator.itemgetter) is type:
@@ -761,16 +792,16 @@ def __getattribute__(self, item):
     def save_file(self, obj):
         """Save a file"""
         try:
-            import StringIO as pystringIO #we can't use cStringIO as it lacks the name attribute
+            import StringIO as pystringIO  # we can't use cStringIO as it lacks the name attribute
         except ImportError:
             import io as pystringIO
 
-        if not hasattr(obj, 'name') or  not hasattr(obj, 'mode'):
+        if not hasattr(obj, 'name') or not hasattr(obj, 'mode'):
             raise pickle.PicklingError("Cannot pickle files that do not map to an actual file")
         if obj is sys.stdout:
-            return self.save_reduce(getattr, (sys,'stdout'), obj=obj)
+            return self.save_reduce(getattr, (sys, 'stdout'), obj=obj)
         if obj is sys.stderr:
-            return self.save_reduce(getattr, (sys,'stderr'), obj=obj)
+            return self.save_reduce(getattr, (sys, 'stderr'), obj=obj)
         if obj is sys.stdin:
             raise pickle.PicklingError("Cannot pickle standard input")
         if obj.closed:
@@ -805,10 +836,10 @@ def save_ellipsis(self, obj):
     def save_not_implemented(self, obj):
         self.save_reduce(_gen_not_implemented, ())
 
-    if PY3:
-        dispatch[io.TextIOWrapper] = save_file
-    else:
+    try:               # Python 2
         dispatch[file] = save_file
+    except NameError:  # Python 3
+        dispatch[io.TextIOWrapper] = save_file
 
     dispatch[type(Ellipsis)] = save_ellipsis
     dispatch[type(NotImplemented)] = save_not_implemented
@@ -823,6 +854,11 @@ def save_logger(self, obj):
 
     dispatch[logging.Logger] = save_logger
 
+    def save_root_logger(self, obj):
+        self.save_reduce(logging.getLogger, (), obj=obj)
+
+    dispatch[logging.RootLogger] = save_root_logger
+
     """Special functions for Add-on libraries"""
     def inject_addons(self):
         """Plug in system. Register additional pickling functions if modules already loaded"""
@@ -928,7 +964,7 @@ def _modules_to_main(modList):
         if type(modname) is str:
             try:
                 mod = __import__(modname)
-            except Exception as e:
+            except Exception:
                 sys.stderr.write('warning: could not import %s\n.  '
                                  'Your function may unexpectedly error due to this import failing;'
                                  'A version mismatch is likely.  Specific error was:\n' % modname)
@@ -937,7 +973,7 @@ def _modules_to_main(modList):
                 setattr(main, mod.__name__, mod)
 
 
-#object generators:
+# object generators:
 def _genpartial(func, args, kwds):
     if not args:
         args = ()
@@ -945,9 +981,11 @@ def _genpartial(func, args, kwds):
         kwds = {}
     return partial(func, *args, **kwds)
 
+
 def _gen_ellipsis():
     return Ellipsis
 
+
 def _gen_not_implemented():
     return NotImplemented
 
@@ -1008,9 +1046,19 @@ def _fill_function(*args):
     else:
         raise ValueError('Unexpected _fill_value arguments: %r' % (args,))
 
-    func.__globals__.update(state['globals'])
+    # Only set global variables that do not exist.
+    for k, v in state['globals'].items():
+        if k not in func.__globals__:
+            func.__globals__[k] = v
+
     func.__defaults__ = state['defaults']
     func.__dict__ = state['dict']
+    if 'annotations' in state:
+        func.__annotations__ = state['annotations']
+    if 'doc' in state:
+        func.__doc__  = state['doc']
+    if 'name' in state:
+        func.__name__ = state['name']
     if 'module' in state:
         func.__module__ = state['module']
     if 'qualname' in state:
@@ -1041,6 +1089,8 @@ def _make_skel_func(code, cell_count, base_globals=None):
     """
     if base_globals is None:
         base_globals = {}
+    elif isinstance(base_globals, str):
+        base_globals = vars(sys.modules[base_globals])
     base_globals['__builtins__'] = __builtins__
 
     closure = (
@@ -1056,15 +1106,23 @@ def _rehydrate_skeleton_class(skeleton_class, class_dict):
 
     See CloudPickler.save_dynamic_class for more info.
     """
+    registry = None
     for attrname, attr in class_dict.items():
-        setattr(skeleton_class, attrname, attr)
+        if attrname == "_abc_impl":
+            registry = attr
+        else:
+            setattr(skeleton_class, attrname, attr)
+    if registry is not None:
+        for subclass in registry:
+            skeleton_class.register(subclass)
+
     return skeleton_class
 
 
 def _find_module(mod_name):
     """
     Iterate over each part instead of calling imp.find_module directly.
-    This function is able to find submodules (e.g. sickit.tree)
+    This function is able to find submodules (e.g. scikit.tree)
     """
     path = None
     for part in mod_name.split('.'):
@@ -1075,9 +1133,11 @@ def _find_module(mod_name):
             file.close()
     return path, description
 
+
 """Constructors for 3rd party libraries
 Note: These can never be renamed due to client compatibility issues"""
 
+
 def _getobject(modname, attribute):
     mod = __import__(modname, fromlist=[attribute])
     return mod.__dict__[attribute]
diff --git a/sklearn/externals/joblib/externals/loky/__init__.py b/sklearn/externals/joblib/externals/loky/__init__.py
index 6480423cffbb..18c01d0a6aa0 100644
--- a/sklearn/externals/joblib/externals/loky/__init__.py
+++ b/sklearn/externals/joblib/externals/loky/__init__.py
@@ -3,10 +3,20 @@
 :class:`ProcessPoolExecutor` and a function :func:`get_reusable_executor` which
 hide the pool management under the hood.
 """
-from .reusable_executor import get_reusable_executor  # noqa: F401
-from .process_executor import ProcessPoolExecutor  # noqa: F401
-from .process_executor import BrokenProcessPool  # noqa: F401
+from ._base import Executor, Future
+from ._base import wait, as_completed
+from ._base import TimeoutError, CancelledError
+from ._base import ALL_COMPLETED, FIRST_COMPLETED, FIRST_EXCEPTION
 
-from .backend.context import cpu_count  # noqa: F401
+from .backend.context import cpu_count
+from .reusable_executor import get_reusable_executor
+from .process_executor import BrokenProcessPool, ProcessPoolExecutor
 
-__version__ = '2.2.0'
+
+__all__ = ["get_reusable_executor", "cpu_count", "wait", "as_completed",
+           "Future", "Executor", "ProcessPoolExecutor",
+           "BrokenProcessPool", "CancelledError", "TimeoutError",
+           "FIRST_COMPLETED", "FIRST_EXCEPTION", "ALL_COMPLETED", ]
+
+
+__version__ = '2.3.0'
diff --git a/sklearn/externals/joblib/externals/loky/_base.py b/sklearn/externals/joblib/externals/loky/_base.py
index ff4ac92cf402..92422bbf3f2a 100644
--- a/sklearn/externals/joblib/externals/loky/_base.py
+++ b/sklearn/externals/joblib/externals/loky/_base.py
@@ -11,46 +11,58 @@
 # Licensed to PSF under a Contributor Agreement.
 
 import sys
-import collections
+import time
 import logging
 import threading
-import time
+import collections
+
+
+if sys.version_info[:2] >= (3, 3):
+
+    from concurrent.futures import wait, as_completed
+    from concurrent.futures import TimeoutError, CancelledError
+    from concurrent.futures import Executor, Future as _BaseFuture
+
+    from concurrent.futures import FIRST_EXCEPTION
+    from concurrent.futures import ALL_COMPLETED, FIRST_COMPLETED
+
+    from concurrent.futures._base import LOGGER
+    from concurrent.futures._base import PENDING, RUNNING, CANCELLED
+    from concurrent.futures._base import CANCELLED_AND_NOTIFIED, FINISHED
+else:
 
-FIRST_COMPLETED = 'FIRST_COMPLETED'
-FIRST_EXCEPTION = 'FIRST_EXCEPTION'
-ALL_COMPLETED = 'ALL_COMPLETED'
-_AS_COMPLETED = '_AS_COMPLETED'
-
-# Possible future states (for internal use by the futures package).
-PENDING = 'PENDING'
-RUNNING = 'RUNNING'
-# The future was cancelled by the user...
-CANCELLED = 'CANCELLED'
-# ...and _Waiter.add_cancelled() was called by a worker.
-CANCELLED_AND_NOTIFIED = 'CANCELLED_AND_NOTIFIED'
-FINISHED = 'FINISHED'
-
-_FUTURE_STATES = [
-    PENDING,
-    RUNNING,
-    CANCELLED,
-    CANCELLED_AND_NOTIFIED,
-    FINISHED
-]
-
-_STATE_TO_DESCRIPTION_MAP = {
-    PENDING: "pending",
-    RUNNING: "running",
-    CANCELLED: "cancelled",
-    CANCELLED_AND_NOTIFIED: "cancelled",
-    FINISHED: "finished"
-}
-
-# Logger for internal use by the futures package.
-LOGGER = logging.getLogger("concurrent.futures")
-
-
-if sys.version_info[:2] < (3, 3):
+    FIRST_COMPLETED = 'FIRST_COMPLETED'
+    FIRST_EXCEPTION = 'FIRST_EXCEPTION'
+    ALL_COMPLETED = 'ALL_COMPLETED'
+    _AS_COMPLETED = '_AS_COMPLETED'
+
+    # Possible future states (for internal use by the futures package).
+    PENDING = 'PENDING'
+    RUNNING = 'RUNNING'
+    # The future was cancelled by the user...
+    CANCELLED = 'CANCELLED'
+    # ...and _Waiter.add_cancelled() was called by a worker.
+    CANCELLED_AND_NOTIFIED = 'CANCELLED_AND_NOTIFIED'
+    FINISHED = 'FINISHED'
+
+    _FUTURE_STATES = [
+        PENDING,
+        RUNNING,
+        CANCELLED,
+        CANCELLED_AND_NOTIFIED,
+        FINISHED
+    ]
+
+    _STATE_TO_DESCRIPTION_MAP = {
+        PENDING: "pending",
+        RUNNING: "running",
+        CANCELLED: "cancelled",
+        CANCELLED_AND_NOTIFIED: "cancelled",
+        FINISHED: "finished"
+    }
+
+    # Logger for internal use by the futures package.
+    LOGGER = logging.getLogger("concurrent.futures")
 
     class Error(Exception):
         """Base class for all future-related exceptions."""
@@ -63,548 +75,553 @@ class CancelledError(Error):
     class TimeoutError(Error):
         """The operation exceeded the given deadline."""
         pass
-else:
-    from concurrent.futures import CancelledError, TimeoutError
 
+    class _Waiter(object):
+        """Provides the event that wait() and as_completed() block on."""
+        def __init__(self):
+            self.event = threading.Event()
+            self.finished_futures = []
 
-class _Waiter(object):
-    """Provides the event that wait() and as_completed() block on."""
-    def __init__(self):
-        self.event = threading.Event()
-        self.finished_futures = []
+        def add_result(self, future):
+            self.finished_futures.append(future)
 
-    def add_result(self, future):
-        self.finished_futures.append(future)
+        def add_exception(self, future):
+            self.finished_futures.append(future)
 
-    def add_exception(self, future):
-        self.finished_futures.append(future)
+        def add_cancelled(self, future):
+            self.finished_futures.append(future)
 
-    def add_cancelled(self, future):
-        self.finished_futures.append(future)
+    class _AsCompletedWaiter(_Waiter):
+        """Used by as_completed()."""
 
+        def __init__(self):
+            super(_AsCompletedWaiter, self).__init__()
+            self.lock = threading.Lock()
 
-class _AsCompletedWaiter(_Waiter):
-    """Used by as_completed()."""
+        def add_result(self, future):
+            with self.lock:
+                super(_AsCompletedWaiter, self).add_result(future)
+                self.event.set()
 
-    def __init__(self):
-        super(_AsCompletedWaiter, self).__init__()
-        self.lock = threading.Lock()
+        def add_exception(self, future):
+            with self.lock:
+                super(_AsCompletedWaiter, self).add_exception(future)
+                self.event.set()
 
-    def add_result(self, future):
-        with self.lock:
-            super(_AsCompletedWaiter, self).add_result(future)
-            self.event.set()
+        def add_cancelled(self, future):
+            with self.lock:
+                super(_AsCompletedWaiter, self).add_cancelled(future)
+                self.event.set()
 
-    def add_exception(self, future):
-        with self.lock:
-            super(_AsCompletedWaiter, self).add_exception(future)
-            self.event.set()
+    class _FirstCompletedWaiter(_Waiter):
+        """Used by wait(return_when=FIRST_COMPLETED)."""
 
-    def add_cancelled(self, future):
-        with self.lock:
-            super(_AsCompletedWaiter, self).add_cancelled(future)
+        def add_result(self, future):
+            super(_FirstCompletedWaiter, self).add_result(future)
             self.event.set()
 
+        def add_exception(self, future):
+            super(_FirstCompletedWaiter, self).add_exception(future)
+            self.event.set()
 
-class _FirstCompletedWaiter(_Waiter):
-    """Used by wait(return_when=FIRST_COMPLETED)."""
-
-    def add_result(self, future):
-        super(_FirstCompletedWaiter, self).add_result(future)
-        self.event.set()
-
-    def add_exception(self, future):
-        super(_FirstCompletedWaiter, self).add_exception(future)
-        self.event.set()
+        def add_cancelled(self, future):
+            super(_FirstCompletedWaiter, self).add_cancelled(future)
+            self.event.set()
 
-    def add_cancelled(self, future):
-        super(_FirstCompletedWaiter, self).add_cancelled(future)
-        self.event.set()
+    class _AllCompletedWaiter(_Waiter):
+        """Used by wait(return_when=FIRST_EXCEPTION and ALL_COMPLETED)."""
 
+        def __init__(self, num_pending_calls, stop_on_exception):
+            self.num_pending_calls = num_pending_calls
+            self.stop_on_exception = stop_on_exception
+            self.lock = threading.Lock()
+            super(_AllCompletedWaiter, self).__init__()
 
-class _AllCompletedWaiter(_Waiter):
-    """Used by wait(return_when=FIRST_EXCEPTION and ALL_COMPLETED)."""
+        def _decrement_pending_calls(self):
+            with self.lock:
+                self.num_pending_calls -= 1
+                if not self.num_pending_calls:
+                    self.event.set()
 
-    def __init__(self, num_pending_calls, stop_on_exception):
-        self.num_pending_calls = num_pending_calls
-        self.stop_on_exception = stop_on_exception
-        self.lock = threading.Lock()
-        super(_AllCompletedWaiter, self).__init__()
+        def add_result(self, future):
+            super(_AllCompletedWaiter, self).add_result(future)
+            self._decrement_pending_calls()
 
-    def _decrement_pending_calls(self):
-        with self.lock:
-            self.num_pending_calls -= 1
-            if not self.num_pending_calls:
+        def add_exception(self, future):
+            super(_AllCompletedWaiter, self).add_exception(future)
+            if self.stop_on_exception:
                 self.event.set()
+            else:
+                self._decrement_pending_calls()
 
-    def add_result(self, future):
-        super(_AllCompletedWaiter, self).add_result(future)
-        self._decrement_pending_calls()
-
-    def add_exception(self, future):
-        super(_AllCompletedWaiter, self).add_exception(future)
-        if self.stop_on_exception:
-            self.event.set()
-        else:
+        def add_cancelled(self, future):
+            super(_AllCompletedWaiter, self).add_cancelled(future)
             self._decrement_pending_calls()
 
-    def add_cancelled(self, future):
-        super(_AllCompletedWaiter, self).add_cancelled(future)
-        self._decrement_pending_calls()
-
-
-class _AcquireFutures(object):
-    """A context manager that does an ordered acquire of Future conditions."""
-
-    def __init__(self, futures):
-        self.futures = sorted(futures, key=id)
-
-    def __enter__(self):
-        for future in self.futures:
-            future._condition.acquire()
+    class _AcquireFutures(object):
+        """A context manager that does an ordered acquire of Future conditions.
+        """
 
-    def __exit__(self, *args):
-        for future in self.futures:
-            future._condition.release()
+        def __init__(self, futures):
+            self.futures = sorted(futures, key=id)
 
+        def __enter__(self):
+            for future in self.futures:
+                future._condition.acquire()
 
-def _create_and_install_waiters(fs, return_when):
-    if return_when == _AS_COMPLETED:
-        waiter = _AsCompletedWaiter()
-    elif return_when == FIRST_COMPLETED:
-        waiter = _FirstCompletedWaiter()
-    else:
-        pending_count = sum(
-                f._state not in [CANCELLED_AND_NOTIFIED, FINISHED] for f in fs)
+        def __exit__(self, *args):
+            for future in self.futures:
+                future._condition.release()
 
-        if return_when == FIRST_EXCEPTION:
-            waiter = _AllCompletedWaiter(pending_count, stop_on_exception=True)
-        elif return_when == ALL_COMPLETED:
-            waiter = _AllCompletedWaiter(pending_count,
-                                         stop_on_exception=False)
+    def _create_and_install_waiters(fs, return_when):
+        if return_when == _AS_COMPLETED:
+            waiter = _AsCompletedWaiter()
+        elif return_when == FIRST_COMPLETED:
+            waiter = _FirstCompletedWaiter()
         else:
-            raise ValueError("Invalid return condition: %r" % return_when)
-
-    for f in fs:
-        f._waiters.append(waiter)
-
-    return waiter
-
-
-def as_completed(fs, timeout=None):
-    """An iterator over the given futures that yields each as it completes.
-
-    Args:
-        fs: The sequence of Futures (possibly created by different Executors)
-            to iterate over.
-        timeout: The maximum number of seconds to wait. If None, then there
-            is no limit on the wait time.
-
-    Returns:
-        An iterator that yields the given Futures as they complete (finished or
-        cancelled). If any given Futures are duplicated, they will be returned
-        once.
-
-    Raises:
-        TimeoutError: If the entire result iterator could not be generated
-            before the given timeout.
-    """
-    if timeout is not None:
-        end_time = timeout + time.time()
-
-    fs = set(fs)
-    with _AcquireFutures(fs):
-        finished = set(
-                f for f in fs
-                if f._state in [CANCELLED_AND_NOTIFIED, FINISHED])
-        pending = fs - finished
-        waiter = _create_and_install_waiters(fs, _AS_COMPLETED)
-
-    try:
-        for future in finished:
-            yield future
-
-        while pending:
-            if timeout is None:
-                wait_timeout = None
+            pending_count = sum(
+                    f._state not in [CANCELLED_AND_NOTIFIED, FINISHED]
+                    for f in fs)
+
+            if return_when == FIRST_EXCEPTION:
+                waiter = _AllCompletedWaiter(pending_count,
+                                             stop_on_exception=True)
+            elif return_when == ALL_COMPLETED:
+                waiter = _AllCompletedWaiter(pending_count,
+                                             stop_on_exception=False)
             else:
-                wait_timeout = end_time - time.time()
-                if wait_timeout < 0:
-                    raise TimeoutError('%d (of %d) futures unfinished' % (
-                        len(pending), len(fs)))
-
-            waiter.event.wait(wait_timeout)
-
-            with waiter.lock:
-                finished = waiter.finished_futures
-                waiter.finished_futures = []
-                waiter.event.clear()
+                raise ValueError("Invalid return condition: %r" % return_when)
 
-            for future in finished:
-                yield future
-                pending.remove(future)
-
-    finally:
         for f in fs:
-            with f._condition:
-                f._waiters.remove(waiter)
-
-
-DoneAndNotDoneFutures = collections.namedtuple(
-        'DoneAndNotDoneFutures', 'done not_done')
-
-
-def wait(fs, timeout=None, return_when=ALL_COMPLETED):
-    """Wait for the futures in the given sequence to complete.
-
-    Args:
-        fs: The sequence of Futures (possibly created by different Executors)
-            to wait upon.
-        timeout: The maximum number of seconds to wait. If None, then there
-            is no limit on the wait time.
-        return_when: Indicates when this function should return. The options
-            are:
-
-            FIRST_COMPLETED - Return when any future finishes or is
-                              cancelled.
-            FIRST_EXCEPTION - Return when any future finishes by raising an
-                              exception. If no future raises an exception
-                              then it is equivalent to ALL_COMPLETED.
-            ALL_COMPLETED -   Return when all futures finish or are cancelled.
-
-    Returns:
-        A named 2-tuple of sets. The first set, named 'done', contains the
-        futures that completed (is finished or cancelled) before the wait
-        completed. The second set, named 'not_done', contains uncompleted
-        futures.
-    """
-    with _AcquireFutures(fs):
-        done = set(f for f in fs
-                   if f._state in [CANCELLED_AND_NOTIFIED, FINISHED])
-        not_done = set(fs) - done
-
-        if (return_when == FIRST_COMPLETED) and done:
-            return DoneAndNotDoneFutures(done, not_done)
-        elif (return_when == FIRST_EXCEPTION) and done:
-            if any(f for f in done
-                   if not f.cancelled() and f.exception() is not None):
-                return DoneAndNotDoneFutures(done, not_done)
-
-        if len(done) == len(fs):
-            return DoneAndNotDoneFutures(done, not_done)
-
-        waiter = _create_and_install_waiters(fs, return_when)
-
-    waiter.event.wait(timeout)
-    for f in fs:
-        with f._condition:
-            f._waiters.remove(waiter)
-
-    done.update(waiter.finished_futures)
-    return DoneAndNotDoneFutures(done, set(fs) - done)
-
-
-class Future(object):
-    """Represents the result of an asynchronous computation."""
-
-    def __init__(self):
-        """Initializes the future. Should not be called by clients."""
-        self._condition = threading.Condition()
-        self._state = PENDING
-        self._result = None
-        self._exception = None
-        self._waiters = []
-        self._done_callbacks = []
-
-    def _invoke_callbacks(self):
-        for callback in self._done_callbacks:
-            try:
-                callback(self)
-            except BaseException:
-                LOGGER.exception('exception calling callback for %r', self)
-
-    def __repr__(self):
-        with self._condition:
-            if self._state == FINISHED:
-                if self._exception:
-                    return '<%s at %#x state=%s raised %s>' % (
-                        self.__class__.__name__,
-                        id(self),
-                        _STATE_TO_DESCRIPTION_MAP[self._state],
-                        self._exception.__class__.__name__)
-                else:
-                    return '<%s at %#x state=%s returned %s>' % (
-                        self.__class__.__name__,
-                        id(self),
-                        _STATE_TO_DESCRIPTION_MAP[self._state],
-                        self._result.__class__.__name__)
-            return '<%s at %#x state=%s>' % (
-                    self.__class__.__name__,
-                    id(self),
-                   _STATE_TO_DESCRIPTION_MAP[self._state])
-
-    def cancel(self):
-        """Cancel the future if possible.
-
-        Returns True if the future was cancelled, False otherwise. A future
-        cannot be cancelled if it is running or has already completed.
-        """
-        with self._condition:
-            if self._state in [RUNNING, FINISHED]:
-                return False
-
-            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
-                return True
-
-            self._state = CANCELLED
-            self._condition.notify_all()
-
-        self._invoke_callbacks()
-        return True
-
-    def cancelled(self):
-        """Return True if the future was cancelled."""
-        with self._condition:
-            return self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]
-
-    def running(self):
-        """Return True if the future is currently executing."""
-        with self._condition:
-            return self._state == RUNNING
-
-    def done(self):
-        """Return True of the future was cancelled or finished executing."""
-        with self._condition:
-            return self._state in [CANCELLED, CANCELLED_AND_NOTIFIED, FINISHED]
-
-    def __get_result(self):
-        if self._exception:
-            raise self._exception
-        else:
-            return self._result
-
-    def add_done_callback(self, fn):
-        """Attaches a callable that will be called when the future finishes.
-
-        Args:
-            fn: A callable that will be called with this future as its only
-                argument when the future completes or is cancelled. The
-                callable will always be called by a thread in the same process
-                in which it was added. If the future has already completed or
-                been cancelled then the callable will be called immediately.
-                These callables are called in the order that they were added.
-        """
-        with self._condition:
-            if self._state not in [CANCELLED, CANCELLED_AND_NOTIFIED,
-                                   FINISHED]:
-                self._done_callbacks.append(fn)
-                return
-        fn(self)
-
-    def result(self, timeout=None):
-        """Return the result of the call that the future represents.
-
-        Args:
-            timeout: The number of seconds to wait for the result if the future
-                isn't done. If None, then there is no limit on the wait time.
+            f._waiters.append(waiter)
 
-        Returns:
-            The result of the call that the future represents.
-
-        Raises:
-            CancelledError: If the future was cancelled.
-            TimeoutError: If the future didn't finish executing before the
-                given timeout.
-            Exception: If the call raised then that exception will be raised.
-        """
-        with self._condition:
-            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
-                raise CancelledError()
-            elif self._state == FINISHED:
-                return self.__get_result()
-
-            self._condition.wait(timeout)
-
-            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
-                raise CancelledError()
-            elif self._state == FINISHED:
-                return self.__get_result()
-            else:
-                raise TimeoutError()
+        return waiter
 
-    def exception(self, timeout=None):
-        """Return the exception raised by the call that the future represents.
+    def as_completed(fs, timeout=None):
+        """An iterator over the given futures that yields each as it completes.
 
         Args:
-            timeout: The number of seconds to wait for the exception if the
-                future isn't done. If None, then there is no limit on the wait
-                time.
+            fs: The sequence of Futures (possibly created by different
+                Executors) to iterate over.
+            timeout: The maximum number of seconds to wait. If None, then there
+                is no limit on the wait time.
 
         Returns:
-            The exception raised by the call that the future represents or None
-            if the call completed without raising.
+            An iterator that yields the given Futures as they complete
+            (finished or cancelled). If any given Futures are duplicated, they
+            will be returned once.
 
         Raises:
-            CancelledError: If the future was cancelled.
-            TimeoutError: If the future didn't finish executing before the
-                given timeout.
+            TimeoutError: If the entire result iterator could not be generated
+                before the given timeout.
         """
+        if timeout is not None:
+            end_time = timeout + time.time()
 
-        with self._condition:
-            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
-                raise CancelledError()
-            elif self._state == FINISHED:
-                return self._exception
-
-            self._condition.wait(timeout)
-
-            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
-                raise CancelledError()
-            elif self._state == FINISHED:
-                return self._exception
-            else:
-                raise TimeoutError()
-
-    # The following methods should only be used by Executors and in tests.
-    def set_running_or_notify_cancel(self):
-        """Mark the future as running or process any cancel notifications.
-
-        Should only be used by Executor implementations and unit tests.
-
-        If the future has been cancelled (cancel() was called and returned
-        True) then any threads waiting on the future completing (though calls
-        to as_completed() or wait()) are notified and False is returned.
-
-        If the future was not cancelled then it is put in the running state
-        (future calls to running() will return True) and True is returned.
-
-        This method should be called by Executor implementations before
-        executing the work associated with this future. If this method returns
-        False then the work should not be executed.
+        fs = set(fs)
+        with _AcquireFutures(fs):
+            finished = set(
+                    f for f in fs
+                    if f._state in [CANCELLED_AND_NOTIFIED, FINISHED])
+            pending = fs - finished
+            waiter = _create_and_install_waiters(fs, _AS_COMPLETED)
 
-        Returns:
-            False if the Future was cancelled, True otherwise.
+        try:
+            for future in finished:
+                yield future
 
-        Raises:
-            RuntimeError: if this method was already called or if set_result()
-                or set_exception() was called.
-        """
-        with self._condition:
-            if self._state == CANCELLED:
-                self._state = CANCELLED_AND_NOTIFIED
-                for waiter in self._waiters:
-                    waiter.add_cancelled(self)
-                # self._condition.notify_all() is not necessary because
-                # self.cancel() triggers a notification.
-                return False
-            elif self._state == PENDING:
-                self._state = RUNNING
-                return True
-            else:
-                LOGGER.critical('Future %s in unexpected state: %s',
-                                id(self),
-                                self._state)
-                raise RuntimeError('Future in unexpected state')
+            while pending:
+                if timeout is None:
+                    wait_timeout = None
+                else:
+                    wait_timeout = end_time - time.time()
+                    if wait_timeout < 0:
+                        raise TimeoutError('%d (of %d) futures unfinished' % (
+                            len(pending), len(fs)))
 
-    def set_result(self, result):
-        """Sets the return value of work associated with the future.
+                waiter.event.wait(wait_timeout)
 
-        Should only be used by Executor implementations and unit tests.
-        """
-        with self._condition:
-            self._result = result
-            self._state = FINISHED
-            for waiter in self._waiters:
-                waiter.add_result(self)
-            self._condition.notify_all()
-        self._invoke_callbacks()
-
-    def set_exception(self, exception):
-        """Sets the result of the future as being the given exception.
-
-        Should only be used by Executor implementations and unit tests.
-        """
-        with self._condition:
-            self._exception = exception
-            self._state = FINISHED
-            for waiter in self._waiters:
-                waiter.add_exception(self)
-            self._condition.notify_all()
-        self._invoke_callbacks()
+                with waiter.lock:
+                    finished = waiter.finished_futures
+                    waiter.finished_futures = []
+                    waiter.event.clear()
 
+                for future in finished:
+                    yield future
+                    pending.remove(future)
 
-class Executor(object):
-    """This is an abstract base class for concrete asynchronous executors."""
+        finally:
+            for f in fs:
+                with f._condition:
+                    f._waiters.remove(waiter)
 
-    def submit(self, fn, *args, **kwargs):
-        """Submits a callable to be executed with the given arguments.
+    DoneAndNotDoneFutures = collections.namedtuple(
+            'DoneAndNotDoneFutures', 'done not_done')
 
-        Schedules the callable to be executed as fn(*args, **kwargs) and
-        returns a Future instance representing the execution of the callable.
-
-        Returns:
-            A Future representing the given call.
-        """
-        raise NotImplementedError()
-
-    def map(self, fn, *iterables, **kwargs):
-        """Returns an iterator equivalent to map(fn, iter).
+    def wait(fs, timeout=None, return_when=ALL_COMPLETED):
+        """Wait for the futures in the given sequence to complete.
 
         Args:
-            fn: A callable that will take as many arguments as there are
-                passed iterables.
+            fs: The sequence of Futures (possibly created by different
+                Executors) to wait upon.
             timeout: The maximum number of seconds to wait. If None, then there
                 is no limit on the wait time.
-            chunksize: The size of the chunks the iterable will be broken into
-                before being passed to a child process. This argument is only
-                used by ProcessPoolExecutor; it is ignored by
-                ThreadPoolExecutor.
+            return_when: Indicates when this function should return. The
+                options are:
 
-        Returns:
-            An iterator equivalent to: map(func, *iterables) but the calls may
-            be evaluated out-of-order.
+                FIRST_COMPLETED - Return when any future finishes or is
+                                cancelled.
+                FIRST_EXCEPTION - Return when any future finishes by raising an
+                                exception. If no future raises an exception
+                                then it is equivalent to ALL_COMPLETED.
+                ALL_COMPLETED -   Return when all futures finish or are
+                                cancelled.
 
-        Raises:
-            TimeoutError: If the entire result iterator could not be generated
-                before the given timeout.
-            Exception: If fn(*args) raises for any values.
+        Returns:
+            A named 2-tuple of sets. The first set, named 'done', contains the
+            futures that completed (is finished or cancelled) before the wait
+            completed. The second set, named 'not_done', contains uncompleted
+            futures.
         """
-        timeout = kwargs.get('timeout')
-        if timeout is not None:
-            end_time = timeout + time.time()
+        with _AcquireFutures(fs):
+            done = set(f for f in fs
+                       if f._state in [CANCELLED_AND_NOTIFIED, FINISHED])
+            not_done = set(fs) - done
 
-        fs = [self.submit(fn, *args) for args in zip(*iterables)]
+            if (return_when == FIRST_COMPLETED) and done:
+                return DoneAndNotDoneFutures(done, not_done)
+            elif (return_when == FIRST_EXCEPTION) and done:
+                if any(f for f in done
+                       if not f.cancelled() and f.exception() is not None):
+                    return DoneAndNotDoneFutures(done, not_done)
 
-        # Yield must be hidden in closure so that the futures are submitted
-        # before the first iterator value is required.
-        def result_iterator():
-            try:
-                for future in fs:
-                    if timeout is None:
-                        yield future.result()
-                    else:
-                        yield future.result(end_time - time.time())
-            finally:
-                for future in fs:
-                    future.cancel()
-        return result_iterator()
+            if len(done) == len(fs):
+                return DoneAndNotDoneFutures(done, not_done)
 
-    def shutdown(self, wait=True):
-        """Clean-up the resources associated with the Executor.
+            waiter = _create_and_install_waiters(fs, return_when)
 
-        It is safe to call this method several times. Otherwise, no other
-        methods can be called after this one.
+        waiter.event.wait(timeout)
+        for f in fs:
+            with f._condition:
+                f._waiters.remove(waiter)
 
-        Args:
-            wait: If True then shutdown will not return until all running
-                futures have finished executing and the resources used by the
-                executor have been reclaimed.
-        """
-        pass
+        done.update(waiter.finished_futures)
+        return DoneAndNotDoneFutures(done, set(fs) - done)
+
+    class _BaseFuture(object):
+        """Represents the result of an asynchronous computation."""
+
+        def __init__(self):
+            """Initializes the future. Should not be called by clients."""
+            self._condition = threading.Condition()
+            self._state = PENDING
+            self._result = None
+            self._exception = None
+            self._waiters = []
+            self._done_callbacks = []
+
+        def __repr__(self):
+            with self._condition:
+                if self._state == FINISHED:
+                    if self._exception:
+                        return '<%s at %#x state=%s raised %s>' % (
+                            self.__class__.__name__,
+                            id(self),
+                            _STATE_TO_DESCRIPTION_MAP[self._state],
+                            self._exception.__class__.__name__)
+                    else:
+                        return '<%s at %#x state=%s returned %s>' % (
+                            self.__class__.__name__,
+                            id(self),
+                            _STATE_TO_DESCRIPTION_MAP[self._state],
+                            self._result.__class__.__name__)
+                return '<%s at %#x state=%s>' % (
+                        self.__class__.__name__,
+                        id(self),
+                        _STATE_TO_DESCRIPTION_MAP[self._state])
+
+        def cancel(self):
+            """Cancel the future if possible.
+
+            Returns True if the future was cancelled, False otherwise. A future
+            cannot be cancelled if it is running or has already completed.
+            """
+            with self._condition:
+                if self._state in [RUNNING, FINISHED]:
+                    return False
+
+                if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
+                    return True
+
+                self._state = CANCELLED
+                self._condition.notify_all()
+
+            self._invoke_callbacks()
+            return True
+
+        def cancelled(self):
+            """Return True if the future was cancelled."""
+            with self._condition:
+                return self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]
+
+        def running(self):
+            """Return True if the future is currently executing."""
+            with self._condition:
+                return self._state == RUNNING
+
+        def done(self):
+            """Return True of the future was cancelled or finished executing.
+            """
+            with self._condition:
+                return self._state in [CANCELLED, CANCELLED_AND_NOTIFIED,
+                                       FINISHED]
+
+        def __get_result(self):
+            if self._exception:
+                raise self._exception
+            else:
+                return self._result
+
+        def add_done_callback(self, fn):
+            """Attaches a callable that will be called when the future finishes.
+
+            Args:
+                fn: A callable that will be called with this future as its only
+                    argument when the future completes or is cancelled. The
+                    callable will always be called by a thread in the same
+                    process in which it was added. If the future has already
+                    completed or been cancelled then the callable will be
+                    called immediately. These callables are called in the order
+                    that they were added.
+            """
+            with self._condition:
+                if self._state not in [CANCELLED, CANCELLED_AND_NOTIFIED,
+                                       FINISHED]:
+                    self._done_callbacks.append(fn)
+                    return
+            fn(self)
+
+        def result(self, timeout=None):
+            """Return the result of the call that the future represents.
+
+            Args:
+                timeout: The number of seconds to wait for the result if the
+                    future isn't done. If None, then there is no limit on the
+                    wait time.
+
+            Returns:
+                The result of the call that the future represents.
+
+            Raises:
+                CancelledError: If the future was cancelled.
+                TimeoutError: If the future didn't finish executing before the
+                    given timeout.
+                Exception: If the call raised then that exception will be
+                raised.
+            """
+            with self._condition:
+                if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
+                    raise CancelledError()
+                elif self._state == FINISHED:
+                    return self.__get_result()
+
+                self._condition.wait(timeout)
+
+                if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
+                    raise CancelledError()
+                elif self._state == FINISHED:
+                    return self.__get_result()
+                else:
+                    raise TimeoutError()
+
+        def exception(self, timeout=None):
+            """Return the exception raised by the call that the future
+            represents.
+
+            Args:
+                timeout: The number of seconds to wait for the exception if the
+                    future isn't done. If None, then there is no limit on the
+                    wait time.
+
+            Returns:
+                The exception raised by the call that the future represents or
+                None if the call completed without raising.
+
+            Raises:
+                CancelledError: If the future was cancelled.
+                TimeoutError: If the future didn't finish executing before the
+                    given timeout.
+            """
+
+            with self._condition:
+                if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
+                    raise CancelledError()
+                elif self._state == FINISHED:
+                    return self._exception
+
+                self._condition.wait(timeout)
+
+                if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
+                    raise CancelledError()
+                elif self._state == FINISHED:
+                    return self._exception
+                else:
+                    raise TimeoutError()
+
+        # The following methods should only be used by Executors and in tests.
+        def set_running_or_notify_cancel(self):
+            """Mark the future as running or process any cancel notifications.
+
+            Should only be used by Executor implementations and unit tests.
+
+            If the future has been cancelled (cancel() was called and returned
+            True) then any threads waiting on the future completing (though
+            calls to as_completed() or wait()) are notified and False is
+            returned.
+
+            If the future was not cancelled then it is put in the running state
+            (future calls to running() will return True) and True is returned.
+
+            This method should be called by Executor implementations before
+            executing the work associated with this future. If this method
+            returns False then the work should not be executed.
+
+            Returns:
+                False if the Future was cancelled, True otherwise.
+
+            Raises:
+                RuntimeError: if this method was already called or if
+                    set_result() or set_exception() was called.
+            """
+            with self._condition:
+                if self._state == CANCELLED:
+                    self._state = CANCELLED_AND_NOTIFIED
+                    for waiter in self._waiters:
+                        waiter.add_cancelled(self)
+                    # self._condition.notify_all() is not necessary because
+                    # self.cancel() triggers a notification.
+                    return False
+                elif self._state == PENDING:
+                    self._state = RUNNING
+                    return True
+                else:
+                    LOGGER.critical('Future %s in unexpected state: %s',
+                                    id(self),
+                                    self._state)
+                    raise RuntimeError('Future in unexpected state')
+
+        def set_result(self, result):
+            """Sets the return value of work associated with the future.
+
+            Should only be used by Executor implementations and unit tests.
+            """
+            with self._condition:
+                self._result = result
+                self._state = FINISHED
+                for waiter in self._waiters:
+                    waiter.add_result(self)
+                self._condition.notify_all()
+            self._invoke_callbacks()
+
+        def set_exception(self, exception):
+            """Sets the result of the future as being the given exception.
+
+            Should only be used by Executor implementations and unit tests.
+            """
+            with self._condition:
+                self._exception = exception
+                self._state = FINISHED
+                for waiter in self._waiters:
+                    waiter.add_exception(self)
+                self._condition.notify_all()
+            self._invoke_callbacks()
 
-    def __enter__(self):
-        return self
+    class Executor(object):
+        """This is an abstract base class for concrete asynchronous executors.
+        """
 
-    def __exit__(self, exc_type, exc_val, exc_tb):
-        self.shutdown(wait=True)
-        return False
+        def submit(self, fn, *args, **kwargs):
+            """Submits a callable to be executed with the given arguments.
+
+            Schedules the callable to be executed as fn(*args, **kwargs) and
+            returns a Future instance representing the execution of the
+            callable.
+
+            Returns:
+                A Future representing the given call.
+            """
+            raise NotImplementedError()
+
+        def map(self, fn, *iterables, **kwargs):
+            """Returns an iterator equivalent to map(fn, iter).
+
+            Args:
+                fn: A callable that will take as many arguments as there are
+                    passed iterables.
+                timeout: The maximum number of seconds to wait. If None, then
+                    there is no limit on the wait time.
+                chunksize: The size of the chunks the iterable will be broken
+                    into before being passed to a child process. This argument
+                    is only used by ProcessPoolExecutor; it is ignored by
+                    ThreadPoolExecutor.
+
+            Returns:
+                An iterator equivalent to: map(func, *iterables) but the calls
+                may be evaluated out-of-order.
+
+            Raises:
+                TimeoutError: If the entire result iterator could not be
+                    generated before the given timeout.
+                Exception: If fn(*args) raises for any values.
+            """
+            timeout = kwargs.get('timeout')
+            if timeout is not None:
+                end_time = timeout + time.time()
+
+            fs = [self.submit(fn, *args) for args in zip(*iterables)]
+
+            # Yield must be hidden in closure so that the futures are submitted
+            # before the first iterator value is required.
+            def result_iterator():
+                try:
+                    for future in fs:
+                        if timeout is None:
+                            yield future.result()
+                        else:
+                            yield future.result(end_time - time.time())
+                finally:
+                    for future in fs:
+                        future.cancel()
+            return result_iterator()
+
+        def shutdown(self, wait=True):
+            """Clean-up the resources associated with the Executor.
+
+            It is safe to call this method several times. Otherwise, no other
+            methods can be called after this one.
+
+            Args:
+                wait: If True then shutdown will not return until all running
+                    futures have finished executing and the resources used by
+                    the executor have been reclaimed.
+            """
+            pass
+
+        def __enter__(self):
+            return self
+
+        def __exit__(self, exc_type, exc_val, exc_tb):
+            self.shutdown(wait=True)
+            return False
+
+
+# To make loky._base.Future instances awaitable  by concurrent.futures.wait,
+# derive our custom Future class from _BaseFuture. _invoke_callback is the only
+# modification made to this class in loky.
+class Future(_BaseFuture):
+    def _invoke_callbacks(self):
+        for callback in self._done_callbacks:
+            try:
+                callback(self)
+            except BaseException:
+                LOGGER.exception('exception calling callback for %r', self)
diff --git a/sklearn/externals/joblib/externals/loky/backend/compat.py b/sklearn/externals/joblib/externals/loky/backend/compat.py
index 6366b23d9f38..729c77c7d9bc 100644
--- a/sklearn/externals/joblib/externals/loky/backend/compat.py
+++ b/sklearn/externals/joblib/externals/loky/backend/compat.py
@@ -9,10 +9,10 @@
 
 if sys.version_info[:2] >= (3, 3):
     import queue
-    from _pickle import PicklingError
 else:
     import Queue as queue
-    from pickle import PicklingError
+
+from pickle import PicklingError
 
 if sys.version_info >= (3, 4):
     from multiprocessing.process import BaseProcess
diff --git a/sklearn/externals/joblib/externals/loky/backend/context.py b/sklearn/externals/joblib/externals/loky/backend/context.py
index 52df5589cb7b..0f744c5918b5 100644
--- a/sklearn/externals/joblib/externals/loky/backend/context.py
+++ b/sklearn/externals/joblib/externals/loky/backend/context.py
@@ -18,25 +18,41 @@
 import multiprocessing as mp
 
 
-from .process import LokyProcess
+from .process import LokyProcess, LokyInitMainProcess
+
+START_METHODS = ['loky', 'loky_init_main']
+_DEFAULT_START_METHOD = None
 
 if sys.version_info[:2] >= (3, 4):
-    from multiprocessing import get_context as get_mp_context
+    from multiprocessing import get_context as mp_get_context
     from multiprocessing.context import assert_spawning, set_spawning_popen
     from multiprocessing.context import get_spawning_popen, BaseContext
 
-    def get_context(method="loky"):
+    START_METHODS += ['spawn']
+    if sys.platform != 'win32':
+        START_METHODS += ['fork', 'forkserver']
+
+    def get_context(method=None):
+        # Try to overload the default context
+        method = method or _DEFAULT_START_METHOD or "loky"
         if method == "fork":
-            warnings.warn("`fork` start method should not be used with `loky` "
-                          "as it does not respect POSIX. Try using `spawn` or "
-                          "`loky` instead.", UserWarning)
-        return get_mp_context(method)
+            # If 'fork' is explicitly requested, warn user about potential
+            # issues.
+            warnings.warn("`fork` start method should not be used with "
+                          "`loky` as it does not respect POSIX. Try using "
+                          "`spawn` or `loky` instead.", UserWarning)
+        try:
+            context = mp_get_context(method)
+        except ValueError:
+            raise ValueError("Unknown context '{}'. Value should be in {}."
+                             .format(method, START_METHODS))
+
+        return context
 
 else:
-    METHODS = ['loky', 'loky_init_main']
     if sys.platform != 'win32':
         import threading
-        # Mecanism to check that the current thread is spawning a child process
+        # Mechanism to check that the current thread is spawning a process
         _tls = threading.local()
         popen_attr = 'spawning_popen'
     else:
@@ -59,14 +75,30 @@ def assert_spawning(obj):
                 ' through inheritance' % type(obj).__name__
             )
 
-    def get_context(method="loky"):
+    def get_context(method=None):
+        method = method or _DEFAULT_START_METHOD or 'loky'
         if method == "loky":
             return LokyContext()
         elif method == "loky_init_main":
             return LokyInitMainContext()
         else:
-            raise ValueError("Method {} is not implemented. The available "
-                             "methods are {}".format(method, METHODS))
+            raise ValueError("Unknown context '{}'. Value should be in {}."
+                             .format(method, START_METHODS))
+
+
+def set_start_method(method, force=False):
+    global _DEFAULT_START_METHOD
+    if _DEFAULT_START_METHOD is not None and not force:
+        raise RuntimeError('context has already been set')
+    assert method is None or method in START_METHODS, (
+        "'{}' is not a valid start_method. It should be in {}"
+        .format(method, START_METHODS))
+
+    _DEFAULT_START_METHOD = method
+
+
+def get_start_method():
+    return _DEFAULT_START_METHOD
 
 
 def cpu_count():
@@ -74,12 +106,13 @@ def cpu_count():
 
     The returned number of CPUs accounts for:
      * the number of CPUs in the system, as given by
-       ``multiprocessing.cpu_count``
+       ``multiprocessing.cpu_count``;
      * the CPU affinity settings of the current process
-       (available with Python 3.4+ on some Unix systems)
-     * CFS scheduler CPU bandwidth limit
-       (available on Linux only)
-    and is given as the minimum of these three constraints.
+       (available with Python 3.4+ on some Unix systems);
+     * CFS scheduler CPU bandwidth limit (available on Linux only, typically
+       set by docker and similar container orchestration systems);
+     * the value of the LOKY_MAX_CPU_COUNT environment variable if defined.
+    and is given as the minimum of these constraints.
     It is also always larger or equal to 1.
     """
     import math
@@ -109,10 +142,15 @@ def cpu_count():
             cfs_period_us = int(fh.read())
 
         if cfs_quota_us > 0 and cfs_period_us > 0:
-            cpu_count_cfs = math.ceil(cfs_quota_us / cfs_period_us)
-            cpu_count_cfs = max(cpu_count_cfs, 1)
+            # Make sure this quantity is an int as math.ceil returns a
+            # float in python2.7. (See issue #165)
+            cpu_count_cfs = int(math.ceil(cfs_quota_us / cfs_period_us))
 
-    return min(cpu_count_mp, cpu_count_affinity, cpu_count_cfs)
+    # User defined soft-limit passed as an loky specific environment variable.
+    cpu_count_loky = int(os.environ.get('LOKY_MAX_CPU_COUNT', cpu_count_mp))
+    aggregate_cpu_count = min(cpu_count_mp, cpu_count_affinity, cpu_count_cfs,
+                              cpu_count_loky)
+    return max(aggregate_cpu_count, 1)
 
 
 class LokyContext(BaseContext):
@@ -138,7 +176,7 @@ def get_context(self):
             return self
 
         def get_start_method(self):
-            return "loky"
+            return self._name
 
         def Pipe(self, duplex=True):
             '''Returns two connection object connected by a pipe'''
@@ -216,12 +254,12 @@ class LokyInitMainContext(LokyContext):
     For more details, see the end of the following section of python doc
     https://docs.python.org/3/library/multiprocessing.html#multiprocessing-programming
     """
-    def Process(self, *args, **kwargs):
-        kwargs.pop('init_main_module', True)
-        return LokyProcess(*args, init_main_module=True, **kwargs)
+    _name = 'loky_init_main'
+    Process = LokyInitMainProcess
 
 
 if sys.version_info > (3, 4):
     """Register loky context so it works with multiprocessing.get_context"""
-    mp.context._concrete_contexts['loky'] = LokyContext()
+    ctx_loky = LokyContext()
+    mp.context._concrete_contexts['loky'] = ctx_loky
     mp.context._concrete_contexts['loky_init_main'] = LokyInitMainContext()
diff --git a/sklearn/externals/joblib/externals/loky/backend/popen_loky_posix.py b/sklearn/externals/joblib/externals/loky/backend/popen_loky_posix.py
index 729c7c71fe68..35a5907d2155 100644
--- a/sklearn/externals/joblib/externals/loky/backend/popen_loky_posix.py
+++ b/sklearn/externals/joblib/externals/loky/backend/popen_loky_posix.py
@@ -127,7 +127,8 @@ def _launch(self, process_obj):
             set_spawning_popen(self)
             try:
                 prep_data = spawn.get_preparation_data(
-                    process_obj._name, process_obj.init_main_module)
+                    process_obj._name,
+                    getattr(process_obj, "init_main_module", True))
                 reduction.dump(prep_data, fp)
                 reduction.dump(process_obj, fp)
 
@@ -150,9 +151,10 @@ def _launch(self, process_obj):
                     cmd_python += ['--semaphore',
                                    str(reduction._mk_inheritable(tracker_fd))]
                 self._fds.extend([child_r, child_w, tracker_fd])
-                util.debug("launch python with cmd:\n%s" % cmd_python)
                 from .fork_exec import fork_exec
                 pid = fork_exec(cmd_python, self._fds)
+                util.debug("launched python with pid {} and cmd:\n{}"
+                           .format(pid, cmd_python))
                 self.sentinel = parent_r
 
                 method = 'getbuffer'
diff --git a/sklearn/externals/joblib/externals/loky/backend/popen_loky_win32.py b/sklearn/externals/joblib/externals/loky/backend/popen_loky_win32.py
index 1f3e909e789a..dccf04bf6534 100644
--- a/sklearn/externals/joblib/externals/loky/backend/popen_loky_win32.py
+++ b/sklearn/externals/joblib/externals/loky/backend/popen_loky_win32.py
@@ -1,10 +1,11 @@
 import os
 import sys
+from pickle import load
+from multiprocessing import process, util
 
-from .context import get_spawning_popen, set_spawning_popen
 from . import spawn
 from . import reduction
-from multiprocessing import util
+from .context import get_spawning_popen, set_spawning_popen
 
 if sys.platform == "win32":
     # Avoid import error by code introspection tools such as test runners
@@ -42,7 +43,7 @@ class Popen(_Popen):
 
     def __init__(self, process_obj):
         prep_data = spawn.get_preparation_data(
-            process_obj._name, process_obj.init_main_module)
+            process_obj._name, getattr(process_obj, "init_main_module", True))
 
         # read end of pipe will be "stolen" by the child process
         # -- see spawn_main() in spawn.py.
@@ -50,8 +51,7 @@ def __init__(self, process_obj):
         if sys.version_info[:2] > (3, 3):
             wfd = msvcrt.open_osfhandle(wfd, 0)
 
-        cmd = spawn.get_command_line(parent_pid=os.getpid(),
-                                     pipe_handle=rhandle)
+        cmd = get_command_line(parent_pid=os.getpid(), pipe_handle=rhandle)
         cmd = ' '.join('"%s"' % x for x in cmd)
 
         try:
@@ -64,7 +64,7 @@ def __init__(self, process_obj):
                         None, None, inherit, 0,
                         None, None, None)
                     _winapi.CloseHandle(ht)
-                except:
+                except BaseException as e:
                     _winapi.CloseHandle(rhandle)
                     raise
 
@@ -97,3 +97,54 @@ def __init__(self, process_obj):
     def duplicate_for_child(self, handle):
         assert self is get_spawning_popen()
         return reduction.duplicate(handle, self.sentinel)
+
+
+if sys.version_info[:2] >= (3, 4):
+    from multiprocessing.spawn import get_command_line
+else:
+    # compatibility for python2.7. Duplicate here the code from
+    # multiprocessing.forking.main to call our prepare function and correctly
+    # set the default start_methods in loky.
+
+    def get_command_line(pipe_handle, **kwds):
+        '''
+        Returns prefix of command line used for spawning a child process
+        '''
+        if getattr(sys, 'frozen', False):
+            return ([sys.executable, '--multiprocessing-fork', pipe_handle])
+        else:
+            prog = 'from sklearn.externals.joblib.externals.loky.backend.popen_loky_win32 import main; main()'
+            opts = util._args_from_interpreter_flags()
+            return [spawn.get_executable()] + opts + [
+                '-c', prog, '--multiprocessing-fork', pipe_handle]
+
+    def is_forking(argv):
+        '''
+        Return whether commandline indicates we are forking
+        '''
+        if len(argv) >= 2 and argv[1] == '--multiprocessing-fork':
+            assert len(argv) == 3
+            return True
+        else:
+            return False
+
+    def main():
+        '''
+        Run code specified by data received over pipe
+        '''
+        assert is_forking(sys.argv)
+
+        handle = int(sys.argv[-1])
+        fd = msvcrt.open_osfhandle(handle, os.O_RDONLY)
+        from_parent = os.fdopen(fd, 'rb')
+
+        process.current_process()._inheriting = True
+        preparation_data = load(from_parent)
+        spawn.prepare(preparation_data)
+        self = load(from_parent)
+        process.current_process()._inheriting = False
+
+        from_parent.close()
+
+        exitcode = self._bootstrap()
+        exit(exitcode)
diff --git a/sklearn/externals/joblib/externals/loky/backend/process.py b/sklearn/externals/joblib/externals/loky/backend/process.py
index 401a46fa4f15..f6a00c90e363 100644
--- a/sklearn/externals/joblib/externals/loky/backend/process.py
+++ b/sklearn/externals/joblib/externals/loky/backend/process.py
@@ -73,6 +73,21 @@ def authkey(self, authkey):
             '''
             self._authkey = AuthenticationKey(authkey)
 
+        def _bootstrap(self):
+            from .context import set_start_method
+            set_start_method(self._start_method)
+            super(LokyProcess, self)._bootstrap()
+
+
+class LokyInitMainProcess(LokyProcess):
+    _start_method = 'loky_init_main'
+
+    def __init__(self, group=None, target=None, name=None, args=(),
+                 kwargs={}, daemon=None):
+        super(LokyInitMainProcess, self).__init__(
+            group=group, target=target, name=name, args=args, kwargs=kwargs,
+            daemon=daemon, init_main_module=True)
+
 
 #
 # We subclass bytes to avoid accidental transmission of auth keys over network
diff --git a/sklearn/externals/joblib/externals/loky/backend/queues.py b/sklearn/externals/joblib/externals/loky/backend/queues.py
index f6401516875f..04f080f3e10e 100644
--- a/sklearn/externals/joblib/externals/loky/backend/queues.py
+++ b/sklearn/externals/joblib/externals/loky/backend/queues.py
@@ -147,16 +147,18 @@ def _feed(buffer, notempty, send_bytes, writelock, close, reducers,
                             return
 
                         # serialize the data before acquiring the lock
-                        obj = CustomizableLokyPickler.dumps(
+                        obj_ = CustomizableLokyPickler.dumps(
                             obj, reducers=reducers)
                         if wacquire is None:
-                            send_bytes(obj)
+                            send_bytes(obj_)
                         else:
                             wacquire()
                             try:
-                                send_bytes(obj)
+                                send_bytes(obj_)
                             finally:
                                 wrelease()
+                        # Remove references early to avoid leaking memory
+                        del obj, obj_
                 except IndexError:
                     pass
             except BaseException as e:
diff --git a/sklearn/externals/joblib/externals/loky/backend/reduction.py b/sklearn/externals/joblib/externals/loky/backend/reduction.py
index 20eb581cbfce..b621a92930c9 100644
--- a/sklearn/externals/joblib/externals/loky/backend/reduction.py
+++ b/sklearn/externals/joblib/externals/loky/backend/reduction.py
@@ -181,12 +181,13 @@ def h(cls):
 register(type(_C.h), _reduce_method)
 
 
-def _reduce_method_descriptor(m):
-    return getattr, (m.__objclass__, m.__name__)
+if not hasattr(sys, "pypy_version_info"):
+    # PyPy uses functions instead of method_descriptors and wrapper_descriptors
+    def _reduce_method_descriptor(m):
+        return getattr, (m.__objclass__, m.__name__)
 
-
-register(type(list.append), _reduce_method_descriptor)
-register(type(int.__add__), _reduce_method_descriptor)
+    register(type(list.append), _reduce_method_descriptor)
+    register(type(int.__add__), _reduce_method_descriptor)
 
 
 # Make partial func pickable
diff --git a/sklearn/externals/joblib/externals/loky/backend/spawn.py b/sklearn/externals/joblib/externals/loky/backend/spawn.py
index a7e57b884c1b..d92d189ddc19 100644
--- a/sklearn/externals/joblib/externals/loky/backend/spawn.py
+++ b/sklearn/externals/joblib/externals/loky/backend/spawn.py
@@ -10,9 +10,10 @@
 import sys
 import runpy
 import types
-import multiprocessing as mp
 from multiprocessing import process, util
 
+from sklearn.externals.joblib.externals.loky.backend import context
+
 
 if sys.platform != 'win32':
     WINEXE = False
@@ -26,21 +27,6 @@
 else:
     _python_exe = sys.executable
 
-if sys.version_info[:2] < (3, 4):
-    def get_command_line(pipe_handle, **kwds):
-        '''
-        Returns prefix of command line used for spawning a child process
-        '''
-        if getattr(sys, 'frozen', False):
-            return ([sys.executable, '--multiprocessing-fork', pipe_handle])
-        else:
-            prog = 'from multiprocessing.forking import main; main()'
-            opts = util._args_from_interpreter_flags()
-            return [_python_exe] + opts + [
-                '-c', prog, '--multiprocessing-fork', pipe_handle]
-else:
-    from multiprocessing.spawn import get_command_line
-
 
 def get_executable():
     return _python_exe
@@ -108,7 +94,7 @@ def get_preparation_data(name, init_main_module=True):
         main_module = sys.modules['__main__']
         try:
             main_mod_name = getattr(main_module.__spec__, "name", None)
-        except:
+        except BaseException:
             main_mod_name = None
         if main_mod_name is not None:
             d['init_main_from_name'] = main_mod_name
@@ -165,9 +151,6 @@ def prepare(data):
     if 'orig_dir' in data:
         process.ORIGINAL_DIR = data['orig_dir']
 
-    if hasattr(mp, 'set_start_method'):
-        mp.set_start_method('loky', force=True)
-
     if 'tacker_pid' in data:
         from . import semaphore_tracker
         semaphore_tracker._semaphore_tracker._pid = data["tracker_pid"]
diff --git a/sklearn/externals/joblib/externals/loky/backend/utils.py b/sklearn/externals/joblib/externals/loky/backend/utils.py
index d54098a816cd..db21c84e020b 100644
--- a/sklearn/externals/joblib/externals/loky/backend/utils.py
+++ b/sklearn/externals/joblib/externals/loky/backend/utils.py
@@ -30,16 +30,14 @@ def _recursive_terminate_with_psutil(process, retries=5):
     except psutil.NoSuchProcess:
         return
 
-    for child in children:
+    # Kill the children in reverse order to avoid killing the parents before
+    # the children in cases where there are more processes nested.
+    for child in children[::-1]:
         try:
-            child.terminate()
+            child.kill()
         except psutil.NoSuchProcess:
             pass
 
-    gone, still_alive = psutil.wait_procs(children, timeout=5)
-    for child_process in still_alive:
-        child_process.kill()
-
     process.terminate()
     process.join()
 
diff --git a/sklearn/externals/joblib/externals/loky/cloudpickle_wrapper.py b/sklearn/externals/joblib/externals/loky/cloudpickle_wrapper.py
new file mode 100644
index 000000000000..6b387e75f14e
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/cloudpickle_wrapper.py
@@ -0,0 +1,53 @@
+import os
+import inspect
+from functools import partial
+
+from .backend import LOKY_PICKLER
+
+try:
+    from cloudpickle import dumps, loads
+    cloudpickle = True
+except ImportError:
+    cloudpickle = False
+
+if not LOKY_PICKLER and cloudpickle:
+    wrap_cache = dict()
+
+    class CloudpickledObjectWrapper(object):
+        def __init__(self, obj):
+            self.pickled_obj = dumps(obj)
+
+        def __reduce__(self):
+            return loads, (self.pickled_obj,)
+
+    def _wrap_non_picklable_objects(obj):
+        need_wrap = "__main__" in getattr(obj, "__module__", "")
+        if isinstance(obj, partial):
+            return partial(
+                _wrap_non_picklable_objects(obj.func),
+                *[_wrap_non_picklable_objects(a) for a in obj.args],
+                **{k: _wrap_non_picklable_objects(v)
+                   for k, v in obj.keywords.items()}
+            )
+        if callable(obj):
+            # Need wrap if the object is a function defined in a local scope of
+            # another function.
+            func_code = getattr(obj, "__code__", "")
+            need_wrap |= getattr(func_code, "co_flags", 0) & inspect.CO_NESTED
+
+            # Need wrap if the obj is a lambda expression
+            func_name = getattr(obj, "__name__", "")
+            need_wrap |= "<lambda>" in func_name
+
+        if not need_wrap:
+            return obj
+
+        wrapped_obj = wrap_cache.get(obj)
+        if wrapped_obj is None:
+            wrapped_obj = CloudpickledObjectWrapper(obj)
+            wrap_cache[obj] = wrapped_obj
+        return wrapped_obj
+
+else:
+    def _wrap_non_picklable_objects(obj):
+        return obj
diff --git a/sklearn/externals/joblib/externals/loky/process_executor.py b/sklearn/externals/joblib/externals/loky/process_executor.py
index 6868f527069d..57a7617d9ab7 100644
--- a/sklearn/externals/joblib/externals/loky/process_executor.py
+++ b/sklearn/externals/joblib/externals/loky/process_executor.py
@@ -60,18 +60,19 @@
 
 
 import os
+import gc
 import sys
 import types
+import struct
 import weakref
 import warnings
 import itertools
 import traceback
 import threading
+from time import time
 import multiprocessing as mp
 from functools import partial
 from pickle import PicklingError
-from time import time
-import gc
 
 from . import _base
 from .backend import get_context
@@ -80,6 +81,7 @@
 from .backend.context import cpu_count
 from .backend.queues import Queue, SimpleQueue, Full
 from .backend.utils import recursive_terminate
+from .cloudpickle_wrapper import _wrap_non_picklable_objects
 
 try:
     from concurrent.futures.process import BrokenProcessPool as _BPPException
@@ -115,14 +117,16 @@
 MAX_DEPTH = int(os.environ.get("LOKY_MAX_DEPTH", 10))
 _CURRENT_DEPTH = 0
 
-# Minimum time interval between two consecutive memory usage checks.
-_MEMORY_CHECK_DELAY = 1.
+# Minimum time interval between two consecutive memory leak protection checks.
+_MEMORY_LEAK_CHECK_DELAY = 1.
 
 # Number of bytes of memory usage allowed over the reference process size.
 _MAX_MEMORY_LEAK_SIZE = int(1e8)
 
+
 try:
     from psutil import Process
+    _USE_PSUTIL = True
 
     def _get_memory_usage(pid, force_gc=False):
         if force_gc:
@@ -131,7 +135,7 @@ def _get_memory_usage(pid, force_gc=False):
         return Process(pid).memory_info().rss
 
 except ImportError:
-    _get_memory_usage = None
+    _USE_PSUTIL = False
 
 
 class _ThreadWakeup:
@@ -263,35 +267,16 @@ def __repr__(self):
         return "CallItem({}, {}, {}, {})".format(
             self.work_id, self.fn, self.args, self.kwargs)
 
-    try:
-        # If cloudpickle is present on the system, use it to pickle the
-        # function. This permits to use interactive terminal for loky calls.
-        # TODO: Add option to deactivate, as it increases pickling time.
-        from .backend import LOKY_PICKLER
-        assert LOKY_PICKLER is None or LOKY_PICKLER == ""
-
-        import cloudpickle  # noqa: F401
-
-        def __getstate__(self):
-            from cloudpickle import dumps
-            if isinstance(self.fn, (types.FunctionType,
-                                    types.LambdaType,
-                                    partial)):
-                cp = True
-                fn = dumps(self.fn)
-            else:
-                cp = False
-                fn = self.fn
-            return (self.work_id, self.args, self.kwargs, fn, cp)
-
-        def __setstate__(self, state):
-            self.work_id, self.args, self.kwargs, self.fn, cp = state
-            if cp:
-                from cloudpickle import loads
-                self.fn = loads(self.fn)
+    def __getstate__(self):
+        return (
+            self.work_id,
+            _wrap_non_picklable_objects(self.fn),
+            [_wrap_non_picklable_objects(a) for a in self.args],
+            {k: _wrap_non_picklable_objects(a) for k, a in self.kwargs.items()}
+        )
 
-    except (ImportError, AssertionError) as e:
-        pass
+    def __setstate__(self, state):
+        self.work_id, self.fn, self.args, self.kwargs = state
 
 
 class _SafeQueue(Queue):
@@ -305,12 +290,17 @@ def __init__(self, max_size=0, ctx=None, pending_work_items=None,
 
     def _on_queue_feeder_error(self, e, obj):
         if isinstance(obj, _CallItem):
-            # fromat traceback only on python3
-            pickling_error = PicklingError(
-                "Could not pickle the task to send it to the workers.")
+            # format traceback only works on python3
+            if isinstance(e, struct.error):
+                raised_error = RuntimeError(
+                    "The task could not be sent to the workers as it is too "
+                    "large for `send_bytes`.")
+            else:
+                raised_error = PicklingError(
+                    "Could not pickle the task to send it to the workers.")
             tb = traceback.format_exception(
                 type(e), e, getattr(e, "__traceback__", None))
-            pickling_error.__cause__ = _RemoteTraceback(
+            raised_error.__cause__ = _RemoteTraceback(
                 '\n"""\n{}"""'.format(''.join(tb)))
             work_item = self.pending_work_items.pop(obj.work_id, None)
             self.running_work_items.remove(obj.work_id)
@@ -318,7 +308,7 @@ def _on_queue_feeder_error(self, e, obj):
             # case, the queue_manager_thread fails all work_items with
             # BrokenProcessPool
             if work_item is not None:
-                work_item.future.set_exception(pickling_error)
+                work_item.future.set_exception(raised_error)
                 del work_item
             self.thread_wakeup.wakeup()
         else:
@@ -394,8 +384,8 @@ def _process_worker(call_queue, result_queue, initializer, initargs,
     # set the global _CURRENT_DEPTH mechanism to limit recursive call
     global _CURRENT_DEPTH
     _CURRENT_DEPTH = current_depth
-    _REFERENCE_PROCESS_SIZE = None
-    _LAST_MEMORY_CHECK = None
+    _process_reference_size = None
+    _last_memory_leak_check = None
     pid = os.getpid()
 
     mp.util.debug('Worker started with timeout=%s' % timeout)
@@ -414,7 +404,13 @@ def _process_worker(call_queue, result_queue, initializer, initargs,
                 mp.util.info("Could not acquire processes_management_lock")
                 continue
         except BaseException as e:
-            traceback.print_exc()
+            previous_tb = traceback.format_exc()
+            try:
+                result_queue.put(_RemoteTraceback(previous_tb))
+            except BaseException:
+                # If we cannot format correctly the exception, at least print
+                # the traceback.
+                print(previous_tb)
             sys.exit(1)
         if call_item is None:
             # Notify queue management thread about clean worker shutdown
@@ -428,21 +424,23 @@ def _process_worker(call_queue, result_queue, initializer, initargs,
             result_queue.put(_ResultItem(call_item.work_id, exception=exc))
         else:
             _sendback_result(result_queue, call_item.work_id, result=r)
+            del r
 
         # Free the resource as soon as possible, to avoid holding onto
         # open files or shared memory that is not needed anymore
         del call_item
 
-        if _get_memory_usage is not None:
-            if _REFERENCE_PROCESS_SIZE is None:
+        if _USE_PSUTIL:
+            if _process_reference_size is None:
                 # Make reference measurement after the first call
-                _REFERENCE_PROCESS_SIZE = _get_memory_usage(pid, force_gc=True)
-                _LAST_MEMORY_CHECK = time()
+                _process_reference_size = _get_memory_usage(pid, force_gc=True)
+                _last_memory_leak_check = time()
                 continue
-            if time() - _LAST_MEMORY_CHECK > _MEMORY_CHECK_DELAY:
+            if time() - _last_memory_leak_check > _MEMORY_LEAK_CHECK_DELAY:
                 mem_usage = _get_memory_usage(pid)
-                _LAST_MEMORY_CHECK = time()
-                if mem_usage - _REFERENCE_PROCESS_SIZE < _MAX_MEMORY_LEAK_SIZE:
+                print(mem_usage)
+                _last_memory_leak_check = time()
+                if mem_usage - _process_reference_size < _MAX_MEMORY_LEAK_SIZE:
                     # Memory usage stays within bounds: everything is fine.
                     continue
 
@@ -450,8 +448,8 @@ def _process_worker(call_queue, result_queue, initializer, initargs,
                 # after a forced garbage collection to break any reference
                 # cycles.
                 mem_usage = _get_memory_usage(pid, force_gc=True)
-                _LAST_MEMORY_CHECK = time()
-                if mem_usage - _REFERENCE_PROCESS_SIZE < _MAX_MEMORY_LEAK_SIZE:
+                _last_memory_leak_check = time()
+                if mem_usage - _process_reference_size < _MAX_MEMORY_LEAK_SIZE:
                     # The GC managed to free the memory: everything is fine.
                     continue
 
@@ -461,6 +459,14 @@ def _process_worker(call_queue, result_queue, initializer, initargs,
                 result_queue.put(pid)
                 with worker_exit_lock:
                     return
+        else:
+            # if psutil is not installed, trigger gc.collect events
+            # regularly to limit potential memory leaks due to reference cycles
+            if ((_last_memory_leak_check is None) or
+                    (time() - _last_memory_leak_check >
+                     _MEMORY_LEAK_CHECK_DELAY)):
+                gc.collect()
+                _last_memory_leak_check = time()
 
 
 def _add_call_item_to_queue(pending_work_items,
@@ -617,6 +623,9 @@ def shutdown_all_workers():
             try:
                 result_item = result_reader.recv()
                 broken = None
+                if isinstance(result_item, _RemoteTraceback):
+                    cause = result_item.tb
+                    broken = ("A task has failed to un-serialize", cause)
             except BaseException as e:
                 tb = getattr(e, "__traceback__", None)
                 if tb is None:
@@ -861,8 +870,8 @@ def __init__(self, max_workers=None, job_reducers=None,
 
         if initializer is not None and not callable(initializer):
             raise TypeError("initializer must be a callable")
-        self._initializer = initializer
-        self._initargs = initargs
+        self._initializer = _wrap_non_picklable_objects(initializer)
+        self._initargs = [_wrap_non_picklable_objects(a) for a in initargs]
 
         _check_max_depth(self._context)
 
diff --git a/sklearn/externals/joblib/func_inspect.py b/sklearn/externals/joblib/func_inspect.py
index e9320b43f182..d4d2670943e5 100644
--- a/sklearn/externals/joblib/func_inspect.py
+++ b/sklearn/externals/joblib/func_inspect.py
@@ -275,7 +275,7 @@ def filter_args(func, ignore_lst, args=(), kwargs=dict()):
         else:
             position = arg_position - len(arg_names)
             if arg_name in kwargs:
-                arg_dict[arg_name] = kwargs.pop(arg_name)
+                arg_dict[arg_name] = kwargs[arg_name]
             else:
                 try:
                     arg_dict[arg_name] = arg_defaults[position]
diff --git a/sklearn/externals/joblib/memory.py b/sklearn/externals/joblib/memory.py
index ae187950bcfa..e31ba2edb72e 100644
--- a/sklearn/externals/joblib/memory.py
+++ b/sklearn/externals/joblib/memory.py
@@ -96,8 +96,11 @@ def register_store_backend(backend_name, backend):
     _STORE_BACKENDS[backend_name] = backend
 
 
-def _store_backend_factory(backend, location, verbose=0, backend_options={}):
+def _store_backend_factory(backend, location, verbose=0, backend_options=None):
     """Return the correct store object for the given location."""
+    if backend_options is None:
+        backend_options = {}
+
     if isinstance(location, StoreBackendBase):
         return location
     elif isinstance(location, _basestring):
@@ -207,8 +210,11 @@ class MemorizedResult(Logger):
     def __init__(self, location, func, args_id, backend='local',
                  mmap_mode=None, verbose=0, timestamp=None, metadata=None):
         Logger.__init__(self)
-        self.func = func
         self.func_id = _build_func_identifier(func)
+        if isinstance(func, _basestring):
+            self.func = func
+        else:
+            self.func = self.func_id
         self.args_id = args_id
         self.store_backend = _store_backend_factory(backend, location,
                                                     verbose=verbose)
@@ -252,14 +258,15 @@ def __repr__(self):
         return ('{class_name}(location="{location}", func="{func}", '
                 'args_id="{args_id}")'
                 .format(class_name=self.__class__.__name__,
-                        location=self.store_backend,
+                        location=self.store_backend.location,
                         func=self.func,
                         args_id=self.args_id
                         ))
-    def __reduce__(self):
-        return (self.__class__,
-                (self.store_backend, self.func, self.args_id),
-                {'mmap_mode': self.mmap_mode})
+
+    def __getstate__(self):
+        state = self.__dict__.copy()
+        state['timestamp'] = None
+        return state
 
 
 class NotMemorizedResult(object):
@@ -324,9 +331,6 @@ def __call__(self, *args, **kwargs):
     def call_and_shelve(self, *args, **kwargs):
         return NotMemorizedResult(self.func(*args, **kwargs))
 
-    def __reduce__(self):
-        return (self.__class__, (self.func,))
-
     def __repr__(self):
         return '{0}(func={1})'.format(self.__class__.__name__, self.func)
 
@@ -386,6 +390,7 @@ def __init__(self, func, location, backend='local', ignore=None,
         self.mmap_mode = mmap_mode
         self.compress = compress
         self.func = func
+
         if ignore is None:
             ignore = []
         self.ignore = ignore
@@ -421,28 +426,44 @@ def __init__(self, func, location, backend='local', ignore=None,
             doc = func.__doc__
         self.__doc__ = 'Memoized version of %s' % doc
 
-    def _cached_call(self, args, kwargs):
+    def _cached_call(self, args, kwargs, shelving=False):
         """Call wrapped function and cache result, or read cache if available.
 
         This function returns the wrapped function output and some metadata.
 
+        Arguments:
+        ----------
+
+        args, kwargs: list and dict
+            input arguments for wrapped function
+
+        shelving: bool
+            True when called via the call_and_shelve function.
+
+
         Returns
         -------
-        output: value or tuple
-            what is returned by wrapped function
+        output: value or tuple or None
+            Output of the wrapped function.
+            If shelving is True and the call has been already cached,
+            output is None.
 
         argument_hash: string
-            hash of function arguments
+            Hash of function arguments.
 
         metadata: dict
-            some metadata about wrapped function call (see _persist_input())
+            Some metadata about wrapped function call (see _persist_input()).
         """
-        # Compare the function code with the previous to see if the
-        # function code has changed
         func_id, args_id = self._get_output_identifiers(*args, **kwargs)
         metadata = None
         msg = None
+
+        # Wether or not the memorized function must be called
+        must_call = False
+
         # FIXME: The statements below should be try/excepted
+        # Compare the function code with the previous to see if the
+        # function code has changed
         if not (self._check_previous_func_code(stacklevel=4) and
                 self.store_backend.contains_item([func_id, args_id])):
             if self._verbose > 10:
@@ -452,16 +473,7 @@ def _cached_call(self, args, kwargs):
                           .format(name, args_id,
                                   self.store_backend.
                                   get_cached_func_info([func_id])['location']))
-            out, metadata = self.call(*args, **kwargs)
-            if self.mmap_mode is not None:
-                # Memmap the output at the first call to be consistent with
-                # later calls
-                if self._verbose:
-                    msg = _format_load_msg(func_id, args_id,
-                                           timestamp=self.timestamp,
-                                           metadata=metadata)
-                out = self.store_backend.load_item([func_id, args_id], msg=msg,
-                                                   verbose=self._verbose)
+            must_call = True
         else:
             try:
                 t0 = time.time()
@@ -469,8 +481,16 @@ def _cached_call(self, args, kwargs):
                     msg = _format_load_msg(func_id, args_id,
                                            timestamp=self.timestamp,
                                            metadata=metadata)
-                out = self.store_backend.load_item([func_id, args_id], msg=msg,
-                                                   verbose=self._verbose)
+
+                if not shelving:
+                    # When shelving, we do not need to load the output
+                    out = self.store_backend.load_item(
+                        [func_id, args_id],
+                        msg=msg,
+                        verbose=self._verbose)
+                else:
+                    out = None
+
                 if self._verbose > 4:
                     t = time.time() - t0
                     _, name = get_func_name(self.func)
@@ -482,8 +502,19 @@ def _cached_call(self, args, kwargs):
                 self.warn('Exception while loading results for '
                           '{}\n {}'.format(signature, traceback.format_exc()))
 
-                out, metadata = self.call(*args, **kwargs)
-                args_id = None
+                must_call = True
+
+        if must_call:
+            out, metadata = self.call(*args, **kwargs)
+            if self.mmap_mode is not None:
+                # Memmap the output at the first call to be consistent with
+                # later calls
+                if self._verbose:
+                    msg = _format_load_msg(func_id, args_id,
+                                           timestamp=self.timestamp,
+                                           metadata=metadata)
+                out = self.store_backend.load_item([func_id, args_id], msg=msg,
+                                                   verbose=self._verbose)
 
         return (out, args_id, metadata)
 
@@ -502,7 +533,7 @@ def call_and_shelve(self, *args, **kwargs):
             class "NotMemorizedResult" is used when there is no cache
             activated (e.g. location=None in Memory).
         """
-        _, args_id, metadata = self._cached_call(args, kwargs)
+        _, args_id, metadata = self._cached_call(args, kwargs, shelving=True)
         return MemorizedResult(self.store_backend, self.func, args_id,
                                metadata=metadata, verbose=self._verbose - 1,
                                timestamp=self.timestamp)
@@ -510,14 +541,13 @@ class "NotMemorizedResult" is used when there is no cache
     def __call__(self, *args, **kwargs):
         return self._cached_call(args, kwargs)[0]
 
-    def __reduce__(self):
+    def __getstate__(self):
         """ We don't store the timestamp when pickling, to avoid the hash
             depending from it.
-            In addition, when unpickling, we run the __init__
         """
-        return (self.__class__, (self.func, None),
-                {k: v for k, v in vars(self).items()
-                 if k not in ('timestamp', 'func')})
+        state = self.__dict__.copy()
+        state['timestamp'] = None
+        return state
 
     # ------------------------------------------------------------------------
     # Private interface
@@ -745,9 +775,10 @@ def _persist_input(self, duration, args, kwargs, this_duration_limit=0.5):
     # ------------------------------------------------------------------------
 
     def __repr__(self):
-        return ("{0}(func={1}, location={2})".format(self.__class__.__name__,
-                                                     self.func,
-                                                     self.store_backend,))
+        return '{class_name}(func={func}, location={location})'.format(
+            class_name=self.__class__.__name__,
+            func=self.func,
+            location=self.store_backend.location,)
 
 
 ###############################################################################
@@ -776,6 +807,12 @@ class Memory(Logger):
             The 'local' backend is using regular filesystem operations to
             manipulate data (open, mv, etc) in the backend.
 
+        cachedir: str or None, optional
+
+            .. deprecated: 0.12
+                'cachedir' has been deprecated in 0.12 and will be
+                removed in 0.14. Use the 'location' parameter instead.
+
         mmap_mode: {None, 'r+', 'r', 'w+', 'c'}, optional
             The memmapping mode used when loading from cache
             numpy arrays. See numpy.load for the meaning of the
@@ -797,20 +834,14 @@ class Memory(Logger):
         backend_options: dict, optional
             Contains a dictionnary of named parameters used to configure
             the store backend.
-
-        cachedir: str or None, optional
-
-            .. deprecated: 0.12
-                'cachedir' has been deprecated in 0.12 and will be
-                removed in 0.14. Use the 'location' parameter instead.
     """
     # ------------------------------------------------------------------------
     # Public interface
     # ------------------------------------------------------------------------
 
-    def __init__(self, location=None, backend='local', mmap_mode=None,
-                 compress=False, verbose=1, bytes_limit=None,
-                 backend_options={}, cachedir=None):
+    def __init__(self, location=None, backend='local', cachedir=None,
+                 mmap_mode=None, compress=False, verbose=1, bytes_limit=None,
+                 backend_options=None):
         # XXX: Bad explanation of the None value of cachedir
         Logger.__init__(self)
         self._verbose = verbose
@@ -818,6 +849,11 @@ def __init__(self, location=None, backend='local', mmap_mode=None,
         self.timestamp = time.time()
         self.bytes_limit = bytes_limit
         self.backend = backend
+        self.compress = compress
+        if backend_options is None:
+            backend_options = {}
+        self.backend_options = backend_options
+
         if compress and mmap_mode is not None:
             warnings.warn('Compressed results cannot be memmapped',
                           stacklevel=2)
@@ -897,9 +933,11 @@ def cache(self, func=None, ignore=None, verbose=None, mmap_mode=False):
             mmap_mode = self.mmap_mode
         if isinstance(func, MemorizedFunc):
             func = func.func
-        return MemorizedFunc(func, self.store_backend, mmap_mode=mmap_mode,
-                             ignore=ignore, verbose=verbose,
-                             timestamp=self.timestamp)
+        return MemorizedFunc(func, location=self.store_backend,
+                             backend=self.backend,
+                             ignore=ignore, mmap_mode=mmap_mode,
+                             compress=self.compress,
+                             verbose=verbose, timestamp=self.timestamp)
 
     def clear(self, warn=True):
         """ Erase the complete cache directory.
@@ -932,14 +970,15 @@ def eval(self, func, *args, **kwargs):
     # ------------------------------------------------------------------------
 
     def __repr__(self):
-        return '{0}(location={1})'.format(
-            self.__class__.__name__, (repr(None) if self.store_backend is None
-                                      else repr(self.store_backend)))
+        return '{class_name}(location={location})'.format(
+            class_name=self.__class__.__name__,
+            location=(None if self.store_backend is None
+                      else self.store_backend.location))
 
-    def __reduce__(self):
+    def __getstate__(self):
         """ We don't store the timestamp when pickling, to avoid the hash
             depending from it.
-            In addition, when unpickling, we run the __init__
         """
-        return (self.__class__, (), {k: v for k, v in vars(self).items()
-                                     if k != 'timestamp'})
+        state = self.__dict__.copy()
+        state['timestamp'] = None
+        return state
diff --git a/sklearn/externals/joblib/numpy_pickle.py b/sklearn/externals/joblib/numpy_pickle.py
index 496429f50f16..bae0df31fa9c 100644
--- a/sklearn/externals/joblib/numpy_pickle.py
+++ b/sklearn/externals/joblib/numpy_pickle.py
@@ -435,7 +435,9 @@ def dump(value, filename, compress=0, protocol=None, cache_size=None):
     else:
         compress_level = compress
 
-    if compress_method == 'lz4' and lz4 is None:
+    # LZ4 compression is only supported and installation checked with
+    # python 3+.
+    if compress_method == 'lz4' and lz4 is None and PY3_OR_LATER:
         raise ValueError(LZ4_NOT_INSTALLED_ERROR)
 
     if (compress_level is not None and
diff --git a/sklearn/externals/joblib/parallel.py b/sklearn/externals/joblib/parallel.py
index 8be0ed2c6ba7..6cca94cde690 100644
--- a/sklearn/externals/joblib/parallel.py
+++ b/sklearn/externals/joblib/parallel.py
@@ -196,11 +196,11 @@ def unregister(self):
 # to set an environment variable to switch the default start method from
 # 'fork' to 'forkserver' or 'spawn' to avoid this issue albeit at the cost
 # of causing semantic changes and some additional pool instantiation overhead.
+DEFAULT_MP_CONTEXT = None
 if hasattr(mp, 'get_context'):
     method = os.environ.get('JOBLIB_START_METHOD', '').strip() or None
-    DEFAULT_MP_CONTEXT = mp.get_context(method=method)
-else:
-    DEFAULT_MP_CONTEXT = None
+    if method is not None:
+        DEFAULT_MP_CONTEXT = mp.get_context(method=method)
 
 
 class CloudpickledObjectWrapper(object):
@@ -679,6 +679,8 @@ def __init__(self, n_jobs=None, backend=None, verbose=0, timeout=None,
         )
         if DEFAULT_MP_CONTEXT is not None:
             self._backend_args['context'] = DEFAULT_MP_CONTEXT
+        elif hasattr(mp, "get_context"):
+            self._backend_args['context'] = mp.get_context()
 
         if backend is None:
             backend = active_backend
diff --git a/sklearn/linear_model/cd_fast.pyx b/sklearn/linear_model/cd_fast.pyx
index cd044824b4b7..c75ad0f667d4 100644
--- a/sklearn/linear_model/cd_fast.pyx
+++ b/sklearn/linear_model/cd_fast.pyx
@@ -251,11 +251,9 @@ def enet_coordinate_descent(floating[::1] w,
 
                 # update the maximum absolute coefficient update
                 d_w_ii = fabs(w[ii] - w_ii)
-                if d_w_ii > d_w_max:
-                    d_w_max = d_w_ii
+                d_w_max = fmax(d_w_max, d_w_ii)
 
-                if fabs(w[ii]) > w_max:
-                    w_max = fabs(w[ii])
+                w_max = fmax(w_max, fabs(w[ii]))
 
             if (w_max == 0.0 or
                 d_w_max / w_max < d_w_tol or
diff --git a/sklearn/metrics/classification.py b/sklearn/metrics/classification.py
index c8130783c240..60f47980d6a1 100644
--- a/sklearn/metrics/classification.py
+++ b/sklearn/metrics/classification.py
@@ -139,9 +139,9 @@ def accuracy_score(y_true, y_pred, normalize=True, sample_weight=None):
     Returns
     -------
     score : float
-        If ``normalize == True``, return the correctly classified samples
-        (float), else it returns the number of correctly classified samples
-        (int).
+        If ``normalize == True``, return the fraction of correctly
+        classified samples (float), else returns the number of correctly
+        classified samples (int).
 
         The best performance is 1 with ``normalize == True`` and the number
         of samples with ``normalize == False``.
@@ -1403,6 +1403,13 @@ def balanced_accuracy_score(y_true, y_pred, sample_weight=None,
     --------
     recall_score, roc_auc_score
 
+    Notes
+    -----
+    Some literature promotes alternative definitions of balanced accuracy. Our
+    definition is equivalent to :func:`accuracy_score` with class-balanced
+    sample weights, and shares desirable properties with the binary case.
+    See the :ref:`User Guide <balanced_accuracy_score>`.
+
     References
     ----------
     .. [1] Brodersen, K.H.; Ong, C.S.; Stephan, K.E.; Buhmann, J.M. (2010).
diff --git a/sklearn/model_selection/tests/test_search.py b/sklearn/model_selection/tests/test_search.py
index 0409794bf08e..969b6288a71e 100644
--- a/sklearn/model_selection/tests/test_search.py
+++ b/sklearn/model_selection/tests/test_search.py
@@ -133,13 +133,13 @@ def assert_grid_iter_equals_getitem(grid):
 
 @pytest.mark.parametrize(
     "input, error_type, error_message",
-    [(0, TypeError, 'Parameter grid is not a dict or a list (0)'),
-     ([{'foo': [0]}, 0], TypeError, 'Parameter grid is not a dict (0)'),
+    [(0, TypeError, 'Parameter grid is not a dict or a list \(0\)'),
+     ([{'foo': [0]}, 0], TypeError, 'Parameter grid is not a dict \(0\)'),
      ({'foo': 0}, TypeError, "Parameter grid value is not iterable "
-      "(key='foo', value=0)")]
+      "\(key='foo', value=0\)")]
 )
 def test_validate_parameter_grid_input(input, error_type, error_message):
-    with pytest.raises(error_type, message=error_message):
+    with pytest.raises(error_type, match=error_message):
         ParameterGrid(input)
 
 
diff --git a/sklearn/neighbors/lof.py b/sklearn/neighbors/lof.py
index 68fe777b3c48..df7b57c54bdd 100644
--- a/sklearn/neighbors/lof.py
+++ b/sklearn/neighbors/lof.py
@@ -171,6 +171,9 @@ def fit_predict(self):
             The query sample or samples to compute the Local Outlier Factor
             w.r.t. to the training samples.
 
+        y : Ignored
+            not used, present for API consistency by convention.
+
         Returns
         -------
         is_inlier : array, shape (n_samples,)
@@ -219,6 +222,9 @@ def fit(self, X, y=None):
             Training data. If array or matrix, shape [n_samples, n_features],
             or [n_samples, n_samples] if metric='precomputed'.
 
+        y : Ignored
+            not used, present for API consistency by convention.
+
         Returns
         -------
         self : object
diff --git a/sklearn/svm/classes.py b/sklearn/svm/classes.py
index 4bb423e79048..a2d96c322b33 100644
--- a/sklearn/svm/classes.py
+++ b/sklearn/svm/classes.py
@@ -1122,6 +1122,9 @@ def fit(self, X, y=None, sample_weight=None, **params):
             Per-sample weights. Rescale C per sample. Higher weights
             force the classifier to put more emphasis on these points.
 
+        y : Ignored
+            not used, present for API consistency by convention.
+
         Returns
         -------
         self : object
diff --git a/sklearn/svm/tests/test_sparse.py b/sklearn/svm/tests/test_sparse.py
index 2628cd741ab9..ce14bda1db34 100644
--- a/sklearn/svm/tests/test_sparse.py
+++ b/sklearn/svm/tests/test_sparse.py
@@ -1,8 +1,9 @@
 import pytest
+
 import numpy as np
-from scipy import sparse
 from numpy.testing import (assert_array_almost_equal, assert_array_equal,
                            assert_equal)
+from scipy import sparse
 
 from sklearn import datasets, svm, linear_model, base
 from sklearn.datasets import make_classification, load_digits, make_blobs
@@ -12,7 +13,6 @@
 from sklearn.utils.testing import (assert_raises, assert_true, assert_false,
                                    assert_warns, assert_raise_message,
                                    ignore_warnings, skip_if_32bit)
-import pytest
 
 
 # test sample 1
diff --git a/sklearn/utils/sparsefuncs_fast.pyx b/sklearn/utils/sparsefuncs_fast.pyx
index d49c0683ae86..b40b843e9432 100644
--- a/sklearn/utils/sparsefuncs_fast.pyx
+++ b/sklearn/utils/sparsefuncs_fast.pyx
@@ -27,7 +27,7 @@ ctypedef np.float64_t DOUBLE
 
 def csr_row_norms(X):
     """L2 norm of each row in CSR matrix X."""
-    if X.dtype != np.float32:
+    if X.dtype not in [np.float32, np.float64]:
         X = X.astype(np.float64)
     return _csr_row_norms(X.data, X.shape, X.indices, X.indptr)
 
@@ -72,7 +72,7 @@ def csr_mean_variance_axis0(X):
         Feature-wise variances
 
     """
-    if X.dtype != np.float32:
+    if X.dtype not in [np.float32, np.float64]:
         X = X.astype(np.float64)
     means, variances, _ =  _csr_mean_variance_axis0(X.data, X.shape[0],
                                                     X.shape[1], X.indices)
@@ -152,7 +152,7 @@ def csc_mean_variance_axis0(X):
         Feature-wise variances
 
     """
-    if X.dtype != np.float32:
+    if X.dtype not in [np.float32, np.float64]:
         X = X.astype(np.float64)
     means, variances, _ = _csc_mean_variance_axis0(X.data, X.shape[0],
                                                    X.shape[1], X.indices,
@@ -260,7 +260,7 @@ def incr_mean_variance_axis0(X, last_mean, last_var, last_n):
     `utils.extmath._batch_mean_variance_update`.
 
     """
-    if X.dtype != np.float32:
+    if X.dtype not in [np.float32, np.float64]:
         X = X.astype(np.float64)
     return _incr_mean_variance_axis0(X.data, X.shape[0], X.shape[1], X.indices,
                                      X.indptr, X.format, last_mean, last_var,
diff --git a/sklearn/utils/tests/test_validation.py b/sklearn/utils/tests/test_validation.py
index 3e577ebaa8ee..5b32d9e2115d 100644
--- a/sklearn/utils/tests/test_validation.py
+++ b/sklearn/utils/tests/test_validation.py
@@ -172,7 +172,7 @@ def test_check_array_force_all_finite_valid(value, force_all_finite, retype):
      (np.inf, 'allow-nan', 'Input contains infinity'),
      (np.nan, True, 'Input contains NaN, infinity'),
      (np.nan, 'allow-inf', 'force_all_finite should be a bool or "allow-nan"'),
-     (np.nan, 1, 'force_all_finite should be a bool or "allow-nan"')]
+     (np.nan, 1, 'Input contains NaN, infinity')]
 )
 @pytest.mark.parametrize(
     "retype",
@@ -182,7 +182,7 @@ def test_check_array_force_all_finiteinvalid(value, force_all_finite,
                                              match_msg, retype):
     X = retype(np.arange(4).reshape(2, 2).astype(np.float))
     X[0, 0] = value
-    with pytest.raises(ValueError, message=match_msg):
+    with pytest.raises(ValueError, match=match_msg):
         check_array(X, force_all_finite=force_all_finite,
                     accept_sparse=True)
 
