diff --git a/.gitignore b/.gitignore
index 9fa8c09bdf0b..55050f0a31ed 100644
--- a/.gitignore
+++ b/.gitignore
@@ -54,6 +54,7 @@ benchmarks/bench_covertype_data/
 *.prefs
 .pydevproject
 .idea
+.vscode
 
 *.c
 *.cpp
diff --git a/.travis.yml b/.travis.yml
index d2612e8d67ad..d931eef67eba 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -3,11 +3,11 @@ sudo: false
 
 language: python
 
-cache:
-  apt: true
-  directories:
-  - $HOME/.cache/pip
-  - $HOME/.ccache
+# cache:
+#   apt: true
+#   directories:
+#   - $HOME/.cache/pip
+#   - $HOME/.ccache
 
 dist: trusty
 
diff --git a/README.rst b/README.rst
index cf011277f539..4df228acd4c4 100644
--- a/README.rst
+++ b/README.rst
@@ -78,6 +78,12 @@ or ``conda``::
 The documentation includes more detailed `installation instructions <http://scikit-learn.org/stable/install.html>`_.
 
 
+Changelog
+---------
+
+See the `changelog <http://scikit-learn.org/dev/whats_new.html>`__
+for a history of notable changes to scikit-learn.
+
 Development
 -----------
 
diff --git a/benchmarks/bench_plot_incremental_pca.py b/benchmarks/bench_plot_incremental_pca.py
index 495d58f0f43e..8579abcae3be 100644
--- a/benchmarks/bench_plot_incremental_pca.py
+++ b/benchmarks/bench_plot_incremental_pca.py
@@ -13,7 +13,7 @@
 from collections import defaultdict
 import matplotlib.pyplot as plt
 from sklearn.datasets import fetch_lfw_people
-from sklearn.decomposition import IncrementalPCA, RandomizedPCA, PCA
+from sklearn.decomposition import IncrementalPCA, PCA
 
 
 def plot_results(X, y, label):
@@ -37,7 +37,6 @@ def plot_feature_times(all_times, batch_size, all_components, data):
     plot_results(all_components, all_times['pca'], label="PCA")
     plot_results(all_components, all_times['ipca'],
                  label="IncrementalPCA, bsize=%i" % batch_size)
-    plot_results(all_components, all_times['rpca'], label="RandomizedPCA")
     plt.legend(loc="upper left")
     plt.suptitle("Algorithm runtime vs. n_components\n \
                  LFW, size %i x %i" % data.shape)
@@ -50,7 +49,6 @@ def plot_feature_errors(all_errors, batch_size, all_components, data):
     plot_results(all_components, all_errors['pca'], label="PCA")
     plot_results(all_components, all_errors['ipca'],
                  label="IncrementalPCA, bsize=%i" % batch_size)
-    plot_results(all_components, all_errors['rpca'], label="RandomizedPCA")
     plt.legend(loc="lower left")
     plt.suptitle("Algorithm error vs. n_components\n"
                  "LFW, size %i x %i" % data.shape)
@@ -61,7 +59,6 @@ def plot_feature_errors(all_errors, batch_size, all_components, data):
 def plot_batch_times(all_times, n_features, all_batch_sizes, data):
     plt.figure()
     plot_results(all_batch_sizes, all_times['pca'], label="PCA")
-    plot_results(all_batch_sizes, all_times['rpca'], label="RandomizedPCA")
     plot_results(all_batch_sizes, all_times['ipca'], label="IncrementalPCA")
     plt.legend(loc="lower left")
     plt.suptitle("Algorithm runtime vs. batch_size for n_components %i\n \
@@ -92,11 +89,9 @@ def fixed_batch_size_comparison(data):
     all_errors = defaultdict(list)
     for n_components in all_features:
         pca = PCA(n_components=n_components)
-        rpca = RandomizedPCA(n_components=n_components, random_state=1999)
         ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size)
         results_dict = {k: benchmark(est, data) for k, est in [('pca', pca),
-                                                               ('ipca', ipca),
-                                                               ('rpca', rpca)]}
+                                                               ('ipca', ipca)]}
 
         for k in sorted(results_dict.keys()):
             all_times[k].append(results_dict[k]['time'])
@@ -116,7 +111,8 @@ def variable_batch_size_comparison(data):
         all_times = defaultdict(list)
         all_errors = defaultdict(list)
         pca = PCA(n_components=n_components)
-        rpca = RandomizedPCA(n_components=n_components, random_state=1999)
+        rpca = PCA(n_components=n_components, svd_solver='randomized',
+                   random_state=1999)
         results_dict = {k: benchmark(est, data) for k, est in [('pca', pca),
                                                                ('rpca', rpca)]}
 
@@ -138,8 +134,6 @@ def variable_batch_size_comparison(data):
             all_errors['ipca'].append(results_dict['ipca']['error'])
 
         plot_batch_times(all_times, n_components, batch_sizes, data)
-        # RandomizedPCA error is always worse (approx 100x) than other PCA
-        # tests
         plot_batch_errors(all_errors, n_components, batch_sizes, data)
 
 faces = fetch_lfw_people(resize=.2, min_faces_per_person=5)
diff --git a/build_tools/circle/list_versions.py b/build_tools/circle/list_versions.py
index 9bf993c20653..f6696177e06d 100755
--- a/build_tools/circle/list_versions.py
+++ b/build_tools/circle/list_versions.py
@@ -47,6 +47,8 @@ def get_pdf_size(version):
             return human_readable_data_quantity(path_details['size'], 1000)
 
 
+print(':orphan:')
+print()
 heading = 'Available documentation for Scikit-learn'
 print(heading)
 print('=' * len(heading))
diff --git a/doc/conf.py b/doc/conf.py
index a420659af53b..fac3b9fc043f 100644
--- a/doc/conf.py
+++ b/doc/conf.py
@@ -70,9 +70,6 @@
 # The encoding of source files.
 #source_encoding = 'utf-8'
 
-# Generate the plots for the gallery
-plot_gallery = True
-
 # The master toctree document.
 master_doc = 'index'
 
@@ -102,7 +99,7 @@
 
 # List of patterns, relative to source directory, that match files and
 # directories to ignore when looking for source files.
-exclude_patterns = ['_build', 'templates', 'includes']
+exclude_patterns = ['_build', 'templates', 'includes', 'themes']
 
 # The reST default role (used for this markup: `text`) to use for all
 # documents.
diff --git a/doc/conftest.py b/doc/conftest.py
index 09e2bc9f9aa8..463df3f38221 100644
--- a/doc/conftest.py
+++ b/doc/conftest.py
@@ -6,6 +6,8 @@
 from sklearn.utils.testing import SkipTest
 from sklearn.utils.testing import check_skip_network
 from sklearn.datasets import get_data_home
+from sklearn.datasets.base import _pkl_filepath
+from sklearn.datasets.twenty_newsgroups import CACHE_NAME
 from sklearn.utils.testing import install_mldata_mock
 from sklearn.utils.testing import uninstall_mldata_mock
 
@@ -47,26 +49,50 @@ def setup_rcv1():
 
 def setup_twenty_newsgroups():
     data_home = get_data_home()
-    if not exists(join(data_home, '20news_home')):
+    cache_path = _pkl_filepath(get_data_home(), CACHE_NAME)
+    if not exists(cache_path):
         raise SkipTest("Skipping dataset loading doctests")
 
 
 def setup_working_with_text_data():
     check_skip_network()
+    cache_path = _pkl_filepath(get_data_home(), CACHE_NAME)
+    if not exists(cache_path):
+        raise SkipTest("Skipping dataset loading doctests")
+
+
+def setup_compose():
+    try:
+        import pandas  # noqa
+    except ImportError:
+        raise SkipTest("Skipping compose.rst, pandas not installed")
+
+
+def setup_impute():
+    try:
+        import pandas  # noqa
+    except ImportError:
+        raise SkipTest("Skipping impute.rst, pandas not installed")
 
 
 def pytest_runtest_setup(item):
     fname = item.fspath.strpath
-    if fname.endswith('datasets/labeled_faces.rst'):
+    is_index = fname.endswith('datasets/index.rst')
+    if fname.endswith('datasets/labeled_faces.rst') or is_index:
         setup_labeled_faces()
-    elif fname.endswith('datasets/mldata.rst'):
+    elif fname.endswith('datasets/mldata.rst') or is_index:
         setup_mldata()
-    elif fname.endswith('datasets/rcv1.rst'):
+    elif fname.endswith('datasets/rcv1.rst') or is_index:
         setup_rcv1()
-    elif fname.endswith('datasets/twenty_newsgroups.rst'):
+    elif fname.endswith('datasets/twenty_newsgroups.rst') or is_index:
         setup_twenty_newsgroups()
-    elif fname.endswith('tutorial/text_analytics/working_with_text_data.rst'):
+    elif fname.endswith('tutorial/text_analytics/working_with_text_data.rst')\
+            or is_index:
         setup_working_with_text_data()
+    elif fname.endswith('modules/compose.rst') or is_index:
+        setup_compose()
+    elif fname.endswith('modules/impute.rst'):
+        setup_impute()
 
 
 def pytest_runtest_teardown(item):
diff --git a/doc/datasets/covtype.rst b/doc/datasets/covtype.rst
index c0ed4ea08af3..4b31eff69cf0 100644
--- a/doc/datasets/covtype.rst
+++ b/doc/datasets/covtype.rst
@@ -1,15 +1,14 @@
-
 .. _covtype:
 
 Forest covertypes
-=================
+-----------------
 
 The samples in this dataset correspond to 30Ã—30m patches of forest in the US,
 collected for the task of predicting each patch's cover type,
 i.e. the dominant species of tree.
 There are seven covertypes, making this a multiclass classification problem.
 Each sample has 54 features, described on the
-`dataset's homepage <http://archive.ics.uci.edu/ml/datasets/Covertype>`_.
+`dataset's homepage <http://archive.ics.uci.edu/ml/datasets/Covertype>`__.
 Some of the features are boolean indicators,
 while others are discrete or continuous measurements.
 
diff --git a/doc/datasets/index.rst b/doc/datasets/index.rst
index 1d27bdfd7f62..73ab66feceaa 100644
--- a/doc/datasets/index.rst
+++ b/doc/datasets/index.rst
@@ -43,12 +43,15 @@ The datasets also contain a description in ``DESCR`` and some contain
 ``feature_names`` and ``target_names``.
 See the dataset descriptions below for details.
 
+.. _toy_datasets:
 
 Toy datasets
 ============
 
 scikit-learn comes with a few small standard datasets that do not
-require to download any file from some external website.
+require to download any file from some external website. 
+
+*desc*
 
 .. autosummary::
 
@@ -67,46 +70,81 @@ These datasets are useful to quickly illustrate the behavior of the
 various algorithms implemented in scikit-learn. They are however often too
 small to be representative of real world machine learning tasks.
 
-.. _sample_images:
+.. toctree::
+    :maxdepth: 2
+    :hidden:
 
-Sample images
-=============
+    boston_house_prices
+    iris
+    diabetes
+    digits
+    linnerud
+    wine_data
+    breast_cancer
 
-Scikit-learn also embed a couple of sample JPEG images published under Creative
-Commons license by their authors. Those image can be useful to test algorithms
-and pipeline on 2D data.
+.. include:: ../../sklearn/datasets/descr/boston_house_prices.rst
+
+.. include:: ../../sklearn/datasets/descr/iris.rst
+
+.. include:: ../../sklearn/datasets/descr/diabetes.rst
+
+.. include:: ../../sklearn/datasets/descr/digits.rst
+
+.. include:: ../../sklearn/datasets/descr/linnerud.rst
+
+.. include:: ../../sklearn/datasets/descr/wine_data.rst
+
+.. include:: ../../sklearn/datasets/descr/breast_cancer.rst
+
+.. _real_world_datasets:
+
+Real world datasets
+===================
+
+*Add desc*
 
 .. autosummary::
 
    :toctree: ../modules/generated/
    :template: function.rst
 
-   load_sample_images
-   load_sample_image
+   fetch_olivetti_faces
+   fetch_20newsgroups
+   fetch_20newsgroups_vectorized
+   fetch_lfw_people
+   fetch_lfw_pairs
+   fetch_covtype
+   fetch_rcv1
+   fetch_kddcup99
 
-.. image:: ../auto_examples/cluster/images/sphx_glr_plot_color_quantization_001.png
-   :target: ../auto_examples/cluster/plot_color_quantization.html
-   :scale: 30
-   :align: right
 
+.. toctree::
+    :maxdepth: 2
+    :hidden:
 
-.. warning::
+    olivetti_faces
+    twenty_newsgroups
+    labeled_faces
+    covtype
+    rcv1
+    kddcup99
 
-  The default coding of images is based on the ``uint8`` dtype to
-  spare memory.  Often machine learning algorithms work best if the
-  input is converted to a floating point representation first.  Also,
-  if you plan to use ``matplotlib.pyplpt.imshow`` don't forget to scale to the range
-  0 - 1 as done in the following example.
+.. include:: ./olivetti_faces.rst
 
-.. topic:: Examples:
+.. include:: ./twenty_newsgroups.rst
 
-    * :ref:`sphx_glr_auto_examples_cluster_plot_color_quantization.py`
+.. include:: ./labeled_faces.rst
+
+.. include:: ./covtype.rst
+
+.. include:: ./rcv1.rst
 
+.. include:: ./kddcup99.rst
 
-.. _sample_generators:
+.. _generated_datasets:
 
-Sample generators
-=================
+Generated datasets
+==================
 
 In addition, scikit-learn includes various random sample generators that
 can be used to build artificial datasets of controlled size and complexity.
@@ -219,10 +257,50 @@ Generators for decomposition
    make_sparse_spd_matrix
 
 
+.. _loading_other_datasets:
+
+Loading other datasets
+======================
+
+.. _sample_images:
+
+Sample images
+-------------
+
+Scikit-learn also embed a couple of sample JPEG images published under Creative
+Commons license by their authors. Those image can be useful to test algorithms
+and pipeline on 2D data.
+
+.. autosummary::
+
+   :toctree: ../modules/generated/
+   :template: function.rst
+
+   load_sample_images
+   load_sample_image
+
+.. image:: ../auto_examples/cluster/images/sphx_glr_plot_color_quantization_001.png
+   :target: ../auto_examples/cluster/plot_color_quantization.html
+   :scale: 30
+   :align: right
+
+
+.. warning::
+
+  The default coding of images is based on the ``uint8`` dtype to
+  spare memory.  Often machine learning algorithms work best if the
+  input is converted to a floating point representation first.  Also,
+  if you plan to use ``matplotlib.pyplpt.imshow`` don't forget to scale to the range
+  0 - 1 as done in the following example.
+
+.. topic:: Examples:
+
+    * :ref:`sphx_glr_auto_examples_cluster_plot_color_quantization.py`
+
 .. _libsvm_loader:
 
 Datasets in svmlight / libsvm format
-====================================
+------------------------------------
 
 scikit-learn includes utility functions for loading
 datasets in the svmlight / libsvm format. In this format, each line
@@ -256,10 +334,93 @@ features::
 
  _`Faster API-compatible implementation`: https://github.com/mblondel/svmlight-loader
 
+..
+    For doctests:
+
+    >>> import numpy as np
+    >>> import os
+    >>> import tempfile
+    >>> # Create a temporary folder for the data fetcher
+    >>> custom_data_home = tempfile.mkdtemp()
+    >>> os.makedirs(os.path.join(custom_data_home, 'mldata'))
+
+
+.. _mldata:
+
+Downloading datasets from the mldata.org repository
+---------------------------------------------------
+
+`mldata.org <http://mldata.org>`_ is a public repository for machine learning
+data, supported by the `PASCAL network <http://www.pascal-network.org>`_ .
+
+The ``sklearn.datasets`` package is able to directly download data
+sets from the repository using the function
+:func:`sklearn.datasets.fetch_mldata`.
+
+For example, to download the MNIST digit recognition database::
+
+  >>> from sklearn.datasets import fetch_mldata
+  >>> mnist = fetch_mldata('MNIST original', data_home=custom_data_home)
+
+The MNIST database contains a total of 70000 examples of handwritten digits
+of size 28x28 pixels, labeled from 0 to 9::
+
+  >>> mnist.data.shape
+  (70000, 784)
+  >>> mnist.target.shape
+  (70000,)
+  >>> np.unique(mnist.target)
+  array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])
+
+After the first download, the dataset is cached locally in the path
+specified by the ``data_home`` keyword argument, which defaults to
+``~/scikit_learn_data/``::
+
+  >>> os.listdir(os.path.join(custom_data_home, 'mldata'))
+  ['mnist-original.mat']
+
+Data sets in `mldata.org <http://mldata.org>`_ do not adhere to a strict
+naming or formatting convention. :func:`sklearn.datasets.fetch_mldata` is
+able to make sense of the most common cases, but allows to tailor the
+defaults to individual datasets:
+
+* The data arrays in `mldata.org <http://mldata.org>`_ are most often
+  shaped as ``(n_features, n_samples)``. This is the opposite of the
+  ``scikit-learn`` convention, so :func:`sklearn.datasets.fetch_mldata`
+  transposes the matrix by default. The ``transpose_data`` keyword controls
+  this behavior::
+
+    >>> iris = fetch_mldata('iris', data_home=custom_data_home)
+    >>> iris.data.shape
+    (150, 4)
+    >>> iris = fetch_mldata('iris', transpose_data=False,
+    ...                     data_home=custom_data_home)
+    >>> iris.data.shape
+    (4, 150)
+
+* For datasets with multiple columns, :func:`sklearn.datasets.fetch_mldata`
+  tries to identify the target and data columns and rename them to ``target``
+  and ``data``. This is done by looking for arrays named ``label`` and
+  ``data`` in the dataset, and failing that by choosing the first array to be
+  ``target`` and the second to be ``data``. This behavior can be changed with
+  the ``target_name`` and ``data_name`` keywords, setting them to a specific
+  name or index number (the name and order of the columns in the datasets
+  can be found at its `mldata.org <http://mldata.org>`_ under the tab "Data"::
+
+    >>> iris2 = fetch_mldata('datasets-UCI iris', target_name=1, data_name=0,
+    ...                      data_home=custom_data_home)
+    >>> iris3 = fetch_mldata('datasets-UCI iris', target_name='class',
+    ...                      data_name='double0', data_home=custom_data_home)
+
+
+..
+    >>> import shutil
+    >>> shutil.rmtree(custom_data_home)
+
 .. _external_datasets:
 
 Loading from external datasets
-==============================
+------------------------------
 
 scikit-learn works on any numeric data stored as numpy arrays or scipy sparse
 matrices. Other types that are convertible to numeric arrays such as pandas
@@ -295,65 +456,11 @@ refer to:
   for reading WAV files into a numpy array
 
 Categorical (or nominal) features stored as strings (common in pandas DataFrames) 
-will need converting to integers, and integer categorical variables may be best 
-exploited when encoded as one-hot variables 
-(:class:`sklearn.preprocessing.OneHotEncoder`) or similar. 
+will need converting to numerical features using :class:`sklearn.preprocessing.OneHotEncoder`
+or :class:`sklearn.preprocessing.OrdinalEncoder` or similar.
 See :ref:`preprocessing`.
 
 Note: if you manage your own numerical data it is recommended to use an 
 optimized file format such as HDF5 to reduce data load times. Various libraries
 such as H5Py, PyTables and pandas provides a Python interface for reading and 
 writing data in that format.
-
-.. make sure everything is in a toc tree
-
-.. toctree::
-    :maxdepth: 2
-    :hidden:
-
-    olivetti_faces
-    twenty_newsgroups
-    mldata
-    labeled_faces
-    covtype
-    rcv1
-    kddcup99
-
-
-.. include:: olivetti_faces.rst
-
-.. include:: twenty_newsgroups.rst
-
-.. include:: mldata.rst
-
-.. include:: labeled_faces.rst
-
-.. include:: covtype.rst
-
-.. include:: rcv1.rst
-
-.. include:: kddcup99.rst
-
-.. _boston_house_prices:
-
-.. include:: ../../sklearn/datasets/descr/boston_house_prices.rst
-
-.. _breast_cancer:
-
-.. include:: ../../sklearn/datasets/descr/breast_cancer.rst
-
-.. _diabetes:
-
-.. include:: ../../sklearn/datasets/descr/diabetes.rst
-
-.. _digits:
-
-.. include:: ../../sklearn/datasets/descr/digits.rst
-
-.. _iris:
-
-.. include:: ../../sklearn/datasets/descr/iris.rst
-
-.. _linnerud:
-
-.. include:: ../../sklearn/datasets/descr/linnerud.rst
diff --git a/doc/datasets/kddcup99.rst b/doc/datasets/kddcup99.rst
index 407b2d8e2c0b..e770a9a2d60e 100644
--- a/doc/datasets/kddcup99.rst
+++ b/doc/datasets/kddcup99.rst
@@ -1,13 +1,12 @@
-
 .. _kddcup99:
 
 Kddcup 99 dataset
-=================
+-----------------
 
 The KDD Cup '99 dataset was created by processing the tcpdump portions
 of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset,
 created by MIT Lincoln Lab. The artificial data (described on the `dataset's
-homepage <http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html>`_) was
+homepage <http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html>`__) was
 generated using a closed network and hand-injected attacks to produce a
 large number of different types of attack with normal activity in the
 background. As the initial goal was to produce a large training set for
diff --git a/doc/datasets/labeled_faces.rst b/doc/datasets/labeled_faces.rst
index 0e70aca8aa70..a7b592ae1a94 100644
--- a/doc/datasets/labeled_faces.rst
+++ b/doc/datasets/labeled_faces.rst
@@ -1,7 +1,7 @@
 .. _labeled_faces_in_the_wild:
 
 The Labeled Faces in the Wild face recognition dataset
-======================================================
+------------------------------------------------------
 
 This dataset is a collection of JPEG pictures of famous people collected
 over the internet, all details are available on the official website:
@@ -25,7 +25,7 @@ face detector from various online websites.
 
 
 Usage
------
+~~~~~
 
 ``scikit-learn`` provides two loaders that will automatically download,
 cache, parse the metadata files, decode the jpeg and convert the
@@ -113,6 +113,6 @@ an evaluation ``10_folds`` set meant to compute performance metrics using a
 
 
 Examples
---------
+~~~~~~~~
 
 :ref:`sphx_glr_auto_examples_applications_plot_face_recognition.py`
diff --git a/doc/datasets/mldata.rst b/doc/datasets/mldata.rst
deleted file mode 100644
index b098abbdcef9..000000000000
--- a/doc/datasets/mldata.rst
+++ /dev/null
@@ -1,82 +0,0 @@
-..
-    For doctests:
-
-    >>> import numpy as np
-    >>> import os
-    >>> import tempfile
-    >>> # Create a temporary folder for the data fetcher
-    >>> custom_data_home = tempfile.mkdtemp()
-    >>> os.makedirs(os.path.join(custom_data_home, 'mldata'))
-
-
-.. _mldata:
-
-Downloading datasets from the mldata.org repository
-===================================================
-
-`mldata.org <http://mldata.org>`_ is a public repository for machine learning
-data, supported by the `PASCAL network <http://www.pascal-network.org>`_ .
-
-The ``sklearn.datasets`` package is able to directly download data
-sets from the repository using the function
-:func:`sklearn.datasets.fetch_mldata`.
-
-For example, to download the MNIST digit recognition database::
-
-  >>> from sklearn.datasets import fetch_mldata
-  >>> mnist = fetch_mldata('MNIST original', data_home=custom_data_home)
-
-The MNIST database contains a total of 70000 examples of handwritten digits
-of size 28x28 pixels, labeled from 0 to 9::
-
-  >>> mnist.data.shape
-  (70000, 784)
-  >>> mnist.target.shape
-  (70000,)
-  >>> np.unique(mnist.target)
-  array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])
-
-After the first download, the dataset is cached locally in the path
-specified by the ``data_home`` keyword argument, which defaults to
-``~/scikit_learn_data/``::
-
-  >>> os.listdir(os.path.join(custom_data_home, 'mldata'))
-  ['mnist-original.mat']
-
-Data sets in `mldata.org <http://mldata.org>`_ do not adhere to a strict
-naming or formatting convention. :func:`sklearn.datasets.fetch_mldata` is
-able to make sense of the most common cases, but allows to tailor the
-defaults to individual datasets:
-
-* The data arrays in `mldata.org <http://mldata.org>`_ are most often
-  shaped as ``(n_features, n_samples)``. This is the opposite of the
-  ``scikit-learn`` convention, so :func:`sklearn.datasets.fetch_mldata`
-  transposes the matrix by default. The ``transpose_data`` keyword controls
-  this behavior::
-
-    >>> iris = fetch_mldata('iris', data_home=custom_data_home)
-    >>> iris.data.shape
-    (150, 4)
-    >>> iris = fetch_mldata('iris', transpose_data=False,
-    ...                     data_home=custom_data_home)
-    >>> iris.data.shape
-    (4, 150)
-
-* For datasets with multiple columns, :func:`sklearn.datasets.fetch_mldata`
-  tries to identify the target and data columns and rename them to ``target``
-  and ``data``. This is done by looking for arrays named ``label`` and
-  ``data`` in the dataset, and failing that by choosing the first array to be
-  ``target`` and the second to be ``data``. This behavior can be changed with
-  the ``target_name`` and ``data_name`` keywords, setting them to a specific
-  name or index number (the name and order of the columns in the datasets
-  can be found at its `mldata.org <http://mldata.org>`_ under the tab "Data"::
-
-    >>> iris2 = fetch_mldata('datasets-UCI iris', target_name=1, data_name=0,
-    ...                      data_home=custom_data_home)
-    >>> iris3 = fetch_mldata('datasets-UCI iris', target_name='class',
-    ...                      data_name='double0', data_home=custom_data_home)
-
-
-..
-    >>> import shutil
-    >>> shutil.rmtree(custom_data_home)
diff --git a/doc/datasets/olivetti_faces.rst b/doc/datasets/olivetti_faces.rst
index 19c5601f7cac..71be4f66a2fc 100644
--- a/doc/datasets/olivetti_faces.rst
+++ b/doc/datasets/olivetti_faces.rst
@@ -2,7 +2,7 @@
 .. _olivetti_faces:
 
 The Olivetti faces dataset
-==========================
+--------------------------
 
 
 `This dataset contains a set of face images`_ taken between April 1992 and April
diff --git a/doc/datasets/rcv1.rst b/doc/datasets/rcv1.rst
index bcc0c95ef8f3..afbe797cc0c0 100644
--- a/doc/datasets/rcv1.rst
+++ b/doc/datasets/rcv1.rst
@@ -2,7 +2,7 @@
 .. _rcv1:
 
 RCV1 dataset
-============
+------------
 
 Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories made available by Reuters, Ltd. for research purposes. The dataset is extensively described in [1]_.
 
diff --git a/doc/datasets/twenty_newsgroups.rst b/doc/datasets/twenty_newsgroups.rst
index a068b18efb8a..5aaca66c5d67 100644
--- a/doc/datasets/twenty_newsgroups.rst
+++ b/doc/datasets/twenty_newsgroups.rst
@@ -1,7 +1,7 @@
 .. _20newsgroups:
 
 The 20 newsgroups text dataset
-==============================
+------------------------------
 
 The 20 newsgroups dataset comprises around 18000 newsgroups posts on
 20 topics split in two subsets: one for training (or development)
@@ -19,7 +19,7 @@ returns ready-to-use features, i.e., it is not necessary to use a feature
 extractor.
 
 Usage
------
+~~~~~
 
 The :func:`sklearn.datasets.fetch_20newsgroups` function is a data
 fetching / caching functions that downloads the data archive from
@@ -62,7 +62,7 @@ attribute is the integer index of the category::
   >>> newsgroups_train.target.shape
   (11314,)
   >>> newsgroups_train.target[:10]
-  array([12,  6,  9,  8,  6,  7,  9,  2, 13, 19])
+  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])
 
 It is possible to load only a sub-selection of the categories by passing the
 list of the categories to load to the
@@ -78,10 +78,10 @@ list of the categories to load to the
   >>> newsgroups_train.target.shape
   (1073,)
   >>> newsgroups_train.target[:10]
-  array([1, 1, 1, 0, 1, 0, 0, 1, 1, 1])
+  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])
 
 Converting text to vectors
---------------------------
+~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 In order to feed predictive or clustering models with the text data,
 one first need to turn the text into vectors of numerical values suitable
@@ -105,7 +105,7 @@ components by sample in a more than 30000-dimensional space
 (less than .5% non-zero features)::
 
   >>> vectors.nnz / float(vectors.shape[0])
-  159.01327433628319
+  159.01327...
 
 :func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which returns
 ready-to-use tfidf features instead of file names.
@@ -115,7 +115,8 @@ ready-to-use tfidf features instead of file names.
 
 
 Filtering text for more realistic training
-------------------------------------------
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
 It is easy for a classifier to overfit on particular things that appear in the
 20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very
 high F-scores, but their results would not generalize to other documents that
@@ -131,11 +132,13 @@ which is fast to train and achieves a decent F-score::
   >>> vectors_test = vectorizer.transform(newsgroups_test.data)
   >>> clf = MultinomialNB(alpha=.01)
   >>> clf.fit(vectors, newsgroups_train.target)
+  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)
+
   >>> pred = clf.predict(vectors_test)
   >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')
-  0.88213592402729568
+  0.88213...
 
-(The example :ref:`sphx_glr_auto_examples_text_document_classification_20newsgroups.py` shuffles
+(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles
 the training and test data, instead of segmenting by time, and in that case
 multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious
 yet of what's going on inside this classifier?)
@@ -150,10 +153,10 @@ Let's take a look at what the most informative features are:
   ...         print("%s: %s" % (category, " ".join(feature_names[top10])))
   ...
   >>> show_top10(clf, vectorizer, newsgroups_train.target_names)
-  alt.atheism: sgi livesey atheists writes people caltech com god keith edu
-  comp.graphics: organization thanks files subject com image lines university edu graphics
-  sci.space: toronto moon gov com alaska access henry nasa edu space
-  talk.religion.misc: article writes kent people christian jesus sandvik edu com god
+  alt.atheism: edu it and in you that is of to the
+  comp.graphics: edu in graphics it is for and of to the
+  sci.space: edu it that is in and space to of the
+  talk.religion.misc: not it you in is that and to of the
 
 You can now see many things that these features have overfit to:
 
@@ -183,7 +186,7 @@ blocks, and quotation blocks respectively.
   >>> vectors_test = vectorizer.transform(newsgroups_test.data)
   >>> pred = clf.predict(vectors_test)
   >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')
-  0.77310350681274775
+  0.77310...
 
 This classifier lost over a lot of its F-score, just because we removed
 metadata that has little to do with topic classification.
@@ -195,10 +198,12 @@ It loses even more if we also strip this metadata from the training data:
   >>> vectors = vectorizer.fit_transform(newsgroups_train.data)
   >>> clf = MultinomialNB(alpha=.01)
   >>> clf.fit(vectors, newsgroups_train.target)
+  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)
+
   >>> vectors_test = vectorizer.transform(newsgroups_test.data)
   >>> pred = clf.predict(vectors_test)
   >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')
-  0.76995175184521725
+  0.76995...
 
 Some other classifiers cope better with this harder version of the task. Try
 running :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without
@@ -215,4 +220,4 @@ the ``--filter`` option to compare the results.
 
    * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`
 
-   * :ref:`sphx_glr_auto_examples_text_document_classification_20newsgroups.py`
+   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`
diff --git a/doc/developers/advanced_installation.rst b/doc/developers/advanced_installation.rst
index 4129ac1cc7c8..2ae1065893f9 100644
--- a/doc/developers/advanced_installation.rst
+++ b/doc/developers/advanced_installation.rst
@@ -140,7 +140,7 @@ From source package
 ~~~~~~~~~~~~~~~~~~~
 
 download the source package from
-`pypi <https://pypi.python.org/pypi/scikit-learn>`_, unpack the sources and
+`pypi <https://pypi.python.org/pypi/scikit-learn>`__, unpack the sources and
 cd into the source directory.
 
 This packages uses distutils, which is the default way of installing
diff --git a/doc/developers/contributing.rst b/doc/developers/contributing.rst
index 0d6174f3b2d4..3293fd0282f4 100644
--- a/doc/developers/contributing.rst
+++ b/doc/developers/contributing.rst
@@ -53,11 +53,36 @@ project maintainers.
 Another way to contribute is to report issues you're facing, and give a "thumbs
 up" on issues that others reported and that are relevant to you.  It also helps
 us if you spread the word: reference the project from your blog and articles,
-link to it from your website, or simply say "I use it":
+link to it from your website, or simply star to say "I use it":
 
 .. raw:: html
 
-   <script type="text/javascript" src="http://www.ohloh.net/p/480792/widgets/project_users.js?style=rainbow"></script>
+   <a class="github-button" href="https://github.com/scikit-learn/scikit-learn"
+   data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star
+   scikit-learn/scikit-learn on GitHub">Star</a>
+   <script async defer src="https://buttons.github.io/buttons.js"></script>
+
+.. topic:: Contributing to related projects
+
+   Scikit-learn thrives in an ecosystem of several related projects, which also
+   may have relevant issues to work on, including smaller projects such as:
+
+   * `scikit-learn-contrib <https://github.com/search?q=org%3Ascikit-learn-contrib+is%3Aissue+is%3Aopen+sort%3Aupdated-desc&type=Issues>`__
+   * `joblib <https://github.com/joblib/joblib/issues>`__
+   * `sphinx-gallery <https://github.com/sphinx-gallery/sphinx-gallery/issues>`__
+   * `numpydoc <https://github.com/numpy/numpydoc/issues>`__
+
+   and larger projects:
+
+   * `numpy <https://github.com/numpy/numpy/issues>`__
+   * `scipy <https://github.com/scipy/scipy/issues>`__
+   * `matplotlib <https://github.com/matplotlib/matplotlib/issues>`__
+   * and so on.
+
+   Look for issues marked "help wanted" or similar.
+   Helping these projects may help Scikit-learn too.
+   See also :ref:`related_projects`.
+
 
 Submitting a bug report or a feature request
 ============================================
@@ -88,7 +113,7 @@ How to make a good bug report
 -----------------------------
 
 When you submit an issue to `Github
-<https://github.com/scikit-learn/scikit-learn/issues>`_, please do your best to
+<https://github.com/scikit-learn/scikit-learn/issues>`__, please do your best to
 follow these guidelines! This will make it a lot easier to provide you with good
 feedback:
 
@@ -416,7 +441,7 @@ underestimate how easy an issue is to solve!
     we use the help wanted tag to mark Pull Requests which have been abandoned
     by their original contributor and are available for someone to pick up where the original
     contributor left off. The list of issues with the help wanted tag can be found
-    `here <https://github.com/scikit-learn/scikit-learn/labels/help%20wanted>`_ .
+    `here <https://github.com/scikit-learn/scikit-learn/labels/help%20wanted>`__ .
 
     Note that not all issues which need contributors will have this tag.
 
@@ -434,10 +459,9 @@ HTML output by building the documentation website.
 Building the documentation
 ^^^^^^^^^^^^^^^^^^^^^^^^^^
 
-Building the documentation requires the ``sphinx``, ``sphinx-gallery``,
-``numpydoc``, ``matplotlib``, and ``Pillow`` packages::
+Building the documentation requires installing some additional packages::
 
-    pip install sphinx sphinx-gallery numpydoc matplotlib Pillow
+    pip install sphinx sphinx-gallery numpydoc matplotlib Pillow pandas scikit-image
 
 To build the documentation, you need to be in the ``doc`` folder::
 
@@ -454,11 +478,12 @@ To generate the full web site, including the example gallery::
 
 Generating the example gallery will run all our examples which takes a
 while. To save some time, you can use:
-    - ``make html-noplot``: this will generate the documentation without the
-      example gallery. This is useful when changing a docstring for example.
-    - ``EXAMPLES_PATTERN=your_regex_goes_here make html``: only the examples
-      matching ``your_regex_goes_here`` will be run. This is particularly
-      useful if you are modifying a few examples.
+
+- ``make html-noplot``: this will generate the documentation without the
+  example gallery. This is useful when changing a docstring for example.
+- ``EXAMPLES_PATTERN=your_regex_goes_here make html``: only the examples
+  matching ``your_regex_goes_here`` will be run. This is particularly
+  useful if you are modifying a few examples.
 
 That should create all the documentation in the ``_build/html/stable``
 directory.  Set the environment variable `NO_MATHJAX=1` if you intend to view
@@ -879,7 +904,7 @@ from high-level questions to a more detailed check-list.
   the tests validate that the code is correct, i.e. doing what the
   documentation says it does? If the change is a bug-fix, is a
   non-regression test included? Look at `this
-  <https://jeffknupp.com/blog/2013/12/09/improve-your-python-understanding-unit-testing>`_
+  <https://jeffknupp.com/blog/2013/12/09/improve-your-python-understanding-unit-testing>`__
   to get started with testing in Python.
 
 - Do the tests pass in the continuous integration build? If
@@ -1153,7 +1178,7 @@ the correct interface more easily.
     and optionally the mixin classes in ``sklearn.base``.
     For example, below is a custom classifier, with more examples included
     in the scikit-learn-contrib
-    `project template <https://github.com/scikit-learn-contrib/project-template/blob/master/skltemplate/template.py>`_.
+    `project template <https://github.com/scikit-learn-contrib/project-template/blob/master/skltemplate/template.py>`__.
 
       >>> import numpy as np
       >>> from sklearn.base import BaseEstimator, ClassifierMixin
diff --git a/doc/developers/performance.rst b/doc/developers/performance.rst
index d3d6204ec328..89ee4af1325f 100644
--- a/doc/developers/performance.rst
+++ b/doc/developers/performance.rst
@@ -388,8 +388,7 @@ Checkout the official joblib documentation:
 
 .. _warm-restarts:
 
-A sample algorithmic trick: warm restarts for cross validation
+A sample algorithmic trick: warm restarts
 ==============================================================
 
-TODO: demonstrate the warm restart tricks for cross validation of linear
-regression with Coordinate Descent.
+See the glossary entry for `warm_start <http://scikit-learn.org/dev/glossary.html#term-warm-start>`_
diff --git a/doc/developers/tips.rst b/doc/developers/tips.rst
index 7e97bcfb2a2c..2334bd769730 100644
--- a/doc/developers/tips.rst
+++ b/doc/developers/tips.rst
@@ -27,7 +27,7 @@ We use CircleCI to build the HTML documentation for every pull request. To
 access that documentation, instructions are provided in the :ref:`documentation
 section of the contributor guide <contribute_documentation>`. To save you a few
 clicks, we provide a `userscript
-<https://raw.githubusercontent.com/lesteve/userscripts/master/add-button-for-pr-circleci-doc.user.js>`_
+<https://raw.githubusercontent.com/lesteve/userscripts/master/add-button-for-pr-circleci-doc.user.js>`__
 that adds a button to every PR. After installing the userscript, navigate to
 any GitHub PR; a new button labeled "See CircleCI doc for this PR" should
 appear in the top-right area.
@@ -37,7 +37,7 @@ Folding and unfolding outdated diffs on pull requests
 
 GitHub hides discussions on PRs when the corresponding lines of code have been
 changed in the mean while. This `userscript
-<https://raw.githubusercontent.com/lesteve/userscripts/master/github-expand-all.user.js>`_
+<https://raw.githubusercontent.com/lesteve/userscripts/master/github-expand-all.user.js>`__
 provides a shortcut (Control-Alt-P at the time of writing but look at the code
 to be sure) to unfold all such hidden discussions at once, so you can catch up.
 
diff --git a/doc/documentation.rst b/doc/documentation.rst
index d8b70104271f..8ec2e41989a7 100644
--- a/doc/documentation.rst
+++ b/doc/documentation.rst
@@ -1,3 +1,5 @@
+:orphan:
+
 .. raw:: html
 
   <div class="container-index">
diff --git a/doc/faq.rst b/doc/faq.rst
index ed58a8719dbc..8d5a6f4f4dde 100644
--- a/doc/faq.rst
+++ b/doc/faq.rst
@@ -355,15 +355,15 @@ instances everywhere and ensure that both estimators and cross-validation
 splitters have their ``random_state`` parameter set.
 
 Why do categorical variables need preprocessing in scikit-learn, compared to other tools?
---------------------------------------------------------------------------------
+-----------------------------------------------------------------------------------------
 
 Most of scikit-learn assumes data is in NumPy arrays or SciPy sparse matrices
 of a single numeric dtype. These do not explicitly represent categorical
 variables at present. Thus, unlike R's data.frames or pandas.DataFrame, we
 require explicit conversion of categorical features to numeric values, as
 discussed in :ref:`preprocessing_categorical_features`.
-See also :ref:`sphx_glr_auto_examples_hetero_feature_union.py` for an example of
-working with heterogeneous (e.g. categorical and numeric) data.
+See also :ref:`sphx_glr_auto_examples_compose_column_transformer_mixed_types.py` for an
+example of working with heterogeneous (e.g. categorical and numeric) data.
 
 Why does Scikit-learn not directly work with, for example, pandas.DataFrame?
 
diff --git a/doc/glossary.rst b/doc/glossary.rst
index a8f31c1b3fdf..f2f17671339c 100644
--- a/doc/glossary.rst
+++ b/doc/glossary.rst
@@ -165,8 +165,10 @@ General Concepts
         tree-based models such as random forests and gradient boosting
         models that often work better and faster with integer-coded
         categorical variables.
-        :class:`~sklearn.preprocessing.CategoricalEncoder` helps
-        encoding string-valued categorical features.
+        :class:`~sklearn.preprocessing.OrdinalEncoder` helps encoding
+        string-valued categorical features as ordinal integers, and
+        :class:`~sklearn.preprocessing.OneHotEncoder` can be used to
+        one-hot encode categorical features.
         See also :ref:`preprocessing_categorical_features` and the
         `http://contrib.scikit-learn.org/categorical-encoding
         <category_encoders>`_ package for tools related to encoding
@@ -433,6 +435,13 @@ General Concepts
     hyper-parameter
         See :term:`parameter`.
 
+    impute
+    imputation
+        Most machine learning algorithms require that their inputs have no
+        :term:`missing values`, and will not work if this requirement is
+        violated. Algorithms that attempt to fill in (or impute) missing values
+        are referred to as imputation algorithms.
+
     indexable
         An :term:`array-like`, :term:`sparse matrix`, pandas DataFrame or
         sequence (usually a list).
@@ -452,6 +461,7 @@ General Concepts
 
     label indicator matrix
     multilabel indicator matrix
+    multilabel indicator matrices
         The format used to represent multilabel data, where each row of a 2d
         array or sparse matrix corresponds to a sample, each column
         corresponds to a class, and each element is 1 if the sample is labeled
@@ -483,7 +493,7 @@ General Concepts
         do (e.g. in :class:`impute.SimpleImputer`), NaN is the preferred
         representation of missing values in float arrays.  If the array has
         integer dtype, NaN cannot be represented. For this reason, we support
-        specifying another ``missing_values`` value when imputation or
+        specifying another ``missing_values`` value when :term:`imputation` or
         learning can be performed in integer space.  :term:`Unlabeled data`
         is a special case of missing values in the :term:`target`.
 
@@ -939,8 +949,9 @@ such as:
 
     scorer
         A non-estimator callable object which evaluates an estimator on given
-        test data, returning a number. See :ref:`scoring_parameter`; see also
-        :term:`evaluation metric`.
+        test data, returning a number. Unlike :term:`evaluation metrics`,
+        a greater returned number must correspond with a *better* score.
+        See :ref:`scoring_parameter`.
 
 Further examples:
 
@@ -1067,6 +1078,13 @@ Target Types
         :func:`~utils.multiclass.type_of_target` will return
         'multilabel-indicator' for multilabel input, whether sparse or dense.
 
+    multioutput
+    multi-output
+        A target where each sample has multiple classification/regression
+        labels. See :term:`multiclass multioutput` and :term:`continuous
+        multioutput`. We do not currently support modelling mixed
+        classification and regression targets.
+
 .. _glossary_methods:
 
 Methods
diff --git a/doc/modules/classes.rst b/doc/modules/classes.rst
index f9dc4e4dfced..df46579f7fd8 100644
--- a/doc/modules/classes.rst
+++ b/doc/modules/classes.rst
@@ -158,8 +158,15 @@ details.
     :toctree: generated
     :template: class.rst
 
+    compose.ColumnTransformer
     compose.TransformedTargetRegressor
 
+.. autosummary::
+   :toctree: generated/
+   :template: function.rst
+
+   compose.make_column_transformer
+
 .. _covariance_ref:
 
 :mod:`sklearn.covariance`: Covariance Estimators
@@ -646,7 +653,7 @@ Kernels:
    :template: class.rst
 
    impute.SimpleImputer
-   impute.MICEImputer
+   impute.ChainedImputer
 
 .. _kernel_approximation_ref:
 
@@ -749,6 +756,7 @@ Kernels:
    linear_model.logistic_regression_path
    linear_model.orthogonal_mp
    linear_model.orthogonal_mp_gram
+   linear_model.ridge_regression
 
 
 .. _manifold_ref:
@@ -1240,7 +1248,7 @@ Model validation
    preprocessing.MinMaxScaler
    preprocessing.Normalizer
    preprocessing.OneHotEncoder
-   preprocessing.CategoricalEncoder
+   preprocessing.OrdinalEncoder
    preprocessing.PolynomialFeatures
    preprocessing.PowerTransformer
    preprocessing.QuantileTransformer
@@ -1463,6 +1471,7 @@ Low-level methods
    utils.testing.assert_raise_message
    utils.testing.all_estimators
 
+
 Recently deprecated
 ===================
 
@@ -1502,46 +1511,3 @@ To be removed in 0.21
 
    datasets.load_mlcomp
    linear_model.lasso_stability_path
-
-
-To be removed in 0.20
----------------------
-
-.. autosummary::
-   :toctree: generated/
-   :template: deprecated_class.rst
-
-   cross_validation.KFold
-   cross_validation.LabelKFold
-   cross_validation.LeaveOneLabelOut
-   cross_validation.LeaveOneOut
-   cross_validation.LeavePOut
-   cross_validation.LeavePLabelOut
-   cross_validation.LabelShuffleSplit
-   cross_validation.ShuffleSplit
-   cross_validation.StratifiedKFold
-   cross_validation.StratifiedShuffleSplit
-   cross_validation.PredefinedSplit
-   decomposition.RandomizedPCA
-   gaussian_process.GaussianProcess
-   grid_search.ParameterGrid
-   grid_search.ParameterSampler
-   grid_search.GridSearchCV
-   grid_search.RandomizedSearchCV
-   mixture.DPGMM
-   mixture.GMM
-   mixture.VBGMM
-
-
-.. autosummary::
-   :toctree: generated/
-   :template: deprecated_function.rst
-
-   cross_validation.check_cv
-   cross_validation.cross_val_predict
-   cross_validation.cross_val_score
-   cross_validation.permutation_test_score
-   cross_validation.train_test_split
-   grid_search.fit_grid_point
-   learning_curve.learning_curve
-   learning_curve.validation_curve
diff --git a/doc/modules/clustering.rst b/doc/modules/clustering.rst
index ce335cef2dd5..21c342d3ff1a 100644
--- a/doc/modules/clustering.rst
+++ b/doc/modules/clustering.rst
@@ -271,7 +271,7 @@ small, as shown in the example and cited reference.
  * :ref:`sphx_glr_auto_examples_cluster_plot_mini_batch_kmeans.py`: Comparison of KMeans and
    MiniBatchKMeans
 
- * :ref:`sphx_glr_auto_examples_text_document_clustering.py`: Document clustering using sparse
+ * :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`: Document clustering using sparse
    MiniBatchKMeans
 
  * :ref:`sphx_glr_auto_examples_cluster_plot_dict_face_patches.py`
diff --git a/doc/modules/compose.rst b/doc/modules/compose.rst
index e51fc074345c..629cbf2663ba 100644
--- a/doc/modules/compose.rst
+++ b/doc/modules/compose.rst
@@ -119,10 +119,10 @@ ignored by setting them to ``None``::
 
  * :ref:`sphx_glr_auto_examples_feature_selection_plot_feature_selection_pipeline.py`
  * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`
- * :ref:`sphx_glr_auto_examples_plot_digits_pipe.py`
+ * :ref:`sphx_glr_auto_examples_compose_plot_digits_pipe.py`
  * :ref:`sphx_glr_auto_examples_plot_kernel_approximation.py`
  * :ref:`sphx_glr_auto_examples_svm_plot_svm_anova.py`
- * :ref:`sphx_glr_auto_examples_plot_compare_reduction.py`
+ * :ref:`sphx_glr_auto_examples_compose_plot_compare_reduction.py`
 
 .. topic:: See also:
 
@@ -218,7 +218,7 @@ object::
 
 .. topic:: Examples:
 
- * :ref:`sphx_glr_auto_examples_plot_compare_reduction.py`
+ * :ref:`sphx_glr_auto_examples_compose_plot_compare_reduction.py`
 
 .. _transformed_target_regressor:
 
@@ -293,6 +293,10 @@ each other. However, it is possible to bypass this checking by setting
    pair of functions ``func`` and ``inverse_func``. However, setting both
    options will raise an error.
 
+.. topic:: Examples:
+
+ * :ref:`sphx_glr_auto_examples_compose_plot_transformed_target.py`
+
 
 .. _feature_union:
 
@@ -304,9 +308,13 @@ FeatureUnion: composite feature spaces
 :class:`FeatureUnion` combines several transformer objects into a new
 transformer that combines their output. A :class:`FeatureUnion` takes
 a list of transformer objects. During fitting, each of these
-is fit to the data independently. For transforming data, the
-transformers are applied in parallel, and the sample vectors they output
-are concatenated end-to-end into larger vectors.
+is fit to the data independently. The transformers are applied in parallel,
+and the feature matrices they output are concatenated side-by-side into a
+larger matrix.
+
+When you want to apply different transformations to each field of the data,
+see the related class :class:`sklearn.compose.ColumnTransformer`
+(see :ref:`user guide <column_transformer>`).
 
 :class:`FeatureUnion` serves the same purposes as :class:`Pipeline` -
 convenience and joint parameter estimation and validation.
@@ -356,5 +364,104 @@ and ignored by setting to ``None``::
 
 .. topic:: Examples:
 
- * :ref:`sphx_glr_auto_examples_plot_feature_stacker.py`
- * :ref:`sphx_glr_auto_examples_hetero_feature_union.py`
+ * :ref:`sphx_glr_auto_examples_compose_plot_feature_union.py`
+
+
+.. _column_transformer:
+
+ColumnTransformer for heterogeneous data
+========================================
+
+.. warning::
+
+    The :class:`compose.ColumnTransformer <sklearn.compose.ColumnTransformer>`
+    class is experimental and the API is subject to change.
+
+Many datasets contain features of different types, say text, floats, and dates,
+where each type of feature requires separate preprocessing or feature
+extraction steps.  Often it is easiest to preprocess data before applying
+scikit-learn methods, for example using `pandas <http://pandas.pydata.org/>`__.
+Processing your data before passing it to scikit-learn might be problematic for
+one of the following reasons:
+
+1. Incorporating statistics from test data into the preprocessors makes
+   cross-validation scores unreliable (known as *data leakage*),
+   for example in the case of scalers or imputing missing values.
+2. You may want to include the parameters of the preprocessors in a
+   :ref:`parameter search <grid_search>`.
+
+The :class:`~sklearn.compose.ColumnTransformer` helps performing different
+transformations for different columns of the data, within a
+:class:`~sklearn.pipeline.Pipeline` that is safe from data leakage and that can
+be parametrized. :class:`~sklearn.compose.ColumnTransformer` works on
+arrays, sparse matrices, and
+`pandas DataFrames <http://pandas.pydata.org/pandas-docs/stable/>`__.
+
+To each column, a different transformation can be applied, such as
+preprocessing or a specific feature extraction method::
+
+  >>> import pandas as pd
+  >>> X = pd.DataFrame(
+  ...     {'city': ['London', 'London', 'Paris', 'Sallisaw'],
+  ...      'title': ["His Last Bow", "How Watson Learned the Trick",
+  ...                "A Moveable Feast", "The Grapes of Wrath"]})
+
+For this data, we might want to encode the ``'city'`` column as a categorical
+variable, but apply a :class:`feature_extraction.text.CountVectorizer
+<sklearn.feature_extraction.text.CountVectorizer>` to the ``'title'`` column.
+As we might use multiple feature extraction methods on the same column, we give
+each transformer a unique name, say ``'city_category'`` and ``'title_bow'``::
+
+  >>> from sklearn.compose import ColumnTransformer
+  >>> from sklearn.feature_extraction.text import CountVectorizer
+  >>> column_trans = ColumnTransformer(
+  ...     [('city_category', CountVectorizer(analyzer=lambda x: [x]), 'city'),
+  ...      ('title_bow', CountVectorizer(), 'title')])
+
+  >>> column_trans.fit(X) # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
+  ColumnTransformer(n_jobs=1, remainder='passthrough', transformer_weights=None,
+      transformers=...)
+
+  >>> column_trans.get_feature_names()
+  ... # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
+  ['city_category__London', 'city_category__Paris', 'city_category__Sallisaw',
+  'title_bow__bow', 'title_bow__feast', 'title_bow__grapes', 'title_bow__his',
+  'title_bow__how', 'title_bow__last', 'title_bow__learned', 'title_bow__moveable',
+  'title_bow__of', 'title_bow__the', 'title_bow__trick', 'title_bow__watson',
+  'title_bow__wrath']
+
+  >>> column_trans.transform(X).toarray()
+  ... # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
+  array([[1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],
+         [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0],
+         [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
+         [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1]]...)
+
+In the above example, the
+:class:`~sklearn.feature_extraction.text.CountVectorizer` expects a 1D array as
+input and therefore the columns were specified as a string (``'city'``).
+However, other transformers generally expect 2D data, and in that case you need
+to specify the column as a list of strings (``['city']``).
+
+Apart from a scalar or a single item list, the column selection can be specified
+as a list of multiple items, an integer array, a slice, or a boolean mask.
+Strings can reference columns if the input is a DataFrame, integers are always
+interpreted as the positional columns.
+
+The :func:`~sklearn.compose.make_columntransformer` function is available
+to more easily create a :class:`~sklearn.compose.ColumnTransformer` object.
+Specifically, the names will be given automatically. The equivalent for the
+above example would be::
+
+  >>> from sklearn.compose import make_column_transformer
+  >>> column_trans = make_column_transformer(
+  ...     ('city', CountVectorizer(analyzer=lambda x: [x])),
+  ...     ('title', CountVectorizer()))
+  >>> column_trans # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
+  ColumnTransformer(n_jobs=1, remainder='passthrough', transformer_weights=None,
+           transformers=[('countvectorizer-1', ...)
+
+.. topic:: Examples:
+
+ * :ref:`sphx_glr_auto_examples_compose_column_transformer.py`
+ * :ref:`sphx_glr_auto_examples_compose_column_transformer_mixed_types.py`
diff --git a/doc/modules/decomposition.rst b/doc/modules/decomposition.rst
index d897377d1626..0ee002fc2fcc 100644
--- a/doc/modules/decomposition.rst
+++ b/doc/modules/decomposition.rst
@@ -347,7 +347,7 @@ compensating for LSA's erroneous assumptions about textual data.
 
 .. topic:: Examples:
 
-   * :ref:`sphx_glr_auto_examples_text_document_clustering.py`
+   * :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`
 
 .. topic:: References:
 
@@ -451,6 +451,32 @@ After using such a procedure to fit the dictionary, the transform is simply a
 sparse coding step that shares the same implementation with all dictionary
 learning objects (see :ref:`SparseCoder`).
 
+It is also possible to constrain the dictionary and/or code to be positive to
+match constraints that may be present in the data. Below are the faces with
+different positivity constraints applied. Red indicates negative values, blue
+indicates positive values, and white represents zeros.
+
+
+.. |dict_img_pos1| image:: ../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_011.png
+    :target: ../auto_examples/decomposition/plot_image_denoising.html
+    :scale: 60%
+
+.. |dict_img_pos2| image:: ../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_012.png
+    :target: ../auto_examples/decomposition/plot_image_denoising.html
+    :scale: 60%
+
+.. |dict_img_pos3| image:: ../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_013.png
+    :target: ../auto_examples/decomposition/plot_image_denoising.html
+    :scale: 60%
+
+.. |dict_img_pos4| image:: ../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_014.png
+    :target: ../auto_examples/decomposition/plot_image_denoising.html
+    :scale: 60%
+
+.. centered:: |dict_img_pos1| |dict_img_pos2|
+.. centered:: |dict_img_pos3| |dict_img_pos4|
+
+
 The following image shows how a dictionary learned from 4x4 pixel image patches
 extracted from part of the image of a raccoon face looks like.
 
diff --git a/doc/modules/ensemble.rst b/doc/modules/ensemble.rst
index 655d5638472a..83da35eb46ca 100644
--- a/doc/modules/ensemble.rst
+++ b/doc/modules/ensemble.rst
@@ -782,7 +782,7 @@ accessed via the ``feature_importances_`` property::
     >>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
     ...     max_depth=1, random_state=0).fit(X, y)
     >>> clf.feature_importances_  # doctest: +ELLIPSIS
-    array([0.11, 0.1 , 0.11, ...
+    array([0.10..., 0.10..., 0.11..., ...
 
 .. topic:: Examples:
 
diff --git a/doc/modules/feature_extraction.rst b/doc/modules/feature_extraction.rst
index 3718819f852c..611c7ecb60ee 100644
--- a/doc/modules/feature_extraction.rst
+++ b/doc/modules/feature_extraction.rst
@@ -657,12 +657,12 @@ In particular in a **supervised setting** it can be successfully combined
 with fast and scalable linear models to train **document classifiers**,
 for instance:
 
- * :ref:`sphx_glr_auto_examples_text_document_classification_20newsgroups.py`
+ * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`
 
 In an **unsupervised setting** it can be used to group similar documents
 together by applying clustering algorithms such as :ref:`k_means`:
 
-  * :ref:`sphx_glr_auto_examples_text_document_clustering.py`
+  * :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`
 
 Finally it is possible to discover the main topics of a corpus by
 relaxing the hard assignment constraint of clustering, for instance by
@@ -916,7 +916,7 @@ Some tips and tricks:
     (Note that this will not filter out punctuation.)
 
 
-    The following example will, for instance, transform some British spelling 
+    The following example will, for instance, transform some British spelling
     to American spelling::
 
         >>> import re
diff --git a/doc/modules/feature_selection.rst b/doc/modules/feature_selection.rst
index fea33ab180b7..ae630af183cd 100644
--- a/doc/modules/feature_selection.rst
+++ b/doc/modules/feature_selection.rst
@@ -198,7 +198,7 @@ alpha parameter, the fewer features selected.
 
 .. topic:: Examples:
 
-    * :ref:`sphx_glr_auto_examples_text_document_classification_20newsgroups.py`: Comparison
+    * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`: Comparison
       of different algorithms for document classification including L1-based
       feature selection.
 
diff --git a/doc/modules/impute.rst b/doc/modules/impute.rst
index f16182510597..0f9089c98178 100644
--- a/doc/modules/impute.rst
+++ b/doc/modules/impute.rst
@@ -13,16 +13,30 @@ array are numerical, and that all have and hold meaning. A basic strategy to use
 incomplete datasets is to discard entire rows and/or columns containing missing
 values. However, this comes at the price of losing data which may be valuable
 (even though incomplete). A better strategy is to impute the missing values,
-i.e., to infer them from the known part of the data.
+i.e., to infer them from the known part of the data. See the :ref:`glossary`
+entry on imputation.
 
 
+Univariate vs. Multivariate Imputation
+======================================
+
+One type of imputation algorithm is univariate, which imputes values in the i-th
+feature dimension using only non-missing values in that feature dimension
+(e.g. :class:`impute.SimpleImputer`). By contrast, multivariate imputation
+algorithms use the entire set of available feature dimensions to estimate the
+missing values (e.g. :class:`impute.ChainedImputer`).
+
+
+.. _single_imputer:
+
 Univariate feature imputation
 =============================
 
 The :class:`SimpleImputer` class provides basic strategies for imputing missing
-values, either using the mean, the median or the most frequent value of
-the row or column in which the missing values are located. This class
-also allows for different missing values encodings.
+values. Missing values can be imputed with a provided constant value, or using
+the statistics (mean, median or most frequent) of each column in which the
+missing values are located. This class also allows for different missing values
+encodings.
 
 The following snippet demonstrates how to replace missing values,
 encoded as ``np.nan``, using the mean value of the columns (axis 0)
@@ -30,9 +44,9 @@ that contain the missing values::
 
     >>> import numpy as np
     >>> from sklearn.impute import SimpleImputer
-    >>> imp = SimpleImputer(missing_values='NaN', strategy='mean')
+    >>> imp = SimpleImputer(missing_values=np.nan, strategy='mean')
     >>> imp.fit([[1, 2], [np.nan, 3], [7, 6]])       # doctest: +NORMALIZE_WHITESPACE
-    SimpleImputer(copy=True, missing_values='NaN', strategy='mean', verbose=0)
+    SimpleImputer(copy=True, fill_value=None, missing_values=nan, strategy='mean', verbose=0)
     >>> X = [[np.nan, 2], [6, np.nan], [7, 6]]
     >>> print(imp.transform(X))           # doctest: +NORMALIZE_WHITESPACE  +ELLIPSIS
     [[4.          2.        ]
@@ -45,7 +59,7 @@ The :class:`SimpleImputer` class also supports sparse matrices::
     >>> X = sp.csc_matrix([[1, 2], [0, 3], [7, 6]])
     >>> imp = SimpleImputer(missing_values=0, strategy='mean')
     >>> imp.fit(X)                  # doctest: +NORMALIZE_WHITESPACE
-    SimpleImputer(copy=True, missing_values=0, strategy='mean', verbose=0)
+    SimpleImputer(copy=True, fill_value=None, missing_values=0, strategy='mean', verbose=0)
     >>> X_test = sp.csc_matrix([[0, 2], [6, 0], [7, 6]])
     >>> print(imp.transform(X_test))      # doctest: +NORMALIZE_WHITESPACE  +ELLIPSIS
     [[4.          2.        ]
@@ -56,35 +70,75 @@ Note that, here, missing values are encoded by 0 and are thus implicitly stored
 in the matrix. This format is thus suitable when there are many more missing
 values than observed values.
 
-.. _mice:
+The :class:`SimpleImputer` class also supports categorical data represented as
+string values or pandas categoricals when using the ``'most_frequent'`` or
+``'constant'`` strategy::
+
+    >>> import pandas as pd
+    >>> df = pd.DataFrame([["a", "x"],
+    ...                    [np.nan, "y"],
+    ...                    ["a", np.nan],
+    ...                    ["b", "y"]], dtype="category")
+    ...
+    >>> imp = SimpleImputer(strategy="most_frequent")
+    >>> print(imp.fit_transform(df))      # doctest: +NORMALIZE_WHITESPACE
+    [['a' 'x']
+     ['a' 'y']
+     ['a' 'y']
+     ['b' 'y']]
+
+.. _chained_imputer:
+
 
 Multivariate feature imputation
 ===============================
 
-A more sophisticated approach is to use the :class:`MICEImputer` class, which
-implements the Multivariate Imputation by Chained Equations technique. MICE
-models each feature with missing values as a function of other features, and
-uses that estimate for imputation. It does so in a round-robin fashion: at
-each step, a feature column is designated as output `y` and the other feature
-columns are treated as inputs `X`. A regressor is fit on `(X, y)` for known `y`.
-Then, the regressor is used to predict the unknown values of `y`. This is
-repeated for each feature, and then is done for a number of imputation rounds.
-Here is an example snippet::
+A more sophisticated approach is to use the :class:`ChainedImputer` class, which
+implements the imputation technique from MICE (Multivariate Imputation by
+Chained Equations). MICE models each feature with missing values as a function of
+other features, and uses that estimate for imputation. It does so in a round-robin
+fashion: at each step, a feature column is designated as output `y` and the other
+feature columns are treated as inputs `X`. A regressor is fit on `(X, y)` for known `y`.
+Then, the regressor is used to predict the unknown values of `y`. This is repeated
+for each feature in a chained fashion, and then is done for a number of imputation
+rounds. Here is an example snippet::
 
     >>> import numpy as np
-    >>> from sklearn.impute import MICEImputer
-    >>> imp = MICEImputer(n_imputations=10, random_state=0)
+    >>> from sklearn.impute import ChainedImputer
+    >>> imp = ChainedImputer(n_imputations=10, random_state=0)
     >>> imp.fit([[1, 2], [np.nan, 3], [7, np.nan]])
-    MICEImputer(imputation_order='ascending', initial_strategy='mean',
-          max_value=None, min_value=None, missing_values='NaN', n_burn_in=10,
-          n_imputations=10, n_nearest_features=None, predictor=None,
-          random_state=0, verbose=False)
+    ChainedImputer(imputation_order='ascending', initial_strategy='mean',
+            max_value=None, min_value=None, missing_values=nan, n_burn_in=10,
+            n_imputations=10, n_nearest_features=None, predictor=None,
+            random_state=0, verbose=False)
     >>> X_test = [[np.nan, 2], [6, np.nan], [np.nan, 6]]
     >>> print(np.round(imp.transform(X_test)))
     [[ 1.  2.]
      [ 6.  4.]
      [13.  6.]]
 
-Both :class:`SimpleImputer` and :class:`MICEImputer` can be used in a Pipeline
+Both :class:`SimpleImputer` and :class:`ChainedImputer` can be used in a Pipeline
 as a way to build a composite estimator that supports imputation.
 See :ref:`sphx_glr_auto_examples_plot_missing_values.py`.
+
+
+.. _multiple_imputation:
+
+Multiple vs. Single Imputation
+==============================
+
+In the statistics community, it is common practice to perform multiple imputations,
+generating, for example, 10 separate imputations for a single feature matrix.
+Each of these 10 imputations is then put through the subsequent analysis pipeline
+(e.g. feature engineering, clustering, regression, classification). The 10 final
+analysis results (e.g. held-out validation error) allow the data scientist to
+obtain understanding of the uncertainty inherent in the missing values. The above
+practice is called multiple imputation. As implemented, the :class:`ChainedImputer`
+class generates a single (averaged) imputation for each missing value because this
+is the most common use case for machine learning applications. However, it can also be used
+for multiple imputations by applying it repeatedly to the same dataset with different
+random seeds with the ``n_imputations`` parameter set to 1.
+
+Note that a call to the ``transform`` method of :class:`ChainedImputer` is not
+allowed to change the number of samples. Therefore multiple imputations cannot be
+achieved by a single call to ``transform``.
diff --git a/doc/modules/linear_model.rst b/doc/modules/linear_model.rst
index 83554c4363a8..e2cc0ba2601a 100644
--- a/doc/modules/linear_model.rst
+++ b/doc/modules/linear_model.rst
@@ -114,7 +114,7 @@ its ``coef_`` member::
 .. topic:: Examples:
 
    * :ref:`sphx_glr_auto_examples_linear_model_plot_ridge_path.py`
-   * :ref:`sphx_glr_auto_examples_text_document_classification_20newsgroups.py`
+   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`
 
 
 Ridge Complexity
@@ -205,6 +205,20 @@ computes the coefficients along the full path of possible values.
       thus be used to perform feature selection, as detailed in
       :ref:`l1_feature_selection`.
 
+The following two references explain the iterations
+used in the coordinate descent solver of scikit-learn, as well as
+the duality gap computation used for convergence control.
+
+.. topic:: References
+
+    * "Regularization Path For Generalized linear Models by Coordinate Descent",
+      Friedman, Hastie & Tibshirani, J Stat Softw, 2010 (`Paper
+      <https://www.jstatsoft.org/article/view/v033i01/v33i01.pdf>`_).
+    * "An Interior-Point Method for Large-Scale L1-Regularized Least Squares,"
+      S. J. Kim, K. Koh, M. Lustig, S. Boyd and D. Gorinevsky,
+      in IEEE Journal of Selected Topics in Signal Processing, 2007
+      (`Paper <https://web.stanford.edu/~boyd/papers/pdf/l1_ls.pdf>`_)
+
 
 Setting regularization parameter
 --------------------------------
@@ -358,7 +372,19 @@ The class :class:`ElasticNetCV` can be used to set the parameters
   * :ref:`sphx_glr_auto_examples_linear_model_plot_lasso_and_elasticnet.py`
   * :ref:`sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py`
 
+The following two references explain the iterations
+used in the coordinate descent solver of scikit-learn, as well as
+the duality gap computation used for convergence control.
+
+.. topic:: References
 
+    * "Regularization Path For Generalized linear Models by Coordinate Descent",
+      Friedman, Hastie & Tibshirani, J Stat Softw, 2010 (`Paper
+      <https://www.jstatsoft.org/article/view/v033i01/v33i01.pdf>`_).
+    * "An Interior-Point Method for Large-Scale L1-Regularized Least Squares,"
+      S. J. Kim, K. Koh, M. Lustig, S. Boyd and D. Gorinevsky,
+      in IEEE Journal of Selected Topics in Signal Processing, 2007
+      (`Paper <https://web.stanford.edu/~boyd/papers/pdf/l1_ls.pdf>`_)
 
 .. _multi_task_elastic_net:
 
diff --git a/doc/modules/model_evaluation.rst b/doc/modules/model_evaluation.rst
index 0fa1037e4d62..eeb058e1440c 100644
--- a/doc/modules/model_evaluation.rst
+++ b/doc/modules/model_evaluation.rst
@@ -523,8 +523,11 @@ Confusion matrix
 ----------------
 
 The :func:`confusion_matrix` function evaluates
-classification accuracy by computing the `confusion matrix
+classification accuracy by computing the confusion matrix
+with each row corresponding to the true class
 <https://en.wikipedia.org/wiki/Confusion_matrix>`_.
+(Wikipedia and other references may use different convention for axes.)
+
 
 By definition, entry :math:`i, j` in a confusion matrix is
 the number of observations actually in group :math:`i`, but
@@ -565,7 +568,7 @@ false negatives and true positives as follows::
     for an example of using a confusion matrix to classify
     hand-written digits.
 
-  * See :ref:`sphx_glr_auto_examples_text_document_classification_20newsgroups.py`
+  * See :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`
     for an example of using a confusion matrix to classify text
     documents.
 
@@ -598,7 +601,7 @@ and inferred labels::
     for an example of classification report usage for
     hand-written digits.
 
-  * See :ref:`sphx_glr_auto_examples_text_document_classification_20newsgroups.py`
+  * See :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`
     for an example of classification report usage for text
     documents.
 
@@ -749,7 +752,7 @@ binary classification and multilabel indicator format.
 
 .. topic:: Examples:
 
-  * See :ref:`sphx_glr_auto_examples_text_document_classification_20newsgroups.py`
+  * See :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`
     for an example of :func:`f1_score` usage to classify  text
     documents.
 
@@ -859,10 +862,10 @@ specified by the ``average`` argument to the
 :func:`average_precision_score` (multilabel only), :func:`f1_score`,
 :func:`fbeta_score`, :func:`precision_recall_fscore_support`,
 :func:`precision_score` and :func:`recall_score` functions, as described
-:ref:`above <average>`. Note that for "micro"-averaging in a multiclass setting
-with all labels included will produce equal precision, recall and :math:`F`,
-while "weighted" averaging may produce an F-score that is not between
-precision and recall.
+:ref:`above <average>`. Note that if all labels are included, "micro"-averaging
+in a multiclass setting will produce precision, recall and :math:`F`
+that are all identical to accuracy. Also note that "weighted" averaging may
+produce an F-score that is not between precision and recall.
 
 To make this more explicit, consider the following notation:
 
diff --git a/doc/modules/model_persistence.rst b/doc/modules/model_persistence.rst
index 15ecf3c2d88f..f5173e5d9f3f 100644
--- a/doc/modules/model_persistence.rst
+++ b/doc/modules/model_persistence.rst
@@ -42,12 +42,12 @@ is often the case for fitted scikit-learn estimators, but can only pickle to the
 disk and not to a string::
 
   >>> from sklearn.externals import joblib
-  >>> joblib.dump(clf, 'filename.pkl') # doctest: +SKIP
+  >>> joblib.dump(clf, 'filename.joblib') # doctest: +SKIP
 
 Later you can load back the pickled model (possibly in another Python process)
 with::
 
-  >>> clf = joblib.load('filename.pkl') # doctest:+SKIP
+  >>> clf = joblib.load('filename.joblib') # doctest:+SKIP
 
 .. note::
 
diff --git a/doc/modules/naive_bayes.rst b/doc/modules/naive_bayes.rst
index b61637c12d87..f3abe5720540 100644
--- a/doc/modules/naive_bayes.rst
+++ b/doc/modules/naive_bayes.rst
@@ -8,17 +8,18 @@ Naive Bayes
 
 
 Naive Bayes methods are a set of supervised learning algorithms
-based on applying Bayes' theorem with the "naive" assumption of independence
-between every pair of features. Given a class variable :math:`y` and a
-dependent feature vector :math:`x_1` through :math:`x_n`,
-Bayes' theorem states the following relationship:
+based on applying Bayes' theorem with the "naive" assumption of
+conditional independence between every pair of features given the
+value of the class variable. Bayes' theorem states the following
+relationship, given class variable :math:`y` and dependent feature
+vector :math:`x_1` through :math:`x_n`, :
 
 .. math::
 
    P(y \mid x_1, \dots, x_n) = \frac{P(y) P(x_1, \dots x_n \mid y)}
                                     {P(x_1, \dots, x_n)}
 
-Using the naive independence assumption that
+Using the naive conditional independence assumption that
 
 .. math::
 
diff --git a/doc/modules/pipeline.rst b/doc/modules/pipeline.rst
index 3ffbc38b6b6c..9f07a9e34cb2 100644
--- a/doc/modules/pipeline.rst
+++ b/doc/modules/pipeline.rst
@@ -1,3 +1,5 @@
+:orphan:
+
 .. raw:: html
 
     <meta http-equiv="refresh" content="1; url=./compose.html" />
diff --git a/doc/modules/preprocessing.rst b/doc/modules/preprocessing.rst
index 19bdfc0d432a..0474a8a66501 100644
--- a/doc/modules/preprocessing.rst
+++ b/doc/modules/preprocessing.rst
@@ -235,9 +235,8 @@ data.
   independently, since a downstream model can further make some assumption
   on the linear independence of the features.
 
-  To address this issue you can use :class:`sklearn.decomposition.PCA`
-  or :class:`sklearn.decomposition.RandomizedPCA` with ``whiten=True``
-  to further remove the linear correlation across features.
+  To address this issue you can use :class:`sklearn.decomposition.PCA` with
+  ``whiten=True`` to further remove the linear correlation across features.
 
 .. topic:: Scaling a 1D array
 
@@ -508,15 +507,13 @@ Such features can be efficiently coded as integers, for instance
 ``[1, 2, 1]``.
 
 To convert categorical features to such integer codes, we can use the
-:class:`CategoricalEncoder`. When specifying that we want to perform an
-ordinal encoding, the estimator transforms each categorical feature to one
+:class:`OrdinalEncoder`. This estimator transforms each categorical feature to one
 new feature of integers (0 to n_categories - 1)::
 
-    >>> enc = preprocessing.CategoricalEncoder(encoding='ordinal')
+    >>> enc = preprocessing.OrdinalEncoder()
     >>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]
     >>> enc.fit(X)  # doctest: +ELLIPSIS
-    CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>,
-              encoding='ordinal', handle_unknown='error')
+    OrdinalEncoder(categories='auto', dtype=<... 'numpy.float64'>)
     >>> enc.transform([['female', 'from US', 'uses Safari']])
     array([[0., 1., 1.]])
 
@@ -528,18 +525,19 @@ browsers was ordered arbitrarily).
 Another possibility to convert categorical features to features that can be used
 with scikit-learn estimators is to use a one-of-K, also known as one-hot or
 dummy encoding.
-This type of encoding is the default behaviour of the :class:`CategoricalEncoder`.
-The :class:`CategoricalEncoder` then transforms each categorical feature with
+This type of encoding can be obtained with the :class:`OneHotEncoder`,
+which transforms each categorical feature with
 ``n_categories`` possible values into ``n_categories`` binary features, with
 one of them 1, and all others 0.
 
 Continuing the example above::
 
-  >>> enc = preprocessing.CategoricalEncoder()
+  >>> enc = preprocessing.OneHotEncoder()
   >>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]
   >>> enc.fit(X)  # doctest: +ELLIPSIS
-  CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>,
-            encoding='onehot', handle_unknown='error')
+  OneHotEncoder(categorical_features=None, categories=None,
+         dtype=<... 'numpy.float64'>, handle_unknown='error',
+         n_values=None, sparse=True)
   >>> enc.transform([['female', 'from US', 'uses Safari'],
   ...                ['male', 'from Europe', 'uses Safari']]).toarray()
   array([[1., 0., 0., 1., 0., 1.],
@@ -558,14 +556,15 @@ dataset::
     >>> genders = ['female', 'male']
     >>> locations = ['from Africa', 'from Asia', 'from Europe', 'from US']
     >>> browsers = ['uses Chrome', 'uses Firefox', 'uses IE', 'uses Safari']
-    >>> enc = preprocessing.CategoricalEncoder(categories=[genders, locations, browsers])
+    >>> enc = preprocessing.OneHotEncoder(categories=[genders, locations, browsers])
     >>> # Note that for there are missing categorical values for the 2nd and 3rd
     >>> # feature
     >>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]
     >>> enc.fit(X) # doctest: +ELLIPSIS
-    CategoricalEncoder(categories=[...],
-              dtype=<... 'numpy.float64'>, encoding='onehot',
-              handle_unknown='error')
+    OneHotEncoder(categorical_features=None,
+           categories=[...],
+           dtype=<... 'numpy.float64'>, handle_unknown='error',
+           n_values=None, sparse=True)
     >>> enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray()
     array([[1., 0., 0., 1., 0., 0., 1., 0., 0., 0.]])
 
@@ -577,11 +576,12 @@ during transform, no error will be raised but the resulting one-hot encoded
 columns for this feature will be all zeros
 (``handle_unknown='ignore'`` is only supported for one-hot encoding)::
 
-    >>> enc = preprocessing.CategoricalEncoder(handle_unknown='ignore')
+    >>> enc = preprocessing.OneHotEncoder(handle_unknown='ignore')
     >>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]
     >>> enc.fit(X) # doctest: +ELLIPSIS
-    CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>,
-              encoding='onehot', handle_unknown='ignore')
+    OneHotEncoder(categorical_features=None, categories=None,
+           dtype=<... 'numpy.float64'>, handle_unknown='ignore',
+           n_values=None, sparse=True)
     >>> enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray()
     array([[1., 0., 0., 0., 0., 0.]])
 
diff --git a/doc/modules/sgd.rst b/doc/modules/sgd.rst
index 6b384e12ce31..64eea91a9fa9 100644
--- a/doc/modules/sgd.rst
+++ b/doc/modules/sgd.rst
@@ -218,7 +218,7 @@ matrix format as defined in `scipy.sparse.csr_matrix
 
 .. topic:: Examples:
 
- - :ref:`sphx_glr_auto_examples_text_document_classification_20newsgroups.py`
+ - :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`
 
 Complexity
 ==========
diff --git a/doc/support.rst b/doc/support.rst
index cf71145c0e00..70efd7a109b0 100644
--- a/doc/support.rst
+++ b/doc/support.rst
@@ -92,7 +92,7 @@ Documentation resources
 
 This documentation is relative to |release|. Documentation for
 other versions can be found `here
-<http://scikit-learn.org/dev/versions.html>`_.
+<http://scikit-learn.org/dev/versions.html>`__.
 
 Printable pdf documentation for old versions can be found `here
 <https://sourceforge.net/projects/scikit-learn/files/documentation/>`_.
diff --git a/doc/testimonials/testimonials.rst b/doc/testimonials/testimonials.rst
index 0ac325002294..f30e14f12a97 100644
--- a/doc/testimonials/testimonials.rst
+++ b/doc/testimonials/testimonials.rst
@@ -120,7 +120,7 @@ Gilad Lotan, Chief Data Scientist
 
 
 `Hugging Face <https://huggingface.co>`_
-------------------------------------
+----------------------------------------
 
 .. raw:: html
 
diff --git a/doc/tutorial/basic/tutorial.rst b/doc/tutorial/basic/tutorial.rst
index ece691f7de97..781495df9931 100644
--- a/doc/tutorial/basic/tutorial.rst
+++ b/doc/tutorial/basic/tutorial.rst
@@ -239,12 +239,12 @@ which is more efficient on big data but it can only pickle to the disk
 and not to a string::
 
   >>> from sklearn.externals import joblib
-  >>> joblib.dump(clf, 'filename.pkl') # doctest: +SKIP
+  >>> joblib.dump(clf, 'filename.joblib') # doctest: +SKIP
 
 Later, you can reload the pickled model (possibly in another Python process)
 with::
 
-  >>> clf = joblib.load('filename.pkl') # doctest:+SKIP
+  >>> clf = joblib.load('filename.joblib') # doctest:+SKIP
 
 .. note::
 
diff --git a/doc/tutorial/machine_learning_map/index.rst b/doc/tutorial/machine_learning_map/index.rst
index b198fa766ec9..3690d76b31bd 100644
--- a/doc/tutorial/machine_learning_map/index.rst
+++ b/doc/tutorial/machine_learning_map/index.rst
@@ -100,7 +100,7 @@ Click on any estimator in the chart below to see its documentation.
 		<area href="../../modules/svm.html#classification" title="SVC Documentation" shape="poly" coords="210,157, 210,157, 210,194, 210,194, 210,194, 333,194, 333,194, 333,194, 333,157, 333,157, 333,157, 210,157, 210,157" data-maphilight='{"strokeColor":"0000ff","strokeWidth":5,"fillColor":"66FF66","fillOpacity":0.4}'></area>
 		<area href="../../modules/svm.html#regression" title="SVR Documentation" shape="poly" coords="1696,692, 1696,692, 1696,732, 1696,732, 1696,732, 1890,732, 1890,732, 1890,732, 1890,692, 1890,692, 1890,692, 1696,692, 1696,692" data-maphilight='{"strokeColor":"0000ff","strokeWidth":5,"fillColor":"66FF66","fillOpacity":0.4}'></area>
 		<area href="../../modules/svm.html#regression" title="SVR Documentation" shape="poly" coords="1831,458, 1831,458, 1831,496, 1831,496, 1831,496, 2052,496, 2052,496, 2052,496, 2052,458, 2052,458, 2052,458, 1831,458, 1831,458" data-maphilight='{"strokeColor":"0000ff","strokeWidth":5,"fillColor":"66FF66","fillOpacity":0.4}'></area>
-		<area href="../../modules/mixture.html#vbgmm-classifier-variational-gaussian-mixtures" title=" VBGMM Documentation" shape="poly" coords="562,994, 562,994, 562,1026, 562,1026, 562,1026, 682,1026, 682,1026, 682,1026, 682,994, 682,994, 682,994, 562,994, 562,994" data-maphilight='{"strokeColor":"0000ff","strokeWidth":5,"fillColor":"66FF66","fillOpacity":0.4}'></area>
+		<area href="../../modules/mixture.html#bgmm" title=" Bayesian GMM Documentation" shape="poly" coords="562,994, 562,994, 562,1026, 562,1026, 562,1026, 682,1026, 682,1026, 682,1026, 682,994, 682,994, 682,994, 562,994, 562,994" data-maphilight='{"strokeColor":"0000ff","strokeWidth":5,"fillColor":"66FF66","fillOpacity":0.4}'></area>
 	    </map>
 	</img>
       </p>
diff --git a/doc/tutorial/statistical_inference/putting_together.rst b/doc/tutorial/statistical_inference/putting_together.rst
index 556b6b8df089..5106958d77e9 100644
--- a/doc/tutorial/statistical_inference/putting_together.rst
+++ b/doc/tutorial/statistical_inference/putting_together.rst
@@ -11,12 +11,12 @@ Pipelining
 We have seen that some estimators can transform data and that some estimators
 can predict variables. We can also create combined estimators:
 
-.. image:: /auto_examples/images/sphx_glr_plot_digits_pipe_001.png
-   :target: ../../auto_examples/plot_digits_pipe.html
+.. image:: ../../auto_examples/compose/images/sphx_glr_plot_digits_pipe_001.png
+   :target: ../../auto_examples/compose/plot_digits_pipe.html
    :scale: 65
    :align: right
 
-.. literalinclude:: ../../auto_examples/plot_digits_pipe.py
+.. literalinclude:: ../../auto_examples/compose/plot_digits_pipe.py
     :lines: 23-63
 
 
diff --git a/doc/tutorial/text_analytics/working_with_text_data.rst b/doc/tutorial/text_analytics/working_with_text_data.rst
index 9be0b0af3fc5..24b0b5b3e371 100644
--- a/doc/tutorial/text_analytics/working_with_text_data.rst
+++ b/doc/tutorial/text_analytics/working_with_text_data.rst
@@ -554,7 +554,7 @@ upon the completion of this tutorial:
   :class:`CountVectorizer`.
 
 * If you don't have labels, try using
-  :ref:`Clustering <sphx_glr_auto_examples_text_document_clustering.py>`
+  :ref:`Clustering <sphx_glr_auto_examples_text_plot_document_clustering.py>`
   on your problem.
 
 * If you have multiple labels per document, e.g categories, have a look
diff --git a/doc/whats_new.rst b/doc/whats_new.rst
index 63b1b309b844..0e7345836f48 100644
--- a/doc/whats_new.rst
+++ b/doc/whats_new.rst
@@ -8,6 +8,9 @@ Release History
 Release notes for current and recent releases are detailed on this page, with
 :ref:`previous releases <previous_releases_whats_new>` linked below.
 
+**Tip:** `Subscribe to scikit-learn releases <https://libraries.io/pypi/scikit-learn>`__
+on libraries.io to be notified when new versions are released.
+
 .. include:: whats_new/v0.20.rst
 .. include:: whats_new/v0.19.rst
 
diff --git a/doc/whats_new/older_versions.rst b/doc/whats_new/older_versions.rst
index eeb672914f03..ad4567ec4db5 100644
--- a/doc/whats_new/older_versions.rst
+++ b/doc/whats_new/older_versions.rst
@@ -1273,7 +1273,7 @@ Examples
 
 - new examples using some of the mlcomp datasets:
   ``sphx_glr_auto_examples_mlcomp_sparse_document_classification.py`` (since removed) and
-  :ref:`sphx_glr_auto_examples_text_document_classification_20newsgroups.py`
+  :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`
 
 - Many more examples. `See here
   <http://scikit-learn.org/stable/auto_examples/index.html>`_
diff --git a/doc/whats_new/v0.18.rst b/doc/whats_new/v0.18.rst
index ad240d578279..ea3548c0b9a0 100644
--- a/doc/whats_new/v0.18.rst
+++ b/doc/whats_new/v0.18.rst
@@ -463,7 +463,7 @@ Model evaluation and meta-estimators
 - Added support for substituting or disabling :class:`pipeline.Pipeline`
   and :class:`pipeline.FeatureUnion` components using the ``set_params``
   interface that powers :mod:`sklearn.grid_search`.
-  See :ref:`sphx_glr_auto_examples_plot_compare_reduction.py`
+  See :ref:`sphx_glr_auto_examples_compose_plot_compare_reduction.py`
   By `Joel Nothman`_ and :user:`Robert McGibbon <rmcgibbo>`.
 
 - The new ``cv_results_`` attribute of :class:`model_selection.GridSearchCV`
diff --git a/doc/whats_new/v0.20.rst b/doc/whats_new/v0.20.rst
index fd7a83a873af..b0d2c31b6933 100644
--- a/doc/whats_new/v0.20.rst
+++ b/doc/whats_new/v0.20.rst
@@ -7,10 +7,37 @@
 Version 0.20 (under development)
 ================================
 
-As well as a plethora of new features and enhancements, this release is the
-first to be accompanied by a :ref:`glossary` developed by `Joel Nothman`_. The
-glossary is a reference resource to help users and contributors become familiar
-with the terminology and conventions used in Scikit-learn.
+This release packs in a mountain of bug fixes, features and enhancements for
+the Scikit-learn library, and improvements to the documentation and examples.
+Thanks to our many contributors!
+
+Highlights
+----------
+
+We have tried to improve our support for common data-science use-cases
+including missing values, categorical variables, heterogeneous data, and
+features/targets with unusual distributions.
+
+Missing values in features, represented by NaNs, are now accepted in
+column-wise preprocessing such as scalers.  Each feature is fitted disregarding
+NaNs, and data containing NaNs can be transformed. The new :mod:`impute`
+module provides estimators for learning despite missing data.
+
+:class:`~compose.ColumnTransformer` handles the case where different features
+or columns of a pandas.DataFrame need different preprocessing.
+String or pandas Categorical columns can now be encoded with
+:class:`~preprocessing.OneHotEncoder` or
+:class:`~preprocessing.OrdinalEncoder`.
+
+:class:`~compose.TransformedTargetRegressor` helps when the regression target
+needs to be transformed to be modeled. :class:`~preprocessing.PowerTransformer`
+joins :class:`~preprocessing.QuantileTransformer` as a non-linear
+transformation.
+
+This release is also the first to be accompanied by a :ref:`glossary` developed
+by `Joel Nothman`_. The glossary is a reference resource to help users and
+contributors become familiar with the terminology and conventions used in
+Scikit-learn.
 
 Changed models
 --------------
@@ -26,9 +53,10 @@ random sampling procedures.
 - :class:`linear_model.OrthogonalMatchingPursuit` (bug fix)
 - :class:`metrics.roc_auc_score` (bug fix)
 - :class:`metrics.roc_curve` (bug fix)
-- :class:`neural_network.BaseMultilayerPerceptron` (bug fix)
 - :class:`neural_network.MLPRegressor` (bug fix)
 - :class:`neural_network.MLPClassifier` (bug fix)
+- :class:`neural_network.BaseMultilayerPerceptron` (bug fix)
+- :class:`ensemble.gradient_boosting.GradientBoostingClassifier` (bug fix affecting feature importances)
 - The v0.19.0 release notes failed to mention a backwards incompatibility with
   :class:`model_selection.StratifiedKFold` when ``shuffle=True`` due to
   :issue:`7823`.
@@ -63,20 +91,26 @@ Classifiers and regressors
   Naive Bayes classifier described in Rennie et al. (2003).
   :issue:`8190` by :user:`Michael A. Alcorn <airalcorn2>`.
 
-- Added :class:`multioutput.RegressorChain` for multi-target
-  regression. :issue:`9257` by :user:`Kumar Ashutosh <thechargedneutron>`.
+- :class:`ensemble.BaggingRegressor` and :class:`ensemble.BaggingClassifier` can now
+  be fit with missing/non-finite values in X and/or multi-output Y to support
+  wrapping pipelines that perform their own imputation.
+  :issue:`9707` by :user:`Jimmy Wan <jimmywan>`.
 
 Preprocessing
 
-- Added :class:`preprocessing.CategoricalEncoder`, which allows to encode
-  categorical features as a numeric array, either using a one-hot (or
-  dummy) encoding scheme or by converting to ordinal integers.
-  Compared to the existing :class:`OneHotEncoder`, this new class handles
+- Expanded :class:`preprocessing.OneHotEncoder` to allow to encode
+  categorical string features as a numeric array using a one-hot (or dummy)
+  encoding scheme, and added :class:`preprocessing.OrdinalEncoder` to
+  convert to ordinal integers.  Those two classes now handle
   encoding of all feature types (also handles string-valued features) and
   derives the categories based on the unique values in the features instead of
-  the maximum value in the features. :issue:`9151` by
+  the maximum value in the features. :issue:`9151` and :issue:`10521` by
   :user:`Vighnesh Birodkar <vighneshbirodkar>` and `Joris Van den Bossche`_.
 
+- Added :class:`compose.ColumnTransformer`, which allows to apply
+  different transformers to different columns of arrays or pandas
+  DataFrames. :issue:`9012` by `Andreas MÃ¼ller`_ and `Joris Van den Bossche`_.
+
 - Added :class:`preprocessing.PowerTransformer`, which implements the Box-Cox
   power transformation, allowing users to map data from any distribution to a
   Gaussian distribution. This is useful as a variance-stabilizing transformation
@@ -84,25 +118,19 @@ Preprocessing
   :issue:`10210` by :user:`Eric Chang <ericchang00>` and
   :user:`Maniteja Nandana <maniteja123>`.
 
-- :class:`preprocessing.QuantileTransformer` handles and ignores NaN values.
-  :issue:`10404` by :user:`Guillaume Lemaitre <glemaitre>`.
-
 - Added the :class:`compose.TransformedTargetRegressor` which transforms
   the target y before fitting a regression model. The predictions are mapped
   back to the original space via an inverse transform. :issue:`9041` by
   `Andreas MÃ¼ller`_ and :user:`Guillaume Lemaitre <glemaitre>`.
 
-- Added :class:`MICEImputer`, which is a strategy for imputing missing
+- Added :class:`impute.ChainedImputer`, which is a strategy for imputing missing
   values by modeling each feature with missing values as a function of
   other features in a round-robin fashion. :issue:`8478` by
   :user:`Sergey Feldman <sergeyf>`.
 
-- Updated :class:`preprocessing.MinMaxScaler` to pass through NaN values. :issue:`10404`
-  by :user:`Lucija Gregov <LucijaGregov>`.
-
 Model evaluation
 
-- Added the :func:`metrics.cluster.davies_bouldin_index` metric for unsupervised
+- Added the :func:`metrics.davies_bouldin_score` metric for unsupervised
   evaluation of clustering models. :issue:`10827` by :user:`Luis Osa <logc>`.
 
 - Added the :func:`metrics.balanced_accuracy_score` metric and a corresponding
@@ -119,12 +147,20 @@ Decomposition, manifold learning and clustering
   sample weights via new parameter ``sample_weight`` in ``fit`` function.
   :issue:`10933` by :user:`Johannes Hansen <jnhansen>`.
 
+- :mod:`dict_learning` functions and models now support positivity constraints.
+  This applies to the dictionary and sparse code.
+  :issue:`6374` by :user:`John Kirkham <jakirkham>`.
+
 Metrics
 
 - Partial AUC is available via ``max_fpr`` parameter in
   :func:`metrics.roc_auc_score`. :issue:`3273` by
   :user:`Alexander NiederbÃ¼hl <Alexander-N>`.
 
+- Added ``output_dict`` parameter in :func:`metrics.classification_report`
+  to return classification statistics as dictionary.
+  :issue:`11160` by :user:`Dan Barkhorn <danielbarkhorn>`.
+
 Misc
 
 - A new configuration parameter, ``working_memory`` was added to control memory
@@ -166,7 +202,7 @@ Classifiers and regressors
   :class:`linear_model.BayesianRidge` for weighted linear regression.
   :issue:`10111` by :user:`Peter St. John <pstjohn>`.
 
-- :class:`dummy.DummyClassifier` and :class:`dummy.DummyRegresssor` now
+- :class:`dummy.DummyClassifier` and :class:`dummy.DummyRegressor` now
   only require X to be an object with finite length or shape.
   :issue:`9832` by :user:`Vrishank Bhardwaj <vrishank97>`.
 
@@ -182,6 +218,10 @@ Classifiers and regressors
 - :func:`manifold.t_sne.trustworthiness` accepts metrics other than
   Euclidean. :issue:`9775` by :user:`William de Vazelhes <wdevazelhes>`.
 
+- :mod:`Nearest neighbors <neighbors>` query methods are now more memory
+  efficient when ``algorithm='brute'``. :issue:`11136` by `Joel Nothman`_
+  and :user:`Aman Dalmia <dalmia>`.
+
 Cluster
 
 - :class:`cluster.KMeans`, :class:`cluster.MiniBatchKMeans` and
@@ -214,6 +254,27 @@ Preprocessing
   classes found which are ignored.
   :issue:`10913` by :user:`Rodrigo Agundez <rragundez>`.
 
+- :class:`preprocessing.QuantileTransformer` handles and ignores NaN values.
+  :issue:`10404` by :user:`Guillaume Lemaitre <glemaitre>`.
+
+- Updated :class:`preprocessing.MinMaxScaler` and
+  :func:`preprocessing.minmax_scale` to pass through NaN values.
+  :issue:`10404` and :issue:`11243` by :user:`Lucija Gregov <LucijaGregov>` and
+  :user:`Guillaume Lemaitre <glemaitre>`.
+
+- :class:`preprocessing.StandardScaler` and :func:`preprocessing.scale`
+  ignore and pass-through NaN values.
+  :issue:`11206` by :user:`Guillaume Lemaitre <glemaitre>`.
+
+- :class:`preprocessing.MaxAbsScaler` and :func:`preprocessing.maxabs_scale`
+  handles and ignores NaN values.
+  :issue:`11011` by `Lucija Gregov <LucihaGregov>` and
+  :user:`Guillaume Lemaitre <glemaitre>`
+
+- :class:`preprocessing.PowerTransformer` and
+  :func:`preprocessing.power_transform` ignore and pass-through NaN values.
+  :issue:`11306` by :user:`Guillaume Lemaitre <glemaitre>`.
+
 Model evaluation and meta-estimators
 
 - The default of ``n_splits`` parameter of :class:`model_selection.KFold` is
@@ -239,8 +300,15 @@ Model evaluation and meta-estimators
   :issue:`9304` by :user:`Breno Freitas <brenolf>`.
 
 - Add `return_estimator` parameter in :func:`model_selection.cross_validate` to
-  return estimators fitted on each split. :issue:`9686` by :user:`AurÃ©lien Bellet
-  <bellet>`.
+  return estimators fitted on each split.
+  :issue:`9686` by :user:`AurÃ©lien Bellet <bellet>`.
+
+- New ``refit_time_`` attribute will be stored in
+  :class:`model_selection.GridSearchCV` and
+  :class:`model_selection.RandomizedSearchCV` if ``refit`` is set to ``True``.
+  This will allow measuring the complete time it takes to perform
+  hyperparameter optimization and refitting the best model on the whole
+  dataset. :issue:`11310` by :user:`Matthias Feurer <mfeurer>`.
 
 Decomposition and manifold learning
 
@@ -251,15 +319,22 @@ Decomposition and manifold learning
 Metrics
 
 - :func:`metrics.roc_auc_score` now supports binary ``y_true`` other than
-  ``{0, 1}`` or ``{-1, 1}``. :issue:`9828` by :user:`Hanmin Qin <qinhanmin2014>`.
+  ``{0, 1}`` or ``{-1, 1}``.
+  :issue:`9828` by :user:`Hanmin Qin <qinhanmin2014>`.
 
-- :func:`metrics.label_ranking_average_precision_score` now supports vector ``sample_weight``.
+- :func:`metrics.label_ranking_average_precision_score` now supports vector
+  ``sample_weight``.
   :issue:`10845` by :user:`Jose Perez-Parras Toledano <jopepato>`.
 
-- Add ``dense_output`` parameter to :func:`metrics.pairwise.linear_kernel`. When
-  False and both inputs are sparse, will return a sparse matrix.
+- Add ``dense_output`` parameter to :func:`metrics.pairwise.linear_kernel`.
+  When False and both inputs are sparse, will return a sparse matrix.
   :issue:`10999` by :user:`Taylor G Smith <tgsmith61591>`.
 
+- :func:`metrics.cluster.silhouette_score` and
+  :func:`metrics.cluster.silhouette_samples` are more memory efficient and run
+  faster. This avoids some reported freezes and MemoryErrors.
+  :issue:`11135` by `Joel Nothman`_.
+
 Linear, kernelized and related models
 
 - Deprecate ``random_state`` parameter in :class:`svm.OneClassSVM` as the
@@ -316,6 +391,11 @@ Classifiers and regressors
   returning incorrect probabilities in the case of binary outcomes.
   :issue:`9939` by :user:`Roger Westover <rwolst>`.
 
+- Fixed a bug in :class:`linear_model.LogisticRegressionCV` where the
+  ``score`` method always computes accuracy, not the metric given by
+  the ``scoring`` parameter.
+  :issue:`10998` by :user:`Thomas Fan <thomasjpfan>`.
+
 - Fixed a bug in :class:`linear_model.OrthogonalMatchingPursuit` that was
   broken when setting ``normalize=False``.
   :issue:`10071` by `Alexandre Gramfort`_.
@@ -362,6 +442,10 @@ Classifiers and regressors
 - Fixed a bug in :class:`sklearn.linear_model.Lasso`
   where the coefficient had wrong shape when ``fit_intercept=False``.
   :issue:`10687` by :user:`Martin Hahn <martin-hahn>`.
+  
+- Fixed a bug in :func:`sklearn.linear_model.LogisticRegression` where the 
+  multi_class='multinomial' with binary output with warm_start = True
+  :issue:`10836` by :user:`Aishwarya Srinivasan <aishgrt1>`.
 
 - Fixed a bug in :class:`linear_model.RidgeCV` where using integer ``alphas``
   raised an error. :issue:`10393` by :user:`Mabel Villalba-JimÃ©nez <mabelvj>`.
@@ -370,6 +454,18 @@ Classifiers and regressors
   and :class:`linear_model.ElasticNet` when working with sparse matrices.
   :issue:`10992` by `Alexandre Gramfort`_.
 
+- Fixed a bug where liblinear and libsvm-based estimators would segfault if
+  passed a scipy.sparse matrix with 64-bit indices. They now raise a
+  ValueError.
+  :issue:`11327` by :user:`Karan Dhingra <kdhingra307>` and `Joel Nothman`_.
+
+- Fixed a bug in :class:`ensemble.gradient_boosting.GradientBoostingRegressor`
+  and :class:`ensemble.gradient_boosting.GradientBoostingClassifier` to have
+  feature importances summed and then normalized, rather than normalizing on a
+  per-tree basis. The previous behavior over-weighted the Gini importance of
+  features that appear in later stages. This issue only affected feature
+  importances. :issue:`11176` by :user:`Gil Forsyth <gforsyth>`.
+
 Decomposition, manifold learning and clustering
 
 - Fix for uninformative error in :class:`decomposition.IncrementalPCA`:
@@ -411,8 +507,8 @@ Decomposition, manifold learning and clustering
   looking for. :issue:`10059` by :user:`Christian Braune <christianbraune79>`.
 
 - Fixed a bug in :func:`datasets.make_circles`, where no odd number of data
-  points could be generated. :issue:`10037` by :user:`Christian Braune
-  <christianbraune79>`_.
+  points could be generated. :issue:`10037`
+  by :user:`Christian Braune <christianbraune79>`.
 
 - Fixed a bug in :func:`cluster.spectral_clustering` where the normalization of
   the spectrum was using a division instead of a multiplication. :issue:`8129`
@@ -424,9 +520,14 @@ Decomposition, manifold learning and clustering
   :class:`mixture.BayesianGaussianMixture`. :issue:`10740` by :user:`Erich
   Schubert <kno10>` and :user:`Guillaume Lemaitre <glemaitre>`.
 
+- Fixed a bug in :class:`decomposition.SparseCoder` when running OMP sparse
+  coding in parallel using readonly memory mapped datastructures. :issue:`5956`
+  by :user:`Vighnesh Birodkar <vighneshbirodkar>` and
+  :user:`Olivier Grisel <ogrisel>`.
+
 Metrics
 
-- Fixed a bug in :func:`metrics.precision_precision_recall_fscore_support`
+- Fixed a bug in :func:`metrics.precision_recall_fscore_support`
   when truncated `range(n_labels)` is passed as value for `labels`.
   :issue:`10377` by :user:`Gaurav Dhingra <gxyd>`.
 
@@ -440,10 +541,10 @@ Metrics
   and :user:`Hanmin Qin <qinhanmin2014>`.
 
 - Fixed a bug to avoid integer overflow. Casted product to 64 bits integer in
-  :func:`mutual_info_score`.
+  :func:`metrics.mutual_info_score`.
   :issue:`9772` by :user:`Kumar Ashutosh <thechargedneutron>`.
 
-- Fixed a bug in :func:`metrics.cluster.fowlkes_mallows_score` to avoid integer
+- Fixed a bug in :func:`metrics.fowlkes_mallows_score` to avoid integer
   overflow. Casted return value of `contingency_matrix` to `int64` and computed
   product of square roots rather than square root of product.
   :issue:`9515` by :user:`Alan Liddell <aliddell>` and
@@ -470,9 +571,16 @@ Feature Extraction
   (words or n-grams). :issue:`9147` by :user:`Claes-Fredrik Mannby <mannby>`
   and `Roman Yurchak`_.
 
+- Fixed bug in :class:`feature_extraction.text.TFIDFVectorizer` which 
+  was ignoring the parameter ``dtype``. In addition,
+  :class:`feature_extraction.text.TFIDFTransformer` will preserve ``dtype``
+  for floating and raise a warning if ``dtype`` requested is integer.
+  :issue:`10441` by :user:`Mayur Kulkarni <maykulkarni>` and
+  :user:`Guillaume Lemaitre <glemaitre>`.
+  
 Utils
 
-- :func:`utils.validation.check_array` yield a ``FutureWarning`` indicating
+- :func:`utils.check_array` yield a ``FutureWarning`` indicating
   that arrays of bytes/strings will be interpreted as decimal numbers
   beginning in version 0.22. :issue:`10229` by :user:`Ryan Lee <rtlee9>`
 
@@ -486,10 +594,23 @@ Preprocessing
   ``inverse_transform`` on unseen labels. :issue:`9816` by :user:`Charlie Newey
   <newey01c>`.
 
+- Fix bug in :class:`preprocessing.OneHotEncoder` which discarded the ``dtype``
+  when returning a sparse matrix output. :issue:`11042` by :user:`Daniel
+  Morales <DanielMorales9>`.
+
+- Fix ``fit`` and ``partial_fit`` in :class:`preprocessing.StandardScaler` in
+  the rare case when `with_mean=False` and `with_std=False` which was crashing
+  by calling ``fit`` more than once and giving inconsistent results for
+  ``mean_`` whether the input was a sparse or a dense matrix. ``mean_`` will be
+  set to ``None`` with both sparse and dense inputs. ``n_samples_seen_`` will
+  be also reported for both input types.
+  :issue:`11235` by :user:`Guillaume Lemaitre <glemaitre>`.
+
 Feature selection
 
-- Fixed computation of ``n_features_to_compute`` for edge case with tied
-  CV scores in :class:`RFECV`. :issue:`9222` by `Nick Hoh <nickypie>`.
+- Fixed computation of ``n_features_to_compute`` for edge case with tied CV
+  scores in :class:`feature_selection.RFECV`. :issue:`9222` by `Nick Hoh
+  <nickypie>`.
 
 Model evaluation and meta-estimators
 
@@ -499,10 +620,10 @@ Model evaluation and meta-estimators
 
 Datasets
 
-- Fixed a bug in :func:`dataset.load_boston` which had a wrong data point.
+- Fixed a bug in :func:`datasets.load_boston` which had a wrong data point.
   :issue:`10801` by :user:`Takeshi Yoshizawa <tarcusx>`.
 
-- Fixed a bug in :func:`dataset.load_iris` which had two wrong data points.
+- Fixed a bug in :func:`datasets.load_iris` which had two wrong data points.
   :issue:`11082` by :user:`Sadhana Srinivasan <rotuna>`
   and :user:`Hanmin Qin <qinhanmin2014>`.
 
@@ -527,15 +648,26 @@ Linear, kernelized and related models
   performed. :issue:`10723` by `Joel Nothman`_.
 
 - The default value of ``gamma`` parameter of :class:`svm.SVC`,
-  :class:`svm.NuSVC`, :class:`svm.SVR`, :class:`NuSVR`, :class:`OneClassSVM`
-  will change from ``'auto'`` to ``'scale'`` in version 0.22 to account
-  better for unscaled features. :issue:`8361` by :user:`Gaurav Dhingra <gxyd>`
-  and :user:`Ting Neo <neokt>`.
+  :class:`~svm.NuSVC`, :class:`~svm.SVR`, :class:`~svm.NuSVR`,
+  :class:`~svm.OneClassSVM` will change from ``'auto'`` to ``'scale'`` in
+  version 0.22 to account better for unscaled features. :issue:`8361` by
+  :user:`Gaurav Dhingra <gxyd>` and :user:`Ting Neo <neokt>`.
 
 - Added convergence warning to :class:`svm.LinearSVC` and
-  :class:`linear_model.logistic.LogisticRegression` when ``verbose`` is set to 0.
+  :class:`linear_model.LogisticRegression` when ``verbose`` is set to 0.
   :issue:`10881` by :user:`Alexandre Sevin <AlexandreSev>`.
 
+Preprocessing
+
+- Deprecate ``n_values`` and ``categorical_features`` parameters and
+  ``active_features_``, ``feature_indices_`` and ``n_values_`` attributes
+  of :class:`preprocessing.OneHotEncoder`. The ``n_values`` parameter can be
+  replaced with the new ``categories`` parameter, and the attributes with the
+  new ``categories_`` attribute. Selecting the categorical features with
+  the ``categorical_features`` parameter is now better supported using the
+  :class:`compose.ColumnTransformer`.
+  :issue:`10521` by `Joris Van den Bossche`_.
+
 Decomposition, manifold learning and clustering
 
 - Deprecate ``precomputed`` parameter in function
@@ -570,12 +702,23 @@ Imputer
   :class:`impute.SimpleImputer`. :issue:`9726` by :user:`Kumar Ashutosh
   <thechargedneutron>`.
 
-- The ``axis`` parameter in :class:`impute.SimpleImputer` was removed. The
-  behavior is equivalent to ``axis=0`` (impute along columns). Row-wise
-  imputation can be performed with FunctionTransformer (e.g.,
-  ``FunctionTransformer(lambda X: Imputer().fit_transform(X.T).T)``).
-  :issue:`10829` by :user:`Guillaume Lemaitre <glemaitre>` and :user:`Gilberto
-  Olimpio <gilbertoolimpio>`.
+- The ``axis`` parameter that was in :class:`preprocessing.Imputer` is no
+  longer present in :class:`impute.SimpleImputer`. The behavior is equivalent
+  to ``axis=0`` (impute along columns). Row-wise imputation can be performed
+  with FunctionTransformer (e.g., ``FunctionTransformer(lambda X:
+  SimpleImputer().fit_transform(X.T).T)``). :issue:`10829` by :user:`Guillaume
+  Lemaitre <glemaitre>` and :user:`Gilberto Olimpio <gilbertoolimpio>`.
+
+- The :class:`impute.SimpleImputer` has a new strategy, ``'constant'``, to
+  complete missing values with a fixed one, given by the ``fill_value``
+  parameter. This strategy supports numeric and non-numeric data, and so does
+  the ``'most_frequent'`` strategy now. :issue:`11211` by :user:`Jeremie du
+  Boisberranger <jeremiedbb>`.
+
+- The NaN marker for the missing values has been changed between the
+  :class:`preprocessing.Imputer` and the :class:`impute.SimpleImputer`.
+  ``missing_values='NaN'``Â should now be ``missing_values=np.nan``.
+  :issue:`11211` by :user:`Jeremie du Boisberranger <jeremiedbb>`.
 
 Outlier Detection models
 
@@ -605,8 +748,9 @@ Covariance
 
 Misc
 
-- Changed warning type from UserWarning to ConvergenceWarning for failing
-  convergence in :func:`linear_model.logistic_regression_path`,
+- Changed warning type from :class:`UserWarning` to
+  :class:`exceptions.ConvergenceWarning` for failing convergence in
+  :func:`linear_model.logistic_regression_path`,
   :class:`linear_model.RANSACRegressor`, :func:`linear_model.ridge_regression`,
   :class:`gaussian_process.GaussianProcessRegressor`,
   :class:`gaussian_process.GaussianProcessClassifier`,
@@ -623,29 +767,33 @@ Misc
 - Invalid input for :class:`model_selection.ParameterGrid` now raises TypeError.
   :issue:`10928` by :user:`Solutus Immensus <solutusimmensus>`
 
+- :func:`utils.check_array` and :func:`utils.check_X_y` now have
+  ``accept_large_sparse`` to control whether scipy.sparse matrices with 64-bit
+  indices should be rejected.
+  :issue:`11327` by :user:`Karan Dhingra <kdhingra307>` and `Joel Nothman`_.
+
 Changes to estimator checks
 ---------------------------
 
-- Allow tests in :func:`estimator_checks.check_estimator` to test functions
+These changes mostly affect library developers.
+
+- Allow tests in :func:`utils.estimator_checks.check_estimator` to test functions
   that accept pairwise data.
   :issue:`9701` by :user:`Kyle Johnson <gkjohns>`
 
-- Allow :func:`estimator_checks.check_estimator` to check that there is no
+- Allow :func:`~utils.estimator_checks.check_estimator` to check that there is no
   private settings apart from parameters during estimator initialization.
   :issue:`9378` by :user:`Herilalaina Rakotoarison <herilalaina>`
 
-- Add test :func:`estimator_checks.check_methods_subset_invariance` to check
-  that estimators methods are invariant if applied to a data subset.
-  :issue:`10420` by :user:`Jonathan Ohayon <Johayon>`
-
-- Add invariance tests for clustering metrics. :issue:`8102` by :user:`Ankita
-  Sinha <anki08>` and :user:`Guillaume Lemaitre <glemaitre>`.
+- Add ``check_methods_subset_invariance`` to
+  :func:`~utils.estimator_checks.check_estimator`, which checks that
+  estimator methods are invariant if applied to a data subset.  :issue:`10420`
+  by :user:`Jonathan Ohayon <Johayon>`
 
-- Add tests in :func:`estimator_checks.check_estimator` to check that an
+- Add tests in :func:`utils.estimator_checks.check_estimator` to check that an
   estimator can handle read-only memmap input data. :issue:`10663` by
   :user:`Arthur Mensch <arthurmensch>` and :user:`LoÃ¯c EstÃ¨ve <lesteve>`.
 
-- :func:`estimator_checks.check_sample_weights_pandas_series` now uses 8 rather
-  than 6 samples to accommodate for the default number of clusters in
-  :class:`cluster.KMeans`.
+- ``check_sample_weights_pandas_series`` now uses 8 rather than 6 samples
+  to accommodate for the default number of clusters in :class:`cluster.KMeans`.
   :issue:`10933` by :user:`Johannes Hansen <jnhansen>`.
diff --git a/examples/README.txt b/examples/README.txt
index 33cfd2e6101b..4ee6efc46d1d 100644
--- a/examples/README.txt
+++ b/examples/README.txt
@@ -4,6 +4,6 @@ Examples
 ========
 
 Miscellaneous examples
-----------------
+----------------------
 
 Miscellaneous and introductory examples for scikit-learn.
diff --git a/examples/calibration/plot_compare_calibration.py b/examples/calibration/plot_compare_calibration.py
index bc1f73a06eb1..2d9d0af0dcbc 100644
--- a/examples/calibration/plot_compare_calibration.py
+++ b/examples/calibration/plot_compare_calibration.py
@@ -21,8 +21,8 @@
 * RandomForestClassifier shows the opposite behavior: the histograms show
   peaks at approx. 0.2 and 0.9 probability, while probabilities close to 0 or 1
   are very rare. An explanation for this is given by Niculescu-Mizil and Caruana
-  [1]: "Methods such as bagging and random forests that average predictions from
-  a base set of models can have difficulty making predictions near 0 and 1
+  [1]_: "Methods such as bagging and random forests that average predictions
+  from a base set of models can have difficulty making predictions near 0 and 1
   because variance in the underlying base models will bias predictions that
   should be near zero or one away from these values. Because predictions are
   restricted to the interval [0,1], errors caused by variance tend to be one-
@@ -39,7 +39,7 @@
 
 * Support Vector Classification (SVC) shows an even more sigmoid curve as
   the  RandomForestClassifier, which is typical for maximum-margin methods
-  (compare Niculescu-Mizil and Caruana [1]), which focus on hard samples
+  (compare Niculescu-Mizil and Caruana [1]_), which focus on hard samples
   that are close to the decision boundary (the support vectors).
 
 .. topic:: References:
diff --git a/examples/compose/README.txt b/examples/compose/README.txt
new file mode 100644
index 000000000000..9330b5a71419
--- /dev/null
+++ b/examples/compose/README.txt
@@ -0,0 +1,6 @@
+.. _compose_examples:
+
+Pipelines and composite estimators
+----------------------------------
+
+Examples of how to compose transformers and pipelines from other estimators. See the :ref:`User Guide <combining_estimators>`.
diff --git a/examples/hetero_feature_union.py b/examples/compose/plot_column_transformer.py
similarity index 59%
rename from examples/hetero_feature_union.py
rename to examples/compose/plot_column_transformer.py
index 0af24a3c05c3..0161af7fe7e4 100644
--- a/examples/hetero_feature_union.py
+++ b/examples/compose/plot_column_transformer.py
@@ -1,7 +1,7 @@
 """
-=============================================
-Feature Union with Heterogeneous Data Sources
-=============================================
+==================================================
+Column Transformer with Heterogeneous Data Sources
+==================================================
 
 Datasets can often contain components of that require different feature
 extraction and processing pipelines.  This scenario might occur when:
@@ -12,12 +12,12 @@
    require different processing pipelines.
 
 This example demonstrates how to use
-:class:`sklearn.feature_extraction.FeatureUnion` on a dataset containing
+:class:`sklearn.compose.ColumnTransformer` on a dataset containing
 different types of features.  We use the 20-newsgroups dataset and compute
 standard bag-of-words features for the subject line and body in separate
 pipelines as well as ad hoc features on the body. We combine them (with
-weights) using a FeatureUnion and finally train a classifier on the combined
-set of features.
+weights) using a ColumnTransformer and finally train a classifier on the
+combined set of features.
 
 The choice of features is not particularly helpful, but serves to illustrate
 the technique.
@@ -38,50 +38,11 @@
 from sklearn.feature_extraction import DictVectorizer
 from sklearn.feature_extraction.text import TfidfVectorizer
 from sklearn.metrics import classification_report
-from sklearn.pipeline import FeatureUnion
 from sklearn.pipeline import Pipeline
+from sklearn.compose import ColumnTransformer
 from sklearn.svm import SVC
 
 
-class ItemSelector(BaseEstimator, TransformerMixin):
-    """For data grouped by feature, select subset of data at a provided key.
-
-    The data is expected to be stored in a 2D data structure, where the first
-    index is over features and the second is over samples.  i.e.
-
-    >> len(data[key]) == n_samples
-
-    Please note that this is the opposite convention to scikit-learn feature
-    matrixes (where the first index corresponds to sample).
-
-    ItemSelector only requires that the collection implement getitem
-    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas
-    DataFrame, numpy record array, etc.
-
-    >> data = {'a': [1, 5, 2, 5, 2, 8],
-               'b': [9, 4, 1, 4, 1, 3]}
-    >> ds = ItemSelector(key='a')
-    >> data['a'] == ds.transform(data)
-
-    ItemSelector is not designed to handle data grouped by sample.  (e.g. a
-    list of dicts).  If your data is structured this way, consider a
-    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.
-
-    Parameters
-    ----------
-    key : hashable, required
-        The key corresponding to the desired value in a mappable.
-    """
-    def __init__(self, key):
-        self.key = key
-
-    def fit(self, x, y=None):
-        return self
-
-    def transform(self, data_dict):
-        return data_dict[self.key]
-
-
 class TextStats(BaseEstimator, TransformerMixin):
     """Extract features from each document for DictVectorizer"""
 
@@ -104,13 +65,14 @@ def fit(self, x, y=None):
         return self
 
     def transform(self, posts):
-        features = np.recarray(shape=(len(posts),),
-                               dtype=[('subject', object), ('body', object)])
+        # construct object dtype array with two columns
+        # first column = 'subject' and second column = 'body'
+        features = np.empty(shape=(len(posts), 2), dtype=object)
         for i, text in enumerate(posts):
             headers, _, bod = text.partition('\n\n')
             bod = strip_newsgroup_footer(bod)
             bod = strip_newsgroup_quoting(bod)
-            features['body'][i] = bod
+            features[i, 1] = bod
 
             prefix = 'Subject:'
             sub = ''
@@ -118,7 +80,7 @@ def transform(self, posts):
                 if line.startswith(prefix):
                     sub = line[len(prefix):]
                     break
-            features['subject'][i] = sub
+            features[i, 0] = sub
 
         return features
 
@@ -127,38 +89,31 @@ def transform(self, posts):
     # Extract the subject & body
     ('subjectbody', SubjectBodyExtractor()),
 
-    # Use FeatureUnion to combine the features from subject and body
-    ('union', FeatureUnion(
-        transformer_list=[
+    # Use C toolumnTransformer to combine the features from subject and body
+    ('union', ColumnTransformer(
+        [
+            # Pulling features from the post's subject line (first column)
+            ('subject', TfidfVectorizer(min_df=50), 0),
 
-            # Pipeline for pulling features from the post's subject line
-            ('subject', Pipeline([
-                ('selector', ItemSelector(key='subject')),
-                ('tfidf', TfidfVectorizer(min_df=50)),
-            ])),
-
-            # Pipeline for standard bag-of-words model for body
+            # Pipeline for standard bag-of-words model for body (second column)
             ('body_bow', Pipeline([
-                ('selector', ItemSelector(key='body')),
                 ('tfidf', TfidfVectorizer()),
                 ('best', TruncatedSVD(n_components=50)),
-            ])),
+            ]), 1),
 
             # Pipeline for pulling ad hoc features from post's body
             ('body_stats', Pipeline([
-                ('selector', ItemSelector(key='body')),
                 ('stats', TextStats()),  # returns a list of dicts
                 ('vect', DictVectorizer()),  # list of dicts -> feature matrix
-            ])),
-
+            ]), 1),
         ],
 
-        # weight components in FeatureUnion
+        # weight components in ColumnTransformer
         transformer_weights={
             'subject': 0.8,
             'body_bow': 0.5,
             'body_stats': 1.0,
-        },
+        }
     )),
 
     # Use a SVC classifier on the combined features
diff --git a/examples/compose/plot_column_transformer_mixed_types.py b/examples/compose/plot_column_transformer_mixed_types.py
new file mode 100644
index 000000000000..64f1a3c88d3d
--- /dev/null
+++ b/examples/compose/plot_column_transformer_mixed_types.py
@@ -0,0 +1,107 @@
+"""
+===================================
+Column Transformer with Mixed Types
+===================================
+
+This example illustrates how to apply different preprocessing and
+feature extraction pipelines to different subsets of features,
+using :class:`sklearn.compose.ColumnTransformer`.
+This is particularly handy for the case of datasets that contain
+heterogeneous data types, since we may want to scale the
+numeric features and one-hot encode the categorical ones.
+
+In this example, the numeric data is standard-scaled after
+mean-imputation, while the categorical data is one-hot
+encoded after imputing missing values with a new category
+(``'missing'``).
+
+Finally, the preprocessing pipeline is integrated in a
+full prediction pipeline using :class:`sklearn.pipeline.Pipeline`,
+together with a simple classification model.
+"""
+
+# Author: Pedro Morales <part.morales@gmail.com>
+#
+# License: BSD 3 clause
+
+from __future__ import print_function
+
+import pandas as pd
+import numpy as np
+
+from sklearn.compose import ColumnTransformer
+from sklearn.pipeline import Pipeline
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler, OneHotEncoder
+from sklearn.linear_model import LogisticRegression
+from sklearn.model_selection import train_test_split, GridSearchCV
+
+np.random.seed(0)
+
+# Read data from Titanic dataset.
+titanic_url = ('https://raw.githubusercontent.com/amueller/'
+               'scipy-2017-sklearn/091d371/notebooks/datasets/titanic3.csv')
+data = pd.read_csv(titanic_url)
+
+# We will train our classifier with the following features:
+# Numeric Features:
+# - age: float.
+# - fare: float.
+# Categorical Features:
+# - embarked: categories encoded as strings {'C', 'S', 'Q'}.
+# - sex: categories encoded as strings {'female', 'male'}.
+# - pclass: ordinal integers {1, 2, 3}.
+
+# We create the preprocessing pipelines for both numeric and categorical data.
+numeric_features = ['age', 'fare']
+numeric_transformer = Pipeline(steps=[
+    ('imputer', SimpleImputer(strategy='median')),
+    ('scaler', StandardScaler())])
+
+categorical_features = ['embarked', 'sex', 'pclass']
+categorical_transformer = Pipeline(steps=[
+    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
+    ('onehot', OneHotEncoder(sparse=False, handle_unknown='ignore'))])
+
+preprocessor = ColumnTransformer(
+    transformers=[
+        ('num', numeric_transformer, numeric_features),
+        ('cat', categorical_transformer, categorical_features)],
+    remainder='drop')
+
+# Append classifier to preprocessing pipeline.
+# Now we have a full prediction pipeline.
+clf = Pipeline(steps=[('preprocessor', preprocessor),
+                      ('classifier', LogisticRegression())])
+
+X = data.drop('survived', axis=1)
+y = data['survived']
+
+X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
+                                                    shuffle=True)
+
+clf.fit(X_train, y_train)
+print("model score: %.3f" % clf.score(X_test, y_test))
+
+
+###############################################################################
+# Using the prediction pipeline in a grid search
+###############################################################################
+# Grid search can also be performed on the different preprocessing steps
+# defined in the ``ColumnTransformer`` object, together with the classifier's
+# hyperparameters as part of the ``Pipeline``.
+# We will search for both the imputer strategy of the numeric preprocessing
+# and the regularization parameter of the logistic regression using
+# :class:`sklearn.model_selection.GridSearchCV`.
+
+
+param_grid = {
+    'preprocessor__num__imputer__strategy': ['mean', 'median'],
+    'classifier__C': [0.1, 1.0, 10, 100],
+}
+
+grid_search = GridSearchCV(clf, param_grid, cv=10, iid=False)
+grid_search.fit(X_train, y_train)
+
+print(("best logistic regression from grid search: %.3f"
+       % grid_search.score(X_test, y_test)))
diff --git a/examples/plot_compare_reduction.py b/examples/compose/plot_compare_reduction.py
similarity index 100%
rename from examples/plot_compare_reduction.py
rename to examples/compose/plot_compare_reduction.py
diff --git a/examples/plot_digits_pipe.py b/examples/compose/plot_digits_pipe.py
similarity index 100%
rename from examples/plot_digits_pipe.py
rename to examples/compose/plot_digits_pipe.py
diff --git a/examples/plot_feature_stacker.py b/examples/compose/plot_feature_union.py
similarity index 100%
rename from examples/plot_feature_stacker.py
rename to examples/compose/plot_feature_union.py
diff --git a/examples/preprocessing/plot_transformed_target.py b/examples/compose/plot_transformed_target.py
similarity index 100%
rename from examples/preprocessing/plot_transformed_target.py
rename to examples/compose/plot_transformed_target.py
diff --git a/examples/decomposition/plot_faces_decomposition.py b/examples/decomposition/plot_faces_decomposition.py
index d29af6ad408f..9c2144a77942 100644
--- a/examples/decomposition/plot_faces_decomposition.py
+++ b/examples/decomposition/plot_faces_decomposition.py
@@ -48,13 +48,13 @@
 print("Dataset consists of %d faces" % n_samples)
 
 
-def plot_gallery(title, images, n_col=n_col, n_row=n_row):
+def plot_gallery(title, images, n_col=n_col, n_row=n_row, cmap=plt.cm.gray):
     plt.figure(figsize=(2. * n_col, 2.26 * n_row))
     plt.suptitle(title, size=16)
     for i, comp in enumerate(images):
         plt.subplot(n_row, n_col, i + 1)
         vmax = max(comp.max(), -comp.min())
-        plt.imshow(comp.reshape(image_shape), cmap=plt.cm.gray,
+        plt.imshow(comp.reshape(image_shape), cmap=cmap,
                    interpolation='nearest',
                    vmin=-vmax, vmax=vmax)
         plt.xticks(())
@@ -137,3 +137,56 @@ def plot_gallery(title, images, n_col=n_col, n_row=n_row):
                  components_[:n_components])
 
 plt.show()
+
+# #############################################################################
+# Various positivity constraints applied to dictionary learning.
+estimators = [
+    ('Dictionary learning',
+        decomposition.MiniBatchDictionaryLearning(n_components=15, alpha=0.1,
+                                                  n_iter=50, batch_size=3,
+                                                  random_state=rng),
+     True),
+    ('Dictionary learning - positive dictionary',
+        decomposition.MiniBatchDictionaryLearning(n_components=15, alpha=0.1,
+                                                  n_iter=50, batch_size=3,
+                                                  random_state=rng,
+                                                  positive_dict=True),
+     True),
+    ('Dictionary learning - positive code',
+        decomposition.MiniBatchDictionaryLearning(n_components=15, alpha=0.1,
+                                                  n_iter=50, batch_size=3,
+                                                  random_state=rng,
+                                                  positive_code=True),
+     True),
+    ('Dictionary learning - positive dictionary & code',
+        decomposition.MiniBatchDictionaryLearning(n_components=15, alpha=0.1,
+                                                  n_iter=50, batch_size=3,
+                                                  random_state=rng,
+                                                  positive_dict=True,
+                                                  positive_code=True),
+     True),
+]
+
+
+# #############################################################################
+# Plot a sample of the input data
+
+plot_gallery("First centered Olivetti faces", faces_centered[:n_components],
+             cmap=plt.cm.RdBu)
+
+# #############################################################################
+# Do the estimation and plot it
+
+for name, estimator, center in estimators:
+    print("Extracting the top %d %s..." % (n_components, name))
+    t0 = time()
+    data = faces
+    if center:
+        data = faces_centered
+    estimator.fit(data)
+    train_time = (time() - t0)
+    print("done in %0.3fs" % train_time)
+    components_ = estimator.components_
+    plot_gallery(name, components_[:n_components], cmap=plt.cm.RdBu)
+
+plt.show()
diff --git a/examples/ensemble/plot_feature_transformation.py b/examples/ensemble/plot_feature_transformation.py
index e4a6a8b313d8..bb1fd7ec6684 100644
--- a/examples/ensemble/plot_feature_transformation.py
+++ b/examples/ensemble/plot_feature_transformation.py
@@ -34,7 +34,7 @@
 from sklearn.linear_model import LogisticRegression
 from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,
                               GradientBoostingClassifier)
-from sklearn.preprocessing import CategoricalEncoder
+from sklearn.preprocessing import OneHotEncoder
 from sklearn.model_selection import train_test_split
 from sklearn.metrics import roc_curve
 from sklearn.pipeline import make_pipeline
@@ -62,7 +62,7 @@
 
 # Supervised transformation based on random forests
 rf = RandomForestClassifier(max_depth=3, n_estimators=n_estimator)
-rf_enc = CategoricalEncoder()
+rf_enc = OneHotEncoder()
 rf_lm = LogisticRegression()
 rf.fit(X_train, y_train)
 rf_enc.fit(rf.apply(X_train))
@@ -72,7 +72,7 @@
 fpr_rf_lm, tpr_rf_lm, _ = roc_curve(y_test, y_pred_rf_lm)
 
 grd = GradientBoostingClassifier(n_estimators=n_estimator)
-grd_enc = CategoricalEncoder()
+grd_enc = OneHotEncoder()
 grd_lm = LogisticRegression()
 grd.fit(X_train, y_train)
 grd_enc.fit(grd.apply(X_train)[:, :, 0])
diff --git a/examples/ensemble/plot_gradient_boosting_early_stopping.py b/examples/ensemble/plot_gradient_boosting_early_stopping.py
index 366d9e0b148d..0ffa36cd0b15 100644
--- a/examples/ensemble/plot_gradient_boosting_early_stopping.py
+++ b/examples/ensemble/plot_gradient_boosting_early_stopping.py
@@ -131,7 +131,7 @@ def autolabel(rects, n_estimators):
 
 #######################################################################
 # Compare fit times with and without early stopping
-# ----------------------------------------------
+# -------------------------------------------------
 
 plt.figure(figsize=(9, 5))
 
diff --git a/examples/ensemble/plot_isolation_forest.py b/examples/ensemble/plot_isolation_forest.py
index 6dc8aacd462a..e6c8ce3bf8ef 100644
--- a/examples/ensemble/plot_isolation_forest.py
+++ b/examples/ensemble/plot_isolation_forest.py
@@ -3,7 +3,8 @@
 IsolationForest example
 ==========================================
 
-An example using IsolationForest for anomaly detection.
+An example using :class:`sklearn.ensemble.IsolationForest` for anomaly
+detection.
 
 The IsolationForest 'isolates' observations by randomly selecting a feature
 and then randomly selecting a split value between the maximum and minimum
@@ -20,9 +21,6 @@
 Hence, when a forest of random trees collectively produce shorter path lengths
 for particular samples, they are highly likely to be anomalies.
 
-.. [1] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. "Isolation forest."
-    Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.
-
 """
 print(__doc__)
 
diff --git a/examples/manifold/plot_compare_methods.py b/examples/manifold/plot_compare_methods.py
index 34e161dfb046..3af18269aeaa 100644
--- a/examples/manifold/plot_compare_methods.py
+++ b/examples/manifold/plot_compare_methods.py
@@ -1,6 +1,6 @@
 """
 =========================================
- Comparison of Manifold Learning methods
+Comparison of Manifold Learning methods
 =========================================
 
 An illustration of dimensionality reduction on the S-curve dataset
diff --git a/examples/manifold/plot_t_sne_perplexity.py b/examples/manifold/plot_t_sne_perplexity.py
index c1cbe0001d0e..0fbade5746af 100644
--- a/examples/manifold/plot_t_sne_perplexity.py
+++ b/examples/manifold/plot_t_sne_perplexity.py
@@ -1,6 +1,6 @@
 """
 =============================================================================
- t-SNE: The effect of various perplexity values on the shape
+t-SNE: The effect of various perplexity values on the shape
 =============================================================================
 
 An illustration of t-SNE on the two concentric circles and the S-curve
diff --git a/examples/model_selection/plot_nested_cross_validation_iris.py b/examples/model_selection/plot_nested_cross_validation_iris.py
index 917746c359d4..b40dc91fc4d8 100644
--- a/examples/model_selection/plot_nested_cross_validation_iris.py
+++ b/examples/model_selection/plot_nested_cross_validation_iris.py
@@ -75,7 +75,7 @@
 
     # Choose cross-validation techniques for the inner and outer loops,
     # independently of the dataset.
-    # E.g "LabelKFold", "LeaveOneOut", "LeaveOneLabelOut", etc.
+    # E.g "GroupKFold", "LeaveOneOut", "LeaveOneGroupOut", etc.
     inner_cv = KFold(n_splits=4, shuffle=True, random_state=i)
     outer_cv = KFold(n_splits=4, shuffle=True, random_state=i)
 
diff --git a/examples/plot_missing_values.py b/examples/plot_missing_values.py
index e32c19fae084..d238a16592ed 100644
--- a/examples/plot_missing_values.py
+++ b/examples/plot_missing_values.py
@@ -8,10 +8,11 @@
 The median is a more robust estimator for data with high magnitude variables
 which could dominate results (otherwise known as a 'long tail').
 
-Another option is the MICE imputer. This uses round-robin linear regression,
-treating every variable as an output in turn. The version implemented assumes
-Gaussian (output) variables. If your features are obviously non-Normal,
-consider transforming them to look more Normal so as to improve performance.
+Another option is the ``ChainedImputer``. This uses round-robin linear
+regression, treating every variable as an output in turn. The version
+implemented assumes Gaussian (output) variables. If your features are obviously
+non-Normal, consider transforming them to look more Normal so as to improve
+performance.
 """
 
 import numpy as np
@@ -21,7 +22,7 @@
 from sklearn.datasets import load_boston
 from sklearn.ensemble import RandomForestRegressor
 from sklearn.pipeline import Pipeline
-from sklearn.impute import SimpleImputer, MICEImputer
+from sklearn.impute import SimpleImputer, ChainedImputer
 from sklearn.model_selection import cross_val_score
 
 rng = np.random.RandomState(0)
@@ -66,18 +67,18 @@ def get_results(dataset):
     mean_impute_scores = cross_val_score(estimator, X_missing, y_missing,
                                          scoring='neg_mean_squared_error')
 
-    # Estimate the score after imputation (MICE strategy) of the missing values
-    estimator = Pipeline([("imputer", MICEImputer(missing_values=0,
-                                                  random_state=0)),
+    # Estimate the score after chained imputation of the missing values
+    estimator = Pipeline([("imputer", ChainedImputer(missing_values=0,
+                                                     random_state=0)),
                           ("forest", RandomForestRegressor(random_state=0,
                                                            n_estimators=100))])
-    mice_impute_scores = cross_val_score(estimator, X_missing, y_missing,
-                                         scoring='neg_mean_squared_error')
+    chained_impute_scores = cross_val_score(estimator, X_missing, y_missing,
+                                            scoring='neg_mean_squared_error')
 
     return ((full_scores.mean(), full_scores.std()),
             (zero_impute_scores.mean(), zero_impute_scores.std()),
             (mean_impute_scores.mean(), mean_impute_scores.std()),
-            (mice_impute_scores.mean(), mice_impute_scores.std()))
+            (chained_impute_scores.mean(), chained_impute_scores.std()))
 
 
 results_diabetes = np.array(get_results(load_diabetes()))
@@ -94,7 +95,7 @@ def get_results(dataset):
 x_labels = ['Full data',
             'Zero imputation',
             'Mean Imputation',
-            'MICE Imputation']
+            'Chained Imputation']
 colors = ['r', 'g', 'b', 'orange']
 
 # plot diabetes results
@@ -104,7 +105,7 @@ def get_results(dataset):
     ax1.barh(j, mses_diabetes[j], xerr=stds_diabetes[j],
              color=colors[j], alpha=0.6, align='center')
 
-ax1.set_title('Feature Selection Techniques with Diabetes Data')
+ax1.set_title('Imputation Techniques with Diabetes Data')
 ax1.set_xlim(left=np.min(mses_diabetes) * 0.9,
              right=np.max(mses_diabetes) * 1.1)
 ax1.set_yticks(xval)
@@ -118,7 +119,7 @@ def get_results(dataset):
     ax2.barh(j, mses_boston[j], xerr=stds_boston[j],
              color=colors[j], alpha=0.6, align='center')
 
-ax2.set_title('Feature Selection Techniques with Boston Data')
+ax2.set_title('Imputation Techniques with Boston Data')
 ax2.set_yticks(xval)
 ax2.set_xlabel('MSE')
 ax2.invert_yaxis()
diff --git a/examples/svm/plot_separating_hyperplane.py b/examples/svm/plot_separating_hyperplane.py
index 9fdbcc785ed2..cbd61abad53e 100644
--- a/examples/svm/plot_separating_hyperplane.py
+++ b/examples/svm/plot_separating_hyperplane.py
@@ -41,5 +41,5 @@
            linestyles=['--', '-', '--'])
 # plot support vectors
 ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,
-           linewidth=1, facecolors='none')
+           linewidth=1, facecolors='none', edgecolors='k')
 plt.show()
diff --git a/examples/svm/plot_separating_hyperplane_unbalanced.py b/examples/svm/plot_separating_hyperplane_unbalanced.py
index 05c768c4a22f..2a0540fead31 100644
--- a/examples/svm/plot_separating_hyperplane_unbalanced.py
+++ b/examples/svm/plot_separating_hyperplane_unbalanced.py
@@ -49,9 +49,8 @@
 wclf = svm.SVC(kernel='linear', class_weight={1: 10})
 wclf.fit(X, y)
 
-# plot separating hyperplanes and samples
+# plot the samples
 plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='k')
-plt.legend()
 
 # plot the decision functions for both classifiers
 ax = plt.gca()
diff --git a/sklearn/__init__.py b/sklearn/__init__.py
index dbb7862d8839..36fb3afdc587 100644
--- a/sklearn/__init__.py
+++ b/sklearn/__init__.py
@@ -65,11 +65,10 @@
     __check_build  # avoid flakes unused variable error
 
     __all__ = ['calibration', 'cluster', 'covariance', 'cross_decomposition',
-               'cross_validation', 'datasets', 'decomposition', 'dummy',
-               'ensemble', 'exceptions', 'externals', 'feature_extraction',
-               'feature_selection', 'gaussian_process', 'grid_search',
-               'isotonic', 'kernel_approximation', 'kernel_ridge',
-               'learning_curve', 'linear_model', 'manifold', 'metrics',
+               'datasets', 'decomposition', 'dummy', 'ensemble', 'exceptions',
+               'externals', 'feature_extraction', 'feature_selection',
+               'gaussian_process', 'isotonic', 'kernel_approximation',
+               'kernel_ridge', 'linear_model', 'manifold', 'metrics',
                'mixture', 'model_selection', 'multiclass', 'multioutput',
                'naive_bayes', 'neighbors', 'neural_network', 'pipeline',
                'preprocessing', 'random_projection', 'semi_supervised',
diff --git a/sklearn/base.py b/sklearn/base.py
index f62c0308fb56..d75adb06d61b 100644
--- a/sklearn/base.py
+++ b/sklearn/base.py
@@ -67,57 +67,10 @@ def clone(estimator, safe=True):
     for name in new_object_params:
         param1 = new_object_params[name]
         param2 = params_set[name]
-        if param1 is param2:
-            # this should always happen
-            continue
-        if isinstance(param1, np.ndarray):
-            # For most ndarrays, we do not test for complete equality
-            if not isinstance(param2, type(param1)):
-                equality_test = False
-            elif (param1.ndim > 0
-                    and param1.shape[0] > 0
-                    and isinstance(param2, np.ndarray)
-                    and param2.ndim > 0
-                    and param2.shape[0] > 0):
-                equality_test = (
-                    param1.shape == param2.shape
-                    and param1.dtype == param2.dtype
-                    and (_first_and_last_element(param1) ==
-                         _first_and_last_element(param2))
-                )
-            else:
-                equality_test = np.all(param1 == param2)
-        elif sparse.issparse(param1):
-            # For sparse matrices equality doesn't work
-            if not sparse.issparse(param2):
-                equality_test = False
-            elif param1.size == 0 or param2.size == 0:
-                equality_test = (
-                    param1.__class__ == param2.__class__
-                    and param1.size == 0
-                    and param2.size == 0
-                )
-            else:
-                equality_test = (
-                    param1.__class__ == param2.__class__
-                    and (_first_and_last_element(param1) ==
-                         _first_and_last_element(param2))
-                    and param1.nnz == param2.nnz
-                    and param1.shape == param2.shape
-                )
-        else:
-            # fall back on standard equality
-            equality_test = param1 == param2
-        if equality_test:
-            warnings.warn("Estimator %s modifies parameters in __init__."
-                          " This behavior is deprecated as of 0.18 and "
-                          "support for this behavior will be removed in 0.20."
-                          % type(estimator).__name__, DeprecationWarning)
-        else:
+        if param1 is not param2:
             raise RuntimeError('Cannot clone object %s, as the constructor '
-                               'does not seem to set parameter %s' %
+                               'either does not set or modifies parameter %s' %
                                (estimator, name))
-
     return new_object
 
 
@@ -354,7 +307,10 @@ def score(self, X, y, sample_weight=None):
         Parameters
         ----------
         X : array-like, shape = (n_samples, n_features)
-            Test samples.
+            Test samples. For some estimators this may be a
+            precomputed kernel matrix instead, shape = (n_samples,
+            n_samples_fitted], where n_samples_fitted is the number of
+            samples used in the fitting for the estimator.
 
         y : array-like, shape = (n_samples) or (n_samples, n_outputs)
             True values for X.
diff --git a/sklearn/cluster/tests/test_dbscan.py b/sklearn/cluster/tests/test_dbscan.py
index 1dee674e49af..f2d6c5836db8 100644
--- a/sklearn/cluster/tests/test_dbscan.py
+++ b/sklearn/cluster/tests/test_dbscan.py
@@ -9,6 +9,8 @@
 from scipy.spatial import distance
 from scipy import sparse
 
+import pytest
+
 from sklearn.utils.testing import assert_equal
 from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_raises
@@ -306,38 +308,38 @@ def test_weighted_dbscan():
     assert_array_equal(label1, est.labels_)
 
 
-def test_dbscan_core_samples_toy():
+@pytest.mark.parametrize('algorithm', ['brute', 'kd_tree', 'ball_tree'])
+def test_dbscan_core_samples_toy(algorithm):
     X = [[0], [2], [3], [4], [6], [8], [10]]
     n_samples = len(X)
 
-    for algorithm in ['brute', 'kd_tree', 'ball_tree']:
-        # Degenerate case: every sample is a core sample, either with its own
-        # cluster or including other close core samples.
-        core_samples, labels = dbscan(X, algorithm=algorithm, eps=1,
-                                      min_samples=1)
-        assert_array_equal(core_samples, np.arange(n_samples))
-        assert_array_equal(labels, [0, 1, 1, 1, 2, 3, 4])
-
-        # With eps=1 and min_samples=2 only the 3 samples from the denser area
-        # are core samples. All other points are isolated and considered noise.
-        core_samples, labels = dbscan(X, algorithm=algorithm, eps=1,
-                                      min_samples=2)
-        assert_array_equal(core_samples, [1, 2, 3])
-        assert_array_equal(labels, [-1, 0, 0, 0, -1, -1, -1])
-
-        # Only the sample in the middle of the dense area is core. Its two
-        # neighbors are edge samples. Remaining samples are noise.
-        core_samples, labels = dbscan(X, algorithm=algorithm, eps=1,
-                                      min_samples=3)
-        assert_array_equal(core_samples, [2])
-        assert_array_equal(labels, [-1, 0, 0, 0, -1, -1, -1])
-
-        # It's no longer possible to extract core samples with eps=1:
-        # everything is noise.
-        core_samples, labels = dbscan(X, algorithm=algorithm, eps=1,
-                                      min_samples=4)
-        assert_array_equal(core_samples, [])
-        assert_array_equal(labels, -np.ones(n_samples))
+    # Degenerate case: every sample is a core sample, either with its own
+    # cluster or including other close core samples.
+    core_samples, labels = dbscan(X, algorithm=algorithm, eps=1,
+                                  min_samples=1)
+    assert_array_equal(core_samples, np.arange(n_samples))
+    assert_array_equal(labels, [0, 1, 1, 1, 2, 3, 4])
+
+    # With eps=1 and min_samples=2 only the 3 samples from the denser area
+    # are core samples. All other points are isolated and considered noise.
+    core_samples, labels = dbscan(X, algorithm=algorithm, eps=1,
+                                  min_samples=2)
+    assert_array_equal(core_samples, [1, 2, 3])
+    assert_array_equal(labels, [-1, 0, 0, 0, -1, -1, -1])
+
+    # Only the sample in the middle of the dense area is core. Its two
+    # neighbors are edge samples. Remaining samples are noise.
+    core_samples, labels = dbscan(X, algorithm=algorithm, eps=1,
+                                  min_samples=3)
+    assert_array_equal(core_samples, [2])
+    assert_array_equal(labels, [-1, 0, 0, 0, -1, -1, -1])
+
+    # It's no longer possible to extract core samples with eps=1:
+    # everything is noise.
+    core_samples, labels = dbscan(X, algorithm=algorithm, eps=1,
+                                  min_samples=4)
+    assert_array_equal(core_samples, [])
+    assert_array_equal(labels, -np.ones(n_samples))
 
 
 def test_dbscan_precomputed_metric_with_degenerate_input_arrays():
diff --git a/sklearn/cluster/tests/test_hierarchical.py b/sklearn/cluster/tests/test_hierarchical.py
index 83ddc9729ceb..b3056b95d225 100644
--- a/sklearn/cluster/tests/test_hierarchical.py
+++ b/sklearn/cluster/tests/test_hierarchical.py
@@ -51,6 +51,7 @@ def test_deprecation_of_n_components_in_linkage_tree():
     assert_equal(n_leaves, n_leaves_t)
     assert_equal(parent, parent_t)
 
+
 def test_linkage_misc():
     # Misc tests on linkage
     rng = np.random.RandomState(42)
@@ -511,7 +512,8 @@ def test_connectivity_callable():
     connectivity = kneighbors_graph(X, 3, include_self=False)
     aglc1 = AgglomerativeClustering(connectivity=connectivity)
     aglc2 = AgglomerativeClustering(
-        connectivity=partial(kneighbors_graph, n_neighbors=3, include_self=False))
+        connectivity=partial(kneighbors_graph, n_neighbors=3,
+                             include_self=False))
     aglc1.fit(X)
     aglc2.fit(X)
     assert_array_equal(aglc1.labels_, aglc2.labels_)
diff --git a/sklearn/cluster/tests/test_k_means.py b/sklearn/cluster/tests/test_k_means.py
index 1c148c4abecc..526c33058fe9 100644
--- a/sklearn/cluster/tests/test_k_means.py
+++ b/sklearn/cluster/tests/test_k_means.py
@@ -4,6 +4,8 @@
 import numpy as np
 from scipy import sparse as sp
 
+import pytest
+
 from sklearn.utils.testing import assert_equal
 from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_array_almost_equal
@@ -274,30 +276,31 @@ def test_k_means_n_init():
     assert_raises_regex(ValueError, "n_init", KMeans(n_init=-1).fit, X)
 
 
-def test_k_means_explicit_init_shape():
+@pytest.mark.parametrize('Class', [KMeans, MiniBatchKMeans])
+def test_k_means_explicit_init_shape(Class):
     # test for sensible errors when giving explicit init
     # with wrong number of features or clusters
     rnd = np.random.RandomState(0)
     X = rnd.normal(size=(40, 3))
-    for Class in [KMeans, MiniBatchKMeans]:
-        # mismatch of number of features
-        km = Class(n_init=1, init=X[:, :2], n_clusters=len(X))
-        msg = "does not match the number of features of the data"
-        assert_raises_regex(ValueError, msg, km.fit, X)
-        # for callable init
-        km = Class(n_init=1,
-                   init=lambda X_, k, random_state: X_[:, :2],
-                   n_clusters=len(X))
-        assert_raises_regex(ValueError, msg, km.fit, X)
-        # mismatch of number of clusters
-        msg = "does not match the number of clusters"
-        km = Class(n_init=1, init=X[:2, :], n_clusters=3)
-        assert_raises_regex(ValueError, msg, km.fit, X)
-        # for callable init
-        km = Class(n_init=1,
-                   init=lambda X_, k, random_state: X_[:2, :],
-                   n_clusters=3)
-        assert_raises_regex(ValueError, msg, km.fit, X)
+
+    # mismatch of number of features
+    km = Class(n_init=1, init=X[:, :2], n_clusters=len(X))
+    msg = "does not match the number of features of the data"
+    assert_raises_regex(ValueError, msg, km.fit, X)
+    # for callable init
+    km = Class(n_init=1,
+               init=lambda X_, k, random_state: X_[:, :2],
+               n_clusters=len(X))
+    assert_raises_regex(ValueError, msg, km.fit, X)
+    # mismatch of number of clusters
+    msg = "does not match the number of clusters"
+    km = Class(n_init=1, init=X[:2, :], n_clusters=3)
+    assert_raises_regex(ValueError, msg, km.fit, X)
+    # for callable init
+    km = Class(n_init=1,
+               init=lambda X_, k, random_state: X_[:2, :],
+               n_clusters=3)
+    assert_raises_regex(ValueError, msg, km.fit, X)
 
 
 def test_k_means_fortran_aligned_data():
@@ -786,46 +789,46 @@ def test_max_iter_error():
                          km.fit, X)
 
 
-def test_float_precision():
-    km = KMeans(n_init=1, random_state=30)
-    mb_km = MiniBatchKMeans(n_init=1, random_state=30)
+@pytest.mark.parametrize('Estimator', [KMeans, MiniBatchKMeans])
+@pytest.mark.parametrize('is_sparse', [False, True])
+def test_float_precision(Estimator, is_sparse):
+
+    estimator = Estimator(n_init=1, random_state=30)
 
     inertia = {}
     X_new = {}
     centers = {}
 
-    for estimator in [km, mb_km]:
-        for is_sparse in [False, True]:
-            for dtype in [np.float64, np.float32]:
-                if is_sparse:
-                    X_test = sp.csr_matrix(X_csr, dtype=dtype)
-                else:
-                    X_test = X.astype(dtype)
-                estimator.fit(X_test)
-                # dtype of cluster centers has to be the dtype of the input
-                # data
-                assert_equal(estimator.cluster_centers_.dtype, dtype)
-                inertia[dtype] = estimator.inertia_
-                X_new[dtype] = estimator.transform(X_test)
-                centers[dtype] = estimator.cluster_centers_
-                # ensure the extracted row is a 2d array
-                assert_equal(estimator.predict(X_test[:1]),
-                             estimator.labels_[0])
-                if hasattr(estimator, 'partial_fit'):
-                    estimator.partial_fit(X_test[0:3])
-                    # dtype of cluster centers has to stay the same after
-                    # partial_fit
-                    assert_equal(estimator.cluster_centers_.dtype, dtype)
-
-            # compare arrays with low precision since the difference between
-            # 32 and 64 bit sometimes makes a difference up to the 4th decimal
-            # place
-            assert_array_almost_equal(inertia[np.float32], inertia[np.float64],
-                                      decimal=4)
-            assert_array_almost_equal(X_new[np.float32], X_new[np.float64],
-                                      decimal=4)
-            assert_array_almost_equal(centers[np.float32], centers[np.float64],
-                                      decimal=4)
+    for dtype in [np.float64, np.float32]:
+        if is_sparse:
+            X_test = sp.csr_matrix(X_csr, dtype=dtype)
+        else:
+            X_test = X.astype(dtype)
+        estimator.fit(X_test)
+        # dtype of cluster centers has to be the dtype of the input
+        # data
+        assert_equal(estimator.cluster_centers_.dtype, dtype)
+        inertia[dtype] = estimator.inertia_
+        X_new[dtype] = estimator.transform(X_test)
+        centers[dtype] = estimator.cluster_centers_
+        # ensure the extracted row is a 2d array
+        assert_equal(estimator.predict(X_test[:1]),
+                     estimator.labels_[0])
+        if hasattr(estimator, 'partial_fit'):
+            estimator.partial_fit(X_test[0:3])
+            # dtype of cluster centers has to stay the same after
+            # partial_fit
+            assert_equal(estimator.cluster_centers_.dtype, dtype)
+
+    # compare arrays with low precision since the difference between
+    # 32 and 64 bit sometimes makes a difference up to the 4th decimal
+    # place
+    assert_array_almost_equal(inertia[np.float32], inertia[np.float64],
+                              decimal=4)
+    assert_array_almost_equal(X_new[np.float32], X_new[np.float64],
+                              decimal=4)
+    assert_array_almost_equal(centers[np.float32], centers[np.float64],
+                              decimal=4)
 
 
 def test_k_means_init_centers():
diff --git a/sklearn/cluster/tests/test_mean_shift.py b/sklearn/cluster/tests/test_mean_shift.py
index 62718e12d6a0..1d6940a947dc 100644
--- a/sklearn/cluster/tests/test_mean_shift.py
+++ b/sklearn/cluster/tests/test_mean_shift.py
@@ -65,6 +65,10 @@ def test_estimate_bandwidth_with_sparse_matrix():
 
 
 def test_parallel():
+    centers = np.array([[1, 1], [-1, -1], [1, -1]]) + 10
+    X, _ = make_blobs(n_samples=50, n_features=2, centers=centers,
+                      cluster_std=0.4, shuffle=True, random_state=11)
+
     ms1 = MeanShift(n_jobs=2)
     ms1.fit(X)
 
diff --git a/sklearn/cluster/tests/test_spectral.py b/sklearn/cluster/tests/test_spectral.py
index 62d9adcc2e34..0c220e7615e6 100644
--- a/sklearn/cluster/tests/test_spectral.py
+++ b/sklearn/cluster/tests/test_spectral.py
@@ -4,6 +4,8 @@
 import numpy as np
 from scipy import sparse
 
+import pytest
+
 from sklearn.externals.six.moves import cPickle
 
 from sklearn.utils import check_random_state
@@ -27,7 +29,9 @@
     amg_loaded = False
 
 
-def test_spectral_clustering():
+@pytest.mark.parametrize('eigen_solver', ('arpack', 'lobpcg'))
+@pytest.mark.parametrize('assign_labels', ('kmeans', 'discretize'))
+def test_spectral_clustering(eigen_solver, assign_labels):
     S = np.array([[1.0, 1.0, 1.0, 0.2, 0.0, 0.0, 0.0],
                   [1.0, 1.0, 1.0, 0.2, 0.0, 0.0, 0.0],
                   [1.0, 1.0, 1.0, 0.2, 0.0, 0.0, 0.0],
@@ -36,24 +40,22 @@ def test_spectral_clustering():
                   [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0],
                   [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]])
 
-    for eigen_solver in ('arpack', 'lobpcg'):
-        for assign_labels in ('kmeans', 'discretize'):
-            for mat in (S, sparse.csr_matrix(S)):
-                model = SpectralClustering(random_state=0, n_clusters=2,
-                                           affinity='precomputed',
-                                           eigen_solver=eigen_solver,
-                                           assign_labels=assign_labels
-                                          ).fit(mat)
-                labels = model.labels_
-                if labels[0] == 0:
-                    labels = 1 - labels
+    for mat in (S, sparse.csr_matrix(S)):
+        model = SpectralClustering(random_state=0, n_clusters=2,
+                                   affinity='precomputed',
+                                   eigen_solver=eigen_solver,
+                                   assign_labels=assign_labels
+                                   ).fit(mat)
+        labels = model.labels_
+        if labels[0] == 0:
+            labels = 1 - labels
 
-                assert adjusted_rand_score(labels, [1, 1, 1, 0, 0, 0, 0]) == 1
+        assert adjusted_rand_score(labels, [1, 1, 1, 0, 0, 0, 0]) == 1
 
-                model_copy = cPickle.loads(cPickle.dumps(model))
-                assert model_copy.n_clusters == model.n_clusters
-                assert model_copy.eigen_solver == model.eigen_solver
-                assert_array_equal(model_copy.labels_, model.labels_)
+        model_copy = cPickle.loads(cPickle.dumps(model))
+        assert model_copy.n_clusters == model.n_clusters
+        assert model_copy.eigen_solver == model.eigen_solver
+        assert_array_equal(model_copy.labels_, model.labels_)
 
 
 def test_spectral_unknown_mode():
@@ -149,25 +151,25 @@ def histogram(x, y, **kwargs):
     assert_raises(ValueError, sp.fit, X)
 
 
-def test_discretize(seed=8):
+@pytest.mark.parametrize('n_samples', [50, 100, 150, 500])
+def test_discretize(n_samples):
     # Test the discretize using a noise assignment matrix
-    random_state = np.random.RandomState(seed)
-    for n_samples in [50, 100, 150, 500]:
-        for n_class in range(2, 10):
-            # random class labels
-            y_true = random_state.randint(0, n_class + 1, n_samples)
-            y_true = np.array(y_true, np.float)
-            # noise class assignment matrix
-            y_indicator = sparse.coo_matrix((np.ones(n_samples),
-                                             (np.arange(n_samples),
-                                              y_true)),
-                                            shape=(n_samples,
+    random_state = np.random.RandomState(seed=8)
+    for n_class in range(2, 10):
+        # random class labels
+        y_true = random_state.randint(0, n_class + 1, n_samples)
+        y_true = np.array(y_true, np.float)
+        # noise class assignment matrix
+        y_indicator = sparse.coo_matrix((np.ones(n_samples),
+                                         (np.arange(n_samples),
+                                          y_true)),
+                                        shape=(n_samples,
+                                               n_class + 1))
+        y_true_noisy = (y_indicator.toarray()
+                        + 0.1 * random_state.randn(n_samples,
                                                    n_class + 1))
-            y_true_noisy = (y_indicator.toarray()
-                            + 0.1 * random_state.randn(n_samples,
-                                                       n_class + 1))
-            y_pred = discretize(y_true_noisy, random_state)
-            assert adjusted_rand_score(y_true, y_pred) > 0.8
+        y_pred = discretize(y_true_noisy, random_state)
+        assert adjusted_rand_score(y_true, y_pred) > 0.8
 
 
 def test_spectral_clustering_with_arpack_amg_solvers():
diff --git a/sklearn/compose/__init__.py b/sklearn/compose/__init__.py
index 1d9ac399ccd8..1cfd53c50d68 100644
--- a/sklearn/compose/__init__.py
+++ b/sklearn/compose/__init__.py
@@ -5,8 +5,12 @@
 
 """
 
+from ._column_transformer import ColumnTransformer, make_column_transformer
 from ._target import TransformedTargetRegressor
 
+
 __all__ = [
+    'ColumnTransformer',
+    'make_column_transformer',
     'TransformedTargetRegressor',
 ]
diff --git a/sklearn/compose/_column_transformer.py b/sklearn/compose/_column_transformer.py
new file mode 100644
index 000000000000..6ea4ccdd6b05
--- /dev/null
+++ b/sklearn/compose/_column_transformer.py
@@ -0,0 +1,662 @@
+"""
+The :mod:`sklearn.compose._column_transformer` module implements utilities
+to work with heterogeneous data and to apply different transformers to
+different columns.
+"""
+# Author: Andreas Mueller
+#         Joris Van den Bossche
+# License: BSD
+
+
+import numpy as np
+from scipy import sparse
+
+from ..base import clone, TransformerMixin
+from ..externals.joblib import Parallel, delayed
+from ..externals import six
+from ..pipeline import (
+    _fit_one_transformer, _fit_transform_one, _transform_one, _name_estimators)
+from ..preprocessing import FunctionTransformer
+from ..utils import Bunch
+from ..utils.metaestimators import _BaseComposition
+from ..utils.validation import check_is_fitted
+
+
+__all__ = ['ColumnTransformer', 'make_column_transformer']
+
+
+_ERR_MSG_1DCOLUMN = ("1D data passed to a transformer that expects 2D data. "
+                     "Try to specify the column selection as a list of one "
+                     "item instead of a scalar.")
+
+
+class ColumnTransformer(_BaseComposition, TransformerMixin):
+    """Applies transformers to columns of an array or pandas DataFrame.
+
+    EXPERIMENTAL: some behaviors may change between releases without
+    deprecation.
+
+    This estimator allows different columns or column subsets of the input
+    to be transformed separately and the results combined into a single
+    feature space.
+    This is useful for heterogeneous or columnar data, to combine several
+    feature extraction mechanisms or transformations into a single transformer.
+
+    Read more in the :ref:`User Guide <column_transformer>`.
+
+    .. versionadded:: 0.20
+
+    Parameters
+    ----------
+    transformers : list of tuples
+        List of (name, transformer, column(s)) tuples specifying the
+        transformer objects to be applied to subsets of the data.
+
+        name : string
+            Like in Pipeline and FeatureUnion, this allows the transformer and
+            its parameters to be set using ``set_params`` and searched in grid
+            search.
+        transformer : estimator or {'passthrough', 'drop'}
+            Estimator must support `fit` and `transform`. Special-cased
+            strings 'drop' and 'passthrough' are accepted as well, to
+            indicate to drop the columns or to pass them through untransformed,
+            respectively.
+        column(s) : string or int, array-like of string or int, slice or \
+boolean mask array
+            Indexes the data on its second axis. Integers are interpreted as
+            positional columns, while strings can reference DataFrame columns
+            by name.  A scalar string or int should be used where
+            ``transformer`` expects X to be a 1d array-like (vector),
+            otherwise a 2d array will be passed to the transformer.
+
+    remainder : {'passthrough', 'drop'}, default 'passthrough'
+        By default, all remaining columns that were not specified in
+        `transformers` will be automatically passed through (default of
+        ``'passthrough'``). This subset of columns is concatenated with the
+        output of the transformers.
+        By using ``remainder='drop'``, only the specified columns in
+        `transformers` are transformed and combined in the output, and the
+        non-specified columns are dropped.
+
+    n_jobs : int, optional
+        Number of jobs to run in parallel (default 1).
+
+    transformer_weights : dict, optional
+        Multiplicative weights for features per transformer. The output of the
+        transformer is multiplied by these weights. Keys are transformer names,
+        values the weights.
+
+    Attributes
+    ----------
+    transformers_ : list
+        The collection of fitted transformers as tuples of
+        (name, fitted_transformer, column).
+
+    named_transformers_ : Bunch object, a dictionary with attribute access
+        Read-only attribute to access any transformer by given name.
+        Keys are transformer names and values are the fitted transformer
+        objects.
+
+    Notes
+    -----
+    The order of the columns in the transformed feature matrix follows the
+    order of how the columns are specified in the `transformers` list.
+    Columns of the original feature matrix that are not specified are
+    dropped from the resulting transformed feature matrix, unless specified
+    in the `passthrough` keyword. Those columns specified with `passthrough`
+    are added at the right to the output of the transformers.
+
+    See also
+    --------
+    sklearn.compose.make_column_transformer : convenience function for
+        combining the outputs of multiple transformer objects applied to
+        column subsets of the original feature space.
+
+    Examples
+    --------
+    >>> from sklearn.compose import ColumnTransformer
+    >>> from sklearn.preprocessing import Normalizer
+    >>> ct = ColumnTransformer(
+    ...     [("norm1", Normalizer(norm='l1'), [0, 1]),
+    ...      ("norm2", Normalizer(norm='l1'), slice(2, 4))])
+    >>> X = np.array([[0., 1., 2., 2.],
+    ...               [1., 1., 0., 1.]])
+    >>> # Normalizer scales each row of X to unit norm. A separate scaling
+    >>> # is applied for the two first and two last elements of each
+    >>> # row independently.
+    >>> ct.fit_transform(X)    # doctest: +NORMALIZE_WHITESPACE
+    array([[0. , 1. , 0.5, 0.5],
+           [0.5, 0.5, 0. , 1. ]])
+
+    """
+
+    def __init__(self, transformers, remainder='passthrough', n_jobs=1,
+                 transformer_weights=None):
+        self.transformers = transformers
+        self.remainder = remainder
+        self.n_jobs = n_jobs
+        self.transformer_weights = transformer_weights
+
+    @property
+    def _transformers(self):
+        """
+        Internal list of transformer only containing the name and
+        transformers, dropping the columns. This is for the implementation
+        of get_params via BaseComposition._get_params which expects lists
+        of tuples of len 2.
+        """
+        return [(name, trans) for name, trans, _ in self.transformers]
+
+    @_transformers.setter
+    def _transformers(self, value):
+        self.transformers = [
+            (name, trans, col) for ((name, trans), (_, _, col))
+            in zip(value, self.transformers)]
+
+    def get_params(self, deep=True):
+        """Get parameters for this estimator.
+
+        Parameters
+        ----------
+        deep : boolean, optional
+            If True, will return the parameters for this estimator and
+            contained subobjects that are estimators.
+
+        Returns
+        -------
+        params : mapping of string to any
+            Parameter names mapped to their values.
+        """
+        return self._get_params('_transformers', deep=deep)
+
+    def set_params(self, **kwargs):
+        """Set the parameters of this estimator.
+
+        Valid parameter keys can be listed with ``get_params()``.
+
+        Returns
+        -------
+        self
+        """
+        self._set_params('_transformers', **kwargs)
+        return self
+
+    def _iter(self, X=None, fitted=False, replace_strings=False):
+        """Generate (name, trans, column, weight) tuples
+        """
+        if fitted:
+            transformers = self.transformers_
+        else:
+            transformers = self.transformers
+        get_weight = (self.transformer_weights or {}).get
+
+        for name, trans, column in transformers:
+            if X is None:
+                sub = X
+            else:
+                sub = _get_column(X, column)
+
+            if replace_strings:
+                # replace 'passthrough' with identity transformer and
+                # skip in case of 'drop'
+                if trans == 'passthrough':
+                    trans = FunctionTransformer(
+                        validate=False, accept_sparse=True,
+                        check_inverse=False)
+                elif trans == 'drop':
+                    continue
+
+            yield (name, trans, sub, get_weight(name))
+
+    def _validate_transformers(self):
+        names, transformers, _, _ = zip(*self._iter())
+
+        # validate names
+        self._validate_names(names)
+
+        # validate estimators
+        for t in transformers:
+            if t in ('drop', 'passthrough'):
+                continue
+            if (not (hasattr(t, "fit") or hasattr(t, "fit_transform")) or not
+                    hasattr(t, "transform")):
+                raise TypeError("All estimators should implement fit and "
+                                "transform, or can be 'drop' or 'passthrough' "
+                                "specifiers. '%s' (type %s) doesn't." %
+                                (t, type(t)))
+
+    def _validate_remainder(self, X):
+        """Generate list of passthrough columns for 'remainder' case."""
+        if self.remainder not in ('drop', 'passthrough'):
+            raise ValueError(
+                "The remainder keyword needs to be one of 'drop' or "
+                "'passthrough'. {0:r} was passed instead")
+
+        n_columns = X.shape[1]
+
+        if self.remainder == 'passthrough':
+            cols = []
+            for _, _, columns in self.transformers:
+                cols.extend(_get_column_indices(X, columns))
+            self._passthrough = sorted(list(set(range(n_columns)) - set(cols)))
+            if not self._passthrough:
+                # empty list -> no need to select passthrough columns
+                self._passthrough = None
+        else:
+            self._passthrough = None
+
+    @property
+    def named_transformers_(self):
+        """Access the fitted transformer by name.
+
+        Read-only attribute to access any transformer by given name.
+        Keys are transformer names and values are the fitted transformer
+        objects.
+
+        """
+        # Use Bunch object to improve autocomplete
+        return Bunch(**dict([(name, trans) for name, trans, _
+                             in self.transformers_]))
+
+    def get_feature_names(self):
+        """Get feature names from all transformers.
+
+        Returns
+        -------
+        feature_names : list of strings
+            Names of the features produced by transform.
+        """
+        check_is_fitted(self, 'transformers_')
+        if self._passthrough is not None:
+            raise NotImplementedError(
+                "get_feature_names is not yet supported when having columns"
+                "that are passed through (you specify remainder='drop' to not "
+                "pass through the unspecified columns).")
+
+        feature_names = []
+        for name, trans, _, _ in self._iter(fitted=True):
+            if trans == 'drop':
+                continue
+            elif trans == 'passthrough':
+                raise NotImplementedError(
+                    "get_feature_names is not yet supported when using "
+                    "a 'passthrough' transformer.")
+            elif not hasattr(trans, 'get_feature_names'):
+                raise AttributeError("Transformer %s (type %s) does not "
+                                     "provide get_feature_names."
+                                     % (str(name), type(trans).__name__))
+            feature_names.extend([name + "__" + f for f in
+                                  trans.get_feature_names()])
+        return feature_names
+
+    def _update_fitted_transformers(self, transformers):
+        # transformers are fitted; excludes 'drop' cases
+        transformers = iter(transformers)
+        transformers_ = []
+
+        for name, old, column in self.transformers:
+            if old == 'drop':
+                trans = 'drop'
+            elif old == 'passthrough':
+                # FunctionTransformer is present in list of transformers,
+                # so get next transformer, but save original string
+                next(transformers)
+                trans = 'passthrough'
+            else:
+                trans = next(transformers)
+
+            transformers_.append((name, trans, column))
+
+        # sanity check that transformers is exhausted
+        assert not list(transformers)
+        self.transformers_ = transformers_
+
+    def _validate_output(self, result):
+        """
+        Ensure that the output of each transformer is 2D. Otherwise
+        hstack can raise an error or produce incorrect results.
+        """
+        names = [name for name, _, _, _ in self._iter(replace_strings=True)]
+        for Xs, name in zip(result, names):
+            if not getattr(Xs, 'ndim', 0) == 2:
+                raise ValueError(
+                    "The output of the '{0}' transformer should be 2D (scipy "
+                    "matrix, array, or pandas DataFrame).".format(name))
+
+    def _fit_transform(self, X, y, func, fitted=False):
+        """
+        Private function to fit and/or transform on demand.
+
+        Return value (transformers and/or transformed X data) depends
+        on the passed function.
+        ``fitted=True`` ensures the fitted transformers are used.
+        """
+        try:
+            return Parallel(n_jobs=self.n_jobs)(
+                delayed(func)(clone(trans) if not fitted else trans,
+                              X_sel, y, weight)
+                for name, trans, X_sel, weight in self._iter(
+                    X=X, fitted=fitted, replace_strings=True))
+        except ValueError as e:
+            if "Expected 2D array, got 1D array instead" in str(e):
+                raise ValueError(_ERR_MSG_1DCOLUMN)
+            else:
+                raise
+
+    def fit(self, X, y=None):
+        """Fit all transformers using X.
+
+        Parameters
+        ----------
+        X : array-like or DataFrame of shape [n_samples, n_features]
+            Input data, of which specified subsets are used to fit the
+            transformers.
+
+        y : array-like, shape (n_samples, ...), optional
+            Targets for supervised learning.
+
+        Returns
+        -------
+        self : ColumnTransformer
+            This estimator
+
+        """
+        self._validate_transformers()
+        self._validate_remainder(X)
+
+        transformers = self._fit_transform(X, y, _fit_one_transformer)
+
+        self._update_fitted_transformers(transformers)
+        return self
+
+    def fit_transform(self, X, y=None):
+        """Fit all transformers, transform the data and concatenate results.
+
+        Parameters
+        ----------
+        X : array-like or DataFrame of shape [n_samples, n_features]
+            Input data, of which specified subsets are used to fit the
+            transformers.
+
+        y : array-like, shape (n_samples, ...), optional
+            Targets for supervised learning.
+
+        Returns
+        -------
+        X_t : array-like or sparse matrix, shape (n_samples, sum_n_components)
+            hstack of results of transformers. sum_n_components is the
+            sum of n_components (output dimension) over transformers. If
+            any result is a sparse matrix, everything will be converted to
+            sparse matrices.
+
+        """
+        self._validate_transformers()
+        self._validate_remainder(X)
+
+        result = self._fit_transform(X, y, _fit_transform_one)
+
+        if not result:
+            # All transformers are None
+            if self._passthrough is None:
+                return np.zeros((X.shape[0], 0))
+            else:
+                return _get_column(X, self._passthrough)
+
+        Xs, transformers = zip(*result)
+
+        self._update_fitted_transformers(transformers)
+        self._validate_output(Xs)
+
+        if self._passthrough is not None:
+            Xs = list(Xs) + [_get_column(X, self._passthrough)]
+
+        if any(sparse.issparse(f) for f in Xs):
+            Xs = sparse.hstack(Xs).tocsr()
+        else:
+            Xs = np.hstack(Xs)
+        return Xs
+
+    def transform(self, X):
+        """Transform X separately by each transformer, concatenate results.
+
+        Parameters
+        ----------
+        X : array-like or DataFrame of shape [n_samples, n_features]
+            The data to be transformed by subset.
+
+        Returns
+        -------
+        X_t : array-like or sparse matrix, shape (n_samples, sum_n_components)
+            hstack of results of transformers. sum_n_components is the
+            sum of n_components (output dimension) over transformers. If
+            any result is a sparse matrix, everything will be converted to
+            sparse matrices.
+
+        """
+        check_is_fitted(self, 'transformers_')
+
+        Xs = self._fit_transform(X, None, _transform_one, fitted=True)
+        self._validate_output(Xs)
+
+        if not Xs:
+            # All transformers are None
+            if self._passthrough is None:
+                return np.zeros((X.shape[0], 0))
+            else:
+                return _get_column(X, self._passthrough)
+
+        if self._passthrough is not None:
+            Xs = list(Xs) + [_get_column(X, self._passthrough)]
+
+        if any(sparse.issparse(f) for f in Xs):
+            Xs = sparse.hstack(Xs).tocsr()
+        else:
+            Xs = np.hstack(Xs)
+        return Xs
+
+
+def _check_key_type(key, superclass):
+    """
+    Check that scalar, list or slice is of a certain type.
+
+    This is only used in _get_column and _get_column_indices to check
+    if the `key` (column specification) is fully integer or fully string-like.
+
+    Parameters
+    ----------
+    key : scalar, list, slice, array-like
+        The column specification to check
+    superclass : int or six.string_types
+        The type for which to check the `key`
+
+    """
+    if isinstance(key, superclass):
+        return True
+    if isinstance(key, slice):
+        return (isinstance(key.start, (superclass, type(None))) and
+                isinstance(key.stop, (superclass, type(None))))
+    if isinstance(key, list):
+        return all(isinstance(x, superclass) for x in key)
+    if hasattr(key, 'dtype'):
+        if superclass is int:
+            return key.dtype.kind == 'i'
+        else:
+            # superclass = six.string_types
+            return key.dtype.kind in ('O', 'U', 'S')
+    return False
+
+
+def _get_column(X, key):
+    """
+    Get feature column(s) from input data X.
+
+    Supported input types (X): numpy arrays, sparse arrays and DataFrames
+
+    Supported key types (key):
+    - scalar: output is 1D
+    - lists, slices, boolean masks: output is 2D
+
+    Supported key data types:
+
+    - integer or boolean mask (positional):
+        - supported for arrays, sparse matrices and dataframes
+    - string (key-based):
+        - only supported for dataframes
+        - So no keys other than strings are allowed (while in principle you
+          can use any hashable object as key).
+
+    """
+    # check whether we have string column names or integers
+    if _check_key_type(key, int):
+        column_names = False
+    elif _check_key_type(key, six.string_types):
+        column_names = True
+    elif hasattr(key, 'dtype') and np.issubdtype(key.dtype, np.bool_):
+        # boolean mask
+        column_names = False
+        if hasattr(X, 'loc'):
+            # pandas boolean masks don't work with iloc, so take loc path
+            column_names = True
+    else:
+        raise ValueError("No valid specification of the columns. Only a "
+                         "scalar, list or slice of all integers or all "
+                         "strings, or boolean mask is allowed")
+
+    if column_names:
+        if hasattr(X, 'loc'):
+            # pandas dataframes
+            return X.loc[:, key]
+        else:
+            raise ValueError("Specifying the columns using strings is only "
+                             "supported for pandas DataFrames")
+    else:
+        if hasattr(X, 'iloc'):
+            # pandas dataframes
+            return X.iloc[:, key]
+        else:
+            # numpy arrays, sparse arrays
+            return X[:, key]
+
+
+def _get_column_indices(X, key):
+    """
+    Get feature column indices for input data X and key.
+
+    For accepted values of `key`, see the docstring of _get_column
+
+    """
+    n_columns = X.shape[1]
+
+    if _check_key_type(key, int):
+        if isinstance(key, int):
+            return [key]
+        elif isinstance(key, slice):
+            return list(range(n_columns)[key])
+        else:
+            return list(key)
+
+    elif _check_key_type(key, six.string_types):
+        try:
+            all_columns = list(X.columns)
+        except AttributeError:
+            raise ValueError("Specifying the columns using strings is only "
+                             "supported for pandas DataFrames")
+        if isinstance(key, six.string_types):
+            columns = [key]
+        elif isinstance(key, slice):
+            start, stop = key.start, key.stop
+            if start is not None:
+                start = all_columns.index(start)
+            if stop is not None:
+                # pandas indexing with strings is endpoint included
+                stop = all_columns.index(stop) + 1
+            else:
+                stop = n_columns + 1
+            return list(range(n_columns)[slice(start, stop)])
+        else:
+            columns = list(key)
+
+        return [all_columns.index(col) for col in columns]
+
+    elif hasattr(key, 'dtype') and np.issubdtype(key.dtype, np.bool_):
+        # boolean mask
+        return list(np.arange(n_columns)[key])
+    else:
+        raise ValueError("No valid specification of the columns. Only a "
+                         "scalar, list or slice of all integers or all "
+                         "strings, or boolean mask is allowed")
+
+
+def _get_transformer_list(estimators):
+    """
+    Construct (name, trans, column) tuples from list
+
+    """
+    transformers = [trans[1] for trans in estimators]
+    columns = [trans[0] for trans in estimators]
+    names = [trans[0] for trans in _name_estimators(transformers)]
+
+    transformer_list = list(zip(names, transformers, columns))
+    return transformer_list
+
+
+def make_column_transformer(*transformers, **kwargs):
+    """Construct a ColumnTransformer from the given transformers.
+
+    This is a shorthand for the ColumnTransformer constructor; it does not
+    require, and does not permit, naming the transformers. Instead, they will
+    be given names automatically based on their types. It also does not allow
+    weighting.
+
+    Parameters
+    ----------
+    *transformers : tuples of column selections and transformers
+
+    remainder : {'passthrough', 'drop'}, default 'passthrough'
+        By default, all remaining columns that were not specified in
+        `transformers` will be automatically passed through (default of
+        ``'passthrough'``). This subset of columns is concatenated with the
+        output of the transformers.
+        By using ``remainder='drop'``, only the specified columns in
+        `transformers` are transformed and combined in the output, and the
+        non-specified columns are dropped.
+
+    n_jobs : int, optional
+        Number of jobs to run in parallel (default 1).
+
+    Returns
+    -------
+    ct : ColumnTransformer
+
+    See also
+    --------
+    sklearn.compose.ColumnTransformer : Class that allows combining the
+        outputs of multiple transformer objects used on column subsets
+        of the data into a single feature space.
+
+    Examples
+    --------
+    >>> from sklearn.preprocessing import StandardScaler, OneHotEncoder
+    >>> from sklearn.compose import make_column_transformer
+    >>> make_column_transformer(
+    ...     (['numerical_column'], StandardScaler()),
+    ...     (['categorical_column'], OneHotEncoder()))
+    ...     # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
+    ColumnTransformer(n_jobs=1, remainder='passthrough',
+             transformer_weights=None,
+             transformers=[('standardscaler',
+                            StandardScaler(...),
+                            ['numerical_column']),
+                           ('onehotencoder',
+                            OneHotEncoder(...),
+                            ['categorical_column'])])
+
+    """
+    n_jobs = kwargs.pop('n_jobs', 1)
+    remainder = kwargs.pop('remainder', 'passthrough')
+    if kwargs:
+        raise TypeError('Unknown keyword arguments: "{}"'
+                        .format(list(kwargs.keys())[0]))
+    transformer_list = _get_transformer_list(transformers)
+    return ColumnTransformer(transformer_list, n_jobs=n_jobs,
+                             remainder=remainder)
diff --git a/sklearn/compose/_target.py b/sklearn/compose/_target.py
index cb3e1cedd0eb..757cec54ee22 100644
--- a/sklearn/compose/_target.py
+++ b/sklearn/compose/_target.py
@@ -25,12 +25,15 @@ class TransformedTargetRegressor(BaseEstimator, RegressorMixin):
     The computation during ``fit`` is::
 
         regressor.fit(X, func(y))
+
     or::
 
         regressor.fit(X, transformer.transform(y))
+
     The computation during ``predict`` is::
 
         inverse_func(regressor.predict(X))
+
     or::
 
         transformer.inverse_transform(regressor.predict(X))
@@ -97,8 +100,8 @@ class TransformedTargetRegressor(BaseEstimator, RegressorMixin):
     to be used by scikit-learn transformers. At the time of prediction, the
     output will be reshaped to a have the same number of dimensions as ``y``.
 
-    See :ref:`examples/preprocessing/plot_transformed_target.py
-    <sphx_glr_auto_examples_preprocessing_plot_transformed_target.py>`.
+    See :ref:`examples/compose/plot_transformed_target.py
+    <sphx_glr_auto_examples_compose_plot_transformed_target.py>`.
 
     """
     def __init__(self, regressor=None, transformer=None,
diff --git a/sklearn/compose/tests/test_column_transformer.py b/sklearn/compose/tests/test_column_transformer.py
new file mode 100644
index 000000000000..2d77f3af85fb
--- /dev/null
+++ b/sklearn/compose/tests/test_column_transformer.py
@@ -0,0 +1,573 @@
+"""
+Test the ColumnTransformer.
+"""
+
+import numpy as np
+from scipy import sparse
+import pytest
+
+from sklearn.utils.testing import assert_raises
+from sklearn.utils.testing import assert_raise_message
+from sklearn.utils.testing import assert_equal
+from sklearn.utils.testing import assert_true
+from sklearn.utils.testing import assert_false
+from sklearn.utils.testing import assert_dict_equal
+from sklearn.utils.testing import assert_array_equal
+from sklearn.utils.testing import assert_allclose_dense_sparse
+
+from sklearn.base import BaseEstimator
+from sklearn.externals import six
+from sklearn.compose import ColumnTransformer, make_column_transformer
+from sklearn.exceptions import NotFittedError
+from sklearn.preprocessing import StandardScaler, Normalizer
+from sklearn.feature_extraction import DictVectorizer
+
+
+class Trans(BaseEstimator):
+    def fit(self, X, y=None):
+        return self
+
+    def transform(self, X, y=None):
+        # 1D Series -> 2D DataFrame
+        if hasattr(X, 'to_frame'):
+            return X.to_frame()
+        # 1D array -> 2D array
+        if X.ndim == 1:
+            return np.atleast_2d(X).T
+        return X
+
+
+class SparseMatrixTrans(BaseEstimator):
+    def fit(self, X, y=None):
+        return self
+
+    def transform(self, X, y=None):
+        n_samples = len(X)
+        return sparse.eye(n_samples, n_samples).tocsr()
+
+
+def test_column_transformer():
+    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T
+
+    X_res_first1D = np.array([0, 1, 2])
+    X_res_second1D = np.array([2, 4, 6])
+    X_res_first = X_res_first1D.reshape(-1, 1)
+    X_res_both = X_array
+
+    cases = [
+        # single column 1D / 2D
+        (0, X_res_first),
+        ([0], X_res_first),
+        # list-like
+        ([0, 1], X_res_both),
+        (np.array([0, 1]), X_res_both),
+        # slice
+        (slice(0, 1), X_res_first),
+        (slice(0, 2), X_res_both),
+        # boolean mask
+        (np.array([True, False]), X_res_first),
+    ]
+
+    for selection, res in cases:
+        ct = ColumnTransformer([('trans', Trans(), selection)],
+                               remainder='drop')
+        assert_array_equal(ct.fit_transform(X_array), res)
+        assert_array_equal(ct.fit(X_array).transform(X_array), res)
+
+    ct = ColumnTransformer([('trans1', Trans(), [0]),
+                            ('trans2', Trans(), [1])])
+    assert_array_equal(ct.fit_transform(X_array), X_res_both)
+    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)
+
+    # test with transformer_weights
+    transformer_weights = {'trans1': .1, 'trans2': 10}
+    both = ColumnTransformer([('trans1', Trans(), [0]),
+                              ('trans2', Trans(), [1])],
+                             transformer_weights=transformer_weights)
+    res = np.vstack([transformer_weights['trans1'] * X_res_first1D,
+                     transformer_weights['trans2'] * X_res_second1D]).T
+    assert_array_equal(both.fit_transform(X_array), res)
+    assert_array_equal(both.fit(X_array).transform(X_array), res)
+
+    both = ColumnTransformer([('trans', Trans(), [0, 1])],
+                             transformer_weights={'trans': .1})
+    assert_array_equal(both.fit_transform(X_array), 0.1 * X_res_both)
+    assert_array_equal(both.fit(X_array).transform(X_array), 0.1 * X_res_both)
+
+
+def test_column_transformer_dataframe():
+    pd = pytest.importorskip('pandas')
+
+    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T
+    X_df = pd.DataFrame(X_array, columns=['first', 'second'])
+
+    X_res_first = np.array([0, 1, 2]).reshape(-1, 1)
+    X_res_both = X_array
+
+    cases = [
+        # String keys: label based
+
+        # scalar
+        ('first', X_res_first),
+        # list
+        (['first'], X_res_first),
+        (['first', 'second'], X_res_both),
+        # slice
+        (slice('first', 'second'), X_res_both),
+
+        # int keys: positional
+
+        # scalar
+        (0, X_res_first),
+        # list
+        ([0], X_res_first),
+        ([0, 1], X_res_both),
+        (np.array([0, 1]), X_res_both),
+        # slice
+        (slice(0, 1), X_res_first),
+        (slice(0, 2), X_res_both),
+
+        # boolean mask
+        (np.array([True, False]), X_res_first),
+        (pd.Series([True, False], index=['first', 'second']), X_res_first),
+    ]
+
+    for selection, res in cases:
+        ct = ColumnTransformer([('trans', Trans(), selection)],
+                               remainder='drop')
+        assert_array_equal(ct.fit_transform(X_df), res)
+        assert_array_equal(ct.fit(X_df).transform(X_df), res)
+
+    ct = ColumnTransformer([('trans1', Trans(), ['first']),
+                            ('trans2', Trans(), ['second'])])
+    assert_array_equal(ct.fit_transform(X_df), X_res_both)
+    assert_array_equal(ct.fit(X_df).transform(X_df), X_res_both)
+
+    ct = ColumnTransformer([('trans1', Trans(), [0]),
+                            ('trans2', Trans(), [1])])
+    assert_array_equal(ct.fit_transform(X_df), X_res_both)
+    assert_array_equal(ct.fit(X_df).transform(X_df), X_res_both)
+
+    # test with transformer_weights
+    transformer_weights = {'trans1': .1, 'trans2': 10}
+    both = ColumnTransformer([('trans1', Trans(), ['first']),
+                              ('trans2', Trans(), ['second'])],
+                             transformer_weights=transformer_weights)
+    res = np.vstack([transformer_weights['trans1'] * X_df['first'],
+                     transformer_weights['trans2'] * X_df['second']]).T
+    assert_array_equal(both.fit_transform(X_df), res)
+    assert_array_equal(both.fit(X_df).transform(X_df), res)
+
+    # test multiple columns
+    both = ColumnTransformer([('trans', Trans(), ['first', 'second'])],
+                             transformer_weights={'trans': .1})
+    assert_array_equal(both.fit_transform(X_df), 0.1 * X_res_both)
+    assert_array_equal(both.fit(X_df).transform(X_df), 0.1 * X_res_both)
+
+    both = ColumnTransformer([('trans', Trans(), [0, 1])],
+                             transformer_weights={'trans': .1})
+    assert_array_equal(both.fit_transform(X_df), 0.1 * X_res_both)
+    assert_array_equal(both.fit(X_df).transform(X_df), 0.1 * X_res_both)
+
+    # ensure pandas object is passes through
+
+    class TransAssert(BaseEstimator):
+
+        def fit(self, X, y=None):
+            return self
+
+        def transform(self, X, y=None):
+            assert_true(isinstance(X, (pd.DataFrame, pd.Series)))
+            if isinstance(X, pd.Series):
+                X = X.to_frame()
+            return X
+
+    ct = ColumnTransformer([('trans', TransAssert(), 'first')],
+                           remainder='drop')
+    ct.fit_transform(X_df)
+    ct = ColumnTransformer([('trans', TransAssert(), ['first', 'second'])])
+    ct.fit_transform(X_df)
+
+    # integer column spec + integer column names -> still use positional
+    X_df2 = X_df.copy()
+    X_df2.columns = [1, 0]
+    ct = ColumnTransformer([('trans', Trans(), 0)], remainder='drop')
+    assert_array_equal(ct.fit_transform(X_df), X_res_first)
+    assert_array_equal(ct.fit(X_df).transform(X_df), X_res_first)
+
+
+def test_column_transformer_sparse_array():
+    X_sparse = sparse.eye(3, 2).tocsr()
+
+    # no distinction between 1D and 2D
+    X_res_first = X_sparse[:, 0]
+    X_res_both = X_sparse
+
+    for col in [0, [0], slice(0, 1)]:
+        for remainder, res in [('drop', X_res_first),
+                               ('passthrough', X_res_both)]:
+            ct = ColumnTransformer([('trans', Trans(), col)],
+                                   remainder=remainder)
+            assert_true(sparse.issparse(ct.fit_transform(X_sparse)))
+            assert_allclose_dense_sparse(ct.fit_transform(X_sparse), res)
+            assert_allclose_dense_sparse(ct.fit(X_sparse).transform(X_sparse),
+                                         res)
+
+    for col in [[0, 1], slice(0, 2)]:
+        ct = ColumnTransformer([('trans', Trans(), col)])
+        assert_true(sparse.issparse(ct.fit_transform(X_sparse)))
+        assert_allclose_dense_sparse(ct.fit_transform(X_sparse), X_res_both)
+        assert_allclose_dense_sparse(ct.fit(X_sparse).transform(X_sparse),
+                                     X_res_both)
+
+
+def test_column_transformer_sparse_stacking():
+    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T
+    col_trans = ColumnTransformer([('trans1', Trans(), [0]),
+                                   ('trans2', SparseMatrixTrans(), 1)])
+    col_trans.fit(X_array)
+    X_trans = col_trans.transform(X_array)
+    assert_true(sparse.issparse(X_trans))
+    assert_equal(X_trans.shape, (X_trans.shape[0], X_trans.shape[0] + 1))
+    assert_array_equal(X_trans.toarray()[:, 1:], np.eye(X_trans.shape[0]))
+
+
+def test_column_transformer_error_msg_1D():
+    X_array = np.array([[0., 1., 2.], [2., 4., 6.]]).T
+
+    col_trans = ColumnTransformer([('trans', StandardScaler(), 0)])
+    assert_raise_message(ValueError, "1D data passed to a transformer",
+                         col_trans.fit, X_array)
+    assert_raise_message(ValueError, "1D data passed to a transformer",
+                         col_trans.fit_transform, X_array)
+
+    class TransRaise(BaseEstimator):
+
+        def fit(self, X, y=None):
+            raise ValueError("specific message")
+
+        def transform(self, X, y=None):
+            raise ValueError("specific message")
+
+    col_trans = ColumnTransformer([('trans', TransRaise(), 0)])
+    for func in [col_trans.fit, col_trans.fit_transform]:
+        assert_raise_message(ValueError, "specific message", func, X_array)
+
+
+def test_2D_transformer_output():
+
+    class TransNo2D(BaseEstimator):
+        def fit(self, X, y=None):
+            return self
+
+        def transform(self, X, y=None):
+            return X
+
+    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T
+
+    # if one transformer is dropped, test that name is still correct
+    ct = ColumnTransformer([('trans1', 'drop', 0),
+                            ('trans2', TransNo2D(), 1)])
+    assert_raise_message(ValueError, "the 'trans2' transformer should be 2D",
+                         ct.fit_transform, X_array)
+    ct.fit(X_array)
+    assert_raise_message(ValueError, "the 'trans2' transformer should be 2D",
+                         ct.transform, X_array)
+
+
+def test_2D_transformer_output_pandas():
+    pd = pytest.importorskip('pandas')
+
+    class TransNo2D(BaseEstimator):
+        def fit(self, X, y=None):
+            return self
+
+        def transform(self, X, y=None):
+            return X
+
+    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T
+    X_df = pd.DataFrame(X_array, columns=['col1', 'col2'])
+
+    # if one transformer is dropped, test that name is still correct
+    ct = ColumnTransformer([('trans1', TransNo2D(), 'col1')])
+    assert_raise_message(ValueError, "the 'trans1' transformer should be 2D",
+                         ct.fit_transform, X_df)
+    ct.fit(X_df)
+    assert_raise_message(ValueError, "the 'trans1' transformer should be 2D",
+                         ct.transform, X_df)
+
+
+@pytest.mark.parametrize("remainder", ['drop', 'passthrough'])
+def test_column_transformer_invalid_columns(remainder):
+    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T
+
+    # general invalid
+    for col in [1.5, ['string', 1], slice(1, 's'), np.array([1.])]:
+        ct = ColumnTransformer([('trans', Trans(), col)], remainder=remainder)
+        assert_raise_message(ValueError, "No valid specification",
+                             ct.fit, X_array)
+
+    # invalid for arrays
+    for col in ['string', ['string', 'other'], slice('a', 'b')]:
+        ct = ColumnTransformer([('trans', Trans(), col)], remainder=remainder)
+        assert_raise_message(ValueError, "Specifying the columns",
+                             ct.fit, X_array)
+
+
+def test_column_transformer_invalid_transformer():
+
+    class NoTrans(BaseEstimator):
+        def fit(self, X, y=None):
+            return self
+
+        def predict(self, X):
+            return X
+
+    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T
+    ct = ColumnTransformer([('trans', NoTrans(), [0])])
+    assert_raise_message(TypeError, "All estimators should implement fit",
+                         ct.fit, X_array)
+
+
+def test_make_column_transformer():
+    scaler = StandardScaler()
+    norm = Normalizer()
+    ct = make_column_transformer(('first', scaler), (['second'], norm))
+    names, transformers, columns = zip(*ct.transformers)
+    assert_equal(names, ("standardscaler", "normalizer"))
+    assert_equal(transformers, (scaler, norm))
+    assert_equal(columns, ('first', ['second']))
+
+
+def test_make_column_transformer_kwargs():
+    scaler = StandardScaler()
+    norm = Normalizer()
+    ct = make_column_transformer(('first', scaler), (['second'], norm),
+                                 n_jobs=3, remainder='drop')
+    assert_equal(
+        ct.transformers,
+        make_column_transformer(('first', scaler),
+                                (['second'], norm)).transformers)
+    assert_equal(ct.n_jobs, 3)
+    assert_equal(ct.remainder, 'drop')
+    # invalid keyword parameters should raise an error message
+    assert_raise_message(
+        TypeError,
+        'Unknown keyword arguments: "transformer_weights"',
+        make_column_transformer, ('first', scaler), (['second'], norm),
+        transformer_weights={'pca': 10, 'Transf': 1}
+    )
+
+
+def test_column_transformer_get_set_params():
+    ct = ColumnTransformer([('trans1', StandardScaler(), [0]),
+                            ('trans2', StandardScaler(), [1])])
+
+    exp = {'n_jobs': 1,
+           'remainder': 'passthrough',
+           'trans1': ct.transformers[0][1],
+           'trans1__copy': True,
+           'trans1__with_mean': True,
+           'trans1__with_std': True,
+           'trans2': ct.transformers[1][1],
+           'trans2__copy': True,
+           'trans2__with_mean': True,
+           'trans2__with_std': True,
+           'transformers': ct.transformers,
+           'transformer_weights': None}
+
+    assert_dict_equal(ct.get_params(), exp)
+
+    ct.set_params(trans1__with_mean=False)
+    assert_false(ct.get_params()['trans1__with_mean'])
+
+    ct.set_params(trans1='passthrough')
+    exp = {'n_jobs': 1,
+           'remainder': 'passthrough',
+           'trans1': 'passthrough',
+           'trans2': ct.transformers[1][1],
+           'trans2__copy': True,
+           'trans2__with_mean': True,
+           'trans2__with_std': True,
+           'transformers': ct.transformers,
+           'transformer_weights': None}
+
+    assert_dict_equal(ct.get_params(), exp)
+
+
+def test_column_transformer_named_estimators():
+    X_array = np.array([[0., 1., 2.], [2., 4., 6.]]).T
+    ct = ColumnTransformer([('trans1', StandardScaler(), [0]),
+                            ('trans2', StandardScaler(with_std=False), [1])])
+    assert_false(hasattr(ct, 'transformers_'))
+    ct.fit(X_array)
+    assert_true(hasattr(ct, 'transformers_'))
+    assert_true(isinstance(ct.named_transformers_['trans1'], StandardScaler))
+    assert_true(isinstance(ct.named_transformers_.trans1, StandardScaler))
+    assert_true(isinstance(ct.named_transformers_['trans2'], StandardScaler))
+    assert_true(isinstance(ct.named_transformers_.trans2, StandardScaler))
+    assert_false(ct.named_transformers_.trans2.with_std)
+    # check it are fitted transformers
+    assert_equal(ct.named_transformers_.trans1.mean_, 1.)
+
+
+def test_column_transformer_cloning():
+    X_array = np.array([[0., 1., 2.], [2., 4., 6.]]).T
+
+    ct = ColumnTransformer([('trans', StandardScaler(), [0])])
+    ct.fit(X_array)
+    assert_false(hasattr(ct.transformers[0][1], 'mean_'))
+    assert_true(hasattr(ct.transformers_[0][1], 'mean_'))
+
+    ct = ColumnTransformer([('trans', StandardScaler(), [0])])
+    ct.fit_transform(X_array)
+    assert_false(hasattr(ct.transformers[0][1], 'mean_'))
+    assert_true(hasattr(ct.transformers_[0][1], 'mean_'))
+
+
+def test_column_transformer_get_feature_names():
+    X_array = np.array([[0., 1., 2.], [2., 4., 6.]]).T
+    ct = ColumnTransformer([('trans', Trans(), [0, 1])])
+    # raise correct error when not fitted
+    assert_raises(NotFittedError, ct.get_feature_names)
+    # raise correct error when no feature names are available
+    ct.fit(X_array)
+    assert_raise_message(AttributeError,
+                         "Transformer trans (type Trans) does not provide "
+                         "get_feature_names", ct.get_feature_names)
+
+    # working example
+    X = np.array([[{'a': 1, 'b': 2}, {'a': 3, 'b': 4}],
+                  [{'c': 5}, {'c': 6}]], dtype=object).T
+    ct = ColumnTransformer(
+        [('col' + str(i), DictVectorizer(), i) for i in range(2)])
+    ct.fit(X)
+    assert_equal(ct.get_feature_names(), ['col0__a', 'col0__b', 'col1__c'])
+
+    # passthrough transformers not supported
+    ct = ColumnTransformer([('trans', 'passthrough', [0, 1])])
+    ct.fit(X)
+    assert_raise_message(
+        NotImplementedError, 'get_feature_names is not yet supported',
+        ct.get_feature_names)
+
+    ct = ColumnTransformer([('trans', DictVectorizer(), 0)])
+    ct.fit(X)
+    assert_raise_message(
+        NotImplementedError, 'get_feature_names is not yet supported',
+        ct.get_feature_names)
+
+    # drop transformer
+    ct = ColumnTransformer(
+        [('col0', DictVectorizer(), 0), ('col1', 'drop', 1)])
+    ct.fit(X)
+    assert_equal(ct.get_feature_names(), ['col0__a', 'col0__b'])
+
+
+def test_column_transformer_special_strings():
+
+    # one 'drop' -> ignore
+    X_array = np.array([[0., 1., 2.], [2., 4., 6.]]).T
+    ct = ColumnTransformer(
+        [('trans1', Trans(), [0]), ('trans2', 'drop', [1])])
+    exp = np.array([[0.], [1.], [2.]])
+    assert_array_equal(ct.fit_transform(X_array), exp)
+    assert_array_equal(ct.fit(X_array).transform(X_array), exp)
+
+    # all 'drop' -> return shape 0 array
+    ct = ColumnTransformer(
+        [('trans1', 'drop', [0]), ('trans2', 'drop', [1])])
+    assert_array_equal(ct.fit(X_array).transform(X_array).shape, (3, 0))
+    assert_array_equal(ct.fit_transform(X_array).shape, (3, 0))
+
+    # 'passthrough'
+    X_array = np.array([[0., 1., 2.], [2., 4., 6.]]).T
+    ct = ColumnTransformer(
+        [('trans1', Trans(), [0]), ('trans2', 'passthrough', [1])])
+    exp = X_array
+    assert_array_equal(ct.fit_transform(X_array), exp)
+    assert_array_equal(ct.fit(X_array).transform(X_array), exp)
+
+    # None itself / other string is not valid
+    for val in [None, 'other']:
+        ct = ColumnTransformer(
+            [('trans1', Trans(), [0]), ('trans2', None, [1])])
+        assert_raise_message(TypeError, "All estimators should implement",
+                             ct.fit_transform, X_array)
+        assert_raise_message(TypeError, "All estimators should implement",
+                             ct.fit, X_array)
+
+
+def test_column_transformer_remainder():
+    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T
+
+    X_res_first = np.array([0, 1, 2]).reshape(-1, 1)
+    X_res_second = np.array([2, 4, 6]).reshape(-1, 1)
+    X_res_both = X_array
+
+    # default passthrough
+    ct = ColumnTransformer([('trans', Trans(), [0])])
+    assert_array_equal(ct.fit_transform(X_array), X_res_both)
+    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)
+
+    # specify to drop remaining columns
+    ct = ColumnTransformer([('trans1', Trans(), [0])],
+                           remainder='drop')
+    assert_array_equal(ct.fit_transform(X_array), X_res_first)
+    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_first)
+
+    # column order is not preserved (passed through added to end)
+    ct = ColumnTransformer([('trans1', Trans(), [1])],
+                           remainder='passthrough')
+    assert_array_equal(ct.fit_transform(X_array), X_res_both[:, ::-1])
+    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both[:, ::-1])
+
+    # passthrough when all actual transformers are skipped
+    ct = ColumnTransformer([('trans1', 'drop', [0])],
+                           remainder='passthrough')
+    assert_array_equal(ct.fit_transform(X_array), X_res_second)
+    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_second)
+
+    # error on invalid arg
+    ct = ColumnTransformer([('trans1', Trans(), [0])], remainder=1)
+    assert_raise_message(
+        ValueError,
+        "remainder keyword needs to be one of \'drop\' or \'passthrough\'",
+        ct.fit, X_array)
+    assert_raise_message(
+        ValueError,
+        "remainder keyword needs to be one of \'drop\' or \'passthrough\'",
+        ct.fit_transform, X_array)
+
+
+@pytest.mark.parametrize("key", [[0], np.array([0]), slice(0, 1),
+                                 np.array([True, False])])
+def test_column_transformer_remainder_numpy(key):
+    # test different ways that columns are specified with passthrough
+    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T
+    X_res_both = X_array
+
+    ct = ColumnTransformer([('trans1', Trans(), key)],
+                           remainder='passthrough')
+    assert_array_equal(ct.fit_transform(X_array), X_res_both)
+    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)
+
+
+@pytest.mark.parametrize(
+    "key", [[0], slice(0, 1), np.array([True, False]), ['first'], 'pd-index',
+            np.array(['first']), np.array(['first'], dtype=object),
+            slice(None, 'first'), slice('first', 'first')])
+def test_column_transformer_remainder_pandas(key):
+    # test different ways that columns are specified with passthrough
+    pd = pytest.importorskip('pandas')
+    if isinstance(key, six.string_types) and key == 'pd-index':
+        key = pd.Index(['first'])
+
+    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T
+    X_df = pd.DataFrame(X_array, columns=['first', 'second'])
+    X_res_both = X_array
+
+    ct = ColumnTransformer([('trans1', Trans(), key)],
+                           remainder='passthrough')
+    assert_array_equal(ct.fit_transform(X_df), X_res_both)
+    assert_array_equal(ct.fit(X_df).transform(X_df), X_res_both)
diff --git a/sklearn/covariance/elliptic_envelope.py b/sklearn/covariance/elliptic_envelope.py
index 874155bdd28d..0f9075abfb2e 100644
--- a/sklearn/covariance/elliptic_envelope.py
+++ b/sklearn/covariance/elliptic_envelope.py
@@ -64,7 +64,7 @@ class EllipticEnvelope(MinCovDet, OutlierMixin):
 
     offset_ : float
         Offset used to define the decision function from the raw scores.
-        We have the relation: decision_function = score_samples - offset_.
+        We have the relation: ``decision_function = score_samples - offset_``.
         The offset depends on the contamination parameter and is defined in
         such a way we obtain the expected number of outliers (samples with
         decision function < 0) in training.
@@ -81,8 +81,9 @@ class EllipticEnvelope(MinCovDet, OutlierMixin):
 
     References
     ----------
-    ..  [1] Rousseeuw, P.J., Van Driessen, K. "A fast algorithm for the minimum
-        covariance determinant estimator" Technometrics 41(3), 212 (1999)
+    .. [1] Rousseeuw, P.J., Van Driessen, K. "A fast algorithm for the
+       minimum covariance determinant estimator" Technometrics 41(3), 212
+       (1999)
 
     """
     def __init__(self, store_precision=True, assume_centered=False,
diff --git a/sklearn/cross_validation.py b/sklearn/cross_validation.py
deleted file mode 100644
index e06466b2fcf2..000000000000
--- a/sklearn/cross_validation.py
+++ /dev/null
@@ -1,2069 +0,0 @@
-"""
-The :mod:`sklearn.cross_validation` module includes utilities for cross-
-validation and performance evaluation.
-"""
-
-# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
-#         Gael Varoquaux <gael.varoquaux@normalesup.org>,
-#         Olivier Grisel <olivier.grisel@ensta.org>
-# License: BSD 3 clause
-
-from __future__ import print_function
-from __future__ import division
-
-import warnings
-from itertools import chain, combinations
-from math import ceil, floor, factorial
-import numbers
-import time
-from abc import ABCMeta, abstractmethod
-
-import numpy as np
-import scipy.sparse as sp
-
-from .base import is_classifier, clone
-from .utils import indexable, check_random_state, safe_indexing
-from .utils.validation import (_is_arraylike, _num_samples,
-                               column_or_1d)
-from .utils.multiclass import type_of_target
-from .externals.joblib import Parallel, delayed, logger
-from .externals.six import with_metaclass
-from .externals.six.moves import zip
-from .metrics.scorer import check_scoring
-from .gaussian_process.kernels import Kernel as GPKernel
-from .exceptions import FitFailedWarning
-
-
-warnings.warn("This module was deprecated in version 0.18 in favor of the "
-              "model_selection module into which all the refactored classes "
-              "and functions are moved. Also note that the interface of the "
-              "new CV iterators are different from that of this module. "
-              "This module will be removed in 0.20.", DeprecationWarning)
-
-
-__all__ = ['KFold',
-           'LabelKFold',
-           'LeaveOneLabelOut',
-           'LeaveOneOut',
-           'LeavePLabelOut',
-           'LeavePOut',
-           'ShuffleSplit',
-           'StratifiedKFold',
-           'StratifiedShuffleSplit',
-           'PredefinedSplit',
-           'LabelShuffleSplit',
-           'check_cv',
-           'cross_val_score',
-           'cross_val_predict',
-           'permutation_test_score',
-           'train_test_split']
-
-
-class _PartitionIterator(with_metaclass(ABCMeta)):
-    """Base class for CV iterators where train_mask = ~test_mask
-
-    Implementations must define `_iter_test_masks` or `_iter_test_indices`.
-
-    Parameters
-    ----------
-    n : int
-        Total number of elements in dataset.
-    """
-
-    def __init__(self, n):
-        if abs(n - int(n)) >= np.finfo('f').eps:
-            raise ValueError("n must be an integer")
-        self.n = int(n)
-
-    def __iter__(self):
-        ind = np.arange(self.n)
-        for test_index in self._iter_test_masks():
-            train_index = np.logical_not(test_index)
-            train_index = ind[train_index]
-            test_index = ind[test_index]
-            yield train_index, test_index
-
-    # Since subclasses must implement either _iter_test_masks or
-    # _iter_test_indices, neither can be abstract.
-    def _iter_test_masks(self):
-        """Generates boolean masks corresponding to test sets.
-
-        By default, delegates to _iter_test_indices()
-        """
-        for test_index in self._iter_test_indices():
-            test_mask = self._empty_mask()
-            test_mask[test_index] = True
-            yield test_mask
-
-    def _iter_test_indices(self):
-        """Generates integer indices corresponding to test sets."""
-        raise NotImplementedError
-
-    def _empty_mask(self):
-        return np.zeros(self.n, dtype=np.bool)
-
-
-class LeaveOneOut(_PartitionIterator):
-    """Leave-One-Out cross validation iterator.
-
-    .. deprecated:: 0.18
-        This module will be removed in 0.20.
-        Use :class:`sklearn.model_selection.LeaveOneOut` instead.
-
-    Provides train/test indices to split data in train test sets. Each
-    sample is used once as a test set (singleton) while the remaining
-    samples form the training set.
-
-    Note: ``LeaveOneOut(n)`` is equivalent to ``KFold(n, n_folds=n)`` and
-    ``LeavePOut(n, p=1)``.
-
-    Due to the high number of test sets (which is the same as the
-    number of samples) this cross validation method can be very costly.
-    For large datasets one should favor KFold, StratifiedKFold or
-    ShuffleSplit.
-
-    Read more in the :ref:`User Guide <cross_validation>`.
-
-    Parameters
-    ----------
-    n : int
-        Total number of elements in dataset.
-
-    Examples
-    --------
-    >>> from sklearn import cross_validation
-    >>> X = np.array([[1, 2], [3, 4]])
-    >>> y = np.array([1, 2])
-    >>> loo = cross_validation.LeaveOneOut(2)
-    >>> len(loo)
-    2
-    >>> print(loo)
-    sklearn.cross_validation.LeaveOneOut(n=2)
-    >>> for train_index, test_index in loo:
-    ...    print("TRAIN:", train_index, "TEST:", test_index)
-    ...    X_train, X_test = X[train_index], X[test_index]
-    ...    y_train, y_test = y[train_index], y[test_index]
-    ...    print(X_train, X_test, y_train, y_test)
-    TRAIN: [1] TEST: [0]
-    [[3 4]] [[1 2]] [2] [1]
-    TRAIN: [0] TEST: [1]
-    [[1 2]] [[3 4]] [1] [2]
-
-    See also
-    --------
-    LeaveOneLabelOut for splitting the data according to explicit,
-    domain-specific stratification of the dataset.
-    """
-
-    def _iter_test_indices(self):
-        return range(self.n)
-
-    def __repr__(self):
-        return '%s.%s(n=%i)' % (
-            self.__class__.__module__,
-            self.__class__.__name__,
-            self.n,
-        )
-
-    def __len__(self):
-        return self.n
-
-
-class LeavePOut(_PartitionIterator):
-    """Leave-P-Out cross validation iterator
-
-    .. deprecated:: 0.18
-        This module will be removed in 0.20.
-        Use :class:`sklearn.model_selection.LeavePOut` instead.
-
-    Provides train/test indices to split data in train test sets. This results
-    in testing on all distinct samples of size p, while the remaining n - p
-    samples form the training set in each iteration.
-
-    Note: ``LeavePOut(n, p)`` is NOT equivalent to ``KFold(n, n_folds=n // p)``
-    which creates non-overlapping test sets.
-
-    Due to the high number of iterations which grows combinatorically with the
-    number of samples this cross validation method can be very costly. For
-    large datasets one should favor KFold, StratifiedKFold or ShuffleSplit.
-
-    Read more in the :ref:`User Guide <cross_validation>`.
-
-    Parameters
-    ----------
-    n : int
-        Total number of elements in dataset.
-
-    p : int
-        Size of the test sets.
-
-    Examples
-    --------
-    >>> from sklearn import cross_validation
-    >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
-    >>> y = np.array([1, 2, 3, 4])
-    >>> lpo = cross_validation.LeavePOut(4, 2)
-    >>> len(lpo)
-    6
-    >>> print(lpo)
-    sklearn.cross_validation.LeavePOut(n=4, p=2)
-    >>> for train_index, test_index in lpo:
-    ...    print("TRAIN:", train_index, "TEST:", test_index)
-    ...    X_train, X_test = X[train_index], X[test_index]
-    ...    y_train, y_test = y[train_index], y[test_index]
-    TRAIN: [2 3] TEST: [0 1]
-    TRAIN: [1 3] TEST: [0 2]
-    TRAIN: [1 2] TEST: [0 3]
-    TRAIN: [0 3] TEST: [1 2]
-    TRAIN: [0 2] TEST: [1 3]
-    TRAIN: [0 1] TEST: [2 3]
-    """
-
-    def __init__(self, n, p):
-        super(LeavePOut, self).__init__(n)
-        self.p = p
-
-    def _iter_test_indices(self):
-        for comb in combinations(range(self.n), self.p):
-            yield np.array(comb)
-
-    def __repr__(self):
-        return '%s.%s(n=%i, p=%i)' % (
-            self.__class__.__module__,
-            self.__class__.__name__,
-            self.n,
-            self.p,
-        )
-
-    def __len__(self):
-        return int(factorial(self.n) / factorial(self.n - self.p)
-                   / factorial(self.p))
-
-
-class _BaseKFold(with_metaclass(ABCMeta, _PartitionIterator)):
-    """Base class to validate KFold approaches"""
-
-    @abstractmethod
-    def __init__(self, n, n_folds, shuffle, random_state):
-        super(_BaseKFold, self).__init__(n)
-
-        if abs(n_folds - int(n_folds)) >= np.finfo('f').eps:
-            raise ValueError("n_folds must be an integer")
-        self.n_folds = n_folds = int(n_folds)
-
-        if n_folds <= 1:
-            raise ValueError(
-                "k-fold cross validation requires at least one"
-                " train / test split by setting n_folds=2 or more,"
-                " got n_folds={0}.".format(n_folds))
-        if n_folds > self.n:
-            raise ValueError(
-                ("Cannot have number of folds n_folds={0} greater"
-                 " than the number of samples: {1}.").format(n_folds, n))
-
-        if not isinstance(shuffle, bool):
-            raise TypeError("shuffle must be True or False;"
-                            " got {0}".format(shuffle))
-        self.shuffle = shuffle
-        self.random_state = random_state
-
-
-class KFold(_BaseKFold):
-    """K-Folds cross validation iterator.
-
-    .. deprecated:: 0.18
-        This module will be removed in 0.20.
-        Use :class:`sklearn.model_selection.KFold` instead.
-
-    Provides train/test indices to split data in train test sets. Split
-    dataset into k consecutive folds (without shuffling by default).
-
-    Each fold is then used as a validation set once while the k - 1 remaining
-    fold(s) form the training set.
-
-    Read more in the :ref:`User Guide <cross_validation>`.
-
-    Parameters
-    ----------
-    n : int
-        Total number of elements.
-
-    n_folds : int, default=3
-        Number of folds. Must be at least 2.
-
-    shuffle : boolean, optional
-        Whether to shuffle the data before splitting into batches.
-
-    random_state : int, RandomState instance or None, optional, default=None
-        If int, random_state is the seed used by the random number
-        generator; If RandomState instance, random_state is the random number
-        generator; If None, the random number generator is the RandomState
-        instance used by `np.random`. Used when ``shuffle`` == True.
-
-    Examples
-    --------
-    >>> from sklearn.cross_validation import KFold
-    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
-    >>> y = np.array([1, 2, 3, 4])
-    >>> kf = KFold(4, n_folds=2)
-    >>> len(kf)
-    2
-    >>> print(kf)  # doctest: +NORMALIZE_WHITESPACE
-    sklearn.cross_validation.KFold(n=4, n_folds=2, shuffle=False,
-                                   random_state=None)
-    >>> for train_index, test_index in kf:
-    ...    print("TRAIN:", train_index, "TEST:", test_index)
-    ...    X_train, X_test = X[train_index], X[test_index]
-    ...    y_train, y_test = y[train_index], y[test_index]
-    TRAIN: [2 3] TEST: [0 1]
-    TRAIN: [0 1] TEST: [2 3]
-
-    Notes
-    -----
-    The first n % n_folds folds have size n // n_folds + 1, other folds have
-    size n // n_folds.
-
-    See also
-    --------
-    StratifiedKFold take label information into account to avoid building
-    folds with imbalanced class distributions (for binary or multiclass
-    classification tasks).
-
-    LabelKFold: K-fold iterator variant with non-overlapping labels.
-    """
-
-    def __init__(self, n, n_folds=3, shuffle=False,
-                 random_state=None):
-        super(KFold, self).__init__(n, n_folds, shuffle, random_state)
-        self.idxs = np.arange(n)
-        if shuffle:
-            rng = check_random_state(self.random_state)
-            rng.shuffle(self.idxs)
-
-    def _iter_test_indices(self):
-        n = self.n
-        n_folds = self.n_folds
-        fold_sizes = (n // n_folds) * np.ones(n_folds, dtype=np.int)
-        fold_sizes[:n % n_folds] += 1
-        current = 0
-        for fold_size in fold_sizes:
-            start, stop = current, current + fold_size
-            yield self.idxs[start:stop]
-            current = stop
-
-    def __repr__(self):
-        return '%s.%s(n=%i, n_folds=%i, shuffle=%s, random_state=%s)' % (
-            self.__class__.__module__,
-            self.__class__.__name__,
-            self.n,
-            self.n_folds,
-            self.shuffle,
-            self.random_state,
-        )
-
-    def __len__(self):
-        return self.n_folds
-
-
-class LabelKFold(_BaseKFold):
-    """K-fold iterator variant with non-overlapping labels.
-
-    .. deprecated:: 0.18
-        This module will be removed in 0.20.
-        Use :class:`sklearn.model_selection.GroupKFold` instead.
-
-    The same label will not appear in two different folds (the number of
-    distinct labels has to be at least equal to the number of folds).
-
-    The folds are approximately balanced in the sense that the number of
-    distinct labels is approximately the same in each fold.
-
-    .. versionadded:: 0.17
-
-    Parameters
-    ----------
-    labels : array-like with shape (n_samples, )
-        Contains a label for each sample.
-        The folds are built so that the same label does not appear in two
-        different folds.
-
-    n_folds : int, default=3
-        Number of folds. Must be at least 2.
-
-    Examples
-    --------
-    >>> from sklearn.cross_validation import LabelKFold
-    >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
-    >>> y = np.array([1, 2, 3, 4])
-    >>> labels = np.array([0, 0, 2, 2])
-    >>> label_kfold = LabelKFold(labels, n_folds=2)
-    >>> len(label_kfold)
-    2
-    >>> print(label_kfold)
-    sklearn.cross_validation.LabelKFold(n_labels=4, n_folds=2)
-    >>> for train_index, test_index in label_kfold:
-    ...     print("TRAIN:", train_index, "TEST:", test_index)
-    ...     X_train, X_test = X[train_index], X[test_index]
-    ...     y_train, y_test = y[train_index], y[test_index]
-    ...     print(X_train, X_test, y_train, y_test)
-    ...
-    TRAIN: [0 1] TEST: [2 3]
-    [[1 2]
-     [3 4]] [[5 6]
-     [7 8]] [1 2] [3 4]
-    TRAIN: [2 3] TEST: [0 1]
-    [[5 6]
-     [7 8]] [[1 2]
-     [3 4]] [3 4] [1 2]
-
-    See also
-    --------
-    LeaveOneLabelOut for splitting the data according to explicit,
-    domain-specific stratification of the dataset.
-    """
-    def __init__(self, labels, n_folds=3):
-        super(LabelKFold, self).__init__(len(labels), n_folds,
-                                         shuffle=False, random_state=None)
-
-        unique_labels, labels = np.unique(labels, return_inverse=True)
-        n_labels = len(unique_labels)
-
-        if n_folds > n_labels:
-            raise ValueError(
-                ("Cannot have number of folds n_folds={0} greater"
-                 " than the number of labels: {1}.").format(n_folds,
-                                                            n_labels))
-
-        # Weight labels by their number of occurrences
-        n_samples_per_label = np.bincount(labels)
-
-        # Distribute the most frequent labels first
-        indices = np.argsort(n_samples_per_label)[::-1]
-        n_samples_per_label = n_samples_per_label[indices]
-
-        # Total weight of each fold
-        n_samples_per_fold = np.zeros(n_folds)
-
-        # Mapping from label index to fold index
-        label_to_fold = np.zeros(len(unique_labels))
-
-        # Distribute samples by adding the largest weight to the lightest fold
-        for label_index, weight in enumerate(n_samples_per_label):
-            lightest_fold = np.argmin(n_samples_per_fold)
-            n_samples_per_fold[lightest_fold] += weight
-            label_to_fold[indices[label_index]] = lightest_fold
-
-        self.idxs = label_to_fold[labels]
-
-    def _iter_test_indices(self):
-        for f in range(self.n_folds):
-            yield np.where(self.idxs == f)[0]
-
-    def __repr__(self):
-        return '{0}.{1}(n_labels={2}, n_folds={3})'.format(
-            self.__class__.__module__,
-            self.__class__.__name__,
-            self.n,
-            self.n_folds,
-        )
-
-    def __len__(self):
-        return self.n_folds
-
-
-class StratifiedKFold(_BaseKFold):
-    """Stratified K-Folds cross validation iterator
-
-    .. deprecated:: 0.18
-        This module will be removed in 0.20.
-        Use :class:`sklearn.model_selection.StratifiedKFold` instead.
-
-    Provides train/test indices to split data in train test sets.
-
-    This cross-validation object is a variation of KFold that
-    returns stratified folds. The folds are made by preserving
-    the percentage of samples for each class.
-
-    Read more in the :ref:`User Guide <cross_validation>`.
-
-    Parameters
-    ----------
-    y : array-like, [n_samples]
-        Samples to split in K folds.
-
-    n_folds : int, default=3
-        Number of folds. Must be at least 2.
-
-    shuffle : boolean, optional
-        Whether to shuffle each stratification of the data before splitting
-        into batches.
-
-    random_state : int, RandomState instance or None, optional, default=None
-        If int, random_state is the seed used by the random number
-        generator; If RandomState instance, random_state is the random number
-        generator; If None, the random number generator is the RandomState
-        instance used by `np.random`. Used when ``shuffle`` == True.
-
-    Examples
-    --------
-    >>> from sklearn.cross_validation import StratifiedKFold
-    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
-    >>> y = np.array([0, 0, 1, 1])
-    >>> skf = StratifiedKFold(y, n_folds=2)
-    >>> len(skf)
-    2
-    >>> print(skf)  # doctest: +NORMALIZE_WHITESPACE
-    sklearn.cross_validation.StratifiedKFold(labels=[0 0 1 1], n_folds=2,
-                                             shuffle=False, random_state=None)
-    >>> for train_index, test_index in skf:
-    ...    print("TRAIN:", train_index, "TEST:", test_index)
-    ...    X_train, X_test = X[train_index], X[test_index]
-    ...    y_train, y_test = y[train_index], y[test_index]
-    TRAIN: [1 3] TEST: [0 2]
-    TRAIN: [0 2] TEST: [1 3]
-
-    Notes
-    -----
-    Train and test sizes may be different in each fold, with a difference of at
-    most ``n_classes``.
-
-    See also
-    --------
-    LabelKFold: K-fold iterator variant with non-overlapping labels.
-    """
-
-    def __init__(self, y, n_folds=3, shuffle=False,
-                 random_state=None):
-        super(StratifiedKFold, self).__init__(
-            len(y), n_folds, shuffle, random_state)
-        y = np.asarray(y)
-        n_samples = y.shape[0]
-        unique_labels, y_inversed = np.unique(y, return_inverse=True)
-        label_counts = np.bincount(y_inversed)
-        min_labels = np.min(label_counts)
-        if np.all(self.n_folds > label_counts):
-            raise ValueError("All the n_labels for individual classes"
-                             " are less than %d folds."
-                             % (self.n_folds))
-        if self.n_folds > min_labels:
-            warnings.warn(("The least populated class in y has only %d"
-                           " members, which is too few. The minimum"
-                           " number of labels for any class cannot"
-                           " be less than n_folds=%d."
-                           % (min_labels, self.n_folds)), Warning)
-
-        # don't want to use the same seed in each label's shuffle
-        if self.shuffle:
-            rng = check_random_state(self.random_state)
-        else:
-            rng = self.random_state
-
-        # pre-assign each sample to a test fold index using individual KFold
-        # splitting strategies for each label so as to respect the
-        # balance of labels
-        per_label_cvs = [
-            KFold(max(c, self.n_folds), self.n_folds, shuffle=self.shuffle,
-                  random_state=rng) for c in label_counts]
-        test_folds = np.zeros(n_samples, dtype=np.int)
-        for test_fold_idx, per_label_splits in enumerate(zip(*per_label_cvs)):
-            for label, (_, test_split) in zip(unique_labels, per_label_splits):
-                label_test_folds = test_folds[y == label]
-                # the test split can be too big because we used
-                # KFold(max(c, self.n_folds), self.n_folds) instead of
-                # KFold(c, self.n_folds) to make it possible to not crash even
-                # if the data is not 100% stratifiable for all the labels
-                # (we use a warning instead of raising an exception)
-                # If this is the case, let's trim it:
-                test_split = test_split[test_split < len(label_test_folds)]
-                label_test_folds[test_split] = test_fold_idx
-                test_folds[y == label] = label_test_folds
-
-        self.test_folds = test_folds
-        self.y = y
-
-    def _iter_test_masks(self):
-        for i in range(self.n_folds):
-            yield self.test_folds == i
-
-    def __repr__(self):
-        return '%s.%s(labels=%s, n_folds=%i, shuffle=%s, random_state=%s)' % (
-            self.__class__.__module__,
-            self.__class__.__name__,
-            self.y,
-            self.n_folds,
-            self.shuffle,
-            self.random_state,
-        )
-
-    def __len__(self):
-        return self.n_folds
-
-
-class LeaveOneLabelOut(_PartitionIterator):
-    """Leave-One-Label_Out cross-validation iterator
-
-    .. deprecated:: 0.18
-        This module will be removed in 0.20.
-        Use :class:`sklearn.model_selection.LeaveOneGroupOut` instead.
-
-    Provides train/test indices to split data according to a third-party
-    provided label. This label information can be used to encode arbitrary
-    domain specific stratifications of the samples as integers.
-
-    For instance the labels could be the year of collection of the samples
-    and thus allow for cross-validation against time-based splits.
-
-    Read more in the :ref:`User Guide <cross_validation>`.
-
-    Parameters
-    ----------
-    labels : array-like of int with shape (n_samples,)
-        Arbitrary domain-specific stratification of the data to be used
-        to draw the splits.
-
-    Examples
-    --------
-    >>> from sklearn import cross_validation
-    >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
-    >>> y = np.array([1, 2, 1, 2])
-    >>> labels = np.array([1, 1, 2, 2])
-    >>> lol = cross_validation.LeaveOneLabelOut(labels)
-    >>> len(lol)
-    2
-    >>> print(lol)
-    sklearn.cross_validation.LeaveOneLabelOut(labels=[1 1 2 2])
-    >>> for train_index, test_index in lol:
-    ...    print("TRAIN:", train_index, "TEST:", test_index)
-    ...    X_train, X_test = X[train_index], X[test_index]
-    ...    y_train, y_test = y[train_index], y[test_index]
-    ...    print(X_train, X_test, y_train, y_test)
-    TRAIN: [2 3] TEST: [0 1]
-    [[5 6]
-     [7 8]] [[1 2]
-     [3 4]] [1 2] [1 2]
-    TRAIN: [0 1] TEST: [2 3]
-    [[1 2]
-     [3 4]] [[5 6]
-     [7 8]] [1 2] [1 2]
-
-    See also
-    --------
-    LabelKFold: K-fold iterator variant with non-overlapping labels.
-    """
-
-    def __init__(self, labels):
-        super(LeaveOneLabelOut, self).__init__(len(labels))
-        # We make a copy of labels to avoid side-effects during iteration
-        self.labels = np.array(labels, copy=True)
-        self.unique_labels = np.unique(labels)
-        self.n_unique_labels = len(self.unique_labels)
-
-    def _iter_test_masks(self):
-        for i in self.unique_labels:
-            yield self.labels == i
-
-    def __repr__(self):
-        return '%s.%s(labels=%s)' % (
-            self.__class__.__module__,
-            self.__class__.__name__,
-            self.labels,
-        )
-
-    def __len__(self):
-        return self.n_unique_labels
-
-
-class LeavePLabelOut(_PartitionIterator):
-    """Leave-P-Label_Out cross-validation iterator
-
-    .. deprecated:: 0.18
-        This module will be removed in 0.20.
-        Use :class:`sklearn.model_selection.LeavePGroupsOut` instead.
-
-    Provides train/test indices to split data according to a third-party
-    provided label. This label information can be used to encode arbitrary
-    domain specific stratifications of the samples as integers.
-
-    For instance the labels could be the year of collection of the samples
-    and thus allow for cross-validation against time-based splits.
-
-    The difference between LeavePLabelOut and LeaveOneLabelOut is that
-    the former builds the test sets with all the samples assigned to
-    ``p`` different values of the labels while the latter uses samples
-    all assigned the same labels.
-
-    Read more in the :ref:`User Guide <cross_validation>`.
-
-    Parameters
-    ----------
-    labels : array-like of int with shape (n_samples,)
-        Arbitrary domain-specific stratification of the data to be used
-        to draw the splits.
-
-    p : int
-        Number of samples to leave out in the test split.
-
-    Examples
-    --------
-    >>> from sklearn import cross_validation
-    >>> X = np.array([[1, 2], [3, 4], [5, 6]])
-    >>> y = np.array([1, 2, 1])
-    >>> labels = np.array([1, 2, 3])
-    >>> lpl = cross_validation.LeavePLabelOut(labels, p=2)
-    >>> len(lpl)
-    3
-    >>> print(lpl)
-    sklearn.cross_validation.LeavePLabelOut(labels=[1 2 3], p=2)
-    >>> for train_index, test_index in lpl:
-    ...    print("TRAIN:", train_index, "TEST:", test_index)
-    ...    X_train, X_test = X[train_index], X[test_index]
-    ...    y_train, y_test = y[train_index], y[test_index]
-    ...    print(X_train, X_test, y_train, y_test)
-    TRAIN: [2] TEST: [0 1]
-    [[5 6]] [[1 2]
-     [3 4]] [1] [1 2]
-    TRAIN: [1] TEST: [0 2]
-    [[3 4]] [[1 2]
-     [5 6]] [2] [1 1]
-    TRAIN: [0] TEST: [1 2]
-    [[1 2]] [[3 4]
-     [5 6]] [1] [2 1]
-
-    See also
-    --------
-    LabelKFold: K-fold iterator variant with non-overlapping labels.
-    """
-
-    def __init__(self, labels, p):
-        # We make a copy of labels to avoid side-effects during iteration
-        super(LeavePLabelOut, self).__init__(len(labels))
-        self.labels = np.array(labels, copy=True)
-        self.unique_labels = np.unique(labels)
-        self.n_unique_labels = len(self.unique_labels)
-        self.p = p
-
-    def _iter_test_masks(self):
-        comb = combinations(range(self.n_unique_labels), self.p)
-        for idx in comb:
-            test_index = self._empty_mask()
-            idx = np.array(idx)
-            for l in self.unique_labels[idx]:
-                test_index[self.labels == l] = True
-            yield test_index
-
-    def __repr__(self):
-        return '%s.%s(labels=%s, p=%s)' % (
-            self.__class__.__module__,
-            self.__class__.__name__,
-            self.labels,
-            self.p,
-        )
-
-    def __len__(self):
-        return int(factorial(self.n_unique_labels) /
-                   factorial(self.n_unique_labels - self.p) /
-                   factorial(self.p))
-
-
-class BaseShuffleSplit(with_metaclass(ABCMeta)):
-    """Base class for ShuffleSplit and StratifiedShuffleSplit"""
-
-    def __init__(self, n, n_iter=10, test_size=0.1, train_size=None,
-                 random_state=None):
-        self.n = n
-        self.n_iter = n_iter
-        self.test_size = test_size
-        self.train_size = train_size
-        self.random_state = random_state
-        self.n_train, self.n_test = _validate_shuffle_split(n, test_size,
-                                                            train_size)
-
-    def __iter__(self):
-        for train, test in self._iter_indices():
-            yield train, test
-        return
-
-    @abstractmethod
-    def _iter_indices(self):
-        """Generate (train, test) indices"""
-
-
-class ShuffleSplit(BaseShuffleSplit):
-    """Random permutation cross-validation iterator.
-
-    .. deprecated:: 0.18
-        This module will be removed in 0.20.
-        Use :class:`sklearn.model_selection.ShuffleSplit` instead.
-
-    Yields indices to split data into training and test sets.
-
-    Note: contrary to other cross-validation strategies, random splits
-    do not guarantee that all folds will be different, although this is
-    still very likely for sizeable datasets.
-
-    Read more in the :ref:`User Guide <cross_validation>`.
-
-    Parameters
-    ----------
-    n : int
-        Total number of elements in the dataset.
-
-    n_iter : int (default 10)
-        Number of re-shuffling & splitting iterations.
-
-    test_size : float (default 0.1), int, or None
-        If float, should be between 0.0 and 1.0 and represent the
-        proportion of the dataset to include in the test split. If
-        int, represents the absolute number of test samples. If None,
-        the value is automatically set to the complement of the train size.
-
-    train_size : float, int, or None (default is None)
-        If float, should be between 0.0 and 1.0 and represent the
-        proportion of the dataset to include in the train split. If
-        int, represents the absolute number of train samples. If None,
-        the value is automatically set to the complement of the test size.
-
-    random_state : int, RandomState instance or None, optional (default None)
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
-
-    Examples
-    --------
-    >>> from sklearn import cross_validation
-    >>> rs = cross_validation.ShuffleSplit(4, n_iter=3,
-    ...     test_size=.25, random_state=0)
-    >>> len(rs)
-    3
-    >>> print(rs)
-    ... # doctest: +ELLIPSIS
-    ShuffleSplit(4, n_iter=3, test_size=0.25, ...)
-    >>> for train_index, test_index in rs:
-    ...    print("TRAIN:", train_index, "TEST:", test_index)
-    ...
-    TRAIN: [3 1 0] TEST: [2]
-    TRAIN: [2 1 3] TEST: [0]
-    TRAIN: [0 2 1] TEST: [3]
-
-    >>> rs = cross_validation.ShuffleSplit(4, n_iter=3,
-    ...     train_size=0.5, test_size=.25, random_state=0)
-    >>> for train_index, test_index in rs:
-    ...    print("TRAIN:", train_index, "TEST:", test_index)
-    ...
-    TRAIN: [3 1] TEST: [2]
-    TRAIN: [2 1] TEST: [0]
-    TRAIN: [0 2] TEST: [3]
-
-    """
-
-    def _iter_indices(self):
-        rng = check_random_state(self.random_state)
-        for i in range(self.n_iter):
-            # random partition
-            permutation = rng.permutation(self.n)
-            ind_test = permutation[:self.n_test]
-            ind_train = permutation[self.n_test:self.n_test + self.n_train]
-            yield ind_train, ind_test
-
-    def __repr__(self):
-        return ('%s(%d, n_iter=%d, test_size=%s, '
-                'random_state=%s)' % (
-                    self.__class__.__name__,
-                    self.n,
-                    self.n_iter,
-                    str(self.test_size),
-                    self.random_state,
-                ))
-
-    def __len__(self):
-        return self.n_iter
-
-
-def _validate_shuffle_split(n, test_size, train_size):
-    if test_size is None and train_size is None:
-        raise ValueError(
-            'test_size and train_size can not both be None')
-
-    if test_size is not None:
-        if np.asarray(test_size).dtype.kind == 'f':
-            if test_size >= 1.:
-                raise ValueError(
-                    'test_size=%f should be smaller '
-                    'than 1.0 or be an integer' % test_size)
-        elif np.asarray(test_size).dtype.kind == 'i':
-            if test_size >= n:
-                raise ValueError(
-                    'test_size=%d should be smaller '
-                    'than the number of samples %d' % (test_size, n))
-        else:
-            raise ValueError("Invalid value for test_size: %r" % test_size)
-
-    if train_size is not None:
-        if np.asarray(train_size).dtype.kind == 'f':
-            if train_size >= 1.:
-                raise ValueError("train_size=%f should be smaller "
-                                 "than 1.0 or be an integer" % train_size)
-            elif np.asarray(test_size).dtype.kind == 'f' and \
-                    train_size + test_size > 1.:
-                raise ValueError('The sum of test_size and train_size = %f, '
-                                 'should be smaller than 1.0. Reduce '
-                                 'test_size and/or train_size.' %
-                                 (train_size + test_size))
-        elif np.asarray(train_size).dtype.kind == 'i':
-            if train_size >= n:
-                raise ValueError("train_size=%d should be smaller "
-                                 "than the number of samples %d" %
-                                 (train_size, n))
-        else:
-            raise ValueError("Invalid value for train_size: %r" % train_size)
-
-    if np.asarray(test_size).dtype.kind == 'f':
-        n_test = ceil(test_size * n)
-    elif np.asarray(test_size).dtype.kind == 'i':
-        n_test = float(test_size)
-
-    if train_size is None:
-        n_train = n - n_test
-    else:
-        if np.asarray(train_size).dtype.kind == 'f':
-            n_train = floor(train_size * n)
-        else:
-            n_train = float(train_size)
-
-    if test_size is None:
-        n_test = n - n_train
-
-    if n_train + n_test > n:
-        raise ValueError('The sum of train_size and test_size = %d, '
-                         'should be smaller than the number of '
-                         'samples %d. Reduce test_size and/or '
-                         'train_size.' % (n_train + n_test, n))
-
-    return int(n_train), int(n_test)
-
-
-def _approximate_mode(class_counts, n_draws, rng):
-    """Computes approximate mode of multivariate hypergeometric.
-
-    This is an approximation to the mode of the multivariate
-    hypergeometric given by class_counts and n_draws.
-    It shouldn't be off by more than one.
-
-    It is the mostly likely outcome of drawing n_draws many
-    samples from the population given by class_counts.
-
-    Parameters
-    ----------
-    class_counts : ndarray of int
-        Population per class.
-    n_draws : int
-        Number of draws (samples to draw) from the overall population.
-    rng : random state
-        Used to break ties.
-
-    Returns
-    -------
-    sampled_classes : ndarray of int
-        Number of samples drawn from each class.
-        np.sum(sampled_classes) == n_draws
-    """
-    # this computes a bad approximation to the mode of the
-    # multivariate hypergeometric given by class_counts and n_draws
-    continuous = n_draws * class_counts / class_counts.sum()
-    # floored means we don't overshoot n_samples, but probably undershoot
-    floored = np.floor(continuous)
-    # we add samples according to how much "left over" probability
-    # they had, until we arrive at n_samples
-    need_to_add = int(n_draws - floored.sum())
-    if need_to_add > 0:
-        remainder = continuous - floored
-        values = np.sort(np.unique(remainder))[::-1]
-        # add according to remainder, but break ties
-        # randomly to avoid biases
-        for value in values:
-            inds, = np.where(remainder == value)
-            # if we need_to_add less than what's in inds
-            # we draw randomly from them.
-            # if we need to add more, we add them all and
-            # go to the next value
-            add_now = min(len(inds), need_to_add)
-            inds = rng.choice(inds, size=add_now, replace=False)
-            floored[inds] += 1
-            need_to_add -= add_now
-            if need_to_add == 0:
-                    break
-    return floored.astype(np.int)
-
-
-class StratifiedShuffleSplit(BaseShuffleSplit):
-    """Stratified ShuffleSplit cross validation iterator
-
-    .. deprecated:: 0.18
-        This module will be removed in 0.20.
-        Use :class:`sklearn.model_selection.StratifiedShuffleSplit` instead.
-
-    Provides train/test indices to split data in train test sets.
-
-    This cross-validation object is a merge of StratifiedKFold and
-    ShuffleSplit, which returns stratified randomized folds. The folds
-    are made by preserving the percentage of samples for each class.
-
-    Note: like the ShuffleSplit strategy, stratified random splits
-    do not guarantee that all folds will be different, although this is
-    still very likely for sizeable datasets.
-
-    Read more in the :ref:`User Guide <cross_validation>`.
-
-    Parameters
-    ----------
-    y : array, [n_samples]
-        Labels of samples.
-
-    n_iter : int (default 10)
-        Number of re-shuffling & splitting iterations.
-
-    test_size : float (default 0.1), int, or None
-        If float, should be between 0.0 and 1.0 and represent the
-        proportion of the dataset to include in the test split. If
-        int, represents the absolute number of test samples. If None,
-        the value is automatically set to the complement of the train size.
-
-    train_size : float, int, or None (default is None)
-        If float, should be between 0.0 and 1.0 and represent the
-        proportion of the dataset to include in the train split. If
-        int, represents the absolute number of train samples. If None,
-        the value is automatically set to the complement of the test size.
-
-    random_state : int, RandomState instance or None, optional (default None)
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
-
-    Examples
-    --------
-    >>> from sklearn.cross_validation import StratifiedShuffleSplit
-    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
-    >>> y = np.array([0, 0, 1, 1])
-    >>> sss = StratifiedShuffleSplit(y, 3, test_size=0.5, random_state=0)
-    >>> len(sss)
-    3
-    >>> print(sss)       # doctest: +ELLIPSIS
-    StratifiedShuffleSplit(labels=[0 0 1 1], n_iter=3, ...)
-    >>> for train_index, test_index in sss:
-    ...    print("TRAIN:", train_index, "TEST:", test_index)
-    ...    X_train, X_test = X[train_index], X[test_index]
-    ...    y_train, y_test = y[train_index], y[test_index]
-    TRAIN: [1 2] TEST: [3 0]
-    TRAIN: [0 2] TEST: [1 3]
-    TRAIN: [0 2] TEST: [3 1]
-    """
-
-    def __init__(self, y, n_iter=10, test_size=0.1, train_size=None,
-                 random_state=None):
-
-        super(StratifiedShuffleSplit, self).__init__(
-            len(y), n_iter, test_size, train_size, random_state)
-
-        self.y = np.array(y)
-        self.classes, self.y_indices = np.unique(y, return_inverse=True)
-        n_cls = self.classes.shape[0]
-
-        if np.min(np.bincount(self.y_indices)) < 2:
-            raise ValueError("The least populated class in y has only 1"
-                             " member, which is too few. The minimum"
-                             " number of labels for any class cannot"
-                             " be less than 2.")
-
-        if self.n_train < n_cls:
-            raise ValueError('The train_size = %d should be greater or '
-                             'equal to the number of classes = %d' %
-                             (self.n_train, n_cls))
-        if self.n_test < n_cls:
-            raise ValueError('The test_size = %d should be greater or '
-                             'equal to the number of classes = %d' %
-                             (self.n_test, n_cls))
-
-    def _iter_indices(self):
-        rng = check_random_state(self.random_state)
-        cls_count = np.bincount(self.y_indices)
-
-        for n in range(self.n_iter):
-            # if there are ties in the class-counts, we want
-            # to make sure to break them anew in each iteration
-            n_i = _approximate_mode(cls_count, self.n_train, rng)
-            class_counts_remaining = cls_count - n_i
-            t_i = _approximate_mode(class_counts_remaining, self.n_test, rng)
-
-            train = []
-            test = []
-
-            for i, _ in enumerate(self.classes):
-                permutation = rng.permutation(cls_count[i])
-                perm_indices_class_i = np.where(
-                    (i == self.y_indices))[0][permutation]
-
-                train.extend(perm_indices_class_i[:n_i[i]])
-                test.extend(perm_indices_class_i[n_i[i]:n_i[i] + t_i[i]])
-            train = rng.permutation(train)
-            test = rng.permutation(test)
-
-            yield train, test
-
-    def __repr__(self):
-        return ('%s(labels=%s, n_iter=%d, test_size=%s, '
-                'random_state=%s)' % (
-                    self.__class__.__name__,
-                    self.y,
-                    self.n_iter,
-                    str(self.test_size),
-                    self.random_state,
-                ))
-
-    def __len__(self):
-        return self.n_iter
-
-
-class PredefinedSplit(_PartitionIterator):
-    """Predefined split cross validation iterator
-
-    .. deprecated:: 0.18
-        This module will be removed in 0.20.
-        Use :class:`sklearn.model_selection.PredefinedSplit` instead.
-
-    Splits the data into training/test set folds according to a predefined
-    scheme. Each sample can be assigned to at most one test set fold, as
-    specified by the user through the ``test_fold`` parameter.
-
-    Read more in the :ref:`User Guide <cross_validation>`.
-
-    Parameters
-    ----------
-    test_fold : "array-like, shape (n_samples,)
-        test_fold[i] gives the test set fold of sample i. A value of -1
-        indicates that the corresponding sample is not part of any test set
-        folds, but will instead always be put into the training fold.
-
-    Examples
-    --------
-    >>> from sklearn.cross_validation import PredefinedSplit
-    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
-    >>> y = np.array([0, 0, 1, 1])
-    >>> ps = PredefinedSplit(test_fold=[0, 1, -1, 1])
-    >>> len(ps)
-    2
-    >>> print(ps)       # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
-    sklearn.cross_validation.PredefinedSplit(test_fold=[ 0  1 -1  1])
-    >>> for train_index, test_index in ps:
-    ...    print("TRAIN:", train_index, "TEST:", test_index)
-    ...    X_train, X_test = X[train_index], X[test_index]
-    ...    y_train, y_test = y[train_index], y[test_index]
-    TRAIN: [1 2 3] TEST: [0]
-    TRAIN: [0 2] TEST: [1 3]
-    """
-
-    def __init__(self, test_fold):
-        super(PredefinedSplit, self).__init__(len(test_fold))
-        self.test_fold = np.array(test_fold, dtype=np.int)
-        self.test_fold = column_or_1d(self.test_fold)
-        self.unique_folds = np.unique(self.test_fold)
-        self.unique_folds = self.unique_folds[self.unique_folds != -1]
-
-    def _iter_test_indices(self):
-        for f in self.unique_folds:
-            yield np.where(self.test_fold == f)[0]
-
-    def __repr__(self):
-        return '%s.%s(test_fold=%s)' % (
-            self.__class__.__module__,
-            self.__class__.__name__,
-            self.test_fold)
-
-    def __len__(self):
-        return len(self.unique_folds)
-
-
-class LabelShuffleSplit(ShuffleSplit):
-    """Shuffle-Labels-Out cross-validation iterator
-
-    .. deprecated:: 0.18
-        This module will be removed in 0.20.
-        Use :class:`sklearn.model_selection.GroupShuffleSplit` instead.
-
-    Provides randomized train/test indices to split data according to a
-    third-party provided label. This label information can be used to encode
-    arbitrary domain specific stratifications of the samples as integers.
-
-    For instance the labels could be the year of collection of the samples
-    and thus allow for cross-validation against time-based splits.
-
-    The difference between LeavePLabelOut and LabelShuffleSplit is that
-    the former generates splits using all subsets of size ``p`` unique labels,
-    whereas LabelShuffleSplit generates a user-determined number of random
-    test splits, each with a user-determined fraction of unique labels.
-
-    For example, a less computationally intensive alternative to
-    ``LeavePLabelOut(labels, p=10)`` would be
-    ``LabelShuffleSplit(labels, test_size=10, n_iter=100)``.
-
-    Note: The parameters ``test_size`` and ``train_size`` refer to labels, and
-    not to samples, as in ShuffleSplit.
-
-    .. versionadded:: 0.17
-
-    Parameters
-    ----------
-    labels :  array, [n_samples]
-        Labels of samples
-
-    n_iter : int (default 5)
-        Number of re-shuffling and splitting iterations.
-
-    test_size : float (default 0.2), int, or None
-        If float, should be between 0.0 and 1.0 and represent the
-        proportion of the labels to include in the test split. If
-        int, represents the absolute number of test labels. If None,
-        the value is automatically set to the complement of the train size.
-
-    train_size : float, int, or None (default is None)
-        If float, should be between 0.0 and 1.0 and represent the
-        proportion of the labels to include in the train split. If
-        int, represents the absolute number of train labels. If None,
-        the value is automatically set to the complement of the test size.
-
-    random_state : int, RandomState instance or None, optional (default None)
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
-
-    """
-    def __init__(self, labels, n_iter=5, test_size=0.2, train_size=None,
-                 random_state=None):
-
-        classes, label_indices = np.unique(labels, return_inverse=True)
-
-        super(LabelShuffleSplit, self).__init__(
-            len(classes),
-            n_iter=n_iter,
-            test_size=test_size,
-            train_size=train_size,
-            random_state=random_state)
-
-        self.labels = labels
-        self.classes = classes
-        self.label_indices = label_indices
-
-    def __repr__(self):
-        return ('%s(labels=%s, n_iter=%d, test_size=%s, '
-                'random_state=%s)' % (
-                    self.__class__.__name__,
-                    self.labels,
-                    self.n_iter,
-                    str(self.test_size),
-                    self.random_state,
-                ))
-
-    def __len__(self):
-        return self.n_iter
-
-    def _iter_indices(self):
-        for label_train, label_test in super(LabelShuffleSplit,
-                                             self)._iter_indices():
-            # these are the indices of classes in the partition
-            # invert them into data indices
-
-            train = np.flatnonzero(np.in1d(self.label_indices, label_train))
-            test = np.flatnonzero(np.in1d(self.label_indices, label_test))
-
-            yield train, test
-
-
-##############################################################################
-def _index_param_value(X, v, indices):
-    """Private helper function for parameter value indexing."""
-    if not _is_arraylike(v) or _num_samples(v) != _num_samples(X):
-        # pass through: skip indexing
-        return v
-    if sp.issparse(v):
-        v = v.tocsr()
-    return safe_indexing(v, indices)
-
-
-def cross_val_predict(estimator, X, y=None, cv=None, n_jobs=1,
-                      verbose=0, fit_params=None, pre_dispatch='2*n_jobs'):
-    """Generate cross-validated estimates for each input data point
-
-    .. deprecated:: 0.18
-        This module will be removed in 0.20.
-        Use :func:`sklearn.model_selection.cross_val_predict` instead.
-
-    Read more in the :ref:`User Guide <cross_validation>`.
-
-    Parameters
-    ----------
-    estimator : estimator object implementing 'fit' and 'predict'
-        The object to use to fit the data.
-
-    X : array-like
-        The data to fit. Can be, for example a list, or an array at least 2d.
-
-    y : array-like, optional, default: None
-        The target variable to try to predict in the case of
-        supervised learning.
-
-    cv : int, cross-validation generator or an iterable, optional
-        Determines the cross-validation splitting strategy.
-        Possible inputs for cv are:
-
-        - None, to use the default 3-fold cross-validation,
-        - integer, to specify the number of folds.
-        - An object to be used as a cross-validation generator.
-        - An iterable yielding train/test splits.
-
-        For integer/None inputs, if the estimator is a classifier and ``y`` is
-        either binary or multiclass, :class:`StratifiedKFold` is used. In all
-        other cases, :class:`KFold` is used.
-
-        Refer :ref:`User Guide <cross_validation>` for the various
-        cross-validation strategies that can be used here.
-
-    n_jobs : integer, optional
-        The number of CPUs to use to do the computation. -1 means
-        'all CPUs'.
-
-    verbose : integer, optional
-        The verbosity level.
-
-    fit_params : dict, optional
-        Parameters to pass to the fit method of the estimator.
-
-    pre_dispatch : int, or string, optional
-        Controls the number of jobs that get dispatched during parallel
-        execution. Reducing this number can be useful to avoid an
-        explosion of memory consumption when more jobs get dispatched
-        than CPUs can process. This parameter can be:
-
-            - None, in which case all the jobs are immediately
-              created and spawned. Use this for lightweight and
-              fast-running jobs, to avoid delays due to on-demand
-              spawning of the jobs
-
-            - An int, giving the exact number of total jobs that are
-              spawned
-
-            - A string, giving an expression as a function of n_jobs,
-              as in '2*n_jobs'
-
-    Returns
-    -------
-    preds : ndarray
-        This is the result of calling 'predict'
-
-    Examples
-    --------
-    >>> from sklearn import datasets, linear_model
-    >>> from sklearn.cross_validation import cross_val_predict
-    >>> diabetes = datasets.load_diabetes()
-    >>> X = diabetes.data[:150]
-    >>> y = diabetes.target[:150]
-    >>> lasso = linear_model.Lasso()
-    >>> y_pred = cross_val_predict(lasso, X, y)
-    """
-    X, y = indexable(X, y)
-
-    cv = check_cv(cv, X, y, classifier=is_classifier(estimator))
-    # We clone the estimator to make sure that all the folds are
-    # independent, and that it is pickle-able.
-    parallel = Parallel(n_jobs=n_jobs, verbose=verbose,
-                        pre_dispatch=pre_dispatch)
-    preds_blocks = parallel(delayed(_fit_and_predict)(clone(estimator), X, y,
-                                                      train, test, verbose,
-                                                      fit_params)
-                            for train, test in cv)
-
-    preds = [p for p, _ in preds_blocks]
-    locs = np.concatenate([loc for _, loc in preds_blocks])
-    if not _check_is_partition(locs, _num_samples(X)):
-        raise ValueError('cross_val_predict only works for partitions')
-    inv_locs = np.empty(len(locs), dtype=int)
-    inv_locs[locs] = np.arange(len(locs))
-
-    # Check for sparse predictions
-    if sp.issparse(preds[0]):
-        preds = sp.vstack(preds, format=preds[0].format)
-    else:
-        preds = np.concatenate(preds)
-    return preds[inv_locs]
-
-
-def _fit_and_predict(estimator, X, y, train, test, verbose, fit_params):
-    """Fit estimator and predict values for a given dataset split.
-
-    Read more in the :ref:`User Guide <cross_validation>`.
-
-    Parameters
-    ----------
-    estimator : estimator object implementing 'fit' and 'predict'
-        The object to use to fit the data.
-
-    X : array-like of shape at least 2D
-        The data to fit.
-
-    y : array-like, optional, default: None
-        The target variable to try to predict in the case of
-        supervised learning.
-
-    train : array-like, shape (n_train_samples,)
-        Indices of training samples.
-
-    test : array-like, shape (n_test_samples,)
-        Indices of test samples.
-
-    verbose : integer
-        The verbosity level.
-
-    fit_params : dict or None
-        Parameters that will be passed to ``estimator.fit``.
-
-    Returns
-    -------
-    preds : sequence
-        Result of calling 'estimator.predict'
-
-    test : array-like
-        This is the value of the test parameter
-    """
-    # Adjust length of sample weights
-    fit_params = fit_params if fit_params is not None else {}
-    fit_params = dict([(k, _index_param_value(X, v, train))
-                      for k, v in fit_params.items()])
-
-    X_train, y_train = _safe_split(estimator, X, y, train)
-    X_test, _ = _safe_split(estimator, X, y, test, train)
-
-    if y_train is None:
-        estimator.fit(X_train, **fit_params)
-    else:
-        estimator.fit(X_train, y_train, **fit_params)
-    preds = estimator.predict(X_test)
-    return preds, test
-
-
-def _check_is_partition(locs, n):
-    """Check whether locs is a reordering of the array np.arange(n)
-
-    Parameters
-    ----------
-    locs : ndarray
-        integer array to test
-    n : int
-        number of expected elements
-
-    Returns
-    -------
-    is_partition : bool
-        True iff sorted(locs) is range(n)
-    """
-    if len(locs) != n:
-        return False
-    hit = np.zeros(n, bool)
-    hit[locs] = True
-    if not np.all(hit):
-        return False
-    return True
-
-
-def cross_val_score(estimator, X, y=None, scoring=None, cv=None, n_jobs=1,
-                    verbose=0, fit_params=None, pre_dispatch='2*n_jobs'):
-    """Evaluate a score by cross-validation
-
-    .. deprecated:: 0.18
-        This module will be removed in 0.20.
-        Use :func:`sklearn.model_selection.cross_val_score` instead.
-
-    Read more in the :ref:`User Guide <cross_validation>`.
-
-    Parameters
-    ----------
-    estimator : estimator object implementing 'fit'
-        The object to use to fit the data.
-
-    X : array-like
-        The data to fit. Can be, for example a list, or an array at least 2d.
-
-    y : array-like, optional, default: None
-        The target variable to try to predict in the case of
-        supervised learning.
-
-    scoring : string, callable or None, optional, default: None
-        A string (see model evaluation documentation) or
-        a scorer callable object / function with signature
-        ``scorer(estimator, X, y)``.
-
-    cv : int, cross-validation generator or an iterable, optional
-        Determines the cross-validation splitting strategy.
-        Possible inputs for cv are:
-
-        - None, to use the default 3-fold cross-validation,
-        - integer, to specify the number of folds.
-        - An object to be used as a cross-validation generator.
-        - An iterable yielding train/test splits.
-
-        For integer/None inputs, if the estimator is a classifier and ``y`` is
-        either binary or multiclass, :class:`StratifiedKFold` is used. In all
-        other cases, :class:`KFold` is used.
-
-        Refer :ref:`User Guide <cross_validation>` for the various
-        cross-validation strategies that can be used here.
-
-    n_jobs : integer, optional
-        The number of CPUs to use to do the computation. -1 means
-        'all CPUs'.
-
-    verbose : integer, optional
-        The verbosity level.
-
-    fit_params : dict, optional
-        Parameters to pass to the fit method of the estimator.
-
-    pre_dispatch : int, or string, optional
-        Controls the number of jobs that get dispatched during parallel
-        execution. Reducing this number can be useful to avoid an
-        explosion of memory consumption when more jobs get dispatched
-        than CPUs can process. This parameter can be:
-
-            - None, in which case all the jobs are immediately
-              created and spawned. Use this for lightweight and
-              fast-running jobs, to avoid delays due to on-demand
-              spawning of the jobs
-
-            - An int, giving the exact number of total jobs that are
-              spawned
-
-            - A string, giving an expression as a function of n_jobs,
-              as in '2*n_jobs'
-
-    Returns
-    -------
-    scores : array of float, shape=(len(list(cv)),)
-        Array of scores of the estimator for each run of the cross validation.
-
-    Examples
-    --------
-    >>> from sklearn import datasets, linear_model
-    >>> from sklearn.cross_validation import cross_val_score
-    >>> diabetes = datasets.load_diabetes()
-    >>> X = diabetes.data[:150]
-    >>> y = diabetes.target[:150]
-    >>> lasso = linear_model.Lasso()
-    >>> print(cross_val_score(lasso, X, y))  # doctest:  +ELLIPSIS
-    [0.33150734 0.08022311 0.03531764]
-
-    See Also
-    ---------
-    :func:`sklearn.metrics.make_scorer`:
-        Make a scorer from a performance metric or loss function.
-
-    """
-    X, y = indexable(X, y)
-
-    cv = check_cv(cv, X, y, classifier=is_classifier(estimator))
-    scorer = check_scoring(estimator, scoring=scoring)
-    # We clone the estimator to make sure that all the folds are
-    # independent, and that it is pickle-able.
-    parallel = Parallel(n_jobs=n_jobs, verbose=verbose,
-                        pre_dispatch=pre_dispatch)
-    scores = parallel(delayed(_fit_and_score)(clone(estimator), X, y, scorer,
-                                              train, test, verbose, None,
-                                              fit_params)
-                      for train, test in cv)
-    return np.array(scores)[:, 0]
-
-
-def _fit_and_score(estimator, X, y, scorer, train, test, verbose,
-                   parameters, fit_params, return_train_score=False,
-                   return_parameters=False, error_score='raise'):
-    """Fit estimator and compute scores for a given dataset split.
-
-    Parameters
-    ----------
-    estimator : estimator object implementing 'fit'
-        The object to use to fit the data.
-
-    X : array-like of shape at least 2D
-        The data to fit.
-
-    y : array-like, optional, default: None
-        The target variable to try to predict in the case of
-        supervised learning.
-
-    scorer : callable
-        A scorer callable object / function with signature
-        ``scorer(estimator, X, y)``.
-
-    train : array-like, shape (n_train_samples,)
-        Indices of training samples.
-
-    test : array-like, shape (n_test_samples,)
-        Indices of test samples.
-
-    verbose : integer
-        The verbosity level.
-
-    error_score : 'raise' (default) or numeric
-        Value to assign to the score if an error occurs in estimator fitting.
-        If set to 'raise', the error is raised. If a numeric value is given,
-        FitFailedWarning is raised. This parameter does not affect the refit
-        step, which will always raise the error.
-
-    parameters : dict or None
-        Parameters to be set on the estimator.
-
-    fit_params : dict or None
-        Parameters that will be passed to ``estimator.fit``.
-
-    return_train_score : boolean, optional, default: False
-        Compute and return score on training set.
-
-    return_parameters : boolean, optional, default: False
-        Return parameters that has been used for the estimator.
-
-    Returns
-    -------
-    train_score : float, optional
-        Score on training set, returned only if `return_train_score` is `True`.
-
-    test_score : float
-        Score on test set.
-
-    n_test_samples : int
-        Number of test samples.
-
-    scoring_time : float
-        Time spent for fitting and scoring in seconds.
-
-    parameters : dict or None, optional
-        The parameters that have been evaluated.
-    """
-    if verbose > 1:
-        if parameters is None:
-            msg = ''
-        else:
-            msg = '%s' % (', '.join('%s=%s' % (k, v)
-                          for k, v in parameters.items()))
-        print("[CV] %s %s" % (msg, (64 - len(msg)) * '.'))
-
-    # Adjust length of sample weights
-    fit_params = fit_params if fit_params is not None else {}
-    fit_params = dict([(k, _index_param_value(X, v, train))
-                      for k, v in fit_params.items()])
-
-    if parameters is not None:
-        estimator.set_params(**parameters)
-
-    start_time = time.time()
-
-    X_train, y_train = _safe_split(estimator, X, y, train)
-    X_test, y_test = _safe_split(estimator, X, y, test, train)
-
-    try:
-        if y_train is None:
-            estimator.fit(X_train, **fit_params)
-        else:
-            estimator.fit(X_train, y_train, **fit_params)
-
-    except Exception as e:
-        if error_score == 'raise':
-            raise
-        elif isinstance(error_score, numbers.Number):
-            test_score = error_score
-            if return_train_score:
-                train_score = error_score
-            warnings.warn("Classifier fit failed. The score on this train-test"
-                          " partition for these parameters will be set to %f. "
-                          "Details: \n%r" % (error_score, e), FitFailedWarning)
-        else:
-            raise ValueError("error_score must be the string 'raise' or a"
-                             " numeric value. (Hint: if using 'raise', please"
-                             " make sure that it has been spelled correctly.)"
-                             )
-
-    else:
-        test_score = _score(estimator, X_test, y_test, scorer)
-        if return_train_score:
-            train_score = _score(estimator, X_train, y_train, scorer)
-
-    scoring_time = time.time() - start_time
-
-    if verbose > 2:
-        msg += ", score=%f" % test_score
-    if verbose > 1:
-        end_msg = "%s -%s" % (msg, logger.short_format_time(scoring_time))
-        print("[CV] %s %s" % ((64 - len(end_msg)) * '.', end_msg))
-
-    ret = [train_score] if return_train_score else []
-    ret.extend([test_score, _num_samples(X_test), scoring_time])
-    if return_parameters:
-        ret.append(parameters)
-    return ret
-
-
-def _safe_split(estimator, X, y, indices, train_indices=None):
-    """Create subset of dataset and properly handle kernels."""
-    if hasattr(estimator, 'kernel') and callable(estimator.kernel) \
-       and not isinstance(estimator.kernel, GPKernel):
-        # cannot compute the kernel values with custom function
-        raise ValueError("Cannot use a custom kernel function. "
-                         "Precompute the kernel matrix instead.")
-
-    if not hasattr(X, "shape"):
-        if getattr(estimator, "_pairwise", False):
-            raise ValueError("Precomputed kernels or affinity matrices have "
-                             "to be passed as arrays or sparse matrices.")
-        X_subset = [X[idx] for idx in indices]
-    else:
-        if getattr(estimator, "_pairwise", False):
-            # X is a precomputed square kernel matrix
-            if X.shape[0] != X.shape[1]:
-                raise ValueError("X should be a square kernel matrix")
-            if train_indices is None:
-                X_subset = X[np.ix_(indices, indices)]
-            else:
-                X_subset = X[np.ix_(indices, train_indices)]
-        else:
-            X_subset = safe_indexing(X, indices)
-
-    if y is not None:
-        y_subset = safe_indexing(y, indices)
-    else:
-        y_subset = None
-
-    return X_subset, y_subset
-
-
-def _score(estimator, X_test, y_test, scorer):
-    """Compute the score of an estimator on a given test set."""
-    if y_test is None:
-        score = scorer(estimator, X_test)
-    else:
-        score = scorer(estimator, X_test, y_test)
-    if hasattr(score, 'item'):
-        try:
-            # e.g. unwrap memmapped scalars
-            score = score.item()
-        except ValueError:
-            # non-scalar?
-            pass
-    if not isinstance(score, numbers.Number):
-        raise ValueError("scoring must return a number, got %s (%s) instead."
-                         % (str(score), type(score)))
-    return score
-
-
-def _permutation_test_score(estimator, X, y, cv, scorer):
-    """Auxiliary function for permutation_test_score"""
-    avg_score = []
-    for train, test in cv:
-        X_train, y_train = _safe_split(estimator, X, y, train)
-        X_test, y_test = _safe_split(estimator, X, y, test, train)
-        estimator.fit(X_train, y_train)
-        avg_score.append(scorer(estimator, X_test, y_test))
-    return np.mean(avg_score)
-
-
-def _shuffle(y, labels, random_state):
-    """Return a shuffled copy of y eventually shuffle among same labels."""
-    if labels is None:
-        ind = random_state.permutation(len(y))
-    else:
-        ind = np.arange(len(labels))
-        for label in np.unique(labels):
-            this_mask = (labels == label)
-            ind[this_mask] = random_state.permutation(ind[this_mask])
-    return safe_indexing(y, ind)
-
-
-def check_cv(cv, X=None, y=None, classifier=False):
-    """Input checker utility for building a CV in a user friendly way.
-
-    .. deprecated:: 0.18
-        This module will be removed in 0.20.
-        Use :func:`sklearn.model_selection.check_cv` instead.
-
-    Parameters
-    ----------
-    cv : int, cross-validation generator or an iterable, optional
-        Determines the cross-validation splitting strategy.
-        Possible inputs for cv are:
-
-        - None, to use the default 3-fold cross-validation,
-        - integer, to specify the number of folds.
-        - An object to be used as a cross-validation generator.
-        - An iterable yielding train/test splits.
-
-        For integer/None inputs, if classifier is True and ``y`` is binary or
-        multiclass, :class:`StratifiedKFold` is used. In all other cases,
-        :class:`KFold` is used.
-
-        Refer :ref:`User Guide <cross_validation>` for the various
-        cross-validation strategies that can be used here.
-
-    X : array-like
-        The data the cross-val object will be applied on.
-
-    y : array-like
-        The target variable for a supervised learning problem.
-
-    classifier : boolean optional
-        Whether the task is a classification task, in which case
-        stratified KFold will be used.
-
-    Returns
-    -------
-    checked_cv : a cross-validation generator instance.
-        The return value is guaranteed to be a cv generator instance, whatever
-        the input type.
-    """
-    is_sparse = sp.issparse(X)
-    if cv is None:
-        cv = 3
-    if isinstance(cv, numbers.Integral):
-        if classifier:
-            if type_of_target(y) in ['binary', 'multiclass']:
-                cv = StratifiedKFold(y, cv)
-            else:
-                cv = KFold(_num_samples(y), cv)
-        else:
-            if not is_sparse:
-                n_samples = len(X)
-            else:
-                n_samples = X.shape[0]
-            cv = KFold(n_samples, cv)
-    return cv
-
-
-def permutation_test_score(estimator, X, y, cv=None,
-                           n_permutations=100, n_jobs=1, labels=None,
-                           random_state=0, verbose=0, scoring=None):
-    """Evaluate the significance of a cross-validated score with permutations
-
-    .. deprecated:: 0.18
-        This module will be removed in 0.20.
-        Use :func:`sklearn.model_selection.permutation_test_score` instead.
-
-    Read more in the :ref:`User Guide <cross_validation>`.
-
-    Parameters
-    ----------
-    estimator : estimator object implementing 'fit'
-        The object to use to fit the data.
-
-    X : array-like of shape at least 2D
-        The data to fit.
-
-    y : array-like
-        The target variable to try to predict in the case of
-        supervised learning.
-
-    scoring : string, callable or None, optional, default: None
-        A string (see model evaluation documentation) or
-        a scorer callable object / function with signature
-        ``scorer(estimator, X, y)``.
-
-    cv : int, cross-validation generator or an iterable, optional
-        Determines the cross-validation splitting strategy.
-        Possible inputs for cv are:
-
-        - None, to use the default 3-fold cross-validation,
-        - integer, to specify the number of folds.
-        - An object to be used as a cross-validation generator.
-        - An iterable yielding train/test splits.
-
-        For integer/None inputs, if the estimator is a classifier and ``y`` is
-        either binary or multiclass, :class:`StratifiedKFold` is used. In all
-        other cases, :class:`KFold` is used.
-
-        Refer :ref:`User Guide <cross_validation>` for the various
-        cross-validation strategies that can be used here.
-
-    n_permutations : integer, optional
-        Number of times to permute ``y``.
-
-    n_jobs : integer, optional
-        The number of CPUs to use to do the computation. -1 means
-        'all CPUs'.
-
-    labels : array-like of shape [n_samples] (optional)
-        Labels constrain the permutation among groups of samples with
-        a same label.
-
-    random_state : int, RandomState instance or None, optional (default=0)
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
-
-    verbose : integer, optional
-        The verbosity level.
-
-    Returns
-    -------
-    score : float
-        The true score without permuting targets.
-
-    permutation_scores : array, shape (n_permutations,)
-        The scores obtained for each permutations.
-
-    pvalue : float
-        The p-value, which approximates the probability that the score would
-        be obtained by chance. This is calculated as:
-
-        `(C + 1) / (n_permutations + 1)`
-
-        Where C is the number of permutations whose score >= the true score.
-
-        The best possible p-value is 1/(n_permutations + 1), the worst is 1.0.
-
-    Notes
-    -----
-    This function implements Test 1 in:
-
-        Ojala and Garriga. Permutation Tests for Studying Classifier
-        Performance.  The Journal of Machine Learning Research (2010)
-        vol. 11
-
-    """
-    X, y = indexable(X, y)
-    cv = check_cv(cv, X, y, classifier=is_classifier(estimator))
-    scorer = check_scoring(estimator, scoring=scoring)
-    random_state = check_random_state(random_state)
-
-    # We clone the estimator to make sure that all the folds are
-    # independent, and that it is pickle-able.
-    score = _permutation_test_score(clone(estimator), X, y, cv, scorer)
-    permutation_scores = Parallel(n_jobs=n_jobs, verbose=verbose)(
-        delayed(_permutation_test_score)(
-            clone(estimator), X, _shuffle(y, labels, random_state), cv,
-            scorer)
-        for _ in range(n_permutations))
-    permutation_scores = np.array(permutation_scores)
-    pvalue = (np.sum(permutation_scores >= score) + 1.0) / (n_permutations + 1)
-    return score, permutation_scores, pvalue
-
-
-def train_test_split(*arrays, **options):
-    """Split arrays or matrices into random train and test subsets
-
-    .. deprecated:: 0.18
-        This module will be removed in 0.20.
-        Use :func:`sklearn.model_selection.train_test_split` instead.
-
-    Quick utility that wraps input validation and
-    ``next(iter(ShuffleSplit(n_samples)))`` and application to input
-    data into a single call for splitting (and optionally subsampling)
-    data in a oneliner.
-
-    Read more in the :ref:`User Guide <cross_validation>`.
-
-    Parameters
-    ----------
-    *arrays : sequence of indexables with same length / shape[0]
-        Allowed inputs are lists, numpy arrays, scipy-sparse
-        matrices or pandas dataframes.
-
-    test_size : float, int, or None (default is None)
-        If float, should be between 0.0 and 1.0 and represent the
-        proportion of the dataset to include in the test split. If
-        int, represents the absolute number of test samples. If None,
-        the value is automatically set to the complement of the train size.
-        If train size is also None, test size is set to 0.25.
-
-    train_size : float, int, or None (default is None)
-        If float, should be between 0.0 and 1.0 and represent the
-        proportion of the dataset to include in the train split. If
-        int, represents the absolute number of train samples. If None,
-        the value is automatically set to the complement of the test size.
-
-    random_state : int, RandomState instance or None, optional (default=None)
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
-
-    stratify : array-like or None (default is None)
-        If not None, data is split in a stratified fashion, using this as
-        the labels array.
-
-        .. versionadded:: 0.17
-           *stratify* splitting
-
-    Returns
-    -------
-    splitting : list, length = 2 * len(arrays),
-        List containing train-test split of inputs.
-
-        .. versionadded:: 0.16
-            If the input is sparse, the output will be a
-            ``scipy.sparse.csr_matrix``. Else, output type is the same as the
-            input type.
-
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from sklearn.cross_validation import train_test_split
-    >>> X, y = np.arange(10).reshape((5, 2)), range(5)
-    >>> X
-    array([[0, 1],
-           [2, 3],
-           [4, 5],
-           [6, 7],
-           [8, 9]])
-    >>> list(y)
-    [0, 1, 2, 3, 4]
-
-    >>> X_train, X_test, y_train, y_test = train_test_split(
-    ...     X, y, test_size=0.33, random_state=42)
-    ...
-    >>> X_train
-    array([[4, 5],
-           [0, 1],
-           [6, 7]])
-    >>> y_train
-    [2, 0, 3]
-    >>> X_test
-    array([[2, 3],
-           [8, 9]])
-    >>> y_test
-    [1, 4]
-
-    """
-    n_arrays = len(arrays)
-    if n_arrays == 0:
-        raise ValueError("At least one array required as input")
-
-    test_size = options.pop('test_size', None)
-    train_size = options.pop('train_size', None)
-    random_state = options.pop('random_state', None)
-    stratify = options.pop('stratify', None)
-
-    if options:
-        raise TypeError("Invalid parameters passed: %s" % str(options))
-
-    if test_size is None and train_size is None:
-        test_size = 0.25
-    arrays = indexable(*arrays)
-    if stratify is not None:
-        cv = StratifiedShuffleSplit(stratify, test_size=test_size,
-                                    train_size=train_size,
-                                    random_state=random_state)
-    else:
-        n_samples = _num_samples(arrays[0])
-        cv = ShuffleSplit(n_samples, test_size=test_size,
-                          train_size=train_size,
-                          random_state=random_state)
-
-    train, test = next(iter(cv))
-    return list(chain.from_iterable((safe_indexing(a, train),
-                                     safe_indexing(a, test)) for a in arrays))
diff --git a/sklearn/datasets/california_housing.py b/sklearn/datasets/california_housing.py
index ce3d5dcf4da2..8973ba59ad21 100644
--- a/sklearn/datasets/california_housing.py
+++ b/sklearn/datasets/california_housing.py
@@ -67,8 +67,9 @@ def fetch_california_housing(data_home=None, download_if_missing=True,
         instead of trying to download the data from the source site.
 
 
-    return_X_y : boolean, default=False. If True, returns ``(data.data,
-    data.target)`` instead of a Bunch object.
+    return_X_y : boolean, default=False.
+        If True, returns ``(data.data, data.target)`` instead of a Bunch
+        object.
 
         .. versionadded:: 0.20
 
diff --git a/sklearn/datasets/covtype.py b/sklearn/datasets/covtype.py
index 7630e92056f0..c7b880b116ea 100644
--- a/sklearn/datasets/covtype.py
+++ b/sklearn/datasets/covtype.py
@@ -65,8 +65,9 @@ def fetch_covtype(data_home=None, download_if_missing=True,
     shuffle : bool, default=False
         Whether to shuffle dataset.
 
-    return_X_y : boolean, default=False. If True, returns ``(data.data,
-    data.target)`` instead of a Bunch object.
+    return_X_y : boolean, default=False.
+        If True, returns ``(data.data, data.target)`` instead of a Bunch
+        object.
 
         .. versionadded:: 0.20
 
diff --git a/sklearn/datasets/descr/boston_house_prices.rst b/sklearn/datasets/descr/boston_house_prices.rst
index 31227b087dff..dec9b999cd59 100644
--- a/sklearn/datasets/descr/boston_house_prices.rst
+++ b/sklearn/datasets/descr/boston_house_prices.rst
@@ -1,15 +1,13 @@
-Boston House Prices dataset
-===========================
+.. _boston_dataset:
 
-Notes
-------
-Data Set Characteristics:  
+Boston house prices dataset
+---------------------------
+
+**Data Set Characteristics:**  
 
     :Number of Instances: 506 
 
-    :Number of Attributes: 13 numeric/categorical predictive
-    
-    :Median Value (attribute 14) is usually the target
+    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.
 
     :Attribute Information (in order):
         - CRIM     per capita crime rate by town
@@ -46,7 +44,7 @@ pages 244-261 of the latter.
 The Boston house-price data has been used in many machine learning papers that address regression
 problems.   
      
-**References**
+.. topic:: References
 
    - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.
    - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.
diff --git a/sklearn/datasets/descr/breast_cancer.rst b/sklearn/datasets/descr/breast_cancer.rst
index 547b41021ef2..fea6b6f017c1 100644
--- a/sklearn/datasets/descr/breast_cancer.rst
+++ b/sklearn/datasets/descr/breast_cancer.rst
@@ -1,9 +1,10 @@
-Breast Cancer Wisconsin (Diagnostic) Database
-=============================================
+.. _breast_cancer_dataset:
+
+Breast cancer wisconsin (diagnostic) dataset
+--------------------------------------------
+
+**Data Set Characteristics:**
 
-Notes
------
-Data Set Characteristics:
     :Number of Instances: 569
 
     :Number of Attributes: 30 numeric, predictive attributes and the class
@@ -103,8 +104,8 @@ This database is also available through the UW CS ftp server:
 ftp ftp.cs.wisc.edu
 cd math-prog/cpo-dataset/machine-learn/WDBC/
 
-References
-----------
+.. topic:: References
+
    - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction 
      for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on 
      Electronic Imaging: Science and Technology, volume 1905, pages 861-870,
@@ -114,4 +115,4 @@ References
      July-August 1995.
    - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques
      to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) 
-     163-171.
+     163-171.
\ No newline at end of file
diff --git a/sklearn/datasets/descr/diabetes.rst b/sklearn/datasets/descr/diabetes.rst
index df102a1bec1f..f75beafd37b9 100644
--- a/sklearn/datasets/descr/diabetes.rst
+++ b/sklearn/datasets/descr/diabetes.rst
@@ -1,15 +1,14 @@
-Diabetes dataset
-================
+.. _diabetes_dataset:
 
-Notes
------
+Diabetes dataset
+----------------
 
 Ten baseline variables, age, sex, body mass index, average blood
 pressure, and six blood serum measurements were obtained for each of n =
 442 diabetes patients, as well as the response of interest, a
 quantitative measure of disease progression one year after baseline.
 
-Data Set Characteristics:
+**Data Set Characteristics:**
 
   :Number of Instances: 442
 
@@ -17,17 +16,17 @@ Data Set Characteristics:
 
   :Target: Column 11 is a quantitative measure of disease progression one year after baseline
 
-  :Attributes:
-    :Age:
-    :Sex:
-    :Body mass index:
-    :Average blood pressure:
-    :S1:
-    :S2:
-    :S3:
-    :S4:
-    :S5:
-    :S6:
+  :Attribute Information:
+      - Age
+      - Sex
+      - Body mass index
+      - Average blood pressure
+      - S1
+      - S2
+      - S3
+      - S4
+      - S5
+      - S6
 
 Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).
 
@@ -36,4 +35,4 @@ http://www4.stat.ncsu.edu/~boos/var.select/diabetes.html
 
 For more information see:
 Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) "Least Angle Regression," Annals of Statistics (with discussion), 407-499.
-(http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)
+(http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)
\ No newline at end of file
diff --git a/sklearn/datasets/descr/digits.rst b/sklearn/datasets/descr/digits.rst
index a30514474f52..b4ecb714a01b 100644
--- a/sklearn/datasets/descr/digits.rst
+++ b/sklearn/datasets/descr/digits.rst
@@ -1,9 +1,10 @@
-Optical Recognition of Handwritten Digits Data Set
-===================================================
+.. _digits_dataset:
+
+Optical recognition of handwritten digits dataset
+--------------------------------------------------
+
+**Data Set Characteristics:**
 
-Notes
------
-Data Set Characteristics:
     :Number of Instances: 5620
     :Number of Attributes: 64
     :Attribute Information: 8x8 image of integer pixels in the range 0..16.
@@ -31,8 +32,8 @@ T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.
 L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,
 1994.
 
-References
-----------
+.. topic:: References
+
   - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their
     Applications to Handwritten Digit Recognition, MSc Thesis, Institute of
     Graduate Studies in Science and Engineering, Bogazici University.
@@ -42,4 +43,4 @@ References
     Electrical and Electronic Engineering Nanyang Technological University.
     2005.
   - Claudio Gentile. A New Approximate Maximal Margin Classification
-    Algorithm. NIPS. 2000.
+    Algorithm. NIPS. 2000.
\ No newline at end of file
diff --git a/sklearn/datasets/descr/iris.rst b/sklearn/datasets/descr/iris.rst
index 25cada25f54f..a35edc728c7d 100644
--- a/sklearn/datasets/descr/iris.rst
+++ b/sklearn/datasets/descr/iris.rst
@@ -1,9 +1,10 @@
-Iris Plants Database
-====================
+.. _iris_dataset:
+
+Iris plants dataset
+--------------------
+
+**Data Set Characteristics:**
 
-Notes
------
-Data Set Characteristics:
     :Number of Instances: 150 (50 in each of three classes)
     :Number of Attributes: 4 numeric, predictive attributes and the class
     :Attribute Information:
@@ -15,6 +16,7 @@ Data Set Characteristics:
                 - Iris-Setosa
                 - Iris-Versicolour
                 - Iris-Virginica
+                
     :Summary Statistics:
 
     ============== ==== ==== ======= ===== ====================
@@ -43,8 +45,8 @@ data set contains 3 classes of 50 instances each, where each class refers to a
 type of iris plant.  One class is linearly separable from the other 2; the
 latter are NOT linearly separable from each other.
 
-References
-----------
+.. topic:: References
+
    - Fisher, R.A. "The use of multiple measurements in taxonomic problems"
      Annual Eugenics, 7, Part II, 179-188 (1936); also in "Contributions to
      Mathematical Statistics" (John Wiley, NY, 1950).
@@ -58,4 +60,4 @@ References
      on Information Theory, May 1972, 431-433.
    - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al"s AUTOCLASS II
      conceptual clustering system finds 3 classes in the data.
-   - Many, many more ...
+   - Many, many more ...
\ No newline at end of file
diff --git a/sklearn/datasets/descr/linnerud.rst b/sklearn/datasets/descr/linnerud.rst
index d790d3c0c908..848ee193e1ad 100644
--- a/sklearn/datasets/descr/linnerud.rst
+++ b/sklearn/datasets/descr/linnerud.rst
@@ -1,9 +1,10 @@
+.. _linnerrud_dataset:
+
 Linnerrud dataset
-=================
+-----------------
+
+**Data Set Characteristics:**
 
-Notes
------
-Data Set Characteristics:
     :Number of Instances: 20
     :Number of Attributes: 3
     :Missing Attribute Values: None
@@ -16,6 +17,6 @@ The Linnerud dataset constains two small dataset:
 - *physiological*: Data frame with 20 observations on 3 physiological variables:
    Chins, Situps and Jumps.
 
-References
-----------
-  * Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic.
+.. topic:: References
+
+  * Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic.
\ No newline at end of file
diff --git a/sklearn/datasets/descr/wine_data.rst b/sklearn/datasets/descr/wine_data.rst
index 3d3341874a58..f43e6524130b 100644
--- a/sklearn/datasets/descr/wine_data.rst
+++ b/sklearn/datasets/descr/wine_data.rst
@@ -1,29 +1,30 @@
-Wine Data Database
-====================
+.. _wine_dataset:
+
+Wine recognition dataset
+------------------------
+
+**Data Set Characteristics:**
 
-Notes
------
-Data Set Characteristics:
     :Number of Instances: 178 (50 in each of three classes)
     :Number of Attributes: 13 numeric, predictive attributes and the class
     :Attribute Information:
- 		- 1) Alcohol
- 		- 2) Malic acid
- 		- 3) Ash
-		- 4) Alcalinity of ash  
- 		- 5) Magnesium
-		- 6) Total phenols
- 		- 7) Flavanoids
- 		- 8) Nonflavanoid phenols
- 		- 9) Proanthocyanins
-		- 10)Color intensity
- 		- 11)Hue
- 		- 12)OD280/OD315 of diluted wines
- 		- 13)Proline
-        	- class:
-                - class_0
-                - class_1
-                - class_2
+ 		- Alcohol
+ 		- Malic acid
+ 		- Ash
+		- Alcalinity of ash  
+ 		- Magnesium
+		- Total phenols
+ 		- Flavanoids
+ 		- Nonflavanoid phenols
+ 		- Proanthocyanins
+		- Color intensity
+ 		- Hue
+ 		- OD280/OD315 of diluted wines
+ 		- Proline
+    - class:
+            - class_0
+            - class_1
+            - class_2
 		
     :Summary Statistics:
     
@@ -72,24 +73,22 @@ Lichman, M. (2013). UCI Machine Learning Repository
 [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California,
 School of Information and Computer Science. 
 
-References
-----------
-(1) 
-S. Aeberhard, D. Coomans and O. de Vel, 
-Comparison of Classifiers in High Dimensional Settings, 
-Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of 
-Mathematics and Statistics, James Cook University of North Queensland. 
-(Also submitted to Technometrics). 
+.. topic:: References
+
+  (1) S. Aeberhard, D. Coomans and O. de Vel, 
+  Comparison of Classifiers in High Dimensional Settings, 
+  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  
+  Mathematics and Statistics, James Cook University of North Queensland. 
+  (Also submitted to Technometrics). 
 
-The data was used with many others for comparing various 
-classifiers. The classes are separable, though only RDA 
-has achieved 100% correct classification. 
-(RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) 
-(All results using the leave-one-out technique) 
+  The data was used with many others for comparing various 
+  classifiers. The classes are separable, though only RDA 
+  has achieved 100% correct classification. 
+  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) 
+  (All results using the leave-one-out technique) 
 
-(2) 
-S. Aeberhard, D. Coomans and O. de Vel, 
-"THE CLASSIFICATION PERFORMANCE OF RDA" 
-Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of 
-Mathematics and Statistics, James Cook University of North Queensland. 
-(Also submitted to Journal of Chemometrics). 
+  (2) S. Aeberhard, D. Coomans and O. de Vel, 
+  "THE CLASSIFICATION PERFORMANCE OF RDA" 
+  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of 
+  Mathematics and Statistics, James Cook University of North Queensland. 
+  (Also submitted to Journal of Chemometrics).
\ No newline at end of file
diff --git a/sklearn/datasets/lfw.py b/sklearn/datasets/lfw.py
index 8c2573b8c4dd..16fb2c2ef744 100644
--- a/sklearn/datasets/lfw.py
+++ b/sklearn/datasets/lfw.py
@@ -287,9 +287,10 @@ def fetch_lfw_people(data_home=None, funneled=True, resize=0.5,
         If False, raise a IOError if the data is not locally available
         instead of trying to download the data from the source site.
 
-    return_X_y : boolean, default=False. If True, returns ``(dataset.data,
-    dataset.target)`` instead of a Bunch object. See below for more
-    information about the `dataset.data` and `dataset.target` object.
+    return_X_y : boolean, default=False.
+        If True, returns ``(dataset.data, dataset.target)`` instead of a Bunch
+        object. See below for more information about the `dataset.data` and
+        `dataset.target` object.
 
         .. versionadded:: 0.20
 
@@ -472,8 +473,7 @@ def fetch_lfw_pairs(subset='train', data_home=None, funneled=True, resize=0.5,
         pixels. Changing the ``slice_``, ``resize`` or ``subset`` parameters
         will change the shape of the output.
 
-    pairs : numpy array of shape (2200, 2, 62, 47). Shape depends on
-            ``subset``.
+    pairs : numpy array of shape (2200, 2, 62, 47). Shape depends on ``subset``
         Each row has 2 face images corresponding to same or different person
         from the dataset containing 5749 people. Changing the ``slice_``,
         ``resize`` or ``subset`` parameters will change the shape of the
diff --git a/sklearn/datasets/mlcomp.py b/sklearn/datasets/mlcomp.py
index 1b0b6daf284f..169df6e55151 100644
--- a/sklearn/datasets/mlcomp.py
+++ b/sklearn/datasets/mlcomp.py
@@ -26,6 +26,8 @@ def _load_document_classification(dataset_path, metadata, set_=None, **kwargs):
 def load_mlcomp(name_or_id, set_="raw", mlcomp_root=None, **kwargs):
     """Load a datasets as downloaded from http://mlcomp.org
 
+    Read more in the :ref:`User Guide <datasets>`.
+
     Parameters
     ----------
 
@@ -33,7 +35,7 @@ def load_mlcomp(name_or_id, set_="raw", mlcomp_root=None, **kwargs):
         The integer id or the string name metadata of the MLComp
         dataset to load
 
-    set_ : str, default='raw'
+    set\_ : str, default='raw'
         Select the portion to load: 'train', 'test' or 'raw'
 
     mlcomp_root : str, optional
@@ -43,8 +45,6 @@ def load_mlcomp(name_or_id, set_="raw", mlcomp_root=None, **kwargs):
 
     **kwargs : domain specific kwargs to be passed to the dataset loader.
 
-    Read more in the :ref:`User Guide <datasets>`.
-
     Returns
     -------
 
diff --git a/sklearn/datasets/rcv1.py b/sklearn/datasets/rcv1.py
index f66823471277..b0ef91972a72 100644
--- a/sklearn/datasets/rcv1.py
+++ b/sklearn/datasets/rcv1.py
@@ -113,9 +113,10 @@ def fetch_rcv1(data_home=None, subset='all', download_if_missing=True,
     shuffle : bool, default=False
         Whether to shuffle dataset.
 
-    return_X_y : boolean, default=False. If True, returns ``(dataset.data,
-    dataset.target)`` instead of a Bunch object. See below for more
-    information about the `dataset.data` and `dataset.target` object.
+    return_X_y : boolean, default=False.
+        If True, returns ``(dataset.data, dataset.target)`` instead of a Bunch
+        object. See below for more information about the `dataset.data` and
+        `dataset.target` object.
 
         .. versionadded:: 0.20
 
diff --git a/sklearn/datasets/tests/test_base.py b/sklearn/datasets/tests/test_base.py
index 9b63190f1b76..bf03c4e3075a 100644
--- a/sklearn/datasets/tests/test_base.py
+++ b/sklearn/datasets/tests/test_base.py
@@ -7,6 +7,8 @@
 from pickle import dumps
 from functools import partial
 
+import pytest
+
 from sklearn.datasets import get_data_home
 from sklearn.datasets import clear_data_home
 from sklearn.datasets import load_files
@@ -31,43 +33,47 @@
 from sklearn.utils.testing import assert_raises
 
 
-DATA_HOME = tempfile.mkdtemp(prefix="scikit_learn_data_home_test_")
-LOAD_FILES_ROOT = tempfile.mkdtemp(prefix="scikit_learn_load_files_test_")
-TEST_CATEGORY_DIR1 = ""
-TEST_CATEGORY_DIR2 = ""
-
-
 def _remove_dir(path):
     if os.path.isdir(path):
         shutil.rmtree(path)
 
 
-def teardown_module():
-    """Test fixture (clean up) run once after all tests of this module"""
-    for path in [DATA_HOME, LOAD_FILES_ROOT]:
-        _remove_dir(path)
+@pytest.fixture(scope="module")
+def data_home(tmpdir_factory):
+    tmp_file = str(tmpdir_factory.mktemp("scikit_learn_data_home_test"))
+    yield tmp_file
+    _remove_dir(tmp_file)
 
 
-def setup_load_files():
-    global TEST_CATEGORY_DIR1
-    global TEST_CATEGORY_DIR2
-    TEST_CATEGORY_DIR1 = tempfile.mkdtemp(dir=LOAD_FILES_ROOT)
-    TEST_CATEGORY_DIR2 = tempfile.mkdtemp(dir=LOAD_FILES_ROOT)
-    sample_file = tempfile.NamedTemporaryFile(dir=TEST_CATEGORY_DIR1,
+@pytest.fixture(scope="module")
+def load_files_root(tmpdir_factory):
+    tmp_file = str(tmpdir_factory.mktemp("scikit_learn_load_files_test"))
+    yield tmp_file
+    _remove_dir(tmp_file)
+
+
+@pytest.fixture
+def test_category_dir_1(load_files_root):
+    test_category_dir1 = tempfile.mkdtemp(dir=load_files_root)
+    sample_file = tempfile.NamedTemporaryFile(dir=test_category_dir1,
                                               delete=False)
     sample_file.write(b("Hello World!\n"))
     sample_file.close()
+    yield str(test_category_dir1)
+    _remove_dir(test_category_dir1)
 
 
-def teardown_load_files():
-    _remove_dir(TEST_CATEGORY_DIR1)
-    _remove_dir(TEST_CATEGORY_DIR2)
+@pytest.fixture
+def test_category_dir_2(load_files_root):
+    test_category_dir2 = tempfile.mkdtemp(dir=load_files_root)
+    yield str(test_category_dir2)
+    _remove_dir(test_category_dir2)
 
 
-def test_data_home():
+def test_data_home(data_home):
     # get_data_home will point to a pre-existing folder
-    data_home = get_data_home(data_home=DATA_HOME)
-    assert_equal(data_home, DATA_HOME)
+    data_home = get_data_home(data_home=data_home)
+    assert_equal(data_home, data_home)
     assert_true(os.path.exists(data_home))
 
     # clear_data_home will delete both the content and the folder it-self
@@ -75,53 +81,44 @@ def test_data_home():
     assert_false(os.path.exists(data_home))
 
     # if the folder is missing it will be created again
-    data_home = get_data_home(data_home=DATA_HOME)
+    data_home = get_data_home(data_home=data_home)
     assert_true(os.path.exists(data_home))
 
 
-def test_default_empty_load_files():
-    res = load_files(LOAD_FILES_ROOT)
+def test_default_empty_load_files(load_files_root):
+    res = load_files(load_files_root)
     assert_equal(len(res.filenames), 0)
     assert_equal(len(res.target_names), 0)
     assert_equal(res.DESCR, None)
 
 
-def test_default_load_files():
-    try:
-        setup_load_files()
-        res = load_files(LOAD_FILES_ROOT)
-        assert_equal(len(res.filenames), 1)
-        assert_equal(len(res.target_names), 2)
-        assert_equal(res.DESCR, None)
-        assert_equal(res.data, [b("Hello World!\n")])
-    finally:
-        teardown_load_files()
+def test_default_load_files(test_category_dir_1, test_category_dir_2,
+                            load_files_root):
+    res = load_files(load_files_root)
+    assert_equal(len(res.filenames), 1)
+    assert_equal(len(res.target_names), 2)
+    assert_equal(res.DESCR, None)
+    assert_equal(res.data, [b("Hello World!\n")])
 
 
-def test_load_files_w_categories_desc_and_encoding():
-    try:
-        setup_load_files()
-        category = os.path.abspath(TEST_CATEGORY_DIR1).split('/').pop()
-        res = load_files(LOAD_FILES_ROOT, description="test",
-                         categories=category, encoding="utf-8")
-        assert_equal(len(res.filenames), 1)
-        assert_equal(len(res.target_names), 1)
-        assert_equal(res.DESCR, "test")
-        assert_equal(res.data, [u("Hello World!\n")])
-    finally:
-        teardown_load_files()
-
-
-def test_load_files_wo_load_content():
-    try:
-        setup_load_files()
-        res = load_files(LOAD_FILES_ROOT, load_content=False)
-        assert_equal(len(res.filenames), 1)
-        assert_equal(len(res.target_names), 2)
-        assert_equal(res.DESCR, None)
-        assert_equal(res.get('data'), None)
-    finally:
-        teardown_load_files()
+def test_load_files_w_categories_desc_and_encoding(
+        test_category_dir_1, test_category_dir_2, load_files_root):
+    category = os.path.abspath(test_category_dir_1).split('/').pop()
+    res = load_files(load_files_root, description="test",
+                     categories=category, encoding="utf-8")
+    assert_equal(len(res.filenames), 1)
+    assert_equal(len(res.target_names), 1)
+    assert_equal(res.DESCR, "test")
+    assert_equal(res.data, [u("Hello World!\n")])
+
+
+def test_load_files_wo_load_content(
+        test_category_dir_1, test_category_dir_2, load_files_root):
+    res = load_files(load_files_root, load_content=False)
+    assert_equal(len(res.filenames), 1)
+    assert_equal(len(res.target_names), 2)
+    assert_equal(res.DESCR, None)
+    assert_equal(res.get('data'), None)
 
 
 def test_load_sample_images():
diff --git a/sklearn/datasets/tests/test_mldata.py b/sklearn/datasets/tests/test_mldata.py
index 7405b8e025c0..65e10a87818f 100644
--- a/sklearn/datasets/tests/test_mldata.py
+++ b/sklearn/datasets/tests/test_mldata.py
@@ -1,9 +1,8 @@
 """Test functionality of mldata fetching utilities."""
 
 import os
-import shutil
-import tempfile
 import scipy as sp
+import shutil
 
 from sklearn import datasets
 from sklearn.datasets import mldata_filename, fetch_mldata
@@ -15,21 +14,16 @@
 from sklearn.utils.testing import assert_raises
 from sklearn.utils.testing import assert_array_equal
 
-
-tmpdir = None
-
-
-def setup_tmpdata():
-    # create temporary dir
-    global tmpdir
-    tmpdir = tempfile.mkdtemp()
-    os.makedirs(os.path.join(tmpdir, 'mldata'))
+import pytest
 
 
-def teardown_tmpdata():
-    # remove temporary dir
-    if tmpdir is not None:
-        shutil.rmtree(tmpdir)
+@pytest.fixture(scope='module')
+def tmpdata(tmpdir_factory):
+    tmpdir = tmpdir_factory.mktemp('tmp')
+    tmpdir_path = str(tmpdir.join('mldata'))
+    os.makedirs(tmpdir_path)
+    yield str(tmpdir)
+    shutil.rmtree(str(tmpdir))
 
 
 def test_mldata_filename():
@@ -42,9 +36,8 @@ def test_mldata_filename():
         assert_equal(mldata_filename(name), desired)
 
 
-def test_download():
+def test_download(tmpdata):
     """Test that fetch_mldata is able to download and cache a data set."""
-    setup_tmpdata()
     _urlopen_ref = datasets.mldata.urlopen
     datasets.mldata.urlopen = mock_mldata_urlopen({
         'mock': {
@@ -53,7 +46,7 @@ def test_download():
         },
     })
     try:
-        mock = fetch_mldata('mock', data_home=tmpdir)
+        mock = fetch_mldata('mock', data_home=tmpdata)
         for n in ["COL_NAMES", "DESCR", "target", "data"]:
             assert_in(n, mock)
 
@@ -64,11 +57,9 @@ def test_download():
                       fetch_mldata, 'not_existing_name')
     finally:
         datasets.mldata.urlopen = _urlopen_ref
-        teardown_tmpdata()
 
 
-def test_fetch_one_column():
-    setup_tmpdata()
+def test_fetch_one_column(tmpdata):
     _urlopen_ref = datasets.mldata.urlopen
     try:
         dataname = 'onecol'
@@ -76,7 +67,7 @@ def test_fetch_one_column():
         x = sp.arange(6).reshape(2, 3)
         datasets.mldata.urlopen = mock_mldata_urlopen({dataname: {'x': x}})
 
-        dset = fetch_mldata(dataname, data_home=tmpdir)
+        dset = fetch_mldata(dataname, data_home=tmpdata)
         for n in ["COL_NAMES", "DESCR", "data"]:
             assert_in(n, dset)
         assert_not_in("target", dset)
@@ -85,15 +76,13 @@ def test_fetch_one_column():
         assert_array_equal(dset.data, x)
 
         # transposing the data array
-        dset = fetch_mldata(dataname, transpose_data=False, data_home=tmpdir)
+        dset = fetch_mldata(dataname, transpose_data=False, data_home=tmpdata)
         assert_equal(dset.data.shape, (3, 2))
     finally:
         datasets.mldata.urlopen = _urlopen_ref
-        teardown_tmpdata()
 
 
-def test_fetch_multiple_column():
-    setup_tmpdata()
+def test_fetch_multiple_column(tmpdata):
     _urlopen_ref = datasets.mldata.urlopen
     try:
         # create fake data set in cache
@@ -114,7 +103,7 @@ def test_fetch_multiple_column():
             ),
         })
 
-        dset = fetch_mldata(dataname, data_home=tmpdir)
+        dset = fetch_mldata(dataname, data_home=tmpdata)
         for n in ["COL_NAMES", "DESCR", "target", "data", "z"]:
             assert_in(n, dset)
         assert_not_in("x", dset)
@@ -130,7 +119,7 @@ def test_fetch_multiple_column():
             dataname: ({'y': y, 'x': x, 'z': z},
                        ['y', 'x', 'z']), })
 
-        dset = fetch_mldata(dataname, data_home=tmpdir)
+        dset = fetch_mldata(dataname, data_home=tmpdata)
         for n in ["COL_NAMES", "DESCR", "target", "data", "z"]:
             assert_in(n, dset)
         assert_not_in("x", dset)
@@ -148,7 +137,7 @@ def test_fetch_multiple_column():
         })
 
         dset = fetch_mldata(dataname, target_name=2, data_name=0,
-                            data_home=tmpdir)
+                            data_home=tmpdata)
         for n in ["COL_NAMES", "DESCR", "target", "data", "x"]:
             assert_in(n, dset)
         assert_not_in("y", dset)
@@ -159,7 +148,7 @@ def test_fetch_multiple_column():
 
         # by name
         dset = fetch_mldata(dataname, target_name='y', data_name='z',
-                            data_home=tmpdir)
+                            data_home=tmpdata)
         for n in ["COL_NAMES", "DESCR", "target", "data", "x"]:
             assert_in(n, dset)
         assert_not_in("y", dset)
@@ -167,4 +156,3 @@ def test_fetch_multiple_column():
 
     finally:
         datasets.mldata.urlopen = _urlopen_ref
-        teardown_tmpdata()
diff --git a/sklearn/datasets/tests/test_svmlight_format.py b/sklearn/datasets/tests/test_svmlight_format.py
index 35808fc5b3c8..3eab1d7c37eb 100644
--- a/sklearn/datasets/tests/test_svmlight_format.py
+++ b/sklearn/datasets/tests/test_svmlight_format.py
@@ -8,6 +8,8 @@
 import shutil
 from tempfile import NamedTemporaryFile
 
+import pytest
+
 from sklearn.externals.six import b
 
 from sklearn.utils.testing import assert_equal
@@ -414,46 +416,42 @@ def test_load_zeros():
         assert_array_almost_equal(X.toarray(), true_X.toarray())
 
 
-def test_load_with_offsets():
-    def check_load_with_offsets(sparsity, n_samples, n_features):
-        rng = np.random.RandomState(0)
-        X = rng.uniform(low=0.0, high=1.0, size=(n_samples, n_features))
-        if sparsity:
-            X[X < sparsity] = 0.0
-        X = sp.csr_matrix(X)
-        y = rng.randint(low=0, high=2, size=n_samples)
+@pytest.mark.parametrize('sparsity', [0, 0.1, .5, 0.99, 1])
+@pytest.mark.parametrize('n_samples', [13, 101])
+@pytest.mark.parametrize('n_features', [2, 7, 41])
+def test_load_with_offsets(sparsity, n_samples, n_features):
+    rng = np.random.RandomState(0)
+    X = rng.uniform(low=0.0, high=1.0, size=(n_samples, n_features))
+    if sparsity:
+        X[X < sparsity] = 0.0
+    X = sp.csr_matrix(X)
+    y = rng.randint(low=0, high=2, size=n_samples)
 
-        f = BytesIO()
-        dump_svmlight_file(X, y, f)
-        f.seek(0)
+    f = BytesIO()
+    dump_svmlight_file(X, y, f)
+    f.seek(0)
 
-        size = len(f.getvalue())
-
-        # put some marks that are likely to happen anywhere in a row
-        mark_0 = 0
-        mark_1 = size // 3
-        length_0 = mark_1 - mark_0
-        mark_2 = 4 * size // 5
-        length_1 = mark_2 - mark_1
-
-        # load the original sparse matrix into 3 independent CSR matrices
-        X_0, y_0 = load_svmlight_file(f, n_features=n_features,
-                                      offset=mark_0, length=length_0)
-        X_1, y_1 = load_svmlight_file(f, n_features=n_features,
-                                      offset=mark_1, length=length_1)
-        X_2, y_2 = load_svmlight_file(f, n_features=n_features,
-                                      offset=mark_2)
-
-        y_concat = np.concatenate([y_0, y_1, y_2])
-        X_concat = sp.vstack([X_0, X_1, X_2])
-        assert_array_almost_equal(y, y_concat)
-        assert_array_almost_equal(X.toarray(), X_concat.toarray())
+    size = len(f.getvalue())
 
-    # Generate a uniformly random sparse matrix
-    for sparsity in [0, 0.1, .5, 0.99, 1]:
-        for n_samples in [13, 101]:
-            for n_features in [2, 7, 41]:
-                yield check_load_with_offsets, sparsity, n_samples, n_features
+    # put some marks that are likely to happen anywhere in a row
+    mark_0 = 0
+    mark_1 = size // 3
+    length_0 = mark_1 - mark_0
+    mark_2 = 4 * size // 5
+    length_1 = mark_2 - mark_1
+
+    # load the original sparse matrix into 3 independent CSR matrices
+    X_0, y_0 = load_svmlight_file(f, n_features=n_features,
+                                  offset=mark_0, length=length_0)
+    X_1, y_1 = load_svmlight_file(f, n_features=n_features,
+                                  offset=mark_1, length=length_1)
+    X_2, y_2 = load_svmlight_file(f, n_features=n_features,
+                                  offset=mark_2)
+
+    y_concat = np.concatenate([y_0, y_1, y_2])
+    X_concat = sp.vstack([X_0, X_1, X_2])
+    assert_array_almost_equal(y, y_concat)
+    assert_array_almost_equal(X.toarray(), X_concat.toarray())
 
 
 def test_load_offset_exhaustive_splits():
diff --git a/sklearn/datasets/twenty_newsgroups.py b/sklearn/datasets/twenty_newsgroups.py
index a51812dd601d..6eed41f0de88 100644
--- a/sklearn/datasets/twenty_newsgroups.py
+++ b/sklearn/datasets/twenty_newsgroups.py
@@ -311,8 +311,9 @@ def fetch_20newsgroups_vectorized(subset="train", remove=(), data_home=None,
         If False, raise an IOError if the data is not locally available
         instead of trying to download the data from the source site.
 
-    return_X_y : boolean, default=False. If True, returns ``(data.data,
-    data.target)`` instead of a Bunch object.
+    return_X_y : boolean, default=False.
+        If True, returns ``(data.data, data.target)`` instead of a Bunch
+        object.
 
         .. versionadded:: 0.20
 
diff --git a/sklearn/decomposition/__init__.py b/sklearn/decomposition/__init__.py
index faca56b91b1d..34ad76ca4607 100644
--- a/sklearn/decomposition/__init__.py
+++ b/sklearn/decomposition/__init__.py
@@ -5,7 +5,7 @@
 """
 
 from .nmf import NMF, non_negative_factorization
-from .pca import PCA, RandomizedPCA
+from .pca import PCA
 from .incremental_pca import IncrementalPCA
 from .kernel_pca import KernelPCA
 from .sparse_pca import SparsePCA, MiniBatchSparsePCA
@@ -26,7 +26,6 @@
            'MiniBatchSparsePCA',
            'NMF',
            'PCA',
-           'RandomizedPCA',
            'SparseCoder',
            'SparsePCA',
            'dict_learning',
diff --git a/sklearn/decomposition/dict_learning.py b/sklearn/decomposition/dict_learning.py
index e4b36d120773..dd0adb0c2a2f 100644
--- a/sklearn/decomposition/dict_learning.py
+++ b/sklearn/decomposition/dict_learning.py
@@ -26,7 +26,8 @@
 
 def _sparse_encode(X, dictionary, gram, cov=None, algorithm='lasso_lars',
                    regularization=None, copy_cov=True,
-                   init=None, max_iter=1000, check_input=True, verbose=0):
+                   init=None, max_iter=1000, check_input=True, verbose=0,
+                   positive=False):
     """Generic sparse coding
 
     Each column of the result is the solution to a Lasso problem.
@@ -79,6 +80,11 @@ def _sparse_encode(X, dictionary, gram, cov=None, algorithm='lasso_lars',
     verbose : int
         Controls the verbosity; the higher, the more messages. Defaults to 0.
 
+    positive: boolean
+        Whether to enforce a positivity constraint on the sparse code.
+
+        .. versionadded:: 0.20
+
     Returns
     -------
     code : array of shape (n_components, n_features)
@@ -113,7 +119,8 @@ def _sparse_encode(X, dictionary, gram, cov=None, algorithm='lasso_lars',
             # corrects the verbosity level.
             lasso_lars = LassoLars(alpha=alpha, fit_intercept=False,
                                    verbose=verbose, normalize=False,
-                                   precompute=gram, fit_path=False)
+                                   precompute=gram, fit_path=False,
+                                   positive=positive)
             lasso_lars.fit(dictionary.T, X.T, Xy=cov)
             new_code = lasso_lars.coef_
         finally:
@@ -126,7 +133,8 @@ def _sparse_encode(X, dictionary, gram, cov=None, algorithm='lasso_lars',
         # sklearn.linear_model.coordinate_descent.enet_path has a verbosity
         # argument that we could pass in from Lasso.
         clf = Lasso(alpha=alpha, fit_intercept=False, normalize=False,
-                    precompute=gram, max_iter=max_iter, warm_start=True)
+                    precompute=gram, max_iter=max_iter, warm_start=True,
+                    positive=positive)
 
         if init is not None:
             clf.coef_ = init
@@ -142,7 +150,7 @@ def _sparse_encode(X, dictionary, gram, cov=None, algorithm='lasso_lars',
             # corrects the verbosity level.
             lars = Lars(fit_intercept=False, verbose=verbose, normalize=False,
                         precompute=gram, n_nonzero_coefs=int(regularization),
-                        fit_path=False)
+                        fit_path=False, positive=positive)
             lars.fit(dictionary.T, X.T, Xy=cov)
             new_code = lars.coef_
         finally:
@@ -151,9 +159,15 @@ def _sparse_encode(X, dictionary, gram, cov=None, algorithm='lasso_lars',
     elif algorithm == 'threshold':
         new_code = ((np.sign(cov) *
                     np.maximum(np.abs(cov) - regularization, 0)).T)
+        if positive:
+            new_code[new_code < 0] = 0
 
     elif algorithm == 'omp':
         # TODO: Should verbose argument be passed to this?
+        if positive:
+            raise ValueError(
+                "Positive constraint not supported for \"omp\" coding method."
+            )
         new_code = orthogonal_mp_gram(
             Gram=gram, Xy=cov, n_nonzero_coefs=int(regularization),
             tol=None, norms_squared=row_norms(X, squared=True),
@@ -170,7 +184,8 @@ def _sparse_encode(X, dictionary, gram, cov=None, algorithm='lasso_lars',
 # XXX : could be moved to the linear_model module
 def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars',
                   n_nonzero_coefs=None, alpha=None, copy_cov=True, init=None,
-                  max_iter=1000, n_jobs=1, check_input=True, verbose=0):
+                  max_iter=1000, n_jobs=1, check_input=True, verbose=0,
+                  positive=False):
     """Sparse coding
 
     Each row of the result is the solution to a sparse coding problem.
@@ -240,6 +255,11 @@ def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars',
     verbose : int, optional
         Controls the verbosity; the higher, the more messages. Defaults to 0.
 
+    positive : boolean, optional
+        Whether to enforce positivity when finding the encoding.
+
+        .. versionadded:: 0.20
+
     Returns
     -------
     code : array of shape (n_samples, n_components)
@@ -287,7 +307,8 @@ def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars',
                               init=init,
                               max_iter=max_iter,
                               check_input=False,
-                              verbose=verbose)
+                              verbose=verbose,
+                              positive=positive)
         return code
 
     # Enter parallel code block
@@ -302,7 +323,8 @@ def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars',
             regularization=regularization, copy_cov=copy_cov,
             init=init[this_slice] if init is not None else None,
             max_iter=max_iter,
-            check_input=False)
+            check_input=False,
+            positive=positive)
         for this_slice in slices)
     for this_slice, this_view in zip(slices, code_views):
         code[this_slice] = this_view
@@ -310,7 +332,7 @@ def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars',
 
 
 def _update_dict(dictionary, Y, code, verbose=False, return_r2=False,
-                 random_state=None):
+                 random_state=None, positive=False):
     """Update the dense dictionary factor in place.
 
     Parameters
@@ -337,6 +359,11 @@ def _update_dict(dictionary, Y, code, verbose=False, return_r2=False,
         If None, the random number generator is the RandomState instance used
         by `np.random`.
 
+    positive : boolean, optional
+        Whether to enforce positivity when finding the dictionary.
+
+        .. versionadded:: 0.20
+
     Returns
     -------
     dictionary : array of shape (n_features, n_components)
@@ -344,7 +371,7 @@ def _update_dict(dictionary, Y, code, verbose=False, return_r2=False,
 
     """
     n_components = len(code)
-    n_samples = Y.shape[0]
+    n_features = Y.shape[0]
     random_state = check_random_state(random_state)
     # Residuals, computed 'in-place' for efficiency
     R = -np.dot(dictionary, code)
@@ -355,6 +382,8 @@ def _update_dict(dictionary, Y, code, verbose=False, return_r2=False,
         # R <- 1.0 * U_k * V_k^T + R
         R = ger(1.0, dictionary[:, k], code[k, :], a=R, overwrite_a=True)
         dictionary[:, k] = np.dot(R, code[k, :].T)
+        if positive:
+            dictionary[:, k][dictionary[:, k] < 0] = 0.0
         # Scale k'th atom
         atom_norm_square = np.dot(dictionary[:, k], dictionary[:, k])
         if atom_norm_square < 1e-20:
@@ -363,7 +392,9 @@ def _update_dict(dictionary, Y, code, verbose=False, return_r2=False,
                 sys.stdout.flush()
             elif verbose:
                 print("Adding new random atom")
-            dictionary[:, k] = random_state.randn(n_samples)
+            dictionary[:, k] = random_state.randn(n_features)
+            if positive:
+                dictionary[:, k][dictionary[:, k] < 0] = 0.0
             # Setting corresponding coefs to 0
             code[k, :] = 0.0
             dictionary[:, k] /= sqrt(np.dot(dictionary[:, k],
@@ -387,7 +418,8 @@ def _update_dict(dictionary, Y, code, verbose=False, return_r2=False,
 def dict_learning(X, n_components, alpha, max_iter=100, tol=1e-8,
                   method='lars', n_jobs=1, dict_init=None, code_init=None,
                   callback=None, verbose=False, random_state=None,
-                  return_n_iter=False):
+                  return_n_iter=False, positive_dict=False,
+                  positive_code=False):
     """Solves a dictionary learning matrix factorization problem.
 
     Finds the best dictionary and the corresponding sparse code for
@@ -449,6 +481,16 @@ def dict_learning(X, n_components, alpha, max_iter=100, tol=1e-8,
     return_n_iter : bool
         Whether or not to return the number of iterations.
 
+    positive_dict : bool
+        Whether to enforce positivity when finding the dictionary.
+
+        .. versionadded:: 0.20
+
+    positive_code : bool
+        Whether to enforce positivity when finding the code.
+
+        .. versionadded:: 0.20
+
     Returns
     -------
     code : array of shape (n_samples, n_components)
@@ -528,11 +570,12 @@ def dict_learning(X, n_components, alpha, max_iter=100, tol=1e-8,
 
         # Update code
         code = sparse_encode(X, dictionary, algorithm=method, alpha=alpha,
-                             init=code, n_jobs=n_jobs)
+                             init=code, n_jobs=n_jobs, positive=positive_code)
         # Update dictionary
         dictionary, residuals = _update_dict(dictionary.T, X.T, code.T,
                                              verbose=verbose, return_r2=True,
-                                             random_state=random_state)
+                                             random_state=random_state,
+                                             positive=positive_dict)
         dictionary = dictionary.T
 
         # Cost function
@@ -563,7 +606,8 @@ def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,
                          batch_size=3, verbose=False, shuffle=True, n_jobs=1,
                          method='lars', iter_offset=0, random_state=None,
                          return_inner_stats=False, inner_stats=None,
-                         return_n_iter=False):
+                         return_n_iter=False, positive_dict=False,
+                         positive_code=False):
     """Solves a dictionary learning matrix factorization problem online.
 
     Finds the best dictionary and the corresponding sparse code for
@@ -647,6 +691,16 @@ def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,
     return_n_iter : bool
         Whether or not to return the number of iterations.
 
+    positive_dict : bool
+        Whether to enforce positivity when finding the dictionary.
+
+        .. versionadded:: 0.20
+
+    positive_code : bool
+        Whether to enforce positivity when finding the code.
+
+        .. versionadded:: 0.20
+
     Returns
     -------
     code : array of shape (n_samples, n_components),
@@ -709,6 +763,8 @@ def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,
 
     dictionary = check_array(dictionary.T, order='F', dtype=np.float64,
                              copy=False)
+    dictionary = np.require(dictionary, requirements='W')
+
     X_train = check_array(X_train, order='C', dtype=np.float64, copy=False)
 
     batches = gen_batches(n_samples, batch_size)
@@ -738,7 +794,8 @@ def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,
                       % (ii, dt, dt / 60))
 
         this_code = sparse_encode(this_X, dictionary.T, algorithm=method,
-                                  alpha=alpha, n_jobs=n_jobs).T
+                                  alpha=alpha, n_jobs=n_jobs,
+                                  positive=positive_code).T
 
         # Update the auxiliary variables
         if ii < batch_size - 1:
@@ -754,7 +811,8 @@ def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,
 
         # Update dictionary
         dictionary = _update_dict(dictionary, B, A, verbose=verbose,
-                                  random_state=random_state)
+                                  random_state=random_state,
+                                  positive=positive_dict)
         # XXX: Can the residuals be of any use?
 
         # Maybe we need a stopping criteria based on the amount of
@@ -773,7 +831,8 @@ def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,
         elif verbose == 1:
             print('|', end=' ')
         code = sparse_encode(X, dictionary.T, algorithm=method, alpha=alpha,
-                             n_jobs=n_jobs, check_input=False)
+                             n_jobs=n_jobs, check_input=False,
+                             positive=positive_code)
         if verbose > 1:
             dt = (time.time() - t0)
             print('done (total time: % 3is, % 4.1fmn)' % (dt, dt / 60))
@@ -795,13 +854,14 @@ def _set_sparse_coding_params(self, n_components,
                                   transform_algorithm='omp',
                                   transform_n_nonzero_coefs=None,
                                   transform_alpha=None, split_sign=False,
-                                  n_jobs=1):
+                                  n_jobs=1, positive_code=False):
         self.n_components = n_components
         self.transform_algorithm = transform_algorithm
         self.transform_n_nonzero_coefs = transform_n_nonzero_coefs
         self.transform_alpha = transform_alpha
         self.split_sign = split_sign
         self.n_jobs = n_jobs
+        self.positive_code = positive_code
 
     def transform(self, X):
         """Encode the data as a sparse combination of the dictionary atoms.
@@ -828,7 +888,8 @@ def transform(self, X):
         code = sparse_encode(
             X, self.components_, algorithm=self.transform_algorithm,
             n_nonzero_coefs=self.transform_n_nonzero_coefs,
-            alpha=self.transform_alpha, n_jobs=self.n_jobs)
+            alpha=self.transform_alpha, n_jobs=self.n_jobs,
+            positive=self.positive_code)
 
         if self.split_sign:
             # feature vector is split into a positive and negative side
@@ -894,6 +955,11 @@ class SparseCoder(BaseEstimator, SparseCodingMixin):
     n_jobs : int,
         number of parallel jobs to run
 
+    positive_code : bool
+        Whether to enforce positivity when finding the code.
+
+        .. versionadded:: 0.20
+
     Attributes
     ----------
     components_ : array, [n_components, n_features]
@@ -911,11 +977,12 @@ class SparseCoder(BaseEstimator, SparseCodingMixin):
 
     def __init__(self, dictionary, transform_algorithm='omp',
                  transform_n_nonzero_coefs=None, transform_alpha=None,
-                 split_sign=False, n_jobs=1):
+                 split_sign=False, n_jobs=1, positive_code=False):
         self._set_sparse_coding_params(dictionary.shape[0],
                                        transform_algorithm,
                                        transform_n_nonzero_coefs,
-                                       transform_alpha, split_sign, n_jobs)
+                                       transform_alpha, split_sign, n_jobs,
+                                       positive_code)
         self.components_ = dictionary
 
     def fit(self, X, y=None):
@@ -1028,6 +1095,16 @@ class DictionaryLearning(BaseEstimator, SparseCodingMixin):
         If None, the random number generator is the RandomState instance used
         by `np.random`.
 
+    positive_code : bool
+        Whether to enforce positivity when finding the code.
+
+        .. versionadded:: 0.20
+
+    positive_dict : bool
+        Whether to enforce positivity when finding the dictionary
+
+        .. versionadded:: 0.20
+
     Attributes
     ----------
     components_ : array, [n_components, n_features]
@@ -1057,11 +1134,13 @@ def __init__(self, n_components=None, alpha=1, max_iter=1000, tol=1e-8,
                  fit_algorithm='lars', transform_algorithm='omp',
                  transform_n_nonzero_coefs=None, transform_alpha=None,
                  n_jobs=1, code_init=None, dict_init=None, verbose=False,
-                 split_sign=False, random_state=None):
+                 split_sign=False, random_state=None,
+                 positive_code=False, positive_dict=False):
 
         self._set_sparse_coding_params(n_components, transform_algorithm,
                                        transform_n_nonzero_coefs,
-                                       transform_alpha, split_sign, n_jobs)
+                                       transform_alpha, split_sign, n_jobs,
+                                       positive_code)
         self.alpha = alpha
         self.max_iter = max_iter
         self.tol = tol
@@ -1070,6 +1149,7 @@ def __init__(self, n_components=None, alpha=1, max_iter=1000, tol=1e-8,
         self.dict_init = dict_init
         self.verbose = verbose
         self.random_state = random_state
+        self.positive_dict = positive_dict
 
     def fit(self, X, y=None):
         """Fit the model from data in X.
@@ -1103,7 +1183,9 @@ def fit(self, X, y=None):
             dict_init=self.dict_init,
             verbose=self.verbose,
             random_state=random_state,
-            return_n_iter=True)
+            return_n_iter=True,
+            positive_dict=self.positive_dict,
+            positive_code=self.positive_code)
         self.components_ = U
         self.error_ = E
         return self
@@ -1193,6 +1275,16 @@ class MiniBatchDictionaryLearning(BaseEstimator, SparseCodingMixin):
         If None, the random number generator is the RandomState instance used
         by `np.random`.
 
+    positive_code : bool
+        Whether to enforce positivity when finding the code.
+
+        .. versionadded:: 0.20
+
+    positive_dict : bool
+        Whether to enforce positivity when finding the dictionary.
+
+        .. versionadded:: 0.20
+
     Attributes
     ----------
     components_ : array, [n_components, n_features]
@@ -1228,11 +1320,13 @@ def __init__(self, n_components=None, alpha=1, n_iter=1000,
                  fit_algorithm='lars', n_jobs=1, batch_size=3,
                  shuffle=True, dict_init=None, transform_algorithm='omp',
                  transform_n_nonzero_coefs=None, transform_alpha=None,
-                 verbose=False, split_sign=False, random_state=None):
+                 verbose=False, split_sign=False, random_state=None,
+                 positive_code=False, positive_dict=False):
 
         self._set_sparse_coding_params(n_components, transform_algorithm,
                                        transform_n_nonzero_coefs,
-                                       transform_alpha, split_sign, n_jobs)
+                                       transform_alpha, split_sign, n_jobs,
+                                       positive_code)
         self.alpha = alpha
         self.n_iter = n_iter
         self.fit_algorithm = fit_algorithm
@@ -1242,6 +1336,7 @@ def __init__(self, n_components=None, alpha=1, n_iter=1000,
         self.batch_size = batch_size
         self.split_sign = split_sign
         self.random_state = random_state
+        self.positive_dict = positive_dict
 
     def fit(self, X, y=None):
         """Fit the model from data in X.
@@ -1270,7 +1365,9 @@ def fit(self, X, y=None):
             batch_size=self.batch_size, shuffle=self.shuffle,
             verbose=self.verbose, random_state=random_state,
             return_inner_stats=True,
-            return_n_iter=True)
+            return_n_iter=True,
+            positive_dict=self.positive_dict,
+            positive_code=self.positive_code)
         self.components_ = U
         # Keep track of the state of the algorithm to be able to do
         # some online fitting (partial_fit)
@@ -1317,7 +1414,9 @@ def partial_fit(self, X, y=None, iter_offset=None):
             batch_size=len(X), shuffle=False,
             verbose=self.verbose, return_code=False,
             iter_offset=iter_offset, random_state=self.random_state_,
-            return_inner_stats=True, inner_stats=inner_stats)
+            return_inner_stats=True, inner_stats=inner_stats,
+            positive_dict=self.positive_dict,
+            positive_code=self.positive_code)
         self.components_ = U
 
         # Keep track of the state of the algorithm to be able to do
diff --git a/sklearn/decomposition/incremental_pca.py b/sklearn/decomposition/incremental_pca.py
index 13e51090dd82..72f1326c5843 100644
--- a/sklearn/decomposition/incremental_pca.py
+++ b/sklearn/decomposition/incremental_pca.py
@@ -136,7 +136,6 @@ class IncrementalPCA(_BasePCA):
     See also
     --------
     PCA
-    RandomizedPCA
     KernelPCA
     SparsePCA
     TruncatedSVD
@@ -243,9 +242,10 @@ def partial_fit(self, X, y=None, check_input=True):
 
         # Update stats - they are 0 if this is the fisrt step
         col_mean, col_var, n_total_samples = \
-            _incremental_mean_and_var(X, last_mean=self.mean_,
-                                      last_variance=self.var_,
-                                      last_sample_count=self.n_samples_seen_)
+            _incremental_mean_and_var(
+                X, last_mean=self.mean_, last_variance=self.var_,
+                last_sample_count=np.repeat(self.n_samples_seen_, X.shape[1]))
+        n_total_samples = n_total_samples[0]
 
         # Whitening
         if self.n_samples_seen_ == 0:
diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py
index c246d37ed262..3316ddb24d2d 100644
--- a/sklearn/decomposition/kernel_pca.py
+++ b/sklearn/decomposition/kernel_pca.py
@@ -107,10 +107,12 @@ class KernelPCA(BaseEstimator, TransformerMixin):
         `remove_zero_eig` are not set, then all components are stored.
 
     dual_coef_ : array, (n_samples, n_features)
-        Inverse transform matrix. Set if `fit_inverse_transform` is True.
+        Inverse transform matrix. Only available when
+        ``fit_inverse_transform`` is True.
 
     X_transformed_fit_ : array, (n_samples, n_components)
         Projection of the fitted data on the kernel principal components.
+        Only available when ``fit_inverse_transform`` is True.
 
     X_fit_ : (n_samples, n_features)
         The data used to fit the model. If `copy_X=False`, then `X_fit_` is
diff --git a/sklearn/decomposition/online_lda.py b/sklearn/decomposition/online_lda.py
index bf35b20f35b8..fa40e2ef6802 100644
--- a/sklearn/decomposition/online_lda.py
+++ b/sklearn/decomposition/online_lda.py
@@ -156,12 +156,11 @@ class LatentDirichletAllocation(BaseEstimator, TransformerMixin):
         to `1 / n_components`.
         In the literature, this is called `beta`.
 
-    learning_method : 'batch' | 'online', default='online'
+    learning_method : 'batch' | 'online', default='batch'
         Method used to update `_component`. Only used in `fit` method.
         In general, if the data size is large, the online update will be much
         faster than the batch update.
-        The default learning method is going to be changed to 'batch' in the
-        0.20 release.
+
         Valid options::
 
             'batch': Batch variational Bayes method. Use all training data in
@@ -172,6 +171,9 @@ class LatentDirichletAllocation(BaseEstimator, TransformerMixin):
                 variable incrementally. The learning rate is controlled by the
                 ``learning_decay`` and the ``learning_offset`` parameters.
 
+        .. versionchanged:: 0.20
+            The default learning method is now ``"batch"``.
+
     learning_decay : float, optional (default=0.7)
         It is a parameter that control learning rate in the online learning
         method. The value should be set between (0.5, 1.0] to guarantee
@@ -262,7 +264,7 @@ class LatentDirichletAllocation(BaseEstimator, TransformerMixin):
     """
 
     def __init__(self, n_components=10, doc_topic_prior=None,
-                 topic_word_prior=None, learning_method=None,
+                 topic_word_prior=None, learning_method='batch',
                  learning_decay=.7, learning_offset=10., max_iter=10,
                  batch_size=128, evaluate_every=-1, total_samples=1e6,
                  perp_tol=1e-1, mean_change_tol=1e-3, max_doc_update_iter=100,
@@ -307,7 +309,7 @@ def _check_params(self):
             raise ValueError("Invalid 'learning_offset' parameter: %r"
                              % self.learning_offset)
 
-        if self.learning_method not in ("batch", "online", None):
+        if self.learning_method not in ("batch", "online"):
             raise ValueError("Invalid 'learning_method' parameter: %r"
                              % self.learning_method)
 
@@ -529,12 +531,6 @@ def fit(self, X, y=None):
         max_iter = self.max_iter
         evaluate_every = self.evaluate_every
         learning_method = self.learning_method
-        if learning_method is None:
-            warnings.warn("The default value for 'learning_method' will be "
-                          "changed from 'online' to 'batch' in the release "
-                          "0.20. This warning was introduced in 0.18.",
-                          DeprecationWarning)
-            learning_method = 'online'
 
         batch_size = self.batch_size
 
diff --git a/sklearn/decomposition/pca.py b/sklearn/decomposition/pca.py
index 1c93f6b00134..5d7d3958251b 100644
--- a/sklearn/decomposition/pca.py
+++ b/sklearn/decomposition/pca.py
@@ -22,9 +22,7 @@
 from ..externals import six
 
 from .base import _BasePCA
-from ..base import BaseEstimator, TransformerMixin
-from ..utils import deprecated
-from ..utils import check_random_state, as_float_array
+from ..utils import check_random_state
 from ..utils import check_array
 from ..utils.extmath import fast_logdet, randomized_svd, svd_flip
 from ..utils.extmath import stable_cumsum
@@ -389,25 +387,25 @@ def _fit(self, X):
             n_components = self.n_components
 
         # Handle svd_solver
-        svd_solver = self.svd_solver
-        if svd_solver == 'auto':
+        self._fit_svd_solver = self.svd_solver
+        if self._fit_svd_solver == 'auto':
             # Small problem or n_components == 'mle', just call full PCA
             if max(X.shape) <= 500 or n_components == 'mle':
-                svd_solver = 'full'
+                self._fit_svd_solver = 'full'
             elif n_components >= 1 and n_components < .8 * min(X.shape):
-                svd_solver = 'randomized'
+                self._fit_svd_solver = 'randomized'
             # This is also the case of n_components in (0,1)
             else:
-                svd_solver = 'full'
+                self._fit_svd_solver = 'full'
 
         # Call different fits for either full or truncated SVD
-        if svd_solver == 'full':
+        if self._fit_svd_solver == 'full':
             return self._fit_full(X, n_components)
-        elif svd_solver in ['arpack', 'randomized']:
-            return self._fit_truncated(X, n_components, svd_solver)
+        elif self._fit_svd_solver in ['arpack', 'randomized']:
+            return self._fit_truncated(X, n_components, self._fit_svd_solver)
         else:
             raise ValueError("Unrecognized svd_solver='{0}'"
-                             "".format(svd_solver))
+                             "".format(self._fit_svd_solver))
 
     def _fit_full(self, X, n_components):
         """Fit the model by computing full SVD on X"""
@@ -591,248 +589,3 @@ def score(self, X, y=None):
             Average log-likelihood of the samples under the current model
         """
         return np.mean(self.score_samples(X))
-
-
-@deprecated("RandomizedPCA was deprecated in 0.18 and will be removed in "
-            "0.20. "
-            "Use PCA(svd_solver='randomized') instead. The new implementation "
-            "DOES NOT store whiten ``components_``. Apply transform to get "
-            "them.")
-class RandomizedPCA(BaseEstimator, TransformerMixin):
-    """Principal component analysis (PCA) using randomized SVD
-
-    .. deprecated:: 0.18
-        This class will be removed in 0.20.
-        Use :class:`PCA` with parameter svd_solver 'randomized' instead.
-        The new implementation DOES NOT store whiten ``components_``.
-        Apply transform to get them.
-
-    Linear dimensionality reduction using approximated Singular Value
-    Decomposition of the data and keeping only the most significant
-    singular vectors to project the data to a lower dimensional space.
-
-    Read more in the :ref:`User Guide <RandomizedPCA>`.
-
-    Parameters
-    ----------
-    n_components : int, optional
-        Maximum number of components to keep. When not given or None, this
-        is set to n_features (the second dimension of the training data).
-
-    copy : bool
-        If False, data passed to fit are overwritten and running
-        fit(X).transform(X) will not yield the expected results,
-        use fit_transform(X) instead.
-
-    iterated_power : int, default=2
-        Number of iterations for the power method.
-
-        .. versionchanged:: 0.18
-
-    whiten : bool, optional
-        When True (False by default) the `components_` vectors are multiplied
-        by the square root of (n_samples) and divided by the singular values to
-        ensure uncorrelated outputs with unit component-wise variances.
-
-        Whitening will remove some information from the transformed signal
-        (the relative variance scales of the components) but can sometime
-        improve the predictive accuracy of the downstream estimators by
-        making their data respect some hard-wired assumptions.
-
-    random_state : int, RandomState instance or None, optional, default=None
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
-
-    Attributes
-    ----------
-    components_ : array, shape (n_components, n_features)
-        Components with maximum variance.
-
-    explained_variance_ratio_ : array, shape (n_components,)
-        Percentage of variance explained by each of the selected components.
-        If k is not set then all components are stored and the sum of explained
-        variances is equal to 1.0.
-
-    singular_values_ : array, shape (n_components,)
-        The singular values corresponding to each of the selected components.
-        The singular values are equal to the 2-norms of the ``n_components``
-        variables in the lower-dimensional space.
-
-    mean_ : array, shape (n_features,)
-        Per-feature empirical mean, estimated from the training set.
-
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from sklearn.decomposition import RandomizedPCA
-    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
-    >>> pca = RandomizedPCA(n_components=2)
-    >>> pca.fit(X)                 # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
-    RandomizedPCA(copy=True, iterated_power=2, n_components=2,
-           random_state=None, whiten=False)
-    >>> print(pca.explained_variance_ratio_)  # doctest: +ELLIPSIS
-    [0.9924... 0.007557...]
-    >>> print(pca.singular_values_)  # doctest: +ELLIPSIS
-    [6.30061... 0.54980...]
-
-    See also
-    --------
-    PCA
-    TruncatedSVD
-
-    References
-    ----------
-
-    .. [Halko2009] `Finding structure with randomness: Stochastic algorithms
-      for constructing approximate matrix decompositions Halko, et al., 2009
-      (arXiv:909)`
-
-    .. [MRT] `A randomized algorithm for the decomposition of matrices
-      Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert`
-
-    """
-
-    def __init__(self, n_components=None, copy=True, iterated_power=2,
-                 whiten=False, random_state=None):
-        self.n_components = n_components
-        self.copy = copy
-        self.iterated_power = iterated_power
-        self.whiten = whiten
-        self.random_state = random_state
-
-    def fit(self, X, y=None):
-        """Fit the model with X by extracting the first principal components.
-
-        Parameters
-        ----------
-        X : array-like, shape (n_samples, n_features)
-            Training data, where n_samples in the number of samples
-            and n_features is the number of features.
-
-        y : Ignored
-
-        Returns
-        -------
-        self : object
-            Returns the instance itself.
-        """
-        self._fit(check_array(X))
-        return self
-
-    def _fit(self, X):
-        """Fit the model to the data X.
-
-        Parameters
-        ----------
-        X : array-like, shape (n_samples, n_features)
-            Training vector, where n_samples in the number of samples and
-            n_features is the number of features.
-
-        Returns
-        -------
-        X : ndarray, shape (n_samples, n_features)
-            The input data, copied, centered and whitened when requested.
-        """
-        random_state = check_random_state(self.random_state)
-        X = np.atleast_2d(as_float_array(X, copy=self.copy))
-
-        n_samples = X.shape[0]
-
-        # Center data
-        self.mean_ = np.mean(X, axis=0)
-        X -= self.mean_
-        if self.n_components is None:
-            n_components = X.shape[1]
-        else:
-            n_components = self.n_components
-
-        U, S, V = randomized_svd(X, n_components,
-                                 n_iter=self.iterated_power,
-                                 random_state=random_state)
-
-        self.explained_variance_ = exp_var = (S ** 2) / (n_samples - 1)
-        full_var = np.var(X, ddof=1, axis=0).sum()
-        self.explained_variance_ratio_ = exp_var / full_var
-        self.singular_values_ = S  # Store the singular values.
-
-        if self.whiten:
-            self.components_ = V / S[:, np.newaxis] * sqrt(n_samples)
-        else:
-            self.components_ = V
-
-        return X
-
-    def transform(self, X):
-        """Apply dimensionality reduction on X.
-
-        X is projected on the first principal components previous extracted
-        from a training set.
-
-        Parameters
-        ----------
-        X : array-like, shape (n_samples, n_features)
-            New data, where n_samples in the number of samples
-            and n_features is the number of features.
-
-        Returns
-        -------
-        X_new : array-like, shape (n_samples, n_components)
-
-        """
-        check_is_fitted(self, 'mean_')
-
-        X = check_array(X)
-        if self.mean_ is not None:
-            X = X - self.mean_
-
-        X = np.dot(X, self.components_.T)
-        return X
-
-    def fit_transform(self, X, y=None):
-        """Fit the model with X and apply the dimensionality reduction on X.
-
-        Parameters
-        ----------
-        X : array-like, shape (n_samples, n_features)
-            New data, where n_samples in the number of samples
-            and n_features is the number of features.
-
-        y : Ignored
-
-        Returns
-        -------
-        X_new : array-like, shape (n_samples, n_components)
-
-        """
-        X = check_array(X)
-        X = self._fit(X)
-        return np.dot(X, self.components_.T)
-
-    def inverse_transform(self, X):
-        """Transform data back to its original space.
-
-        Returns an array X_original whose transform would be X.
-
-        Parameters
-        ----------
-        X : array-like, shape (n_samples, n_components)
-            New data, where n_samples in the number of samples
-            and n_components is the number of components.
-
-        Returns
-        -------
-        X_original array-like, shape (n_samples, n_features)
-
-        Notes
-        -----
-        If whitening is enabled, inverse_transform does not compute the
-        exact inverse operation of transform.
-        """
-        check_is_fitted(self, 'mean_')
-
-        X_original = np.dot(X, self.components_)
-        if self.mean_ is not None:
-            X_original = X_original + self.mean_
-        return X_original
diff --git a/sklearn/decomposition/tests/test_dict_learning.py b/sklearn/decomposition/tests/test_dict_learning.py
index df3c32632d2e..831af46e4613 100644
--- a/sklearn/decomposition/tests/test_dict_learning.py
+++ b/sklearn/decomposition/tests/test_dict_learning.py
@@ -1,3 +1,6 @@
+from __future__ import division
+import pytest
+
 import numpy as np
 import itertools
 
@@ -55,6 +58,38 @@ def test_dict_learning_overcomplete():
     assert_true(dico.components_.shape == (n_components, n_features))
 
 
+@pytest.mark.parametrize("transform_algorithm", [
+    "lasso_lars",
+    "lasso_cd",
+    "lars",
+    "threshold",
+])
+@pytest.mark.parametrize("positive_code", [
+    False,
+    True,
+])
+@pytest.mark.parametrize("positive_dict", [
+    False,
+    True,
+])
+def test_dict_learning_positivity(transform_algorithm,
+                                  positive_code,
+                                  positive_dict):
+    n_components = 5
+    dico = DictionaryLearning(
+        n_components, transform_algorithm=transform_algorithm, random_state=0,
+        positive_code=positive_code, positive_dict=positive_dict).fit(X)
+    code = dico.transform(X)
+    if positive_dict:
+        assert_true((dico.components_ >= 0).all())
+    else:
+        assert_true((dico.components_ < 0).any())
+    if positive_code:
+        assert_true((code >= 0).all())
+    else:
+        assert_true((code < 0).any())
+
+
 def test_dict_learning_reconstruction():
     n_components = 12
     dico = DictionaryLearning(n_components, transform_algorithm='omp',
@@ -135,6 +170,53 @@ def test_dict_learning_online_shapes():
     assert_equal(np.dot(code, dictionary).shape, X.shape)
 
 
+@pytest.mark.parametrize("transform_algorithm", [
+    "lasso_lars",
+    "lasso_cd",
+    "lars",
+    "threshold",
+])
+@pytest.mark.parametrize("positive_code", [
+    False,
+    True,
+])
+@pytest.mark.parametrize("positive_dict", [
+    False,
+    True,
+])
+def test_dict_learning_online_positivity(transform_algorithm,
+                                         positive_code,
+                                         positive_dict):
+    rng = np.random.RandomState(0)
+    n_components = 8
+
+    dico = MiniBatchDictionaryLearning(
+        n_components, transform_algorithm=transform_algorithm, random_state=0,
+        positive_code=positive_code, positive_dict=positive_dict).fit(X)
+    code = dico.transform(X)
+    if positive_dict:
+        assert_true((dico.components_ >= 0).all())
+    else:
+        assert_true((dico.components_ < 0).any())
+    if positive_code:
+        assert_true((code >= 0).all())
+    else:
+        assert_true((code < 0).any())
+
+    code, dictionary = dict_learning_online(X, n_components=n_components,
+                                            alpha=1, random_state=rng,
+                                            positive_dict=positive_dict,
+                                            positive_code=positive_code)
+    if positive_dict:
+        assert_true((dictionary >= 0).all())
+    else:
+        assert_true((dictionary < 0).any())
+    if positive_code:
+        assert_true((code >= 0).all())
+    else:
+        assert_true((code < 0).any())
+
+
 def test_dict_learning_online_verbosity():
     n_components = 5
     # test verbosity
@@ -183,6 +265,15 @@ def test_dict_learning_online_initialization():
     assert_array_equal(dico.components_, V)
 
 
+def test_dict_learning_online_readonly_initialization():
+    n_components = 12
+    rng = np.random.RandomState(0)
+    V = rng.randn(n_components, n_features)
+    V.setflags(write=False)
+    MiniBatchDictionaryLearning(n_components, n_iter=1, dict_init=V,
+                                random_state=0, shuffle=False).fit(X)
+
+
 def test_dict_learning_online_partial_fit():
     n_components = 12
     rng = np.random.RandomState(0)
@@ -215,6 +306,29 @@ def test_sparse_encode_shapes():
         assert_equal(code.shape, (n_samples, n_components))
 
 
+@pytest.mark.parametrize("positive", [
+    False,
+    True,
+])
+def test_sparse_encode_positivity(positive):
+    n_components = 12
+    rng = np.random.RandomState(0)
+    V = rng.randn(n_components, n_features)  # random init
+    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]
+    for algo in ('lasso_lars', 'lasso_cd', 'lars', 'threshold'):
+        code = sparse_encode(X, V, algorithm=algo, positive=positive)
+        if positive:
+            assert_true((code >= 0).all())
+        else:
+            assert_true((code < 0).any())
+
+    try:
+        sparse_encode(X, V, algorithm='omp', positive=positive)
+    except ValueError:
+        if not positive:
+            raise
+
+
 def test_sparse_encode_input():
     n_components = 100
     rng = np.random.RandomState(0)
@@ -262,3 +376,22 @@ def test_sparse_coder_estimator():
                        transform_alpha=0.001).transform(X)
     assert_true(not np.all(code == 0))
     assert_less(np.sqrt(np.sum((np.dot(code, V) - X) ** 2)), 0.1)
+
+
+def test_sparse_coder_parallel_mmap():
+    # Non-regression test for:
+    # https://github.com/scikit-learn/scikit-learn/issues/5956
+    # Test that SparseCoder does not error by passing reading only
+    # arrays to child processes
+
+    rng = np.random.RandomState(777)
+    n_components, n_features = 40, 64
+    init_dict = rng.rand(n_components, n_features)
+    # Ensure that `data` is >2M. Joblib memory maps arrays
+    # if they are larger than 1MB. The 4 accounts for float32
+    # data type
+    n_samples = int(2e6) // (4 * n_features)
+    data = np.random.rand(n_samples, n_features).astype(np.float32)
+
+    sc = SparseCoder(init_dict, transform_algorithm='omp', n_jobs=2)
+    sc.fit_transform(data)
diff --git a/sklearn/decomposition/tests/test_nmf.py b/sklearn/decomposition/tests/test_nmf.py
index 6692aa23d003..8ae157176647 100644
--- a/sklearn/decomposition/tests/test_nmf.py
+++ b/sklearn/decomposition/tests/test_nmf.py
@@ -7,6 +7,8 @@
 from sklearn.decomposition import nmf   # For testing internals
 from scipy.sparse import csc_matrix
 
+import pytest
+
 from sklearn.utils.testing import assert_true
 from sklearn.utils.testing import assert_false
 from sklearn.utils.testing import assert_raise_message, assert_no_warnings
@@ -95,26 +97,26 @@ def test_nmf_fit_nn_output():
                          (transf < 0).any())
 
 
-def test_nmf_fit_close():
+@pytest.mark.parametrize('solver', ('cd', 'mu'))
+def test_nmf_fit_close(solver):
     rng = np.random.mtrand.RandomState(42)
     # Test that the fit is not too far away
-    for solver in ('cd', 'mu'):
-        pnmf = NMF(5, solver=solver, init='nndsvdar', random_state=0,
-                   max_iter=600)
-        X = np.abs(rng.randn(6, 5))
-        assert_less(pnmf.fit(X).reconstruction_err_, 0.1)
+    pnmf = NMF(5, solver=solver, init='nndsvdar', random_state=0,
+               max_iter=600)
+    X = np.abs(rng.randn(6, 5))
+    assert_less(pnmf.fit(X).reconstruction_err_, 0.1)
 
 
-def test_nmf_transform():
+@pytest.mark.parametrize('solver', ('cd', 'mu'))
+def test_nmf_transform(solver):
     # Test that NMF.transform returns close values
     rng = np.random.mtrand.RandomState(42)
     A = np.abs(rng.randn(6, 5))
-    for solver in ['cd', 'mu']:
-        m = NMF(solver=solver, n_components=3, init='random',
-                random_state=0, tol=1e-5)
-        ft = m.fit_transform(A)
-        t = m.transform(A)
-        assert_array_almost_equal(ft, t, decimal=2)
+    m = NMF(solver=solver, n_components=3, init='random',
+            random_state=0, tol=1e-5)
+    ft = m.fit_transform(A)
+    t = m.transform(A)
+    assert_array_almost_equal(ft, t, decimal=2)
 
 
 def test_nmf_transform_custom_init():
@@ -132,16 +134,16 @@ def test_nmf_transform_custom_init():
     m.transform(A)
 
 
-def test_nmf_inverse_transform():
+@pytest.mark.parametrize('solver', ('cd', 'mu'))
+def test_nmf_inverse_transform(solver):
     # Test that NMF.inverse_transform returns close values
     random_state = np.random.RandomState(0)
     A = np.abs(random_state.randn(6, 4))
-    for solver in ('cd', 'mu'):
-        m = NMF(solver=solver, n_components=4, init='random', random_state=0,
-                max_iter=1000)
-        ft = m.fit_transform(A)
-        A_new = m.inverse_transform(ft)
-        assert_array_almost_equal(A, A_new, decimal=2)
+    m = NMF(solver=solver, n_components=4, init='random', random_state=0,
+            max_iter=1000)
+    ft = m.fit_transform(A)
+    A_new = m.inverse_transform(ft)
+    assert_array_almost_equal(A, A_new, decimal=2)
 
 
 def test_n_components_greater_n_features():
diff --git a/sklearn/decomposition/tests/test_online_lda.py b/sklearn/decomposition/tests/test_online_lda.py
index b69788ed566f..b8b636d5a6fd 100644
--- a/sklearn/decomposition/tests/test_online_lda.py
+++ b/sklearn/decomposition/tests/test_online_lda.py
@@ -5,6 +5,8 @@
 from scipy.sparse import csr_matrix
 from scipy.special import psi
 
+import pytest
+
 from sklearn.decomposition import LatentDirichletAllocation
 from sklearn.decomposition._online_lda import (_dirichlet_expectation_1d,
                                                _dirichlet_expectation_2d)
@@ -128,17 +130,17 @@ def test_lda_transform():
                               np.ones(X_trans.shape[0]))
 
 
-def test_lda_fit_transform():
+@pytest.mark.parametrize('method', ('online', 'batch'))
+def test_lda_fit_transform(method):
     # Test LDA fit_transform & transform
     # fit_transform and transform result should be the same
-    for method in ('online', 'batch'):
-        rng = np.random.RandomState(0)
-        X = rng.randint(10, size=(50, 20))
-        lda = LatentDirichletAllocation(n_components=5, learning_method=method,
-                                        random_state=rng)
-        X_fit = lda.fit_transform(X)
-        X_trans = lda.transform(X)
-        assert_array_almost_equal(X_fit, X_trans, 4)
+    rng = np.random.RandomState(0)
+    X = rng.randint(10, size=(50, 20))
+    lda = LatentDirichletAllocation(n_components=5, learning_method=method,
+                                    random_state=rng)
+    X_fit = lda.fit_transform(X)
+    X_trans = lda.transform(X)
+    assert_array_almost_equal(X_fit, X_trans, 4)
 
 
 def test_lda_partial_fit_dim_mismatch():
@@ -205,20 +207,20 @@ def test_lda_transform_mismatch():
 
 
 @if_safe_multiprocessing_with_blas
-def test_lda_multi_jobs():
+@pytest.mark.parametrize('method', ('online', 'batch'))
+def test_lda_multi_jobs(method):
     n_components, X = _build_sparse_mtx()
     # Test LDA batch training with multi CPU
-    for method in ('online', 'batch'):
-        rng = np.random.RandomState(0)
-        lda = LatentDirichletAllocation(n_components=n_components, n_jobs=2,
-                                        learning_method=method,
-                                        evaluate_every=1, random_state=rng)
-        lda.fit(X)
+    rng = np.random.RandomState(0)
+    lda = LatentDirichletAllocation(n_components=n_components, n_jobs=2,
+                                    learning_method=method,
+                                    evaluate_every=1, random_state=rng)
+    lda.fit(X)
 
-        correct_idx_grps = [(0, 1, 2), (3, 4, 5), (6, 7, 8)]
-        for c in lda.components_:
-            top_idx = set(c.argsort()[-3:][::-1])
-            assert_true(tuple(sorted(top_idx)) in correct_idx_grps)
+    correct_idx_grps = [(0, 1, 2), (3, 4, 5), (6, 7, 8)]
+    for c in lda.components_:
+        top_idx = set(c.argsort()[-3:][::-1])
+        assert_true(tuple(sorted(top_idx)) in correct_idx_grps)
 
 
 @if_safe_multiprocessing_with_blas
@@ -259,46 +261,46 @@ def test_lda_preplexity_mismatch():
                          invalid_n_components)
 
 
-def test_lda_perplexity():
+@pytest.mark.parametrize('method', ('online', 'batch'))
+def test_lda_perplexity(method):
     # Test LDA perplexity for batch training
     # perplexity should be lower after each iteration
     n_components, X = _build_sparse_mtx()
-    for method in ('online', 'batch'):
-        lda_1 = LatentDirichletAllocation(n_components=n_components,
-                                          max_iter=1, learning_method=method,
-                                          total_samples=100, random_state=0)
-        lda_2 = LatentDirichletAllocation(n_components=n_components,
-                                          max_iter=10, learning_method=method,
-                                          total_samples=100, random_state=0)
-        lda_1.fit(X)
-        perp_1 = lda_1.perplexity(X, sub_sampling=False)
-
-        lda_2.fit(X)
-        perp_2 = lda_2.perplexity(X, sub_sampling=False)
-        assert_greater_equal(perp_1, perp_2)
-
-        perp_1_subsampling = lda_1.perplexity(X, sub_sampling=True)
-        perp_2_subsampling = lda_2.perplexity(X, sub_sampling=True)
-        assert_greater_equal(perp_1_subsampling, perp_2_subsampling)
-
-
-def test_lda_score():
+    lda_1 = LatentDirichletAllocation(n_components=n_components,
+                                      max_iter=1, learning_method=method,
+                                      total_samples=100, random_state=0)
+    lda_2 = LatentDirichletAllocation(n_components=n_components,
+                                      max_iter=10, learning_method=method,
+                                      total_samples=100, random_state=0)
+    lda_1.fit(X)
+    perp_1 = lda_1.perplexity(X, sub_sampling=False)
+
+    lda_2.fit(X)
+    perp_2 = lda_2.perplexity(X, sub_sampling=False)
+    assert_greater_equal(perp_1, perp_2)
+
+    perp_1_subsampling = lda_1.perplexity(X, sub_sampling=True)
+    perp_2_subsampling = lda_2.perplexity(X, sub_sampling=True)
+    assert_greater_equal(perp_1_subsampling, perp_2_subsampling)
+
+
+@pytest.mark.parametrize('method', ('online', 'batch'))
+def test_lda_score(method):
     # Test LDA score for batch training
     # score should be higher after each iteration
     n_components, X = _build_sparse_mtx()
-    for method in ('online', 'batch'):
-        lda_1 = LatentDirichletAllocation(n_components=n_components,
-                                          max_iter=1, learning_method=method,
-                                          total_samples=100, random_state=0)
-        lda_2 = LatentDirichletAllocation(n_components=n_components,
-                                          max_iter=10, learning_method=method,
-                                          total_samples=100, random_state=0)
-        lda_1.fit_transform(X)
-        score_1 = lda_1.score(X)
+    lda_1 = LatentDirichletAllocation(n_components=n_components,
+                                      max_iter=1, learning_method=method,
+                                      total_samples=100, random_state=0)
+    lda_2 = LatentDirichletAllocation(n_components=n_components,
+                                      max_iter=10, learning_method=method,
+                                      total_samples=100, random_state=0)
+    lda_1.fit_transform(X)
+    score_1 = lda_1.score(X)
 
-        lda_2.fit_transform(X)
-        score_2 = lda_2.score(X)
-        assert_greater_equal(score_2, score_1)
+    lda_2.fit_transform(X)
+    score_2 = lda_2.score(X)
+    assert_greater_equal(score_2, score_1)
 
 
 def test_perplexity_input_format():
@@ -402,16 +404,17 @@ def check_verbosity(verbose, evaluate_every, expected_lines,
     assert_equal(expected_perplexities, n_perplexity)
 
 
-def test_verbosity():
-    for verbose, evaluate_every, expected_lines, expected_perplexities in [
-        (False, 1, 0, 0),
-        (False, 0, 0, 0),
-        (True, 0, 3, 0),
-        (True, 1, 3, 3),
-        (True, 2, 3, 1),
-    ]:
-        yield (check_verbosity, verbose, evaluate_every, expected_lines,
-               expected_perplexities)
+@pytest.mark.parametrize(
+        'verbose,evaluate_every,expected_lines,expected_perplexities',
+        [(False, 1, 0, 0),
+         (False, 0, 0, 0),
+         (True, 0, 3, 0),
+         (True, 1, 3, 3),
+         (True, 2, 3, 1)])
+def test_verbosity(verbose, evaluate_every, expected_lines,
+                   expected_perplexities):
+    check_verbosity(verbose, evaluate_every, expected_lines,
+                    expected_perplexities)
 
 
 def test_lda_n_topics_deprecation():
diff --git a/sklearn/decomposition/tests/test_pca.py b/sklearn/decomposition/tests/test_pca.py
index 4efb0254fa55..748436712715 100644
--- a/sklearn/decomposition/tests/test_pca.py
+++ b/sklearn/decomposition/tests/test_pca.py
@@ -2,6 +2,8 @@
 import scipy as sp
 from itertools import product
 
+import pytest
+
 from sklearn.utils.testing import assert_almost_equal
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_true
@@ -17,7 +19,6 @@
 
 from sklearn import datasets
 from sklearn.decomposition import PCA
-from sklearn.decomposition import RandomizedPCA
 from sklearn.decomposition.pca import _assess_dimension_
 from sklearn.decomposition.pca import _infer_dimension_
 
@@ -350,67 +351,67 @@ def test_pca_inverse():
         assert_almost_equal(X, Y_inverse, decimal=3)
 
 
-def test_pca_validation():
+@pytest.mark.parametrize('solver', solver_list)
+def test_pca_validation(solver):
     # Ensures that solver-specific extreme inputs for the n_components
     # parameter raise errors
     X = np.array([[0, 1, 0], [1, 0, 0]])
     smallest_d = 2  # The smallest dimension
     lower_limit = {'randomized': 1, 'arpack': 1, 'full': 0, 'auto': 0}
 
-    for solver in solver_list:
-        # We conduct the same test on X.T so that it is invariant to axis.
-        for data in [X, X.T]:
-            for n_components in [-1, 3]:
-
-                if solver == 'auto':
-                    solver_reported = 'full'
-                else:
-                    solver_reported = solver
-
-                assert_raises_regex(ValueError,
-                                    "n_components={}L? must be between "
-                                    r"{}L? and min\(n_samples, n_features\)="
-                                    "{}L? with svd_solver=\'{}\'"
-                                    .format(n_components,
-                                            lower_limit[solver],
-                                            smallest_d,
-                                            solver_reported),
-                                    PCA(n_components,
-                                        svd_solver=solver).fit, data)
-            if solver == 'arpack':
-
-                n_components = smallest_d
-
-                assert_raises_regex(ValueError,
-                                    "n_components={}L? must be "
-                                    "strictly less than "
-                                    r"min\(n_samples, n_features\)={}L?"
-                                    " with svd_solver=\'arpack\'"
-                                    .format(n_components, smallest_d),
-                                    PCA(n_components, svd_solver=solver)
-                                    .fit, data)
-
-        n_components = 1.0
-        type_ncom = type(n_components)
-        assert_raise_message(ValueError,
-                             "n_components={} must be of type int "
-                             "when greater than or equal to 1, was of type={}"
-                             .format(n_components, type_ncom),
-                             PCA(n_components, svd_solver=solver).fit, data)
-
-
-def test_n_components_none():
+    # We conduct the same test on X.T so that it is invariant to axis.
+    for data in [X, X.T]:
+        for n_components in [-1, 3]:
+
+            if solver == 'auto':
+                solver_reported = 'full'
+            else:
+                solver_reported = solver
+
+            assert_raises_regex(ValueError,
+                                "n_components={}L? must be between "
+                                r"{}L? and min\(n_samples, n_features\)="
+                                "{}L? with svd_solver=\'{}\'"
+                                .format(n_components,
+                                        lower_limit[solver],
+                                        smallest_d,
+                                        solver_reported),
+                                PCA(n_components,
+                                    svd_solver=solver).fit, data)
+        if solver == 'arpack':
+
+            n_components = smallest_d
+
+            assert_raises_regex(ValueError,
+                                "n_components={}L? must be "
+                                "strictly less than "
+                                r"min\(n_samples, n_features\)={}L?"
+                                " with svd_solver=\'arpack\'"
+                                .format(n_components, smallest_d),
+                                PCA(n_components, svd_solver=solver)
+                                .fit, data)
+
+    n_components = 1.0
+    type_ncom = type(n_components)
+    assert_raise_message(ValueError,
+                         "n_components={} must be of type int "
+                         "when greater than or equal to 1, was of type={}"
+                         .format(n_components, type_ncom),
+                         PCA(n_components, svd_solver=solver).fit, data)
+
+
+@pytest.mark.parametrize('solver', solver_list)
+def test_n_components_none(solver):
     # Ensures that n_components == None is handled correctly
     X = iris.data
     # We conduct the same test on X.T so that it is invariant to axis.
     for data in [X, X.T]:
-        for solver in solver_list:
-            pca = PCA(svd_solver=solver)
-            pca.fit(data)
-            if solver == 'arpack':
-                assert_equal(pca.n_components_, min(data.shape) - 1)
-            else:
-                assert_equal(pca.n_components_, min(data.shape))
+        pca = PCA(svd_solver=solver)
+        pca.fit(data)
+        if solver == 'arpack':
+            assert_equal(pca.n_components_, min(data.shape) - 1)
+        else:
+            assert_equal(pca.n_components_, min(data.shape))
 
 
 def test_randomized_pca_check_projection():
@@ -684,35 +685,16 @@ def test_svd_solver_auto():
     assert_array_almost_equal(pca.components_, pca_test.components_)
 
 
-def test_deprecation_randomized_pca():
-    rng = np.random.RandomState(0)
-    X = rng.random_sample((5, 4))
-
-    depr_message = ("Class RandomizedPCA is deprecated; RandomizedPCA was "
-                    "deprecated in 0.18 and will be "
-                    "removed in 0.20. Use PCA(svd_solver='randomized') "
-                    "instead. The new implementation DOES NOT store "
-                    "whiten ``components_``. Apply transform to get them.")
-
-    def fit_deprecated(X):
-        global Y
-        rpca = RandomizedPCA(random_state=0)
-        Y = rpca.fit_transform(X)
-
-    assert_warns_message(DeprecationWarning, depr_message, fit_deprecated, X)
-    Y_pca = PCA(svd_solver='randomized', random_state=0).fit_transform(X)
-    assert_array_almost_equal(Y, Y_pca)
-
 
-def test_pca_sparse_input():
+@pytest.mark.parametrize('svd_solver', solver_list)
+def test_pca_sparse_input(svd_solver):
     X = np.random.RandomState(0).rand(5, 4)
     X = sp.sparse.csr_matrix(X)
     assert(sp.sparse.issparse(X))
 
-    for svd_solver in solver_list:
-        pca = PCA(n_components=3, svd_solver=svd_solver)
+    pca = PCA(n_components=3, svd_solver=svd_solver)
 
-        assert_raises(TypeError, pca.fit, X)
+    assert_raises(TypeError, pca.fit, X)
 
 
 def test_pca_bad_solver():
@@ -721,10 +703,10 @@ def test_pca_bad_solver():
     assert_raises(ValueError, pca.fit, X)
 
 
-def test_pca_dtype_preservation():
-    for svd_solver in solver_list:
-        yield check_pca_float_dtype_preservation, svd_solver
-        yield check_pca_int_dtype_upcast_to_double, svd_solver
+@pytest.mark.parametrize('svd_solver', solver_list)
+def test_pca_dtype_preservation(svd_solver):
+    check_pca_float_dtype_preservation(svd_solver)
+    check_pca_int_dtype_upcast_to_double(svd_solver)
 
 
 def check_pca_float_dtype_preservation(svd_solver):
diff --git a/sklearn/decomposition/tests/test_truncated_svd.py b/sklearn/decomposition/tests/test_truncated_svd.py
index 6d853642e1ce..205944883a41 100644
--- a/sklearn/decomposition/tests/test_truncated_svd.py
+++ b/sklearn/decomposition/tests/test_truncated_svd.py
@@ -3,11 +3,13 @@
 import numpy as np
 import scipy.sparse as sp
 
-from sklearn.decomposition import TruncatedSVD
+import pytest
+
+from sklearn.decomposition import TruncatedSVD, PCA
 from sklearn.utils import check_random_state
 from sklearn.utils.testing import (assert_array_almost_equal, assert_equal,
                                    assert_raises, assert_greater,
-                                   assert_array_less)
+                                   assert_array_less, assert_allclose)
 
 
 # Make an X that looks somewhat like a small tf-idf matrix.
@@ -43,31 +45,31 @@ def test_attributes():
         assert_equal(tsvd.components_.shape, (n_components, n_features))
 
 
-def test_too_many_components():
-    for algorithm in ["arpack", "randomized"]:
-        for n_components in (n_features, n_features + 1):
-            tsvd = TruncatedSVD(n_components=n_components, algorithm=algorithm)
-            assert_raises(ValueError, tsvd.fit, X)
+@pytest.mark.parametrize('algorithm', ("arpack", "randomized"))
+def test_too_many_components(algorithm):
+    for n_components in (n_features, n_features + 1):
+        tsvd = TruncatedSVD(n_components=n_components, algorithm=algorithm)
+        assert_raises(ValueError, tsvd.fit, X)
 
 
-def test_sparse_formats():
-    for fmt in ("array", "csr", "csc", "coo", "lil"):
-        Xfmt = Xdense if fmt == "dense" else getattr(X, "to" + fmt)()
-        tsvd = TruncatedSVD(n_components=11)
-        Xtrans = tsvd.fit_transform(Xfmt)
-        assert_equal(Xtrans.shape, (n_samples, 11))
-        Xtrans = tsvd.transform(Xfmt)
-        assert_equal(Xtrans.shape, (n_samples, 11))
+@pytest.mark.parametrize('fmt', ("array", "csr", "csc", "coo", "lil"))
+def test_sparse_formats(fmt):
+    Xfmt = Xdense if fmt == "dense" else getattr(X, "to" + fmt)()
+    tsvd = TruncatedSVD(n_components=11)
+    Xtrans = tsvd.fit_transform(Xfmt)
+    assert_equal(Xtrans.shape, (n_samples, 11))
+    Xtrans = tsvd.transform(Xfmt)
+    assert_equal(Xtrans.shape, (n_samples, 11))
 
 
-def test_inverse_transform():
-    for algo in ("arpack", "randomized"):
-        # We need a lot of components for the reconstruction to be "almost
-        # equal" in all positions. XXX Test means or sums instead?
-        tsvd = TruncatedSVD(n_components=52, random_state=42, algorithm=algo)
-        Xt = tsvd.fit_transform(X)
-        Xinv = tsvd.inverse_transform(Xt)
-        assert_array_almost_equal(Xinv, Xdense, decimal=1)
+@pytest.mark.parametrize('algo', ("arpack", "randomized"))
+def test_inverse_transform(algo):
+    # We need a lot of components for the reconstruction to be "almost
+    # equal" in all positions. XXX Test means or sums instead?
+    tsvd = TruncatedSVD(n_components=52, random_state=42, algorithm=algo)
+    Xt = tsvd.fit_transform(X)
+    Xinv = tsvd.inverse_transform(Xt)
+    assert_array_almost_equal(Xinv, Xdense, decimal=1)
 
 
 def test_integers():
@@ -220,3 +222,21 @@ def test_singular_values():
     rpca.fit(X_hat_rpca)
     assert_array_almost_equal(apca.singular_values_, [3.142, 2.718, 1.0], 14)
     assert_array_almost_equal(rpca.singular_values_, [3.142, 2.718, 1.0], 14)
+
+
+def test_truncated_svd_eq_pca():
+    # TruncatedSVD should be equal to PCA on centered data
+
+    X_c = X - X.mean(axis=0)
+
+    params = dict(n_components=10, random_state=42)
+
+    svd = TruncatedSVD(algorithm='arpack', **params)
+    pca = PCA(svd_solver='arpack', **params)
+
+    Xt_svd = svd.fit_transform(X_c)
+    Xt_pca = pca.fit_transform(X_c)
+
+    assert_allclose(Xt_svd, Xt_pca, rtol=1e-9)
+    assert_allclose(pca.mean_, 0, atol=1e-9)
+    assert_allclose(svd.components_, pca.components_)
diff --git a/sklearn/decomposition/truncated_svd.py b/sklearn/decomposition/truncated_svd.py
index 049c165baea2..74e8ffa44408 100644
--- a/sklearn/decomposition/truncated_svd.py
+++ b/sklearn/decomposition/truncated_svd.py
@@ -100,7 +100,6 @@ class TruncatedSVD(BaseEstimator, TransformerMixin):
     See also
     --------
     PCA
-    RandomizedPCA
 
     References
     ----------
diff --git a/sklearn/ensemble/bagging.py b/sklearn/ensemble/bagging.py
index b8952484023f..30324d6e0c87 100644
--- a/sklearn/ensemble/bagging.py
+++ b/sklearn/ensemble/bagging.py
@@ -8,9 +8,10 @@
 import itertools
 import numbers
 import numpy as np
-from warnings import warn
 from abc import ABCMeta, abstractmethod
+from warnings import warn
 
+from .base import BaseEnsemble, _partition_estimators
 from ..base import ClassifierMixin, RegressorMixin
 from ..externals.joblib import Parallel, delayed
 from ..externals.six import with_metaclass
@@ -18,13 +19,11 @@
 from ..metrics import r2_score, accuracy_score
 from ..tree import DecisionTreeClassifier, DecisionTreeRegressor
 from ..utils import check_random_state, check_X_y, check_array, column_or_1d
-from ..utils.random import sample_without_replacement
-from ..utils.validation import has_fit_parameter, check_is_fitted
 from ..utils import indices_to_mask, check_consistent_length
 from ..utils.metaestimators import if_delegate_has_method
 from ..utils.multiclass import check_classification_targets
-
-from .base import BaseEnsemble, _partition_estimators
+from ..utils.random import sample_without_replacement
+from ..utils.validation import has_fit_parameter, check_is_fitted
 
 
 __all__ = ["BaggingClassifier",
@@ -277,8 +276,11 @@ def _fit(self, X, y, max_samples=None, max_depth=None, sample_weight=None):
         """
         random_state = check_random_state(self.random_state)
 
-        # Convert data
-        X, y = check_X_y(X, y, ['csr', 'csc'])
+        # Convert data (X is required to be 2d and indexable)
+        X, y = check_X_y(
+            X, y, ['csr', 'csc'], dtype=None, force_all_finite=False,
+            multi_output=True
+        )
         if sample_weight is not None:
             sample_weight = check_array(sample_weight, ensure_2d=False)
             check_consistent_length(y, sample_weight)
@@ -388,8 +390,10 @@ def _set_oob_score(self, X, y):
         """Calculate out of bag predictions and score."""
 
     def _validate_y(self, y):
-        # Default implementation
-        return column_or_1d(y, warn=True)
+        if len(y.shape) == 1 or y.shape[1] == 1:
+            return column_or_1d(y, warn=True)
+        else:
+            return y
 
     def _get_estimators_indices(self):
         # Get drawn indices along both sample and feature axes
@@ -496,7 +500,7 @@ class BaggingClassifier(BaseBagging, ClassifierMixin):
         by `np.random`.
 
     verbose : int, optional (default=0)
-        Controls the verbosity of the building process.
+        Controls the verbosity when fitting and predicting.
 
     Attributes
     ----------
@@ -667,7 +671,10 @@ def predict_proba(self, X):
         """
         check_is_fitted(self, "classes_")
         # Check data
-        X = check_array(X, accept_sparse=['csr', 'csc'])
+        X = check_array(
+            X, accept_sparse=['csr', 'csc'], dtype=None,
+            force_all_finite=False
+        )
 
         if self.n_features_ != X.shape[1]:
             raise ValueError("Number of features of the model must "
@@ -714,7 +721,10 @@ def predict_log_proba(self, X):
         check_is_fitted(self, "classes_")
         if hasattr(self.base_estimator_, "predict_log_proba"):
             # Check data
-            X = check_array(X, accept_sparse=['csr', 'csc'])
+            X = check_array(
+                X, accept_sparse=['csr', 'csc'], dtype=None,
+                force_all_finite=False
+            )
 
             if self.n_features_ != X.shape[1]:
                 raise ValueError("Number of features of the model must "
@@ -769,7 +779,10 @@ def decision_function(self, X):
         check_is_fitted(self, "classes_")
 
         # Check data
-        X = check_array(X, accept_sparse=['csr', 'csc'])
+        X = check_array(
+            X, accept_sparse=['csr', 'csc'], dtype=None,
+            force_all_finite=False
+        )
 
         if self.n_features_ != X.shape[1]:
             raise ValueError("Number of features of the model must "
@@ -863,7 +876,7 @@ class BaggingRegressor(BaseBagging, RegressorMixin):
         by `np.random`.
 
     verbose : int, optional (default=0)
-        Controls the verbosity of the building process.
+        Controls the verbosity when fitting and predicting.
 
     Attributes
     ----------
@@ -947,7 +960,10 @@ def predict(self, X):
         """
         check_is_fitted(self, "estimators_features_")
         # Check data
-        X = check_array(X, accept_sparse=['csr', 'csc'])
+        X = check_array(
+            X, accept_sparse=['csr', 'csc'], dtype=None,
+            force_all_finite=False
+        )
 
         # Parallel loop
         n_jobs, n_estimators, starts = _partition_estimators(self.n_estimators,
diff --git a/sklearn/ensemble/forest.py b/sklearn/ensemble/forest.py
index 05ba33faab37..b7a349d4b5a8 100644
--- a/sklearn/ensemble/forest.py
+++ b/sklearn/ensemble/forest.py
@@ -861,15 +861,15 @@ class RandomForestClassifier(ForestClassifier):
         by `np.random`.
 
     verbose : int, optional (default=0)
-        Controls the verbosity of the tree building process.
+        Controls the verbosity when fitting and predicting.
 
     warm_start : bool, optional (default=False)
         When set to ``True``, reuse the solution of the previous call to fit
         and add more estimators to the ensemble, otherwise, just fit a whole
         new forest. See :term:`the Glossary <warm_start>`.
 
-    class_weight : dict, list of dicts, "balanced",
-        "balanced_subsample" or None, optional (default=None)
+    class_weight : dict, list of dicts, "balanced", "balanced_subsample" or \
+    None, optional (default=None)
         Weights associated with classes in the form ``{class_label: weight}``.
         If not given, all classes are supposed to have weight one. For
         multi-output problems, a list of dicts can be provided in the same
@@ -1139,7 +1139,7 @@ class RandomForestRegressor(ForestRegressor):
         by `np.random`.
 
     verbose : int, optional (default=0)
-        Controls the verbosity of the tree building process.
+        Controls the verbosity when fitting and predicting.
 
     warm_start : bool, optional (default=False)
         When set to ``True``, reuse the solution of the previous call to fit
@@ -1370,14 +1370,15 @@ class ExtraTreesClassifier(ForestClassifier):
         by `np.random`.
 
     verbose : int, optional (default=0)
-        Controls the verbosity of the tree building process.
+        Controls the verbosity when fitting and predicting.
 
     warm_start : bool, optional (default=False)
         When set to ``True``, reuse the solution of the previous call to fit
         and add more estimators to the ensemble, otherwise, just fit a whole
         new forest. See :term:`the Glossary <warm_start>`.
 
-    class_weight : dict, list of dicts, "balanced", "balanced_subsample" or None, optional (default=None)
+    class_weight : dict, list of dicts, "balanced", "balanced_subsample" or \
+    None, optional (default=None)
         Weights associated with classes in the form ``{class_label: weight}``.
         If not given, all classes are supposed to have weight one. For
         multi-output problems, a list of dicts can be provided in the same
@@ -1618,7 +1619,7 @@ class ExtraTreesRegressor(ForestRegressor):
         by `np.random`.
 
     verbose : int, optional (default=0)
-        Controls the verbosity of the tree building process.
+        Controls the verbosity when fitting and predicting.
 
     warm_start : bool, optional (default=False)
         When set to ``True``, reuse the solution of the previous call to fit
@@ -1809,7 +1810,7 @@ class RandomTreesEmbedding(BaseForest):
         by `np.random`.
 
     verbose : int, optional (default=0)
-        Controls the verbosity of the tree building process.
+        Controls the verbosity when fitting and predicting.
 
     warm_start : bool, optional (default=False)
         When set to ``True``, reuse the solution of the previous call to fit
@@ -1931,7 +1932,8 @@ def fit_transform(self, X, y=None, sample_weight=None):
         super(RandomTreesEmbedding, self).fit(X, y,
                                               sample_weight=sample_weight)
 
-        self.one_hot_encoder_ = OneHotEncoder(sparse=self.sparse_output)
+        self.one_hot_encoder_ = OneHotEncoder(sparse=self.sparse_output,
+                                              categories='auto')
         return self.one_hot_encoder_.fit_transform(self.apply(X))
 
     def transform(self, X):
diff --git a/sklearn/ensemble/gradient_boosting.py b/sklearn/ensemble/gradient_boosting.py
index d4fedd763ada..4edf4dd1fa68 100644
--- a/sklearn/ensemble/gradient_boosting.py
+++ b/sklearn/ensemble/gradient_boosting.py
@@ -1229,11 +1229,12 @@ def feature_importances_(self):
 
         total_sum = np.zeros((self.n_features_, ), dtype=np.float64)
         for stage in self.estimators_:
-            stage_sum = sum(tree.feature_importances_
-                            for tree in stage) / len(stage)
+            stage_sum = sum(tree.tree_.compute_feature_importances(
+                normalize=False) for tree in stage) / len(stage)
             total_sum += stage_sum
 
         importances = total_sum / len(self.estimators_)
+        importances /= importances.sum()
         return importances
 
     def _validate_y(self, y, sample_weight):
diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py
index dc89d18af6da..a1eb7ccd286b 100644
--- a/sklearn/ensemble/iforest.py
+++ b/sklearn/ensemble/iforest.py
@@ -109,7 +109,7 @@ class IsolationForest(BaseBagging, OutlierMixin):
 
     offset_ : float
         Offset used to define the decision function from the raw scores.
-        We have the relation: decision_function = score_samples - offset_.
+        We have the relation: ``decision_function = score_samples - offset_``.
         When the contamination parameter is set to "auto", the offset is equal
         to -0.5 as the scores of inliers are close to 0 and the scores of
         outliers are close to -1. When a contamination parameter different
diff --git a/sklearn/ensemble/tests/test_bagging.py b/sklearn/ensemble/tests/test_bagging.py
index 014e613ccd26..626b34f58e5a 100644
--- a/sklearn/ensemble/tests/test_bagging.py
+++ b/sklearn/ensemble/tests/test_bagging.py
@@ -33,6 +33,7 @@
 from sklearn.model_selection import train_test_split
 from sklearn.datasets import load_boston, load_iris, make_hastie_10_2
 from sklearn.utils import check_random_state
+from sklearn.preprocessing import Imputer
 
 from scipy.sparse import csc_matrix, csr_matrix
 
@@ -752,3 +753,76 @@ def test_set_oob_score_label_encoding():
     x3 = BaggingClassifier(oob_score=True,
                            random_state=random_state).fit(X, Y3).oob_score_
     assert_equal([x1, x2], [x3, x3])
+
+
+def test_bagging_regressor_with_missing_inputs():
+    # Check that BaggingRegressor can accept X with missing/infinite data
+    X = np.array([
+        [1, 3, 5],
+        [2, None, 6],
+        [2, np.nan, 6],
+        [2, np.inf, 6],
+        [2, np.NINF, 6],
+    ])
+    y_values = [
+        np.array([2, 3, 3, 3, 3]),
+        np.array([
+            [2, 1, 9],
+            [3, 6, 8],
+            [3, 6, 8],
+            [3, 6, 8],
+            [3, 6, 8],
+        ])
+    ]
+    for y in y_values:
+        regressor = DecisionTreeRegressor()
+        pipeline = make_pipeline(
+            Imputer(),
+            Imputer(missing_values=np.inf),
+            Imputer(missing_values=np.NINF),
+            regressor
+        )
+        pipeline.fit(X, y).predict(X)
+        bagging_regressor = BaggingRegressor(pipeline)
+        y_hat = bagging_regressor.fit(X, y).predict(X)
+        assert_equal(y.shape, y_hat.shape)
+
+        # Verify that exceptions can be raised by wrapper regressor
+        regressor = DecisionTreeRegressor()
+        pipeline = make_pipeline(regressor)
+        assert_raises(ValueError, pipeline.fit, X, y)
+        bagging_regressor = BaggingRegressor(pipeline)
+        assert_raises(ValueError, bagging_regressor.fit, X, y)
+
+
+def test_bagging_classifier_with_missing_inputs():
+    # Check that BaggingClassifier can accept X with missing/infinite data
+    X = np.array([
+        [1, 3, 5],
+        [2, None, 6],
+        [2, np.nan, 6],
+        [2, np.inf, 6],
+        [2, np.NINF, 6],
+    ])
+    y = np.array([3, 6, 6, 6, 6])
+    classifier = DecisionTreeClassifier()
+    pipeline = make_pipeline(
+        Imputer(),
+        Imputer(missing_values=np.inf),
+        Imputer(missing_values=np.NINF),
+        classifier
+    )
+    pipeline.fit(X, y).predict(X)
+    bagging_classifier = BaggingClassifier(pipeline)
+    bagging_classifier.fit(X, y)
+    y_hat = bagging_classifier.predict(X)
+    assert_equal(y.shape, y_hat.shape)
+    bagging_classifier.predict_log_proba(X)
+    bagging_classifier.predict_proba(X)
+
+    # Verify that exceptions can be raised by wrapper classifier
+    classifier = DecisionTreeClassifier()
+    pipeline = make_pipeline(classifier)
+    assert_raises(ValueError, pipeline.fit, X, y)
+    bagging_classifier = BaggingClassifier(pipeline)
+    assert_raises(ValueError, bagging_classifier.fit, X, y)
diff --git a/sklearn/ensemble/tests/test_gradient_boosting.py b/sklearn/ensemble/tests/test_gradient_boosting.py
index c18008d0b31c..6f7654c7d606 100644
--- a/sklearn/ensemble/tests/test_gradient_boosting.py
+++ b/sklearn/ensemble/tests/test_gradient_boosting.py
@@ -12,7 +12,7 @@
 
 from sklearn import datasets
 from sklearn.base import clone
-from sklearn.datasets import make_classification
+from sklearn.datasets import make_classification, fetch_california_housing
 from sklearn.ensemble import GradientBoostingClassifier
 from sklearn.ensemble import GradientBoostingRegressor
 from sklearn.ensemble.gradient_boosting import ZeroEstimator
@@ -452,6 +452,34 @@ def test_max_feature_regression():
     assert_true(deviance < 0.5, "GB failed with deviance %.4f" % deviance)
 
 
+def test_feature_importance_regression():
+    """Test that Gini importance is calculated correctly.
+
+    This test follows the example from [1]_ (pg. 373).
+
+    .. [1] Friedman, J., Hastie, T., & Tibshirani, R. (2001). The elements
+       of statistical learning. New York: Springer series in statistics.
+    """
+    california = fetch_california_housing()
+    X, y = california.data, california.target
+    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
+
+    reg = GradientBoostingRegressor(loss='huber', learning_rate=0.1,
+                                    max_leaf_nodes=6, n_estimators=100,
+                                    random_state=0)
+    reg.fit(X_train, y_train)
+    sorted_idx = np.argsort(reg.feature_importances_)[::-1]
+    sorted_features = [california.feature_names[s] for s in sorted_idx]
+
+    # The most important feature is the median income by far.
+    assert sorted_features[0] == 'MedInc'
+
+    # The three subsequent features are the following. Their relative ordering
+    # might change a bit depending on the randomness of the trees and the
+    # train / test split.
+    assert set(sorted_features[1:4]) == {'Longitude', 'AveOccup', 'Latitude'}
+
+
 def test_max_feature_auto():
     # Test if max features is set properly for floats and str.
     X, y = datasets.make_hastie_10_2(n_samples=12000, random_state=1)
diff --git a/sklearn/ensemble/tests/test_iforest.py b/sklearn/ensemble/tests/test_iforest.py
index c88c0ce4ab2e..3833227ecfc2 100644
--- a/sklearn/ensemble/tests/test_iforest.py
+++ b/sklearn/ensemble/tests/test_iforest.py
@@ -6,6 +6,8 @@
 #          Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>
 # License: BSD 3 clause
 
+import pytest
+
 import numpy as np
 
 from sklearn.utils.fixes import euler_gamma
@@ -15,7 +17,6 @@
 from sklearn.utils.testing import assert_raises
 from sklearn.utils.testing import assert_warns_message
 from sklearn.utils.testing import assert_equal
-from sklearn.utils.testing import assert_no_warnings
 from sklearn.utils.testing import assert_greater
 from sklearn.utils.testing import ignore_warnings
 
@@ -105,8 +106,20 @@ def test_iforest_error():
     assert_warns_message(UserWarning,
                          "max_samples will be set to n_samples for estimation",
                          IsolationForest(max_samples=1000).fit, X)
-    assert_no_warnings(IsolationForest(max_samples='auto').fit, X)
-    assert_no_warnings(IsolationForest(max_samples=np.int64(2)).fit, X)
+    # note that assert_no_warnings does not apply since it enables a
+    # PendingDeprecationWarning triggered by scipy.sparse's use of
+    # np.matrix. See issue #11251.
+    with pytest.warns(None) as record:
+        IsolationForest(max_samples='auto').fit(X)
+    user_warnings = [each for each in record
+                     if issubclass(each.category, UserWarning)]
+    assert len(user_warnings) == 0
+    with pytest.warns(None) as record:
+        IsolationForest(max_samples=np.int64(2)).fit(X)
+    user_warnings = [each for each in record
+                     if issubclass(each.category, UserWarning)]
+    assert len(user_warnings) == 0
+
     assert_raises(ValueError, IsolationForest(max_samples='foobar').fit, X)
     assert_raises(ValueError, IsolationForest(max_samples=1.5).fit, X)
 
diff --git a/sklearn/ensemble/voting_classifier.py b/sklearn/ensemble/voting_classifier.py
index 81db37de290d..2b0d63d2140b 100644
--- a/sklearn/ensemble/voting_classifier.py
+++ b/sklearn/ensemble/voting_classifier.py
@@ -282,13 +282,16 @@ def transform(self, X):
 
         Returns
         -------
-        If `voting='soft'` and `flatten_transform=True`:
-          array-like = (n_classifiers, n_samples * n_classes)
-          otherwise array-like = (n_classifiers, n_samples, n_classes)
-            Class probabilities calculated by each classifier.
-        If `voting='hard'`:
-          array-like = [n_samples, n_classifiers]
-            Class labels predicted by each classifier.
+        probabilities_or_labels
+            If `voting='soft'` and `flatten_transform=True`:
+                returns array-like of shape (n_classifiers, n_samples *
+                n_classes), being class probabilities calculated by each
+                classifier.
+            If `voting='soft' and `flatten_transform=False`:
+                array-like of shape (n_classifiers, n_samples, n_classes)
+            If `voting='hard'`:
+                array-like of shape (n_samples, n_classifiers), being
+                class labels predicted by each classifier.
         """
         check_is_fitted(self, 'estimators_')
 
diff --git a/sklearn/exceptions.py b/sklearn/exceptions.py
index e58f42760663..9cf207e40fdd 100644
--- a/sklearn/exceptions.py
+++ b/sklearn/exceptions.py
@@ -30,7 +30,7 @@ class NotFittedError(ValueError, AttributeError):
     ... except NotFittedError as e:
     ...     print(repr(e))
     ...                        # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
-    NotFittedError('This LinearSVC instance is not fitted yet',)
+    NotFittedError('This LinearSVC instance is not fitted yet'...)
 
     .. versionchanged:: 0.18
        Moved from sklearn.utils.validation.
@@ -121,7 +121,7 @@ class FitFailedWarning(RuntimeWarning):
     ... # doctest: +NORMALIZE_WHITESPACE
     FitFailedWarning('Estimator fit failed. The score on this train-test
     partition for these parameters will be set to 0.000000.
-    Details: \\nValueError: Penalty term must be positive; got (C=-2)\\n',)
+    Details: \\nValueError: Penalty term must be positive; got (C=-2)\\n'...)
 
     .. versionchanged:: 0.18
        Moved from sklearn.cross_validation.
diff --git a/sklearn/feature_extraction/dict_vectorizer.py b/sklearn/feature_extraction/dict_vectorizer.py
index 5b8f932e889c..3ab717d1cf29 100644
--- a/sklearn/feature_extraction/dict_vectorizer.py
+++ b/sklearn/feature_extraction/dict_vectorizer.py
@@ -39,7 +39,7 @@ class DictVectorizer(BaseEstimator, TransformerMixin):
     However, note that this transformer will only do a binary one-hot encoding
     when feature values are of type string. If categorical features are
     represented as numeric values such as int, the DictVectorizer can be
-    followed by :class:`sklearn.preprocessing.CategoricalEncoder` to complete
+    followed by :class:`sklearn.preprocessing.OneHotEncoder` to complete
     binary one-hot encoding.
 
     Features that do not occur in a sample (mapping) will have a zero value
@@ -89,7 +89,7 @@ class DictVectorizer(BaseEstimator, TransformerMixin):
     See also
     --------
     FeatureHasher : performs vectorization using only a hash function.
-    sklearn.preprocessing.CategoricalEncoder : handles nominal/categorical
+    sklearn.preprocessing.OrdinalEncoder : handles nominal/categorical
       features encoded as columns of arbitrary data types.
     """
 
diff --git a/sklearn/feature_extraction/hashing.py b/sklearn/feature_extraction/hashing.py
index d586e6302e54..9795d30aa675 100644
--- a/sklearn/feature_extraction/hashing.py
+++ b/sklearn/feature_extraction/hashing.py
@@ -81,8 +81,7 @@ class FeatureHasher(BaseEstimator, TransformerMixin):
     See also
     --------
     DictVectorizer : vectorizes string-valued features using a hash table.
-    sklearn.preprocessing.OneHotEncoder : handles nominal/categorical features
-        encoded as columns of integers.
+    sklearn.preprocessing.OneHotEncoder : handles nominal/categorical features.
     """
 
     def __init__(self, n_features=(2 ** 20), input_type="dict",
diff --git a/sklearn/feature_extraction/tests/test_dict_vectorizer.py b/sklearn/feature_extraction/tests/test_dict_vectorizer.py
index 72ebbb1653c4..66d678421e90 100644
--- a/sklearn/feature_extraction/tests/test_dict_vectorizer.py
+++ b/sklearn/feature_extraction/tests/test_dict_vectorizer.py
@@ -5,8 +5,10 @@
 from random import Random
 import numpy as np
 import scipy.sparse as sp
-
 from numpy.testing import assert_array_equal
+
+import pytest
+
 from sklearn.utils.testing import (assert_equal, assert_in,
                                    assert_false, assert_true)
 
@@ -14,34 +16,34 @@
 from sklearn.feature_selection import SelectKBest, chi2
 
 
-def test_dictvectorizer():
+@pytest.mark.parametrize('sparse', (True, False))
+@pytest.mark.parametrize('dtype', (int, np.float32, np.int16))
+@pytest.mark.parametrize('sort', (True, False))
+@pytest.mark.parametrize('iterable', (True, False))
+def test_dictvectorizer(sparse, dtype, sort, iterable):
     D = [{"foo": 1, "bar": 3},
          {"bar": 4, "baz": 2},
          {"bar": 1, "quux": 1, "quuux": 2}]
 
-    for sparse in (True, False):
-        for dtype in (int, np.float32, np.int16):
-            for sort in (True, False):
-                for iterable in (True, False):
-                    v = DictVectorizer(sparse=sparse, dtype=dtype, sort=sort)
-                    X = v.fit_transform(iter(D) if iterable else D)
-
-                    assert_equal(sp.issparse(X), sparse)
-                    assert_equal(X.shape, (3, 5))
-                    assert_equal(X.sum(), 14)
-                    assert_equal(v.inverse_transform(X), D)
-
-                    if sparse:
-                        # CSR matrices can't be compared for equality
-                        assert_array_equal(X.A, v.transform(iter(D) if iterable
-                                                            else D).A)
-                    else:
-                        assert_array_equal(X, v.transform(iter(D) if iterable
-                                                          else D))
-
-                    if sort:
-                        assert_equal(v.feature_names_,
-                                     sorted(v.feature_names_))
+    v = DictVectorizer(sparse=sparse, dtype=dtype, sort=sort)
+    X = v.fit_transform(iter(D) if iterable else D)
+
+    assert_equal(sp.issparse(X), sparse)
+    assert_equal(X.shape, (3, 5))
+    assert_equal(X.sum(), 14)
+    assert_equal(v.inverse_transform(X), D)
+
+    if sparse:
+        # CSR matrices can't be compared for equality
+        assert_array_equal(X.A, v.transform(iter(D) if iterable
+                                            else D).A)
+    else:
+        assert_array_equal(X, v.transform(iter(D) if iterable
+                                          else D))
+
+    if sort:
+        assert_equal(v.feature_names_,
+                     sorted(v.feature_names_))
 
 
 def test_feature_selection():
diff --git a/sklearn/feature_extraction/tests/test_text.py b/sklearn/feature_extraction/tests/test_text.py
index ff4dd6bd86a7..9d39f01945b1 100644
--- a/sklearn/feature_extraction/tests/test_text.py
+++ b/sklearn/feature_extraction/tests/test_text.py
@@ -1,6 +1,9 @@
 from __future__ import unicode_literals
 import warnings
 
+import pytest
+from scipy import sparse
+
 from sklearn.feature_extraction.text import strip_tags
 from sklearn.feature_extraction.text import strip_accents_unicode
 from sklearn.feature_extraction.text import strip_accents_ascii
@@ -28,15 +31,14 @@
                                    assert_in, assert_less, assert_greater,
                                    assert_warns_message, assert_raise_message,
                                    clean_warning_registry, ignore_warnings,
-                                   SkipTest, assert_raises)
+                                   SkipTest, assert_raises,
+                                   assert_allclose_dense_sparse)
 
 from collections import defaultdict, Mapping
 from functools import partial
 import pickle
 from io import StringIO
 
-import pytest
-
 JUNK_FOOD_DOCS = (
     "the pizza pizza beer copyright",
     "the pizza burger beer copyright",
@@ -115,42 +117,42 @@ def test_to_ascii():
     assert_equal(strip_accents_ascii(a), expected)
 
 
-def test_word_analyzer_unigrams():
-    for Vectorizer in (CountVectorizer, HashingVectorizer):
-        wa = Vectorizer(strip_accents='ascii').build_analyzer()
-        text = ("J'ai mang\xe9 du kangourou  ce midi, "
-                "c'\xe9tait pas tr\xeas bon.")
-        expected = ['ai', 'mange', 'du', 'kangourou', 'ce', 'midi',
-                    'etait', 'pas', 'tres', 'bon']
-        assert_equal(wa(text), expected)
-
-        text = "This is a test, really.\n\n I met Harry yesterday."
-        expected = ['this', 'is', 'test', 'really', 'met', 'harry',
-                    'yesterday']
-        assert_equal(wa(text), expected)
-
-        wa = Vectorizer(input='file').build_analyzer()
-        text = StringIO("This is a test with a file-like object!")
-        expected = ['this', 'is', 'test', 'with', 'file', 'like',
-                    'object']
-        assert_equal(wa(text), expected)
-
-        # with custom preprocessor
-        wa = Vectorizer(preprocessor=uppercase).build_analyzer()
-        text = ("J'ai mang\xe9 du kangourou  ce midi, "
-                " c'\xe9tait pas tr\xeas bon.")
-        expected = ['AI', 'MANGE', 'DU', 'KANGOUROU', 'CE', 'MIDI',
-                    'ETAIT', 'PAS', 'TRES', 'BON']
-        assert_equal(wa(text), expected)
-
-        # with custom tokenizer
-        wa = Vectorizer(tokenizer=split_tokenize,
-                        strip_accents='ascii').build_analyzer()
-        text = ("J'ai mang\xe9 du kangourou  ce midi, "
-                "c'\xe9tait pas tr\xeas bon.")
-        expected = ["j'ai", 'mange', 'du', 'kangourou', 'ce', 'midi,',
-                    "c'etait", 'pas', 'tres', 'bon.']
-        assert_equal(wa(text), expected)
+@pytest.mark.parametrize('Vectorizer', (CountVectorizer, HashingVectorizer))
+def test_word_analyzer_unigrams(Vectorizer):
+    wa = Vectorizer(strip_accents='ascii').build_analyzer()
+    text = ("J'ai mang\xe9 du kangourou  ce midi, "
+            "c'\xe9tait pas tr\xeas bon.")
+    expected = ['ai', 'mange', 'du', 'kangourou', 'ce', 'midi',
+                'etait', 'pas', 'tres', 'bon']
+    assert_equal(wa(text), expected)
+
+    text = "This is a test, really.\n\n I met Harry yesterday."
+    expected = ['this', 'is', 'test', 'really', 'met', 'harry',
+                'yesterday']
+    assert_equal(wa(text), expected)
+
+    wa = Vectorizer(input='file').build_analyzer()
+    text = StringIO("This is a test with a file-like object!")
+    expected = ['this', 'is', 'test', 'with', 'file', 'like',
+                'object']
+    assert_equal(wa(text), expected)
+
+    # with custom preprocessor
+    wa = Vectorizer(preprocessor=uppercase).build_analyzer()
+    text = ("J'ai mang\xe9 du kangourou  ce midi, "
+            " c'\xe9tait pas tr\xeas bon.")
+    expected = ['AI', 'MANGE', 'DU', 'KANGOUROU', 'CE', 'MIDI',
+                'ETAIT', 'PAS', 'TRES', 'BON']
+    assert_equal(wa(text), expected)
+
+    # with custom tokenizer
+    wa = Vectorizer(tokenizer=split_tokenize,
+                    strip_accents='ascii').build_analyzer()
+    text = ("J'ai mang\xe9 du kangourou  ce midi, "
+            "c'\xe9tait pas tr\xeas bon.")
+    expected = ["j'ai", 'mange', 'du', 'kangourou', 'ce', 'midi,',
+                "c'etait", 'pas', 'tres', 'bon.']
+    assert_equal(wa(text), expected)
 
 
 def test_word_analyzer_unigrams_and_bigrams():
@@ -574,22 +576,17 @@ def test_feature_names():
         assert_equal(idx, cv.vocabulary_.get(name))
 
 
-def test_vectorizer_max_features():
-    vec_factories = (
-        CountVectorizer,
-        TfidfVectorizer,
-    )
-
+@pytest.mark.parametrize('Vectorizer', (CountVectorizer, TfidfVectorizer))
+def test_vectorizer_max_features(Vectorizer):
     expected_vocabulary = set(['burger', 'beer', 'salad', 'pizza'])
     expected_stop_words = set([u'celeri', u'tomato', u'copyright', u'coke',
                                u'sparkling', u'water', u'the'])
 
-    for vec_factory in vec_factories:
-        # test bounded number of extracted features
-        vectorizer = vec_factory(max_df=0.6, max_features=4)
-        vectorizer.fit(ALL_FOOD_DOCS)
-        assert_equal(set(vectorizer.vocabulary_), expected_vocabulary)
-        assert_equal(vectorizer.stop_words_, expected_stop_words)
+    # test bounded number of extracted features
+    vectorizer = Vectorizer(max_df=0.6, max_features=4)
+    vectorizer.fit(ALL_FOOD_DOCS)
+    assert_equal(set(vectorizer.vocabulary_), expected_vocabulary)
+    assert_equal(vectorizer.stop_words_, expected_stop_words)
 
 
 def test_count_vectorizer_max_features():
@@ -713,23 +710,24 @@ def test_hashed_binary_occurrences():
     assert_equal(X.dtype, np.float64)
 
 
-def test_vectorizer_inverse_transform():
+@pytest.mark.parametrize('Vectorizer', (CountVectorizer, TfidfVectorizer))
+def test_vectorizer_inverse_transform(Vectorizer):
     # raw documents
     data = ALL_FOOD_DOCS
-    for vectorizer in (TfidfVectorizer(), CountVectorizer()):
-        transformed_data = vectorizer.fit_transform(data)
-        inversed_data = vectorizer.inverse_transform(transformed_data)
-        analyze = vectorizer.build_analyzer()
-        for doc, inversed_terms in zip(data, inversed_data):
-            terms = np.sort(np.unique(analyze(doc)))
-            inversed_terms = np.sort(np.unique(inversed_terms))
-            assert_array_equal(terms, inversed_terms)
-
-        # Test that inverse_transform also works with numpy arrays
-        transformed_data = transformed_data.toarray()
-        inversed_data2 = vectorizer.inverse_transform(transformed_data)
-        for terms, terms2 in zip(inversed_data, inversed_data2):
-            assert_array_equal(np.sort(terms), np.sort(terms2))
+    vectorizer = Vectorizer()
+    transformed_data = vectorizer.fit_transform(data)
+    inversed_data = vectorizer.inverse_transform(transformed_data)
+    analyze = vectorizer.build_analyzer()
+    for doc, inversed_terms in zip(data, inversed_data):
+        terms = np.sort(np.unique(analyze(doc)))
+        inversed_terms = np.sort(np.unique(inversed_terms))
+        assert_array_equal(terms, inversed_terms)
+
+    # Test that inverse_transform also works with numpy arrays
+    transformed_data = transformed_data.toarray()
+    inversed_data2 = vectorizer.inverse_transform(transformed_data)
+    for terms, terms2 in zip(inversed_data, inversed_data2):
+        assert_array_equal(np.sort(terms), np.sort(terms2))
 
 
 def test_count_vectorizer_pipeline_grid_selection():
@@ -1030,16 +1028,52 @@ def test_vectorizer_vocab_clone():
     assert_equal(vect_vocab_clone.vocabulary_, vect_vocab.vocabulary_)
 
 
-def test_vectorizer_string_object_as_input():
+@pytest.mark.parametrize('Vectorizer',
+                         (CountVectorizer, TfidfVectorizer, HashingVectorizer))
+def test_vectorizer_string_object_as_input(Vectorizer):
     message = ("Iterable over raw text documents expected, "
                "string object received.")
-    for vec in [CountVectorizer(), TfidfVectorizer(), HashingVectorizer()]:
-        assert_raise_message(
+    vec = Vectorizer()
+    assert_raise_message(
             ValueError, message, vec.fit_transform, "hello world!")
-        assert_raise_message(
-            ValueError, message, vec.fit, "hello world!")
-        assert_raise_message(
-            ValueError, message, vec.transform, "hello world!")
+    assert_raise_message(ValueError, message, vec.fit, "hello world!")
+    assert_raise_message(ValueError, message, vec.transform, "hello world!")
+
+
+@pytest.mark.parametrize("X_dtype", [np.float32, np.float64])
+def test_tfidf_transformer_type(X_dtype):
+    X = sparse.rand(10, 20000, dtype=X_dtype, random_state=42)
+    X_trans = TfidfTransformer().fit_transform(X)
+    assert X_trans.dtype == X.dtype
+
+
+def test_tfidf_transformer_sparse():
+    X = sparse.rand(10, 20000, dtype=np.float64, random_state=42)
+    X_csc = sparse.csc_matrix(X)
+    X_csr = sparse.csr_matrix(X)
+
+    X_trans_csc = TfidfTransformer().fit_transform(X_csc)
+    X_trans_csr = TfidfTransformer().fit_transform(X_csr)
+    assert_allclose_dense_sparse(X_trans_csc, X_trans_csr)
+    assert X_trans_csc.format == X_trans_csr.format
+
+
+@pytest.mark.parametrize(
+    "vectorizer_dtype, output_dtype, expected_warning, msg_warning",
+    [(np.int32, np.float64, UserWarning, "'dtype' should be used."),
+     (np.int64, np.float64, UserWarning, "'dtype' should be used."),
+     (np.float32, np.float32, None, None),
+     (np.float64, np.float64, None, None)]
+)
+def test_tfidf_vectorizer_type(vectorizer_dtype, output_dtype,
+                               expected_warning, msg_warning):
+    X = np.array(["numpy", "scipy", "sklearn"])
+    vectorizer = TfidfVectorizer(dtype=vectorizer_dtype)
+    with pytest.warns(expected_warning, match=msg_warning) as record:
+            X_idf = vectorizer.fit_transform(X)
+    if expected_warning is None:
+        assert len(record) == 0
+    assert X_idf.dtype == output_dtype
 
 
 @pytest.mark.parametrize("vec", [
diff --git a/sklearn/feature_extraction/text.py b/sklearn/feature_extraction/text.py
index df0582d3d4f5..e96693599da7 100644
--- a/sklearn/feature_extraction/text.py
+++ b/sklearn/feature_extraction/text.py
@@ -11,7 +11,7 @@
 The :mod:`sklearn.feature_extraction.text` submodule gathers utilities to
 build feature vectors from text documents.
 """
-from __future__ import unicode_literals
+from __future__ import unicode_literals, division
 
 import array
 from collections import Mapping, defaultdict
@@ -19,6 +19,7 @@
 from operator import itemgetter
 import re
 import unicodedata
+import warnings
 
 import numpy as np
 import scipy.sparse as sp
@@ -29,7 +30,7 @@
 from ..preprocessing import normalize
 from .hashing import FeatureHasher
 from .stop_words import ENGLISH_STOP_WORDS
-from ..utils.validation import check_is_fitted
+from ..utils.validation import check_is_fitted, check_array, FLOAT_DTYPES
 from ..utils.fixes import sp_version
 
 __all__ = ['CountVectorizer',
@@ -573,7 +574,7 @@ def _document_frequency(X):
     if sp.isspmatrix_csr(X):
         return np.bincount(X.indices, minlength=X.shape[1])
     else:
-        return np.diff(sp.csc_matrix(X, copy=False).indptr)
+        return np.diff(X.indptr)
 
 
 class CountVectorizer(BaseEstimator, VectorizerMixin):
@@ -1117,11 +1118,14 @@ def fit(self, X, y=None):
         X : sparse matrix, [n_samples, n_features]
             a matrix of term/token counts
         """
+        X = check_array(X, accept_sparse=('csr', 'csc'))
         if not sp.issparse(X):
-            X = sp.csc_matrix(X)
+            X = sp.csr_matrix(X)
+        dtype = X.dtype if X.dtype in FLOAT_DTYPES else np.float64
+
         if self.use_idf:
             n_samples, n_features = X.shape
-            df = _document_frequency(X)
+            df = _document_frequency(X).astype(dtype)
 
             # perform idf smoothing if required
             df += int(self.smooth_idf)
@@ -1129,9 +1133,11 @@ def fit(self, X, y=None):
 
             # log+1 instead of log makes sure terms with zero idf don't get
             # suppressed entirely.
-            idf = np.log(float(n_samples) / df) + 1.0
-            self._idf_diag = sp.spdiags(idf, diags=0, m=n_features,
-                                        n=n_features, format='csr')
+            idf = np.log(n_samples / df) + 1
+            self._idf_diag = sp.diags(idf, offsets=0,
+                                      shape=(n_features, n_features),
+                                      format='csr',
+                                      dtype=dtype)
 
         return self
 
@@ -1151,12 +1157,9 @@ def transform(self, X, copy=True):
         -------
         vectors : sparse matrix, [n_samples, n_features]
         """
-        if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.floating):
-            # preserve float family dtype
-            X = sp.csr_matrix(X, copy=copy)
-        else:
-            # convert counts or binary occurrences to floats
-            X = sp.csr_matrix(X, dtype=np.float64, copy=copy)
+        X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES, copy=copy)
+        if not sp.issparse(X):
+            X = sp.csr_matrix(X, dtype=np.float64)
 
         n_samples, n_features = X.shape
 
@@ -1367,7 +1370,7 @@ def __init__(self, input='content', encoding='utf-8',
                  stop_words=None, token_pattern=r"(?u)\b\w\w+\b",
                  ngram_range=(1, 1), max_df=1.0, min_df=1,
                  max_features=None, vocabulary=None, binary=False,
-                 dtype=np.int64, norm='l2', use_idf=True, smooth_idf=True,
+                 dtype=np.float64, norm='l2', use_idf=True, smooth_idf=True,
                  sublinear_tf=False):
 
         super(TfidfVectorizer, self).__init__(
@@ -1432,6 +1435,13 @@ def idf_(self, value):
                                  (len(value), len(self.vocabulary)))
         self._tfidf.idf_ = value
 
+    def _check_params(self):
+        if self.dtype not in FLOAT_DTYPES:
+            warnings.warn("Only {} 'dtype' should be used. {} 'dtype' will "
+                          "be converted to np.float64."
+                          .format(FLOAT_DTYPES, self.dtype),
+                          UserWarning)
+
     def fit(self, raw_documents, y=None):
         """Learn vocabulary and idf from training set.
 
@@ -1444,6 +1454,7 @@ def fit(self, raw_documents, y=None):
         -------
         self : TfidfVectorizer
         """
+        self._check_params()
         X = super(TfidfVectorizer, self).fit_transform(raw_documents)
         self._tfidf.fit(X)
         return self
@@ -1464,6 +1475,7 @@ def fit_transform(self, raw_documents, y=None):
         X : sparse matrix, [n_samples, n_features]
             Tf-idf-weighted document-term matrix.
         """
+        self._check_params()
         X = super(TfidfVectorizer, self).fit_transform(raw_documents)
         self._tfidf.fit(X)
         # X is already a transformed view of raw_documents so
diff --git a/sklearn/feature_selection/rfe.py b/sklearn/feature_selection/rfe.py
index cfa32129db7a..84761451c8f2 100644
--- a/sklearn/feature_selection/rfe.py
+++ b/sklearn/feature_selection/rfe.py
@@ -9,6 +9,7 @@
 import numpy as np
 from ..utils import check_X_y, safe_sqr
 from ..utils.metaestimators import if_delegate_has_method
+from ..utils.metaestimators import _safe_split
 from ..utils.validation import check_is_fitted
 from ..base import BaseEstimator
 from ..base import MetaEstimatorMixin
@@ -16,7 +17,7 @@
 from ..base import is_classifier
 from ..externals.joblib import Parallel, delayed
 from ..model_selection import check_cv
-from ..model_selection._validation import _safe_split, _score
+from ..model_selection._validation import _score
 from ..metrics.scorer import check_scoring
 from .base import SelectorMixin
 
diff --git a/sklearn/feature_selection/tests/test_feature_select.py b/sklearn/feature_selection/tests/test_feature_select.py
index d3f1eca333cd..14e621473090 100644
--- a/sklearn/feature_selection/tests/test_feature_select.py
+++ b/sklearn/feature_selection/tests/test_feature_select.py
@@ -7,7 +7,6 @@
 import numpy as np
 from scipy import stats, sparse
 
-from numpy.testing import run_module_suite
 from sklearn.utils.testing import assert_equal
 from sklearn.utils.testing import assert_almost_equal
 from sklearn.utils.testing import assert_raises
@@ -670,7 +669,3 @@ def test_mutual_info_regression():
     gtruth = np.zeros(10)
     gtruth[:2] = 1
     assert_array_equal(support, gtruth)
-
-
-if __name__ == '__main__':
-    run_module_suite()
diff --git a/sklearn/feature_selection/tests/test_mutual_info.py b/sklearn/feature_selection/tests/test_mutual_info.py
index 615abf1c5b0a..f05e0b52b370 100644
--- a/sklearn/feature_selection/tests/test_mutual_info.py
+++ b/sklearn/feature_selection/tests/test_mutual_info.py
@@ -1,7 +1,6 @@
 from __future__ import division
 
 import numpy as np
-from numpy.testing import run_module_suite
 from scipy.sparse import csr_matrix
 
 from sklearn.utils import check_random_state
@@ -200,7 +199,3 @@ def test_mutual_info_options():
         assert_array_equal(mi_3, mi_4)
 
     assert_false(np.allclose(mi_1, mi_3))
-
-
-if __name__ == '__main__':
-    run_module_suite()
diff --git a/sklearn/gaussian_process/__init__.py b/sklearn/gaussian_process/__init__.py
index 48d9aa05aaf8..377f15795ee5 100644
--- a/sklearn/gaussian_process/__init__.py
+++ b/sklearn/gaussian_process/__init__.py
@@ -14,10 +14,9 @@
 from .gpc import GaussianProcessClassifier
 from . import kernels
 
-from .gaussian_process import GaussianProcess
 from . import correlation_models
 from . import regression_models
 
-__all__ = ['GaussianProcess', 'correlation_models', 'regression_models',
+__all__ = ['correlation_models', 'regression_models',
            'GaussianProcessRegressor', 'GaussianProcessClassifier',
            'kernels']
diff --git a/sklearn/gaussian_process/gaussian_process.py b/sklearn/gaussian_process/gaussian_process.py
deleted file mode 100644
index 8c7491e648d3..000000000000
--- a/sklearn/gaussian_process/gaussian_process.py
+++ /dev/null
@@ -1,882 +0,0 @@
-# -*- coding: utf-8 -*-
-
-# Author: Vincent Dubourg <vincent.dubourg@gmail.com>
-#         (mostly translation, see implementation details)
-# License: BSD 3 clause
-
-from __future__ import print_function
-
-import numpy as np
-from scipy import linalg, optimize
-
-from ..base import BaseEstimator, RegressorMixin
-from ..metrics.pairwise import manhattan_distances
-from ..utils import check_random_state, check_array, check_X_y
-from ..utils.validation import check_is_fitted
-from . import regression_models as regression
-from . import correlation_models as correlation
-from ..utils import deprecated
-
-MACHINE_EPSILON = np.finfo(np.double).eps
-
-
-@deprecated("l1_cross_distances was deprecated in version 0.18 "
-            "and will be removed in 0.20.")
-def l1_cross_distances(X):
-    """
-    Computes the nonzero componentwise L1 cross-distances between the vectors
-    in X.
-
-    Parameters
-    ----------
-
-    X : array_like
-        An array with shape (n_samples, n_features)
-
-    Returns
-    -------
-
-    D : array with shape (n_samples * (n_samples - 1) / 2, n_features)
-        The array of componentwise L1 cross-distances.
-
-    ij : arrays with shape (n_samples * (n_samples - 1) / 2, 2)
-        The indices i and j of the vectors in X associated to the cross-
-        distances in D: D[k] = np.abs(X[ij[k, 0]] - Y[ij[k, 1]]).
-    """
-    X = check_array(X)
-    n_samples, n_features = X.shape
-    n_nonzero_cross_dist = n_samples * (n_samples - 1) // 2
-    ij = np.zeros((n_nonzero_cross_dist, 2), dtype=np.int)
-    D = np.zeros((n_nonzero_cross_dist, n_features))
-    ll_1 = 0
-    for k in range(n_samples - 1):
-        ll_0 = ll_1
-        ll_1 = ll_0 + n_samples - k - 1
-        ij[ll_0:ll_1, 0] = k
-        ij[ll_0:ll_1, 1] = np.arange(k + 1, n_samples)
-        D[ll_0:ll_1] = np.abs(X[k] - X[(k + 1):n_samples])
-
-    return D, ij
-
-
-@deprecated("GaussianProcess was deprecated in version 0.18 and will be "
-            "removed in 0.20. Use the GaussianProcessRegressor instead.")
-class GaussianProcess(BaseEstimator, RegressorMixin):
-    """The legacy Gaussian Process model class.
-
-    .. deprecated:: 0.18
-        This class will be removed in 0.20.
-        Use the :class:`GaussianProcessRegressor` instead.
-
-    Read more in the :ref:`User Guide <gaussian_process>`.
-
-    Parameters
-    ----------
-    regr : string or callable, optional
-        A regression function returning an array of outputs of the linear
-        regression functional basis. The number of observations n_samples
-        should be greater than the size p of this basis.
-        Default assumes a simple constant regression trend.
-        Available built-in regression models are::
-
-            'constant', 'linear', 'quadratic'
-
-    corr : string or callable, optional
-        A stationary autocorrelation function returning the autocorrelation
-        between two points x and x'.
-        Default assumes a squared-exponential autocorrelation model.
-        Built-in correlation models are::
-
-            'absolute_exponential', 'squared_exponential',
-            'generalized_exponential', 'cubic', 'linear'
-
-    beta0 : double array_like, optional
-        The regression weight vector to perform Ordinary Kriging (OK).
-        Default assumes Universal Kriging (UK) so that the vector beta of
-        regression weights is estimated using the maximum likelihood
-        principle.
-
-    storage_mode : string, optional
-        A string specifying whether the Cholesky decomposition of the
-        correlation matrix should be stored in the class (storage_mode =
-        'full') or not (storage_mode = 'light').
-        Default assumes storage_mode = 'full', so that the
-        Cholesky decomposition of the correlation matrix is stored.
-        This might be a useful parameter when one is not interested in the
-        MSE and only plan to estimate the BLUP, for which the correlation
-        matrix is not required.
-
-    verbose : boolean, optional
-        A boolean specifying the verbose level.
-        Default is verbose = False.
-
-    theta0 : double array_like, optional
-        An array with shape (n_features, ) or (1, ).
-        The parameters in the autocorrelation model.
-        If thetaL and thetaU are also specified, theta0 is considered as
-        the starting point for the maximum likelihood estimation of the
-        best set of parameters.
-        Default assumes isotropic autocorrelation model with theta0 = 1e-1.
-
-    thetaL : double array_like, optional
-        An array with shape matching theta0's.
-        Lower bound on the autocorrelation parameters for maximum
-        likelihood estimation.
-        Default is None, so that it skips maximum likelihood estimation and
-        it uses theta0.
-
-    thetaU : double array_like, optional
-        An array with shape matching theta0's.
-        Upper bound on the autocorrelation parameters for maximum
-        likelihood estimation.
-        Default is None, so that it skips maximum likelihood estimation and
-        it uses theta0.
-
-    normalize : boolean, optional
-        Input X and observations y are centered and reduced wrt
-        means and standard deviations estimated from the n_samples
-        observations provided.
-        Default is normalize = True so that data is normalized to ease
-        maximum likelihood estimation.
-
-    nugget : double or ndarray, optional
-        Introduce a nugget effect to allow smooth predictions from noisy
-        data.  If nugget is an ndarray, it must be the same length as the
-        number of data points used for the fit.
-        The nugget is added to the diagonal of the assumed training covariance;
-        in this way it acts as a Tikhonov regularization in the problem.  In
-        the special case of the squared exponential correlation function, the
-        nugget mathematically represents the variance of the input values.
-        Default assumes a nugget close to machine precision for the sake of
-        robustness (nugget = 10. * MACHINE_EPSILON).
-
-    optimizer : string, optional
-        A string specifying the optimization algorithm to be used.
-        Default uses 'fmin_cobyla' algorithm from scipy.optimize.
-        Available optimizers are::
-
-            'fmin_cobyla', 'Welch'
-
-        'Welch' optimizer is dued to Welch et al., see reference [WBSWM1992]_.
-        It consists in iterating over several one-dimensional optimizations
-        instead of running one single multi-dimensional optimization.
-
-    random_start : int, optional
-        The number of times the Maximum Likelihood Estimation should be
-        performed from a random starting point.
-        The first MLE always uses the specified starting point (theta0),
-        the next starting points are picked at random according to an
-        exponential distribution (log-uniform on [thetaL, thetaU]).
-        Default does not use random starting point (random_start = 1).
-
-    random_state : int, RandomState instance or None, optional (default=None)
-        The generator used to shuffle the sequence of coordinates of theta in
-        the Welch optimizer. If int, random_state is the seed used by the
-        random number generator; If RandomState instance, random_state is the
-        random number generator; If None, the random number generator is the
-        RandomState instance used by `np.random`.
-
-    Attributes
-    ----------
-    theta_ : array
-        Specified theta OR the best set of autocorrelation parameters (the \
-        sought maximizer of the reduced likelihood function).
-
-    reduced_likelihood_function_value_ : array
-        The optimal reduced likelihood function value.
-
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from sklearn.gaussian_process import GaussianProcess
-    >>> X = np.array([[1., 3., 5., 6., 7., 8.]]).T
-    >>> y = (X * np.sin(X)).ravel()
-    >>> gp = GaussianProcess(theta0=0.1, thetaL=.001, thetaU=1.)
-    >>> gp.fit(X, y)                                      # doctest: +ELLIPSIS
-    GaussianProcess(beta0=None...
-            ...
-
-    Notes
-    -----
-    The presentation implementation is based on a translation of the DACE
-    Matlab toolbox, see reference [NLNS2002]_.
-
-    References
-    ----------
-
-    .. [NLNS2002] `H.B. Nielsen, S.N. Lophaven, H. B. Nielsen and J.
-        Sondergaard.  DACE - A MATLAB Kriging Toolbox.` (2002)
-        http://imedea.uib-csic.es/master/cambioglobal/Modulo_V_cod101615/Lab/lab_maps/krigging/DACE-krigingsoft/dace/dace.pdf
-
-    .. [WBSWM1992] `W.J. Welch, R.J. Buck, J. Sacks, H.P. Wynn, T.J. Mitchell,
-        and M.D.  Morris (1992). Screening, predicting, and computer
-        experiments.  Technometrics, 34(1) 15--25.`
-        http://www.jstor.org/stable/1269548
-    """
-
-    _regression_types = {
-        'constant': regression.constant,
-        'linear': regression.linear,
-        'quadratic': regression.quadratic}
-
-    _correlation_types = {
-        'absolute_exponential': correlation.absolute_exponential,
-        'squared_exponential': correlation.squared_exponential,
-        'generalized_exponential': correlation.generalized_exponential,
-        'cubic': correlation.cubic,
-        'linear': correlation.linear}
-
-    _optimizer_types = [
-        'fmin_cobyla',
-        'Welch']
-
-    def __init__(self, regr='constant', corr='squared_exponential', beta0=None,
-                 storage_mode='full', verbose=False, theta0=1e-1,
-                 thetaL=None, thetaU=None, optimizer='fmin_cobyla',
-                 random_start=1, normalize=True,
-                 nugget=10. * MACHINE_EPSILON, random_state=None):
-
-        self.regr = regr
-        self.corr = corr
-        self.beta0 = beta0
-        self.storage_mode = storage_mode
-        self.verbose = verbose
-        self.theta0 = theta0
-        self.thetaL = thetaL
-        self.thetaU = thetaU
-        self.normalize = normalize
-        self.nugget = nugget
-        self.optimizer = optimizer
-        self.random_start = random_start
-        self.random_state = random_state
-
-    def fit(self, X, y):
-        """
-        The Gaussian Process model fitting method.
-
-        Parameters
-        ----------
-        X : double array_like
-            An array with shape (n_samples, n_features) with the input at which
-            observations were made.
-
-        y : double array_like
-            An array with shape (n_samples, ) or shape (n_samples, n_targets)
-            with the observations of the output to be predicted.
-
-        Returns
-        -------
-        gp : self
-            A fitted Gaussian Process model object awaiting data to perform
-            predictions.
-        """
-        # Run input checks
-        self._check_params()
-
-        self.random_state = check_random_state(self.random_state)
-
-        # Force data to 2D numpy.array
-        X, y = check_X_y(X, y, multi_output=True, y_numeric=True)
-        self.y_ndim_ = y.ndim
-        if y.ndim == 1:
-            y = y[:, np.newaxis]
-
-        # Check shapes of DOE & observations
-        n_samples, n_features = X.shape
-        _, n_targets = y.shape
-
-        # Run input checks
-        self._check_params(n_samples)
-
-        # Normalize data or don't
-        if self.normalize:
-            X_mean = np.mean(X, axis=0)
-            X_std = np.std(X, axis=0)
-            y_mean = np.mean(y, axis=0)
-            y_std = np.std(y, axis=0)
-            X_std[X_std == 0.] = 1.
-            y_std[y_std == 0.] = 1.
-            # center and scale X if necessary
-            X = (X - X_mean) / X_std
-            y = (y - y_mean) / y_std
-        else:
-            X_mean = np.zeros(1)
-            X_std = np.ones(1)
-            y_mean = np.zeros(1)
-            y_std = np.ones(1)
-
-        # Calculate matrix of distances D between samples
-        D, ij = l1_cross_distances(X)
-        if (np.min(np.sum(D, axis=1)) == 0.
-                and self.corr != correlation.pure_nugget):
-            raise Exception("Multiple input features cannot have the same"
-                            " target value.")
-
-        # Regression matrix and parameters
-        F = self.regr(X)
-        n_samples_F = F.shape[0]
-        if F.ndim > 1:
-            p = F.shape[1]
-        else:
-            p = 1
-        if n_samples_F != n_samples:
-            raise Exception("Number of rows in F and X do not match. Most "
-                            "likely something is going wrong with the "
-                            "regression model.")
-        if p > n_samples_F:
-            raise Exception(("Ordinary least squares problem is undetermined "
-                             "n_samples=%d must be greater than the "
-                             "regression model size p=%d.") % (n_samples, p))
-        if self.beta0 is not None:
-            if self.beta0.shape[0] != p:
-                raise Exception("Shapes of beta0 and F do not match.")
-
-        # Set attributes
-        self.X = X
-        self.y = y
-        self.D = D
-        self.ij = ij
-        self.F = F
-        self.X_mean, self.X_std = X_mean, X_std
-        self.y_mean, self.y_std = y_mean, y_std
-
-        # Determine Gaussian Process model parameters
-        if self.thetaL is not None and self.thetaU is not None:
-            # Maximum Likelihood Estimation of the parameters
-            if self.verbose:
-                print("Performing Maximum Likelihood Estimation of the "
-                      "autocorrelation parameters...")
-            self.theta_, self.reduced_likelihood_function_value_, par = \
-                self._arg_max_reduced_likelihood_function()
-            if np.isinf(self.reduced_likelihood_function_value_):
-                raise Exception("Bad parameter region. "
-                                "Try increasing upper bound")
-
-        else:
-            # Given parameters
-            if self.verbose:
-                print("Given autocorrelation parameters. "
-                      "Computing Gaussian Process model parameters...")
-            self.theta_ = self.theta0
-            self.reduced_likelihood_function_value_, par = \
-                self.reduced_likelihood_function()
-            if np.isinf(self.reduced_likelihood_function_value_):
-                raise Exception("Bad point. Try increasing theta0.")
-
-        self.beta = par['beta']
-        self.gamma = par['gamma']
-        self.sigma2 = par['sigma2']
-        self.C = par['C']
-        self.Ft = par['Ft']
-        self.G = par['G']
-
-        if self.storage_mode == 'light':
-            # Delete heavy data (it will be computed again if required)
-            # (it is required only when MSE is wanted in self.predict)
-            if self.verbose:
-                print("Light storage mode specified. "
-                      "Flushing autocorrelation matrix...")
-            self.D = None
-            self.ij = None
-            self.F = None
-            self.C = None
-            self.Ft = None
-            self.G = None
-
-        return self
-
-    def predict(self, X, eval_MSE=False, batch_size=None):
-        """
-        This function evaluates the Gaussian Process model at x.
-
-        Parameters
-        ----------
-        X : array_like
-            An array with shape (n_eval, n_features) giving the point(s) at
-            which the prediction(s) should be made.
-
-        eval_MSE : boolean, optional
-            A boolean specifying whether the Mean Squared Error should be
-            evaluated or not.
-            Default assumes evalMSE = False and evaluates only the BLUP (mean
-            prediction).
-
-        batch_size : integer, optional
-            An integer giving the maximum number of points that can be
-            evaluated simultaneously (depending on the available memory).
-            Default is None so that all given points are evaluated at the same
-            time.
-
-        Returns
-        -------
-        y : array_like, shape (n_samples, ) or (n_samples, n_targets)
-            An array with shape (n_eval, ) if the Gaussian Process was trained
-            on an array of shape (n_samples, ) or an array with shape
-            (n_eval, n_targets) if the Gaussian Process was trained on an array
-            of shape (n_samples, n_targets) with the Best Linear Unbiased
-            Prediction at x.
-
-        MSE : array_like, optional (if eval_MSE == True)
-            An array with shape (n_eval, ) or (n_eval, n_targets) as with y,
-            with the Mean Squared Error at x.
-        """
-        check_is_fitted(self, "X")
-
-        # Check input shapes
-        X = check_array(X)
-        n_eval, _ = X.shape
-        n_samples, n_features = self.X.shape
-        n_samples_y, n_targets = self.y.shape
-
-        # Run input checks
-        self._check_params(n_samples)
-
-        if X.shape[1] != n_features:
-            raise ValueError(("The number of features in X (X.shape[1] = %d) "
-                              "should match the number of features used "
-                              "for fit() "
-                              "which is %d.") % (X.shape[1], n_features))
-
-        if batch_size is None:
-            # No memory management
-            # (evaluates all given points in a single batch run)
-
-            # Normalize input
-            X = (X - self.X_mean) / self.X_std
-
-            # Get pairwise componentwise L1-distances to the input training set
-            dx = manhattan_distances(X, Y=self.X, sum_over_features=False)
-            # Get regression function and correlation
-            f = self.regr(X)
-            r = self.corr(self.theta_, dx).reshape(n_eval, n_samples)
-
-            # Scaled predictor
-            y_ = np.dot(f, self.beta) + np.dot(r, self.gamma)
-
-            # Predictor
-            y = (self.y_mean + self.y_std * y_).reshape(n_eval, n_targets)
-
-            if self.y_ndim_ == 1:
-                y = y.ravel()
-
-            # Mean Squared Error
-            if eval_MSE:
-                C = self.C
-                if C is None:
-                    # Light storage mode (need to recompute C, F, Ft and G)
-                    if self.verbose:
-                        print("This GaussianProcess used 'light' storage mode "
-                              "at instantiation. Need to recompute "
-                              "autocorrelation matrix...")
-                    reduced_likelihood_function_value, par = \
-                        self.reduced_likelihood_function()
-                    self.C = par['C']
-                    self.Ft = par['Ft']
-                    self.G = par['G']
-
-                rt = linalg.solve_triangular(self.C, r.T, lower=True)
-
-                if self.beta0 is None:
-                    # Universal Kriging
-                    u = linalg.solve_triangular(self.G.T,
-                                                np.dot(self.Ft.T, rt) - f.T,
-                                                lower=True)
-                else:
-                    # Ordinary Kriging
-                    u = np.zeros((n_targets, n_eval))
-
-                MSE = np.dot(self.sigma2.reshape(n_targets, 1),
-                             (1. - (rt ** 2.).sum(axis=0)
-                              + (u ** 2.).sum(axis=0))[np.newaxis, :])
-                MSE = np.sqrt((MSE ** 2.).sum(axis=0) / n_targets)
-
-                # Mean Squared Error might be slightly negative depending on
-                # machine precision: force to zero!
-                MSE[MSE < 0.] = 0.
-
-                if self.y_ndim_ == 1:
-                    MSE = MSE.ravel()
-
-                return y, MSE
-
-            else:
-
-                return y
-
-        else:
-            # Memory management
-
-            if type(batch_size) is not int or batch_size <= 0:
-                raise Exception("batch_size must be a positive integer")
-
-            if eval_MSE:
-
-                y, MSE = np.zeros(n_eval), np.zeros(n_eval)
-                for k in range(max(1, int(n_eval / batch_size))):
-                    batch_from = k * batch_size
-                    batch_to = min([(k + 1) * batch_size + 1, n_eval + 1])
-                    y[batch_from:batch_to], MSE[batch_from:batch_to] = \
-                        self.predict(X[batch_from:batch_to],
-                                     eval_MSE=eval_MSE, batch_size=None)
-
-                return y, MSE
-
-            else:
-
-                y = np.zeros(n_eval)
-                for k in range(max(1, int(n_eval / batch_size))):
-                    batch_from = k * batch_size
-                    batch_to = min([(k + 1) * batch_size + 1, n_eval + 1])
-                    y[batch_from:batch_to] = \
-                        self.predict(X[batch_from:batch_to],
-                                     eval_MSE=eval_MSE, batch_size=None)
-
-                return y
-
-    def reduced_likelihood_function(self, theta=None):
-        """
-        This function determines the BLUP parameters and evaluates the reduced
-        likelihood function for the given autocorrelation parameters theta.
-
-        Maximizing this function wrt the autocorrelation parameters theta is
-        equivalent to maximizing the likelihood of the assumed joint Gaussian
-        distribution of the observations y evaluated onto the design of
-        experiments X.
-
-        Parameters
-        ----------
-        theta : array_like, optional
-            An array containing the autocorrelation parameters at which the
-            Gaussian Process model parameters should be determined.
-            Default uses the built-in autocorrelation parameters
-            (ie ``theta = self.theta_``).
-
-        Returns
-        -------
-        reduced_likelihood_function_value : double
-            The value of the reduced likelihood function associated to the
-            given autocorrelation parameters theta.
-
-        par : dict
-            A dictionary containing the requested Gaussian Process model
-            parameters:
-
-            - ``sigma2`` is the Gaussian Process variance.
-            - ``beta`` is the generalized least-squares regression weights for
-              Universal Kriging or given beta0 for Ordinary Kriging.
-            - ``gamma`` is the Gaussian Process weights.
-            - ``C`` is the Cholesky decomposition of the correlation
-              matrix [R].
-            - ``Ft`` is the solution of the linear equation system
-              [R] x Ft = F
-            - ``G`` is the QR decomposition of the matrix Ft.
-        """
-        check_is_fitted(self, "X")
-
-        if theta is None:
-            # Use built-in autocorrelation parameters
-            theta = self.theta_
-
-        # Initialize output
-        reduced_likelihood_function_value = - np.inf
-        par = {}
-
-        # Retrieve data
-        n_samples = self.X.shape[0]
-        D = self.D
-        ij = self.ij
-        F = self.F
-
-        if D is None:
-            # Light storage mode (need to recompute D, ij and F)
-            D, ij = l1_cross_distances(self.X)
-            if (np.min(np.sum(D, axis=1)) == 0.
-                    and self.corr != correlation.pure_nugget):
-                raise Exception("Multiple X are not allowed")
-            F = self.regr(self.X)
-
-        # Set up R
-        r = self.corr(theta, D)
-        R = np.eye(n_samples) * (1. + self.nugget)
-        R[ij[:, 0], ij[:, 1]] = r
-        R[ij[:, 1], ij[:, 0]] = r
-
-        # Cholesky decomposition of R
-        try:
-            C = linalg.cholesky(R, lower=True)
-        except linalg.LinAlgError:
-            return reduced_likelihood_function_value, par
-
-        # Get generalized least squares solution
-        Ft = linalg.solve_triangular(C, F, lower=True)
-        Q, G = linalg.qr(Ft, mode='economic')
-
-        sv = linalg.svd(G, compute_uv=False)
-        rcondG = sv[-1] / sv[0]
-        if rcondG < 1e-10:
-            # Check F
-            sv = linalg.svd(F, compute_uv=False)
-            condF = sv[0] / sv[-1]
-            if condF > 1e15:
-                raise Exception("F is too ill conditioned. Poor combination "
-                                "of regression model and observations.")
-            else:
-                # Ft is too ill conditioned, get out (try different theta)
-                return reduced_likelihood_function_value, par
-
-        Yt = linalg.solve_triangular(C, self.y, lower=True)
-        if self.beta0 is None:
-            # Universal Kriging
-            beta = linalg.solve_triangular(G, np.dot(Q.T, Yt))
-        else:
-            # Ordinary Kriging
-            beta = np.array(self.beta0)
-
-        rho = Yt - np.dot(Ft, beta)
-        sigma2 = (rho ** 2.).sum(axis=0) / n_samples
-        # The determinant of R is equal to the squared product of the diagonal
-        # elements of its Cholesky decomposition C
-        detR = (np.diag(C) ** (2. / n_samples)).prod()
-
-        # Compute/Organize output
-        reduced_likelihood_function_value = - sigma2.sum() * detR
-        par['sigma2'] = sigma2 * self.y_std ** 2.
-        par['beta'] = beta
-        par['gamma'] = linalg.solve_triangular(C.T, rho)
-        par['C'] = C
-        par['Ft'] = Ft
-        par['G'] = G
-
-        return reduced_likelihood_function_value, par
-
-    def _arg_max_reduced_likelihood_function(self):
-        """
-        This function estimates the autocorrelation parameters theta as the
-        maximizer of the reduced likelihood function.
-        (Minimization of the opposite reduced likelihood function is used for
-        convenience)
-
-        Parameters
-        ----------
-        self : All parameters are stored in the Gaussian Process model object.
-
-        Returns
-        -------
-        optimal_theta : array_like
-            The best set of autocorrelation parameters (the sought maximizer of
-            the reduced likelihood function).
-
-        optimal_reduced_likelihood_function_value : double
-            The optimal reduced likelihood function value.
-
-        optimal_par : dict
-            The BLUP parameters associated to thetaOpt.
-        """
-
-        # Initialize output
-        best_optimal_theta = []
-        best_optimal_rlf_value = []
-        best_optimal_par = []
-
-        if self.verbose:
-            print("The chosen optimizer is: " + str(self.optimizer))
-            if self.random_start > 1:
-                print(str(self.random_start) + " random starts are required.")
-
-        percent_completed = 0.
-
-        # Force optimizer to fmin_cobyla if the model is meant to be isotropic
-        if self.optimizer == 'Welch' and self.theta0.size == 1:
-            self.optimizer = 'fmin_cobyla'
-
-        if self.optimizer == 'fmin_cobyla':
-
-            def minus_reduced_likelihood_function(log10t):
-                return - self.reduced_likelihood_function(
-                    theta=10. ** log10t)[0]
-
-            constraints = []
-            for i in range(self.theta0.size):
-                constraints.append(lambda log10t, i=i:
-                                   log10t[i] - np.log10(self.thetaL[0, i]))
-                constraints.append(lambda log10t, i=i:
-                                   np.log10(self.thetaU[0, i]) - log10t[i])
-
-            for k in range(self.random_start):
-
-                if k == 0:
-                    # Use specified starting point as first guess
-                    theta0 = self.theta0
-                else:
-                    # Generate a random starting point log10-uniformly
-                    # distributed between bounds
-                    log10theta0 = (np.log10(self.thetaL)
-                                   + self.random_state.rand(*self.theta0.shape)
-                                   * np.log10(self.thetaU / self.thetaL))
-                    theta0 = 10. ** log10theta0
-
-                # Run Cobyla
-                try:
-                    log10_optimal_theta = \
-                        optimize.fmin_cobyla(minus_reduced_likelihood_function,
-                                             np.log10(theta0).ravel(),
-                                             constraints, disp=0)
-                except ValueError as ve:
-                    print("Optimization failed. Try increasing the ``nugget``")
-                    raise ve
-
-                optimal_theta = 10. ** log10_optimal_theta
-                optimal_rlf_value, optimal_par = \
-                    self.reduced_likelihood_function(theta=optimal_theta)
-
-                # Compare the new optimizer to the best previous one
-                if k > 0:
-                    if optimal_rlf_value > best_optimal_rlf_value:
-                        best_optimal_rlf_value = optimal_rlf_value
-                        best_optimal_par = optimal_par
-                        best_optimal_theta = optimal_theta
-                else:
-                    best_optimal_rlf_value = optimal_rlf_value
-                    best_optimal_par = optimal_par
-                    best_optimal_theta = optimal_theta
-                if self.verbose and self.random_start > 1:
-                    if (20 * k) / self.random_start > percent_completed:
-                        percent_completed = (20 * k) / self.random_start
-                        print("%s completed" % (5 * percent_completed))
-
-            optimal_rlf_value = best_optimal_rlf_value
-            optimal_par = best_optimal_par
-            optimal_theta = best_optimal_theta
-
-        elif self.optimizer == 'Welch':
-
-            # Backup of the given attributes
-            theta0, thetaL, thetaU = self.theta0, self.thetaL, self.thetaU
-            corr = self.corr
-            verbose = self.verbose
-
-            # This will iterate over fmin_cobyla optimizer
-            self.optimizer = 'fmin_cobyla'
-            self.verbose = False
-
-            # Initialize under isotropy assumption
-            if verbose:
-                print("Initialize under isotropy assumption...")
-            self.theta0 = check_array(self.theta0.min())
-            self.thetaL = check_array(self.thetaL.min())
-            self.thetaU = check_array(self.thetaU.max())
-            theta_iso, optimal_rlf_value_iso, par_iso = \
-                self._arg_max_reduced_likelihood_function()
-            optimal_theta = theta_iso + np.zeros(theta0.shape)
-
-            # Iterate over all dimensions of theta allowing for anisotropy
-            if verbose:
-                print("Now improving allowing for anisotropy...")
-            for i in self.random_state.permutation(theta0.size):
-                if verbose:
-                    print("Proceeding along dimension %d..." % (i + 1))
-                self.theta0 = check_array(theta_iso)
-                self.thetaL = check_array(thetaL[0, i])
-                self.thetaU = check_array(thetaU[0, i])
-
-                def corr_cut(t, d):
-                    return corr(check_array(np.hstack([optimal_theta[0][0:i],
-                                                       t[0],
-                                                       optimal_theta[0][(i +
-                                                                         1)::]])),
-                                d)
-
-                self.corr = corr_cut
-                optimal_theta[0, i], optimal_rlf_value, optimal_par = \
-                    self._arg_max_reduced_likelihood_function()
-
-            # Restore the given attributes
-            self.theta0, self.thetaL, self.thetaU = theta0, thetaL, thetaU
-            self.corr = corr
-            self.optimizer = 'Welch'
-            self.verbose = verbose
-
-        else:
-
-            raise NotImplementedError("This optimizer ('%s') is not "
-                                      "implemented yet. Please contribute!"
-                                      % self.optimizer)
-
-        return optimal_theta, optimal_rlf_value, optimal_par
-
-    def _check_params(self, n_samples=None):
-
-        # Check regression model
-        if not callable(self.regr):
-            if self.regr in self._regression_types:
-                self.regr = self._regression_types[self.regr]
-            else:
-                raise ValueError("regr should be one of %s or callable, "
-                                 "%s was given."
-                                 % (self._regression_types.keys(), self.regr))
-
-        # Check regression weights if given (Ordinary Kriging)
-        if self.beta0 is not None:
-            self.beta0 = np.atleast_2d(self.beta0)
-            if self.beta0.shape[1] != 1:
-                # Force to column vector
-                self.beta0 = self.beta0.T
-
-        # Check correlation model
-        if not callable(self.corr):
-            if self.corr in self._correlation_types:
-                self.corr = self._correlation_types[self.corr]
-            else:
-                raise ValueError("corr should be one of %s or callable, "
-                                 "%s was given."
-                                 % (self._correlation_types.keys(), self.corr))
-
-        # Check storage mode
-        if self.storage_mode != 'full' and self.storage_mode != 'light':
-            raise ValueError("Storage mode should either be 'full' or "
-                             "'light', %s was given." % self.storage_mode)
-
-        # Check correlation parameters
-        self.theta0 = np.atleast_2d(self.theta0)
-        lth = self.theta0.size
-
-        if self.thetaL is not None and self.thetaU is not None:
-            self.thetaL = np.atleast_2d(self.thetaL)
-            self.thetaU = np.atleast_2d(self.thetaU)
-            if self.thetaL.size != lth or self.thetaU.size != lth:
-                raise ValueError("theta0, thetaL and thetaU must have the "
-                                 "same length.")
-            if np.any(self.thetaL <= 0) or np.any(self.thetaU < self.thetaL):
-                raise ValueError("The bounds must satisfy O < thetaL <= "
-                                 "thetaU.")
-
-        elif self.thetaL is None and self.thetaU is None:
-            if np.any(self.theta0 <= 0):
-                raise ValueError("theta0 must be strictly positive.")
-
-        elif self.thetaL is None or self.thetaU is None:
-            raise ValueError("thetaL and thetaU should either be both or "
-                             "neither specified.")
-
-        # Force verbose type to bool
-        self.verbose = bool(self.verbose)
-
-        # Force normalize type to bool
-        self.normalize = bool(self.normalize)
-
-        # Check nugget value
-        self.nugget = np.asarray(self.nugget)
-        if np.any(self.nugget) < 0.:
-            raise ValueError("nugget must be positive or zero.")
-        if (n_samples is not None
-                and self.nugget.shape not in [(), (n_samples,)]):
-            raise ValueError("nugget must be either a scalar "
-                             "or array of length n_samples.")
-
-        # Check optimizer
-        if self.optimizer not in self._optimizer_types:
-            raise ValueError("optimizer should be one of %s"
-                             % self._optimizer_types)
-
-        # Force random_start type to int
-        self.random_start = int(self.random_start)
diff --git a/sklearn/gaussian_process/tests/test_gaussian_process.py b/sklearn/gaussian_process/tests/test_gaussian_process.py
deleted file mode 100644
index 37d872fc99fb..000000000000
--- a/sklearn/gaussian_process/tests/test_gaussian_process.py
+++ /dev/null
@@ -1,175 +0,0 @@
-"""
-Testing for Gaussian Process module (sklearn.gaussian_process)
-"""
-
-# Author: Vincent Dubourg <vincent.dubourg@gmail.com>
-# License: BSD 3 clause
-
-import numpy as np
-
-from sklearn.gaussian_process import GaussianProcess
-from sklearn.gaussian_process import regression_models as regression
-from sklearn.gaussian_process import correlation_models as correlation
-from sklearn.datasets import make_regression
-from sklearn.utils.testing import assert_greater, assert_true, assert_raises
-
-
-f = lambda x: x * np.sin(x)
-X = np.atleast_2d([1., 3., 5., 6., 7., 8.]).T
-X2 = np.atleast_2d([2., 4., 5.5, 6.5, 7.5]).T
-y = f(X).ravel()
-
-
-def test_1d(regr=regression.constant, corr=correlation.squared_exponential,
-            random_start=10, beta0=None):
-    # MLE estimation of a one-dimensional Gaussian Process model.
-    # Check random start optimization.
-    # Test the interpolating property.
-    gp = GaussianProcess(regr=regr, corr=corr, beta0=beta0,
-                         theta0=1e-2, thetaL=1e-4, thetaU=1e-1,
-                         random_start=random_start, verbose=False).fit(X, y)
-    y_pred, MSE = gp.predict(X, eval_MSE=True)
-    y2_pred, MSE2 = gp.predict(X2, eval_MSE=True)
-
-    assert_true(np.allclose(y_pred, y) and np.allclose(MSE, 0.)
-                and np.allclose(MSE2, 0., atol=10))
-
-
-def test_2d(regr=regression.constant, corr=correlation.squared_exponential,
-            random_start=10, beta0=None):
-    # MLE estimation of a two-dimensional Gaussian Process model accounting for
-    # anisotropy. Check random start optimization.
-    # Test the interpolating property.
-    b, kappa, e = 5., .5, .1
-    g = lambda x: b - x[:, 1] - kappa * (x[:, 0] - e) ** 2.
-    X = np.array([[-4.61611719, -6.00099547],
-                  [4.10469096, 5.32782448],
-                  [0.00000000, -0.50000000],
-                  [-6.17289014, -4.6984743],
-                  [1.3109306, -6.93271427],
-                  [-5.03823144, 3.10584743],
-                  [-2.87600388, 6.74310541],
-                  [5.21301203, 4.26386883]])
-    y = g(X).ravel()
-
-    thetaL = [1e-4] * 2
-    thetaU = [1e-1] * 2
-    gp = GaussianProcess(regr=regr, corr=corr, beta0=beta0,
-                         theta0=[1e-2] * 2, thetaL=thetaL,
-                         thetaU=thetaU,
-                         random_start=random_start, verbose=False)
-    gp.fit(X, y)
-    y_pred, MSE = gp.predict(X, eval_MSE=True)
-
-    assert_true(np.allclose(y_pred, y) and np.allclose(MSE, 0.))
-
-    eps = np.finfo(gp.theta_.dtype).eps
-    assert_true(np.all(gp.theta_ >= thetaL - eps))  # Lower bounds of hyperparameters
-    assert_true(np.all(gp.theta_ <= thetaU + eps))  # Upper bounds of hyperparameters
-
-
-def test_2d_2d(regr=regression.constant, corr=correlation.squared_exponential,
-               random_start=10, beta0=None):
-    # MLE estimation of a two-dimensional Gaussian Process model accounting for
-    # anisotropy. Check random start optimization.
-    # Test the GP interpolation for 2D output
-    b, kappa, e = 5., .5, .1
-    g = lambda x: b - x[:, 1] - kappa * (x[:, 0] - e) ** 2.
-    f = lambda x: np.vstack((g(x), g(x))).T
-    X = np.array([[-4.61611719, -6.00099547],
-                  [4.10469096, 5.32782448],
-                  [0.00000000, -0.50000000],
-                  [-6.17289014, -4.6984743],
-                  [1.3109306, -6.93271427],
-                  [-5.03823144, 3.10584743],
-                  [-2.87600388, 6.74310541],
-                  [5.21301203, 4.26386883]])
-    y = f(X)
-    gp = GaussianProcess(regr=regr, corr=corr, beta0=beta0,
-                         theta0=[1e-2] * 2, thetaL=[1e-4] * 2,
-                         thetaU=[1e-1] * 2,
-                         random_start=random_start, verbose=False)
-    gp.fit(X, y)
-    y_pred, MSE = gp.predict(X, eval_MSE=True)
-
-    assert_true(np.allclose(y_pred, y) and np.allclose(MSE, 0.))
-
-
-def test_wrong_number_of_outputs():
-    gp = GaussianProcess()
-    assert_raises(ValueError, gp.fit, [[1, 2, 3], [4, 5, 6]], [1, 2, 3])
-
-
-def test_more_builtin_correlation_models(random_start=1):
-    # Repeat test_1d and test_2d for several built-in correlation
-    # models specified as strings.
-    all_corr = ['absolute_exponential', 'squared_exponential', 'cubic',
-                'linear']
-
-    for corr in all_corr:
-        test_1d(regr='constant', corr=corr, random_start=random_start)
-        test_2d(regr='constant', corr=corr, random_start=random_start)
-        test_2d_2d(regr='constant', corr=corr, random_start=random_start)
-
-
-def test_ordinary_kriging():
-    # Repeat test_1d and test_2d with given regression weights (beta0) for
-    # different regression models (Ordinary Kriging).
-    test_1d(regr='linear', beta0=[0., 0.5])
-    test_1d(regr='quadratic', beta0=[0., 0.5, 0.5])
-    test_2d(regr='linear', beta0=[0., 0.5, 0.5])
-    test_2d(regr='quadratic', beta0=[0., 0.5, 0.5, 0.5, 0.5, 0.5])
-    test_2d_2d(regr='linear', beta0=[0., 0.5, 0.5])
-    test_2d_2d(regr='quadratic', beta0=[0., 0.5, 0.5, 0.5, 0.5, 0.5])
-
-
-def test_no_normalize():
-    gp = GaussianProcess(normalize=False).fit(X, y)
-    y_pred = gp.predict(X)
-    assert_true(np.allclose(y_pred, y))
-
-
-def test_batch_size():
-    # TypeError when using batch_size on Python 3, see
-    # https://github.com/scikit-learn/scikit-learn/issues/7329 for more
-    # details
-    gp = GaussianProcess()
-    gp.fit(X, y)
-    gp.predict(X, batch_size=1)
-    gp.predict(X, batch_size=1, eval_MSE=True)
-
-
-def test_random_starts():
-    # Test that an increasing number of random-starts of GP fitting only
-    # increases the reduced likelihood function of the optimal theta.
-    n_samples, n_features = 50, 3
-    rng = np.random.RandomState(0)
-    X = rng.randn(n_samples, n_features) * 2 - 1
-    y = np.sin(X).sum(axis=1) + np.sin(3 * X).sum(axis=1)
-    best_likelihood = -np.inf
-    for random_start in range(1, 5):
-        gp = GaussianProcess(regr="constant", corr="squared_exponential",
-                             theta0=[1e-0] * n_features,
-                             thetaL=[1e-4] * n_features,
-                             thetaU=[1e+1] * n_features,
-                             random_start=random_start, random_state=0,
-                             verbose=False).fit(X, y)
-        rlf = gp.reduced_likelihood_function()[0]
-        assert_greater(rlf, best_likelihood - np.finfo(np.float32).eps)
-        best_likelihood = rlf
-
-
-def test_mse_solving():
-    # test the MSE estimate to be sane.
-    # non-regression test for ignoring off-diagonals of feature covariance,
-    # testing with nugget that renders covariance useless, only
-    # using the mean function, with low effective rank of data
-    gp = GaussianProcess(corr='absolute_exponential', theta0=1e-4,
-                         thetaL=1e-12, thetaU=1e-2, nugget=1e-2,
-                         optimizer='Welch', regr="linear", random_state=0)
-
-    X, y = make_regression(n_informative=3, n_features=60, noise=50,
-                           random_state=0, effective_rank=1)
-
-    gp.fit(X, y)
-    assert_greater(1000, gp.predict(X, eval_MSE=True)[1].mean())
diff --git a/sklearn/gaussian_process/tests/test_gpc.py b/sklearn/gaussian_process/tests/test_gpc.py
index ba98cddb77f7..25aaa05fac3a 100644
--- a/sklearn/gaussian_process/tests/test_gpc.py
+++ b/sklearn/gaussian_process/tests/test_gpc.py
@@ -7,6 +7,8 @@
 
 from scipy.optimize import approx_fprime
 
+import pytest
+
 from sklearn.gaussian_process import GaussianProcessClassifier
 from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
 
@@ -31,62 +33,60 @@ def f(x):
            RBF(length_scale=1.0, length_scale_bounds=(1e-3, 1e3)),
            C(1.0, (1e-2, 1e2)) *
            RBF(length_scale=1.0, length_scale_bounds=(1e-3, 1e3))]
+non_fixed_kernels = [kernel for kernel in kernels
+                     if kernel != fixed_kernel]
 
 
-def test_predict_consistent():
+@pytest.mark.parametrize('kernel', kernels)
+def test_predict_consistent(kernel):
     # Check binary predict decision has also predicted probability above 0.5.
-    for kernel in kernels:
-        gpc = GaussianProcessClassifier(kernel=kernel).fit(X, y)
-        assert_array_equal(gpc.predict(X),
-                           gpc.predict_proba(X)[:, 1] >= 0.5)
+    gpc = GaussianProcessClassifier(kernel=kernel).fit(X, y)
+    assert_array_equal(gpc.predict(X),
+                       gpc.predict_proba(X)[:, 1] >= 0.5)
 
 
-def test_lml_improving():
+@pytest.mark.parametrize('kernel', non_fixed_kernels)
+def test_lml_improving(kernel):
     # Test that hyperparameter-tuning improves log-marginal likelihood.
-    for kernel in kernels:
-        if kernel == fixed_kernel:
-            continue
-        gpc = GaussianProcessClassifier(kernel=kernel).fit(X, y)
-        assert_greater(gpc.log_marginal_likelihood(gpc.kernel_.theta),
-                       gpc.log_marginal_likelihood(kernel.theta))
+    gpc = GaussianProcessClassifier(kernel=kernel).fit(X, y)
+    assert_greater(gpc.log_marginal_likelihood(gpc.kernel_.theta),
+                   gpc.log_marginal_likelihood(kernel.theta))
 
 
-def test_lml_precomputed():
+@pytest.mark.parametrize('kernel', kernels)
+def test_lml_precomputed(kernel):
     # Test that lml of optimized kernel is stored correctly.
-    for kernel in kernels:
-        gpc = GaussianProcessClassifier(kernel=kernel).fit(X, y)
-        assert_almost_equal(gpc.log_marginal_likelihood(gpc.kernel_.theta),
-                            gpc.log_marginal_likelihood(), 7)
+    gpc = GaussianProcessClassifier(kernel=kernel).fit(X, y)
+    assert_almost_equal(gpc.log_marginal_likelihood(gpc.kernel_.theta),
+                        gpc.log_marginal_likelihood(), 7)
 
 
-def test_converged_to_local_maximum():
+@pytest.mark.parametrize('kernel', non_fixed_kernels)
+def test_converged_to_local_maximum(kernel):
     # Test that we are in local maximum after hyperparameter-optimization.
-    for kernel in kernels:
-        if kernel == fixed_kernel:
-            continue
-        gpc = GaussianProcessClassifier(kernel=kernel).fit(X, y)
+    gpc = GaussianProcessClassifier(kernel=kernel).fit(X, y)
 
-        lml, lml_gradient = \
-            gpc.log_marginal_likelihood(gpc.kernel_.theta, True)
+    lml, lml_gradient = \
+        gpc.log_marginal_likelihood(gpc.kernel_.theta, True)
 
-        assert_true(np.all((np.abs(lml_gradient) < 1e-4) |
-                           (gpc.kernel_.theta == gpc.kernel_.bounds[:, 0]) |
-                           (gpc.kernel_.theta == gpc.kernel_.bounds[:, 1])))
+    assert_true(np.all((np.abs(lml_gradient) < 1e-4) |
+                       (gpc.kernel_.theta == gpc.kernel_.bounds[:, 0]) |
+                       (gpc.kernel_.theta == gpc.kernel_.bounds[:, 1])))
 
 
-def test_lml_gradient():
+@pytest.mark.parametrize('kernel', kernels)
+def test_lml_gradient(kernel):
     # Compare analytic and numeric gradient of log marginal likelihood.
-    for kernel in kernels:
-        gpc = GaussianProcessClassifier(kernel=kernel).fit(X, y)
+    gpc = GaussianProcessClassifier(kernel=kernel).fit(X, y)
 
-        lml, lml_gradient = gpc.log_marginal_likelihood(kernel.theta, True)
-        lml_gradient_approx = \
-            approx_fprime(kernel.theta,
-                          lambda theta: gpc.log_marginal_likelihood(theta,
-                                                                    False),
-                          1e-10)
+    lml, lml_gradient = gpc.log_marginal_likelihood(kernel.theta, True)
+    lml_gradient_approx = \
+        approx_fprime(kernel.theta,
+                      lambda theta: gpc.log_marginal_likelihood(theta,
+                                                                False),
+                      1e-10)
 
-        assert_almost_equal(lml_gradient, lml_gradient_approx, 3)
+    assert_almost_equal(lml_gradient, lml_gradient_approx, 3)
 
 
 def test_random_starts():
@@ -110,7 +110,8 @@ def test_random_starts():
         last_lml = lml
 
 
-def test_custom_optimizer():
+@pytest.mark.parametrize('kernel', non_fixed_kernels)
+def test_custom_optimizer(kernel):
     # Test that GPC can use externally defined optimizers.
     # Define a dummy optimizer that simply tests 50 random hyperparameters
     def optimizer(obj_func, initial_theta, bounds):
@@ -125,38 +126,35 @@ def optimizer(obj_func, initial_theta, bounds):
                 theta_opt, func_min = theta, f
         return theta_opt, func_min
 
-    for kernel in kernels:
-        if kernel == fixed_kernel:
-            continue
-        gpc = GaussianProcessClassifier(kernel=kernel, optimizer=optimizer)
-        gpc.fit(X, y_mc)
-        # Checks that optimizer improved marginal likelihood
-        assert_greater(gpc.log_marginal_likelihood(gpc.kernel_.theta),
-                       gpc.log_marginal_likelihood(kernel.theta))
+    gpc = GaussianProcessClassifier(kernel=kernel, optimizer=optimizer)
+    gpc.fit(X, y_mc)
+    # Checks that optimizer improved marginal likelihood
+    assert_greater(gpc.log_marginal_likelihood(gpc.kernel_.theta),
+                   gpc.log_marginal_likelihood(kernel.theta))
 
 
-def test_multi_class():
+@pytest.mark.parametrize('kernel', kernels)
+def test_multi_class(kernel):
     # Test GPC for multi-class classification problems.
-    for kernel in kernels:
-        gpc = GaussianProcessClassifier(kernel=kernel)
-        gpc.fit(X, y_mc)
+    gpc = GaussianProcessClassifier(kernel=kernel)
+    gpc.fit(X, y_mc)
 
-        y_prob = gpc.predict_proba(X2)
-        assert_almost_equal(y_prob.sum(1), 1)
+    y_prob = gpc.predict_proba(X2)
+    assert_almost_equal(y_prob.sum(1), 1)
 
-        y_pred = gpc.predict(X2)
-        assert_array_equal(np.argmax(y_prob, 1), y_pred)
+    y_pred = gpc.predict(X2)
+    assert_array_equal(np.argmax(y_prob, 1), y_pred)
 
 
-def test_multi_class_n_jobs():
+@pytest.mark.parametrize('kernel', kernels)
+def test_multi_class_n_jobs(kernel):
     # Test that multi-class GPC produces identical results with n_jobs>1.
-    for kernel in kernels:
-        gpc = GaussianProcessClassifier(kernel=kernel)
-        gpc.fit(X, y_mc)
+    gpc = GaussianProcessClassifier(kernel=kernel)
+    gpc.fit(X, y_mc)
 
-        gpc_2 = GaussianProcessClassifier(kernel=kernel, n_jobs=2)
-        gpc_2.fit(X, y_mc)
+    gpc_2 = GaussianProcessClassifier(kernel=kernel, n_jobs=2)
+    gpc_2.fit(X, y_mc)
 
-        y_prob = gpc.predict_proba(X2)
-        y_prob_2 = gpc_2.predict_proba(X2)
-        assert_almost_equal(y_prob, y_prob_2)
+    y_prob = gpc.predict_proba(X2)
+    y_prob_2 = gpc_2.predict_proba(X2)
+    assert_almost_equal(y_prob, y_prob_2)
diff --git a/sklearn/gaussian_process/tests/test_gpr.py b/sklearn/gaussian_process/tests/test_gpr.py
index 602b2b88ae9c..18f82b00fb7f 100644
--- a/sklearn/gaussian_process/tests/test_gpr.py
+++ b/sklearn/gaussian_process/tests/test_gpr.py
@@ -7,6 +7,8 @@
 
 from scipy.optimize import approx_fprime
 
+import pytest
+
 from sklearn.gaussian_process import GaussianProcessRegressor
 from sklearn.gaussian_process.kernels \
     import RBF, ConstantKernel as C, WhiteKernel
@@ -37,110 +39,106 @@ def f(x):
            C(0.1, (1e-2, 1e2)) *
            RBF(length_scale=1.0, length_scale_bounds=(1e-3, 1e3)) +
            C(1e-5, (1e-5, 1e2))]
+non_fixed_kernels = [kernel for kernel in kernels
+                     if kernel != fixed_kernel]
 
 
-def test_gpr_interpolation():
+@pytest.mark.parametrize('kernel', kernels)
+def test_gpr_interpolation(kernel):
     # Test the interpolating property for different kernels.
-    for kernel in kernels:
-        gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
-        y_pred, y_cov = gpr.predict(X, return_cov=True)
+    gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
+    y_pred, y_cov = gpr.predict(X, return_cov=True)
 
-        assert_almost_equal(y_pred, y)
-        assert_almost_equal(np.diag(y_cov), 0.)
+    assert_almost_equal(y_pred, y)
+    assert_almost_equal(np.diag(y_cov), 0.)
 
 
-def test_lml_improving():
+@pytest.mark.parametrize('kernel', non_fixed_kernels)
+def test_lml_improving(kernel):
     # Test that hyperparameter-tuning improves log-marginal likelihood.
-    for kernel in kernels:
-        if kernel == fixed_kernel:
-            continue
-        gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
-        assert_greater(gpr.log_marginal_likelihood(gpr.kernel_.theta),
-                       gpr.log_marginal_likelihood(kernel.theta))
+    gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
+    assert_greater(gpr.log_marginal_likelihood(gpr.kernel_.theta),
+                   gpr.log_marginal_likelihood(kernel.theta))
 
 
-def test_lml_precomputed():
+@pytest.mark.parametrize('kernel', kernels)
+def test_lml_precomputed(kernel):
     # Test that lml of optimized kernel is stored correctly.
-    for kernel in kernels:
-        gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
-        assert_equal(gpr.log_marginal_likelihood(gpr.kernel_.theta),
-                     gpr.log_marginal_likelihood())
+    gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
+    assert_equal(gpr.log_marginal_likelihood(gpr.kernel_.theta),
+                 gpr.log_marginal_likelihood())
 
 
-def test_converged_to_local_maximum():
+@pytest.mark.parametrize('kernel', non_fixed_kernels)
+def test_converged_to_local_maximum(kernel):
     # Test that we are in local maximum after hyperparameter-optimization.
-    for kernel in kernels:
-        if kernel == fixed_kernel:
-            continue
-        gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
+    gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
 
-        lml, lml_gradient = \
-            gpr.log_marginal_likelihood(gpr.kernel_.theta, True)
+    lml, lml_gradient = \
+        gpr.log_marginal_likelihood(gpr.kernel_.theta, True)
 
-        assert_true(np.all((np.abs(lml_gradient) < 1e-4) |
-                           (gpr.kernel_.theta == gpr.kernel_.bounds[:, 0]) |
-                           (gpr.kernel_.theta == gpr.kernel_.bounds[:, 1])))
+    assert_true(np.all((np.abs(lml_gradient) < 1e-4) |
+                       (gpr.kernel_.theta == gpr.kernel_.bounds[:, 0]) |
+                       (gpr.kernel_.theta == gpr.kernel_.bounds[:, 1])))
 
 
-def test_solution_inside_bounds():
+@pytest.mark.parametrize('kernel', non_fixed_kernels)
+def test_solution_inside_bounds(kernel):
     # Test that hyperparameter-optimization remains in bounds#
-    for kernel in kernels:
-        if kernel == fixed_kernel:
-            continue
-        gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
+    gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
 
-        bounds = gpr.kernel_.bounds
-        max_ = np.finfo(gpr.kernel_.theta.dtype).max
-        tiny = 1e-10
-        bounds[~np.isfinite(bounds[:, 1]), 1] = max_
+    bounds = gpr.kernel_.bounds
+    max_ = np.finfo(gpr.kernel_.theta.dtype).max
+    tiny = 1e-10
+    bounds[~np.isfinite(bounds[:, 1]), 1] = max_
 
-        assert_array_less(bounds[:, 0], gpr.kernel_.theta + tiny)
-        assert_array_less(gpr.kernel_.theta, bounds[:, 1] + tiny)
+    assert_array_less(bounds[:, 0], gpr.kernel_.theta + tiny)
+    assert_array_less(gpr.kernel_.theta, bounds[:, 1] + tiny)
 
 
-def test_lml_gradient():
+@pytest.mark.parametrize('kernel', kernels)
+def test_lml_gradient(kernel):
     # Compare analytic and numeric gradient of log marginal likelihood.
-    for kernel in kernels:
-        gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
+    gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
 
-        lml, lml_gradient = gpr.log_marginal_likelihood(kernel.theta, True)
-        lml_gradient_approx = \
-            approx_fprime(kernel.theta,
-                          lambda theta: gpr.log_marginal_likelihood(theta,
-                                                                    False),
-                          1e-10)
+    lml, lml_gradient = gpr.log_marginal_likelihood(kernel.theta, True)
+    lml_gradient_approx = \
+        approx_fprime(kernel.theta,
+                      lambda theta: gpr.log_marginal_likelihood(theta,
+                                                                False),
+                      1e-10)
 
-        assert_almost_equal(lml_gradient, lml_gradient_approx, 3)
+    assert_almost_equal(lml_gradient, lml_gradient_approx, 3)
 
 
-def test_prior():
+@pytest.mark.parametrize('kernel', kernels)
+def test_prior(kernel):
     # Test that GP prior has mean 0 and identical variances.
-    for kernel in kernels:
-        gpr = GaussianProcessRegressor(kernel=kernel)
+    gpr = GaussianProcessRegressor(kernel=kernel)
 
-        y_mean, y_cov = gpr.predict(X, return_cov=True)
+    y_mean, y_cov = gpr.predict(X, return_cov=True)
 
-        assert_almost_equal(y_mean, 0, 5)
-        if len(gpr.kernel.theta) > 1:
-            # XXX: quite hacky, works only for current kernels
-            assert_almost_equal(np.diag(y_cov), np.exp(kernel.theta[0]), 5)
-        else:
-            assert_almost_equal(np.diag(y_cov), 1, 5)
+    assert_almost_equal(y_mean, 0, 5)
+    if len(gpr.kernel.theta) > 1:
+        # XXX: quite hacky, works only for current kernels
+        assert_almost_equal(np.diag(y_cov), np.exp(kernel.theta[0]), 5)
+    else:
+        assert_almost_equal(np.diag(y_cov), 1, 5)
 
 
-def test_sample_statistics():
+@pytest.mark.parametrize('kernel', kernels)
+def test_sample_statistics(kernel):
     # Test that statistics of samples drawn from GP are correct.
-    for kernel in kernels:
-        gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
+    gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
 
-        y_mean, y_cov = gpr.predict(X2, return_cov=True)
+    y_mean, y_cov = gpr.predict(X2, return_cov=True)
 
-        samples = gpr.sample_y(X2, 300000)
+    samples = gpr.sample_y(X2, 300000)
 
-        # More digits accuracy would require many more samples
-        assert_almost_equal(y_mean, np.mean(samples, 1), 1)
-        assert_almost_equal(np.diag(y_cov) / np.diag(y_cov).max(),
-                            np.var(samples, 1) / np.diag(y_cov).max(), 1)
+    # More digits accuracy would require many more samples
+    assert_almost_equal(y_mean, np.mean(samples, 1), 1)
+    assert_almost_equal(np.diag(y_cov) / np.diag(y_cov).max(),
+                        np.var(samples, 1) / np.diag(y_cov).max(), 1)
 
 
 def test_no_optimizer():
@@ -150,13 +148,13 @@ def test_no_optimizer():
     assert_equal(np.exp(gpr.kernel_.theta), 1.0)
 
 
-def test_predict_cov_vs_std():
+@pytest.mark.parametrize('kernel', kernels)
+def test_predict_cov_vs_std(kernel):
     # Test that predicted std.-dev. is consistent with cov's diagonal.
-    for kernel in kernels:
-        gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
-        y_mean, y_cov = gpr.predict(X2, return_cov=True)
-        y_mean, y_std = gpr.predict(X2, return_std=True)
-        assert_almost_equal(np.sqrt(np.diag(y_cov)), y_std)
+    gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
+    y_mean, y_cov = gpr.predict(X2, return_cov=True)
+    y_mean, y_std = gpr.predict(X2, return_std=True)
+    assert_almost_equal(np.sqrt(np.diag(y_cov)), y_std)
 
 
 def test_anisotropic_kernel():
@@ -197,32 +195,33 @@ def test_random_starts():
         last_lml = lml
 
 
-def test_y_normalization():
+@pytest.mark.parametrize('kernel', kernels)
+def test_y_normalization(kernel):
     # Test normalization of the target values in GP
 
     # Fitting non-normalizing GP on normalized y and fitting normalizing GP
     # on unnormalized y should yield identical results
     y_mean = y.mean(0)
     y_norm = y - y_mean
-    for kernel in kernels:
-        # Fit non-normalizing GP on normalized y
-        gpr = GaussianProcessRegressor(kernel=kernel)
-        gpr.fit(X, y_norm)
-        # Fit normalizing GP on unnormalized y
-        gpr_norm = GaussianProcessRegressor(kernel=kernel, normalize_y=True)
-        gpr_norm.fit(X, y)
 
-        # Compare predicted mean, std-devs and covariances
-        y_pred, y_pred_std = gpr.predict(X2, return_std=True)
-        y_pred = y_mean + y_pred
-        y_pred_norm, y_pred_std_norm = gpr_norm.predict(X2, return_std=True)
+    # Fit non-normalizing GP on normalized y
+    gpr = GaussianProcessRegressor(kernel=kernel)
+    gpr.fit(X, y_norm)
+    # Fit normalizing GP on unnormalized y
+    gpr_norm = GaussianProcessRegressor(kernel=kernel, normalize_y=True)
+    gpr_norm.fit(X, y)
+
+    # Compare predicted mean, std-devs and covariances
+    y_pred, y_pred_std = gpr.predict(X2, return_std=True)
+    y_pred = y_mean + y_pred
+    y_pred_norm, y_pred_std_norm = gpr_norm.predict(X2, return_std=True)
 
-        assert_almost_equal(y_pred, y_pred_norm)
-        assert_almost_equal(y_pred_std, y_pred_std_norm)
+    assert_almost_equal(y_pred, y_pred_norm)
+    assert_almost_equal(y_pred_std, y_pred_std_norm)
 
-        _, y_cov = gpr.predict(X2, return_cov=True)
-        _, y_cov_norm = gpr_norm.predict(X2, return_cov=True)
-        assert_almost_equal(y_cov, y_cov_norm)
+    _, y_cov = gpr.predict(X2, return_cov=True)
+    _, y_cov_norm = gpr_norm.predict(X2, return_cov=True)
+    assert_almost_equal(y_cov, y_cov_norm)
 
 
 def test_y_multioutput():
@@ -268,7 +267,8 @@ def test_y_multioutput():
         assert_almost_equal(gpr.kernel_.theta, gpr_2d.kernel_.theta, 4)
 
 
-def test_custom_optimizer():
+@pytest.mark.parametrize('kernel', non_fixed_kernels)
+def test_custom_optimizer(kernel):
     # Test that GPR can use externally defined optimizers.
     # Define a dummy optimizer that simply tests 50 random hyperparameters
     def optimizer(obj_func, initial_theta, bounds):
@@ -283,14 +283,11 @@ def optimizer(obj_func, initial_theta, bounds):
                 theta_opt, func_min = theta, f
         return theta_opt, func_min
 
-    for kernel in kernels:
-        if kernel == fixed_kernel:
-            continue
-        gpr = GaussianProcessRegressor(kernel=kernel, optimizer=optimizer)
-        gpr.fit(X, y)
-        # Checks that optimizer improved marginal likelihood
-        assert_greater(gpr.log_marginal_likelihood(gpr.kernel_.theta),
-                       gpr.log_marginal_likelihood(gpr.kernel.theta))
+    gpr = GaussianProcessRegressor(kernel=kernel, optimizer=optimizer)
+    gpr.fit(X, y)
+    # Checks that optimizer improved marginal likelihood
+    assert_greater(gpr.log_marginal_likelihood(gpr.kernel_.theta),
+                   gpr.log_marginal_likelihood(gpr.kernel.theta))
 
 
 def test_gpr_correct_error_message():
@@ -306,30 +303,28 @@ def test_gpr_correct_error_message():
                          % kernel, gpr.fit, X, y)
 
 
-def test_duplicate_input():
+@pytest.mark.parametrize('kernel', kernels)
+def test_duplicate_input(kernel):
     # Test GPR can handle two different output-values for the same input.
-    for kernel in kernels:
-        gpr_equal_inputs = \
-            GaussianProcessRegressor(kernel=kernel, alpha=1e-2)
-        gpr_similar_inputs = \
-            GaussianProcessRegressor(kernel=kernel, alpha=1e-2)
+    gpr_equal_inputs = GaussianProcessRegressor(kernel=kernel, alpha=1e-2)
+    gpr_similar_inputs = GaussianProcessRegressor(kernel=kernel, alpha=1e-2)
 
-        X_ = np.vstack((X, X[0]))
-        y_ = np.hstack((y, y[0] + 1))
-        gpr_equal_inputs.fit(X_, y_)
+    X_ = np.vstack((X, X[0]))
+    y_ = np.hstack((y, y[0] + 1))
+    gpr_equal_inputs.fit(X_, y_)
 
-        X_ = np.vstack((X, X[0] + 1e-15))
-        y_ = np.hstack((y, y[0] + 1))
-        gpr_similar_inputs.fit(X_, y_)
+    X_ = np.vstack((X, X[0] + 1e-15))
+    y_ = np.hstack((y, y[0] + 1))
+    gpr_similar_inputs.fit(X_, y_)
 
-        X_test = np.linspace(0, 10, 100)[:, None]
-        y_pred_equal, y_std_equal = \
-            gpr_equal_inputs.predict(X_test, return_std=True)
-        y_pred_similar, y_std_similar = \
-            gpr_similar_inputs.predict(X_test, return_std=True)
+    X_test = np.linspace(0, 10, 100)[:, None]
+    y_pred_equal, y_std_equal = \
+        gpr_equal_inputs.predict(X_test, return_std=True)
+    y_pred_similar, y_std_similar = \
+        gpr_similar_inputs.predict(X_test, return_std=True)
 
-        assert_almost_equal(y_pred_equal, y_pred_similar)
-        assert_almost_equal(y_std_equal, y_std_similar)
+    assert_almost_equal(y_pred_equal, y_pred_similar)
+    assert_almost_equal(y_std_equal, y_std_similar)
 
 
 def test_no_fit_default_predict():
@@ -348,19 +343,20 @@ def test_no_fit_default_predict():
     assert_array_almost_equal(y_cov1, y_cov2)
 
 
-def test_K_inv_reset():
+@pytest.mark.parametrize('kernel', kernels)
+def test_K_inv_reset(kernel):
     y2 = f(X2).ravel()
-    for kernel in kernels:
-        # Test that self._K_inv is reset after a new fit
-        gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
-        assert_true(hasattr(gpr, '_K_inv'))
-        assert_true(gpr._K_inv is None)
-        gpr.predict(X, return_std=True)
-        assert_true(gpr._K_inv is not None)
-        gpr.fit(X2, y2)
-        assert_true(gpr._K_inv is None)
-        gpr.predict(X2, return_std=True)
-        gpr2 = GaussianProcessRegressor(kernel=kernel).fit(X2, y2)
-        gpr2.predict(X2, return_std=True)
-        # the value of K_inv should be independent of the first fit
-        assert_array_equal(gpr._K_inv, gpr2._K_inv)
+
+    # Test that self._K_inv is reset after a new fit
+    gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
+    assert_true(hasattr(gpr, '_K_inv'))
+    assert_true(gpr._K_inv is None)
+    gpr.predict(X, return_std=True)
+    assert_true(gpr._K_inv is not None)
+    gpr.fit(X2, y2)
+    assert_true(gpr._K_inv is None)
+    gpr.predict(X2, return_std=True)
+    gpr2 = GaussianProcessRegressor(kernel=kernel).fit(X2, y2)
+    gpr2.predict(X2, return_std=True)
+    # the value of K_inv should be independent of the first fit
+    assert_array_equal(gpr._K_inv, gpr2._K_inv)
diff --git a/sklearn/gaussian_process/tests/test_kernels.py b/sklearn/gaussian_process/tests/test_kernels.py
index 8c40139480e1..d5949f60ff6f 100644
--- a/sklearn/gaussian_process/tests/test_kernels.py
+++ b/sklearn/gaussian_process/tests/test_kernels.py
@@ -3,6 +3,7 @@
 # Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
 # License: BSD 3 clause
 
+import pytest
 import numpy as np
 
 from sklearn.utils.fixes import signature
@@ -47,98 +48,101 @@
     kernels.append(PairwiseKernel(gamma=1.0, metric=metric))
 
 
-def test_kernel_gradient():
+@pytest.mark.parametrize('kernel', kernels)
+def test_kernel_gradient(kernel):
     # Compare analytic and numeric gradient of kernels.
-    for kernel in kernels:
-        K, K_gradient = kernel(X, eval_gradient=True)
+    K, K_gradient = kernel(X, eval_gradient=True)
 
-        assert_equal(K_gradient.shape[0], X.shape[0])
-        assert_equal(K_gradient.shape[1], X.shape[0])
-        assert_equal(K_gradient.shape[2], kernel.theta.shape[0])
+    assert_equal(K_gradient.shape[0], X.shape[0])
+    assert_equal(K_gradient.shape[1], X.shape[0])
+    assert_equal(K_gradient.shape[2], kernel.theta.shape[0])
 
-        def eval_kernel_for_theta(theta):
-            kernel_clone = kernel.clone_with_theta(theta)
-            K = kernel_clone(X, eval_gradient=False)
-            return K
+    def eval_kernel_for_theta(theta):
+        kernel_clone = kernel.clone_with_theta(theta)
+        K = kernel_clone(X, eval_gradient=False)
+        return K
 
-        K_gradient_approx = \
-            _approx_fprime(kernel.theta, eval_kernel_for_theta, 1e-10)
+    K_gradient_approx = \
+        _approx_fprime(kernel.theta, eval_kernel_for_theta, 1e-10)
 
-        assert_almost_equal(K_gradient, K_gradient_approx, 4)
+    assert_almost_equal(K_gradient, K_gradient_approx, 4)
 
 
-def test_kernel_theta():
+@pytest.mark.parametrize(
+        'kernel',
+        [kernel for kernel in kernels
+         # skip non-basic kernels
+         if not (isinstance(kernel, KernelOperator)
+                 or isinstance(kernel, Exponentiation))])
+def test_kernel_theta(kernel):
     # Check that parameter vector theta of kernel is set correctly.
-    for kernel in kernels:
-        if isinstance(kernel, KernelOperator) \
-           or isinstance(kernel, Exponentiation):  # skip non-basic kernels
-            continue
-        theta = kernel.theta
-        _, K_gradient = kernel(X, eval_gradient=True)
-
-        # Determine kernel parameters that contribute to theta
-        init_sign = signature(kernel.__class__.__init__).parameters.values()
-        args = [p.name for p in init_sign if p.name != 'self']
-        theta_vars = map(lambda s: s[0:-len("_bounds")],
-                         filter(lambda s: s.endswith("_bounds"), args))
-        assert_equal(
-            set(hyperparameter.name
-                for hyperparameter in kernel.hyperparameters),
-            set(theta_vars))
-
-        # Check that values returned in theta are consistent with
-        # hyperparameter values (being their logarithms)
-        for i, hyperparameter in enumerate(kernel.hyperparameters):
-            assert_equal(theta[i],
-                         np.log(getattr(kernel, hyperparameter.name)))
-
-        # Fixed kernel parameters must be excluded from theta and gradient.
-        for i, hyperparameter in enumerate(kernel.hyperparameters):
-            # create copy with certain hyperparameter fixed
-            params = kernel.get_params()
-            params[hyperparameter.name + "_bounds"] = "fixed"
-            kernel_class = kernel.__class__
-            new_kernel = kernel_class(**params)
-            # Check that theta and K_gradient are identical with the fixed
-            # dimension left out
-            _, K_gradient_new = new_kernel(X, eval_gradient=True)
-            assert_equal(theta.shape[0], new_kernel.theta.shape[0] + 1)
-            assert_equal(K_gradient.shape[2], K_gradient_new.shape[2] + 1)
-            if i > 0:
-                assert_equal(theta[:i], new_kernel.theta[:i])
-                assert_array_equal(K_gradient[..., :i],
-                                   K_gradient_new[..., :i])
-            if i + 1 < len(kernel.hyperparameters):
-                assert_equal(theta[i + 1:], new_kernel.theta[i:])
-                assert_array_equal(K_gradient[..., i + 1:],
-                                   K_gradient_new[..., i:])
-
-        # Check that values of theta are modified correctly
-        for i, hyperparameter in enumerate(kernel.hyperparameters):
-            theta[i] = np.log(42)
-            kernel.theta = theta
-            assert_almost_equal(getattr(kernel, hyperparameter.name), 42)
-
-            setattr(kernel, hyperparameter.name, 43)
-            assert_almost_equal(kernel.theta[i], np.log(43))
-
-
-def test_auto_vs_cross():
+    theta = kernel.theta
+    _, K_gradient = kernel(X, eval_gradient=True)
+
+    # Determine kernel parameters that contribute to theta
+    init_sign = signature(kernel.__class__.__init__).parameters.values()
+    args = [p.name for p in init_sign if p.name != 'self']
+    theta_vars = map(lambda s: s[0:-len("_bounds")],
+                     filter(lambda s: s.endswith("_bounds"), args))
+    assert_equal(
+        set(hyperparameter.name
+            for hyperparameter in kernel.hyperparameters),
+        set(theta_vars))
+
+    # Check that values returned in theta are consistent with
+    # hyperparameter values (being their logarithms)
+    for i, hyperparameter in enumerate(kernel.hyperparameters):
+        assert_equal(theta[i],
+                     np.log(getattr(kernel, hyperparameter.name)))
+
+    # Fixed kernel parameters must be excluded from theta and gradient.
+    for i, hyperparameter in enumerate(kernel.hyperparameters):
+        # create copy with certain hyperparameter fixed
+        params = kernel.get_params()
+        params[hyperparameter.name + "_bounds"] = "fixed"
+        kernel_class = kernel.__class__
+        new_kernel = kernel_class(**params)
+        # Check that theta and K_gradient are identical with the fixed
+        # dimension left out
+        _, K_gradient_new = new_kernel(X, eval_gradient=True)
+        assert_equal(theta.shape[0], new_kernel.theta.shape[0] + 1)
+        assert_equal(K_gradient.shape[2], K_gradient_new.shape[2] + 1)
+        if i > 0:
+            assert_equal(theta[:i], new_kernel.theta[:i])
+            assert_array_equal(K_gradient[..., :i],
+                               K_gradient_new[..., :i])
+        if i + 1 < len(kernel.hyperparameters):
+            assert_equal(theta[i + 1:], new_kernel.theta[i:])
+            assert_array_equal(K_gradient[..., i + 1:],
+                               K_gradient_new[..., i:])
+
+    # Check that values of theta are modified correctly
+    for i, hyperparameter in enumerate(kernel.hyperparameters):
+        theta[i] = np.log(42)
+        kernel.theta = theta
+        assert_almost_equal(getattr(kernel, hyperparameter.name), 42)
+
+        setattr(kernel, hyperparameter.name, 43)
+        assert_almost_equal(kernel.theta[i], np.log(43))
+
+
+@pytest.mark.parametrize('kernel',
+                         [kernel for kernel in kernels
+                          # Identity is not satisfied on diagonal
+                          if kernel != kernel_white])
+def test_auto_vs_cross(kernel):
     # Auto-correlation and cross-correlation should be consistent.
-    for kernel in kernels:
-        if kernel == kernel_white:
-            continue  # Identity is not satisfied on diagonal
-        K_auto = kernel(X)
-        K_cross = kernel(X, X)
-        assert_almost_equal(K_auto, K_cross, 5)
+    K_auto = kernel(X)
+    K_cross = kernel(X, X)
+    assert_almost_equal(K_auto, K_cross, 5)
 
 
-def test_kernel_diag():
+@pytest.mark.parametrize('kernel', kernels)
+def test_kernel_diag(kernel):
     # Test that diag method of kernel returns consistent results.
-    for kernel in kernels:
-        K_call_diag = np.diag(kernel(X))
-        K_diag = kernel.diag(X)
-        assert_almost_equal(K_call_diag, K_diag, 5)
+    K_call_diag = np.diag(kernel(X))
+    K_diag = kernel.diag(X)
+    assert_almost_equal(K_call_diag, K_diag, 5)
 
 
 def test_kernel_operator_commutative():
@@ -173,13 +177,13 @@ def test_kernel_anisotropic():
     assert_array_equal(kernel.k2.length_scale, [1.0, 4.0])
 
 
-def test_kernel_stationary():
+@pytest.mark.parametrize('kernel',
+                         [kernel for kernel in kernels
+                          if kernel.is_stationary()])
+def test_kernel_stationary(kernel):
     # Test stationarity of kernels.
-    for kernel in kernels:
-        if not kernel.is_stationary():
-            continue
-        K = kernel(X, X + 1)
-        assert_almost_equal(K[0, 0], np.diag(K))
+    K = kernel(X, X + 1)
+    assert_almost_equal(K[0, 0], np.diag(K))
 
 
 def check_hyperparameters_equal(kernel1, kernel2):
@@ -191,24 +195,25 @@ def check_hyperparameters_equal(kernel1, kernel2):
             assert_equal(attr_value1, attr_value2)
 
 
-def test_kernel_clone():
+@pytest.mark.parametrize("kernel", kernels)
+def test_kernel_clone(kernel):
     # Test that sklearn's clone works correctly on kernels.
-    for kernel in kernels:
-        kernel_cloned = clone(kernel)
+    kernel_cloned = clone(kernel)
 
-        # XXX: Should this be fixed?
-        # This differs from the sklearn's estimators equality check.
-        assert_equal(kernel, kernel_cloned)
-        assert_not_equal(id(kernel), id(kernel_cloned))
+    # XXX: Should this be fixed?
+    # This differs from the sklearn's estimators equality check.
+    assert_equal(kernel, kernel_cloned)
+    assert_not_equal(id(kernel), id(kernel_cloned))
 
-        # Check that all constructor parameters are equal.
-        assert_equal(kernel.get_params(), kernel_cloned.get_params())
+    # Check that all constructor parameters are equal.
+    assert_equal(kernel.get_params(), kernel_cloned.get_params())
 
-        # Check that all hyperparameters are equal.
-        yield check_hyperparameters_equal, kernel, kernel_cloned
+    # Check that all hyperparameters are equal.
+    check_hyperparameters_equal(kernel, kernel_cloned)
 
 
-def test_kernel_clone_after_set_params():
+@pytest.mark.parametrize('kernel', kernels)
+def test_kernel_clone_after_set_params(kernel):
     # This test is to verify that using set_params does not
     # break clone on kernels.
     # This used to break because in kernels such as the RBF, non-trivial
@@ -216,27 +221,25 @@ def test_kernel_clone_after_set_params():
     # See https://github.com/scikit-learn/scikit-learn/issues/6961
     # for more details.
     bounds = (1e-5, 1e5)
-    for kernel in kernels:
-        kernel_cloned = clone(kernel)
-        params = kernel.get_params()
-        # RationalQuadratic kernel is isotropic.
-        isotropic_kernels = (ExpSineSquared, RationalQuadratic)
-        if 'length_scale' in params and not isinstance(kernel,
-                                                       isotropic_kernels):
-            length_scale = params['length_scale']
-            if np.iterable(length_scale):
-                params['length_scale'] = length_scale[0]
-                params['length_scale_bounds'] = bounds
-            else:
-                params['length_scale'] = [length_scale] * 2
-                params['length_scale_bounds'] = bounds * 2
-            kernel_cloned.set_params(**params)
-            kernel_cloned_clone = clone(kernel_cloned)
-            assert_equal(kernel_cloned_clone.get_params(),
-                         kernel_cloned.get_params())
-            assert_not_equal(id(kernel_cloned_clone), id(kernel_cloned))
-            yield (check_hyperparameters_equal, kernel_cloned,
-                   kernel_cloned_clone)
+    kernel_cloned = clone(kernel)
+    params = kernel.get_params()
+    # RationalQuadratic kernel is isotropic.
+    isotropic_kernels = (ExpSineSquared, RationalQuadratic)
+    if 'length_scale' in params and not isinstance(kernel,
+                                                   isotropic_kernels):
+        length_scale = params['length_scale']
+        if np.iterable(length_scale):
+            params['length_scale'] = length_scale[0]
+            params['length_scale_bounds'] = bounds
+        else:
+            params['length_scale'] = [length_scale] * 2
+            params['length_scale_bounds'] = bounds * 2
+        kernel_cloned.set_params(**params)
+        kernel_cloned_clone = clone(kernel_cloned)
+        assert_equal(kernel_cloned_clone.get_params(),
+                     kernel_cloned.get_params())
+        assert_not_equal(id(kernel_cloned_clone), id(kernel_cloned))
+        check_hyperparameters_equal(kernel_cloned, kernel_cloned_clone)
 
 
 def test_matern_kernel():
@@ -258,63 +261,65 @@ def test_matern_kernel():
         assert_array_almost_equal(K1, K2)
 
 
-def test_kernel_versus_pairwise():
+@pytest.mark.parametrize("kernel", kernels)
+def test_kernel_versus_pairwise(kernel):
     # Check that GP kernels can also be used as pairwise kernels.
-    for kernel in kernels:
-        # Test auto-kernel
-        if kernel != kernel_white:
-            # For WhiteKernel: k(X) != k(X,X). This is assumed by
-            # pairwise_kernels
-            K1 = kernel(X)
-            K2 = pairwise_kernels(X, metric=kernel)
-            assert_array_almost_equal(K1, K2)
-
-        # Test cross-kernel
-        K1 = kernel(X, Y)
-        K2 = pairwise_kernels(X, Y, metric=kernel)
+
+    # Test auto-kernel
+    if kernel != kernel_white:
+        # For WhiteKernel: k(X) != k(X,X). This is assumed by
+        # pairwise_kernels
+        K1 = kernel(X)
+        K2 = pairwise_kernels(X, metric=kernel)
         assert_array_almost_equal(K1, K2)
 
+    # Test cross-kernel
+    K1 = kernel(X, Y)
+    K2 = pairwise_kernels(X, Y, metric=kernel)
+    assert_array_almost_equal(K1, K2)
 
-def test_set_get_params():
+
+@pytest.mark.parametrize("kernel", kernels)
+def test_set_get_params(kernel):
     # Check that set_params()/get_params() is consistent with kernel.theta.
-    for kernel in kernels:
-        # Test get_params()
-        index = 0
-        params = kernel.get_params()
-        for hyperparameter in kernel.hyperparameters:
-            if isinstance("string", type(hyperparameter.bounds)):
-                if hyperparameter.bounds == "fixed":
-                    continue
-            size = hyperparameter.n_elements
-            if size > 1:  # anisotropic kernels
-                assert_almost_equal(np.exp(kernel.theta[index:index + size]),
-                                    params[hyperparameter.name])
-                index += size
-            else:
-                assert_almost_equal(np.exp(kernel.theta[index]),
-                                    params[hyperparameter.name])
-                index += 1
-        # Test set_params()
-        index = 0
-        value = 10  # arbitrary value
-        for hyperparameter in kernel.hyperparameters:
-            if isinstance("string", type(hyperparameter.bounds)):
-                if hyperparameter.bounds == "fixed":
-                    continue
-            size = hyperparameter.n_elements
-            if size > 1:  # anisotropic kernels
-                kernel.set_params(**{hyperparameter.name: [value] * size})
-                assert_almost_equal(np.exp(kernel.theta[index:index + size]),
-                                    [value] * size)
-                index += size
-            else:
-                kernel.set_params(**{hyperparameter.name: value})
-                assert_almost_equal(np.exp(kernel.theta[index]), value)
-                index += 1
-
-
-def test_repr_kernels():
+
+    # Test get_params()
+    index = 0
+    params = kernel.get_params()
+    for hyperparameter in kernel.hyperparameters:
+        if isinstance("string", type(hyperparameter.bounds)):
+            if hyperparameter.bounds == "fixed":
+                continue
+        size = hyperparameter.n_elements
+        if size > 1:  # anisotropic kernels
+            assert_almost_equal(np.exp(kernel.theta[index:index + size]),
+                                params[hyperparameter.name])
+            index += size
+        else:
+            assert_almost_equal(np.exp(kernel.theta[index]),
+                                params[hyperparameter.name])
+            index += 1
+    # Test set_params()
+    index = 0
+    value = 10  # arbitrary value
+    for hyperparameter in kernel.hyperparameters:
+        if isinstance("string", type(hyperparameter.bounds)):
+            if hyperparameter.bounds == "fixed":
+                continue
+        size = hyperparameter.n_elements
+        if size > 1:  # anisotropic kernels
+            kernel.set_params(**{hyperparameter.name: [value] * size})
+            assert_almost_equal(np.exp(kernel.theta[index:index + size]),
+                                [value] * size)
+            index += size
+        else:
+            kernel.set_params(**{hyperparameter.name: value})
+            assert_almost_equal(np.exp(kernel.theta[index]), value)
+            index += 1
+
+
+@pytest.mark.parametrize("kernel", kernels)
+def test_repr_kernels(kernel):
     # Smoke-test for repr in kernels.
 
-    for kernel in kernels:
-        repr(kernel)
+    repr(kernel)
diff --git a/sklearn/grid_search.py b/sklearn/grid_search.py
deleted file mode 100644
index 6ba673d2fbf7..000000000000
--- a/sklearn/grid_search.py
+++ /dev/null
@@ -1,1046 +0,0 @@
-"""
-The :mod:`sklearn.grid_search` includes utilities to fine-tune the parameters
-of an estimator.
-"""
-from __future__ import print_function
-
-# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
-#         Gael Varoquaux <gael.varoquaux@normalesup.org>
-#         Andreas Mueller <amueller@ais.uni-bonn.de>
-#         Olivier Grisel <olivier.grisel@ensta.org>
-# License: BSD 3 clause
-
-from abc import ABCMeta, abstractmethod
-from collections import Mapping, namedtuple, Sized
-from functools import partial, reduce
-from itertools import product
-import operator
-import warnings
-
-import numpy as np
-
-from .base import BaseEstimator, is_classifier, clone
-from .base import MetaEstimatorMixin
-from .cross_validation import check_cv
-from .cross_validation import _fit_and_score
-from .externals.joblib import Parallel, delayed
-from .externals import six
-from .utils import check_random_state
-from .utils.random import sample_without_replacement
-from .utils.validation import _num_samples, indexable
-from .utils.metaestimators import if_delegate_has_method
-from .metrics.scorer import check_scoring
-
-
-__all__ = ['GridSearchCV', 'ParameterGrid', 'fit_grid_point',
-           'ParameterSampler', 'RandomizedSearchCV']
-
-
-warnings.warn("This module was deprecated in version 0.18 in favor of the "
-              "model_selection module into which all the refactored classes "
-              "and functions are moved. This module will be removed in 0.20.",
-              DeprecationWarning)
-
-
-class ParameterGrid(object):
-    """Grid of parameters with a discrete number of values for each.
-
-    .. deprecated:: 0.18
-        This module will be removed in 0.20.
-        Use :class:`sklearn.model_selection.ParameterGrid` instead.
-
-    Can be used to iterate over parameter value combinations with the
-    Python built-in function iter.
-
-    Read more in the :ref:`User Guide <grid_search>`.
-
-    Parameters
-    ----------
-    param_grid : dict of string to sequence, or sequence of such
-        The parameter grid to explore, as a dictionary mapping estimator
-        parameters to sequences of allowed values.
-
-        An empty dict signifies default parameters.
-
-        A sequence of dicts signifies a sequence of grids to search, and is
-        useful to avoid exploring parameter combinations that make no sense
-        or have no effect. See the examples below.
-
-    Examples
-    --------
-    >>> from sklearn.grid_search import ParameterGrid
-    >>> param_grid = {'a': [1, 2], 'b': [True, False]}
-    >>> list(ParameterGrid(param_grid)) == (
-    ...    [{'a': 1, 'b': True}, {'a': 1, 'b': False},
-    ...     {'a': 2, 'b': True}, {'a': 2, 'b': False}])
-    True
-
-    >>> grid = [{'kernel': ['linear']}, {'kernel': ['rbf'], 'gamma': [1, 10]}]
-    >>> list(ParameterGrid(grid)) == [{'kernel': 'linear'},
-    ...                               {'kernel': 'rbf', 'gamma': 1},
-    ...                               {'kernel': 'rbf', 'gamma': 10}]
-    True
-    >>> ParameterGrid(grid)[1] == {'kernel': 'rbf', 'gamma': 1}
-    True
-
-    See also
-    --------
-    :class:`GridSearchCV`:
-        uses ``ParameterGrid`` to perform a full parallelized parameter search.
-    """
-
-    def __init__(self, param_grid):
-        if isinstance(param_grid, Mapping):
-            # wrap dictionary in a singleton list to support either dict
-            # or list of dicts
-            param_grid = [param_grid]
-        self.param_grid = param_grid
-
-    def __iter__(self):
-        """Iterate over the points in the grid.
-
-        Returns
-        -------
-        params : iterator over dict of string to any
-            Yields dictionaries mapping each estimator parameter to one of its
-            allowed values.
-        """
-        for p in self.param_grid:
-            # Always sort the keys of a dictionary, for reproducibility
-            items = sorted(p.items())
-            if not items:
-                yield {}
-            else:
-                keys, values = zip(*items)
-                for v in product(*values):
-                    params = dict(zip(keys, v))
-                    yield params
-
-    def __len__(self):
-        """Number of points on the grid."""
-        # Product function that can handle iterables (np.product can't).
-        product = partial(reduce, operator.mul)
-        return sum(product(len(v) for v in p.values()) if p else 1
-                   for p in self.param_grid)
-
-    def __getitem__(self, ind):
-        """Get the parameters that would be ``ind``th in iteration
-
-        Parameters
-        ----------
-        ind : int
-            The iteration index
-
-        Returns
-        -------
-        params : dict of string to any
-            Equal to list(self)[ind]
-        """
-        # This is used to make discrete sampling without replacement memory
-        # efficient.
-        for sub_grid in self.param_grid:
-            # XXX: could memoize information used here
-            if not sub_grid:
-                if ind == 0:
-                    return {}
-                else:
-                    ind -= 1
-                    continue
-
-            # Reverse so most frequent cycling parameter comes first
-            keys, values_lists = zip(*sorted(sub_grid.items())[::-1])
-            sizes = [len(v_list) for v_list in values_lists]
-            total = np.product(sizes)
-
-            if ind >= total:
-                # Try the next grid
-                ind -= total
-            else:
-                out = {}
-                for key, v_list, n in zip(keys, values_lists, sizes):
-                    ind, offset = divmod(ind, n)
-                    out[key] = v_list[offset]
-                return out
-
-        raise IndexError('ParameterGrid index out of range')
-
-
-class ParameterSampler(object):
-    """Generator on parameters sampled from given distributions.
-
-    .. deprecated:: 0.18
-        This module will be removed in 0.20.
-        Use :class:`sklearn.model_selection.ParameterSampler` instead.
-
-    Non-deterministic iterable over random candidate combinations for hyper-
-    parameter search. If all parameters are presented as a list,
-    sampling without replacement is performed. If at least one parameter
-    is given as a distribution, sampling with replacement is used.
-    It is highly recommended to use continuous distributions for continuous
-    parameters.
-
-    Note that as of SciPy 0.12, the ``scipy.stats.distributions`` do not accept
-    a custom RNG instance and always use the singleton RNG from
-    ``numpy.random``. Hence setting ``random_state`` will not guarantee a
-    deterministic iteration whenever ``scipy.stats`` distributions are used to
-    define the parameter search space.
-
-    Read more in the :ref:`User Guide <grid_search>`.
-
-    Parameters
-    ----------
-    param_distributions : dict
-        Dictionary where the keys are parameters and values
-        are distributions from which a parameter is to be sampled.
-        Distributions either have to provide a ``rvs`` function
-        to sample from them, or can be given as a list of values,
-        where a uniform distribution is assumed.
-
-    n_iter : integer
-        Number of parameter settings that are produced.
-
-    random_state : int, RandomState instance or None, optional (default=None)
-        Pseudo random number generator state used for random uniform sampling
-        from lists of possible values instead of scipy.stats distributions.
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
-
-    Returns
-    -------
-    params : dict of string to any
-        **Yields** dictionaries mapping each estimator parameter to
-        as sampled value.
-
-    Examples
-    --------
-    >>> from sklearn.grid_search import ParameterSampler
-    >>> from scipy.stats.distributions import expon
-    >>> import numpy as np
-    >>> np.random.seed(0)
-    >>> param_grid = {'a':[1, 2], 'b': expon()}
-    >>> param_list = list(ParameterSampler(param_grid, n_iter=4))
-    >>> rounded_list = [dict((k, round(v, 6)) for (k, v) in d.items())
-    ...                 for d in param_list]
-    >>> rounded_list == [{'b': 0.89856, 'a': 1},
-    ...                  {'b': 0.923223, 'a': 1},
-    ...                  {'b': 1.878964, 'a': 2},
-    ...                  {'b': 1.038159, 'a': 2}]
-    True
-    """
-    def __init__(self, param_distributions, n_iter, random_state=None):
-        self.param_distributions = param_distributions
-        self.n_iter = n_iter
-        self.random_state = random_state
-
-    def __iter__(self):
-        # check if all distributions are given as lists
-        # in this case we want to sample without replacement
-        all_lists = np.all([not hasattr(v, "rvs")
-                            for v in self.param_distributions.values()])
-        rnd = check_random_state(self.random_state)
-
-        if all_lists:
-            # look up sampled parameter settings in parameter grid
-            param_grid = ParameterGrid(self.param_distributions)
-            grid_size = len(param_grid)
-
-            if grid_size < self.n_iter:
-                raise ValueError(
-                    "The total space of parameters %d is smaller "
-                    "than n_iter=%d." % (grid_size, self.n_iter)
-                    + " For exhaustive searches, use GridSearchCV.")
-            for i in sample_without_replacement(grid_size, self.n_iter,
-                                                random_state=rnd):
-                yield param_grid[i]
-
-        else:
-            # Always sort the keys of a dictionary, for reproducibility
-            items = sorted(self.param_distributions.items())
-            for _ in six.moves.range(self.n_iter):
-                params = dict()
-                for k, v in items:
-                    if hasattr(v, "rvs"):
-                        params[k] = v.rvs()
-                    else:
-                        params[k] = v[rnd.randint(len(v))]
-                yield params
-
-    def __len__(self):
-        """Number of points that will be sampled."""
-        return self.n_iter
-
-
-def fit_grid_point(X, y, estimator, parameters, train, test, scorer,
-                   verbose, error_score='raise', **fit_params):
-    """Run fit on one set of parameters.
-
-    .. deprecated:: 0.18
-        This module will be removed in 0.20.
-        Use :func:`sklearn.model_selection.fit_grid_point` instead.
-
-    Parameters
-    ----------
-    X : array-like, sparse matrix or list
-        Input data.
-
-    y : array-like or None
-        Targets for input data.
-
-    estimator : estimator object
-        A object of that type is instantiated for each grid point.
-        This is assumed to implement the scikit-learn estimator interface.
-        Either estimator needs to provide a ``score`` function,
-        or ``scoring`` must be passed.
-
-    parameters : dict
-        Parameters to be set on estimator for this grid point.
-
-    train : ndarray, dtype int or bool
-        Boolean mask or indices for training set.
-
-    test : ndarray, dtype int or bool
-        Boolean mask or indices for test set.
-
-    scorer : callable or None.
-        If provided must be a scorer callable object / function with signature
-        ``scorer(estimator, X, y)``.
-
-    verbose : int
-        Verbosity level.
-
-    **fit_params : kwargs
-        Additional parameter passed to the fit function of the estimator.
-
-    error_score : 'raise' (default) or numeric
-        Value to assign to the score if an error occurs in estimator fitting.
-        If set to 'raise', the error is raised. If a numeric value is given,
-        FitFailedWarning is raised. This parameter does not affect the refit
-        step, which will always raise the error.
-
-    Returns
-    -------
-    score : float
-        Score of this parameter setting on given training / test split.
-
-    parameters : dict
-        The parameters that have been evaluated.
-
-    n_samples_test : int
-        Number of test samples in this split.
-    """
-    score, n_samples_test, _ = _fit_and_score(estimator, X, y, scorer, train,
-                                              test, verbose, parameters,
-                                              fit_params, error_score)
-    return score, parameters, n_samples_test
-
-
-def _check_param_grid(param_grid):
-    if hasattr(param_grid, 'items'):
-        param_grid = [param_grid]
-
-    for p in param_grid:
-        for name, v in p.items():
-            if isinstance(v, np.ndarray) and v.ndim > 1:
-                raise ValueError("Parameter array should be one-dimensional.")
-
-            check = [isinstance(v, k) for k in (list, tuple, np.ndarray)]
-            if True not in check:
-                raise ValueError("Parameter values for parameter ({0}) need "
-                                 "to be a sequence.".format(name))
-
-            if len(v) == 0:
-                raise ValueError("Parameter values for parameter ({0}) need "
-                                 "to be a non-empty sequence.".format(name))
-
-
-class _CVScoreTuple (namedtuple('_CVScoreTuple',
-                                ('parameters',
-                                 'mean_validation_score',
-                                 'cv_validation_scores'))):
-    # A raw namedtuple is very memory efficient as it packs the attributes
-    # in a struct to get rid of the __dict__ of attributes in particular it
-    # does not copy the string for the keys on each instance.
-    # By deriving a namedtuple class just to introduce the __repr__ method we
-    # would also reintroduce the __dict__ on the instance. By telling the
-    # Python interpreter that this subclass uses static __slots__ instead of
-    # dynamic attributes. Furthermore we don't need any additional slot in the
-    # subclass so we set __slots__ to the empty tuple.
-    __slots__ = ()
-
-    def __repr__(self):
-        """Simple custom repr to summarize the main info"""
-        return "mean: {0:.5f}, std: {1:.5f}, params: {2}".format(
-            self.mean_validation_score,
-            np.std(self.cv_validation_scores),
-            self.parameters)
-
-
-class BaseSearchCV(six.with_metaclass(ABCMeta, BaseEstimator,
-                                      MetaEstimatorMixin)):
-    """Base class for hyper parameter search with cross-validation."""
-
-    @abstractmethod
-    def __init__(self, estimator, scoring=None,
-                 fit_params=None, n_jobs=1, iid=True,
-                 refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs',
-                 error_score='raise'):
-
-        self.scoring = scoring
-        self.estimator = estimator
-        self.n_jobs = n_jobs
-        self.fit_params = fit_params if fit_params is not None else {}
-        self.iid = iid
-        self.refit = refit
-        self.cv = cv
-        self.verbose = verbose
-        self.pre_dispatch = pre_dispatch
-        self.error_score = error_score
-
-    @property
-    def _estimator_type(self):
-        return self.estimator._estimator_type
-
-    @property
-    def classes_(self):
-        return self.best_estimator_.classes_
-
-    def score(self, X, y=None):
-        """Returns the score on the given data, if the estimator has been refit.
-
-        This uses the score defined by ``scoring`` where provided, and the
-        ``best_estimator_.score`` method otherwise.
-
-        Parameters
-        ----------
-        X : array-like, shape = [n_samples, n_features]
-            Input data, where n_samples is the number of samples and
-            n_features is the number of features.
-
-        y : array-like, shape = [n_samples] or [n_samples, n_output], optional
-            Target relative to X for classification or regression;
-            None for unsupervised learning.
-
-        Returns
-        -------
-        score : float
-
-        Notes
-        -----
-         * The long-standing behavior of this method changed in version 0.16.
-         * It no longer uses the metric provided by ``estimator.score`` if the
-           ``scoring`` parameter was set when fitting.
-
-        """
-        if self.scorer_ is None:
-            raise ValueError("No score function explicitly defined, "
-                             "and the estimator doesn't provide one %s"
-                             % self.best_estimator_)
-        return self.scorer_(self.best_estimator_, X, y)
-
-    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
-    def predict(self, X):
-        """Call predict on the estimator with the best found parameters.
-
-        Only available if ``refit=True`` and the underlying estimator supports
-        ``predict``.
-
-        Parameters
-        -----------
-        X : indexable, length n_samples
-            Must fulfill the input assumptions of the
-            underlying estimator.
-
-        """
-        return self.best_estimator_.predict(X)
-
-    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
-    def predict_proba(self, X):
-        """Call predict_proba on the estimator with the best found parameters.
-
-        Only available if ``refit=True`` and the underlying estimator supports
-        ``predict_proba``.
-
-        Parameters
-        -----------
-        X : indexable, length n_samples
-            Must fulfill the input assumptions of the
-            underlying estimator.
-
-        """
-        return self.best_estimator_.predict_proba(X)
-
-    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
-    def predict_log_proba(self, X):
-        """Call predict_log_proba on the estimator with the best found parameters.
-
-        Only available if ``refit=True`` and the underlying estimator supports
-        ``predict_log_proba``.
-
-        Parameters
-        -----------
-        X : indexable, length n_samples
-            Must fulfill the input assumptions of the
-            underlying estimator.
-
-        """
-        return self.best_estimator_.predict_log_proba(X)
-
-    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
-    def decision_function(self, X):
-        """Call decision_function on the estimator with the best found parameters.
-
-        Only available if ``refit=True`` and the underlying estimator supports
-        ``decision_function``.
-
-        Parameters
-        -----------
-        X : indexable, length n_samples
-            Must fulfill the input assumptions of the
-            underlying estimator.
-
-        """
-        return self.best_estimator_.decision_function(X)
-
-    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
-    def transform(self, X):
-        """Call transform on the estimator with the best found parameters.
-
-        Only available if the underlying estimator supports ``transform`` and
-        ``refit=True``.
-
-        Parameters
-        -----------
-        X : indexable, length n_samples
-            Must fulfill the input assumptions of the
-            underlying estimator.
-
-        """
-        return self.best_estimator_.transform(X)
-
-    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
-    def inverse_transform(self, Xt):
-        """Call inverse_transform on the estimator with the best found parameters.
-
-        Only available if the underlying estimator implements ``inverse_transform`` and
-        ``refit=True``.
-
-        Parameters
-        -----------
-        Xt : indexable, length n_samples
-            Must fulfill the input assumptions of the
-            underlying estimator.
-
-        """
-        return self.best_estimator_.inverse_transform(Xt)
-
-    def _fit(self, X, y, parameter_iterable):
-        """Actual fitting,  performing the search over parameters."""
-
-        estimator = self.estimator
-        cv = self.cv
-        self.scorer_ = check_scoring(self.estimator, scoring=self.scoring)
-
-        n_samples = _num_samples(X)
-        X, y = indexable(X, y)
-
-        if y is not None:
-            if len(y) != n_samples:
-                raise ValueError('Target variable (y) has a different number '
-                                 'of samples (%i) than data (X: %i samples)'
-                                 % (len(y), n_samples))
-        cv = check_cv(cv, X, y, classifier=is_classifier(estimator))
-
-        if self.verbose > 0:
-            if isinstance(parameter_iterable, Sized):
-                n_candidates = len(parameter_iterable)
-                print("Fitting {0} folds for each of {1} candidates, totalling"
-                      " {2} fits".format(len(cv), n_candidates,
-                                         n_candidates * len(cv)))
-
-        base_estimator = clone(self.estimator)
-
-        pre_dispatch = self.pre_dispatch
-
-        out = Parallel(
-            n_jobs=self.n_jobs, verbose=self.verbose,
-            pre_dispatch=pre_dispatch
-        )(
-            delayed(_fit_and_score)(clone(base_estimator), X, y, self.scorer_,
-                                    train, test, self.verbose, parameters,
-                                    self.fit_params, return_parameters=True,
-                                    error_score=self.error_score)
-                for parameters in parameter_iterable
-                for train, test in cv)
-
-        # Out is a list of triplet: score, estimator, n_test_samples
-        n_fits = len(out)
-        n_folds = len(cv)
-
-        scores = list()
-        grid_scores = list()
-        for grid_start in range(0, n_fits, n_folds):
-            n_test_samples = 0
-            score = 0
-            all_scores = []
-            for this_score, this_n_test_samples, _, parameters in \
-                    out[grid_start:grid_start + n_folds]:
-                all_scores.append(this_score)
-                if self.iid:
-                    this_score *= this_n_test_samples
-                    n_test_samples += this_n_test_samples
-                score += this_score
-            if self.iid:
-                score /= float(n_test_samples)
-            else:
-                score /= float(n_folds)
-            scores.append((score, parameters))
-            # TODO: shall we also store the test_fold_sizes?
-            grid_scores.append(_CVScoreTuple(
-                parameters,
-                score,
-                np.array(all_scores)))
-        # Store the computed scores
-        self.grid_scores_ = grid_scores
-
-        # Find the best parameters by comparing on the mean validation score:
-        # note that `sorted` is deterministic in the way it breaks ties
-        best = sorted(grid_scores, key=lambda x: x.mean_validation_score,
-                      reverse=True)[0]
-        self.best_params_ = best.parameters
-        self.best_score_ = best.mean_validation_score
-
-        if self.refit:
-            # fit the best estimator using the entire dataset
-            # clone first to work around broken estimators
-            best_estimator = clone(base_estimator).set_params(
-                **best.parameters)
-            if y is not None:
-                best_estimator.fit(X, y, **self.fit_params)
-            else:
-                best_estimator.fit(X, **self.fit_params)
-            self.best_estimator_ = best_estimator
-        return self
-
-
-class GridSearchCV(BaseSearchCV):
-    """Exhaustive search over specified parameter values for an estimator.
-
-    .. deprecated:: 0.18
-        This module will be removed in 0.20.
-        Use :class:`sklearn.model_selection.GridSearchCV` instead.
-
-    Important members are fit, predict.
-
-    GridSearchCV implements a "fit" and a "score" method.
-    It also implements "predict", "predict_proba", "decision_function",
-    "transform" and "inverse_transform" if they are implemented in the
-    estimator used.
-
-    The parameters of the estimator used to apply these methods are optimized
-    by cross-validated grid-search over a parameter grid.
-
-    Read more in the :ref:`User Guide <grid_search>`.
-
-    Parameters
-    ----------
-    estimator : estimator object.
-        A object of that type is instantiated for each grid point.
-        This is assumed to implement the scikit-learn estimator interface.
-        Either estimator needs to provide a ``score`` function,
-        or ``scoring`` must be passed.
-
-    param_grid : dict or list of dictionaries
-        Dictionary with parameters names (string) as keys and lists of
-        parameter settings to try as values, or a list of such
-        dictionaries, in which case the grids spanned by each dictionary
-        in the list are explored. This enables searching over any sequence
-        of parameter settings.
-
-    scoring : string, callable or None, default=None
-        A string (see model evaluation documentation) or
-        a scorer callable object / function with signature
-        ``scorer(estimator, X, y)``.
-        If ``None``, the ``score`` method of the estimator is used.
-
-    fit_params : dict, optional
-        Parameters to pass to the fit method.
-
-    n_jobs: int, default: 1 :
-        The maximum number of estimators fit in parallel.
-
-            - If -1 all CPUs are used.
-
-            - If 1 is given, no parallel computing code is used at all,
-              which is useful for debugging.
-
-            - For ``n_jobs`` below -1, ``(n_cpus + n_jobs + 1)`` are used.
-              For example, with ``n_jobs = -2`` all CPUs but one are used.
-
-        .. versionchanged:: 0.17
-           Upgraded to joblib 0.9.3.
-
-    pre_dispatch : int, or string, optional
-        Controls the number of jobs that get dispatched during parallel
-        execution. Reducing this number can be useful to avoid an
-        explosion of memory consumption when more jobs get dispatched
-        than CPUs can process. This parameter can be:
-
-            - None, in which case all the jobs are immediately
-              created and spawned. Use this for lightweight and
-              fast-running jobs, to avoid delays due to on-demand
-              spawning of the jobs
-
-            - An int, giving the exact number of total jobs that are
-              spawned
-
-            - A string, giving an expression as a function of n_jobs,
-              as in '2*n_jobs'
-
-    iid : boolean, default=True
-        If True, the data is assumed to be identically distributed across
-        the folds, and the loss minimized is the total loss per sample,
-        and not the mean loss across the folds.
-
-    cv : int, cross-validation generator or an iterable, optional
-        Determines the cross-validation splitting strategy.
-        Possible inputs for cv are:
-
-        - None, to use the default 3-fold cross-validation,
-        - integer, to specify the number of folds.
-        - An object to be used as a cross-validation generator.
-        - An iterable yielding train/test splits.
-
-        For integer/None inputs, if the estimator is a classifier and ``y`` is
-        either binary or multiclass,
-        :class:`sklearn.model_selection.StratifiedKFold` is used. In all
-        other cases, :class:`sklearn.model_selection.KFold` is used.
-
-        Refer :ref:`User Guide <cross_validation>` for the various
-        cross-validation strategies that can be used here.
-
-    refit : boolean, default=True
-        Refit the best estimator with the entire dataset.
-        If "False", it is impossible to make predictions using
-        this GridSearchCV instance after fitting.
-
-    verbose : integer
-        Controls the verbosity: the higher, the more messages.
-
-    error_score : 'raise' (default) or numeric
-        Value to assign to the score if an error occurs in estimator fitting.
-        If set to 'raise', the error is raised. If a numeric value is given,
-        FitFailedWarning is raised. This parameter does not affect the refit
-        step, which will always raise the error.
-
-
-    Examples
-    --------
-    >>> from sklearn import svm, grid_search, datasets
-    >>> iris = datasets.load_iris()
-    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
-    >>> svr = svm.SVC(gamma="scale")
-    >>> clf = grid_search.GridSearchCV(svr, parameters)
-    >>> clf.fit(iris.data, iris.target)
-    ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
-    GridSearchCV(cv=None, error_score=...,
-           estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,
-                         decision_function_shape='ovr', degree=..., gamma=...,
-                         kernel='rbf', max_iter=-1, probability=False,
-                         random_state=None, shrinking=True, tol=...,
-                         verbose=False),
-           fit_params={}, iid=..., n_jobs=1,
-           param_grid=..., pre_dispatch=..., refit=...,
-           scoring=..., verbose=...)
-
-
-    Attributes
-    ----------
-    grid_scores_ : list of namedtuples
-        Contains scores for all parameter combinations in param_grid.
-        Each entry corresponds to one parameter setting.
-        Each namedtuple has the attributes:
-
-            * ``parameters``, a dict of parameter settings
-            * ``mean_validation_score``, the mean score over the
-              cross-validation folds
-            * ``cv_validation_scores``, the list of scores for each fold
-
-    best_estimator_ : estimator
-        Estimator that was chosen by the search, i.e. estimator
-        which gave highest score (or smallest loss if specified)
-        on the left out data. Not available if refit=False.
-
-    best_score_ : float
-        Score of best_estimator on the left out data.
-
-    best_params_ : dict
-        Parameter setting that gave the best results on the hold out data.
-
-    scorer_ : function
-        Scorer function used on the held out data to choose the best
-        parameters for the model.
-
-    Notes
-    ------
-    The parameters selected are those that maximize the score of the left out
-    data, unless an explicit score is passed in which case it is used instead.
-
-    If `n_jobs` was set to a value higher than one, the data is copied for each
-    point in the grid (and not `n_jobs` times). This is done for efficiency
-    reasons if individual jobs take very little time, but may raise errors if
-    the dataset is large and not enough memory is available.  A workaround in
-    this case is to set `pre_dispatch`. Then, the memory is copied only
-    `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *
-    n_jobs`.
-
-    See Also
-    ---------
-    :class:`ParameterGrid`:
-        generates all the combinations of a hyperparameter grid.
-
-    :func:`sklearn.cross_validation.train_test_split`:
-        utility function to split the data into a development set usable
-        for fitting a GridSearchCV instance and an evaluation set for
-        its final evaluation.
-
-    :func:`sklearn.metrics.make_scorer`:
-        Make a scorer from a performance metric or loss function.
-
-    """
-
-    def __init__(self, estimator, param_grid, scoring=None, fit_params=None,
-                 n_jobs=1, iid=True, refit=True, cv=None, verbose=0,
-                 pre_dispatch='2*n_jobs', error_score='raise'):
-
-        super(GridSearchCV, self).__init__(
-            estimator, scoring, fit_params, n_jobs, iid,
-            refit, cv, verbose, pre_dispatch, error_score)
-        self.param_grid = param_grid
-        _check_param_grid(param_grid)
-
-    def fit(self, X, y=None):
-        """Run fit with all sets of parameters.
-
-        Parameters
-        ----------
-
-        X : array-like, shape = [n_samples, n_features]
-            Training vector, where n_samples is the number of samples and
-            n_features is the number of features.
-
-        y : array-like, shape = [n_samples] or [n_samples, n_output], optional
-            Target relative to X for classification or regression;
-            None for unsupervised learning.
-
-        """
-        return self._fit(X, y, ParameterGrid(self.param_grid))
-
-
-class RandomizedSearchCV(BaseSearchCV):
-    """Randomized search on hyper parameters.
-
-    .. deprecated:: 0.18
-        This module will be removed in 0.20.
-        Use :class:`sklearn.model_selection.RandomizedSearchCV` instead.
-
-    RandomizedSearchCV implements a "fit" and a "score" method.
-    It also implements "predict", "predict_proba", "decision_function",
-    "transform" and "inverse_transform" if they are implemented in the
-    estimator used.
-
-    The parameters of the estimator used to apply these methods are optimized
-    by cross-validated search over parameter settings.
-
-    In contrast to GridSearchCV, not all parameter values are tried out, but
-    rather a fixed number of parameter settings is sampled from the specified
-    distributions. The number of parameter settings that are tried is
-    given by n_iter.
-
-    If all parameters are presented as a list,
-    sampling without replacement is performed. If at least one parameter
-    is given as a distribution, sampling with replacement is used.
-    It is highly recommended to use continuous distributions for continuous
-    parameters.
-
-    Read more in the :ref:`User Guide <randomized_parameter_search>`.
-
-    Parameters
-    ----------
-    estimator : estimator object.
-        A object of that type is instantiated for each grid point.
-        This is assumed to implement the scikit-learn estimator interface.
-        Either estimator needs to provide a ``score`` function,
-        or ``scoring`` must be passed.
-
-    param_distributions : dict
-        Dictionary with parameters names (string) as keys and distributions
-        or lists of parameters to try. Distributions must provide a ``rvs``
-        method for sampling (such as those from scipy.stats.distributions).
-        If a list is given, it is sampled uniformly.
-
-    n_iter : int, default=10
-        Number of parameter settings that are sampled. n_iter trades
-        off runtime vs quality of the solution.
-
-    scoring : string, callable or None, default=None
-        A string (see model evaluation documentation) or
-        a scorer callable object / function with signature
-        ``scorer(estimator, X, y)``.
-        If ``None``, the ``score`` method of the estimator is used.
-
-    fit_params : dict, optional
-        Parameters to pass to the fit method.
-
-    n_jobs: int, default: 1 :
-        The maximum number of estimators fit in parallel.
-
-            - If -1 all CPUs are used.
-
-            - If 1 is given, no parallel computing code is used at all,
-              which is useful for debugging.
-
-            - For ``n_jobs`` below -1, ``(n_cpus + n_jobs + 1)`` are used.
-              For example, with ``n_jobs = -2`` all CPUs but one are used.
-
-    pre_dispatch : int, or string, optional
-        Controls the number of jobs that get dispatched during parallel
-        execution. Reducing this number can be useful to avoid an
-        explosion of memory consumption when more jobs get dispatched
-        than CPUs can process. This parameter can be:
-
-            - None, in which case all the jobs are immediately
-              created and spawned. Use this for lightweight and
-              fast-running jobs, to avoid delays due to on-demand
-              spawning of the jobs
-
-            - An int, giving the exact number of total jobs that are
-              spawned
-
-            - A string, giving an expression as a function of n_jobs,
-              as in '2*n_jobs'
-
-    iid : boolean, default=True
-        If True, the data is assumed to be identically distributed across
-        the folds, and the loss minimized is the total loss per sample,
-        and not the mean loss across the folds.
-
-    cv : int, cross-validation generator or an iterable, optional
-        Determines the cross-validation splitting strategy.
-        Possible inputs for cv are:
-
-        - None, to use the default 3-fold cross-validation,
-        - integer, to specify the number of folds.
-        - An object to be used as a cross-validation generator.
-        - An iterable yielding train/test splits.
-
-        For integer/None inputs, if the estimator is a classifier and ``y`` is
-        either binary or multiclass,
-        :class:`sklearn.model_selection.StratifiedKFold` is used. In all
-        other cases, :class:`sklearn.model_selection.KFold` is used.
-
-        Refer :ref:`User Guide <cross_validation>` for the various
-        cross-validation strategies that can be used here.
-
-    refit : boolean, default=True
-        Refit the best estimator with the entire dataset.
-        If "False", it is impossible to make predictions using
-        this RandomizedSearchCV instance after fitting.
-
-    verbose : integer
-        Controls the verbosity: the higher, the more messages.
-
-    random_state : int, RandomState instance or None, optional, default=None
-        Pseudo random number generator state used for random uniform sampling
-        from lists of possible values instead of scipy.stats distributions.
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
-
-    error_score : 'raise' (default) or numeric
-        Value to assign to the score if an error occurs in estimator fitting.
-        If set to 'raise', the error is raised. If a numeric value is given,
-        FitFailedWarning is raised. This parameter does not affect the refit
-        step, which will always raise the error.
-
-
-    Attributes
-    ----------
-    grid_scores_ : list of namedtuples
-        Contains scores for all parameter combinations in param_grid.
-        Each entry corresponds to one parameter setting.
-        Each namedtuple has the attributes:
-
-            * ``parameters``, a dict of parameter settings
-            * ``mean_validation_score``, the mean score over the
-              cross-validation folds
-            * ``cv_validation_scores``, the list of scores for each fold
-
-    best_estimator_ : estimator
-        Estimator that was chosen by the search, i.e. estimator
-        which gave highest score (or smallest loss if specified)
-        on the left out data. Not available if refit=False.
-
-    best_score_ : float
-        Score of best_estimator on the left out data.
-
-    best_params_ : dict
-        Parameter setting that gave the best results on the hold out data.
-
-    Notes
-    -----
-    The parameters selected are those that maximize the score of the held-out
-    data, according to the scoring parameter.
-
-    If `n_jobs` was set to a value higher than one, the data is copied for each
-    parameter setting(and not `n_jobs` times). This is done for efficiency
-    reasons if individual jobs take very little time, but may raise errors if
-    the dataset is large and not enough memory is available.  A workaround in
-    this case is to set `pre_dispatch`. Then, the memory is copied only
-    `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *
-    n_jobs`.
-
-    See Also
-    --------
-    :class:`GridSearchCV`:
-        Does exhaustive search over a grid of parameters.
-
-    :class:`ParameterSampler`:
-        A generator over parameter settings, constructed from
-        param_distributions.
-
-    """
-
-    def __init__(self, estimator, param_distributions, n_iter=10, scoring=None,
-                 fit_params=None, n_jobs=1, iid=True, refit=True, cv=None,
-                 verbose=0, pre_dispatch='2*n_jobs', random_state=None,
-                 error_score='raise'):
-
-        self.param_distributions = param_distributions
-        self.n_iter = n_iter
-        self.random_state = random_state
-        super(RandomizedSearchCV, self).__init__(
-            estimator=estimator, scoring=scoring, fit_params=fit_params,
-            n_jobs=n_jobs, iid=iid, refit=refit, cv=cv, verbose=verbose,
-            pre_dispatch=pre_dispatch, error_score=error_score)
-
-    def fit(self, X, y=None):
-        """Run fit on the estimator with randomly drawn parameters.
-
-        Parameters
-        ----------
-        X : array-like, shape = [n_samples, n_features]
-            Training vector, where n_samples in the number of samples and
-            n_features is the number of features.
-
-        y : array-like, shape = [n_samples] or [n_samples, n_output], optional
-            Target relative to X for classification or regression;
-            None for unsupervised learning.
-
-        """
-        sampled_params = ParameterSampler(self.param_distributions,
-                                          self.n_iter,
-                                          random_state=self.random_state)
-        return self._fit(X, y, sampled_params)
diff --git a/sklearn/impute.py b/sklearn/impute.py
index fe772d6a3a0c..f9fb156103ff 100644
--- a/sklearn/impute.py
+++ b/sklearn/impute.py
@@ -7,6 +7,7 @@
 
 import warnings
 from time import time
+import numbers
 
 import numpy as np
 import numpy.ma as ma
@@ -21,28 +22,40 @@
 from .utils.sparsefuncs import _get_median
 from .utils.validation import check_is_fitted
 from .utils.validation import FLOAT_DTYPES
+from .utils.fixes import _object_dtype_isnan
+from .utils import is_scalar_nan
 
 from .externals import six
 
 zip = six.moves.zip
 map = six.moves.map
 
-MICETriplet = namedtuple('MICETriplet', ['feat_idx',
-                                         'neighbor_feat_idx',
-                                         'predictor'])
+ImputerTriplet = namedtuple('ImputerTriplet', ['feat_idx',
+                                               'neighbor_feat_idx',
+                                               'predictor'])
 
 __all__ = [
     'SimpleImputer',
-    'MICEImputer',
+    'ChainedImputer',
 ]
 
 
 def _get_mask(X, value_to_mask):
     """Compute the boolean mask X == missing_values."""
-    if value_to_mask == "NaN" or np.isnan(value_to_mask):
-        return np.isnan(X)
+    if value_to_mask is np.nan:
+        if X.dtype.kind == "f":
+            return np.isnan(X)
+        elif X.dtype.kind in ("i", "u"):
+            # can't have NaNs in integer array.
+            return np.zeros(X.shape, dtype=bool)
+        else:
+            # np.isnan does not work on object dtypes.
+            return _object_dtype_isnan(X)
+
     else:
-        return X == value_to_mask
+        # X == value_to_mask with object dytpes does not always perform
+        # element-wise for old versions of numpy
+        return np.equal(X, value_to_mask)
 
 
 def _most_frequent(array, extra_value, n_repeat):
@@ -51,7 +64,13 @@ def _most_frequent(array, extra_value, n_repeat):
        of the array."""
     # Compute the most frequent value in array only
     if array.size > 0:
-        mode = stats.mode(array)
+        with warnings.catch_warnings():
+            # stats.mode raises a warning when input array contains objects due
+            # to incapacity to detect NaNs. Irrelevant here since input array
+            # has already been NaN-masked.
+            warnings.simplefilter("ignore", RuntimeWarning)
+            mode = stats.mode(array)
+
         most_frequent_value = mode[0][0]
         most_frequent_count = mode[1][0]
     else:
@@ -80,20 +99,30 @@ class SimpleImputer(BaseEstimator, TransformerMixin):
 
     Parameters
     ----------
-    missing_values : integer or "NaN", optional (default="NaN")
+    missing_values : number, string, np.nan (default) or None
         The placeholder for the missing values. All occurrences of
-        `missing_values` will be imputed. For missing values encoded as np.nan,
-        use the string value "NaN".
+        `missing_values` will be imputed.
 
     strategy : string, optional (default="mean")
         The imputation strategy.
 
         - If "mean", then replace missing values using the mean along
-          each column.
+          each column. Can only be used with numeric data.
         - If "median", then replace missing values using the median along
-          each column.
+          each column. Can only be used with numeric data.
         - If "most_frequent", then replace missing using the most frequent
-          value along each column.
+          value along each column. Can be used with strings or numeric data.
+        - If "constant", then replace missing values with fill_value. Can be
+          used with strings or numeric data.
+
+        .. versionadded:: 0.20
+           strategy="constant" for fixed value imputation.
+
+    fill_value : string or numerical value, optional
+        When strategy == "constant", fill_value is used to replace all
+        occurrences of missing_values.
+        If left to the default, fill_value will be 0 when imputing numerical
+        data and "missing_value" for strings or object data types.
 
     verbose : integer, optional (default=0)
         Controls the verbosity of the imputer.
@@ -115,16 +144,55 @@ class SimpleImputer(BaseEstimator, TransformerMixin):
     Notes
     -----
     Columns which only contained missing values at `fit` are discarded upon
-    `transform`.
+    `transform` if strategy is not "constant".
 
     """
-    def __init__(self, missing_values="NaN", strategy="mean",
-                 verbose=0, copy=True):
+    def __init__(self, missing_values=np.nan, strategy="mean",
+                 fill_value=None, verbose=0, copy=True):
         self.missing_values = missing_values
         self.strategy = strategy
+        self.fill_value = fill_value
         self.verbose = verbose
         self.copy = copy
 
+    def _validate_input(self, X):
+        allowed_strategies = ["mean", "median", "most_frequent", "constant"]
+        if self.strategy not in allowed_strategies:
+            raise ValueError("Can only use these strategies: {0} "
+                             " got strategy={1}".format(allowed_strategies,
+                                                        self.strategy))
+
+        if self.strategy in ("most_frequent", "constant"):
+            dtype = None
+        else:
+            dtype = FLOAT_DTYPES
+
+        if not is_scalar_nan(self.missing_values):
+            force_all_finite = True
+        else:
+            force_all_finite = "allow-nan"
+
+        try:
+            X = check_array(X, accept_sparse='csc', dtype=dtype,
+                            force_all_finite=force_all_finite, copy=self.copy)
+        except ValueError as ve:
+            if "could not convert" in str(ve):
+                raise ValueError("Cannot use {0} strategy with non-numeric "
+                                 "data. Received datatype :{1}."
+                                 "".format(self.strategy, X.dtype.kind))
+            else:
+                raise ve
+
+        if X.dtype.kind not in ("i", "u", "f", "O"):
+            raise ValueError("SimpleImputer does not support data with dtype "
+                             "{0}. Please provide either a numeric array (with"
+                             " a floating point or integer dtype) or "
+                             "categorical data represented either as an array "
+                             "with integer dtype or an array of string values "
+                             "with an object dtype.".format(X.dtype))
+
+        return X
+
     def fit(self, X, y=None):
         """Fit the imputer on X.
 
@@ -138,30 +206,40 @@ def fit(self, X, y=None):
         -------
         self : SimpleImputer
         """
-        # Check parameters
-        allowed_strategies = ["mean", "median", "most_frequent"]
-        if self.strategy not in allowed_strategies:
-            raise ValueError("Can only use these strategies: {0} "
-                             " got strategy={1}".format(allowed_strategies,
-                                                        self.strategy))
+        X = self._validate_input(X)
 
-        X = check_array(X, accept_sparse='csc', dtype=FLOAT_DTYPES,
-                        force_all_finite='allow-nan'
-                        if self.missing_values == 'NaN'
-                        or np.isnan(self.missing_values) else True)
+        # default fill_value is 0 for numerical input and "missing_value"
+        # otherwise
+        if self.fill_value is None:
+            if X.dtype.kind in ("i", "u", "f"):
+                fill_value = 0
+            else:
+                fill_value = "missing_value"
+        else:
+            fill_value = self.fill_value
+
+        # fill_value should be numerical in case of numerical input
+        if (self.strategy == "constant" and
+                X.dtype.kind in ("i", "u", "f") and
+                not isinstance(fill_value, numbers.Real)):
+            raise ValueError("'fill_value'={0} is invalid. Expected a "
+                             "numerical value when imputing numerical "
+                             "data".format(fill_value))
 
         if sparse.issparse(X):
             self.statistics_ = self._sparse_fit(X,
                                                 self.strategy,
-                                                self.missing_values)
+                                                self.missing_values,
+                                                fill_value)
         else:
             self.statistics_ = self._dense_fit(X,
                                                self.strategy,
-                                               self.missing_values)
+                                               self.missing_values,
+                                               fill_value)
 
         return self
 
-    def _sparse_fit(self, X, strategy, missing_values):
+    def _sparse_fit(self, X, strategy, missing_values, fill_value):
         """Fit the transformer on sparse data."""
         # Count the zeros
         if missing_values == 0:
@@ -203,7 +281,7 @@ def _sparse_fit(self, X, strategy, missing_values):
             with np.errstate(all="ignore"):
                 return np.ravel(sums) / np.ravel(n_non_missing)
 
-        # Median + Most frequent
+        # Median + Most frequent + Constant
         else:
             # Remove the missing values, for each column
             columns_all = np.hsplit(X.data, X.indptr[1:-1])
@@ -234,11 +312,12 @@ def _sparse_fit(self, X, strategy, missing_values):
 
                 return most_frequent
 
-    def _dense_fit(self, X, strategy, missing_values):
+            # Constant
+            elif strategy == "constant":
+                return np.full(X.shape[1], fill_value)
+
+    def _dense_fit(self, X, strategy, missing_values, fill_value):
         """Fit the transformer on dense data."""
-        X = check_array(X, force_all_finite='allow-nan'
-                        if self.missing_values == 'NaN'
-                        or np.isnan(self.missing_values) else True)
         mask = _get_mask(X, missing_values)
         masked_X = ma.masked_array(X, mask=mask)
 
@@ -271,7 +350,10 @@ def _dense_fit(self, X, strategy, missing_values):
             X = X.transpose()
             mask = mask.transpose()
 
-            most_frequent = np.empty(X.shape[0])
+            if X.dtype.kind == "O":
+                most_frequent = np.empty(X.shape[0], dtype=object)
+            else:
+                most_frequent = np.empty(X.shape[0])
 
             for i, (row, row_mask) in enumerate(zip(X[:], mask[:])):
                 row_mask = np.logical_not(row_mask).astype(np.bool)
@@ -280,6 +362,10 @@ def _dense_fit(self, X, strategy, missing_values):
 
             return most_frequent
 
+        # Constant
+        elif strategy == "constant":
+            return np.full(X.shape[1], fill_value, dtype=X.dtype)
+
     def transform(self, X):
         """Impute all missing values in X.
 
@@ -289,28 +375,31 @@ def transform(self, X):
             The input data to complete.
         """
         check_is_fitted(self, 'statistics_')
-        X = check_array(X, accept_sparse='csc', dtype=FLOAT_DTYPES,
-                        force_all_finite='allow-nan'
-                        if self.missing_values == 'NaN'
-                        or np.isnan(self.missing_values) else True,
-                        copy=self.copy)
+
+        X = self._validate_input(X)
+
         statistics = self.statistics_
+
         if X.shape[1] != statistics.shape[0]:
             raise ValueError("X has %d features per sample, expected %d"
                              % (X.shape[1], self.statistics_.shape[0]))
 
-        # Delete the invalid columns
-        invalid_mask = np.isnan(statistics)
-        valid_mask = np.logical_not(invalid_mask)
-        valid_statistics = statistics[valid_mask]
-        valid_statistics_indexes = np.flatnonzero(valid_mask)
-        missing = np.arange(X.shape[1])[invalid_mask]
-
-        if invalid_mask.any():
-            if self.verbose:
-                warnings.warn("Deleting features without "
-                              "observed values: %s" % missing)
-            X = X[:, valid_statistics_indexes]
+        # Delete the invalid columns if strategy is not constant
+        if self.strategy == "constant":
+            valid_statistics = statistics
+        else:
+            # same as np.isnan but also works for object dtypes
+            invalid_mask = _get_mask(statistics, np.nan)
+            valid_mask = np.logical_not(invalid_mask)
+            valid_statistics = statistics[valid_mask]
+            valid_statistics_indexes = np.flatnonzero(valid_mask)
+
+            if invalid_mask.any():
+                missing = np.arange(X.shape[1])[invalid_mask]
+                if self.verbose:
+                    warnings.warn("Deleting features without "
+                                  "observed values: %s" % missing)
+                X = X[:, valid_statistics_indexes]
 
         # Do actual imputation
         if sparse.issparse(X) and self.missing_values != 0:
@@ -327,7 +416,6 @@ def transform(self, X):
             mask = _get_mask(X, self.missing_values)
             n_missing = np.sum(mask, axis=0)
             values = np.repeat(valid_statistics, n_missing)
-
             coordinates = np.where(mask.transpose())[::-1]
 
             X[coordinates] = values
@@ -335,21 +423,20 @@ def transform(self, X):
         return X
 
 
-class MICEImputer(BaseEstimator, TransformerMixin):
-    """MICE transformer to impute missing values.
+class ChainedImputer(BaseEstimator, TransformerMixin):
+    """Chained imputer transformer to impute missing values.
 
-    Basic implementation of MICE (Multivariate Imputations by Chained
-    Equations) package from R. This version assumes all of the features are
-    Gaussian.
+    Basic implementation of chained imputer from MICE (Multivariate
+    Imputations by Chained Equations) package from R. This version assumes all
+    of the features are Gaussian.
 
     Read more in the :ref:`User Guide <mice>`.
 
     Parameters
     ----------
-    missing_values : int or "NaN", optional (default="NaN")
+    missing_values : int, np.nan, optional (default=np.nan)
         The placeholder for the missing values. All occurrences of
-        ``missing_values`` will be imputed. For missing values encoded as
-        np.nan, use the string value "NaN".
+        ``missing_values`` will be imputed.
 
     imputation_order : str, optional (default="ascending")
         The order in which the features will be imputed. Possible values:
@@ -366,11 +453,11 @@ class MICEImputer(BaseEstimator, TransformerMixin):
             A random order for each round.
 
     n_imputations : int, optional (default=100)
-        Number of MICE rounds to perform, the results of which will be
-        used in the final average.
+        Number of chained imputation rounds to perform, the results of which
+        will be used in the final average.
 
     n_burn_in : int, optional (default=10)
-        Number of initial MICE rounds to perform the results of which
+        Number of initial imputation rounds to perform the results of which
         will not be returned.
 
     predictor : estimator object, default=BayesianRidge()
@@ -386,8 +473,8 @@ class MICEImputer(BaseEstimator, TransformerMixin):
 
     initial_strategy : str, optional (default="mean")
         Which strategy to use to initialize the missing values. Same as the
-        ``strategy`` parameter in :class:`sklearn.preprocessing.Imputer`
-        Valid values: {"mean", "median", or "most_frequent"}.
+        ``strategy`` parameter in :class:`sklearn.impute.SimpleImputer`
+        Valid values: {"mean", "median", "most_frequent", or "constant"}.
 
     min_value : float, optional (default=None)
         Minimum possible imputed value. Default of ``None`` will set minimum
@@ -444,7 +531,7 @@ class MICEImputer(BaseEstimator, TransformerMixin):
     """
 
     def __init__(self,
-                 missing_values='NaN',
+                 missing_values=np.nan,
                  imputation_order='ascending',
                  n_imputations=100,
                  n_burn_in=10,
@@ -694,10 +781,13 @@ def _initial_imputation(self, X):
             Input data's missing indicator matrix, where "n_samples" is the
             number of samples and "n_features" is the number of features.
         """
+        if is_scalar_nan(self.missing_values):
+            force_all_finite = "allow-nan"
+        else:
+            force_all_finite = True
+
         X = check_array(X, dtype=FLOAT_DTYPES, order="F",
-                        force_all_finite='allow-nan'
-                        if self.missing_values == 'NaN'
-                        or np.isnan(self.missing_values) else True)
+                        force_all_finite=force_all_finite)
 
         mask_missing_values = _get_mask(X, self.missing_values)
         if self.initial_imputer_ is None:
@@ -768,7 +858,8 @@ def fit_transform(self, X, y=None):
         Xt = np.zeros((n_samples, n_features), dtype=X.dtype)
         self.imputation_sequence_ = []
         if self.verbose > 0:
-            print("[MICE] Completing matrix with shape %s" % (X.shape,))
+            print("[ChainedImputer] Completing matrix with shape %s"
+                  % (X.shape,))
         start_t = time()
         for i_rnd in range(n_rounds):
             if self.imputation_order == 'random':
@@ -781,15 +872,15 @@ def fit_transform(self, X, y=None):
                 X_filled, predictor = self._impute_one_feature(
                     X_filled, mask_missing_values, feat_idx, neighbor_feat_idx,
                     predictor=None, fit_mode=True)
-                predictor_triplet = MICETriplet(feat_idx,
-                                                neighbor_feat_idx,
-                                                predictor)
+                predictor_triplet = ImputerTriplet(feat_idx,
+                                                   neighbor_feat_idx,
+                                                   predictor)
                 self.imputation_sequence_.append(predictor_triplet)
 
             if i_rnd >= self.n_burn_in:
                 Xt += X_filled
             if self.verbose > 0:
-                print('[MICE] Ending imputation round '
+                print('[ChainedImputer] Ending imputation round '
                       '%d/%d, elapsed time %0.2f'
                       % (i_rnd + 1, n_rounds, time() - start_t))
 
@@ -831,7 +922,8 @@ def transform(self, X):
         i_rnd = 0
         Xt = np.zeros(X.shape, dtype=X.dtype)
         if self.verbose > 0:
-            print("[MICE] Completing matrix with shape %s" % (X.shape,))
+            print("[ChainedImputer] Completing matrix with shape %s"
+                  % (X.shape,))
         start_t = time()
         for it, predictor_triplet in enumerate(self.imputation_sequence_):
             X_filled, _ = self._impute_one_feature(
@@ -846,7 +938,7 @@ def transform(self, X):
                 if i_rnd >= self.n_burn_in:
                     Xt += X_filled
                 if self.verbose > 1:
-                    print('[MICE] Ending imputation round '
+                    print('[ChainedImputer] Ending imputation round '
                           '%d/%d, elapsed time %0.2f'
                           % (i_rnd + 1, n_rounds, time() - start_t))
                 i_rnd += 1
diff --git a/sklearn/isotonic.py b/sklearn/isotonic.py
index 77bb7e903a18..7b74048e1859 100644
--- a/sklearn/isotonic.py
+++ b/sklearn/isotonic.py
@@ -8,7 +8,6 @@
 from scipy.stats import spearmanr
 from .base import BaseEstimator, TransformerMixin, RegressorMixin
 from .utils import as_float_array, check_array, check_consistent_length
-from .utils import deprecated
 from ._isotonic import _inplace_contiguous_isotonic_regression, _make_unique
 import warnings
 import math
@@ -189,7 +188,7 @@ class IsotonicRegression(BaseEstimator, TransformerMixin, RegressorMixin):
         Maximum value of input array `X_` for right bound.
 
     f_ : function
-        The stepwise interpolating function that covers the domain `X_`.
+        The stepwise interpolating function that covers the input domain ``X``.
 
     Notes
     -----
@@ -217,34 +216,6 @@ def __init__(self, y_min=None, y_max=None, increasing=True,
         self.increasing = increasing
         self.out_of_bounds = out_of_bounds
 
-    @property
-    @deprecated("Attribute ``X_`` is deprecated in version 0.18 and will be"
-                " removed in version 0.20.")
-    def X_(self):
-        return self._X_
-
-    @X_.setter
-    def X_(self, value):
-        self._X_ = value
-
-    @X_.deleter
-    def X_(self):
-        del self._X_
-
-    @property
-    @deprecated("Attribute ``y_`` is deprecated in version 0.18 and will"
-                " be removed in version 0.20.")
-    def y_(self):
-        return self._y_
-
-    @y_.setter
-    def y_(self, value):
-        self._y_ = value
-
-    @y_.deleter
-    def y_(self):
-        del self._y_
-
     def _check_fit_data(self, X, y, sample_weight=None):
         if len(X.shape) != 1:
             raise ValueError("X should be a 1d array")
diff --git a/sklearn/kernel_ridge.py b/sklearn/kernel_ridge.py
index 308d0661bced..91e693fae4f3 100644
--- a/sklearn/kernel_ridge.py
+++ b/sklearn/kernel_ridge.py
@@ -48,7 +48,9 @@ class KernelRidge(BaseEstimator, RegressorMixin):
     kernel : string or callable, default="linear"
         Kernel mapping used internally. A callable should accept two arguments
         and the keyword arguments passed to this object as kernel_params, and
-        should return a floating point number.
+        should return a floating point number. Set to "precomputed" in
+        order to pass a precomputed kernel matrix to the estimator
+        methods instead of samples.
 
     gamma : float, default=None
         Gamma parameter for the RBF, laplacian, polynomial, exponential chi2
@@ -73,7 +75,9 @@ class KernelRidge(BaseEstimator, RegressorMixin):
         Representation of weight vector(s) in kernel space
 
     X_fit_ : {array-like, sparse matrix}, shape = [n_samples, n_features]
-        Training data, which is also required for prediction
+        Training data, which is also required for prediction. If
+        kernel == "precomputed" this is instead the precomputed
+        training matrix, shape = [n_samples, n_samples].
 
     References
     ----------
@@ -130,7 +134,9 @@ def fit(self, X, y=None, sample_weight=None):
         Parameters
         ----------
         X : {array-like, sparse matrix}, shape = [n_samples, n_features]
-            Training data
+            Training data. If kernel == "precomputed" this is instead
+            a precomputed kernel matrix, shape = [n_samples,
+            n_samples].
 
         y : array-like, shape = [n_samples] or [n_samples, n_targets]
             Target values
@@ -173,7 +179,10 @@ def predict(self, X):
         Parameters
         ----------
         X : {array-like, sparse matrix}, shape = [n_samples, n_features]
-            Samples.
+            Samples. If kernel == "precomputed" this is instead a
+            precomputed kernel matrix, shape = [n_samples,
+            n_samples_fitted], where n_samples_fitted is the number of
+            samples used in the fitting for this estimator.
 
         Returns
         -------
diff --git a/sklearn/learning_curve.py b/sklearn/learning_curve.py
deleted file mode 100644
index 5571138d68d8..000000000000
--- a/sklearn/learning_curve.py
+++ /dev/null
@@ -1,360 +0,0 @@
-"""Utilities to evaluate models with respect to a variable
-"""
-# Author: Alexander Fabisch <afabisch@informatik.uni-bremen.de>
-#
-# License: BSD 3 clause
-
-import warnings
-
-import numpy as np
-
-from .base import is_classifier, clone
-from .cross_validation import check_cv
-from .externals.joblib import Parallel, delayed
-from .cross_validation import _safe_split, _score, _fit_and_score
-from .metrics.scorer import check_scoring
-from .utils import indexable
-
-
-warnings.warn("This module was deprecated in version 0.18 in favor of the "
-              "model_selection module into which all the functions are moved."
-              " This module will be removed in 0.20",
-              DeprecationWarning)
-
-
-__all__ = ['learning_curve', 'validation_curve']
-
-
-def learning_curve(estimator, X, y, train_sizes=np.linspace(0.1, 1.0, 5),
-                   cv=None, scoring=None, exploit_incremental_learning=False,
-                   n_jobs=1, pre_dispatch="all", verbose=0,
-                   error_score='raise'):
-    """Learning curve.
-
-    .. deprecated:: 0.18
-        This module will be removed in 0.20.
-        Use :func:`sklearn.model_selection.learning_curve` instead.
-
-    Determines cross-validated training and test scores for different training
-    set sizes.
-
-    A cross-validation generator splits the whole dataset k times in training
-    and test data. Subsets of the training set with varying sizes will be used
-    to train the estimator and a score for each training subset size and the
-    test set will be computed. Afterwards, the scores will be averaged over
-    all k runs for each training subset size.
-
-    Read more in the :ref:`User Guide <learning_curves>`.
-
-    Parameters
-    ----------
-    estimator : object type that implements the "fit" and "predict" methods
-        An object of that type which is cloned for each validation.
-
-    X : array-like, shape (n_samples, n_features)
-        Training vector, where n_samples is the number of samples and
-        n_features is the number of features.
-
-    y : array-like, shape (n_samples) or (n_samples, n_features), optional
-        Target relative to X for classification or regression;
-        None for unsupervised learning.
-
-    train_sizes : array-like, shape (n_ticks,), dtype float or int
-        Relative or absolute numbers of training examples that will be used to
-        generate the learning curve. If the dtype is float, it is regarded as a
-        fraction of the maximum size of the training set (that is determined
-        by the selected validation method), i.e. it has to be within (0, 1].
-        Otherwise it is interpreted as absolute sizes of the training sets.
-        Note that for classification the number of samples usually have to
-        be big enough to contain at least one sample from each class.
-        (default: np.linspace(0.1, 1.0, 5))
-
-    cv : int, cross-validation generator or an iterable, optional
-        Determines the cross-validation splitting strategy.
-        Possible inputs for cv are:
-
-        - None, to use the default 3-fold cross-validation,
-        - integer, to specify the number of folds.
-        - An object to be used as a cross-validation generator.
-        - An iterable yielding train/test splits.
-
-        For integer/None inputs, if the estimator is a classifier and ``y`` is
-        either binary or multiclass,
-        :class:`sklearn.model_selection.StratifiedKFold` is used. In all
-        other cases, :class:`sklearn.model_selection.KFold` is used.
-
-        Refer :ref:`User Guide <cross_validation>` for the various
-        cross-validation strategies that can be used here.
-
-    scoring : string, callable or None, optional, default: None
-        A string (see model evaluation documentation) or
-        a scorer callable object / function with signature
-        ``scorer(estimator, X, y)``.
-
-    exploit_incremental_learning : boolean, optional, default: False
-        If the estimator supports incremental learning, this will be
-        used to speed up fitting for different training set sizes.
-
-    n_jobs : integer, optional
-        Number of jobs to run in parallel (default 1).
-
-    pre_dispatch : integer or string, optional
-        Number of predispatched jobs for parallel execution (default is
-        all). The option can reduce the allocated memory. The string can
-        be an expression like '2*n_jobs'.
-
-    verbose : integer, optional
-        Controls the verbosity: the higher, the more messages.
-
-    error_score : 'raise' (default) or numeric
-        Value to assign to the score if an error occurs in estimator fitting.
-        If set to 'raise', the error is raised. If a numeric value is given,
-        FitFailedWarning is raised. This parameter does not affect the refit
-        step, which will always raise the error.
-
-    Returns
-    -------
-    train_sizes_abs : array, shape = (n_unique_ticks,), dtype int
-        Numbers of training examples that has been used to generate the
-        learning curve. Note that the number of ticks might be less
-        than n_ticks because duplicate entries will be removed.
-
-    train_scores : array, shape (n_ticks, n_cv_folds)
-        Scores on training sets.
-
-    test_scores : array, shape (n_ticks, n_cv_folds)
-        Scores on test set.
-
-    Notes
-    -----
-    See :ref:`examples/model_selection/plot_learning_curve.py
-    <sphx_glr_auto_examples_model_selection_plot_learning_curve.py>`
-    """
-    if exploit_incremental_learning and not hasattr(estimator, "partial_fit"):
-        raise ValueError("An estimator must support the partial_fit interface "
-                         "to exploit incremental learning")
-
-    X, y = indexable(X, y)
-    # Make a list since we will be iterating multiple times over the folds
-    cv = list(check_cv(cv, X, y, classifier=is_classifier(estimator)))
-    scorer = check_scoring(estimator, scoring=scoring)
-
-    # HACK as long as boolean indices are allowed in cv generators
-    if cv[0][0].dtype == bool:
-        new_cv = []
-        for i in range(len(cv)):
-            new_cv.append((np.nonzero(cv[i][0])[0], np.nonzero(cv[i][1])[0]))
-        cv = new_cv
-
-    n_max_training_samples = len(cv[0][0])
-    # Because the lengths of folds can be significantly different, it is
-    # not guaranteed that we use all of the available training data when we
-    # use the first 'n_max_training_samples' samples.
-    train_sizes_abs = _translate_train_sizes(train_sizes,
-                                             n_max_training_samples)
-    n_unique_ticks = train_sizes_abs.shape[0]
-    if verbose > 0:
-        print("[learning_curve] Training set sizes: " + str(train_sizes_abs))
-
-    parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch,
-                        verbose=verbose)
-    if exploit_incremental_learning:
-        classes = np.unique(y) if is_classifier(estimator) else None
-        out = parallel(delayed(_incremental_fit_estimator)(
-            clone(estimator), X, y, classes, train, test, train_sizes_abs,
-            scorer, verbose) for train, test in cv)
-    else:
-        out = parallel(delayed(_fit_and_score)(
-            clone(estimator), X, y, scorer, train[:n_train_samples], test,
-            verbose, parameters=None, fit_params=None, return_train_score=True,
-            error_score=error_score)
-            for train, test in cv for n_train_samples in train_sizes_abs)
-        out = np.array(out)[:, :2]
-        n_cv_folds = out.shape[0] // n_unique_ticks
-        out = out.reshape(n_cv_folds, n_unique_ticks, 2)
-
-    out = np.asarray(out).transpose((2, 1, 0))
-
-    return train_sizes_abs, out[0], out[1]
-
-
-def _translate_train_sizes(train_sizes, n_max_training_samples):
-    """Determine absolute sizes of training subsets and validate 'train_sizes'.
-
-    Examples:
-        _translate_train_sizes([0.5, 1.0], 10) -> [5, 10]
-        _translate_train_sizes([5, 10], 10) -> [5, 10]
-
-    Parameters
-    ----------
-    train_sizes : array-like, shape (n_ticks,), dtype float or int
-        Numbers of training examples that will be used to generate the
-        learning curve. If the dtype is float, it is regarded as a
-        fraction of 'n_max_training_samples', i.e. it has to be within (0, 1].
-
-    n_max_training_samples : int
-        Maximum number of training samples (upper bound of 'train_sizes').
-
-    Returns
-    -------
-    train_sizes_abs : array, shape (n_unique_ticks,), dtype int
-        Numbers of training examples that will be used to generate the
-        learning curve. Note that the number of ticks might be less
-        than n_ticks because duplicate entries will be removed.
-    """
-    train_sizes_abs = np.asarray(train_sizes)
-    n_ticks = train_sizes_abs.shape[0]
-    n_min_required_samples = np.min(train_sizes_abs)
-    n_max_required_samples = np.max(train_sizes_abs)
-    if np.issubdtype(train_sizes_abs.dtype, np.floating):
-        if n_min_required_samples <= 0.0 or n_max_required_samples > 1.0:
-            raise ValueError("train_sizes has been interpreted as fractions "
-                             "of the maximum number of training samples and "
-                             "must be within (0, 1], but is within [%f, %f]."
-                             % (n_min_required_samples,
-                                n_max_required_samples))
-        train_sizes_abs = (train_sizes_abs * n_max_training_samples).astype(
-                                 dtype=np.int, copy=False)
-        train_sizes_abs = np.clip(train_sizes_abs, 1,
-                                  n_max_training_samples)
-    else:
-        if (n_min_required_samples <= 0 or
-                n_max_required_samples > n_max_training_samples):
-            raise ValueError("train_sizes has been interpreted as absolute "
-                             "numbers of training samples and must be within "
-                             "(0, %d], but is within [%d, %d]."
-                             % (n_max_training_samples,
-                                n_min_required_samples,
-                                n_max_required_samples))
-
-    train_sizes_abs = np.unique(train_sizes_abs)
-    if n_ticks > train_sizes_abs.shape[0]:
-        warnings.warn("Removed duplicate entries from 'train_sizes'. Number "
-                      "of ticks will be less than the size of "
-                      "'train_sizes' %d instead of %d)."
-                      % (train_sizes_abs.shape[0], n_ticks), RuntimeWarning)
-
-    return train_sizes_abs
-
-
-def _incremental_fit_estimator(estimator, X, y, classes, train, test,
-                               train_sizes, scorer, verbose):
-    """Train estimator on training subsets incrementally and compute scores."""
-    train_scores, test_scores = [], []
-    partitions = zip(train_sizes, np.split(train, train_sizes)[:-1])
-    for n_train_samples, partial_train in partitions:
-        train_subset = train[:n_train_samples]
-        X_train, y_train = _safe_split(estimator, X, y, train_subset)
-        X_partial_train, y_partial_train = _safe_split(estimator, X, y,
-                                                       partial_train)
-        X_test, y_test = _safe_split(estimator, X, y, test, train_subset)
-        if y_partial_train is None:
-            estimator.partial_fit(X_partial_train, classes=classes)
-        else:
-            estimator.partial_fit(X_partial_train, y_partial_train,
-                                  classes=classes)
-        train_scores.append(_score(estimator, X_train, y_train, scorer))
-        test_scores.append(_score(estimator, X_test, y_test, scorer))
-    return np.array((train_scores, test_scores)).T
-
-
-def validation_curve(estimator, X, y, param_name, param_range, cv=None,
-                     scoring=None, n_jobs=1, pre_dispatch="all", verbose=0):
-    """Validation curve.
-
-    .. deprecated:: 0.18
-        This module will be removed in 0.20.
-        Use :func:`sklearn.model_selection.validation_curve` instead.
-
-    Determine training and test scores for varying parameter values.
-
-    Compute scores for an estimator with different values of a specified
-    parameter. This is similar to grid search with one parameter. However, this
-    will also compute training scores and is merely a utility for plotting the
-    results.
-
-    Read more in the :ref:`User Guide <validation_curve>`.
-
-    Parameters
-    ----------
-    estimator : object type that implements the "fit" and "predict" methods
-        An object of that type which is cloned for each validation.
-
-    X : array-like, shape (n_samples, n_features)
-        Training vector, where n_samples is the number of samples and
-        n_features is the number of features.
-
-    y : array-like, shape (n_samples) or (n_samples, n_features), optional
-        Target relative to X for classification or regression;
-        None for unsupervised learning.
-
-    param_name : string
-        Name of the parameter that will be varied.
-
-    param_range : array-like, shape (n_values,)
-        The values of the parameter that will be evaluated.
-
-    cv : int, cross-validation generator or an iterable, optional
-        Determines the cross-validation splitting strategy.
-        Possible inputs for cv are:
-
-        - None, to use the default 3-fold cross-validation,
-        - integer, to specify the number of folds.
-        - An object to be used as a cross-validation generator.
-        - An iterable yielding train/test splits.
-
-        For integer/None inputs, if the estimator is a classifier and ``y`` is
-        either binary or multiclass,
-        :class:`sklearn.model_selection.StratifiedKFold` is used. In all
-        other cases, :class:`sklearn.model_selection.KFold` is used.
-
-        Refer :ref:`User Guide <cross_validation>` for the various
-        cross-validation strategies that can be used here.
-
-    scoring : string, callable or None, optional, default: None
-        A string (see model evaluation documentation) or
-        a scorer callable object / function with signature
-        ``scorer(estimator, X, y)``.
-
-    n_jobs : integer, optional
-        Number of jobs to run in parallel (default 1).
-
-    pre_dispatch : integer or string, optional
-        Number of predispatched jobs for parallel execution (default is
-        all). The option can reduce the allocated memory. The string can
-        be an expression like '2*n_jobs'.
-
-    verbose : integer, optional
-        Controls the verbosity: the higher, the more messages.
-
-    Returns
-    -------
-    train_scores : array, shape (n_ticks, n_cv_folds)
-        Scores on training sets.
-
-    test_scores : array, shape (n_ticks, n_cv_folds)
-        Scores on test set.
-
-    Notes
-    -----
-    See
-    :ref:`examples/model_selection/plot_validation_curve.py
-    <sphx_glr_auto_examples_model_selection_plot_validation_curve.py>`
-    """
-    X, y = indexable(X, y)
-    cv = check_cv(cv, X, y, classifier=is_classifier(estimator))
-    scorer = check_scoring(estimator, scoring=scoring)
-
-    parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch,
-                        verbose=verbose)
-    out = parallel(delayed(_fit_and_score)(
-        clone(estimator), X, y, scorer, train, test, verbose,
-        parameters={param_name: v}, fit_params=None, return_train_score=True)
-        for train, test in cv for v in param_range)
-
-    out = np.asarray(out)[:, :2]
-    n_params = len(param_range)
-    n_cv_folds = out.shape[0] // n_params
-    out = out.reshape(n_cv_folds, n_params, 2).transpose((2, 1, 0))
-
-    return out[0], out[1]
diff --git a/sklearn/linear_model/base.py b/sklearn/linear_model/base.py
index 6bcdd624083e..09c389cb336d 100644
--- a/sklearn/linear_model/base.py
+++ b/sklearn/linear_model/base.py
@@ -26,7 +26,7 @@
 from ..externals import six
 from ..externals.joblib import Parallel, delayed
 from ..base import BaseEstimator, ClassifierMixin, RegressorMixin
-from ..utils import check_array, check_X_y, deprecated, as_float_array
+from ..utils import check_array, check_X_y
 from ..utils.validation import FLOAT_DTYPES
 from ..utils import check_random_state
 from ..utils.extmath import safe_sparse_dot
@@ -67,80 +67,6 @@ def make_dataset(X, y, sample_weight, random_state=None):
     return dataset, intercept_decay
 
 
-@deprecated("sparse_center_data was deprecated in version 0.18 and will be "
-            "removed in 0.20. Use utilities in preprocessing.data instead")
-def sparse_center_data(X, y, fit_intercept, normalize=False):
-    """
-    Compute information needed to center data to have mean zero along
-    axis 0. Be aware that X will not be centered since it would break
-    the sparsity, but will be normalized if asked so.
-    """
-    if fit_intercept:
-        # we might require not to change the csr matrix sometimes
-        # store a copy if normalize is True.
-        # Change dtype to float64 since mean_variance_axis accepts
-        # it that way.
-        if sp.isspmatrix(X) and X.getformat() == 'csr':
-            X = sp.csr_matrix(X, copy=normalize, dtype=np.float64)
-        else:
-            X = sp.csc_matrix(X, copy=normalize, dtype=np.float64)
-
-        X_offset, X_var = mean_variance_axis(X, axis=0)
-        if normalize:
-            # transform variance to std in-place
-            X_var *= X.shape[0]
-            X_std = np.sqrt(X_var, X_var)
-            del X_var
-            X_std[X_std == 0] = 1
-            inplace_column_scale(X, 1. / X_std)
-        else:
-            X_std = np.ones(X.shape[1])
-        y_offset = y.mean(axis=0)
-        y = y - y_offset
-    else:
-        X_offset = np.zeros(X.shape[1])
-        X_std = np.ones(X.shape[1])
-        y_offset = 0. if y.ndim == 1 else np.zeros(y.shape[1], dtype=X.dtype)
-
-    return X, y, X_offset, y_offset, X_std
-
-
-@deprecated("center_data was deprecated in version 0.18 and will be removed "
-            "in 0.20. Use utilities in preprocessing.data instead")
-def center_data(X, y, fit_intercept, normalize=False, copy=True,
-                sample_weight=None):
-    """
-    Centers data to have mean zero along axis 0. This is here because
-    nearly all linear models will want their data to be centered.
-    If sample_weight is not None, then the weighted mean of X and y
-    is zero, and not the mean itself
-    """
-    X = as_float_array(X, copy)
-    if fit_intercept:
-        if isinstance(sample_weight, numbers.Number):
-            sample_weight = None
-        if sp.issparse(X):
-            X_offset = np.zeros(X.shape[1])
-            X_std = np.ones(X.shape[1])
-        else:
-            X_offset = np.average(X, axis=0, weights=sample_weight)
-            X -= X_offset
-            # XXX: currently scaled to variance=n_samples
-            if normalize:
-                X_std = np.sqrt(np.sum(X ** 2, axis=0))
-                X_std[X_std == 0] = 1
-                X /= X_std
-            else:
-                X_std = np.ones(X.shape[1])
-        y_offset = np.average(y, axis=0, weights=sample_weight)
-        y = y - y_offset
-    else:
-        X_offset = np.zeros(X.shape[1])
-        X_std = np.ones(X.shape[1])
-        y_offset = 0. if y.ndim == 1 else np.zeros(y.shape[1], dtype=X.dtype)
-    return X, y, X_offset, y_offset, X_std
-
-
 def _preprocess_data(X, y, fit_intercept, normalize=False, copy=True,
                      sample_weight=None, return_mean=False):
     """
diff --git a/sklearn/linear_model/coordinate_descent.py b/sklearn/linear_model/coordinate_descent.py
index 0bbfb87ebe4c..bdad75bc6197 100644
--- a/sklearn/linear_model/coordinate_descent.py
+++ b/sklearn/linear_model/coordinate_descent.py
@@ -126,7 +126,7 @@ def _alpha_grid(X, y, Xy=None, l1_ratio=1.0, fit_intercept=True,
 def lasso_path(X, y, eps=1e-3, n_alphas=100, alphas=None,
                precompute='auto', Xy=None, copy_X=True, coef_init=None,
                verbose=False, return_n_iter=False, positive=False, **params):
-    r"""Compute Lasso path with coordinate descent
+    """Compute Lasso path with coordinate descent
 
     The Lasso optimization function varies for mono and multi-outputs.
 
@@ -140,7 +140,7 @@ def lasso_path(X, y, eps=1e-3, n_alphas=100, alphas=None,
 
     Where::
 
-        ||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}
+        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}
 
     i.e. the sum of norm of each row.
 
@@ -269,7 +269,7 @@ def enet_path(X, y, l1_ratio=0.5, eps=1e-3, n_alphas=100, alphas=None,
               precompute='auto', Xy=None, copy_X=True, coef_init=None,
               verbose=False, return_n_iter=False, positive=False,
               check_input=True, **params):
-    r"""Compute elastic net path with coordinate descent
+    """Compute elastic net path with coordinate descent
 
     The elastic net optimization function varies for mono and multi-outputs.
 
@@ -287,7 +287,7 @@ def enet_path(X, y, l1_ratio=0.5, eps=1e-3, n_alphas=100, alphas=None,
 
     Where::
 
-        ||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}
+        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}
 
     i.e. the sum of norm of each row.
 
@@ -1599,7 +1599,7 @@ def __init__(self, l1_ratio=0.5, eps=1e-3, n_alphas=100, alphas=None,
 
 
 class MultiTaskElasticNet(Lasso):
-    r"""Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer
+    """Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer
 
     The optimization objective for MultiTaskElasticNet is::
 
@@ -1609,7 +1609,7 @@ class MultiTaskElasticNet(Lasso):
 
     Where::
 
-        ||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}
+        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}
 
     i.e. the sum of norm of each row.
 
@@ -1676,7 +1676,7 @@ class MultiTaskElasticNet(Lasso):
         Independent term in decision function.
 
     coef_ : array, shape (n_tasks, n_features)
-        Parameter vector (W in the cost function formula). If a 1D y is \
+        Parameter vector (W in the cost function formula). If a 1D y is
         passed in at fit (non multi-task usage), ``coef_`` is then a 1D array.
         Note that ``coef_`` stores the transpose of ``W``, ``W.T``.
 
@@ -1798,7 +1798,7 @@ def fit(self, X, y):
 
 
 class MultiTaskLasso(MultiTaskElasticNet):
-    r"""Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer
+    """Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer
 
     The optimization objective for Lasso is::
 
@@ -1806,7 +1806,7 @@ class MultiTaskLasso(MultiTaskElasticNet):
 
     Where::
 
-        ||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}
+        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}
 
     i.e. the sum of norm of each row.
 
@@ -1917,7 +1917,7 @@ def __init__(self, alpha=1.0, fit_intercept=True, normalize=False,
 
 
 class MultiTaskElasticNetCV(LinearModelCV, RegressorMixin):
-    r"""Multi-task L1/L2 ElasticNet with built-in cross-validation.
+    """Multi-task L1/L2 ElasticNet with built-in cross-validation.
 
     The optimization objective for MultiTaskElasticNet is::
 
@@ -1927,7 +1927,7 @@ class MultiTaskElasticNetCV(LinearModelCV, RegressorMixin):
 
     Where::
 
-        ||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}
+        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}
 
     i.e. the sum of norm of each row.
 
@@ -2098,7 +2098,7 @@ def __init__(self, l1_ratio=0.5, eps=1e-3, n_alphas=100, alphas=None,
 
 
 class MultiTaskLassoCV(LinearModelCV, RegressorMixin):
-    r"""Multi-task L1/L2 Lasso with built-in cross-validation.
+    """Multi-task L1/L2 Lasso with built-in cross-validation.
 
     The optimization objective for MultiTaskLasso is::
 
@@ -2106,7 +2106,7 @@ class MultiTaskLassoCV(LinearModelCV, RegressorMixin):
 
     Where::
 
-        ||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}
+        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}
 
     i.e. the sum of norm of each row.
 
diff --git a/sklearn/linear_model/least_angle.py b/sklearn/linear_model/least_angle.py
index 929e6ab6d08b..cd10edcc4e94 100644
--- a/sklearn/linear_model/least_angle.py
+++ b/sklearn/linear_model/least_angle.py
@@ -544,7 +544,7 @@ class Lars(LinearModel, RegressorMixin):
         remove fit_intercept which is set True by default.
 
         .. deprecated:: 0.20
-        
+
             The option is broken and deprecated. It will be removed in v0.22.
 
     Attributes
@@ -619,10 +619,8 @@ def _fit(self, X, y, max_iter, alpha, fit_path, Xy=None):
         """Auxiliary method to fit the model using X, y as training data"""
         n_features = X.shape[1]
 
-        X, y, X_offset, y_offset, X_scale = self._preprocess_data(X, y,
-                                                        self.fit_intercept,
-                                                        self.normalize,
-                                                        self.copy_X)
+        X, y, X_offset, y_offset, X_scale = self._preprocess_data(
+            X, y, self.fit_intercept, self.normalize, self.copy_X)
 
         if y.ndim == 1:
             y = y[:, np.newaxis]
@@ -1174,12 +1172,6 @@ def alpha(self):
         # impedance matching for the above Lars.fit (should not be documented)
         return self.alpha_
 
-    @property
-    @deprecated("Attribute ``cv_mse_path_`` is deprecated in 0.18 and "
-                "will be removed in 0.20. Use ``mse_path_`` instead")
-    def cv_mse_path_(self):
-        return self.mse_path_
-
 
 class LassoLarsCV(LarsCV):
     """Cross-validated Lasso, using the LARS algorithm
diff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py
index 3e8a104d57d7..e4ea696ce714 100644
--- a/sklearn/linear_model/logistic.py
+++ b/sklearn/linear_model/logistic.py
@@ -29,7 +29,8 @@
 from ..utils.fixes import logsumexp
 from ..utils.optimize import newton_cg
 from ..utils.validation import check_X_y
-from ..exceptions import NotFittedError, ConvergenceWarning
+from ..exceptions import (NotFittedError, ConvergenceWarning,
+                          ChangedBehaviorWarning)
 from ..utils.multiclass import check_classification_targets
 from ..externals.joblib import Parallel, delayed
 from ..model_selection import check_cv
@@ -594,7 +595,8 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
 
     # Preprocessing.
     if check_input:
-        X = check_array(X, accept_sparse='csr', dtype=np.float64)
+        X = check_array(X, accept_sparse='csr', dtype=np.float64,
+                        accept_large_sparse=solver != 'liblinear')
         y = check_array(y, ensure_2d=False, dtype=None)
         check_consistent_length(X, y)
     _, n_features = X.shape
@@ -675,7 +677,13 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
                     'shape (%d, %d) or (%d, %d)' % (
                         coef.shape[0], coef.shape[1], classes.size,
                         n_features, classes.size, n_features + 1))
-            w0[:, :coef.shape[1]] = coef
+
+            if n_classes == 1:
+                w0[0, :coef.shape[1]] = -coef
+                w0[1, :coef.shape[1]] = coef
+            else:
+                w0[:, :coef.shape[1]] = coef
+
 
     if multi_class == 'multinomial':
         # fmin_l_bfgs_b and newton-cg accepts only ravelled parameters.
@@ -1083,8 +1091,8 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,
 
     n_jobs : int, default: 1
         Number of CPU cores used when parallelizing over classes if
-        multi_class='ovr'". This parameter is ignored when the ``solver``is set
-        to 'liblinear' regardless of whether 'multi_class' is specified or
+        multi_class='ovr'". This parameter is ignored when the ``solver`` is
+        set to 'liblinear' regardless of whether 'multi_class' is specified or
         not. If given a value of -1, all cores are used.
 
     Attributes
@@ -1213,8 +1221,8 @@ def fit(self, X, y, sample_weight=None):
         else:
             _dtype = np.float64
 
-        X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype,
-                         order="C")
+        X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order="C",
+                         accept_large_sparse=self.solver != 'liblinear')
         check_classification_targets(y)
         self.classes_ = np.unique(y)
         n_samples, n_features = X.shape
@@ -1614,7 +1622,8 @@ def fit(self, X, y, sample_weight=None):
                              "positive; got (tol=%r)" % self.tol)
 
         X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64,
-                         order="C")
+                         order="C",
+                         accept_large_sparse=self.solver != 'liblinear')
         check_classification_targets(y)
 
         class_weight = self.class_weight
@@ -1789,3 +1798,37 @@ def fit(self, X, y, sample_weight=None):
 
         self.C_ = np.asarray(self.C_)
         return self
+
+    def score(self, X, y, sample_weight=None):
+        """Returns the score using the `scoring` option on the given
+        test data and labels.
+
+        Parameters
+        ----------
+        X : array-like, shape = (n_samples, n_features)
+            Test samples.
+
+        y : array-like, shape = (n_samples,)
+            True labels for X.
+
+        sample_weight : array-like, shape = [n_samples], optional
+            Sample weights.
+
+        Returns
+        -------
+        score : float
+            Score of self.predict(X) wrt. y.
+
+        """
+
+        if self.scoring is not None:
+            warnings.warn("The long-standing behavior to use the "
+                          "accuracy score has changed. The scoring "
+                          "parameter is now used. "
+                          "This warning will disappear in version 0.22.",
+                          ChangedBehaviorWarning)
+        scoring = self.scoring or 'accuracy'
+        if isinstance(scoring, six.string_types):
+            scoring = get_scorer(scoring)
+
+        return scoring(self, X, y, sample_weight=sample_weight)
diff --git a/sklearn/linear_model/omp.py b/sklearn/linear_model/omp.py
index 298a1fa4259d..777b915d0339 100644
--- a/sklearn/linear_model/omp.py
+++ b/sklearn/linear_model/omp.py
@@ -191,7 +191,7 @@ def _gram_omp(Gram, Xy, n_nonzero_coefs, tol_0=None, tol=None,
     """
     Gram = Gram.copy('F') if copy_Gram else np.asfortranarray(Gram)
 
-    if copy_Xy:
+    if copy_Xy or not Xy.flags.writeable:
         Xy = Xy.copy()
 
     min_float = np.finfo(Gram.dtype).eps
@@ -491,6 +491,9 @@ def orthogonal_mp_gram(Gram, Xy, n_nonzero_coefs=None, tol=None,
         Xy = Xy[:, np.newaxis]
         if tol is not None:
             norms_squared = [norms_squared]
+    if copy_Xy or not Xy.flags.writeable:
+        # Make the copy once instead of many times in _gram_omp itself.
+        Xy = Xy.copy()
 
     if n_nonzero_coefs is None and tol is None:
         n_nonzero_coefs = int(0.1 * len(Gram))
@@ -515,7 +518,7 @@ def orthogonal_mp_gram(Gram, Xy, n_nonzero_coefs=None, tol=None,
         out = _gram_omp(
             Gram, Xy[:, k], n_nonzero_coefs,
             norms_squared[k] if tol is not None else None, tol,
-            copy_Gram=copy_Gram, copy_Xy=copy_Xy,
+            copy_Gram=copy_Gram, copy_Xy=False,
             return_path=return_path)
         if return_path:
             _, idx, coefs, n_iter = out
diff --git a/sklearn/linear_model/ransac.py b/sklearn/linear_model/ransac.py
index f366e29cab00..f9c372d8ed46 100644
--- a/sklearn/linear_model/ransac.py
+++ b/sklearn/linear_model/ransac.py
@@ -136,17 +136,6 @@ class RANSACRegressor(BaseEstimator, MetaEstimatorMixin, RegressorMixin):
         as 0.99 (the default) and e is the current fraction of inliers w.r.t.
         the total number of samples.
 
-    residual_metric : callable, optional
-        Metric to reduce the dimensionality of the residuals to 1 for
-        multi-dimensional target values ``y.shape[1] > 1``. By default the sum
-        of absolute differences is used::
-
-            lambda dy: np.sum(np.abs(dy), axis=1)
-
-        .. deprecated:: 0.18
-           ``residual_metric`` is deprecated from 0.18 and will be removed in
-           0.20. Use ``loss`` instead.
-
     loss : string, callable, optional, default "absolute_loss"
         String inputs, "absolute_loss" and "squared_loss" are supported which
         find the absolute loss and squared loss per sample
@@ -206,8 +195,8 @@ def __init__(self, base_estimator=None, min_samples=None,
                  residual_threshold=None, is_data_valid=None,
                  is_model_valid=None, max_trials=100, max_skips=np.inf,
                  stop_n_inliers=np.inf, stop_score=np.inf,
-                 stop_probability=0.99, residual_metric=None,
-                 loss='absolute_loss', random_state=None):
+                 stop_probability=0.99, loss='absolute_loss',
+                 random_state=None):
 
         self.base_estimator = base_estimator
         self.min_samples = min_samples
@@ -219,7 +208,6 @@ def __init__(self, base_estimator=None, min_samples=None,
         self.stop_n_inliers = stop_n_inliers
         self.stop_score = stop_score
         self.stop_probability = stop_probability
-        self.residual_metric = residual_metric
         self.random_state = random_state
         self.loss = loss
 
@@ -282,12 +270,6 @@ def fit(self, X, y, sample_weight=None):
         else:
             residual_threshold = self.residual_threshold
 
-        if self.residual_metric is not None:
-            warnings.warn(
-                "'residual_metric' was deprecated in version 0.18 and "
-                "will be removed in version 0.20. Use 'loss' instead.",
-                DeprecationWarning)
-
         if self.loss == "absolute_loss":
             if y.ndim == 1:
                 loss_function = lambda y_true, y_pred: np.abs(y_true - y_pred)
@@ -380,15 +362,7 @@ def fit(self, X, y, sample_weight=None):
 
             # residuals of all data for current random sample model
             y_pred = base_estimator.predict(X)
-
-            # XXX: Deprecation: Remove this if block in 0.20
-            if self.residual_metric is not None:
-                diff = y_pred - y
-                if diff.ndim == 1:
-                    diff = diff.reshape(-1, 1)
-                residuals_subset = self.residual_metric(diff)
-            else:
-                residuals_subset = loss_function(y, y_pred)
+            residuals_subset = loss_function(y, y_pred)
 
             # classify data into inliers and outliers
             inlier_mask_subset = residuals_subset < residual_threshold
diff --git a/sklearn/linear_model/stochastic_gradient.py b/sklearn/linear_model/stochastic_gradient.py
index c6ad4c2754c5..35551dfc39a9 100644
--- a/sklearn/linear_model/stochastic_gradient.py
+++ b/sklearn/linear_model/stochastic_gradient.py
@@ -371,7 +371,8 @@ def _partial_fit(self, X, y, alpha, C,
                      loss, learning_rate, max_iter,
                      classes, sample_weight,
                      coef_init, intercept_init):
-        X, y = check_X_y(X, y, 'csr', dtype=np.float64, order="C")
+        X, y = check_X_y(X, y, 'csr', dtype=np.float64, order="C",
+                         accept_large_sparse=False)
 
         n_samples, n_features = X.shape
 
@@ -419,7 +420,8 @@ def _fit(self, X, y, alpha, C, loss, learning_rate, coef_init=None,
         if hasattr(self, "classes_"):
             self.classes_ = None
 
-        X, y = check_X_y(X, y, 'csr', dtype=np.float64, order="C")
+        X, y = check_X_y(X, y, 'csr', dtype=np.float64, order="C",
+                         accept_large_sparse=False)
         n_samples, n_features = X.shape
 
         # labels can be encoded as float, int, or string literals
@@ -949,7 +951,8 @@ def __init__(self, loss="squared_loss", penalty="l2", alpha=0.0001,
 
     def _partial_fit(self, X, y, alpha, C, loss, learning_rate,
                      max_iter, sample_weight, coef_init, intercept_init):
-        X, y = check_X_y(X, y, "csr", copy=False, order='C', dtype=np.float64)
+        X, y = check_X_y(X, y, "csr", copy=False, order='C', dtype=np.float64,
+                         accept_large_sparse=False)
         y = y.astype(np.float64, copy=False)
 
         n_samples, n_features = X.shape
diff --git a/sklearn/linear_model/tests/test_base.py b/sklearn/linear_model/tests/test_base.py
index ed53e1fbb4aa..30e4cfdcced4 100644
--- a/sklearn/linear_model/tests/test_base.py
+++ b/sklearn/linear_model/tests/test_base.py
@@ -6,17 +6,14 @@
 import numpy as np
 from scipy import sparse
 from scipy import linalg
-from itertools import product
 
 
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_almost_equal
 from sklearn.utils.testing import assert_equal
-from sklearn.utils.testing import ignore_warnings
 
 from sklearn.linear_model.base import LinearRegression
 from sklearn.linear_model.base import _preprocess_data
-from sklearn.linear_model.base import sparse_center_data, center_data
 from sklearn.linear_model.base import _rescale_data
 from sklearn.utils import check_random_state
 from sklearn.utils.testing import assert_greater
@@ -402,74 +399,3 @@ def test_rescale_data():
     rescaled_y2 = y * np.sqrt(sample_weight)
     assert_array_almost_equal(rescaled_X, rescaled_X2)
     assert_array_almost_equal(rescaled_y, rescaled_y2)
-
-
-@ignore_warnings  # all deprecation warnings
-def test_deprecation_center_data():
-    n_samples = 200
-    n_features = 2
-
-    w = 1.0 + rng.rand(n_samples)
-    X = rng.rand(n_samples, n_features)
-    y = rng.rand(n_samples)
-
-    param_grid = product([True, False], [True, False], [True, False],
-                         [None, w])
-
-    for (fit_intercept, normalize, copy, sample_weight) in param_grid:
-
-        XX = X.copy()  # such that we can try copy=False as well
-
-        X1, y1, X1_mean, X1_var, y1_mean = \
-            center_data(XX, y, fit_intercept=fit_intercept,
-                        normalize=normalize, copy=copy,
-                        sample_weight=sample_weight)
-
-        XX = X.copy()
-
-        X2, y2, X2_mean, X2_var, y2_mean = \
-            _preprocess_data(XX, y, fit_intercept=fit_intercept,
-                             normalize=normalize, copy=copy,
-                             sample_weight=sample_weight)
-
-        assert_array_almost_equal(X1, X2)
-        assert_array_almost_equal(y1, y2)
-        assert_array_almost_equal(X1_mean, X2_mean)
-        assert_array_almost_equal(X1_var, X2_var)
-        assert_array_almost_equal(y1_mean, y2_mean)
-
-    # Sparse cases
-    X = sparse.csr_matrix(X)
-
-    for (fit_intercept, normalize, copy, sample_weight) in param_grid:
-
-        X1, y1, X1_mean, X1_var, y1_mean = \
-            center_data(X, y, fit_intercept=fit_intercept, normalize=normalize,
-                        copy=copy, sample_weight=sample_weight)
-
-        X2, y2, X2_mean, X2_var, y2_mean = \
-            _preprocess_data(X, y, fit_intercept=fit_intercept,
-                             normalize=normalize, copy=copy,
-                             sample_weight=sample_weight, return_mean=False)
-
-        assert_array_almost_equal(X1.toarray(), X2.toarray())
-        assert_array_almost_equal(y1, y2)
-        assert_array_almost_equal(X1_mean, X2_mean)
-        assert_array_almost_equal(X1_var, X2_var)
-        assert_array_almost_equal(y1_mean, y2_mean)
-
-    for (fit_intercept, normalize) in product([True, False], [True, False]):
-
-        X1, y1, X1_mean, X1_var, y1_mean = \
-            sparse_center_data(X, y, fit_intercept=fit_intercept,
-                               normalize=normalize)
-
-        X2, y2, X2_mean, X2_var, y2_mean = \
-            _preprocess_data(X, y, fit_intercept=fit_intercept,
-                             normalize=normalize, return_mean=True)
-
-        assert_array_almost_equal(X1.toarray(), X2.toarray())
-        assert_array_almost_equal(y1, y2)
-        assert_array_almost_equal(X1_mean, X2_mean)
-        assert_array_almost_equal(X1_var, X2_var)
-        assert_array_almost_equal(y1_mean, y2_mean)
diff --git a/sklearn/linear_model/tests/test_coordinate_descent.py b/sklearn/linear_model/tests/test_coordinate_descent.py
index a3b35f40a88d..fb65d800e78b 100644
--- a/sklearn/linear_model/tests/test_coordinate_descent.py
+++ b/sklearn/linear_model/tests/test_coordinate_descent.py
@@ -706,17 +706,17 @@ def test_overrided_gram_matrix():
                          clf.fit, X, y)
 
 
-def test_lasso_non_float_y():
+@pytest.mark.parametrize('model', [ElasticNet, Lasso])
+def test_lasso_non_float_y(model):
     X = [[0, 0], [1, 1], [-1, -1]]
     y = [0, 1, 2]
     y_float = [0.0, 1.0, 2.0]
 
-    for model in [ElasticNet, Lasso]:
-        clf = model(fit_intercept=False)
-        clf.fit(X, y)
-        clf_float = model(fit_intercept=False)
-        clf_float.fit(X, y_float)
-        assert_array_equal(clf.coef_, clf_float.coef_)
+    clf = model(fit_intercept=False)
+    clf.fit(X, y)
+    clf_float = model(fit_intercept=False)
+    clf_float.fit(X, y_float)
+    assert_array_equal(clf.coef_, clf_float.coef_)
 
 
 def test_enet_float_precision():
diff --git a/sklearn/linear_model/tests/test_least_angle.py b/sklearn/linear_model/tests/test_least_angle.py
index c2173caa7e6a..dc996ffa4eb2 100644
--- a/sklearn/linear_model/tests/test_least_angle.py
+++ b/sklearn/linear_model/tests/test_least_angle.py
@@ -3,6 +3,8 @@
 import numpy as np
 from scipy import linalg
 
+import pytest
+
 from sklearn.model_selection import train_test_split
 from sklearn.utils.testing import assert_equal
 from sklearn.utils.testing import assert_array_almost_equal
@@ -172,18 +174,20 @@ def test_no_path_all_precomputed():
     assert_true(alpha_ == alphas_[-1])
 
 
-def test_lars_precompute():
+@pytest.mark.parametrize(
+        'classifier',
+        [linear_model.Lars, linear_model.LarsCV, linear_model.LassoLarsIC])
+def test_lars_precompute(classifier):
     # Check for different values of precompute
     X, y = diabetes.data, diabetes.target
     G = np.dot(X.T, X)
-    for classifier in [linear_model.Lars, linear_model.LarsCV,
-                       linear_model.LassoLarsIC]:
-        clf = classifier(precompute=G)
-        output_1 = ignore_warnings(clf.fit)(X, y).coef_
-        for precompute in [True, False, 'auto', None]:
-            clf = classifier(precompute=precompute)
-            output_2 = clf.fit(X, y).coef_
-            assert_array_almost_equal(output_1, output_2, decimal=8)
+
+    clf = classifier(precompute=G)
+    output_1 = ignore_warnings(clf.fit)(X, y).coef_
+    for precompute in [True, False, 'auto', None]:
+        clf = classifier(precompute=precompute)
+        output_2 = clf.fit(X, y).coef_
+        assert_array_almost_equal(output_1, output_2, decimal=8)
 
 
 def test_singular_matrix():
@@ -424,6 +428,7 @@ def test_lars_cv():
 
 def test_lars_cv_max_iter():
     with warnings.catch_warnings(record=True) as w:
+        warnings.simplefilter(action='ignore', category=FutureWarning)
         X = diabetes.data
         y = diabetes.target
         rng = np.random.RandomState(42)
@@ -431,8 +436,7 @@ def test_lars_cv_max_iter():
         X = np.c_[X, x, x]  # add correlated features
         lars_cv = linear_model.LassoLarsCV(max_iter=5)
         lars_cv.fit(X, y)
-    # Expected single FutureWarning for deprecation of n_splits=3
-    assert_true(len(w) != 0)
+    assert_true(len(w) == 0)
 
 
 def test_lasso_lars_ic():
diff --git a/sklearn/linear_model/tests/test_logistic.py b/sklearn/linear_model/tests/test_logistic.py
index a179c89e199a..56be87f71015 100644
--- a/sklearn/linear_model/tests/test_logistic.py
+++ b/sklearn/linear_model/tests/test_logistic.py
@@ -1,12 +1,16 @@
 import numpy as np
 import scipy.sparse as sp
 from scipy import linalg, optimize, sparse
+
+import pytest
+
 from sklearn.datasets import load_iris, make_classification
 from sklearn.metrics import log_loss
 from sklearn.model_selection import StratifiedKFold
 from sklearn.preprocessing import LabelEncoder
 from sklearn.utils import compute_class_weight
 from sklearn.utils.testing import assert_almost_equal
+from sklearn.utils.testing import assert_allclose
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_equal
@@ -19,6 +23,7 @@
 from sklearn.utils.testing import assert_warns_message
 
 from sklearn.exceptions import ConvergenceWarning
+from sklearn.exceptions import ChangedBehaviorWarning
 from sklearn.linear_model.logistic import (
     LogisticRegression,
     logistic_regression_path, LogisticRegressionCV,
@@ -89,6 +94,49 @@ def test_error():
         assert_raise_message(ValueError, msg, LR(max_iter="test").fit, X, Y1)
 
 
+def test_logistic_cv_mock_scorer():
+
+    class MockScorer(object):
+        def __init__(self):
+            self.calls = 0
+            self.scores = [0.1, 0.4, 0.8, 0.5]
+
+        def __call__(self, model, X, y, sample_weight=None):
+            score = self.scores[self.calls % len(self.scores)]
+            self.calls += 1
+            return score
+
+    mock_scorer = MockScorer()
+    Cs = [1, 2, 3, 4]
+    cv = 2
+
+    lr = LogisticRegressionCV(Cs=Cs, scoring=mock_scorer, cv=cv)
+    lr.fit(X, Y1)
+
+    # Cs[2] has the highest score (0.8) from MockScorer
+    assert lr.C_[0] == Cs[2]
+
+    # scorer called 8 times (cv*len(Cs))
+    assert mock_scorer.calls == cv * len(Cs)
+
+    # reset mock_scorer
+    mock_scorer.calls = 0
+    with pytest.warns(ChangedBehaviorWarning):
+        custom_score = lr.score(X, lr.predict(X))
+
+    assert custom_score == mock_scorer.scores[0]
+    assert mock_scorer.calls == 1
+
+
+def test_logistic_cv_score_does_not_warn_by_default():
+    lr = LogisticRegressionCV(cv=2)
+    lr.fit(X, Y1)
+
+    with pytest.warns(None) as record:
+        lr.score(X, lr.predict(X))
+    assert len(record) == 0
+
+
 def test_lr_liblinear_warning():
     n_samples, n_features = iris.data.shape
     target = iris.target_names[iris.target]
@@ -139,63 +187,63 @@ def test_predict_iris():
         assert_greater(np.mean(pred == target), .95)
 
 
-def test_multinomial_validation():
-    for solver in ['lbfgs', 'newton-cg', 'sag', 'saga']:
-        lr = LogisticRegression(C=-1, solver=solver, multi_class='multinomial')
-        assert_raises(ValueError, lr.fit, [[0, 1], [1, 0]], [0, 1])
+@pytest.mark.parametrize('solver', ['lbfgs', 'newton-cg', 'sag', 'saga'])
+def test_multinomial_validation(solver):
+    lr = LogisticRegression(C=-1, solver=solver, multi_class='multinomial')
+    assert_raises(ValueError, lr.fit, [[0, 1], [1, 0]], [0, 1])
 
 
-def test_check_solver_option():
+@pytest.mark.parametrize('LR', [LogisticRegression, LogisticRegressionCV])
+def test_check_solver_option(LR):
     X, y = iris.data, iris.target
-    for LR in [LogisticRegression, LogisticRegressionCV]:
 
-        msg = ('Logistic Regression supports only liblinear, newton-cg, '
-               'lbfgs, sag and saga solvers, got wrong_name')
-        lr = LR(solver="wrong_name")
+    msg = ('Logistic Regression supports only liblinear, newton-cg, '
+           'lbfgs, sag and saga solvers, got wrong_name')
+    lr = LR(solver="wrong_name")
+    assert_raise_message(ValueError, msg, lr.fit, X, y)
+
+    msg = "multi_class should be either multinomial or ovr, got wrong_name"
+    lr = LR(solver='newton-cg', multi_class="wrong_name")
+    assert_raise_message(ValueError, msg, lr.fit, X, y)
+
+    # only 'liblinear' solver
+    msg = "Solver liblinear does not support a multinomial backend."
+    lr = LR(solver='liblinear', multi_class='multinomial')
+    assert_raise_message(ValueError, msg, lr.fit, X, y)
+
+    # all solvers except 'liblinear'
+    for solver in ['newton-cg', 'lbfgs', 'sag']:
+        msg = ("Solver %s supports only l2 penalties, got l1 penalty." %
+               solver)
+        lr = LR(solver=solver, penalty='l1')
         assert_raise_message(ValueError, msg, lr.fit, X, y)
-
-        msg = "multi_class should be either multinomial or ovr, got wrong_name"
-        lr = LR(solver='newton-cg', multi_class="wrong_name")
+    for solver in ['newton-cg', 'lbfgs', 'sag', 'saga']:
+        msg = ("Solver %s supports only dual=False, got dual=True" %
+               solver)
+        lr = LR(solver=solver, dual=True)
         assert_raise_message(ValueError, msg, lr.fit, X, y)
 
-        # only 'liblinear' solver
-        msg = "Solver liblinear does not support a multinomial backend."
-        lr = LR(solver='liblinear', multi_class='multinomial')
-        assert_raise_message(ValueError, msg, lr.fit, X, y)
-
-        # all solvers except 'liblinear'
-        for solver in ['newton-cg', 'lbfgs', 'sag']:
-            msg = ("Solver %s supports only l2 penalties, got l1 penalty." %
-                   solver)
-            lr = LR(solver=solver, penalty='l1')
-            assert_raise_message(ValueError, msg, lr.fit, X, y)
-        for solver in ['newton-cg', 'lbfgs', 'sag', 'saga']:
-            msg = ("Solver %s supports only dual=False, got dual=True" %
-                   solver)
-            lr = LR(solver=solver, dual=True)
-            assert_raise_message(ValueError, msg, lr.fit, X, y)
-
 
-def test_multinomial_binary():
+@pytest.mark.parametrize('solver', ['lbfgs', 'newton-cg', 'sag', 'saga'])
+def test_multinomial_binary(solver):
     # Test multinomial LR on a binary problem.
     target = (iris.target > 0).astype(np.intp)
     target = np.array(["setosa", "not-setosa"])[target]
 
-    for solver in ['lbfgs', 'newton-cg', 'sag', 'saga']:
-        clf = LogisticRegression(solver=solver, multi_class='multinomial',
-                                 random_state=42, max_iter=2000)
-        clf.fit(iris.data, target)
+    clf = LogisticRegression(solver=solver, multi_class='multinomial',
+                             random_state=42, max_iter=2000)
+    clf.fit(iris.data, target)
 
-        assert_equal(clf.coef_.shape, (1, iris.data.shape[1]))
-        assert_equal(clf.intercept_.shape, (1,))
-        assert_array_equal(clf.predict(iris.data), target)
+    assert_equal(clf.coef_.shape, (1, iris.data.shape[1]))
+    assert_equal(clf.intercept_.shape, (1,))
+    assert_array_equal(clf.predict(iris.data), target)
 
-        mlr = LogisticRegression(solver=solver, multi_class='multinomial',
-                                 random_state=42, fit_intercept=False)
-        mlr.fit(iris.data, target)
-        pred = clf.classes_[np.argmax(clf.predict_log_proba(iris.data),
-                                      axis=1)]
-        assert_greater(np.mean(pred == target), .9)
+    mlr = LogisticRegression(solver=solver, multi_class='multinomial',
+                             random_state=42, fit_intercept=False)
+    mlr.fit(iris.data, target)
+    pred = clf.classes_[np.argmax(clf.predict_log_proba(iris.data),
+                                  axis=1)]
+    assert_greater(np.mean(pred == target), .9)
 
 
 def test_multinomial_binary_probabilities():
@@ -1043,7 +1091,9 @@ def test_max_iter():
                 assert_equal(lr.n_iter_[0], max_iter)
 
 
-def test_n_iter():
+@pytest.mark.parametrize('solver',
+                         ['newton-cg', 'liblinear', 'sag', 'saga', 'lbfgs'])
+def test_n_iter(solver):
     # Test that self.n_iter_ has the correct format.
     X, y = iris.data, iris.target
     y_bin = y.copy()
@@ -1052,76 +1102,73 @@ def test_n_iter():
     n_Cs = 4
     n_cv_fold = 2
 
-    for solver in ['newton-cg', 'liblinear', 'sag', 'saga', 'lbfgs']:
-        # OvR case
-        n_classes = 1 if solver == 'liblinear' else np.unique(y).shape[0]
-        clf = LogisticRegression(tol=1e-2, multi_class='ovr',
-                                 solver=solver, C=1.,
-                                 random_state=42, max_iter=100)
-        clf.fit(X, y)
-        assert_equal(clf.n_iter_.shape, (n_classes,))
+    # OvR case
+    n_classes = 1 if solver == 'liblinear' else np.unique(y).shape[0]
+    clf = LogisticRegression(tol=1e-2, multi_class='ovr',
+                             solver=solver, C=1.,
+                             random_state=42, max_iter=100)
+    clf.fit(X, y)
+    assert_equal(clf.n_iter_.shape, (n_classes,))
 
-        n_classes = np.unique(y).shape[0]
-        clf = LogisticRegressionCV(tol=1e-2, multi_class='ovr',
-                                   solver=solver, Cs=n_Cs, cv=n_cv_fold,
-                                   random_state=42, max_iter=100)
-        clf.fit(X, y)
-        assert_equal(clf.n_iter_.shape, (n_classes, n_cv_fold, n_Cs))
-        clf.fit(X, y_bin)
-        assert_equal(clf.n_iter_.shape, (1, n_cv_fold, n_Cs))
-
-        # multinomial case
-        n_classes = 1
-        if solver in ('liblinear', 'sag', 'saga'):
-            break
-
-        clf = LogisticRegression(tol=1e-2, multi_class='multinomial',
-                                 solver=solver, C=1.,
-                                 random_state=42, max_iter=100)
-        clf.fit(X, y)
-        assert_equal(clf.n_iter_.shape, (n_classes,))
+    n_classes = np.unique(y).shape[0]
+    clf = LogisticRegressionCV(tol=1e-2, multi_class='ovr',
+                               solver=solver, Cs=n_Cs, cv=n_cv_fold,
+                               random_state=42, max_iter=100)
+    clf.fit(X, y)
+    assert_equal(clf.n_iter_.shape, (n_classes, n_cv_fold, n_Cs))
+    clf.fit(X, y_bin)
+    assert_equal(clf.n_iter_.shape, (1, n_cv_fold, n_Cs))
+
+    # multinomial case
+    n_classes = 1
+    if solver in ('liblinear', 'sag', 'saga'):
+        return
+
+    clf = LogisticRegression(tol=1e-2, multi_class='multinomial',
+                             solver=solver, C=1.,
+                             random_state=42, max_iter=100)
+    clf.fit(X, y)
+    assert_equal(clf.n_iter_.shape, (n_classes,))
 
-        clf = LogisticRegressionCV(tol=1e-2, multi_class='multinomial',
-                                   solver=solver, Cs=n_Cs, cv=n_cv_fold,
-                                   random_state=42, max_iter=100)
-        clf.fit(X, y)
-        assert_equal(clf.n_iter_.shape, (n_classes, n_cv_fold, n_Cs))
-        clf.fit(X, y_bin)
-        assert_equal(clf.n_iter_.shape, (1, n_cv_fold, n_Cs))
+    clf = LogisticRegressionCV(tol=1e-2, multi_class='multinomial',
+                               solver=solver, Cs=n_Cs, cv=n_cv_fold,
+                               random_state=42, max_iter=100)
+    clf.fit(X, y)
+    assert_equal(clf.n_iter_.shape, (n_classes, n_cv_fold, n_Cs))
+    clf.fit(X, y_bin)
+    assert_equal(clf.n_iter_.shape, (1, n_cv_fold, n_Cs))
 
 
-def test_warm_start():
+@pytest.mark.parametrize('solver', ('newton-cg', 'sag', 'saga', 'lbfgs'))
+@pytest.mark.parametrize('warm_start', (True, False))
+@pytest.mark.parametrize('fit_intercept', (True, False))
+@pytest.mark.parametrize('multi_class', ['ovr', 'multinomial'])
+def test_warm_start(solver, warm_start, fit_intercept, multi_class):
     # A 1-iteration second fit on same data should give almost same result
     # with warm starting, and quite different result without warm starting.
     # Warm starting does not work with liblinear solver.
     X, y = iris.data, iris.target
 
-    solvers = ['newton-cg', 'sag', 'saga', 'lbfgs']
-
-    for warm_start in [True, False]:
-        for fit_intercept in [True, False]:
-            for solver in solvers:
-                for multi_class in ['ovr', 'multinomial']:
-                    clf = LogisticRegression(tol=1e-4, multi_class=multi_class,
-                                             warm_start=warm_start,
-                                             solver=solver,
-                                             random_state=42, max_iter=100,
-                                             fit_intercept=fit_intercept)
-                    with ignore_warnings(category=ConvergenceWarning):
-                        clf.fit(X, y)
-                        coef_1 = clf.coef_
-
-                        clf.max_iter = 1
-                        clf.fit(X, y)
-                    cum_diff = np.sum(np.abs(coef_1 - clf.coef_))
-                    msg = ("Warm starting issue with %s solver in %s mode "
-                           "with fit_intercept=%s and warm_start=%s"
-                           % (solver, multi_class, str(fit_intercept),
-                              str(warm_start)))
-                    if warm_start:
-                        assert_greater(2.0, cum_diff, msg)
-                    else:
-                        assert_greater(cum_diff, 2.0, msg)
+    clf = LogisticRegression(tol=1e-4, multi_class=multi_class,
+                             warm_start=warm_start,
+                             solver=solver,
+                             random_state=42, max_iter=100,
+                             fit_intercept=fit_intercept)
+    with ignore_warnings(category=ConvergenceWarning):
+        clf.fit(X, y)
+        coef_1 = clf.coef_
+
+        clf.max_iter = 1
+        clf.fit(X, y)
+    cum_diff = np.sum(np.abs(coef_1 - clf.coef_))
+    msg = ("Warm starting issue with %s solver in %s mode "
+           "with fit_intercept=%s and warm_start=%s"
+           % (solver, multi_class, str(fit_intercept),
+              str(warm_start)))
+    if warm_start:
+        assert_greater(2.0, cum_diff, msg)
+    else:
+        assert_greater(cum_diff, 2.0, msg)
 
 
 def test_saga_vs_liblinear():
@@ -1192,3 +1239,24 @@ def test_dtype_match():
             lr_64.fit(X_64, y_64)
             assert_equal(lr_64.coef_.dtype, X_64.dtype)
             assert_almost_equal(lr_32.coef_, lr_64.coef_.astype(np.float32))
+
+
+def test_warm_start_converge_LR():
+    # Test to see that the logistic regression converges on warm start,
+    # with multi_class='multinomial'. Non-regressive test for #10836
+
+    rng = np.random.RandomState(0)
+    X = np.concatenate((rng.randn(100, 2) + [1, 1], rng.randn(100, 2)))
+    y = np.array([1] * 100 + [-1] * 100)
+    lr_no_ws = LogisticRegression(multi_class='multinomial',
+                                  solver='sag', warm_start=False,
+                                  random_state=0)
+    lr_ws = LogisticRegression(multi_class='multinomial',
+                               solver='sag', warm_start=True,
+                               random_state=0)
+
+    lr_no_ws_loss = log_loss(y, lr_no_ws.fit(X, y).predict_proba(X))
+    for i in range(5):
+        lr_ws.fit(X, y)
+    lr_ws_loss = log_loss(y, lr_ws.predict_proba(X))
+    assert_allclose(lr_no_ws_loss, lr_ws_loss, rtol=1e-5)
diff --git a/sklearn/linear_model/tests/test_omp.py b/sklearn/linear_model/tests/test_omp.py
index 355c2eaf697b..d083e745f829 100644
--- a/sklearn/linear_model/tests/test_omp.py
+++ b/sklearn/linear_model/tests/test_omp.py
@@ -104,6 +104,20 @@ def test_perfect_signal_recovery():
     assert_array_almost_equal(gamma[:, 0], gamma_gram, decimal=2)
 
 
+def test_orthogonal_mp_gram_readonly():
+    # Non-regression test for:
+    # https://github.com/scikit-learn/scikit-learn/issues/5956
+    idx, = gamma[:, 0].nonzero()
+    G_readonly = G.copy()
+    G_readonly.setflags(write=False)
+    Xy_readonly = Xy.copy()
+    Xy_readonly.setflags(write=False)
+    gamma_gram = orthogonal_mp_gram(G_readonly, Xy_readonly[:, 0], 5,
+                                    copy_Gram=False, copy_Xy=False)
+    assert_array_equal(idx, np.flatnonzero(gamma_gram))
+    assert_array_almost_equal(gamma[:, 0], gamma_gram, decimal=2)
+
+
 def test_estimator():
     omp = OrthogonalMatchingPursuit(n_nonzero_coefs=n_nonzero_coefs)
     omp.fit(X, y[:, 0])
diff --git a/sklearn/linear_model/tests/test_passive_aggressive.py b/sklearn/linear_model/tests/test_passive_aggressive.py
index 5620c29e1837..ee519b7390c5 100644
--- a/sklearn/linear_model/tests/test_passive_aggressive.py
+++ b/sklearn/linear_model/tests/test_passive_aggressive.py
@@ -2,6 +2,8 @@
 import numpy as np
 import scipy.sparse as sp
 
+import pytest
+
 from sklearn.utils.testing import assert_less
 from sklearn.utils.testing import assert_greater
 from sklearn.utils.testing import assert_array_almost_equal, assert_array_equal
@@ -111,23 +113,22 @@ def test_classifier_refit():
     assert_array_equal(clf.classes_, iris.target_names)
 
 
-def test_classifier_correctness():
+@pytest.mark.parametrize('loss', ("hinge", "squared_hinge"))
+def test_classifier_correctness(loss):
     y_bin = y.copy()
     y_bin[y != 1] = -1
 
-    for loss in ("hinge", "squared_hinge"):
-
-        clf1 = MyPassiveAggressive(
-            C=1.0, loss=loss, fit_intercept=True, n_iter=2)
-        clf1.fit(X, y_bin)
+    clf1 = MyPassiveAggressive(
+        C=1.0, loss=loss, fit_intercept=True, n_iter=2)
+    clf1.fit(X, y_bin)
 
-        for data in (X, X_csr):
-            clf2 = PassiveAggressiveClassifier(
-                C=1.0, loss=loss, fit_intercept=True, max_iter=2,
-                shuffle=False, tol=None)
-            clf2.fit(data, y_bin)
+    for data in (X, X_csr):
+        clf2 = PassiveAggressiveClassifier(
+            C=1.0, loss=loss, fit_intercept=True, max_iter=2,
+            shuffle=False, tol=None)
+        clf2.fit(data, y_bin)
 
-            assert_array_almost_equal(clf1.w, clf2.coef_.ravel(), decimal=2)
+        assert_array_almost_equal(clf1.w, clf2.coef_.ravel(), decimal=2)
 
 
 def test_classifier_undefined_methods():
@@ -248,22 +249,24 @@ def test_regressor_partial_fit():
                 assert_true(hasattr(reg, 'standard_coef_'))
 
 
-def test_regressor_correctness():
+@pytest.mark.parametrize(
+        'loss',
+        ("epsilon_insensitive", "squared_epsilon_insensitive"))
+def test_regressor_correctness(loss):
     y_bin = y.copy()
     y_bin[y != 1] = -1
 
-    for loss in ("epsilon_insensitive", "squared_epsilon_insensitive"):
-        reg1 = MyPassiveAggressive(
-            C=1.0, loss=loss, fit_intercept=True, n_iter=2)
-        reg1.fit(X, y_bin)
+    reg1 = MyPassiveAggressive(
+        C=1.0, loss=loss, fit_intercept=True, n_iter=2)
+    reg1.fit(X, y_bin)
 
-        for data in (X, X_csr):
-            reg2 = PassiveAggressiveRegressor(
-                C=1.0, tol=None, loss=loss, fit_intercept=True, max_iter=2,
-                shuffle=False)
-            reg2.fit(data, y_bin)
+    for data in (X, X_csr):
+        reg2 = PassiveAggressiveRegressor(
+            C=1.0, tol=None, loss=loss, fit_intercept=True, max_iter=2,
+            shuffle=False)
+        reg2.fit(data, y_bin)
 
-            assert_array_almost_equal(reg1.w, reg2.coef_.ravel(), decimal=2)
+        assert_array_almost_equal(reg1.w, reg2.coef_.ravel(), decimal=2)
 
 
 def test_regressor_undefined_methods():
diff --git a/sklearn/linear_model/tests/test_ransac.py b/sklearn/linear_model/tests/test_ransac.py
index 31dd1e8d4d73..e328cf94d310 100644
--- a/sklearn/linear_model/tests/test_ransac.py
+++ b/sklearn/linear_model/tests/test_ransac.py
@@ -353,39 +353,6 @@ def test_ransac_multi_dimensional_targets():
     assert_equal(ransac_estimator.inlier_mask_, ref_inlier_mask)
 
 
-# XXX: Remove in 0.20
-def test_ransac_residual_metric():
-    residual_metric1 = lambda dy: np.sum(np.abs(dy), axis=1)
-    residual_metric2 = lambda dy: np.sum(dy ** 2, axis=1)
-
-    yyy = np.column_stack([y, y, y])
-
-    base_estimator = LinearRegression()
-    ransac_estimator0 = RANSACRegressor(base_estimator, min_samples=2,
-                                        residual_threshold=5, random_state=0)
-    ransac_estimator1 = RANSACRegressor(base_estimator, min_samples=2,
-                                        residual_threshold=5, random_state=0,
-                                        residual_metric=residual_metric1)
-    ransac_estimator2 = RANSACRegressor(base_estimator, min_samples=2,
-                                        residual_threshold=5, random_state=0,
-                                        residual_metric=residual_metric2)
-
-    # multi-dimensional
-    ransac_estimator0.fit(X, yyy)
-    assert_warns(DeprecationWarning, ransac_estimator1.fit, X, yyy)
-    assert_warns(DeprecationWarning, ransac_estimator2.fit, X, yyy)
-    assert_array_almost_equal(ransac_estimator0.predict(X),
-                              ransac_estimator1.predict(X))
-    assert_array_almost_equal(ransac_estimator0.predict(X),
-                              ransac_estimator2.predict(X))
-
-    # one-dimensional
-    ransac_estimator0.fit(X, y)
-    assert_warns(DeprecationWarning, ransac_estimator2.fit, X, y)
-    assert_array_almost_equal(ransac_estimator0.predict(X),
-                              ransac_estimator2.predict(X))
-
-
 def test_ransac_residual_loss():
     loss_multi1 = lambda y_true, y_pred: np.sum(np.abs(y_true - y_pred), axis=1)
     loss_multi2 = lambda y_true, y_pred: np.sum((y_true - y_pred) ** 2, axis=1)
diff --git a/sklearn/linear_model/tests/test_ridge.py b/sklearn/linear_model/tests/test_ridge.py
index a2f2a135b3ae..2f574b88ba7b 100644
--- a/sklearn/linear_model/tests/test_ridge.py
+++ b/sklearn/linear_model/tests/test_ridge.py
@@ -3,6 +3,8 @@
 from scipy import linalg
 from itertools import product
 
+import pytest
+
 from sklearn.utils.testing import assert_true
 from sklearn.utils.testing import assert_almost_equal
 from sklearn.utils.testing import assert_array_almost_equal
@@ -57,41 +59,42 @@
 SPARSE_FILTER = lambda X: sp.csr_matrix(X)
 
 
-def test_ridge():
+@pytest.mark.parametrize('solver',
+                         ("svd", "sparse_cg", "cholesky", "lsqr", "sag"))
+def test_ridge(solver):
     # Ridge regression convergence test using score
     # TODO: for this test to be robust, we should use a dataset instead
     # of np.random.
     rng = np.random.RandomState(0)
     alpha = 1.0
 
-    for solver in ("svd", "sparse_cg", "cholesky", "lsqr", "sag"):
-        # With more samples than features
-        n_samples, n_features = 6, 5
-        y = rng.randn(n_samples)
-        X = rng.randn(n_samples, n_features)
+    # With more samples than features
+    n_samples, n_features = 6, 5
+    y = rng.randn(n_samples)
+    X = rng.randn(n_samples, n_features)
 
-        ridge = Ridge(alpha=alpha, solver=solver)
-        ridge.fit(X, y)
-        assert_equal(ridge.coef_.shape, (X.shape[1], ))
-        assert_greater(ridge.score(X, y), 0.47)
+    ridge = Ridge(alpha=alpha, solver=solver)
+    ridge.fit(X, y)
+    assert_equal(ridge.coef_.shape, (X.shape[1], ))
+    assert_greater(ridge.score(X, y), 0.47)
 
-        if solver in ("cholesky", "sag"):
-            # Currently the only solvers to support sample_weight.
-            ridge.fit(X, y, sample_weight=np.ones(n_samples))
-            assert_greater(ridge.score(X, y), 0.47)
+    if solver in ("cholesky", "sag"):
+        # Currently the only solvers to support sample_weight.
+        ridge.fit(X, y, sample_weight=np.ones(n_samples))
+        assert_greater(ridge.score(X, y), 0.47)
 
-        # With more features than samples
-        n_samples, n_features = 5, 10
-        y = rng.randn(n_samples)
-        X = rng.randn(n_samples, n_features)
-        ridge = Ridge(alpha=alpha, solver=solver)
-        ridge.fit(X, y)
-        assert_greater(ridge.score(X, y), .9)
+    # With more features than samples
+    n_samples, n_features = 5, 10
+    y = rng.randn(n_samples)
+    X = rng.randn(n_samples, n_features)
+    ridge = Ridge(alpha=alpha, solver=solver)
+    ridge.fit(X, y)
+    assert_greater(ridge.score(X, y), .9)
 
-        if solver in ("cholesky", "sag"):
-            # Currently the only solvers to support sample_weight.
-            ridge.fit(X, y, sample_weight=np.ones(n_samples))
-            assert_greater(ridge.score(X, y), 0.9)
+    if solver in ("cholesky", "sag"):
+        # Currently the only solvers to support sample_weight.
+        ridge.fit(X, y, sample_weight=np.ones(n_samples))
+        assert_greater(ridge.score(X, y), 0.9)
 
 
 def test_primal_dual_relationship():
@@ -153,6 +156,8 @@ def test_ridge_regression_convergence_fail():
 
 def test_ridge_sample_weights():
     # TODO: loop over sparse data as well
+    # Note: parametrizing this test with pytest results in failed
+    #       assertions, meaning that is is not extremely robust
 
     rng = np.random.RandomState(0)
     param_grid = product((1.0, 1e-2), (True, False),
@@ -483,15 +488,13 @@ def check_dense_sparse(test_func):
         assert_array_almost_equal(ret_dense, ret_sparse, decimal=3)
 
 
-def test_dense_sparse():
-    for test_func in (_test_ridge_loo,
-                      _test_ridge_cv,
-                      _test_ridge_cv_normalize,
-                      _test_ridge_diabetes,
-                      _test_multi_ridge_diabetes,
-                      _test_ridge_classifiers,
-                      _test_tolerance):
-        yield check_dense_sparse, test_func
+@pytest.mark.parametrize(
+        'test_func',
+        (_test_ridge_loo, _test_ridge_cv, _test_ridge_cv_normalize,
+         _test_ridge_diabetes, _test_multi_ridge_diabetes,
+         _test_ridge_classifiers, _test_tolerance))
+def test_dense_sparse(test_func):
+    check_dense_sparse(test_func)
 
 
 def test_ridge_cv_sparse_svd():
@@ -543,33 +546,33 @@ def test_class_weights():
     assert_array_almost_equal(reg.intercept_, rega.intercept_)
 
 
-def test_class_weight_vs_sample_weight():
+@pytest.mark.parametrize('reg', (RidgeClassifier, RidgeClassifierCV))
+def test_class_weight_vs_sample_weight(reg):
     """Check class_weights resemble sample_weights behavior."""
-    for reg in (RidgeClassifier, RidgeClassifierCV):
-
-        # Iris is balanced, so no effect expected for using 'balanced' weights
-        reg1 = reg()
-        reg1.fit(iris.data, iris.target)
-        reg2 = reg(class_weight='balanced')
-        reg2.fit(iris.data, iris.target)
-        assert_almost_equal(reg1.coef_, reg2.coef_)
-
-        # Inflate importance of class 1, check against user-defined weights
-        sample_weight = np.ones(iris.target.shape)
-        sample_weight[iris.target == 1] *= 100
-        class_weight = {0: 1., 1: 100., 2: 1.}
-        reg1 = reg()
-        reg1.fit(iris.data, iris.target, sample_weight)
-        reg2 = reg(class_weight=class_weight)
-        reg2.fit(iris.data, iris.target)
-        assert_almost_equal(reg1.coef_, reg2.coef_)
-
-        # Check that sample_weight and class_weight are multiplicative
-        reg1 = reg()
-        reg1.fit(iris.data, iris.target, sample_weight ** 2)
-        reg2 = reg(class_weight=class_weight)
-        reg2.fit(iris.data, iris.target, sample_weight)
-        assert_almost_equal(reg1.coef_, reg2.coef_)
+
+    # Iris is balanced, so no effect expected for using 'balanced' weights
+    reg1 = reg()
+    reg1.fit(iris.data, iris.target)
+    reg2 = reg(class_weight='balanced')
+    reg2.fit(iris.data, iris.target)
+    assert_almost_equal(reg1.coef_, reg2.coef_)
+
+    # Inflate importance of class 1, check against user-defined weights
+    sample_weight = np.ones(iris.target.shape)
+    sample_weight[iris.target == 1] *= 100
+    class_weight = {0: 1., 1: 100., 2: 1.}
+    reg1 = reg()
+    reg1.fit(iris.data, iris.target, sample_weight)
+    reg2 = reg(class_weight=class_weight)
+    reg2.fit(iris.data, iris.target)
+    assert_almost_equal(reg1.coef_, reg2.coef_)
+
+    # Check that sample_weight and class_weight are multiplicative
+    reg1 = reg()
+    reg1.fit(iris.data, iris.target, sample_weight ** 2)
+    reg2 = reg(class_weight=class_weight)
+    reg2.fit(iris.data, iris.target, sample_weight)
+    assert_almost_equal(reg1.coef_, reg2.coef_)
 
 
 def test_class_weights_cv():
diff --git a/sklearn/linear_model/tests/test_sgd.py b/sklearn/linear_model/tests/test_sgd.py
index 9f372f706ca7..18bc07313965 100644
--- a/sklearn/linear_model/tests/test_sgd.py
+++ b/sklearn/linear_model/tests/test_sgd.py
@@ -1174,16 +1174,16 @@ def test_numerical_stability_large_gradient():
     assert_true(np.isfinite(model.coef_).all())
 
 
-def test_large_regularization():
+@pytest.mark.parametrize('penalty', ['l2', 'l1', 'elasticnet'])
+def test_large_regularization(penalty):
     # Non regression tests for numerical stability issues caused by large
     # regularization parameters
-    for penalty in ['l2', 'l1', 'elasticnet']:
-        model = SGDClassifier(alpha=1e5, learning_rate='constant', eta0=0.1,
-                              penalty=penalty, shuffle=False,
-                              tol=None, max_iter=6)
-        with np.errstate(all='raise'):
-            model.fit(iris.data, iris.target)
-        assert_array_almost_equal(model.coef_, np.zeros_like(model.coef_))
+    model = SGDClassifier(alpha=1e5, learning_rate='constant', eta0=0.1,
+                          penalty=penalty, shuffle=False,
+                          tol=None, max_iter=6)
+    with np.errstate(all='raise'):
+        model.fit(iris.data, iris.target)
+    assert_array_almost_equal(model.coef_, np.zeros_like(model.coef_))
 
 
 def test_tol_parameter():
diff --git a/sklearn/manifold/tests/test_t_sne.py b/sklearn/manifold/tests/test_t_sne.py
index 6b1d87bb18bf..cc692ae0d0cd 100644
--- a/sklearn/manifold/tests/test_t_sne.py
+++ b/sklearn/manifold/tests/test_t_sne.py
@@ -3,6 +3,8 @@
 import numpy as np
 import scipy.sparse as sp
 
+import pytest
+
 from sklearn.neighbors import BallTree
 from sklearn.neighbors import NearestNeighbors
 from sklearn.utils.testing import assert_less_equal
@@ -596,35 +598,35 @@ def test_no_sparse_on_barnes_hut():
                          tsne.fit_transform, X_csr)
 
 
-def test_64bit():
+@pytest.mark.parametrize('method', ['barnes_hut', 'exact'])
+@pytest.mark.parametrize('dt', [np.float32, np.float64])
+def test_64bit(method, dt):
     # Ensure 64bit arrays are handled correctly.
     random_state = check_random_state(0)
-    methods = ['barnes_hut', 'exact']
-    for method in methods:
-        for dt in [np.float32, np.float64]:
-            X = random_state.randn(50, 2).astype(dt)
-            tsne = TSNE(n_components=2, perplexity=2, learning_rate=100.0,
-                        random_state=0, method=method, verbose=0)
-            X_embedded = tsne.fit_transform(X)
-            effective_type = X_embedded.dtype
 
-            # tsne cython code is only single precision, so the output will
-            # always be single precision, irrespectively of the input dtype
-            assert effective_type == np.float32
+    X = random_state.randn(50, 2).astype(dt)
+    tsne = TSNE(n_components=2, perplexity=2, learning_rate=100.0,
+                random_state=0, method=method, verbose=0)
+    X_embedded = tsne.fit_transform(X)
+    effective_type = X_embedded.dtype
 
+    # tsne cython code is only single precision, so the output will
+    # always be single precision, irrespectively of the input dtype
+    assert effective_type == np.float32
 
-def test_kl_divergence_not_nan():
+
+@pytest.mark.parametrize('method', ['barnes_hut', 'exact'])
+def test_kl_divergence_not_nan(method):
     # Ensure kl_divergence_ is computed at last iteration
     # even though n_iter % n_iter_check != 0, i.e. 1003 % 50 != 0
     random_state = check_random_state(0)
-    methods = ['barnes_hut', 'exact']
-    for method in methods:
-        X = random_state.randn(50, 2)
-        tsne = TSNE(n_components=2, perplexity=2, learning_rate=100.0,
-                    random_state=0, method=method, verbose=0, n_iter=1003)
-        tsne.fit_transform(X)
 
-        assert not np.isnan(tsne.kl_divergence_)
+    X = random_state.randn(50, 2)
+    tsne = TSNE(n_components=2, perplexity=2, learning_rate=100.0,
+                random_state=0, method=method, verbose=0, n_iter=1003)
+    tsne.fit_transform(X)
+
+    assert not np.isnan(tsne.kl_divergence_)
 
 
 def test_barnes_hut_angle():
@@ -807,9 +809,9 @@ def assert_uniform_grid(Y, try_name=None):
     assert_less(largest_to_mean, 2, msg=try_name)
 
 
-def test_uniform_grid():
-    for method in ['barnes_hut', 'exact']:
-        yield check_uniform_grid, method
+@pytest.mark.parametrize('method', ['barnes_hut', 'exact'])
+def test_uniform_grid(method):
+    check_uniform_grid(method)
 
 
 def test_bh_match_exact():
diff --git a/sklearn/metrics/classification.py b/sklearn/metrics/classification.py
index b370c1749404..f372461873f6 100644
--- a/sklearn/metrics/classification.py
+++ b/sklearn/metrics/classification.py
@@ -223,6 +223,8 @@ def confusion_matrix(y_true, y_pred, labels=None, sample_weight=None):
     ----------
     .. [1] `Wikipedia entry for the Confusion matrix
            <https://en.wikipedia.org/wiki/Confusion_matrix>`_
+           (Wikipedia and other references may use a different
+           convention for axes)
 
     Examples
     --------
@@ -1427,7 +1429,7 @@ def balanced_accuracy_score(y_true, y_pred, sample_weight=None):
 
 
 def classification_report(y_true, y_pred, labels=None, target_names=None,
-                          sample_weight=None, digits=2):
+                          sample_weight=None, digits=2, output_dict=False):
     """Build a text report showing the main classification metrics
 
     Read more in the :ref:`User Guide <classification_report>`.
@@ -1452,10 +1454,23 @@ def classification_report(y_true, y_pred, labels=None, target_names=None,
     digits : int
         Number of digits for formatting output floating point values
 
+    output_dict: bool (default = False)
+        If True, return output as dict
+
     Returns
     -------
-    report : string
+    report : string / dict
         Text summary of the precision, recall, F1 score for each class.
+        Dictionary returned if output_dict is True. Dictionary has the
+        following structure::
+
+            {'label 1': {'precision':0.5,
+                         'recall':1.0,
+                         'f1-score':0.67,
+                         'support':1},
+             'label 2': { ... },
+              ...
+            }
 
         The reported averages are a prevalence-weighted macro-average across
         classes (equivalent to :func:`precision_recall_fscore_support` with
@@ -1522,24 +1537,36 @@ class 2       1.00      0.67      0.80         3
 
     row_fmt = u'{:>{width}s} ' + u' {:>9.{digits}f}' * 3 + u' {:>9}\n'
     rows = zip(target_names, p, r, f1, s)
+
+    avg_total = [np.average(p, weights=s),
+                 np.average(r, weights=s),
+                 np.average(f1, weights=s),
+                 np.sum(s)]
+
+    if output_dict:
+        report_dict = {label[0]: label[1:] for label in rows}
+
+        for label, scores in report_dict.items():
+            report_dict[label] = dict(zip(headers, scores))
+
+        report_dict['avg / total'] = dict(zip(headers, avg_total))
+
+        return report_dict
+
     for row in rows:
         report += row_fmt.format(*row, width=width, digits=digits)
 
     report += u'\n'
 
-    # compute averages
+    # append averages
     report += row_fmt.format(last_line_heading,
-                             np.average(p, weights=s),
-                             np.average(r, weights=s),
-                             np.average(f1, weights=s),
-                             np.sum(s),
+                             *avg_total,
                              width=width, digits=digits)
 
     return report
 
 
-def hamming_loss(y_true, y_pred, labels=None, sample_weight=None,
-                 classes=None):
+def hamming_loss(y_true, y_pred, labels=None, sample_weight=None):
     """Compute the average Hamming loss.
 
     The Hamming loss is the fraction of labels that are incorrectly predicted.
@@ -1565,13 +1592,6 @@ def hamming_loss(y_true, y_pred, labels=None, sample_weight=None,
 
         .. versionadded:: 0.18
 
-    classes : array, shape = [n_labels], optional
-        Integer array of labels.
-
-        .. deprecated:: 0.18
-           This parameter has been deprecated in favor of ``labels`` in
-           version 0.18 and will be removed in 0.20. Use ``labels`` instead.
-
     Returns
     -------
     loss : float or int,
@@ -1619,10 +1639,6 @@ def hamming_loss(y_true, y_pred, labels=None, sample_weight=None,
     >>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))
     0.75
     """
-    if classes is not None:
-        warnings.warn("'classes' was renamed to 'labels' in version 0.18 and "
-                      "will be removed in 0.20.", DeprecationWarning)
-        labels = classes
 
     y_type, y_true, y_pred = _check_targets(y_true, y_pred)
     check_consistent_length(y_true, y_pred, sample_weight)
diff --git a/sklearn/metrics/cluster/supervised.py b/sklearn/metrics/cluster/supervised.py
index 19bc461c9e9f..db73380fafbf 100644
--- a/sklearn/metrics/cluster/supervised.py
+++ b/sklearn/metrics/cluster/supervised.py
@@ -528,7 +528,7 @@ def v_measure_score(labels_true, labels_pred):
 
 
 def mutual_info_score(labels_true, labels_pred, contingency=None):
-    r"""Mutual Information between two clusterings.
+    """Mutual Information between two clusterings.
 
     The Mutual Information is a measure of the similarity between two labels of
     the same data. Where :math:`|U_i|` is the number of the samples
@@ -538,8 +538,8 @@ def mutual_info_score(labels_true, labels_pred, contingency=None):
 
     .. math::
 
-        MI(U,V)=\sum_{i=1}^{|U|} \sum_{j=1}^{|V|} \\frac{|U_i\cap V_j|}{N}
-        \log\\frac{N|U_i \cap V_j|}{|U_i||V_j|}
+        MI(U,V)=\\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i\\cap V_j|}{N}
+        \\log\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}
 
     This metric is independent of the absolute values of the labels:
     a permutation of the class or cluster label values won't change the
@@ -560,7 +560,7 @@ def mutual_info_score(labels_true, labels_pred, contingency=None):
     labels_pred : array, shape = [n_samples]
         A clustering of the data into disjoint subsets.
 
-    contingency : {None, array, sparse matrix},
+    contingency : {None, array, sparse matrix}, \
                   shape = [n_classes_true, n_classes_pred]
         A contingency matrix given by the :func:`contingency_matrix` function.
         If value is ``None``, it will be computed, otherwise the given value is
diff --git a/sklearn/metrics/cluster/tests/test_common.py b/sklearn/metrics/cluster/tests/test_common.py
index 71534380fe6e..a7e54d22cc7c 100644
--- a/sklearn/metrics/cluster/tests/test_common.py
+++ b/sklearn/metrics/cluster/tests/test_common.py
@@ -101,10 +101,7 @@ def test_non_symmetry(metric_name, y1, y2):
     assert metric(y1, y2) != pytest.approx(metric(y2, y1))
 
 
-@pytest.mark.parametrize(
-    "metric_name",
-    [name for name in NORMALIZED_METRICS]
-)
+@pytest.mark.parametrize("metric_name", NORMALIZED_METRICS)
 def test_normalized_output(metric_name):
     upper_bound_1 = [0, 0, 0, 1, 1, 1]
     upper_bound_2 = [0, 0, 0, 1, 1, 1]
@@ -126,7 +123,7 @@ def test_normalized_output(metric_name):
 # that is when 0 and 1 exchanged.
 @pytest.mark.parametrize(
     "metric_name",
-    [name for name in dict(SUPERVISED_METRICS, **UNSUPERVISED_METRICS)]
+    dict(SUPERVISED_METRICS, **UNSUPERVISED_METRICS)
 )
 def test_permute_labels(metric_name):
     y_label = np.array([0, 0, 0, 1, 1, 0, 1])
@@ -147,7 +144,7 @@ def test_permute_labels(metric_name):
 # For all clustering metrics Input parameters can be both
 @pytest.mark.parametrize(
     "metric_name",
-    [name for name in dict(SUPERVISED_METRICS, **UNSUPERVISED_METRICS)]
+    dict(SUPERVISED_METRICS, **UNSUPERVISED_METRICS)
 )
 # in the form of arrays lists, positive, negetive or string
 def test_format_invariance(metric_name):
diff --git a/sklearn/metrics/cluster/unsupervised.py b/sklearn/metrics/cluster/unsupervised.py
index 6ed380161ddd..7c954acea518 100644
--- a/sklearn/metrics/cluster/unsupervised.py
+++ b/sklearn/metrics/cluster/unsupervised.py
@@ -5,11 +5,16 @@
 #          Thierry Guillemot <thierry.guillemot.work@gmail.com>
 # License: BSD 3 clause
 
+from __future__ import division
+
+import functools
+
 import numpy as np
 
 from ...utils import check_random_state
 from ...utils import check_X_y
 from ...utils import safe_indexing
+from ..pairwise import pairwise_distances_chunked
 from ..pairwise import pairwise_distances
 from ...preprocessing import LabelEncoder
 
@@ -102,6 +107,38 @@ def silhouette_score(X, labels, metric='euclidean', sample_size=None,
     return np.mean(silhouette_samples(X, labels, metric=metric, **kwds))
 
 
+def _silhouette_reduce(D_chunk, start, labels, label_freqs):
+    """Accumulate silhouette statistics for vertical chunk of X
+
+    Parameters
+    ----------
+    D_chunk : shape (n_chunk_samples, n_samples)
+        precomputed distances for a chunk
+    start : int
+        first index in chunk
+    labels : array, shape (n_samples,)
+        corresponding cluster labels, encoded as {0, ..., n_clusters-1}
+    label_freqs : array
+        distribution of cluster labels in ``labels``
+    """
+    # accumulate distances from each sample to each cluster
+    clust_dists = np.zeros((len(D_chunk), len(label_freqs)),
+                           dtype=D_chunk.dtype)
+    for i in range(len(D_chunk)):
+        clust_dists[i] += np.bincount(labels, weights=D_chunk[i],
+                                      minlength=len(label_freqs))
+
+    # intra_index selects intra-cluster distances within clust_dists
+    intra_index = (np.arange(len(D_chunk)), labels[start:start + len(D_chunk)])
+    # intra_clust_dists are averaged over cluster size outside this function
+    intra_clust_dists = clust_dists[intra_index]
+    # of the remaining distances we normalise and extract the minimum
+    clust_dists[intra_index] = np.inf
+    clust_dists /= label_freqs
+    inter_clust_dists = clust_dists.min(axis=1)
+    return intra_clust_dists, inter_clust_dists
+
+
 def silhouette_samples(X, labels, metric='euclidean', **kwds):
     """Compute the Silhouette Coefficient for each sample.
 
@@ -140,7 +177,7 @@ def silhouette_samples(X, labels, metric='euclidean', **kwds):
         allowed by :func:`sklearn.metrics.pairwise.pairwise_distances`. If X is
         the distance array itself, use "precomputed" as the metric.
 
-    **kwds : optional keyword parameters
+    `**kwds` : optional keyword parameters
         Any further parameters are passed directly to the distance function.
         If using a ``scipy.spatial.distance`` metric, the parameters are still
         metric dependent. See the scipy docs for usage examples.
@@ -165,48 +202,28 @@ def silhouette_samples(X, labels, metric='euclidean', **kwds):
     X, labels = check_X_y(X, labels, accept_sparse=['csc', 'csr'])
     le = LabelEncoder()
     labels = le.fit_transform(labels)
-    check_number_of_labels(len(le.classes_), X.shape[0])
-
-    distances = pairwise_distances(X, metric=metric, **kwds)
-    unique_labels = le.classes_
-    n_samples_per_label = np.bincount(labels, minlength=len(unique_labels))
-
-    # For sample i, store the mean distance of the cluster to which
-    # it belongs in intra_clust_dists[i]
-    intra_clust_dists = np.zeros(distances.shape[0], dtype=distances.dtype)
-
-    # For sample i, store the mean distance of the second closest
-    # cluster in inter_clust_dists[i]
-    inter_clust_dists = np.inf + intra_clust_dists
-
-    for curr_label in range(len(unique_labels)):
-
-        # Find inter_clust_dist for all samples belonging to the same
-        # label.
-        mask = labels == curr_label
-        current_distances = distances[mask]
-
-        # Leave out current sample.
-        n_samples_curr_lab = n_samples_per_label[curr_label] - 1
-        if n_samples_curr_lab != 0:
-            intra_clust_dists[mask] = np.sum(
-                current_distances[:, mask], axis=1) / n_samples_curr_lab
-
-        # Now iterate over all other labels, finding the mean
-        # cluster distance that is closest to every sample.
-        for other_label in range(len(unique_labels)):
-            if other_label != curr_label:
-                other_mask = labels == other_label
-                other_distances = np.mean(
-                    current_distances[:, other_mask], axis=1)
-                inter_clust_dists[mask] = np.minimum(
-                    inter_clust_dists[mask], other_distances)
+    n_samples = len(labels)
+    label_freqs = np.bincount(labels)
+    check_number_of_labels(len(le.classes_), n_samples)
+
+    kwds['metric'] = metric
+    reduce_func = functools.partial(_silhouette_reduce,
+                                    labels=labels, label_freqs=label_freqs)
+    results = zip(*pairwise_distances_chunked(X, reduce_func=reduce_func,
+                                              **kwds))
+    intra_clust_dists, inter_clust_dists = results
+    intra_clust_dists = np.concatenate(intra_clust_dists)
+    inter_clust_dists = np.concatenate(inter_clust_dists)
+
+    denom = (label_freqs - 1).take(labels, mode='clip')
+    with np.errstate(divide="ignore", invalid="ignore"):
+        intra_clust_dists /= denom
 
     sil_samples = inter_clust_dists - intra_clust_dists
-    sil_samples /= np.maximum(intra_clust_dists, inter_clust_dists)
-    # score 0 for clusters of size 1, according to the paper
-    sil_samples[n_samples_per_label.take(labels) == 1] = 0
-    return sil_samples
+    with np.errstate(divide="ignore", invalid="ignore"):
+        sil_samples /= np.maximum(intra_clust_dists, inter_clust_dists)
+    # nan values are for clusters of size 1, and should be 0
+    return np.nan_to_num(sil_samples)
 
 
 def calinski_harabaz_score(X, labels):
@@ -285,9 +302,11 @@ def davies_bouldin_score(X, labels):
 
     References
     ----------
-    .. [1] `Davies, David L.; Bouldin, Donald W. (1979).
-       "A Cluster Separation Measure". IEEE Transactions on
-       Pattern Analysis and Machine Intelligence. PAMI-1 (2): 224-227`_
+    .. [1] Davies, David L.; Bouldin, Donald W. (1979).
+       `"A Cluster Separation Measure"
+       <http://ieeexplore.ieee.org/document/4766909>`__.
+       IEEE Transactions on Pattern Analysis and Machine Intelligence.
+       PAMI-1 (2): 224-227
     """
     X, labels = check_X_y(X, labels)
     le = LabelEncoder()
diff --git a/sklearn/metrics/scorer.py b/sklearn/metrics/scorer.py
index c13d59a8daa2..590b9826b4ef 100644
--- a/sklearn/metrics/scorer.py
+++ b/sklearn/metrics/scorer.py
@@ -18,17 +18,17 @@
 #          Arnaud Joly <arnaud.v.joly@gmail.com>
 # License: Simplified BSD
 
-from abc import ABCMeta, abstractmethod
+from abc import ABCMeta
 from collections import Iterable
-import warnings
 
 import numpy as np
 
 from . import (r2_score, median_absolute_error, mean_absolute_error,
                mean_squared_error, mean_squared_log_error, accuracy_score,
                f1_score, roc_auc_score, average_precision_score,
-               precision_score, recall_score, log_loss, balanced_accuracy_score,
-               explained_variance_score, brier_score_loss)
+               precision_score, recall_score, log_loss,
+               balanced_accuracy_score, explained_variance_score,
+               brier_score_loss)
 
 from .cluster import adjusted_rand_score
 from .cluster import homogeneity_score
@@ -49,16 +49,6 @@ def __init__(self, score_func, sign, kwargs):
         self._kwargs = kwargs
         self._score_func = score_func
         self._sign = sign
-        # XXX After removing the deprecated scorers (v0.20) remove the
-        # XXX deprecation_msg property again and remove __call__'s body again
-        self._deprecation_msg = None
-
-    @abstractmethod
-    def __call__(self, estimator, X, y, sample_weight=None):
-        if self._deprecation_msg is not None:
-            warnings.warn(self._deprecation_msg,
-                          category=DeprecationWarning,
-                          stacklevel=2)
 
     def __repr__(self):
         kwargs_string = "".join([", %s=%s" % (str(k), str(v))
@@ -97,8 +87,7 @@ def __call__(self, estimator, X, y_true, sample_weight=None):
         score : float
             Score function applied to prediction of estimator on X.
         """
-        super(_PredictScorer, self).__call__(estimator, X, y_true,
-                                             sample_weight=sample_weight)
+
         y_pred = estimator.predict(X)
         if sample_weight is not None:
             return self._sign * self._score_func(y_true, y_pred,
@@ -134,8 +123,6 @@ def __call__(self, clf, X, y, sample_weight=None):
         score : float
             Score function applied to prediction of estimator on X.
         """
-        super(_ProbaScorer, self).__call__(clf, X, y,
-                                           sample_weight=sample_weight)
         y_type = type_of_target(y)
         y_pred = clf.predict_proba(X)
         if y_type == "binary":
@@ -178,8 +165,6 @@ def __call__(self, clf, X, y, sample_weight=None):
         score : float
             Score function applied to prediction of estimator on X.
         """
-        super(_ThresholdScorer, self).__call__(clf, X, y,
-                                               sample_weight=sample_weight)
         y_type = type_of_target(y)
         if y_type not in ("binary", "multilabel-indicator"):
             raise ValueError("{0} format is not supported".format(y_type))
@@ -226,18 +211,13 @@ def get_scorer(scoring):
     scorer : callable
         The scorer.
     """
-    valid = True
     if isinstance(scoring, six.string_types):
         try:
             scorer = SCORERS[scoring]
         except KeyError:
-            scorers = [scorer for scorer in SCORERS
-                       if SCORERS[scorer]._deprecation_msg is None]
-            valid = False  # Don't raise here to make the error message elegant
-        if not valid:
             raise ValueError('%r is not a valid scoring value. '
                              'Valid options are %s'
-                             % (scoring, sorted(scorers)))
+                             % (scoring, sorted(SCORERS.keys())))
     else:
         scorer = scoring
     return scorer
@@ -476,31 +456,13 @@ def make_scorer(score_func, greater_is_better=True, needs_proba=False,
 r2_scorer = make_scorer(r2_score)
 neg_mean_squared_error_scorer = make_scorer(mean_squared_error,
                                             greater_is_better=False)
-deprecation_msg = ('Scoring method mean_squared_error was renamed to '
-                   'neg_mean_squared_error in version 0.18 and will '
-                   'be removed in 0.20.')
-mean_squared_error_scorer = make_scorer(mean_squared_error,
-                                        greater_is_better=False)
-mean_squared_error_scorer._deprecation_msg = deprecation_msg
 neg_mean_squared_log_error_scorer = make_scorer(mean_squared_log_error,
                                                 greater_is_better=False)
 neg_mean_absolute_error_scorer = make_scorer(mean_absolute_error,
                                              greater_is_better=False)
-deprecation_msg = ('Scoring method mean_absolute_error was renamed to '
-                   'neg_mean_absolute_error in version 0.18 and will '
-                   'be removed in 0.20.')
-mean_absolute_error_scorer = make_scorer(mean_absolute_error,
-                                         greater_is_better=False)
-mean_absolute_error_scorer._deprecation_msg = deprecation_msg
+
 neg_median_absolute_error_scorer = make_scorer(median_absolute_error,
                                                greater_is_better=False)
-deprecation_msg = ('Scoring method median_absolute_error was renamed to '
-                   'neg_median_absolute_error in version 0.18 and will '
-                   'be removed in 0.20.')
-median_absolute_error_scorer = make_scorer(median_absolute_error,
-                                           greater_is_better=False)
-median_absolute_error_scorer._deprecation_msg = deprecation_msg
-
 
 # Standard Classification Scores
 accuracy_scorer = make_scorer(accuracy_score)
@@ -518,11 +480,6 @@ def make_scorer(score_func, greater_is_better=True, needs_proba=False,
 # Score function for probabilistic classification
 neg_log_loss_scorer = make_scorer(log_loss, greater_is_better=False,
                                   needs_proba=True)
-deprecation_msg = ('Scoring method log_loss was renamed to '
-                   'neg_log_loss in version 0.18 and will be removed in 0.20.')
-log_loss_scorer = make_scorer(log_loss, greater_is_better=False,
-                              needs_proba=True)
-log_loss_scorer._deprecation_msg = deprecation_msg
 brier_score_loss_scorer = make_scorer(brier_score_loss,
                                       greater_is_better=False,
                                       needs_proba=True)
@@ -545,13 +502,9 @@ def make_scorer(score_func, greater_is_better=True, needs_proba=False,
                neg_mean_absolute_error=neg_mean_absolute_error_scorer,
                neg_mean_squared_error=neg_mean_squared_error_scorer,
                neg_mean_squared_log_error=neg_mean_squared_log_error_scorer,
-               median_absolute_error=median_absolute_error_scorer,
-               mean_absolute_error=mean_absolute_error_scorer,
-               mean_squared_error=mean_squared_error_scorer,
                accuracy=accuracy_scorer, roc_auc=roc_auc_scorer,
                balanced_accuracy=balanced_accuracy_scorer,
                average_precision=average_precision_scorer,
-               log_loss=log_loss_scorer,
                neg_log_loss=neg_log_loss_scorer,
                brier_score_loss=brier_score_loss_scorer,
                # Cluster metrics that use supervised evaluation
diff --git a/sklearn/metrics/tests/test_classification.py b/sklearn/metrics/tests/test_classification.py
index cae78e721bc8..539d889082a1 100644
--- a/sklearn/metrics/tests/test_classification.py
+++ b/sklearn/metrics/tests/test_classification.py
@@ -6,13 +6,15 @@
 from itertools import product
 import warnings
 
+import pytest
+
 from sklearn import datasets
 from sklearn import svm
 
 from sklearn.datasets import make_multilabel_classification
 from sklearn.preprocessing import label_binarize
 from sklearn.utils.validation import check_random_state
-
+from sklearn.utils.testing import assert_dict_equal
 from sklearn.utils.testing import assert_raises, clean_warning_registry
 from sklearn.utils.testing import assert_raise_message
 from sklearn.utils.testing import assert_equal
@@ -101,6 +103,36 @@ def make_prediction(dataset=None, binary=False):
 ###############################################################################
 # Tests
 
+def test_classification_report_dictionary_output():
+
+    # Test performance report with dictionary output
+    iris = datasets.load_iris()
+    y_true, y_pred, _ = make_prediction(dataset=iris, binary=False)
+
+    # print classification report with class names
+    expected_report = {'setosa': {'precision': 0.82608695652173914,
+                                  'recall': 0.79166666666666663,
+                                  'f1-score': 0.8085106382978724,
+                                  'support': 24},
+                       'versicolor': {'precision': 0.33333333333333331,
+                                      'recall': 0.096774193548387094,
+                                      'f1-score': 0.15000000000000002,
+                                      'support': 31},
+                       'virginica': {'precision': 0.41860465116279072,
+                                     'recall': 0.90000000000000002,
+                                     'f1-score': 0.57142857142857151,
+                                     'support': 20},
+                       'avg / total': {'precision': 0.51375351084147847,
+                                       'recall': 0.53333333333333333,
+                                       'f1-score': 0.47310435663627154,
+                                       'support': 75}}
+
+    report = classification_report(
+        y_true, y_pred, labels=np.arange(len(iris.target_names)),
+        target_names=iris.target_names, output_dict=True)
+
+    assert_dict_equal(report, expected_report)
+
 
 def test_multilabel_accuracy_score_subset_accuracy():
     # Dense label indicator matrix format
@@ -490,7 +522,8 @@ def test_matthews_corrcoef_multiclass():
     assert_almost_equal(mcc, 0.)
 
 
-def test_matthews_corrcoef_overflow():
+@pytest.mark.parametrize('n_points', [100, 10000, 1000000])
+def test_matthews_corrcoef_overflow(n_points):
     # https://github.com/scikit-learn/scikit-learn/issues/9622
     rng = np.random.RandomState(20170906)
 
@@ -513,16 +546,15 @@ def random_ys(n_points):    # binary
         y_pred = (x_pred > 0.5)
         return y_true, y_pred
 
-    for n_points in [100, 10000, 1000000]:
-        arr = np.repeat([0., 1.], n_points)  # binary
-        assert_almost_equal(matthews_corrcoef(arr, arr), 1.0)
-        arr = np.repeat([0., 1., 2.], n_points)  # multiclass
-        assert_almost_equal(matthews_corrcoef(arr, arr), 1.0)
+    arr = np.repeat([0., 1.], n_points)  # binary
+    assert_almost_equal(matthews_corrcoef(arr, arr), 1.0)
+    arr = np.repeat([0., 1., 2.], n_points)  # multiclass
+    assert_almost_equal(matthews_corrcoef(arr, arr), 1.0)
 
-        y_true, y_pred = random_ys(n_points)
-        assert_almost_equal(matthews_corrcoef(y_true, y_true), 1.0)
-        assert_almost_equal(matthews_corrcoef(y_true, y_pred),
-                            mcc_safe(y_true, y_pred))
+    y_true, y_pred = random_ys(n_points)
+    assert_almost_equal(matthews_corrcoef(y_true, y_true), 1.0)
+    assert_almost_equal(matthews_corrcoef(y_true, y_pred),
+                        mcc_safe(y_true, y_pred))
 
 
 def test_precision_recall_f1_score_multiclass():
@@ -580,18 +612,19 @@ def test_precision_recall_f1_score_multiclass():
     assert_array_equal(s, [24, 20, 31])
 
 
-def test_precision_refcall_f1_score_multilabel_unordered_labels():
+@pytest.mark.parametrize('average',
+                         ['samples', 'micro', 'macro', 'weighted', None])
+def test_precision_refcall_f1_score_multilabel_unordered_labels(average):
     # test that labels need not be sorted in the multilabel case
     y_true = np.array([[1, 1, 0, 0]])
     y_pred = np.array([[0, 0, 1, 1]])
-    for average in ['samples', 'micro', 'macro', 'weighted', None]:
-        p, r, f, s = precision_recall_fscore_support(
-            y_true, y_pred, labels=[3, 0, 1, 2], warn_for=[], average=average)
-        assert_array_equal(p, 0)
-        assert_array_equal(r, 0)
-        assert_array_equal(f, 0)
-        if average is None:
-            assert_array_equal(s, [0, 1, 1, 0])
+    p, r, f, s = precision_recall_fscore_support(
+        y_true, y_pred, labels=[3, 0, 1, 2], warn_for=[], average=average)
+    assert_array_equal(p, 0)
+    assert_array_equal(r, 0)
+    assert_array_equal(f, 0)
+    if average is None:
+        assert_array_equal(s, [0, 1, 1, 0])
 
 
 def test_precision_recall_f1_score_binary_averaged():
@@ -957,7 +990,6 @@ def test_multilabel_hamming_loss():
     assert_equal(hamming_loss(y1, np.zeros_like(y1), sample_weight=w), 2. / 3)
     # sp_hamming only works with 1-D arrays
     assert_equal(hamming_loss(y1[0], y2[0]), sp_hamming(y1[0], y2[0]))
-    assert_warns(DeprecationWarning, hamming_loss, y1, y2, classes=[0, 1])
 
 
 def test_multilabel_jaccard_similarity_score():
@@ -1177,10 +1209,33 @@ def test_precision_recall_f1_score_with_an_empty_prediction():
                         0.333, 2)
 
 
-def test_precision_recall_f1_no_labels():
+@pytest.mark.parametrize('beta', [1])
+@pytest.mark.parametrize('average', ["macro", "micro", "weighted", "samples"])
+def test_precision_recall_f1_no_labels(beta, average):
     y_true = np.zeros((20, 3))
     y_pred = np.zeros_like(y_true)
 
+    p, r, f, s = assert_warns(UndefinedMetricWarning,
+                              precision_recall_fscore_support,
+                              y_true, y_pred, average=average,
+                              beta=beta)
+    assert_almost_equal(p, 0)
+    assert_almost_equal(r, 0)
+    assert_almost_equal(f, 0)
+    assert_equal(s, None)
+
+    fbeta = assert_warns(UndefinedMetricWarning, fbeta_score,
+                         y_true, y_pred,
+                         beta=beta, average=average)
+    assert_almost_equal(fbeta, 0)
+
+
+def test_precision_recall_f1_no_labels_average_none():
+    y_true = np.zeros((20, 3))
+    y_pred = np.zeros_like(y_true)
+
+    beta = 1
+
     # tp = [0, 0, 0]
     # fn = [0, 0, 0]
     # fp = [0, 0, 0]
@@ -1189,33 +1244,17 @@ def test_precision_recall_f1_no_labels():
     # |y_i| = [0, 0, 0]
     # |y_hat_i| = [0, 0, 0]
 
-    for beta in [1]:
-        p, r, f, s = assert_warns(UndefinedMetricWarning,
-                                  precision_recall_fscore_support,
-                                  y_true, y_pred, average=None, beta=beta)
-        assert_array_almost_equal(p, [0, 0, 0], 2)
-        assert_array_almost_equal(r, [0, 0, 0], 2)
-        assert_array_almost_equal(f, [0, 0, 0], 2)
-        assert_array_almost_equal(s, [0, 0, 0], 2)
-
-        fbeta = assert_warns(UndefinedMetricWarning, fbeta_score,
-                             y_true, y_pred, beta=beta, average=None)
-        assert_array_almost_equal(fbeta, [0, 0, 0], 2)
-
-        for average in ["macro", "micro", "weighted", "samples"]:
-            p, r, f, s = assert_warns(UndefinedMetricWarning,
-                                      precision_recall_fscore_support,
-                                      y_true, y_pred, average=average,
-                                      beta=beta)
-            assert_almost_equal(p, 0)
-            assert_almost_equal(r, 0)
-            assert_almost_equal(f, 0)
-            assert_equal(s, None)
-
-            fbeta = assert_warns(UndefinedMetricWarning, fbeta_score,
-                                 y_true, y_pred,
-                                 beta=beta, average=average)
-            assert_almost_equal(fbeta, 0)
+    p, r, f, s = assert_warns(UndefinedMetricWarning,
+                              precision_recall_fscore_support,
+                              y_true, y_pred, average=None, beta=beta)
+    assert_array_almost_equal(p, [0, 0, 0], 2)
+    assert_array_almost_equal(r, [0, 0, 0], 2)
+    assert_array_almost_equal(f, [0, 0, 0], 2)
+    assert_array_almost_equal(s, [0, 0, 0], 2)
+
+    fbeta = assert_warns(UndefinedMetricWarning, fbeta_score,
+                         y_true, y_pred, beta=beta, average=None)
+    assert_array_almost_equal(fbeta, [0, 0, 0], 2)
 
 
 def test_prf_warnings():
diff --git a/sklearn/metrics/tests/test_common.py b/sklearn/metrics/tests/test_common.py
index 680b78c3dd43..f835fdd50776 100644
--- a/sklearn/metrics/tests/test_common.py
+++ b/sklearn/metrics/tests/test_common.py
@@ -2,10 +2,13 @@
 
 from functools import partial
 from itertools import product
+from itertools import chain
 
 import numpy as np
 import scipy.sparse as sp
 
+import pytest
+
 from sklearn.datasets import make_multilabel_classification
 from sklearn.preprocessing import LabelBinarizer
 from sklearn.utils.multiclass import type_of_target
@@ -193,7 +196,7 @@
 # is already written.
 
 # Those metrics don't support binary inputs
-METRIC_UNDEFINED_BINARY = [
+METRIC_UNDEFINED_BINARY = {
     "samples_f0.5_score",
     "samples_f1_score",
     "samples_f2_score",
@@ -209,10 +212,10 @@
 
     "label_ranking_loss",
     "label_ranking_average_precision_score",
-]
+}
 
 # Those metrics don't support multiclass inputs
-METRIC_UNDEFINED_MULTICLASS = [
+METRIC_UNDEFINED_MULTICLASS = {
     "brier_score_loss",
     "balanced_accuracy_score",
 
@@ -229,24 +232,24 @@
     "f1_score",
     "f2_score",
     "f0.5_score",
-]
+}
 
 # Metric undefined with "binary" or "multiclass" input
-METRIC_UNDEFINED_BINARY_MULTICLASS = set(METRIC_UNDEFINED_BINARY).union(
-    set(METRIC_UNDEFINED_MULTICLASS))
+METRIC_UNDEFINED_BINARY_MULTICLASS = METRIC_UNDEFINED_BINARY.union(
+    METRIC_UNDEFINED_MULTICLASS)
 
 # Metrics with an "average" argument
-METRICS_WITH_AVERAGING = [
+METRICS_WITH_AVERAGING = {
     "precision_score", "recall_score", "f1_score", "f2_score", "f0.5_score"
-]
+}
 
 # Threshold-based metrics with an "average" argument
-THRESHOLDED_METRICS_WITH_AVERAGING = [
+THRESHOLDED_METRICS_WITH_AVERAGING = {
     "roc_auc_score", "average_precision_score", "partial_roc_auc",
-]
+}
 
 # Metrics with a "pos_label" argument
-METRICS_WITH_POS_LABEL = [
+METRICS_WITH_POS_LABEL = {
     "roc_curve",
 
     "brier_score_loss",
@@ -262,12 +265,12 @@
 
     "macro_f0.5_score", "macro_f1_score", "macro_f2_score",
     "macro_precision_score", "macro_recall_score",
-]
+}
 
 # Metrics with a "labels" argument
 # TODO: Handle multi_class metrics that has a labels argument as well as a
 # decision function argument. e.g hinge_loss
-METRICS_WITH_LABELS = [
+METRICS_WITH_LABELS = {
     "confusion_matrix",
 
     "hamming_loss",
@@ -284,17 +287,17 @@
     "macro_precision_score", "macro_recall_score",
 
     "cohen_kappa_score",
-]
+}
 
 # Metrics with a "normalize" option
-METRICS_WITH_NORMALIZE_OPTION = [
+METRICS_WITH_NORMALIZE_OPTION = {
     "accuracy_score",
     "jaccard_similarity_score",
     "zero_one_loss",
-]
+}
 
 # Threshold-based metrics with "multilabel-indicator" format support
-THRESHOLDED_MULTILABEL_METRICS = [
+THRESHOLDED_MULTILABEL_METRICS = {
     "log_loss",
     "unnormalized_log_loss",
 
@@ -307,10 +310,10 @@
 
     "coverage_error", "label_ranking_loss",
     "label_ranking_average_precision_score",
-]
+}
 
 # Classification metrics with  "multilabel-indicator" format
-MULTILABELS_METRICS = [
+MULTILABELS_METRICS = {
     "accuracy_score", "unnormalized_accuracy_score",
     "hamming_loss",
     "jaccard_similarity_score", "unnormalized_jaccard_similarity_score",
@@ -327,17 +330,17 @@
 
     "samples_f0.5_score", "samples_f1_score", "samples_f2_score",
     "samples_precision_score", "samples_recall_score",
-]
+}
 
 # Regression metrics with "multioutput-continuous" format support
-MULTIOUTPUT_METRICS = [
+MULTIOUTPUT_METRICS = {
     "mean_absolute_error", "mean_squared_error", "r2_score",
     "explained_variance_score"
-]
+}
 
 # Symmetric with respect to their input arguments y_true and y_pred
 # metric(y_true, y_pred) == metric(y_pred, y_true).
-SYMMETRIC_METRICS = [
+SYMMETRIC_METRICS = {
     "accuracy_score", "unnormalized_accuracy_score",
     "hamming_loss",
     "jaccard_similarity_score", "unnormalized_jaccard_similarity_score",
@@ -353,11 +356,11 @@
     "median_absolute_error",
 
     "cohen_kappa_score",
-]
+}
 
 # Asymmetric with respect to their input arguments y_true and y_pred
 # metric(y_true, y_pred) != metric(y_pred, y_true).
-NOT_SYMMETRIC_METRICS = [
+NOT_SYMMETRIC_METRICS = {
     "balanced_accuracy_score",
     "explained_variance_score",
     "r2_score",
@@ -370,18 +373,18 @@
 
     "macro_f0.5_score", "macro_f2_score", "macro_precision_score",
     "macro_recall_score", "log_loss", "hinge_loss"
-]
+}
 
 
 # No Sample weight support
-METRICS_WITHOUT_SAMPLE_WEIGHT = [
+METRICS_WITHOUT_SAMPLE_WEIGHT = {
     "confusion_matrix", # Left this one here because the tests in this file do
                         # not work for confusion_matrix, as its output is a
                         # matrix instead of a number. Testing of
                         # confusion_matrix with sample_weight is in
                         # test_classification.py
     "median_absolute_error",
-]
+}
 
 
 @ignore_warnings
@@ -392,13 +395,13 @@ def test_symmetry():
     y_pred = random_state.randint(0, 2, size=(20, ))
 
     # We shouldn't forget any metrics
-    assert_equal(set(SYMMETRIC_METRICS).union(
-        NOT_SYMMETRIC_METRICS, THRESHOLDED_METRICS,
+    assert_equal(SYMMETRIC_METRICS.union(
+        NOT_SYMMETRIC_METRICS, set(THRESHOLDED_METRICS),
         METRIC_UNDEFINED_BINARY_MULTICLASS),
         set(ALL_METRICS))
 
     assert_equal(
-        set(SYMMETRIC_METRICS).intersection(set(NOT_SYMMETRIC_METRICS)),
+        SYMMETRIC_METRICS.intersection(NOT_SYMMETRIC_METRICS),
         set([]))
 
     # Symmetric metric
@@ -415,17 +418,17 @@ def test_symmetry():
                     msg="%s seems to be symmetric" % name)
 
 
-@ignore_warnings
-def test_sample_order_invariance():
+@pytest.mark.parametrize(
+        'name',
+        set(ALL_METRICS) - METRIC_UNDEFINED_BINARY_MULTICLASS)
+def test_sample_order_invariance(name):
     random_state = check_random_state(0)
     y_true = random_state.randint(0, 2, size=(20, ))
     y_pred = random_state.randint(0, 2, size=(20, ))
     y_true_shuffle, y_pred_shuffle = shuffle(y_true, y_pred, random_state=0)
 
-    for name, metric in ALL_METRICS.items():
-        if name in METRIC_UNDEFINED_BINARY_MULTICLASS:
-            continue
-
+    with ignore_warnings():
+        metric = ALL_METRICS[name]
         assert_almost_equal(metric(y_true, y_pred),
                             metric(y_true_shuffle, y_pred_shuffle),
                             err_msg="%s is not sample order invariant"
@@ -472,8 +475,10 @@ def test_sample_order_invariance_multilabel_and_multioutput():
                                     % name)
 
 
-@ignore_warnings
-def test_format_invariance_with_1d_vectors():
+@pytest.mark.parametrize(
+        'name',
+        set(ALL_METRICS) - METRIC_UNDEFINED_BINARY_MULTICLASS)
+def test_format_invariance_with_1d_vectors(name):
     random_state = check_random_state(0)
     y1 = random_state.randint(0, 2, size=(20, ))
     y2 = random_state.randint(0, 2, size=(20, ))
@@ -489,9 +494,8 @@ def test_format_invariance_with_1d_vectors():
     y1_row = np.reshape(y1_1d, (1, -1))
     y2_row = np.reshape(y2_1d, (1, -1))
 
-    for name, metric in ALL_METRICS.items():
-        if name in METRIC_UNDEFINED_BINARY_MULTICLASS:
-            continue
+    with ignore_warnings():
+        metric = ALL_METRICS[name]
 
         measure = metric(y1, y2)
 
@@ -546,14 +550,16 @@ def test_format_invariance_with_1d_vectors():
 
         # NB: We do not test for y1_row, y2_row as these may be
         # interpreted as multilabel or multioutput data.
-        if (name not in (MULTIOUTPUT_METRICS + THRESHOLDED_MULTILABEL_METRICS +
+        if (name not in (MULTIOUTPUT_METRICS | THRESHOLDED_MULTILABEL_METRICS |
                          MULTILABELS_METRICS)):
             assert_raises(ValueError, metric, y1_row, y2_row)
 
 
-@ignore_warnings
-def test_invariance_string_vs_numbers_labels():
-    # Ensure that classification metrics with string labels
+@pytest.mark.parametrize(
+       'name',
+       set(CLASSIFICATION_METRICS) - METRIC_UNDEFINED_BINARY_MULTICLASS)
+def test_classification_invariance_string_vs_numbers_labels(name):
+    # Ensure that classification metrics with string labels are invariant
     random_state = check_random_state(0)
     y1 = random_state.randint(0, 2, size=(20, ))
     y2 = random_state.randint(0, 2, size=(20, ))
@@ -564,10 +570,8 @@ def test_invariance_string_vs_numbers_labels():
     pos_label_str = "spam"
     labels_str = ["eggs", "spam"]
 
-    for name, metric in CLASSIFICATION_METRICS.items():
-        if name in METRIC_UNDEFINED_BINARY_MULTICLASS:
-            continue
-
+    with ignore_warnings():
+        metric = CLASSIFICATION_METRICS[name]
         measure_with_number = metric(y1, y2)
 
         # Ugly, but handle case with a pos_label and label
@@ -600,7 +604,20 @@ def test_invariance_string_vs_numbers_labels():
                                err_msg="{0} failed string vs number  "
                                        "invariance test".format(name))
 
-    for name, metric in THRESHOLDED_METRICS.items():
+
+@pytest.mark.parametrize('name', THRESHOLDED_METRICS)
+def test_thresholded_invariance_string_vs_numbers_labels(name):
+    # Ensure that thresholded metrics with string labels are invariant
+    random_state = check_random_state(0)
+    y1 = random_state.randint(0, 2, size=(20, ))
+    y2 = random_state.randint(0, 2, size=(20, ))
+
+    y1_str = np.array(["eggs", "spam"])[y1]
+
+    pos_label_str = "spam"
+
+    with ignore_warnings():
+        metric = THRESHOLDED_METRICS[name]
         if name not in METRIC_UNDEFINED_BINARY:
             # Ugly, but handle case with a pos_label and label
             metric_str = metric
@@ -623,28 +640,30 @@ def test_invariance_string_vs_numbers_labels():
             assert_raises(ValueError, metric, y1_str.astype('O'), y2)
 
 
-def test_inf_nan_input():
-    invalids =[([0, 1], [np.inf, np.inf]),
-               ([0, 1], [np.nan, np.nan]),
-               ([0, 1], [np.nan, np.inf])]
+invalids = [([0, 1], [np.inf, np.inf]),
+            ([0, 1], [np.nan, np.nan]),
+            ([0, 1], [np.nan, np.inf])]
+
+
+@pytest.mark.parametrize(
+        'metric',
+        chain(THRESHOLDED_METRICS.values(), REGRESSION_METRICS.values()))
+def test_regression_thresholded_inf_nan_input(metric):
 
-    METRICS = dict()
-    METRICS.update(THRESHOLDED_METRICS)
-    METRICS.update(REGRESSION_METRICS)
+    for y_true, y_score in invalids:
+        assert_raise_message(ValueError,
+                             "contains NaN, infinity",
+                             metric, y_true, y_score)
 
-    for metric in METRICS.values():
-        for y_true, y_score in invalids:
-            assert_raise_message(ValueError,
-                                 "contains NaN, infinity",
-                                 metric, y_true, y_score)
 
+@pytest.mark.parametrize('metric', CLASSIFICATION_METRICS.values())
+def test_classification_inf_nan_input(metric):
     # Classification metrics all raise a mixed input exception
-    for metric in CLASSIFICATION_METRICS.values():
-        for y_true, y_score in invalids:
-            assert_raise_message(ValueError,
-                                 "Classification metrics can't handle a mix "
-                                 "of binary and continuous targets",
-                                 metric, y_true, y_score)
+    for y_true, y_score in invalids:
+        assert_raise_message(ValueError,
+                             "Classification metrics can't handle a mix "
+                             "of binary and continuous targets",
+                             metric, y_true, y_score)
 
 
 @ignore_warnings
@@ -667,45 +686,47 @@ def check_single_sample_multioutput(name):
         metric(np.array([[i, j]]), np.array([[k, l]]))
 
 
-def test_single_sample():
-    for name in ALL_METRICS:
-        if (name in METRIC_UNDEFINED_BINARY_MULTICLASS or
-                name in THRESHOLDED_METRICS):
-            # Those metrics are not always defined with one sample
-            # or in multiclass classification
-            continue
+@pytest.mark.parametrize(
+        'name',
+        (set(ALL_METRICS)
+         # Those metrics are not always defined with one sample
+         # or in multiclass classification
+         - METRIC_UNDEFINED_BINARY_MULTICLASS
+         - set(THRESHOLDED_METRICS)))
+def test_single_sample(name):
+    check_single_sample(name)
 
-        yield check_single_sample, name
 
-    for name in MULTIOUTPUT_METRICS + MULTILABELS_METRICS:
-        yield check_single_sample_multioutput, name
+@pytest.mark.parametrize('name', MULTIOUTPUT_METRICS | MULTILABELS_METRICS)
+def test_single_sample_multioutput(name):
+    check_single_sample_multioutput(name)
 
 
-def test_multioutput_number_of_output_differ():
+@pytest.mark.parametrize('name', MULTIOUTPUT_METRICS)
+def test_multioutput_number_of_output_differ(name):
     y_true = np.array([[1, 0, 0, 1], [0, 1, 1, 1], [1, 1, 0, 1]])
     y_pred = np.array([[0, 0], [1, 0], [0, 0]])
 
-    for name in MULTIOUTPUT_METRICS:
-        metric = ALL_METRICS[name]
-        assert_raises(ValueError, metric, y_true, y_pred)
+    metric = ALL_METRICS[name]
+    assert_raises(ValueError, metric, y_true, y_pred)
 
 
-def test_multioutput_regression_invariance_to_dimension_shuffling():
+@pytest.mark.parametrize('name', MULTIOUTPUT_METRICS)
+def test_multioutput_regression_invariance_to_dimension_shuffling(name):
     # test invariance to dimension shuffling
     random_state = check_random_state(0)
     y_true = random_state.uniform(0, 2, size=(20, 5))
     y_pred = random_state.uniform(0, 2, size=(20, 5))
 
-    for name in MULTIOUTPUT_METRICS:
-        metric = ALL_METRICS[name]
-        error = metric(y_true, y_pred)
+    metric = ALL_METRICS[name]
+    error = metric(y_true, y_pred)
 
-        for _ in range(3):
-            perm = random_state.permutation(y_true.shape[1])
-            assert_almost_equal(metric(y_true[:, perm], y_pred[:, perm]),
-                                error,
-                                err_msg="%s is not dimension shuffling "
-                                        "invariant" % name)
+    for _ in range(3):
+        perm = random_state.permutation(y_true.shape[1])
+        assert_almost_equal(metric(y_true[:, perm], y_pred[:, perm]),
+                            error,
+                            err_msg="%s is not dimension shuffling "
+                                    "invariant" % name)
 
 
 @ignore_warnings
@@ -747,7 +768,8 @@ def test_multilabel_representation_invariance():
                                     "formats." % name)
 
 
-def test_raise_value_error_multilabel_sequences():
+@pytest.mark.parametrize('name', MULTILABELS_METRICS)
+def test_raise_value_error_multilabel_sequences(name):
     # make sure the multilabel-sequence format raises ValueError
     multilabel_sequences = [
         [[0, 1]],
@@ -757,41 +779,41 @@ def test_raise_value_error_multilabel_sequences():
         [()],
         np.array([[], [1, 2]], dtype='object')]
 
-    for name in MULTILABELS_METRICS:
-        metric = ALL_METRICS[name]
-        for seq in multilabel_sequences:
-            assert_raises(ValueError, metric, seq, seq)
+    metric = ALL_METRICS[name]
+    for seq in multilabel_sequences:
+        assert_raises(ValueError, metric, seq, seq)
 
 
-def test_normalize_option_binary_classification(n_samples=20):
+@pytest.mark.parametrize('name', METRICS_WITH_NORMALIZE_OPTION)
+def test_normalize_option_binary_classification(name):
     # Test in the binary case
+    n_samples = 20
     random_state = check_random_state(0)
     y_true = random_state.randint(0, 2, size=(n_samples, ))
     y_pred = random_state.randint(0, 2, size=(n_samples, ))
 
-    for name in METRICS_WITH_NORMALIZE_OPTION:
-        metrics = ALL_METRICS[name]
-        measure = metrics(y_true, y_pred, normalize=True)
-        assert_greater(measure, 0,
-                       msg="We failed to test correctly the normalize option")
-        assert_almost_equal(metrics(y_true, y_pred, normalize=False)
-                            / n_samples, measure)
+    metrics = ALL_METRICS[name]
+    measure = metrics(y_true, y_pred, normalize=True)
+    assert_greater(measure, 0,
+                   msg="We failed to test correctly the normalize option")
+    assert_almost_equal(metrics(y_true, y_pred, normalize=False)
+                        / n_samples, measure)
 
 
-def test_normalize_option_multiclass_classification():
+@pytest.mark.parametrize('name', METRICS_WITH_NORMALIZE_OPTION)
+def test_normalize_option_multiclass_classification(name):
     # Test in the multiclass case
     random_state = check_random_state(0)
     y_true = random_state.randint(0, 4, size=(20, ))
     y_pred = random_state.randint(0, 4, size=(20, ))
     n_samples = y_true.shape[0]
 
-    for name in METRICS_WITH_NORMALIZE_OPTION:
-        metrics = ALL_METRICS[name]
-        measure = metrics(y_true, y_pred, normalize=True)
-        assert_greater(measure, 0,
-                       msg="We failed to test correctly the normalize option")
-        assert_almost_equal(metrics(y_true, y_pred, normalize=False)
-                            / n_samples, measure)
+    metrics = ALL_METRICS[name]
+    measure = metrics(y_true, y_pred, normalize=True)
+    assert_greater(measure, 0,
+                   msg="We failed to test correctly the normalize option")
+    assert_almost_equal(metrics(y_true, y_pred, normalize=False)
+                        / n_samples, measure)
 
 
 def test_normalize_option_multilabel_classification():
@@ -886,7 +908,9 @@ def check_averaging(name, y_true, y_true_binarize, y_pred, y_pred_binarize,
         raise ValueError("Metric is not recorded as having an average option")
 
 
-def test_averaging_multiclass(n_samples=50, n_classes=3):
+@pytest.mark.parametrize('name', METRICS_WITH_AVERAGING)
+def test_averaging_multiclass(name):
+    n_samples, n_classes = 50, 3
     random_state = check_random_state(0)
     y_true = random_state.randint(0, n_classes, size=(n_samples, ))
     y_pred = random_state.randint(0, n_classes, size=(n_samples, ))
@@ -896,12 +920,14 @@ def test_averaging_multiclass(n_samples=50, n_classes=3):
     y_true_binarize = lb.transform(y_true)
     y_pred_binarize = lb.transform(y_pred)
 
-    for name in METRICS_WITH_AVERAGING:
-        yield (check_averaging, name, y_true, y_true_binarize,
-               y_pred, y_pred_binarize, y_score)
+    check_averaging(name, y_true, y_true_binarize,
+                    y_pred, y_pred_binarize, y_score)
 
 
-def test_averaging_multilabel(n_classes=5, n_samples=40):
+@pytest.mark.parametrize(
+        'name', METRICS_WITH_AVERAGING | THRESHOLDED_METRICS_WITH_AVERAGING)
+def test_averaging_multilabel(name):
+    n_samples, n_classes = 40, 5
     _, y = make_multilabel_classification(n_features=1, n_classes=n_classes,
                                           random_state=5, n_samples=n_samples,
                                           allow_unlabeled=False)
@@ -911,22 +937,27 @@ def test_averaging_multilabel(n_classes=5, n_samples=40):
     y_true_binarize = y_true
     y_pred_binarize = y_pred
 
-    for name in METRICS_WITH_AVERAGING + THRESHOLDED_METRICS_WITH_AVERAGING:
-        yield (check_averaging, name, y_true, y_true_binarize,
-               y_pred, y_pred_binarize, y_score)
+    check_averaging(name, y_true, y_true_binarize,
+                    y_pred, y_pred_binarize, y_score)
 
 
-def test_averaging_multilabel_all_zeroes():
+@pytest.mark.parametrize('name', METRICS_WITH_AVERAGING)
+def test_averaging_multilabel_all_zeroes(name):
     y_true = np.zeros((20, 3))
     y_pred = np.zeros((20, 3))
     y_score = np.zeros((20, 3))
     y_true_binarize = y_true
     y_pred_binarize = y_pred
 
-    for name in METRICS_WITH_AVERAGING:
-        yield (check_averaging, name, y_true, y_true_binarize,
-               y_pred, y_pred_binarize, y_score)
+    check_averaging(name, y_true, y_true_binarize,
+                    y_pred, y_pred_binarize, y_score)
+
 
+def test_averaging_binary_multilabel_all_zeroes():
+    y_true = np.zeros((20, 3))
+    y_pred = np.zeros((20, 3))
+    y_true_binarize = y_true
+    y_pred_binarize = y_pred
     # Test _average_binary_score for weight.sum() == 0
     binary_metric = (lambda y_true, y_score, average="macro":
                      _average_binary_score(
@@ -935,16 +966,16 @@ def test_averaging_multilabel_all_zeroes():
                      y_pred_binarize, is_multilabel=True)
 
 
-def test_averaging_multilabel_all_ones():
+@pytest.mark.parametrize('name', METRICS_WITH_AVERAGING)
+def test_averaging_multilabel_all_ones(name):
     y_true = np.ones((20, 3))
     y_pred = np.ones((20, 3))
     y_score = np.ones((20, 3))
     y_true_binarize = y_true
     y_pred_binarize = y_pred
 
-    for name in METRICS_WITH_AVERAGING:
-        yield (check_averaging, name, y_true, y_true_binarize,
-               y_pred, y_pred_binarize, y_score)
+    check_averaging(name, y_true, y_true_binarize,
+                    y_pred, y_pred_binarize, y_score)
 
 
 @ignore_warnings
@@ -1022,54 +1053,64 @@ def check_sample_weight_invariance(name, metric, y1, y2):
                                                   sample_weight]))
 
 
-def test_sample_weight_invariance(n_samples=50):
+@pytest.mark.parametrize(
+        'name',
+        (set(ALL_METRICS).intersection(set(REGRESSION_METRICS))
+         - METRICS_WITHOUT_SAMPLE_WEIGHT))
+def test_regression_sample_weight_invariance(name):
+    n_samples = 50
     random_state = check_random_state(0)
     # regression
     y_true = random_state.random_sample(size=(n_samples,))
     y_pred = random_state.random_sample(size=(n_samples,))
-    for name in ALL_METRICS:
-        if name not in REGRESSION_METRICS:
-            continue
-        if name in METRICS_WITHOUT_SAMPLE_WEIGHT:
-            continue
-        metric = ALL_METRICS[name]
-        yield check_sample_weight_invariance, name, metric, y_true, y_pred
+    metric = ALL_METRICS[name]
+    check_sample_weight_invariance(name, metric, y_true, y_pred)
+
 
+@pytest.mark.parametrize(
+        'name',
+        (set(ALL_METRICS) - set(REGRESSION_METRICS)
+         - METRICS_WITHOUT_SAMPLE_WEIGHT - METRIC_UNDEFINED_BINARY))
+def test_binary_sample_weight_invariance(name):
     # binary
+    n_samples = 50
     random_state = check_random_state(0)
     y_true = random_state.randint(0, 2, size=(n_samples, ))
     y_pred = random_state.randint(0, 2, size=(n_samples, ))
     y_score = random_state.random_sample(size=(n_samples,))
-    for name in ALL_METRICS:
-        if name in REGRESSION_METRICS:
-            continue
-        if (name in METRICS_WITHOUT_SAMPLE_WEIGHT or
-                name in METRIC_UNDEFINED_BINARY):
-            continue
-        metric = ALL_METRICS[name]
-        if name in THRESHOLDED_METRICS:
-            yield check_sample_weight_invariance, name, metric, y_true, y_score
-        else:
-            yield check_sample_weight_invariance, name, metric, y_true, y_pred
+    metric = ALL_METRICS[name]
+    if name in THRESHOLDED_METRICS:
+        check_sample_weight_invariance(name, metric, y_true, y_score)
+    else:
+        check_sample_weight_invariance(name, metric, y_true, y_pred)
+
 
+@pytest.mark.parametrize(
+        'name',
+        (set(ALL_METRICS) - set(REGRESSION_METRICS)
+         - METRICS_WITHOUT_SAMPLE_WEIGHT
+         - METRIC_UNDEFINED_BINARY_MULTICLASS))
+def test_multiclass_sample_weight_invariance(name):
     # multiclass
+    n_samples = 50
     random_state = check_random_state(0)
     y_true = random_state.randint(0, 5, size=(n_samples, ))
     y_pred = random_state.randint(0, 5, size=(n_samples, ))
     y_score = random_state.random_sample(size=(n_samples, 5))
-    for name in ALL_METRICS:
-        if name in REGRESSION_METRICS:
-            continue
-        if (name in METRICS_WITHOUT_SAMPLE_WEIGHT or
-                name in METRIC_UNDEFINED_BINARY_MULTICLASS):
-            continue
-        metric = ALL_METRICS[name]
-        if name in THRESHOLDED_METRICS:
-            yield check_sample_weight_invariance, name, metric, y_true, y_score
-        else:
-            yield check_sample_weight_invariance, name, metric, y_true, y_pred
+    metric = ALL_METRICS[name]
+    if name in THRESHOLDED_METRICS:
+        check_sample_weight_invariance(name, metric, y_true, y_score)
+    else:
+        check_sample_weight_invariance(name, metric, y_true, y_pred)
 
+
+@pytest.mark.parametrize(
+        'name',
+        (MULTILABELS_METRICS | THRESHOLDED_MULTILABEL_METRICS |
+         MULTIOUTPUT_METRICS) - METRICS_WITHOUT_SAMPLE_WEIGHT)
+def test_multilabel_sample_weight_invariance(name):
     # multilabel indicator
+    random_state = check_random_state(0)
     _, ya = make_multilabel_classification(n_features=1, n_classes=20,
                                            random_state=0, n_samples=100,
                                            allow_unlabeled=False)
@@ -1080,18 +1121,11 @@ def test_sample_weight_invariance(n_samples=50):
     y_pred = np.vstack([ya, ya])
     y_score = random_state.randint(1, 4, size=y_true.shape)
 
-    for name in (MULTILABELS_METRICS + THRESHOLDED_MULTILABEL_METRICS +
-                 MULTIOUTPUT_METRICS):
-        if name in METRICS_WITHOUT_SAMPLE_WEIGHT:
-            continue
-
-        metric = ALL_METRICS[name]
-        if name in THRESHOLDED_METRICS:
-            yield (check_sample_weight_invariance, name, metric,
-                   y_true, y_score)
-        else:
-            yield (check_sample_weight_invariance, name, metric,
-                   y_true, y_pred)
+    metric = ALL_METRICS[name]
+    if name in THRESHOLDED_METRICS:
+        check_sample_weight_invariance(name, metric, y_true, y_score)
+    else:
+        check_sample_weight_invariance(name, metric, y_true, y_pred)
 
 
 @ignore_warnings
diff --git a/sklearn/metrics/tests/test_pairwise.py b/sklearn/metrics/tests/test_pairwise.py
index 0ef089c7a361..e63219a817be 100644
--- a/sklearn/metrics/tests/test_pairwise.py
+++ b/sklearn/metrics/tests/test_pairwise.py
@@ -2,11 +2,12 @@
 
 import numpy as np
 from numpy import linalg
-import pytest
 
 from scipy.sparse import dok_matrix, csr_matrix, issparse
 from scipy.spatial.distance import cosine, cityblock, minkowski, wminkowski
 
+import pytest
+
 from sklearn.utils.testing import assert_greater
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_allclose
@@ -129,52 +130,52 @@ def test_pairwise_distances():
     assert_raises(ValueError, pairwise_distances, X, Y, metric="blah")
 
 
-# ignore conversion to boolean in pairwise_distances
-@ignore_warnings(category=DataConversionWarning)
-def test_pairwise_boolean_distance():
+@pytest.mark.parametrize('metric', PAIRWISE_BOOLEAN_FUNCTIONS)
+def test_pairwise_boolean_distance(metric):
     # test that we convert to boolean arrays for boolean distances
     rng = np.random.RandomState(0)
     X = rng.randn(5, 4)
     Y = X.copy()
     Y[0, 0] = 1 - Y[0, 0]
 
-    for metric in PAIRWISE_BOOLEAN_FUNCTIONS:
+    # ignore conversion to boolean in pairwise_distances
+    with ignore_warnings(category=DataConversionWarning):
         for Z in [Y, None]:
             res = pairwise_distances(X, Z, metric=metric)
             res[np.isnan(res)] = 0
             assert_true(np.sum(res != 0) == 0)
 
 
-def test_pairwise_precomputed():
-    for func in [pairwise_distances, pairwise_kernels]:
-        # Test correct shape
-        assert_raises_regexp(ValueError, '.* shape .*',
-                             func, np.zeros((5, 3)), metric='precomputed')
-        # with two args
-        assert_raises_regexp(ValueError, '.* shape .*',
-                             func, np.zeros((5, 3)), np.zeros((4, 4)),
-                             metric='precomputed')
-        # even if shape[1] agrees (although thus second arg is spurious)
-        assert_raises_regexp(ValueError, '.* shape .*',
-                             func, np.zeros((5, 3)), np.zeros((4, 3)),
-                             metric='precomputed')
-
-        # Test not copied (if appropriate dtype)
-        S = np.zeros((5, 5))
-        S2 = func(S, metric="precomputed")
-        assert_true(S is S2)
-        # with two args
-        S = np.zeros((5, 3))
-        S2 = func(S, np.zeros((3, 3)), metric="precomputed")
-        assert_true(S is S2)
-
-        # Test always returns float dtype
-        S = func(np.array([[1]], dtype='int'), metric='precomputed')
-        assert_equal('f', S.dtype.kind)
-
-        # Test converts list to array-like
-        S = func([[1.]], metric='precomputed')
-        assert_true(isinstance(S, np.ndarray))
+@pytest.mark.parametrize('func', [pairwise_distances, pairwise_kernels])
+def test_pairwise_precomputed(func):
+    # Test correct shape
+    assert_raises_regexp(ValueError, '.* shape .*',
+                         func, np.zeros((5, 3)), metric='precomputed')
+    # with two args
+    assert_raises_regexp(ValueError, '.* shape .*',
+                         func, np.zeros((5, 3)), np.zeros((4, 4)),
+                         metric='precomputed')
+    # even if shape[1] agrees (although thus second arg is spurious)
+    assert_raises_regexp(ValueError, '.* shape .*',
+                         func, np.zeros((5, 3)), np.zeros((4, 3)),
+                         metric='precomputed')
+
+    # Test not copied (if appropriate dtype)
+    S = np.zeros((5, 5))
+    S2 = func(S, metric="precomputed")
+    assert_true(S is S2)
+    # with two args
+    S = np.zeros((5, 3))
+    S2 = func(S, np.zeros((3, 3)), metric="precomputed")
+    assert_true(S is S2)
+
+    # Test always returns float dtype
+    S = func(np.array([[1]], dtype='int'), metric='precomputed')
+    assert_equal('f', S.dtype.kind)
+
+    # Test converts list to array-like
+    S = func([[1.]], metric='precomputed')
+    assert_true(isinstance(S, np.ndarray))
 
 
 def check_pairwise_parallel(func, metric, kwds):
@@ -202,16 +203,24 @@ def check_pairwise_parallel(func, metric, kwds):
         assert_array_almost_equal(S, S2)
 
 
-def test_pairwise_parallel():
-    wminkowski_kwds = {'w': np.arange(1, 5).astype('double'), 'p': 1}
-    metrics = [(pairwise_distances, 'euclidean', {}),
-               (pairwise_distances, wminkowski, wminkowski_kwds),
-               (pairwise_distances, 'wminkowski', wminkowski_kwds),
-               (pairwise_kernels, 'polynomial', {'degree': 1}),
-               (pairwise_kernels, callable_rbf_kernel, {'gamma': .1}),
-               ]
-    for func, metric, kwds in metrics:
-        yield check_pairwise_parallel, func, metric, kwds
+_wminkowski_kwds = {'w': np.arange(1, 5).astype('double'), 'p': 1}
+
+
+def callable_rbf_kernel(x, y, **kwds):
+    # Callable version of pairwise.rbf_kernel.
+    K = rbf_kernel(np.atleast_2d(x), np.atleast_2d(y), **kwds)
+    return K
+
+
+@pytest.mark.parametrize(
+        'func, metric, kwds',
+        [(pairwise_distances, 'euclidean', {}),
+         (pairwise_distances, wminkowski, _wminkowski_kwds),
+         (pairwise_distances, 'wminkowski', _wminkowski_kwds),
+         (pairwise_kernels, 'polynomial', {'degree': 1}),
+         (pairwise_kernels, callable_rbf_kernel, {'gamma': .1})])
+def test_pairwise_parallel(func, metric, kwds):
+    check_pairwise_parallel(func, metric, kwds)
 
 
 def test_pairwise_callable_nonstrict_metric():
@@ -221,47 +230,51 @@ def test_pairwise_callable_nonstrict_metric():
     assert_equal(pairwise_distances([[1.]], metric=lambda x, y: 5)[0, 0], 5)
 
 
-def callable_rbf_kernel(x, y, **kwds):
-    # Callable version of pairwise.rbf_kernel.
-    K = rbf_kernel(np.atleast_2d(x), np.atleast_2d(y), **kwds)
-    return K
+# Test with all metrics that should be in PAIRWISE_KERNEL_FUNCTIONS.
+@pytest.mark.parametrize(
+        'metric',
+        ["rbf", "laplacian", "sigmoid", "polynomial", "linear",
+         "chi2", "additive_chi2"])
+def test_pairwise_kernels(metric):
+    # Test the pairwise_kernels helper function.
+
+    rng = np.random.RandomState(0)
+    X = rng.random_sample((5, 4))
+    Y = rng.random_sample((2, 4))
+    function = PAIRWISE_KERNEL_FUNCTIONS[metric]
+    # Test with Y=None
+    K1 = pairwise_kernels(X, metric=metric)
+    K2 = function(X)
+    assert_array_almost_equal(K1, K2)
+    # Test with Y=Y
+    K1 = pairwise_kernels(X, Y=Y, metric=metric)
+    K2 = function(X, Y=Y)
+    assert_array_almost_equal(K1, K2)
+    # Test with tuples as X and Y
+    X_tuples = tuple([tuple([v for v in row]) for row in X])
+    Y_tuples = tuple([tuple([v for v in row]) for row in Y])
+    K2 = pairwise_kernels(X_tuples, Y_tuples, metric=metric)
+    assert_array_almost_equal(K1, K2)
 
+    # Test with sparse X and Y
+    X_sparse = csr_matrix(X)
+    Y_sparse = csr_matrix(Y)
+    if metric in ["chi2", "additive_chi2"]:
+        # these don't support sparse matrices yet
+        assert_raises(ValueError, pairwise_kernels,
+                      X_sparse, Y=Y_sparse, metric=metric)
+        return
+    K1 = pairwise_kernels(X_sparse, Y=Y_sparse, metric=metric)
+    assert_array_almost_equal(K1, K2)
 
-def test_pairwise_kernels():    # Test the pairwise_kernels helper function.
 
+def test_pairwise_kernels_callable():
+    # Test the pairwise_kernels helper function
+    # with a callable function, with given keywords.
     rng = np.random.RandomState(0)
     X = rng.random_sample((5, 4))
     Y = rng.random_sample((2, 4))
-    # Test with all metrics that should be in PAIRWISE_KERNEL_FUNCTIONS.
-    test_metrics = ["rbf", "laplacian", "sigmoid", "polynomial", "linear",
-                    "chi2", "additive_chi2"]
-    for metric in test_metrics:
-        function = PAIRWISE_KERNEL_FUNCTIONS[metric]
-        # Test with Y=None
-        K1 = pairwise_kernels(X, metric=metric)
-        K2 = function(X)
-        assert_array_almost_equal(K1, K2)
-        # Test with Y=Y
-        K1 = pairwise_kernels(X, Y=Y, metric=metric)
-        K2 = function(X, Y=Y)
-        assert_array_almost_equal(K1, K2)
-        # Test with tuples as X and Y
-        X_tuples = tuple([tuple([v for v in row]) for row in X])
-        Y_tuples = tuple([tuple([v for v in row]) for row in Y])
-        K2 = pairwise_kernels(X_tuples, Y_tuples, metric=metric)
-        assert_array_almost_equal(K1, K2)
 
-        # Test with sparse X and Y
-        X_sparse = csr_matrix(X)
-        Y_sparse = csr_matrix(Y)
-        if metric in ["chi2", "additive_chi2"]:
-            # these don't support sparse matrices yet
-            assert_raises(ValueError, pairwise_kernels,
-                          X_sparse, Y=Y_sparse, metric=metric)
-            continue
-        K1 = pairwise_kernels(X_sparse, Y=Y_sparse, metric=metric)
-        assert_array_almost_equal(K1, K2)
-    # Test with a callable function, with given keywords.
     metric = callable_rbf_kernel
     kwds = {'gamma': 0.1}
     K1 = pairwise_kernels(X, Y=Y, metric=metric, **kwds)
@@ -286,27 +299,37 @@ def test_pairwise_kernels_filter_param():
     assert_raises(TypeError, pairwise_kernels, X, Y, "rbf", **params)
 
 
-def test_paired_distances():
+@pytest.mark.parametrize('metric, func', iteritems(PAIRED_DISTANCES))
+def test_paired_distances(metric, func):
     # Test the pairwise_distance helper function.
     rng = np.random.RandomState(0)
     # Euclidean distance should be equivalent to calling the function.
     X = rng.random_sample((5, 4))
     # Euclidean distance, with Y != X.
     Y = rng.random_sample((5, 4))
-    for metric, func in iteritems(PAIRED_DISTANCES):
-        S = paired_distances(X, Y, metric=metric)
-        S2 = func(X, Y)
-        assert_array_almost_equal(S, S2)
-        S3 = func(csr_matrix(X), csr_matrix(Y))
-        assert_array_almost_equal(S, S3)
-        if metric in PAIRWISE_DISTANCE_FUNCTIONS:
-            # Check the pairwise_distances implementation
-            # gives the same value
-            distances = PAIRWISE_DISTANCE_FUNCTIONS[metric](X, Y)
-            distances = np.diag(distances)
-            assert_array_almost_equal(distances, S)
-
-    # Check the callable implementation
+
+    S = paired_distances(X, Y, metric=metric)
+    S2 = func(X, Y)
+    assert_array_almost_equal(S, S2)
+    S3 = func(csr_matrix(X), csr_matrix(Y))
+    assert_array_almost_equal(S, S3)
+    if metric in PAIRWISE_DISTANCE_FUNCTIONS:
+        # Check the pairwise_distances implementation
+        # gives the same value
+        distances = PAIRWISE_DISTANCE_FUNCTIONS[metric](X, Y)
+        distances = np.diag(distances)
+        assert_array_almost_equal(distances, S)
+
+
+def test_paired_distances_callable():
+    # Test the pairwise_distance helper function
+    # with the callable implementation
+    rng = np.random.RandomState(0)
+    # Euclidean distance should be equivalent to calling the function.
+    X = rng.random_sample((5, 4))
+    # Euclidean distance, with Y != X.
+    Y = rng.random_sample((5, 4))
+
     S = paired_distances(X, Y, metric='manhattan')
     S2 = paired_distances(X, Y, metric=lambda x, y: np.abs(x - y).sum(axis=0))
     assert_array_almost_equal(S, S2)
@@ -637,25 +660,29 @@ def test_chi_square_kernel():
                   csr_matrix(X), csr_matrix(Y))
 
 
-def test_kernel_symmetry():
+@pytest.mark.parametrize(
+        'kernel',
+        (linear_kernel, polynomial_kernel, rbf_kernel,
+         laplacian_kernel, sigmoid_kernel, cosine_similarity))
+def test_kernel_symmetry(kernel):
     # Valid kernels should be symmetric
     rng = np.random.RandomState(0)
     X = rng.random_sample((5, 4))
-    for kernel in (linear_kernel, polynomial_kernel, rbf_kernel,
-                   laplacian_kernel, sigmoid_kernel, cosine_similarity):
-        K = kernel(X, X)
-        assert_array_almost_equal(K, K.T, 15)
+    K = kernel(X, X)
+    assert_array_almost_equal(K, K.T, 15)
 
 
-def test_kernel_sparse():
+@pytest.mark.parametrize(
+        'kernel',
+        (linear_kernel, polynomial_kernel, rbf_kernel,
+         laplacian_kernel, sigmoid_kernel, cosine_similarity))
+def test_kernel_sparse(kernel):
     rng = np.random.RandomState(0)
     X = rng.random_sample((5, 4))
     X_sparse = csr_matrix(X)
-    for kernel in (linear_kernel, polynomial_kernel, rbf_kernel,
-                   laplacian_kernel, sigmoid_kernel, cosine_similarity):
-        K = kernel(X, X)
-        K2 = kernel(X_sparse, X_sparse)
-        assert_array_almost_equal(K, K2)
+    K = kernel(X, X)
+    K2 = kernel(X_sparse, X_sparse)
+    assert_array_almost_equal(K, K2)
 
 
 def test_linear_kernel():
diff --git a/sklearn/metrics/tests/test_ranking.py b/sklearn/metrics/tests/test_ranking.py
index 07c35c609358..28b79e9b8474 100644
--- a/sklearn/metrics/tests/test_ranking.py
+++ b/sklearn/metrics/tests/test_ranking.py
@@ -2,7 +2,6 @@
 
 import pytest
 import numpy as np
-from itertools import product
 import warnings
 from scipy.sparse import csr_matrix
 
@@ -177,19 +176,19 @@ def _partial_roc(y_true, y_predict, max_fpr):
     return 0.5 * (1 + (partial_auc - min_area) / (max_area - min_area))
 
 
-def test_roc_curve():
+@pytest.mark.parametrize('drop', [True, False])
+def test_roc_curve(drop):
     # Test Area under Receiver Operating Characteristic (ROC) curve
     y_true, _, probas_pred = make_prediction(binary=True)
     expected_auc = _auc(y_true, probas_pred)
 
-    for drop in [True, False]:
-        fpr, tpr, thresholds = roc_curve(y_true, probas_pred,
-                                         drop_intermediate=drop)
-        roc_auc = auc(fpr, tpr)
-        assert_array_almost_equal(roc_auc, expected_auc, decimal=2)
-        assert_almost_equal(roc_auc, roc_auc_score(y_true, probas_pred))
-        assert_equal(fpr.shape, tpr.shape)
-        assert_equal(fpr.shape, thresholds.shape)
+    fpr, tpr, thresholds = roc_curve(y_true, probas_pred,
+                                     drop_intermediate=drop)
+    roc_auc = auc(fpr, tpr)
+    assert_array_almost_equal(roc_auc, expected_auc, decimal=2)
+    assert_almost_equal(roc_auc, roc_auc_score(y_true, probas_pred))
+    assert_equal(fpr.shape, tpr.shape)
+    assert_equal(fpr.shape, thresholds.shape)
 
 
 def test_roc_curve_end_points():
@@ -923,18 +922,29 @@ def check_alternative_lrap_implementation(lrap_score, n_classes=5,
     assert_almost_equal(score_lrap, score_my_lrap)
 
 
-def test_label_ranking_avp():
-    for fn in [label_ranking_average_precision_score, _my_lrap]:
-        yield check_lrap_toy, fn
-        yield check_lrap_without_tie_and_increasing_score, fn
-        yield check_lrap_only_ties, fn
-        yield check_zero_or_all_relevant_labels, fn
-        yield check_lrap_error_raised, label_ranking_average_precision_score
+@pytest.mark.parametrize(
+        'check',
+        (check_lrap_toy,
+         check_lrap_without_tie_and_increasing_score,
+         check_lrap_only_ties,
+         check_zero_or_all_relevant_labels))
+@pytest.mark.parametrize(
+        'func',
+        (label_ranking_average_precision_score, _my_lrap))
+def test_label_ranking_avp(check, func):
+    check(func)
+
+
+def test_lrap_error_raised():
+    check_lrap_error_raised(label_ranking_average_precision_score)
+
+
+@pytest.mark.parametrize('n_samples', (1, 2, 8, 20))
+@pytest.mark.parametrize('n_classes', (2, 5, 10))
+@pytest.mark.parametrize('random_state', range(1))
+def test_alternative_lrap_implementation(n_samples, n_classes, random_state):
 
-    for n_samples, n_classes, random_state in product((1, 2, 8, 20),
-                                                      (2, 5, 10),
-                                                      range(1)):
-        yield (check_alternative_lrap_implementation,
+    check_alternative_lrap_implementation(
                label_ranking_average_precision_score,
                n_classes, n_samples, random_state)
 
diff --git a/sklearn/metrics/tests/test_score_objects.py b/sklearn/metrics/tests/test_score_objects.py
index 6af6418635d5..6ce2955a127d 100644
--- a/sklearn/metrics/tests/test_score_objects.py
+++ b/sklearn/metrics/tests/test_score_objects.py
@@ -6,6 +6,8 @@
 
 import numpy as np
 
+import pytest
+
 from sklearn.utils.testing import assert_almost_equal
 from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_equal
@@ -491,31 +493,12 @@ def check_scorer_memmap(scorer_name):
     assert isinstance(score, numbers.Number), scorer_name
 
 
-def test_scorer_memmap_input():
+@pytest.mark.parametrize('name', SCORERS)
+def test_scorer_memmap_input(name):
     # Non-regression test for #6147: some score functions would
     # return singleton memmap when computed on memmap data instead of scalar
     # float values.
-    for name in SCORERS.keys():
-        yield check_scorer_memmap, name
-
-
-def test_deprecated_names():
-    X, y = make_blobs(random_state=0, centers=2)
-    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
-    clf = LogisticRegression(random_state=0)
-    clf.fit(X_train, y_train)
-
-    for name in ('mean_absolute_error', 'mean_squared_error',
-                 'median_absolute_error', 'log_loss'):
-        warning_msg = "Scoring method %s was renamed to" % name
-        for scorer in (get_scorer(name), SCORERS[name]):
-            assert_warns_message(DeprecationWarning,
-                                 warning_msg,
-                                 scorer, clf, X, y)
-
-        assert_warns_message(DeprecationWarning,
-                             warning_msg,
-                             cross_val_score, clf, X, y, scoring=name)
+    check_scorer_memmap(name)
 
 
 def test_scoring_is_not_metric():
diff --git a/sklearn/mixture/__init__.py b/sklearn/mixture/__init__.py
index 3622518352ca..08f55802e201 100644
--- a/sklearn/mixture/__init__.py
+++ b/sklearn/mixture/__init__.py
@@ -2,21 +2,9 @@
 The :mod:`sklearn.mixture` module implements mixture modeling algorithms.
 """
 
-from .gmm import sample_gaussian, log_multivariate_normal_density
-from .gmm import GMM, distribute_covar_matrix_to_match_covariance_type
-from .gmm import _validate_covars
-from .dpgmm import DPGMM, VBGMM
-
 from .gaussian_mixture import GaussianMixture
 from .bayesian_mixture import BayesianGaussianMixture
 
 
-__all__ = ['DPGMM',
-           'GMM',
-           'VBGMM',
-           '_validate_covars',
-           'distribute_covar_matrix_to_match_covariance_type',
-           'log_multivariate_normal_density',
-           'sample_gaussian',
-           'GaussianMixture',
+__all__ = ['GaussianMixture',
            'BayesianGaussianMixture']
diff --git a/sklearn/mixture/dpgmm.py b/sklearn/mixture/dpgmm.py
deleted file mode 100644
index 9cf77fee74f1..000000000000
--- a/sklearn/mixture/dpgmm.py
+++ /dev/null
@@ -1,859 +0,0 @@
-"""Bayesian Gaussian Mixture Models and
-Dirichlet Process Gaussian Mixture Models"""
-from __future__ import print_function
-
-# Author: Alexandre Passos (alexandre.tp@gmail.com)
-#         Bertrand Thirion <bertrand.thirion@inria.fr>
-#
-# Based on mixture.py by:
-#         Ron Weiss <ronweiss@gmail.com>
-#         Fabian Pedregosa <fabian.pedregosa@inria.fr>
-#
-
-# Important note for the deprecation cleaning of 0.20 :
-# All the function and classes of this file have been deprecated in 0.18.
-# When you remove this file please also remove the related files
-# - 'sklearn/mixture/gmm.py'
-# - 'sklearn/mixture/test_dpgmm.py'
-# - 'sklearn/mixture/test_gmm.py'
-
-import numpy as np
-from scipy.special import digamma as _digamma, gammaln as _gammaln
-from scipy import linalg
-from scipy.linalg import pinvh
-from scipy.spatial.distance import cdist
-
-from ..externals.six.moves import xrange
-from ..utils import check_random_state, check_array, deprecated
-from ..utils.fixes import logsumexp
-from ..utils.extmath import squared_norm, stable_cumsum
-from ..utils.validation import check_is_fitted
-from .. import cluster
-from .gmm import _GMMBase
-
-
-@deprecated("The function digamma is deprecated in 0.18 and "
-            "will be removed in 0.20. Use scipy.special.digamma instead.")
-def digamma(x):
-    return _digamma(x + np.finfo(np.float32).eps)
-
-
-@deprecated("The function gammaln is deprecated in 0.18 and "
-            "will be removed in 0.20. Use scipy.special.gammaln instead.")
-def gammaln(x):
-    return _gammaln(x + np.finfo(np.float32).eps)
-
-
-@deprecated("The function log_normalize is deprecated in 0.18 and "
-            "will be removed in 0.20.")
-def log_normalize(v, axis=0):
-    """Normalized probabilities from unnormalized log-probabilities"""
-    v = np.rollaxis(v, axis)
-    v = v.copy()
-    v -= v.max(axis=0)
-    out = logsumexp(v)
-    v = np.exp(v - out)
-    v += np.finfo(np.float32).eps
-    v /= np.sum(v, axis=0)
-    return np.swapaxes(v, 0, axis)
-
-
-@deprecated("The function wishart_log_det is deprecated in 0.18 and "
-            "will be removed in 0.20.")
-def wishart_log_det(a, b, detB, n_features):
-    """Expected value of the log of the determinant of a Wishart
-
-    The expected value of the logarithm of the determinant of a
-    wishart-distributed random variable with the specified parameters."""
-    l = np.sum(digamma(0.5 * (a - np.arange(-1, n_features - 1))))
-    l += n_features * np.log(2)
-    return l + detB
-
-
-@deprecated("The function wishart_logz is deprecated in 0.18 and "
-            "will be removed in 0.20.")
-def wishart_logz(v, s, dets, n_features):
-    "The logarithm of the normalization constant for the wishart distribution"
-    z = 0.
-    z += 0.5 * v * n_features * np.log(2)
-    z += (0.25 * (n_features * (n_features - 1)) * np.log(np.pi))
-    z += 0.5 * v * np.log(dets)
-    z += np.sum(gammaln(0.5 * (v - np.arange(n_features) + 1)))
-    return z
-
-
-def _bound_wishart(a, B, detB):
-    """Returns a function of the dof, scale matrix and its determinant
-    used as an upper bound in variational approximation of the evidence"""
-    n_features = B.shape[0]
-    logprior = wishart_logz(a, B, detB, n_features)
-    logprior -= wishart_logz(n_features,
-                             np.identity(n_features),
-                             1, n_features)
-    logprior += 0.5 * (a - 1) * wishart_log_det(a, B, detB, n_features)
-    logprior += 0.5 * a * np.trace(B)
-    return logprior
-
-
-##############################################################################
-# Variational bound on the log likelihood of each class
-##############################################################################
-
-
-def _sym_quad_form(x, mu, A):
-    """helper function to calculate symmetric quadratic form x.T * A * x"""
-    q = (cdist(x, mu[np.newaxis], "mahalanobis", VI=A) ** 2).reshape(-1)
-    return q
-
-
-def _bound_state_log_lik(X, initial_bound, precs, means, covariance_type):
-    """Update the bound with likelihood terms, for standard covariance types"""
-    n_components, n_features = means.shape
-    n_samples = X.shape[0]
-    bound = np.empty((n_samples, n_components))
-    bound[:] = initial_bound
-    if covariance_type in ['diag', 'spherical']:
-        for k in range(n_components):
-            d = X - means[k]
-            bound[:, k] -= 0.5 * np.sum(d * d * precs[k], axis=1)
-    elif covariance_type == 'tied':
-        for k in range(n_components):
-            bound[:, k] -= 0.5 * _sym_quad_form(X, means[k], precs)
-    elif covariance_type == 'full':
-        for k in range(n_components):
-            bound[:, k] -= 0.5 * _sym_quad_form(X, means[k], precs[k])
-    return bound
-
-
-class _DPGMMBase(_GMMBase):
-    """Variational Inference for the Infinite Gaussian Mixture Model.
-
-    DPGMM stands for Dirichlet Process Gaussian Mixture Model, and it
-    is an infinite mixture model with the Dirichlet Process as a prior
-    distribution on the number of clusters. In practice the
-    approximate inference algorithm uses a truncated distribution with
-    a fixed maximum number of components, but almost always the number
-    of components actually used depends on the data.
-
-    Stick-breaking Representation of a Gaussian mixture model
-    probability distribution. This class allows for easy and efficient
-    inference of an approximate posterior distribution over the
-    parameters of a Gaussian mixture model with a variable number of
-    components (smaller than the truncation parameter n_components).
-
-    Initialization is with normally-distributed means and identity
-    covariance, for proper convergence.
-
-    Read more in the :ref:`User Guide <dpgmm>`.
-
-    Parameters
-    ----------
-    n_components : int, default 1
-        Number of mixture components.
-
-    covariance_type : string, default 'diag'
-        String describing the type of covariance parameters to
-        use.  Must be one of 'spherical', 'tied', 'diag', 'full'.
-
-    alpha : float, default 1
-        Real number representing the concentration parameter of
-        the dirichlet process. Intuitively, the Dirichlet Process
-        is as likely to start a new cluster for a point as it is
-        to add that point to a cluster with alpha elements. A
-        higher alpha means more clusters, as the expected number
-        of clusters is ``alpha*log(N)``.
-
-    tol : float, default 1e-3
-        Convergence threshold.
-
-    n_iter : int, default 10
-        Maximum number of iterations to perform before convergence.
-
-    params : string, default 'wmc'
-        Controls which parameters are updated in the training
-        process.  Can contain any combination of 'w' for weights,
-        'm' for means, and 'c' for covars.
-
-    init_params : string, default 'wmc'
-        Controls which parameters are updated in the initialization
-        process.  Can contain any combination of 'w' for weights,
-        'm' for means, and 'c' for covars.  Defaults to 'wmc'.
-
-    verbose : int, default 0
-        Controls output verbosity.
-
-    Attributes
-    ----------
-    covariance_type : string
-        String describing the type of covariance parameters used by
-        the DP-GMM.  Must be one of 'spherical', 'tied', 'diag', 'full'.
-
-    n_components : int
-        Number of mixture components.
-
-    weights_ : array, shape (`n_components`,)
-        Mixing weights for each mixture component.
-
-    means_ : array, shape (`n_components`, `n_features`)
-        Mean parameters for each mixture component.
-
-    precs_ : array
-        Precision (inverse covariance) parameters for each mixture
-        component.  The shape depends on `covariance_type`::
-
-            (`n_components`, 'n_features')                if 'spherical',
-            (`n_features`, `n_features`)                  if 'tied',
-            (`n_components`, `n_features`)                if 'diag',
-            (`n_components`, `n_features`, `n_features`)  if 'full'
-
-    converged_ : bool
-        True when convergence was reached in fit(), False otherwise.
-
-    See Also
-    --------
-    GMM : Finite Gaussian mixture model fit with EM
-
-    VBGMM : Finite Gaussian mixture model fit with a variational
-        algorithm, better for situations where there might be too little
-        data to get a good estimate of the covariance matrix.
-    """
-    def __init__(self, n_components=1, covariance_type='diag', alpha=1.0,
-                 random_state=None, tol=1e-3, verbose=0, min_covar=None,
-                 n_iter=10, params='wmc', init_params='wmc'):
-        self.alpha = alpha
-        super(_DPGMMBase, self).__init__(n_components, covariance_type,
-                                         random_state=random_state,
-                                         tol=tol, min_covar=min_covar,
-                                         n_iter=n_iter, params=params,
-                                         init_params=init_params,
-                                         verbose=verbose)
-
-    def _get_precisions(self):
-        """Return precisions as a full matrix."""
-        if self.covariance_type == 'full':
-            return self.precs_
-        elif self.covariance_type in ['diag', 'spherical']:
-            return [np.diag(cov) for cov in self.precs_]
-        elif self.covariance_type == 'tied':
-            return [self.precs_] * self.n_components
-
-    def _get_covars(self):
-        return [pinvh(c) for c in self._get_precisions()]
-
-    def _set_covars(self, covars):
-        raise NotImplementedError("""The variational algorithm does
-        not support setting the covariance parameters.""")
-
-    def score_samples(self, X):
-        """Return the likelihood of the data under the model.
-
-        Compute the bound on log probability of X under the model
-        and return the posterior distribution (responsibilities) of
-        each mixture component for each element of X.
-
-        This is done by computing the parameters for the mean-field of
-        z for each observation.
-
-        Parameters
-        ----------
-        X : array_like, shape (n_samples, n_features)
-            List of n_features-dimensional data points.  Each row
-            corresponds to a single data point.
-
-        Returns
-        -------
-        logprob : array_like, shape (n_samples,)
-            Log probabilities of each data point in X
-        responsibilities : array_like, shape (n_samples, n_components)
-            Posterior probabilities of each mixture component for each
-            observation
-        """
-        check_is_fitted(self, 'gamma_')
-
-        X = check_array(X)
-        if X.ndim == 1:
-            X = X[:, np.newaxis]
-        sd = digamma(self.gamma_.T[1] + self.gamma_.T[2])
-        dgamma1 = digamma(self.gamma_.T[1]) - sd
-        dgamma2 = np.zeros(self.n_components)
-        dgamma2[0] = digamma(self.gamma_[0, 2]) - digamma(self.gamma_[0, 1] +
-                                                          self.gamma_[0, 2])
-        for j in range(1, self.n_components):
-            dgamma2[j] = dgamma2[j - 1] + digamma(self.gamma_[j - 1, 2])
-            dgamma2[j] -= sd[j - 1]
-        dgamma = dgamma1 + dgamma2
-        # Free memory and developers cognitive load:
-        del dgamma1, dgamma2, sd
-
-        if self.covariance_type not in ['full', 'tied', 'diag', 'spherical']:
-            raise NotImplementedError("This ctype is not implemented: %s"
-                                      % self.covariance_type)
-        p = _bound_state_log_lik(X, self._initial_bound + self.bound_prec_,
-                                 self.precs_, self.means_,
-                                 self.covariance_type)
-        z = p + dgamma
-        z = log_normalize(z, axis=-1)
-        bound = np.sum(z * p, axis=-1)
-        return bound, z
-
-    def _update_concentration(self, z):
-        """Update the concentration parameters for each cluster"""
-        sz = np.sum(z, axis=0)
-        self.gamma_.T[1] = 1. + sz
-        self.gamma_.T[2].fill(0)
-        for i in range(self.n_components - 2, -1, -1):
-            self.gamma_[i, 2] = self.gamma_[i + 1, 2] + sz[i]
-        self.gamma_.T[2] += self.alpha
-
-    def _update_means(self, X, z):
-        """Update the variational distributions for the means"""
-        n_features = X.shape[1]
-        for k in range(self.n_components):
-            if self.covariance_type in ['spherical', 'diag']:
-                num = np.sum(z.T[k].reshape((-1, 1)) * X, axis=0)
-                num *= self.precs_[k]
-                den = 1. + self.precs_[k] * np.sum(z.T[k])
-                self.means_[k] = num / den
-            elif self.covariance_type in ['tied', 'full']:
-                if self.covariance_type == 'tied':
-                    cov = self.precs_
-                else:
-                    cov = self.precs_[k]
-                den = np.identity(n_features) + cov * np.sum(z.T[k])
-                num = np.sum(z.T[k].reshape((-1, 1)) * X, axis=0)
-                num = np.dot(cov, num)
-                self.means_[k] = linalg.lstsq(den, num)[0]
-
-    def _update_precisions(self, X, z):
-        """Update the variational distributions for the precisions"""
-        n_features = X.shape[1]
-        if self.covariance_type == 'spherical':
-            self.dof_ = 0.5 * n_features * np.sum(z, axis=0)
-            for k in range(self.n_components):
-                # could be more memory efficient ?
-                sq_diff = np.sum((X - self.means_[k]) ** 2, axis=1)
-                self.scale_[k] = 1.
-                self.scale_[k] += 0.5 * np.sum(z.T[k] * (sq_diff + n_features))
-                self.bound_prec_[k] = (
-                    0.5 * n_features * (
-                        digamma(self.dof_[k]) - np.log(self.scale_[k])))
-            self.precs_ = np.tile(self.dof_ / self.scale_, [n_features, 1]).T
-
-        elif self.covariance_type == 'diag':
-            for k in range(self.n_components):
-                self.dof_[k].fill(1. + 0.5 * np.sum(z.T[k], axis=0))
-                sq_diff = (X - self.means_[k]) ** 2  # see comment above
-                self.scale_[k] = np.ones(n_features) + 0.5 * np.dot(
-                    z.T[k], (sq_diff + 1))
-                self.precs_[k] = self.dof_[k] / self.scale_[k]
-                self.bound_prec_[k] = 0.5 * np.sum(digamma(self.dof_[k])
-                                                   - np.log(self.scale_[k]))
-                self.bound_prec_[k] -= 0.5 * np.sum(self.precs_[k])
-
-        elif self.covariance_type == 'tied':
-            self.dof_ = 2 + X.shape[0] + n_features
-            self.scale_ = (X.shape[0] + 1) * np.identity(n_features)
-            for k in range(self.n_components):
-                diff = X - self.means_[k]
-                self.scale_ += np.dot(diff.T, z[:, k:k + 1] * diff)
-            self.scale_ = pinvh(self.scale_)
-            self.precs_ = self.dof_ * self.scale_
-            self.det_scale_ = linalg.det(self.scale_)
-            self.bound_prec_ = 0.5 * wishart_log_det(
-                self.dof_, self.scale_, self.det_scale_, n_features)
-            self.bound_prec_ -= 0.5 * self.dof_ * np.trace(self.scale_)
-
-        elif self.covariance_type == 'full':
-            for k in range(self.n_components):
-                sum_resp = np.sum(z.T[k])
-                self.dof_[k] = 2 + sum_resp + n_features
-                self.scale_[k] = (sum_resp + 1) * np.identity(n_features)
-                diff = X - self.means_[k]
-                self.scale_[k] += np.dot(diff.T, z[:, k:k + 1] * diff)
-                self.scale_[k] = pinvh(self.scale_[k])
-                self.precs_[k] = self.dof_[k] * self.scale_[k]
-                self.det_scale_[k] = linalg.det(self.scale_[k])
-                self.bound_prec_[k] = 0.5 * wishart_log_det(
-                    self.dof_[k], self.scale_[k], self.det_scale_[k],
-                    n_features)
-                self.bound_prec_[k] -= 0.5 * self.dof_[k] * np.trace(
-                    self.scale_[k])
-
-    def _monitor(self, X, z, n, end=False):
-        """Monitor the lower bound during iteration
-
-        Debug method to help see exactly when it is failing to converge as
-        expected.
-
-        Note: this is very expensive and should not be used by default."""
-        if self.verbose > 0:
-            print("Bound after updating %8s: %f" % (n, self.lower_bound(X, z)))
-            if end:
-                print("Cluster proportions:", self.gamma_.T[1])
-                print("covariance_type:", self.covariance_type)
-
-    def _do_mstep(self, X, z, params):
-        """Maximize the variational lower bound
-
-        Update each of the parameters to maximize the lower bound."""
-        self._monitor(X, z, "z")
-        self._update_concentration(z)
-        self._monitor(X, z, "gamma")
-        if 'm' in params:
-            self._update_means(X, z)
-        self._monitor(X, z, "mu")
-        if 'c' in params:
-            self._update_precisions(X, z)
-        self._monitor(X, z, "a and b", end=True)
-
-    def _initialize_gamma(self):
-        "Initializes the concentration parameters"
-        self.gamma_ = self.alpha * np.ones((self.n_components, 3))
-
-    def _bound_concentration(self):
-        """The variational lower bound for the concentration parameter."""
-        logprior = gammaln(self.alpha) * self.n_components
-        logprior += np.sum((self.alpha - 1) * (
-            digamma(self.gamma_.T[2]) - digamma(self.gamma_.T[1] +
-                                                self.gamma_.T[2])))
-        logprior += np.sum(- gammaln(self.gamma_.T[1] + self.gamma_.T[2]))
-        logprior += np.sum(gammaln(self.gamma_.T[1]) +
-                           gammaln(self.gamma_.T[2]))
-        logprior -= np.sum((self.gamma_.T[1] - 1) * (
-            digamma(self.gamma_.T[1]) - digamma(self.gamma_.T[1] +
-                                                self.gamma_.T[2])))
-        logprior -= np.sum((self.gamma_.T[2] - 1) * (
-            digamma(self.gamma_.T[2]) - digamma(self.gamma_.T[1] +
-                                                self.gamma_.T[2])))
-        return logprior
-
-    def _bound_means(self):
-        "The variational lower bound for the mean parameters"
-        logprior = 0.
-        logprior -= 0.5 * squared_norm(self.means_)
-        logprior -= 0.5 * self.means_.shape[1] * self.n_components
-        return logprior
-
-    def _bound_precisions(self):
-        """Returns the bound term related to precisions"""
-        logprior = 0.
-        if self.covariance_type == 'spherical':
-            logprior += np.sum(gammaln(self.dof_))
-            logprior -= np.sum(
-                (self.dof_ - 1) * digamma(np.maximum(0.5, self.dof_)))
-            logprior += np.sum(- np.log(self.scale_) + self.dof_
-                               - self.precs_[:, 0])
-        elif self.covariance_type == 'diag':
-            logprior += np.sum(gammaln(self.dof_))
-            logprior -= np.sum(
-                (self.dof_ - 1) * digamma(np.maximum(0.5, self.dof_)))
-            logprior += np.sum(- np.log(self.scale_) + self.dof_ - self.precs_)
-        elif self.covariance_type == 'tied':
-            logprior += _bound_wishart(self.dof_, self.scale_, self.det_scale_)
-        elif self.covariance_type == 'full':
-            for k in range(self.n_components):
-                logprior += _bound_wishart(self.dof_[k],
-                                           self.scale_[k],
-                                           self.det_scale_[k])
-        return logprior
-
-    def _bound_proportions(self, z):
-        """Returns the bound term related to proportions"""
-        dg12 = digamma(self.gamma_.T[1] + self.gamma_.T[2])
-        dg1 = digamma(self.gamma_.T[1]) - dg12
-        dg2 = digamma(self.gamma_.T[2]) - dg12
-
-        cz = stable_cumsum(z[:, ::-1], axis=-1)[:, -2::-1]
-        logprior = np.sum(cz * dg2[:-1]) + np.sum(z * dg1)
-        del cz  # Save memory
-        z_non_zeros = z[z > np.finfo(np.float32).eps]
-        logprior -= np.sum(z_non_zeros * np.log(z_non_zeros))
-        return logprior
-
-    def _logprior(self, z):
-        logprior = self._bound_concentration()
-        logprior += self._bound_means()
-        logprior += self._bound_precisions()
-        logprior += self._bound_proportions(z)
-        return logprior
-
-    def lower_bound(self, X, z):
-        """returns a lower bound on model evidence based on X and membership"""
-        check_is_fitted(self, 'means_')
-
-        if self.covariance_type not in ['full', 'tied', 'diag', 'spherical']:
-            raise NotImplementedError("This ctype is not implemented: %s"
-                                      % self.covariance_type)
-        X = np.asarray(X)
-        if X.ndim == 1:
-            X = X[:, np.newaxis]
-        c = np.sum(z * _bound_state_log_lik(X, self._initial_bound +
-                                            self.bound_prec_, self.precs_,
-                                            self.means_, self.covariance_type))
-
-        return c + self._logprior(z)
-
-    def _set_weights(self):
-        for i in xrange(self.n_components):
-            self.weights_[i] = self.gamma_[i, 1] / (self.gamma_[i, 1]
-                                                    + self.gamma_[i, 2])
-        self.weights_ /= np.sum(self.weights_)
-
-    def _fit(self, X, y=None):
-        """Estimate model parameters with the variational
-        algorithm.
-
-        A initialization step is performed before entering the em
-        algorithm. If you want to avoid this step, set the keyword
-        argument init_params to the empty string '' when creating
-        the object. Likewise, if you would like just to do an
-        initialization, set n_iter=0.
-
-        Parameters
-        ----------
-        X : array_like, shape (n, n_features)
-            List of n_features-dimensional data points.  Each row
-            corresponds to a single data point.
-
-        Returns
-        -------
-        responsibilities : array, shape (n_samples, n_components)
-            Posterior probabilities of each mixture component for each
-            observation.
-        """
-        self.random_state_ = check_random_state(self.random_state)
-
-        # initialization step
-        X = check_array(X)
-        if X.ndim == 1:
-            X = X[:, np.newaxis]
-
-        n_samples, n_features = X.shape
-        z = np.ones((n_samples, self.n_components))
-        z /= self.n_components
-
-        self._initial_bound = - 0.5 * n_features * np.log(2 * np.pi)
-        self._initial_bound -= np.log(2 * np.pi * np.e)
-
-        if (self.init_params != '') or not hasattr(self, 'gamma_'):
-            self._initialize_gamma()
-
-        if 'm' in self.init_params or not hasattr(self, 'means_'):
-            self.means_ = cluster.KMeans(
-                n_clusters=self.n_components,
-                random_state=self.random_state_).fit(X).cluster_centers_[::-1]
-
-        if 'w' in self.init_params or not hasattr(self, 'weights_'):
-            self.weights_ = np.tile(1.0 / self.n_components, self.n_components)
-
-        if 'c' in self.init_params or not hasattr(self, 'precs_'):
-            if self.covariance_type == 'spherical':
-                self.dof_ = np.ones(self.n_components)
-                self.scale_ = np.ones(self.n_components)
-                self.precs_ = np.ones((self.n_components, n_features))
-                self.bound_prec_ = 0.5 * n_features * (
-                    digamma(self.dof_) - np.log(self.scale_))
-            elif self.covariance_type == 'diag':
-                self.dof_ = 1 + 0.5 * n_features
-                self.dof_ *= np.ones((self.n_components, n_features))
-                self.scale_ = np.ones((self.n_components, n_features))
-                self.precs_ = np.ones((self.n_components, n_features))
-                self.bound_prec_ = 0.5 * (np.sum(digamma(self.dof_) -
-                                                 np.log(self.scale_), 1))
-                self.bound_prec_ -= 0.5 * np.sum(self.precs_, 1)
-            elif self.covariance_type == 'tied':
-                self.dof_ = 1.
-                self.scale_ = np.identity(n_features)
-                self.precs_ = np.identity(n_features)
-                self.det_scale_ = 1.
-                self.bound_prec_ = 0.5 * wishart_log_det(
-                    self.dof_, self.scale_, self.det_scale_, n_features)
-                self.bound_prec_ -= 0.5 * self.dof_ * np.trace(self.scale_)
-            elif self.covariance_type == 'full':
-                self.dof_ = (1 + self.n_components + n_samples)
-                self.dof_ *= np.ones(self.n_components)
-                self.scale_ = [2 * np.identity(n_features)
-                               for _ in range(self.n_components)]
-                self.precs_ = [np.identity(n_features)
-                               for _ in range(self.n_components)]
-                self.det_scale_ = np.ones(self.n_components)
-                self.bound_prec_ = np.zeros(self.n_components)
-                for k in range(self.n_components):
-                    self.bound_prec_[k] = wishart_log_det(
-                        self.dof_[k], self.scale_[k], self.det_scale_[k],
-                        n_features)
-                    self.bound_prec_[k] -= (self.dof_[k] *
-                                            np.trace(self.scale_[k]))
-                self.bound_prec_ *= 0.5
-
-        # EM algorithms
-        current_log_likelihood = None
-        # reset self.converged_ to False
-        self.converged_ = False
-
-        for i in range(self.n_iter):
-            prev_log_likelihood = current_log_likelihood
-            # Expectation step
-            curr_logprob, z = self.score_samples(X)
-
-            current_log_likelihood = (
-                curr_logprob.mean() + self._logprior(z) / n_samples)
-
-            # Check for convergence.
-            if prev_log_likelihood is not None:
-                change = abs(current_log_likelihood - prev_log_likelihood)
-                if change < self.tol:
-                    self.converged_ = True
-                    break
-
-            # Maximization step
-            self._do_mstep(X, z, self.params)
-
-        if self.n_iter == 0:
-            # Need to make sure that there is a z value to output
-            # Output zeros because it was just a quick initialization
-            z = np.zeros((X.shape[0], self.n_components))
-
-        self._set_weights()
-
-        return z
-
-
-@deprecated("The `DPGMM` class is not working correctly and it's better "
-            "to use `sklearn.mixture.BayesianGaussianMixture` class with "
-            "parameter `weight_concentration_prior_type='dirichlet_process'` "
-            "instead. DPGMM is deprecated in 0.18 and will be "
-            "removed in 0.20.")
-class DPGMM(_DPGMMBase):
-    """Dirichlet Process Gaussian Mixture Models
-
-    .. deprecated:: 0.18
-        This class will be removed in 0.20.
-        Use :class:`sklearn.mixture.BayesianGaussianMixture` with
-        parameter ``weight_concentration_prior_type='dirichlet_process'``
-        instead.
-
-    """
-
-    def __init__(self, n_components=1, covariance_type='diag', alpha=1.0,
-                 random_state=None, tol=1e-3, verbose=0, min_covar=None,
-                 n_iter=10, params='wmc', init_params='wmc'):
-        super(DPGMM, self).__init__(
-            n_components=n_components, covariance_type=covariance_type,
-            alpha=alpha, random_state=random_state, tol=tol, verbose=verbose,
-            min_covar=min_covar, n_iter=n_iter, params=params,
-            init_params=init_params)
-
-
-@deprecated("The `VBGMM` class is not working correctly and it's better "
-            "to use `sklearn.mixture.BayesianGaussianMixture` class with "
-            "parameter `weight_concentration_prior_type="
-            "'dirichlet_distribution'` instead. "
-            "VBGMM is deprecated in 0.18 and will be removed in 0.20.")
-class VBGMM(_DPGMMBase):
-    """Variational Inference for the Gaussian Mixture Model
-
-    .. deprecated:: 0.18
-        This class will be removed in 0.20.
-        Use :class:`sklearn.mixture.BayesianGaussianMixture` with parameter
-        ``weight_concentration_prior_type='dirichlet_distribution'`` instead.
-
-    Variational inference for a Gaussian mixture model probability
-    distribution. This class allows for easy and efficient inference
-    of an approximate posterior distribution over the parameters of a
-    Gaussian mixture model with a fixed number of components.
-
-    Initialization is with normally-distributed means and identity
-    covariance, for proper convergence.
-
-    Read more in the :ref:`User Guide <bgmm>`.
-
-    Parameters
-    ----------
-    n_components : int, default 1
-        Number of mixture components.
-
-    covariance_type : string, default 'diag'
-        String describing the type of covariance parameters to
-        use.  Must be one of 'spherical', 'tied', 'diag', 'full'.
-
-    alpha : float, default 1
-        Real number representing the concentration parameter of
-        the dirichlet distribution. Intuitively, the higher the
-        value of alpha the more likely the variational mixture of
-        Gaussians model will use all components it can.
-
-    tol : float, default 1e-3
-        Convergence threshold.
-
-    n_iter : int, default 10
-        Maximum number of iterations to perform before convergence.
-
-    params : string, default 'wmc'
-        Controls which parameters are updated in the training
-        process.  Can contain any combination of 'w' for weights,
-        'm' for means, and 'c' for covars.
-
-    init_params : string, default 'wmc'
-        Controls which parameters are updated in the initialization
-        process.  Can contain any combination of 'w' for weights,
-        'm' for means, and 'c' for covars.  Defaults to 'wmc'.
-
-    verbose : int, default 0
-        Controls output verbosity.
-
-    Attributes
-    ----------
-    covariance_type : string
-        String describing the type of covariance parameters used by
-        the DP-GMM.  Must be one of 'spherical', 'tied', 'diag', 'full'.
-
-    n_features : int
-        Dimensionality of the Gaussians.
-
-    n_components : int (read-only)
-        Number of mixture components.
-
-    weights_ : array, shape (`n_components`,)
-        Mixing weights for each mixture component.
-
-    means_ : array, shape (`n_components`, `n_features`)
-        Mean parameters for each mixture component.
-
-    precs_ : array
-        Precision (inverse covariance) parameters for each mixture
-        component.  The shape depends on `covariance_type`::
-
-            (`n_components`, 'n_features')                if 'spherical',
-            (`n_features`, `n_features`)                  if 'tied',
-            (`n_components`, `n_features`)                if 'diag',
-            (`n_components`, `n_features`, `n_features`)  if 'full'
-
-    converged_ : bool
-        True when convergence was reached in fit(), False
-        otherwise.
-
-    See Also
-    --------
-    GMM : Finite Gaussian mixture model fit with EM
-    DPGMM : Infinite Gaussian mixture model, using the dirichlet
-        process, fit with a variational algorithm
-    """
-
-    def __init__(self, n_components=1, covariance_type='diag', alpha=1.0,
-                 random_state=None, tol=1e-3, verbose=0,
-                 min_covar=None, n_iter=10, params='wmc', init_params='wmc'):
-        super(VBGMM, self).__init__(
-            n_components, covariance_type, random_state=random_state,
-            tol=tol, verbose=verbose, min_covar=min_covar,
-            n_iter=n_iter, params=params, init_params=init_params)
-        self.alpha = alpha
-
-    def _fit(self, X, y=None):
-        """Estimate model parameters with the variational algorithm.
-
-        A initialization step is performed before entering the EM
-        algorithm. If you want to avoid this step, set the keyword
-        argument init_params to the empty string '' when creating
-        the object. Likewise, if you just would like to do an
-        initialization, set n_iter=0.
-
-        Parameters
-        ----------
-        X : array_like, shape (n, n_features)
-            List of n_features-dimensional data points.  Each row
-            corresponds to a single data point.
-
-        Returns
-        -------
-        responsibilities : array, shape (n_samples, n_components)
-            Posterior probabilities of each mixture component for each
-            observation.
-        """
-        self.alpha_ = float(self.alpha) / self.n_components
-        return super(VBGMM, self)._fit(X, y)
-
-    def score_samples(self, X):
-        """Return the likelihood of the data under the model.
-
-        Compute the bound on log probability of X under the model
-        and return the posterior distribution (responsibilities) of
-        each mixture component for each element of X.
-
-        This is done by computing the parameters for the mean-field of
-        z for each observation.
-
-        Parameters
-        ----------
-        X : array_like, shape (n_samples, n_features)
-            List of n_features-dimensional data points.  Each row
-            corresponds to a single data point.
-
-        Returns
-        -------
-        logprob : array_like, shape (n_samples,)
-            Log probabilities of each data point in X
-        responsibilities : array_like, shape (n_samples, n_components)
-            Posterior probabilities of each mixture component for each
-            observation
-        """
-        check_is_fitted(self, 'gamma_')
-
-        X = check_array(X)
-        if X.ndim == 1:
-            X = X[:, np.newaxis]
-        dg = digamma(self.gamma_) - digamma(np.sum(self.gamma_))
-
-        if self.covariance_type not in ['full', 'tied', 'diag', 'spherical']:
-            raise NotImplementedError("This ctype is not implemented: %s"
-                                      % self.covariance_type)
-        p = _bound_state_log_lik(X, self._initial_bound + self.bound_prec_,
-                                 self.precs_, self.means_,
-                                 self.covariance_type)
-
-        z = p + dg
-        z = log_normalize(z, axis=-1)
-        bound = np.sum(z * p, axis=-1)
-        return bound, z
-
-    def _update_concentration(self, z):
-        for i in range(self.n_components):
-            self.gamma_[i] = self.alpha_ + np.sum(z.T[i])
-
-    def _initialize_gamma(self):
-        self.gamma_ = self.alpha_ * np.ones(self.n_components)
-
-    def _bound_proportions(self, z):
-        logprior = 0.
-        dg = digamma(self.gamma_)
-        dg -= digamma(np.sum(self.gamma_))
-        logprior += np.sum(dg.reshape((-1, 1)) * z.T)
-        z_non_zeros = z[z > np.finfo(np.float32).eps]
-        logprior -= np.sum(z_non_zeros * np.log(z_non_zeros))
-        return logprior
-
-    def _bound_concentration(self):
-        logprior = gammaln(np.sum(self.gamma_)) - gammaln(self.n_components
-                                                          * self.alpha_)
-        logprior -= np.sum(gammaln(self.gamma_) - gammaln(self.alpha_))
-        sg = digamma(np.sum(self.gamma_))
-        logprior += np.sum((self.gamma_ - self.alpha_)
-                           * (digamma(self.gamma_) - sg))
-        return logprior
-
-    def _monitor(self, X, z, n, end=False):
-        """Monitor the lower bound during iteration
-
-        Debug method to help see exactly when it is failing to converge as
-        expected.
-
-        Note: this is very expensive and should not be used by default."""
-        if self.verbose > 0:
-            print("Bound after updating %8s: %f" % (n, self.lower_bound(X, z)))
-            if end:
-                print("Cluster proportions:", self.gamma_)
-                print("covariance_type:", self.covariance_type)
-
-    def _set_weights(self):
-        self.weights_[:] = self.gamma_
-        self.weights_ /= np.sum(self.weights_)
diff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py
index d58a9e326c69..5673db5f98a0 100644
--- a/sklearn/mixture/gaussian_mixture.py
+++ b/sklearn/mixture/gaussian_mixture.py
@@ -448,15 +448,18 @@ class GaussianMixture(BaseMixture):
     n_components : int, defaults to 1.
         The number of mixture components.
 
-    covariance_type : {'full', 'tied', 'diag', 'spherical'},
-            defaults to 'full'.
+    covariance_type : {'full' (default), 'tied', 'diag', 'spherical'}
         String describing the type of covariance parameters to use.
-        Must be one of::
-
-            'full' (each component has its own general covariance matrix),
-            'tied' (all components share the same general covariance matrix),
-            'diag' (each component has its own diagonal covariance matrix),
-            'spherical' (each component has its own single variance).
+        Must be one of:
+
+        'full'
+            each component has its own general covariance matrix
+        'tied'
+            all components share the same general covariance matrix
+        'diag'
+            each component has its own diagonal covariance matrix
+        'spherical'
+            each component has its own single variance
 
     tol : float, defaults to 1e-3.
         The convergence threshold. EM iterations will stop when the
diff --git a/sklearn/mixture/gmm.py b/sklearn/mixture/gmm.py
deleted file mode 100644
index b3c231314cc2..000000000000
--- a/sklearn/mixture/gmm.py
+++ /dev/null
@@ -1,853 +0,0 @@
-"""
-Gaussian Mixture Models.
-
-This implementation corresponds to frequentist (non-Bayesian) formulation
-of Gaussian Mixture Models.
-"""
-
-# Author: Ron Weiss <ronweiss@gmail.com>
-#         Fabian Pedregosa <fabian.pedregosa@inria.fr>
-#         Bertrand Thirion <bertrand.thirion@inria.fr>
-
-# Important note for the deprecation cleaning of 0.20 :
-# All the functions and classes of this file have been deprecated in 0.18.
-# When you remove this file please also remove the related files
-# - 'sklearn/mixture/dpgmm.py'
-# - 'sklearn/mixture/test_dpgmm.py'
-# - 'sklearn/mixture/test_gmm.py'
-from time import time
-
-import numpy as np
-from scipy import linalg
-
-from ..base import BaseEstimator
-from ..utils import check_random_state, check_array, deprecated
-from ..utils.fixes import logsumexp
-from ..utils.validation import check_is_fitted
-from .. import cluster
-
-from sklearn.externals.six.moves import zip
-
-EPS = np.finfo(float).eps
-
-@deprecated("The function log_multivariate_normal_density is deprecated in 0.18"
-            " and will be removed in 0.20.")
-def log_multivariate_normal_density(X, means, covars, covariance_type='diag'):
-    """Compute the log probability under a multivariate Gaussian distribution.
-
-    Parameters
-    ----------
-    X : array_like, shape (n_samples, n_features)
-        List of n_features-dimensional data points. Each row corresponds to a
-        single data point.
-
-    means : array_like, shape (n_components, n_features)
-        List of n_features-dimensional mean vectors for n_components Gaussians.
-        Each row corresponds to a single mean vector.
-
-    covars : array_like
-        List of n_components covariance parameters for each Gaussian. The shape
-        depends on `covariance_type`:
-            (n_components, n_features)      if 'spherical',
-            (n_features, n_features)    if 'tied',
-            (n_components, n_features)    if 'diag',
-            (n_components, n_features, n_features) if 'full'
-
-    covariance_type : string
-        Type of the covariance parameters.  Must be one of
-        'spherical', 'tied', 'diag', 'full'.  Defaults to 'diag'.
-
-    Returns
-    -------
-    lpr : array_like, shape (n_samples, n_components)
-        Array containing the log probabilities of each data point in
-        X under each of the n_components multivariate Gaussian distributions.
-    """
-    log_multivariate_normal_density_dict = {
-        'spherical': _log_multivariate_normal_density_spherical,
-        'tied': _log_multivariate_normal_density_tied,
-        'diag': _log_multivariate_normal_density_diag,
-        'full': _log_multivariate_normal_density_full}
-    return log_multivariate_normal_density_dict[covariance_type](
-        X, means, covars)
-
-
-@deprecated("The function sample_gaussian is deprecated in 0.18"
-            " and will be removed in 0.20."
-            " Use numpy.random.multivariate_normal instead.")
-def sample_gaussian(mean, covar, covariance_type='diag', n_samples=1,
-                    random_state=None):
-    """Generate random samples from a Gaussian distribution.
-
-    Parameters
-    ----------
-    mean : array_like, shape (n_features,)
-        Mean of the distribution.
-
-    covar : array_like
-        Covariance of the distribution. The shape depends on `covariance_type`:
-            scalar if 'spherical',
-            (n_features) if 'diag',
-            (n_features, n_features)  if 'tied', or 'full'
-
-    covariance_type : string, optional
-        Type of the covariance parameters. Must be one of
-        'spherical', 'tied', 'diag', 'full'. Defaults to 'diag'.
-
-    n_samples : int, optional
-        Number of samples to generate. Defaults to 1.
-
-    Returns
-    -------
-    X : array
-        Randomly generated sample. The shape depends on `n_samples`:
-        (n_features,) if `1`
-        (n_features, n_samples) otherwise
-    """
-    return _sample_gaussian(mean, covar, covariance_type='diag', n_samples=1,
-                            random_state=None)
-
-
-def _sample_gaussian(mean, covar, covariance_type='diag', n_samples=1,
-                     random_state=None):
-    rng = check_random_state(random_state)
-    n_dim = len(mean)
-    rand = rng.randn(n_dim, n_samples)
-    if n_samples == 1:
-        rand.shape = (n_dim,)
-
-    if covariance_type == 'spherical':
-        rand *= np.sqrt(covar)
-    elif covariance_type == 'diag':
-        rand = np.dot(np.diag(np.sqrt(covar)), rand)
-    else:
-        s, U = linalg.eigh(covar)
-        s.clip(0, out=s)  # get rid of tiny negatives
-        np.sqrt(s, out=s)
-        U *= s
-        rand = np.dot(U, rand)
-
-    return (rand.T + mean).T
-
-
-class _GMMBase(BaseEstimator):
-    """Gaussian Mixture Model.
-
-    Representation of a Gaussian mixture model probability distribution.
-    This class allows for easy evaluation of, sampling from, and
-    maximum-likelihood estimation of the parameters of a GMM distribution.
-
-    Initializes parameters such that every mixture component has zero
-    mean and identity covariance.
-
-    Read more in the :ref:`User Guide <gmm>`.
-
-    Parameters
-    ----------
-    n_components : int, optional
-        Number of mixture components. Defaults to 1.
-
-    covariance_type : string, optional
-        String describing the type of covariance parameters to
-        use.  Must be one of 'spherical', 'tied', 'diag', 'full'.
-        Defaults to 'diag'.
-
-    random_state : int, RandomState instance or None, optional (default=None)
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
-
-    min_covar : float, optional
-        Floor on the diagonal of the covariance matrix to prevent
-        overfitting. Defaults to 1e-3.
-
-    tol : float, optional
-        Convergence threshold. EM iterations will stop when average
-        gain in log-likelihood is below this threshold. Defaults to 1e-3.
-
-    n_iter : int, optional
-        Number of EM iterations to perform.
-
-    n_init : int, optional
-        Number of initializations to perform. The best results is kept.
-
-    params : string, optional
-        Controls which parameters are updated in the training
-        process.  Can contain any combination of 'w' for weights,
-        'm' for means, and 'c' for covars. Defaults to 'wmc'.
-
-    init_params : string, optional
-        Controls which parameters are updated in the initialization
-        process.  Can contain any combination of 'w' for weights,
-        'm' for means, and 'c' for covars. Defaults to 'wmc'.
-
-    verbose : int, default: 0
-        Enable verbose output. If 1 then it always prints the current
-        initialization and iteration step. If greater than 1 then
-        it prints additionally the change and time needed for each step.
-
-    Attributes
-    ----------
-    weights_ : array, shape (`n_components`,)
-        This attribute stores the mixing weights for each mixture component.
-
-    means_ : array, shape (`n_components`, `n_features`)
-        Mean parameters for each mixture component.
-
-    covars_ : array
-        Covariance parameters for each mixture component.  The shape
-        depends on `covariance_type`::
-
-            (n_components, n_features)             if 'spherical',
-            (n_features, n_features)               if 'tied',
-            (n_components, n_features)             if 'diag',
-            (n_components, n_features, n_features) if 'full'
-
-    converged_ : bool
-        True when convergence was reached in fit(), False otherwise.
-
-    See Also
-    --------
-
-    DPGMM : Infinite gaussian mixture model, using the Dirichlet
-        process, fit with a variational algorithm
-
-
-    VBGMM : Finite gaussian mixture model fit with a variational
-        algorithm, better for situations where there might be too little
-        data to get a good estimate of the covariance matrix.
-
-    Examples
-    --------
-
-    >>> import numpy as np
-    >>> from sklearn import mixture
-    >>> np.random.seed(1)
-    >>> g = mixture.GMM(n_components=2)
-    >>> # Generate random observations with two modes centered on 0
-    >>> # and 10 to use for training.
-    >>> obs = np.concatenate((np.random.randn(100, 1),
-    ...                       10 + np.random.randn(300, 1)))
-    >>> g.fit(obs)  # doctest: +NORMALIZE_WHITESPACE
-    GMM(covariance_type='diag', init_params='wmc', min_covar=0.001,
-            n_components=2, n_init=1, n_iter=100, params='wmc',
-            random_state=None, tol=0.001, verbose=0)
-    >>> np.round(g.weights_, 2)
-    array([0.75, 0.25])
-    >>> np.round(g.means_, 2)
-    array([[10.05],
-           [ 0.06]])
-    >>> np.round(g.covars_, 2) # doctest: +SKIP
-    array([[[ 1.02]],
-           [[ 0.96]]])
-    >>> g.predict([[0], [2], [9], [10]]) # doctest: +ELLIPSIS
-    array([1, 1, 0, 0]...)
-    >>> np.round(g.score([[0], [2], [9], [10]]), 2)
-    array([-2.19, -4.58, -1.75, -1.21])
-    >>> # Refit the model on new data (initial parameters remain the
-    >>> # same), this time with an even split between the two modes.
-    >>> g.fit(20 * [[0]] + 20 * [[10]])  # doctest: +NORMALIZE_WHITESPACE
-    GMM(covariance_type='diag', init_params='wmc', min_covar=0.001,
-            n_components=2, n_init=1, n_iter=100, params='wmc',
-            random_state=None, tol=0.001, verbose=0)
-    >>> np.round(g.weights_, 2)
-    array([0.5, 0.5])
-
-    """
-
-    def __init__(self, n_components=1, covariance_type='diag',
-                 random_state=None, tol=1e-3, min_covar=1e-3,
-                 n_iter=100, n_init=1, params='wmc', init_params='wmc',
-                 verbose=0):
-        self.n_components = n_components
-        self.covariance_type = covariance_type
-        self.tol = tol
-        self.min_covar = min_covar
-        self.random_state = random_state
-        self.n_iter = n_iter
-        self.n_init = n_init
-        self.params = params
-        self.init_params = init_params
-        self.verbose = verbose
-
-        if covariance_type not in ['spherical', 'tied', 'diag', 'full']:
-            raise ValueError('Invalid value for covariance_type: %s' %
-                             covariance_type)
-
-        if n_init < 1:
-            raise ValueError('GMM estimation requires at least one run')
-
-    def _get_covars(self):
-        """Covariance parameters for each mixture component.
-
-        The shape depends on ``cvtype``::
-
-            (n_states, n_features)                if 'spherical',
-            (n_features, n_features)              if 'tied',
-            (n_states, n_features)                if 'diag',
-            (n_states, n_features, n_features)    if 'full'
-
-        """
-        if self.covariance_type == 'full':
-            return self.covars_
-        elif self.covariance_type == 'diag':
-            return [np.diag(cov) for cov in self.covars_]
-        elif self.covariance_type == 'tied':
-            return [self.covars_] * self.n_components
-        elif self.covariance_type == 'spherical':
-            return [np.diag(cov) for cov in self.covars_]
-
-    def _set_covars(self, covars):
-        """Provide values for covariance."""
-        covars = np.asarray(covars)
-        _validate_covars(covars, self.covariance_type, self.n_components)
-        self.covars_ = covars
-
-    def score_samples(self, X):
-        """Return the per-sample likelihood of the data under the model.
-
-        Compute the log probability of X under the model and
-        return the posterior distribution (responsibilities) of each
-        mixture component for each element of X.
-
-        Parameters
-        ----------
-        X : array_like, shape (n_samples, n_features)
-            List of n_features-dimensional data points. Each row
-            corresponds to a single data point.
-
-        Returns
-        -------
-        logprob : array_like, shape (n_samples,)
-            Log probabilities of each data point in X.
-
-        responsibilities : array_like, shape (n_samples, n_components)
-            Posterior probabilities of each mixture component for each
-            observation
-        """
-        check_is_fitted(self, 'means_')
-
-        X = check_array(X)
-        if X.ndim == 1:
-            X = X[:, np.newaxis]
-        if X.size == 0:
-            return np.array([]), np.empty((0, self.n_components))
-        if X.shape[1] != self.means_.shape[1]:
-            raise ValueError('The shape of X  is not compatible with self')
-
-        lpr = (log_multivariate_normal_density(X, self.means_, self.covars_,
-                                               self.covariance_type) +
-               np.log(self.weights_))
-        logprob = logsumexp(lpr, axis=1)
-        responsibilities = np.exp(lpr - logprob[:, np.newaxis])
-        return logprob, responsibilities
-
-    def score(self, X, y=None):
-        """Compute the log probability under the model.
-
-        Parameters
-        ----------
-        X : array_like, shape (n_samples, n_features)
-            List of n_features-dimensional data points. Each row
-            corresponds to a single data point.
-
-        Returns
-        -------
-        logprob : array_like, shape (n_samples,)
-            Log probabilities of each data point in X
-        """
-        logprob, _ = self.score_samples(X)
-        return logprob
-
-    def predict(self, X):
-        """Predict label for data.
-
-        Parameters
-        ----------
-        X : array-like, shape = [n_samples, n_features]
-
-        Returns
-        -------
-        C : array, shape = (n_samples,) component memberships
-        """
-        logprob, responsibilities = self.score_samples(X)
-        return responsibilities.argmax(axis=1)
-
-    def predict_proba(self, X):
-        """Predict posterior probability of data under each Gaussian
-        in the model.
-
-        Parameters
-        ----------
-        X : array-like, shape = [n_samples, n_features]
-
-        Returns
-        -------
-        responsibilities : array-like, shape = (n_samples, n_components)
-            Returns the probability of the sample for each Gaussian
-            (state) in the model.
-        """
-        logprob, responsibilities = self.score_samples(X)
-        return responsibilities
-
-    def sample(self, n_samples=1, random_state=None):
-        """Generate random samples from the model.
-
-        Parameters
-        ----------
-        n_samples : int, optional
-            Number of samples to generate. Defaults to 1.
-
-        Returns
-        -------
-        X : array_like, shape (n_samples, n_features)
-            List of samples
-        """
-        check_is_fitted(self, 'means_')
-
-        if random_state is None:
-            random_state = self.random_state
-        random_state = check_random_state(random_state)
-        weight_cdf = np.cumsum(self.weights_)
-
-        X = np.empty((n_samples, self.means_.shape[1]))
-        rand = random_state.rand(n_samples)
-        # decide which component to use for each sample
-        comps = weight_cdf.searchsorted(rand)
-        # for each component, generate all needed samples
-        for comp in range(self.n_components):
-            # occurrences of current component in X
-            comp_in_X = (comp == comps)
-            # number of those occurrences
-            num_comp_in_X = comp_in_X.sum()
-            if num_comp_in_X > 0:
-                if self.covariance_type == 'tied':
-                    cv = self.covars_
-                elif self.covariance_type == 'spherical':
-                    cv = self.covars_[comp][0]
-                else:
-                    cv = self.covars_[comp]
-                X[comp_in_X] = _sample_gaussian(
-                    self.means_[comp], cv, self.covariance_type,
-                    num_comp_in_X, random_state=random_state).T
-        return X
-
-    def fit_predict(self, X, y=None):
-        """Fit and then predict labels for data.
-
-        Warning: Due to the final maximization step in the EM algorithm,
-        with low iterations the prediction may not be 100%  accurate.
-
-        .. versionadded:: 0.17
-           *fit_predict* method in Gaussian Mixture Model.
-
-        Parameters
-        ----------
-        X : array-like, shape = [n_samples, n_features]
-
-        Returns
-        -------
-        C : array, shape = (n_samples,) component memberships
-        """
-        return self._fit(X, y).argmax(axis=1)
-
-    def _fit(self, X, y=None, do_prediction=False):
-        """Estimate model parameters with the EM algorithm.
-
-        A initialization step is performed before entering the
-        expectation-maximization (EM) algorithm. If you want to avoid
-        this step, set the keyword argument init_params to the empty
-        string '' when creating the GMM object. Likewise, if you would
-        like just to do an initialization, set n_iter=0.
-
-        Parameters
-        ----------
-        X : array_like, shape (n, n_features)
-            List of n_features-dimensional data points. Each row
-            corresponds to a single data point.
-
-        Returns
-        -------
-        responsibilities : array, shape (n_samples, n_components)
-            Posterior probabilities of each mixture component for each
-            observation.
-        """
-
-        # initialization step
-        X = check_array(X, dtype=np.float64, ensure_min_samples=2,
-                        estimator=self)
-        if X.shape[0] < self.n_components:
-            raise ValueError(
-                'GMM estimation with %s components, but got only %s samples' %
-                (self.n_components, X.shape[0]))
-
-        max_log_prob = -np.infty
-
-        if self.verbose > 0:
-            print('Expectation-maximization algorithm started.')
-
-        for init in range(self.n_init):
-            if self.verbose > 0:
-                print('Initialization ' + str(init + 1))
-                start_init_time = time()
-
-            if 'm' in self.init_params or not hasattr(self, 'means_'):
-                self.means_ = cluster.KMeans(
-                    n_clusters=self.n_components,
-                    random_state=self.random_state).fit(X).cluster_centers_
-                if self.verbose > 1:
-                    print('\tMeans have been initialized.')
-
-            if 'w' in self.init_params or not hasattr(self, 'weights_'):
-                self.weights_ = np.tile(1.0 / self.n_components,
-                                        self.n_components)
-                if self.verbose > 1:
-                    print('\tWeights have been initialized.')
-
-            if 'c' in self.init_params or not hasattr(self, 'covars_'):
-                cv = np.cov(X.T) + self.min_covar * np.eye(X.shape[1])
-                if not cv.shape:
-                    cv.shape = (1, 1)
-                self.covars_ = \
-                    distribute_covar_matrix_to_match_covariance_type(
-                        cv, self.covariance_type, self.n_components)
-                if self.verbose > 1:
-                    print('\tCovariance matrices have been initialized.')
-
-            # EM algorithms
-            current_log_likelihood = None
-            # reset self.converged_ to False
-            self.converged_ = False
-
-            for i in range(self.n_iter):
-                if self.verbose > 0:
-                    print('\tEM iteration ' + str(i + 1))
-                    start_iter_time = time()
-                prev_log_likelihood = current_log_likelihood
-                # Expectation step
-                log_likelihoods, responsibilities = self.score_samples(X)
-                current_log_likelihood = log_likelihoods.mean()
-
-                # Check for convergence.
-                if prev_log_likelihood is not None:
-                    change = abs(current_log_likelihood - prev_log_likelihood)
-                    if self.verbose > 1:
-                        print('\t\tChange: ' + str(change))
-                    if change < self.tol:
-                        self.converged_ = True
-                        if self.verbose > 0:
-                            print('\t\tEM algorithm converged.')
-                        break
-
-                # Maximization step
-                self._do_mstep(X, responsibilities, self.params,
-                               self.min_covar)
-                if self.verbose > 1:
-                    print('\t\tEM iteration ' + str(i + 1) + ' took {0:.5f}s'.format(
-                        time() - start_iter_time))
-
-            # if the results are better, keep it
-            if self.n_iter:
-                if current_log_likelihood > max_log_prob:
-                    max_log_prob = current_log_likelihood
-                    best_params = {'weights': self.weights_,
-                                   'means': self.means_,
-                                   'covars': self.covars_}
-                    if self.verbose > 1:
-                        print('\tBetter parameters were found.')
-
-            if self.verbose > 1:
-                print('\tInitialization ' + str(init + 1) + ' took {0:.5f}s'.format(
-                    time() - start_init_time))
-
-        # check the existence of an init param that was not subject to
-        # likelihood computation issue.
-        if np.isneginf(max_log_prob) and self.n_iter:
-            raise RuntimeError(
-                "EM algorithm was never able to compute a valid likelihood " +
-                "given initial parameters. Try different init parameters " +
-                "(or increasing n_init) or check for degenerate data.")
-
-        if self.n_iter:
-            self.covars_ = best_params['covars']
-            self.means_ = best_params['means']
-            self.weights_ = best_params['weights']
-        else:  # self.n_iter == 0 occurs when using GMM within HMM
-            # Need to make sure that there are responsibilities to output
-            # Output zeros because it was just a quick initialization
-            responsibilities = np.zeros((X.shape[0], self.n_components))
-
-        return responsibilities
-
-    def fit(self, X, y=None):
-        """Estimate model parameters with the EM algorithm.
-
-        A initialization step is performed before entering the
-        expectation-maximization (EM) algorithm. If you want to avoid
-        this step, set the keyword argument init_params to the empty
-        string '' when creating the GMM object. Likewise, if you would
-        like just to do an initialization, set n_iter=0.
-
-        Parameters
-        ----------
-        X : array_like, shape (n, n_features)
-            List of n_features-dimensional data points.  Each row
-            corresponds to a single data point.
-
-        Returns
-        -------
-        self
-        """
-        self._fit(X, y)
-        return self
-
-    def _do_mstep(self, X, responsibilities, params, min_covar=0):
-        """Perform the Mstep of the EM algorithm and return the cluster weights.
-        """
-        weights = responsibilities.sum(axis=0)
-        weighted_X_sum = np.dot(responsibilities.T, X)
-        inverse_weights = 1.0 / (weights[:, np.newaxis] + 10 * EPS)
-
-        if 'w' in params:
-            self.weights_ = (weights / (weights.sum() + 10 * EPS) + EPS)
-        if 'm' in params:
-            self.means_ = weighted_X_sum * inverse_weights
-        if 'c' in params:
-            covar_mstep_func = _covar_mstep_funcs[self.covariance_type]
-            self.covars_ = covar_mstep_func(
-                self, X, responsibilities, weighted_X_sum, inverse_weights,
-                min_covar)
-        return weights
-
-    def _n_parameters(self):
-        """Return the number of free parameters in the model."""
-        ndim = self.means_.shape[1]
-        if self.covariance_type == 'full':
-            cov_params = self.n_components * ndim * (ndim + 1) / 2.
-        elif self.covariance_type == 'diag':
-            cov_params = self.n_components * ndim
-        elif self.covariance_type == 'tied':
-            cov_params = ndim * (ndim + 1) / 2.
-        elif self.covariance_type == 'spherical':
-            cov_params = self.n_components
-        mean_params = ndim * self.n_components
-        return int(cov_params + mean_params + self.n_components - 1)
-
-    def bic(self, X):
-        """Bayesian information criterion for the current model fit
-        and the proposed data.
-
-        Parameters
-        ----------
-        X : array of shape(n_samples, n_dimensions)
-
-        Returns
-        -------
-        bic : float (the lower the better)
-        """
-        return (-2 * self.score(X).sum() +
-                self._n_parameters() * np.log(X.shape[0]))
-
-    def aic(self, X):
-        """Akaike information criterion for the current model fit
-        and the proposed data.
-
-        Parameters
-        ----------
-        X : array of shape(n_samples, n_dimensions)
-
-        Returns
-        -------
-        aic : float (the lower the better)
-        """
-        return - 2 * self.score(X).sum() + 2 * self._n_parameters()
-
-
-@deprecated("The class GMM is deprecated in 0.18 and will be "
-            " removed in 0.20. Use class GaussianMixture instead.")
-class GMM(_GMMBase):
-    """
-    Legacy Gaussian Mixture Model
-
-    .. deprecated:: 0.18
-        This class will be removed in 0.20.
-        Use :class:`sklearn.mixture.GaussianMixture` instead.
-
-    """
-
-    def __init__(self, n_components=1, covariance_type='diag',
-                 random_state=None, tol=1e-3, min_covar=1e-3,
-                 n_iter=100, n_init=1, params='wmc', init_params='wmc',
-                 verbose=0):
-        super(GMM, self).__init__(
-            n_components=n_components, covariance_type=covariance_type,
-            random_state=random_state, tol=tol, min_covar=min_covar,
-            n_iter=n_iter, n_init=n_init, params=params,
-            init_params=init_params, verbose=verbose)
-
-#########################################################################
-# some helper routines
-#########################################################################
-
-
-def _log_multivariate_normal_density_diag(X, means, covars):
-    """Compute Gaussian log-density at X for a diagonal model."""
-    n_samples, n_dim = X.shape
-    lpr = -0.5 * (n_dim * np.log(2 * np.pi) + np.sum(np.log(covars), 1)
-                  + np.sum((means ** 2) / covars, 1)
-                  - 2 * np.dot(X, (means / covars).T)
-                  + np.dot(X ** 2, (1.0 / covars).T))
-    return lpr
-
-
-def _log_multivariate_normal_density_spherical(X, means, covars):
-    """Compute Gaussian log-density at X for a spherical model."""
-    cv = covars.copy()
-    if covars.ndim == 1:
-        cv = cv[:, np.newaxis]
-    if cv.shape[1] == 1:
-        cv = np.tile(cv, (1, X.shape[-1]))
-    return _log_multivariate_normal_density_diag(X, means, cv)
-
-
-def _log_multivariate_normal_density_tied(X, means, covars):
-    """Compute Gaussian log-density at X for a tied model."""
-    cv = np.tile(covars, (means.shape[0], 1, 1))
-    return _log_multivariate_normal_density_full(X, means, cv)
-
-
-def _log_multivariate_normal_density_full(X, means, covars, min_covar=1.e-7):
-    """Log probability for full covariance matrices."""
-    n_samples, n_dim = X.shape
-    nmix = len(means)
-    log_prob = np.empty((n_samples, nmix))
-    for c, (mu, cv) in enumerate(zip(means, covars)):
-        try:
-            cv_chol = linalg.cholesky(cv, lower=True)
-        except linalg.LinAlgError:
-            # The model is most probably stuck in a component with too
-            # few observations, we need to reinitialize this components
-            try:
-                cv_chol = linalg.cholesky(cv + min_covar * np.eye(n_dim),
-                                          lower=True)
-            except linalg.LinAlgError:
-                raise ValueError("'covars' must be symmetric, "
-                                 "positive-definite")
-
-        cv_log_det = 2 * np.sum(np.log(np.diagonal(cv_chol)))
-        cv_sol = linalg.solve_triangular(cv_chol, (X - mu).T, lower=True).T
-        log_prob[:, c] = - .5 * (np.sum(cv_sol ** 2, axis=1) +
-                                 n_dim * np.log(2 * np.pi) + cv_log_det)
-
-    return log_prob
-
-
-def _validate_covars(covars, covariance_type, n_components):
-    """Do basic checks on matrix covariance sizes and values."""
-    from scipy import linalg
-    if covariance_type == 'spherical':
-        if len(covars) != n_components:
-            raise ValueError("'spherical' covars have length n_components")
-        elif np.any(covars <= 0):
-            raise ValueError("'spherical' covars must be non-negative")
-    elif covariance_type == 'tied':
-        if covars.shape[0] != covars.shape[1]:
-            raise ValueError("'tied' covars must have shape (n_dim, n_dim)")
-        elif (not np.allclose(covars, covars.T)
-              or np.any(linalg.eigvalsh(covars) <= 0)):
-            raise ValueError("'tied' covars must be symmetric, "
-                             "positive-definite")
-    elif covariance_type == 'diag':
-        if len(covars.shape) != 2:
-            raise ValueError("'diag' covars must have shape "
-                             "(n_components, n_dim)")
-        elif np.any(covars <= 0):
-            raise ValueError("'diag' covars must be non-negative")
-    elif covariance_type == 'full':
-        if len(covars.shape) != 3:
-            raise ValueError("'full' covars must have shape "
-                             "(n_components, n_dim, n_dim)")
-        elif covars.shape[1] != covars.shape[2]:
-            raise ValueError("'full' covars must have shape "
-                             "(n_components, n_dim, n_dim)")
-        for n, cv in enumerate(covars):
-            if (not np.allclose(cv, cv.T)
-                    or np.any(linalg.eigvalsh(cv) <= 0)):
-                raise ValueError("component %d of 'full' covars must be "
-                                 "symmetric, positive-definite" % n)
-    else:
-        raise ValueError("covariance_type must be one of " +
-                         "'spherical', 'tied', 'diag', 'full'")
-
-
-@deprecated("The function distribute_covar_matrix_to_match_covariance_type"
-            "is deprecated in 0.18 and will be removed in 0.20.")
-def distribute_covar_matrix_to_match_covariance_type(
-        tied_cv, covariance_type, n_components):
-    """Create all the covariance matrices from a given template."""
-    if covariance_type == 'spherical':
-        cv = np.tile(tied_cv.mean() * np.ones(tied_cv.shape[1]),
-                     (n_components, 1))
-    elif covariance_type == 'tied':
-        cv = tied_cv
-    elif covariance_type == 'diag':
-        cv = np.tile(np.diag(tied_cv), (n_components, 1))
-    elif covariance_type == 'full':
-        cv = np.tile(tied_cv, (n_components, 1, 1))
-    else:
-        raise ValueError("covariance_type must be one of " +
-                         "'spherical', 'tied', 'diag', 'full'")
-    return cv
-
-
-def _covar_mstep_diag(gmm, X, responsibilities, weighted_X_sum, norm,
-                      min_covar):
-    """Perform the covariance M step for diagonal cases."""
-    avg_X2 = np.dot(responsibilities.T, X * X) * norm
-    avg_means2 = gmm.means_ ** 2
-    avg_X_means = gmm.means_ * weighted_X_sum * norm
-    return avg_X2 - 2 * avg_X_means + avg_means2 + min_covar
-
-
-def _covar_mstep_spherical(*args):
-    """Perform the covariance M step for spherical cases."""
-    cv = _covar_mstep_diag(*args)
-    return np.tile(cv.mean(axis=1)[:, np.newaxis], (1, cv.shape[1]))
-
-
-def _covar_mstep_full(gmm, X, responsibilities, weighted_X_sum, norm,
-                      min_covar):
-    """Perform the covariance M step for full cases."""
-    # Eq. 12 from K. Murphy, "Fitting a Conditional Linear Gaussian
-    # Distribution"
-    n_features = X.shape[1]
-    cv = np.empty((gmm.n_components, n_features, n_features))
-    for c in range(gmm.n_components):
-        post = responsibilities[:, c]
-        mu = gmm.means_[c]
-        diff = X - mu
-        with np.errstate(under='ignore'):
-            # Underflow Errors in doing post * X.T are  not important
-            avg_cv = np.dot(post * diff.T, diff) / (post.sum() + 10 * EPS)
-        cv[c] = avg_cv + min_covar * np.eye(n_features)
-    return cv
-
-
-def _covar_mstep_tied(gmm, X, responsibilities, weighted_X_sum, norm,
-                      min_covar):
-    """Perform the covariance M step for tied cases."""
-    # Eq. 15 from K. Murphy, "Fitting a Conditional Linear Gaussian
-    # Distribution"
-    avg_X2 = np.dot(X.T, X)
-    avg_means2 = np.dot(gmm.means_.T, weighted_X_sum)
-    out = avg_X2 - avg_means2
-    out *= 1. / X.shape[0]
-    out.flat[::len(out) + 1] += min_covar
-    return out
-
-_covar_mstep_funcs = {'spherical': _covar_mstep_spherical,
-                      'diag': _covar_mstep_diag,
-                      'tied': _covar_mstep_tied,
-                      'full': _covar_mstep_full,
-                      }
diff --git a/sklearn/mixture/tests/test_dpgmm.py b/sklearn/mixture/tests/test_dpgmm.py
deleted file mode 100644
index 8ca38626b4ce..000000000000
--- a/sklearn/mixture/tests/test_dpgmm.py
+++ /dev/null
@@ -1,237 +0,0 @@
-# Important note for the deprecation cleaning of 0.20 :
-# All the function and classes of this file have been deprecated in 0.18.
-# When you remove this file please also remove the related files
-# - 'sklearn/mixture/dpgmm.py'
-# - 'sklearn/mixture/gmm.py'
-# - 'sklearn/mixture/test_gmm.py'
-import unittest
-import sys
-
-import numpy as np
-
-from sklearn.mixture import DPGMM, VBGMM
-from sklearn.mixture.dpgmm import log_normalize
-from sklearn.datasets import make_blobs
-from sklearn.utils.testing import assert_array_less, assert_equal
-from sklearn.utils.testing import assert_warns_message, ignore_warnings
-from sklearn.mixture.tests.test_gmm import GMMTester
-from sklearn.externals.six.moves import cStringIO as StringIO
-from sklearn.mixture.dpgmm import digamma, gammaln
-from sklearn.mixture.dpgmm import wishart_log_det, wishart_logz
-
-
-np.seterr(all='warn')
-
-
-@ignore_warnings(category=DeprecationWarning)
-def test_class_weights():
-    # check that the class weights are updated
-    # simple 3 cluster dataset
-    X, y = make_blobs(random_state=1)
-    for Model in [DPGMM, VBGMM]:
-        dpgmm = Model(n_components=10, random_state=1, alpha=20, n_iter=50)
-        dpgmm.fit(X)
-        # get indices of components that are used:
-        indices = np.unique(dpgmm.predict(X))
-        active = np.zeros(10, dtype=np.bool)
-        active[indices] = True
-        # used components are important
-        assert_array_less(.1, dpgmm.weights_[active])
-        # others are not
-        assert_array_less(dpgmm.weights_[~active], .05)
-
-
-@ignore_warnings(category=DeprecationWarning)
-def test_verbose_boolean():
-    # checks that the output for the verbose output is the same
-    # for the flag values '1' and 'True'
-    # simple 3 cluster dataset
-    X, y = make_blobs(random_state=1)
-    for Model in [DPGMM, VBGMM]:
-        dpgmm_bool = Model(n_components=10, random_state=1, alpha=20,
-                           n_iter=50, verbose=True)
-        dpgmm_int = Model(n_components=10, random_state=1, alpha=20,
-                          n_iter=50, verbose=1)
-
-        old_stdout = sys.stdout
-        sys.stdout = StringIO()
-        try:
-            # generate output with the boolean flag
-            dpgmm_bool.fit(X)
-            verbose_output = sys.stdout
-            verbose_output.seek(0)
-            bool_output = verbose_output.readline()
-            # generate output with the int flag
-            dpgmm_int.fit(X)
-            verbose_output = sys.stdout
-            verbose_output.seek(0)
-            int_output = verbose_output.readline()
-            assert_equal(bool_output, int_output)
-        finally:
-            sys.stdout = old_stdout
-
-
-@ignore_warnings(category=DeprecationWarning)
-def test_verbose_first_level():
-    # simple 3 cluster dataset
-    X, y = make_blobs(random_state=1)
-    for Model in [DPGMM, VBGMM]:
-        dpgmm = Model(n_components=10, random_state=1, alpha=20, n_iter=50,
-                      verbose=1)
-
-        old_stdout = sys.stdout
-        sys.stdout = StringIO()
-        try:
-            dpgmm.fit(X)
-        finally:
-            sys.stdout = old_stdout
-
-
-@ignore_warnings(category=DeprecationWarning)
-def test_verbose_second_level():
-    # simple 3 cluster dataset
-    X, y = make_blobs(random_state=1)
-    for Model in [DPGMM, VBGMM]:
-        dpgmm = Model(n_components=10, random_state=1, alpha=20, n_iter=50,
-                      verbose=2)
-
-        old_stdout = sys.stdout
-        sys.stdout = StringIO()
-        try:
-            dpgmm.fit(X)
-        finally:
-            sys.stdout = old_stdout
-
-
-@ignore_warnings(category=DeprecationWarning)
-def test_digamma():
-    assert_warns_message(DeprecationWarning, "The function digamma is"
-                         " deprecated in 0.18 and will be removed in 0.20. "
-                         "Use scipy.special.digamma instead.", digamma, 3)
-
-
-@ignore_warnings(category=DeprecationWarning)
-def test_gammaln():
-    assert_warns_message(DeprecationWarning, "The function gammaln"
-                         " is deprecated in 0.18 and will be removed"
-                         " in 0.20. Use scipy.special.gammaln instead.",
-                         gammaln, 3)
-
-
-@ignore_warnings(category=DeprecationWarning)
-def test_log_normalize():
-    v = np.array([0.1, 0.8, 0.01, 0.09])
-    a = np.log(2 * v)
-    result = assert_warns_message(DeprecationWarning, "The function "
-                                  "log_normalize is deprecated in 0.18 and"
-                                  " will be removed in 0.20.",
-                                  log_normalize, a)
-    assert np.allclose(v, result, rtol=0.01)
-
-
-@ignore_warnings(category=DeprecationWarning)
-def test_wishart_log_det():
-    a = np.array([0.1, 0.8, 0.01, 0.09])
-    b = np.array([0.2, 0.7, 0.05, 0.1])
-    assert_warns_message(DeprecationWarning, "The function "
-                         "wishart_log_det is deprecated in 0.18 and"
-                         " will be removed in 0.20.",
-                         wishart_log_det, a, b, 2, 4)
-
-
-@ignore_warnings(category=DeprecationWarning)
-def test_wishart_logz():
-    assert_warns_message(DeprecationWarning, "The function "
-                         "wishart_logz is deprecated in 0.18 and "
-                         "will be removed in 0.20.", wishart_logz,
-                         3, np.identity(3), 1, 3)
-
-
-@ignore_warnings(category=DeprecationWarning)
-def test_DPGMM_deprecation():
-    assert_warns_message(
-      DeprecationWarning, "The `DPGMM` class is not working correctly and "
-      "it's better to use `sklearn.mixture.BayesianGaussianMixture` class "
-      "with parameter `weight_concentration_prior_type='dirichlet_process'` "
-      "instead. DPGMM is deprecated in 0.18 and will be removed in 0.20.",
-      DPGMM)
-
-
-def do_model(self, **kwds):
-    return VBGMM(verbose=False, **kwds)
-
-
-class DPGMMTester(GMMTester):
-    model = DPGMM
-    do_test_eval = False
-
-    def score(self, g, train_obs):
-        _, z = g.score_samples(train_obs)
-        return g.lower_bound(train_obs, z)
-
-
-class TestDPGMMWithSphericalCovars(unittest.TestCase, DPGMMTester):
-    covariance_type = 'spherical'
-    setUp = GMMTester._setUp
-
-
-class TestDPGMMWithDiagCovars(unittest.TestCase, DPGMMTester):
-    covariance_type = 'diag'
-    setUp = GMMTester._setUp
-
-
-class TestDPGMMWithTiedCovars(unittest.TestCase, DPGMMTester):
-    covariance_type = 'tied'
-    setUp = GMMTester._setUp
-
-
-class TestDPGMMWithFullCovars(unittest.TestCase, DPGMMTester):
-    covariance_type = 'full'
-    setUp = GMMTester._setUp
-
-
-def test_VBGMM_deprecation():
-    assert_warns_message(
-        DeprecationWarning, "The `VBGMM` class is not working correctly and "
-        "it's better to use `sklearn.mixture.BayesianGaussianMixture` class "
-        "with parameter `weight_concentration_prior_type="
-        "'dirichlet_distribution'` instead. VBGMM is deprecated "
-        "in 0.18 and will be removed in 0.20.", VBGMM)
-
-
-class VBGMMTester(GMMTester):
-    model = do_model
-    do_test_eval = False
-
-    def score(self, g, train_obs):
-        _, z = g.score_samples(train_obs)
-        return g.lower_bound(train_obs, z)
-
-
-class TestVBGMMWithSphericalCovars(unittest.TestCase, VBGMMTester):
-    covariance_type = 'spherical'
-    setUp = GMMTester._setUp
-
-
-class TestVBGMMWithDiagCovars(unittest.TestCase, VBGMMTester):
-    covariance_type = 'diag'
-    setUp = GMMTester._setUp
-
-
-class TestVBGMMWithTiedCovars(unittest.TestCase, VBGMMTester):
-    covariance_type = 'tied'
-    setUp = GMMTester._setUp
-
-
-class TestVBGMMWithFullCovars(unittest.TestCase, VBGMMTester):
-    covariance_type = 'full'
-    setUp = GMMTester._setUp
-
-
-def test_vbgmm_no_modify_alpha():
-    alpha = 2.
-    n_components = 3
-    X, y = make_blobs(random_state=1)
-    vbgmm = VBGMM(n_components=n_components, alpha=alpha, n_iter=1)
-    assert_equal(vbgmm.alpha, alpha)
-    assert_equal(vbgmm.fit(X).alpha_, float(alpha) / n_components)
diff --git a/sklearn/mixture/tests/test_gmm.py b/sklearn/mixture/tests/test_gmm.py
deleted file mode 100644
index 137703adfcad..000000000000
--- a/sklearn/mixture/tests/test_gmm.py
+++ /dev/null
@@ -1,534 +0,0 @@
-# Important note for the deprecation cleaning of 0.20 :
-# All the functions and classes of this file have been deprecated in 0.18.
-# When you remove this file please remove the related files
-# - 'sklearn/mixture/dpgmm.py'
-# - 'sklearn/mixture/gmm.py'
-# - 'sklearn/mixture/test_dpgmm.py'
-import unittest
-import copy
-import sys
-
-import numpy as np
-from numpy.testing import assert_array_equal, assert_array_almost_equal
-
-from scipy import stats
-from sklearn import mixture
-from sklearn.datasets.samples_generator import make_spd_matrix
-from sklearn.utils.testing import (assert_true, assert_greater,
-                                   assert_raise_message, assert_warns_message,
-                                   ignore_warnings, assert_raises)
-from sklearn.metrics.cluster import adjusted_rand_score
-from sklearn.externals.six.moves import cStringIO as StringIO
-
-
-rng = np.random.RandomState(0)
-
-
-def test_sample_gaussian():
-    # Test sample generation from mixture.sample_gaussian where covariance
-    # is diagonal, spherical and full
-
-    n_features, n_samples = 2, 300
-    axis = 1
-    mu = rng.randint(10) * rng.rand(n_features)
-    cv = (rng.rand(n_features) + 1.0) ** 2
-
-    samples = mixture.gmm._sample_gaussian(
-        mu, cv, covariance_type='diag', n_samples=n_samples)
-
-    assert_true(np.allclose(samples.mean(axis), mu, atol=1.3))
-    assert_true(np.allclose(samples.var(axis), cv, atol=1.5))
-
-    # the same for spherical covariances
-    cv = (rng.rand() + 1.0) ** 2
-    samples = mixture.gmm._sample_gaussian(
-        mu, cv, covariance_type='spherical', n_samples=n_samples)
-
-    assert_true(np.allclose(samples.mean(axis), mu, atol=1.5))
-    assert_true(np.allclose(
-        samples.var(axis), np.repeat(cv, n_features), atol=1.5))
-
-    # and for full covariances
-    A = rng.randn(n_features, n_features)
-    cv = np.dot(A.T, A) + np.eye(n_features)
-    samples = mixture.gmm._sample_gaussian(
-        mu, cv, covariance_type='full', n_samples=n_samples)
-    assert_true(np.allclose(samples.mean(axis), mu, atol=1.3))
-    assert_true(np.allclose(np.cov(samples), cv, atol=2.5))
-
-    # Numerical stability check: in SciPy 0.12.0 at least, eigh may return
-    # tiny negative values in its second return value.
-    x = mixture.gmm._sample_gaussian(
-        [0, 0], [[4, 3], [1, .1]], covariance_type='full', random_state=42)
-    assert_true(np.isfinite(x).all())
-
-
-def _naive_lmvnpdf_diag(X, mu, cv):
-    # slow and naive implementation of lmvnpdf
-    ref = np.empty((len(X), len(mu)))
-    stds = np.sqrt(cv)
-    for i, (m, std) in enumerate(zip(mu, stds)):
-        ref[:, i] = np.log(stats.norm.pdf(X, m, std)).sum(axis=1)
-    return ref
-
-
-def test_lmvnpdf_diag():
-    # test a slow and naive implementation of lmvnpdf and
-    # compare it to the vectorized version (mixture.lmvnpdf) to test
-    # for correctness
-    n_features, n_components, n_samples = 2, 3, 10
-    mu = rng.randint(10) * rng.rand(n_components, n_features)
-    cv = (rng.rand(n_components, n_features) + 1.0) ** 2
-    X = rng.randint(10) * rng.rand(n_samples, n_features)
-
-    ref = _naive_lmvnpdf_diag(X, mu, cv)
-    lpr = assert_warns_message(DeprecationWarning, "The function"
-                             " log_multivariate_normal_density is "
-                             "deprecated in 0.18 and will be removed in 0.20.",
-                             mixture.log_multivariate_normal_density,
-                             X, mu, cv, 'diag')
-    assert_array_almost_equal(lpr, ref)
-
-
-def test_lmvnpdf_spherical():
-    n_features, n_components, n_samples = 2, 3, 10
-
-    mu = rng.randint(10) * rng.rand(n_components, n_features)
-    spherecv = rng.rand(n_components, 1) ** 2 + 1
-    X = rng.randint(10) * rng.rand(n_samples, n_features)
-
-    cv = np.tile(spherecv, (n_features, 1))
-    reference = _naive_lmvnpdf_diag(X, mu, cv)
-    lpr = assert_warns_message(DeprecationWarning, "The function"
-                             " log_multivariate_normal_density is "
-                             "deprecated in 0.18 and will be removed in 0.20.",
-                             mixture.log_multivariate_normal_density,
-                             X, mu, spherecv, 'spherical')
-    assert_array_almost_equal(lpr, reference)
-
-def test_lmvnpdf_full():
-    n_features, n_components, n_samples = 2, 3, 10
-
-    mu = rng.randint(10) * rng.rand(n_components, n_features)
-    cv = (rng.rand(n_components, n_features) + 1.0) ** 2
-    X = rng.randint(10) * rng.rand(n_samples, n_features)
-
-    fullcv = np.array([np.diag(x) for x in cv])
-
-    reference = _naive_lmvnpdf_diag(X, mu, cv)
-    lpr = assert_warns_message(DeprecationWarning, "The function"
-                             " log_multivariate_normal_density is "
-                             "deprecated in 0.18 and will be removed in 0.20.",
-                             mixture.log_multivariate_normal_density,
-                             X, mu, fullcv, 'full')
-    assert_array_almost_equal(lpr, reference)
-
-
-def test_lvmpdf_full_cv_non_positive_definite():
-    n_features, n_samples = 2, 10
-    rng = np.random.RandomState(0)
-    X = rng.randint(10) * rng.rand(n_samples, n_features)
-    mu = np.mean(X, 0)
-    cv = np.array([[[-1, 0], [0, 1]]])
-    expected_message = "'covars' must be symmetric, positive-definite"
-    assert_raise_message(ValueError, expected_message,
-                         mixture.log_multivariate_normal_density,
-                         X, mu, cv, 'full')
-
-
-# This function tests the deprecated old GMM class
-@ignore_warnings(category=DeprecationWarning)
-def test_GMM_attributes():
-    n_components, n_features = 10, 4
-    covariance_type = 'diag'
-    g = mixture.GMM(n_components, covariance_type, random_state=rng)
-    weights = rng.rand(n_components)
-    weights = weights / weights.sum()
-    means = rng.randint(-20, 20, (n_components, n_features))
-
-    assert_true(g.n_components == n_components)
-    assert_true(g.covariance_type == covariance_type)
-
-    g.weights_ = weights
-    assert_array_almost_equal(g.weights_, weights)
-    g.means_ = means
-    assert_array_almost_equal(g.means_, means)
-
-    covars = (0.1 + 2 * rng.rand(n_components, n_features)) ** 2
-    g.covars_ = covars
-    assert_array_almost_equal(g.covars_, covars)
-    assert_raises(ValueError, g._set_covars, [])
-    assert_raises(ValueError, g._set_covars,
-                  np.zeros((n_components - 2, n_features)))
-
-    assert_raises(ValueError, mixture.GMM, n_components=20,
-                  covariance_type='badcovariance_type')
-
-
-class GMMTester():
-    do_test_eval = True
-
-    def _setUp(self):
-        self.n_components = 10
-        self.n_features = 4
-        self.weights = rng.rand(self.n_components)
-        self.weights = self.weights / self.weights.sum()
-        self.means = rng.randint(-20, 20, (self.n_components, self.n_features))
-        self.threshold = -0.5
-        self.I = np.eye(self.n_features)
-        self.covars = {
-            'spherical': (0.1 + 2 * rng.rand(self.n_components,
-                                             self.n_features)) ** 2,
-            'tied': (make_spd_matrix(self.n_features, random_state=0)
-                     + 5 * self.I),
-            'diag': (0.1 + 2 * rng.rand(self.n_components,
-                                        self.n_features)) ** 2,
-            'full': np.array([make_spd_matrix(self.n_features, random_state=0)
-                              + 5 * self.I for x in range(self.n_components)])}
-
-    # This function tests the deprecated old GMM class
-    @ignore_warnings(category=DeprecationWarning)
-    def test_eval(self):
-        if not self.do_test_eval:
-            return  # DPGMM does not support setting the means and
-        # covariances before fitting There is no way of fixing this
-        # due to the variational parameters being more expressive than
-        # covariance matrices
-        g = self.model(n_components=self.n_components,
-                       covariance_type=self.covariance_type, random_state=rng)
-        # Make sure the means are far apart so responsibilities.argmax()
-        # picks the actual component used to generate the observations.
-        g.means_ = 20 * self.means
-        g.covars_ = self.covars[self.covariance_type]
-        g.weights_ = self.weights
-
-        gaussidx = np.repeat(np.arange(self.n_components), 5)
-        n_samples = len(gaussidx)
-        X = rng.randn(n_samples, self.n_features) + g.means_[gaussidx]
-
-        with ignore_warnings(category=DeprecationWarning):
-            ll, responsibilities = g.score_samples(X)
-
-        self.assertEqual(len(ll), n_samples)
-        self.assertEqual(responsibilities.shape,
-                         (n_samples, self.n_components))
-        assert_array_almost_equal(responsibilities.sum(axis=1),
-                                  np.ones(n_samples))
-        assert_array_equal(responsibilities.argmax(axis=1), gaussidx)
-
-    # This function tests the deprecated old GMM class
-    @ignore_warnings(category=DeprecationWarning)
-    def test_sample(self, n=100):
-        g = self.model(n_components=self.n_components,
-                       covariance_type=self.covariance_type,
-                       random_state=rng)
-        # Make sure the means are far apart so responsibilities.argmax()
-        # picks the actual component used to generate the observations.
-        g.means_ = 20 * self.means
-        g.covars_ = np.maximum(self.covars[self.covariance_type], 0.1)
-        g.weights_ = self.weights
-
-        with ignore_warnings(category=DeprecationWarning):
-            samples = g.sample(n)
-        self.assertEqual(samples.shape, (n, self.n_features))
-
-    # This function tests the deprecated old GMM class
-    @ignore_warnings(category=DeprecationWarning)
-    def test_train(self, params='wmc'):
-        g = mixture.GMM(n_components=self.n_components,
-                        covariance_type=self.covariance_type)
-        with ignore_warnings(category=DeprecationWarning):
-            g.weights_ = self.weights
-            g.means_ = self.means
-            g.covars_ = 20 * self.covars[self.covariance_type]
-
-        # Create a training set by sampling from the predefined distribution.
-        with ignore_warnings(category=DeprecationWarning):
-            X = g.sample(n_samples=100)
-            g = self.model(n_components=self.n_components,
-                           covariance_type=self.covariance_type,
-                           random_state=rng, min_covar=1e-1,
-                           n_iter=1, init_params=params)
-            g.fit(X)
-
-        # Do one training iteration at a time so we can keep track of
-        # the log likelihood to make sure that it increases after each
-        # iteration.
-        trainll = []
-        with ignore_warnings(category=DeprecationWarning):
-            for _ in range(5):
-                g.params = params
-                g.init_params = ''
-                g.fit(X)
-                trainll.append(self.score(g, X))
-            g.n_iter = 10
-            g.init_params = ''
-            g.params = params
-            g.fit(X)  # finish fitting
-
-        # Note that the log likelihood will sometimes decrease by a
-        # very small amount after it has more or less converged due to
-        # the addition of min_covar to the covariance (to prevent
-        # underflow).  This is why the threshold is set to -0.5
-        # instead of 0.
-        with ignore_warnings(category=DeprecationWarning):
-            delta_min = np.diff(trainll).min()
-        self.assertTrue(
-            delta_min > self.threshold,
-            "The min nll increase is %f which is lower than the admissible"
-            " threshold of %f, for model %s. The likelihoods are %s."
-            % (delta_min, self.threshold, self.covariance_type, trainll))
-
-    # This function tests the deprecated old GMM class
-    @ignore_warnings(category=DeprecationWarning)
-    def test_train_degenerate(self, params='wmc'):
-        # Train on degenerate data with 0 in some dimensions
-        # Create a training set by sampling from the predefined
-        # distribution.
-        X = rng.randn(100, self.n_features)
-        X.T[1:] = 0
-        g = self.model(n_components=2,
-                       covariance_type=self.covariance_type,
-                       random_state=rng, min_covar=1e-3, n_iter=5,
-                       init_params=params)
-        with ignore_warnings(category=DeprecationWarning):
-            g.fit(X)
-            trainll = g.score(X)
-        self.assertTrue(np.sum(np.abs(trainll / 100 / X.shape[1])) < 5)
-
-    # This function tests the deprecated old GMM class
-    @ignore_warnings(category=DeprecationWarning)
-    def test_train_1d(self, params='wmc'):
-        # Train on 1-D data
-        # Create a training set by sampling from the predefined
-        # distribution.
-        X = rng.randn(100, 1)
-        # X.T[1:] = 0
-        g = self.model(n_components=2,
-                       covariance_type=self.covariance_type,
-                       random_state=rng, min_covar=1e-7, n_iter=5,
-                       init_params=params)
-        with ignore_warnings(category=DeprecationWarning):
-            g.fit(X)
-            trainll = g.score(X)
-            if isinstance(g, mixture.dpgmm._DPGMMBase):
-                self.assertTrue(np.sum(np.abs(trainll / 100)) < 5)
-            else:
-                self.assertTrue(np.sum(np.abs(trainll / 100)) < 2)
-
-    # This function tests the deprecated old GMM class
-    @ignore_warnings(category=DeprecationWarning)
-    def score(self, g, X):
-        with ignore_warnings(category=DeprecationWarning):
-            return g.score(X).sum()
-
-
-class TestGMMWithSphericalCovars(unittest.TestCase, GMMTester):
-    covariance_type = 'spherical'
-    model = mixture.GMM
-    setUp = GMMTester._setUp
-
-
-class TestGMMWithDiagonalCovars(unittest.TestCase, GMMTester):
-    covariance_type = 'diag'
-    model = mixture.GMM
-    setUp = GMMTester._setUp
-
-
-class TestGMMWithTiedCovars(unittest.TestCase, GMMTester):
-    covariance_type = 'tied'
-    model = mixture.GMM
-    setUp = GMMTester._setUp
-
-
-class TestGMMWithFullCovars(unittest.TestCase, GMMTester):
-    covariance_type = 'full'
-    model = mixture.GMM
-    setUp = GMMTester._setUp
-
-
-# This function tests the deprecated old GMM class
-@ignore_warnings(category=DeprecationWarning)
-def test_multiple_init():
-    # Test that multiple inits does not much worse than a single one
-    X = rng.randn(30, 5)
-    X[:10] += 2
-    g = mixture.GMM(n_components=2, covariance_type='spherical',
-                    random_state=rng, min_covar=1e-7, n_iter=5)
-    with ignore_warnings(category=DeprecationWarning):
-        train1 = g.fit(X).score(X).sum()
-        g.n_init = 5
-        train2 = g.fit(X).score(X).sum()
-    assert_true(train2 >= train1 - 1.e-2)
-
-
-# This function tests the deprecated old GMM class
-@ignore_warnings(category=DeprecationWarning)
-def test_n_parameters():
-    n_samples, n_dim, n_components = 7, 5, 2
-    X = rng.randn(n_samples, n_dim)
-    n_params = {'spherical': 13, 'diag': 21, 'tied': 26, 'full': 41}
-    for cv_type in ['full', 'tied', 'diag', 'spherical']:
-        with ignore_warnings(category=DeprecationWarning):
-            g = mixture.GMM(n_components=n_components, covariance_type=cv_type,
-                            random_state=rng, min_covar=1e-7, n_iter=1)
-            g.fit(X)
-            assert_true(g._n_parameters() == n_params[cv_type])
-
-
-# This function tests the deprecated old GMM class
-@ignore_warnings(category=DeprecationWarning)
-def test_1d_1component():
-    # Test all of the covariance_types return the same BIC score for
-    # 1-dimensional, 1 component fits.
-    n_samples, n_dim, n_components = 100, 1, 1
-    X = rng.randn(n_samples, n_dim)
-    g_full = mixture.GMM(n_components=n_components, covariance_type='full',
-                         random_state=rng, min_covar=1e-7, n_iter=1)
-    with ignore_warnings(category=DeprecationWarning):
-        g_full.fit(X)
-        g_full_bic = g_full.bic(X)
-        for cv_type in ['tied', 'diag', 'spherical']:
-            g = mixture.GMM(n_components=n_components, covariance_type=cv_type,
-                            random_state=rng, min_covar=1e-7, n_iter=1)
-            g.fit(X)
-            assert_array_almost_equal(g.bic(X), g_full_bic)
-
-
-def assert_fit_predict_correct(model, X):
-    model2 = copy.deepcopy(model)
-
-    predictions_1 = model.fit(X).predict(X)
-    predictions_2 = model2.fit_predict(X)
-
-    assert adjusted_rand_score(predictions_1, predictions_2) == 1.0
-
-
-# This function tests the deprecated old GMM class
-@ignore_warnings(category=DeprecationWarning)
-def test_fit_predict():
-    """
-    test that gmm.fit_predict is equivalent to gmm.fit + gmm.predict
-    """
-    lrng = np.random.RandomState(101)
-
-    n_samples, n_dim, n_comps = 100, 2, 2
-    mu = np.array([[8, 8]])
-    component_0 = lrng.randn(n_samples, n_dim)
-    component_1 = lrng.randn(n_samples, n_dim) + mu
-    X = np.vstack((component_0, component_1))
-
-    for m_constructor in (mixture.GMM, mixture.VBGMM, mixture.DPGMM):
-        model = m_constructor(n_components=n_comps, covariance_type='full',
-                              min_covar=1e-7, n_iter=5,
-                              random_state=np.random.RandomState(0))
-        assert_fit_predict_correct(model, X)
-
-    model = mixture.GMM(n_components=n_comps, n_iter=0)
-    z = model.fit_predict(X)
-    assert np.all(z == 0), "Quick Initialization Failed!"
-
-
-# This function tests the deprecated old GMM class
-@ignore_warnings(category=DeprecationWarning)
-def test_aic():
-    # Test the aic and bic criteria
-    n_samples, n_dim, n_components = 50, 3, 2
-    X = rng.randn(n_samples, n_dim)
-    SGH = 0.5 * (X.var() + np.log(2 * np.pi))  # standard gaussian entropy
-
-    for cv_type in ['full', 'tied', 'diag', 'spherical']:
-        g = mixture.GMM(n_components=n_components, covariance_type=cv_type,
-                        random_state=rng, min_covar=1e-7)
-        g.fit(X)
-        aic = 2 * n_samples * SGH * n_dim + 2 * g._n_parameters()
-        bic = (2 * n_samples * SGH * n_dim +
-               np.log(n_samples) * g._n_parameters())
-        bound = n_dim * 3. / np.sqrt(n_samples)
-        assert_true(np.abs(g.aic(X) - aic) / n_samples < bound)
-        assert_true(np.abs(g.bic(X) - bic) / n_samples < bound)
-
-
-# This function tests the deprecated old GMM class
-@ignore_warnings(category=DeprecationWarning)
-def check_positive_definite_covars(covariance_type):
-    r"""Test that covariance matrices do not become non positive definite
-
-    Due to the accumulation of round-off errors, the computation of the
-    covariance  matrices during the learning phase could lead to non-positive
-    definite covariance matrices. Namely the use of the formula:
-
-    .. math:: C = (\sum_i w_i  x_i x_i^T) - \mu \mu^T
-
-    instead of:
-
-    .. math:: C = \sum_i w_i (x_i - \mu)(x_i - \mu)^T
-
-    while mathematically equivalent, was observed a ``LinAlgError`` exception,
-    when computing a ``GMM`` with full covariance matrices and fixed mean.
-
-    This function ensures that some later optimization will not introduce the
-    problem again.
-    """
-    rng = np.random.RandomState(1)
-    # we build a dataset with 2 2d component. The components are unbalanced
-    # (respective weights 0.9 and 0.1)
-    X = rng.randn(100, 2)
-    X[-10:] += (3, 3)  # Shift the 10 last points
-
-    gmm = mixture.GMM(2, params="wc", covariance_type=covariance_type,
-                      min_covar=1e-3)
-
-    # This is a non-regression test for issue #2640. The following call used
-    # to trigger:
-    # numpy.linalg.linalg.LinAlgError: 2-th leading minor not positive definite
-    gmm.fit(X)
-
-    if covariance_type == "diag" or covariance_type == "spherical":
-        assert_greater(gmm.covars_.min(), 0)
-    else:
-        if covariance_type == "tied":
-            covs = [gmm.covars_]
-        else:
-            covs = gmm.covars_
-
-        for c in covs:
-            assert_greater(np.linalg.det(c), 0)
-
-
-def test_positive_definite_covars():
-    # Check positive definiteness for all covariance types
-    for covariance_type in ["full", "tied", "diag", "spherical"]:
-        yield check_positive_definite_covars, covariance_type
-
-
-# This function tests the deprecated old GMM class
-@ignore_warnings(category=DeprecationWarning)
-def test_verbose_first_level():
-    # Create sample data
-    X = rng.randn(30, 5)
-    X[:10] += 2
-    g = mixture.GMM(n_components=2, n_init=2, verbose=1)
-
-    old_stdout = sys.stdout
-    sys.stdout = StringIO()
-    try:
-        g.fit(X)
-    finally:
-        sys.stdout = old_stdout
-
-
-# This function tests the deprecated old GMM class
-@ignore_warnings(category=DeprecationWarning)
-def test_verbose_second_level():
-    # Create sample data
-    X = rng.randn(30, 5)
-    X[:10] += 2
-    g = mixture.GMM(n_components=2, n_init=2, verbose=2)
-
-    old_stdout = sys.stdout
-    sys.stdout = StringIO()
-    try:
-        g.fit(X)
-    finally:
-        sys.stdout = old_stdout
diff --git a/sklearn/model_selection/_search.py b/sklearn/model_selection/_search.py
index 99d6096af73d..a45a4bf5b4e6 100644
--- a/sklearn/model_selection/_search.py
+++ b/sklearn/model_selection/_search.py
@@ -17,6 +17,7 @@
 from functools import partial, reduce
 from itertools import product
 import operator
+import time
 import warnings
 
 import numpy as np
@@ -766,10 +767,13 @@ def _store(key_name, array, weights=None, splits=False, rank=False):
         if self.refit:
             self.best_estimator_ = clone(base_estimator).set_params(
                 **self.best_params_)
+            refit_start_time = time.time()
             if y is not None:
                 self.best_estimator_.fit(X, y, **fit_params)
             else:
                 self.best_estimator_.fit(X, **fit_params)
+            refit_end_time = time.time()
+            self.refit_time_ = refit_end_time - refit_start_time
 
         # Store the only scorer not as a dict for single metric evaluation
         self.scorer_ = scorers if self.multimetric_ else scorers['score']
@@ -779,32 +783,6 @@ def _store(key_name, array, weights=None, splits=False, rank=False):
 
         return self
 
-    @property
-    def grid_scores_(self):
-        check_is_fitted(self, 'cv_results_')
-        if self.multimetric_:
-            raise AttributeError("grid_scores_ attribute is not available for"
-                                 " multi-metric evaluation.")
-        warnings.warn(
-            "The grid_scores_ attribute was deprecated in version 0.18"
-            " in favor of the more elaborate cv_results_ attribute."
-            " The grid_scores_ attribute will not be available from 0.20",
-            DeprecationWarning)
-
-        grid_scores = list()
-
-        for i, (params, mean, std) in enumerate(zip(
-                self.cv_results_['params'],
-                self.cv_results_['mean_test_score'],
-                self.cv_results_['std_test_score'])):
-            scores = np.array(list(self.cv_results_['split%d_test_score'
-                                                    % s][i]
-                                   for s in range(self.n_splits_)),
-                              dtype=np.float64)
-            grid_scores.append(_CVScoreTuple(params, mean, scores))
-
-        return grid_scores
-
 
 class GridSearchCV(BaseSearchCV):
     """Exhaustive search over specified parameter values for an estimator.
@@ -887,17 +865,18 @@ class GridSearchCV(BaseSearchCV):
         will change to False in version 0.21, to correspond to the standard
         definition of cross-validation.
 
-        ..versionchanged:: 0.20
+        .. versionchanged:: 0.20
             Parameter ``iid`` will change from True to False by default in
             version 0.22, and will be removed in 0.24.
 
     cv : int, cross-validation generator or an iterable, optional
         Determines the cross-validation splitting strategy.
         Possible inputs for cv are:
-          - None, to use the default 3-fold cross validation,
-          - integer, to specify the number of folds in a `(Stratified)KFold`,
-          - An object to be used as a cross-validation generator.
-          - An iterable yielding train, test splits.
+
+        - None, to use the default 3-fold cross validation,
+        - integer, to specify the number of folds in a `(Stratified)KFold`,
+        - An object to be used as a cross-validation generator.
+        - An iterable yielding train, test splits.
 
         For integer/None inputs, if the estimator is a classifier and ``y`` is
         either binary or multiclass, :class:`StratifiedKFold` is used. In all
@@ -1076,6 +1055,11 @@ class GridSearchCV(BaseSearchCV):
     n_splits_ : int
         The number of cross-validation splits (folds/iterations).
 
+    refit_time_ : float
+        Seconds used for refitting the best model on the whole dataset.
+
+        This is present only if ``refit`` is not False.
+
     Notes
     ------
     The parameters selected are those that maximize the score of the left out
@@ -1221,17 +1205,18 @@ class RandomizedSearchCV(BaseSearchCV):
         will change to False in version 0.21, to correspond to the standard
         definition of cross-validation.
 
-        ..versionchanged:: 0.20
+        .. versionchanged:: 0.20
             Parameter ``iid`` will change from True to False by default in
             version 0.22, and will be removed in 0.24.
 
     cv : int, cross-validation generator or an iterable, optional
         Determines the cross-validation splitting strategy.
         Possible inputs for cv are:
-          - None, to use the default 3-fold cross validation,
-          - integer, to specify the number of folds in a `(Stratified)KFold`,
-          - An object to be used as a cross-validation generator.
-          - An iterable yielding train, test splits.
+
+        - None, to use the default 3-fold cross validation,
+        - integer, to specify the number of folds in a `(Stratified)KFold`,
+        - An object to be used as a cross-validation generator.
+        - An iterable yielding train, test splits.
 
         For integer/None inputs, if the estimator is a classifier and ``y`` is
         either binary or multiclass, :class:`StratifiedKFold` is used. In all
@@ -1387,6 +1372,11 @@ class RandomizedSearchCV(BaseSearchCV):
     n_splits_ : int
         The number of cross-validation splits (folds/iterations).
 
+    refit_time_ : float
+        Seconds used for refitting the best model on the whole dataset.
+
+        This is present only if ``refit`` is not False.
+
     Notes
     -----
     The parameters selected are those that maximize the score of the held-out
diff --git a/sklearn/model_selection/_split.py b/sklearn/model_selection/_split.py
index bca41c0eeb8c..dc1b78657d23 100644
--- a/sklearn/model_selection/_split.py
+++ b/sklearn/model_selection/_split.py
@@ -76,8 +76,8 @@ def split(self, X, y=None, groups=None):
             Group labels for the samples used while splitting the dataset into
             train/test set.
 
-        Returns
-        -------
+        Yields
+        ------
         train : ndarray
             The training set indices for that split.
 
@@ -301,8 +301,8 @@ def split(self, X, y=None, groups=None):
             Group labels for the samples used while splitting the dataset into
             train/test set.
 
-        Returns
-        -------
+        Yields
+        ------
         train : ndarray
             The training set indices for that split.
 
@@ -651,8 +651,8 @@ def split(self, X, y, groups=None):
         groups : object
             Always ignored, exists for compatibility.
 
-        Returns
-        -------
+        Yields
+        ------
         train : ndarray
             The training set indices for that split.
 
@@ -738,8 +738,8 @@ def split(self, X, y=None, groups=None):
         groups : array-like, with shape (n_samples,), optional
             Always ignored, exists for compatibility.
 
-        Returns
-        -------
+        Yields
+        ------
         train : ndarray
             The training set indices for that split.
 
@@ -1010,8 +1010,8 @@ def split(self, X, y=None, groups=None):
             Group labels for the samples used while splitting the dataset into
             train/test set.
 
-        Returns
-        -------
+        Yields
+        ------
         train : ndarray
             The training set indices for that split.
 
@@ -1186,8 +1186,8 @@ def split(self, X, y=None, groups=None):
             Group labels for the samples used while splitting the dataset into
             train/test set.
 
-        Returns
-        -------
+        Yields
+        ------
         train : ndarray
             The training set indices for that split.
 
@@ -1607,8 +1607,8 @@ def split(self, X, y, groups=None):
         groups : object
             Always ignored, exists for compatibility.
 
-        Returns
-        -------
+        Yields
+        ------
         train : ndarray
             The training set indices for that split.
 
@@ -1767,8 +1767,8 @@ def split(self, X=None, y=None, groups=None):
         groups : object
             Always ignored, exists for compatibility.
 
-        Returns
-        -------
+        Yields
+        ------
         train : ndarray
             The training set indices for that split.
 
@@ -1851,8 +1851,8 @@ def split(self, X=None, y=None, groups=None):
         groups : object
             Always ignored, exists for compatibility.
 
-        Returns
-        -------
+        Yields
+        ------
         train : ndarray
             The training set indices for that split.
 
diff --git a/sklearn/model_selection/_validation.py b/sklearn/model_selection/_validation.py
index 50af9b5dd550..9241d05fd432 100644
--- a/sklearn/model_selection/_validation.py
+++ b/sklearn/model_selection/_validation.py
@@ -79,10 +79,11 @@ def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv=None,
     cv : int, cross-validation generator or an iterable, optional
         Determines the cross-validation splitting strategy.
         Possible inputs for cv are:
-          - None, to use the default 3-fold cross validation,
-          - integer, to specify the number of folds in a `(Stratified)KFold`,
-          - An object to be used as a cross-validation generator.
-          - An iterable yielding train, test splits.
+
+        - None, to use the default 3-fold cross validation,
+        - integer, to specify the number of folds in a `(Stratified)KFold`,
+        - An object to be used as a cross-validation generator.
+        - An iterable yielding train, test splits.
 
         For integer/None inputs, if the estimator is a classifier and ``y`` is
         either binary or multiclass, :class:`StratifiedKFold` is used. In all
diff --git a/sklearn/model_selection/tests/test_search.py b/sklearn/model_selection/tests/test_search.py
index f54f9b98d18f..81a8af4f0b2a 100644
--- a/sklearn/model_selection/tests/test_search.py
+++ b/sklearn/model_selection/tests/test_search.py
@@ -26,6 +26,7 @@
 from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_almost_equal
+from sklearn.utils.testing import assert_greater_equal
 from sklearn.utils.testing import ignore_warnings
 from sklearn.utils.mocking import CheckingClassifier, MockDataFrame
 
@@ -419,7 +420,6 @@ def test_classes__property():
 
 def test_trivial_cv_results_attr():
     # Test search over a "grid" with only one point.
-    # Non-regression test: grid_scores_ wouldn't be set by GridSearchCV.
     clf = MockClassifier()
     grid_search = GridSearchCV(clf, {'foo_param': [1]})
     grid_search.fit(X, y)
@@ -802,30 +802,6 @@ def check_cv_results_keys(cv_results, param_keys, score_keys, n_cand):
                     for key in param_keys + score_keys))
 
 
-def check_cv_results_grid_scores_consistency(search):
-    # TODO Remove test in 0.20
-    if search.multimetric_:
-        assert_raise_message(AttributeError, "not available for multi-metric",
-                             getattr, search, 'grid_scores_')
-    else:
-        cv_results = search.cv_results_
-        res_scores = np.vstack(list([cv_results["split%d_test_score" % i]
-                                     for i in range(search.n_splits_)])).T
-        res_means = cv_results["mean_test_score"]
-        res_params = cv_results["params"]
-        n_cand = len(res_params)
-        grid_scores = assert_warns(DeprecationWarning, getattr,
-                                   search, 'grid_scores_')
-        assert_equal(len(grid_scores), n_cand)
-        # Check consistency of the structure of grid_scores
-        for i in range(n_cand):
-            assert_equal(grid_scores[i].parameters, res_params[i])
-            assert_array_equal(grid_scores[i].cv_validation_scores,
-                               res_scores[i, :])
-            assert_array_equal(grid_scores[i].mean_validation_score,
-                               res_means[i])
-
-
 def test_grid_search_cv_results():
     X, y = make_classification(n_samples=50, n_features=4,
                                random_state=42)
@@ -876,7 +852,6 @@ def test_grid_search_cv_results():
                          cv_results['param_degree'].mask[i])
                         for i in range(n_candidates)
                         if cv_results['param_kernel'][i] == 'rbf'))
-        check_cv_results_grid_scores_consistency(search)
 
 
 def test_random_search_cv_results():
@@ -911,7 +886,6 @@ def test_random_search_cv_results():
         # For random_search, all the param array vals should be unmasked
         assert_false(any(cv_results['param_C'].mask) or
                      any(cv_results['param_gamma'].mask))
-        check_cv_results_grid_scores_consistency(search)
 
 
 @ignore_warnings(category=DeprecationWarning)
@@ -1176,6 +1150,10 @@ def test_search_cv_timing():
             assert_true(search.cv_results_[key][0] == 0.0)
             assert_true(np.all(search.cv_results_[key] < 1))
 
+        assert_true(hasattr(search, "refit_time_"))
+        assert_true(isinstance(search.refit_time_, float))
+        assert_greater_equal(search.refit_time_, 0)
+
 
 def test_grid_search_correct_score_results():
     # test that correct scores are used
@@ -1317,7 +1295,7 @@ def test_grid_search_allows_nans():
     X[2, :] = np.nan
     y = [0, 0, 1, 1, 1]
     p = Pipeline([
-        ('imputer', SimpleImputer(strategy='mean', missing_values='NaN')),
+        ('imputer', SimpleImputer(strategy='mean', missing_values=np.nan)),
         ('classifier', MockClassifier()),
     ])
     GridSearchCV(p, {'classifier__foo_param': [1, 2, 3]}, cv=2).fit(X, y)
diff --git a/sklearn/model_selection/tests/test_split.py b/sklearn/model_selection/tests/test_split.py
index 3f54aaf3c66f..0071129d8ce7 100644
--- a/sklearn/model_selection/tests/test_split.py
+++ b/sklearn/model_selection/tests/test_split.py
@@ -1210,36 +1210,10 @@ def test_check_cv():
     cv = check_cv(3, y_multioutput, classifier=True)
     np.testing.assert_equal(list(KFold(3).split(X)), list(cv.split(X)))
 
-    # Check if the old style classes are wrapped to have a split method
-    X = np.ones(9)
-    y_multiclass = np.array([0, 1, 0, 1, 2, 1, 2, 0, 2])
-    cv1 = check_cv(3, y_multiclass, classifier=True)
-
-    with warnings.catch_warnings(record=True):
-        from sklearn.cross_validation import StratifiedKFold as OldSKF
-
-    cv2 = check_cv(OldSKF(y_multiclass, n_folds=3))
-    np.testing.assert_equal(list(cv1.split(X, y_multiclass)),
-                            list(cv2.split()))
-
     assert_raises(ValueError, check_cv, cv="lolo")
 
 
 def test_cv_iterable_wrapper():
-    y_multiclass = np.array([0, 1, 0, 1, 2, 1, 2, 0, 2])
-
-    with warnings.catch_warnings(record=True):
-        from sklearn.cross_validation import StratifiedKFold as OldSKF
-
-    cv = OldSKF(y_multiclass, n_folds=3)
-    wrapped_old_skf = _CVIterableWrapper(cv)
-
-    # Check if split works correctly
-    np.testing.assert_equal(list(cv), list(wrapped_old_skf.split()))
-
-    # Check if get_n_splits works correctly
-    assert_equal(len(cv), wrapped_old_skf.get_n_splits())
-
     kf_iter = KFold(n_splits=5).split(X, y)
     kf_iter_wrapped = check_cv(kf_iter)
     # Since the wrapped iterable is enlisted and stored,
diff --git a/sklearn/model_selection/tests/test_validation.py b/sklearn/model_selection/tests/test_validation.py
index 6c7718379886..432ee8d91449 100644
--- a/sklearn/model_selection/tests/test_validation.py
+++ b/sklearn/model_selection/tests/test_validation.py
@@ -12,7 +12,7 @@
 from scipy.sparse import coo_matrix, csr_matrix
 from sklearn.exceptions import FitFailedWarning
 
-from sklearn.tests.test_grid_search import FailingClassifier
+from sklearn.model_selection.tests.test_search import FailingClassifier
 
 from sklearn.utils.testing import assert_true
 from sklearn.utils.testing import assert_false
@@ -387,8 +387,8 @@ def test_cross_validate():
         scores = (train_mse_scores, test_mse_scores, train_r2_scores,
                   test_r2_scores, fitted_estimators)
 
-        yield check_cross_validate_single_metric, est, X, y, scores
-        yield check_cross_validate_multi_metric, est, X, y, scores
+        check_cross_validate_single_metric(est, X, y, scores)
+        check_cross_validate_multi_metric(est, X, y, scores)
 
 
 def test_cross_validate_return_train_score_warn():
@@ -746,7 +746,7 @@ def test_permutation_test_score_allow_nans():
     X[2, :] = np.nan
     y = np.repeat([0, 1], X.shape[0] / 2)
     p = Pipeline([
-        ('imputer', SimpleImputer(strategy='mean', missing_values='NaN')),
+        ('imputer', SimpleImputer(strategy='mean', missing_values=np.nan)),
         ('classifier', MockClassifier()),
     ])
     permutation_test_score(p, X, y, cv=5)
@@ -758,7 +758,7 @@ def test_cross_val_score_allow_nans():
     X[2, :] = np.nan
     y = np.repeat([0, 1], X.shape[0] / 2)
     p = Pipeline([
-        ('imputer', SimpleImputer(strategy='mean', missing_values='NaN')),
+        ('imputer', SimpleImputer(strategy='mean', missing_values=np.nan)),
         ('classifier', MockClassifier()),
     ])
     cross_val_score(p, X, y, cv=5)
diff --git a/sklearn/multioutput.py b/sklearn/multioutput.py
index 14707f8d460e..4d9b9e10f4fb 100644
--- a/sklearn/multioutput.py
+++ b/sklearn/multioutput.py
@@ -508,9 +508,10 @@ class ClassifierChain(_BaseChain, ClassifierMixin, MetaEstimatorMixin):
         labels for the results of previous estimators in the chain.
         If cv is None the true labels are used when fitting. Otherwise
         possible inputs for cv are:
-            * integer, to specify the number of folds in a (Stratified)KFold,
-            * An object to be used as a cross-validation generator.
-            * An iterable yielding train, test splits.
+
+        * integer, to specify the number of folds in a (Stratified)KFold,
+        * An object to be used as a cross-validation generator.
+        * An iterable yielding train, test splits.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
@@ -547,6 +548,7 @@ class labels for each estimator in the chain.
 
     def fit(self, X, Y):
         """Fit the model to data matrix X and targets Y.
+
         Parameters
         ----------
         X : {array-like, sparse matrix}, shape (n_samples, n_features)
@@ -662,9 +664,10 @@ class RegressorChain(_BaseChain, RegressorMixin, MetaEstimatorMixin):
         labels for the results of previous estimators in the chain.
         If cv is None the true labels are used when fitting. Otherwise
         possible inputs for cv are:
-            * integer, to specify the number of folds in a (Stratified)KFold,
-            * An object to be used as a cross-validation generator.
-            * An iterable yielding train, test splits.
+
+        * integer, to specify the number of folds in a (Stratified)KFold,
+        * An object to be used as a cross-validation generator.
+        * An iterable yielding train, test splits.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
diff --git a/sklearn/neighbors/base.py b/sklearn/neighbors/base.py
index d983c124679f..14810e65b016 100644
--- a/sklearn/neighbors/base.py
+++ b/sklearn/neighbors/base.py
@@ -6,6 +6,8 @@
 #          Multi-output support by Arnaud Joly <a.joly@ulg.ac.be>
 #
 # License: BSD 3 clause (C) INRIA, University of Amsterdam
+from functools import partial
+
 import warnings
 from abc import ABCMeta, abstractmethod
 
@@ -15,7 +17,7 @@
 from .ball_tree import BallTree
 from .kd_tree import KDTree
 from ..base import BaseEstimator
-from ..metrics import pairwise_distances
+from ..metrics import pairwise_distances_chunked
 from ..metrics.pairwise import PAIRWISE_DISTANCE_FUNCTIONS
 from ..utils import check_X_y, check_array, _get_n_jobs, gen_even_slices
 from ..utils.multiclass import check_classification_targets
@@ -276,9 +278,43 @@ def _pairwise(self):
 class KNeighborsMixin(object):
     """Mixin for k-neighbors searches"""
 
+    def _kneighbors_reduce_func(self, dist, start,
+                                n_neighbors, return_distance):
+        """Reduce a chunk of distances to the nearest neighbors
+
+        Callback to :func:`sklearn.metrics.pairwise.pairwise_distances_chunked`
+
+        Parameters
+        ----------
+        dist : array of shape (n_samples_chunk, n_samples)
+        start : int
+            The index in X which the first row of dist corresponds to.
+        n_neighbors : int
+        return_distance : bool
+
+        Returns
+        -------
+        dist : array of shape (n_samples_chunk, n_neighbors), optional
+            Returned only if return_distance
+        neigh : array of shape (n_samples_chunk, n_neighbors)
+        """
+        sample_range = np.arange(dist.shape[0])[:, None]
+        neigh_ind = np.argpartition(dist, n_neighbors - 1, axis=1)
+        neigh_ind = neigh_ind[:, :n_neighbors]
+        # argpartition doesn't guarantee sorted order, so we sort again
+        neigh_ind = neigh_ind[
+            sample_range, np.argsort(dist[sample_range, neigh_ind])]
+        if return_distance:
+            if self.effective_metric_ == 'euclidean':
+                result = np.sqrt(dist[sample_range, neigh_ind]), neigh_ind
+            else:
+                result = dist[sample_range, neigh_ind], neigh_ind
+        else:
+            result = neigh_ind
+        return result
+
     def kneighbors(self, X=None, n_neighbors=None, return_distance=True):
         """Finds the K-neighbors of a point.
-
         Returns indices of and distances to the neighbors of each point.
 
         Parameters
@@ -367,28 +403,19 @@ class from an array representing our data set and ask who's
 
         n_jobs = _get_n_jobs(self.n_jobs)
         if self._fit_method == 'brute':
-            # for efficiency, use squared euclidean distances
-            if self.effective_metric_ == 'euclidean':
-                dist = pairwise_distances(X, self._fit_X, 'euclidean',
-                                          n_jobs=n_jobs, squared=True)
-            else:
-                dist = pairwise_distances(
-                    X, self._fit_X, self.effective_metric_, n_jobs=n_jobs,
-                    **self.effective_metric_params_)
 
-            neigh_ind = np.argpartition(dist, n_neighbors - 1, axis=1)
-            neigh_ind = neigh_ind[:, :n_neighbors]
-            # argpartition doesn't guarantee sorted order, so we sort again
-            neigh_ind = neigh_ind[
-                sample_range, np.argsort(dist[sample_range, neigh_ind])]
+            reduce_func = partial(self._kneighbors_reduce_func,
+                                  n_neighbors=n_neighbors,
+                                  return_distance=return_distance)
 
-            if return_distance:
-                if self.effective_metric_ == 'euclidean':
-                    result = np.sqrt(dist[sample_range, neigh_ind]), neigh_ind
-                else:
-                    result = dist[sample_range, neigh_ind], neigh_ind
-            else:
-                result = neigh_ind
+            # for efficiency, use squared euclidean distances
+            kwds = ({'squared': True} if self.effective_metric_ == 'euclidean'
+                    else self.effective_metric_params_)
+
+            result = pairwise_distances_chunked(
+                X, self._fit_X, reduce_func=reduce_func,
+                metric=self.effective_metric_, n_jobs=n_jobs,
+                **kwds)
 
         elif self._fit_method in ['ball_tree', 'kd_tree']:
             if issparse(X):
@@ -400,14 +427,15 @@ class from an array representing our data set and ask who's
                     X[s], n_neighbors, return_distance)
                 for s in gen_even_slices(X.shape[0], n_jobs)
             )
-            if return_distance:
-                dist, neigh_ind = tuple(zip(*result))
-                result = np.vstack(dist), np.vstack(neigh_ind)
-            else:
-                result = np.vstack(result)
         else:
             raise ValueError("internal: _fit_method not recognized")
 
+        if return_distance:
+            dist, neigh_ind = zip(*result)
+            result = np.vstack(dist), np.vstack(neigh_ind)
+        else:
+            result = np.vstack(result)
+
         if not query_is_train:
             return result
         else:
@@ -519,6 +547,40 @@ def kneighbors_graph(self, X=None, n_neighbors=None,
 class RadiusNeighborsMixin(object):
     """Mixin for radius-based neighbors searches"""
 
+    def _radius_neighbors_reduce_func(self, dist, start,
+                                      radius, return_distance):
+        """Reduce a chunk of distances to the nearest neighbors
+
+        Callback to :func:`sklearn.metrics.pairwise.pairwise_distances_chunked`
+
+        Parameters
+        ----------
+        dist : array of shape (n_samples_chunk, n_samples)
+        start : int
+            The index in X which the first row of dist corresponds to.
+        radius : float
+        return_distance : bool
+
+        Returns
+        -------
+        dist : list of n_samples_chunk 1d arrays, optional
+            Returned only if return_distance
+        neigh : list of n_samples_chunk 1d arrays
+        """
+        neigh_ind = [np.where(d <= radius)[0] for d in dist]
+
+        if return_distance:
+            if self.effective_metric_ == 'euclidean':
+                dist = [np.sqrt(d[neigh_ind[i]])
+                        for i, d in enumerate(dist)]
+            else:
+                dist = [d[neigh_ind[i]]
+                        for i, d in enumerate(dist)]
+            results = dist, neigh_ind
+        else:
+            results = neigh_ind
+        return results
+
     def radius_neighbors(self, X=None, radius=None, return_distance=True):
         """Finds the neighbors within a given radius of a point or points.
 
@@ -597,39 +659,37 @@ class from an array representing our data set and ask who's
         if radius is None:
             radius = self.radius
 
-        n_samples = X.shape[0]
         if self._fit_method == 'brute':
             # for efficiency, use squared euclidean distances
             if self.effective_metric_ == 'euclidean':
-                dist = pairwise_distances(X, self._fit_X, 'euclidean',
-                                          n_jobs=self.n_jobs, squared=True)
                 radius *= radius
+                kwds = {'squared': True}
             else:
-                dist = pairwise_distances(X, self._fit_X,
-                                          self.effective_metric_,
-                                          n_jobs=self.n_jobs,
-                                          **self.effective_metric_params_)
-
-            neigh_ind_list = [np.where(d <= radius)[0] for d in dist]
+                kwds = self.effective_metric_params_
 
-            # See https://github.com/numpy/numpy/issues/5456
-            # if you want to understand why this is initialized this way.
-            neigh_ind = np.empty(n_samples, dtype='object')
-            neigh_ind[:] = neigh_ind_list
+            reduce_func = partial(self._radius_neighbors_reduce_func,
+                                  radius=radius,
+                                  return_distance=return_distance)
 
+            results = pairwise_distances_chunked(
+                X, self._fit_X, reduce_func=reduce_func,
+                metric=self.effective_metric_, n_jobs=self.n_jobs,
+                **kwds)
             if return_distance:
-                dist_array = np.empty(n_samples, dtype='object')
-                if self.effective_metric_ == 'euclidean':
-                    dist_list = [np.sqrt(d[neigh_ind[i]])
-                                 for i, d in enumerate(dist)]
-                else:
-                    dist_list = [d[neigh_ind[i]]
-                                 for i, d in enumerate(dist)]
-                dist_array[:] = dist_list
-
-                results = dist_array, neigh_ind
+                dist_chunks, neigh_ind_chunks = zip(*results)
+                dist_list = sum(dist_chunks, [])
+                neigh_ind_list = sum(neigh_ind_chunks, [])
+                # See https://github.com/numpy/numpy/issues/5456
+                # if you want to understand why this is initialized this way.
+                dist = np.empty(len(dist_list), dtype='object')
+                dist[:] = dist_list
+                neigh_ind = np.empty(len(neigh_ind_list), dtype='object')
+                neigh_ind[:] = neigh_ind_list
+                results = dist, neigh_ind
             else:
-                results = neigh_ind
+                neigh_ind_list = sum(results, [])
+                results = np.empty(len(neigh_ind_list), dtype='object')
+                results[:] = neigh_ind_list
 
         elif self._fit_method in ['ball_tree', 'kd_tree']:
             if issparse(X):
diff --git a/sklearn/neighbors/lof.py b/sklearn/neighbors/lof.py
index f7f1a16ebeb2..a2589f792331 100644
--- a/sklearn/neighbors/lof.py
+++ b/sklearn/neighbors/lof.py
@@ -110,7 +110,7 @@ class LocalOutlierFactor(NeighborsBase, KNeighborsMixin, UnsupervisedMixin,
     ----------
     negative_outlier_factor_ : numpy array, shape (n_samples,)
         The opposite LOF of the training samples. The higher, the more normal.
-        Inliers tend to have a LOF score close to 1 (negative_outlier_factor_
+        Inliers tend to have a LOF score close to 1 (`negative_outlier_factor_`
         close to -1), while outliers tend to have a larger LOF score.
 
         The local outlier factor (LOF) of a sample captures its
@@ -123,8 +123,8 @@ class LocalOutlierFactor(NeighborsBase, KNeighborsMixin, UnsupervisedMixin,
 
     offset_ : float
         Offset used to obtain binary labels from the raw scores.
-        Observations having a negative_outlier_factor smaller than offset_ are
-        detected as abnormal.
+        Observations having a negative_outlier_factor smaller than `offset_`
+        are detected as abnormal.
         The offset is set to -1.5 (inliers score around -1), except when a
         contamination parameter different than "auto" is provided. In that
         case, the offset is defined in such a way we obtain the expected
diff --git a/sklearn/neighbors/tests/test_ball_tree.py b/sklearn/neighbors/tests/test_ball_tree.py
index a91e4ac4edd2..de0d166fb889 100644
--- a/sklearn/neighbors/tests/test_ball_tree.py
+++ b/sklearn/neighbors/tests/test_ball_tree.py
@@ -1,12 +1,15 @@
 import pickle
+import itertools
+
 import numpy as np
+import pytest
 from numpy.testing import assert_array_almost_equal
 from sklearn.neighbors.ball_tree import (BallTree, NeighborsHeap,
                                          simultaneous_sort, kernel_norm,
                                          nodeheap_sort, DTYPE, ITYPE)
 from sklearn.neighbors.dist_metrics import DistanceMetric
 from sklearn.utils import check_random_state
-from sklearn.utils.testing import SkipTest, assert_allclose
+from sklearn.utils.testing import assert_allclose
 
 rng = np.random.RandomState(10)
 V_mahalanobis = rng.rand(3, 3)
@@ -42,60 +45,44 @@ def brute_force_neighbors(X, Y, k, metric, **kwargs):
     return dist, ind
 
 
-def test_ball_tree_query():
+@pytest.mark.parametrize('metric', METRICS)
+@pytest.mark.parametrize('k', (1, 3, 5))
+@pytest.mark.parametrize('dualtree', (True, False))
+@pytest.mark.parametrize('breadth_first', (True, False))
+def test_ball_tree_query(metric, k, dualtree, breadth_first):
     rng = check_random_state(0)
     X = rng.random_sample((40, DIMENSION))
     Y = rng.random_sample((10, DIMENSION))
 
-    def check_neighbors(dualtree, breadth_first, k, metric, kwargs):
-        bt = BallTree(X, leaf_size=1, metric=metric, **kwargs)
-        dist1, ind1 = bt.query(Y, k, dualtree=dualtree,
-                               breadth_first=breadth_first)
-        dist2, ind2 = brute_force_neighbors(X, Y, k, metric, **kwargs)
+    kwargs = METRICS[metric]
 
-        # don't check indices here: if there are any duplicate distances,
-        # the indices may not match.  Distances should not have this problem.
-        assert_array_almost_equal(dist1, dist2)
+    bt = BallTree(X, leaf_size=1, metric=metric, **kwargs)
+    dist1, ind1 = bt.query(Y, k, dualtree=dualtree,
+                           breadth_first=breadth_first)
+    dist2, ind2 = brute_force_neighbors(X, Y, k, metric, **kwargs)
 
-    for (metric, kwargs) in METRICS.items():
-        for k in (1, 3, 5):
-            for dualtree in (True, False):
-                for breadth_first in (True, False):
-                    yield (check_neighbors,
-                           dualtree, breadth_first,
-                           k, metric, kwargs)
+    # don't check indices here: if there are any duplicate distances,
+    # the indices may not match.  Distances should not have this problem.
+    assert_array_almost_equal(dist1, dist2)
 
 
-def test_ball_tree_query_boolean_metrics():
+@pytest.mark.parametrize('metric',
+                         itertools.chain(BOOLEAN_METRICS, DISCRETE_METRICS))
+def test_ball_tree_query_metrics(metric):
     rng = check_random_state(0)
-    X = rng.random_sample((40, 10)).round(0)
-    Y = rng.random_sample((10, 10)).round(0)
-    k = 5
-
-    def check_neighbors(metric):
-        bt = BallTree(X, leaf_size=1, metric=metric)
-        dist1, ind1 = bt.query(Y, k)
-        dist2, ind2 = brute_force_neighbors(X, Y, k, metric)
-        assert_array_almost_equal(dist1, dist2)
+    if metric in BOOLEAN_METRICS:
+        X = rng.random_sample((40, 10)).round(0)
+        Y = rng.random_sample((10, 10)).round(0)
+    elif metric in DISCRETE_METRICS:
+        X = (4 * rng.random_sample((40, 10))).round(0)
+        Y = (4 * rng.random_sample((10, 10))).round(0)
 
-    for metric in BOOLEAN_METRICS:
-        yield check_neighbors, metric
-
-
-def test_ball_tree_query_discrete_metrics():
-    rng = check_random_state(0)
-    X = (4 * rng.random_sample((40, 10))).round(0)
-    Y = (4 * rng.random_sample((10, 10))).round(0)
     k = 5
 
-    def check_neighbors(metric):
-        bt = BallTree(X, leaf_size=1, metric=metric)
-        dist1, ind1 = bt.query(Y, k)
-        dist2, ind2 = brute_force_neighbors(X, Y, k, metric)
-        assert_array_almost_equal(dist1, dist2)
-
-    for metric in DISCRETE_METRICS:
-        yield check_neighbors, metric
+    bt = BallTree(X, leaf_size=1, metric=metric)
+    dist1, ind1 = bt.query(Y, k)
+    dist2, ind2 = brute_force_neighbors(X, Y, k, metric)
+    assert_array_almost_equal(dist1, dist2)
 
 
 def test_ball_tree_query_radius(n_samples=100, n_features=10):
@@ -157,7 +144,21 @@ def compute_kernel_slow(Y, X, kernel, h):
         raise ValueError('kernel not recognized')
 
 
-def check_results(kernel, h, atol, rtol, breadth_first, bt, Y, dens_true):
+@pytest.mark.parametrize("kernel", ['gaussian', 'tophat', 'epanechnikov',
+                                    'exponential', 'linear', 'cosine'])
+@pytest.mark.parametrize("h", [0.01, 0.1, 1])
+@pytest.mark.parametrize("rtol", [0, 1E-5])
+@pytest.mark.parametrize("atol", [1E-6, 1E-2])
+@pytest.mark.parametrize("breadth_first", [True, False])
+def test_ball_tree_kde(kernel, h, rtol, atol, breadth_first, n_samples=100,
+                       n_features=3):
+    np.random.seed(0)
+    X = np.random.random((n_samples, n_features))
+    Y = np.random.random((n_samples, n_features))
+    bt = BallTree(X, leaf_size=10)
+
+    dens_true = compute_kernel_slow(Y, X, kernel, h)
+
     dens = bt.kernel_density(Y, h, atol=atol, rtol=rtol,
                              kernel=kernel,
                              breadth_first=breadth_first)
@@ -165,24 +166,6 @@ def check_results(kernel, h, atol, rtol, breadth_first, bt, Y, dens_true):
                     atol=atol, rtol=max(rtol, 1e-7))
 
 
-def test_ball_tree_kde(n_samples=100, n_features=3):
-    rng = check_random_state(0)
-    X = rng.random_sample((n_samples, n_features))
-    Y = rng.random_sample((n_samples, n_features))
-    bt = BallTree(X, leaf_size=10)
-
-    for kernel in ['gaussian', 'tophat', 'epanechnikov',
-                   'exponential', 'linear', 'cosine']:
-        for h in [0.01, 0.1, 1]:
-            dens_true = compute_kernel_slow(Y, X, kernel, h)
-
-            for rtol in [0, 1E-5]:
-                for atol in [1E-6, 1E-2]:
-                    for breadth_first in (True, False):
-                        yield (check_results, kernel, h, atol, rtol,
-                               breadth_first, bt, Y, dens_true)
-
-
 def test_gaussian_kde(n_samples=1000):
     # Compare gaussian KDE results to scipy.stats.gaussian_kde
     from scipy.stats import gaussian_kde
@@ -215,7 +198,7 @@ def check_two_point(r, dualtree):
         assert_array_almost_equal(counts, counts_true)
 
     for dualtree in (True, False):
-        yield check_two_point, r, dualtree
+        check_two_point(r, dualtree)
 
 
 def test_ball_tree_pickle():
@@ -246,7 +229,7 @@ def check_pickle_protocol(protocol):
         assert_array_almost_equal(dist1_pyfunc, dist2_pyfunc)
 
     for protocol in (0, 1, 2):
-        yield check_pickle_protocol, protocol
+        check_pickle_protocol(protocol)
 
 
 def test_neighbors_heap(n_pts=5, n_nbrs=10):
diff --git a/sklearn/neighbors/tests/test_dist_metrics.py b/sklearn/neighbors/tests/test_dist_metrics.py
index 23b7656cb313..f4d6dc3e74c5 100644
--- a/sklearn/neighbors/tests/test_dist_metrics.py
+++ b/sklearn/neighbors/tests/test_dist_metrics.py
@@ -4,6 +4,8 @@
 import numpy as np
 from numpy.testing import assert_array_almost_equal
 
+import pytest
+
 from scipy.spatial.distance import cdist
 from sklearn.neighbors.dist_metrics import DistanceMetric
 from sklearn.neighbors import BallTree
@@ -15,107 +17,117 @@ def dist_func(x1, x2, p):
     return np.sum((x1 - x2) ** p) ** (1. / p)
 
 
-class TestMetrics(object):
-    n1 = 20
-    n2 = 25
-    d = 4
-    zero_frac = 0.5
-    rseed = 0
-    dtype = np.float64
-    rng = check_random_state(rseed)
-    X1 = rng.random_sample((n1, d)).astype(dtype)
-    X2 = rng.random_sample((n2, d)).astype(dtype)
-
-    # make boolean arrays: ones and zeros
-    X1_bool = X1.round(0)
-    X2_bool = X2.round(0)
-
-    V = rng.random_sample((d, d))
-    VI = np.dot(V, V.T)
-
-    metrics = {'euclidean': {},
-               'cityblock': {},
-               'minkowski': dict(p=(1, 1.5, 2, 3)),
-               'chebyshev': {},
-               'seuclidean': dict(V=(rng.random_sample(d),)),
-               'wminkowski': dict(p=(1, 1.5, 3),
-                                  w=(rng.random_sample(d),)),
-               'mahalanobis': dict(VI=(VI,)),
-               'hamming': {},
-               'canberra': {},
-               'braycurtis': {}}
-
-    bool_metrics = ['matching', 'jaccard', 'dice',
-                    'kulsinski', 'rogerstanimoto', 'russellrao',
-                    'sokalmichener', 'sokalsneath']
-
-    def test_cdist(self):
-        for metric, argdict in self.metrics.items():
-            keys = argdict.keys()
-            for vals in itertools.product(*argdict.values()):
-                kwargs = dict(zip(keys, vals))
-                D_true = cdist(self.X1, self.X2, metric, **kwargs)
-                yield self.check_cdist, metric, kwargs, D_true
-
-        for metric in self.bool_metrics:
-            D_true = cdist(self.X1_bool, self.X2_bool, metric)
-            yield self.check_cdist_bool, metric, D_true
-
-    def check_cdist(self, metric, kwargs, D_true):
-        dm = DistanceMetric.get_metric(metric, **kwargs)
-        D12 = dm.pairwise(self.X1, self.X2)
-        assert_array_almost_equal(D12, D_true)
-
-    def check_cdist_bool(self, metric, D_true):
-        dm = DistanceMetric.get_metric(metric)
-        D12 = dm.pairwise(self.X1_bool, self.X2_bool)
-        assert_array_almost_equal(D12, D_true)
-
-    def test_pdist(self):
-        for metric, argdict in self.metrics.items():
-            keys = argdict.keys()
-            for vals in itertools.product(*argdict.values()):
-                kwargs = dict(zip(keys, vals))
-                D_true = cdist(self.X1, self.X1, metric, **kwargs)
-                yield self.check_pdist, metric, kwargs, D_true
-
-        for metric in self.bool_metrics:
-            D_true = cdist(self.X1_bool, self.X1_bool, metric)
-            yield self.check_pdist_bool, metric, D_true
-
-    def check_pdist(self, metric, kwargs, D_true):
-        dm = DistanceMetric.get_metric(metric, **kwargs)
-        D12 = dm.pairwise(self.X1)
-        assert_array_almost_equal(D12, D_true)
-
-    def check_pdist_bool(self, metric, D_true):
-        dm = DistanceMetric.get_metric(metric)
-        D12 = dm.pairwise(self.X1_bool)
-        assert_array_almost_equal(D12, D_true)
-
-    def test_pickle(self):
-        for metric, argdict in self.metrics.items():
-            keys = argdict.keys()
-            for vals in itertools.product(*argdict.values()):
-                kwargs = dict(zip(keys, vals))
-                yield self.check_pickle, metric, kwargs
-
-        for metric in self.bool_metrics:
-            yield self.check_pickle_bool, metric
-
-    def check_pickle_bool(self, metric):
-        dm = DistanceMetric.get_metric(metric)
-        D1 = dm.pairwise(self.X1_bool)
-        dm2 = pickle.loads(pickle.dumps(dm))
-        D2 = dm2.pairwise(self.X1_bool)
-        assert_array_almost_equal(D1, D2)
-
-    def check_pickle(self, metric, kwargs):
-        dm = DistanceMetric.get_metric(metric, **kwargs)
-        D1 = dm.pairwise(self.X1)
-        dm2 = pickle.loads(pickle.dumps(dm))
-        D2 = dm2.pairwise(self.X1)
-        assert_array_almost_equal(D1, D2)
+rng = check_random_state(0)
+d = 4
+n1 = 20
+n2 = 25
+X1 = rng.random_sample((n1, d)).astype('float64')
+X2 = rng.random_sample((n2, d)).astype('float64')
+
+# make boolean arrays: ones and zeros
+X1_bool = X1.round(0)
+X2_bool = X2.round(0)
+
+V = rng.random_sample((d, d))
+VI = np.dot(V, V.T)
+
+BOOL_METRICS = ['matching', 'jaccard', 'dice',
+                'kulsinski', 'rogerstanimoto', 'russellrao',
+                'sokalmichener', 'sokalsneath']
+
+METRICS_DEFAULT_PARAMS = {'euclidean': {},
+                          'cityblock': {},
+                          'minkowski': dict(p=(1, 1.5, 2, 3)),
+                          'chebyshev': {},
+                          'seuclidean': dict(V=(rng.random_sample(d),)),
+                          'wminkowski': dict(p=(1, 1.5, 3),
+                                             w=(rng.random_sample(d),)),
+                          'mahalanobis': dict(VI=(VI,)),
+                          'hamming': {},
+                          'canberra': {},
+                          'braycurtis': {}}
+
+
+@pytest.mark.parametrize('metric', METRICS_DEFAULT_PARAMS)
+def test_cdist(metric):
+    argdict = METRICS_DEFAULT_PARAMS[metric]
+    keys = argdict.keys()
+    for vals in itertools.product(*argdict.values()):
+        kwargs = dict(zip(keys, vals))
+        D_true = cdist(X1, X2, metric, **kwargs)
+        check_cdist(metric, kwargs, D_true)
+
+
+@pytest.mark.parametrize('metric', BOOL_METRICS)
+def test_cdist_bool_metric(metric):
+    D_true = cdist(X1_bool, X2_bool, metric)
+    check_cdist_bool(metric, D_true)
+
+
+def check_cdist(metric, kwargs, D_true):
+    dm = DistanceMetric.get_metric(metric, **kwargs)
+    D12 = dm.pairwise(X1, X2)
+    assert_array_almost_equal(D12, D_true)
+
+
+def check_cdist_bool(metric, D_true):
+    dm = DistanceMetric.get_metric(metric)
+    D12 = dm.pairwise(X1_bool, X2_bool)
+    assert_array_almost_equal(D12, D_true)
+
+
+@pytest.mark.parametrize('metric', METRICS_DEFAULT_PARAMS)
+def test_pdist(metric):
+    argdict = METRICS_DEFAULT_PARAMS[metric]
+    keys = argdict.keys()
+    for vals in itertools.product(*argdict.values()):
+        kwargs = dict(zip(keys, vals))
+        D_true = cdist(X1, X1, metric, **kwargs)
+        check_pdist(metric, kwargs, D_true)
+
+
+@pytest.mark.parametrize('metric', BOOL_METRICS)
+def test_pdist_bool_metrics(metric):
+    D_true = cdist(X1_bool, X1_bool, metric)
+    check_pdist_bool(metric, D_true)
+
+
+def check_pdist(metric, kwargs, D_true):
+    dm = DistanceMetric.get_metric(metric, **kwargs)
+    D12 = dm.pairwise(X1)
+    assert_array_almost_equal(D12, D_true)
+
+
+def check_pdist_bool(metric, D_true):
+    dm = DistanceMetric.get_metric(metric)
+    D12 = dm.pairwise(X1_bool)
+    assert_array_almost_equal(D12, D_true)
+
+
+@pytest.mark.parametrize('metric', METRICS_DEFAULT_PARAMS)
+def test_pickle(metric):
+    argdict = METRICS_DEFAULT_PARAMS[metric]
+    keys = argdict.keys()
+    for vals in itertools.product(*argdict.values()):
+        kwargs = dict(zip(keys, vals))
+        check_pickle(metric, kwargs)
+
+
+@pytest.mark.parametrize('metric', BOOL_METRICS)
+def test_pickle_bool_metrics(metric):
+    dm = DistanceMetric.get_metric(metric)
+    D1 = dm.pairwise(X1_bool)
+    dm2 = pickle.loads(pickle.dumps(dm))
+    D2 = dm2.pairwise(X1_bool)
+    assert_array_almost_equal(D1, D2)
+
+
+def check_pickle(metric, kwargs):
+    dm = DistanceMetric.get_metric(metric, **kwargs)
+    D1 = dm.pairwise(X1)
+    dm2 = pickle.loads(pickle.dumps(dm))
+    D2 = dm2.pairwise(X1)
+    assert_array_almost_equal(D1, D2)
 
 
 def test_haversine_metric():
diff --git a/sklearn/neighbors/tests/test_kd_tree.py b/sklearn/neighbors/tests/test_kd_tree.py
index e1b7cb196598..46cddc711e76 100644
--- a/sklearn/neighbors/tests/test_kd_tree.py
+++ b/sklearn/neighbors/tests/test_kd_tree.py
@@ -1,5 +1,8 @@
 import numpy as np
 from numpy.testing import assert_array_almost_equal
+
+import pytest
+
 from sklearn.neighbors.kd_tree import (KDTree, NeighborsHeap,
                                        simultaneous_sort, kernel_norm,
                                        nodeheap_sort, DTYPE, ITYPE)
@@ -37,18 +40,17 @@ def check_neighbors(dualtree, breadth_first, k, metric, X, Y, kwargs):
     assert_array_almost_equal(dist1, dist2)
 
 
-def test_kd_tree_query():
+@pytest.mark.parametrize('metric', METRICS)
+@pytest.mark.parametrize('k', (1, 3, 5))
+@pytest.mark.parametrize('dualtree', (True, False))
+@pytest.mark.parametrize('breadth_first', (True, False))
+def test_kd_tree_query(metric, k, dualtree, breadth_first):
     rng = check_random_state(0)
     X = rng.random_sample((40, DIMENSION))
     Y = rng.random_sample((10, DIMENSION))
 
-    for (metric, kwargs) in METRICS.items():
-        for k in (1, 3, 5):
-            for dualtree in (True, False):
-                for breadth_first in (True, False):
-                    yield (check_neighbors,
-                           dualtree, breadth_first,
-                           k, metric, X, Y, kwargs)
+    kwargs = METRICS[metric]
+    check_neighbors(dualtree, breadth_first, k, metric, X, Y, kwargs)
 
 
 def test_kd_tree_query_radius(n_samples=100, n_features=10):
@@ -118,22 +120,24 @@ def check_results(kernel, h, atol, rtol, breadth_first, Y, kdt, dens_true):
                     rtol=max(rtol, 1e-7))
 
 
-def test_kd_tree_kde(n_samples=100, n_features=3):
+@pytest.mark.parametrize('kernel',
+                         ['gaussian', 'tophat', 'epanechnikov',
+                          'exponential', 'linear', 'cosine'])
+@pytest.mark.parametrize('h', [0.01, 0.1, 1])
+def test_kd_tree_kde(kernel, h):
+    n_samples, n_features = (100, 3)
     rng = check_random_state(0)
     X = rng.random_sample((n_samples, n_features))
     Y = rng.random_sample((n_samples, n_features))
     kdt = KDTree(X, leaf_size=10)
 
-    for kernel in ['gaussian', 'tophat', 'epanechnikov',
-                   'exponential', 'linear', 'cosine']:
-        for h in [0.01, 0.1, 1]:
-            dens_true = compute_kernel_slow(Y, X, kernel, h)
+    dens_true = compute_kernel_slow(Y, X, kernel, h)
 
-            for rtol in [0, 1E-5]:
-                for atol in [1E-6, 1E-2]:
-                    for breadth_first in (True, False):
-                        yield (check_results, kernel, h, atol, rtol,
-                               breadth_first, Y, kdt, dens_true)
+    for rtol in [0, 1E-5]:
+        for atol in [1E-6, 1E-2]:
+            for breadth_first in (True, False):
+                check_results(kernel, h, atol, rtol,
+                              breadth_first, Y, kdt, dens_true)
 
 
 def test_gaussian_kde(n_samples=1000):
@@ -153,7 +157,9 @@ def test_gaussian_kde(n_samples=1000):
         assert_array_almost_equal(dens_kdt, dens_gkde, decimal=3)
 
 
-def test_kd_tree_two_point(n_samples=100, n_features=3):
+@pytest.mark.parametrize('dualtree', (True, False))
+def test_kd_tree_two_point(dualtree):
+    n_samples, n_features = (100, 3)
     rng = check_random_state(0)
     X = rng.random_sample((n_samples, n_features))
     Y = rng.random_sample((n_samples, n_features))
@@ -163,15 +169,12 @@ def test_kd_tree_two_point(n_samples=100, n_features=3):
     D = DistanceMetric.get_metric("euclidean").pairwise(Y, X)
     counts_true = [(D <= ri).sum() for ri in r]
 
-    def check_two_point(r, dualtree):
-        counts = kdt.two_point_correlation(Y, r=r, dualtree=dualtree)
-        assert_array_almost_equal(counts, counts_true)
-
-    for dualtree in (True, False):
-        yield check_two_point, r, dualtree
+    counts = kdt.two_point_correlation(Y, r=r, dualtree=dualtree)
+    assert_array_almost_equal(counts, counts_true)
 
 
-def test_kd_tree_pickle():
+@pytest.mark.parametrize('protocol', (0, 1, 2))
+def test_kd_tree_pickle(protocol):
     import pickle
     rng = check_random_state(0)
     X = rng.random_sample((10, 3))
@@ -185,8 +188,7 @@ def check_pickle_protocol(protocol):
         assert_array_almost_equal(ind1, ind2)
         assert_array_almost_equal(dist1, dist2)
 
-    for protocol in (0, 1, 2):
-        yield check_pickle_protocol, protocol
+    check_pickle_protocol(protocol)
 
 
 def test_neighbors_heap(n_pts=5, n_nbrs=10):
diff --git a/sklearn/neighbors/tests/test_kde.py b/sklearn/neighbors/tests/test_kde.py
index 60f294a3df0a..caffb662608e 100644
--- a/sklearn/neighbors/tests/test_kde.py
+++ b/sklearn/neighbors/tests/test_kde.py
@@ -1,4 +1,7 @@
 import numpy as np
+
+import pytest
+
 from sklearn.utils.testing import (assert_allclose, assert_raises,
                                    assert_equal)
 from sklearn.neighbors import KernelDensity, KDTree, NearestNeighbors
@@ -40,21 +43,25 @@ def check_results(kernel, bandwidth, atol, rtol, X, Y, dens_true):
                     atol=atol, rtol=max(1E-7, rtol))
 
 
-def test_kernel_density(n_samples=100, n_features=3):
+@pytest.mark.parametrize(
+        'kernel',
+        ['gaussian', 'tophat', 'epanechnikov',
+         'exponential', 'linear', 'cosine'])
+@pytest.mark.parametrize('bandwidth', [0.01, 0.1, 1])
+def test_kernel_density(kernel, bandwidth):
+    n_samples, n_features = (100, 3)
+
     rng = np.random.RandomState(0)
     X = rng.randn(n_samples, n_features)
     Y = rng.randn(n_samples, n_features)
 
-    for kernel in ['gaussian', 'tophat', 'epanechnikov',
-                   'exponential', 'linear', 'cosine']:
-        for bandwidth in [0.01, 0.1, 1]:
-            dens_true = compute_kernel_slow(Y, X, kernel, bandwidth)
+    dens_true = compute_kernel_slow(Y, X, kernel, bandwidth)
 
-            for rtol in [0, 1E-5]:
-                for atol in [1E-6, 1E-2]:
-                    for breadth_first in (True, False):
-                        yield (check_results, kernel, bandwidth, atol, rtol,
-                               X, Y, dens_true)
+    for rtol in [0, 1E-5]:
+        for atol in [1E-6, 1E-2]:
+            for breadth_first in (True, False):
+                check_results(kernel, bandwidth, atol, rtol,
+                              X, Y, dens_true)
 
 
 def test_kernel_density_sampling(n_samples=100, n_features=3):
@@ -91,23 +98,24 @@ def test_kernel_density_sampling(n_samples=100, n_features=3):
     assert_equal(kde.sample().shape, (1, 1))
 
 
-def test_kde_algorithm_metric_choice():
+@pytest.mark.parametrize('algorithm', ['auto', 'ball_tree', 'kd_tree'])
+@pytest.mark.parametrize('metric',
+                         ['euclidean', 'minkowski', 'manhattan',
+                          'chebyshev', 'haversine'])
+def test_kde_algorithm_metric_choice(algorithm, metric):
     # Smoke test for various metrics and algorithms
     rng = np.random.RandomState(0)
     X = rng.randn(10, 2)    # 2 features required for haversine dist.
     Y = rng.randn(10, 2)
 
-    for algorithm in ['auto', 'ball_tree', 'kd_tree']:
-        for metric in ['euclidean', 'minkowski', 'manhattan',
-                       'chebyshev', 'haversine']:
-            if algorithm == 'kd_tree' and metric not in KDTree.valid_metrics:
-                assert_raises(ValueError, KernelDensity,
-                              algorithm=algorithm, metric=metric)
-            else:
-                kde = KernelDensity(algorithm=algorithm, metric=metric)
-                kde.fit(X)
-                y_dens = kde.score_samples(Y)
-                assert_equal(y_dens.shape, Y.shape[:1])
+    if algorithm == 'kd_tree' and metric not in KDTree.valid_metrics:
+        assert_raises(ValueError, KernelDensity,
+                      algorithm=algorithm, metric=metric)
+    else:
+        kde = KernelDensity(algorithm=algorithm, metric=metric)
+        kde.fit(X)
+        y_dens = kde.score_samples(Y)
+        assert_equal(y_dens.shape, Y.shape[:1])
 
 
 def test_kde_score(n_samples=100, n_features=3):
diff --git a/sklearn/neighbors/tests/test_neighbors.py b/sklearn/neighbors/tests/test_neighbors.py
index a95a906ad3cb..e1acaa4c6f13 100644
--- a/sklearn/neighbors/tests/test_neighbors.py
+++ b/sklearn/neighbors/tests/test_neighbors.py
@@ -4,6 +4,8 @@
 from scipy.sparse import (bsr_matrix, coo_matrix, csc_matrix, csr_matrix,
                           dok_matrix, lil_matrix, issparse)
 
+import pytest
+
 from sklearn import metrics
 from sklearn import neighbors, datasets
 from sklearn.exceptions import DataConversionWarning
@@ -1260,63 +1262,57 @@ def test_include_self_neighbors_graph():
     assert_array_equal(rng_not_self, [[0., 1.], [1., 0.]])
 
 
-def test_same_knn_parallel():
+@pytest.mark.parametrize('algorithm', ALGORITHMS)
+def test_same_knn_parallel(algorithm):
     X, y = datasets.make_classification(n_samples=30, n_features=5,
                                         n_redundant=0, random_state=0)
     X_train, X_test, y_train, y_test = train_test_split(X, y)
 
-    def check_same_knn_parallel(algorithm):
-        clf = neighbors.KNeighborsClassifier(n_neighbors=3,
-                                             algorithm=algorithm)
-        clf.fit(X_train, y_train)
-        y = clf.predict(X_test)
-        dist, ind = clf.kneighbors(X_test)
-        graph = clf.kneighbors_graph(X_test, mode='distance').toarray()
-
-        clf.set_params(n_jobs=3)
-        clf.fit(X_train, y_train)
-        y_parallel = clf.predict(X_test)
-        dist_parallel, ind_parallel = clf.kneighbors(X_test)
-        graph_parallel = \
-            clf.kneighbors_graph(X_test, mode='distance').toarray()
-
-        assert_array_equal(y, y_parallel)
-        assert_array_almost_equal(dist, dist_parallel)
-        assert_array_equal(ind, ind_parallel)
-        assert_array_almost_equal(graph, graph_parallel)
+    clf = neighbors.KNeighborsClassifier(n_neighbors=3,
+                                         algorithm=algorithm)
+    clf.fit(X_train, y_train)
+    y = clf.predict(X_test)
+    dist, ind = clf.kneighbors(X_test)
+    graph = clf.kneighbors_graph(X_test, mode='distance').toarray()
 
-    for algorithm in ALGORITHMS:
-        yield check_same_knn_parallel, algorithm
+    clf.set_params(n_jobs=3)
+    clf.fit(X_train, y_train)
+    y_parallel = clf.predict(X_test)
+    dist_parallel, ind_parallel = clf.kneighbors(X_test)
+    graph_parallel = \
+        clf.kneighbors_graph(X_test, mode='distance').toarray()
+
+    assert_array_equal(y, y_parallel)
+    assert_array_almost_equal(dist, dist_parallel)
+    assert_array_equal(ind, ind_parallel)
+    assert_array_almost_equal(graph, graph_parallel)
 
 
-def test_same_radius_neighbors_parallel():
+@pytest.mark.parametrize('algorithm', ALGORITHMS)
+def test_same_radius_neighbors_parallel(algorithm):
     X, y = datasets.make_classification(n_samples=30, n_features=5,
                                         n_redundant=0, random_state=0)
     X_train, X_test, y_train, y_test = train_test_split(X, y)
 
-    def check_same_radius_neighbors_parallel(algorithm):
-        clf = neighbors.RadiusNeighborsClassifier(radius=10,
-                                                  algorithm=algorithm)
-        clf.fit(X_train, y_train)
-        y = clf.predict(X_test)
-        dist, ind = clf.radius_neighbors(X_test)
-        graph = clf.radius_neighbors_graph(X_test, mode='distance').toarray()
-
-        clf.set_params(n_jobs=3)
-        clf.fit(X_train, y_train)
-        y_parallel = clf.predict(X_test)
-        dist_parallel, ind_parallel = clf.radius_neighbors(X_test)
-        graph_parallel = \
-            clf.radius_neighbors_graph(X_test, mode='distance').toarray()
-
-        assert_array_equal(y, y_parallel)
-        for i in range(len(dist)):
-            assert_array_almost_equal(dist[i], dist_parallel[i])
-            assert_array_equal(ind[i], ind_parallel[i])
-        assert_array_almost_equal(graph, graph_parallel)
-
-    for algorithm in ALGORITHMS:
-        yield check_same_radius_neighbors_parallel, algorithm
+    clf = neighbors.RadiusNeighborsClassifier(radius=10,
+                                              algorithm=algorithm)
+    clf.fit(X_train, y_train)
+    y = clf.predict(X_test)
+    dist, ind = clf.radius_neighbors(X_test)
+    graph = clf.radius_neighbors_graph(X_test, mode='distance').toarray()
+
+    clf.set_params(n_jobs=3)
+    clf.fit(X_train, y_train)
+    y_parallel = clf.predict(X_test)
+    dist_parallel, ind_parallel = clf.radius_neighbors(X_test)
+    graph_parallel = \
+        clf.radius_neighbors_graph(X_test, mode='distance').toarray()
+
+    assert_array_equal(y, y_parallel)
+    for i in range(len(dist)):
+        assert_array_almost_equal(dist[i], dist_parallel[i])
+        assert_array_equal(ind[i], ind_parallel[i])
+    assert_array_almost_equal(graph, graph_parallel)
 
 
 def test_dtype_convert():
diff --git a/sklearn/neighbors/tests/test_quad_tree.py b/sklearn/neighbors/tests/test_quad_tree.py
index 6cfa4bcc562e..156bfc232a55 100644
--- a/sklearn/neighbors/tests/test_quad_tree.py
+++ b/sklearn/neighbors/tests/test_quad_tree.py
@@ -1,5 +1,8 @@
 import pickle
 import numpy as np
+
+import pytest
+
 from sklearn.neighbors.quad_tree import _QuadTree
 from sklearn.utils import check_random_state
 
@@ -58,50 +61,43 @@ def test_quadtree_similar_point():
         tree._check_coherence()
 
 
-def test_quad_tree_pickle():
+@pytest.mark.parametrize('n_dimensions', (2, 3))
+@pytest.mark.parametrize('protocol', (0, 1, 2))
+def test_quad_tree_pickle(n_dimensions, protocol):
     rng = check_random_state(0)
 
-    for n_dimensions in (2, 3):
-        X = rng.random_sample((10, n_dimensions))
-
-        tree = _QuadTree(n_dimensions=n_dimensions, verbose=0)
-        tree.build_tree(X)
+    X = rng.random_sample((10, n_dimensions))
 
-        def check_pickle_protocol(protocol):
-            s = pickle.dumps(tree, protocol=protocol)
-            bt2 = pickle.loads(s)
+    tree = _QuadTree(n_dimensions=n_dimensions, verbose=0)
+    tree.build_tree(X)
 
-            for x in X:
-                cell_x_tree = tree.get_cell(x)
-                cell_x_bt2 = bt2.get_cell(x)
-                assert cell_x_tree == cell_x_bt2
+    s = pickle.dumps(tree, protocol=protocol)
+    bt2 = pickle.loads(s)
 
-        for protocol in (0, 1, 2):
-            yield check_pickle_protocol, protocol
+    for x in X:
+        cell_x_tree = tree.get_cell(x)
+        cell_x_bt2 = bt2.get_cell(x)
+        assert cell_x_tree == cell_x_bt2
 
 
-def test_qt_insert_duplicate():
+@pytest.mark.parametrize('n_dimensions', (2, 3))
+def test_qt_insert_duplicate(n_dimensions):
     rng = check_random_state(0)
 
-    def check_insert_duplicate(n_dimensions=2):
-
-        X = rng.random_sample((10, n_dimensions))
-        Xd = np.r_[X, X[:5]]
-        tree = _QuadTree(n_dimensions=n_dimensions, verbose=0)
-        tree.build_tree(Xd)
-
-        cumulative_size = tree.cumulative_size
-        leafs = tree.leafs
+    X = rng.random_sample((10, n_dimensions))
+    Xd = np.r_[X, X[:5]]
+    tree = _QuadTree(n_dimensions=n_dimensions, verbose=0)
+    tree.build_tree(Xd)
 
-        # Assert that the first 5 are indeed duplicated and that the next
-        # ones are single point leaf
-        for i, x in enumerate(X):
-            cell_id = tree.get_cell(x)
-            assert leafs[cell_id]
-            assert cumulative_size[cell_id] == 1 + (i < 5)
+    cumulative_size = tree.cumulative_size
+    leafs = tree.leafs
 
-    for n_dimensions in (2, 3):
-        yield check_insert_duplicate, n_dimensions
+    # Assert that the first 5 are indeed duplicated and that the next
+    # ones are single point leaf
+    for i, x in enumerate(X):
+        cell_id = tree.get_cell(x)
+        assert leafs[cell_id]
+        assert cumulative_size[cell_id] == 1 + (i < 5)
 
 
 def test_summarize():
diff --git a/sklearn/pipeline.py b/sklearn/pipeline.py
index c25af3bea90e..1e99dd54615a 100644
--- a/sklearn/pipeline.py
+++ b/sklearn/pipeline.py
@@ -68,6 +68,11 @@ class Pipeline(_BaseComposition):
         Read-only attribute to access any step parameter by user given name.
         Keys are step names and values are steps parameters.
 
+    See also
+    --------
+    sklearn.pipeline.make_pipeline : convenience function for simplified
+        pipeline construction.
+
     Examples
     --------
     >>> from sklearn import svm
@@ -209,7 +214,7 @@ def _fit(self, X, y=None, **fit_params):
                     cloned_transformer = clone(transformer)
                 # Fit or load from cache the current transfomer
                 Xt, fitted_transformer = fit_transform_one_cached(
-                    cloned_transformer, None, Xt, y,
+                    cloned_transformer, Xt, y, None,
                     **fit_params_steps[name])
                 # Replace the transformer of the step with the fitted
                 # transformer. This is necessary when loading the transformer
@@ -549,6 +554,11 @@ def make_pipeline(*steps, **kwargs):
         inspect estimators within the pipeline. Caching the
         transformers is advantageous when fitting is time consuming.
 
+    See also
+    --------
+    sklearn.pipeline.Pipeline : Class for creating a pipeline of
+        transforms with a final estimator.
+
     Examples
     --------
     >>> from sklearn.naive_bayes import GaussianNB
@@ -572,11 +582,14 @@ def make_pipeline(*steps, **kwargs):
     return Pipeline(_name_estimators(steps), memory=memory)
 
 
-def _fit_one_transformer(transformer, X, y):
+# weight and fit_params are not used but it allows _fit_one_transformer,
+# _transform_one and _fit_transform_one to have the same signature to
+#  factorize the code in ColumnTransformer
+def _fit_one_transformer(transformer, X, y, weight=None, **fit_params):
     return transformer.fit(X, y)
 
 
-def _transform_one(transformer, weight, X):
+def _transform_one(transformer, X, y, weight, **fit_params):
     res = transformer.transform(X)
     # if we have a weight for this transformer, multiply output
     if weight is None:
@@ -584,8 +597,7 @@ def _transform_one(transformer, weight, X):
     return res * weight
 
 
-def _fit_transform_one(transformer, weight, X, y,
-                       **fit_params):
+def _fit_transform_one(transformer, X, y, weight, **fit_params):
     if hasattr(transformer, 'fit_transform'):
         res = transformer.fit_transform(X, y, **fit_params)
     else:
@@ -623,6 +635,21 @@ class FeatureUnion(_BaseComposition, TransformerMixin):
         Multiplicative weights for features per transformer.
         Keys are transformer names, values the weights.
 
+    See also
+    --------
+    sklearn.pipeline.make_union : convenience function for simplified
+        feature union construction.
+
+    Examples
+    --------
+    >>> from sklearn.pipeline import FeatureUnion
+    >>> from sklearn.decomposition import PCA, TruncatedSVD
+    >>> union = FeatureUnion([("pca", PCA(n_components=1)),
+    ...                       ("svd", TruncatedSVD(n_components=2))])
+    >>> X = [[0., 1., 3], [2., 2., 5]]
+    >>> union.fit_transform(X)    # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
+    array([[ 1.5       ,  3.0...,  0.8...],
+           [-1.5       ,  5.7..., -0.4...]])
     """
     def __init__(self, transformer_list, n_jobs=1, transformer_weights=None):
         self.transformer_list = transformer_list
@@ -675,7 +702,8 @@ def _validate_transformers(self):
                                 (t, type(t)))
 
     def _iter(self):
-        """Generate (name, est, weight) tuples excluding None transformers
+        """
+        Generate (name, trans, weight) tuples excluding None transformers
         """
         get_weight = (self.transformer_weights or {}).get
         return ((name, trans, get_weight(name))
@@ -743,7 +771,7 @@ def fit_transform(self, X, y=None, **fit_params):
         """
         self._validate_transformers()
         result = Parallel(n_jobs=self.n_jobs)(
-            delayed(_fit_transform_one)(trans, weight, X, y,
+            delayed(_fit_transform_one)(trans, X, y, weight,
                                         **fit_params)
             for name, trans, weight in self._iter())
 
@@ -773,7 +801,7 @@ def transform(self, X):
             sum of n_components (output dimension) over transformers.
         """
         Xs = Parallel(n_jobs=self.n_jobs)(
-            delayed(_transform_one)(trans, weight, X)
+            delayed(_transform_one)(trans, X, None, weight)
             for name, trans, weight in self._iter())
         if not Xs:
             # All transformers are None
@@ -810,6 +838,11 @@ def make_union(*transformers, **kwargs):
     -------
     f : FeatureUnion
 
+    See also
+    --------
+    sklearn.pipeline.FeatureUnion : Class for concatenating the results
+        of multiple transformer objects.
+
     Examples
     --------
     >>> from sklearn.decomposition import PCA, TruncatedSVD
diff --git a/sklearn/preprocessing/__init__.py b/sklearn/preprocessing/__init__.py
index ba0884613c12..85bade9b81c1 100644
--- a/sklearn/preprocessing/__init__.py
+++ b/sklearn/preprocessing/__init__.py
@@ -22,11 +22,12 @@
 from .data import minmax_scale
 from .data import quantile_transform
 from .data import power_transform
-from .data import OneHotEncoder
 from .data import PowerTransformer
-from .data import CategoricalEncoder
 from .data import PolynomialFeatures
 
+from ._encoders import OneHotEncoder
+from ._encoders import OrdinalEncoder
+
 from .label import label_binarize
 from .label import LabelBinarizer
 from .label import LabelEncoder
@@ -34,6 +35,8 @@
 
 from .imputation import Imputer
 
+# stub, remove in version 0.21
+from .data import CategoricalEncoder  # noqa
 
 __all__ = [
     'Binarizer',
@@ -48,7 +51,7 @@
     'QuantileTransformer',
     'Normalizer',
     'OneHotEncoder',
-    'CategoricalEncoder',
+    'OrdinalEncoder',
     'PowerTransformer',
     'RobustScaler',
     'StandardScaler',
diff --git a/sklearn/preprocessing/_encoders.py b/sklearn/preprocessing/_encoders.py
new file mode 100644
index 000000000000..b2aca044e036
--- /dev/null
+++ b/sklearn/preprocessing/_encoders.py
@@ -0,0 +1,835 @@
+# Authors: Andreas Mueller <amueller@ais.uni-bonn.de>
+#          Joris Van den Bossche <jorisvandenbossche@gmail.com>
+# License: BSD 3 clause
+
+from __future__ import division
+
+import numbers
+import warnings
+
+import numpy as np
+from scipy import sparse
+
+from ..base import BaseEstimator, TransformerMixin
+from ..externals import six
+from ..utils import check_array
+from ..utils import deprecated
+from ..utils.fixes import _argmax
+from ..utils.validation import check_is_fitted, FLOAT_DTYPES
+from .label import LabelEncoder
+
+
+range = six.moves.range
+
+
+__all__ = [
+    'OneHotEncoder',
+    'OrdinalEncoder'
+]
+
+
+def _transform_selected(X, transform, dtype, selected="all", copy=True):
+    """Apply a transform function to portion of selected features
+
+    Parameters
+    ----------
+    X : {array-like, sparse matrix}, shape [n_samples, n_features]
+        Dense array or sparse matrix.
+
+    transform : callable
+        A callable transform(X) -> X_transformed
+
+    dtype : number type
+        Desired dtype of output.
+
+    copy : boolean, optional
+        Copy X even if it could be avoided.
+
+    selected: "all" or array of indices or mask
+        Specify which features to apply the transform to.
+
+    Returns
+    -------
+    X : array or sparse matrix, shape=(n_samples, n_features_new)
+    """
+    X = check_array(X, accept_sparse='csc', copy=copy, dtype=FLOAT_DTYPES)
+
+    if isinstance(selected, six.string_types) and selected == "all":
+        return transform(X)
+
+    if len(selected) == 0:
+        return X
+
+    n_features = X.shape[1]
+    ind = np.arange(n_features)
+    sel = np.zeros(n_features, dtype=bool)
+    sel[np.asarray(selected)] = True
+    not_sel = np.logical_not(sel)
+    n_selected = np.sum(sel)
+
+    if n_selected == 0:
+        # No features selected.
+        return X
+    elif n_selected == n_features:
+        # All features selected.
+        return transform(X)
+    else:
+        X_sel = transform(X[:, ind[sel]])
+        # The columns of X which are not transformed need
+        # to be casted to the desire dtype before concatenation.
+        # Otherwise, the stacking will cast to the higher-precision dtype.
+        X_not_sel = X[:, ind[not_sel]].astype(dtype)
+
+        if sparse.issparse(X_sel) or sparse.issparse(X_not_sel):
+            return sparse.hstack((X_sel, X_not_sel))
+        else:
+            return np.hstack((X_sel, X_not_sel))
+
+
+class _BaseEncoder(BaseEstimator, TransformerMixin):
+    """
+    Base class for encoders that includes the code to categorize and
+    transform the input features.
+
+    """
+
+    def _fit(self, X, handle_unknown='error'):
+
+        X_temp = check_array(X, dtype=None)
+        if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):
+            X = check_array(X, dtype=np.object)
+        else:
+            X = X_temp
+
+        n_samples, n_features = X.shape
+
+        if self._categories != 'auto':
+            for cats in self._categories:
+                if not np.all(np.sort(cats) == np.array(cats)):
+                    raise ValueError("Unsorted categories are not yet "
+                                     "supported")
+            if len(self._categories) != n_features:
+                raise ValueError("Shape mismatch: if n_values is an array,"
+                                 " it has to be of shape (n_features,).")
+
+        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]
+
+        for i in range(n_features):
+            le = self._label_encoders_[i]
+            Xi = X[:, i]
+            if self._categories == 'auto':
+                le.fit(Xi)
+            else:
+                if handle_unknown == 'error':
+                    valid_mask = np.in1d(Xi, self._categories[i])
+                    if not np.all(valid_mask):
+                        diff = np.unique(Xi[~valid_mask])
+                        msg = ("Found unknown categories {0} in column {1}"
+                               " during fit".format(diff, i))
+                        raise ValueError(msg)
+                le.classes_ = np.array(self._categories[i])
+
+        self.categories_ = [le.classes_ for le in self._label_encoders_]
+
+    def _transform(self, X, handle_unknown='error'):
+
+        X_temp = check_array(X, dtype=None)
+        if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):
+            X = check_array(X, dtype=np.object)
+        else:
+            X = X_temp
+
+        _, n_features = X.shape
+        X_int = np.zeros_like(X, dtype=np.int)
+        X_mask = np.ones_like(X, dtype=np.bool)
+
+        for i in range(n_features):
+            Xi = X[:, i]
+            valid_mask = np.in1d(Xi, self.categories_[i])
+
+            if not np.all(valid_mask):
+                if handle_unknown == 'error':
+                    diff = np.unique(X[~valid_mask, i])
+                    msg = ("Found unknown categories {0} in column {1}"
+                           " during transform".format(diff, i))
+                    raise ValueError(msg)
+                else:
+                    # Set the problematic rows to an acceptable value and
+                    # continue `The rows are marked `X_mask` and will be
+                    # removed later.
+                    X_mask[:, i] = valid_mask
+                    Xi = Xi.copy()
+                    Xi[~valid_mask] = self.categories_[i][0]
+            X_int[:, i] = self._label_encoders_[i].transform(Xi)
+
+        return X_int, X_mask
+
+
+class OneHotEncoder(_BaseEncoder):
+    """Encode categorical integer features as a one-hot numeric array.
+
+    The input to this transformer should be an array-like of integers or
+    strings, denoting the values taken on by categorical (discrete) features.
+    The features are encoded using a one-hot (aka 'one-of-K' or 'dummy')
+    encoding scheme. This creates a binary column for each category and
+    returns a sparse matrix or dense array.
+
+    By default, the encoder derives the categories based on the unique values
+    in each feature. Alternatively, you can also specify the `categories`
+    manually.
+    The OneHotEncoder previously assumed that the input features take on
+    values in the range [0, max(values)). This behaviour is deprecated.
+
+    This encoding is needed for feeding categorical data to many scikit-learn
+    estimators, notably linear models and SVMs with the standard kernels.
+
+    Note: a one-hot encoding of y labels should use a LabelBinarizer
+    instead.
+
+    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.
+
+    Parameters
+    ----------
+    categories : 'auto' or a list of lists/arrays of values.
+        Categories (unique values) per feature:
+
+        - 'auto' : Determine categories automatically from the training data.
+        - list : ``categories[i]`` holds the categories expected in the ith
+          column. The passed categories must be sorted and should not mix
+          strings and numeric values.
+
+        The used categories can be found in the ``categories_`` attribute.
+
+    sparse : boolean, default=True
+        Will return sparse matrix if set True else will return an array.
+
+    dtype : number type, default=np.float
+        Desired dtype of output.
+
+    handle_unknown : 'error' (default) or 'ignore'
+        Whether to raise an error or ignore if an unknown categorical feature
+        is present during transform (default is to raise). When this parameter
+        is set to 'ignore' and an unknown category is encountered during
+        transform, the resulting one-hot encoded columns for this feature
+        will be all zeros. In the inverse transform, an unknown category
+        will be denoted as None.
+
+    n_values : 'auto', int or array of ints
+        Number of values per feature.
+
+        - 'auto' : determine value range from training data.
+        - int : number of categorical values per feature.
+                Each feature value should be in ``range(n_values)``
+        - array : ``n_values[i]`` is the number of categorical values in
+                  ``X[:, i]``. Each feature value should be
+                  in ``range(n_values[i])``
+
+        .. deprecated:: 0.20
+            The `n_values` keyword was deprecated in version 0.20 and will
+            be removed in 0.22. Use `categories` instead.
+
+    categorical_features : "all" or array of indices or mask
+        Specify what features are treated as categorical.
+
+        - 'all' (default): All features are treated as categorical.
+        - array of indices: Array of categorical feature indices.
+        - mask: Array of length n_features and with dtype=bool.
+
+        Non-categorical features are always stacked to the right of the matrix.
+
+        .. deprecated:: 0.20
+            The `categorical_features` keyword was deprecated in version
+            0.20 and will be removed in 0.22.
+            You can use the ``ColumnTransformer`` instead.
+
+    Attributes
+    ----------
+    categories_ : list of arrays
+        The categories of each feature determined during fitting
+        (in order of the features in X and corresponding with the output
+        of ``transform``).
+
+    active_features_ : array
+        Indices for active features, meaning values that actually occur
+        in the training set. Only available when n_values is ``'auto'``.
+
+        .. deprecated:: 0.20
+            The `active_features_` attribute was deprecated in version
+            0.20 and will be removed in 0.22.
+
+    feature_indices_ : array of shape (n_features,)
+        Indices to feature ranges.
+        Feature ``i`` in the original data is mapped to features
+        from ``feature_indices_[i]`` to ``feature_indices_[i+1]``
+        (and then potentially masked by `active_features_` afterwards)
+
+        .. deprecated:: 0.20
+            The `feature_indices_` attribute was deprecated in version
+            0.20 and will be removed in 0.22.
+
+    n_values_ : array of shape (n_features,)
+        Maximum number of values per feature.
+
+        .. deprecated:: 0.20
+            The `n_values_` attribute was deprecated in version
+            0.20 and will be removed in 0.22.
+
+    Examples
+    --------
+    Given a dataset with two features, we let the encoder find the unique
+    values per feature and transform the data to a binary one-hot encoding.
+
+    >>> from sklearn.preprocessing import OneHotEncoder
+    >>> enc = OneHotEncoder(handle_unknown='ignore')
+    >>> X = [['Male', 1], ['Female', 3], ['Female', 2]]
+    >>> enc.fit(X)
+    ... # doctest: +ELLIPSIS
+    OneHotEncoder(categorical_features=None, categories=None,
+           dtype=<... 'numpy.float64'>, handle_unknown='ignore',
+           n_values=None, sparse=True)
+
+    >>> enc.categories_
+    [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]
+    >>> enc.transform([['Female', 1], ['Male', 4]]).toarray()
+    array([[1., 0., 1., 0., 0.],
+           [0., 1., 0., 0., 0.]])
+    >>> enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])
+    array([['Male', 1],
+           [None, 2]], dtype=object)
+
+    See also
+    --------
+    sklearn.preprocessing.OrdinalEncoder : performs an ordinal (integer)
+      encoding of the categorical features.
+    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of
+      dictionary items (also handles string-valued features).
+    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot
+      encoding of dictionary items or strings.
+    sklearn.preprocessing.LabelBinarizer : binarizes labels in a one-vs-all
+      fashion.
+    sklearn.preprocessing.MultiLabelBinarizer : transforms between iterable of
+      iterables and a multilabel format, e.g. a (samples x classes) binary
+      matrix indicating the presence of a class label.
+    """
+
+    def __init__(self, n_values=None, categorical_features=None,
+                 categories=None, sparse=True, dtype=np.float64,
+                 handle_unknown='error'):
+        self.categories = categories
+        self.sparse = sparse
+        self.dtype = dtype
+        self.handle_unknown = handle_unknown
+        self.n_values = n_values
+        self.categorical_features = categorical_features
+
+    # Deprecated attributes
+
+    @property
+    @deprecated("The 'active_features_' attribute was deprecated in version "
+                "0.20 and will be removed 0.22.")
+    def active_features_(self):
+        check_is_fitted(self, 'categories_')
+        return self._active_features_
+
+    @property
+    @deprecated("The 'feature_indices_' attribute was deprecated in version "
+                "0.20 and will be removed 0.22.")
+    def feature_indices_(self):
+        check_is_fitted(self, 'categories_')
+        return self._feature_indices_
+
+    @property
+    @deprecated("The 'n_values_' attribute was deprecated in version "
+                "0.20 and will be removed 0.22.")
+    def n_values_(self):
+        check_is_fitted(self, 'categories_')
+        return self._n_values_
+
+    def _handle_deprecations(self, X):
+
+        # internal version of the attributes to handle deprecations
+        self._categories = getattr(self, '_categories', None)
+        self._categorical_features = getattr(self, '_categorical_features',
+                                             None)
+
+        # user manually set the categories or second fit -> never legacy mode
+        if self.categories is not None or self._categories is not None:
+            self._legacy_mode = False
+            if self.categories is not None:
+                self._categories = self.categories
+
+        # categories not set -> infer if we need legacy mode or not
+        elif self.n_values is not None and self.n_values != 'auto':
+            msg = (
+                "Passing 'n_values' is deprecated in version 0.20 and will be "
+                "removed in 0.22. You can use the 'categories' keyword "
+                "instead. 'n_values=n' corresponds to 'categories=[range(n)]'."
+            )
+            warnings.warn(msg, DeprecationWarning)
+            self._legacy_mode = True
+
+        else:  # n_values = 'auto'
+            if self.handle_unknown == 'ignore':
+                # no change in behaviour, no need to raise deprecation warning
+                self._legacy_mode = False
+                self._categories = 'auto'
+                if self.n_values == 'auto':
+                    # user manually specified this
+                    msg = (
+                        "Passing 'n_values' is deprecated in version 0.20 and "
+                        "will be removed in 0.22. n_values='auto' can be "
+                        "replaced with categories='auto'."
+                    )
+                    warnings.warn(msg, DeprecationWarning)
+            else:
+
+                # check if we have integer or categorical input
+                try:
+                    X = check_array(X, dtype=np.int)
+                except ValueError:
+                    self._legacy_mode = False
+                    self._categories = 'auto'
+                else:
+                    msg = (
+                        "The handling of integer data will change in version "
+                        "0.22. Currently, the categories are determined "
+                        "based on the range [0, max(values)], while in the "
+                        "future they will be determined based on the unique "
+                        "values.\nIf you want the future behaviour and "
+                        "silence this warning, you can specify "
+                        "\"categories='auto'\".\n"
+                        "In case you used a LabelEncoder before this "
+                        "OneHotEncoder to convert the categories to integers, "
+                        "then you can now use the OneHotEncoder directly."
+                    )
+                    warnings.warn(msg, FutureWarning)
+                    self._legacy_mode = True
+                    self.n_values = 'auto'
+
+        # if user specified categorical_features -> always use legacy mode
+        if self.categorical_features is not None:
+            if (isinstance(self.categorical_features, six.string_types)
+                    and self.categorical_features == 'all'):
+                warnings.warn(
+                    "The 'categorical_features' keyword is deprecated in "
+                    "version 0.20 and will be removed in 0.22. The passed "
+                    "value of 'all' is the default and can simply be removed.",
+                    DeprecationWarning)
+            else:
+                if self.categories is not None:
+                    raise ValueError(
+                        "The 'categorical_features' keyword is deprecated, "
+                        "and cannot be used together with specifying "
+                        "'categories'.")
+                warnings.warn(
+                    "The 'categorical_features' keyword is deprecated in "
+                    "version 0.20 and will be removed in 0.22. You can "
+                    "use the ColumnTransformer instead.", DeprecationWarning)
+                self._legacy_mode = True
+            self._categorical_features = self.categorical_features
+        else:
+            self._categorical_features = 'all'
+
+    def fit(self, X, y=None):
+        """Fit OneHotEncoder to X.
+
+        Parameters
+        ----------
+        X : array-like, shape [n_samples, n_feature]
+            The data to determine the categories of each feature.
+
+        Returns
+        -------
+        self
+        """
+        if self.handle_unknown not in ('error', 'ignore'):
+            msg = ("handle_unknown should be either 'error' or 'ignore', "
+                   "got {0}.".format(self.handle_unknown))
+            raise ValueError(msg)
+
+        self._handle_deprecations(X)
+
+        if self._legacy_mode:
+            _transform_selected(X, self._legacy_fit_transform, self.dtype,
+                                self._categorical_features,
+                                copy=True)
+            return self
+        else:
+            self._fit(X, handle_unknown=self.handle_unknown)
+            return self
+
+    def _legacy_fit_transform(self, X):
+        """Assumes X contains only categorical features."""
+        dtype = getattr(X, 'dtype', None)
+        X = check_array(X, dtype=np.int)
+        if np.any(X < 0):
+            raise ValueError("X needs to contain only non-negative integers.")
+        n_samples, n_features = X.shape
+        if (isinstance(self.n_values, six.string_types) and
+                self.n_values == 'auto'):
+            n_values = np.max(X, axis=0) + 1
+        elif isinstance(self.n_values, numbers.Integral):
+            if (np.max(X, axis=0) >= self.n_values).any():
+                raise ValueError("Feature out of bounds for n_values=%d"
+                                 % self.n_values)
+            n_values = np.empty(n_features, dtype=np.int)
+            n_values.fill(self.n_values)
+        else:
+            try:
+                n_values = np.asarray(self.n_values, dtype=int)
+            except (ValueError, TypeError):
+                raise TypeError("Wrong type for parameter `n_values`. Expected"
+                                " 'auto', int or array of ints, got %r"
+                                % type(X))
+            if n_values.ndim < 1 or n_values.shape[0] != X.shape[1]:
+                raise ValueError("Shape mismatch: if n_values is an array,"
+                                 " it has to be of shape (n_features,).")
+
+        self._n_values_ = n_values
+        self.categories_ = [np.arange(n_val - 1, dtype=dtype)
+                            for n_val in n_values]
+        n_values = np.hstack([[0], n_values])
+        indices = np.cumsum(n_values)
+        self._feature_indices_ = indices
+
+        column_indices = (X + indices[:-1]).ravel()
+        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),
+                                n_features)
+        data = np.ones(n_samples * n_features)
+        out = sparse.coo_matrix((data, (row_indices, column_indices)),
+                                shape=(n_samples, indices[-1]),
+                                dtype=self.dtype).tocsr()
+
+        if (isinstance(self.n_values, six.string_types) and
+                self.n_values == 'auto'):
+            mask = np.array(out.sum(axis=0)).ravel() != 0
+            active_features = np.where(mask)[0]
+            out = out[:, active_features]
+            self._active_features_ = active_features
+
+            self.categories_ = [
+                np.unique(X[:, i]).astype(dtype) if dtype
+                else np.unique(X[:, i]) for i in range(n_features)]
+
+        return out if self.sparse else out.toarray()
+
+    def fit_transform(self, X, y=None):
+        """Fit OneHotEncoder to X, then transform X.
+
+        Equivalent to self.fit(X).transform(X), but more convenient and more
+        efficient. See fit for the parameters, transform for the return value.
+
+        Parameters
+        ----------
+        X : array-like, shape [n_samples, n_feature]
+            Input array of type int.
+        """
+        if self.handle_unknown not in ('error', 'ignore'):
+            msg = ("handle_unknown should be either 'error' or 'ignore', "
+                   "got {0}.".format(self.handle_unknown))
+            raise ValueError(msg)
+
+        self._handle_deprecations(X)
+
+        if self._legacy_mode:
+            return _transform_selected(
+                X, self._legacy_fit_transform, self.dtype,
+                self._categorical_features, copy=True)
+        else:
+            return self.fit(X).transform(X)
+
+    def _legacy_transform(self, X):
+        """Assumes X contains only categorical features."""
+        X = check_array(X, dtype=np.int)
+        if np.any(X < 0):
+            raise ValueError("X needs to contain only non-negative integers.")
+        n_samples, n_features = X.shape
+
+        indices = self._feature_indices_
+        if n_features != indices.shape[0] - 1:
+            raise ValueError("X has different shape than during fitting."
+                             " Expected %d, got %d."
+                             % (indices.shape[0] - 1, n_features))
+
+        # We use only those categorical features of X that are known using fit.
+        # i.e lesser than n_values_ using mask.
+        # This means, if self.handle_unknown is "ignore", the row_indices and
+        # col_indices corresponding to the unknown categorical feature are
+        # ignored.
+        mask = (X < self._n_values_).ravel()
+        if np.any(~mask):
+            if self.handle_unknown not in ['error', 'ignore']:
+                raise ValueError("handle_unknown should be either error or "
+                                 "unknown got %s" % self.handle_unknown)
+            if self.handle_unknown == 'error':
+                raise ValueError("unknown categorical feature present %s "
+                                 "during transform." % X.ravel()[~mask])
+
+        column_indices = (X + indices[:-1]).ravel()[mask]
+        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),
+                                n_features)[mask]
+        data = np.ones(np.sum(mask))
+        out = sparse.coo_matrix((data, (row_indices, column_indices)),
+                                shape=(n_samples, indices[-1]),
+                                dtype=self.dtype).tocsr()
+        if (isinstance(self.n_values, six.string_types) and
+                self.n_values == 'auto'):
+            out = out[:, self._active_features_]
+
+        return out if self.sparse else out.toarray()
+
+    def _transform_new(self, X):
+        """New implementation assuming categorical input"""
+        X_temp = check_array(X, dtype=None)
+        if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):
+            X = check_array(X, dtype=np.object)
+        else:
+            X = X_temp
+
+        n_samples, n_features = X.shape
+
+        X_int, X_mask = self._transform(X, handle_unknown=self.handle_unknown)
+
+        mask = X_mask.ravel()
+        n_values = [cats.shape[0] for cats in self.categories_]
+        n_values = np.array([0] + n_values)
+        feature_indices = np.cumsum(n_values)
+
+        indices = (X_int + feature_indices[:-1]).ravel()[mask]
+        indptr = X_mask.sum(axis=1).cumsum()
+        indptr = np.insert(indptr, 0, 0)
+        data = np.ones(n_samples * n_features)[mask]
+
+        out = sparse.csr_matrix((data, indices, indptr),
+                                shape=(n_samples, feature_indices[-1]),
+                                dtype=self.dtype)
+        if not self.sparse:
+            return out.toarray()
+        else:
+            return out
+
+    def transform(self, X):
+        """Transform X using one-hot encoding.
+
+        Parameters
+        ----------
+        X : array-like, shape [n_samples, n_features]
+            The data to encode.
+
+        Returns
+        -------
+        X_out : sparse matrix if sparse=True else a 2-d array
+            Transformed input.
+        """
+        if self._legacy_mode:
+            return _transform_selected(X, self._legacy_transform, self.dtype,
+                                       self._categorical_features,
+                                       copy=True)
+        else:
+            return self._transform_new(X)
+
+    def inverse_transform(self, X):
+        """Convert the back data to the original representation.
+
+        In case unknown categories are encountered (all zero's in the
+        one-hot encoding), ``None`` is used to represent this category.
+
+        Parameters
+        ----------
+        X : array-like or sparse matrix, shape [n_samples, n_encoded_features]
+            The transformed data.
+
+        Returns
+        -------
+        X_tr : array-like, shape [n_samples, n_features]
+            Inverse transformed array.
+
+        """
+        # if self._legacy_mode:
+        #     raise ValueError("only supported for categorical features")
+
+        check_is_fitted(self, 'categories_')
+        X = check_array(X, accept_sparse='csr')
+
+        n_samples, _ = X.shape
+        n_features = len(self.categories_)
+        n_transformed_features = sum([len(cats) for cats in self.categories_])
+
+        # validate shape of passed X
+        msg = ("Shape of the passed X data is not correct. Expected {0} "
+               "columns, got {1}.")
+        if X.shape[1] != n_transformed_features:
+            raise ValueError(msg.format(n_transformed_features, X.shape[1]))
+
+        # create resulting array of appropriate dtype
+        dt = np.find_common_type([cat.dtype for cat in self.categories_], [])
+        X_tr = np.empty((n_samples, n_features), dtype=dt)
+
+        j = 0
+        found_unknown = {}
+
+        for i in range(n_features):
+            n_categories = len(self.categories_[i])
+            sub = X[:, j:j + n_categories]
+
+            # for sparse X argmax returns 2D matrix, ensure 1D array
+            labels = np.asarray(_argmax(sub, axis=1)).flatten()
+            X_tr[:, i] = self.categories_[i][labels]
+
+            if self.handle_unknown == 'ignore':
+                # ignored unknown categories: we have a row of all zero's
+                unknown = np.asarray(sub.sum(axis=1) == 0).flatten()
+                if unknown.any():
+                    found_unknown[i] = unknown
+
+            j += n_categories
+
+        # if ignored are found: potentially need to upcast result to
+        # insert None values
+        if found_unknown:
+            if X_tr.dtype != object:
+                X_tr = X_tr.astype(object)
+
+            for idx, mask in found_unknown.items():
+                X_tr[mask, idx] = None
+
+        return X_tr
+
+
+class OrdinalEncoder(_BaseEncoder):
+    """Encode categorical features as an integer array.
+
+    The input to this transformer should be an array-like of integers or
+    strings, denoting the values taken on by categorical (discrete) features.
+    The features are converted to ordinal integers. This results in
+    a single column of integers (0 to n_categories - 1) per feature.
+
+    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.
+
+    Parameters
+    ----------
+    categories : 'auto' or a list of lists/arrays of values.
+        Categories (unique values) per feature:
+
+        - 'auto' : Determine categories automatically from the training data.
+        - list : ``categories[i]`` holds the categories expected in the ith
+          column. The passed categories must be sorted and should not mix
+          strings and numeric values.
+
+        The used categories can be found in the ``categories_`` attribute.
+
+    dtype : number type, default np.float64
+        Desired dtype of output.
+
+    Attributes
+    ----------
+    categories_ : list of arrays
+        The categories of each feature determined during fitting
+        (in order of the features in X and corresponding with the output
+        of ``transform``).
+
+    Examples
+    --------
+    Given a dataset with two features, we let the encoder find the unique
+    values per feature and transform the data to an ordinal encoding.
+
+    >>> from sklearn.preprocessing import OrdinalEncoder
+    >>> enc = OrdinalEncoder()
+    >>> X = [['Male', 1], ['Female', 3], ['Female', 2]]
+    >>> enc.fit(X)
+    ... # doctest: +ELLIPSIS
+    OrdinalEncoder(categories='auto', dtype=<... 'numpy.float64'>)
+    >>> enc.categories_
+    [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]
+    >>> enc.transform([['Female', 3], ['Male', 1]])
+    array([[0., 2.],
+           [1., 0.]])
+
+    >>> enc.inverse_transform([[1, 0], [0, 1]])
+    array([['Male', 1],
+           ['Female', 2]], dtype=object)
+
+    See also
+    --------
+    sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of
+      categorical features.
+    sklearn.preprocessing.LabelEncoder : encodes target labels with values
+      between 0 and n_classes-1.
+    """
+
+    def __init__(self, categories='auto', dtype=np.float64):
+        self.categories = categories
+        self.dtype = dtype
+
+    def fit(self, X, y=None):
+        """Fit the OrdinalEncoder to X.
+
+        Parameters
+        ----------
+        X : array-like, shape [n_samples, n_features]
+            The data to determine the categories of each feature.
+
+        Returns
+        -------
+        self
+
+        """
+        # base classes uses _categories to deal with deprecations in
+        # OneHoteEncoder: can be removed once deprecations are removed
+        self._categories = self.categories
+        self._fit(X)
+
+        return self
+
+    def transform(self, X):
+        """Transform X to ordinal codes.
+
+        Parameters
+        ----------
+        X : array-like, shape [n_samples, n_features]
+            The data to encode.
+
+        Returns
+        -------
+        X_out : sparse matrix or a 2-d array
+            Transformed input.
+
+        """
+        X_int, _ = self._transform(X)
+        return X_int.astype(self.dtype, copy=False)
+
+    def inverse_transform(self, X):
+        """Convert the data back to the original representation.
+
+        Parameters
+        ----------
+        X : array-like or sparse matrix, shape [n_samples, n_encoded_features]
+            The transformed data.
+
+        Returns
+        -------
+        X_tr : array-like, shape [n_samples, n_features]
+            Inverse transformed array.
+
+        """
+        check_is_fitted(self, 'categories_')
+        X = check_array(X, accept_sparse='csr')
+
+        n_samples, _ = X.shape
+        n_features = len(self.categories_)
+
+        # validate shape of passed X
+        msg = ("Shape of the passed X data is not correct. Expected {0} "
+               "columns, got {1}.")
+        if X.shape[1] != n_features:
+            raise ValueError(msg.format(n_features, X.shape[1]))
+
+        # create resulting array of appropriate dtype
+        dt = np.find_common_type([cat.dtype for cat in self.categories_], [])
+        X_tr = np.empty((n_samples, n_features), dtype=dt)
+
+        for i in range(n_features):
+            labels = X[:, i].astype('int64')
+            X_tr[:, i] = self.categories_[i][labels]
+
+        return X_tr
diff --git a/sklearn/preprocessing/data.py b/sklearn/preprocessing/data.py
index fb8f443e9c7a..7c014a07481b 100644
--- a/sklearn/preprocessing/data.py
+++ b/sklearn/preprocessing/data.py
@@ -10,7 +10,6 @@
 from __future__ import division
 
 from itertools import chain, combinations
-import numbers
 import warnings
 from itertools import combinations_with_replacement as combinations_w_r
 from distutils.version import LooseVersion
@@ -25,7 +24,7 @@
 from ..utils import check_array
 from ..utils.extmath import row_norms
 from ..utils.extmath import _incremental_mean_and_var
-from ..utils.fixes import _argmax, nanpercentile
+from ..utils.fixes import boxcox, nanpercentile
 from ..utils.sparsefuncs_fast import (inplace_csr_row_normalize_l1,
                                       inplace_csr_row_normalize_l2)
 from ..utils.sparsefuncs import (inplace_column_scale,
@@ -33,7 +32,8 @@
                                  min_max_axis)
 from ..utils.validation import (check_is_fitted, check_random_state,
                                 FLOAT_DTYPES)
-from .label import LabelEncoder
+
+from ._encoders import OneHotEncoder
 
 
 BOUNDS_THRESHOLD = 1e-7
@@ -126,6 +126,9 @@ def scale(X, axis=0, with_mean=True, with_std=True, copy=True):
 
     To avoid memory copy the caller should pass a CSC matrix.
 
+    NaNs are treated as missing values: disregarded to compute the statistics,
+    and maintained during the data transformation.
+
     For a comparison of the different scalers, transformers, and normalizers,
     see :ref:`examples/preprocessing/plot_all_scaling.py
     <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.
@@ -138,7 +141,7 @@ def scale(X, axis=0, with_mean=True, with_std=True, copy=True):
     """  # noqa
     X = check_array(X, accept_sparse='csc', copy=copy, ensure_2d=False,
                     warn_on_dtype=True, estimator='the scale function',
-                    dtype=FLOAT_DTYPES)
+                    dtype=FLOAT_DTYPES, force_all_finite='allow-nan')
     if sparse.issparse(X):
         if with_mean:
             raise ValueError(
@@ -154,15 +157,15 @@ def scale(X, axis=0, with_mean=True, with_std=True, copy=True):
     else:
         X = np.asarray(X)
         if with_mean:
-            mean_ = np.mean(X, axis)
+            mean_ = np.nanmean(X, axis)
         if with_std:
-            scale_ = np.std(X, axis)
+            scale_ = np.nanstd(X, axis)
         # Xr is a view on the original array that enables easy use of
         # broadcasting on the axis in which we are interested in
         Xr = np.rollaxis(X, axis)
         if with_mean:
             Xr -= mean_
-            mean_1 = Xr.mean(axis=0)
+            mean_1 = np.nanmean(Xr, axis=0)
             # Verify that mean_1 is 'close to zero'. If X contains very
             # large values, mean_1 can also be very large, due to a lack of
             # precision of mean_. In this case, a pre-scaling of the
@@ -179,7 +182,7 @@ def scale(X, axis=0, with_mean=True, with_std=True, copy=True):
             scale_ = _handle_zeros_in_scale(scale_, copy=False)
             Xr /= scale_
             if with_mean:
-                mean_2 = Xr.mean(axis=0)
+                mean_2 = np.nanmean(Xr, axis=0)
                 # If mean_2 is not 'close to zero', it comes from the fact that
                 # scale_ is very small so that mean_2 = mean_1/scale_ > 0, even
                 # if mean_1 was close to zero. The problem is thus essentially
@@ -455,7 +458,7 @@ def minmax_scale(X, feature_range=(0, 1), axis=0, copy=True):
     # Unlike the scaler object, this function allows 1d input.
     # If copy is required, it will be done inside the scaler object.
     X = check_array(X, copy=False, ensure_2d=False, warn_on_dtype=True,
-                    dtype=FLOAT_DTYPES)
+                    dtype=FLOAT_DTYPES, force_all_finite='allow-nan')
     original_ndim = X.ndim
 
     if original_ndim == 1:
@@ -520,27 +523,31 @@ class StandardScaler(BaseEstimator, TransformerMixin):
 
     Attributes
     ----------
-    scale_ : ndarray, shape (n_features,)
-        Per feature relative scaling of the data.
+    scale_ : ndarray or None, shape (n_features,)
+        Per feature relative scaling of the data. Equal to ``None`` when
+        ``with_std=False``.
 
         .. versionadded:: 0.17
            *scale_*
 
-    mean_ : array of floats with shape [n_features]
+    mean_ : ndarray or None, shape (n_features,)
         The mean value for each feature in the training set.
+        Equal to ``None`` when ``with_mean=False``.
 
-    var_ : array of floats with shape [n_features]
+    var_ : ndarray or None, shape (n_features,)
         The variance for each feature in the training set. Used to compute
-        `scale_`
+        `scale_`. Equal to ``None`` when ``with_std=False``.
 
-    n_samples_seen_ : int
-        The number of samples processed by the estimator. Will be reset on
-        new calls to fit, but increments across ``partial_fit`` calls.
+    n_samples_seen_ : int or array, shape (n_features,)
+        The number of samples processed by the estimator for each feature.
+        If there are not missing samples, the ``n_samples_seen`` will be an
+        integer, otherwise it will be an array.
+        Will be reset on new calls to fit, but increments across
+        ``partial_fit`` calls.
 
     Examples
     --------
     >>> from sklearn.preprocessing import StandardScaler
-    >>>
     >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]
     >>> scaler = StandardScaler()
     >>> print(scaler.fit(data))
@@ -564,6 +571,9 @@ class StandardScaler(BaseEstimator, TransformerMixin):
 
     Notes
     -----
+    NaNs are treated as missing values: disregarded in fit, and maintained in
+    transform.
+
     For a comparison of the different scalers, transformers, and normalizers,
     see :ref:`examples/preprocessing/plot_all_scaling.py
     <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.
@@ -626,22 +636,41 @@ def partial_fit(self, X, y=None):
             Ignored
         """
         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,
-                        warn_on_dtype=True, estimator=self, dtype=FLOAT_DTYPES)
+                        warn_on_dtype=True, estimator=self, dtype=FLOAT_DTYPES,
+                        force_all_finite='allow-nan')
 
         # Even in the case of `with_mean=False`, we update the mean anyway
         # This is needed for the incremental computation of the var
         # See incr_mean_variance_axis and _incremental_mean_variance_axis
 
+        # if n_samples_seen_ is an integer (i.e. no missing values), we need to
+        # transform it to a NumPy array of shape (n_features,) required by
+        # incr_mean_variance_axis and _incremental_variance_axis
+        if (hasattr(self, 'n_samples_seen_') and
+                isinstance(self.n_samples_seen_, (int, np.integer))):
+            self.n_samples_seen_ = np.repeat(self.n_samples_seen_,
+                                             X.shape[1]).astype(np.int64)
+
         if sparse.issparse(X):
             if self.with_mean:
                 raise ValueError(
                     "Cannot center sparse matrices: pass `with_mean=False` "
                     "instead. See docstring for motivation and alternatives.")
+
+            sparse_constructor = (sparse.csr_matrix
+                                  if X.format == 'csr' else sparse.csc_matrix)
+            counts_nan = sparse_constructor(
+                        (np.isnan(X.data), X.indices, X.indptr),
+                        shape=X.shape).sum(axis=0).A.ravel()
+
+            if not hasattr(self, 'n_samples_seen_'):
+                self.n_samples_seen_ = (X.shape[0] -
+                                        counts_nan).astype(np.int64)
+
             if self.with_std:
                 # First pass
-                if not hasattr(self, 'n_samples_seen_'):
+                if not hasattr(self, 'scale_'):
                     self.mean_, self.var_ = mean_variance_axis(X, axis=0)
-                    self.n_samples_seen_ = X.shape[0]
                 # Next passes
                 else:
                     self.mean_, self.var_, self.n_samples_seen_ = \
@@ -652,19 +681,34 @@ def partial_fit(self, X, y=None):
             else:
                 self.mean_ = None
                 self.var_ = None
+                if hasattr(self, 'scale_'):
+                    self.n_samples_seen_ += X.shape[0] - counts_nan
         else:
-            # First pass
             if not hasattr(self, 'n_samples_seen_'):
+                self.n_samples_seen_ = np.zeros(X.shape[1], dtype=np.int64)
+
+            # First pass
+            if not hasattr(self, 'scale_'):
                 self.mean_ = .0
-                self.n_samples_seen_ = 0
                 if self.with_std:
                     self.var_ = .0
                 else:
                     self.var_ = None
 
-            self.mean_, self.var_, self.n_samples_seen_ = \
-                _incremental_mean_and_var(X, self.mean_, self.var_,
-                                          self.n_samples_seen_)
+            if not self.with_mean and not self.with_std:
+                self.mean_ = None
+                self.var_ = None
+                self.n_samples_seen_ += X.shape[0] - np.isnan(X).sum(axis=0)
+            else:
+                self.mean_, self.var_, self.n_samples_seen_ = \
+                    _incremental_mean_and_var(X, self.mean_, self.var_,
+                                              self.n_samples_seen_)
+
+        # for backward-compatibility, reduce n_samples_seen_ to an integer
+        # if the number of samples is the same for each feature (i.e. no
+        # missing values)
+        if np.ptp(self.n_samples_seen_) == 0:
+            self.n_samples_seen_ = self.n_samples_seen_[0]
 
         if self.with_std:
             self.scale_ = _handle_zeros_in_scale(np.sqrt(self.var_))
@@ -695,7 +739,8 @@ def transform(self, X, y='deprecated', copy=None):
 
         copy = copy if copy is not None else self.copy
         X = check_array(X, accept_sparse='csr', copy=copy, warn_on_dtype=True,
-                        estimator=self, dtype=FLOAT_DTYPES)
+                        estimator=self, dtype=FLOAT_DTYPES,
+                        force_all_finite='allow-nan')
 
         if sparse.issparse(X):
             if self.with_mean:
@@ -791,6 +836,9 @@ class MaxAbsScaler(BaseEstimator, TransformerMixin):
 
     Notes
     -----
+    NaNs are treated as missing values: disregarded in fit, and maintained in
+    transform.
+
     For a comparison of the different scalers, transformers, and normalizers,
     see :ref:`examples/preprocessing/plot_all_scaling.py
     <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.
@@ -842,13 +890,14 @@ def partial_fit(self, X, y=None):
             Ignored
         """
         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,
-                        estimator=self, dtype=FLOAT_DTYPES)
+                        estimator=self, dtype=FLOAT_DTYPES,
+                        force_all_finite='allow-nan')
 
         if sparse.issparse(X):
-            mins, maxs = min_max_axis(X, axis=0)
+            mins, maxs = min_max_axis(X, axis=0, ignore_nan=True)
             max_abs = np.maximum(np.abs(mins), np.abs(maxs))
         else:
-            max_abs = np.abs(X).max(axis=0)
+            max_abs = np.nanmax(np.abs(X), axis=0)
 
         # First pass
         if not hasattr(self, 'n_samples_seen_'):
@@ -872,7 +921,8 @@ def transform(self, X):
         """
         check_is_fitted(self, 'scale_')
         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,
-                        estimator=self, dtype=FLOAT_DTYPES)
+                        estimator=self, dtype=FLOAT_DTYPES,
+                        force_all_finite='allow-nan')
 
         if sparse.issparse(X):
             inplace_column_scale(X, 1.0 / self.scale_)
@@ -890,7 +940,8 @@ def inverse_transform(self, X):
         """
         check_is_fitted(self, 'scale_')
         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,
-                        estimator=self, dtype=FLOAT_DTYPES)
+                        estimator=self, dtype=FLOAT_DTYPES,
+                        force_all_finite='allow-nan')
 
         if sparse.issparse(X):
             inplace_column_scale(X, self.scale_)
@@ -928,6 +979,9 @@ def maxabs_scale(X, axis=0, copy=True):
 
     Notes
     -----
+    NaNs are treated as missing values: disregarded to compute the statistics,
+    and maintained during the data transformation.
+
     For a comparison of the different scalers, transformers, and normalizers,
     see :ref:`examples/preprocessing/plot_all_scaling.py
     <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.
@@ -936,7 +990,8 @@ def maxabs_scale(X, axis=0, copy=True):
 
     # If copy is required, it will be done inside the scaler object.
     X = check_array(X, accept_sparse=('csr', 'csc'), copy=False,
-                    ensure_2d=False, dtype=FLOAT_DTYPES)
+                    ensure_2d=False, dtype=FLOAT_DTYPES,
+                    force_all_finite='allow-nan')
     original_ndim = X.ndim
 
     if original_ndim == 1:
@@ -1825,302 +1880,6 @@ def add_dummy_feature(X, value=1.0):
         return np.hstack((np.ones((n_samples, 1)) * value, X))
 
 
-def _transform_selected(X, transform, selected="all", copy=True):
-    """Apply a transform function to portion of selected features
-
-    Parameters
-    ----------
-    X : {array-like, sparse matrix}, shape [n_samples, n_features]
-        Dense array or sparse matrix.
-
-    transform : callable
-        A callable transform(X) -> X_transformed
-
-    copy : boolean, optional
-        Copy X even if it could be avoided.
-
-    selected: "all" or array of indices or mask
-        Specify which features to apply the transform to.
-
-    Returns
-    -------
-    X : array or sparse matrix, shape=(n_samples, n_features_new)
-    """
-    X = check_array(X, accept_sparse='csc', copy=copy, dtype=FLOAT_DTYPES)
-
-    if isinstance(selected, six.string_types) and selected == "all":
-        return transform(X)
-
-    if len(selected) == 0:
-        return X
-
-    n_features = X.shape[1]
-    ind = np.arange(n_features)
-    sel = np.zeros(n_features, dtype=bool)
-    sel[np.asarray(selected)] = True
-    not_sel = np.logical_not(sel)
-    n_selected = np.sum(sel)
-
-    if n_selected == 0:
-        # No features selected.
-        return X
-    elif n_selected == n_features:
-        # All features selected.
-        return transform(X)
-    else:
-        X_sel = transform(X[:, ind[sel]])
-        X_not_sel = X[:, ind[not_sel]]
-
-        if sparse.issparse(X_sel) or sparse.issparse(X_not_sel):
-            return sparse.hstack((X_sel, X_not_sel))
-        else:
-            return np.hstack((X_sel, X_not_sel))
-
-
-class OneHotEncoder(BaseEstimator, TransformerMixin):
-    """Encode categorical integer features using a one-hot aka one-of-K scheme.
-
-    The input to this transformer should be a matrix of integers, denoting
-    the values taken on by categorical (discrete) features. The output will be
-    a sparse matrix where each column corresponds to one possible value of one
-    feature. It is assumed that input features take on values in the range
-    [0, n_values). For an encoder based on the unique values of the input
-    features of any type, see the
-    :class:`~sklearn.preprocessing.CategoricalEncoder`.
-
-    This encoding is needed for feeding categorical data to many scikit-learn
-    estimators, notably linear models and SVMs with the standard kernels.
-
-    Note: a one-hot encoding of y labels should use a LabelBinarizer
-    instead.
-
-    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.
-
-    Parameters
-    ----------
-    n_values : 'auto', int or array of ints
-        Number of values per feature.
-
-        - 'auto' : determine value range from training data.
-        - int : number of categorical values per feature.
-                Each feature value should be in ``range(n_values)``
-        - array : ``n_values[i]`` is the number of categorical values in
-                  ``X[:, i]``. Each feature value should be
-                  in ``range(n_values[i])``
-
-    categorical_features : "all" or array of indices or mask
-        Specify what features are treated as categorical.
-
-        - 'all' (default): All features are treated as categorical.
-        - array of indices: Array of categorical feature indices.
-        - mask: Array of length n_features and with dtype=bool.
-
-        Non-categorical features are always stacked to the right of the matrix.
-
-    dtype : number type, default=np.float
-        Desired dtype of output.
-
-    sparse : boolean, default=True
-        Will return sparse matrix if set True else will return an array.
-
-    handle_unknown : str, 'error' or 'ignore'
-        Whether to raise an error or ignore if a unknown categorical feature is
-        present during transform.
-
-    Attributes
-    ----------
-    active_features_ : array
-        Indices for active features, meaning values that actually occur
-        in the training set. Only available when n_values is ``'auto'``.
-
-    feature_indices_ : array of shape (n_features,)
-        Indices to feature ranges.
-        Feature ``i`` in the original data is mapped to features
-        from ``feature_indices_[i]`` to ``feature_indices_[i+1]``
-        (and then potentially masked by `active_features_` afterwards)
-
-    n_values_ : array of shape (n_features,)
-        Maximum number of values per feature.
-
-    Examples
-    --------
-    Given a dataset with three features and four samples, we let the encoder
-    find the maximum value per feature and transform the data to a binary
-    one-hot encoding.
-
-    >>> from sklearn.preprocessing import OneHotEncoder
-    >>> enc = OneHotEncoder()
-    >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], \
-[1, 0, 2]])  # doctest: +ELLIPSIS
-    OneHotEncoder(categorical_features='all', dtype=<... 'numpy.float64'>,
-           handle_unknown='error', n_values='auto', sparse=True)
-    >>> enc.n_values_
-    array([2, 3, 4])
-    >>> enc.feature_indices_
-    array([0, 2, 5, 9])
-    >>> enc.transform([[0, 1, 1]]).toarray()
-    array([[1., 0., 0., 1., 0., 0., 1., 0., 0.]])
-
-    See also
-    --------
-    sklearn.preprocessing.CategoricalEncoder : performs a one-hot or ordinal
-      encoding of all features (also handles string-valued features). This
-      encoder derives the categories based on the unique values in each
-      feature.
-    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of
-      dictionary items (also handles string-valued features).
-    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot
-      encoding of dictionary items or strings.
-    sklearn.preprocessing.LabelBinarizer : binarizes labels in a one-vs-all
-      fashion.
-    sklearn.preprocessing.MultiLabelBinarizer : transforms between iterable of
-      iterables and a multilabel format, e.g. a (samples x classes) binary
-      matrix indicating the presence of a class label.
-    sklearn.preprocessing.LabelEncoder : encodes labels with values between 0
-      and n_classes-1.
-    """
-    def __init__(self, n_values="auto", categorical_features="all",
-                 dtype=np.float64, sparse=True, handle_unknown='error'):
-        self.n_values = n_values
-        self.categorical_features = categorical_features
-        self.dtype = dtype
-        self.sparse = sparse
-        self.handle_unknown = handle_unknown
-
-    def fit(self, X, y=None):
-        """Fit OneHotEncoder to X.
-
-        Parameters
-        ----------
-        X : array-like, shape [n_samples, n_feature]
-            Input array of type int.
-
-        Returns
-        -------
-        self
-        """
-        self.fit_transform(X)
-        return self
-
-    def _fit_transform(self, X):
-        """Assumes X contains only categorical features."""
-        X = check_array(X, dtype=np.int)
-        if np.any(X < 0):
-            raise ValueError("X needs to contain only non-negative integers.")
-        n_samples, n_features = X.shape
-        if (isinstance(self.n_values, six.string_types) and
-                self.n_values == 'auto'):
-            n_values = np.max(X, axis=0) + 1
-        elif isinstance(self.n_values, numbers.Integral):
-            if (np.max(X, axis=0) >= self.n_values).any():
-                raise ValueError("Feature out of bounds for n_values=%d"
-                                 % self.n_values)
-            n_values = np.empty(n_features, dtype=np.int)
-            n_values.fill(self.n_values)
-        else:
-            try:
-                n_values = np.asarray(self.n_values, dtype=int)
-            except (ValueError, TypeError):
-                raise TypeError("Wrong type for parameter `n_values`. Expected"
-                                " 'auto', int or array of ints, got %r"
-                                % type(X))
-            if n_values.ndim < 1 or n_values.shape[0] != X.shape[1]:
-                raise ValueError("Shape mismatch: if n_values is an array,"
-                                 " it has to be of shape (n_features,).")
-
-        self.n_values_ = n_values
-        n_values = np.hstack([[0], n_values])
-        indices = np.cumsum(n_values)
-        self.feature_indices_ = indices
-
-        column_indices = (X + indices[:-1]).ravel()
-        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),
-                                n_features)
-        data = np.ones(n_samples * n_features)
-        out = sparse.coo_matrix((data, (row_indices, column_indices)),
-                                shape=(n_samples, indices[-1]),
-                                dtype=self.dtype).tocsr()
-
-        if (isinstance(self.n_values, six.string_types) and
-                self.n_values == 'auto'):
-            mask = np.array(out.sum(axis=0)).ravel() != 0
-            active_features = np.where(mask)[0]
-            out = out[:, active_features]
-            self.active_features_ = active_features
-
-        return out if self.sparse else out.toarray()
-
-    def fit_transform(self, X, y=None):
-        """Fit OneHotEncoder to X, then transform X.
-
-        Equivalent to self.fit(X).transform(X), but more convenient and more
-        efficient. See fit for the parameters, transform for the return value.
-
-        Parameters
-        ----------
-        X : array-like, shape [n_samples, n_feature]
-            Input array of type int.
-        """
-        return _transform_selected(X, self._fit_transform,
-                                   self.categorical_features, copy=True)
-
-    def _transform(self, X):
-        """Assumes X contains only categorical features."""
-        X = check_array(X, dtype=np.int)
-        if np.any(X < 0):
-            raise ValueError("X needs to contain only non-negative integers.")
-        n_samples, n_features = X.shape
-
-        indices = self.feature_indices_
-        if n_features != indices.shape[0] - 1:
-            raise ValueError("X has different shape than during fitting."
-                             " Expected %d, got %d."
-                             % (indices.shape[0] - 1, n_features))
-
-        # We use only those categorical features of X that are known using fit.
-        # i.e lesser than n_values_ using mask.
-        # This means, if self.handle_unknown is "ignore", the row_indices and
-        # col_indices corresponding to the unknown categorical feature are
-        # ignored.
-        mask = (X < self.n_values_).ravel()
-        if np.any(~mask):
-            if self.handle_unknown not in ['error', 'ignore']:
-                raise ValueError("handle_unknown should be either error or "
-                                 "unknown got %s" % self.handle_unknown)
-            if self.handle_unknown == 'error':
-                raise ValueError("unknown categorical feature present %s "
-                                 "during transform." % X.ravel()[~mask])
-
-        column_indices = (X + indices[:-1]).ravel()[mask]
-        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),
-                                n_features)[mask]
-        data = np.ones(np.sum(mask))
-        out = sparse.coo_matrix((data, (row_indices, column_indices)),
-                                shape=(n_samples, indices[-1]),
-                                dtype=self.dtype).tocsr()
-        if (isinstance(self.n_values, six.string_types) and
-                self.n_values == 'auto'):
-            out = out[:, self.active_features_]
-
-        return out if self.sparse else out.toarray()
-
-    def transform(self, X):
-        """Transform X using one-hot encoding.
-
-        Parameters
-        ----------
-        X : array-like, shape [n_samples, n_features]
-            Input array of type int.
-
-        Returns
-        -------
-        X_out : sparse matrix if sparse=True else a 2-d array, dtype=int
-            Transformed input.
-        """
-        return _transform_selected(X, self._transform,
-                                   self.categorical_features, copy=True)
-
-
 class QuantileTransformer(BaseEstimator, TransformerMixin):
     """Transform features using quantiles information.
 
@@ -2355,7 +2114,8 @@ def _transform_col(self, X_col, quantiles, inverse):
             lower_bound_y = quantiles[0]
             upper_bound_y = quantiles[-1]
             #  for inverse transform, match a uniform PDF
-            X_col = output_distribution.cdf(X_col)
+            with np.errstate(invalid='ignore'):  # hide NaN comparison warnings
+                X_col = output_distribution.cdf(X_col)
         # find index for lower and higher bounds
         with np.errstate(invalid='ignore'):  # hide NaN comparison warnings
             lower_bounds_idx = (X_col - BOUNDS_THRESHOLD <
@@ -2680,6 +2440,9 @@ class PowerTransformer(BaseEstimator, TransformerMixin):
 
     Notes
     -----
+    NaNs are treated as missing values: disregarded in fit, and maintained in
+    transform.
+
     For a comparison of the different scalers, transformers, and normalizers,
     see :ref:`examples/preprocessing/plot_all_scaling.py
     <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.
@@ -2719,7 +2482,10 @@ def fit(self, X, y=None):
         transformed = []
 
         for col in X.T:
-            col_trans, lmbda = stats.boxcox(col, lmbda=None)
+            # the computation of lambda is influenced by NaNs and we need to
+            # get rid of them to compute them.
+            _, lmbda = stats.boxcox(col[~np.isnan(col)], lmbda=None)
+            col_trans = boxcox(col, lmbda)
             self.lambdas_.append(lmbda)
             transformed.append(col_trans)
 
@@ -2744,7 +2510,7 @@ def transform(self, X):
         X = self._check_input(X, check_positive=True, check_shape=True)
 
         for i, lmbda in enumerate(self.lambdas_):
-            X[:, i] = stats.boxcox(X[:, i], lmbda=lmbda)
+            X[:, i] = boxcox(X[:, i], lmbda)
 
         if self.standardize:
             X = self._scaler.transform(X)
@@ -2799,11 +2565,16 @@ def _check_input(self, X, check_positive=False, check_shape=False,
         check_method : bool
             If True, check that the transformation method is valid.
         """
-        X = check_array(X, ensure_2d=True, dtype=FLOAT_DTYPES, copy=self.copy)
+        X = check_array(X, ensure_2d=True, dtype=FLOAT_DTYPES, copy=self.copy,
+                        force_all_finite='allow-nan')
 
-        if check_positive and self.method == 'box-cox' and np.any(X <= 0):
-            raise ValueError("The Box-Cox transformation can only be applied "
-                             "to strictly positive data")
+        with np.warnings.catch_warnings():
+            np.warnings.filterwarnings(
+                'ignore', r'All-NaN (slice|axis) encountered')
+            if (check_positive and self.method == 'box-cox' and
+                    np.nanmin(X) <= 0):
+                raise ValueError("The Box-Cox transformation can only be "
+                                 "applied to strictly positive data")
 
         if check_shape and not X.shape[1] == len(self.lambdas_):
             raise ValueError("Input data has a different number of features "
@@ -2873,6 +2644,9 @@ def power_transform(X, method='box-cox', standardize=True, copy=True):
 
     Notes
     -----
+    NaNs are treated as missing values: disregarded to compute the statistics,
+    and maintained during the data transformation.
+
     For a comparison of the different scalers, transformers, and normalizers,
     see :ref:`examples/preprocessing/plot_all_scaling.py
     <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.
@@ -2886,297 +2660,15 @@ def power_transform(X, method='box-cox', standardize=True, copy=True):
     return pt.fit_transform(X)
 
 
-class CategoricalEncoder(BaseEstimator, TransformerMixin):
-    """Encode categorical features as a numeric array.
-
-    The input to this transformer should be an array-like of integers or
-    strings, denoting the values taken on by categorical (discrete) features.
-    The features can be encoded using a one-hot (aka one-of-K or dummy)
-    encoding scheme (``encoding='onehot'``, the default) or converted
-    to ordinal integers (``encoding='ordinal'``).
-
-    This encoding is needed for feeding categorical data to many scikit-learn
-    estimators, notably linear models and SVMs with the standard kernels.
-
-    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.
-
-    Parameters
-    ----------
-    encoding : str, 'onehot', 'onehot-dense' or 'ordinal'
-        The type of encoding to use (default is 'onehot'):
-
-        - 'onehot': encode the features using a one-hot aka one-of-K scheme
-          (or also called 'dummy' encoding). This creates a binary column for
-          each category and returns a sparse matrix.
-        - 'onehot-dense': the same as 'onehot' but returns a dense array
-          instead of a sparse matrix.
-        - 'ordinal': encode the features as ordinal integers. This results in
-          a single column of integers (0 to n_categories - 1) per feature.
-
-    categories : 'auto' or a list of lists/arrays of values.
-        Categories (unique values) per feature:
-
-        - 'auto' : Determine categories automatically from the training data.
-        - list : ``categories[i]`` holds the categories expected in the ith
-          column. The passed categories must be sorted and should not mix
-          strings and numeric values.
-
-        The used categories can be found in the ``categories_`` attribute.
-
-    dtype : number type, default np.float64
-        Desired dtype of output.
-
-    handle_unknown : 'error' (default) or 'ignore'
-        Whether to raise an error or ignore if a unknown categorical feature is
-        present during transform (default is to raise). When this parameter
-        is set to 'ignore' and an unknown category is encountered during
-        transform, the resulting one-hot encoded columns for this feature
-        will be all zeros. In the inverse transform, an unknown category
-        will be denoted as None.
-        Ignoring unknown categories is not supported for
-        ``encoding='ordinal'``.
-
-    Attributes
-    ----------
-    categories_ : list of arrays
-        The categories of each feature determined during fitting
-        (in order corresponding with output of ``transform``).
-
-    Examples
-    --------
-    Given a dataset with two features, we let the encoder find the unique
-    values per feature and transform the data to a binary one-hot encoding.
-
-    >>> from sklearn.preprocessing import CategoricalEncoder
-    >>> enc = CategoricalEncoder(handle_unknown='ignore')
-    >>> X = [['Male', 1], ['Female', 3], ['Female', 2]]
-    >>> enc.fit(X)
-    ... # doctest: +ELLIPSIS
-    CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>,
-              encoding='onehot', handle_unknown='ignore')
-    >>> enc.categories_
-    [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]
-    >>> enc.transform([['Female', 1], ['Male', 4]]).toarray()
-    array([[1., 0., 1., 0., 0.],
-           [0., 1., 0., 0., 0.]])
-    >>> enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])
-    array([['Male', 1],
-           [None, 2]], dtype=object)
-
-    See also
-    --------
-    sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of
-      integer ordinal features. The ``OneHotEncoder assumes`` that input
-      features take on values in the range ``[0, max(feature)]`` instead of
-      using the unique values.
-    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of
-      dictionary items (also handles string-valued features).
-    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot
-      encoding of dictionary items or strings.
+class CategoricalEncoder:
+    """
+    CategoricalEncoder briefly existed in 0.20dev. Its functionality
+    has been rolled into the OneHotEncoder and OrdinalEncoder.
+    This stub will be removed in version 0.21.
     """
 
-    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,
-                 handle_unknown='error'):
-        self.encoding = encoding
-        self.categories = categories
-        self.dtype = dtype
-        self.handle_unknown = handle_unknown
-
-    def fit(self, X, y=None):
-        """Fit the CategoricalEncoder to X.
-
-        Parameters
-        ----------
-        X : array-like, shape [n_samples, n_features]
-            The data to determine the categories of each feature.
-
-        Returns
-        -------
-        self
-
-        """
-        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:
-            template = ("encoding should be either 'onehot', 'onehot-dense' "
-                        "or 'ordinal', got %s")
-            raise ValueError(template % self.handle_unknown)
-
-        if self.handle_unknown not in ['error', 'ignore']:
-            template = ("handle_unknown should be either 'error' or "
-                        "'ignore', got %s")
-            raise ValueError(template % self.handle_unknown)
-
-        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':
-            raise ValueError("handle_unknown='ignore' is not supported for"
-                             " encoding='ordinal'")
-
-        if self.categories != 'auto':
-            for cats in self.categories:
-                if not np.all(np.sort(cats) == np.array(cats)):
-                    raise ValueError("Unsorted categories are not yet "
-                                     "supported")
-
-        X_temp = check_array(X, dtype=None)
-        if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):
-            X = check_array(X, dtype=np.object)
-        else:
-            X = X_temp
-
-        n_samples, n_features = X.shape
-
-        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]
-
-        for i in range(n_features):
-            le = self._label_encoders_[i]
-            Xi = X[:, i]
-            if self.categories == 'auto':
-                le.fit(Xi)
-            else:
-                if self.handle_unknown == 'error':
-                    valid_mask = np.in1d(Xi, self.categories[i])
-                    if not np.all(valid_mask):
-                        diff = np.unique(Xi[~valid_mask])
-                        msg = ("Found unknown categories {0} in column {1}"
-                               " during fit".format(diff, i))
-                        raise ValueError(msg)
-                le.classes_ = np.array(self.categories[i])
-
-        self.categories_ = [le.classes_ for le in self._label_encoders_]
-
-        return self
-
-    def transform(self, X):
-        """Transform X using specified encoding scheme.
-
-        Parameters
-        ----------
-        X : array-like, shape [n_samples, n_features]
-            The data to encode.
-
-        Returns
-        -------
-        X_out : sparse matrix or a 2-d array
-            Transformed input.
-
-        """
-        X_temp = check_array(X, dtype=None)
-        if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):
-            X = check_array(X, dtype=np.object)
-        else:
-            X = X_temp
-
-        n_samples, n_features = X.shape
-        X_int = np.zeros_like(X, dtype=np.int)
-        X_mask = np.ones_like(X, dtype=np.bool)
-
-        for i in range(n_features):
-            Xi = X[:, i]
-            valid_mask = np.in1d(Xi, self.categories_[i])
-
-            if not np.all(valid_mask):
-                if self.handle_unknown == 'error':
-                    diff = np.unique(X[~valid_mask, i])
-                    msg = ("Found unknown categories {0} in column {1}"
-                           " during transform".format(diff, i))
-                    raise ValueError(msg)
-                else:
-                    # Set the problematic rows to an acceptable value and
-                    # continue `The rows are marked `X_mask` and will be
-                    # removed later.
-                    X_mask[:, i] = valid_mask
-                    Xi = Xi.copy()
-                    Xi[~valid_mask] = self.categories_[i][0]
-            X_int[:, i] = self._label_encoders_[i].transform(Xi)
-
-        if self.encoding == 'ordinal':
-            return X_int.astype(self.dtype, copy=False)
-
-        mask = X_mask.ravel()
-        n_values = [cats.shape[0] for cats in self.categories_]
-        n_values = np.array([0] + n_values)
-        feature_indices = np.cumsum(n_values)
-
-        indices = (X_int + feature_indices[:-1]).ravel()[mask]
-        indptr = X_mask.sum(axis=1).cumsum()
-        indptr = np.insert(indptr, 0, 0)
-        data = np.ones(n_samples * n_features)[mask]
-
-        out = sparse.csr_matrix((data, indices, indptr),
-                                shape=(n_samples, feature_indices[-1]),
-                                dtype=self.dtype)
-        if self.encoding == 'onehot-dense':
-            return out.toarray()
-        else:
-            return out
-
-    def inverse_transform(self, X):
-        """Convert back the data to the original representation.
-
-        In case unknown categories are encountered (all zero's in the
-        one-hot encoding), ``None`` is used to represent this category.
-
-        Parameters
-        ----------
-        X : array-like or sparse matrix, shape [n_samples, n_encoded_features]
-            The transformed data.
-
-        Returns
-        -------
-        X_tr : array-like, shape [n_samples, n_features]
-            Inverse transformed array.
-
-        """
-        check_is_fitted(self, 'categories_')
-        X = check_array(X, accept_sparse='csr')
-
-        n_samples, _ = X.shape
-        n_features = len(self.categories_)
-        n_transformed_features = sum([len(cats) for cats in self.categories_])
-
-        # validate shape of passed X
-        msg = ("Shape of the passed X data is not correct. Expected {0} "
-               "columns, got {1}.")
-        if self.encoding == 'ordinal' and X.shape[1] != n_features:
-            raise ValueError(msg.format(n_features, X.shape[1]))
-        elif (self.encoding.startswith('onehot')
-                and X.shape[1] != n_transformed_features):
-            raise ValueError(msg.format(n_transformed_features, X.shape[1]))
-
-        # create resulting array of appropriate dtype
-        dt = np.find_common_type([cat.dtype for cat in self.categories_], [])
-        X_tr = np.empty((n_samples, n_features), dtype=dt)
-
-        if self.encoding == 'ordinal':
-            for i in range(n_features):
-                labels = X[:, i].astype('int64')
-                X_tr[:, i] = self.categories_[i][labels]
-
-        else:  # encoding == 'onehot' / 'onehot-dense'
-            j = 0
-            found_unknown = {}
-
-            for i in range(n_features):
-                n_categories = len(self.categories_[i])
-                sub = X[:, j:j + n_categories]
-
-                # for sparse X argmax returns 2D matrix, ensure 1D array
-                labels = np.asarray(_argmax(sub, axis=1)).flatten()
-                X_tr[:, i] = self.categories_[i][labels]
-
-                if self.handle_unknown == 'ignore':
-                    # ignored unknown categories: we have a row of all zero's
-                    unknown = np.asarray(sub.sum(axis=1) == 0).flatten()
-                    if unknown.any():
-                        found_unknown[i] = unknown
-
-                j += n_categories
-
-            # if ignored are found: potentially need to upcast result to
-            # insert None values
-            if found_unknown:
-                if X_tr.dtype != object:
-                    X_tr = X_tr.astype(object)
-
-                for idx, mask in found_unknown.items():
-                    X_tr[mask, idx] = None
-
-        return X_tr
+    def __init__(*args, **kwargs):
+        raise RuntimeError(
+            "CategoricalEncoder briefly existed in 0.20dev. Its functionality "
+            "has been rolled into the OneHotEncoder and OrdinalEncoder. "
+            "This stub will be removed in version 0.21.")
diff --git a/sklearn/preprocessing/label.py b/sklearn/preprocessing/label.py
index 7f95a1426c87..043067fa37a8 100644
--- a/sklearn/preprocessing/label.py
+++ b/sklearn/preprocessing/label.py
@@ -16,7 +16,7 @@
 
 from ..base import BaseEstimator, TransformerMixin
 
-from ..utils.fixes import sparse_min_max
+from ..utils.sparsefuncs import min_max_axis
 from ..utils import column_or_1d
 from ..utils.validation import check_array
 from ..utils.validation import check_is_fitted
@@ -77,7 +77,7 @@ class LabelEncoder(BaseEstimator, TransformerMixin):
 
     See also
     --------
-    sklearn.preprocessing.CategoricalEncoder : encode categorical features
+    sklearn.preprocessing.OrdinalEncoder : encode categorical features
         using a one-hot or ordinal encoding scheme.
     """
 
@@ -251,7 +251,7 @@ class LabelBinarizer(BaseEstimator, TransformerMixin):
     --------
     label_binarize : function to perform the transform operation of
         LabelBinarizer with fixed classes.
-    sklearn.preprocessing.OneHotEncoder : encode categorical integer features
+    sklearn.preprocessing.OneHotEncoder : encode categorical features
         using a one-hot aka one-of-K scheme.
     """
 
@@ -567,7 +567,7 @@ def _inverse_binarize_multiclass(y, classes):
         y = y.tocsr()
         n_samples, n_outputs = y.shape
         outputs = np.arange(n_outputs)
-        row_max = sparse_min_max(y, 1)[1]
+        row_max = min_max_axis(y, 1)[1]
         row_nnz = np.diff(y.indptr)
 
         y_data_repeated_max = np.repeat(row_max, row_nnz)
@@ -682,7 +682,7 @@ class MultiLabelBinarizer(BaseEstimator, TransformerMixin):
 
     See also
     --------
-    sklearn.preprocessing.OneHotEncoder : encode categorical integer features
+    sklearn.preprocessing.OneHotEncoder : encode categorical features
         using a one-hot aka one-of-K scheme.
     """
 
diff --git a/sklearn/preprocessing/tests/test_common.py b/sklearn/preprocessing/tests/test_common.py
index 1488ceaba12c..4abc73d6ef44 100644
--- a/sklearn/preprocessing/tests/test_common.py
+++ b/sklearn/preprocessing/tests/test_common.py
@@ -8,8 +8,17 @@
 
 from sklearn.base import clone
 
-from sklearn.preprocessing import QuantileTransformer
+from sklearn.preprocessing import maxabs_scale
+from sklearn.preprocessing import minmax_scale
+from sklearn.preprocessing import scale
+from sklearn.preprocessing import power_transform
+from sklearn.preprocessing import quantile_transform
+
+from sklearn.preprocessing import MaxAbsScaler
 from sklearn.preprocessing import MinMaxScaler
+from sklearn.preprocessing import StandardScaler
+from sklearn.preprocessing import PowerTransformer
+from sklearn.preprocessing import QuantileTransformer
 
 from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_allclose
@@ -23,17 +32,23 @@ def _get_valid_samples_by_column(X, col):
 
 
 @pytest.mark.parametrize(
-    "est, support_sparse",
-    [(MinMaxScaler(), False),
-     (QuantileTransformer(n_quantiles=10, random_state=42), True)]
+    "est, func, support_sparse, strictly_positive",
+    [(MaxAbsScaler(), maxabs_scale, True, False),
+     (MinMaxScaler(), minmax_scale, False, False),
+     (StandardScaler(), scale, False, False),
+     (StandardScaler(with_mean=False), scale, True, False),
+     (PowerTransformer(), power_transform, False, True),
+     (QuantileTransformer(n_quantiles=10), quantile_transform, True, False)]
 )
-def test_missing_value_handling(est, support_sparse):
+def test_missing_value_handling(est, func, support_sparse, strictly_positive):
     # check that the preprocessing method let pass nan
     rng = np.random.RandomState(42)
     X = iris.data.copy()
     n_missing = 50
     X[rng.randint(X.shape[0], size=n_missing),
       rng.randint(X.shape[1], size=n_missing)] = np.nan
+    if strictly_positive:
+        X += np.nanmin(X) + 0.1
     X_train, X_test = train_test_split(X, random_state=1)
     # sanity check
     assert not np.all(np.isnan(X_train), axis=0).any()
@@ -41,10 +56,21 @@ def test_missing_value_handling(est, support_sparse):
     assert np.any(np.isnan(X_test), axis=0).all()
     X_test[:, 0] = np.nan  # make sure this boundary case is tested
 
-    Xt = est.fit(X_train).transform(X_test)
+    with pytest.warns(None) as records:
+        Xt = est.fit(X_train).transform(X_test)
+    # ensure no warnings are raised
+    assert len(records) == 0
     # missing values should still be missing, and only them
     assert_array_equal(np.isnan(Xt), np.isnan(X_test))
 
+    # check that the function leads to the same results as the class
+    with pytest.warns(None) as records:
+        Xt_class = est.transform(X_train)
+    assert len(records) == 0
+    Xt_func = func(X_train, **est.get_params())
+    assert_array_equal(np.isnan(Xt_func), np.isnan(Xt_class))
+    assert_allclose(Xt_func[~np.isnan(Xt_func)], Xt_class[~np.isnan(Xt_class)])
+
     # check that the inverse transform keep NaN
     Xt_inv = est.inverse_transform(Xt)
     assert_array_equal(np.isnan(Xt_inv), np.isnan(X_test))
@@ -56,8 +82,10 @@ def test_missing_value_handling(est, support_sparse):
         # train only on non-NaN
         est.fit(_get_valid_samples_by_column(X_train, i))
         # check transforming with NaN works even when training without NaN
-        Xt_col = est.transform(X_test[:, [i]])
-        assert_array_equal(Xt_col, Xt[:, [i]])
+        with pytest.warns(None) as records:
+            Xt_col = est.transform(X_test[:, [i]])
+        assert len(records) == 0
+        assert_allclose(Xt_col, Xt[:, [i]])
         # check non-NaN is handled as before - the 1st column is all nan
         if not np.isnan(X_test[:, i]).all():
             Xt_col_nonan = est.transform(
@@ -69,15 +97,23 @@ def test_missing_value_handling(est, support_sparse):
         est_dense = clone(est)
         est_sparse = clone(est)
 
-        Xt_dense = est_dense.fit(X_train).transform(X_test)
-        Xt_inv_dense = est_dense.inverse_transform(Xt_dense)
+        with pytest.warns(None) as records:
+            Xt_dense = est_dense.fit(X_train).transform(X_test)
+            Xt_inv_dense = est_dense.inverse_transform(Xt_dense)
+        assert len(records) == 0
         for sparse_constructor in (sparse.csr_matrix, sparse.csc_matrix,
                                    sparse.bsr_matrix, sparse.coo_matrix,
                                    sparse.dia_matrix, sparse.dok_matrix,
                                    sparse.lil_matrix):
             # check that the dense and sparse inputs lead to the same results
-            Xt_sparse = (est_sparse.fit(sparse_constructor(X_train))
-                         .transform(sparse_constructor(X_test)))
-            assert_allclose(Xt_sparse.A, Xt_dense)
-            Xt_inv_sparse = est_sparse.inverse_transform(Xt_sparse)
-            assert_allclose(Xt_inv_sparse.A, Xt_inv_dense)
+            # precompute the matrix to avoid catching side warnings
+            X_train_sp = sparse_constructor(X_train)
+            X_test_sp = sparse_constructor(X_test)
+            with pytest.warns(None) as records:
+                Xt_sp = est_sparse.fit(X_train_sp).transform(X_test_sp)
+            assert len(records) == 0
+            assert_allclose(Xt_sp.A, Xt_dense)
+            with pytest.warns(None) as records:
+                Xt_inv_sp = est_sparse.inverse_transform(Xt_sp)
+            assert len(records) == 0
+            assert_allclose(Xt_inv_sp.A, Xt_inv_dense)
diff --git a/sklearn/preprocessing/tests/test_data.py b/sklearn/preprocessing/tests/test_data.py
index e3bf4096750d..f90fbee278c0 100644
--- a/sklearn/preprocessing/tests/test_data.py
+++ b/sklearn/preprocessing/tests/test_data.py
@@ -7,6 +7,7 @@
 
 import warnings
 import re
+import itertools
 
 import numpy as np
 import numpy.linalg as la
@@ -32,18 +33,15 @@
 from sklearn.utils.testing import assert_warns_message
 from sklearn.utils.testing import assert_no_warnings
 from sklearn.utils.testing import assert_allclose
+from sklearn.utils.testing import assert_allclose_dense_sparse
 from sklearn.utils.testing import skip_if_32bit
-from sklearn.utils.testing import SkipTest
 
 from sklearn.utils.sparsefuncs import mean_variance_axis
-from sklearn.preprocessing.data import _transform_selected
 from sklearn.preprocessing.data import _handle_zeros_in_scale
 from sklearn.preprocessing.data import Binarizer
 from sklearn.preprocessing.data import KernelCenterer
 from sklearn.preprocessing.data import Normalizer
 from sklearn.preprocessing.data import normalize
-from sklearn.preprocessing.data import OneHotEncoder
-from sklearn.preprocessing.data import CategoricalEncoder
 from sklearn.preprocessing.data import StandardScaler
 from sklearn.preprocessing.data import scale
 from sklearn.preprocessing.data import MinMaxScaler
@@ -60,6 +58,7 @@
 from sklearn.preprocessing.data import power_transform
 from sklearn.exceptions import DataConversionWarning, NotFittedError
 
+from sklearn.base import clone
 from sklearn.pipeline import Pipeline
 from sklearn.model_selection import cross_val_predict
 from sklearn.svm import SVR
@@ -701,6 +700,85 @@ def test_scaler_without_centering():
     assert_array_almost_equal(X_csc_scaled_back.toarray(), X)
 
 
+@pytest.mark.parametrize("with_mean", [True, False])
+@pytest.mark.parametrize("with_std", [True, False])
+@pytest.mark.parametrize("array_constructor",
+                         [np.asarray, sparse.csc_matrix, sparse.csr_matrix])
+def test_scaler_n_samples_seen_with_nan(with_mean, with_std,
+                                        array_constructor):
+    X = np.array([[0, 1, 3],
+                  [np.nan, 6, 10],
+                  [5, 4, np.nan],
+                  [8, 0, np.nan]],
+                 dtype=np.float64)
+    X = array_constructor(X)
+
+    if sparse.issparse(X) and with_mean:
+        pytest.skip("'with_mean=True' cannot be used with sparse matrix.")
+
+    transformer = StandardScaler(with_mean=with_mean, with_std=with_std)
+    transformer.fit(X)
+
+    assert_array_equal(transformer.n_samples_seen_, np.array([3, 4, 2]))
+
+
+def _check_identity_scalers_attributes(scaler_1, scaler_2):
+    assert scaler_1.mean_ is scaler_2.mean_ is None
+    assert scaler_1.var_ is scaler_2.var_ is None
+    assert scaler_1.scale_ is scaler_2.scale_ is None
+    assert scaler_1.n_samples_seen_ == scaler_2.n_samples_seen_
+
+
+def test_scaler_return_identity():
+    # test that the scaler return identity when with_mean and with_std are
+    # False
+    X_dense = np.array([[0, 1, 3],
+                        [5, 6, 0],
+                        [8, 0, 10]],
+                       dtype=np.float64)
+    X_csr = sparse.csr_matrix(X_dense)
+    X_csc = X_csr.tocsc()
+
+    transformer_dense = StandardScaler(with_mean=False, with_std=False)
+    X_trans_dense = transformer_dense.fit_transform(X_dense)
+
+    transformer_csr = clone(transformer_dense)
+    X_trans_csr = transformer_csr.fit_transform(X_csr)
+
+    transformer_csc = clone(transformer_dense)
+    X_trans_csc = transformer_csc.fit_transform(X_csc)
+
+    assert_allclose_dense_sparse(X_trans_csr, X_csr)
+    assert_allclose_dense_sparse(X_trans_csc, X_csc)
+    assert_allclose(X_trans_dense, X_dense)
+
+    for trans_1, trans_2 in itertools.combinations([transformer_dense,
+                                                    transformer_csr,
+                                                    transformer_csc],
+                                                   2):
+        _check_identity_scalers_attributes(trans_1, trans_2)
+
+    transformer_dense.partial_fit(X_dense)
+    transformer_csr.partial_fit(X_csr)
+    transformer_csc.partial_fit(X_csc)
+
+    for trans_1, trans_2 in itertools.combinations([transformer_dense,
+                                                    transformer_csr,
+                                                    transformer_csc],
+                                                   2):
+        _check_identity_scalers_attributes(trans_1, trans_2)
+
+    transformer_dense.fit(X_dense)
+    transformer_csr.fit(X_csr)
+    transformer_csc.fit(X_csc)
+
+    for trans_1, trans_2 in itertools.combinations([transformer_dense,
+                                                    transformer_csr,
+                                                    transformer_csc],
+                                                   2):
+        _check_identity_scalers_attributes(trans_1, trans_2)
+
+
 def test_scaler_int():
     # test that scaler converts integer input to floating
     # for both sparse and dense matrices
@@ -822,14 +900,9 @@ def test_scale_sparse_with_mean_raise_exception():
 
 def test_scale_input_finiteness_validation():
     # Check if non finite inputs raise ValueError
-    X = [[np.nan, 5, 6, 7, 8]]
-    assert_raises_regex(ValueError,
-                        "Input contains NaN, infinity or a value too large",
-                        scale, X)
-
     X = [[np.inf, 5, 6, 7, 8]]
     assert_raises_regex(ValueError,
-                        "Input contains NaN, infinity or a value too large",
+                        "Input contains infinity or a value too large",
                         scale, X)
 
 
@@ -1836,416 +1909,6 @@ def test_add_dummy_feature_csr():
     assert_array_equal(X.toarray(), [[1, 1, 0], [1, 0, 1], [1, 0, 1]])
 
 
-def test_one_hot_encoder_sparse():
-    # Test OneHotEncoder's fit and transform.
-    X = [[3, 2, 1], [0, 1, 1]]
-    enc = OneHotEncoder()
-    # discover max values automatically
-    X_trans = enc.fit_transform(X).toarray()
-    assert_equal(X_trans.shape, (2, 5))
-    assert_array_equal(enc.active_features_,
-                       np.where([1, 0, 0, 1, 0, 1, 1, 0, 1])[0])
-    assert_array_equal(enc.feature_indices_, [0, 4, 7, 9])
-
-    # check outcome
-    assert_array_equal(X_trans,
-                       [[0., 1., 0., 1., 1.],
-                        [1., 0., 1., 0., 1.]])
-
-    # max value given as 3
-    enc = OneHotEncoder(n_values=4)
-    X_trans = enc.fit_transform(X)
-    assert_equal(X_trans.shape, (2, 4 * 3))
-    assert_array_equal(enc.feature_indices_, [0, 4, 8, 12])
-
-    # max value given per feature
-    enc = OneHotEncoder(n_values=[3, 2, 2])
-    X = [[1, 0, 1], [0, 1, 1]]
-    X_trans = enc.fit_transform(X)
-    assert_equal(X_trans.shape, (2, 3 + 2 + 2))
-    assert_array_equal(enc.n_values_, [3, 2, 2])
-    # check that testing with larger feature works:
-    X = np.array([[2, 0, 1], [0, 1, 1]])
-    enc.transform(X)
-
-    # test that an error is raised when out of bounds:
-    X_too_large = [[0, 2, 1], [0, 1, 1]]
-    assert_raises(ValueError, enc.transform, X_too_large)
-    error_msg = r"unknown categorical feature present \[2\] during transform."
-    assert_raises_regex(ValueError, error_msg, enc.transform, X_too_large)
-    assert_raises(ValueError, OneHotEncoder(n_values=2).fit_transform, X)
-
-    # test that error is raised when wrong number of features
-    assert_raises(ValueError, enc.transform, X[:, :-1])
-    # test that error is raised when wrong number of features in fit
-    # with prespecified n_values
-    assert_raises(ValueError, enc.fit, X[:, :-1])
-    # test exception on wrong init param
-    assert_raises(TypeError, OneHotEncoder(n_values=np.int).fit, X)
-
-    enc = OneHotEncoder()
-    # test negative input to fit
-    assert_raises(ValueError, enc.fit, [[0], [-1]])
-
-    # test negative input to transform
-    enc.fit([[0], [1]])
-    assert_raises(ValueError, enc.transform, [[0], [-1]])
-
-
-def test_one_hot_encoder_dense():
-    # check for sparse=False
-    X = [[3, 2, 1], [0, 1, 1]]
-    enc = OneHotEncoder(sparse=False)
-    # discover max values automatically
-    X_trans = enc.fit_transform(X)
-    assert_equal(X_trans.shape, (2, 5))
-    assert_array_equal(enc.active_features_,
-                       np.where([1, 0, 0, 1, 0, 1, 1, 0, 1])[0])
-    assert_array_equal(enc.feature_indices_, [0, 4, 7, 9])
-
-    # check outcome
-    assert_array_equal(X_trans,
-                       np.array([[0., 1., 0., 1., 1.],
-                                 [1., 0., 1., 0., 1.]]))
-
-
-def _check_transform_selected(X, X_expected, sel):
-    for M in (X, sparse.csr_matrix(X)):
-        Xtr = _transform_selected(M, Binarizer().transform, sel)
-        assert_array_equal(toarray(Xtr), X_expected)
-
-
-def test_transform_selected():
-    X = [[3, 2, 1], [0, 1, 1]]
-
-    X_expected = [[1, 2, 1], [0, 1, 1]]
-    _check_transform_selected(X, X_expected, [0])
-    _check_transform_selected(X, X_expected, [True, False, False])
-
-    X_expected = [[1, 1, 1], [0, 1, 1]]
-    _check_transform_selected(X, X_expected, [0, 1, 2])
-    _check_transform_selected(X, X_expected, [True, True, True])
-    _check_transform_selected(X, X_expected, "all")
-
-    _check_transform_selected(X, X, [])
-    _check_transform_selected(X, X, [False, False, False])
-
-
-def test_transform_selected_copy_arg():
-    # transformer that alters X
-    def _mutating_transformer(X):
-        X[0, 0] = X[0, 0] + 1
-        return X
-
-    original_X = np.asarray([[1, 2], [3, 4]])
-    expected_Xtr = [[2, 2], [3, 4]]
-
-    X = original_X.copy()
-    Xtr = _transform_selected(X, _mutating_transformer, copy=True,
-                              selected='all')
-
-    assert_array_equal(toarray(X), toarray(original_X))
-    assert_array_equal(toarray(Xtr), expected_Xtr)
-
-
-def _run_one_hot(X, X2, cat):
-    enc = OneHotEncoder(categorical_features=cat)
-    Xtr = enc.fit_transform(X)
-    X2tr = enc.transform(X2)
-    return Xtr, X2tr
-
-
-def _check_one_hot(X, X2, cat, n_features):
-    ind = np.where(cat)[0]
-    # With mask
-    A, B = _run_one_hot(X, X2, cat)
-    # With indices
-    C, D = _run_one_hot(X, X2, ind)
-    # Check shape
-    assert_equal(A.shape, (2, n_features))
-    assert_equal(B.shape, (1, n_features))
-    assert_equal(C.shape, (2, n_features))
-    assert_equal(D.shape, (1, n_features))
-    # Check that mask and indices give the same results
-    assert_array_equal(toarray(A), toarray(C))
-    assert_array_equal(toarray(B), toarray(D))
-
-
-def test_one_hot_encoder_categorical_features():
-    X = np.array([[3, 2, 1], [0, 1, 1]])
-    X2 = np.array([[1, 1, 1]])
-
-    cat = [True, False, False]
-    _check_one_hot(X, X2, cat, 4)
-
-    # Edge case: all non-categorical
-    cat = [False, False, False]
-    _check_one_hot(X, X2, cat, 3)
-
-    # Edge case: all categorical
-    cat = [True, True, True]
-    _check_one_hot(X, X2, cat, 5)
-
-
-def test_one_hot_encoder_unknown_transform():
-    X = np.array([[0, 2, 1], [1, 0, 3], [1, 0, 2]])
-    y = np.array([[4, 1, 1]])
-
-    # Test that one hot encoder raises error for unknown features
-    # present during transform.
-    oh = OneHotEncoder(handle_unknown='error')
-    oh.fit(X)
-    assert_raises(ValueError, oh.transform, y)
-
-    # Test the ignore option, ignores unknown features.
-    oh = OneHotEncoder(handle_unknown='ignore')
-    oh.fit(X)
-    assert_array_equal(
-        oh.transform(y).toarray(),
-        np.array([[0.,  0.,  0.,  0.,  1.,  0.,  0.]]))
-
-    # Raise error if handle_unknown is neither ignore or error.
-    oh = OneHotEncoder(handle_unknown='42')
-    oh.fit(X)
-    assert_raises(ValueError, oh.transform, y)
-
-
-def check_categorical_onehot(X):
-    enc = CategoricalEncoder(encoding='onehot')
-    Xtr1 = enc.fit_transform(X)
-
-    enc = CategoricalEncoder(encoding='onehot-dense')
-    Xtr2 = enc.fit_transform(X)
-
-    assert_allclose(Xtr1.toarray(), Xtr2)
-
-    assert sparse.isspmatrix_csr(Xtr1)
-    return Xtr1.toarray()
-
-
-def test_categorical_encoder_onehot():
-    X = [['abc', 1, 55], ['def', 2, 55]]
-
-    Xtr = check_categorical_onehot(np.array(X)[:, [0]])
-    assert_allclose(Xtr, [[1, 0], [0, 1]])
-
-    Xtr = check_categorical_onehot(np.array(X)[:, [0, 1]])
-    assert_allclose(Xtr, [[1, 0, 1, 0], [0, 1, 0, 1]])
-
-    Xtr = CategoricalEncoder().fit_transform(X)
-    assert_allclose(Xtr.toarray(), [[1, 0, 1, 0,  1], [0, 1, 0, 1, 1]])
-
-
-def test_categorical_encoder_onehot_inverse():
-    for encoding in ['onehot', 'onehot-dense']:
-        X = [['abc', 2, 55], ['def', 1, 55], ['abc', 3, 55]]
-        enc = CategoricalEncoder(encoding=encoding)
-        X_tr = enc.fit_transform(X)
-        exp = np.array(X, dtype=object)
-        assert_array_equal(enc.inverse_transform(X_tr), exp)
-
-        X = [[2, 55], [1, 55], [3, 55]]
-        enc = CategoricalEncoder(encoding=encoding)
-        X_tr = enc.fit_transform(X)
-        exp = np.array(X)
-        assert_array_equal(enc.inverse_transform(X_tr), exp)
-
-        # with unknown categories
-        X = [['abc', 2, 55], ['def', 1, 55], ['abc', 3, 55]]
-        enc = CategoricalEncoder(encoding=encoding, handle_unknown='ignore',
-                                 categories=[['abc', 'def'], [1, 2],
-                                             [54, 55, 56]])
-        X_tr = enc.fit_transform(X)
-        exp = np.array(X, dtype=object)
-        exp[2, 1] = None
-        assert_array_equal(enc.inverse_transform(X_tr), exp)
-
-        # with an otherwise numerical output, still object if unknown
-        X = [[2, 55], [1, 55], [3, 55]]
-        enc = CategoricalEncoder(encoding=encoding,
-                                 categories=[[1, 2], [54, 56]],
-                                 handle_unknown='ignore')
-        X_tr = enc.fit_transform(X)
-        exp = np.array(X, dtype=object)
-        exp[2, 0] = None
-        exp[:, 1] = None
-        assert_array_equal(enc.inverse_transform(X_tr), exp)
-
-        # incorrect shape raises
-        X_tr = np.array([[0, 1, 1], [1, 0, 1]])
-        msg = re.escape('Shape of the passed X data is not correct')
-        assert_raises_regex(ValueError, msg, enc.inverse_transform, X_tr)
-
-
-def test_categorical_encoder_handle_unknown():
-    X = np.array([[1, 2, 3], [4, 5, 6]])
-    X2 = np.array([[7, 5, 3]])
-
-    # Test that encoder raises error for unknown features during transform.
-    enc = CategoricalEncoder()
-    enc.fit(X)
-    msg = re.escape('unknown categories [7] in column 0')
-    assert_raises_regex(ValueError, msg, enc.transform, X2)
-
-    # With 'ignore' you get all 0's in result
-    enc = CategoricalEncoder(handle_unknown='ignore')
-    enc.fit(X)
-    X2_passed = X2.copy()
-    Xtr = enc.transform(X2_passed)
-    assert_allclose(Xtr.toarray(), [[0, 0, 0, 1, 1, 0]])
-    # ensure transformed data was not modified in place
-    assert_allclose(X2, X2_passed)
-
-    # Invalid option
-    enc = CategoricalEncoder(handle_unknown='invalid')
-    assert_raises(ValueError, enc.fit, X)
-
-
-def test_categorical_encoder_categories():
-    X = [['abc', 1, 55], ['def', 2, 55]]
-
-    # order of categories should not depend on order of samples
-    for Xi in [X, X[::-1]]:
-        enc = CategoricalEncoder()
-        enc.fit(Xi)
-        assert enc.categories == 'auto'
-        assert isinstance(enc.categories_, list)
-        cat_exp = [['abc', 'def'], [1, 2], [55]]
-        for res, exp in zip(enc.categories_, cat_exp):
-            assert res.tolist() == exp
-
-
-def test_categorical_encoder_specified_categories():
-    X = np.array([['a', 'b']], dtype=object).T
-
-    enc = CategoricalEncoder(categories=[['a', 'b', 'c']])
-    exp = np.array([[1., 0., 0.],
-                    [0., 1., 0.]])
-    assert_array_equal(enc.fit_transform(X).toarray(), exp)
-    assert enc.categories[0] == ['a', 'b', 'c']
-    assert enc.categories_[0].tolist() == ['a', 'b', 'c']
-    assert np.issubdtype(enc.categories_[0].dtype, np.str_)
-
-    # unsorted passed categories raises for now
-    enc = CategoricalEncoder(categories=[['c', 'b', 'a']])
-    msg = re.escape('Unsorted categories are not yet supported')
-    assert_raises_regex(ValueError, msg, enc.fit_transform, X)
-
-    # multiple columns
-    X = np.array([['a', 'b'], [0, 2]], dtype=object).T
-    enc = CategoricalEncoder(categories=[['a', 'b', 'c'], [0, 1, 2]])
-    exp = np.array([[1., 0., 0., 1., 0., 0.],
-                    [0., 1., 0., 0., 0., 1.]])
-    assert_array_equal(enc.fit_transform(X).toarray(), exp)
-    assert enc.categories_[0].tolist() == ['a', 'b', 'c']
-    assert np.issubdtype(enc.categories_[0].dtype, np.str_)
-    assert enc.categories_[1].tolist() == [0, 1, 2]
-    assert np.issubdtype(enc.categories_[1].dtype, np.integer)
-
-    # when specifying categories manually, unknown categories should already
-    # raise when fitting
-    X = np.array([['a', 'b', 'c']]).T
-    enc = CategoricalEncoder(categories=[['a', 'b']])
-    assert_raises(ValueError, enc.fit, X)
-    enc = CategoricalEncoder(categories=[['a', 'b']], handle_unknown='ignore')
-    exp = np.array([[1., 0.], [0., 1.], [0., 0.]])
-    assert_array_equal(enc.fit(X).transform(X).toarray(), exp)
-
-
-def test_categorical_encoder_pandas():
-    try:
-        import pandas as pd
-    except ImportError:
-        raise SkipTest("pandas is not installed")
-
-    X_df = pd.DataFrame({'A': ['a', 'b'], 'B': [1, 2]})
-
-    Xtr = check_categorical_onehot(X_df)
-    assert_allclose(Xtr, [[1, 0, 1, 0], [0, 1, 0, 1]])
-
-
-def test_categorical_encoder_ordinal():
-    X = [['abc', 2, 55], ['def', 1, 55]]
-
-    enc = CategoricalEncoder(encoding='other')
-    assert_raises(ValueError, enc.fit, X)
-
-    enc = CategoricalEncoder(encoding='ordinal', handle_unknown='ignore')
-    assert_raises(ValueError, enc.fit, X)
-
-    enc = CategoricalEncoder(encoding='ordinal')
-    exp = np.array([[0, 1, 0],
-                    [1, 0, 0]], dtype='int64')
-    assert_array_equal(enc.fit_transform(X), exp.astype('float64'))
-    enc = CategoricalEncoder(encoding='ordinal', dtype='int64')
-    assert_array_equal(enc.fit_transform(X), exp)
-
-
-def test_categorical_encoder_ordinal_inverse():
-    X = [['abc', 2, 55], ['def', 1, 55]]
-    enc = CategoricalEncoder(encoding='ordinal')
-    X_tr = enc.fit_transform(X)
-    exp = np.array(X, dtype=object)
-    assert_array_equal(enc.inverse_transform(X_tr), exp)
-
-    # incorrect shape raises
-    X_tr = np.array([[0, 1, 1, 2], [1, 0, 1, 0]])
-    msg = re.escape('Shape of the passed X data is not correct')
-    assert_raises_regex(ValueError, msg, enc.inverse_transform, X_tr)
-
-
-def test_categorical_encoder_dtypes():
-    # check that dtypes are preserved when determining categories
-    enc = CategoricalEncoder()
-    exp = np.array([[1., 0., 1., 0.], [0., 1., 0., 1.]], dtype='float64')
-
-    for X in [np.array([[1, 2], [3, 4]], dtype='int64'),
-              np.array([[1, 2], [3, 4]], dtype='float64'),
-              np.array([['a', 'b'], ['c', 'd']]),  # string dtype
-              np.array([[1, 'a'], [3, 'b']], dtype='object')]:
-        enc.fit(X)
-        assert all([enc.categories_[i].dtype == X.dtype for i in range(2)])
-        assert_array_equal(enc.transform(X).toarray(), exp)
-
-    X = [[1, 2], [3, 4]]
-    enc.fit(X)
-    assert all([np.issubdtype(enc.categories_[i].dtype, np.integer)
-                for i in range(2)])
-    assert_array_equal(enc.transform(X).toarray(), exp)
-
-    X = [[1, 'a'], [3, 'b']]
-    enc.fit(X)
-    assert all([enc.categories_[i].dtype == 'object' for i in range(2)])
-    assert_array_equal(enc.transform(X).toarray(), exp)
-
-
-def test_categorical_encoder_dtypes_pandas():
-    # check dtype (similar to test_categorical_encoder_dtypes for dataframes)
-    try:
-        import pandas as pd
-    except ImportError:
-        raise SkipTest("pandas is not installed")
-
-    enc = CategoricalEncoder()
-    exp = np.array([[1., 0., 1., 0.], [0., 1., 0., 1.]], dtype='float64')
-
-    X = pd.DataFrame({'A': [1, 2], 'B': [3, 4]}, dtype='int64')
-    enc.fit(X)
-    assert all([enc.categories_[i].dtype == 'int64' for i in range(2)])
-    assert_array_equal(enc.transform(X).toarray(), exp)
-
-    X = pd.DataFrame({'A': [1, 2], 'B': ['a', 'b']})
-    enc.fit(X)
-    assert all([enc.categories_[i].dtype == 'object' for i in range(2)])
-    assert_array_equal(enc.transform(X).toarray(), exp)
-
-
-def test_categorical_encoder_warning():
-    enc = CategoricalEncoder()
-    X = [['Male', 1], ['Female', 3]]
-    np.testing.assert_no_warnings(enc.fit_transform, X)
-
-
 def test_fit_cold_start():
     X = iris.data
     X_2d = X[:, :2]
diff --git a/sklearn/preprocessing/tests/test_encoders.py b/sklearn/preprocessing/tests/test_encoders.py
new file mode 100644
index 000000000000..e9abce28c863
--- /dev/null
+++ b/sklearn/preprocessing/tests/test_encoders.py
@@ -0,0 +1,542 @@
+from __future__ import division
+
+import re
+
+import numpy as np
+from scipy import sparse
+import pytest
+
+from sklearn.utils.testing import assert_array_equal
+from sklearn.utils.testing import assert_equal
+from sklearn.utils.testing import assert_raises
+from sklearn.utils.testing import assert_raises_regex
+from sklearn.utils.testing import assert_allclose
+from sklearn.utils.testing import ignore_warnings
+from sklearn.utils.testing import assert_warns
+from sklearn.utils.testing import assert_warns_message
+from sklearn.utils.testing import assert_no_warnings
+
+from sklearn.preprocessing._encoders import _transform_selected
+from sklearn.preprocessing.data import Binarizer
+from sklearn.preprocessing import OneHotEncoder
+from sklearn.preprocessing import OrdinalEncoder
+
+
+def toarray(a):
+    if hasattr(a, "toarray"):
+        a = a.toarray()
+    return a
+
+
+def test_one_hot_encoder_sparse():
+    # Test OneHotEncoder's fit and transform.
+    X = [[3, 2, 1], [0, 1, 1]]
+    enc = OneHotEncoder()
+    with ignore_warnings(category=(DeprecationWarning, FutureWarning)):
+        # discover max values automatically
+        X_trans = enc.fit_transform(X).toarray()
+        assert_equal(X_trans.shape, (2, 5))
+        assert_array_equal(enc.active_features_,
+                           np.where([1, 0, 0, 1, 0, 1, 1, 0, 1])[0])
+        assert_array_equal(enc.feature_indices_, [0, 4, 7, 9])
+
+        # check outcome
+        assert_array_equal(X_trans,
+                           [[0., 1., 0., 1., 1.],
+                            [1., 0., 1., 0., 1.]])
+
+    # max value given as 3
+    # enc = assert_warns(DeprecationWarning, OneHotEncoder, n_values=4)
+    enc = OneHotEncoder(n_values=4)
+    with ignore_warnings(category=DeprecationWarning):
+        X_trans = enc.fit_transform(X)
+        assert_equal(X_trans.shape, (2, 4 * 3))
+        assert_array_equal(enc.feature_indices_, [0, 4, 8, 12])
+
+    # max value given per feature
+    # enc = assert_warns(DeprecationWarning, OneHotEncoder, n_values=[3, 2, 2])
+    enc = OneHotEncoder(n_values=[3, 2, 2])
+    with ignore_warnings(category=DeprecationWarning):
+        X = [[1, 0, 1], [0, 1, 1]]
+        X_trans = enc.fit_transform(X)
+        assert_equal(X_trans.shape, (2, 3 + 2 + 2))
+        assert_array_equal(enc.n_values_, [3, 2, 2])
+    # check that testing with larger feature works:
+    X = np.array([[2, 0, 1], [0, 1, 1]])
+    enc.transform(X)
+
+    # test that an error is raised when out of bounds:
+    X_too_large = [[0, 2, 1], [0, 1, 1]]
+    assert_raises(ValueError, enc.transform, X_too_large)
+    error_msg = r"unknown categorical feature present \[2\] during transform"
+    assert_raises_regex(ValueError, error_msg, enc.transform, X_too_large)
+    with ignore_warnings(category=DeprecationWarning):
+        assert_raises(
+            ValueError,
+            OneHotEncoder(n_values=2).fit_transform, X)
+
+    # test that error is raised when wrong number of features
+    assert_raises(ValueError, enc.transform, X[:, :-1])
+
+    # test that error is raised when wrong number of features in fit
+    # with prespecified n_values
+    with ignore_warnings(category=DeprecationWarning):
+        assert_raises(ValueError, enc.fit, X[:, :-1])
+    # test exception on wrong init param
+    with ignore_warnings(category=DeprecationWarning):
+        assert_raises(
+            TypeError, OneHotEncoder(n_values=np.int).fit, X)
+
+    enc = OneHotEncoder()
+    # test negative input to fit
+    with ignore_warnings(category=FutureWarning):
+        assert_raises(ValueError, enc.fit, [[0], [-1]])
+
+    # test negative input to transform
+    with ignore_warnings(category=FutureWarning):
+        enc.fit([[0], [1]])
+    assert_raises(ValueError, enc.transform, [[0], [-1]])
+
+
+def test_one_hot_encoder_dense():
+    # check for sparse=False
+    X = [[3, 2, 1], [0, 1, 1]]
+    enc = OneHotEncoder(sparse=False)
+    with ignore_warnings(category=(DeprecationWarning, FutureWarning)):
+        # discover max values automatically
+        X_trans = enc.fit_transform(X)
+        assert_equal(X_trans.shape, (2, 5))
+        assert_array_equal(enc.active_features_,
+                           np.where([1, 0, 0, 1, 0, 1, 1, 0, 1])[0])
+        assert_array_equal(enc.feature_indices_, [0, 4, 7, 9])
+
+    # check outcome
+    assert_array_equal(X_trans,
+                       np.array([[0., 1., 0., 1., 1.],
+                                 [1., 0., 1., 0., 1.]]))
+
+
+def test_one_hot_encoder_deprecationwarnings():
+    for X in [[[3, 2, 1], [0, 1, 1]],
+              [[3., 2., 1.], [0., 1., 1.]]]:
+        enc = OneHotEncoder()
+        assert_warns_message(FutureWarning, "handling of integer",
+                             enc.fit, X)
+        enc = OneHotEncoder()
+        assert_warns_message(FutureWarning, "handling of integer",
+                             enc.fit_transform, X)
+
+        # check it still works correctly as well
+        with ignore_warnings(category=FutureWarning):
+            X_trans = enc.fit_transform(X).toarray()
+        res = [[0., 1., 0., 1., 1.],
+               [1., 0., 1., 0., 1.]]
+        assert_array_equal(X_trans, res)
+
+        # check deprecated attributes
+        assert_warns(DeprecationWarning, lambda: enc.active_features_)
+        assert_warns(DeprecationWarning, lambda: enc.feature_indices_)
+        assert_warns(DeprecationWarning, lambda: enc.n_values_)
+
+        # check no warning is raised if keyword is specified
+        enc = OneHotEncoder(categories='auto')
+        assert_no_warnings(enc.fit, X)
+        enc = OneHotEncoder(categories='auto')
+        assert_no_warnings(enc.fit_transform, X)
+        X_trans = enc.fit_transform(X).toarray()
+        assert_array_equal(X_trans, res)
+
+        # check there is also a warning if the default is passed
+        enc = OneHotEncoder(n_values='auto', handle_unknown='ignore')
+        assert_warns(DeprecationWarning, enc.fit, X)
+
+    X = np.array([['cat1', 'cat2']], dtype=object).T
+    enc = OneHotEncoder(categorical_features='all')
+    assert_warns(DeprecationWarning, enc.fit, X)
+
+
+def test_one_hot_encoder_force_new_behaviour():
+    # ambiguous integer case (non secutive range of categories)
+    X = np.array([[1, 2]]).T
+    X2 = np.array([[0, 1]]).T
+
+    # without argument -> by default using legacy behaviour with warnings
+    enc = OneHotEncoder()
+
+    with ignore_warnings(category=FutureWarning):
+        enc.fit(X)
+
+    res = enc.transform(X2)
+    exp = np.array([[0, 0], [1, 0]])
+    assert_array_equal(res.toarray(), exp)
+
+    # with explicit auto argument -> don't use legacy behaviour
+    # (so will raise an error on unseen value within range)
+    enc = OneHotEncoder(categories='auto')
+    enc.fit(X)
+    assert_raises(ValueError, enc.transform, X2)
+
+
+def _check_transform_selected(X, X_expected, dtype, sel):
+    for M in (X, sparse.csr_matrix(X)):
+        Xtr = _transform_selected(M, Binarizer().transform, dtype, sel)
+        assert_array_equal(toarray(Xtr), X_expected)
+
+
+@pytest.mark.parametrize("output_dtype", [np.int32, np.float32, np.float64])
+@pytest.mark.parametrize("input_dtype", [np.int32, np.float32, np.float64])
+def test_transform_selected(output_dtype, input_dtype):
+    X = np.asarray([[3, 2, 1], [0, 1, 1]], dtype=input_dtype)
+
+    X_expected = np.asarray([[1, 2, 1], [0, 1, 1]], dtype=output_dtype)
+    _check_transform_selected(X, X_expected, output_dtype, [0])
+    _check_transform_selected(X, X_expected, output_dtype,
+                              [True, False, False])
+
+    X_expected = np.asarray([[1, 1, 1], [0, 1, 1]], dtype=output_dtype)
+    _check_transform_selected(X, X_expected, output_dtype, [0, 1, 2])
+    _check_transform_selected(X, X_expected, output_dtype, [True, True, True])
+    _check_transform_selected(X, X_expected, output_dtype, "all")
+
+    _check_transform_selected(X, X, output_dtype, [])
+    _check_transform_selected(X, X, output_dtype, [False, False, False])
+
+
+@pytest.mark.parametrize("output_dtype", [np.int32, np.float32, np.float64])
+@pytest.mark.parametrize("input_dtype", [np.int32, np.float32, np.float64])
+def test_transform_selected_copy_arg(output_dtype, input_dtype):
+    # transformer that alters X
+    def _mutating_transformer(X):
+        X[0, 0] = X[0, 0] + 1
+        return X
+
+    original_X = np.asarray([[1, 2], [3, 4]], dtype=input_dtype)
+    expected_Xtr = np.asarray([[2, 2], [3, 4]], dtype=output_dtype)
+
+    X = original_X.copy()
+    Xtr = _transform_selected(X, _mutating_transformer, output_dtype,
+                              copy=True, selected='all')
+
+    assert_array_equal(toarray(X), toarray(original_X))
+    assert_array_equal(toarray(Xtr), expected_Xtr)
+
+
+def _run_one_hot(X, X2, cat):
+    # enc = assert_warns(
+    #     DeprecationWarning,
+    #     OneHotEncoder, categorical_features=cat)
+    enc = OneHotEncoder(categorical_features=cat)
+    with ignore_warnings(category=(DeprecationWarning, FutureWarning)):
+        Xtr = enc.fit_transform(X)
+    with ignore_warnings(category=(DeprecationWarning, FutureWarning)):
+        X2tr = enc.fit(X).transform(X2)
+    return Xtr, X2tr
+
+
+def _check_one_hot(X, X2, cat, n_features):
+    ind = np.where(cat)[0]
+    # With mask
+    A, B = _run_one_hot(X, X2, cat)
+    # With indices
+    C, D = _run_one_hot(X, X2, ind)
+    # Check shape
+    assert_equal(A.shape, (2, n_features))
+    assert_equal(B.shape, (1, n_features))
+    assert_equal(C.shape, (2, n_features))
+    assert_equal(D.shape, (1, n_features))
+    # Check that mask and indices give the same results
+    assert_array_equal(toarray(A), toarray(C))
+    assert_array_equal(toarray(B), toarray(D))
+
+
+def test_one_hot_encoder_categorical_features():
+    X = np.array([[3, 2, 1], [0, 1, 1]])
+    X2 = np.array([[1, 1, 1]])
+
+    cat = [True, False, False]
+    _check_one_hot(X, X2, cat, 4)
+
+    # Edge case: all non-categorical
+    cat = [False, False, False]
+    _check_one_hot(X, X2, cat, 3)
+
+    # Edge case: all categorical
+    cat = [True, True, True]
+    _check_one_hot(X, X2, cat, 5)
+
+    # check error raised if also specifying categories
+    oh = OneHotEncoder(categories=[range(3)],
+                       categorical_features=[True, False, False])
+    assert_raises(ValueError, oh.fit, X)
+
+
+def test_one_hot_encoder_handle_unknown():
+    X = np.array([[0, 2, 1], [1, 0, 3], [1, 0, 2]])
+    X2 = np.array([[4, 1, 1]])
+
+    # Test that one hot encoder raises error for unknown features
+    # present during transform.
+    oh = OneHotEncoder(handle_unknown='error')
+    assert_warns(FutureWarning, oh.fit, X)
+    assert_raises(ValueError, oh.transform, X2)
+
+    # Test the ignore option, ignores unknown features (giving all 0's)
+    oh = OneHotEncoder(handle_unknown='ignore')
+    oh.fit(X)
+    X2_passed = X2.copy()
+    assert_array_equal(
+        oh.transform(X2_passed).toarray(),
+        np.array([[0.,  0.,  0.,  0.,  1.,  0.,  0.]]))
+    # ensure transformed data was not modified in place
+    assert_allclose(X2, X2_passed)
+
+    # Raise error if handle_unknown is neither ignore or error.
+    oh = OneHotEncoder(handle_unknown='42')
+    assert_raises(ValueError, oh.fit, X)
+
+
+@pytest.mark.parametrize("output_dtype", [np.int32, np.float32, np.float64])
+@pytest.mark.parametrize("input_dtype", [np.int32, np.float32, np.float64])
+def test_one_hot_encoder_dtype(input_dtype, output_dtype):
+    X = np.asarray([[0, 1]], dtype=input_dtype).T
+    X_expected = np.asarray([[1, 0], [0, 1]], dtype=output_dtype)
+
+    oh = OneHotEncoder(categories='auto', dtype=output_dtype)
+    assert_array_equal(oh.fit_transform(X).toarray(), X_expected)
+    assert_array_equal(oh.fit(X).transform(X).toarray(), X_expected)
+
+    oh = OneHotEncoder(categories='auto', dtype=output_dtype, sparse=False)
+    assert_array_equal(oh.fit_transform(X), X_expected)
+    assert_array_equal(oh.fit(X).transform(X), X_expected)
+
+
+@pytest.mark.parametrize("output_dtype", [np.int32, np.float32, np.float64])
+def test_one_hot_encoder_dtype_pandas(output_dtype):
+    pd = pytest.importorskip('pandas')
+
+    X_df = pd.DataFrame({'A': ['a', 'b'], 'B': [1, 2]})
+    X_expected = np.array([[1, 0, 1, 0], [0, 1, 0, 1]], dtype=output_dtype)
+
+    oh = OneHotEncoder(dtype=output_dtype)
+    assert_array_equal(oh.fit_transform(X_df).toarray(), X_expected)
+    assert_array_equal(oh.fit(X_df).transform(X_df).toarray(), X_expected)
+
+    oh = OneHotEncoder(dtype=output_dtype, sparse=False)
+    assert_array_equal(oh.fit_transform(X_df), X_expected)
+    assert_array_equal(oh.fit(X_df).transform(X_df), X_expected)
+
+
+def test_one_hot_encoder_set_params():
+    X = np.array([[1, 2]]).T
+    oh = OneHotEncoder()
+    # set params on not yet fitted object
+    oh.set_params(categories=[[0, 1, 2, 3]])
+    assert oh.get_params()['categories'] == [[0, 1, 2, 3]]
+    assert oh.fit_transform(X).toarray().shape == (2, 4)
+    # set params on already fitted object
+    oh.set_params(categories=[[0, 1, 2, 3, 4]])
+    assert oh.fit_transform(X).toarray().shape == (2, 5)
+
+
+def check_categorical_onehot(X):
+    enc = OneHotEncoder()
+    Xtr1 = enc.fit_transform(X)
+
+    enc = OneHotEncoder(sparse=False)
+    Xtr2 = enc.fit_transform(X)
+
+    assert_allclose(Xtr1.toarray(), Xtr2)
+
+    assert sparse.isspmatrix_csr(Xtr1)
+    return Xtr1.toarray()
+
+
+def test_one_hot_encoder():
+    X = [['abc', 1, 55], ['def', 2, 55]]
+
+    Xtr = check_categorical_onehot(np.array(X)[:, [0]])
+    assert_allclose(Xtr, [[1, 0], [0, 1]])
+
+    Xtr = check_categorical_onehot(np.array(X)[:, [0, 1]])
+    assert_allclose(Xtr, [[1, 0, 1, 0], [0, 1, 0, 1]])
+
+    Xtr = OneHotEncoder().fit_transform(X)
+    assert_allclose(Xtr.toarray(), [[1, 0, 1, 0,  1], [0, 1, 0, 1, 1]])
+
+
+def test_one_hot_encoder_inverse():
+    for sparse_ in [True, False]:
+        X = [['abc', 2, 55], ['def', 1, 55], ['abc', 3, 55]]
+        enc = OneHotEncoder(sparse=sparse_)
+        X_tr = enc.fit_transform(X)
+        exp = np.array(X, dtype=object)
+        assert_array_equal(enc.inverse_transform(X_tr), exp)
+
+        X = [[2, 55], [1, 55], [3, 55]]
+        enc = OneHotEncoder(sparse=sparse_, categories='auto')
+        X_tr = enc.fit_transform(X)
+        exp = np.array(X)
+        assert_array_equal(enc.inverse_transform(X_tr), exp)
+
+        # with unknown categories
+        X = [['abc', 2, 55], ['def', 1, 55], ['abc', 3, 55]]
+        enc = OneHotEncoder(sparse=sparse_, handle_unknown='ignore',
+                            categories=[['abc', 'def'], [1, 2],
+                                        [54, 55, 56]])
+        X_tr = enc.fit_transform(X)
+        exp = np.array(X, dtype=object)
+        exp[2, 1] = None
+        assert_array_equal(enc.inverse_transform(X_tr), exp)
+
+        # with an otherwise numerical output, still object if unknown
+        X = [[2, 55], [1, 55], [3, 55]]
+        enc = OneHotEncoder(sparse=sparse_, categories=[[1, 2], [54, 56]],
+                            handle_unknown='ignore')
+        X_tr = enc.fit_transform(X)
+        exp = np.array(X, dtype=object)
+        exp[2, 0] = None
+        exp[:, 1] = None
+        assert_array_equal(enc.inverse_transform(X_tr), exp)
+
+        # incorrect shape raises
+        X_tr = np.array([[0, 1, 1], [1, 0, 1]])
+        msg = re.escape('Shape of the passed X data is not correct')
+        assert_raises_regex(ValueError, msg, enc.inverse_transform, X_tr)
+
+
+def test_one_hot_encoder_categories():
+    X = [['abc', 1, 55], ['def', 2, 55]]
+
+    # order of categories should not depend on order of samples
+    for Xi in [X, X[::-1]]:
+        enc = OneHotEncoder()
+        enc.fit(Xi)
+        # assert enc.categories == 'auto'
+        assert isinstance(enc.categories_, list)
+        cat_exp = [['abc', 'def'], [1, 2], [55]]
+        for res, exp in zip(enc.categories_, cat_exp):
+            assert res.tolist() == exp
+
+
+def test_one_hot_encoder_specified_categories():
+    X = np.array([['a', 'b']], dtype=object).T
+
+    enc = OneHotEncoder(categories=[['a', 'b', 'c']])
+    exp = np.array([[1., 0., 0.],
+                    [0., 1., 0.]])
+    assert_array_equal(enc.fit_transform(X).toarray(), exp)
+    assert enc.categories[0] == ['a', 'b', 'c']
+    assert enc.categories_[0].tolist() == ['a', 'b', 'c']
+    assert np.issubdtype(enc.categories_[0].dtype, np.str_)
+
+    # unsorted passed categories raises for now
+    enc = OneHotEncoder(categories=[['c', 'b', 'a']])
+    msg = re.escape('Unsorted categories are not yet supported')
+    assert_raises_regex(ValueError, msg, enc.fit_transform, X)
+
+    # multiple columns
+    X = np.array([['a', 'b'], [0, 2]], dtype=object).T
+    enc = OneHotEncoder(categories=[['a', 'b', 'c'], [0, 1, 2]])
+    exp = np.array([[1., 0., 0., 1., 0., 0.],
+                    [0., 1., 0., 0., 0., 1.]])
+    assert_array_equal(enc.fit_transform(X).toarray(), exp)
+    assert enc.categories_[0].tolist() == ['a', 'b', 'c']
+    assert np.issubdtype(enc.categories_[0].dtype, np.str_)
+    assert enc.categories_[1].tolist() == [0, 1, 2]
+    assert np.issubdtype(enc.categories_[1].dtype, np.integer)
+
+    # when specifying categories manually, unknown categories should already
+    # raise when fitting
+    X = np.array([['a', 'b', 'c']]).T
+    enc = OneHotEncoder(categories=[['a', 'b']])
+    assert_raises(ValueError, enc.fit, X)
+    enc = OneHotEncoder(categories=[['a', 'b']], handle_unknown='ignore')
+    exp = np.array([[1., 0.], [0., 1.], [0., 0.]])
+    assert_array_equal(enc.fit(X).transform(X).toarray(), exp)
+
+
+def test_one_hot_encoder_pandas():
+    pd = pytest.importorskip('pandas')
+
+    X_df = pd.DataFrame({'A': ['a', 'b'], 'B': [1, 2]})
+
+    Xtr = check_categorical_onehot(X_df)
+    assert_allclose(Xtr, [[1, 0, 1, 0], [0, 1, 0, 1]])
+
+
+def test_ordinal_encoder():
+    X = [['abc', 2, 55], ['def', 1, 55]]
+
+    enc = OrdinalEncoder()
+    exp = np.array([[0, 1, 0],
+                    [1, 0, 0]], dtype='int64')
+    assert_array_equal(enc.fit_transform(X), exp.astype('float64'))
+    enc = OrdinalEncoder(dtype='int64')
+    assert_array_equal(enc.fit_transform(X), exp)
+
+
+def test_ordinal_encoder_inverse():
+    X = [['abc', 2, 55], ['def', 1, 55]]
+    enc = OrdinalEncoder()
+    X_tr = enc.fit_transform(X)
+    exp = np.array(X, dtype=object)
+    assert_array_equal(enc.inverse_transform(X_tr), exp)
+
+    # incorrect shape raises
+    X_tr = np.array([[0, 1, 1, 2], [1, 0, 1, 0]])
+    msg = re.escape('Shape of the passed X data is not correct')
+    assert_raises_regex(ValueError, msg, enc.inverse_transform, X_tr)
+
+
+def test_encoder_dtypes():
+    # check that dtypes are preserved when determining categories
+    enc = OneHotEncoder(categories='auto')
+    exp = np.array([[1., 0., 1., 0.], [0., 1., 0., 1.]], dtype='float64')
+
+    for X in [np.array([[1, 2], [3, 4]], dtype='int64'),
+              np.array([[1, 2], [3, 4]], dtype='float64'),
+              np.array([['a', 'b'], ['c', 'd']]),  # string dtype
+              np.array([[1, 'a'], [3, 'b']], dtype='object')]:
+        enc.fit(X)
+        assert all([enc.categories_[i].dtype == X.dtype for i in range(2)])
+        assert_array_equal(enc.transform(X).toarray(), exp)
+
+    X = [[1, 2], [3, 4]]
+    enc.fit(X)
+    assert all([np.issubdtype(enc.categories_[i].dtype, np.integer)
+                for i in range(2)])
+    assert_array_equal(enc.transform(X).toarray(), exp)
+
+    X = [[1, 'a'], [3, 'b']]
+    enc.fit(X)
+    assert all([enc.categories_[i].dtype == 'object' for i in range(2)])
+    assert_array_equal(enc.transform(X).toarray(), exp)
+
+
+def test_encoder_dtypes_pandas():
+    # check dtype (similar to test_categorical_encoder_dtypes for dataframes)
+    pd = pytest.importorskip('pandas')
+
+    enc = OneHotEncoder(categories='auto')
+    exp = np.array([[1., 0., 1., 0.], [0., 1., 0., 1.]], dtype='float64')
+
+    X = pd.DataFrame({'A': [1, 2], 'B': [3, 4]}, dtype='int64')
+    enc.fit(X)
+    assert all([enc.categories_[i].dtype == 'int64' for i in range(2)])
+    assert_array_equal(enc.transform(X).toarray(), exp)
+
+    X = pd.DataFrame({'A': [1, 2], 'B': ['a', 'b']})
+    enc.fit(X)
+    assert all([enc.categories_[i].dtype == 'object' for i in range(2)])
+    assert_array_equal(enc.transform(X).toarray(), exp)
+
+
+def test_one_hot_encoder_warning():
+    enc = OneHotEncoder()
+    X = [['Male', 1], ['Female', 3]]
+    np.testing.assert_no_warnings(enc.fit_transform, X)
+
+
+def test_categorical_encoder_stub():
+    from sklearn.preprocessing import CategoricalEncoder
+    assert_raises(RuntimeError, CategoricalEncoder, encoding='ordinal')
diff --git a/sklearn/preprocessing/tests/test_label.py b/sklearn/preprocessing/tests/test_label.py
index 14788b14b521..faa0cc3ce275 100644
--- a/sklearn/preprocessing/tests/test_label.py
+++ b/sklearn/preprocessing/tests/test_label.py
@@ -482,7 +482,7 @@ def test_label_binarize_binary():
     neg_label = -1
     expected = np.array([[2, -1], [-1, 2], [2, -1]])[:, 1].reshape((-1, 1))
 
-    yield check_binarized_results, y, classes, pos_label, neg_label, expected
+    check_binarized_results(y, classes, pos_label, neg_label, expected)
 
     # Binary case where sparse_output = True will not result in a ValueError
     y = [0, 1, 0]
@@ -491,7 +491,7 @@ def test_label_binarize_binary():
     neg_label = 0
     expected = np.array([[3, 0], [0, 3], [3, 0]])[:, 1].reshape((-1, 1))
 
-    yield check_binarized_results, y, classes, pos_label, neg_label, expected
+    check_binarized_results(y, classes, pos_label, neg_label, expected)
 
 
 def test_label_binarize_multiclass():
@@ -501,7 +501,7 @@ def test_label_binarize_multiclass():
     neg_label = 0
     expected = 2 * np.eye(3)
 
-    yield check_binarized_results, y, classes, pos_label, neg_label, expected
+    check_binarized_results(y, classes, pos_label, neg_label, expected)
 
     assert_raises(ValueError, label_binarize, y, classes, neg_label=-1,
                   pos_label=pos_label, sparse_output=True)
@@ -518,8 +518,8 @@ def test_label_binarize_multilabel():
                                       dok_matrix, lil_matrix]]
 
     for y in [y_ind] + y_sparse:
-        yield (check_binarized_results, y, classes, pos_label, neg_label,
-               expected)
+        check_binarized_results(y, classes, pos_label, neg_label,
+                                expected)
 
     assert_raises(ValueError, label_binarize, y, classes, neg_label=-1,
                   pos_label=pos_label, sparse_output=True)
diff --git a/sklearn/svm/base.py b/sklearn/svm/base.py
index 0abc852c837b..248ae196d940 100644
--- a/sklearn/svm/base.py
+++ b/sklearn/svm/base.py
@@ -14,7 +14,7 @@
 from ..utils import column_or_1d, check_X_y
 from ..utils import compute_class_weight
 from ..utils.extmath import safe_sparse_dot
-from ..utils.validation import check_is_fitted
+from ..utils.validation import check_is_fitted, _check_large_sparse
 from ..utils.multiclass import check_classification_targets
 from ..externals import six
 from ..exceptions import ConvergenceWarning
@@ -144,7 +144,9 @@ def fit(self, X, y, sample_weight=None):
             raise TypeError("Sparse precomputed kernels are not supported.")
         self._sparse = sparse and not callable(self.kernel)
 
-        X, y = check_X_y(X, y, dtype=np.float64, order='C', accept_sparse='csr')
+        X, y = check_X_y(X, y, dtype=np.float64,
+                         order='C', accept_sparse='csr',
+                         accept_large_sparse=False)
         y = self._validate_targets(y)
 
         sample_weight = np.asarray([]
@@ -213,7 +215,8 @@ def fit(self, X, y, sample_weight=None):
         self.shape_fit_ = X.shape
 
         # In binary case, we need to flip the sign of coef, intercept and
-        # decision function. Use self._intercept_ and self._dual_coef_ internally.
+        # decision function. Use self._intercept_ and self._dual_coef_
+        # internally.
         self._intercept_ = self.intercept_.copy()
         self._dual_coef_ = self.dual_coef_
         if self._impl in ['c_svc', 'nu_svc'] and len(self.classes_) == 2:
@@ -327,7 +330,7 @@ def _dense_predict(self, X):
         n_samples, n_features = X.shape
         X = self._compute_kernel(X)
         if X.ndim == 1:
-            X = check_array(X, order='C')
+            X = check_array(X, order='C', accept_large_sparse=False)
 
         kernel = self.kernel
         if callable(self.kernel):
@@ -411,7 +414,8 @@ def _decision_function(self, X):
         return dec_func
 
     def _dense_decision_function(self, X):
-        X = check_array(X, dtype=np.float64, order="C")
+        X = check_array(X, dtype=np.float64, order="C",
+                        accept_large_sparse=False)
 
         kernel = self.kernel
         if callable(kernel):
@@ -450,7 +454,8 @@ def _sparse_decision_function(self, X):
     def _validate_for_predict(self, X):
         check_is_fitted(self, 'support_')
 
-        X = check_array(X, accept_sparse='csr', dtype=np.float64, order="C")
+        X = check_array(X, accept_sparse='csr', dtype=np.float64, order="C",
+                        accept_large_sparse=False)
         if self._sparse and not sp.isspmatrix(X):
             X = sp.csr_matrix(X)
         if self._sparse:
@@ -889,6 +894,10 @@ def _fit_liblinear(X, y, C, fit_intercept, intercept_scaling, class_weight,
     libsvm_sparse.set_verbosity_wrap(verbose)
     liblinear.set_verbosity_wrap(verbose)
 
+    # Liblinear doesn't support 64bit sparse matrix indices yet
+    if sp.issparse(X):
+        _check_large_sparse(X)
+
     # LibLinear wants targets as doubles, even for classification
     y_ind = np.asarray(y_ind, dtype=np.float64).ravel()
     y_ind = np.require(y_ind, requirements="W")
diff --git a/sklearn/svm/classes.py b/sklearn/svm/classes.py
index db9360e64fb4..7b34a40f25b9 100644
--- a/sklearn/svm/classes.py
+++ b/sklearn/svm/classes.py
@@ -226,7 +226,8 @@ def fit(self, X, y, sample_weight=None):
                              % self.C)
 
         X, y = check_X_y(X, y, accept_sparse='csr',
-                         dtype=np.float64, order="C")
+                         dtype=np.float64, order="C",
+                         accept_large_sparse=False)
         check_classification_targets(y)
         self.classes_ = np.unique(y)
 
@@ -412,7 +413,8 @@ def fit(self, X, y, sample_weight=None):
                              % self.C)
 
         X, y = check_X_y(X, y, accept_sparse='csr',
-                         dtype=np.float64, order="C")
+                         dtype=np.float64, order="C",
+                         accept_large_sparse=False)
         penalty = 'l2'  # SVR only accepts l2 penalty
         self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(
             X, y, self.C, self.fit_intercept, self.intercept_scaling,
@@ -1089,8 +1091,8 @@ class OneClassSVM(BaseLibSVM, OutlierMixin):
 
     offset_ : float
         Offset used to define the decision function from the raw scores.
-        We have the relation: decision_function = score_samples - offset_.
-        The offset is the opposite of intercept_ and is provided for
+        We have the relation: decision_function = score_samples - `offset_`.
+        The offset is the opposite of `intercept_` and is provided for
         consistency with other outlier detection algorithms.
 
     """
diff --git a/sklearn/svm/tests/test_bounds.py b/sklearn/svm/tests/test_bounds.py
index e46dbb92df44..d02c53b05d8b 100644
--- a/sklearn/svm/tests/test_bounds.py
+++ b/sklearn/svm/tests/test_bounds.py
@@ -1,6 +1,8 @@
 import numpy as np
 from scipy import sparse as sp
 
+import pytest
+
 from sklearn.svm.bounds import l1_min_c
 from sklearn.svm import LinearSVC
 from sklearn.linear_model.logistic import LogisticRegression
@@ -16,25 +18,24 @@
 Y2 = [2, 1, 0, 0]
 
 
-def test_l1_min_c():
-    losses = ['squared_hinge', 'log']
+@pytest.mark.parametrize('loss', ['squared_hinge', 'log'])
+@pytest.mark.parametrize('X_label', ['sparse', 'dense'])
+@pytest.mark.parametrize('Y_label', ['two-classes', 'multi-class'])
+@pytest.mark.parametrize('intercept_label', ['no-intercept', 'fit-intercept'])
+def test_l1_min_c(loss, X_label, Y_label, intercept_label):
     Xs = {'sparse': sparse_X, 'dense': dense_X}
     Ys = {'two-classes': Y1, 'multi-class': Y2}
     intercepts = {'no-intercept': {'fit_intercept': False},
                   'fit-intercept': {'fit_intercept': True,
                                     'intercept_scaling': 10}}
 
-    for loss in losses:
-        for X_label, X in Xs.items():
-            for Y_label, Y in Ys.items():
-                for intercept_label, intercept_params in intercepts.items():
-                    check = lambda: check_l1_min_c(X, Y, loss,
-                                                   **intercept_params)
-                    check.description = ('Test l1_min_c loss=%r %s %s %s' %
-                                         (loss, X_label, Y_label,
-                                          intercept_label))
-                    yield check
+    X = Xs[X_label]
+    Y = Ys[Y_label]
+    intercept_params = intercepts[intercept_label]
+    check_l1_min_c(X, Y, loss, **intercept_params)
+
 
+def test_l1_min_c_l2_loss():
     # loss='l2' should raise ValueError
     assert_raise_message(ValueError, "loss type not in",
                          l1_min_c, dense_X, Y1, "l2")
diff --git a/sklearn/tests/test_base.py b/sklearn/tests/test_base.py
index 4620dcbd0360..31c4d80967a1 100644
--- a/sklearn/tests/test_base.py
+++ b/sklearn/tests/test_base.py
@@ -24,7 +24,6 @@
 from sklearn.tree import DecisionTreeClassifier
 from sklearn.tree import DecisionTreeRegressor
 from sklearn import datasets
-from sklearn.utils import deprecated
 
 from sklearn.base import TransformerMixin
 from sklearn.utils.mocking import MockDataFrame
@@ -132,6 +131,9 @@ def test_clone_buggy():
     varg_est = VargEstimator()
     assert_raises(RuntimeError, clone, varg_est)
 
+    est = ModifyInitParams()
+    assert_raises(RuntimeError, clone, est)
+
 
 def test_clone_empty_array():
     # Regression test for cloning estimators with empty arrays
@@ -152,16 +154,6 @@ def test_clone_nan():
     assert_true(clf.empty is clf2.empty)
 
 
-def test_clone_copy_init_params():
-    # test for deprecation warning when copying or casting an init parameter
-    est = ModifyInitParams()
-    message = ("Estimator ModifyInitParams modifies parameters in __init__. "
-               "This behavior is deprecated as of 0.18 and support "
-               "for this behavior will be removed in 0.20.")
-
-    assert_warns_message(DeprecationWarning, message, clone, est)
-
-
 def test_clone_sparse_matrices():
     sparse_matrix_classes = [
         getattr(sp, name)
diff --git a/sklearn/tests/test_common.py b/sklearn/tests/test_common.py
index 62a3bee5fc12..1f4c41ec8285 100644
--- a/sklearn/tests/test_common.py
+++ b/sklearn/tests/test_common.py
@@ -22,6 +22,7 @@
 from sklearn.utils.testing import assert_greater
 from sklearn.utils.testing import assert_in
 from sklearn.utils.testing import ignore_warnings
+from sklearn.exceptions import ConvergenceWarning
 
 import sklearn
 from sklearn.cluster.bicluster import BiclusterMixin
@@ -91,18 +92,22 @@ def _rename_partial(val):
 )
 def test_non_meta_estimators(name, Estimator, check):
     # Common tests for non-meta estimators
-    estimator = Estimator()
-    set_checking_parameters(estimator)
-    check(name, estimator)
+    with ignore_warnings(category=(DeprecationWarning, ConvergenceWarning,
+                                   UserWarning, FutureWarning)):
+        estimator = Estimator()
+        set_checking_parameters(estimator)
+        check(name, estimator)
 
 
 @pytest.mark.parametrize("name, Estimator",
                          _tested_non_meta_estimators())
 def test_no_attributes_set_in_init(name, Estimator):
     # input validation etc for non-meta estimators
-    estimator = Estimator()
-    # check this on class
-    check_no_attributes_set_in_init(name, estimator)
+    with ignore_warnings(category=(DeprecationWarning, ConvergenceWarning,
+                                   UserWarning, FutureWarning)):
+        estimator = Estimator()
+        # check this on class
+        check_no_attributes_set_in_init(name, estimator)
 
 
 def test_configure():
diff --git a/sklearn/tests/test_cross_validation.py b/sklearn/tests/test_cross_validation.py
deleted file mode 100644
index 6a29a621ff58..000000000000
--- a/sklearn/tests/test_cross_validation.py
+++ /dev/null
@@ -1,1252 +0,0 @@
-"""Test the cross_validation module"""
-from __future__ import division
-import warnings
-
-import numpy as np
-from scipy.sparse import coo_matrix
-from scipy.sparse import csr_matrix
-from scipy import stats
-
-from sklearn.exceptions import ConvergenceWarning
-from sklearn.utils.testing import assert_true
-from sklearn.utils.testing import assert_false
-from sklearn.utils.testing import assert_equal
-from sklearn.utils.testing import assert_almost_equal
-from sklearn.utils.testing import assert_raises
-from sklearn.utils.testing import assert_greater
-from sklearn.utils.testing import assert_greater_equal
-from sklearn.utils.testing import assert_less
-from sklearn.utils.testing import assert_not_equal
-from sklearn.utils.testing import assert_array_almost_equal
-from sklearn.utils.testing import assert_array_equal
-from sklearn.utils.testing import assert_warns_message
-from sklearn.utils.testing import assert_raise_message
-from sklearn.utils.testing import ignore_warnings
-from sklearn.utils.mocking import CheckingClassifier, MockDataFrame
-
-with warnings.catch_warnings():
-    warnings.simplefilter('ignore')
-    from sklearn import cross_validation as cval
-
-from sklearn.datasets import make_regression
-from sklearn.datasets import load_boston
-from sklearn.datasets import load_digits
-from sklearn.datasets import load_iris
-from sklearn.datasets import make_multilabel_classification
-from sklearn.metrics import explained_variance_score
-from sklearn.metrics import make_scorer
-from sklearn.metrics import precision_score
-from sklearn.externals import six
-from sklearn.externals.six.moves import zip
-
-from sklearn.linear_model import Ridge
-from sklearn.multiclass import OneVsRestClassifier
-from sklearn.neighbors import KNeighborsClassifier
-from sklearn.svm import SVC
-from sklearn.cluster import KMeans
-
-from sklearn.preprocessing import Imputer
-from sklearn.pipeline import Pipeline
-
-
-class MockClassifier(object):
-    """Dummy classifier to test the cross-validation"""
-
-    def __init__(self, a=0, allow_nd=False):
-        self.a = a
-        self.allow_nd = allow_nd
-
-    def fit(self, X, Y=None, sample_weight=None, class_prior=None,
-            sparse_sample_weight=None, sparse_param=None, dummy_int=None,
-            dummy_str=None, dummy_obj=None, callback=None):
-        """The dummy arguments are to test that this fit function can
-        accept non-array arguments through cross-validation, such as:
-            - int
-            - str (this is actually array-like)
-            - object
-            - function
-        """
-        self.dummy_int = dummy_int
-        self.dummy_str = dummy_str
-        self.dummy_obj = dummy_obj
-        if callback is not None:
-            callback(self)
-
-        if self.allow_nd:
-            X = X.reshape(len(X), -1)
-        if X.ndim >= 3 and not self.allow_nd:
-            raise ValueError('X cannot be d')
-        if sample_weight is not None:
-            assert_true(sample_weight.shape[0] == X.shape[0],
-                        'MockClassifier extra fit_param sample_weight.shape[0]'
-                        ' is {0}, should be {1}'.format(sample_weight.shape[0],
-                                                        X.shape[0]))
-        if class_prior is not None:
-            assert_true(class_prior.shape[0] == len(np.unique(y)),
-                        'MockClassifier extra fit_param class_prior.shape[0]'
-                        ' is {0}, should be {1}'.format(class_prior.shape[0],
-                                                        len(np.unique(y))))
-        if sparse_sample_weight is not None:
-            fmt = ('MockClassifier extra fit_param sparse_sample_weight'
-                   '.shape[0] is {0}, should be {1}')
-            assert_true(sparse_sample_weight.shape[0] == X.shape[0],
-                        fmt.format(sparse_sample_weight.shape[0], X.shape[0]))
-        if sparse_param is not None:
-            fmt = ('MockClassifier extra fit_param sparse_param.shape '
-                   'is ({0}, {1}), should be ({2}, {3})')
-            assert_true(sparse_param.shape == P_sparse.shape,
-                        fmt.format(sparse_param.shape[0],
-                                   sparse_param.shape[1],
-                                   P_sparse.shape[0], P_sparse.shape[1]))
-        return self
-
-    def predict(self, T):
-        if self.allow_nd:
-            T = T.reshape(len(T), -1)
-        return T[:, 0]
-
-    def score(self, X=None, Y=None):
-        return 1. / (1 + np.abs(self.a))
-
-    def get_params(self, deep=False):
-        return {'a': self.a, 'allow_nd': self.allow_nd}
-
-X = np.ones((10, 2))
-X_sparse = coo_matrix(X)
-W_sparse = coo_matrix((np.array([1]), (np.array([1]), np.array([0]))),
-                      shape=(10, 1))
-P_sparse = coo_matrix(np.eye(5))
-
-# avoid StratifiedKFold's Warning about least populated class in y
-y = np.arange(10) % 3
-
-##############################################################################
-# Tests
-
-
-def check_valid_split(train, test, n_samples=None):
-    # Use python sets to get more informative assertion failure messages
-    train, test = set(train), set(test)
-
-    # Train and test split should not overlap
-    assert_equal(train.intersection(test), set())
-
-    if n_samples is not None:
-        # Check that the union of train an test split cover all the indices
-        assert_equal(train.union(test), set(range(n_samples)))
-
-
-def check_cv_coverage(cv, expected_n_iter=None, n_samples=None):
-    # Check that a all the samples appear at least once in a test fold
-    if expected_n_iter is not None:
-        assert_equal(len(cv), expected_n_iter)
-    else:
-        expected_n_iter = len(cv)
-
-    collected_test_samples = set()
-    iterations = 0
-    for train, test in cv:
-        check_valid_split(train, test, n_samples=n_samples)
-        iterations += 1
-        collected_test_samples.update(test)
-
-    # Check that the accumulated test samples cover the whole dataset
-    assert_equal(iterations, expected_n_iter)
-    if n_samples is not None:
-        assert_equal(collected_test_samples, set(range(n_samples)))
-
-
-def test_kfold_valueerrors():
-    # Check that errors are raised if there is not enough samples
-    assert_raises(ValueError, cval.KFold, 3, 4)
-
-    # Check that a warning is raised if the least populated class has too few
-    # members.
-    y = [3, 3, -1, -1, 3]
-
-    cv = assert_warns_message(Warning, "The least populated class",
-                              cval.StratifiedKFold, y, 3)
-
-    # Check that despite the warning the folds are still computed even
-    # though all the classes are not necessarily represented at on each
-    # side of the split at each split
-    check_cv_coverage(cv, expected_n_iter=3, n_samples=len(y))
-
-    # Check that errors are raised if all n_labels for individual
-    # classes are less than n_folds.
-    y = [3, 3, -1, -1, 2]
-
-    assert_raises(ValueError, cval.StratifiedKFold, y, 3)
-
-    # Error when number of folds is <= 1
-    assert_raises(ValueError, cval.KFold, 2, 0)
-    assert_raises(ValueError, cval.KFold, 2, 1)
-    error_string = ("k-fold cross validation requires at least one"
-                    " train / test split")
-    assert_raise_message(ValueError, error_string,
-                         cval.StratifiedKFold, y, 0)
-    assert_raise_message(ValueError, error_string,
-                         cval.StratifiedKFold, y, 1)
-
-    # When n is not integer:
-    assert_raises(ValueError, cval.KFold, 2.5, 2)
-
-    # When n_folds is not integer:
-    assert_raises(ValueError, cval.KFold, 5, 1.5)
-    assert_raises(ValueError, cval.StratifiedKFold, y, 1.5)
-
-
-def test_kfold_indices():
-    # Check all indices are returned in the test folds
-    kf = cval.KFold(300, 3)
-    check_cv_coverage(kf, expected_n_iter=3, n_samples=300)
-
-    # Check all indices are returned in the test folds even when equal-sized
-    # folds are not possible
-    kf = cval.KFold(17, 3)
-    check_cv_coverage(kf, expected_n_iter=3, n_samples=17)
-
-
-def test_kfold_no_shuffle():
-    # Manually check that KFold preserves the data ordering on toy datasets
-    splits = iter(cval.KFold(4, 2))
-    train, test = next(splits)
-    assert_array_equal(test, [0, 1])
-    assert_array_equal(train, [2, 3])
-
-    train, test = next(splits)
-    assert_array_equal(test, [2, 3])
-    assert_array_equal(train, [0, 1])
-
-    splits = iter(cval.KFold(5, 2))
-    train, test = next(splits)
-    assert_array_equal(test, [0, 1, 2])
-    assert_array_equal(train, [3, 4])
-
-    train, test = next(splits)
-    assert_array_equal(test, [3, 4])
-    assert_array_equal(train, [0, 1, 2])
-
-
-def test_stratified_kfold_no_shuffle():
-    # Manually check that StratifiedKFold preserves the data ordering as much
-    # as possible on toy datasets in order to avoid hiding sample dependencies
-    # when possible
-    splits = iter(cval.StratifiedKFold([1, 1, 0, 0], 2))
-    train, test = next(splits)
-    assert_array_equal(test, [0, 2])
-    assert_array_equal(train, [1, 3])
-
-    train, test = next(splits)
-    assert_array_equal(test, [1, 3])
-    assert_array_equal(train, [0, 2])
-
-    splits = iter(cval.StratifiedKFold([1, 1, 1, 0, 0, 0, 0], 2))
-    train, test = next(splits)
-    assert_array_equal(test, [0, 1, 3, 4])
-    assert_array_equal(train, [2, 5, 6])
-
-    train, test = next(splits)
-    assert_array_equal(test, [2, 5, 6])
-    assert_array_equal(train, [0, 1, 3, 4])
-
-
-def test_stratified_kfold_ratios():
-    # Check that stratified kfold preserves label ratios in individual splits
-    # Repeat with shuffling turned off and on
-    n_samples = 1000
-    labels = np.array([4] * int(0.10 * n_samples) +
-                      [0] * int(0.89 * n_samples) +
-                      [1] * int(0.01 * n_samples))
-    for shuffle in [False, True]:
-        for train, test in cval.StratifiedKFold(labels, 5, shuffle=shuffle):
-            assert_almost_equal(np.sum(labels[train] == 4) / len(train), 0.10,
-                                2)
-            assert_almost_equal(np.sum(labels[train] == 0) / len(train), 0.89,
-                                2)
-            assert_almost_equal(np.sum(labels[train] == 1) / len(train), 0.01,
-                                2)
-            assert_almost_equal(np.sum(labels[test] == 4) / len(test), 0.10, 2)
-            assert_almost_equal(np.sum(labels[test] == 0) / len(test), 0.89, 2)
-            assert_almost_equal(np.sum(labels[test] == 1) / len(test), 0.01, 2)
-
-
-def test_kfold_balance():
-    # Check that KFold returns folds with balanced sizes
-    for kf in [cval.KFold(i, 5) for i in range(11, 17)]:
-        sizes = []
-        for _, test in kf:
-            sizes.append(len(test))
-
-        assert_true((np.max(sizes) - np.min(sizes)) <= 1)
-        assert_equal(np.sum(sizes), kf.n)
-
-
-def test_stratifiedkfold_balance():
-    # Check that KFold returns folds with balanced sizes (only when
-    # stratification is possible)
-    # Repeat with shuffling turned off and on
-    labels = [0] * 3 + [1] * 14
-    for shuffle in [False, True]:
-        for skf in [cval.StratifiedKFold(labels[:i], 3, shuffle=shuffle)
-                    for i in range(11, 17)]:
-            sizes = []
-            for _, test in skf:
-                sizes.append(len(test))
-
-            assert_true((np.max(sizes) - np.min(sizes)) <= 1)
-            assert_equal(np.sum(sizes), skf.n)
-
-
-def test_shuffle_kfold():
-    # Check the indices are shuffled properly, and that all indices are
-    # returned in the different test folds
-    kf = cval.KFold(300, 3, shuffle=True, random_state=0)
-    ind = np.arange(300)
-
-    all_folds = None
-    for train, test in kf:
-        assert_true(np.any(np.arange(100) != ind[test]))
-        assert_true(np.any(np.arange(100, 200) != ind[test]))
-        assert_true(np.any(np.arange(200, 300) != ind[test]))
-
-        if all_folds is None:
-            all_folds = ind[test].copy()
-        else:
-            all_folds = np.concatenate((all_folds, ind[test]))
-
-    all_folds.sort()
-    assert_array_equal(all_folds, ind)
-
-
-def test_shuffle_stratifiedkfold():
-    # Check that shuffling is happening when requested, and for proper
-    # sample coverage
-    labels = [0] * 20 + [1] * 20
-    kf0 = list(cval.StratifiedKFold(labels, 5, shuffle=True, random_state=0))
-    kf1 = list(cval.StratifiedKFold(labels, 5, shuffle=True, random_state=1))
-    for (_, test0), (_, test1) in zip(kf0, kf1):
-        assert_true(set(test0) != set(test1))
-    check_cv_coverage(kf0, expected_n_iter=5, n_samples=40)
-
-
-def test_kfold_can_detect_dependent_samples_on_digits():  # see #2372
-    # The digits samples are dependent: they are apparently grouped by authors
-    # although we don't have any information on the groups segment locations
-    # for this data. We can highlight this fact be computing k-fold cross-
-    # validation with and without shuffling: we observe that the shuffling case
-    # wrongly makes the IID assumption and is therefore too optimistic: it
-    # estimates a much higher accuracy (around 0.96) than the non
-    # shuffling variant (around 0.86).
-
-    digits = load_digits()
-    X, y = digits.data[:800], digits.target[:800]
-    model = SVC(C=10, gamma=0.005)
-    n = len(y)
-
-    cv = cval.KFold(n, 5, shuffle=False)
-    mean_score = cval.cross_val_score(model, X, y, cv=cv).mean()
-    assert_greater(0.88, mean_score)
-    assert_greater(mean_score, 0.85)
-
-    # Shuffling the data artificially breaks the dependency and hides the
-    # overfitting of the model with regards to the writing style of the authors
-    # by yielding a seriously overestimated score:
-
-    cv = cval.KFold(n, 5, shuffle=True, random_state=0)
-    mean_score = cval.cross_val_score(model, X, y, cv=cv).mean()
-    assert_greater(mean_score, 0.95)
-
-    cv = cval.KFold(n, 5, shuffle=True, random_state=1)
-    mean_score = cval.cross_val_score(model, X, y, cv=cv).mean()
-    assert_greater(mean_score, 0.95)
-
-    # Similarly, StratifiedKFold should try to shuffle the data as little
-    # as possible (while respecting the balanced class constraints)
-    # and thus be able to detect the dependency by not overestimating
-    # the CV score either. As the digits dataset is approximately balanced
-    # the estimated mean score is close to the score measured with
-    # non-shuffled KFold
-
-    cv = cval.StratifiedKFold(y, 5)
-    mean_score = cval.cross_val_score(model, X, y, cv=cv).mean()
-    assert_greater(0.88, mean_score)
-    assert_greater(mean_score, 0.85)
-
-
-def test_label_kfold():
-    rng = np.random.RandomState(0)
-
-    # Parameters of the test
-    n_labels = 15
-    n_samples = 1000
-    n_folds = 5
-
-    # Construct the test data
-    tolerance = 0.05 * n_samples  # 5 percent error allowed
-    labels = rng.randint(0, n_labels, n_samples)
-    folds = cval.LabelKFold(labels, n_folds=n_folds).idxs
-    ideal_n_labels_per_fold = n_samples // n_folds
-
-    # Check that folds have approximately the same size
-    assert_equal(len(folds), len(labels))
-    for i in np.unique(folds):
-        assert_greater_equal(tolerance,
-                             abs(sum(folds == i) - ideal_n_labels_per_fold))
-
-    # Check that each label appears only in 1 fold
-    for label in np.unique(labels):
-        assert_equal(len(np.unique(folds[labels == label])), 1)
-
-    # Check that no label is on both sides of the split
-    labels = np.asarray(labels, dtype=object)
-    for train, test in cval.LabelKFold(labels, n_folds=n_folds):
-        assert_equal(len(np.intersect1d(labels[train], labels[test])), 0)
-
-    # Construct the test data
-    labels = ['Albert', 'Jean', 'Bertrand', 'Michel', 'Jean',
-              'Francis', 'Robert', 'Michel', 'Rachel', 'Lois',
-              'Michelle', 'Bernard', 'Marion', 'Laura', 'Jean',
-              'Rachel', 'Franck', 'John', 'Gael', 'Anna', 'Alix',
-              'Robert', 'Marion', 'David', 'Tony', 'Abel', 'Becky',
-              'Madmood', 'Cary', 'Mary', 'Alexandre', 'David', 'Francis',
-              'Barack', 'Abdoul', 'Rasha', 'Xi', 'Silvia']
-    labels = np.asarray(labels, dtype=object)
-
-    n_labels = len(np.unique(labels))
-    n_samples = len(labels)
-    n_folds = 5
-    tolerance = 0.05 * n_samples  # 5 percent error allowed
-    folds = cval.LabelKFold(labels, n_folds=n_folds).idxs
-    ideal_n_labels_per_fold = n_samples // n_folds
-
-    # Check that folds have approximately the same size
-    assert_equal(len(folds), len(labels))
-    for i in np.unique(folds):
-        assert_greater_equal(tolerance,
-                             abs(sum(folds == i) - ideal_n_labels_per_fold))
-
-    # Check that each label appears only in 1 fold
-    for label in np.unique(labels):
-        assert_equal(len(np.unique(folds[labels == label])), 1)
-
-    # Check that no label is on both sides of the split
-    for train, test in cval.LabelKFold(labels, n_folds=n_folds):
-        assert_equal(len(np.intersect1d(labels[train], labels[test])), 0)
-
-    # Should fail if there are more folds than labels
-    labels = np.array([1, 1, 1, 2, 2])
-    assert_raises(ValueError, cval.LabelKFold, labels, n_folds=3)
-
-
-def test_shuffle_split():
-    ss1 = cval.ShuffleSplit(10, test_size=0.2, random_state=0)
-    ss2 = cval.ShuffleSplit(10, test_size=2, random_state=0)
-    ss3 = cval.ShuffleSplit(10, test_size=np.int32(2), random_state=0)
-    for typ in six.integer_types:
-        ss4 = cval.ShuffleSplit(10, test_size=typ(2), random_state=0)
-    for t1, t2, t3, t4 in zip(ss1, ss2, ss3, ss4):
-        assert_array_equal(t1[0], t2[0])
-        assert_array_equal(t2[0], t3[0])
-        assert_array_equal(t3[0], t4[0])
-        assert_array_equal(t1[1], t2[1])
-        assert_array_equal(t2[1], t3[1])
-        assert_array_equal(t3[1], t4[1])
-
-
-def test_stratified_shuffle_split_init():
-    y = np.asarray([0, 1, 1, 1, 2, 2, 2])
-    # Check that error is raised if there is a class with only one sample
-    assert_raises(ValueError, cval.StratifiedShuffleSplit, y, 3, 0.2)
-
-    # Check that error is raised if the test set size is smaller than n_classes
-    assert_raises(ValueError, cval.StratifiedShuffleSplit, y, 3, 2)
-    # Check that error is raised if the train set size is smaller than
-    # n_classes
-    assert_raises(ValueError, cval.StratifiedShuffleSplit, y, 3, 3, 2)
-
-    y = np.asarray([0, 0, 0, 1, 1, 1, 2, 2, 2])
-    # Check that errors are raised if there is not enough samples
-    assert_raises(ValueError, cval.StratifiedShuffleSplit, y, 3, 0.5, 0.6)
-    assert_raises(ValueError, cval.StratifiedShuffleSplit, y, 3, 8, 0.6)
-    assert_raises(ValueError, cval.StratifiedShuffleSplit, y, 3, 0.6, 8)
-
-    # Train size or test size too small
-    assert_raises(ValueError, cval.StratifiedShuffleSplit, y, train_size=2)
-    assert_raises(ValueError, cval.StratifiedShuffleSplit, y, test_size=2)
-
-
-def test_stratified_shuffle_split_iter():
-    ys = [np.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]),
-          np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]),
-          np.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2] * 2),
-          np.array([1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4]),
-          np.array([-1] * 800 + [1] * 50)
-          ]
-
-    for y in ys:
-        sss = cval.StratifiedShuffleSplit(y, 6, test_size=0.33,
-                                          random_state=0)
-        test_size = np.ceil(0.33 * len(y))
-        train_size = len(y) - test_size
-        for train, test in sss:
-            assert_array_equal(np.unique(y[train]), np.unique(y[test]))
-            # Checks if folds keep classes proportions
-            p_train = (np.bincount(np.unique(y[train],
-                                   return_inverse=True)[1]) /
-                       float(len(y[train])))
-            p_test = (np.bincount(np.unique(y[test],
-                                  return_inverse=True)[1]) /
-                      float(len(y[test])))
-            assert_array_almost_equal(p_train, p_test, 1)
-            assert_equal(len(train) + len(test), y.size)
-            assert_equal(len(train), train_size)
-            assert_equal(len(test), test_size)
-            assert_array_equal(np.lib.arraysetops.intersect1d(train, test), [])
-
-
-def test_stratified_shuffle_split_even():
-    # Test the StratifiedShuffleSplit, indices are drawn with a
-    # equal chance
-    n_folds = 5
-    n_iter = 1000
-
-    def assert_counts_are_ok(idx_counts, p):
-        # Here we test that the distribution of the counts
-        # per index is close enough to a binomial
-        threshold = 0.05 / n_splits
-        bf = stats.binom(n_splits, p)
-        for count in idx_counts:
-            p = bf.pmf(count)
-            assert_true(p > threshold,
-                        "An index is not drawn with chance corresponding "
-                        "to even draws")
-
-    for n_samples in (6, 22):
-        labels = np.array((n_samples // 2) * [0, 1])
-        splits = cval.StratifiedShuffleSplit(labels, n_iter=n_iter,
-                                             test_size=1. / n_folds,
-                                             random_state=0)
-
-        train_counts = [0] * n_samples
-        test_counts = [0] * n_samples
-        n_splits = 0
-        for train, test in splits:
-            n_splits += 1
-            for counter, ids in [(train_counts, train), (test_counts, test)]:
-                for id in ids:
-                    counter[id] += 1
-        assert_equal(n_splits, n_iter)
-
-        assert_equal(len(train), splits.n_train)
-        assert_equal(len(test), splits.n_test)
-        assert_equal(len(set(train).intersection(test)), 0)
-
-        label_counts = np.unique(labels)
-        assert_equal(splits.test_size, 1.0 / n_folds)
-        assert_equal(splits.n_train + splits.n_test, len(labels))
-        assert_equal(len(label_counts), 2)
-        ex_test_p = float(splits.n_test) / n_samples
-        ex_train_p = float(splits.n_train) / n_samples
-
-        assert_counts_are_ok(train_counts, ex_train_p)
-        assert_counts_are_ok(test_counts, ex_test_p)
-
-
-def test_stratified_shuffle_split_overlap_train_test_bug():
-    # See https://github.com/scikit-learn/scikit-learn/issues/6121 for
-    # the original bug report
-    labels = [0, 1, 2, 3] * 3 + [4, 5] * 5
-
-    splits = cval.StratifiedShuffleSplit(labels, n_iter=1,
-                                         test_size=0.5, random_state=0)
-    train, test = next(iter(splits))
-
-    assert_array_equal(np.intersect1d(train, test), [])
-
-
-def test_predefinedsplit_with_kfold_split():
-    # Check that PredefinedSplit can reproduce a split generated by Kfold.
-    folds = -1 * np.ones(10)
-    kf_train = []
-    kf_test = []
-    for i, (train_ind, test_ind) in enumerate(cval.KFold(10, 5, shuffle=True)):
-        kf_train.append(train_ind)
-        kf_test.append(test_ind)
-        folds[test_ind] = i
-    ps_train = []
-    ps_test = []
-    ps = cval.PredefinedSplit(folds)
-    for train_ind, test_ind in ps:
-        ps_train.append(train_ind)
-        ps_test.append(test_ind)
-    assert_array_equal(ps_train, kf_train)
-    assert_array_equal(ps_test, kf_test)
-
-
-def test_label_shuffle_split():
-    ys = [np.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]),
-          np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]),
-          np.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2]),
-          np.array([1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4]),
-          ]
-
-    for y in ys:
-        n_iter = 6
-        test_size = 1. / 3
-        slo = cval.LabelShuffleSplit(y, n_iter, test_size=test_size,
-                                     random_state=0)
-
-        # Make sure the repr works
-        repr(slo)
-
-        # Test that the length is correct
-        assert_equal(len(slo), n_iter)
-
-        y_unique = np.unique(y)
-
-        for train, test in slo:
-            # First test: no train label is in the test set and vice versa
-            y_train_unique = np.unique(y[train])
-            y_test_unique = np.unique(y[test])
-            assert_false(np.any(np.in1d(y[train], y_test_unique)))
-            assert_false(np.any(np.in1d(y[test], y_train_unique)))
-
-            # Second test: train and test add up to all the data
-            assert_equal(y[train].size + y[test].size, y.size)
-
-            # Third test: train and test are disjoint
-            assert_array_equal(np.intersect1d(train, test), [])
-
-            # Fourth test: # unique train and test labels are correct,
-            #              +- 1 for rounding error
-            assert_true(abs(len(y_test_unique) -
-                            round(test_size * len(y_unique))) <= 1)
-            assert_true(abs(len(y_train_unique) -
-                            round((1.0 - test_size) * len(y_unique))) <= 1)
-
-
-def test_leave_label_out_changing_labels():
-    # Check that LeaveOneLabelOut and LeavePLabelOut work normally if
-    # the labels variable is changed before calling __iter__
-    labels = np.array([0, 1, 2, 1, 1, 2, 0, 0])
-    labels_changing = np.array(labels, copy=True)
-    lolo = cval.LeaveOneLabelOut(labels)
-    lolo_changing = cval.LeaveOneLabelOut(labels_changing)
-    lplo = cval.LeavePLabelOut(labels, p=2)
-    lplo_changing = cval.LeavePLabelOut(labels_changing, p=2)
-    labels_changing[:] = 0
-    for llo, llo_changing in [(lolo, lolo_changing), (lplo, lplo_changing)]:
-        for (train, test), (train_chan, test_chan) in zip(llo, llo_changing):
-            assert_array_equal(train, train_chan)
-            assert_array_equal(test, test_chan)
-
-
-def test_cross_val_score():
-    clf = MockClassifier()
-    for a in range(-10, 10):
-        clf.a = a
-        # Smoke test
-        scores = cval.cross_val_score(clf, X, y)
-        assert_array_equal(scores, clf.score(X, y))
-
-        # test with multioutput y
-        scores = cval.cross_val_score(clf, X_sparse, X)
-        assert_array_equal(scores, clf.score(X_sparse, X))
-
-        scores = cval.cross_val_score(clf, X_sparse, y)
-        assert_array_equal(scores, clf.score(X_sparse, y))
-
-        # test with multioutput y
-        scores = cval.cross_val_score(clf, X_sparse, X)
-        assert_array_equal(scores, clf.score(X_sparse, X))
-
-    # test with X and y as list
-    list_check = lambda x: isinstance(x, list)
-    clf = CheckingClassifier(check_X=list_check)
-    scores = cval.cross_val_score(clf, X.tolist(), y.tolist())
-
-    clf = CheckingClassifier(check_y=list_check)
-    scores = cval.cross_val_score(clf, X, y.tolist())
-
-    assert_raises(ValueError, cval.cross_val_score, clf, X, y,
-                  scoring="sklearn")
-
-    # test with 3d X and
-    X_3d = X[:, :, np.newaxis]
-    clf = MockClassifier(allow_nd=True)
-    scores = cval.cross_val_score(clf, X_3d, y)
-
-    clf = MockClassifier(allow_nd=False)
-    assert_raises(ValueError, cval.cross_val_score, clf, X_3d, y)
-
-
-def test_cross_val_score_pandas():
-    # check cross_val_score doesn't destroy pandas dataframe
-    types = [(MockDataFrame, MockDataFrame)]
-    try:
-        from pandas import Series, DataFrame
-        types.append((Series, DataFrame))
-    except ImportError:
-        pass
-    for TargetType, InputFeatureType in types:
-        # X dataframe, y series
-        X_df, y_ser = InputFeatureType(X), TargetType(y)
-        check_df = lambda x: isinstance(x, InputFeatureType)
-        check_series = lambda x: isinstance(x, TargetType)
-        clf = CheckingClassifier(check_X=check_df, check_y=check_series)
-        cval.cross_val_score(clf, X_df, y_ser)
-
-
-def test_cross_val_score_mask():
-    # test that cross_val_score works with boolean masks
-    svm = SVC(kernel="linear")
-    iris = load_iris()
-    X, y = iris.data, iris.target
-    cv_indices = cval.KFold(len(y), 5)
-    scores_indices = cval.cross_val_score(svm, X, y, cv=cv_indices)
-    cv_indices = cval.KFold(len(y), 5)
-    cv_masks = []
-    for train, test in cv_indices:
-        mask_train = np.zeros(len(y), dtype=np.bool)
-        mask_test = np.zeros(len(y), dtype=np.bool)
-        mask_train[train] = 1
-        mask_test[test] = 1
-        cv_masks.append((train, test))
-    scores_masks = cval.cross_val_score(svm, X, y, cv=cv_masks)
-    assert_array_equal(scores_indices, scores_masks)
-
-
-def test_cross_val_score_precomputed():
-    # test for svm with precomputed kernel
-    svm = SVC(kernel="precomputed")
-    iris = load_iris()
-    X, y = iris.data, iris.target
-    linear_kernel = np.dot(X, X.T)
-    score_precomputed = cval.cross_val_score(svm, linear_kernel, y)
-    svm = SVC(kernel="linear")
-    score_linear = cval.cross_val_score(svm, X, y)
-    assert_array_equal(score_precomputed, score_linear)
-
-    # Error raised for non-square X
-    svm = SVC(kernel="precomputed")
-    assert_raises(ValueError, cval.cross_val_score, svm, X, y)
-
-    # test error is raised when the precomputed kernel is not array-like
-    # or sparse
-    assert_raises(ValueError, cval.cross_val_score, svm,
-                  linear_kernel.tolist(), y)
-
-
-def test_cross_val_score_fit_params():
-    clf = MockClassifier()
-    n_samples = X.shape[0]
-    n_classes = len(np.unique(y))
-
-    DUMMY_INT = 42
-    DUMMY_STR = '42'
-    DUMMY_OBJ = object()
-
-    def assert_fit_params(clf):
-        # Function to test that the values are passed correctly to the
-        # classifier arguments for non-array type
-
-        assert_equal(clf.dummy_int, DUMMY_INT)
-        assert_equal(clf.dummy_str, DUMMY_STR)
-        assert_equal(clf.dummy_obj, DUMMY_OBJ)
-
-    fit_params = {'sample_weight': np.ones(n_samples),
-                  'class_prior': np.ones(n_classes) / n_classes,
-                  'sparse_sample_weight': W_sparse,
-                  'sparse_param': P_sparse,
-                  'dummy_int': DUMMY_INT,
-                  'dummy_str': DUMMY_STR,
-                  'dummy_obj': DUMMY_OBJ,
-                  'callback': assert_fit_params}
-    cval.cross_val_score(clf, X, y, fit_params=fit_params)
-
-
-def test_cross_val_score_score_func():
-    clf = MockClassifier()
-    _score_func_args = []
-
-    def score_func(y_test, y_predict):
-        _score_func_args.append((y_test, y_predict))
-        return 1.0
-
-    with warnings.catch_warnings(record=True):
-        scoring = make_scorer(score_func)
-        score = cval.cross_val_score(clf, X, y, scoring=scoring)
-    assert_array_equal(score, [1.0, 1.0, 1.0])
-    assert len(_score_func_args) == 3
-
-
-def test_cross_val_score_errors():
-    class BrokenEstimator:
-        pass
-
-    assert_raises(TypeError, cval.cross_val_score, BrokenEstimator(), X)
-
-
-def test_train_test_split_errors():
-    assert_raises(ValueError, cval.train_test_split)
-    assert_raises(ValueError, cval.train_test_split, range(3), train_size=1.1)
-    assert_raises(ValueError, cval.train_test_split, range(3), test_size=0.6,
-                  train_size=0.6)
-    assert_raises(ValueError, cval.train_test_split, range(3),
-                  test_size=np.float32(0.6), train_size=np.float32(0.6))
-    assert_raises(ValueError, cval.train_test_split, range(3),
-                  test_size="wrong_type")
-    assert_raises(ValueError, cval.train_test_split, range(3), test_size=2,
-                  train_size=4)
-    assert_raises(TypeError, cval.train_test_split, range(3),
-                  some_argument=1.1)
-    assert_raises(ValueError, cval.train_test_split, range(3), range(42))
-
-
-def test_train_test_split():
-    X = np.arange(100).reshape((10, 10))
-    X_s = coo_matrix(X)
-    y = np.arange(10)
-
-    # simple test
-    split = cval.train_test_split(X, y, test_size=None, train_size=.5)
-    X_train, X_test, y_train, y_test = split
-    assert_equal(len(y_test), len(y_train))
-    # test correspondence of X and y
-    assert_array_equal(X_train[:, 0], y_train * 10)
-    assert_array_equal(X_test[:, 0], y_test * 10)
-
-    # conversion of lists to arrays (deprecated?)
-    with warnings.catch_warnings(record=True):
-        split = cval.train_test_split(X, X_s, y.tolist())
-    X_train, X_test, X_s_train, X_s_test, y_train, y_test = split
-    assert_array_equal(X_train, X_s_train.toarray())
-    assert_array_equal(X_test, X_s_test.toarray())
-
-    # don't convert lists to anything else by default
-    split = cval.train_test_split(X, X_s, y.tolist())
-    X_train, X_test, X_s_train, X_s_test, y_train, y_test = split
-    assert_true(isinstance(y_train, list))
-    assert_true(isinstance(y_test, list))
-
-    # allow nd-arrays
-    X_4d = np.arange(10 * 5 * 3 * 2).reshape(10, 5, 3, 2)
-    y_3d = np.arange(10 * 7 * 11).reshape(10, 7, 11)
-    split = cval.train_test_split(X_4d, y_3d)
-    assert_equal(split[0].shape, (7, 5, 3, 2))
-    assert_equal(split[1].shape, (3, 5, 3, 2))
-    assert_equal(split[2].shape, (7, 7, 11))
-    assert_equal(split[3].shape, (3, 7, 11))
-
-    # test stratification option
-    y = np.array([1, 1, 1, 1, 2, 2, 2, 2])
-    for test_size, exp_test_size in zip([2, 4, 0.25, 0.5, 0.75],
-                                        [2, 4, 2, 4, 6]):
-        train, test = cval.train_test_split(y,
-                                            test_size=test_size,
-                                            stratify=y,
-                                            random_state=0)
-        assert_equal(len(test), exp_test_size)
-        assert_equal(len(test) + len(train), len(y))
-        # check the 1:1 ratio of ones and twos in the data is preserved
-        assert_equal(np.sum(train == 1), np.sum(train == 2))
-
-
-def train_test_split_pandas():
-    # check cross_val_score doesn't destroy pandas dataframe
-    types = [MockDataFrame]
-    try:
-        from pandas import DataFrame
-        types.append(DataFrame)
-    except ImportError:
-        pass
-    for InputFeatureType in types:
-        # X dataframe
-        X_df = InputFeatureType(X)
-        X_train, X_test = cval.train_test_split(X_df)
-        assert_true(isinstance(X_train, InputFeatureType))
-        assert_true(isinstance(X_test, InputFeatureType))
-
-def train_test_split_mock_pandas():
-    # X mock dataframe
-    X_df = MockDataFrame(X)
-    X_train, X_test = cval.train_test_split(X_df)
-    assert_true(isinstance(X_train, MockDataFrame))
-    assert_true(isinstance(X_test, MockDataFrame))
-
-
-def test_cross_val_score_with_score_func_classification():
-    iris = load_iris()
-    clf = SVC(kernel='linear')
-
-    # Default score (should be the accuracy score)
-    scores = cval.cross_val_score(clf, iris.data, iris.target, cv=5)
-    assert_array_almost_equal(scores, [0.97, 1., 0.97, 0.97, 1.], 2)
-
-    # Correct classification score (aka. zero / one score) - should be the
-    # same as the default estimator score
-    zo_scores = cval.cross_val_score(clf, iris.data, iris.target,
-                                     scoring="accuracy", cv=5)
-    assert_array_almost_equal(zo_scores, [0.97, 1., 0.97, 0.97, 1.], 2)
-
-    # F1 score (class are balanced so f1_score should be equal to zero/one
-    # score
-    f1_scores = cval.cross_val_score(clf, iris.data, iris.target,
-                                     scoring="f1_weighted", cv=5)
-    assert_array_almost_equal(f1_scores, [0.97, 1., 0.97, 0.97, 1.], 2)
-
-
-def test_cross_val_score_with_score_func_regression():
-    X, y = make_regression(n_samples=30, n_features=20, n_informative=5,
-                           random_state=0)
-    reg = Ridge()
-
-    # Default score of the Ridge regression estimator
-    scores = cval.cross_val_score(reg, X, y, cv=5)
-    assert_array_almost_equal(scores, [0.94, 0.97, 0.97, 0.99, 0.92], 2)
-
-    # R2 score (aka. determination coefficient) - should be the
-    # same as the default estimator score
-    r2_scores = cval.cross_val_score(reg, X, y, scoring="r2", cv=5)
-    assert_array_almost_equal(r2_scores, [0.94, 0.97, 0.97, 0.99, 0.92], 2)
-
-    # Mean squared error; this is a loss function, so "scores" are negative
-    neg_mse_scores = cval.cross_val_score(reg, X, y, cv=5,
-                                          scoring="neg_mean_squared_error")
-    expected_neg_mse = np.array([-763.07, -553.16, -274.38, -273.26, -1681.99])
-    assert_array_almost_equal(neg_mse_scores, expected_neg_mse, 2)
-
-    # Explained variance
-    scoring = make_scorer(explained_variance_score)
-    ev_scores = cval.cross_val_score(reg, X, y, cv=5, scoring=scoring)
-    assert_array_almost_equal(ev_scores, [0.94, 0.97, 0.97, 0.99, 0.92], 2)
-
-
-def test_permutation_score():
-    iris = load_iris()
-    X = iris.data
-    X_sparse = coo_matrix(X)
-    y = iris.target
-    svm = SVC(kernel='linear')
-    cv = cval.StratifiedKFold(y, 2)
-
-    score, scores, pvalue = cval.permutation_test_score(
-        svm, X, y, n_permutations=30, cv=cv, scoring="accuracy")
-    assert_greater(score, 0.9)
-    assert_almost_equal(pvalue, 0.0, 1)
-
-    score_label, _, pvalue_label = cval.permutation_test_score(
-        svm, X, y, n_permutations=30, cv=cv, scoring="accuracy",
-        labels=np.ones(y.size), random_state=0)
-    assert_true(score_label == score)
-    assert_true(pvalue_label == pvalue)
-
-    # check that we obtain the same results with a sparse representation
-    svm_sparse = SVC(kernel='linear')
-    cv_sparse = cval.StratifiedKFold(y, 2)
-    score_label, _, pvalue_label = cval.permutation_test_score(
-        svm_sparse, X_sparse, y, n_permutations=30, cv=cv_sparse,
-        scoring="accuracy", labels=np.ones(y.size), random_state=0)
-
-    assert_true(score_label == score)
-    assert_true(pvalue_label == pvalue)
-
-    # test with custom scoring object
-    def custom_score(y_true, y_pred):
-        return (((y_true == y_pred).sum() - (y_true != y_pred).sum())
-                / y_true.shape[0])
-
-    scorer = make_scorer(custom_score)
-    score, _, pvalue = cval.permutation_test_score(
-        svm, X, y, n_permutations=100, scoring=scorer, cv=cv, random_state=0)
-    assert_almost_equal(score, .93, 2)
-    assert_almost_equal(pvalue, 0.01, 3)
-
-    # set random y
-    y = np.mod(np.arange(len(y)), 3)
-
-    score, scores, pvalue = cval.permutation_test_score(
-        svm, X, y, n_permutations=30, cv=cv, scoring="accuracy")
-
-    assert_less(score, 0.5)
-    assert_greater(pvalue, 0.2)
-
-
-def test_cross_val_generator_with_indices():
-    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
-    y = np.array([1, 1, 2, 2])
-    labels = np.array([1, 2, 3, 4])
-    # explicitly passing indices value is deprecated
-    loo = cval.LeaveOneOut(4)
-    lpo = cval.LeavePOut(4, 2)
-    kf = cval.KFold(4, 2)
-    skf = cval.StratifiedKFold(y, 2)
-    lolo = cval.LeaveOneLabelOut(labels)
-    lopo = cval.LeavePLabelOut(labels, 2)
-    ps = cval.PredefinedSplit([1, 1, 2, 2])
-    ss = cval.ShuffleSplit(2)
-    for cv in [loo, lpo, kf, skf, lolo, lopo, ss, ps]:
-        for train, test in cv:
-            assert_not_equal(np.asarray(train).dtype.kind, 'b')
-            assert_not_equal(np.asarray(train).dtype.kind, 'b')
-            X[train], X[test]
-            y[train], y[test]
-
-
-@ignore_warnings
-def test_cross_val_generator_with_default_indices():
-    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
-    y = np.array([1, 1, 2, 2])
-    labels = np.array([1, 2, 3, 4])
-    loo = cval.LeaveOneOut(4)
-    lpo = cval.LeavePOut(4, 2)
-    kf = cval.KFold(4, 2)
-    skf = cval.StratifiedKFold(y, 2)
-    lolo = cval.LeaveOneLabelOut(labels)
-    lopo = cval.LeavePLabelOut(labels, 2)
-    ss = cval.ShuffleSplit(2)
-    ps = cval.PredefinedSplit([1, 1, 2, 2])
-    for cv in [loo, lpo, kf, skf, lolo, lopo, ss, ps]:
-        for train, test in cv:
-            assert_not_equal(np.asarray(train).dtype.kind, 'b')
-            assert_not_equal(np.asarray(train).dtype.kind, 'b')
-            X[train], X[test]
-            y[train], y[test]
-
-
-def test_shufflesplit_errors():
-    assert_raises(ValueError, cval.ShuffleSplit, 10, test_size=2.0)
-    assert_raises(ValueError, cval.ShuffleSplit, 10, test_size=1.0)
-    assert_raises(ValueError, cval.ShuffleSplit, 10, test_size=0.1,
-                  train_size=0.95)
-    assert_raises(ValueError, cval.ShuffleSplit, 10, test_size=11)
-    assert_raises(ValueError, cval.ShuffleSplit, 10, test_size=10)
-    assert_raises(ValueError, cval.ShuffleSplit, 10, test_size=8, train_size=3)
-    assert_raises(ValueError, cval.ShuffleSplit, 10, train_size=1j)
-    assert_raises(ValueError, cval.ShuffleSplit, 10, test_size=None,
-                  train_size=None)
-
-
-def test_shufflesplit_reproducible():
-    # Check that iterating twice on the ShuffleSplit gives the same
-    # sequence of train-test when the random_state is given
-    ss = cval.ShuffleSplit(10, random_state=21)
-    assert_array_equal(list(a for a, b in ss), list(a for a, b in ss))
-
-
-def test_safe_split_with_precomputed_kernel():
-    clf = SVC(gamma="scale")
-    clfp = SVC(kernel="precomputed")
-
-    iris = load_iris()
-    X, y = iris.data, iris.target
-    K = np.dot(X, X.T)
-
-    cv = cval.ShuffleSplit(X.shape[0], test_size=0.25, random_state=0)
-    tr, te = list(cv)[0]
-
-    X_tr, y_tr = cval._safe_split(clf, X, y, tr)
-    K_tr, y_tr2 = cval._safe_split(clfp, K, y, tr)
-    assert_array_almost_equal(K_tr, np.dot(X_tr, X_tr.T))
-
-    X_te, y_te = cval._safe_split(clf, X, y, te, tr)
-    K_te, y_te2 = cval._safe_split(clfp, K, y, te, tr)
-    assert_array_almost_equal(K_te, np.dot(X_te, X_tr.T))
-
-
-def test_cross_val_score_allow_nans():
-    # Check that cross_val_score allows input data with NaNs
-    X = np.arange(200, dtype=np.float64).reshape(10, -1)
-    X[2, :] = np.nan
-    y = np.repeat([0, 1], X.shape[0] / 2)
-    p = Pipeline([
-        ('imputer', Imputer(strategy='mean', missing_values='NaN')),
-        ('classifier', MockClassifier()),
-    ])
-    cval.cross_val_score(p, X, y, cv=5)
-
-
-def test_train_test_split_allow_nans():
-    # Check that train_test_split allows input data with NaNs
-    X = np.arange(200, dtype=np.float64).reshape(10, -1)
-    X[2, :] = np.nan
-    y = np.repeat([0, 1], X.shape[0] / 2)
-    cval.train_test_split(X, y, test_size=0.2, random_state=42)
-
-
-def test_permutation_test_score_allow_nans():
-    # Check that permutation_test_score allows input data with NaNs
-    X = np.arange(200, dtype=np.float64).reshape(10, -1)
-    X[2, :] = np.nan
-    y = np.repeat([0, 1], X.shape[0] / 2)
-    p = Pipeline([
-        ('imputer', Imputer(strategy='mean', missing_values='NaN')),
-        ('classifier', MockClassifier()),
-    ])
-    cval.permutation_test_score(p, X, y, cv=5)
-
-
-def test_check_cv_return_types():
-    X = np.ones((9, 2))
-    cv = cval.check_cv(3, X, classifier=False)
-    assert_true(isinstance(cv, cval.KFold))
-
-    y_binary = np.array([0, 1, 0, 1, 0, 0, 1, 1, 1])
-    cv = cval.check_cv(3, X, y_binary, classifier=True)
-    assert_true(isinstance(cv, cval.StratifiedKFold))
-
-    y_multiclass = np.array([0, 1, 0, 1, 2, 1, 2, 0, 2])
-    cv = cval.check_cv(3, X, y_multiclass, classifier=True)
-    assert_true(isinstance(cv, cval.StratifiedKFold))
-
-    X = np.ones((5, 2))
-    y_multilabel = [[1, 0, 1], [1, 1, 0], [0, 0, 0], [0, 1, 1], [1, 0, 0]]
-    cv = cval.check_cv(3, X, y_multilabel, classifier=True)
-    assert_true(isinstance(cv, cval.KFold))
-
-    y_multioutput = np.array([[1, 2], [0, 3], [0, 0], [3, 1], [2, 0]])
-    cv = cval.check_cv(3, X, y_multioutput, classifier=True)
-    assert_true(isinstance(cv, cval.KFold))
-
-
-def test_cross_val_score_multilabel():
-    X = np.array([[-3, 4], [2, 4], [3, 3], [0, 2], [-3, 1],
-                  [-2, 1], [0, 0], [-2, -1], [-1, -2], [1, -2]])
-    y = np.array([[1, 1], [0, 1], [0, 1], [0, 1], [1, 1],
-                  [0, 1], [1, 0], [1, 1], [1, 0], [0, 0]])
-    clf = KNeighborsClassifier(n_neighbors=1)
-    scoring_micro = make_scorer(precision_score, average='micro')
-    scoring_macro = make_scorer(precision_score, average='macro')
-    scoring_samples = make_scorer(precision_score, average='samples')
-    score_micro = cval.cross_val_score(clf, X, y, scoring=scoring_micro, cv=5)
-    score_macro = cval.cross_val_score(clf, X, y, scoring=scoring_macro, cv=5)
-    score_samples = cval.cross_val_score(clf, X, y,
-                                         scoring=scoring_samples, cv=5)
-    assert_almost_equal(score_micro, [1, 1 / 2, 3 / 4, 1 / 2, 1 / 3])
-    assert_almost_equal(score_macro, [1, 1 / 2, 3 / 4, 1 / 2, 1 / 4])
-    assert_almost_equal(score_samples, [1, 1 / 2, 3 / 4, 1 / 2, 1 / 4])
-
-
-def test_cross_val_predict():
-    boston = load_boston()
-    X, y = boston.data, boston.target
-    cv = cval.KFold(len(boston.target))
-
-    est = Ridge()
-
-    # Naive loop (should be same as cross_val_predict):
-    preds2 = np.zeros_like(y)
-    for train, test in cv:
-        est.fit(X[train], y[train])
-        preds2[test] = est.predict(X[test])
-
-    preds = cval.cross_val_predict(est, X, y, cv=cv)
-    assert_array_almost_equal(preds, preds2)
-
-    preds = cval.cross_val_predict(est, X, y)
-    assert_equal(len(preds), len(y))
-
-    cv = cval.LeaveOneOut(len(y))
-    preds = cval.cross_val_predict(est, X, y, cv=cv)
-    assert_equal(len(preds), len(y))
-
-    Xsp = X.copy()
-    Xsp *= (Xsp > np.median(Xsp))
-    Xsp = coo_matrix(Xsp)
-    preds = cval.cross_val_predict(est, Xsp, y)
-    assert_array_almost_equal(len(preds), len(y))
-
-    preds = cval.cross_val_predict(KMeans(), X)
-    assert_equal(len(preds), len(y))
-
-    def bad_cv():
-        for i in range(4):
-            yield np.array([0, 1, 2, 3]), np.array([4, 5, 6, 7, 8])
-
-    assert_raises(ValueError, cval.cross_val_predict, est, X, y, cv=bad_cv())
-
-
-def test_cross_val_predict_input_types():
-    clf = Ridge()
-    # Smoke test
-    predictions = cval.cross_val_predict(clf, X, y)
-    assert_equal(predictions.shape, (10,))
-
-    # test with multioutput y
-    with ignore_warnings(category=ConvergenceWarning):
-        predictions = cval.cross_val_predict(clf, X_sparse, X)
-    assert_equal(predictions.shape, (10, 2))
-
-    predictions = cval.cross_val_predict(clf, X_sparse, y)
-    assert_array_equal(predictions.shape, (10,))
-
-    # test with multioutput y
-    with ignore_warnings(category=ConvergenceWarning):
-        predictions = cval.cross_val_predict(clf, X_sparse, X)
-    assert_array_equal(predictions.shape, (10, 2))
-
-    # test with X and y as list
-    list_check = lambda x: isinstance(x, list)
-    clf = CheckingClassifier(check_X=list_check)
-    predictions = cval.cross_val_predict(clf, X.tolist(), y.tolist())
-
-    clf = CheckingClassifier(check_y=list_check)
-    predictions = cval.cross_val_predict(clf, X, y.tolist())
-
-    # test with 3d X and
-    X_3d = X[:, :, np.newaxis]
-    check_3d = lambda x: x.ndim == 3
-    clf = CheckingClassifier(check_X=check_3d)
-    predictions = cval.cross_val_predict(clf, X_3d, y)
-    assert_array_equal(predictions.shape, (10,))
-
-
-def test_cross_val_predict_pandas():
-    # check cross_val_score doesn't destroy pandas dataframe
-    types = [(MockDataFrame, MockDataFrame)]
-    try:
-        from pandas import Series, DataFrame
-        types.append((Series, DataFrame))
-    except ImportError:
-        pass
-    for TargetType, InputFeatureType in types:
-        # X dataframe, y series
-        X_df, y_ser = InputFeatureType(X), TargetType(y)
-        check_df = lambda x: isinstance(x, InputFeatureType)
-        check_series = lambda x: isinstance(x, TargetType)
-        clf = CheckingClassifier(check_X=check_df, check_y=check_series)
-        cval.cross_val_predict(clf, X_df, y_ser)
-
-
-def test_sparse_fit_params():
-    iris = load_iris()
-    X, y = iris.data, iris.target
-    clf = MockClassifier()
-    fit_params = {'sparse_sample_weight': coo_matrix(np.eye(X.shape[0]))}
-    a = cval.cross_val_score(clf, X, y, fit_params=fit_params)
-    assert_array_equal(a, np.ones(3))
-
-
-def test_check_is_partition():
-    p = np.arange(100)
-    assert_true(cval._check_is_partition(p, 100))
-    assert_false(cval._check_is_partition(np.delete(p, 23), 100))
-
-    p[0] = 23
-    assert_false(cval._check_is_partition(p, 100))
-
-
-def test_cross_val_predict_sparse_prediction():
-    # check that cross_val_predict gives same result for sparse and dense input
-    X, y = make_multilabel_classification(n_classes=2, n_labels=1,
-                                          allow_unlabeled=False,
-                                          return_indicator=True,
-                                          random_state=1)
-    X_sparse = csr_matrix(X)
-    y_sparse = csr_matrix(y)
-    classif = OneVsRestClassifier(SVC(kernel='linear'))
-    preds = cval.cross_val_predict(classif, X, y, cv=10)
-    preds_sparse = cval.cross_val_predict(classif, X_sparse, y_sparse, cv=10)
-    preds_sparse = preds_sparse.toarray()
-    assert_array_almost_equal(preds_sparse, preds)
diff --git a/sklearn/tests/test_grid_search.py b/sklearn/tests/test_grid_search.py
deleted file mode 100644
index 7a42757daea8..000000000000
--- a/sklearn/tests/test_grid_search.py
+++ /dev/null
@@ -1,815 +0,0 @@
-"""
-Testing for grid search module (sklearn.grid_search)
-
-"""
-
-from collections import Iterable, Sized
-from sklearn.externals.six.moves import cStringIO as StringIO
-from sklearn.externals.six.moves import xrange
-from itertools import chain, product
-import pickle
-import warnings
-import sys
-
-import numpy as np
-import scipy.sparse as sp
-
-from sklearn.utils.testing import assert_equal
-from sklearn.utils.testing import assert_not_equal
-from sklearn.utils.testing import assert_raises
-from sklearn.utils.testing import assert_warns
-from sklearn.utils.testing import assert_raise_message
-from sklearn.utils.testing import assert_false, assert_true
-from sklearn.utils.testing import assert_array_equal
-from sklearn.utils.testing import assert_almost_equal
-from sklearn.utils.testing import assert_array_almost_equal
-from sklearn.utils.testing import assert_no_warnings
-from sklearn.utils.testing import ignore_warnings
-from sklearn.utils.mocking import CheckingClassifier, MockDataFrame
-
-from scipy.stats import bernoulli, expon, uniform
-
-from sklearn.externals.six.moves import zip
-from sklearn.base import BaseEstimator
-from sklearn.datasets import make_classification
-from sklearn.datasets import make_blobs
-from sklearn.datasets import make_multilabel_classification
-from sklearn.svm import LinearSVC, SVC
-from sklearn.tree import DecisionTreeRegressor
-from sklearn.tree import DecisionTreeClassifier
-from sklearn.cluster import KMeans
-from sklearn.neighbors import KernelDensity
-from sklearn.metrics import f1_score
-from sklearn.metrics import make_scorer
-from sklearn.metrics import roc_auc_score
-from sklearn.linear_model import Ridge
-
-from sklearn.exceptions import FitFailedWarning
-
-with warnings.catch_warnings():
-    warnings.simplefilter('ignore')
-    from sklearn.grid_search import (GridSearchCV, RandomizedSearchCV,
-                                     ParameterGrid, ParameterSampler)
-    from sklearn.cross_validation import KFold, StratifiedKFold
-
-from sklearn.preprocessing import Imputer
-from sklearn.pipeline import Pipeline
-
-
-# Neither of the following two estimators inherit from BaseEstimator,
-# to test hyperparameter search on user-defined classifiers.
-class MockClassifier(object):
-    """Dummy classifier to test the cross-validation"""
-    def __init__(self, foo_param=0):
-        self.foo_param = foo_param
-
-    def fit(self, X, Y):
-        assert_true(len(X) == len(Y))
-        return self
-
-    def predict(self, T):
-        return T.shape[0]
-
-    def transform(self, X):
-        return X - self.foo_param
-
-    def inverse_transform(self, X):
-        return X + self.foo_param
-
-    predict_proba = predict
-    decision_function = predict
-
-    def score(self, X=None, Y=None):
-        if self.foo_param > 1:
-            score = 1.
-        else:
-            score = 0.
-        return score
-
-    def get_params(self, deep=False):
-        return {'foo_param': self.foo_param}
-
-    def set_params(self, **params):
-        self.foo_param = params['foo_param']
-        return self
-
-
-class LinearSVCNoScore(LinearSVC):
-    """An LinearSVC classifier that has no score method."""
-    @property
-    def score(self):
-        raise AttributeError
-
-X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
-y = np.array([1, 1, 2, 2])
-
-
-def assert_grid_iter_equals_getitem(grid):
-    assert_equal(list(grid), [grid[i] for i in range(len(grid))])
-
-
-def test_parameter_grid():
-    # Test basic properties of ParameterGrid.
-    params1 = {"foo": [1, 2, 3]}
-    grid1 = ParameterGrid(params1)
-    assert_true(isinstance(grid1, Iterable))
-    assert_true(isinstance(grid1, Sized))
-    assert_equal(len(grid1), 3)
-    assert_grid_iter_equals_getitem(grid1)
-
-    params2 = {"foo": [4, 2],
-               "bar": ["ham", "spam", "eggs"]}
-    grid2 = ParameterGrid(params2)
-    assert_equal(len(grid2), 6)
-
-    # loop to assert we can iterate over the grid multiple times
-    for i in xrange(2):
-        # tuple + chain transforms {"a": 1, "b": 2} to ("a", 1, "b", 2)
-        points = set(tuple(chain(*(sorted(p.items())))) for p in grid2)
-        assert_equal(points,
-                     set(("bar", x, "foo", y)
-                         for x, y in product(params2["bar"], params2["foo"])))
-
-    assert_grid_iter_equals_getitem(grid2)
-
-    # Special case: empty grid (useful to get default estimator settings)
-    empty = ParameterGrid({})
-    assert_equal(len(empty), 1)
-    assert_equal(list(empty), [{}])
-    assert_grid_iter_equals_getitem(empty)
-    assert_raises(IndexError, lambda: empty[1])
-
-    has_empty = ParameterGrid([{'C': [1, 10]}, {}, {'C': [.5]}])
-    assert_equal(len(has_empty), 4)
-    assert_equal(list(has_empty), [{'C': 1}, {'C': 10}, {}, {'C': .5}])
-    assert_grid_iter_equals_getitem(has_empty)
-
-
-def test_grid_search():
-    # Test that the best estimator contains the right value for foo_param
-    clf = MockClassifier()
-    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, verbose=3)
-    # make sure it selects the smallest parameter in case of ties
-    old_stdout = sys.stdout
-    sys.stdout = StringIO()
-    grid_search.fit(X, y)
-    sys.stdout = old_stdout
-    assert_equal(grid_search.best_estimator_.foo_param, 2)
-
-    for i, foo_i in enumerate([1, 2, 3]):
-        assert_true(grid_search.grid_scores_[i][0]
-                    == {'foo_param': foo_i})
-    # Smoke test the score etc:
-    grid_search.score(X, y)
-    grid_search.predict_proba(X)
-    grid_search.decision_function(X)
-    grid_search.transform(X)
-
-    # Test exception handling on scoring
-    grid_search.scoring = 'sklearn'
-    assert_raises(ValueError, grid_search.fit, X, y)
-
-
-def test_transform_inverse_transform_round_trip():
-    clf = MockClassifier()
-    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, verbose=3)
-    grid_search.fit(X, y)
-    X_round_trip = grid_search.inverse_transform(grid_search.transform(X))
-    assert_array_equal(X, X_round_trip)
-
-
-@ignore_warnings
-def test_grid_search_no_score():
-    # Test grid-search on classifier that has no score function.
-    clf = LinearSVC(random_state=0)
-    X, y = make_blobs(random_state=0, centers=2)
-    Cs = [.1, 1, 10]
-    clf_no_score = LinearSVCNoScore(random_state=0)
-    grid_search = GridSearchCV(clf, {'C': Cs}, scoring='accuracy')
-    grid_search.fit(X, y)
-
-    grid_search_no_score = GridSearchCV(clf_no_score, {'C': Cs},
-                                        scoring='accuracy')
-    # smoketest grid search
-    grid_search_no_score.fit(X, y)
-
-    # check that best params are equal
-    assert_equal(grid_search_no_score.best_params_, grid_search.best_params_)
-    # check that we can call score and that it gives the correct result
-    assert_equal(grid_search.score(X, y), grid_search_no_score.score(X, y))
-
-    # giving no scoring function raises an error
-    grid_search_no_score = GridSearchCV(clf_no_score, {'C': Cs})
-    assert_raise_message(TypeError, "no scoring", grid_search_no_score.fit,
-                         [[1]])
-
-
-def test_grid_search_score_method():
-    X, y = make_classification(n_samples=100, n_classes=2, flip_y=.2,
-                               random_state=0)
-    clf = LinearSVC(random_state=0)
-    grid = {'C': [.1]}
-
-    search_no_scoring = GridSearchCV(clf, grid, scoring=None).fit(X, y)
-    search_accuracy = GridSearchCV(clf, grid, scoring='accuracy').fit(X, y)
-    search_no_score_method_auc = GridSearchCV(LinearSVCNoScore(), grid,
-                                              scoring='roc_auc').fit(X, y)
-    search_auc = GridSearchCV(clf, grid, scoring='roc_auc').fit(X, y)
-
-    # ChangedBehaviourWarning occurred previously (prior to #9005)
-    score_no_scoring = assert_no_warnings(search_no_scoring.score, X, y)
-    score_accuracy = assert_no_warnings(search_accuracy.score, X, y)
-    score_no_score_auc = assert_no_warnings(search_no_score_method_auc.score,
-                                            X, y)
-    score_auc = assert_no_warnings(search_auc.score, X, y)
-
-    # ensure the test is sane
-    assert_true(score_auc < 1.0)
-    assert_true(score_accuracy < 1.0)
-    assert_not_equal(score_auc, score_accuracy)
-
-    assert_almost_equal(score_accuracy, score_no_scoring)
-    assert_almost_equal(score_auc, score_no_score_auc)
-
-
-def test_trivial_grid_scores():
-    # Test search over a "grid" with only one point.
-    # Non-regression test: grid_scores_ wouldn't be set by GridSearchCV.
-    clf = MockClassifier()
-    grid_search = GridSearchCV(clf, {'foo_param': [1]})
-    grid_search.fit(X, y)
-    assert_true(hasattr(grid_search, "grid_scores_"))
-
-    random_search = RandomizedSearchCV(clf, {'foo_param': [0]}, n_iter=1)
-    random_search.fit(X, y)
-    assert_true(hasattr(random_search, "grid_scores_"))
-
-
-def test_no_refit():
-    # Test that grid search can be used for model selection only
-    clf = MockClassifier()
-    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, refit=False)
-    grid_search.fit(X, y)
-    assert_true(hasattr(grid_search, "best_params_"))
-
-
-def test_grid_search_error():
-    # Test that grid search will capture errors on data with different
-    # length
-    X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
-
-    clf = LinearSVC()
-    cv = GridSearchCV(clf, {'C': [0.1, 1.0]})
-    assert_raises(ValueError, cv.fit, X_[:180], y_)
-
-
-def test_grid_search_iid():
-    # test the iid parameter
-    # noise-free simple 2d-data
-    X, y = make_blobs(centers=[[0, 0], [1, 0], [0, 1], [1, 1]], random_state=0,
-                      cluster_std=0.1, shuffle=False, n_samples=80)
-    # split dataset into two folds that are not iid
-    # first one contains data of all 4 blobs, second only from two.
-    mask = np.ones(X.shape[0], dtype=np.bool)
-    mask[np.where(y == 1)[0][::2]] = 0
-    mask[np.where(y == 2)[0][::2]] = 0
-    # this leads to perfect classification on one fold and a score of 1/3 on
-    # the other
-    svm = SVC(kernel='linear')
-    # create "cv" for splits
-    cv = [[mask, ~mask], [~mask, mask]]
-    # once with iid=True (default)
-    grid_search = GridSearchCV(svm, param_grid={'C': [1, 10]}, cv=cv)
-    grid_search.fit(X, y)
-    first = grid_search.grid_scores_[0]
-    assert_equal(first.parameters['C'], 1)
-    assert_array_almost_equal(first.cv_validation_scores, [1, 1. / 3.])
-    # for first split, 1/4 of dataset is in test, for second 3/4.
-    # take weighted average
-    assert_almost_equal(first.mean_validation_score,
-                        1 * 1. / 4. + 1. / 3. * 3. / 4.)
-
-    # once with iid=False
-    grid_search = GridSearchCV(svm, param_grid={'C': [1, 10]}, cv=cv,
-                               iid=False)
-    grid_search.fit(X, y)
-    first = grid_search.grid_scores_[0]
-    assert_equal(first.parameters['C'], 1)
-    # scores are the same as above
-    assert_array_almost_equal(first.cv_validation_scores, [1, 1. / 3.])
-    # averaged score is just mean of scores
-    assert_almost_equal(first.mean_validation_score,
-                        np.mean(first.cv_validation_scores))
-
-
-def test_grid_search_one_grid_point():
-    X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
-    param_dict = {"C": [1.0], "kernel": ["rbf"], "gamma": [0.1]}
-
-    clf = SVC()
-    cv = GridSearchCV(clf, param_dict)
-    cv.fit(X_, y_)
-
-    clf = SVC(C=1.0, kernel="rbf", gamma=0.1)
-    clf.fit(X_, y_)
-
-    assert_array_equal(clf.dual_coef_, cv.best_estimator_.dual_coef_)
-
-
-def test_grid_search_bad_param_grid():
-    param_dict = {"C": 1.0}
-    clf = SVC()
-    assert_raises(ValueError, GridSearchCV, clf, param_dict)
-
-    param_dict = {"C": []}
-    clf = SVC()
-    assert_raises(ValueError, GridSearchCV, clf, param_dict)
-
-    param_dict = {"C": np.ones(6).reshape(3, 2)}
-    clf = SVC()
-    assert_raises(ValueError, GridSearchCV, clf, param_dict)
-
-
-def test_grid_search_sparse():
-    # Test that grid search works with both dense and sparse matrices
-    X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
-
-    clf = LinearSVC()
-    cv = GridSearchCV(clf, {'C': [0.1, 1.0]})
-    cv.fit(X_[:180], y_[:180])
-    y_pred = cv.predict(X_[180:])
-    C = cv.best_estimator_.C
-
-    X_ = sp.csr_matrix(X_)
-    clf = LinearSVC()
-    cv = GridSearchCV(clf, {'C': [0.1, 1.0]})
-    cv.fit(X_[:180].tocoo(), y_[:180])
-    y_pred2 = cv.predict(X_[180:])
-    C2 = cv.best_estimator_.C
-
-    assert_true(np.mean(y_pred == y_pred2) >= .9)
-    assert_equal(C, C2)
-
-
-def test_grid_search_sparse_scoring():
-    X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
-
-    clf = LinearSVC()
-    cv = GridSearchCV(clf, {'C': [0.1, 1.0]}, scoring="f1")
-    cv.fit(X_[:180], y_[:180])
-    y_pred = cv.predict(X_[180:])
-    C = cv.best_estimator_.C
-
-    X_ = sp.csr_matrix(X_)
-    clf = LinearSVC()
-    cv = GridSearchCV(clf, {'C': [0.1, 1.0]}, scoring="f1")
-    cv.fit(X_[:180], y_[:180])
-    y_pred2 = cv.predict(X_[180:])
-    C2 = cv.best_estimator_.C
-
-    assert_array_equal(y_pred, y_pred2)
-    assert_equal(C, C2)
-    # Smoke test the score
-    # np.testing.assert_allclose(f1_score(cv.predict(X_[:180]), y[:180]),
-    #                            cv.score(X_[:180], y[:180]))
-
-    # test loss where greater is worse
-    def f1_loss(y_true_, y_pred_):
-        return -f1_score(y_true_, y_pred_)
-    F1Loss = make_scorer(f1_loss, greater_is_better=False)
-    cv = GridSearchCV(clf, {'C': [0.1, 1.0]}, scoring=F1Loss)
-    cv.fit(X_[:180], y_[:180])
-    y_pred3 = cv.predict(X_[180:])
-    C3 = cv.best_estimator_.C
-
-    assert_equal(C, C3)
-    assert_array_equal(y_pred, y_pred3)
-
-
-def test_grid_search_precomputed_kernel():
-    # Test that grid search works when the input features are given in the
-    # form of a precomputed kernel matrix
-    X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
-
-    # compute the training kernel matrix corresponding to the linear kernel
-    K_train = np.dot(X_[:180], X_[:180].T)
-    y_train = y_[:180]
-
-    clf = SVC(kernel='precomputed')
-    cv = GridSearchCV(clf, {'C': [0.1, 1.0]})
-    cv.fit(K_train, y_train)
-
-    assert_true(cv.best_score_ >= 0)
-
-    # compute the test kernel matrix
-    K_test = np.dot(X_[180:], X_[:180].T)
-    y_test = y_[180:]
-
-    y_pred = cv.predict(K_test)
-
-    assert_true(np.mean(y_pred == y_test) >= 0)
-
-    # test error is raised when the precomputed kernel is not array-like
-    # or sparse
-    assert_raises(ValueError, cv.fit, K_train.tolist(), y_train)
-
-
-def test_grid_search_precomputed_kernel_error_nonsquare():
-    # Test that grid search returns an error with a non-square precomputed
-    # training kernel matrix
-    K_train = np.zeros((10, 20))
-    y_train = np.ones((10, ))
-    clf = SVC(kernel='precomputed')
-    cv = GridSearchCV(clf, {'C': [0.1, 1.0]})
-    assert_raises(ValueError, cv.fit, K_train, y_train)
-
-
-def test_grid_search_precomputed_kernel_error_kernel_function():
-    # Test that grid search returns an error when using a kernel_function
-    X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
-    kernel_function = lambda x1, x2: np.dot(x1, x2.T)
-    clf = SVC(kernel=kernel_function)
-    cv = GridSearchCV(clf, {'C': [0.1, 1.0]})
-    assert_raises(ValueError, cv.fit, X_, y_)
-
-
-class BrokenClassifier(BaseEstimator):
-    """Broken classifier that cannot be fit twice"""
-
-    def __init__(self, parameter=None):
-        self.parameter = parameter
-
-    def fit(self, X, y):
-        assert_true(not hasattr(self, 'has_been_fit_'))
-        self.has_been_fit_ = True
-
-    def predict(self, X):
-        return np.zeros(X.shape[0])
-
-
-@ignore_warnings
-def test_refit():
-    # Regression test for bug in refitting
-    # Simulates re-fitting a broken estimator; this used to break with
-    # sparse SVMs.
-    X = np.arange(100).reshape(10, 10)
-    y = np.array([0] * 5 + [1] * 5)
-
-    clf = GridSearchCV(BrokenClassifier(), [{'parameter': [0, 1]}],
-                       scoring="precision", refit=True)
-    clf.fit(X, y)
-
-
-def test_gridsearch_nd():
-    # Pass X as list in GridSearchCV
-    X_4d = np.arange(10 * 5 * 3 * 2).reshape(10, 5, 3, 2)
-    y_3d = np.arange(10 * 7 * 11).reshape(10, 7, 11)
-    check_X = lambda x: x.shape[1:] == (5, 3, 2)
-    check_y = lambda x: x.shape[1:] == (7, 11)
-    clf = CheckingClassifier(check_X=check_X, check_y=check_y)
-    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]})
-    grid_search.fit(X_4d, y_3d).score(X, y)
-    assert_true(hasattr(grid_search, "grid_scores_"))
-
-
-def test_X_as_list():
-    # Pass X as list in GridSearchCV
-    X = np.arange(100).reshape(10, 10)
-    y = np.array([0] * 5 + [1] * 5)
-
-    clf = CheckingClassifier(check_X=lambda x: isinstance(x, list))
-    cv = KFold(n=len(X), n_folds=3)
-    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, cv=cv)
-    grid_search.fit(X.tolist(), y).score(X, y)
-    assert_true(hasattr(grid_search, "grid_scores_"))
-
-
-def test_y_as_list():
-    # Pass y as list in GridSearchCV
-    X = np.arange(100).reshape(10, 10)
-    y = np.array([0] * 5 + [1] * 5)
-
-    clf = CheckingClassifier(check_y=lambda x: isinstance(x, list))
-    cv = KFold(n=len(X), n_folds=3)
-    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, cv=cv)
-    grid_search.fit(X, y.tolist()).score(X, y)
-    assert_true(hasattr(grid_search, "grid_scores_"))
-
-
-def test_pandas_input():
-    # check cross_val_score doesn't destroy pandas dataframe
-    types = [(MockDataFrame, MockDataFrame)]
-    try:
-        from pandas import Series, DataFrame
-        types.append((DataFrame, Series))
-    except ImportError:
-        pass
-
-    X = np.arange(100).reshape(10, 10)
-    y = np.array([0] * 5 + [1] * 5)
-
-    for InputFeatureType, TargetType in types:
-        # X dataframe, y series
-        X_df, y_ser = InputFeatureType(X), TargetType(y)
-        check_df = lambda x: isinstance(x, InputFeatureType)
-        check_series = lambda x: isinstance(x, TargetType)
-        clf = CheckingClassifier(check_X=check_df, check_y=check_series)
-
-        grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]})
-        grid_search.fit(X_df, y_ser).score(X_df, y_ser)
-        grid_search.predict(X_df)
-        assert_true(hasattr(grid_search, "grid_scores_"))
-
-
-def test_unsupervised_grid_search():
-    # test grid-search with unsupervised estimator
-    X, y = make_blobs(random_state=0)
-    km = KMeans(random_state=0)
-    grid_search = GridSearchCV(km, param_grid=dict(n_clusters=[2, 3, 4]),
-                               scoring='adjusted_rand_score')
-    grid_search.fit(X, y)
-    # ARI can find the right number :)
-    assert_equal(grid_search.best_params_["n_clusters"], 3)
-
-    # Now without a score, and without y
-    grid_search = GridSearchCV(km, param_grid=dict(n_clusters=[2, 3, 4]))
-    grid_search.fit(X)
-    assert_equal(grid_search.best_params_["n_clusters"], 4)
-
-
-def test_gridsearch_no_predict():
-    # test grid-search with an estimator without predict.
-    # slight duplication of a test from KDE
-    def custom_scoring(estimator, X):
-        return 42 if estimator.bandwidth == .1 else 0
-    X, _ = make_blobs(cluster_std=.1, random_state=1,
-                      centers=[[0, 1], [1, 0], [0, 0]])
-    search = GridSearchCV(KernelDensity(),
-                          param_grid=dict(bandwidth=[.01, .1, 1]),
-                          scoring=custom_scoring)
-    search.fit(X)
-    assert_equal(search.best_params_['bandwidth'], .1)
-    assert_equal(search.best_score_, 42)
-
-
-def test_param_sampler():
-    # test basic properties of param sampler
-    param_distributions = {"kernel": ["rbf", "linear"],
-                           "C": uniform(0, 1)}
-    sampler = ParameterSampler(param_distributions=param_distributions,
-                               n_iter=10, random_state=0)
-    samples = [x for x in sampler]
-    assert_equal(len(samples), 10)
-    for sample in samples:
-        assert_true(sample["kernel"] in ["rbf", "linear"])
-        assert_true(0 <= sample["C"] <= 1)
-
-
-def test_randomized_search_grid_scores():
-    # Make a dataset with a lot of noise to get various kind of prediction
-    # errors across CV folds and parameter settings
-    X, y = make_classification(n_samples=200, n_features=100, n_informative=3,
-                               random_state=0)
-
-    # XXX: as of today (scipy 0.12) it's not possible to set the random seed
-    # of scipy.stats distributions: the assertions in this test should thus
-    # not depend on the randomization
-    params = dict(C=expon(scale=10),
-                  gamma=expon(scale=0.1))
-    n_cv_iter = 3
-    n_search_iter = 30
-    search = RandomizedSearchCV(SVC(), n_iter=n_search_iter, cv=n_cv_iter,
-                                param_distributions=params, iid=False)
-    search.fit(X, y)
-    assert_equal(len(search.grid_scores_), n_search_iter)
-
-    # Check consistency of the structure of each cv_score item
-    for cv_score in search.grid_scores_:
-        assert_equal(len(cv_score.cv_validation_scores), n_cv_iter)
-        # Because we set iid to False, the mean_validation score is the
-        # mean of the fold mean scores instead of the aggregate sample-wise
-        # mean score
-        assert_almost_equal(np.mean(cv_score.cv_validation_scores),
-                            cv_score.mean_validation_score)
-        assert_equal(list(sorted(cv_score.parameters.keys())),
-                     list(sorted(params.keys())))
-
-    # Check the consistency with the best_score_ and best_params_ attributes
-    sorted_grid_scores = list(sorted(search.grid_scores_,
-                              key=lambda x: x.mean_validation_score))
-    best_score = sorted_grid_scores[-1].mean_validation_score
-    assert_equal(search.best_score_, best_score)
-
-    tied_best_params = [s.parameters for s in sorted_grid_scores
-                        if s.mean_validation_score == best_score]
-    assert_true(search.best_params_ in tied_best_params,
-                "best_params_={0} is not part of the"
-                " tied best models: {1}".format(
-                    search.best_params_, tied_best_params))
-
-
-def test_grid_search_score_consistency():
-    # test that correct scores are used
-    clf = LinearSVC(random_state=0)
-    X, y = make_blobs(random_state=0, centers=2)
-    Cs = [.1, 1, 10]
-    for score in ['f1', 'roc_auc']:
-        grid_search = GridSearchCV(clf, {'C': Cs}, scoring=score)
-        grid_search.fit(X, y)
-        cv = StratifiedKFold(n_folds=3, y=y)
-        for C, scores in zip(Cs, grid_search.grid_scores_):
-            clf.set_params(C=C)
-            scores = scores[2]  # get the separate runs from grid scores
-            i = 0
-            for train, test in cv:
-                clf.fit(X[train], y[train])
-                if score == "f1":
-                    correct_score = f1_score(y[test], clf.predict(X[test]))
-                elif score == "roc_auc":
-                    dec = clf.decision_function(X[test])
-                    correct_score = roc_auc_score(y[test], dec)
-                assert_almost_equal(correct_score, scores[i])
-                i += 1
-
-
-def test_pickle():
-    # Test that a fit search can be pickled
-    clf = MockClassifier()
-    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, refit=True)
-    grid_search.fit(X, y)
-    pickle.dumps(grid_search)  # smoke test
-
-    random_search = RandomizedSearchCV(clf, {'foo_param': [1, 2, 3]},
-                                       refit=True, n_iter=3)
-    random_search.fit(X, y)
-    pickle.dumps(random_search)  # smoke test
-
-
-def test_grid_search_with_multioutput_data():
-    # Test search with multi-output estimator
-
-    X, y = make_multilabel_classification(random_state=0)
-
-    est_parameters = {"max_depth": [1, 2, 3, 4]}
-    cv = KFold(y.shape[0], random_state=0)
-
-    estimators = [DecisionTreeRegressor(random_state=0),
-                  DecisionTreeClassifier(random_state=0)]
-
-    # Test with grid search cv
-    for est in estimators:
-        grid_search = GridSearchCV(est, est_parameters, cv=cv)
-        grid_search.fit(X, y)
-        for parameters, _, cv_validation_scores in grid_search.grid_scores_:
-            est.set_params(**parameters)
-
-            for i, (train, test) in enumerate(cv):
-                est.fit(X[train], y[train])
-                correct_score = est.score(X[test], y[test])
-                assert_almost_equal(correct_score,
-                                    cv_validation_scores[i])
-
-    # Test with a randomized search
-    for est in estimators:
-        random_search = RandomizedSearchCV(est, est_parameters,
-                                           cv=cv, n_iter=3)
-        random_search.fit(X, y)
-        for parameters, _, cv_validation_scores in random_search.grid_scores_:
-            est.set_params(**parameters)
-
-            for i, (train, test) in enumerate(cv):
-                est.fit(X[train], y[train])
-                correct_score = est.score(X[test], y[test])
-                assert_almost_equal(correct_score,
-                                    cv_validation_scores[i])
-
-
-def test_predict_proba_disabled():
-    # Test predict_proba when disabled on estimator.
-    X = np.arange(20).reshape(5, -1)
-    y = [0, 0, 1, 1, 1]
-    clf = SVC(gamma='scale', probability=False)
-    gs = GridSearchCV(clf, {}, cv=2).fit(X, y)
-    assert_false(hasattr(gs, "predict_proba"))
-
-
-def test_grid_search_allows_nans():
-    # Test GridSearchCV with Imputer
-    X = np.arange(20, dtype=np.float64).reshape(5, -1)
-    X[2, :] = np.nan
-    y = [0, 0, 1, 1, 1]
-    p = Pipeline([
-        ('imputer', Imputer(strategy='mean', missing_values='NaN')),
-        ('classifier', MockClassifier()),
-    ])
-    GridSearchCV(p, {'classifier__foo_param': [1, 2, 3]}, cv=2).fit(X, y)
-
-
-class FailingClassifier(BaseEstimator):
-    """Classifier that raises a ValueError on fit()"""
-
-    FAILING_PARAMETER = 2
-
-    def __init__(self, parameter=None):
-        self.parameter = parameter
-
-    def fit(self, X, y=None):
-        if self.parameter == FailingClassifier.FAILING_PARAMETER:
-            raise ValueError("Failing classifier failed as required")
-
-    def predict(self, X):
-        return np.zeros(X.shape[0])
-
-
-def test_grid_search_failing_classifier():
-    # GridSearchCV with on_error != 'raise'
-    # Ensures that a warning is raised and score reset where appropriate.
-
-    X, y = make_classification(n_samples=20, n_features=10, random_state=0)
-
-    clf = FailingClassifier()
-
-    # refit=False because we only want to check that errors caused by fits
-    # to individual folds will be caught and warnings raised instead. If
-    # refit was done, then an exception would be raised on refit and not
-    # caught by grid_search (expected behavior), and this would cause an
-    # error in this test.
-    gs = GridSearchCV(clf, [{'parameter': [0, 1, 2]}], scoring='accuracy',
-                      refit=False, error_score=0.0)
-
-    assert_warns(FitFailedWarning, gs.fit, X, y)
-
-    # Ensure that grid scores were set to zero as required for those fits
-    # that are expected to fail.
-    assert all(np.all(this_point.cv_validation_scores == 0.0)
-               for this_point in gs.grid_scores_
-               if this_point.parameters['parameter'] ==
-               FailingClassifier.FAILING_PARAMETER)
-
-    gs = GridSearchCV(clf, [{'parameter': [0, 1, 2]}], scoring='accuracy',
-                      refit=False, error_score=float('nan'))
-    assert_warns(FitFailedWarning, gs.fit, X, y)
-    assert all(np.all(np.isnan(this_point.cv_validation_scores))
-               for this_point in gs.grid_scores_
-               if this_point.parameters['parameter'] ==
-               FailingClassifier.FAILING_PARAMETER)
-
-
-def test_grid_search_failing_classifier_raise():
-    # GridSearchCV with on_error == 'raise' raises the error
-
-    X, y = make_classification(n_samples=20, n_features=10, random_state=0)
-
-    clf = FailingClassifier()
-
-    # refit=False because we want to test the behaviour of the grid search part
-    gs = GridSearchCV(clf, [{'parameter': [0, 1, 2]}], scoring='accuracy',
-                      refit=False, error_score='raise')
-
-    # FailingClassifier issues a ValueError so this is what we look for.
-    assert_raises(ValueError, gs.fit, X, y)
-
-
-def test_parameters_sampler_replacement():
-    # raise error if n_iter too large
-    params = {'first': [0, 1], 'second': ['a', 'b', 'c']}
-    sampler = ParameterSampler(params, n_iter=7)
-    assert_raises(ValueError, list, sampler)
-    # degenerates to GridSearchCV if n_iter the same as grid_size
-    sampler = ParameterSampler(params, n_iter=6)
-    samples = list(sampler)
-    assert_equal(len(samples), 6)
-    for values in ParameterGrid(params):
-        assert_true(values in samples)
-
-    # test sampling without replacement in a large grid
-    params = {'a': range(10), 'b': range(10), 'c': range(10)}
-    sampler = ParameterSampler(params, n_iter=99, random_state=42)
-    samples = list(sampler)
-    assert_equal(len(samples), 99)
-    hashable_samples = ["a%db%dc%d" % (p['a'], p['b'], p['c'])
-                        for p in samples]
-    assert_equal(len(set(hashable_samples)), 99)
-
-    # doesn't go into infinite loops
-    params_distribution = {'first': bernoulli(.5), 'second': ['a', 'b', 'c']}
-    sampler = ParameterSampler(params_distribution, n_iter=7)
-    samples = list(sampler)
-    assert_equal(len(samples), 7)
-
-
-def test_classes__property():
-    # Test that classes_ property matches best_esimator_.classes_
-    X = np.arange(100).reshape(10, 10)
-    y = np.array([0] * 5 + [1] * 5)
-    Cs = [.1, 1, 10]
-
-    grid_search = GridSearchCV(LinearSVC(random_state=0), {'C': Cs})
-    grid_search.fit(X, y)
-    assert_array_equal(grid_search.best_estimator_.classes_,
-                       grid_search.classes_)
-
-    # Test that regressors do not have a classes_ attribute
-    grid_search = GridSearchCV(Ridge(), {'alpha': [1.0, 2.0]})
-    grid_search.fit(X, y)
-    assert_false(hasattr(grid_search, 'classes_'))
diff --git a/sklearn/tests/test_impute.py b/sklearn/tests/test_impute.py
index 954a016a835b..f5c42f744348 100644
--- a/sklearn/tests/test_impute.py
+++ b/sklearn/tests/test_impute.py
@@ -5,13 +5,15 @@
 import numpy as np
 from scipy import sparse
 
+import io
+
 from sklearn.utils.testing import assert_allclose
+from sklearn.utils.testing import assert_allclose_dense_sparse
 from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_array_almost_equal
-from sklearn.utils.testing import assert_raises
 from sklearn.utils.testing import assert_false
 
-from sklearn.impute import SimpleImputer, MICEImputer
+from sklearn.impute import SimpleImputer, ChainedImputer
 from sklearn.dummy import DummyRegressor
 from sklearn.linear_model import BayesianRidge, ARDRegression
 from sklearn.pipeline import Pipeline
@@ -24,18 +26,17 @@ def _check_statistics(X, X_true,
                       strategy, statistics, missing_values):
     """Utility function for testing imputation for a given strategy.
 
-    Test:
-        - along the two axes
-        - with dense and sparse arrays
+    Test with dense and sparse arrays
 
     Check that:
         - the statistics (mean, median, mode) are correct
         - the missing values are imputed correctly"""
 
     err_msg = "Parameters: strategy = %s, missing_values = %s, " \
-              "axis = {0}, sparse = {1}" % (strategy, missing_values)
+              "sparse = {0}" % (strategy, missing_values)
 
     assert_ae = assert_array_equal
+
     if X.dtype.kind == 'f' or X_true.dtype.kind == 'f':
         assert_ae = assert_array_almost_equal
 
@@ -43,8 +44,8 @@ def _check_statistics(X, X_true,
     imputer = SimpleImputer(missing_values, strategy=strategy)
     X_trans = imputer.fit(X).transform(X.copy())
     assert_ae(imputer.statistics_, statistics,
-              err_msg=err_msg.format(0, False))
-    assert_ae(X_trans, X_true, err_msg=err_msg.format(0, False))
+              err_msg=err_msg.format(False))
+    assert_ae(X_trans, X_true, err_msg=err_msg.format(False))
 
     # Sparse matrix
     imputer = SimpleImputer(missing_values, strategy=strategy)
@@ -55,8 +56,8 @@ def _check_statistics(X, X_true,
         X_trans = X_trans.toarray()
 
     assert_ae(imputer.statistics_, statistics,
-              err_msg=err_msg.format(0, True))
-    assert_ae(X_trans, X_true, err_msg=err_msg.format(0, True))
+              err_msg=err_msg.format(True))
+    assert_ae(X_trans, X_true, err_msg=err_msg.format(True))
 
 
 def test_imputation_shape():
@@ -64,18 +65,38 @@ def test_imputation_shape():
     X = np.random.randn(10, 2)
     X[::2] = np.nan
 
-    for strategy in ['mean', 'median', 'most_frequent']:
+    for strategy in ['mean', 'median', 'most_frequent', "constant"]:
         imputer = SimpleImputer(strategy=strategy)
         X_imputed = imputer.fit_transform(sparse.csr_matrix(X))
         assert X_imputed.shape == (10, 2)
         X_imputed = imputer.fit_transform(X)
         assert X_imputed.shape == (10, 2)
 
-        mice_imputer = MICEImputer(initial_strategy=strategy)
-        X_imputed = mice_imputer.fit_transform(X)
+        chained_imputer = ChainedImputer(initial_strategy=strategy)
+        X_imputed = chained_imputer.fit_transform(X)
         assert X_imputed.shape == (10, 2)
 
 
+@pytest.mark.parametrize("strategy", ["const", 101, None])
+def test_imputation_error_invalid_strategy(strategy):
+    X = np.ones((3, 5))
+    X[0, 0] = np.nan
+
+    with pytest.raises(ValueError, match=str(strategy)):
+        imputer = SimpleImputer(strategy=strategy)
+        imputer.fit_transform(X)
+
+
+@pytest.mark.parametrize("strategy", ["mean", "median", "most_frequent"])
+def test_imputation_deletion_warning(strategy):
+    X = np.ones((3, 5))
+    X[:, 0] = np.nan
+
+    with pytest.warns(UserWarning, match="Deleting"):
+        imputer = SimpleImputer(strategy=strategy, verbose=True)
+        imputer.fit_transform(X)
+
+
 def safe_median(arr, *args, **kwargs):
     # np.median([]) raises a TypeError for numpy >= 1.10.1
     length = arr.size if hasattr(arr, 'size') else len(arr)
@@ -101,9 +122,10 @@ def test_imputation_mean_median():
     values = np.arange(1, shape[0] + 1)
     values[4::2] = - values[4::2]
 
-    tests = [("mean", "NaN", lambda z, v, p: safe_mean(np.hstack((z, v)))),
+    tests = [("mean", np.nan, lambda z, v, p: safe_mean(np.hstack((z, v)))),
              ("mean", 0, lambda z, v, p: np.mean(v)),
-             ("median", "NaN", lambda z, v, p: safe_median(np.hstack((z, v)))),
+             ("median", np.nan,
+              lambda z, v, p: safe_median(np.hstack((z, v)))),
              ("median", 0, lambda z, v, p: np.median(v))]
 
     for strategy, test_missing_values, true_value_fun in tests:
@@ -184,7 +206,37 @@ def test_imputation_median_special_cases():
     statistics_median = [0, 5, 0, -2.5, 2.5, 4.5, -4.5, .5]
 
     _check_statistics(X, X_imputed_median, "median",
-                      statistics_median, 'NaN')
+                      statistics_median, np.nan)
+
+
+@pytest.mark.parametrize("strategy", ["mean", "median"])
+@pytest.mark.parametrize("dtype", [None, object, str])
+def test_imputation_mean_median_error_invalid_type(strategy, dtype):
+    X = np.array([["a", "b", 3],
+                  [4, "e", 6],
+                  ["g", "h", 9]], dtype=dtype)
+
+    with pytest.raises(ValueError, match="non-numeric data"):
+        imputer = SimpleImputer(strategy=strategy)
+        imputer.fit_transform(X)
+
+
+@pytest.mark.parametrize("strategy", ["constant", "most_frequent"])
+@pytest.mark.parametrize("dtype", [str, np.dtype('U'), np.dtype('S')])
+def test_imputation_const_mostf_error_invalid_types(strategy, dtype):
+    # Test imputation on non-numeric data using "most_frequent" and "constant"
+    # strategy
+    X = np.array([
+        [np.nan, np.nan, "a", "f"],
+        [np.nan, "c", np.nan, "d"],
+        [np.nan, "b", "d", np.nan],
+        [np.nan, "c", "d", "h"],
+    ], dtype=dtype)
+
+    err_msg = "SimpleImputer does not support data"
+    with pytest.raises(ValueError, match=err_msg):
+        imputer = SimpleImputer(strategy=strategy)
+        imputer.fit(X).transform(X)
 
 
 def test_imputation_most_frequent():
@@ -210,6 +262,169 @@ def test_imputation_most_frequent():
     _check_statistics(X, X_true, "most_frequent", [np.nan, 2, 3, 3], -1)
 
 
+@pytest.mark.parametrize("marker", [None, np.nan, "NAN", "", 0])
+def test_imputation_most_frequent_objects(marker):
+    # Test imputation using the most-frequent strategy.
+    X = np.array([
+        [marker, marker, "a", "f"],
+        [marker, "c", marker, "d"],
+        [marker, "b", "d", marker],
+        [marker, "c", "d", "h"],
+    ], dtype=object)
+
+    X_true = np.array([
+        ["c", "a", "f"],
+        ["c", "d", "d"],
+        ["b", "d", "d"],
+        ["c", "d", "h"],
+    ], dtype=object)
+
+    imputer = SimpleImputer(missing_values=marker,
+                            strategy="most_frequent")
+    X_trans = imputer.fit(X).transform(X)
+
+    assert_array_equal(X_trans, X_true)
+
+
+@pytest.mark.parametrize("dtype", [object, "category"])
+def test_imputation_most_frequent_pandas(dtype):
+    # Test imputation using the most frequent strategy on pandas df
+    pd = pytest.importorskip("pandas")
+
+    f = io.StringIO(u"Cat1,Cat2,Cat3,Cat4\n"
+                    ",i,x,\n"
+                    "a,,y,\n"
+                    "a,j,,\n"
+                    "b,j,x,")
+
+    df = pd.read_csv(f, dtype=dtype)
+
+    X_true = np.array([
+        ["a", "i", "x"],
+        ["a", "j", "y"],
+        ["a", "j", "x"],
+        ["b", "j", "x"]
+    ], dtype=object)
+
+    imputer = SimpleImputer(strategy="most_frequent")
+    X_trans = imputer.fit_transform(df)
+
+    assert_array_equal(X_trans, X_true)
+
+
+@pytest.mark.parametrize("X_data, missing_value", [(1, 0), (1., np.nan)])
+def test_imputation_constant_error_invalid_type(X_data, missing_value):
+    # Verify that exceptions are raised on invalid fill_value type
+    X = np.full((3, 5), X_data)
+    X[0, 0] = missing_value
+
+    with pytest.raises(ValueError, match="imputing numerical"):
+        imputer = SimpleImputer(missing_values=missing_value,
+                                strategy="constant",
+                                fill_value="x")
+        imputer.fit_transform(X)
+
+
+def test_imputation_constant_integer():
+    # Test imputation using the constant strategy on integers
+    X = np.array([
+        [-1, 2, 3, -1],
+        [4, -1, 5, -1],
+        [6, 7, -1, -1],
+        [8, 9, 0, -1]
+    ])
+
+    X_true = np.array([
+        [0, 2, 3, 0],
+        [4, 0, 5, 0],
+        [6, 7, 0, 0],
+        [8, 9, 0, 0]
+    ])
+
+    imputer = SimpleImputer(missing_values=-1, strategy="constant",
+                            fill_value=0)
+    X_trans = imputer.fit_transform(X)
+
+    assert_array_equal(X_trans, X_true)
+
+
+@pytest.mark.parametrize("array_constructor", [sparse.csr_matrix, np.asarray])
+def test_imputation_constant_float(array_constructor):
+    # Test imputation using the constant strategy on floats
+    X = np.array([
+        [np.nan, 1.1, 0, np.nan],
+        [1.2, np.nan, 1.3, np.nan],
+        [0, 0, np.nan, np.nan],
+        [1.4, 1.5, 0, np.nan]
+    ])
+
+    X_true = np.array([
+        [-1, 1.1, 0, -1],
+        [1.2, -1, 1.3, -1],
+        [0, 0, -1, -1],
+        [1.4, 1.5, 0, -1]
+    ])
+
+    X = array_constructor(X)
+
+    X_true = array_constructor(X_true)
+
+    imputer = SimpleImputer(strategy="constant", fill_value=-1)
+    X_trans = imputer.fit_transform(X)
+
+    assert_allclose_dense_sparse(X_trans, X_true)
+
+
+@pytest.mark.parametrize("marker", [None, np.nan, "NAN", "", 0])
+def test_imputation_constant_object(marker):
+    # Test imputation using the constant strategy on objects
+    X = np.array([
+        [marker, "a", "b", marker],
+        ["c", marker, "d", marker],
+        ["e", "f", marker, marker],
+        ["g", "h", "i", marker]
+    ], dtype=object)
+
+    X_true = np.array([
+        ["missing", "a", "b", "missing"],
+        ["c", "missing", "d", "missing"],
+        ["e", "f", "missing", "missing"],
+        ["g", "h", "i", "missing"]
+    ], dtype=object)
+
+    imputer = SimpleImputer(missing_values=marker, strategy="constant",
+                            fill_value="missing")
+    X_trans = imputer.fit_transform(X)
+
+    assert_array_equal(X_trans, X_true)
+
+
+@pytest.mark.parametrize("dtype", [object, "category"])
+def test_imputation_constant_pandas(dtype):
+    # Test imputation using the constant strategy on pandas df
+    pd = pytest.importorskip("pandas")
+
+    f = io.StringIO(u"Cat1,Cat2,Cat3,Cat4\n"
+                    ",i,x,\n"
+                    "a,,y,\n"
+                    "a,j,,\n"
+                    "b,j,x,")
+
+    df = pd.read_csv(f, dtype=dtype)
+
+    X_true = np.array([
+        ["missing_value", "i", "x", "missing_value"],
+        ["a", "missing_value", "y", "missing_value"],
+        ["a", "j", "missing_value", "missing_value"],
+        ["b", "j", "x", "missing_value"]
+    ], dtype=object)
+
+    imputer = SimpleImputer(strategy="constant")
+    X_trans = imputer.fit_transform(df)
+
+    assert_array_equal(X_trans, X_true)
+
+
 def test_imputation_pipeline_grid_search():
     # Test imputation within a pipeline + gridsearch.
     pipeline = Pipeline([('imputer', SimpleImputer(missing_values=0)),
@@ -271,7 +486,7 @@ def test_imputation_copy():
     # made, even if copy=False.
 
 
-def test_mice_rank_one():
+def test_chained_imputer_rank_one():
     rng = np.random.RandomState(0)
     d = 100
     A = rng.rand(d, 1)
@@ -281,10 +496,10 @@ def test_mice_rank_one():
     X_missing = X.copy()
     X_missing[nan_mask] = np.nan
 
-    imputer = MICEImputer(n_imputations=5,
-                          n_burn_in=5,
-                          verbose=True,
-                          random_state=rng)
+    imputer = ChainedImputer(n_imputations=5,
+                             n_burn_in=5,
+                             verbose=True,
+                             random_state=rng)
     X_filled = imputer.fit_transform(X_missing)
     assert_allclose(X_filled, X, atol=0.001)
 
@@ -293,22 +508,22 @@ def test_mice_rank_one():
     "imputation_order",
     ['random', 'roman', 'ascending', 'descending', 'arabic']
 )
-def test_mice_imputation_order(imputation_order):
+def test_chained_imputer_imputation_order(imputation_order):
     rng = np.random.RandomState(0)
     n = 100
     d = 10
     X = sparse_random_matrix(n, d, density=0.10, random_state=rng).toarray()
-    X[:, 0] = 1  # this column should not be discarded by MICEImputer
-
-    imputer = MICEImputer(missing_values=0,
-                          n_imputations=1,
-                          n_burn_in=1,
-                          n_nearest_features=5,
-                          min_value=0,
-                          max_value=1,
-                          verbose=False,
-                          imputation_order=imputation_order,
-                          random_state=rng)
+    X[:, 0] = 1  # this column should not be discarded by ChainedImputer
+
+    imputer = ChainedImputer(missing_values=0,
+                             n_imputations=1,
+                             n_burn_in=1,
+                             n_nearest_features=5,
+                             min_value=0,
+                             max_value=1,
+                             verbose=False,
+                             imputation_order=imputation_order,
+                             random_state=rng)
     imputer.fit_transform(X)
     ordered_idx = [i.feat_idx for i in imputer.imputation_sequence_]
     if imputation_order == 'roman':
@@ -327,18 +542,18 @@ def test_mice_imputation_order(imputation_order):
     "predictor",
     [DummyRegressor(), BayesianRidge(), ARDRegression()]
 )
-def test_mice_predictors(predictor):
+def test_chained_imputer_predictors(predictor):
     rng = np.random.RandomState(0)
 
     n = 100
     d = 10
     X = sparse_random_matrix(n, d, density=0.10, random_state=rng).toarray()
 
-    imputer = MICEImputer(missing_values=0,
-                          n_imputations=1,
-                          n_burn_in=1,
-                          predictor=predictor,
-                          random_state=rng)
+    imputer = ChainedImputer(missing_values=0,
+                             n_imputations=1,
+                             n_burn_in=1,
+                             predictor=predictor,
+                             random_state=rng)
     imputer.fit_transform(X)
 
     # check that types are correct for predictors
@@ -351,19 +566,19 @@ def test_mice_predictors(predictor):
     assert len(set(hashes)) == len(hashes)
 
 
-def test_mice_clip():
+def test_chained_imputer_clip():
     rng = np.random.RandomState(0)
     n = 100
     d = 10
     X = sparse_random_matrix(n, d, density=0.10,
                              random_state=rng).toarray()
 
-    imputer = MICEImputer(missing_values=0,
-                          n_imputations=1,
-                          n_burn_in=1,
-                          min_value=0.1,
-                          max_value=0.2,
-                          random_state=rng)
+    imputer = ChainedImputer(missing_values=0,
+                             n_imputations=1,
+                             n_burn_in=1,
+                             min_value=0.1,
+                             max_value=0.2,
+                             random_state=rng)
 
     Xt = imputer.fit_transform(X)
     assert_allclose(np.min(Xt[X == 0]), 0.1)
@@ -375,7 +590,7 @@ def test_mice_clip():
     "strategy",
     ["mean", "median", "most_frequent"]
 )
-def test_mice_missing_at_transform(strategy):
+def test_chained_imputer_missing_at_transform(strategy):
     rng = np.random.RandomState(0)
     n = 100
     d = 10
@@ -385,31 +600,31 @@ def test_mice_missing_at_transform(strategy):
     X_train[:, 0] = 1  # definitely no missing values in 0th column
     X_test[0, 0] = 0  # definitely missing value in 0th column
 
-    mice = MICEImputer(missing_values=0,
-                       n_imputations=1,
-                       n_burn_in=1,
-                       initial_strategy=strategy,
-                       random_state=rng).fit(X_train)
+    imputer = ChainedImputer(missing_values=0,
+                             n_imputations=1,
+                             n_burn_in=1,
+                             initial_strategy=strategy,
+                             random_state=rng).fit(X_train)
     initial_imputer = SimpleImputer(missing_values=0,
                                     strategy=strategy).fit(X_train)
 
-    # if there were no missing values at time of fit, then mice will
+    # if there were no missing values at time of fit, then imputer will
     # only use the initial imputer for that feature at transform
-    assert np.all(mice.transform(X_test)[:, 0] ==
+    assert np.all(imputer.transform(X_test)[:, 0] ==
                   initial_imputer.transform(X_test)[:, 0])
 
 
-def test_mice_transform_stochasticity():
+def test_chained_imputer_transform_stochasticity():
     rng = np.random.RandomState(0)
     n = 100
     d = 10
     X = sparse_random_matrix(n, d, density=0.10,
                              random_state=rng).toarray()
 
-    imputer = MICEImputer(missing_values=0,
-                          n_imputations=1,
-                          n_burn_in=1,
-                          random_state=rng)
+    imputer = ChainedImputer(missing_values=0,
+                             n_imputations=1,
+                             n_burn_in=1,
+                             random_state=rng)
     imputer.fit(X)
 
     X_fitted_1 = imputer.transform(X)
@@ -419,12 +634,12 @@ def test_mice_transform_stochasticity():
     assert np.mean(X_fitted_1) != pytest.approx(np.mean(X_fitted_2))
 
 
-def test_mice_no_missing():
+def test_chained_imputer_no_missing():
     rng = np.random.RandomState(0)
     X = rng.rand(100, 100)
     X[:, 0] = np.nan
-    m1 = MICEImputer(n_imputations=10, random_state=rng)
-    m2 = MICEImputer(n_imputations=10, random_state=rng)
+    m1 = ChainedImputer(n_imputations=10, random_state=rng)
+    m2 = ChainedImputer(n_imputations=10, random_state=rng)
     pred1 = m1.fit(X).transform(X)
     pred2 = m2.fit_transform(X)
     # should exclude the first column entirely
@@ -437,7 +652,7 @@ def test_mice_no_missing():
     "rank",
     [3, 5]
 )
-def test_mice_transform_recovery(rank):
+def test_chained_imputer_transform_recovery(rank):
     rng = np.random.RandomState(0)
     n = 100
     d = 100
@@ -455,15 +670,15 @@ def test_mice_transform_recovery(rank):
     X_test_filled = X_filled[n:]
     X_test = X_missing[n:]
 
-    imputer = MICEImputer(n_imputations=10,
-                          n_burn_in=10,
-                          verbose=True,
-                          random_state=rng).fit(X_train)
+    imputer = ChainedImputer(n_imputations=10,
+                             n_burn_in=10,
+                             verbose=True,
+                             random_state=rng).fit(X_train)
     X_test_est = imputer.transform(X_test)
     assert_allclose(X_test_filled, X_test_est, rtol=1e-5, atol=0.1)
 
 
-def test_mice_additive_matrix():
+def test_chained_imputer_additive_matrix():
     rng = np.random.RandomState(0)
     n = 100
     d = 10
@@ -484,9 +699,9 @@ def test_mice_additive_matrix():
     X_test_filled = X_filled[n:]
     X_test = X_missing[n:]
 
-    imputer = MICEImputer(n_imputations=25,
-                          n_burn_in=10,
-                          verbose=True,
-                          random_state=rng).fit(X_train)
+    imputer = ChainedImputer(n_imputations=25,
+                             n_burn_in=10,
+                             verbose=True,
+                             random_state=rng).fit(X_train)
     X_test_est = imputer.transform(X_test)
     assert_allclose(X_test_filled, X_test_est, atol=0.01)
diff --git a/sklearn/tests/test_learning_curve.py b/sklearn/tests/test_learning_curve.py
deleted file mode 100644
index afaae84b92b0..000000000000
--- a/sklearn/tests/test_learning_curve.py
+++ /dev/null
@@ -1,312 +0,0 @@
-# Author: Alexander Fabisch <afabisch@informatik.uni-bremen.de>
-#
-# License: BSD 3 clause
-
-import sys
-from sklearn.externals.six.moves import cStringIO as StringIO
-import numpy as np
-import warnings
-from sklearn.base import BaseEstimator
-from sklearn.utils.testing import assert_raises
-from sklearn.utils.testing import assert_warns
-from sklearn.utils.testing import assert_equal
-from sklearn.utils.testing import assert_array_equal
-from sklearn.utils.testing import assert_array_almost_equal
-from sklearn.utils.testing import assert_false
-from sklearn.datasets import make_classification
-
-with warnings.catch_warnings():
-    warnings.simplefilter('ignore')
-    from sklearn.learning_curve import learning_curve, validation_curve
-    from sklearn.cross_validation import KFold
-
-from sklearn.linear_model import PassiveAggressiveClassifier
-
-
-class MockImprovingEstimator(BaseEstimator):
-    """Dummy classifier to test the learning curve"""
-    def __init__(self, n_max_train_sizes):
-        self.n_max_train_sizes = n_max_train_sizes
-        self.train_sizes = 0
-        self.X_subset = None
-
-    def fit(self, X_subset, y_subset=None):
-        self.X_subset = X_subset
-        self.train_sizes = X_subset.shape[0]
-        return self
-
-    def predict(self, X):
-        raise NotImplementedError
-
-    def score(self, X=None, Y=None):
-        # training score becomes worse (2 -> 1), test error better (0 -> 1)
-        if self._is_training_data(X):
-            return 2. - float(self.train_sizes) / self.n_max_train_sizes
-        else:
-            return float(self.train_sizes) / self.n_max_train_sizes
-
-    def _is_training_data(self, X):
-        return X is self.X_subset
-
-
-class MockIncrementalImprovingEstimator(MockImprovingEstimator):
-    """Dummy classifier that provides partial_fit"""
-    def __init__(self, n_max_train_sizes):
-        super(MockIncrementalImprovingEstimator,
-              self).__init__(n_max_train_sizes)
-        self.x = None
-
-    def _is_training_data(self, X):
-        return self.x in X
-
-    def partial_fit(self, X, y=None, **params):
-        self.train_sizes += X.shape[0]
-        self.x = X[0]
-
-
-class MockEstimatorWithParameter(BaseEstimator):
-    """Dummy classifier to test the validation curve"""
-    def __init__(self, param=0.5):
-        self.X_subset = None
-        self.param = param
-
-    def fit(self, X_subset, y_subset):
-        self.X_subset = X_subset
-        self.train_sizes = X_subset.shape[0]
-        return self
-
-    def predict(self, X):
-        raise NotImplementedError
-
-    def score(self, X=None, y=None):
-        return self.param if self._is_training_data(X) else 1 - self.param
-
-    def _is_training_data(self, X):
-        return X is self.X_subset
-
-
-class MockEstimatorFailing(BaseEstimator):
-    """Dummy classifier to test error_score in learning curve"""
-    def fit(self, X_subset, y_subset):
-        raise ValueError()
-
-    def score(self, X=None, y=None):
-        return None
-
-
-class MockEstimatorWithSingleFitCallAllowed(MockEstimatorWithParameter):
-    """Dummy classifier that disallows repeated calls of fit method"""
-
-    def fit(self, X_subset, y_subset):
-        assert_false(
-            hasattr(self, 'fit_called_'),
-            'fit is called the second time'
-        )
-        self.fit_called_ = True
-        return super(type(self), self).fit(X_subset, y_subset)
-
-
-def test_learning_curve():
-    X, y = make_classification(n_samples=30, n_features=1, n_informative=1,
-                               n_redundant=0, n_classes=2,
-                               n_clusters_per_class=1, random_state=0)
-    estimator = MockImprovingEstimator(20)
-    with warnings.catch_warnings(record=True) as w:
-        train_sizes, train_scores, test_scores = learning_curve(
-            estimator, X, y, cv=3, train_sizes=np.linspace(0.1, 1.0, 10))
-    if len(w) > 0:
-        raise RuntimeError("Unexpected warning: %r" % w[0].message)
-    assert_equal(train_scores.shape, (10, 3))
-    assert_equal(test_scores.shape, (10, 3))
-    assert_array_equal(train_sizes, np.linspace(2, 20, 10))
-    assert_array_almost_equal(train_scores.mean(axis=1),
-                              np.linspace(1.9, 1.0, 10))
-    assert_array_almost_equal(test_scores.mean(axis=1),
-                              np.linspace(0.1, 1.0, 10))
-
-
-def test_learning_curve_unsupervised():
-    X, _ = make_classification(n_samples=30, n_features=1, n_informative=1,
-                               n_redundant=0, n_classes=2,
-                               n_clusters_per_class=1, random_state=0)
-    estimator = MockImprovingEstimator(20)
-    train_sizes, train_scores, test_scores = learning_curve(
-        estimator, X, y=None, cv=3, train_sizes=np.linspace(0.1, 1.0, 10))
-    assert_array_equal(train_sizes, np.linspace(2, 20, 10))
-    assert_array_almost_equal(train_scores.mean(axis=1),
-                              np.linspace(1.9, 1.0, 10))
-    assert_array_almost_equal(test_scores.mean(axis=1),
-                              np.linspace(0.1, 1.0, 10))
-
-
-def test_learning_curve_verbose():
-    X, y = make_classification(n_samples=30, n_features=1, n_informative=1,
-                               n_redundant=0, n_classes=2,
-                               n_clusters_per_class=1, random_state=0)
-    estimator = MockImprovingEstimator(20)
-
-    old_stdout = sys.stdout
-    sys.stdout = StringIO()
-    try:
-        train_sizes, train_scores, test_scores = \
-            learning_curve(estimator, X, y, cv=3, verbose=1)
-    finally:
-        out = sys.stdout.getvalue()
-        sys.stdout.close()
-        sys.stdout = old_stdout
-
-    assert("[learning_curve]" in out)
-
-
-def test_learning_curve_error_score():
-    X, y = make_classification(n_samples=30, n_features=1, n_informative=1,
-                               n_redundant=0, n_classes=2,
-                               n_clusters_per_class=1, random_state=0)
-    estimator = MockEstimatorFailing()
-    _, _, test_scores = learning_curve(estimator, X, y, cv=3, error_score=0)
-    all_zeros = not np.any(test_scores)
-    assert(all_zeros)
-
-
-def test_learning_curve_error_score_default_raise():
-    X, y = make_classification(n_samples=30, n_features=1, n_informative=1,
-                               n_redundant=0, n_classes=2,
-                               n_clusters_per_class=1, random_state=0)
-    estimator = MockEstimatorFailing()
-    assert_raises(ValueError, learning_curve, estimator, X, y, cv=3)
-
-
-def test_learning_curve_incremental_learning_not_possible():
-    X, y = make_classification(n_samples=2, n_features=1, n_informative=1,
-                               n_redundant=0, n_classes=2,
-                               n_clusters_per_class=1, random_state=0)
-    # The mockup does not have partial_fit()
-    estimator = MockImprovingEstimator(1)
-    assert_raises(ValueError, learning_curve, estimator, X, y,
-                  exploit_incremental_learning=True)
-
-
-def test_learning_curve_incremental_learning():
-    X, y = make_classification(n_samples=30, n_features=1, n_informative=1,
-                               n_redundant=0, n_classes=2,
-                               n_clusters_per_class=1, random_state=0)
-    estimator = MockIncrementalImprovingEstimator(20)
-    train_sizes, train_scores, test_scores = learning_curve(
-        estimator, X, y, cv=3, exploit_incremental_learning=True,
-        train_sizes=np.linspace(0.1, 1.0, 10))
-    assert_array_equal(train_sizes, np.linspace(2, 20, 10))
-    assert_array_almost_equal(train_scores.mean(axis=1),
-                              np.linspace(1.9, 1.0, 10))
-    assert_array_almost_equal(test_scores.mean(axis=1),
-                              np.linspace(0.1, 1.0, 10))
-
-
-def test_learning_curve_incremental_learning_unsupervised():
-    X, _ = make_classification(n_samples=30, n_features=1, n_informative=1,
-                               n_redundant=0, n_classes=2,
-                               n_clusters_per_class=1, random_state=0)
-    estimator = MockIncrementalImprovingEstimator(20)
-    train_sizes, train_scores, test_scores = learning_curve(
-        estimator, X, y=None, cv=3, exploit_incremental_learning=True,
-        train_sizes=np.linspace(0.1, 1.0, 10))
-    assert_array_equal(train_sizes, np.linspace(2, 20, 10))
-    assert_array_almost_equal(train_scores.mean(axis=1),
-                              np.linspace(1.9, 1.0, 10))
-    assert_array_almost_equal(test_scores.mean(axis=1),
-                              np.linspace(0.1, 1.0, 10))
-
-
-def test_learning_curve_batch_and_incremental_learning_are_equal():
-    X, y = make_classification(n_samples=30, n_features=1, n_informative=1,
-                               n_redundant=0, n_classes=2,
-                               n_clusters_per_class=1, random_state=0)
-    train_sizes = np.linspace(0.2, 1.0, 5)
-    estimator = PassiveAggressiveClassifier(max_iter=1, tol=None,
-                                            shuffle=False)
-
-    train_sizes_inc, train_scores_inc, test_scores_inc = \
-        learning_curve(
-            estimator, X, y, train_sizes=train_sizes,
-            cv=3, exploit_incremental_learning=True)
-    train_sizes_batch, train_scores_batch, test_scores_batch = \
-        learning_curve(
-            estimator, X, y, cv=3, train_sizes=train_sizes,
-            exploit_incremental_learning=False)
-
-    assert_array_equal(train_sizes_inc, train_sizes_batch)
-    assert_array_almost_equal(train_scores_inc.mean(axis=1),
-                              train_scores_batch.mean(axis=1))
-    assert_array_almost_equal(test_scores_inc.mean(axis=1),
-                              test_scores_batch.mean(axis=1))
-
-
-def test_learning_curve_n_sample_range_out_of_bounds():
-    X, y = make_classification(n_samples=30, n_features=1, n_informative=1,
-                               n_redundant=0, n_classes=2,
-                               n_clusters_per_class=1, random_state=0)
-    estimator = MockImprovingEstimator(20)
-    assert_raises(ValueError, learning_curve, estimator, X, y, cv=3,
-                  train_sizes=[0, 1])
-    assert_raises(ValueError, learning_curve, estimator, X, y, cv=3,
-                  train_sizes=[0.0, 1.0])
-    assert_raises(ValueError, learning_curve, estimator, X, y, cv=3,
-                  train_sizes=[0.1, 1.1])
-    assert_raises(ValueError, learning_curve, estimator, X, y, cv=3,
-                  train_sizes=[0, 20])
-    assert_raises(ValueError, learning_curve, estimator, X, y, cv=3,
-                  train_sizes=[1, 21])
-
-
-def test_learning_curve_remove_duplicate_sample_sizes():
-    X, y = make_classification(n_samples=3, n_features=1, n_informative=1,
-                               n_redundant=0, n_classes=2,
-                               n_clusters_per_class=1, random_state=0)
-    estimator = MockImprovingEstimator(2)
-    train_sizes, _, _ = assert_warns(
-        RuntimeWarning, learning_curve, estimator, X, y, cv=3,
-        train_sizes=np.linspace(0.33, 1.0, 3))
-    assert_array_equal(train_sizes, [1, 2])
-
-
-def test_learning_curve_with_boolean_indices():
-    X, y = make_classification(n_samples=30, n_features=1, n_informative=1,
-                               n_redundant=0, n_classes=2,
-                               n_clusters_per_class=1, random_state=0)
-    estimator = MockImprovingEstimator(20)
-    cv = KFold(n=30, n_folds=3)
-    train_sizes, train_scores, test_scores = learning_curve(
-        estimator, X, y, cv=cv, train_sizes=np.linspace(0.1, 1.0, 10))
-    assert_array_equal(train_sizes, np.linspace(2, 20, 10))
-    assert_array_almost_equal(train_scores.mean(axis=1),
-                              np.linspace(1.9, 1.0, 10))
-    assert_array_almost_equal(test_scores.mean(axis=1),
-                              np.linspace(0.1, 1.0, 10))
-
-
-def test_validation_curve():
-    X, y = make_classification(n_samples=2, n_features=1, n_informative=1,
-                               n_redundant=0, n_classes=2,
-                               n_clusters_per_class=1, random_state=0)
-    param_range = np.linspace(0, 1, 10)
-    with warnings.catch_warnings(record=True) as w:
-        train_scores, test_scores = validation_curve(
-            MockEstimatorWithParameter(), X, y, param_name="param",
-            param_range=param_range, cv=2
-        )
-    if len(w) > 0:
-        raise RuntimeError("Unexpected warning: %r" % w[0].message)
-
-    assert_array_almost_equal(train_scores.mean(axis=1), param_range)
-    assert_array_almost_equal(test_scores.mean(axis=1), 1 - param_range)
-
-
-def test_validation_curve_clone_estimator():
-    X, y = make_classification(n_samples=2, n_features=1, n_informative=1,
-                               n_redundant=0, n_classes=2,
-                               n_clusters_per_class=1, random_state=0)
-
-    param_range = np.linspace(1, 0, 10)
-    _, _ = validation_curve(
-        MockEstimatorWithSingleFitCallAllowed(), X, y,
-        param_name="param", param_range=param_range, cv=2
-    )
diff --git a/sklearn/tests/test_naive_bayes.py b/sklearn/tests/test_naive_bayes.py
index 2f15163d09dd..6b090ce4684f 100644
--- a/sklearn/tests/test_naive_bayes.py
+++ b/sklearn/tests/test_naive_bayes.py
@@ -4,6 +4,7 @@
 from io import BytesIO
 import numpy as np
 import scipy.sparse
+import pytest
 
 from sklearn.datasets import load_digits, load_iris
 
@@ -177,51 +178,56 @@ def test_discrete_prior():
                                   clf.class_log_prior_, 8)
 
 
-def test_mnnb():
+@pytest.mark.parametrize('kind', ('dense', 'sparse'))
+def test_mnnb(kind):
     # Test Multinomial Naive Bayes classification.
     # This checks that MultinomialNB implements fit and predict and returns
     # correct values for a simple toy dataset.
 
-    for X in [X2, scipy.sparse.csr_matrix(X2)]:
-        # Check the ability to predict the learning set.
-        clf = MultinomialNB()
-        assert_raises(ValueError, clf.fit, -X, y2)
-        y_pred = clf.fit(X, y2).predict(X)
+    if kind == 'dense':
+        X = X2
+    elif kind == 'sparse':
+        X = scipy.sparse.csr_matrix(X2)
 
-        assert_array_equal(y_pred, y2)
+    # Check the ability to predict the learning set.
+    clf = MultinomialNB()
+    assert_raises(ValueError, clf.fit, -X, y2)
+    y_pred = clf.fit(X, y2).predict(X)
+
+    assert_array_equal(y_pred, y2)
 
-        # Verify that np.log(clf.predict_proba(X)) gives the same results as
-        # clf.predict_log_proba(X)
-        y_pred_proba = clf.predict_proba(X)
-        y_pred_log_proba = clf.predict_log_proba(X)
-        assert_array_almost_equal(np.log(y_pred_proba), y_pred_log_proba, 8)
+    # Verify that np.log(clf.predict_proba(X)) gives the same results as
+    # clf.predict_log_proba(X)
+    y_pred_proba = clf.predict_proba(X)
+    y_pred_log_proba = clf.predict_log_proba(X)
+    assert_array_almost_equal(np.log(y_pred_proba), y_pred_log_proba, 8)
 
-        # Check that incremental fitting yields the same results
-        clf2 = MultinomialNB()
-        clf2.partial_fit(X[:2], y2[:2], classes=np.unique(y2))
-        clf2.partial_fit(X[2:5], y2[2:5])
-        clf2.partial_fit(X[5:], y2[5:])
+    # Check that incremental fitting yields the same results
+    clf2 = MultinomialNB()
+    clf2.partial_fit(X[:2], y2[:2], classes=np.unique(y2))
+    clf2.partial_fit(X[2:5], y2[2:5])
+    clf2.partial_fit(X[5:], y2[5:])
 
-        y_pred2 = clf2.predict(X)
-        assert_array_equal(y_pred2, y2)
+    y_pred2 = clf2.predict(X)
+    assert_array_equal(y_pred2, y2)
 
-        y_pred_proba2 = clf2.predict_proba(X)
-        y_pred_log_proba2 = clf2.predict_log_proba(X)
-        assert_array_almost_equal(np.log(y_pred_proba2), y_pred_log_proba2, 8)
-        assert_array_almost_equal(y_pred_proba2, y_pred_proba)
-        assert_array_almost_equal(y_pred_log_proba2, y_pred_log_proba)
+    y_pred_proba2 = clf2.predict_proba(X)
+    y_pred_log_proba2 = clf2.predict_log_proba(X)
+    assert_array_almost_equal(np.log(y_pred_proba2), y_pred_log_proba2, 8)
+    assert_array_almost_equal(y_pred_proba2, y_pred_proba)
+    assert_array_almost_equal(y_pred_log_proba2, y_pred_log_proba)
 
-        # Partial fit on the whole data at once should be the same as fit too
-        clf3 = MultinomialNB()
-        clf3.partial_fit(X, y2, classes=np.unique(y2))
+    # Partial fit on the whole data at once should be the same as fit too
+    clf3 = MultinomialNB()
+    clf3.partial_fit(X, y2, classes=np.unique(y2))
 
-        y_pred3 = clf3.predict(X)
-        assert_array_equal(y_pred3, y2)
-        y_pred_proba3 = clf3.predict_proba(X)
-        y_pred_log_proba3 = clf3.predict_log_proba(X)
-        assert_array_almost_equal(np.log(y_pred_proba3), y_pred_log_proba3, 8)
-        assert_array_almost_equal(y_pred_proba3, y_pred_proba)
-        assert_array_almost_equal(y_pred_log_proba3, y_pred_log_proba)
+    y_pred3 = clf3.predict(X)
+    assert_array_equal(y_pred3, y2)
+    y_pred_proba3 = clf3.predict_proba(X)
+    y_pred_log_proba3 = clf3.predict_log_proba(X)
+    assert_array_almost_equal(np.log(y_pred_proba3), y_pred_log_proba3, 8)
+    assert_array_almost_equal(y_pred_proba3, y_pred_proba)
+    assert_array_almost_equal(y_pred_log_proba3, y_pred_log_proba)
 
 
 def check_partial_fit(cls):
@@ -240,9 +246,9 @@ def check_partial_fit(cls):
     assert_array_equal(clf1.feature_count_, clf3.feature_count_)
 
 
-def test_discretenb_partial_fit():
-    for cls in [MultinomialNB, BernoulliNB]:
-        yield check_partial_fit, cls
+@pytest.mark.parametrize("cls", [MultinomialNB, BernoulliNB])
+def test_discretenb_partial_fit(cls):
+    check_partial_fit(cls)
 
 
 def test_gnb_partial_fit():
@@ -259,62 +265,63 @@ def test_gnb_partial_fit():
     assert_array_almost_equal(clf.class_prior_, clf_pf2.class_prior_)
 
 
-def test_discretenb_pickle():
+@pytest.mark.parametrize('cls', [BernoulliNB, MultinomialNB, GaussianNB])
+def test_discretenb_pickle(cls):
     # Test picklability of discrete naive Bayes classifiers
 
-    for cls in [BernoulliNB, MultinomialNB, GaussianNB]:
-        clf = cls().fit(X2, y2)
-        y_pred = clf.predict(X2)
+    clf = cls().fit(X2, y2)
+    y_pred = clf.predict(X2)
 
-        store = BytesIO()
-        pickle.dump(clf, store)
-        clf = pickle.load(BytesIO(store.getvalue()))
+    store = BytesIO()
+    pickle.dump(clf, store)
+    clf = pickle.load(BytesIO(store.getvalue()))
 
-        assert_array_equal(y_pred, clf.predict(X2))
+    assert_array_equal(y_pred, clf.predict(X2))
 
-        if cls is not GaussianNB:
-            # TODO re-enable me when partial_fit is implemented for GaussianNB
+    if cls is not GaussianNB:
+        # TODO re-enable me when partial_fit is implemented for GaussianNB
 
-            # Test pickling of estimator trained with partial_fit
-            clf2 = cls().partial_fit(X2[:3], y2[:3], classes=np.unique(y2))
-            clf2.partial_fit(X2[3:], y2[3:])
-            store = BytesIO()
-            pickle.dump(clf2, store)
-            clf2 = pickle.load(BytesIO(store.getvalue()))
-            assert_array_equal(y_pred, clf2.predict(X2))
+        # Test pickling of estimator trained with partial_fit
+        clf2 = cls().partial_fit(X2[:3], y2[:3], classes=np.unique(y2))
+        clf2.partial_fit(X2[3:], y2[3:])
+        store = BytesIO()
+        pickle.dump(clf2, store)
+        clf2 = pickle.load(BytesIO(store.getvalue()))
+        assert_array_equal(y_pred, clf2.predict(X2))
 
 
-def test_input_check_fit():
+@pytest.mark.parametrize('cls', [BernoulliNB, MultinomialNB, GaussianNB])
+def test_input_check_fit(cls):
     # Test input checks for the fit method
-    for cls in [BernoulliNB, MultinomialNB, GaussianNB]:
-        # check shape consistency for number of samples at fit time
-        assert_raises(ValueError, cls().fit, X2, y2[:-1])
 
-        # check shape consistency for number of input features at predict time
-        clf = cls().fit(X2, y2)
-        assert_raises(ValueError, clf.predict, X2[:, :-1])
+    # check shape consistency for number of samples at fit time
+    assert_raises(ValueError, cls().fit, X2, y2[:-1])
 
+    # check shape consistency for number of input features at predict time
+    clf = cls().fit(X2, y2)
+    assert_raises(ValueError, clf.predict, X2[:, :-1])
 
-def test_input_check_partial_fit():
-    for cls in [BernoulliNB, MultinomialNB]:
-        # check shape consistency
-        assert_raises(ValueError, cls().partial_fit, X2, y2[:-1],
-                      classes=np.unique(y2))
 
-        # classes is required for first call to partial fit
-        assert_raises(ValueError, cls().partial_fit, X2, y2)
+@pytest.mark.parametrize('cls', [BernoulliNB, MultinomialNB])
+def test_input_check_partial_fit(cls):
+    # check shape consistency
+    assert_raises(ValueError, cls().partial_fit, X2, y2[:-1],
+                  classes=np.unique(y2))
+
+    # classes is required for first call to partial fit
+    assert_raises(ValueError, cls().partial_fit, X2, y2)
 
-        # check consistency of consecutive classes values
-        clf = cls()
-        clf.partial_fit(X2, y2, classes=np.unique(y2))
-        assert_raises(ValueError, clf.partial_fit, X2, y2,
-                      classes=np.arange(42))
+    # check consistency of consecutive classes values
+    clf = cls()
+    clf.partial_fit(X2, y2, classes=np.unique(y2))
+    assert_raises(ValueError, clf.partial_fit, X2, y2,
+                  classes=np.arange(42))
 
-        # check consistency of input shape for partial_fit
-        assert_raises(ValueError, clf.partial_fit, X2[:, :-1], y2)
+    # check consistency of input shape for partial_fit
+    assert_raises(ValueError, clf.partial_fit, X2[:, :-1], y2)
 
-        # check consistency of input shape for predict
-        assert_raises(ValueError, clf.predict, X2[:, :-1])
+    # check consistency of input shape for predict
+    assert_raises(ValueError, clf.predict, X2[:, :-1])
 
 
 def test_discretenb_predict_proba():
@@ -348,34 +355,35 @@ def test_discretenb_predict_proba():
         assert_almost_equal(np.sum(np.exp(clf.intercept_)), 1)
 
 
-def test_discretenb_uniform_prior():
+@pytest.mark.parametrize('cls', [BernoulliNB, MultinomialNB])
+def test_discretenb_uniform_prior(cls):
     # Test whether discrete NB classes fit a uniform prior
     # when fit_prior=False and class_prior=None
 
-    for cls in [BernoulliNB, MultinomialNB]:
-        clf = cls()
-        clf.set_params(fit_prior=False)
-        clf.fit([[0], [0], [1]], [0, 0, 1])
-        prior = np.exp(clf.class_log_prior_)
-        assert_array_almost_equal(prior, np.array([.5, .5]))
+    clf = cls()
+    clf.set_params(fit_prior=False)
+    clf.fit([[0], [0], [1]], [0, 0, 1])
+    prior = np.exp(clf.class_log_prior_)
+    assert_array_almost_equal(prior, np.array([.5, .5]))
 
 
-def test_discretenb_provide_prior():
+@pytest.mark.parametrize('cls', [BernoulliNB, MultinomialNB])
+def test_discretenb_provide_prior(cls):
     # Test whether discrete NB classes use provided prior
 
-    for cls in [BernoulliNB, MultinomialNB]:
-        clf = cls(class_prior=[0.5, 0.5])
-        clf.fit([[0], [0], [1]], [0, 0, 1])
-        prior = np.exp(clf.class_log_prior_)
-        assert_array_almost_equal(prior, np.array([.5, .5]))
+    clf = cls(class_prior=[0.5, 0.5])
+    clf.fit([[0], [0], [1]], [0, 0, 1])
+    prior = np.exp(clf.class_log_prior_)
+    assert_array_almost_equal(prior, np.array([.5, .5]))
 
-        # Inconsistent number of classes with prior
-        assert_raises(ValueError, clf.fit, [[0], [1], [2]], [0, 1, 2])
-        assert_raises(ValueError, clf.partial_fit, [[0], [1]], [0, 1],
-                      classes=[0, 1, 1])
+    # Inconsistent number of classes with prior
+    assert_raises(ValueError, clf.fit, [[0], [1], [2]], [0, 1, 2])
+    assert_raises(ValueError, clf.partial_fit, [[0], [1]], [0, 1],
+                  classes=[0, 1, 1])
 
 
-def test_discretenb_provide_prior_with_partial_fit():
+@pytest.mark.parametrize('cls', [BernoulliNB, MultinomialNB])
+def test_discretenb_provide_prior_with_partial_fit(cls):
     # Test whether discrete NB classes use provided prior
     # when using partial_fit
 
@@ -383,22 +391,21 @@ def test_discretenb_provide_prior_with_partial_fit():
     iris_data1, iris_data2, iris_target1, iris_target2 = train_test_split(
         iris.data, iris.target, test_size=0.4, random_state=415)
 
-    for cls in [BernoulliNB, MultinomialNB]:
-        for prior in [None, [0.3, 0.3, 0.4]]:
-            clf_full = cls(class_prior=prior)
-            clf_full.fit(iris.data, iris.target)
-            clf_partial = cls(class_prior=prior)
-            clf_partial.partial_fit(iris_data1, iris_target1,
-                                    classes=[0, 1, 2])
-            clf_partial.partial_fit(iris_data2, iris_target2)
-            assert_array_almost_equal(clf_full.class_log_prior_,
-                                      clf_partial.class_log_prior_)
-
-
-def test_sample_weight_multiclass():
-    for cls in [BernoulliNB, MultinomialNB]:
-        # check shape consistency for number of samples at fit time
-        yield check_sample_weight_multiclass, cls
+    for prior in [None, [0.3, 0.3, 0.4]]:
+        clf_full = cls(class_prior=prior)
+        clf_full.fit(iris.data, iris.target)
+        clf_partial = cls(class_prior=prior)
+        clf_partial.partial_fit(iris_data1, iris_target1,
+                                classes=[0, 1, 2])
+        clf_partial.partial_fit(iris_data2, iris_target2)
+        assert_array_almost_equal(clf_full.class_log_prior_,
+                                  clf_partial.class_log_prior_)
+
+
+@pytest.mark.parametrize('cls', [BernoulliNB, MultinomialNB])
+def test_sample_weight_multiclass(cls):
+    # check shape consistency for number of samples at fit time
+    check_sample_weight_multiclass(cls)
 
 
 def check_sample_weight_multiclass(cls):
diff --git a/sklearn/tests/test_random_projection.py b/sklearn/tests/test_random_projection.py
index dcbe97c7d6d7..975922a34116 100644
--- a/sklearn/tests/test_random_projection.py
+++ b/sklearn/tests/test_random_projection.py
@@ -1,7 +1,10 @@
 from __future__ import division
 
+import functools
+
 import numpy as np
 import scipy.sparse as sp
+import pytest
 
 from sklearn.metrics import euclidean_distances
 
@@ -113,21 +116,21 @@ def check_input_with_sparse_random_matrix(random_matrix):
                       random_matrix, n_components, n_features, density=density)
 
 
-def test_basic_property_of_random_matrix():
+@pytest.mark.parametrize("random_matrix", all_random_matrix)
+def test_basic_property_of_random_matrix(random_matrix):
     # Check basic properties of random matrix generation
-    for random_matrix in all_random_matrix:
-        yield check_input_size_random_matrix, random_matrix
-        yield check_size_generated, random_matrix
-        yield check_zero_mean_and_unit_norm, random_matrix
-
-    for random_matrix in all_sparse_random_matrix:
-        yield check_input_with_sparse_random_matrix, random_matrix
-
-        random_matrix_dense = \
-            lambda n_components, n_features, random_state: random_matrix(
-                n_components, n_features, random_state=random_state,
-                density=1.0)
-        yield check_zero_mean_and_unit_norm, random_matrix_dense
+    check_input_size_random_matrix(random_matrix)
+    check_size_generated(random_matrix)
+    check_zero_mean_and_unit_norm(random_matrix)
+
+
+@pytest.mark.parametrize("random_matrix", all_sparse_random_matrix)
+def test_basic_property_of_sparse_random_matrix(random_matrix):
+    check_input_with_sparse_random_matrix(random_matrix)
+
+    random_matrix_dense = functools.partial(random_matrix, density=1.0)
+
+    check_zero_mean_and_unit_norm(random_matrix_dense)
 
 
 def test_gaussian_random_matrix():
diff --git a/sklearn/tree/export.py b/sklearn/tree/export.py
index 65e0695e4a6a..ef13790e65b4 100644
--- a/sklearn/tree/export.py
+++ b/sklearn/tree/export.py
@@ -14,7 +14,6 @@
 from numbers import Integral
 
 import numpy as np
-import warnings
 
 from ..externals import six
 from ..utils.validation import check_is_fitted
@@ -73,7 +72,7 @@ def __repr__(self):
 SENTINEL = Sentinel()
 
 
-def export_graphviz(decision_tree, out_file=SENTINEL, max_depth=None,
+def export_graphviz(decision_tree, out_file=None, max_depth=None,
                     feature_names=None, class_names=None, label='all',
                     filled=False, leaves_parallel=False, impurity=True,
                     node_ids=False, proportion=False, rotate=False,
@@ -97,9 +96,12 @@ def export_graphviz(decision_tree, out_file=SENTINEL, max_depth=None,
     decision_tree : decision tree regressor or classifier
         The decision tree to be exported to GraphViz.
 
-    out_file : file object or string, optional (default='tree.dot')
+    out_file : file object or string, optional (default=None)
         Handle or name of the output file. If ``None``, the result is
-        returned as a string. This will the default from version 0.20.
+        returned as a string.
+
+        .. versionchanged:: 0.20
+            Default of out_file changed from "tree.dot" to None.
 
     max_depth : int, optional (default=None)
         The maximum depth of the representation. If None, the tree is fully
@@ -395,12 +397,6 @@ def recurse(tree, node_id, criterion, parent=None, depth=0):
     own_file = False
     return_string = False
     try:
-        if out_file == SENTINEL:
-            warnings.warn("out_file can be set to None starting from 0.18. "
-                          "This will be the default in 0.20.",
-                          DeprecationWarning)
-            out_file = "tree.dot"
-
         if isinstance(out_file, six.string_types):
             if six.PY3:
                 out_file = open(out_file, "w", encoding="utf-8")
diff --git a/sklearn/tree/tests/test_tree.py b/sklearn/tree/tests/test_tree.py
index f85493543b1e..bb117d8a2986 100644
--- a/sklearn/tree/tests/test_tree.py
+++ b/sklearn/tree/tests/test_tree.py
@@ -7,6 +7,7 @@
 from itertools import product
 import struct
 
+import pytest
 import numpy as np
 from scipy.sparse import csc_matrix
 from scipy.sparse import csr_matrix
@@ -701,14 +702,14 @@ def check_min_weight_fraction_leaf(name, datasets, sparse=False):
                 name, est.min_weight_fraction_leaf))
 
 
-def test_min_weight_fraction_leaf():
-    # Check on dense input
-    for name in ALL_TREES:
-        yield check_min_weight_fraction_leaf, name, "iris"
+@pytest.mark.parametrize("name", ALL_TREES)
+def test_min_weight_fraction_leaf_on_dense_input(name):
+    check_min_weight_fraction_leaf(name, "iris")
 
-    # Check on sparse input
-    for name in SPARSE_TREES:
-        yield check_min_weight_fraction_leaf, name, "multilabel", True
+
+@pytest.mark.parametrize("name", SPARSE_TREES)
+def test_min_weight_fraction_leaf_on_sparse_input(name):
+    check_min_weight_fraction_leaf(name, "multilabel", True)
 
 
 def check_min_weight_fraction_leaf_with_min_samples_leaf(name, datasets,
@@ -775,16 +776,15 @@ def check_min_weight_fraction_leaf_with_min_samples_leaf(name, datasets,
                                           est.min_samples_leaf))
 
 
-def test_min_weight_fraction_leaf_with_min_samples_leaf():
-    # Check on dense input
-    for name in ALL_TREES:
-        yield (check_min_weight_fraction_leaf_with_min_samples_leaf,
-               name, "iris")
+@pytest.mark.parametrize("name", ALL_TREES)
+def test_min_weight_fraction_leaf_with_min_samples_leaf_on_dense_input(name):
+    check_min_weight_fraction_leaf_with_min_samples_leaf(name, "iris")
+
 
-    # Check on sparse input
-    for name in SPARSE_TREES:
-        yield (check_min_weight_fraction_leaf_with_min_samples_leaf,
-               name, "multilabel", True)
+@pytest.mark.parametrize("name", SPARSE_TREES)
+def test_min_weight_fraction_leaf_with_min_samples_leaf_on_sparse_input(name):
+    check_min_weight_fraction_leaf_with_min_samples_leaf(
+            name, "multilabel", True)
 
 
 def test_min_impurity_split():
@@ -1178,9 +1178,9 @@ def check_class_weights(name):
     assert_almost_equal(clf1.feature_importances_, clf2.feature_importances_)
 
 
-def test_class_weights():
-    for name in CLF_TREES:
-        yield check_class_weights, name
+@pytest.mark.parametrize("name", CLF_TREES)
+def test_class_weights(name):
+    check_class_weights(name)
 
 
 def check_class_weight_errors(name):
@@ -1202,9 +1202,9 @@ def check_class_weight_errors(name):
     assert_raises(ValueError, clf.fit, X, _y)
 
 
-def test_class_weight_errors():
-    for name in CLF_TREES:
-        yield check_class_weight_errors, name
+@pytest.mark.parametrize("name", CLF_TREES)
+def test_class_weight_errors(name):
+    check_class_weight_errors(name)
 
 
 def test_max_leaf_nodes():
@@ -1364,20 +1364,25 @@ def check_sparse_input(tree, dataset, max_depth=None):
                                           y_log_proba)
 
 
-def test_sparse_input():
-    for tree_type, dataset in product(SPARSE_TREES, ("clf_small", "toy",
-                                                     "digits", "multilabel",
-                                                     "sparse-pos",
-                                                     "sparse-neg",
-                                                     "sparse-mix", "zeros")):
-        max_depth = 3 if dataset == "digits" else None
-        yield (check_sparse_input, tree_type, dataset, max_depth)
+@pytest.mark.parametrize("tree_type", SPARSE_TREES)
+@pytest.mark.parametrize(
+        "dataset",
+        ("clf_small", "toy", "digits", "multilabel",
+         "sparse-pos", "sparse-neg", "sparse-mix",
+         "zeros")
+)
+def test_sparse_input(tree_type, dataset):
+    max_depth = 3 if dataset == "digits" else None
+    check_sparse_input(tree_type, dataset, max_depth)
 
+
+@pytest.mark.parametrize("tree_type",
+                         set(SPARSE_TREES).intersection(REG_TREES))
+@pytest.mark.parametrize("dataset", ["boston", "reg_small"])
+def test_sparse_input_reg_trees(tree_type, dataset):
     # Due to numerical instability of MSE and too strict test, we limit the
     # maximal depth
-    for tree_type, dataset in product(SPARSE_TREES, ["boston", "reg_small"]):
-        if tree_type in REG_TREES:
-            yield (check_sparse_input, tree_type, dataset, 2)
+    check_sparse_input(tree_type, dataset, 2)
 
 
 def check_sparse_parameters(tree, dataset):
@@ -1424,13 +1429,6 @@ def check_sparse_parameters(tree, dataset):
     assert_array_almost_equal(s.predict(X), d.predict(X))
 
 
-def test_sparse_parameters():
-    for tree_type, dataset in product(SPARSE_TREES, ["sparse-pos",
-                                                     "sparse-neg",
-                                                     "sparse-mix", "zeros"]):
-        yield (check_sparse_parameters, tree_type, dataset)
-
-
 def check_sparse_criterion(tree, dataset):
     TreeEstimator = ALL_TREES[tree]
     X = DATASETS[dataset]["X"]
@@ -1451,11 +1449,13 @@ def check_sparse_criterion(tree, dataset):
         assert_array_almost_equal(s.predict(X), d.predict(X))
 
 
-def test_sparse_criterion():
-    for tree_type, dataset in product(SPARSE_TREES, ["sparse-pos",
-                                                     "sparse-neg",
-                                                     "sparse-mix", "zeros"]):
-        yield (check_sparse_criterion, tree_type, dataset)
+@pytest.mark.parametrize("tree_type", SPARSE_TREES)
+@pytest.mark.parametrize("dataset",
+                         ["sparse-pos", "sparse-neg", "sparse-mix", "zeros"])
+@pytest.mark.parametrize("check",
+                         [check_sparse_parameters, check_sparse_criterion])
+def test_sparse(tree_type, dataset, check):
+    check(tree_type, dataset)
 
 
 def check_explicit_sparse_zeros(tree, max_depth=3,
@@ -1527,9 +1527,9 @@ def check_explicit_sparse_zeros(tree, max_depth=3,
                                       d.predict_proba(X2))
 
 
-def test_explicit_sparse_zeros():
-    for tree_type in SPARSE_TREES:
-        yield (check_explicit_sparse_zeros, tree_type)
+@pytest.mark.parametrize("tree_type", SPARSE_TREES)
+def test_explicit_sparse_zeros(tree_type):
+    check_explicit_sparse_zeros(tree_type)
 
 
 @ignore_warnings
@@ -1547,10 +1547,10 @@ def check_raise_error_on_1d_input(name):
     assert_raises(ValueError, est.predict, [X])
 
 
-@ignore_warnings
-def test_1d_input():
-    for name in ALL_TREES:
-        yield check_raise_error_on_1d_input, name
+@pytest.mark.parametrize("name", ALL_TREES)
+def test_1d_input(name):
+    with ignore_warnings():
+        check_raise_error_on_1d_input(name)
 
 
 def _check_min_weight_leaf_split_level(TreeEstimator, X, y, sample_weight):
@@ -1576,9 +1576,9 @@ def check_min_weight_leaf_split_level(name):
                                            sample_weight)
 
 
-def test_min_weight_leaf_split_level():
-    for name in ALL_TREES:
-        yield check_min_weight_leaf_split_level, name
+@pytest.mark.parametrize("name", ALL_TREES)
+def test_min_weight_leaf_split_level(name):
+    check_min_weight_leaf_split_level(name)
 
 
 def check_public_apply(name):
@@ -1599,12 +1599,14 @@ def check_public_apply_sparse(name):
                        est.tree_.apply(X_small32))
 
 
-def test_public_apply():
-    for name in ALL_TREES:
-        yield (check_public_apply, name)
+@pytest.mark.parametrize("name", ALL_TREES)
+def test_public_apply_all_trees(name):
+    check_public_apply(name)
 
-    for name in SPARSE_TREES:
-        yield (check_public_apply_sparse, name)
+
+@pytest.mark.parametrize("name", SPARSE_TREES)
+def test_public_apply_sparse_trees(name):
+    check_public_apply_sparse(name)
 
 
 def check_presort_sparse(est, X, y):
@@ -1623,19 +1625,18 @@ def test_presort_sparse():
     y = y[:, 0]
 
     for est, sparse_matrix in product(ests, sparse_matrices):
-        yield check_presort_sparse, est, sparse_matrix(X), y
+        check_presort_sparse(est, sparse_matrix(X), y)
 
 
-def test_invalid_presort():
-    classes = (DecisionTreeRegressor, DecisionTreeClassifier)
+@pytest.mark.parametrize('cls',
+                         (DecisionTreeRegressor, DecisionTreeClassifier))
+def test_invalid_presort(cls):
     allowed_presort = ('auto', True, False)
     invalid_presort = 'invalid'
     msg = ("'presort' should be in {}. "
            "Got {!r} instead.".format(allowed_presort, invalid_presort))
-    for cls in classes:
-        est = cls(presort=invalid_presort)
-        assert_raise_message(ValueError, msg,
-                             est.fit, X, y)
+    est = cls(presort=invalid_presort)
+    assert_raise_message(ValueError, msg, est.fit, X, y)
 
 
 def test_decision_path_hardcoded():
@@ -1674,9 +1675,9 @@ def check_decision_path(name):
     assert_less_equal(est.tree_.max_depth, max_depth)
 
 
-def test_decision_path():
-    for name in ALL_TREES:
-        yield (check_decision_path, name)
+@pytest.mark.parametrize("name", ALL_TREES)
+def test_decision_path(name):
+    check_decision_path(name)
 
 
 def check_no_sparse_y_support(name):
@@ -1685,10 +1686,10 @@ def check_no_sparse_y_support(name):
     assert_raises(TypeError, TreeEstimator(random_state=0).fit, X, y)
 
 
-def test_no_sparse_y_support():
+@pytest.mark.parametrize("name", ALL_TREES)
+def test_no_sparse_y_support(name):
     # Currently we don't support sparse y
-    for name in ALL_TREES:
-        yield (check_no_sparse_y_support, name)
+    check_no_sparse_y_support(name)
 
 
 def test_mae():
diff --git a/sklearn/tree/tree.py b/sklearn/tree/tree.py
index dc52eee06814..af216f1906eb 100644
--- a/sklearn/tree/tree.py
+++ b/sklearn/tree/tree.py
@@ -672,7 +672,10 @@ class DecisionTreeClassifier(BaseDecisionTree, ClassifierMixin):
         The number of outputs when ``fit`` is performed.
 
     tree_ : Tree object
-        The underlying Tree object.
+        The underlying Tree object. Please refer to
+        ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and
+        :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`
+        for basic usage of these attributes.
 
     Notes
     -----
@@ -1008,7 +1011,10 @@ class DecisionTreeRegressor(BaseDecisionTree, RegressorMixin):
         The number of outputs when ``fit`` is performed.
 
     tree_ : Tree object
-        The underlying Tree object.
+        The underlying Tree object. Please refer to
+        ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and
+        :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`
+        for basic usage of these attributes.
 
     Notes
     -----
@@ -1260,7 +1266,8 @@ class ExtraTreeClassifier(DecisionTreeClassifier):
 
     See also
     --------
-    ExtraTreeRegressor, ExtraTreesClassifier, ExtraTreesRegressor
+    ExtraTreeRegressor, sklearn.ensemble.ExtraTreesClassifier,
+    sklearn.ensemble.ExtraTreesRegressor
 
     Notes
     -----
@@ -1423,7 +1430,8 @@ class ExtraTreeRegressor(DecisionTreeRegressor):
 
     See also
     --------
-    ExtraTreeClassifier, ExtraTreesClassifier, ExtraTreesRegressor
+    ExtraTreeClassifier, sklearn.ensemble.ExtraTreesClassifier,
+    sklearn.ensemble.ExtraTreesRegressor
 
     Notes
     -----
diff --git a/sklearn/utils/__init__.py b/sklearn/utils/__init__.py
index e3d1e7faaabd..bb1f383505fe 100644
--- a/sklearn/utils/__init__.py
+++ b/sklearn/utils/__init__.py
@@ -2,6 +2,7 @@
 The :mod:`sklearn.utils` module includes various utilities.
 """
 from collections import Sequence
+import numbers
 
 import numpy as np
 from scipy.sparse import issparse
@@ -553,3 +554,38 @@ def get_chunk_n_rows(row_bytes, max_n_rows=None,
                       (working_memory, np.ceil(row_bytes * 2 ** -20)))
         chunk_n_rows = 1
     return chunk_n_rows
+
+
+def is_scalar_nan(x):
+    """Tests if x is NaN
+
+    This function is meant to overcome the issue that np.isnan does not allow
+    non-numerical types as input, and that np.nan is not np.float('nan').
+
+    Parameters
+    ----------
+    x : any type
+
+    Returns
+    -------
+    boolean
+
+    Examples
+    --------
+    >>> is_scalar_nan(np.nan)
+    True
+    >>> is_scalar_nan(float("nan"))
+    True
+    >>> is_scalar_nan(None)
+    False
+    >>> is_scalar_nan("")
+    False
+    >>> is_scalar_nan([np.nan])
+    False
+    """
+
+    # convert from numpy.bool_ to python bool to ensure that testing
+    # is_scalar_nan(x) is True does not fail.
+    # Redondant np.floating is needed because numbers can't match np.float32
+    # in python 2.
+    return bool(isinstance(x, (numbers.Real, np.floating)) and np.isnan(x))
diff --git a/sklearn/utils/deprecation.py b/sklearn/utils/deprecation.py
index 5621f436d9ba..fc06f9bc84d3 100644
--- a/sklearn/utils/deprecation.py
+++ b/sklearn/utils/deprecation.py
@@ -78,6 +78,9 @@ def wrapped(*args, **kwargs):
             return fun(*args, **kwargs)
 
         wrapped.__doc__ = self._update_doc(wrapped.__doc__)
+        # Add a reference to the wrapped function so that we can introspect
+        # on function arguments in Python 2 (already works in Python 3)
+        wrapped.__wrapped__ = fun
 
         return wrapped
 
diff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py
index 5e4c454f4b1a..6c8fffd103d4 100644
--- a/sklearn/utils/estimator_checks.py
+++ b/sklearn/utils/estimator_checks.py
@@ -36,10 +36,11 @@
 from sklearn.utils.testing import ignore_warnings
 from sklearn.utils.testing import assert_dict_equal
 from sklearn.utils.testing import create_memmap_backed_data
+from sklearn.utils import is_scalar_nan
 from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
 
 
-from sklearn.base import (clone, TransformerMixin, ClusterMixin,
+from sklearn.base import (clone, ClusterMixin,
                           BaseEstimator, is_classifier, is_regressor,
                           is_outlier_detector)
 
@@ -59,7 +60,8 @@
 
 from sklearn.utils import shuffle
 from sklearn.utils.fixes import signature
-from sklearn.utils.validation import has_fit_parameter, _num_samples
+from sklearn.utils.validation import (has_fit_parameter, _num_samples,
+                                      LARGE_SPARSE_SUPPORTED)
 from sklearn.preprocessing import StandardScaler
 from sklearn.datasets import load_iris, load_boston, make_blobs
 
@@ -67,7 +69,7 @@
 BOSTON = None
 CROSS_DECOMPOSITION = ['PLSCanonical', 'PLSRegression', 'CCA', 'PLSSVD']
 MULTI_OUTPUT = ['CCA', 'DecisionTreeRegressor', 'ElasticNet',
-                'ExtraTreeRegressor', 'ExtraTreesRegressor', 'GaussianProcess',
+                'ExtraTreeRegressor', 'ExtraTreesRegressor',
                 'GaussianProcessRegressor', 'TransformedTargetRegressor',
                 'KNeighborsRegressor', 'KernelRidge', 'Lars', 'Lasso',
                 'LassoLars', 'LinearRegression', 'MultiTaskElasticNet',
@@ -76,8 +78,9 @@
                 'RANSACRegressor', 'RadiusNeighborsRegressor',
                 'RandomForestRegressor', 'Ridge', 'RidgeCV']
 
-ALLOW_NAN = ['Imputer', 'SimpleImputer', 'MICEImputer',
-             'MinMaxScaler', 'QuantileTransformer']
+ALLOW_NAN = ['Imputer', 'SimpleImputer', 'ChainedImputer',
+             'MaxAbsScaler', 'MinMaxScaler', 'StandardScaler',
+             'PowerTransformer', 'QuantileTransformer']
 
 
 def _yield_non_meta_checks(name, estimator):
@@ -104,10 +107,8 @@ def _yield_non_meta_checks(name, estimator):
         # Test that all estimators check their input for NaN's and infs
         yield check_estimators_nan_inf
 
-    if name not in ['GaussianProcess']:
-        # FIXME!
-        # in particular GaussianProcess!
-        yield check_estimators_overwrite_params
+    yield check_estimators_overwrite_params
+
     if hasattr(estimator, 'sparsify'):
         yield check_sparsify_coefficients
 
@@ -138,7 +139,6 @@ def _yield_classifier_checks(name, classifier):
 
         yield check_supervised_y_2d
     yield check_supervised_y_no_nan
-    # test if NotFittedError is raised
     yield check_estimators_unfitted
     if 'class_weight' in classifier.get_params().keys():
         yield check_class_weight_classifiers
@@ -186,7 +186,7 @@ def _yield_regressor_checks(name, regressor):
         # check that the regressor handles int input
         yield check_regressors_int
     if name != "GaussianProcessRegressor":
-        # Test if NotFittedError is raised
+        # test if NotFittedError is raised
         yield check_estimators_unfitted
     yield check_non_transformer_estimators_n_iter
 
@@ -258,9 +258,7 @@ def _yield_all_checks(name, estimator):
             yield check
     yield check_fit2d_predict1d
     yield check_methods_subset_invariance
-    if name != 'GaussianProcess':  # FIXME
-        # XXX GaussianProcess deprecated in 0.20
-        yield check_fit2d_1sample
+    yield check_fit2d_1sample
     yield check_fit2d_1feature
     yield check_fit1d
     yield check_get_params_invariance
@@ -301,10 +299,10 @@ def check_estimator(Estimator):
     for check in _yield_all_checks(name, estimator):
         try:
             check(name, estimator)
-        except SkipTest as message:
+        except SkipTest as exception:
             # the only SkipTest thrown currently results from not
             # being able to import pandas.
-            warnings.warn(message, SkipTestWarning)
+            warnings.warn(str(exception), SkipTestWarning)
 
 
 def _boston_subset(n_samples=200):
@@ -327,7 +325,6 @@ def set_checking_parameters(estimator):
             and not isinstance(estimator, BaseSGD)):
         estimator.set_params(n_iter=5)
     if "max_iter" in params:
-        warnings.simplefilter("ignore", ConvergenceWarning)
         if estimator.max_iter is not None:
             estimator.set_params(max_iter=min(5, estimator.max_iter))
         # LinearSVR, LinearSVC
@@ -434,6 +431,40 @@ def pairwise_estimator_convert_X(X, estimator, kernel=linear_kernel):
     return X
 
 
+def _generate_sparse_matrix(X_csr):
+    """Generate sparse matrices with {32,64}bit indices of diverse format
+
+        Parameters
+        ----------
+        X_csr: CSR Matrix
+            Input matrix in CSR format
+
+        Returns
+        -------
+        out: iter(Matrices)
+            In format['dok', 'lil', 'dia', 'bsr', 'csr', 'csc', 'coo',
+             'coo_64', 'csc_64', 'csr_64']
+    """
+
+    assert X_csr.format == 'csr'
+    yield 'csr', X_csr.copy()
+    for sparse_format in ['dok', 'lil', 'dia', 'bsr', 'csc', 'coo']:
+        yield sparse_format, X_csr.asformat(sparse_format)
+
+    if LARGE_SPARSE_SUPPORTED:
+        # Generate large indices matrix only if its supported by scipy
+        X_coo = X_csr.asformat('coo')
+        X_coo.row = X_coo.row.astype('int64')
+        X_coo.col = X_coo.col.astype('int64')
+        yield "coo_64", X_coo
+
+        for sparse_format in ['csc', 'csr']:
+            X = X_csr.asformat(sparse_format)
+            X.indices = X.indices.astype('int64')
+            X.indptr = X.indptr.astype('int64')
+            yield sparse_format + "_64", X
+
+
 def check_estimator_sparse_data(name, estimator_orig):
 
     rng = np.random.RandomState(0)
@@ -446,8 +477,7 @@ def check_estimator_sparse_data(name, estimator_orig):
     with ignore_warnings(category=DeprecationWarning):
         estimator = clone(estimator_orig)
     y = multioutput_estimator_convert_y_2d(estimator, y)
-    for sparse_format in ['csr', 'csc', 'dok', 'lil', 'coo', 'dia', 'bsr']:
-        X = X_csr.asformat(sparse_format)
+    for matrix_format, X in _generate_sparse_matrix(X_csr):
         # catch deprecation warnings
         with ignore_warnings(category=(DeprecationWarning, FutureWarning)):
             if name in ['Scaler', 'StandardScaler']:
@@ -466,12 +496,18 @@ def check_estimator_sparse_data(name, estimator_orig):
                 assert_equal(probs.shape, (X.shape[0], 4))
         except (TypeError, ValueError) as e:
             if 'sparse' not in repr(e).lower():
-                print("Estimator %s doesn't seem to fail gracefully on "
-                      "sparse data: error message state explicitly that "
-                      "sparse input is not supported if this is not the case."
-                      % name)
-                raise
-        except Exception:
+                if "64" in matrix_format:
+                    msg = ("Estimator %s doesn't seem to support %s matrix, "
+                           "and is not failing gracefully, e.g. by using "
+                           "check_array(X, accept_large_sparse=False)")
+                    raise AssertionError(msg % (name, matrix_format))
+                else:
+                    print("Estimator %s doesn't seem to fail gracefully on "
+                          "sparse data: error message state explicitly that "
+                          "sparse input is not supported if this is not"
+                          " the case." % name)
+                    raise
+        except Exception as e:
             print("Estimator %s doesn't seem to fail gracefully on "
                   "sparse data: it should raise a TypeError if sparse input "
                   "is explicitly not supported." % name)
@@ -676,7 +712,7 @@ def check_fit2d_predict1d(name, estimator_orig):
                                  getattr(estimator, method), X[0])
 
 
-def _apply_func(func, X):
+def _apply_on_subsets(func, X):
     # apply function on the whole set and on mini batches
     result_full = func(X)
     n_features = X.shape[1]
@@ -687,6 +723,9 @@ def _apply_func(func, X):
         result_full = result_full[0]
         result_by_batch = list(map(lambda x: x[0], result_by_batch))
 
+    if sparse.issparse(result_full):
+        result_full = result_full.A
+        result_by_batch = [x.A for x in result_by_batch]
     return np.ravel(result_full), np.ravel(result_by_batch)
 
 
@@ -722,7 +761,7 @@ def check_methods_subset_invariance(name, estimator_orig):
             raise SkipTest(msg)
 
         if hasattr(estimator, method):
-            result_full, result_by_batch = _apply_func(
+            result_full, result_by_batch = _apply_on_subsets(
                 getattr(estimator, method), X)
             assert_allclose(result_full, result_by_batch,
                             atol=1e-7, err_msg=msg)
@@ -1605,6 +1644,7 @@ def check_classifiers_predictions(X, y, name, classifier_orig):
 def choose_check_classifiers_labels(name, y, y_names):
     return y if name in ["LabelPropagation", "LabelSpreading"] else y_names
 
+
 def check_classifiers_classes(name, classifier_orig):
     X_multiclass, y_multiclass = make_blobs(n_samples=30, random_state=0,
                                             cluster_std=0.1)
@@ -2000,11 +2040,13 @@ def param_filter(p):
 
             init_params = [p for p in signature(init).parameters.values()
                            if param_filter(p)]
+
         except (TypeError, ValueError):
             # init is not a python function.
             # true for mixins
             return
         params = estimator.get_params()
+
         if name in META_ESTIMATORS:
             # they can need a non-default argument
             init_params = init_params[1:]
@@ -2030,7 +2072,11 @@ def param_filter(p):
             if isinstance(param_value, np.ndarray):
                 assert_array_equal(param_value, init_param.default)
             else:
-                assert_equal(param_value, init_param.default, init_param.name)
+                if is_scalar_nan(param_value):
+                    # Allows to set default parameters to np.nan
+                    assert param_value is init_param.default, init_param.name
+                else:
+                    assert param_value == init_param.default, init_param.name
 
 
 def multioutput_estimator_convert_y_2d(estimator, y):
diff --git a/sklearn/utils/extmath.py b/sklearn/utils/extmath.py
index c6c13c4f933a..218733145a0d 100644
--- a/sklearn/utils/extmath.py
+++ b/sklearn/utils/extmath.py
@@ -15,8 +15,7 @@
 import warnings
 
 import numpy as np
-from scipy import linalg
-from scipy.sparse import issparse, csr_matrix
+from scipy import linalg, sparse
 
 from . import check_random_state, deprecated
 from .fixes import np_version
@@ -60,9 +59,9 @@ def row_norms(X, squared=False):
 
     Performs no input validation.
     """
-    if issparse(X):
-        if not isinstance(X, csr_matrix):
-            X = csr_matrix(X)
+    if sparse.issparse(X):
+        if not isinstance(X, sparse.csr_matrix):
+            X = sparse.csr_matrix(X)
         norms = csr_row_norms(X)
     else:
         norms = np.einsum('ij,ij->i', X, X)
@@ -131,7 +130,7 @@ def safe_sparse_dot(a, b, dense_output=False):
     dot_product : array or sparse matrix
         sparse if ``a`` or ``b`` is sparse and ``dense_output=False``.
     """
-    if issparse(a) or issparse(b):
+    if sparse.issparse(a) or sparse.issparse(b):
         ret = a * b
         if dense_output and hasattr(ret, "toarray"):
             ret = ret.toarray()
@@ -162,7 +161,7 @@ def randomized_range_finder(A, size, n_iter,
         (the fastest but numerically unstable when `n_iter` is large, e.g.
         typically 5 or larger), or 'LU' factorization (numerically stable
         but can lose slightly in accuracy). The 'auto' mode applies no
-        normalization if `n_iter`<=2 and switches to LU otherwise.
+        normalization if `n_iter` <= 2 and switches to LU otherwise.
 
         .. versionadded:: 0.18
 
@@ -259,7 +258,7 @@ def randomized_svd(M, n_components, n_oversamples=10, n_iter='auto',
         (the fastest but numerically unstable when `n_iter` is large, e.g.
         typically 5 or larger), or 'LU' factorization (numerically stable
         but can lose slightly in accuracy). The 'auto' mode applies no
-        normalization if `n_iter`<=2 and switches to LU otherwise.
+        normalization if `n_iter` <= 2 and switches to LU otherwise.
 
         .. versionadded:: 0.18
 
@@ -307,6 +306,12 @@ def randomized_svd(M, n_components, n_oversamples=10, n_iter='auto',
       analysis
       A. Szlam et al. 2014
     """
+    if isinstance(M, (sparse.lil_matrix, sparse.dok_matrix)):
+        warnings.warn("Calculating SVD of a {} is expensive. "
+                      "csr_matrix is more efficient.".format(
+                          type(M).__name__),
+                      sparse.SparseEfficiencyWarning)
+
     random_state = check_random_state(random_state)
     n_random = n_components + n_oversamples
     n_samples, n_features = M.shape
@@ -593,7 +598,7 @@ def softmax(X, copy=True):
 
     Parameters
     ----------
-    X : array-like, shape (M, N)
+    X : array-like of floats, shape (M, N)
         Argument to the logistic function
 
     copy : bool, optional
@@ -620,7 +625,7 @@ def safe_min(X):
     Adapated from http://stackoverflow.com/q/13426580
 
     """
-    if issparse(X):
+    if sparse.issparse(X):
         if len(X.data) == 0:
             return 0
         m = X.data.min()
@@ -633,7 +638,7 @@ def make_nonnegative(X, min_value=0):
     """Ensure `X.min()` >= `min_value`."""
     min_ = safe_min(X)
     if min_ < min_value:
-        if issparse(X):
+        if sparse.issparse(X):
             raise ValueError("Cannot make the data matrix"
                              " nonnegative because it is sparse."
                              " Adding a value to every entry would"
@@ -642,8 +647,7 @@ def make_nonnegative(X, min_value=0):
     return X
 
 
-def _incremental_mean_and_var(X, last_mean=.0, last_variance=None,
-                              last_sample_count=0):
+def _incremental_mean_and_var(X, last_mean, last_variance, last_sample_count):
     """Calculate mean update and a Youngs and Cramer variance update.
 
     last_mean and last_variance are statistics computed at the last step by the
@@ -664,7 +668,7 @@ def _incremental_mean_and_var(X, last_mean=.0, last_variance=None,
 
     last_variance : array-like, shape: (n_features,)
 
-    last_sample_count : int
+    last_sample_count : array-like, shape (n_features,)
 
     Returns
     -------
@@ -673,7 +677,11 @@ def _incremental_mean_and_var(X, last_mean=.0, last_variance=None,
     updated_variance : array, shape (n_features,)
         If None, only mean is computed
 
-    updated_sample_count : int
+    updated_sample_count : array, shape (n_features,)
+
+    Notes
+    -----
+    NaNs are ignored during the algorithm.
 
     References
     ----------
@@ -689,9 +697,9 @@ def _incremental_mean_and_var(X, last_mean=.0, last_variance=None,
     # new = the current increment
     # updated = the aggregated stats
     last_sum = last_mean * last_sample_count
-    new_sum = X.sum(axis=0)
+    new_sum = np.nansum(X, axis=0)
 
-    new_sample_count = X.shape[0]
+    new_sample_count = np.sum(~np.isnan(X), axis=0)
     updated_sample_count = last_sample_count + new_sample_count
 
     updated_mean = (last_sum + new_sum) / updated_sample_count
@@ -699,17 +707,18 @@ def _incremental_mean_and_var(X, last_mean=.0, last_variance=None,
     if last_variance is None:
         updated_variance = None
     else:
-        new_unnormalized_variance = X.var(axis=0) * new_sample_count
-        if last_sample_count == 0:  # Avoid division by 0
-            updated_unnormalized_variance = new_unnormalized_variance
-        else:
+        new_unnormalized_variance = np.nanvar(X, axis=0) * new_sample_count
+        last_unnormalized_variance = last_variance * last_sample_count
+
+        with np.errstate(divide='ignore', invalid='ignore'):
             last_over_new_count = last_sample_count / new_sample_count
-            last_unnormalized_variance = last_variance * last_sample_count
             updated_unnormalized_variance = (
-                last_unnormalized_variance +
-                new_unnormalized_variance +
+                last_unnormalized_variance + new_unnormalized_variance +
                 last_over_new_count / updated_sample_count *
                 (last_sum / last_over_new_count - new_sum) ** 2)
+
+        zeros = last_sample_count == 0
+        updated_unnormalized_variance[zeros] = new_unnormalized_variance[zeros]
         updated_variance = updated_unnormalized_variance / updated_sample_count
 
     return updated_mean, updated_variance, updated_sample_count
diff --git a/sklearn/utils/fixes.py b/sklearn/utils/fixes.py
index f7d9d6a29f9f..011777008417 100644
--- a/sklearn/utils/fixes.py
+++ b/sklearn/utils/fixes.py
@@ -10,7 +10,6 @@
 #
 # License: BSD 3 clause
 
-import warnings
 import os
 import errno
 
@@ -71,70 +70,15 @@ def divide(x1, x2, out=None, dtype=None):
         return out
 
 
-try:
-    with warnings.catch_warnings(record=True):
-        # Don't raise the numpy deprecation warnings that appear in
-        # 1.9, but avoid Python bug due to simplefilter('ignore')
-        warnings.simplefilter('always')
-        sp.csr_matrix([1.0, 2.0, 3.0]).max(axis=0)
-except (TypeError, AttributeError):
-    # in scipy < 0.14.0, sparse matrix min/max doesn't accept `axis` argument
-    # the following code is taken from the scipy 0.14 codebase
-
-    def _minor_reduce(X, ufunc):
-        major_index = np.flatnonzero(np.diff(X.indptr))
-        value = ufunc.reduceat(X.data, X.indptr[major_index])
-        return major_index, value
-
-    def _min_or_max_axis(X, axis, min_or_max):
-        N = X.shape[axis]
-        if N == 0:
-            raise ValueError("zero-size array to reduction operation")
-        M = X.shape[1 - axis]
-        mat = X.tocsc() if axis == 0 else X.tocsr()
-        mat.sum_duplicates()
-        major_index, value = _minor_reduce(mat, min_or_max)
-        not_full = np.diff(mat.indptr)[major_index] < N
-        value[not_full] = min_or_max(value[not_full], 0)
-        mask = value != 0
-        major_index = np.compress(mask, major_index)
-        value = np.compress(mask, value)
-
-        from scipy.sparse import coo_matrix
-        if axis == 0:
-            res = coo_matrix((value, (np.zeros(len(value)), major_index)),
-                             dtype=X.dtype, shape=(1, M))
-        else:
-            res = coo_matrix((value, (major_index, np.zeros(len(value)))),
-                             dtype=X.dtype, shape=(M, 1))
-        return res.A.ravel()
-
-    def _sparse_min_or_max(X, axis, min_or_max):
-        if axis is None:
-            if 0 in X.shape:
-                raise ValueError("zero-size array to reduction operation")
-            zero = X.dtype.type(0)
-            if X.nnz == 0:
-                return zero
-            m = min_or_max.reduce(X.data.ravel())
-            if X.nnz != np.product(X.shape):
-                m = min_or_max(zero, m)
-            return m
-        if axis < 0:
-            axis += 2
-        if (axis == 0) or (axis == 1):
-            return _min_or_max_axis(X, axis, min_or_max)
-        else:
-            raise ValueError("invalid axis, use 0 for rows, or 1 for columns")
-
-    def sparse_min_max(X, axis):
-        return (_sparse_min_or_max(X, axis, np.minimum),
-                _sparse_min_or_max(X, axis, np.maximum))
+# boxcox ignore NaN in scipy.special.boxcox after 0.14
+if sp_version < (0, 14):
+    from scipy import stats
 
+    def boxcox(x, lmbda):
+        with np.errstate(invalid='ignore'):
+            return stats.boxcox(x, lmbda)
 else:
-    def sparse_min_max(X, axis):
-        return (X.min(axis=axis).toarray().ravel(),
-                X.max(axis=axis).toarray().ravel())
+    from scipy.special import boxcox  # noqa
 
 
 if sp_version < (0, 15):
@@ -334,3 +278,20 @@ def nanpercentile(a, q):
             return np.array([np.nan] * size_q)
 else:
     from numpy import nanpercentile  # noqa
+
+
+# Fix for behavior inconsistency on numpy.equal for object dtypes.
+# For numpy versions < 1.13, numpy.equal tests element-wise identity of objects
+# instead of equality. This fix returns the mask of NaNs in an array of
+# numerical or object values for all nupy versions.
+
+_nan_object_array = np.array([np.nan], dtype=object)
+_nan_object_mask = _nan_object_array != _nan_object_array
+
+if np.array_equal(_nan_object_mask, np.array([True])):
+    def _object_dtype_isnan(X):
+        return X != X
+
+else:
+    def _object_dtype_isnan(X):
+        return np.frompyfunc(lambda x: x != x, 1, 1)(X).astype(bool)
diff --git a/sklearn/utils/metaestimators.py b/sklearn/utils/metaestimators.py
index ae909140b10e..541ffc583108 100644
--- a/sklearn/utils/metaestimators.py
+++ b/sklearn/utils/metaestimators.py
@@ -29,10 +29,10 @@ def _get_params(self, attr, deep=True):
         estimators = getattr(self, attr)
         out.update(estimators)
         for name, estimator in estimators:
-            if estimator is None:
-                continue
-            for key, value in six.iteritems(estimator.get_params(deep=True)):
-                out['%s__%s' % (name, key)] = value
+            if hasattr(estimator, 'get_params'):
+                for key, value in six.iteritems(
+                        estimator.get_params(deep=True)):
+                    out['%s__%s' % (name, key)] = value
         return out
 
     def _set_params(self, attr, **params):
@@ -152,7 +152,7 @@ def _safe_split(estimator, X, y, indices, train_indices=None):
     we slice rows using ``indices`` (assumed the test set) and columns
     using ``train_indices``, indicating the training set.
 
-    Labels y will always be sliced only along the last axis.
+    Labels y will always be indexed only along the first axis.
 
     Parameters
     ----------
@@ -161,11 +161,11 @@ def _safe_split(estimator, X, y, indices, train_indices=None):
         columns.
 
     X : array-like, sparse matrix or iterable
-        Data to be sliced. If ``estimator._pairwise is True``,
+        Data to be indexed. If ``estimator._pairwise is True``,
         this needs to be a square array-like or sparse matrix.
 
     y : array-like, sparse matrix or iterable
-        Targets to be sliced.
+        Targets to be indexed.
 
     indices : array of int
         Rows to select from X and y.
@@ -178,11 +178,11 @@ def _safe_split(estimator, X, y, indices, train_indices=None):
 
     Returns
     -------
-    X_sliced : array-like, sparse matrix or list
-        Sliced data.
+    X_subset : array-like, sparse matrix or list
+        Indexed data.
 
-    y_sliced : array-like, sparse matrix or list
-        Sliced targets.
+    y_subset : array-like, sparse matrix or list
+        Indexed targets.
 
     """
     if getattr(estimator, "_pairwise", False):
diff --git a/sklearn/utils/sparsefuncs.py b/sklearn/utils/sparsefuncs.py
index 9f85a0ad6cfc..ccaa6eeb28e6 100644
--- a/sklearn/utils/sparsefuncs.py
+++ b/sklearn/utils/sparsefuncs.py
@@ -6,7 +6,6 @@
 import scipy.sparse as sp
 import numpy as np
 
-from .fixes import sparse_min_max
 from .sparsefuncs_fast import (
     csr_mean_variance_axis0 as _csr_mean_var_axis0,
     csc_mean_variance_axis0 as _csc_mean_var_axis0,
@@ -122,7 +121,7 @@ def incr_mean_variance_axis(X, axis, last_mean, last_var, last_n):
     last_var : float array with shape (n_features,)
         Array of feature-wise var to update with the new data X.
 
-    last_n : int
+    last_n : int with shape (n_features,)
         Number of samples seen so far, excluded X.
 
     Returns
@@ -134,9 +133,13 @@ def incr_mean_variance_axis(X, axis, last_mean, last_var, last_n):
     variances : float array with shape (n_features,)
         Updated feature-wise variances.
 
-    n : int
+    n : int with shape (n_features,)
         Updated number of seen samples.
 
+    Notes
+    -----
+    NaNs are ignored in the algorithm.
+
     """
     _raise_error_wrong_axis(axis)
 
@@ -336,8 +339,67 @@ def inplace_swap_column(X, m, n):
         _raise_typeerror(X)
 
 
-def min_max_axis(X, axis):
-    """Compute minimum and maximum along an axis on a CSR or CSC matrix
+def _minor_reduce(X, ufunc):
+    major_index = np.flatnonzero(np.diff(X.indptr))
+    value = ufunc.reduceat(X.data, X.indptr[major_index])
+    return major_index, value
+
+
+def _min_or_max_axis(X, axis, min_or_max):
+    N = X.shape[axis]
+    if N == 0:
+        raise ValueError("zero-size array to reduction operation")
+    M = X.shape[1 - axis]
+    mat = X.tocsc() if axis == 0 else X.tocsr()
+    mat.sum_duplicates()
+    major_index, value = _minor_reduce(mat, min_or_max)
+    not_full = np.diff(mat.indptr)[major_index] < N
+    value[not_full] = min_or_max(value[not_full], 0)
+    mask = value != 0
+    major_index = np.compress(mask, major_index)
+    value = np.compress(mask, value)
+
+    if axis == 0:
+        res = sp.coo_matrix((value, (np.zeros(len(value)), major_index)),
+                            dtype=X.dtype, shape=(1, M))
+    else:
+        res = sp.coo_matrix((value, (major_index, np.zeros(len(value)))),
+                            dtype=X.dtype, shape=(M, 1))
+    return res.A.ravel()
+
+
+def _sparse_min_or_max(X, axis, min_or_max):
+    if axis is None:
+        if 0 in X.shape:
+            raise ValueError("zero-size array to reduction operation")
+        zero = X.dtype.type(0)
+        if X.nnz == 0:
+            return zero
+        m = min_or_max.reduce(X.data.ravel())
+        if X.nnz != np.product(X.shape):
+            m = min_or_max(zero, m)
+        return m
+    if axis < 0:
+        axis += 2
+    if (axis == 0) or (axis == 1):
+        return _min_or_max_axis(X, axis, min_or_max)
+    else:
+        raise ValueError("invalid axis, use 0 for rows, or 1 for columns")
+
+
+def _sparse_min_max(X, axis):
+        return (_sparse_min_or_max(X, axis, np.minimum),
+                _sparse_min_or_max(X, axis, np.maximum))
+
+
+def _sparse_nan_min_max(X, axis):
+    return(_sparse_min_or_max(X, axis, np.fmin),
+           _sparse_min_or_max(X, axis, np.fmax))
+
+
+def min_max_axis(X, axis, ignore_nan=False):
+    """Compute minimum and maximum along an axis on a CSR or CSC matrix and
+    optionally ignore NaN values.
 
     Parameters
     ----------
@@ -347,6 +409,11 @@ def min_max_axis(X, axis):
     axis : int (either 0 or 1)
         Axis along which the axis should be computed.
 
+    ignore_nan : bool, default is False
+        Ignore or passing through NaN values.
+
+        .. versionadded:: 0.20
+
     Returns
     -------
 
@@ -357,7 +424,10 @@ def min_max_axis(X, axis):
         Feature-wise maxima
     """
     if isinstance(X, sp.csr_matrix) or isinstance(X, sp.csc_matrix):
-        return sparse_min_max(X, axis=axis)
+        if ignore_nan:
+            return _sparse_nan_min_max(X, axis=axis)
+        else:
+            return _sparse_min_max(X, axis=axis)
     else:
         _raise_typeerror(X)
 
diff --git a/sklearn/utils/sparsefuncs_fast.pyx b/sklearn/utils/sparsefuncs_fast.pyx
index efad40a2ea28..7de906cdaa14 100644
--- a/sklearn/utils/sparsefuncs_fast.pyx
+++ b/sklearn/utils/sparsefuncs_fast.pyx
@@ -15,6 +15,7 @@ import numpy as np
 import scipy.sparse as sp
 cimport cython
 from cython cimport floating
+from numpy.math cimport isnan
 
 np.import_array()
 
@@ -64,7 +65,6 @@ def csr_mean_variance_axis0(X):
 
     Returns
     -------
-
     means : float array with shape (n_features,)
         Feature-wise means
 
@@ -74,26 +74,26 @@ def csr_mean_variance_axis0(X):
     """
     if X.dtype != np.float32:
         X = X.astype(np.float64)
-    return _csr_mean_variance_axis0(X.data, X.shape, X.indices)
+    means, variances, _ =  _csr_mean_variance_axis0(X.data, X.shape[0],
+                                                    X.shape[1], X.indices)
+    return means, variances
 
 
 def _csr_mean_variance_axis0(np.ndarray[floating, ndim=1, mode="c"] X_data,
-                             shape,
-                             np.ndarray[int, ndim=1] X_indices):
+                             unsigned long long n_samples,
+                             unsigned long long n_features,
+                             np.ndarray[integral, ndim=1] X_indices):
     # Implement the function here since variables using fused types
     # cannot be declared directly and can only be passed as function arguments
-    cdef unsigned int n_samples = shape[0]
-    cdef unsigned int n_features = shape[1]
-
-    cdef unsigned int i
-    cdef unsigned int non_zero = X_indices.shape[0]
-    cdef unsigned int col_ind
-    cdef floating diff
-
-    # means[j] contains the mean of feature j
-    cdef np.ndarray[floating, ndim=1] means
-    # variances[j] contains the variance of feature j
-    cdef np.ndarray[floating, ndim=1] variances
+    cdef:
+        np.npy_intp i
+        unsigned long long non_zero = X_indices.shape[0]
+        np.npy_intp col_ind
+        floating diff
+        # means[j] contains the mean of feature j
+        np.ndarray[floating, ndim=1] means
+        # variances[j] contains the variance of feature j
+        np.ndarray[floating, ndim=1] variances
 
     if floating is float:
         dtype = np.float32
@@ -103,27 +103,36 @@ def _csr_mean_variance_axis0(np.ndarray[floating, ndim=1, mode="c"] X_data,
     means = np.zeros(n_features, dtype=dtype)
     variances = np.zeros_like(means, dtype=dtype)
 
-    # counts[j] contains the number of samples where feature j is non-zero
-    cdef np.ndarray[int, ndim=1] counts = np.zeros(n_features,
-                                                   dtype=np.int32)
+    cdef:
+        # counts[j] contains the number of samples where feature j is non-zero
+        np.ndarray[np.int64_t, ndim=1] counts = np.zeros(n_features,
+                                                         dtype=np.int64)
+        # counts_nan[j] contains the number of NaNs for feature j
+        np.ndarray[np.int64_t, ndim=1] counts_nan = np.zeros(n_features,
+                                                             dtype=np.int64)
 
     for i in xrange(non_zero):
         col_ind = X_indices[i]
-        means[col_ind] += X_data[i]
+        if not isnan(X_data[i]):
+            means[col_ind] += X_data[i]
+        else:
+            counts_nan[col_ind] += 1
 
-    means /= n_samples
+    for i in xrange(n_features):
+        means[i] /= (n_samples - counts_nan[i])
 
     for i in xrange(non_zero):
         col_ind = X_indices[i]
-        diff = X_data[i] - means[col_ind]
-        variances[col_ind] += diff * diff
-        counts[col_ind] += 1
+        if not isnan(X_data[i]):
+            diff = X_data[i] - means[col_ind]
+            variances[col_ind] += diff * diff
+            counts[col_ind] += 1
 
     for i in xrange(n_features):
-        variances[i] += (n_samples - counts[i]) * means[i] ** 2
-        variances[i] /= n_samples
+        variances[i] += (n_samples - counts_nan[i] - counts[i]) * means[i]**2
+        variances[i] /= (n_samples - counts_nan[i])
 
-    return means, variances
+    return means, variances, counts_nan
 
 
 def csc_mean_variance_axis0(X):
@@ -136,7 +145,6 @@ def csc_mean_variance_axis0(X):
 
     Returns
     -------
-
     means : float array with shape (n_features,)
         Feature-wise means
 
@@ -146,29 +154,30 @@ def csc_mean_variance_axis0(X):
     """
     if X.dtype != np.float32:
         X = X.astype(np.float64)
-    return _csc_mean_variance_axis0(X.data, X.shape, X.indices, X.indptr)
+    means, variances, _ = _csc_mean_variance_axis0(X.data, X.shape[0],
+                                                   X.shape[1], X.indices,
+                                                  X.indptr)
+    return means, variances
 
 
 def _csc_mean_variance_axis0(np.ndarray[floating, ndim=1] X_data,
-                             shape,
-                             np.ndarray[int, ndim=1] X_indices,
-                             np.ndarray[int, ndim=1] X_indptr):
+                             unsigned long long n_samples,
+                             unsigned long long n_features,
+                             np.ndarray[integral, ndim=1] X_indices,
+                             np.ndarray[integral, ndim=1] X_indptr):
     # Implement the function here since variables using fused types
     # cannot be declared directly and can only be passed as function arguments
-    cdef unsigned int n_samples = shape[0]
-    cdef unsigned int n_features = shape[1]
-
-    cdef unsigned int i
-    cdef unsigned int j
-    cdef unsigned int counts
-    cdef unsigned int startptr
-    cdef unsigned int endptr
-    cdef floating diff
-
-    # means[j] contains the mean of feature j
-    cdef np.ndarray[floating, ndim=1] means
-    # variances[j] contains the variance of feature j
-    cdef np.ndarray[floating, ndim=1] variances
+    cdef:
+        np.npy_intp i, j
+        unsigned long long counts
+        unsigned long long startptr
+        unsigned long long endptr
+        floating diff
+        # means[j] contains the mean of feature j
+        np.ndarray[floating, ndim=1] means
+        # variances[j] contains the variance of feature j
+        np.ndarray[floating, ndim=1] variances
+
     if floating is float:
         dtype = np.float32
     else:
@@ -177,6 +186,9 @@ def _csc_mean_variance_axis0(np.ndarray[floating, ndim=1] X_data,
     means = np.zeros(n_features, dtype=dtype)
     variances = np.zeros_like(means, dtype=dtype)
 
+    cdef np.ndarray[np.int64_t, ndim=1] counts_nan = np.zeros(n_features,
+                                                              dtype=np.int64)
+
     for i in xrange(n_features):
 
         startptr = X_indptr[i]
@@ -184,20 +196,25 @@ def _csc_mean_variance_axis0(np.ndarray[floating, ndim=1] X_data,
         counts = endptr - startptr
 
         for j in xrange(startptr, endptr):
-            means[i] += X_data[j]
-        means[i] /= n_samples
+            if not isnan(X_data[j]):
+                means[i] += X_data[j]
+            else:
+                counts_nan[i] += 1
+        counts -= counts_nan[i]
+        means[i] /= (n_samples - counts_nan[i])
 
         for j in xrange(startptr, endptr):
-            diff = X_data[j] - means[i]
-            variances[i] += diff * diff
+            if not isnan(X_data[j]):
+                diff = X_data[j] - means[i]
+                variances[i] += diff * diff
 
-        variances[i] += (n_samples - counts) * means[i] * means[i]
-        variances[i] /= n_samples
+        variances[i] += (n_samples - counts_nan[i] - counts) * means[i]**2
+        variances[i] /= (n_samples - counts_nan[i])
 
-    return means, variances
+    return means, variances, counts_nan
 
 
-def incr_mean_variance_axis0(X, last_mean, last_var, unsigned long last_n):
+def incr_mean_variance_axis0(X, last_mean, last_var, last_n):
     """Compute mean and variance along axis 0 on a CSR or CSC matrix.
 
     last_mean, last_var are the statistics computed at the last step by this
@@ -215,24 +232,26 @@ def incr_mean_variance_axis0(X, last_mean, last_var, unsigned long last_n):
     last_var : float array with shape (n_features,)
       Array of feature-wise var to update with the new data X.
 
-    last_n : int
+    last_n : int array with shape (n_features,)
       Number of samples seen so far, before X.
 
     Returns
     -------
-
     updated_mean : float array with shape (n_features,)
       Feature-wise means
 
     updated_variance : float array with shape (n_features,)
       Feature-wise variances
 
-    updated_n : int
+    updated_n : int array with shape (n_features,)
       Updated number of samples seen
 
+    Notes
+    -----
+    NaNs are ignored during the computation.
+
     References
     ----------
-
     T. Chan, G. Golub, R. LeVeque. Algorithms for computing the sample
       variance: recommendations, The American Statistician, Vol. 37, No. 3,
       pp. 242-247
@@ -243,32 +262,35 @@ def incr_mean_variance_axis0(X, last_mean, last_var, unsigned long last_n):
     """
     if X.dtype != np.float32:
         X = X.astype(np.float64)
-    return _incr_mean_variance_axis0(X.data, X.shape, X.indices, X.indptr,
-                                     X.format, last_mean, last_var, last_n)
+    return _incr_mean_variance_axis0(X.data, X.shape[0], X.shape[1], X.indices,
+                                     X.indptr, X.format, last_mean, last_var,
+                                     last_n)
 
 
 def _incr_mean_variance_axis0(np.ndarray[floating, ndim=1] X_data,
-                              shape,
-                              np.ndarray[int, ndim=1] X_indices,
-                              np.ndarray[int, ndim=1] X_indptr,
-                              X_format,
-                              last_mean,
-                              last_var,
-                              unsigned long last_n):
+                              unsigned long long n_samples,
+                              unsigned long long n_features,
+                              np.ndarray[integral, ndim=1] X_indices,
+                              np.ndarray[integral, ndim=1] X_indptr,
+                              str X_format,
+                              np.ndarray[floating, ndim=1] last_mean,
+                              np.ndarray[floating, ndim=1] last_var,
+                              np.ndarray[np.int64_t, ndim=1] last_n):
     # Implement the function here since variables using fused types
     # cannot be declared directly and can only be passed as function arguments
-    cdef unsigned long n_samples = shape[0]
-    cdef unsigned int n_features = shape[1]
-    cdef unsigned int i
+    cdef:
+        np.npy_intp i
 
     # last = stats until now
     # new = the current increment
     # updated = the aggregated stats
     # when arrays, they are indexed by i per-feature
-    cdef np.ndarray[floating, ndim=1] new_mean
-    cdef np.ndarray[floating, ndim=1] new_var
-    cdef np.ndarray[floating, ndim=1] updated_mean
-    cdef np.ndarray[floating, ndim=1] updated_var
+    cdef:
+        np.ndarray[floating, ndim=1] new_mean
+        np.ndarray[floating, ndim=1] new_var
+        np.ndarray[floating, ndim=1] updated_mean
+        np.ndarray[floating, ndim=1] updated_var
+
     if floating is float:
         dtype = np.float32
     else:
@@ -279,40 +301,57 @@ def _incr_mean_variance_axis0(np.ndarray[floating, ndim=1] X_data,
     updated_mean = np.zeros_like(new_mean, dtype=dtype)
     updated_var = np.zeros_like(new_mean, dtype=dtype)
 
-    cdef unsigned long new_n
-    cdef unsigned long updated_n
-    cdef floating last_over_new_n
+    cdef:
+        np.ndarray[np.int64_t, ndim=1] new_n
+        np.ndarray[np.int64_t, ndim=1] updated_n
+        np.ndarray[floating, ndim=1] last_over_new_n
+        np.ndarray[np.int64_t, ndim=1] counts_nan
 
     # Obtain new stats first
-    new_n = n_samples
+    new_n = np.ones(n_features, dtype=np.int64) * n_samples
+    updated_n = np.zeros_like(new_n, dtype=np.int64)
+    last_over_new_n = np.zeros_like(new_n, dtype=dtype)
 
     if X_format == 'csr':
         # X is a CSR matrix
-        new_mean, new_var = _csr_mean_variance_axis0(X_data, shape, X_indices)
+        new_mean, new_var, counts_nan = _csr_mean_variance_axis0(
+            X_data, n_samples, n_features, X_indices)
     else:
         # X is a CSC matrix
-        new_mean, new_var = _csc_mean_variance_axis0(X_data, shape, X_indices,
-                                                     X_indptr)
+        new_mean, new_var, counts_nan = _csc_mean_variance_axis0(
+            X_data, n_samples, n_features, X_indices, X_indptr)
+
+    for i in xrange(n_features):
+        new_n[i] -= counts_nan[i]
 
     # First pass
-    if last_n == 0:
+    cdef bint is_first_pass = True
+    for i in xrange(n_features):
+        if last_n[i] > 0:
+            is_first_pass = False
+            break
+    if is_first_pass:
         return new_mean, new_var, new_n
 
     # Next passes
-    updated_n = last_n + new_n
-    last_over_new_n = last_n / new_n
+    for i in xrange(n_features):
+        updated_n[i] = last_n[i] + new_n[i]
+        last_over_new_n[i] = last_n[i] / new_n[i]
 
     # Unnormalized stats
-    last_mean *= last_n
-    last_var *= last_n
-    new_mean *= new_n
-    new_var *= new_n
+    for i in xrange(n_features):
+        last_mean[i] *= last_n[i]
+        last_var[i] *= last_n[i]
+        new_mean[i] *= new_n[i]
+        new_var[i] *= new_n[i]
 
     # Update stats
-    updated_var = (last_var + new_var + last_over_new_n / updated_n *
-                   (last_mean / last_over_new_n - new_mean) ** 2)
-    updated_mean = (last_mean + new_mean) / updated_n
-    updated_var /= updated_n
+    for i in xrange(n_features):
+        updated_var[i] = (last_var[i] + new_var[i] +
+                          last_over_new_n[i] / updated_n[i] *
+                          (last_mean[i] / last_over_new_n[i] - new_mean[i])**2)
+        updated_mean[i] = (last_mean[i] + new_mean[i]) / updated_n[i]
+        updated_var[i] /= updated_n[i]
 
     return updated_mean, updated_var, updated_n
 
diff --git a/sklearn/utils/testing.py b/sklearn/utils/testing.py
index 6f959687c56e..c67a314e2fc5 100644
--- a/sklearn/utils/testing.py
+++ b/sklearn/utils/testing.py
@@ -49,6 +49,7 @@
 from sklearn.utils.fixes import signature
 from sklearn.utils import deprecated
 
+
 additional_names_in_all = []
 try:
     from nose.tools import raises as _nose_raises
@@ -136,7 +137,6 @@ def assert_warns(warning_class, func, *args, **kw):
     result : the return value of `func`
 
     """
-    # very important to avoid uncontrolled state propagation
     clean_warning_registry()
     with warnings.catch_warnings(record=True) as w:
         # Cause all warnings to always be triggered.
@@ -320,7 +320,6 @@ def __call__(self, fn):
         """Decorator to catch and hide warnings without visual nesting."""
         @wraps(fn)
         def wrapper(*args, **kwargs):
-            # very important to avoid uncontrolled state propagation
             clean_warning_registry()
             with warnings.catch_warnings():
                 warnings.simplefilter("ignore", self.category)
@@ -338,14 +337,14 @@ def __repr__(self):
         return "%s(%s)" % (name, ", ".join(args))
 
     def __enter__(self):
-        clean_warning_registry()  # be safe and not propagate state + chaos
-        warnings.simplefilter("ignore", self.category)
         if self._entered:
             raise RuntimeError("Cannot enter %r twice" % self)
         self._entered = True
         self._filters = self._module.filters
         self._module.filters = self._filters[:]
         self._showwarning = self._module.showwarning
+        clean_warning_registry()
+        warnings.simplefilter("ignore", self.category)
 
     def __exit__(self, *exc_info):
         if not self._entered:
@@ -353,7 +352,7 @@ def __exit__(self, *exc_info):
         self._module.filters = self._filters
         self._module.showwarning = self._showwarning
         self.log[:] = []
-        clean_warning_registry()  # be safe and not propagate state + chaos
+        clean_warning_registry()
 
 
 assert_less = _dummy.assertLess
@@ -543,14 +542,14 @@ def uninstall_mldata_mock():
                    "RegressorChain"]
 # estimators that there is no way to default-construct sensibly
 OTHER = ["Pipeline", "FeatureUnion", "GridSearchCV", "RandomizedSearchCV",
-         "SelectFromModel"]
+         "SelectFromModel", "ColumnTransformer"]
 
 # some strange ones
 DONT_TEST = ['SparseCoder', 'DictVectorizer',
              'LabelBinarizer', 'LabelEncoder',
              'MultiLabelBinarizer', 'TfidfTransformer',
              'TfidfVectorizer', 'IsotonicRegression',
-             'OneHotEncoder', 'RandomTreesEmbedding', 'CategoricalEncoder',
+             'OneHotEncoder', 'RandomTreesEmbedding', 'OrdinalEncoder',
              'FeatureHasher', 'DummyClassifier', 'DummyRegressor',
              'TruncatedSVD', 'PolynomialFeatures',
              'GaussianRandomProjectionHash', 'HashingVectorizer',
@@ -696,41 +695,40 @@ def run_test(*args, **kwargs):
     skip_travis = pytest.mark.skipif(os.environ.get('TRAVIS') == 'true',
                                      reason='skip on travis')
 
+    #  Decorator for tests involving both BLAS calls and multiprocessing.
+    #
+    #  Under POSIX (e.g. Linux or OSX), using multiprocessing in conjunction
+    #  with some implementation of BLAS (or other libraries that manage an
+    #  internal posix thread pool) can cause a crash or a freeze of the Python
+    #  process.
+    #
+    #  In practice all known packaged distributions (from Linux distros or
+    #  Anaconda) of BLAS under Linux seems to be safe. So we this problem seems
+    #  to only impact OSX users.
+    #
+    #  This wrapper makes it possible to skip tests that can possibly cause
+    #  this crash under OS X with.
+    #
+    #  Under Python 3.4+ it is possible to use the `forkserver` start method
+    #  for multiprocessing to avoid this issue. However it can cause pickling
+    #  errors on interactively defined functions. It therefore not enabled by
+    #  default.
+
+    if_safe_multiprocessing_with_blas = pytest.mark.skipif(
+            sys.platform == 'darwin',
+            reason="Possible multi-process bug with some BLAS")
 except ImportError:
     pass
 
 
-def if_safe_multiprocessing_with_blas(func):
-    """Decorator for tests involving both BLAS calls and multiprocessing.
-
-    Under POSIX (e.g. Linux or OSX), using multiprocessing in conjunction with
-    some implementation of BLAS (or other libraries that manage an internal
-    posix thread pool) can cause a crash or a freeze of the Python process.
-
-    In practice all known packaged distributions (from Linux distros or
-    Anaconda) of BLAS under Linux seems to be safe. So we this problem seems to
-    only impact OSX users.
+def clean_warning_registry():
+    """Clean Python warning registry for easier testing of warning messages.
 
-    This wrapper makes it possible to skip tests that can possibly cause
-    this crash under OS X with.
+    We may not need to do this any more when getting rid of Python 2, not
+    entirely sure. See https://bugs.python.org/issue4180 and
+    https://bugs.python.org/issue21724 for more details.
 
-    Under Python 3.4+ it is possible to use the `forkserver` start method
-    for multiprocessing to avoid this issue. However it can cause pickling
-    errors on interactively defined functions. It therefore not enabled by
-    default.
     """
-    @wraps(func)
-    def run_test(*args, **kwargs):
-        if sys.platform == 'darwin':
-            raise SkipTest(
-                "Possible multi-process bug with some BLAS")
-        return func(*args, **kwargs)
-    return run_test
-
-
-def clean_warning_registry():
-    """Safe way to reset warnings."""
-    warnings.resetwarnings()
     reg = "__warningregistry__"
     for mod_name, mod in list(sys.modules.items()):
         if 'six.moves' in mod_name:
diff --git a/sklearn/utils/tests/test_estimator_checks.py b/sklearn/utils/tests/test_estimator_checks.py
index 950a3ce095b3..9b4a9c4f87c1 100644
--- a/sklearn/utils/tests/test_estimator_checks.py
+++ b/sklearn/utils/tests/test_estimator_checks.py
@@ -2,19 +2,20 @@
 import sys
 
 import numpy as np
-
 import scipy.sparse as sp
 
 from sklearn.externals.six.moves import cStringIO as StringIO
 from sklearn.externals import joblib
 
 from sklearn.base import BaseEstimator, ClassifierMixin
+from sklearn.utils import deprecated
 from sklearn.utils.testing import (assert_raises_regex, assert_true,
                                    assert_equal, ignore_warnings)
 from sklearn.utils.estimator_checks import check_estimator
 from sklearn.utils.estimator_checks import set_random_state
 from sklearn.utils.estimator_checks import set_checking_parameters
 from sklearn.utils.estimator_checks import check_estimators_unfitted
+from sklearn.utils.estimator_checks import check_fit_score_takes_y
 from sklearn.utils.estimator_checks import check_no_attributes_set_in_init
 from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier
 from sklearn.linear_model import LinearRegression, SGDClassifier
@@ -24,7 +25,8 @@
 from sklearn.linear_model import MultiTaskElasticNet
 from sklearn.svm import SVC
 from sklearn.neighbors import KNeighborsRegressor
-from sklearn.utils.validation import check_X_y, check_array
+from sklearn.utils.validation import (check_X_y, check_array,
+                                      LARGE_SPARSE_SUPPORTED)
 
 
 class CorrectNotFittedError(ValueError):
@@ -161,6 +163,54 @@ def predict(self, X):
         return np.zeros(X.shape[0])
 
 
+class LargeSparseNotSupportedClassifier(BaseEstimator):
+    def fit(self, X, y):
+        X, y = check_X_y(X, y,
+                         accept_sparse=("csr", "csc", "coo"),
+                         accept_large_sparse=True,
+                         multi_output=True,
+                         y_numeric=True)
+        if sp.issparse(X):
+            if X.getformat() == "coo":
+                if X.row.dtype == "int64" or X.col.dtype == "int64":
+                    raise ValueError(
+                        "Estimator doesn't support 64-bit indices")
+            elif X.getformat() in ["csc", "csr"]:
+                if X.indices.dtype == "int64" or X.indptr.dtype == "int64":
+                    raise ValueError(
+                        "Estimator doesn't support 64-bit indices")
+
+        return self
+
+
+class SparseTransformer(BaseEstimator):
+    def fit(self, X, y=None):
+        self.X_shape_ = check_array(X).shape
+        return self
+
+    def fit_transform(self, X, y=None):
+        return self.fit(X, y).transform(X)
+
+    def transform(self, X):
+        X = check_array(X)
+        if X.shape[1] != self.X_shape_[1]:
+            raise ValueError('Bad number of features')
+        return sp.csr_matrix(X)
+
+
+def test_check_fit_score_takes_y_works_on_deprecated_fit():
+    # Tests that check_fit_score_takes_y works on a class with
+    # a deprecated fit method
+
+    class TestEstimatorWithDeprecatedFitMethod(BaseEstimator):
+        @deprecated("Deprecated for the purpose of testing "
+                    "check_fit_score_takes_y")
+        def fit(self, X, y):
+            return self
+
+    check_fit_score_takes_y("test", TestEstimatorWithDeprecatedFitMethod())
+
+
 def test_check_estimator():
     # tests that the estimator actually fails on "bad" estimators.
     # not a complete test of all checks, which are very extensive.
@@ -235,6 +285,17 @@ def test_check_estimator():
         sys.stdout = old_stdout
     assert_true(msg in string_buffer.getvalue())
 
+    # Large indices test on bad estimator
+    msg = ('Estimator LargeSparseNotSupportedClassifier doesn\'t seem to '
+           r'support \S{3}_64 matrix, and is not failing gracefully.*')
+    # only supported by scipy version more than 0.14.0
+    if LARGE_SPARSE_SUPPORTED:
+        assert_raises_regex(AssertionError, msg, check_estimator,
+                            LargeSparseNotSupportedClassifier)
+
+    # non-regression test for estimators transforming to sparse data
+    check_estimator(SparseTransformer())
+
     # doesn't error on actual estimator
     check_estimator(AdaBoostClassifier)
     check_estimator(AdaBoostClassifier())
diff --git a/sklearn/utils/tests/test_extmath.py b/sklearn/utils/tests/test_extmath.py
index f53b814c7008..ee08e016abe6 100644
--- a/sklearn/utils/tests/test_extmath.py
+++ b/sklearn/utils/tests/test_extmath.py
@@ -9,8 +9,11 @@
 from scipy import linalg
 from scipy import stats
 
+import pytest
+
 from sklearn.utils.testing import assert_equal
 from sklearn.utils.testing import assert_almost_equal
+from sklearn.utils.testing import assert_allclose
 from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_true
@@ -170,9 +173,10 @@ def check_randomized_svd_low_rank(dtype):
         assert_almost_equal(s[:rank], sa[:rank], decimal=decimal)
 
 
-def test_randomized_svd_low_rank_all_dtypes():
-    for dtype in (np.int32, np.int64, np.float32, np.float64):
-        yield check_randomized_svd_low_rank, dtype
+@pytest.mark.parametrize('dtype',
+                         (np.int32, np.int64, np.float32, np.float64))
+def test_randomized_svd_low_rank_all_dtypes(dtype):
+    check_randomized_svd_low_rank(dtype)
 
 
 @ignore_warnings  # extmath.norm is deprecated to be removed in 0.21
@@ -191,34 +195,35 @@ def test_norm_squared_norm():
                     squared_norm, X.astype(int))
 
 
-def test_row_norms():
+@pytest.mark.parametrize('dtype',
+                         (np.float32, np.float64))
+def test_row_norms(dtype):
     X = np.random.RandomState(42).randn(100, 100)
-    for dtype in (np.float32, np.float64):
-        if dtype is np.float32:
-            precision = 4
-        else:
-            precision = 5
-
-        X = X.astype(dtype)
-        sq_norm = (X ** 2).sum(axis=1)
-
-        assert_array_almost_equal(sq_norm, row_norms(X, squared=True),
+    if dtype is np.float32:
+        precision = 4
+    else:
+        precision = 5
+
+    X = X.astype(dtype)
+    sq_norm = (X ** 2).sum(axis=1)
+
+    assert_array_almost_equal(sq_norm, row_norms(X, squared=True),
+                              precision)
+    assert_array_almost_equal(np.sqrt(sq_norm), row_norms(X), precision)
+
+    for csr_index_dtype in [np.int32, np.int64]:
+        Xcsr = sparse.csr_matrix(X, dtype=dtype)
+        # csr_matrix will use int32 indices by default,
+        # up-casting those to int64 when necessary
+        if csr_index_dtype is np.int64:
+            Xcsr.indptr = Xcsr.indptr.astype(csr_index_dtype)
+            Xcsr.indices = Xcsr.indices.astype(csr_index_dtype)
+        assert Xcsr.indices.dtype == csr_index_dtype
+        assert Xcsr.indptr.dtype == csr_index_dtype
+        assert_array_almost_equal(sq_norm, row_norms(Xcsr, squared=True),
+                                  precision)
+        assert_array_almost_equal(np.sqrt(sq_norm), row_norms(Xcsr),
                                   precision)
-        assert_array_almost_equal(np.sqrt(sq_norm), row_norms(X), precision)
-
-        for csr_index_dtype in [np.int32, np.int64]:
-            Xcsr = sparse.csr_matrix(X, dtype=dtype)
-            # csr_matrix will use int32 indices by default,
-            # up-casting those to int64 when necessary
-            if csr_index_dtype is np.int64:
-                Xcsr.indptr = Xcsr.indptr.astype(csr_index_dtype)
-                Xcsr.indices = Xcsr.indices.astype(csr_index_dtype)
-            assert Xcsr.indices.dtype == csr_index_dtype
-            assert Xcsr.indptr.dtype == csr_index_dtype
-            assert_array_almost_equal(sq_norm, row_norms(Xcsr, squared=True),
-                                      precision)
-            assert_array_almost_equal(np.sqrt(sq_norm), row_norms(Xcsr),
-                                      precision)
 
 
 def test_randomized_svd_low_rank_with_noise():
@@ -361,6 +366,21 @@ def test_randomized_svd_power_iteration_normalizer():
             assert_greater(15, np.abs(error_2 - error))
 
 
+def test_randomized_svd_sparse_warnings():
+    # randomized_svd throws a warning for lil and dok matrix
+    rng = np.random.RandomState(42)
+    X = make_low_rank_matrix(50, 20, effective_rank=10, random_state=rng)
+    n_components = 5
+    for cls in (sparse.lil_matrix, sparse.dok_matrix):
+        X = cls(X)
+        assert_warns_message(
+            sparse.SparseEfficiencyWarning,
+            "Calculating SVD of a {} is expensive. "
+            "csr_matrix is more efficient.".format(cls.__name__),
+            randomized_svd, X, n_components, n_iter=1,
+            power_iteration_normalizer='none')
+
+
 def test_svd_flip():
     # Check that svd_flip works in both situations, and reconstructs input.
     rs = np.random.RandomState(1999)
@@ -480,7 +500,7 @@ def test_incremental_variance_update_formulas():
 
     old_means = X1.mean(axis=0)
     old_variances = X1.var(axis=0)
-    old_sample_count = X1.shape[0]
+    old_sample_count = np.ones(X1.shape[1], dtype=np.int32) * X1.shape[0]
     final_means, final_variances, final_count = \
         _incremental_mean_and_var(X2, old_means, old_variances,
                                   old_sample_count)
@@ -489,6 +509,30 @@ def test_incremental_variance_update_formulas():
     assert_almost_equal(final_count, A.shape[0])
 
 
+def test_incremental_mean_and_variance_ignore_nan():
+    old_means = np.array([535., 535., 535., 535.])
+    old_variances = np.array([4225., 4225., 4225., 4225.])
+    old_sample_count = np.array([2, 2, 2, 2], dtype=np.int32)
+
+    X = np.array([[170, 170, 170, 170],
+                  [430, 430, 430, 430],
+                  [300, 300, 300, 300]])
+
+    X_nan = np.array([[170, np.nan, 170, 170],
+                      [np.nan, 170, 430, 430],
+                      [430, 430, np.nan, 300],
+                      [300, 300, 300, np.nan]])
+
+    X_means, X_variances, X_count = _incremental_mean_and_var(
+        X, old_means, old_variances, old_sample_count)
+    X_nan_means, X_nan_variances, X_nan_count = _incremental_mean_and_var(
+        X_nan, old_means, old_variances, old_sample_count)
+
+    assert_allclose(X_nan_means, X_means)
+    assert_allclose(X_nan_variances, X_variances)
+    assert_allclose(X_nan_count, X_count)
+
+
 @skip_if_32bit
 def test_incremental_variance_numerical_stability():
     # Test Youngs and Cramer incremental variance formulas.
@@ -558,12 +602,13 @@ def naive_mean_variance_update(x, last_mean, last_variance,
     assert_greater(np.abs(stable_var(A) - var).max(), tol)
 
     # Robust implementation: <tol (177)
-    mean, var, n = A0[0, :], np.zeros(n_features), n_samples // 2
+    mean, var = A0[0, :], np.zeros(n_features)
+    n = np.ones(n_features, dtype=np.int32) * (n_samples // 2)
     for i in range(A1.shape[0]):
         mean, var, n = \
             _incremental_mean_and_var(A1[i, :].reshape((1, A1.shape[1])),
                                       mean, var, n)
-    assert_equal(n, A.shape[0])
+    assert_array_equal(n, A.shape[0])
     assert_array_almost_equal(A.mean(axis=0), mean)
     assert_greater(tol, np.abs(stable_var(A) - var).max())
 
@@ -585,7 +630,8 @@ def test_incremental_variance_ddof():
                 incremental_variances = batch.var(axis=0)
                 # Assign this twice so that the test logic is consistent
                 incremental_count = batch.shape[0]
-                sample_count = batch.shape[0]
+                sample_count = (np.ones(batch.shape[1], dtype=np.int32) *
+                                batch.shape[0])
             else:
                 result = _incremental_mean_and_var(
                     batch, incremental_means, incremental_variances,
@@ -599,7 +645,7 @@ def test_incremental_variance_ddof():
             assert_almost_equal(incremental_means, calculated_means, 6)
             assert_almost_equal(incremental_variances,
                                 calculated_variances, 6)
-            assert_equal(incremental_count, sample_count)
+            assert_array_equal(incremental_count, sample_count)
 
 
 def test_vector_sign_flip():
diff --git a/sklearn/utils/tests/test_sparsefuncs.py b/sklearn/utils/tests/test_sparsefuncs.py
index f2b35e745983..838435a0deab 100644
--- a/sklearn/utils/tests/test_sparsefuncs.py
+++ b/sklearn/utils/tests/test_sparsefuncs.py
@@ -1,3 +1,4 @@
+import pytest
 import numpy as np
 import scipy.sparse as sp
 
@@ -19,6 +20,7 @@
                                             inplace_csr_row_normalize_l1,
                                             inplace_csr_row_normalize_l2)
 from sklearn.utils.testing import assert_raises
+from sklearn.utils.testing import assert_allclose
 
 
 def test_mean_variance_axis0():
@@ -94,7 +96,7 @@ def test_incr_mean_variance_axis():
         # default params for incr_mean_variance
         last_mean = np.zeros(n_features)
         last_var = np.zeros_like(last_mean)
-        last_n = 0
+        last_n = np.zeros_like(last_mean, dtype=np.int64)
 
         # Test errors
         X = np.array(data_chunks[0])
@@ -136,6 +138,8 @@ def test_incr_mean_variance_axis():
         for input_dtype, output_dtype in expected_dtypes:
             for X_sparse in (X_csr, X_csc):
                 X_sparse = X_sparse.astype(input_dtype)
+                last_mean = last_mean.astype(output_dtype)
+                last_var = last_var.astype(output_dtype)
                 X_means, X_vars = mean_variance_axis(X_sparse, axis)
                 X_means_incr, X_vars_incr, n_incr = \
                     incr_mean_variance_axis(X_sparse, axis, last_mean,
@@ -147,6 +151,43 @@ def test_incr_mean_variance_axis():
                 assert_equal(X.shape[axis], n_incr)
 
 
+@pytest.mark.parametrize("axis", [0, 1])
+@pytest.mark.parametrize("sparse_constructor", [sp.csc_matrix, sp.csr_matrix])
+def test_incr_mean_variance_axis_ignore_nan(axis, sparse_constructor):
+    old_means = np.array([535., 535., 535., 535.])
+    old_variances = np.array([4225., 4225., 4225., 4225.])
+    old_sample_count = np.array([2, 2, 2, 2], dtype=np.int64)
+
+    X = sparse_constructor(
+        np.array([[170, 170, 170, 170],
+                  [430, 430, 430, 430],
+                  [300, 300, 300, 300]]))
+
+    X_nan = sparse_constructor(
+        np.array([[170, np.nan, 170, 170],
+                  [np.nan, 170, 430, 430],
+                  [430, 430, np.nan, 300],
+                  [300, 300, 300, np.nan]]))
+
+    # we avoid creating specific data for axis 0 and 1: translating the data is
+    # enough.
+    if axis:
+        X = X.T
+        X_nan = X_nan.T
+
+    # take a copy of the old statistics since they are modified in place.
+    X_means, X_vars, X_sample_count = incr_mean_variance_axis(
+        X, axis, old_means.copy(), old_variances.copy(),
+        old_sample_count.copy())
+    X_nan_means, X_nan_vars, X_nan_sample_count = incr_mean_variance_axis(
+        X_nan, axis, old_means.copy(), old_variances.copy(),
+        old_sample_count.copy())
+
+    assert_allclose(X_nan_means, X_means)
+    assert_allclose(X_nan_vars, X_vars)
+    assert_allclose(X_nan_sample_count, X_sample_count)
+
+
 def test_mean_variance_illegal_axis():
     X, _ = make_classification(5, 4, random_state=0)
     # Sparsify the array a little bit
@@ -344,60 +385,27 @@ def test_inplace_swap_column():
     assert_raises(TypeError, inplace_swap_column, X_csr.tolil())
 
 
-def test_min_max_axis0():
+@pytest.mark.parametrize("dtype", [np.float32, np.float64])
+@pytest.mark.parametrize("axis", [0, 1, None])
+@pytest.mark.parametrize("sparse_format", [sp.csr_matrix, sp.csc_matrix])
+@pytest.mark.parametrize(
+    "missing_values, min_func, max_func, ignore_nan",
+    [(0, np.min, np.max, False),
+     (np.nan, np.nanmin, np.nanmax, True)]
+)
+def test_min_max(dtype, axis, sparse_format, missing_values, min_func,
+                 max_func, ignore_nan):
     X = np.array([[0, 3, 0],
-                  [2, -1, 0],
+                  [2, -1, missing_values],
                   [0, 0, 0],
-                  [9, 8, 7],
-                  [4, 0, 5]], dtype=np.float64)
-    X_csr = sp.csr_matrix(X)
-    X_csc = sp.csc_matrix(X)
-
-    mins_csr, maxs_csr = min_max_axis(X_csr, axis=0)
-    assert_array_equal(mins_csr, X.min(axis=0))
-    assert_array_equal(maxs_csr, X.max(axis=0))
-
-    mins_csc, maxs_csc = min_max_axis(X_csc, axis=0)
-    assert_array_equal(mins_csc, X.min(axis=0))
-    assert_array_equal(maxs_csc, X.max(axis=0))
-
-    X = X.astype(np.float32)
-    X_csr = sp.csr_matrix(X)
-    X_csc = sp.csc_matrix(X)
-    mins_csr, maxs_csr = min_max_axis(X_csr, axis=0)
-    assert_array_equal(mins_csr, X.min(axis=0))
-    assert_array_equal(maxs_csr, X.max(axis=0))
-    mins_csc, maxs_csc = min_max_axis(X_csc, axis=0)
-    assert_array_equal(mins_csc, X.min(axis=0))
-    assert_array_equal(maxs_csc, X.max(axis=0))
-
-
-def test_min_max_axis1():
-    X = np.array([[0, 3, 0],
-                  [2, -1, 0],
-                  [0, 0, 0],
-                  [9, 8, 7],
-                  [4, 0, 5]], dtype=np.float64)
-    X_csr = sp.csr_matrix(X)
-    X_csc = sp.csc_matrix(X)
-
-    mins_csr, maxs_csr = min_max_axis(X_csr, axis=1)
-    assert_array_equal(mins_csr, X.min(axis=1))
-    assert_array_equal(maxs_csr, X.max(axis=1))
-
-    mins_csc, maxs_csc = min_max_axis(X_csc, axis=1)
-    assert_array_equal(mins_csc, X.min(axis=1))
-    assert_array_equal(maxs_csc, X.max(axis=1))
-
-    X = X.astype(np.float32)
-    X_csr = sp.csr_matrix(X)
-    X_csc = sp.csc_matrix(X)
-    mins_csr, maxs_csr = min_max_axis(X_csr, axis=1)
-    assert_array_equal(mins_csr, X.min(axis=1))
-    assert_array_equal(maxs_csr, X.max(axis=1))
-    mins_csc, maxs_csc = min_max_axis(X_csc, axis=1)
-    assert_array_equal(mins_csc, X.min(axis=1))
-    assert_array_equal(maxs_csc, X.max(axis=1))
+                  [9, missing_values, 7],
+                  [4, 0, 5]], dtype=dtype)
+    X_sparse = sparse_format(X)
+
+    mins_sparse, maxs_sparse = min_max_axis(X_sparse, axis=axis,
+                                            ignore_nan=ignore_nan)
+    assert_array_equal(mins_sparse, min_func(X, axis=axis))
+    assert_array_equal(maxs_sparse, max_func(X, axis=axis))
 
 
 def test_min_max_axis_errors():
diff --git a/sklearn/utils/tests/test_stats.py b/sklearn/utils/tests/test_stats.py
index fbd05031c87b..36e3bf72b609 100644
--- a/sklearn/utils/tests/test_stats.py
+++ b/sklearn/utils/tests/test_stats.py
@@ -1,3 +1,4 @@
+import pytest
 from sklearn.utils.testing import assert_array_equal, ignore_warnings
 
 from sklearn.utils.stats import rankdata
@@ -13,12 +14,10 @@
 )
 
 
-@ignore_warnings  # Test deprecated backport to be removed in 0.21
-def test_cases():
+@pytest.mark.parametrize("values, method, expected", _cases)
+def test_cases_rankdata(values, method, expected):
 
-    def check_case(values, method, expected):
+    # Test deprecated backport to be removed in 0.21
+    with ignore_warnings():
         r = rankdata(values, method=method)
         assert_array_equal(r, expected)
-
-    for values, method, expected in _cases:
-        yield check_case, values, method, expected
diff --git a/sklearn/utils/tests/test_testing.py b/sklearn/utils/tests/test_testing.py
index 6b55431d21d7..eb9512f177ed 100644
--- a/sklearn/utils/tests/test_testing.py
+++ b/sklearn/utils/tests/test_testing.py
@@ -211,26 +211,19 @@ def context_manager_no_user_multiple_warning():
     assert_warns(DeprecationWarning, context_manager_no_user_multiple_warning)
 
 
-# This class is inspired from numpy 1.7 with an alteration to check
-# the reset warning filters after calls to assert_warns.
-# This assert_warns behavior is specific to scikit-learn because
-# `clean_warning_registry()` is called internally by assert_warns
-# and clears all previous filters.
 class TestWarns(unittest.TestCase):
     def test_warn(self):
         def f():
             warnings.warn("yo")
             return 3
 
-        # Test that assert_warns is not impacted by externally set
-        # filters and is reset internally.
-        # This is because `clean_warning_registry()` is called internally by
-        # assert_warns and clears all previous filters.
-        warnings.simplefilter("ignore", UserWarning)
-        assert_equal(assert_warns(UserWarning, f), 3)
-
-        # Test that the warning registry is empty after assert_warns
-        assert_equal(sys.modules['warnings'].filters, [])
+        with warnings.catch_warnings():
+            warnings.simplefilter("ignore", UserWarning)
+            filters_orig = warnings.filters[:]
+            assert_equal(assert_warns(UserWarning, f), 3)
+            # test that assert_warns doesn't have side effects on warnings
+            # filters
+            assert_equal(warnings.filters, filters_orig)
 
         assert_raises(AssertionError, assert_no_warnings, f)
         assert_equal(assert_no_warnings(lambda x: x, 1), 1)
diff --git a/sklearn/utils/tests/test_utils.py b/sklearn/utils/tests/test_utils.py
index 1f1efed825c8..c2474c58c13f 100644
--- a/sklearn/utils/tests/test_utils.py
+++ b/sklearn/utils/tests/test_utils.py
@@ -21,6 +21,7 @@
 from sklearn.utils import shuffle
 from sklearn.utils import gen_even_slices
 from sklearn.utils import get_chunk_n_rows
+from sklearn.utils import is_scalar_nan
 from sklearn.utils.extmath import pinvh
 from sklearn.utils.arpack import eigsh
 from sklearn.utils.mocking import MockDataFrame
@@ -314,3 +315,18 @@ def check_warning(*args, **kw):
                                max_n_rows=max_n_rows)
         assert actual == expected
         assert type(actual) is type(expected)
+
+
+@pytest.mark.parametrize("value, result", [(float("nan"), True),
+                                           (np.nan, True),
+                                           (np.float("nan"), True),
+                                           (np.float32("nan"), True),
+                                           (np.float64("nan"), True),
+                                           (0, False),
+                                           (0., False),
+                                           (None, False),
+                                           ("", False),
+                                           ("nan", False),
+                                           ([np.nan], False)])
+def test_is_scalar_nan(value, result):
+    assert is_scalar_nan(value) is result
diff --git a/sklearn/utils/tests/test_validation.py b/sklearn/utils/tests/test_validation.py
index 076e6d88440f..00182fa0aa71 100644
--- a/sklearn/utils/tests/test_validation.py
+++ b/sklearn/utils/tests/test_validation.py
@@ -9,6 +9,7 @@
 import pytest
 import numpy as np
 import scipy.sparse as sp
+from scipy import __version__ as scipy_version
 
 from sklearn.utils.testing import assert_true, assert_false, assert_equal
 from sklearn.utils.testing import assert_raises
@@ -22,6 +23,7 @@
 from sklearn.utils.testing import assert_allclose_dense_sparse
 from sklearn.utils import as_float_array, check_array, check_symmetric
 from sklearn.utils import check_X_y
+from sklearn.utils import deprecated
 from sklearn.utils.mocking import MockDataFrame
 from sklearn.utils.estimator_checks import NotAnArray
 from sklearn.random_projection import sparse_random_matrix
@@ -35,7 +37,8 @@
     check_is_fitted,
     check_consistent_length,
     assert_all_finite,
-    check_memory
+    check_memory,
+    LARGE_SPARSE_SUPPORTED
 )
 import sklearn
 
@@ -464,6 +467,42 @@ def test_check_array_accept_sparse_no_exception():
     check_array(X_csr, accept_sparse=('csr',))
 
 
+@pytest.fixture(params=['csr', 'csc', 'coo', 'bsr'])
+def X_64bit(request):
+    X = sp.rand(20, 10, format=request.param)
+    for attr in ['indices', 'indptr', 'row', 'col']:
+        if hasattr(X, attr):
+            setattr(X, attr, getattr(X, attr).astype('int64'))
+    yield X
+
+
+def test_check_array_accept_large_sparse_no_exception(X_64bit):
+    # When large sparse are allowed
+    if LARGE_SPARSE_SUPPORTED:
+        check_array(X_64bit, accept_large_sparse=True, accept_sparse=True)
+
+
+def test_check_array_accept_large_sparse_raise_exception(X_64bit):
+    # When large sparse are not allowed
+    if LARGE_SPARSE_SUPPORTED:
+        msg = ("Only sparse matrices with 32-bit integer indices "
+               "are accepted. Got int64 indices.")
+        assert_raise_message(ValueError, msg,
+                             check_array, X_64bit,
+                             accept_sparse=True,
+                             accept_large_sparse=False)
+
+
+def test_check_array_large_indices_non_supported_scipy_version(X_64bit):
+    # Large indices should not be allowed for scipy<0.14.0
+    if not LARGE_SPARSE_SUPPORTED:
+        msg = ("Scipy version %s does not support large"
+               " indices, please upgrade your scipy"
+               " to 0.14.0 or above" % scipy_version)
+        assert_raise_message(ValueError, msg, check_array,
+                             X_64bit, accept_sparse='csc')
+
+
 def test_check_array_min_samples_and_features_messages():
     # empty list is considered 2D by default:
     msg = "0 feature(s) (shape=(1, 0)) while a minimum of 1 is required."
@@ -563,6 +602,15 @@ def test_has_fit_parameter():
     assert_true(has_fit_parameter(SVR, "sample_weight"))
     assert_true(has_fit_parameter(SVR(), "sample_weight"))
 
+    class TestClassWithDeprecatedFitMethod:
+        @deprecated("Deprecated for the purpose of testing has_fit_parameter")
+        def fit(self, X, y, sample_weight=None):
+            pass
+
+    assert has_fit_parameter(TestClassWithDeprecatedFitMethod,
+                             "sample_weight"), \
+        "has_fit_parameter fails for class with deprecated fit method."
+
 
 def test_check_symmetric():
     arr_sym = np.array([[0, 1], [1, 2]])
diff --git a/sklearn/utils/validation.py b/sklearn/utils/validation.py
index 5fd54dc49b07..fe1f7236ecac 100644
--- a/sklearn/utils/validation.py
+++ b/sklearn/utils/validation.py
@@ -13,6 +13,9 @@
 
 import numpy as np
 import scipy.sparse as sp
+from scipy import __version__ as scipy_version
+from distutils.version import LooseVersion
+
 from numpy.core.numeric import ComplexWarning
 
 from ..externals import six
@@ -30,6 +33,9 @@
 # performance profiling.
 warnings.simplefilter('ignore', NonBLASDotWarning)
 
+# checking whether large sparse are supported by scipy or not
+LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'
+
 
 def _assert_all_finite(X, allow_nan=False):
     """Like assert_all_finite, but only for ndarray."""
@@ -248,7 +254,7 @@ def indexable(*iterables):
 
 
 def _ensure_sparse_format(spmatrix, accept_sparse, dtype, copy,
-                          force_all_finite):
+                          force_all_finite, accept_large_sparse):
     """Convert a sparse matrix to a given format.
 
     Checks the sparse format of spmatrix and converts if necessary.
@@ -297,6 +303,9 @@ def _ensure_sparse_format(spmatrix, accept_sparse, dtype, copy,
     if isinstance(accept_sparse, six.string_types):
         accept_sparse = [accept_sparse]
 
+    # Indices dtype validation
+    _check_large_sparse(spmatrix, accept_large_sparse)
+
     if accept_sparse is False:
         raise TypeError('A sparse matrix was passed, but dense '
                         'data is required. Use X.toarray() to '
@@ -342,10 +351,11 @@ def _ensure_no_complex_data(array):
                          "{}\n".format(array))
 
 
-def check_array(array, accept_sparse=False, dtype="numeric", order=None,
-                copy=False, force_all_finite=True, ensure_2d=True,
-                allow_nd=False, ensure_min_samples=1, ensure_min_features=1,
-                warn_on_dtype=False, estimator=None):
+def check_array(array, accept_sparse=False, accept_large_sparse=True,
+                dtype="numeric", order=None, copy=False, force_all_finite=True,
+                ensure_2d=True, allow_nd=False, ensure_min_samples=1,
+                ensure_min_features=1, warn_on_dtype=False, estimator=None):
+
     """Input validation on an array, list, sparse matrix or similar.
 
     By default, the input is converted to an at least 2D numpy array.
@@ -369,6 +379,13 @@ def check_array(array, accept_sparse=False, dtype="numeric", order=None,
            deprecated in version 0.19 "and will be removed in 0.21. Use
            ``accept_sparse=False`` instead.
 
+    accept_large_sparse : bool (default=True)
+        If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by
+        accept_sparse, accept_large_sparse=False will cause it to be accepted
+        only if its indices are stored with a 32-bit dtype.
+
+        .. versionadded:: 0.20
+
     dtype : string, type, list of types or None (default="numeric")
         Data type of result. If None, the dtype of the input is preserved.
         If "numeric", dtype is preserved unless array.dtype is object.
@@ -480,8 +497,10 @@ def check_array(array, accept_sparse=False, dtype="numeric", order=None,
 
     if sp.issparse(array):
         _ensure_no_complex_data(array)
-        array = _ensure_sparse_format(array, accept_sparse, dtype, copy,
-                                      force_all_finite)
+        array = _ensure_sparse_format(array, accept_sparse=accept_sparse,
+                                      dtype=dtype, copy=copy,
+                                      force_all_finite=force_all_finite,
+                                      accept_large_sparse=accept_large_sparse)
     else:
         # If np.array(..) gives ComplexWarning, then we convert the warning
         # to an error. This is needed because specifying a non complex
@@ -565,10 +584,33 @@ def check_array(array, accept_sparse=False, dtype="numeric", order=None,
     return array
 
 
-def check_X_y(X, y, accept_sparse=False, dtype="numeric", order=None,
-              copy=False, force_all_finite=True, ensure_2d=True,
-              allow_nd=False, multi_output=False, ensure_min_samples=1,
-              ensure_min_features=1, y_numeric=False,
+def _check_large_sparse(X, accept_large_sparse=False):
+    """Raise a ValueError if X has 64bit indices and accept_large_sparse=False
+    """
+    if not (accept_large_sparse and LARGE_SPARSE_SUPPORTED):
+        supported_indices = ["int32"]
+        if X.getformat() == "coo":
+            index_keys = ['col', 'row']
+        elif X.getformat() in ["csr", "csc", "bsr"]:
+            index_keys = ['indices', 'indptr']
+        else:
+            return
+        for key in index_keys:
+            indices_datatype = getattr(X, key).dtype
+            if (indices_datatype not in supported_indices):
+                if not LARGE_SPARSE_SUPPORTED:
+                    raise ValueError("Scipy version %s does not support large"
+                                     " indices, please upgrade your scipy"
+                                     " to 0.14.0 or above" % scipy_version)
+                raise ValueError("Only sparse matrices with 32-bit integer"
+                                 " indices are accepted. Got %s indices."
+                                 % indices_datatype)
+
+
+def check_X_y(X, y, accept_sparse=False, accept_large_sparse=True,
+              dtype="numeric", order=None, copy=False, force_all_finite=True,
+              ensure_2d=True, allow_nd=False, multi_output=False,
+              ensure_min_samples=1, ensure_min_features=1, y_numeric=False,
               warn_on_dtype=False, estimator=None):
     """Input validation for standard estimators.
 
@@ -598,6 +640,13 @@ def check_X_y(X, y, accept_sparse=False, dtype="numeric", order=None,
            deprecated in version 0.19 "and will be removed in 0.21. Use
            ``accept_sparse=False`` instead.
 
+    accept_large_sparse : bool (default=True)
+        If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by
+        accept_sparse, accept_large_sparse will cause it to be accepted only
+        if its indices are stored with a 32-bit dtype.
+
+        .. versionadded:: 0.20
+
     dtype : string, type, list of types or None (default="numeric")
         Data type of result. If None, the dtype of the input is preserved.
         If "numeric", dtype is preserved unless array.dtype is object.
@@ -666,9 +715,15 @@ def check_X_y(X, y, accept_sparse=False, dtype="numeric", order=None,
     y_converted : object
         The converted and validated y.
     """
-    X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,
-                    ensure_2d, allow_nd, ensure_min_samples,
-                    ensure_min_features, warn_on_dtype, estimator)
+    X = check_array(X, accept_sparse=accept_sparse,
+                    accept_large_sparse=accept_large_sparse,
+                    dtype=dtype, order=order, copy=copy,
+                    force_all_finite=force_all_finite,
+                    ensure_2d=ensure_2d, allow_nd=allow_nd,
+                    ensure_min_samples=ensure_min_samples,
+                    ensure_min_features=ensure_min_features,
+                    warn_on_dtype=warn_on_dtype,
+                    estimator=estimator)
     if multi_output:
         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,
                         dtype=None)
