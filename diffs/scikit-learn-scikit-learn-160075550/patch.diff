diff --git a/AUTHORS.rst b/AUTHORS.rst
index 40140feb1cc4..70012765afea 100644
--- a/AUTHORS.rst
+++ b/AUTHORS.rst
@@ -64,3 +64,8 @@ The following people have been core contributors to scikit-learn's development a
   * Nelle Varoquaux
   * `Gael Varoquaux <http://gael-varoquaux.info/>`_
   * Ron Weiss
+
+Please do not email the authors directly to ask for assistance or report issues.
+Instead, please see `What's the best way to ask questions about scikit-learn
+<http://scikit-learn.org/stable/faq.html#what-s-the-best-way-to-ask-questions-about-scikit-learn>`_
+in the FAQ.
diff --git a/benchmarks/bench_isotonic.py b/benchmarks/bench_isotonic.py
index e201d8980089..0a4fb6de448b 100644
--- a/benchmarks/bench_isotonic.py
+++ b/benchmarks/bench_isotonic.py
@@ -22,8 +22,8 @@
 
 
 def generate_perturbed_logarithm_dataset(size):
-    return np.random.randint(-50, 50, size=n) \
-        + 50. * np.log(1 + np.arange(n))
+    return (np.random.randint(-50, 50, size=size) +
+            50. * np.log(1 + np.arange(size)))
 
 
 def generate_logistic_dataset(size):
@@ -31,9 +31,17 @@ def generate_logistic_dataset(size):
     return np.random.random(size=size) < 1.0 / (1.0 + np.exp(-X))
 
 
+def generate_pathological_dataset(size):
+    # Triggers O(n^2) complexity on the original implementation.
+    return np.r_[np.arange(size),
+                 np.arange(-(size - 1), size),
+                 np.arange(-(size - 1), 1)]
+
+
 DATASET_GENERATORS = {
     'perturbed_logarithm': generate_perturbed_logarithm_dataset,
-    'logistic': generate_logistic_dataset
+    'logistic': generate_logistic_dataset,
+    'pathological': generate_pathological_dataset,
 }
 
 
@@ -53,6 +61,8 @@ def bench_isotonic_regression(Y):
 if __name__ == '__main__':
     parser = argparse.ArgumentParser(
         description="Isotonic Regression benchmark tool")
+    parser.add_argument('--seed', type=int,
+                        help="RNG seed")
     parser.add_argument('--iterations', type=int, required=True,
                         help="Number of iterations to average timings over "
                         "for each problem size")
@@ -67,6 +77,8 @@ def bench_isotonic_regression(Y):
 
     args = parser.parse_args()
 
+    np.random.seed(args.seed)
+
     timings = []
     for exponent in range(args.log_min_problem_size,
                           args.log_max_problem_size):
diff --git a/build_tools/travis/flake8_diff.sh b/build_tools/travis/flake8_diff.sh
index 9f639c5734a0..20fb3d0a1bfe 100755
--- a/build_tools/travis/flake8_diff.sh
+++ b/build_tools/travis/flake8_diff.sh
@@ -38,10 +38,20 @@ fi
 if [[ "$TRAVIS" == "true" ]]; then
     if [[ "$TRAVIS_PULL_REQUEST" == "false" ]]
     then
-        # Travis does the git clone with a limited depth (50 at the time of
-        # writing). This may not be enough to find the common ancestor with
-        # $REMOTE/master so we unshallow the git checkout
-        git fetch --unshallow || echo "Unshallowing the git checkout failed"
+        # In main repo, using TRAVIS_COMMIT_RANGE to test the commits
+        # that were pushed into a branch
+        if [[ "$PROJECT" == "$TRAVIS_REPO_SLUG" ]]; then
+            if [[ -z "$TRAVIS_COMMIT_RANGE" ]]; then
+                echo "New branch, no commit range from Travis so passing this test by convention"
+                exit 0
+            fi
+            COMMIT_RANGE=$TRAVIS_COMMIT_RANGE
+        else
+            # Travis does the git clone with a limited depth (50 at the time of
+            # writing). This may not be enough to find the common ancestor with
+            # $REMOTE/master so we unshallow the git checkout
+            git fetch --unshallow || echo "Unshallowing the git checkout failed"
+        fi
     else
         # We want to fetch the code as it is in the PR branch and not
         # the result of the merge into master. This way line numbers
@@ -57,28 +67,36 @@ echo -e '\nLast 2 commits:'
 echo '--------------------------------------------------------------------------------'
 git log -2 --pretty=short
 
-git fetch $REMOTE master
-REMOTE_MASTER_REF="$REMOTE/master"
+# If not using the commit range from Travis we need to find the common
+# ancestor between HEAD and $REMOTE/master
+if [[ -z "$COMMIT_RANGE" ]]; then
+    REMOTE_MASTER_REF="$REMOTE/master"
+    # Make sure that $REMOTE_MASTER_REF is a valid reference
+    git fetch $REMOTE master:refs/$REMOTE_MASTER_REF
 
-# Find common ancestor between HEAD and remotes/$REMOTE/master
-COMMIT=$(git merge-base @ $REMOTE_MASTER_REF) || \
-    echo "No common ancestor found for $(git show @ -q) and $(git show $REMOTE_MASTER_REF -q)"
+    COMMIT=$(git merge-base @ $REMOTE_MASTER_REF) || \
+        echo "No common ancestor found for $(git show @ -q) and $(git show $REMOTE_MASTER_REF -q)"
 
-if [[ -n "$TMP_REMOTE" ]]; then
-    git remote remove $TMP_REMOTE
-fi
+    if [[ -n "$TMP_REMOTE" ]]; then
+        git remote remove $TMP_REMOTE
+    fi
 
-if [ -z "$COMMIT" ]; then
-    exit 1
-fi
+    if [ -z "$COMMIT" ]; then
+        exit 1
+    fi
 
-echo -e "\nCommon ancestor between HEAD and $REMOTE_MASTER_REF is:"
-echo '--------------------------------------------------------------------------------'
-git show --no-patch $COMMIT
+    echo -e "\nCommon ancestor between HEAD and $REMOTE_MASTER_REF is:"
+    echo '--------------------------------------------------------------------------------'
+    git show --no-patch $COMMIT
+
+    COMMIT_RANGE="$(git rev-parse --short $COMMIT)..$(git rev-parse --short @)"
+
+else
+    echo "Got the commit range from Travis: $COMMIT_RANGE"
+fi
 
-echo -e '\nRunning flake8 on the diff in the range'\
-     "$(git rev-parse --short $COMMIT)..$(git rev-parse --short @)" \
-     "($(git rev-list $COMMIT.. | wc -l) commit(s)):"
+echo -e '\nRunning flake8 on the diff in the range' "$COMMIT_RANGE" \
+     "($(git rev-list $COMMIT_RANGE | wc -l) commit(s)):"
 echo '--------------------------------------------------------------------------------'
 
 # We ignore files from sklearn/externals. Unfortunately there is no
@@ -88,10 +106,11 @@ echo '--------------------------------------------------------------------------
 # uses git 1.8.
 # We need the following command to exit with 0 hence the echo in case
 # there is no match
-MODIFIED_FILES=$(git diff --name-only $COMMIT | grep -v 'sklearn/externals' || echo "no_match")
+MODIFIED_FILES=$(git diff --name-only $COMMIT_RANGE | grep -v 'sklearn/externals' | \
+                     grep -v 'doc/sphinxext/sphinx_gallery' || echo "no_match")
 
 if [[ "$MODIFIED_FILES" == "no_match" ]]; then
-    echo "No file outside sklearn/externals has been modified"
+    echo "No file outside sklearn/externals and doc/sphinxext/sphinx_gallery has been modified"
 else
     # Conservative approach: diff without context so that code that
     # was not changed does not create failures
diff --git a/doc/developers/advanced_installation.rst b/doc/developers/advanced_installation.rst
index 9029f5991afa..c21e1b6bf13e 100644
--- a/doc/developers/advanced_installation.rst
+++ b/doc/developers/advanced_installation.rst
@@ -52,31 +52,31 @@ Linux
 -----
 
 At this time scikit-learn does not provide official binary packages for Linux
-so you have to build from source if you want the lastest version.
+so you have to build from source if you want the latest version.
 If you don't need the newest version, consider using your package manager to
-install scikit-learn. it is usually the easiest way, but might not provide the
+install scikit-learn. It is usually the easiest way, but might not provide the
 newest version.
 
-installing build dependencies
+Installing build dependencies
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
-installing from source requires you to have installed the scikit-learn runtime
-dependencies, python development headers and a working C/C++ compiler.
-under debian-based operating systems, which include ubuntu, if you have
-python 2 you can install all these requirements by issuing::
+Installing from source requires you to have installed the scikit-learn runtime
+dependencies, Python development headers and a working C/C++ compiler.
+Under Debian-based operating systems, which include Ubuntu, if you have
+Python 2 you can install all these requirements by issuing::
 
     sudo apt-get install build-essential python-dev python-setuptools \
                          python-numpy python-scipy \
                          libatlas-dev libatlas3gf-base
 
-if you have python 3::
+If you have Python 3::
 
     sudo apt-get install build-essential python3-dev python3-setuptools \
                          python3-numpy python3-scipy \
                          libatlas-dev libatlas3gf-base
 
-on recent debian and ubuntu (e.g. ubuntu 13.04 or later) make sure that atlas
-is used to provide the implementation of the blas and lapack linear algebra
+On recent Debian and Ubuntu (e.g. Ubuntu 13.04 or later) make sure that ATLAS
+is used to provide the implementation of the BLAS and LAPACK linear algebra
 routines::
 
     sudo update-alternatives --set libblas.so.3 \
@@ -86,24 +86,24 @@ routines::
 
 .. note::
 
-    in order to build the documentation and run the example code contains in
+    In order to build the documentation and run the example code contains in
     this documentation you will need matplotlib::
 
         sudo apt-get install python-matplotlib
 
 .. note::
 
-    the above installs the atlas implementation of blas
-    (the basic linear algebra subprograms library).
-    ubuntu 11.10 and later, and recent (testing) versions of debian,
-    offer an alternative implementation called openblas.
+    The above installs the ATLAS implementation of BLAS
+    (the Basic Linear Algebra Subprograms library).
+    Ubuntu 11.10 and later, and recent (testing) versions of Debian,
+    offer an alternative implementation called OpenBLAS.
 
-    using openblas can give speedups in some scikit-learn modules,
-    but can freeze joblib/multiprocessing prior to openblas version 0.2.8-4,
+    Using OpenBLAS can give speedups in some scikit-learn modules,
+    but can freeze joblib/multiprocessing prior to OpenBLAS version 0.2.8-4,
     so using it is not recommended unless you know what you're doing.
 
-    if you do want to use openblas, then replacing atlas only requires a couple
-    of commands. atlas has to be removed, otherwise numpy may not work::
+    If you do want to use OpenBLAS, then replacing ATLAS only requires a couple
+    of commands. ATLAS has to be removed, otherwise NumPy may not work::
 
         sudo apt-get remove libatlas3gf-base libatlas-dev
         sudo apt-get install libopenblas-dev
@@ -113,30 +113,30 @@ routines::
         sudo update-alternatives --set liblapack.so.3 \
             /usr/lib/lapack/liblapack.so.3
 
-on red hat and clones (e.g. centos), install the dependencies using::
+On Red Hat and clones (e.g. CentOS), install the dependencies using::
 
     sudo yum -y install gcc gcc-c++ numpy python-devel scipy
 
 
-building scikit-learn with pip
+Building scikit-learn with pip
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
-this is usually the fastest way to install or upgrade to the latest stable
+This is usually the fastest way to install or upgrade to the latest stable
 release::
 
-    pip install --user --install-option="--prefix=" -u scikit-learn
+    pip install --user --install-option="--prefix=" -U scikit-learn
 
-the ``--user`` flag asks pip to install scikit-learn in the ``$home/.local``
-folder therefore not requiring root permission. this flag should make pip
+The ``--user`` flag asks pip to install scikit-learn in the ``$HOME/.local``
+folder therefore not requiring root permission. This flag should make pip
 ignore any old version of scikit-learn previously installed on the system while
-benefiting from system packages for numpy and scipy. those dependencies can
+benefiting from system packages for numpy and scipy. Those dependencies can
 be long and complex to build correctly from source.
 
-the ``--install-option="--prefix="`` flag is only required if python has a
+The ``--install-option="--prefix="`` flag is only required if Python has a
 ``distutils.cfg`` configuration with a predefined ``prefix=`` entry.
 
 
-from source package
+From source package
 ~~~~~~~~~~~~~~~~~~~
 
 download the source package from
@@ -144,7 +144,7 @@ download the source package from
 cd into the source directory.
 
 This packages uses distutils, which is the default way of installing
-python modules. the install command is::
+python modules. The install command is::
 
     python setup.py install
 
@@ -154,27 +154,27 @@ or alternatively (also from within the scikit-learn source folder)::
 
 .. warning::
 
-   packages installed with the ``python setup.py install`` command cannot
-   be uninstalled nor upgraded by ``pip`` later. to properly uninstall
+   Packages installed with the ``python setup.py install`` command cannot
+   be uninstalled nor upgraded by ``pip`` later. To properly uninstall
    scikit-learn in that case it is necessary to delete the ``sklearn`` folder
-   from your python ``site-packages`` directory.
+   from your Python ``site-packages`` directory.
 
 
-windows
+Windows
 -------
 
-first, you need to install `numpy <http://www.numpy.org/>`_ and `scipy
+First, you need to install `numpy <http://www.numpy.org/>`_ and `scipy
 <http://www.scipy.org/>`_ from their own official installers.
 
-wheel packages (.whl files) for scikit-learn from `pypi
+Wheel packages (.whl files) for scikit-learn from `pypi
 <https://pypi.python.org/pypi/scikit-learn/>`_ can be installed with the `pip
 <https://pip.readthedocs.io/en/stable/installing/>`_ utility.
-open a console and type the following to install or upgrade scikit-learn to the
+Open a console and type the following to install or upgrade scikit-learn to the
 latest stable release::
 
-    pip install -u scikit-learn
+    pip install -U scikit-learn
 
-if there are no binary packages matching your python, version you might
+If there are no binary packages matching your python, version you might
 to try to install scikit-learn and its dependencies from `christoph gohlke
 unofficial windows installers
 <http://www.lfd.uci.edu/~gohlke/pythonlibs/#scikit-learn>`_
@@ -186,23 +186,23 @@ or from a :ref:`python distribution <install_by_distribution>` instead.
 Third party distributions of scikit-learn
 =========================================
 
-some third-party distributions are now providing versions of
+Some third-party distributions are now providing versions of
 scikit-learn integrated with their package-management systems.
 
-these can make installation and upgrading much easier for users since
+These can make installation and upgrading much easier for users since
 the integration includes the ability to automatically install
 dependencies (numpy, scipy) that scikit-learn requires.
 
-the following is an incomplete list of python and os distributions
+The following is an incomplete list of python and os distributions
 that provide their own version of scikit-learn.
 
 
-macports for mac osx
+MacPorts for Mac OSX
 --------------------
 
-the macports package is named ``py<xy>-scikits-learn``,
-where ``xy`` denotes the python version.
-it can be installed by typing the following
+The MacPorts package is named ``py<XY>-scikits-learn``,
+where ``XY`` denotes the Python version.
+It can be installed by typing the following
 command::
 
     sudo port install py26-scikit-learn
@@ -212,39 +212,39 @@ or::
     sudo port install py27-scikit-learn
 
 
-arch linux
+Arch Linux
 ----------
 
-arch linux's package is provided through the `official repositories
+Arch Linux's package is provided through the `official repositories
 <https://www.archlinux.org/packages/?q=scikit-learn>`_ as
-``python-scikit-learn`` for python 3 and ``python2-scikit-learn`` for python 2.
-it can be installed by typing the following command:
+``python-scikit-learn`` for Python 3 and ``python2-scikit-learn`` for Python 2.
+It can be installed by typing the following command:
 
 .. code-block:: none
 
-     # pacman -s python-scikit-learn
+     # pacman -S python-scikit-learn
 
 or:
 
 .. code-block:: none
 
-     # pacman -s python2-scikit-learn
+     # pacman -S python2-scikit-learn
 
-depending on the version of python you use.
+depending on the version of Python you use.
 
 
-netbsd
+NetBSD
 ------
 
 scikit-learn is available via `pkgsrc-wip <http://pkgsrc-wip.sourceforge.net/>`_:
 
     http://pkgsrc.se/wip/py-scikit_learn
 
-fedora
+Fedora
 ------
 
-the fedora package is called ``python-scikit-learn`` for the python 2 version
-and ``python3-scikit-learn`` for the python 3 version. both versions can
+The Fedora package is called ``python-scikit-learn`` for the Python 2 version
+and ``python3-scikit-learn`` for the Python 3 version. Both versions can
 be installed using ``yum``::
 
     $ sudo yum install python-scikit-learn
@@ -254,103 +254,103 @@ or::
     $ sudo yum install python3-scikit-learn
 
 
-building on windows
+Building on windows
 -------------------
 
-to build scikit-learn on windows you need a working C/C++ compiler in
+To build scikit-learn on Windows you need a working C/C++ compiler in
 addition to numpy, scipy and setuptools.
 
-picking the right compiler depends on the version of python (2 or 3)
-and the architecture of the python interpreter, 32-bit or 64-bit.
-you can check the python version by running the following in ``cmd`` or
+Picking the right compiler depends on the version of Python (2 or 3)
+and the architecture of the Python interpreter, 32-bit or 64-bit.
+You can check the Python version by running the following in ``cmd`` or
 ``powershell`` console::
 
     python --version
 
 and the architecture with::
 
-    python -c "import struct; print(struct.calcsize('p') * 8)"
+    python -c "import struct; print(struct.calcsize('P') * 8)"
 
-the above commands assume that you have the python installation folder in your
-path environment variable.
+The above commands assume that you have the Python installation folder in your
+PATH environment variable.
 
 
-32-bit python
+32-bit Python
 -------------
 
-for 32-bit python it is possible use the standalone installers for
+For 32-bit python it is possible use the standalone installers for
 `microsoft visual c++ express 2008 <http://download.microsoft.com/download/A/5/4/A54BADB6-9C3F-478D-8657-93B3FC9FE62D/vcsetup.exe>`_
-for python 2 or microsoft visual c++ express 2010 for python 3.
+for Python 2 or Microsoft Visual C++ Express 2010 for Python 3.
 
-once installed you should be able to build scikit-learn without any
+Once installed you should be able to build scikit-learn without any
 particular configuration by running the following command in the scikit-learn
 folder::
 
    python setup.py install
 
 
-64-bit python
+64-bit Python
 -------------
 
-for the 64-bit architecture, you either need the full visual studio or
-the free windows sdks that can be downloaded from the links below.
+For the 64-bit architecture, you either need the full Visual Studio or
+the free Windows SDKs that can be downloaded from the links below.
 
-the windows sdks include the msvc compilers both for 32 and 64-bit
-architectures. they come as a ``grmsdkx_en_dvd.iso`` file that can be mounted
+The Windows SDKs include the MSVC compilers both for 32 and 64-bit
+architectures. They come as a ``GRMSDKX_EN_DVD.iso`` file that can be mounted
 as a new drive with a ``setup.exe`` installer in it.
 
-- for python 2 you need sdk **v7.0**: `ms windows sdk for windows 7 and .net
-  framework 3.5 sp1
+- For Python 2 you need SDK **v7.0**: `MS Windows SDK for Windows 7 and .NET
+  Framework 3.5 SP1
   <https://www.microsoft.com/en-us/download/details.aspx?id=18950>`_
 
-- for python 3 you need sdk **v7.1**: `ms windows sdk for windows 7 and .net
-  framework 4
+- For Python 3 you need SDK **v7.1**: `MS Windows SDK for Windows 7 and .NET
+  Framework 4
   <https://www.microsoft.com/en-us/download/details.aspx?id=8442>`_
 
-both sdks can be installed in parallel on the same host. to use the windows
-sdks, you need to setup the environment of a ``cmd`` console launched with the
-following flags (at least for sdk v7.0)::
+Both SDKs can be installed in parallel on the same host. To use the Windows
+SDKs, you need to setup the environment of a ``cmd`` console launched with the
+following flags (at least for SDK v7.0)::
 
-    cmd /e:on /v:on /k
+    cmd /E:ON /V:ON /K
 
-then configure the build environment with::
+Then configure the build environment with::
 
-    set distutils_use_sdk=1
-    set mssdk=1
-    "c:\program files\microsoft sdks\windows\v7.0\setup\windowssdkver.exe" -q -version:v7.0
-    "c:\program files\microsoft sdks\windows\v7.0\bin\setenv.cmd" /x64 /release
+    SET DISTUTILS_USE_SDK=1
+    SET MSSdk=1
+    "C:\Program Files\Microsoft SDKs\Windows\v7.0\Setup\WindowsSdkVer.exe" -q -version:v7.0
+    "C:\Program Files\Microsoft SDKs\Windows\v7.0\Bin\SetEnv.cmd" /x64 /release
 
-finally you can build scikit-learn in the same ``cmd`` console::
+Finally you can build scikit-learn in the same ``cmd`` console::
 
     python setup.py install
 
-replace ``v7.0`` by the ``v7.1`` in the above commands to do the same for
-python 3 instead of python 2.
+Replace ``v7.0`` by the ``v7.1`` in the above commands to do the same for
+Python 3 instead of Python 2.
 
-replace ``/x64`` by ``/x86``  to build for 32-bit python instead of 64-bit
-python.
+Replace ``/x64`` by ``/x86``  to build for 32-bit Python instead of 64-bit
+Python.
 
 
-building binary packages and installers
+Building binary packages and installers
 ---------------------------------------
 
-the ``.whl`` package and ``.exe`` installers can be built with::
+The ``.whl`` package and ``.exe`` installers can be built with::
 
     pip install wheel
     python setup.py bdist_wheel bdist_wininst -b doc/logos/scikit-learn-logo.bmp
 
-the resulting packages are generated in the ``dist/`` folder.
+The resulting packages are generated in the ``dist/`` folder.
 
 
-using an alternative compiler
+Using an alternative compiler
 -----------------------------
 
-it is possible to use `mingw <http://www.mingw.org>`_ (a port of gcc to windows
-os) as an alternative to msvc for 32-bit python. not that extensions built with
-mingw32 can be redistributed as reusable packages as they depend on gcc runtime
+It is possible to use `MinGW <http://www.mingw.org>`_ (a port of GCC to Windows
+OS) as an alternative to MSVC for 32-bit Python. Not that extensions built with
+mingw32 can be redistributed as reusable packages as they depend on GCC runtime
 libraries typically not installed on end-users environment.
 
-to force the use of a particular compiler, pass the ``--compiler`` flag to the
+To force the use of a particular compiler, pass the ``--compiler`` flag to the
 build step::
 
     python setup.py build --compiler=my_compiler install
@@ -360,62 +360,62 @@ where ``my_compiler`` should be one of ``mingw32`` or ``msvc``.
 
 .. _install_bleeding_edge:
 
-bleeding edge
+Bleeding Edge
 =============
 
-see section :ref:`git_repo` on how to get the development version. then follow
+See section :ref:`git_repo` on how to get the development version. Then follow
 the previous instructions to build from source depending on your platform.
 You will also require Cython >=0.23 in order to build the development version.
 
 
 .. _testing:
 
-testing
+Testing
 =======
 
-testing scikit-learn once installed
+Testing scikit-learn once installed
 -----------------------------------
 
-testing requires having the `nose
-<https://nose.readthedocs.io/en/latest/>`_ library. after
+Testing requires having the `nose
+<https://somethingaboutorange.com/mrl/projects/nose/>`_ library. After
 installation, the package can be tested by executing *from outside* the
 source directory::
 
     $ nosetests -v sklearn
 
-under windows, it is recommended to use the following command (adjust the path
+Under Windows, it is recommended to use the following command (adjust the path
 to the ``python.exe`` program) as using the ``nosetests.exe`` program can badly
 interact with tests that use ``multiprocessing``::
 
-    c:\python34\python.exe -c "import nose; nose.main()" -v sklearn
+    C:\Python34\python.exe -c "import nose; nose.main()" -v sklearn
 
-this should give you a lot of output (and some warnings) but
+This should give you a lot of output (and some warnings) but
 eventually should finish with a message similar to::
 
-    ran 3246 tests in 260.618s
-    ok (skip=20)
+    Ran 3246 tests in 260.618s
+    OK (SKIP=20)
 
-otherwise, please consider posting an issue into the `bug tracker
+Otherwise, please consider posting an issue into the `bug tracker
 <https://github.com/scikit-learn/scikit-learn/issues>`_ or to the
 :ref:`mailing_lists` including the traceback of the individual failures
-and errors. please include your operation system, your version of numpy, scipy
+and errors. Please include your operating system, your version of NumPy, SciPy
 and scikit-learn, and how you installed scikit-learn.
 
 
-testing scikit-learn from within the source folder
+Testing scikit-learn from within the source folder
 --------------------------------------------------
 
-scikit-learn can also be tested without having the package
-installed. for this you must compile the sources inplace from the
+Scikit-learn can also be tested without having the package
+installed. For this you must compile the sources inplace from the
 source directory::
 
     python setup.py build_ext --inplace
 
-test can now be run using nosetests::
+Test can now be run using nosetests::
 
     nosetests -v sklearn/
 
-this is automated by the commands::
+This is automated by the commands::
 
     make in
 
@@ -424,7 +424,7 @@ and::
     make test
 
 
-you can also install a symlink named ``site-packages/scikit-learn.egg-link``
+You can also install a symlink named ``site-packages/scikit-learn.egg-link``
 to the development folder of scikit-learn with::
 
     pip install --editable .
diff --git a/doc/faq.rst b/doc/faq.rst
index 2b911cf0093b..d5d15a1ed024 100644
--- a/doc/faq.rst
+++ b/doc/faq.rst
@@ -8,7 +8,8 @@ Here we try to give some answers to questions that regularly pop up on the maili
 
 What is the project name (a lot of people get it wrong)?
 --------------------------------------------------------
-scikit-learn, but not scikit or SciKit nor sci-kit learn. Also not scikits.learn or scikits-learn, which where previously used.
+scikit-learn, but not scikit or SciKit nor sci-kit learn.
+Also not scikits.learn or scikits-learn, which were previously used.
 
 How do you pronounce the project name?
 ------------------------------------------
@@ -26,6 +27,41 @@ See :ref:`contributing`. Before wanting to add a new algorithm, which is
 usually a major and lengthy undertaking, it is recommended to start with :ref:`known
 issues <easy_issues>`.
 
+What's the best way to get help on scikit-learn usage?
+--------------------------------------------------------------
+**For general machine learning questions**, please use
+`Cross Validated <http://stats.stackexchange.com>`_ with the ``[machine-learning]`` tag.
+
+**For scikit-learn usage questions**, please use `Stack Overflow <http://stackoverflow.com/questions/tagged/scikit-learn>`_
+with the ``[scikit-learn]`` and ``[python]`` tags. You can alternatively use the `mailing list
+<https://mail.python.org/mailman/listinfo/scikit-learn>`_.
+
+Please make sure to include a minimal reproduction code snippet (ideally shorter
+than 10 lines) that highlights your problem on a toy dataset (for instance from
+``sklearn.datasets`` or randomly generated with functions of ``numpy.random`` with
+a fixed random seed). Please remove any line of code that is not necessary to
+reproduce your problem.
+
+The problem should be reproducible by simply copy-pasting your code snippet in a Python
+shell with scikit-learn installed. Do not forget to include the import statements.
+
+More guidance to write good reproduction code snippets can be found at:
+
+http://stackoverflow.com/help/mcve
+
+If your problem raises an exception that you do not understand (even after googling it),
+please make sure to include the full traceback that you obtain when running the
+reproduction script.
+
+For bug reports or feature requests, please make use of the
+`issue tracker on Github <https://github.com/scikit-learn/scikit-learn/issues>`_.
+
+There is also a `scikit-learn Gitter channel
+<https://gitter.im/scikit-learn/scikit-learn>`_ where some users and developers
+might be found. 
+
+**Please do not email any authors directly to ask for assistance, report bugs,
+or for any other issue related to scikit-learn.**
 
 How can I create a bunch object?
 ------------------------------------------------
@@ -201,7 +237,6 @@ DBSCAN with Levenshtein distances::
 Similar tricks can be used, with some care, for tree kernels, graph kernels,
 etc.
 
-
 Why do I sometime get a crash/freeze with n_jobs > 1 under OSX or Linux?
 ------------------------------------------------------------------------
 
@@ -249,9 +284,9 @@ program: Insert the following instructions in your main script::
 You can find more default on the new start methods in the `multiprocessing
 documentation <https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods>`_.
 
-
 Why is there no support for deep or reinforcement learning / Will there be support for deep or reinforcement learning in scikit-learn?
 --------------------------------------------------------------------------------------------------------------------------------------
+
 Deep learning and reinforcement learning both require a rich vocabulary to
 define an architecture, with deep learning additionally requiring
 GPUs for efficient computing. However, neither of these fit within
@@ -259,9 +294,9 @@ the design constraints of scikit-learn; as a result, deep learning
 and reinforcement learning are currently out of scope for what
 scikit-learn seeks to achieve.
 
-
 Why is my pull request not getting any attention?
 -------------------------------------------------
+
 The scikit-learn review process takes a significant amount of time, and
 contributors should not be discouraged by a lack of activity or review on
 their pull request. We care a lot about getting things right
@@ -276,3 +311,23 @@ If a review of your pull request comes slowly, it is likely because the
 reviewers are busy. We ask for your understanding and request that you
 not close your pull request or discontinue your work solely because of
 this reason.
+
+How do I set a ``random_state`` for an entire execution?
+----------------------------------------------------
+
+For testing and replicability, it is often important to have the entire execution
+controlled by a single seed for the pseudo-random number generator used in
+algorithms that have a randomized component. Scikit-learn does not use its own
+global random state; whenever a RandomState instance or an integer random seed
+is not provided as an argument, it relies on the numpy global random state,
+which can be set using :func:`numpy.random.seed`.
+For example, to set an execution's numpy global random state to 42, one could
+execute the following in his or her script::
+
+    import numpy as np
+    np.random.seed(42)
+
+However, a global random state is prone to modification by other code during
+execution. Thus, the only way to ensure replicability is to pass ``RandomState``
+instances everywhere and ensure that both estimators and cross-validation
+splitters have their ``random_state`` parameter set.
diff --git a/doc/modules/classes.rst b/doc/modules/classes.rst
index d1d8eec4dd4e..494b2146945e 100644
--- a/doc/modules/classes.rst
+++ b/doc/modules/classes.rst
@@ -164,14 +164,14 @@ Splitter Classes
    :template: class.rst
 
    model_selection.KFold
-   model_selection.LabelKFold
+   model_selection.GroupKFold
    model_selection.StratifiedKFold
-   model_selection.LeaveOneLabelOut
-   model_selection.LeavePLabelOut
+   model_selection.LeaveOneGroupOut
+   model_selection.LeavePGroupsOut
    model_selection.LeaveOneOut
    model_selection.LeavePOut
    model_selection.ShuffleSplit
-   model_selection.LabelShuffleSplit
+   model_selection.GroupShuffleSplit
    model_selection.StratifiedShuffleSplit
    model_selection.PredefinedSplit
    model_selection.TimeSeriesSplit
@@ -259,6 +259,7 @@ Loaders
    datasets.load_mlcomp
    datasets.load_sample_image
    datasets.load_sample_images
+   datasets.fetch_species_distributions
    datasets.load_svmlight_file
    datasets.load_svmlight_files
    datasets.dump_svmlight_file
diff --git a/doc/modules/clustering.rst b/doc/modules/clustering.rst
index 27a3178d58db..9f67b3885665 100644
--- a/doc/modules/clustering.rst
+++ b/doc/modules/clustering.rst
@@ -718,7 +718,7 @@ More formally, we define a core sample as being a sample in the dataset such
 that there exist ``min_samples`` other samples within a distance of
 ``eps``, which are defined as *neighbors* of the core sample. This tells
 us that the core sample is in a dense area of the vector space. A cluster
-is a set of core samples, that can be built by recursively by taking a core
+is a set of core samples that can be built by recursively taking a core
 sample, finding all of its neighbors that are core samples, finding all of
 *their* neighbors that are core samples, and so on. A cluster also has a
 set of non-core samples, which are samples that are neighbors of a core sample
diff --git a/doc/modules/ensemble.rst b/doc/modules/ensemble.rst
index 887e0400ca59..52f1f0bc0ff6 100644
--- a/doc/modules/ensemble.rst
+++ b/doc/modules/ensemble.rst
@@ -204,7 +204,7 @@ classification tasks (where ``n_features`` is the number of features
 in the data). Good results are often achieved when setting ``max_depth=None``
 in combination with ``min_samples_split=1`` (i.e., when fully developing the
 trees). Bear in mind though that these values are usually not optimal, and
-might result in models that consume a lot of ram. The best parameter values
+might result in models that consume a lot of RAM. The best parameter values
 should always be cross-validated. In addition, note that in random forests,
 bootstrap samples are used by default (``bootstrap=True``)
 while the default strategy for extra-trees is to use the whole dataset
diff --git a/doc/modules/lda_qda.rst b/doc/modules/lda_qda.rst
index 5c228aaabb29..caec0f3e3abf 100644
--- a/doc/modules/lda_qda.rst
+++ b/doc/modules/lda_qda.rst
@@ -96,7 +96,7 @@ In the case of QDA, there are no assumptions on the covariance matrices
 .. note:: **Relation with Gaussian Naive Bayes**
 
 	  If in the QDA model one assumes that the covariance matrices are diagonal,
-	  then this means that we assume the classes are conditionally independent,
+	  then the inputs are assumed to be conditionally independent in each class,
 	  and the resulting classifier is equivalent to the Gaussian Naive Bayes
 	  classifier :class:`naive_bayes.GaussianNB`.
 
diff --git a/doc/modules/model_evaluation.rst b/doc/modules/model_evaluation.rst
index a44e753fb289..a720f4c0d4d6 100644
--- a/doc/modules/model_evaluation.rst
+++ b/doc/modules/model_evaluation.rst
@@ -409,6 +409,15 @@ from the :ref:`sphx_glr_auto_examples_model_selection_plot_confusion_matrix.py`
    :scale: 75
    :align: center
 
+For binary problems, we can get counts of true negatives, false positives,
+false negatives and true positives as follows::
+
+  >>> y_true = [0, 0, 0, 1, 1, 1, 1, 1]
+  >>> y_pred = [0, 1, 0, 1, 0, 1, 0, 1]
+  >>> tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
+  >>> tn, fp, fn, tp
+  (2, 1, 2, 3)
+
 .. topic:: Example:
 
   * See :ref:`sphx_glr_auto_examples_model_selection_plot_confusion_matrix.py`
diff --git a/doc/modules/model_persistence.rst b/doc/modules/model_persistence.rst
index 29080ed65fc6..b6b93886e98d 100644
--- a/doc/modules/model_persistence.rst
+++ b/doc/modules/model_persistence.rst
@@ -56,6 +56,7 @@ with::
    filesystem. All files are required in the same folder when reloading the
    model with joblib.load.
 
+.. _persistence_limitations:
 
 Security & maintainability limitations
 --------------------------------------
diff --git a/doc/modules/neural_networks_supervised.rst b/doc/modules/neural_networks_supervised.rst
index 036ea71cce76..f94ecd72d4a4 100644
--- a/doc/modules/neural_networks_supervised.rst
+++ b/doc/modules/neural_networks_supervised.rst
@@ -86,14 +86,16 @@ training samples::
     >>> from sklearn.neural_network import MLPClassifier
     >>> X = [[0., 0.], [1., 1.]]
     >>> y = [0, 1]
-    >>> clf = MLPClassifier(algorithm='l-bfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)
-    >>> clf.fit(X, y) # doctest: +NORMALIZE_WHITESPACE
-    MLPClassifier(activation='relu', algorithm='l-bfgs', alpha=1e-05,
-           batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,
+    >>> clf = MLPClassifier(solver='lbgfs', alpha=1e-5,
+    ...                     hidden_layer_sizes=(5, 2), random_state=1)
+    ...
+    >>> clf.fit(X, y)                         # doctest: +NORMALIZE_WHITESPACE
+    MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',
+           beta_1=0.9, beta_2=0.999, early_stopping=False,
            epsilon=1e-08, hidden_layer_sizes=(5, 2), learning_rate='constant',
            learning_rate_init=0.001, max_iter=200, momentum=0.9,
            nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,
-           tol=0.0001, validation_fraction=0.1, verbose=False,
+           solver='lbgfs', tol=0.0001, validation_fraction=0.1, verbose=False,
            warm_start=False)
 
 After fitting (training), the model can predict labels for new samples::
@@ -107,14 +109,6 @@ contains the weight matrices that constitute the model parameters::
     >>> [coef.shape for coef in clf.coefs_]
     [(2, 5), (5, 2), (2, 1)]
 
-To get the raw values before applying the output activation function, run the
-following command,
-
-use :meth:`MLPClassifier.decision_function`::
-
-    >>> clf.decision_function([[2., 2.], [1., 2.]])  # doctest: +ELLIPSIS
-    array([ 47.6...,  47.6...])
-
 Currently, :class:`MLPClassifier` supports only the
 Cross-Entropy loss function, which allows probability estimates by running the
 ``predict_proba`` method.
@@ -125,30 +119,31 @@ classification, it minimizes the Cross-Entropy loss function, giving a vector
 of probability estimates :math:`P(y|x)` per sample :math:`x`::
 
     >>> clf.predict_proba([[2., 2.], [1., 2.]])  # doctest: +ELLIPSIS
-    array([[ 0.,  1.],
-           [ 0.,  1.]])
+    array([[  1.967...e-04,   9.998...-01],
+           [  1.967...e-04,   9.998...-01]])
 
 :class:`MLPClassifier` supports multi-class classification by
 applying `Softmax <https://en.wikipedia.org/wiki/Softmax_activation_function>`_
 as the output function.
 
-Further, the algorithm supports :ref:`multi-label classification <multiclass>`
-in which a sample can belong to more than one class. For each class, the output
-of :meth:`MLPClassifier.decision_function` passes through the
-logistic function. Values larger or equal to `0.5` are rounded to `1`,
-otherwise to `0`. For a predicted output of a sample, the indices where the
-value is `1` represents the assigned classes of that sample::
+Further, the model supports :ref:`multi-label classification <multiclass>`
+in which a sample can belong to more than one class. For each class, the raw
+output passes through the logistic function. Values larger or equal to `0.5`
+are rounded to `1`, otherwise to `0`. For a predicted output of a sample, the
+indices where the value is `1` represents the assigned classes of that sample::
 
     >>> X = [[0., 0.], [1., 1.]]
     >>> y = [[0, 1], [1, 1]]
-    >>> clf = MLPClassifier(algorithm='l-bfgs', alpha=1e-5, hidden_layer_sizes=(15,), random_state=1)
-    >>> clf.fit(X, y)
-    MLPClassifier(activation='relu', algorithm='l-bfgs', alpha=1e-05,
-           batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,
+    >>> clf = MLPClassifier(solver='lbgfs', alpha=1e-5,
+    ...                     hidden_layer_sizes=(15,), random_state=1)
+    ...
+    >>> clf.fit(X, y)                         # doctest: +NORMALIZE_WHITESPACE
+    MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',
+           beta_1=0.9, beta_2=0.999, early_stopping=False,
            epsilon=1e-08, hidden_layer_sizes=(15,), learning_rate='constant',
            learning_rate_init=0.001, max_iter=200, momentum=0.9,
            nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,
-           tol=0.0001, validation_fraction=0.1, verbose=False,
+           solver='lbgfs', tol=0.0001, validation_fraction=0.1, verbose=False,
            warm_start=False)
     >>> clf.predict([1., 2.])
     array([[1, 1]])
@@ -216,19 +211,19 @@ for the network.
 More details can be found in the documentation of
 `SGD <http://scikit-learn.org/stable/modules/sgd.html>`_
 
-Adam is similar to SGD in a sense that it is a stochastic optimization
-algorithm, but it can automatically adjust the amount to update parameters
-based on adaptive estimates of lower-order moments.
+Adam is similar to SGD in a sense that it is a stochastic optimizer, but it can
+automatically adjust the amount to update parameters based on adaptive estimates
+of lower-order moments.
 
 With SGD or Adam, training supports online and mini-batch learning.
 
-L-BFGS is a fast learning algorithm that approximates the Hessian matrix which
-represents the second-order partial derivative of a function. Further it
-approximates the inverse of the Hessian matrix to perform parameter updates.
-The implementation uses the Scipy version of
-`L-BFGS <http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html>`__..
+L-BFGS is a solver that approximates the Hessian matrix which represents the
+second-order partial derivative of a function. Further it approximates the
+inverse of the Hessian matrix to perform parameter updates. The implementation
+uses the Scipy version of `L-BFGS
+<http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html>`_.
 
-If the selected algorithm is 'L-BFGS', training does not support online nor
+If the selected solver is 'L-BFGS', training does not support online nor
 mini-batch learning.
 
 
diff --git a/doc/modules/preprocessing.rst b/doc/modules/preprocessing.rst
index 22e4a930bb25..807bc9decfff 100644
--- a/doc/modules/preprocessing.rst
+++ b/doc/modules/preprocessing.rst
@@ -521,6 +521,8 @@ Note that polynomial features are used implicitily in `kernel methods <https://e
 
 See :ref:`sphx_glr_auto_examples_linear_model_plot_polynomial_interpolation.py` for Ridge regression using created polynomial features.
 
+.. _function_transformer:
+
 Custom transformers
 ===================
 
diff --git a/doc/sphinxext/sphinx_gallery/__init__.py b/doc/sphinxext/sphinx_gallery/__init__.py
index c5ff4151ee6f..94a22d34ce56 100644
--- a/doc/sphinxext/sphinx_gallery/__init__.py
+++ b/doc/sphinxext/sphinx_gallery/__init__.py
@@ -5,7 +5,7 @@
 
 """
 import os
-__version__ = '0.1.2'
+__version__ = '0.1.4'
 
 
 def glr_path_static():
diff --git a/doc/sphinxext/sphinx_gallery/_static/gallery.css b/doc/sphinxext/sphinx_gallery/_static/gallery.css
index dd599b402d0d..ac10f8bd69eb 100644
--- a/doc/sphinxext/sphinx_gallery/_static/gallery.css
+++ b/doc/sphinxext/sphinx_gallery/_static/gallery.css
@@ -1,8 +1,8 @@
 /*
-Sphinx-Gallery is has compatible CSS to fix default sphinx themes
+Sphinx-Gallery has compatible CSS to fix default sphinx themes
 Tested for Sphinx 1.3.1 for all themes: default, alabaster, sphinxdoc,
 scrolls, agogo, traditional, nature, haiku, pyramid
-Tested for Read the docs theme 0.1.7 */
+Tested for Read the Docs theme 0.1.7 */
 .sphx-glr-thumbcontainer {
   background: #fff;
   border: solid #fff 1px;
@@ -103,16 +103,33 @@ thumbnail with its default link Background color */
 blockquote.sphx-glr-script-out {
   margin-left: 0pt;
 }
-.sphx-glr-download {
+div.sphx-glr-download {
+  display: inline-block;
+  margin: 1em auto 1ex 2ex;
+}
+
+div.sphx-glr-download a {
   background-color: #ffc;
-  border: 1px solid #c2c22d;
+  background-image: linear-gradient(to bottom, #FFC, #d5d57e);
   border-radius: 4px;
-  margin: 1em auto 1ex auto;
+  border: 1px solid #c2c22d;
+  color: #000;
+  display: inline-block;
+  font-weight: bold;
   max-width: 45ex;
   padding: 1ex;
+  text-align: center;
+}
+
+div.sphx-glr-download code.download {
+  display: inline-block;
 }
-.sphx-glr-download a {
-  color: #4b4600;
+
+div.sphx-glr-download a:hover {
+  box-shadow: inset 0 1px 0 rgba(255,255,255,.1), 0 1px 5px rgba(0,0,0,.25);
+  text-decoration: none;
+  background-image: none;
+  background-color: #d5d57e;
 }
 
 ul.sphx-glr-horizontal {
@@ -125,3 +142,12 @@ ul.sphx-glr-horizontal li {
 ul.sphx-glr-horizontal img {
   height: auto !important;
 }
+
+p.sphx-glr-signature a.reference.external {
+  background-color: #EBECED;
+  -moz-border-radius: 5px;
+  -webkit-border-radius: 5px;
+  border-radius: 5px;
+  padding: 3px;
+  font-size: 75%;
+}
diff --git a/doc/sphinxext/sphinx_gallery/backreferences.py b/doc/sphinxext/sphinx_gallery/backreferences.py
index 67aaca966a92..4fe579c6ad47 100644
--- a/doc/sphinxext/sphinx_gallery/backreferences.py
+++ b/doc/sphinxext/sphinx_gallery/backreferences.py
@@ -75,7 +75,7 @@ def get_short_module_name(module_name, obj_name):
         short_name = '.'.join(parts[:i])
         try:
             exec('from %s import %s' % (short_name, obj_name))
-        except ImportError:
+        except Exception:  # libraries can throw all sorts of exceptions...
             # get the last working module name
             short_name = '.'.join(parts[:(i + 1)])
             break
diff --git a/doc/sphinxext/sphinx_gallery/docs_resolv.py b/doc/sphinxext/sphinx_gallery/docs_resolv.py
index fb596fdb1f1c..0675a131d50d 100644
--- a/doc/sphinxext/sphinx_gallery/docs_resolv.py
+++ b/doc/sphinxext/sphinx_gallery/docs_resolv.py
@@ -13,18 +13,18 @@
 
 # Try Python 2 first, otherwise load from Python 3
 try:
-    from StringIO import StringIO
     import cPickle as pickle
     import urllib2 as urllib
     from urllib2 import HTTPError, URLError
 except ImportError:
-    from io import StringIO
     import pickle
     import urllib.request
     import urllib.error
     import urllib.parse
     from urllib.error import HTTPError, URLError
 
+from io import StringIO
+
 
 def _get_data(url):
     """Helper function to get data over http or from a local file"""
diff --git a/doc/sphinxext/sphinx_gallery/downloads.py b/doc/sphinxext/sphinx_gallery/downloads.py
new file mode 100644
index 000000000000..d67ad54e0d02
--- /dev/null
+++ b/doc/sphinxext/sphinx_gallery/downloads.py
@@ -0,0 +1,113 @@
+# -*- coding: utf-8 -*-
+r"""
+Utilities for downloadable items
+================================
+
+"""
+# Author: Óscar Nájera
+# License: 3-clause BSD
+
+from __future__ import absolute_import, division, print_function
+
+import os
+import zipfile
+
+CODE_DOWNLOAD = """
+\n.. container:: sphx-glr-download
+
+    :download:`Download Python source code: {0} <{0}>`\n
+
+\n.. container:: sphx-glr-download
+
+    :download:`Download Jupyter notebook: {1} <{1}>`\n"""
+
+CODE_ZIP_DOWNLOAD = """
+\n.. container:: sphx-glr-download
+
+    :download:`Download all examples in Python source code: {0} </{1}>`\n
+
+\n.. container:: sphx-glr-download
+
+    :download:`Download all examples in Jupyter notebook files: {2} </{3}>`\n"""
+
+
+def python_zip(file_list, gallery_path, extension='.py'):
+    """Stores all files in file_list into an zip file
+
+    Parameters
+    ----------
+    file_list : list of strings
+        Holds all the file names to be included in zip file
+    gallery_path : string
+        path to where the zipfile is stored
+    extension : str
+        '.py' or '.ipynb' In order to deal with downloads of python
+        sources and jupyter notebooks the file extension from files in
+        file_list will be removed and replace with the value of this
+        variable while generating the zip file
+    Returns
+    -------
+    zipname : string
+        zip file name, written as `target_dir_{python,jupyter}.zip`
+        depending on the extension
+    """
+    zipname = gallery_path.replace(os.path.sep, '_')
+    zipname += '_python' if extension == '.py' else '_jupyter'
+    zipname = os.path.join(gallery_path, zipname + '.zip')
+
+    zipf = zipfile.ZipFile(zipname, mode='w')
+    for fname in file_list:
+        file_src = os.path.splitext(fname)[0] + extension
+        zipf.write(file_src)
+    zipf.close()
+
+    return zipname
+
+
+def list_downloadable_sources(target_dir):
+    """Returns a list of python source files is target_dir
+
+    Parameters
+    ----------
+    target_dir : string
+        path to the directory where python source file are
+    Returns
+    -------
+    list
+        list of paths to all Python source files in `target_dir`
+    """
+    return [os.path.join(target_dir, fname)
+            for fname in os.listdir(target_dir)
+            if fname.endswith('.py')]
+
+
+def generate_zipfiles(gallery_dir):
+    """
+    Collects all Python source files and Jupyter notebooks in
+    gallery_dir and makes zipfiles of them
+
+    Parameters
+    ----------
+    gallery_dir : string
+        path of the gallery to collect downloadable sources
+
+    Return
+    ------
+    download_rst: string
+        RestructuredText to include download buttons to the generated files
+    """
+
+    listdir = list_downloadable_sources(gallery_dir)
+    for directory in sorted(os.listdir(gallery_dir)):
+        if os.path.isdir(os.path.join(gallery_dir, directory)):
+            target_dir = os.path.join(gallery_dir, directory)
+            listdir.extend(list_downloadable_sources(target_dir))
+
+    py_zipfile = python_zip(listdir, gallery_dir)
+    jy_zipfile = python_zip(listdir, gallery_dir, ".ipynb")
+
+    dw_rst = CODE_ZIP_DOWNLOAD.format(os.path.basename(py_zipfile),
+                                      py_zipfile,
+                                      os.path.basename(jy_zipfile),
+                                      jy_zipfile)
+    return dw_rst
diff --git a/doc/sphinxext/sphinx_gallery/gen_gallery.py b/doc/sphinxext/sphinx_gallery/gen_gallery.py
index 5baf6f6301c2..352a71879747 100644
--- a/doc/sphinxext/sphinx_gallery/gen_gallery.py
+++ b/doc/sphinxext/sphinx_gallery/gen_gallery.py
@@ -12,11 +12,28 @@
 
 
 from __future__ import division, print_function, absolute_import
+import copy
 import re
 import os
 from . import glr_path_static
-from .gen_rst import generate_dir_rst
+from .gen_rst import generate_dir_rst, SPHX_GLR_SIG
 from .docs_resolv import embed_code_links
+from .downloads import generate_zipfiles
+
+DEFAULT_GALLERY_CONF = {
+    'filename_pattern': re.escape(os.sep) + 'plot',
+    'examples_dirs': os.path.join('..', 'examples'),
+    'gallery_dirs': 'auto_examples',
+    'mod_example_dir': os.path.join('modules', 'generated'),
+    'doc_module': (),
+    'reference_url': {},
+    # build options
+    'plot_gallery': True,
+    'download_all_examples': True,
+    'abort_on_example_error': False,
+    'failing_examples': {},
+    'expected_failing_examples': set(),
+}
 
 
 def clean_gallery_out(build_dir):
@@ -55,9 +72,11 @@ def generate_gallery_rst(app):
     except TypeError:
         plot_gallery = bool(app.builder.config.plot_gallery)
 
+    gallery_conf = copy.deepcopy(DEFAULT_GALLERY_CONF)
     gallery_conf.update(app.config.sphinx_gallery_conf)
     gallery_conf.update(plot_gallery=plot_gallery)
-    gallery_conf.update(abort_on_example_error=app.builder.config.abort_on_example_error)
+    gallery_conf.update(
+        abort_on_example_error=app.builder.config.abort_on_example_error)
 
     # this assures I can call the config in other places
     app.config.sphinx_gallery_conf = gallery_conf
@@ -77,6 +96,11 @@ def generate_gallery_rst(app):
                                        app.builder.srcdir)
     seen_backrefs = set()
 
+    computation_times = []
+
+    # cd to the appropriate directory regardless of sphinx configuration
+    working_dir = os.getcwd()
+    os.chdir(app.builder.srcdir)
     for examples_dir, gallery_dir in zip(examples_dirs, gallery_dirs):
         examples_dir = os.path.relpath(examples_dir,
                                        app.builder.srcdir)
@@ -86,22 +110,44 @@ def generate_gallery_rst(app):
         for workdir in [examples_dir, gallery_dir, mod_examples_dir]:
             if not os.path.exists(workdir):
                 os.makedirs(workdir)
-
         # we create an index.rst with all examples
         fhindex = open(os.path.join(gallery_dir, 'index.rst'), 'w')
         # Here we don't use an os.walk, but we recurse only twice: flat is
         # better than nested.
-        fhindex.write(generate_dir_rst(examples_dir, gallery_dir, gallery_conf,
-                                       seen_backrefs))
+        this_fhindex, this_computation_times = \
+            generate_dir_rst(examples_dir, gallery_dir, gallery_conf,
+                             seen_backrefs)
+
+        computation_times += this_computation_times
+
+        fhindex.write(this_fhindex)
         for directory in sorted(os.listdir(examples_dir)):
             if os.path.isdir(os.path.join(examples_dir, directory)):
                 src_dir = os.path.join(examples_dir, directory)
                 target_dir = os.path.join(gallery_dir, directory)
-                fhindex.write(generate_dir_rst(src_dir, target_dir,
-                                               gallery_conf,
-                                               seen_backrefs))
+                this_fhindex, this_computation_times = \
+                    generate_dir_rst(src_dir, target_dir, gallery_conf,
+                                     seen_backrefs)
+                fhindex.write(this_fhindex)
+                computation_times += this_computation_times
+
+        if gallery_conf['download_all_examples']:
+            download_fhindex = generate_zipfiles(gallery_dir)
+            fhindex.write(download_fhindex)
+
+        fhindex.write(SPHX_GLR_SIG)
         fhindex.flush()
 
+    # Back to initial directory
+    os.chdir(working_dir)
+
+    print("Computation time summary:")
+    for time_elapsed, fname in sorted(computation_times)[::-1]:
+        if time_elapsed is not None:
+            print("\t- %s : %.2g sec" % (fname, time_elapsed))
+        else:
+            print("\t- %s : not run" % fname)
+
 
 def touch_empty_backreferences(app, what, name, obj, options, lines):
     """Generate empty back-reference example files
@@ -110,7 +156,8 @@ def touch_empty_backreferences(app, what, name, obj, options, lines):
     examples for a class / module that is being parsed by autodoc"""
 
     examples_path = os.path.join(app.srcdir,
-                                 app.config.sphinx_gallery_conf["mod_example_dir"],
+                                 app.config.sphinx_gallery_conf[
+                                     "mod_example_dir"],
                                  "%s.examples" % name)
 
     if not os.path.exists(examples_path):
@@ -118,21 +165,68 @@ def touch_empty_backreferences(app, what, name, obj, options, lines):
         open(examples_path, 'w').close()
 
 
-gallery_conf = {
-    'filename_pattern': re.escape(os.sep) + 'plot',
-    'examples_dirs': '../examples',
-    'gallery_dirs': 'auto_examples',
-    'mod_example_dir': os.path.join('modules', 'generated'),
-    'doc_module': (),
-    'reference_url': {},
-}
+def sumarize_failing_examples(app, exception):
+    """Collects the list of falling examples during build and prints them with the traceback
+
+    Raises ValueError if there where failing examples
+    """
+    if exception is not None:
+        return
+
+    # Under no-plot Examples are not run so nothing to summarize
+    if not app.config.sphinx_gallery_conf['plot_gallery']:
+        return
+
+    gallery_conf = app.config.sphinx_gallery_conf
+    failing_examples = set(gallery_conf['failing_examples'])
+    expected_failing_examples = set(gallery_conf['expected_failing_examples'])
+
+    examples_expected_to_fail = failing_examples.intersection(
+        expected_failing_examples)
+    expected_fail_msg = []
+    if examples_expected_to_fail:
+        expected_fail_msg.append("Examples failing as expected:")
+        for fail_example in examples_expected_to_fail:
+            expected_fail_msg.append(fail_example + ' failed leaving traceback:\n' +
+                                     gallery_conf['failing_examples'][fail_example] + '\n')
+        print("\n".join(expected_fail_msg))
+
+    examples_not_expected_to_fail = failing_examples.difference(
+        expected_failing_examples)
+    fail_msgs = []
+    if examples_not_expected_to_fail:
+        fail_msgs.append("Unexpected failing examples:")
+        for fail_example in examples_not_expected_to_fail:
+            fail_msgs.append(fail_example + ' failed leaving traceback:\n' +
+                             gallery_conf['failing_examples'][fail_example] + '\n')
+
+    examples_not_expected_to_pass = expected_failing_examples.difference(
+        failing_examples)
+    if examples_not_expected_to_pass:
+        fail_msgs.append("Examples expected to fail, but not failling:\n" +
+                         "Please remove this examples from\n" +
+                         "sphinx_gallery_conf['expected_failing_examples']\n" +
+                         "in your conf.py file"
+                         "\n".join(examples_not_expected_to_pass))
+
+    if fail_msgs:
+        raise ValueError("Here is a summary of the problems encountered when "
+                         "running the examples\n\n" + "\n".join(fail_msgs) +
+                         "\n" + "-" * 79)
+
+
+def get_default_config_value(key):
+    def default_getter(conf):
+        return conf['sphinx_gallery_conf'].get(key, DEFAULT_GALLERY_CONF[key])
+    return default_getter
 
 
 def setup(app):
     """Setup sphinx-gallery sphinx extension"""
-    app.add_config_value('plot_gallery', True, 'html')
-    app.add_config_value('abort_on_example_error', False, 'html')
-    app.add_config_value('sphinx_gallery_conf', gallery_conf, 'html')
+    app.add_config_value('sphinx_gallery_conf', DEFAULT_GALLERY_CONF, 'html')
+    for key in ['plot_gallery', 'abort_on_example_error']:
+        app.add_config_value(key, get_default_config_value(key), 'html')
+
     app.add_stylesheet('gallery.css')
 
     if 'sphinx.ext.autodoc' in app._extensions:
@@ -140,6 +234,7 @@ def setup(app):
 
     app.connect('builder-inited', generate_gallery_rst)
 
+    app.connect('build-finished', sumarize_failing_examples)
     app.connect('build-finished', embed_code_links)
 
 
diff --git a/doc/sphinxext/sphinx_gallery/gen_rst.py b/doc/sphinxext/sphinx_gallery/gen_rst.py
index cb02c5509494..329f4623b9d9 100644
--- a/doc/sphinxext/sphinx_gallery/gen_rst.py
+++ b/doc/sphinxext/sphinx_gallery/gen_rst.py
@@ -12,9 +12,12 @@
 Files that generate images should start with 'plot'
 
 """
+# Don't use unicode_literals here (be explicit with u"..." instead) otherwise
+# tricky errors come up with exec(code_blocks, ...) calls
 from __future__ import division, print_function, absolute_import
 from time import time
 import ast
+import codecs
 import hashlib
 import os
 import re
@@ -24,6 +27,8 @@
 import traceback
 import warnings
 
+from .downloads import CODE_DOWNLOAD
+
 
 # Try Python 2 first, otherwise load from Python 3
 from textwrap import dedent
@@ -48,10 +53,7 @@ def prefixed_lines():
                 yield (prefix + line if predicate(line) else line)
         return ''.join(prefixed_lines())
 
-try:
-    from StringIO import StringIO
-except ImportError:
-    from io import StringIO
+from io import StringIO
 
 try:
     # make sure that the Agg backend is set before importing any
@@ -72,6 +74,7 @@ def prefixed_lines():
     basestring
 except NameError:
     basestring = str
+    unicode = str
 
 
 ###############################################################################
@@ -92,17 +95,21 @@ def flush(self):
         self.file1.flush()
         self.file2.flush()
 
+    # When called from a local terminal seaborn needs it in Python3
+    def isatty(self):
+        self.file1.isatty()
 
-###############################################################################
-CODE_DOWNLOAD = """**Total running time of the script:**
-({0:.0f} minutes {1:.3f} seconds)\n\n
-\n.. container:: sphx-glr-download
 
-    **Download Python source code:** :download:`{2} <{2}>`\n
-\n.. container:: sphx-glr-download
+class MixedEncodingStringIO(StringIO):
+    """Helper when both ASCII and unicode strings will be written"""
+
+    def write(self, data):
+        if not isinstance(data, unicode):
+            data = data.decode('utf-8')
+        StringIO.write(self, data)
 
-    **Download IPython notebook:** :download:`{3} <{3}>`\n"""
 
+###############################################################################
 # The following strings are used when we have several pictures: we use
 # an html div tag that our CSS uses to turn the lists into horizontal
 # lists.
@@ -124,11 +131,17 @@ def flush(self):
 """
 
 
-CODE_OUTPUT = """.. rst-class:: sphx-glr-script-out
+# This one could contain unicode
+CODE_OUTPUT = u""".. rst-class:: sphx-glr-script-out
 
  Out::
 
-  {0}\n"""
+{0}\n"""
+
+
+SPHX_GLR_SIG = """\n.. rst-class:: sphx-glr-signature
+
+    `Generated by Sphinx-Gallery <http://sphinx-gallery.readthedocs.io>`_\n"""
 
 
 def get_docstring_and_rest(filename):
@@ -143,7 +156,10 @@ def get_docstring_and_rest(filename):
     rest: str
         `filename` content without the docstring
     """
-    with open(filename) as f:
+    # can't use codecs.open(filename, 'r', 'utf-8') here b/c ast doesn't
+    # seem to work with unicode strings in Python2.7
+    # "SyntaxError: encoding declaration in Unicode string"
+    with open(filename, 'rb') as f:
         content = f.read()
 
     node = ast.parse(content)
@@ -154,9 +170,11 @@ def get_docstring_and_rest(filename):
        isinstance(node.body[0].value, ast.Str):
         docstring_node = node.body[0]
         docstring = docstring_node.value.s
+        if hasattr(docstring, 'decode'):  # python2.7
+            docstring = docstring.decode('utf-8')
         # This get the content of the file after the docstring last line
         # Note: 'maxsplit' argument is not a keyword argument in python2
-        rest = content.split('\n', docstring_node.lineno)[-1]
+        rest = content.decode('utf-8').split('\n', docstring_node.lineno)[-1]
         return docstring, rest
     else:
         raise ValueError(('Could not find docstring in file "{0}". '
@@ -174,7 +192,6 @@ def split_code_and_text_blocks(source_file):
         and content string of block.
     """
     docstring, rest_of_content = get_docstring_and_rest(source_file)
-
     blocks = [('text', docstring)]
 
     pattern = re.compile(
@@ -187,7 +204,7 @@ def split_code_and_text_blocks(source_file):
         code_block_content = rest_of_content[pos_so_far:match_start_pos]
         text_content = match.group('text_content')
         sub_pat = re.compile('^#', flags=re.M)
-        text_block_content = dedent(re.sub(sub_pat, '', text_content))
+        text_block_content = dedent(re.sub(sub_pat, '', text_content)).lstrip()
         if code_block_content.strip():
             blocks.append(('code', code_block_content))
         if text_block_content.strip():
@@ -213,7 +230,24 @@ def text2string(content):
     try:
         return ast.literal_eval(content) + '\n'
     except Exception:
-        return content
+        return content + '\n'
+
+
+def extract_thumbnail_number(text):
+    """ Pull out the thumbnail image number specified in the docstring. """
+
+    # check whether the user has specified a specific thumbnail image
+    pattr = re.compile(
+        r"^\s*#\s*sphinx_gallery_thumbnail_number\s*=\s*([0-9]+)\s*$", flags=re.MULTILINE)
+    match = pattr.search(text)
+
+    if match is None:
+        # by default, use the first figure created
+        thumbnail_number = 1
+    else:
+        thumbnail_number = int(match.groups()[0])
+
+    return thumbnail_number
 
 
 def extract_intro(filename):
@@ -250,35 +284,19 @@ def get_md5sum(src_file):
     return src_md5
 
 
-def check_md5sum_change(src_file):
-    """Returns True if src_file has a different md5sum"""
+def md5sum_is_current(src_file):
+    """Returns True if src_file has the same md5 hash as the one stored on disk"""
 
     src_md5 = get_md5sum(src_file)
 
     src_md5_file = src_file + '.md5'
-    src_file_changed = True
     if os.path.exists(src_md5_file):
         with open(src_md5_file, 'r') as file_checksum:
             ref_md5 = file_checksum.read()
-        if src_md5 == ref_md5:
-            src_file_changed = False
-
-    if src_file_changed:
-        with open(src_md5_file, 'w') as file_checksum:
-            file_checksum.write(src_md5)
-
-    return src_file_changed
 
+        return src_md5 == ref_md5
 
-def _plots_are_current(src_file, image_file):
-    """Test existence of image file and no change in md5sum of
-    example"""
-
-    first_image_file = image_file.format(1)
-    has_image = os.path.exists(first_image_file)
-    src_file_changed = check_md5sum_change(src_file)
-
-    return has_image and not src_file_changed
+    return False
 
 
 def save_figures(image_path, fig_count, gallery_conf):
@@ -290,10 +308,15 @@ def save_figures(image_path, fig_count, gallery_conf):
         Path where plots are saved (format string which accepts figure number)
     fig_count : int
         Previous figure number count. Figure number add from this number
+    gallery_conf : dict
+        Contains the configuration of Sphinx-Gallery
 
     Returns
     -------
-    list of strings containing the full path to each figure
+    figure_list : list of str
+        strings containing the full path to each figure
+    images_rst : str
+        rst code to embed the images in the document
     """
     figure_list = []
 
@@ -317,9 +340,9 @@ def save_figures(image_path, fig_count, gallery_conf):
     if gallery_conf.get('find_mayavi_figures', False):
         from mayavi import mlab
         e = mlab.get_engine()
-        last_matplotlib_fig_num = len(figure_list)
+        last_matplotlib_fig_num = fig_count + len(figure_list)
         total_fig_num = last_matplotlib_fig_num + len(e.scenes)
-        mayavi_fig_nums = range(last_matplotlib_fig_num, total_fig_num)
+        mayavi_fig_nums = range(last_matplotlib_fig_num + 1, total_fig_num + 1)
 
         for scene, mayavi_fig_num in zip(e.scenes, mayavi_fig_nums):
             current_fig = image_path.format(mayavi_fig_num)
@@ -329,7 +352,18 @@ def save_figures(image_path, fig_count, gallery_conf):
             figure_list.append(current_fig)
         mlab.close(all=True)
 
-    return figure_list
+    # Depending on whether we have one or more figures, we're using a
+    # horizontal list or a single rst call to 'image'.
+    images_rst = ""
+    if len(figure_list) == 1:
+        figure_name = figure_list[0]
+        images_rst = SINGLE_IMAGE % figure_name.lstrip('/')
+    elif len(figure_list) > 1:
+        images_rst = HLIST_HEADER
+        for figure_name in figure_list:
+            images_rst += HLIST_IMAGE_TEMPLATE % figure_name.lstrip('/')
+
+    return figure_list, images_rst
 
 
 def scale_image(in_fname, out_fname, max_width, max_height):
@@ -377,18 +411,28 @@ def scale_image(in_fname, out_fname, max_width, max_height):
                           generated images')
 
 
-def save_thumbnail(image_path, base_image_name, gallery_conf):
+def save_thumbnail(image_path_template, src_file, gallery_conf):
     """Save the thumbnail image"""
-    first_image_file = image_path.format(1)
-    thumb_dir = os.path.join(os.path.dirname(first_image_file), 'thumb')
+    # read specification of the figure to display as thumbnail from main text
+    _, content = get_docstring_and_rest(src_file)
+    thumbnail_number = extract_thumbnail_number(content)
+    thumbnail_image_path = image_path_template.format(thumbnail_number)
+
+    thumb_dir = os.path.join(os.path.dirname(thumbnail_image_path), 'thumb')
     if not os.path.exists(thumb_dir):
         os.makedirs(thumb_dir)
 
+    base_image_name = os.path.splitext(os.path.basename(src_file))[0]
     thumb_file = os.path.join(thumb_dir,
                               'sphx_glr_%s_thumb.png' % base_image_name)
 
-    if os.path.exists(first_image_file):
-        scale_image(first_image_file, thumb_file, 400, 280)
+    if src_file in gallery_conf['failing_examples']:
+        broken_img = os.path.join(glr_path_static(), 'broken_example.png')
+        scale_image(broken_img, thumb_file, 200, 140)
+
+    elif os.path.exists(thumbnail_image_path):
+        scale_image(thumbnail_image_path, thumb_file, 400, 280)
+
     elif not os.path.exists(thumb_file):
         # create something to replace the thumbnail
         default_thumb_file = os.path.join(glr_path_static(), 'no_image.png')
@@ -405,17 +449,20 @@ def generate_dir_rst(src_dir, target_dir, gallery_conf, seen_backrefs):
               src_dir)
         print('Skipping this directory')
         print(80 * '_')
-        return ""  # because string is an expected return type
+        return "", []  # because string is an expected return type
 
     fhindex = open(os.path.join(src_dir, 'README.txt')).read()
+
     if not os.path.exists(target_dir):
         os.makedirs(target_dir)
     sorted_listdir = [fname for fname in sorted(os.listdir(src_dir))
                       if fname.endswith('.py')]
     entries_text = []
+    computation_times = []
     for fname in sorted_listdir:
-        amount_of_code = generate_file_rst(fname, target_dir, src_dir,
-                                           gallery_conf)
+        amount_of_code, time_elapsed = \
+            generate_file_rst(fname, target_dir, src_dir, gallery_conf)
+        computation_times.append((time_elapsed, fname))
         new_fname = os.path.join(src_dir, fname)
         intro = extract_intro(new_fname)
         write_backreferences(seen_backrefs, gallery_conf,
@@ -438,95 +485,119 @@ def generate_dir_rst(src_dir, target_dir, gallery_conf, seen_backrefs):
     fhindex += """.. raw:: html\n
     <div style='clear:both'></div>\n\n"""
 
-    return fhindex
+    return fhindex, computation_times
 
 
-def execute_script(code_block, example_globals, image_path, fig_count,
-                   src_file, gallery_conf):
+def execute_code_block(code_block, example_globals,
+                       block_vars, gallery_conf):
     """Executes the code block of the example file"""
     time_elapsed = 0
     stdout = ''
 
-    # We need to execute the code
-    print('plotting code blocks in %s' % src_file)
+    # If example is not suitable to run, skip executing its blocks
+    if not block_vars['execute_script']:
+        return stdout, time_elapsed
 
     plt.close('all')
     cwd = os.getcwd()
     # Redirect output to stdout and
     orig_stdout = sys.stdout
+    src_file = block_vars['src_file']
 
     try:
         # First cd in the original example dir, so that any file
         # created by the example get created in this directory
         os.chdir(os.path.dirname(src_file))
-        my_buffer = StringIO()
+        my_buffer = MixedEncodingStringIO()
         my_stdout = Tee(sys.stdout, my_buffer)
         sys.stdout = my_stdout
 
         t_start = time()
+        # don't use unicode_literals at the top of this file or you get
+        # nasty errors here on Py2.7
         exec(code_block, example_globals)
         time_elapsed = time() - t_start
 
         sys.stdout = orig_stdout
 
         my_stdout = my_buffer.getvalue().strip().expandtabs()
+        # raise RuntimeError
         if my_stdout:
-            stdout = CODE_OUTPUT.format(indent(my_stdout, ' ' * 4))
+            stdout = CODE_OUTPUT.format(indent(my_stdout, u' ' * 4))
         os.chdir(cwd)
-        figure_list = save_figures(image_path, fig_count, gallery_conf)
-
-        # Depending on whether we have one or more figures, we're using a
-        # horizontal list or a single rst call to 'image'.
-        image_list = ""
-        if len(figure_list) == 1:
-            figure_name = figure_list[0]
-            image_list = SINGLE_IMAGE % figure_name.lstrip('/')
-        elif len(figure_list) > 1:
-            image_list = HLIST_HEADER
-            for figure_name in figure_list:
-                image_list += HLIST_IMAGE_TEMPLATE % figure_name.lstrip('/')
+        fig_list, images_rst = save_figures(
+            block_vars['image_path'], block_vars['fig_count'], gallery_conf)
+        fig_num = len(fig_list)
 
     except Exception:
         formatted_exception = traceback.format_exc()
 
-        print(80 * '_')
-        print('%s is not compiling:' % src_file)
-        print(formatted_exception)
-        print(80 * '_')
-
-        figure_list = []
-        image_list = codestr2rst(formatted_exception, lang='pytb')
+        fail_example_warning = 80 * '_' + '\n' + \
+            '%s failed to execute correctly:' % src_file + \
+            formatted_exception + 80 * '_' + '\n'
+        warnings.warn(fail_example_warning)
 
-        # Overrides the output thumbnail in the gallery for easy identification
-        broken_img = os.path.join(glr_path_static(), 'broken_example.png')
-        shutil.copyfile(broken_img, os.path.join(cwd, image_path.format(1)))
-        fig_count += 1  # raise count to avoid overwriting image
+        fig_num = 0
+        images_rst = codestr2rst(formatted_exception, lang='pytb')
 
         # Breaks build on first example error
-
+        # XXX This check can break during testing e.g. if you uncomment the
+        # `raise RuntimeError` by the `my_stdout` call, maybe use `.get()`?
         if gallery_conf['abort_on_example_error']:
             raise
+        # Stores failing file
+        gallery_conf['failing_examples'][src_file] = formatted_exception
+        block_vars['execute_script'] = False
 
     finally:
         os.chdir(cwd)
         sys.stdout = orig_stdout
 
-    print(" - time elapsed : %.2g sec" % time_elapsed)
-    code_output = "\n{0}\n\n{1}\n\n".format(image_list, stdout)
+    code_output = u"\n{0}\n\n{1}\n\n".format(images_rst, stdout)
+    block_vars['fig_count'] += fig_num
+
+    return code_output, time_elapsed
+
 
-    return code_output, time_elapsed, fig_count + len(figure_list)
+def clean_modules():
+    """Remove "unload" seaborn from the name space
+
+    After a script is executed it can load a variety of setting that one
+    does not want to influence in other examples in the gallery."""
+
+    # Horrible code to 'unload' seaborn, so that it resets
+    # its default when is load
+    # Python does not support unloading of modules
+    # https://bugs.python.org/issue9072
+    for module in list(sys.modules.keys()):
+        if 'seaborn' in module:
+            del sys.modules[module]
+
+    # Reset Matplotlib to default
+    plt.rcdefaults()
 
 
 def generate_file_rst(fname, target_dir, src_dir, gallery_conf):
-    """ Generate the rst file for a given example.
+    """Generate the rst file for a given example.
 
-        Returns the amout of code (in characters) of the corresponding
-        files.
+    Returns
+    -------
+    amount_of_code : int
+        character count of the corresponding python script in file
+    time_elapsed : float
+        seconds required to run the script
     """
 
     src_file = os.path.join(src_dir, fname)
     example_file = os.path.join(target_dir, fname)
     shutil.copyfile(src_file, example_file)
+    script_blocks = split_code_and_text_blocks(src_file)
+    amount_of_code = sum([len(bcontent)
+                          for blabel, bcontent in script_blocks
+                          if blabel == 'code'])
+
+    if md5sum_is_current(example_file):
+        return amount_of_code, 0
 
     image_dir = os.path.join(target_dir, 'images')
     if not os.path.exists(image_dir):
@@ -534,82 +605,80 @@ def generate_file_rst(fname, target_dir, src_dir, gallery_conf):
 
     base_image_name = os.path.splitext(fname)[0]
     image_fname = 'sphx_glr_' + base_image_name + '_{0:03}.png'
-    image_path = os.path.join(image_dir, image_fname)
-
-    script_blocks = split_code_and_text_blocks(example_file)
-
-    amount_of_code = sum([len(bcontent)
-                          for blabel, bcontent in script_blocks
-                          if blabel == 'code'])
-
-    if _plots_are_current(example_file, image_path):
-        return amount_of_code
-
-    time_elapsed = 0
+    image_path_template = os.path.join(image_dir, image_fname)
 
     ref_fname = example_file.replace(os.path.sep, '_')
     example_rst = """\n\n.. _sphx_glr_{0}:\n\n""".format(ref_fname)
     example_nb = Notebook(fname, target_dir)
 
     filename_pattern = gallery_conf.get('filename_pattern')
-    if re.search(filename_pattern, src_file) and gallery_conf['plot_gallery']:
-        example_globals = {
-            # A lot of examples contains 'print(__doc__)' for example in
-            # scikit-learn so that running the example prints some useful
-            # information. Because the docstring has been separated from
-            # the code blocks in sphinx-gallery, __doc__ is actually
-            # __builtin__.__doc__ in the execution context and we do not
-            # want to print it
-            '__doc__': '',
-            # Examples may contain if __name__ == '__main__' guards
-            # for in example scikit-learn if the example uses multiprocessing
-            '__name__': '__main__'}
-
-        fig_count = 0
-        # A simple example has two blocks: one for the
-        # example introduction/explanation and one for the code
-        is_example_notebook_like = len(script_blocks) > 2
-        for blabel, bcontent in script_blocks:
-            if blabel == 'code':
-                code_output, rtime, fig_count = execute_script(bcontent,
-                                                               example_globals,
-                                                               image_path,
-                                                               fig_count,
-                                                               src_file,
-                                                               gallery_conf)
-
-                time_elapsed += rtime
-                example_nb.add_code_cell(bcontent)
-
-                if is_example_notebook_like:
-                    example_rst += codestr2rst(bcontent) + '\n'
-                    example_rst += code_output
-                else:
-                    example_rst += code_output
-                    if 'sphx-glr-script-out' in code_output:
-                        # Add some vertical space after output
-                        example_rst += "\n\n|\n\n"
-                    example_rst += codestr2rst(bcontent) + '\n'
-
-            else:
-                example_rst += text2string(bcontent) + '\n'
-                example_nb.add_markdown_cell(text2string(bcontent))
-    else:
-        for blabel, bcontent in script_blocks:
-            if blabel == 'code':
+    execute_script = re.search(filename_pattern, src_file) and gallery_conf[
+        'plot_gallery']
+    example_globals = {
+        # A lot of examples contains 'print(__doc__)' for example in
+        # scikit-learn so that running the example prints some useful
+        # information. Because the docstring has been separated from
+        # the code blocks in sphinx-gallery, __doc__ is actually
+        # __builtin__.__doc__ in the execution context and we do not
+        # want to print it
+        '__doc__': '',
+        # Examples may contain if __name__ == '__main__' guards
+        # for in example scikit-learn if the example uses multiprocessing
+        '__name__': '__main__',
+    }
+
+    # A simple example has two blocks: one for the
+    # example introduction/explanation and one for the code
+    is_example_notebook_like = len(script_blocks) > 2
+    time_elapsed = 0
+    block_vars = {'execute_script': execute_script, 'fig_count': 0,
+                  'image_path': image_path_template, 'src_file': src_file}
+    print('Executing file %s' % src_file)
+    for blabel, bcontent in script_blocks:
+        if blabel == 'code':
+            code_output, rtime = execute_code_block(bcontent,
+                                                    example_globals,
+                                                    block_vars,
+                                                    gallery_conf)
+
+            time_elapsed += rtime
+            example_nb.add_code_cell(bcontent)
+
+            if is_example_notebook_like:
                 example_rst += codestr2rst(bcontent) + '\n'
-                example_nb.add_code_cell(bcontent)
+                example_rst += code_output
             else:
-                example_rst += bcontent + '\n'
-                example_nb.add_markdown_cell(text2string(bcontent))
+                example_rst += code_output
+                if 'sphx-glr-script-out' in code_output:
+                    # Add some vertical space after output
+                    example_rst += "\n\n|\n\n"
+                example_rst += codestr2rst(bcontent) + '\n'
+
+        else:
+            example_rst += text2string(bcontent) + '\n'
+            example_nb.add_markdown_cell(text2string(bcontent))
 
-    save_thumbnail(image_path, base_image_name, gallery_conf)
+    clean_modules()
+
+    # Writes md5 checksum if example has build correctly
+    # not failed and was initially meant to run(no-plot shall not cache md5sum)
+    if block_vars['execute_script']:
+        with open(example_file + '.md5', 'w') as file_checksum:
+            file_checksum.write(get_md5sum(example_file))
+
+    save_thumbnail(image_path_template, src_file, gallery_conf)
 
     time_m, time_s = divmod(time_elapsed, 60)
     example_nb.save_file()
-    with open(os.path.join(target_dir, base_image_name + '.rst'), 'w') as f:
-        example_rst += CODE_DOWNLOAD.format(time_m, time_s, fname,
-                                            example_nb.file_name)
+    with codecs.open(os.path.join(target_dir, base_image_name + '.rst'),
+                     mode='w', encoding='utf-8') as f:
+        example_rst += "**Total running time of the script:**" \
+                       " ({0: .0f} minutes {1: .3f} seconds)\n\n".format(
+                           time_m, time_s)
+        example_rst += CODE_DOWNLOAD.format(fname, example_nb.file_name)
+        example_rst += SPHX_GLR_SIG
         f.write(example_rst)
 
-    return amount_of_code
+    print("{0} ran in : {1:.2g} seconds\n".format(src_file, time_elapsed))
+
+    return amount_of_code, time_elapsed
diff --git a/doc/sphinxext/sphinx_gallery/notebook.py b/doc/sphinxext/sphinx_gallery/notebook.py
index c0ee5cd80b71..fc0fccfb6b98 100644
--- a/doc/sphinxext/sphinx_gallery/notebook.py
+++ b/doc/sphinxext/sphinx_gallery/notebook.py
@@ -4,18 +4,20 @@
 Parser for Jupyter notebooks
 ============================
 
-Class that holds the Ipython notebook information
+Class that holds the Jupyter notebook information
 
 """
 # Author: Óscar Nájera
 # License: 3-clause BSD
 
 from __future__ import division, absolute_import, print_function
+from functools import partial
 import json
 import os
 import re
 import sys
 
+
 def ipy_notebook_skeleton():
     """Returns a dictionary with the elements of a Jupyter notebook"""
     py_version = sys.version_info
@@ -46,25 +48,58 @@ def ipy_notebook_skeleton():
     return notebook_skeleton
 
 
+def directive_fun(match, directive):
+    """Helper to fill in directives"""
+    directive_to_alert = dict(note="info", warning="danger")
+    return ('<div class="alert alert-{0}"><h4>{1}</h4><p>{2}</p></div>'
+            .format(directive_to_alert[directive], directive.capitalize(),
+                    match.group(1).strip()))
+
+
 def rst2md(text):
     """Converts the RST text from the examples docstrigs and comments
-    into markdown text for the IPython notebooks"""
+    into markdown text for the Jupyter notebooks"""
 
     top_heading = re.compile(r'^=+$\s^([\w\s-]+)^=+$', flags=re.M)
     text = re.sub(top_heading, r'# \1', text)
 
     math_eq = re.compile(r'^\.\. math::((?:.+)?(?:\n+^  .+)*)', flags=re.M)
     text = re.sub(math_eq,
-                  lambda match: r'$${0}$$'.format(match.group(1).strip()),
+                  lambda match: r'\begin{{align}}{0}\end{{align}}'.format(
+                      match.group(1).strip()),
                   text)
-    inline_math = re.compile(r':math:`(.+)`')
+    inline_math = re.compile(r':math:`(.+?)`', re.DOTALL)
     text = re.sub(inline_math, r'$\1$', text)
 
+    directives = ('warning', 'note')
+    for directive in directives:
+        directive_re = re.compile(r'^\.\. %s::((?:.+)?(?:\n+^  .+)*)'
+                                  % directive, flags=re.M)
+        text = re.sub(directive_re,
+                      partial(directive_fun, directive=directive), text)
+
+    links = re.compile(r'^ *\.\. _.*:.*$\n', flags=re.M)
+    text = re.sub(links, '', text)
+
+    refs = re.compile(r':ref:`')
+    text = re.sub(refs, '`', text)
+
+    contents = re.compile(r'^\s*\.\. contents::.*$(\n +:\S+: *$)*\n',
+                          flags=re.M)
+    text = re.sub(contents, '', text)
+
+    images = re.compile(
+        r'^\.\. image::(.*$)(?:\n *:alt:(.*$)\n)?(?: +:\S+:.*$\n)*',
+        flags=re.M)
+    text = re.sub(
+        images, lambda match: '![{1}]({0})\n'.format(
+            match.group(1).strip(), (match.group(2) or '').strip()), text)
+
     return text
 
 
 class Notebook(object):
-    """Ipython notebook object
+    """Jupyter notebook object
 
     Constructs the file cell-by-cell and writes it at the end"""
 
diff --git a/doc/themes/scikit-learn/layout.html b/doc/themes/scikit-learn/layout.html
index 6ed1db7688c5..d6b377a30ac3 100644
--- a/doc/themes/scikit-learn/layout.html
+++ b/doc/themes/scikit-learn/layout.html
@@ -12,7 +12,6 @@
     :license: BSD
 #}
 {% extends "basic/layout.html" %}
-{% set css_files = css_files + ["_static/gallery.css"] %}
 
 {% block htmltitle %}
   {{ super() }}
diff --git a/doc/themes/scikit-learn/static/gallery.css b/doc/themes/scikit-learn/static/gallery.css
deleted file mode 100644
index 27dce45618b4..000000000000
--- a/doc/themes/scikit-learn/static/gallery.css
+++ /dev/null
@@ -1,73 +0,0 @@
-/******** For the gallery **************/
-
-/* ------- Fix maximum size of thumbnails in example gallery -------------- */
-div.thumbnailContainer img {
-    max-width: 160px;
-    max-height: 160px;
-    /*display: block;*/
-    margin: auto auto auto -8px;
-    display: inline;
-}
-
-div.thumbnailContainer div.figure {
-    float: left;
-    margin: 10px 11px 7em 11px;
-    -webkit-border-radius: 10px;
-    -moz-border-radius: 10px;
-    border-radius: 10px;
-    border: 2px solid #FFF;
-    background-color: #FFF;
-    width: 150px;
-    height: 100px;
-    -webkit-background-size: 150px 100px;
-    -moz-background-size: 150px 100px;
-}
-
-*[tooltip] {
-    position: relative;
-    float: left;
-}
-
-*[tooltip]:hover:after{
-    background: rgba(0,0,0,.8);
-    border-radius: 5px;
-    color: white;
-    content: attr(tooltip);
-    left: 95%;
-    padding: 5px 15px;
-    position: absolute;
-    z-index: 98;
-    width: 220px;
-    bottom: 52%
-}
-
-
-*[tooltip]:hover:before {
-    content: "";
-    position: absolute;
-    z-index: 99;
-    border: solid;
-    border-color: #333 transparent;
-    border-width: 18px 0px 0px 20px;
-    left: 85%;
-    bottom: 58%
-}
-
-div.thumbnailContainer {
-    box-shadow: none;
-    border: solid white 1px;
-    padding-top: 5px;
-}
-
-div.thumbnailContainer:hover {
-    box-shadow: 0 0 15px rgba(142, 176, 202, 0.5);
-    border: solid #B4DDFC 1px;
-}
-
-div.thumbnailContainer div p {
-    margin: 0 0 .1em 0;
-}
-
-div.thumbnailContainer div a {
-    display: block; /* To have a better hover behavior */
-}
diff --git a/doc/themes/scikit-learn/static/nature.css_t b/doc/themes/scikit-learn/static/nature.css_t
index c9c228fc7b7e..e7ec024ea9ba 100644
--- a/doc/themes/scikit-learn/static/nature.css_t
+++ b/doc/themes/scikit-learn/static/nature.css_t
@@ -129,7 +129,7 @@ div.navbar div.nav-icon {
         margin-left: 6px;
     }
     div.navbar.responsive > ul li.btn-li + li {
-        margin-top: -5px;   
+        margin-top: -5px;
     }
     div.navbar.responsive > ul {
         visiblity: visible;
@@ -777,6 +777,11 @@ div.sprint-wrapper:hover {
 
 /*-----------------------The Examples Gallery-------------------------------*/
 
+div.sphx-glr-download code.download {
+  display: inline-block;
+  white-space: normal;
+}
+
 /* ------- Zoom plots to make them fit in layout -------------------------- */
 div.body img.align-center {
     max-width:805px;
diff --git a/doc/whats_new.rst b/doc/whats_new.rst
index 9e4e9bfebde8..8db58b884d1f 100644
--- a/doc/whats_new.rst
+++ b/doc/whats_new.rst
@@ -19,9 +19,10 @@ Model Selection Enhancements and API Changes
   - **The ``model_selection`` module**
 
     The new module :mod:`sklearn.model_selection`, which groups together the
-    functionalities of formerly :mod:`cross_validation`, :mod:`grid_search` and
-    :mod:`learning_curve`, introduces new possibilities such as nested
-    cross-validation and better manipulation of parameter searches with Pandas.
+    functionalities of formerly :mod:`sklearn.cross_validation`,
+    :mod:`sklearn.grid_search` and :mod:`sklearn.learning_curve`, introduces new
+    possibilities such as nested cross-validation and better manipulation of
+    parameter searches with Pandas.
 
     Many things will stay the same but there are some key differences. Read
     below to know more about the changes.
@@ -245,7 +246,7 @@ Enhancements
    - Prediction of out-of-sample events with Isotonic Regression is now much
      faster (over 1000x in tests with synthetic data). By `Jonathan Arfa`_.
 
-   - Added ``inverse_transform`` function to :class:`decomposition.nmf` to compute
+   - Added ``inverse_transform`` function to :class:`decomposition.NMF` to compute
      data matrix of original shape. By `Anish Shah`_.
 
    - :class:`naive_bayes.GaussianNB` now accepts data-independent class-priors
@@ -254,8 +255,10 @@ Enhancements
    - Add option to show ``indicator features`` in the output of Imputer.
      By `Mani Teja`_.
 
-   - Reduce the memory usage for 32-bit float input arrays of :func:`utils.mean_variance_axis` and
-     :func:`utils.incr_mean_variance_axis` by supporting cython fused types. By `YenChen Lin`_.
+   - Reduce the memory usage for 32-bit float input arrays of
+     :func:`utils.sparse_func.mean_variance_axis` and
+     :func:`utils.sparse_func.incr_mean_variance_axis` by supporting cython
+     fused types. By `YenChen Lin`_.
 
    - The :func: `ignore_warnings` now accept a category argument to ignore only
      the warnings of a specified type. By `Thierry Guillemot`_.
@@ -285,15 +288,16 @@ Enhancements
      `#7154 <https://github.com/scikit-learn/scikit-learn/pull/7154>`_ by
      `Manvendra Singh`_.
 
-   - :class:`RobustScaler` now accepts ``quantile_range`` parameter.
+   - :class:`preprocessing.RobustScaler` now accepts ``quantile_range`` parameter.
      (`#5929 <https://github.com/scikit-learn/scikit-learn/pull/5929>`_)
      By `Konstantin Podshumok`_.
 
-   - The memory footprint is reduced (sometimes greatly) for :class:`BaseBagging`
-     and classes that inherit from it, i.e, :class:`BaggingClassifier`,
-     :class:`BaggingRegressor`, and :class:`IsolationForest`, by dynamically
-     generating attribute ``estimators_samples_`` only when it is needed.
-     By `David Staub`_.
+   - The memory footprint is reduced (sometimes greatly) for
+     :class:`ensemble.bagging.BaseBagging` and classes that inherit from it,
+     i.e, :class:`ensemble.BaggingClassifier`,
+     :class:`ensemble.BaggingRegressor`, and :class:`ensemble.IsolationForest`,
+     by dynamically generating attribute ``estimators_samples_`` only when it is
+     needed. By `David Staub`_.
 
    - :class:`linear_model.ElasticNet` and :class:`linear_model.Lasso`
      now works with ``np.float32`` input data without converting it
@@ -307,8 +311,8 @@ Enhancements
      (`#7239 <https://github.com/scikit-learn/scikit-learn/pull/7239/>`_)
      by `Hong Guangguo`_ with help from `Mads Jensen`_ and `Nelson Liu`_.
 
-   - Added ``n_jobs`` and ``sample_weights`` parameters for :class:`VotingClassifier`
-     to fit underlying estimators in parallel.
+   - Added ``n_jobs`` and ``sample_weights`` parameters for
+     :class:`ensemble.VotingClassifier` to fit underlying estimators in parallel.
      (`#5805 <https://github.com/scikit-learn/scikit-learn/pull/5805>`_)
      By `Ibraim Ganiev`_.
 
@@ -320,21 +324,41 @@ Enhancements
 
    - Simplification of the ``clone`` function, deprecate support for estimators
      that modify parameters in ``__init__``.
-     (`#5540 <https://github.com/scikit-learn/scikit-learn/pull/5540>_`)
+     (`#5540 <https://github.com/scikit-learn/scikit-learn/pull/5540>`_)
      By `Andreas Müller`_.
 
+   - When unpickling a scikit-learn estimator in a different version than the one
+     the estimator was trained with, a ``UserWarning`` is raised, see :ref:`the documentation
+     on model persistence <persistence_limitations>`
+     for more details.
+     (`#7248 <https://github.com/scikit-learn/scikit-learn/pull/7248>`_)
+     By `Andreas Müller`_.
+
+   - Support sparse contingency matrices in cluster evaluation
+     (:mod:`metrics.cluster.supervised`) to scale to a large number of
+     clusters.
+     (`#7419 <https://github.com/scikit-learn/scikit-learn/pull/7419>_`)
+     By `Gregory Stupp`_ and `Joel Nothman`_.
+
+   - Isotonic regression (:mod:`isotonic`) now uses a better algorithm to avoid
+     `O(n^2)` behavior in pathological cases, and is also generally faster
+     (`#6601 <https://github.com/scikit-learn/scikit-learn/pull/6691>`).
+     By `Antony Lee`_.
+
+
 Bug fixes
 .........
 
-    - :func: `model_selection.tests._search._check_param_grid` now works correctly with all types
+    - :func:`model_selection.tests._search._check_param_grid` now works correctly with all types
       that extends/implements `Sequence` (except string), including range (Python 3.x) and xrange
       (Python 2.x).
       (`#7323 <https://github.com/scikit-learn/scikit-learn/pull/7323>`_) by `Viacheslav Kovalevskyi`_.
 
-    - :class:`StratifiedKFold` now raises error if all n_labels for individual classes is less than n_folds.
+    - :class:`model_selection.StratifiedKFold` now raises error if all n_labels
+      for individual classes is less than n_folds.
       (`#6182 <https://github.com/scikit-learn/scikit-learn/pull/6182>`_) by `Devashish Deshpande`_.
 
-    - :class:`RandomizedPCA` default number of `iterated_power` is 4 instead of 3.
+    - :class:`decomposition.RandomizedPCA` default number of `iterated_power` is 4 instead of 3.
       (`#5141 <https://github.com/scikit-learn/scikit-learn/pull/5141>`_) by `Giorgio Patrini`_.
 
     - :func:`utils.extmath.randomized_svd` performs 4 power iterations by default, instead or 0.
@@ -359,7 +383,7 @@ Bug fixes
       Laplacian matrix was incorrectly set to 1. (`#4995 <https://github.com/scikit-learn/scikit-learn/pull/4995>`_) By `Peter Fischer`_.
 
     - Fixed incorrect initialization of :func:`utils.arpack.eigsh` on all
-      occurrences. Affects :class:`cluster.SpectralBiclustering`,
+      occurrences. Affects :class:`cluster.bicluster.SpectralBiclustering`,
       :class:`decomposition.KernelPCA`, :class:`manifold.LocallyLinearEmbedding`,
       and :class:`manifold.SpectralEmbedding` (`#5012 <https://github.com/scikit-learn/scikit-learn/pull/5012>`_). By `Peter Fischer`_.
 
@@ -367,7 +391,7 @@ Bug fixes
       won't accept anymore ``min_samples_split=1`` as at least 2 samples
       are required to split a decision tree node. By `Arnaud Joly`_
 
-    - :class:`VotingClassifier` now raises ``NotFittedError`` if ``predict``,
+    - :class:`ensemble.VotingClassifier` now raises ``NotFittedError`` if ``predict``,
       ``transform`` or ``predict_proba`` are called on the non-fitted estimator.
       by `Sebastian Raschka`_.
 
@@ -402,8 +426,8 @@ Bug fixes
       <https://github.com/scikit-learn/scikit-learn/issues/6902>`_).  By
       `LeonieBorne <https://github.com/LeonieBorne>`_.
 
-    - :func:`pairwise_distances` now converts arrays to boolean arrays when
-      required in scipy.spatial.distance.
+    - :func:`metrics.pairwise.pairwise_distances` now converts arrays to
+      boolean arrays when required in scipy.spatial.distance.
       (`#5460 <https://github.com/scikit-learn/scikit-learn/pull/5460>`_)
       By `Tom Dupre la Tour`_.
 
@@ -411,11 +435,11 @@ Bug fixes
       (`#7101 <https://github.com/scikit-learn/scikit-learn/pull/7101>`_)
       By `Ibraim Ganiev`_.
 
-    - Fix sparse input support in :func:`silhouette_score` as well as example
-      examples/text/document_clustering.py. By `YenChen Lin`_.
+    - Fix sparse input support in :func:`metrics.silhouette_score` as well as
+      example examples/text/document_clustering.py. By `YenChen Lin`_.
 
-    - :func:`_transform_selected` now always passes a copy of `X` to transform
-      function when `copy=True` (`#7194
+    - :func:`preprocessing.data._transform_selected` now always passes a copy
+      of `X` to transform function when `copy=True` (`#7194
       <https://github.com/scikit-learn/scikit-learn/issues/7194>`_). By `Caio
       Oliveira <https://github.com/caioaao>`_.
 
@@ -424,7 +448,7 @@ Bug fixes
 
     - Fix in :class:`sklearn.model_selection.StratifiedShuffleSplit` to
       return splits of size ``train_size`` and ``test_size`` in all cases
-      (`#6472 <https://github.com/scikit-learn/scikit-learn/pull/6472>`).
+      (`#6472 <https://github.com/scikit-learn/scikit-learn/pull/6472>`_).
       By `Andreas Müller`_.
 
     - :func:`metrics.roc_curve` and :func:`metrics.precision_recall_curve` no
@@ -432,13 +456,20 @@ Bug fixes
       problems for users with very small differences in scores (`#7353
       <https://github.com/scikit-learn/scikit-learn/pull/7353>`_).
 
+    - Fix incomplete ``predict_proba`` method delegation from
+      :class:`model_selection.GridSearchCV` to
+      :class:`linear_model.SGDClassifier` (`#7159
+      <https://github.com/scikit-learn/scikit-learn/pull/7159>`_)
+      by `Yichuan Liu <https://github.com/yl565>`_.
+
+
 API changes summary
 -------------------
 
    - The :mod:`sklearn.cross_validation`, :mod:`sklearn.grid_search` and
      :mod:`sklearn.learning_curve` have been deprecated and the classes and
-     functions have been reorganized into the :mod:`model_selection` module.
-     Ref :ref:`model_selection_changes` for more information.
+     functions have been reorganized into the :mod:`sklearn.model_selection`
+     module. Ref :ref:`model_selection_changes` for more information.
      (`#4294 <https://github.com/scikit-learn/scikit-learn/pull/4294>`_) by
      `Raghav R V`_.
 
@@ -448,17 +479,17 @@ API changes summary
    - Access to public attributes ``.X_`` and ``.y_`` has been deprecated in
      :class:`isotonic.IsotonicRegression`. By `Jonathan Arfa`_.
 
-   - The old :class:`VBGMM` is deprecated in favor of the new
-     :class:`BayesianGaussianMixture` (with the parameter
-     ``weight_concentration_prior_type='dirichlet_distribution'``).
+   - The old :class:`mixture.DPGMM` is deprecated in favor of the new
+     :class:`mixture.BayesianGaussianMixture` (with the parameter
+     ``weight_concentration_prior_type='dirichlet_process'``).
      The new class solves the computational
      problems of the old class and computes the Gaussian mixture with a
      Dirichlet process prior faster than before.
      (`#7295 <https://github.com/scikit-learn/scikit-learn/pull/7295>`_) by
      `Wei Xue`_ and `Thierry Guillemot`_.
 
-   - The old :class:`VBGMM` is deprecated in favor of the new
-     :class:`BayesianGaussianMixture` (with the parameter
+   - The old :class:`mixture.VBGMM` is deprecated in favor of the new
+     :class:`mixture.BayesianGaussianMixture` (with the parameter
      ``weight_concentration_prior_type='dirichlet_distribution'``).
      The new class solves the computational
      problems of the old class and computes the Variational Bayesian Gaussian
@@ -466,8 +497,8 @@ API changes summary
      (`#6651 <https://github.com/scikit-learn/scikit-learn/pull/6651>`_) by
      `Wei Xue`_ and `Thierry Guillemot`_.
 
-   - The old :class:`GMM` is deprecated in favor of the new
-     :class:`GaussianMixture`. The new class computes the Gaussian mixture
+   - The old :class:`mixture.GMM` is deprecated in favor of the new
+     :class:`mixture.GaussianMixture`. The new class computes the Gaussian mixture
      faster than before and some of computational problems have been solved.
      (`#6666 <https://github.com/scikit-learn/scikit-learn/pull/6666>`_) by
      `Wei Xue`_ and `Thierry Guillemot`_.
@@ -485,12 +516,12 @@ API changes summary
      (`#7187 <https://github.com/scikit-learn/scikit-learn/pull/7187>`_)
      by `YenChen Lin`_.
 
-    - ``classes`` parameter was renamed to ``labels`` in
-      :func:`metrics.classification.hamming_loss`.
-      (`#7260 <https://github.com/scikit-learn/scikit-learn/pull/7260>`_) by
-      `Sebastián Vanrell`_.
-   
-    - The splitter classes ``LabelKFold``, ``LabelShuffleSplit``,
+   - ``classes`` parameter was renamed to ``labels`` in
+     :func:`metrics.classification.hamming_loss`.
+     (`#7260 <https://github.com/scikit-learn/scikit-learn/pull/7260>`_) by
+     `Sebastián Vanrell`_.
+
+   - The splitter classes ``LabelKFold``, ``LabelShuffleSplit``,
      ``LeaveOneLabelOut`` and ``LeavePLabelsOut`` are renamed to
      :class:`model_selection.GroupKFold`,
      :class:`model_selection.GroupShuffleSplit`,
@@ -981,6 +1012,45 @@ API changes summary
       gamma to ``1. / n_features`` is deprecated and will be removed in 0.19.
       Use ``gamma="auto"`` instead.
 
+      People
+      ------
+
+      List of contributors for release 0.17, ordered alphabetically:
+      Aaron Schumacher, Adithya Ganesh, akitty, Alexandre Gramfort, Alexey Grigorev,
+      Ali Baharev, Allen Riddell, Ando Saabas, Andreas Mueller, Andrew Lamb,
+      Anish Shah, Ankur Ankan, Anthony Erlinger, Ari Rouvinen, Arnaud Joly,
+      Arnaud Rachez, Arthur Mensch, banilo, Barmaley.exe, benjaminirving,
+      Boyuan Deng, Brett Naul, Brian McFee, Buddha Prakash, Chi Zhang,
+      Chih-Wei Chang, Christof Angermueller, Christoph Gohlke, Christophe Bourguignat,
+      Christopher Erick Moody, Chyi-Kwei Yau, Cindy Sridharan, CJ Carey, Clyde-fare,
+      Cory Lorenz, Dan Blanchard, Daniel Galvez, Daniel Kronovet, Danny Sullivan,
+      Data1010, David, David D Lowe, David Dotson, djipey, Dmitry Spikhalskiy,
+      Donne Martin, Dougal J. Sutherland, Dougal Sutherland, edson duarte,
+      Eduardo Caro, Eric Larson, Eric Martin, Erich Schubert, Fernando Carrillo,
+      Frank C. Eckert, Frank Zalkow, Gael Varoquaux, Ganiev Ibraim, Gilles Louppe,
+      Giorgio Patrini, giorgiop, Graham Clenaghan, Gryllos Prokopis, gwulfs,
+      Henry Lin, Hsuan-Tien Lin, Immanuel Bayer, Ishank Gulati, Jack Martin,
+      Jacob Schreiber, Jaidev Deshpande, Jake VanderPlas, Jan Hendrik Metzen,
+      Jean Kossaifi, Jeffrey04, Jeremy, jfraj, Jiali Mei, jnothman, Joe Jevnik,
+      Joel Nothman, John Kirkham, John Wittenauer, Joseph, Joshua Loyal,
+      Jungkook Park, KamalakerDadi, Kashif Rasul, Keith Goodman, Kian Ho,
+      Konstantin Shmelkov, Kyler Brown, Lars Buitinck, Lilian Besson,
+      Loic Esteve, Louis Tiao, maheshakya, Maheshakya Wijewardena, Manoj Kumar,
+      MarkTab marktab.net, Martin Ku, Martin Spacek, MartinBpr, martinosorb,
+      MaryanMorel, Masafumi Oyamada, Mathieu Blondel, Matt Krump, Matti Lyra,
+      Maxim Kolganov, mbillinger, mhg, Michael Heilman, Michael Patterson,
+      Miroslav Batchkarov, Nelle Varoquaux, Nicolas, Nikolay Mayorov, Olivier Grisel,
+      Omer Katz, Óscar Nájera, Pauli Virtanen, Peter Fischer, Peter Prettenhofer,
+      Phil Roth, pianomania, Preston Parry, Raghav, Raghav R V, rasbt, Rob Zinkov,
+      Robert Layton, Rohan Ramanath, Saket Choudhary, Sam Zhang, santi,
+      saurabh.bansod, scls19fr, Sebastian Saeger, Shivan Sornarajah, SimonPL,
+      sinhrks, Skipper Seabold, Sonny Hu, sseg, Stephen Hoover, Steven De Gryze,
+      Steven Seguin, Theodore Vasiloudis, Thomas Unterthiner, Tiago Freitas Pereira,
+      Tian Wang, Tim Head, Timothy Hopper, tokoroten, Tom DLT, Tom Dupré la Tour,
+      TomDLT, trevorstephens, unknown, Valentin Stolbunov, Vighnesh Birodkar,
+      Vinayak Mehta, Vincent, Vincent Michel, vstolbunov, wangz10, Wei Xue,
+      Yucheng Low, Yury Zhauniarovich, Zac Stewart, zhai_pro, Zichen Wang
+
 .. _changes_0_1_16:
 
 Version 0.16.1
@@ -1483,6 +1553,43 @@ API changes summary
     - :class:`cluster.DBSCAN` now uses a deterministic initialization. The
       `random_state` parameter is deprecated. By `Erich Schubert`_.
 
+      People
+      ------
+
+      List of contributors for release 0.16, ordered alphabetically.
+      A. Flaxman, Aaron Schumacher, Aaron Staple, abhishek thakur, Akshay,
+      akshayah3, Aldrian Obaja, Alexander Fabisch, Alexandre Gramfort, Alexis Mignon,
+      Anders Aagaard, Andreas Mueller, Andreas van Cranenburgh, Andrew Tulloch,
+      Andrew Walker, Antony Lee, Arnaud Joly, banilo, Barmaley.exe, Ben Davies,
+      Benedikt Koehler, bhsu, Boris Feld, Borja Ayerdi, Boyuan Deng, Brent Pedersen,
+      Brian Wignall, Brooke Osborn, Calvin Giles, Cathy Deng, Celeo, cgohlke,
+      chebee7i, Christian Stade-Schuldt, Christof Angermueller, Chyi-Kwei Yau,
+      CJ Carey, Clemens Brunner, Daiki Aminaka, Dan Blanchard, danfrankj,
+      Danny Sullivan, David Fletcher, Dmitrijs Milajevs, Dougal J. Sutherland,
+      Erich Schubert, Fabian Pedregosa, Florian Wilhelm, floydsoft,
+      Félix-Antoine Fortin, Gael Varoquaux, Garrett-R, Gilles Louppe, gpassino,
+      gwulfs, Hampus Bengtsson, Hamzeh Alsalhi, Hanna Wallach, Harry Mavroforakis,
+      Hasil Sharma, Helder, Herve Bredin, Hsiang-Fu Yu, Hugues SALAMIN, Ian Gilmore,
+      Ilambharathi Kanniah, Imran Haque, isms, Jake VanderPlas, Jan Dlabal,
+      Jan Hendrik Metzen, Jatin Shah, Javier López Peña, jdcaballero, Jean Kossaifi,
+      Jeff Hammerbacher, Joel Nothman, Jonathan Helmus, Joseph, Kaicheng Zhang,
+      Kevin Markham, Kyle Beauchamp, Kyle Kastner, Lagacherie Matthieu, Lars Buitinck,
+      Laurent Direr, leepei, Loic Esteve, Luis Pedro Coelho, Lukas Michelbacher,
+      maheshakya, Manoj Kumar, Manuel, Mario Michael Krell, Martin, Martin Billinger,
+      Martin Ku, Mateusz Susik, Mathieu Blondel, Matt Pico, Matt Terry,
+      Matteo Visconti dOC, Matti Lyra, Max Linke, Mehdi Cherti, Michael Bommarito,
+      Michael Eickenberg, Michal Romaniuk, MLG, mr.Shu, Nelle Varoquaux,
+      Nicola Montecchio, Nicolas, Nikolay Mayorov, Noel Dawe, Okal Billy,
+      Olivier Grisel, Óscar Nájera, Paolo Puggioni, Peter Prettenhofer, Pratap Vardhan,
+      pvnguyen, queqichao, Rafael Carrascosa, Raghav R V, Rahiel Kasim, Randall Mason,
+      Rob Zinkov, Robert Bradshaw, Saket Choudhary, Sam Nicholls, Samuel Charron,
+      Saurabh Jha, sethdandridge, sinhrks, snuderl, Stefan Otte, Stefan van der Walt,
+      Steve Tjoa, swu, Sylvain Zimmer, tejesh95, terrycojones, Thomas Delteil,
+      Thomas Unterthiner, Tomas Kazmar, trevorstephens, tttthomasssss, Tzu-Ming Kuo,
+      ugurcaliskan, ugurthemaster, Vinayak Mehta, Vincent Dubourg, Vjacheslav Murashkin,
+      Vlad Niculae, wadawson, Wei Xue, Will Lamond, Wu Jiang, x0l, Xinfan Meng,
+      Yan Yi, Yu-Chin
+
 .. _changes_0_15_2:
 
 Version 0.15.2
@@ -4529,3 +4636,5 @@ David Huard, Dave Morrill, Ed Schofield, Travis Oliphant, Pearu Peterson.
 .. _Sebastián Vanrell: https://github.com/srvanrell
 
 .. _Robert McGibbon: https://github.com/rmcgibbo
+
+.. _Gregory Stupp: https://github.com/stuppie
diff --git a/examples/neural_networks/plot_mlp_training_curves.py b/examples/neural_networks/plot_mlp_training_curves.py
index c6456ab12c71..89ca2747bdd4 100644
--- a/examples/neural_networks/plot_mlp_training_curves.py
+++ b/examples/neural_networks/plot_mlp_training_curves.py
@@ -8,6 +8,9 @@
 use several small datasets, for which L-BFGS might be more suitable. The
 general trend shown in these examples seems to carry over to larger datasets,
 however.
+
+Note that those results can be highly dependent on the value of
+``learning_rate_init``.
 """
 
 print(__doc__)
@@ -17,19 +20,19 @@
 from sklearn import datasets
 
 # different learning rate schedules and momentum parameters
-params = [{'algorithm': 'sgd', 'learning_rate': 'constant', 'momentum': 0,
+params = [{'solver': 'sgd', 'learning_rate': 'constant', 'momentum': 0,
            'learning_rate_init': 0.2},
-          {'algorithm': 'sgd', 'learning_rate': 'constant', 'momentum': .9,
+          {'solver': 'sgd', 'learning_rate': 'constant', 'momentum': .9,
            'nesterovs_momentum': False, 'learning_rate_init': 0.2},
-          {'algorithm': 'sgd', 'learning_rate': 'constant', 'momentum': .9,
+          {'solver': 'sgd', 'learning_rate': 'constant', 'momentum': .9,
            'nesterovs_momentum': True, 'learning_rate_init': 0.2},
-          {'algorithm': 'sgd', 'learning_rate': 'invscaling', 'momentum': 0,
+          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': 0,
            'learning_rate_init': 0.2},
-          {'algorithm': 'sgd', 'learning_rate': 'invscaling', 'momentum': .9,
+          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': .9,
            'nesterovs_momentum': True, 'learning_rate_init': 0.2},
-          {'algorithm': 'sgd', 'learning_rate': 'invscaling', 'momentum': .9,
+          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': .9,
            'nesterovs_momentum': False, 'learning_rate_init': 0.2},
-          {'algorithm': 'adam'}]
+          {'solver': 'adam', 'learning_rate_init': 0.01}]
 
 labels = ["constant learning-rate", "constant with momentum",
           "constant with Nesterov's momentum",
diff --git a/examples/neural_networks/plot_mnist_filters.py b/examples/neural_networks/plot_mnist_filters.py
index dc63f1caeb91..6c3b8b2284ea 100644
--- a/examples/neural_networks/plot_mnist_filters.py
+++ b/examples/neural_networks/plot_mnist_filters.py
@@ -33,9 +33,9 @@
 y_train, y_test = y[:60000], y[60000:]
 
 # mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,
-#                     algorithm='sgd', verbose=10, tol=1e-4, random_state=1)
+#                     solver='sgd', verbose=10, tol=1e-4, random_state=1)
 mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4,
-                    algorithm='sgd', verbose=10, tol=1e-4, random_state=1,
+                    solver='sgd', verbose=10, tol=1e-4, random_state=1,
                     learning_rate_init=.1)
 
 mlp.fit(X_train, y_train)
diff --git a/setup.py b/setup.py
index 3c32655c51de..1a50d3b61806 100755
--- a/setup.py
+++ b/setup.py
@@ -3,8 +3,6 @@
 # Copyright (C) 2007-2009 Cournapeau David <cournape@gmail.com>
 #               2010 Fabian Pedregosa <fabian.pedregosa@inria.fr>
 # License: 3-clause BSD
-import subprocess
-
 descr = """A set of python modules for machine learning and data mining"""
 
 import sys
@@ -12,6 +10,8 @@
 import shutil
 from distutils.command.clean import clean as Clean
 from pkg_resources import parse_version
+import traceback
+import subprocess
 
 if sys.version_info[0] < 3:
     import __builtin__ as builtins
@@ -155,6 +155,7 @@ def get_scipy_status():
             scipy_version) >= parse_version(SCIPY_MIN_VERSION)
         scipy_status['version'] = scipy_version
     except ImportError:
+        traceback.print_exc()
         scipy_status['up_to_date'] = False
         scipy_status['version'] = ""
     return scipy_status
@@ -174,6 +175,7 @@ def get_numpy_status():
             numpy_version) >= parse_version(NUMPY_MIN_VERSION)
         numpy_status['version'] = numpy_version
     except ImportError:
+        traceback.print_exc()
         numpy_status['up_to_date'] = False
         numpy_status['version'] = ""
     return numpy_status
diff --git a/sklearn/__init__.py b/sklearn/__init__.py
index 8adf598c3ad5..3d1f4fc93ade 100644
--- a/sklearn/__init__.py
+++ b/sklearn/__init__.py
@@ -37,7 +37,7 @@
 # Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
 # 'X.Y.dev0' is the canonical version of 'X.Y.dev'
 #
-__version__ = '0.18.dev0'
+__version__ = '0.19.dev0'
 
 
 try:
diff --git a/sklearn/_isotonic.pyx b/sklearn/_isotonic.pyx
index e3ee2a54830a..1cec075fc6fc 100644
--- a/sklearn/_isotonic.pyx
+++ b/sklearn/_isotonic.pyx
@@ -1,4 +1,4 @@
-# Author: Nelle Varoquaux, Andrew Tulloch
+# Author: Nelle Varoquaux, Andrew Tulloch, Antony Lee
 
 # Uses the pool adjacent violators algorithm (PAVA), with the
 # enhancement of searching for the longest decreasing subsequence to
@@ -10,73 +10,67 @@ cimport cython
 
 ctypedef np.float64_t DOUBLE
 
-np.import_array()
-
 
 @cython.boundscheck(False)
 @cython.wraparound(False)
 @cython.cdivision(True)
-def _isotonic_regression(np.ndarray[DOUBLE, ndim=1] y,
-                         np.ndarray[DOUBLE, ndim=1] weight,
-                         np.ndarray[DOUBLE, ndim=1] solution):
+def _inplace_contiguous_isotonic_regression(DOUBLE[::1] y, DOUBLE[::1] w):
     cdef:
-        DOUBLE numerator, denominator, ratio
-        Py_ssize_t i, pooled, n, k
-
-    n = y.shape[0]
-    # The algorithm proceeds by iteratively updating the solution
-    # array.
+        Py_ssize_t n = y.shape[0], i, k
+        DOUBLE prev_y, sum_wy, sum_w
+        Py_ssize_t[::1] target = np.arange(n, dtype=np.intp)
 
-    # TODO - should we just pass in a pre-copied solution
-    # array and mutate that?
-    for i in range(n):
-        solution[i] = y[i]
+    # target describes a list of blocks.  At any time, if [i..j] (inclusive) is
+    # an active block, then target[i] := j and target[j] := i.
 
-    if n <= 1:
-        return solution
+    # For "active" indices (block starts):
+    # w[i] := sum{w_orig[j], j=[i..target[i]]}
+    # y[i] := sum{y_orig[j]*w_orig[j], j=[i..target[i]]} / w[i]
 
-    n -= 1
-    while 1:
-        # repeat until there are no more adjacent violators.
+    with nogil:
         i = 0
-        pooled = 0
         while i < n:
-            k = i
-            while k < n and solution[k] >= solution[k + 1]:
-                k += 1
-            if solution[i] != solution[k]:
-                # solution[i:k + 1] is a decreasing subsequence, so
-                # replace each point in the subsequence with the
-                # weighted average of the subsequence.
-
-                # TODO: explore replacing each subsequence with a
-                # _single_ weighted point, and reconstruct the whole
-                # sequence from the sequence of collapsed points.
-                # Theoretically should reduce running time, though
-                # initial experiments weren't promising.
-                numerator = 0.0
-                denominator = 0.0
-                for j in range(i, k + 1):
-                    numerator += solution[j] * weight[j]
-                    denominator += weight[j]
-                ratio = numerator / denominator
-                for j in range(i, k + 1):
-                    solution[j] = ratio
-                pooled = 1
-            i = k + 1
-        # Check for convergence
-        if pooled == 0:
-            break
-
-    return solution
+            k = target[i] + 1
+            if k == n:
+                break
+            if y[i] < y[k]:
+                i = k
+                continue
+            sum_wy = w[i] * y[i]
+            sum_w = w[i]
+            while True:
+                # We are within a decreasing subsequence.
+                prev_y = y[k]
+                sum_wy += w[k] * y[k]
+                sum_w += w[k]
+                k = target[k] + 1
+                if k == n or prev_y < y[k]:
+                    # Non-singleton decreasing subsequence is finished,
+                    # update first entry.
+                    y[i] = sum_wy / sum_w
+                    w[i] = sum_w
+                    target[i] = k - 1
+                    target[k - 1] = i
+                    if i > 0:
+                        # Backtrack if we can.  This makes the algorithm
+                        # single-pass and ensures O(n) complexity.
+                        i = target[i - 1]
+                    # Otherwise, restart from the same point.
+                    break
+        # Reconstruct the solution.
+        i = 0
+        while i < n:
+            k = target[i] + 1
+            y[i + 1 : k] = y[i]
+            i = k
 
 
 @cython.boundscheck(False)
 @cython.wraparound(False)
 @cython.cdivision(True)
 def _make_unique(np.ndarray[dtype=np.float64_t] X,
-                  np.ndarray[dtype=np.float64_t] y,
-                  np.ndarray[dtype=np.float64_t] sample_weights):
+                 np.ndarray[dtype=np.float64_t] y,
+                 np.ndarray[dtype=np.float64_t] sample_weights):
     """Average targets for duplicate X, drop duplicates.
 
     Aggregates duplicate X values into a single X value where
diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py
index fdd1f852c6af..1b31562b465b 100644
--- a/sklearn/decomposition/kernel_pca.py
+++ b/sklearn/decomposition/kernel_pca.py
@@ -34,15 +34,15 @@ class KernelPCA(BaseEstimator, TransformerMixin):
     degree : int, default=3
         Degree for poly kernels. Ignored by other kernels.
 
-    gamma : float, optional
-        Kernel coefficient for rbf and poly kernels. Default: 1/n_features.
-        Ignored by other kernels.
+    gamma : float, default=1/n_features
+        Kernel coefficient for rbf and poly kernels. Ignored by other
+        kernels.
 
-    coef0 : float, optional
+    coef0 : float, default=1
         Independent term in poly and sigmoid kernels.
         Ignored by other kernels.
 
-    kernel_params : mapping of string to any, optional
+    kernel_params : mapping of string to any, default=None
         Parameters (keyword arguments) and values for kernel passed as
         callable object. Ignored by other kernels.
 
@@ -54,18 +54,18 @@ class KernelPCA(BaseEstimator, TransformerMixin):
         Learn the inverse transform for non-precomputed kernels.
         (i.e. learn to find the pre-image of a point)
 
-    eigen_solver : string ['auto'|'dense'|'arpack']
+    eigen_solver : string ['auto'|'dense'|'arpack'], default='auto'
         Select eigensolver to use. If n_components is much less than
         the number of training samples, arpack may be more efficient
         than the dense eigensolver.
 
     tol : float, default=0
-        Convergence tolerance for arpack. If zero, optimal value will be
-        chosen by arpack.
+        Convergence tolerance for arpack.
+        If 0, optimal value will be chosen by arpack.
 
     max_iter : int, default=None
-        Maximum number of iterations for arpack. If None, optimal value will
-        be chosen by arpack.
+        Maximum number of iterations for arpack.
+        If None, optimal value will be chosen by arpack.
 
     remove_zero_eig : boolean, default=False
         If True, then all components with zero eigenvalues are removed, so
@@ -99,15 +99,14 @@ class KernelPCA(BaseEstimator, TransformerMixin):
         `remove_zero_eig` are not set, then all components are stored.
 
     dual_coef_ : array, (n_samples, n_features)
-        Inverse transform matrix. If `fit_inverse_transform=False`,
-        ``dual_coef_`` is not present.
+        Inverse transform matrix. Set if `fit_inverse_transform` is True.
 
     X_transformed_fit_ : array, (n_samples, n_components)
         Projection of the fitted data on the kernel principal components.
 
     X_fit_ : (n_samples, n_features)
         The data used to fit the model. If `copy_X=False`, then `X_fit_` is
-        a reference.
+        a reference. This attribute is used for the calls to transform.
 
     References
     ----------
diff --git a/sklearn/feature_extraction/text.py b/sklearn/feature_extraction/text.py
index d291cc54d60b..3bc2e8cb0ca3 100644
--- a/sklearn/feature_extraction/text.py
+++ b/sklearn/feature_extraction/text.py
@@ -685,9 +685,11 @@ def _sort_features(self, X, vocabulary):
         sorted_features = sorted(six.iteritems(vocabulary))
         map_index = np.empty(len(sorted_features), dtype=np.int32)
         for new_val, (term, old_val) in enumerate(sorted_features):
-            map_index[new_val] = old_val
             vocabulary[term] = new_val
-        return X[:, map_index]
+            map_index[old_val] = new_val
+
+        X.indices = map_index.take(X.indices, mode='clip')
+        return X
 
     def _limit_features(self, X, vocabulary, high=None, low=None,
                         limit=None):
@@ -741,16 +743,25 @@ def _count_vocab(self, raw_documents, fixed_vocab):
             vocabulary.default_factory = vocabulary.__len__
 
         analyze = self.build_analyzer()
-        j_indices = _make_int_array()
+        j_indices = []
         indptr = _make_int_array()
+        values = _make_int_array()
         indptr.append(0)
         for doc in raw_documents:
+            feature_counter = {}
             for feature in analyze(doc):
                 try:
-                    j_indices.append(vocabulary[feature])
+                    feature_idx = vocabulary[feature]
+                    if feature_idx not in feature_counter:
+                        feature_counter[feature_idx] = 1
+                    else:
+                        feature_counter[feature_idx] += 1
                 except KeyError:
                     # Ignore out-of-vocabulary items for fixed_vocab=True
                     continue
+
+            j_indices.extend(feature_counter.keys())
+            values.extend(feature_counter.values())
             indptr.append(len(j_indices))
 
         if not fixed_vocab:
@@ -760,14 +771,14 @@ def _count_vocab(self, raw_documents, fixed_vocab):
                 raise ValueError("empty vocabulary; perhaps the documents only"
                                  " contain stop words")
 
-        j_indices = frombuffer_empty(j_indices, dtype=np.intc)
+        j_indices = np.asarray(j_indices, dtype=np.intc)
         indptr = np.frombuffer(indptr, dtype=np.intc)
-        values = np.ones(len(j_indices))
+        values = frombuffer_empty(values, dtype=np.intc)
 
         X = sp.csr_matrix((values, j_indices, indptr),
                           shape=(len(indptr) - 1, len(vocabulary)),
                           dtype=self.dtype)
-        X.sum_duplicates()
+        X.sort_indices()
         return vocabulary, X
 
     def fit(self, raw_documents, y=None):
diff --git a/sklearn/gaussian_process/tests/test_gpc.py b/sklearn/gaussian_process/tests/test_gpc.py
index d429018cb36b..16b2507e45f1 100644
--- a/sklearn/gaussian_process/tests/test_gpc.py
+++ b/sklearn/gaussian_process/tests/test_gpc.py
@@ -34,8 +34,7 @@ def f(x):
 
 
 def test_predict_consistent():
-    """ Check binary predict decision has also predicted probability above 0.5.
-    """
+    # Check binary predict decision has also predicted probability above 0.5.
     for kernel in kernels:
         gpc = GaussianProcessClassifier(kernel=kernel).fit(X, y)
         assert_array_equal(gpc.predict(X),
@@ -43,7 +42,7 @@ def test_predict_consistent():
 
 
 def test_lml_improving():
-    """ Test that hyperparameter-tuning improves log-marginal likelihood. """
+    # Test that hyperparameter-tuning improves log-marginal likelihood.
     for kernel in kernels:
         if kernel == fixed_kernel:
             continue
@@ -53,7 +52,7 @@ def test_lml_improving():
 
 
 def test_lml_precomputed():
-    """ Test that lml of optimized kernel is stored correctly. """
+    # Test that lml of optimized kernel is stored correctly.
     for kernel in kernels:
         gpc = GaussianProcessClassifier(kernel=kernel).fit(X, y)
         assert_almost_equal(gpc.log_marginal_likelihood(gpc.kernel_.theta),
@@ -61,7 +60,7 @@ def test_lml_precomputed():
 
 
 def test_converged_to_local_maximum():
-    """ Test that we are in local maximum after hyperparameter-optimization."""
+    # Test that we are in local maximum after hyperparameter-optimization.
     for kernel in kernels:
         if kernel == fixed_kernel:
             continue
@@ -76,7 +75,7 @@ def test_converged_to_local_maximum():
 
 
 def test_lml_gradient():
-    """ Compare analytic and numeric gradient of log marginal likelihood. """
+    # Compare analytic and numeric gradient of log marginal likelihood.
     for kernel in kernels:
         gpc = GaussianProcessClassifier(kernel=kernel).fit(X, y)
 
@@ -91,10 +90,8 @@ def test_lml_gradient():
 
 
 def test_random_starts():
-    """
-    Test that an increasing number of random-starts of GP fitting only
-    increases the log marginal likelihood of the chosen theta.
-    """
+    # Test that an increasing number of random-starts of GP fitting only
+    # increases the log marginal likelihood of the chosen theta.
     n_samples, n_features = 25, 2
     np.random.seed(0)
     rng = np.random.RandomState(0)
@@ -115,7 +112,7 @@ def test_random_starts():
 
 
 def test_custom_optimizer():
-    """ Test that GPC can use externally defined optimizers. """
+    # Test that GPC can use externally defined optimizers.
     # Define a dummy optimizer that simply tests 50 random hyperparameters
     def optimizer(obj_func, initial_theta, bounds):
         rng = np.random.RandomState(0)
@@ -140,7 +137,7 @@ def optimizer(obj_func, initial_theta, bounds):
 
 
 def test_multi_class():
-    """ Test GPC for multi-class classification problems. """
+    # Test GPC for multi-class classification problems.
     for kernel in kernels:
         gpc = GaussianProcessClassifier(kernel=kernel)
         gpc.fit(X, y_mc)
@@ -153,7 +150,7 @@ def test_multi_class():
 
 
 def test_multi_class_n_jobs():
-    """ Test that multi-class GPC produces identical results with n_jobs>1. """
+    # Test that multi-class GPC produces identical results with n_jobs>1.
     for kernel in kernels:
         gpc = GaussianProcessClassifier(kernel=kernel)
         gpc.fit(X, y_mc)
diff --git a/sklearn/gaussian_process/tests/test_gpr.py b/sklearn/gaussian_process/tests/test_gpr.py
index 98b2d63f6dd0..e62a2c1b14d3 100644
--- a/sklearn/gaussian_process/tests/test_gpr.py
+++ b/sklearn/gaussian_process/tests/test_gpr.py
@@ -36,7 +36,7 @@ def f(x):
 
 
 def test_gpr_interpolation():
-    """Test the interpolating property for different kernels."""
+    # Test the interpolating property for different kernels.
     for kernel in kernels:
         gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
         y_pred, y_cov = gpr.predict(X, return_cov=True)
@@ -46,7 +46,7 @@ def test_gpr_interpolation():
 
 
 def test_lml_improving():
-    """ Test that hyperparameter-tuning improves log-marginal likelihood. """
+    # Test that hyperparameter-tuning improves log-marginal likelihood.
     for kernel in kernels:
         if kernel == fixed_kernel:
             continue
@@ -56,7 +56,7 @@ def test_lml_improving():
 
 
 def test_lml_precomputed():
-    """ Test that lml of optimized kernel is stored correctly. """
+    # Test that lml of optimized kernel is stored correctly.
     for kernel in kernels:
         gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
         assert_equal(gpr.log_marginal_likelihood(gpr.kernel_.theta),
@@ -64,7 +64,7 @@ def test_lml_precomputed():
 
 
 def test_converged_to_local_maximum():
-    """ Test that we are in local maximum after hyperparameter-optimization."""
+    # Test that we are in local maximum after hyperparameter-optimization.
     for kernel in kernels:
         if kernel == fixed_kernel:
             continue
@@ -79,7 +79,7 @@ def test_converged_to_local_maximum():
 
 
 def test_solution_inside_bounds():
-    """ Test that hyperparameter-optimization remains in bounds"""
+    # Test that hyperparameter-optimization remains in bounds#
     for kernel in kernels:
         if kernel == fixed_kernel:
             continue
@@ -95,7 +95,7 @@ def test_solution_inside_bounds():
 
 
 def test_lml_gradient():
-    """ Compare analytic and numeric gradient of log marginal likelihood. """
+    # Compare analytic and numeric gradient of log marginal likelihood.
     for kernel in kernels:
         gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
 
@@ -110,7 +110,7 @@ def test_lml_gradient():
 
 
 def test_prior():
-    """ Test that GP prior has mean 0 and identical variances."""
+    # Test that GP prior has mean 0 and identical variances.
     for kernel in kernels:
         gpr = GaussianProcessRegressor(kernel=kernel)
 
@@ -125,7 +125,7 @@ def test_prior():
 
 
 def test_sample_statistics():
-    """ Test that statistics of samples drawn from GP are correct."""
+    # Test that statistics of samples drawn from GP are correct.
     for kernel in kernels:
         gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
 
@@ -140,14 +140,14 @@ def test_sample_statistics():
 
 
 def test_no_optimizer():
-    """ Test that kernel parameters are unmodified when optimizer is None."""
+    # Test that kernel parameters are unmodified when optimizer is None.
     kernel = RBF(1.0)
     gpr = GaussianProcessRegressor(kernel=kernel, optimizer=None).fit(X, y)
     assert_equal(np.exp(gpr.kernel_.theta), 1.0)
 
 
 def test_predict_cov_vs_std():
-    """ Test that predicted std.-dev. is consistent with cov's diagonal."""
+    # Test that predicted std.-dev. is consistent with cov's diagonal.
     for kernel in kernels:
         gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
         y_mean, y_cov = gpr.predict(X2, return_cov=True)
@@ -156,7 +156,7 @@ def test_predict_cov_vs_std():
 
 
 def test_anisotropic_kernel():
-    """ Test that GPR can identify meaningful anisotropic length-scales. """
+    # Test that GPR can identify meaningful anisotropic length-scales.
     # We learn a function which varies in one dimension ten-times slower
     # than in the other. The corresponding length-scales should differ by at
     # least a factor 5
@@ -171,10 +171,8 @@ def test_anisotropic_kernel():
 
 
 def test_random_starts():
-    """
-    Test that an increasing number of random-starts of GP fitting only
-    increases the log marginal likelihood of the chosen theta.
-    """
+    # Test that an increasing number of random-starts of GP fitting only
+    # increases the log marginal likelihood of the chosen theta.
     n_samples, n_features = 25, 2
     np.random.seed(0)
     rng = np.random.RandomState(0)
@@ -197,11 +195,10 @@ def test_random_starts():
 
 
 def test_y_normalization():
-    """ Test normalization of the target values in GP
+    # Test normalization of the target values in GP
 
-    Fitting non-normalizing GP on normalized y and fitting normalizing GP
-    on unnormalized y should yield identical results
-    """
+    # Fitting non-normalizing GP on normalized y and fitting normalizing GP
+    # on unnormalized y should yield identical results
     y_mean = y.mean(0)
     y_norm = y - y_mean
     for kernel in kernels:
@@ -226,7 +223,7 @@ def test_y_normalization():
 
 
 def test_y_multioutput():
-    """ Test that GPR can deal with multi-dimensional target values"""
+    # Test that GPR can deal with multi-dimensional target values
     y_2d = np.vstack((y, y * 2)).T
 
     # Test for fixed kernel that first dimension of 2d GP equals the output
@@ -269,7 +266,7 @@ def test_y_multioutput():
 
 
 def test_custom_optimizer():
-    """ Test that GPR can use externally defined optimizers. """
+    # Test that GPR can use externally defined optimizers.
     # Define a dummy optimizer that simply tests 50 random hyperparameters
     def optimizer(obj_func, initial_theta, bounds):
         rng = np.random.RandomState(0)
@@ -294,7 +291,7 @@ def optimizer(obj_func, initial_theta, bounds):
 
 
 def test_duplicate_input():
-    """ Test GPR can handle two different output-values for the same input. """
+    # Test GPR can handle two different output-values for the same input.
     for kernel in kernels:
         gpr_equal_inputs = \
             GaussianProcessRegressor(kernel=kernel, alpha=1e-2)
diff --git a/sklearn/gaussian_process/tests/test_kernels.py b/sklearn/gaussian_process/tests/test_kernels.py
index 116fad8ddaf9..e670e5330889 100644
--- a/sklearn/gaussian_process/tests/test_kernels.py
+++ b/sklearn/gaussian_process/tests/test_kernels.py
@@ -3,7 +3,6 @@
 # Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
 # License: BSD 3 clause
 
-from collections import Hashable
 from sklearn.externals.funcsigs import signature
 
 import numpy as np
@@ -50,7 +49,7 @@
 
 
 def test_kernel_gradient():
-    """ Compare analytic and numeric gradient of kernels. """
+    # Compare analytic and numeric gradient of kernels.
     for kernel in kernels:
         K, K_gradient = kernel(X, eval_gradient=True)
 
@@ -70,7 +69,7 @@ def eval_kernel_for_theta(theta):
 
 
 def test_kernel_theta():
-    """ Check that parameter vector theta of kernel is set correctly. """
+    # Check that parameter vector theta of kernel is set correctly.
     for kernel in kernels:
         if isinstance(kernel, KernelOperator) \
            or isinstance(kernel, Exponentiation):  # skip non-basic kernels
@@ -111,8 +110,8 @@ def test_kernel_theta():
                 assert_array_equal(K_gradient[..., :i],
                                    K_gradient_new[..., :i])
             if i + 1 < len(kernel.hyperparameters):
-                assert_equal(theta[i+1:], new_kernel.theta[i:])
-                assert_array_equal(K_gradient[..., i+1:],
+                assert_equal(theta[i + 1:], new_kernel.theta[i:])
+                assert_array_equal(K_gradient[..., i + 1:],
                                    K_gradient_new[..., i:])
 
         # Check that values of theta are modified correctly
@@ -126,7 +125,7 @@ def test_kernel_theta():
 
 
 def test_auto_vs_cross():
-    """ Auto-correlation and cross-correlation should be consistent. """
+    # Auto-correlation and cross-correlation should be consistent.
     for kernel in kernels:
         if kernel == kernel_white:
             continue  # Identity is not satisfied on diagonal
@@ -136,7 +135,7 @@ def test_auto_vs_cross():
 
 
 def test_kernel_diag():
-    """ Test that diag method of kernel returns consistent results. """
+    # Test that diag method of kernel returns consistent results.
     for kernel in kernels:
         K_call_diag = np.diag(kernel(X))
         K_diag = kernel.diag(X)
@@ -144,7 +143,7 @@ def test_kernel_diag():
 
 
 def test_kernel_operator_commutative():
-    """ Adding kernels and multiplying kernels should be commutative. """
+    # Adding kernels and multiplying kernels should be commutative.
     # Check addition
     assert_almost_equal((RBF(2.0) + 1.0)(X),
                         (1.0 + RBF(2.0))(X))
@@ -155,7 +154,7 @@ def test_kernel_operator_commutative():
 
 
 def test_kernel_anisotropic():
-    """ Anisotropic kernel should be consistent with isotropic kernels."""
+    # Anisotropic kernel should be consistent with isotropic kernels.
     kernel = 3.0 * RBF([0.5, 2.0])
 
     K = kernel(X)
@@ -176,7 +175,7 @@ def test_kernel_anisotropic():
 
 
 def test_kernel_stationary():
-    """ Test stationarity of kernels."""
+    # Test stationarity of kernels.
     for kernel in kernels:
         if not kernel.is_stationary():
             continue
@@ -185,7 +184,7 @@ def test_kernel_stationary():
 
 
 def check_hyperparameters_equal(kernel1, kernel2):
-    """Check that hyperparameters of two kernels are equal"""
+    # Check that hyperparameters of two kernels are equal
     for attr in set(dir(kernel1) + dir(kernel2)):
         if attr.startswith("hyperparameter_"):
             attr_value1 = getattr(kernel1, attr)
@@ -194,7 +193,7 @@ def check_hyperparameters_equal(kernel1, kernel2):
 
 
 def test_kernel_clone():
-    """ Test that sklearn's clone works correctly on kernels. """
+    # Test that sklearn's clone works correctly on kernels.
     bounds = (1e-5, 1e5)
     for kernel in kernels:
         kernel_cloned = clone(kernel)
@@ -219,7 +218,8 @@ def test_kernel_clone():
         params = kernel.get_params()
         # RationalQuadratic kernel is isotropic.
         isotropic_kernels = (ExpSineSquared, RationalQuadratic)
-        if 'length_scale' in params and not isinstance(kernel, isotropic_kernels):
+        if 'length_scale' in params and not isinstance(kernel,
+                                                       isotropic_kernels):
             length_scale = params['length_scale']
             if np.iterable(length_scale):
                 params['length_scale'] = length_scale[0]
@@ -232,11 +232,12 @@ def test_kernel_clone():
             assert_equal(kernel_cloned_clone.get_params(),
                          kernel_cloned.get_params())
             assert_not_equal(id(kernel_cloned_clone), id(kernel_cloned))
-            yield check_hyperparameters_equal, kernel_cloned, kernel_cloned_clone
+            yield (check_hyperparameters_equal, kernel_cloned,
+                   kernel_cloned_clone)
 
 
 def test_matern_kernel():
-    """ Test consistency of Matern kernel for special values of nu. """
+    # Test consistency of Matern kernel for special values of nu.
     K = Matern(nu=1.5, length_scale=1.0)(X)
     # the diagonal elements of a matern kernel are 1
     assert_array_almost_equal(np.diag(K), np.ones(X.shape[0]))
@@ -255,7 +256,7 @@ def test_matern_kernel():
 
 
 def test_kernel_versus_pairwise():
-    """Check that GP kernels can also be used as pairwise kernels."""
+    # Check that GP kernels can also be used as pairwise kernels.
     for kernel in kernels:
         # Test auto-kernel
         if kernel != kernel_white:
@@ -272,17 +273,17 @@ def test_kernel_versus_pairwise():
 
 
 def test_set_get_params():
-    """Check that set_params()/get_params() is consistent with kernel.theta."""
+    # Check that set_params()/get_params() is consistent with kernel.theta.
     for kernel in kernels:
         # Test get_params()
         index = 0
         params = kernel.get_params()
         for hyperparameter in kernel.hyperparameters:
-            if hyperparameter.bounds is "fixed":
+            if hyperparameter.bounds == "fixed":
                 continue
             size = hyperparameter.n_elements
             if size > 1:  # anisotropic kernels
-                assert_almost_equal(np.exp(kernel.theta[index:index+size]),
+                assert_almost_equal(np.exp(kernel.theta[index:index + size]),
                                     params[hyperparameter.name])
                 index += size
             else:
@@ -293,13 +294,13 @@ def test_set_get_params():
         index = 0
         value = 10  # arbitrary value
         for hyperparameter in kernel.hyperparameters:
-            if hyperparameter.bounds is "fixed":
+            if hyperparameter.bounds == "fixed":
                 continue
             size = hyperparameter.n_elements
             if size > 1:  # anisotropic kernels
-                kernel.set_params(**{hyperparameter.name: [value]*size})
-                assert_almost_equal(np.exp(kernel.theta[index:index+size]),
-                                    [value]*size)
+                kernel.set_params(**{hyperparameter.name: [value] * size})
+                assert_almost_equal(np.exp(kernel.theta[index:index + size]),
+                                    [value] * size)
                 index += size
             else:
                 kernel.set_params(**{hyperparameter.name: value})
@@ -308,7 +309,7 @@ def test_set_get_params():
 
 
 def test_repr_kernels():
-    """Smoke-test for repr in kernels."""
+    # Smoke-test for repr in kernels.
 
     for kernel in kernels:
         repr(kernel)
diff --git a/sklearn/grid_search.py b/sklearn/grid_search.py
index 202327574ee7..0de08ee9e89f 100644
--- a/sklearn/grid_search.py
+++ b/sklearn/grid_search.py
@@ -426,7 +426,7 @@ def score(self, X, y=None):
                           ChangedBehaviorWarning)
         return self.scorer_(self.best_estimator_, X, y)
 
-    @if_delegate_has_method(delegate='estimator')
+    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
     def predict(self, X):
         """Call predict on the estimator with the best found parameters.
 
@@ -442,7 +442,7 @@ def predict(self, X):
         """
         return self.best_estimator_.predict(X)
 
-    @if_delegate_has_method(delegate='estimator')
+    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
     def predict_proba(self, X):
         """Call predict_proba on the estimator with the best found parameters.
 
@@ -458,7 +458,7 @@ def predict_proba(self, X):
         """
         return self.best_estimator_.predict_proba(X)
 
-    @if_delegate_has_method(delegate='estimator')
+    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
     def predict_log_proba(self, X):
         """Call predict_log_proba on the estimator with the best found parameters.
 
@@ -474,7 +474,7 @@ def predict_log_proba(self, X):
         """
         return self.best_estimator_.predict_log_proba(X)
 
-    @if_delegate_has_method(delegate='estimator')
+    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
     def decision_function(self, X):
         """Call decision_function on the estimator with the best found parameters.
 
@@ -490,7 +490,7 @@ def decision_function(self, X):
         """
         return self.best_estimator_.decision_function(X)
 
-    @if_delegate_has_method(delegate='estimator')
+    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
     def transform(self, X):
         """Call transform on the estimator with the best found parameters.
 
@@ -506,7 +506,7 @@ def transform(self, X):
         """
         return self.best_estimator_.transform(X)
 
-    @if_delegate_has_method(delegate='estimator')
+    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
     def inverse_transform(self, Xt):
         """Call inverse_transform on the estimator with the best found parameters.
 
diff --git a/sklearn/isotonic.py b/sklearn/isotonic.py
index 0585438e8708..62f9f0300b16 100644
--- a/sklearn/isotonic.py
+++ b/sklearn/isotonic.py
@@ -10,7 +10,7 @@
 from .utils import as_float_array, check_array, check_consistent_length
 from .utils import deprecated
 from .utils.fixes import astype
-from ._isotonic import _isotonic_regression, _make_unique
+from ._isotonic import _inplace_contiguous_isotonic_regression, _make_unique
 import warnings
 import math
 
@@ -120,28 +120,22 @@ def isotonic_regression(y, sample_weight=None, y_min=None, y_max=None,
     "Active set algorithms for isotonic regression; A unifying framework"
     by Michael J. Best and Nilotpal Chakravarti, section 3.
     """
-    y = np.asarray(y, dtype=np.float64)
+    order = np.s_[:] if increasing else np.s_[::-1]
+    y = np.array(y[order], dtype=np.float64)
     if sample_weight is None:
-        sample_weight = np.ones(len(y), dtype=y.dtype)
+        sample_weight = np.ones(len(y), dtype=np.float64)
     else:
-        sample_weight = np.asarray(sample_weight, dtype=np.float64)
-    if not increasing:
-        y = y[::-1]
-        sample_weight = sample_weight[::-1]
-
-    solution = np.empty(len(y))
-    y_ = _isotonic_regression(y, sample_weight, solution)
-    if not increasing:
-        y_ = y_[::-1]
+        sample_weight = np.array(sample_weight[order], dtype=np.float64)
 
+    _inplace_contiguous_isotonic_regression(y, sample_weight)
     if y_min is not None or y_max is not None:
         # Older versions of np.clip don't accept None as a bound, so use np.inf
         if y_min is None:
             y_min = -np.inf
         if y_max is None:
             y_max = np.inf
-        np.clip(y_, y_min, y_max, y_)
-    return y_
+        np.clip(y, y_min, y_max, y)
+    return y[order]
 
 
 class IsotonicRegression(BaseEstimator, TransformerMixin, RegressorMixin):
diff --git a/sklearn/kernel_ridge.py b/sklearn/kernel_ridge.py
index 682811cb610d..c782886c7376 100644
--- a/sklearn/kernel_ridge.py
+++ b/sklearn/kernel_ridge.py
@@ -69,8 +69,8 @@ class KernelRidge(BaseEstimator, RegressorMixin):
 
     Attributes
     ----------
-    dual_coef_ : array, shape = [n_features] or [n_targets, n_features]
-        Weight vector(s) in kernel space
+    dual_coef_ : array, shape = [n_samples] or [n_samples, n_targets]
+        Representation of weight vector(s) in kernel space
 
     X_fit_ : {array-like, sparse matrix}, shape = [n_samples, n_features]
         Training data, which is also required for prediction
diff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py
index 007bdedd72cb..b62275065d20 100644
--- a/sklearn/linear_model/logistic.py
+++ b/sklearn/linear_model/logistic.py
@@ -1075,12 +1075,18 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,
 
     Attributes
     ----------
-    coef_ : array, shape (n_classes, n_features)
+
+    coef_ : array, shape (1, n_features) or (n_classes, n_features)
         Coefficient of the features in the decision function.
 
-    intercept_ : array, shape (n_classes,)
+        `coef_` is of shape (1, n_features) when the given problem
+        is binary.
+
+    intercept_ : array, shape (1,) or (n_classes,)
         Intercept (a.k.a. bias) added to the decision function.
+
         If `fit_intercept` is set to False, the intercept is set to zero.
+        `intercept_` is of shape(1,) when the problem is binary.
 
     n_iter_ : array, shape (n_classes,) or (1, )
         Actual number of iterations for all classes. If binary or multinomial,
@@ -1451,13 +1457,12 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,
 
         `coef_` is of shape (1, n_features) when the given problem
         is binary.
-        `coef_` is readonly property derived from `raw_coef_` that
-        follows the internal memory layout of liblinear.
 
     intercept_ : array, shape (1,) or (n_classes,)
         Intercept (a.k.a. bias) added to the decision function.
-        It is available only when parameter intercept is set to True
-        and is of shape(1,) when the problem is binary.
+
+        If `fit_intercept` is set to False, the intercept is set to zero.
+        `intercept_` is of shape(1,) when the problem is binary.
 
     Cs_ : array
         Array of C i.e. inverse of regularization parameter values used
diff --git a/sklearn/metrics/classification.py b/sklearn/metrics/classification.py
index 2a7be716ee48..7821060a950e 100644
--- a/sklearn/metrics/classification.py
+++ b/sklearn/metrics/classification.py
@@ -186,6 +186,10 @@ def confusion_matrix(y_true, y_pred, labels=None, sample_weight=None):
     is equal to the number of observations known to be in group :math:`i` but
     predicted to be in group :math:`j`.
 
+    Thus in binary classification, the count of true negatives is
+    :math:`C_{0,0}`, false negatives is :math:`C_{1,0}`, true positives is
+    :math:`C_{1,1}` and false positives is :math:`C_{0,1}`.
+
     Read more in the :ref:`User Guide <confusion_matrix>`.
 
     Parameters
diff --git a/sklearn/metrics/cluster/expected_mutual_info_fast.pyx b/sklearn/metrics/cluster/expected_mutual_info_fast.pyx
index d0c08be8d238..ddb735be59f8 100644
--- a/sklearn/metrics/cluster/expected_mutual_info_fast.pyx
+++ b/sklearn/metrics/cluster/expected_mutual_info_fast.pyx
@@ -28,8 +28,8 @@ def expected_mutual_information(contingency, int n_samples):
     #cdef np.ndarray[int, ndim=2] start, end
     R, C = contingency.shape
     N = <DOUBLE>n_samples
-    a = np.sum(contingency, axis=1).astype(np.int32)
-    b = np.sum(contingency, axis=0).astype(np.int32)
+    a = np.ravel(contingency.sum(axis=1).astype(np.int32))
+    b = np.ravel(contingency.sum(axis=0).astype(np.int32))
     # There are three major terms to the EMI equation, which are multiplied to
     # and then summed over varying nij values.
     # While nijs[0] will never be used, having it simplifies the indexing.
diff --git a/sklearn/metrics/cluster/supervised.py b/sklearn/metrics/cluster/supervised.py
index 131c14b5078c..3fc3373558ef 100644
--- a/sklearn/metrics/cluster/supervised.py
+++ b/sklearn/metrics/cluster/supervised.py
@@ -9,16 +9,21 @@
 #          Diego Molla <dmolla-aliod@gmail.com>
 #          Arnaud Fouchet <foucheta@gmail.com>
 #          Thierry Guillemot <thierry.guillemot.work@gmail.com>
+#          Gregory Stupp <stuppie@gmail.com>
+#          Joel Nothman <joel.nothman@gmail.com>
 # License: BSD 3 clause
 
+from __future__ import division
+
 from math import log
 
-from scipy.misc import comb
-from scipy.sparse import coo_matrix
 import numpy as np
+from scipy.misc import comb
+from scipy import sparse as sp
 
 from .expected_mutual_info_fast import expected_mutual_information
 from ...utils.fixes import bincount
+from ...utils.validation import check_array
 
 
 def comb2(n):
@@ -46,7 +51,7 @@ def check_clusterings(labels_true, labels_pred):
     return labels_true, labels_pred
 
 
-def contingency_matrix(labels_true, labels_pred, eps=None, max_n_classes=5000):
+def contingency_matrix(labels_true, labels_pred, eps=None, sparse=False):
     """Build a contingency matrix describing the relationship between labels.
 
     Parameters
@@ -57,52 +62,55 @@ def contingency_matrix(labels_true, labels_pred, eps=None, max_n_classes=5000):
     labels_pred : array, shape = [n_samples]
         Cluster labels to evaluate
 
-    eps: None or float
+    eps : None or float, optional.
         If a float, that value is added to all values in the contingency
         matrix. This helps to stop NaN propagation.
         If ``None``, nothing is adjusted.
 
-    max_n_classes : int, optional (default=5000)
-        Maximal number of classeses handled for contingency_matrix.
-        This help to avoid Memory error with regression target
-        for mutual_information.
+    sparse : boolean, optional.
+        If True, return a sparse CSR continency matrix. If ``eps is not None``,
+        and ``sparse is True``, will throw ValueError.
+
+        .. versionadded:: 0.18
 
     Returns
     -------
-    contingency: array, shape=[n_classes_true, n_classes_pred]
+    contingency : {array-like, sparse}, shape=[n_classes_true, n_classes_pred]
         Matrix :math:`C` such that :math:`C_{i, j}` is the number of samples in
         true class :math:`i` and in predicted class :math:`j`. If
         ``eps is None``, the dtype of this array will be integer. If ``eps`` is
         given, the dtype will be float.
+        Will be a ``scipy.sparse.csr_matrix`` if ``sparse=True``.
     """
+
+    if eps is not None and sparse:
+        raise ValueError("Cannot set 'eps' when sparse=True")
+
     classes, class_idx = np.unique(labels_true, return_inverse=True)
     clusters, cluster_idx = np.unique(labels_pred, return_inverse=True)
     n_classes = classes.shape[0]
     n_clusters = clusters.shape[0]
-    if n_classes > max_n_classes:
-        raise ValueError("Too many classes for a clustering metric. If you "
-                         "want to increase the limit, pass parameter "
-                         "max_n_classes to the scoring function")
-    if n_clusters > max_n_classes:
-        raise ValueError("Too many clusters for a clustering metric. If you "
-                         "want to increase the limit, pass parameter "
-                         "max_n_classes to the scoring function")
     # Using coo_matrix to accelerate simple histogram calculation,
     # i.e. bins are consecutive integers
     # Currently, coo_matrix is faster than histogram2d for simple cases
-    contingency = coo_matrix((np.ones(class_idx.shape[0]),
-                              (class_idx, cluster_idx)),
-                             shape=(n_classes, n_clusters),
-                             dtype=np.int).toarray()
-    if eps is not None:
-        # don't use += as contingency is integer
-        contingency = contingency + eps
+    contingency = sp.coo_matrix((np.ones(class_idx.shape[0]),
+                                 (class_idx, cluster_idx)),
+                                shape=(n_classes, n_clusters),
+                                dtype=np.int)
+    if sparse:
+        contingency = contingency.tocsr()
+        contingency.sum_duplicates()
+    else:
+        contingency = contingency.toarray()
+        if eps is not None:
+            # don't use += as contingency is integer
+            contingency = contingency + eps
     return contingency
 
 
 # clustering measures
 
-def adjusted_rand_score(labels_true, labels_pred, max_n_classes=5000):
+def adjusted_rand_score(labels_true, labels_pred):
     """Rand index adjusted for chance.
 
     The Rand Index computes a similarity measure between two clusterings
@@ -134,11 +142,6 @@ def adjusted_rand_score(labels_true, labels_pred, max_n_classes=5000):
     labels_pred : array, shape = [n_samples]
         Cluster labels to evaluate
 
-    max_n_classes: int, optional (default=5000)
-        Maximal number of classes handled by the adjusted_rand_score
-        metric. Setting it too high can lead to MemoryError or OS
-        freeze
-
     Returns
     -------
     ari : float
@@ -190,31 +193,29 @@ def adjusted_rand_score(labels_true, labels_pred, max_n_classes=5000):
     """
     labels_true, labels_pred = check_clusterings(labels_true, labels_pred)
     n_samples = labels_true.shape[0]
-    classes = np.unique(labels_true)
-    clusters = np.unique(labels_pred)
+    n_classes = np.unique(labels_true).shape[0]
+    n_clusters = np.unique(labels_pred).shape[0]
+
     # Special limit cases: no clustering since the data is not split;
     # or trivial clustering where each document is assigned a unique cluster.
     # These are perfect matches hence return 1.0.
-    if (classes.shape[0] == clusters.shape[0] == 1 or
-            classes.shape[0] == clusters.shape[0] == 0 or
-            classes.shape[0] == clusters.shape[0] == len(labels_true)):
+    if (n_classes == n_clusters == 1 or
+            n_classes == n_clusters == 0 or
+            n_classes == n_clusters == n_samples):
         return 1.0
 
-    contingency = contingency_matrix(labels_true, labels_pred,
-                                     max_n_classes=max_n_classes)
-
     # Compute the ARI using the contingency data
-    sum_comb_c = sum(comb2(n_c) for n_c in contingency.sum(axis=1))
-    sum_comb_k = sum(comb2(n_k) for n_k in contingency.sum(axis=0))
+    contingency = contingency_matrix(labels_true, labels_pred, sparse=True)
+    sum_comb_c = sum(comb2(n_c) for n_c in np.ravel(contingency.sum(axis=1)))
+    sum_comb_k = sum(comb2(n_k) for n_k in np.ravel(contingency.sum(axis=0)))
+    sum_comb = sum(comb2(n_ij) for n_ij in contingency.data)
 
-    sum_comb = sum(comb2(n_ij) for n_ij in contingency.flatten())
-    prod_comb = (sum_comb_c * sum_comb_k) / float(comb(n_samples, 2))
+    prod_comb = (sum_comb_c * sum_comb_k) / comb(n_samples, 2)
     mean_comb = (sum_comb_k + sum_comb_c) / 2.
-    return ((sum_comb - prod_comb) / (mean_comb - prod_comb))
+    return (sum_comb - prod_comb) / (mean_comb - prod_comb)
 
 
-def homogeneity_completeness_v_measure(labels_true, labels_pred,
-                                       max_n_classes=5000):
+def homogeneity_completeness_v_measure(labels_true, labels_pred):
     """Compute the homogeneity and completeness and V-Measure scores at once.
 
     Those metrics are based on normalized conditional entropy measures of
@@ -248,11 +249,6 @@ def homogeneity_completeness_v_measure(labels_true, labels_pred,
     labels_pred : array, shape = [n_samples]
         cluster labels to evaluate
 
-    max_n_classes: int, optional (default=5000)
-        Maximal number of classes handled by the adjusted_rand_score
-        metric. Setting it too high can lead to MemoryError or OS
-        freeze
-
     Returns
     -------
     homogeneity: float
@@ -278,8 +274,8 @@ def homogeneity_completeness_v_measure(labels_true, labels_pred,
     entropy_C = entropy(labels_true)
     entropy_K = entropy(labels_pred)
 
-    MI = mutual_info_score(labels_true, labels_pred,
-                           max_n_classes=max_n_classes)
+    contingency = contingency_matrix(labels_true, labels_pred, sparse=True)
+    MI = mutual_info_score(None, None, contingency=contingency)
 
     homogeneity = MI / (entropy_C) if entropy_C else 1.0
     completeness = MI / (entropy_K) if entropy_K else 1.0
@@ -293,7 +289,7 @@ def homogeneity_completeness_v_measure(labels_true, labels_pred,
     return homogeneity, completeness, v_measure_score
 
 
-def homogeneity_score(labels_true, labels_pred, max_n_classes=5000):
+def homogeneity_score(labels_true, labels_pred):
     """Homogeneity metric of a cluster labeling given a ground truth.
 
     A clustering result satisfies homogeneity if all of its clusters
@@ -317,11 +313,6 @@ def homogeneity_score(labels_true, labels_pred, max_n_classes=5000):
     labels_pred : array, shape = [n_samples]
         cluster labels to evaluate
 
-    max_n_classes: int, optional (default=5000)
-        Maximal number of classes handled by the adjusted_rand_score
-        metric. Setting it too high can lead to MemoryError or OS
-        freeze
-
     Returns
     -------
     homogeneity: float
@@ -369,11 +360,10 @@ def homogeneity_score(labels_true, labels_pred, max_n_classes=5000):
       0.0...
 
     """
-    return homogeneity_completeness_v_measure(labels_true, labels_pred,
-                                              max_n_classes)[0]
+    return homogeneity_completeness_v_measure(labels_true, labels_pred)[0]
 
 
-def completeness_score(labels_true, labels_pred, max_n_classes=5000):
+def completeness_score(labels_true, labels_pred):
     """Completeness metric of a cluster labeling given a ground truth.
 
     A clustering result satisfies completeness if all the data points
@@ -397,11 +387,6 @@ def completeness_score(labels_true, labels_pred, max_n_classes=5000):
     labels_pred : array, shape = [n_samples]
         cluster labels to evaluate
 
-    max_n_classes: int, optional (default=5000)
-        Maximal number of classes handled by the adjusted_rand_score
-        metric. Setting it too high can lead to MemoryError or OS
-        freeze
-
     Returns
     -------
     completeness: float
@@ -445,11 +430,10 @@ def completeness_score(labels_true, labels_pred, max_n_classes=5000):
       0.0
 
     """
-    return homogeneity_completeness_v_measure(labels_true, labels_pred,
-                                              max_n_classes)[1]
+    return homogeneity_completeness_v_measure(labels_true, labels_pred)[1]
 
 
-def v_measure_score(labels_true, labels_pred, max_n_classes=5000):
+def v_measure_score(labels_true, labels_pred):
     """V-measure cluster labeling given a ground truth.
 
     This score is identical to :func:`normalized_mutual_info_score`.
@@ -477,11 +461,6 @@ def v_measure_score(labels_true, labels_pred, max_n_classes=5000):
     labels_pred : array, shape = [n_samples]
         cluster labels to evaluate
 
-    max_n_classes: int, optional (default=5000)
-        Maximal number of classes handled by the adjusted_rand_score
-        metric. Setting it too high can lead to MemoryError or OS
-        freeze
-
     Returns
     -------
     v_measure: float
@@ -546,12 +525,10 @@ def v_measure_score(labels_true, labels_pred, max_n_classes=5000):
       0.0...
 
     """
-    return homogeneity_completeness_v_measure(labels_true, labels_pred,
-                                              max_n_classes)[2]
+    return homogeneity_completeness_v_measure(labels_true, labels_pred)[2]
 
 
-def mutual_info_score(labels_true, labels_pred, contingency=None,
-                      max_n_classes=5000):
+def mutual_info_score(labels_true, labels_pred, contingency=None):
     """Mutual Information between two clusterings.
 
     The Mutual Information is a measure of the similarity between two labels of
@@ -586,16 +563,12 @@ def mutual_info_score(labels_true, labels_pred, contingency=None,
     labels_pred : array, shape = [n_samples]
         A clustering of the data into disjoint subsets.
 
-    contingency: None or array, shape = [n_classes_true, n_classes_pred]
+    contingency : {None, array, sparse matrix},
+                  shape = [n_classes_true, n_classes_pred]
         A contingency matrix given by the :func:`contingency_matrix` function.
         If value is ``None``, it will be computed, otherwise the given value is
         used, with ``labels_true`` and ``labels_pred`` ignored.
 
-    max_n_classes: int, optional (default=5000)
-        Maximal number of classes handled by the mutual_info_score
-        metric. Setting it too high can lead to MemoryError or OS
-        freeze
-
     Returns
     -------
     mi: float
@@ -608,27 +581,37 @@ def mutual_info_score(labels_true, labels_pred, contingency=None,
     """
     if contingency is None:
         labels_true, labels_pred = check_clusterings(labels_true, labels_pred)
-        contingency = contingency_matrix(labels_true, labels_pred,
-                                         max_n_classes=max_n_classes)
-    contingency = np.array(contingency, dtype='float')
-    contingency_sum = np.sum(contingency)
-    pi = np.sum(contingency, axis=1)
-    pj = np.sum(contingency, axis=0)
-    outer = np.outer(pi, pj)
-    nnz = contingency != 0.0
-    # normalized contingency
-    contingency_nm = contingency[nnz]
-    log_contingency_nm = np.log(contingency_nm)
-    contingency_nm /= contingency_sum
-    # log(a / b) should be calculated as log(a) - log(b) for
-    # possible loss of precision
-    log_outer = -np.log(outer[nnz]) + log(pi.sum()) + log(pj.sum())
+        contingency = contingency_matrix(labels_true, labels_pred, sparse=True)
+    else:
+        contingency = check_array(contingency,
+                                  accept_sparse=['csr', 'csc', 'coo'],
+                                  dtype=[int, np.int32, np.int64])
+
+    if isinstance(contingency, np.ndarray):
+        # For an array
+        nzx, nzy = np.nonzero(contingency)
+        nz_val = contingency[nzx, nzy]
+    elif sp.issparse(contingency):
+        # For a sparse matrix
+        nzx, nzy, nz_val = sp.find(contingency)
+    else:
+        raise ValueError("Unsupported type for 'contingency': %s" %
+                         type(contingency))
+
+    contingency_sum = contingency.sum()
+    pi = np.ravel(contingency.sum(axis=1))
+    pj = np.ravel(contingency.sum(axis=0))
+    log_contingency_nm = np.log(nz_val)
+    contingency_nm = nz_val / contingency_sum
+    # Don't need to calculate the full outer product, just for non-zeroes
+    outer = pi.take(nzx) * pj.take(nzy)
+    log_outer = -np.log(outer) + log(pi.sum()) + log(pj.sum())
     mi = (contingency_nm * (log_contingency_nm - log(contingency_sum)) +
           contingency_nm * log_outer)
     return mi.sum()
 
 
-def adjusted_mutual_info_score(labels_true, labels_pred, max_n_classes=5000):
+def adjusted_mutual_info_score(labels_true, labels_pred):
     """Adjusted Mutual Information between two clusterings.
 
     Adjusted Mutual Information (AMI) is an adjustment of the Mutual
@@ -661,11 +644,6 @@ def adjusted_mutual_info_score(labels_true, labels_pred, max_n_classes=5000):
     labels_pred : array, shape = [n_samples]
         A clustering of the data into disjoint subsets.
 
-    max_n_classes: int, optional (default=5000)
-        Maximal number of classes handled by the adjusted_rand_score
-        metric. Setting it too high can lead to MemoryError or OS
-        freeze
-
     Returns
     -------
     ami: float(upperlimited by 1.0)
@@ -716,9 +694,8 @@ def adjusted_mutual_info_score(labels_true, labels_pred, max_n_classes=5000):
     if (classes.shape[0] == clusters.shape[0] == 1 or
             classes.shape[0] == clusters.shape[0] == 0):
         return 1.0
-    contingency = contingency_matrix(labels_true, labels_pred,
-                                     max_n_classes=max_n_classes)
-    contingency = np.array(contingency, dtype='float')
+    contingency = contingency_matrix(labels_true, labels_pred, sparse=True)
+    contingency = contingency.astype(np.float64)
     # Calculate the MI for the two clusterings
     mi = mutual_info_score(labels_true, labels_pred,
                            contingency=contingency)
@@ -730,7 +707,7 @@ def adjusted_mutual_info_score(labels_true, labels_pred, max_n_classes=5000):
     return ami
 
 
-def normalized_mutual_info_score(labels_true, labels_pred, max_n_classes=5000):
+def normalized_mutual_info_score(labels_true, labels_pred):
     """Normalized Mutual Information between two clusterings.
 
     Normalized Mutual Information (NMI) is an normalization of the Mutual
@@ -760,11 +737,6 @@ def normalized_mutual_info_score(labels_true, labels_pred, max_n_classes=5000):
     labels_pred : array, shape = [n_samples]
         A clustering of the data into disjoint subsets.
 
-    max_n_classes: int, optional (default=5000)
-        Maximal number of classes handled by the adjusted_rand_score
-        metric. Setting it too high can lead to MemoryError or OS
-        freeze
-
     Returns
     -------
     nmi: float
@@ -803,9 +775,8 @@ def normalized_mutual_info_score(labels_true, labels_pred, max_n_classes=5000):
     if (classes.shape[0] == clusters.shape[0] == 1 or
             classes.shape[0] == clusters.shape[0] == 0):
         return 1.0
-    contingency = contingency_matrix(labels_true, labels_pred,
-                                     max_n_classes=max_n_classes)
-    contingency = np.array(contingency, dtype='float')
+    contingency = contingency_matrix(labels_true, labels_pred, sparse=True)
+    contingency = contingency.astype(np.float64)
     # Calculate the MI for the two clusterings
     mi = mutual_info_score(labels_true, labels_pred,
                            contingency=contingency)
@@ -816,7 +787,7 @@ def normalized_mutual_info_score(labels_true, labels_pred, max_n_classes=5000):
     return nmi
 
 
-def fowlkes_mallows_score(labels_true, labels_pred, max_n_classes=5000):
+def fowlkes_mallows_score(labels_true, labels_pred, sparse=False):
     """Measure the similarity of two clusterings of a set of points.
 
     The Fowlkes-Mallows index (FMI) is defined as the geometric mean between of
@@ -845,11 +816,6 @@ def fowlkes_mallows_score(labels_true, labels_pred, max_n_classes=5000):
     labels_pred : array, shape = (``n_samples``, )
         A clustering of the data into disjoint subsets.
 
-    max_n_classes : int, optional (default=5000)
-        Maximal number of classes handled by the Fowlkes-Mallows
-        metric. Setting it too high can lead to MemoryError or OS
-        freeze
-
     Returns
     -------
     score : float
@@ -883,15 +849,13 @@ def fowlkes_mallows_score(labels_true, labels_pred, max_n_classes=5000):
     .. [2] `Wikipedia entry for the Fowlkes-Mallows Index
            <https://en.wikipedia.org/wiki/Fowlkes-Mallows_index>`_
     """
-    labels_true, labels_pred = check_clusterings(labels_true, labels_pred,)
+    labels_true, labels_pred = check_clusterings(labels_true, labels_pred)
     n_samples, = labels_true.shape
 
-    c = contingency_matrix(labels_true, labels_pred,
-                           max_n_classes=max_n_classes)
-    tk = np.dot(c.ravel(), c.ravel()) - n_samples
-    pk = np.sum(np.sum(c, axis=0) ** 2) - n_samples
-    qk = np.sum(np.sum(c, axis=1) ** 2) - n_samples
-
+    c = contingency_matrix(labels_true, labels_pred, sparse=True)
+    tk = np.dot(c.data, c.data) - n_samples
+    pk = np.sum(np.asarray(c.sum(axis=0)).ravel() ** 2) - n_samples
+    qk = np.sum(np.asarray(c.sum(axis=1)).ravel() ** 2) - n_samples
     return tk / np.sqrt(pk * qk) if tk != 0. else 0.
 
 
@@ -900,7 +864,7 @@ def entropy(labels):
     if len(labels) == 0:
         return 1.0
     label_idx = np.unique(labels, return_inverse=True)[1]
-    pi = bincount(label_idx).astype(np.float)
+    pi = bincount(label_idx).astype(np.float64)
     pi = pi[pi > 0]
     pi_sum = np.sum(pi)
     # log(a / b) should be calculated as log(a) - log(b) for
diff --git a/sklearn/metrics/cluster/tests/test_supervised.py b/sklearn/metrics/cluster/tests/test_supervised.py
index 828c2c544574..b50f681fd148 100644
--- a/sklearn/metrics/cluster/tests/test_supervised.py
+++ b/sklearn/metrics/cluster/tests/test_supervised.py
@@ -1,23 +1,21 @@
 import numpy as np
+from nose.tools import assert_almost_equal
+from nose.tools import assert_equal
+from numpy.testing import assert_array_almost_equal
 
+from sklearn.metrics.cluster import adjusted_mutual_info_score
 from sklearn.metrics.cluster import adjusted_rand_score
-from sklearn.metrics.cluster import homogeneity_score
 from sklearn.metrics.cluster import completeness_score
-from sklearn.metrics.cluster import v_measure_score
-from sklearn.metrics.cluster import homogeneity_completeness_v_measure
-from sklearn.metrics.cluster import adjusted_mutual_info_score
-from sklearn.metrics.cluster import normalized_mutual_info_score
-from sklearn.metrics.cluster import mutual_info_score
-from sklearn.metrics.cluster import expected_mutual_information
 from sklearn.metrics.cluster import contingency_matrix
-from sklearn.metrics.cluster import fowlkes_mallows_score
 from sklearn.metrics.cluster import entropy
-
+from sklearn.metrics.cluster import expected_mutual_information
+from sklearn.metrics.cluster import fowlkes_mallows_score
+from sklearn.metrics.cluster import homogeneity_completeness_v_measure
+from sklearn.metrics.cluster import homogeneity_score
+from sklearn.metrics.cluster import mutual_info_score
+from sklearn.metrics.cluster import normalized_mutual_info_score
+from sklearn.metrics.cluster import v_measure_score
 from sklearn.utils.testing import assert_raise_message
-from nose.tools import assert_almost_equal
-from nose.tools import assert_equal
-from numpy.testing import assert_array_almost_equal
-
 
 score_funcs = [
     adjusted_rand_score,
@@ -141,9 +139,16 @@ def test_adjusted_mutual_info_score():
     # Mutual information
     mi = mutual_info_score(labels_a, labels_b)
     assert_almost_equal(mi, 0.41022, 5)
-    # Expected mutual information
+    # with provided sparse contingency
+    C = contingency_matrix(labels_a, labels_b, sparse=True)
+    mi = mutual_info_score(labels_a, labels_b, contingency=C)
+    assert_almost_equal(mi, 0.41022, 5)
+    # with provided dense contingency
     C = contingency_matrix(labels_a, labels_b)
-    n_samples = np.sum(C)
+    mi = mutual_info_score(labels_a, labels_b, contingency=C)
+    assert_almost_equal(mi, 0.41022, 5)
+    # Expected mutual information
+    n_samples = C.sum()
     emi = expected_mutual_information(C, n_samples)
     assert_almost_equal(emi, 0.15042, 5)
     # Adjusted mutual information
@@ -183,55 +188,40 @@ def test_contingency_matrix():
     assert_array_almost_equal(C, C2 + .1)
 
 
+def test_contingency_matrix_sparse():
+    labels_a = np.array([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3])
+    labels_b = np.array([1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 3, 1, 3, 3, 3, 2, 2])
+    C = contingency_matrix(labels_a, labels_b)
+    C_sparse = contingency_matrix(labels_a, labels_b, sparse=True).toarray()
+    assert_array_almost_equal(C, C_sparse)
+    C_sparse = assert_raise_message(ValueError,
+                                    "Cannot set 'eps' when sparse=True",
+                                    contingency_matrix, labels_a, labels_b,
+                                    eps=1e-10, sparse=True)
+
+
 def test_exactly_zero_info_score():
     # Check numerical stability when information is exactly zero
     for i in np.logspace(1, 4, 4).astype(np.int):
-        labels_a, labels_b = np.ones(i, dtype=np.int),\
-            np.arange(i, dtype=np.int)
-        assert_equal(normalized_mutual_info_score(labels_a, labels_b,
-                                                  max_n_classes=1e4), 0.0)
-        assert_equal(v_measure_score(labels_a, labels_b,
-                                     max_n_classes=1e4), 0.0)
-        assert_equal(adjusted_mutual_info_score(labels_a, labels_b,
-                                                max_n_classes=1e4), 0.0)
-        assert_equal(normalized_mutual_info_score(labels_a, labels_b,
-                                                  max_n_classes=1e4), 0.0)
+        labels_a, labels_b = (np.ones(i, dtype=np.int),
+                              np.arange(i, dtype=np.int))
+        assert_equal(normalized_mutual_info_score(labels_a, labels_b), 0.0)
+        assert_equal(v_measure_score(labels_a, labels_b), 0.0)
+        assert_equal(adjusted_mutual_info_score(labels_a, labels_b), 0.0)
+        assert_equal(normalized_mutual_info_score(labels_a, labels_b), 0.0)
 
 
 def test_v_measure_and_mutual_information(seed=36):
     # Check relation between v_measure, entropy and mutual information
     for i in np.logspace(1, 4, 4).astype(np.int):
         random_state = np.random.RandomState(seed)
-        labels_a, labels_b = random_state.randint(0, 10, i),\
-            random_state.randint(0, 10, i)
+        labels_a, labels_b = (random_state.randint(0, 10, i),
+                              random_state.randint(0, 10, i))
         assert_almost_equal(v_measure_score(labels_a, labels_b),
                             2.0 * mutual_info_score(labels_a, labels_b) /
                             (entropy(labels_a) + entropy(labels_b)), 0)
 
 
-def test_max_n_classes():
-    rng = np.random.RandomState(seed=0)
-    labels_true = rng.rand(53)
-    labels_pred = rng.rand(53)
-    labels_zero = np.zeros(53)
-    labels_true[:2] = 0
-    labels_zero[:3] = 1
-    labels_pred[:2] = 0
-    for score_func in score_funcs:
-        expected = ("Too many classes for a clustering metric. If you "
-                    "want to increase the limit, pass parameter "
-                    "max_n_classes to the scoring function")
-        assert_raise_message(ValueError, expected, score_func,
-                             labels_true, labels_pred,
-                             max_n_classes=50)
-        expected = ("Too many clusters for a clustering metric. If you "
-                    "want to increase the limit, pass parameter "
-                    "max_n_classes to the scoring function")
-        assert_raise_message(ValueError, expected, score_func,
-                             labels_zero, labels_pred,
-                             max_n_classes=50)
-
-
 def test_fowlkes_mallows_score():
     # General case
     score = fowlkes_mallows_score([0, 0, 0, 1, 1, 1],
diff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py
index 94c3a07d9307..57fa24e89183 100644
--- a/sklearn/mixture/gaussian_mixture.py
+++ b/sklearn/mixture/gaussian_mixture.py
@@ -120,7 +120,7 @@ def _check_precisions(precisions, covariance_type, n_components, n_features):
     """
     precisions = check_array(precisions, dtype=[np.float64, np.float32],
                              ensure_2d=False,
-                             allow_nd=covariance_type is 'full')
+                             allow_nd=covariance_type == 'full')
 
     precisions_shape = {'full': (n_components, n_features, n_features),
                         'tied': (n_features, n_features),
@@ -321,7 +321,7 @@ def _compute_precision_cholesky(covariances, covariance_type):
             precisions_chol[k] = linalg.solve_triangular(cov_chol,
                                                          np.eye(n_features),
                                                          lower=True).T
-    elif covariance_type is 'tied':
+    elif covariance_type == 'tied':
         _, n_features = covariances.shape
         try:
             cov_chol = linalg.cholesky(covariances, lower=True)
@@ -633,11 +633,11 @@ def _initialize(self, X, resp):
             self.covariances_ = covariances
             self.precisions_cholesky_ = _compute_precision_cholesky(
                 covariances, self.covariance_type)
-        elif self.covariance_type is 'full':
+        elif self.covariance_type == 'full':
             self.precisions_cholesky_ = np.array(
                 [linalg.cholesky(prec_init, lower=True)
                  for prec_init in self.precisions_init])
-        elif self.covariance_type is 'tied':
+        elif self.covariance_type == 'tied':
             self.precisions_cholesky_ = linalg.cholesky(self.precisions_init,
                                                         lower=True)
         else:
@@ -686,12 +686,12 @@ def _set_parameters(self, params):
         # Attributes computation
         _, n_features = self.means_.shape
 
-        if self.covariance_type is 'full':
+        if self.covariance_type == 'full':
             self.precisions_ = np.empty(self.precisions_cholesky_.shape)
             for k, prec_chol in enumerate(self.precisions_cholesky_):
                 self.precisions_[k] = np.dot(prec_chol, prec_chol.T)
 
-        elif self.covariance_type is 'tied':
+        elif self.covariance_type == 'tied':
             self.precisions_ = np.dot(self.precisions_cholesky_,
                                       self.precisions_cholesky_.T)
         else:
diff --git a/sklearn/model_selection/_search.py b/sklearn/model_selection/_search.py
index f1880555df33..7c6344c02c85 100644
--- a/sklearn/model_selection/_search.py
+++ b/sklearn/model_selection/_search.py
@@ -426,7 +426,7 @@ def _check_is_fitted(self, method_name):
         else:
             check_is_fitted(self, 'best_estimator_')
 
-    @if_delegate_has_method(delegate='estimator')
+    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
     def predict(self, X):
         """Call predict on the estimator with the best found parameters.
 
@@ -443,7 +443,7 @@ def predict(self, X):
         self._check_is_fitted('predict')
         return self.best_estimator_.predict(X)
 
-    @if_delegate_has_method(delegate='estimator')
+    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
     def predict_proba(self, X):
         """Call predict_proba on the estimator with the best found parameters.
 
@@ -460,7 +460,7 @@ def predict_proba(self, X):
         self._check_is_fitted('predict_proba')
         return self.best_estimator_.predict_proba(X)
 
-    @if_delegate_has_method(delegate='estimator')
+    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
     def predict_log_proba(self, X):
         """Call predict_log_proba on the estimator with the best found parameters.
 
@@ -477,7 +477,7 @@ def predict_log_proba(self, X):
         self._check_is_fitted('predict_log_proba')
         return self.best_estimator_.predict_log_proba(X)
 
-    @if_delegate_has_method(delegate='estimator')
+    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
     def decision_function(self, X):
         """Call decision_function on the estimator with the best found parameters.
 
@@ -494,7 +494,7 @@ def decision_function(self, X):
         self._check_is_fitted('decision_function')
         return self.best_estimator_.decision_function(X)
 
-    @if_delegate_has_method(delegate='estimator')
+    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
     def transform(self, X):
         """Call transform on the estimator with the best found parameters.
 
@@ -511,7 +511,7 @@ def transform(self, X):
         self._check_is_fitted('transform')
         return self.best_estimator_.transform(X)
 
-    @if_delegate_has_method(delegate='estimator')
+    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
     def inverse_transform(self, Xt):
         """Call inverse_transform on the estimator with the best found params.
 
diff --git a/sklearn/model_selection/_validation.py b/sklearn/model_selection/_validation.py
index febf5634a380..bf1d70b0819c 100644
--- a/sklearn/model_selection/_validation.py
+++ b/sklearn/model_selection/_validation.py
@@ -27,39 +27,11 @@
 from ..metrics.scorer import check_scoring
 from ..exceptions import FitFailedWarning
 
-from ._split import KFold
-from ._split import GroupKFold
-from ._split import LeaveOneGroupOut
-from ._split import LeaveOneOut
-from ._split import LeavePGroupsOut
-from ._split import LeavePOut
-from ._split import ShuffleSplit
-from ._split import GroupShuffleSplit
-from ._split import StratifiedKFold
-from ._split import StratifiedShuffleSplit
-from ._split import PredefinedSplit
 from ._split import check_cv, _safe_split
 
 __all__ = ['cross_val_score', 'cross_val_predict', 'permutation_test_score',
            'learning_curve', 'validation_curve']
 
-ALL_CVS = {'KFold': KFold,
-           'GroupKFold': GroupKFold,
-           'LeaveOneGroupOut': LeaveOneGroupOut,
-           'LeaveOneOut': LeaveOneOut,
-           'LeavePGroupsOut': LeavePGroupsOut,
-           'LeavePOut': LeavePOut,
-           'ShuffleSplit': ShuffleSplit,
-           'GroupShuffleSplit': GroupShuffleSplit,
-           'StratifiedKFold': StratifiedKFold,
-           'StratifiedShuffleSplit': StratifiedShuffleSplit,
-           'PredefinedSplit': PredefinedSplit}
-
-GROUP_CVS = {'GroupKFold': GroupKFold,
-             'LeaveOneGroupOut': LeaveOneGroupOut,
-             'LeavePGroupsOut': LeavePGroupsOut,
-             'GroupShuffleSplit': GroupShuffleSplit}
-
 
 def cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv=None,
                     n_jobs=1, verbose=0, fit_params=None,
@@ -504,11 +476,11 @@ def _check_is_permutation(indices, n_samples):
     Returns
     -------
     is_partition : bool
-        True iff sorted(locs) is range(n)
+        True iff sorted(indices) is np.arange(n)
     """
     if len(indices) != n_samples:
         return False
-    hit = np.zeros(n_samples, bool)
+    hit = np.zeros(n_samples, dtype=bool)
     hit[indices] = True
     if not np.all(hit):
         return False
diff --git a/sklearn/model_selection/tests/test_search.py b/sklearn/model_selection/tests/test_search.py
index 141c1a21b46e..bb21a386d35b 100644
--- a/sklearn/model_selection/tests/test_search.py
+++ b/sklearn/model_selection/tests/test_search.py
@@ -58,6 +58,7 @@
 from sklearn.metrics import roc_auc_score
 from sklearn.preprocessing import Imputer
 from sklearn.pipeline import Pipeline
+from sklearn.linear_model import SGDClassifier
 
 
 # Neither of the following two estimators inherit from BaseEstimator,
@@ -967,11 +968,13 @@ def test_grid_search_failing_classifier():
                       refit=False, error_score=0.0)
     assert_warns(FitFailedWarning, gs.fit, X, y)
     n_candidates = len(gs.cv_results_['params'])
+
     # Ensure that grid scores were set to zero as required for those fits
     # that are expected to fail.
-    get_cand_scores = lambda i: np.array(list(
-        gs.cv_results_['split%d_test_score' % s][i]
-        for s in range(gs.n_splits_)))
+    def get_cand_scores(i):
+        return np.array(list(gs.cv_results_['split%d_test_score' % s][i]
+                             for s in range(gs.n_splits_)))
+
     assert all((np.all(get_cand_scores(cand_i) == 0.0)
                 for cand_i in range(n_candidates)
                 if gs.cv_results_['param_parameter'][cand_i] ==
@@ -1028,3 +1031,33 @@ def test_parameters_sampler_replacement():
     sampler = ParameterSampler(params_distribution, n_iter=7)
     samples = list(sampler)
     assert_equal(len(samples), 7)
+
+
+def test_stochastic_gradient_loss_param():
+    # Make sure the predict_proba works when loss is specified
+    # as one of the parameters in the param_grid.
+    param_grid = {
+        'loss': ['log'],
+    }
+    X = np.arange(20).reshape(5, -1)
+    y = [0, 0, 1, 1, 1]
+    clf = GridSearchCV(estimator=SGDClassifier(loss='hinge'),
+                       param_grid=param_grid)
+
+    # When the estimator is not fitted, `predict_proba` is not available as the
+    # loss is 'hinge'.
+    assert_false(hasattr(clf, "predict_proba"))
+    clf.fit(X, y)
+    clf.predict_proba(X)
+    clf.predict_log_proba(X)
+
+    # Make sure `predict_proba` is not available when setting loss=['hinge']
+    # in param_grid
+    param_grid = {
+        'loss': ['hinge'],
+    }
+    clf = GridSearchCV(estimator=SGDClassifier(loss='hinge'),
+                       param_grid=param_grid)
+    assert_false(hasattr(clf, "predict_proba"))
+    clf.fit(X, y)
+    assert_false(hasattr(clf, "predict_proba"))
diff --git a/sklearn/model_selection/tests/test_validation.py b/sklearn/model_selection/tests/test_validation.py
index 67937711ec2d..4f45a079fbd7 100644
--- a/sklearn/model_selection/tests/test_validation.py
+++ b/sklearn/model_selection/tests/test_validation.py
@@ -731,13 +731,18 @@ def test_validation_curve():
 
 
 def test_check_is_permutation():
+    rng = np.random.RandomState(0)
     p = np.arange(100)
+    rng.shuffle(p)
     assert_true(_check_is_permutation(p, 100))
     assert_false(_check_is_permutation(np.delete(p, 23), 100))
 
     p[0] = 23
     assert_false(_check_is_permutation(p, 100))
 
+    # Check if the additional duplicate indices are caught
+    assert_false(_check_is_permutation(np.hstack((p, 0)), 100))
+
 
 def test_cross_val_predict_sparse_prediction():
     # check that cross_val_predict gives same result for sparse and dense input
diff --git a/sklearn/neighbors/base.py b/sklearn/neighbors/base.py
index 164543333fb4..a2339aeb2d38 100644
--- a/sklearn/neighbors/base.py
+++ b/sklearn/neighbors/base.py
@@ -583,11 +583,12 @@ class from an array representing our data set and ask who's
             # for efficiency, use squared euclidean distances
             if self.effective_metric_ == 'euclidean':
                 dist = pairwise_distances(X, self._fit_X, 'euclidean',
-                                          squared=True)
+                                          n_jobs=self.n_jobs, squared=True)
                 radius *= radius
             else:
                 dist = pairwise_distances(X, self._fit_X,
                                           self.effective_metric_,
+                                          n_jobs=self.n_jobs,
                                           **self.effective_metric_params_)
 
             neigh_ind_list = [np.where(d <= radius)[0] for d in dist]
diff --git a/sklearn/neural_network/multilayer_perceptron.py b/sklearn/neural_network/multilayer_perceptron.py
index 72562aafce02..87ea951533eb 100644
--- a/sklearn/neural_network/multilayer_perceptron.py
+++ b/sklearn/neural_network/multilayer_perceptron.py
@@ -13,7 +13,6 @@
 import warnings
 
 from ..base import BaseEstimator, ClassifierMixin, RegressorMixin
-from ._base import logistic, softmax
 from ._base import ACTIVATIONS, DERIVATIVES, LOSS_FUNCTIONS
 from ._stochastic_optimizers import SGDOptimizer, AdamOptimizer
 from ..model_selection import train_test_split
@@ -25,10 +24,11 @@
 from ..exceptions import ConvergenceWarning
 from ..utils.extmath import safe_sparse_dot
 from ..utils.validation import check_is_fitted
-from ..utils.multiclass import _check_partial_fit_first_call
+from ..utils.multiclass import _check_partial_fit_first_call, unique_labels
+from ..utils.multiclass import type_of_target
 
 
-_STOCHASTIC_ALGOS = ['sgd', 'adam']
+_STOCHASTIC_SOLVERS = ['sgd', 'adam']
 
 
 def _pack(coefs_, intercepts_):
@@ -44,13 +44,13 @@ class BaseMultilayerPerceptron(six.with_metaclass(ABCMeta, BaseEstimator)):
     """
 
     @abstractmethod
-    def __init__(self, hidden_layer_sizes, activation, algorithm,
+    def __init__(self, hidden_layer_sizes, activation, solver,
                  alpha, batch_size, learning_rate, learning_rate_init, power_t,
                  max_iter, loss, shuffle, random_state, tol, verbose,
                  warm_start, momentum, nesterovs_momentum, early_stopping,
                  validation_fraction, beta_1, beta_2, epsilon):
         self.activation = activation
-        self.algorithm = algorithm
+        self.solver = solver
         self.alpha = alpha
         self.batch_size = batch_size
         self.learning_rate = learning_rate
@@ -81,7 +81,7 @@ def _unpack(self, packed_parameters):
             start, end = self._intercept_indptr[i]
             self.intercepts_[i] = packed_parameters[start:end]
 
-    def _forward_pass(self, activations, with_output_activation=True):
+    def _forward_pass(self, activations):
         """Perform a forward pass on the network by computing the values
         of the neurons in the hidden layers and the output layer.
 
@@ -107,9 +107,8 @@ def _forward_pass(self, activations, with_output_activation=True):
                 activations[i + 1] = hidden_activation(activations[i + 1])
 
         # For the last layer
-        if with_output_activation:
-            output_activation = ACTIVATIONS[self.out_activation_]
-            activations[i + 1] = output_activation(activations[i + 1])
+        output_activation = ACTIVATIONS[self.out_activation_]
+        activations[i + 1] = output_activation(activations[i + 1])
 
         return activations
 
@@ -135,7 +134,7 @@ def _loss_grad_lbfgs(self, packed_coef_inter, X, y, activations, deltas,
         with respect to the different parameters given in the initialization.
 
         Returned gradients are packed in a single vector so it can be used
-        in l-bfgs
+        in lbgfs
 
         Parameters
         ----------
@@ -222,7 +221,10 @@ def _backprop(self, X, y, activations, deltas, coef_grads,
         activations = self._forward_pass(activations)
 
         # Get loss
-        loss = LOSS_FUNCTIONS[self.loss](y, activations[-1])
+        loss_func_name = self.loss
+        if loss_func_name == 'log_loss' and self.out_activation_ == 'logistic':
+            loss_func_name = 'binary_log_loss'
+        loss = LOSS_FUNCTIONS[loss_func_name](y, activations[-1])
         # Add L2 regularization term to loss
         values = np.sum(
             np.array([np.dot(s.ravel(), s.ravel()) for s in self.coefs_]))
@@ -267,27 +269,23 @@ def _initialize(self, y, layer_units):
         if not isinstance(self, ClassifierMixin):
             self.out_activation_ = 'identity'
         # Output for multi class
-        elif self.label_binarizer_.y_type_ == 'multiclass':
+        elif self._label_binarizer.y_type_ == 'multiclass':
             self.out_activation_ = 'softmax'
         # Output for binary class and multi-label
         else:
             self.out_activation_ = 'logistic'
-            if self.loss == 'log_loss':
-                self.loss = 'binary_log_loss'
 
         # Initialize coefficient and intercept layers
         self.coefs_ = []
         self.intercepts_ = []
 
         for i in range(self.n_layers_ - 1):
-            rng = check_random_state(self.random_state)
             coef_init, intercept_init = self._init_coef(layer_units[i],
-                                                        layer_units[i + 1],
-                                                        rng)
+                                                        layer_units[i + 1])
             self.coefs_.append(coef_init)
             self.intercepts_.append(intercept_init)
 
-        if self.algorithm in _STOCHASTIC_ALGOS:
+        if self.solver in _STOCHASTIC_SOLVERS:
             self.loss_curve_ = []
             self._no_improvement_count = 0
             if self.early_stopping:
@@ -296,7 +294,7 @@ def _initialize(self, y, layer_units):
             else:
                 self.best_loss_ = np.inf
 
-    def _init_coef(self, fan_in, fan_out, rng):
+    def _init_coef(self, fan_in, fan_out):
         if self.activation == 'logistic':
             # Use the initialization method recommended by
             # Glorot et al.
@@ -308,8 +306,10 @@ def _init_coef(self, fan_in, fan_out, rng):
             raise ValueError("Unknown activation function %s" %
                              self.activation)
 
-        coef_init = rng.uniform(-init_bound, init_bound, (fan_in, fan_out))
-        intercept_init = rng.uniform(-init_bound, init_bound, fan_out)
+        coef_init = self._random_state.uniform(-init_bound, init_bound,
+                                               (fan_in, fan_out))
+        intercept_init = self._random_state.uniform(-init_bound, init_bound,
+                                                    fan_out)
         return coef_init, intercept_init
 
     def _fit(self, X, y, incremental=False):
@@ -337,13 +337,16 @@ def _fit(self, X, y, incremental=False):
         layer_units = ([n_features] + hidden_layer_sizes +
                        [self.n_outputs_])
 
+        # check random state
+        self._random_state = check_random_state(self.random_state)
+
         if not hasattr(self, 'coefs_') or (not self.warm_start and not
                                            incremental):
             # First time training the model
             self._initialize(y, layer_units)
 
-        # l-bfgs does not support mini-batches
-        if self.algorithm == 'l-bfgs':
+        # lbgfs does not support mini-batches
+        if self.solver == 'lbgfs':
             batch_size = n_samples
         elif self.batch_size == 'auto':
             batch_size = min(200, n_samples)
@@ -366,13 +369,13 @@ def _fit(self, X, y, incremental=False):
         intercept_grads = [np.empty(n_fan_out_) for n_fan_out_ in
                            layer_units[1:]]
 
-        # Run the Stochastic optimization algorithm
-        if self.algorithm in _STOCHASTIC_ALGOS:
+        # Run the Stochastic optimization solver
+        if self.solver in _STOCHASTIC_SOLVERS:
             self._fit_stochastic(X, y, activations, deltas, coef_grads,
                                  intercept_grads, layer_units, incremental)
 
-        # Run the LBFGS algorithm
-        elif self.algorithm == 'l-bfgs':
+        # Run the LBFGS solver
+        elif self.solver == 'lbgfs':
             self._fit_lbfgs(X, y, activations, deltas, coef_grads,
                             intercept_grads, layer_units)
         return self
@@ -419,9 +422,11 @@ def _validate_hyperparameters(self):
         if self.learning_rate not in ["constant", "invscaling", "adaptive"]:
             raise ValueError("learning rate %s is not supported. " %
                              self.learning_rate)
-        if self.algorithm not in _STOCHASTIC_ALGOS + ["l-bfgs"]:
-            raise ValueError("The algorithm %s is not supported. " %
-                             self.algorithm)
+        supported_solvers = _STOCHASTIC_SOLVERS + ["lbgfs"]
+        if self.solver not in supported_solvers:
+            raise ValueError("The solver %s is not supported. "
+                             " Expected one of: %s" %
+                             (self.solver, ", ".join(supported_solvers)))
 
     def _fit_lbfgs(self, X, y, activations, deltas, coef_grads,
                    intercept_grads, layer_units):
@@ -465,16 +470,15 @@ def _fit_lbfgs(self, X, y, activations, deltas, coef_grads,
 
     def _fit_stochastic(self, X, y, activations, deltas, coef_grads,
                         intercept_grads, layer_units, incremental):
-        rng = check_random_state(self.random_state)
 
         if not incremental or not hasattr(self, '_optimizer'):
             params = self.coefs_ + self.intercepts_
 
-            if self.algorithm == 'sgd':
+            if self.solver == 'sgd':
                 self._optimizer = SGDOptimizer(
                     params, self.learning_rate_init, self.learning_rate,
                     self.momentum, self.nesterovs_momentum, self.power_t)
-            elif self.algorithm == 'adam':
+            elif self.solver == 'adam':
                 self._optimizer = AdamOptimizer(
                     params, self.learning_rate_init, self.beta_1, self.beta_2,
                     self.epsilon)
@@ -483,10 +487,10 @@ def _fit_stochastic(self, X, y, activations, deltas, coef_grads,
         early_stopping = self.early_stopping and not incremental
         if early_stopping:
             X, X_val, y, y_val = train_test_split(
-                X, y, random_state=self.random_state,
+                X, y, random_state=self._random_state,
                 test_size=self.validation_fraction)
             if isinstance(self, ClassifierMixin):
-                y_val = self.label_binarizer_.inverse_transform(y_val)
+                y_val = self._label_binarizer.inverse_transform(y_val)
         else:
             X_val = None
             y_val = None
@@ -500,7 +504,7 @@ def _fit_stochastic(self, X, y, activations, deltas, coef_grads,
 
         try:
             for it in range(self.max_iter):
-                X, y = shuffle(X, y, random_state=rng)
+                X, y = shuffle(X, y, random_state=self._random_state)
                 accumulated_loss = 0.0
                 for batch_slice in gen_batches(n_samples, batch_size):
                     activations[0] = X[batch_slice]
@@ -627,16 +631,16 @@ def partial_fit(self):
         -------
         self : returns a trained MLP model.
         """
-        if self.algorithm not in _STOCHASTIC_ALGOS:
+        if self.solver not in _STOCHASTIC_SOLVERS:
             raise AttributeError("partial_fit is only available for stochastic"
-                                 "optimization algorithms. %s is not"
-                                 " stochastic" % self.algorithm)
+                                 " optimizers. %s is not stochastic."
+                                 % self.solver)
         return self._partial_fit
 
     def _partial_fit(self, X, y, classes=None):
         return self._fit(X, y, incremental=True)
 
-    def _decision_scores(self, X):
+    def _predict(self, X):
         """Predict using the trained model
 
         Parameters
@@ -667,7 +671,7 @@ def _decision_scores(self, X):
             activations.append(np.empty((X.shape[0],
                                          layer_units[i + 1])))
         # forward propagate
-        self._forward_pass(activations, with_output_activation=False)
+        self._forward_pass(activations)
         y_pred = activations[-1]
 
         return y_pred
@@ -676,8 +680,8 @@ def _decision_scores(self, X):
 class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
     """Multi-layer Perceptron classifier.
 
-    This algorithm optimizes the log-loss function using l-bfgs or gradient
-    descent.
+    This model optimizes the log-loss function using LBFGS or stochastic
+    gradient descent.
 
     Parameters
     ----------
@@ -700,21 +704,20 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
         - 'relu', the rectified linear unit function,
           returns f(x) = max(0, x)
 
-    algorithm : {'l-bfgs', 'sgd', 'adam'}, default 'adam'
-        The algorithm for weight optimization.
+    solver : {'lbgfs', 'sgd', 'adam'}, default 'adam'
+        The solver for weight optimization.
 
-        - 'l-bfgs' is an optimization algorithm in the family of
-          quasi-Newton methods.
+        - 'lbgfs' is an optimizer in the family of quasi-Newton methods.
 
         - 'sgd' refers to stochastic gradient descent.
 
-        - 'adam' refers to a stochastic gradient-based optimization algorithm
-          proposed by Kingma, Diederik, and Jimmy Ba
+        - 'adam' refers to a stochastic gradient-based optimizer proposed
+          by Kingma, Diederik, and Jimmy Ba
 
-        Note: The default algorithm 'adam' works pretty well on relatively
+        Note: The default solver 'adam' works pretty well on relatively
         large datasets (with thousands of training samples or more) in terms of
         both training time and validation score.
-        For small datasets, however, 'l-bfgs' can converge faster and perform
+        For small datasets, however, 'lbgfs' can converge faster and perform
         better.
 
     alpha : float, optional, default 0.0001
@@ -722,7 +725,7 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
 
     batch_size : int, optional, default 'auto'
         Size of minibatches for stochastic optimizers.
-        If the algorithm is 'l-bfgs', the classifier will not use minibatch.
+        If the solver is 'lbgfs', the classifier will not use minibatch.
         When set to "auto", `batch_size=min(200, n_samples)`
 
     learning_rate : {'constant', 'invscaling', 'adaptive'}, default 'constant'
@@ -741,10 +744,10 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
           least tol, or fail to increase validation score by at least tol if
           'early_stopping' is on, the current learning rate is divided by 5.
 
-        Only used when ``algorithm='sgd'``.
+        Only used when ``solver='sgd'``.
 
     max_iter : int, optional, default 200
-        Maximum number of iterations. The algorithm iterates until convergence
+        Maximum number of iterations. The solver iterates until convergence
         (determined by 'tol') or this number of iterations.
 
     random_state : int or RandomState, optional, default None
@@ -752,7 +755,7 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
 
     shuffle : bool, optional, default True
         Whether to shuffle samples in each iteration. Only used when
-        algorithm='sgd' or 'adam'.
+        solver='sgd' or 'adam'.
 
     tol : float, optional, default 1e-4
         Tolerance for the optimization. When the loss or score is not improving
@@ -762,12 +765,12 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
 
     learning_rate_init : double, optional, default 0.001
         The initial learning rate used. It controls the step-size
-        in updating the weights. Only used when algorithm='sgd' or 'adam'.
+        in updating the weights. Only used when solver='sgd' or 'adam'.
 
     power_t : double, optional, default 0.5
         The exponent for inverse scaling learning rate.
         It is used in updating effective learning rate when the learning_rate
-        is set to 'invscaling'. Only used when algorithm='sgd'.
+        is set to 'invscaling'. Only used when solver='sgd'.
 
     verbose : bool, optional, default False
         Whether to print progress messages to stdout.
@@ -779,10 +782,10 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
 
     momentum : float, default 0.9
         Momentum for gradient descent update. Should be between 0 and 1. Only
-        used when algorithm='sgd'.
+        used when solver='sgd'.
 
     nesterovs_momentum : boolean, default True
-        Whether to use Nesterov's momentum. Only used when algorithm='sgd' and
+        Whether to use Nesterov's momentum. Only used when solver='sgd' and
         momentum > 0.
 
     early_stopping : bool, default False
@@ -791,7 +794,7 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
         aside 10% of training data as validation and terminate training when
         validation score is not improving by at least tol for two consecutive
         epochs.
-        Only effective when algorithm='sgd' or 'adam'
+        Only effective when solver='sgd' or 'adam'
 
     validation_fraction : float, optional, default 0.1
         The proportion of training data to set aside as validation set for
@@ -800,14 +803,14 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
 
     beta_1 : float, optional, default 0.9
         Exponential decay rate for estimates of first moment vector in adam,
-        should be in [0, 1). Only used when algorithm='adam'
+        should be in [0, 1). Only used when solver='adam'
 
     beta_2 : float, optional, default 0.999
         Exponential decay rate for estimates of second moment vector in adam,
-        should be in [0, 1). Only used when algorithm='adam'
+        should be in [0, 1). Only used when solver='adam'
 
     epsilon : float, optional, default 1e-8
-        Value for numerical stability in adam. Only used when algorithm='adam'
+        Value for numerical stability in adam. Only used when solver='adam'
 
     Attributes
     ----------
@@ -817,9 +820,6 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
     `loss_` : float
         The current loss computed with the loss function.
 
-    `label_binarizer_` : LabelBinarizer
-        A LabelBinarizer object trained on the training set.
-
     `coefs_` : list, length n_layers - 1
         The ith element in the list represents the weight matrix corresponding
         to layer i.
@@ -829,7 +829,7 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
         layer i + 1.
 
     n_iter_ : int,
-        The number of iterations the algorithm has ran.
+        The number of iterations the solver has ran.
 
     n_layers_ : int
         Number of layers.
@@ -870,7 +870,7 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
         optimization." arXiv preprint arXiv:1412.6980 (2014).
     """
     def __init__(self, hidden_layer_sizes=(100,), activation="relu",
-                 algorithm='adam', alpha=0.0001,
+                 solver='adam', alpha=0.0001,
                  batch_size='auto', learning_rate="constant",
                  learning_rate_init=0.001, power_t=0.5, max_iter=200,
                  shuffle=True, random_state=None, tol=1e-4,
@@ -881,7 +881,7 @@ def __init__(self, hidden_layer_sizes=(100,), activation="relu",
 
         sup = super(MLPClassifier, self)
         sup.__init__(hidden_layer_sizes=hidden_layer_sizes,
-                     activation=activation, algorithm=algorithm, alpha=alpha,
+                     activation=activation, solver=solver, alpha=alpha,
                      batch_size=batch_size, learning_rate=learning_rate,
                      learning_rate_init=learning_rate_init, power_t=power_t,
                      max_iter=max_iter, loss='log_loss', shuffle=shuffle,
@@ -892,48 +892,26 @@ def __init__(self, hidden_layer_sizes=(100,), activation="relu",
                      validation_fraction=validation_fraction,
                      beta_1=beta_1, beta_2=beta_2, epsilon=epsilon)
 
-        self.label_binarizer_ = LabelBinarizer()
-
     def _validate_input(self, X, y, incremental):
         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],
                          multi_output=True)
         if y.ndim == 2 and y.shape[1] == 1:
             y = column_or_1d(y, warn=True)
-        self.label_binarizer_.fit(y)
 
-        if not hasattr(self, 'classes_') or not incremental:
-            self.classes_ = self.label_binarizer_.classes_
+        if not incremental:
+            self._label_binarizer = LabelBinarizer()
+            self._label_binarizer.fit(y)
+            self.classes_ = self._label_binarizer.classes_
         else:
-            classes = self.label_binarizer_.classes_
-            if not np.all(np.in1d(classes, self.classes_)):
+            classes = unique_labels(y)
+            if np.setdiff1d(classes, self.classes_, assume_unique=True):
                 raise ValueError("`y` has classes not in `self.classes_`."
                                  " `self.classes_` has %s. 'y' has %s." %
                                  (self.classes_, classes))
 
-        y = self.label_binarizer_.transform(y)
+        y = self._label_binarizer.transform(y)
         return X, y
 
-    def decision_function(self, X):
-        """Decision function of the mlp model
-
-        Parameters
-        ----------
-        X : {array-like, sparse matrix}, shape (n_samples, n_features)
-            The input data.
-
-        Returns
-        -------
-        y : array-like, shape (n_samples,) or (n_samples, n_classes)
-            The values of decision function for each class in the model.
-        """
-        check_is_fitted(self, "coefs_")
-        y_scores = self._decision_scores(X)
-
-        if self.n_outputs_ == 1:
-            return y_scores.ravel()
-        else:
-            return y_scores
-
     def predict(self, X):
         """Predict using the multi-layer perceptron classifier
 
@@ -948,10 +926,12 @@ def predict(self, X):
             The predicted classes.
         """
         check_is_fitted(self, "coefs_")
-        y_scores = self.decision_function(X)
-        y_scores = ACTIVATIONS[self.out_activation_](y_scores)
+        y_pred = self._predict(X)
 
-        return self.label_binarizer_.inverse_transform(y_scores)
+        if self.n_outputs_ == 1:
+            y_pred = y_pred.ravel()
+
+        return self._label_binarizer.inverse_transform(y_pred)
 
     @property
     def partial_fit(self):
@@ -977,14 +957,19 @@ def partial_fit(self):
         -------
         self : returns a trained MLP model.
         """
-        if self.algorithm not in _STOCHASTIC_ALGOS:
+        if self.solver not in _STOCHASTIC_SOLVERS:
             raise AttributeError("partial_fit is only available for stochastic"
-                                 "optimization algorithms. %s is not"
-                                 " stochastic" % self.algorithm)
+                                 " optimizer. %s is not stochastic"
+                                 % self.solver)
         return self._partial_fit
 
     def _partial_fit(self, X, y, classes=None):
-        _check_partial_fit_first_call(self, classes)
+        if _check_partial_fit_first_call(self, classes):
+            self._label_binarizer = LabelBinarizer()
+            if type_of_target(y).startswith('multilabel'):
+                self._label_binarizer.fit(y)
+            else:
+                self._label_binarizer.fit(classes)
 
         super(MLPClassifier, self)._partial_fit(X, y)
 
@@ -1022,19 +1007,23 @@ def predict_proba(self, X):
             The predicted probability of the sample for each class in the
             model, where classes are ordered as they are in `self.classes_`.
         """
-        y_scores = self.decision_function(X)
+        check_is_fitted(self, "coefs_")
+        y_pred = self._predict(X)
 
-        if y_scores.ndim == 1:
-            y_scores = logistic(y_scores)
-            return np.vstack([1 - y_scores, y_scores]).T
+        if self.n_outputs_ == 1:
+            y_pred = y_pred.ravel()
+
+        if y_pred.ndim == 1:
+            return np.vstack([1 - y_pred, y_pred]).T
         else:
-            return softmax(y_scores)
+            return y_pred
 
 
 class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):
     """Multi-layer Perceptron regressor.
 
-    This algorithm optimizes the squared-loss using l-bfgs or gradient descent.
+    This model optimizes the squared-loss using LBFGS or stochastic gradient
+    descent.
 
     Parameters
     ----------
@@ -1057,21 +1046,20 @@ class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):
         - 'relu', the rectified linear unit function,
           returns f(x) = max(0, x)
 
-    algorithm : {'l-bfgs', 'sgd', 'adam'}, default 'adam'
-        The algorithm for weight optimization.
+    solver : {'lbgfs', 'sgd', 'adam'}, default 'adam'
+        The solver for weight optimization.
 
-        - 'l-bfgs' is an optimization algorithm in the family of
-          quasi-Newton methods.
+        - 'lbgfs' is an optimizer in the family of quasi-Newton methods.
 
         - 'sgd' refers to stochastic gradient descent.
 
-        - 'adam' refers to a stochastic gradient-based optimization algorithm
-          proposed by Kingma, Diederik, and Jimmy Ba
+        - 'adam' refers to a stochastic gradient-based optimizer proposed by
+          Kingma, Diederik, and Jimmy Ba
 
-        Note: The default algorithm 'adam' works pretty well on relatively
+        Note: The default solver 'adam' works pretty well on relatively
         large datasets (with thousands of training samples or more) in terms of
         both training time and validation score.
-        For small datasets, however, 'l-bfgs' can converge faster and perform
+        For small datasets, however, 'lbgfs' can converge faster and perform
         better.
 
     alpha : float, optional, default 0.0001
@@ -1079,7 +1067,7 @@ class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):
 
     batch_size : int, optional, default 'auto'
         Size of minibatches for stochastic optimizers.
-        If the algorithm is 'l-bfgs', the classifier will not use minibatch.
+        If the solver is 'lbgfs', the classifier will not use minibatch.
         When set to "auto", `batch_size=min(200, n_samples)`
 
     learning_rate : {'constant', 'invscaling', 'adaptive'}, default 'constant'
@@ -1098,10 +1086,10 @@ class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):
           least tol, or fail to increase validation score by at least tol if
           'early_stopping' is on, the current learning rate is divided by 5.
 
-        Only used when algorithm='sgd'.
+        Only used when solver='sgd'.
 
     max_iter : int, optional, default 200
-        Maximum number of iterations. The algorithm iterates until convergence
+        Maximum number of iterations. The solver iterates until convergence
         (determined by 'tol') or this number of iterations.
 
     random_state : int or RandomState, optional, default None
@@ -1109,7 +1097,7 @@ class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):
 
     shuffle : bool, optional, default True
         Whether to shuffle samples in each iteration. Only used when
-        algorithm='sgd' or 'adam'.
+        solver='sgd' or 'adam'.
 
     tol : float, optional, default 1e-4
         Tolerance for the optimization. When the loss or score is not improving
@@ -1119,12 +1107,12 @@ class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):
 
     learning_rate_init : double, optional, default 0.001
         The initial learning rate used. It controls the step-size
-        in updating the weights. Only used when algorithm='sgd' or 'adam'.
+        in updating the weights. Only used when solver='sgd' or 'adam'.
 
     power_t : double, optional, default 0.5
         The exponent for inverse scaling learning rate.
         It is used in updating effective learning rate when the learning_rate
-        is set to 'invscaling'. Only used when algorithm='sgd'.
+        is set to 'invscaling'. Only used when solver='sgd'.
 
     verbose : bool, optional, default False
         Whether to print progress messages to stdout.
@@ -1136,10 +1124,10 @@ class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):
 
     momentum : float, default 0.9
         Momentum for gradient descent update.  Should be between 0 and 1. Only
-        used when algorithm='sgd'.
+        used when solver='sgd'.
 
     nesterovs_momentum : boolean, default True
-        Whether to use Nesterov's momentum. Only used when algorithm='sgd' and
+        Whether to use Nesterov's momentum. Only used when solver='sgd' and
         momentum > 0.
 
     early_stopping : bool, default False
@@ -1148,7 +1136,7 @@ class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):
         aside 10% of training data as validation and terminate training when
         validation score is not improving by at least tol for two consecutive
         epochs.
-        Only effective when algorithm='sgd' or 'adam'
+        Only effective when solver='sgd' or 'adam'
 
     validation_fraction : float, optional, default 0.1
         The proportion of training data to set aside as validation set for
@@ -1157,14 +1145,14 @@ class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):
 
     beta_1 : float, optional, default 0.9
         Exponential decay rate for estimates of first moment vector in adam,
-        should be in [0, 1). Only used when algorithm='adam'
+        should be in [0, 1). Only used when solver='adam'
 
     beta_2 : float, optional, default 0.999
         Exponential decay rate for estimates of second moment vector in adam,
-        should be in [0, 1). Only used when algorithm='adam'
+        should be in [0, 1). Only used when solver='adam'
 
     epsilon : float, optional, default 1e-8
-        Value for numerical stability in adam. Only used when algorithm='adam'
+        Value for numerical stability in adam. Only used when solver='adam'
 
     Attributes
     ----------
@@ -1180,7 +1168,7 @@ class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):
         layer i + 1.
 
     n_iter_ : int,
-        The number of iterations the algorithm has ran.
+        The number of iterations the solver has ran.
 
     n_layers_ : int
         Number of layers.
@@ -1221,7 +1209,7 @@ class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):
         optimization." arXiv preprint arXiv:1412.6980 (2014).
     """
     def __init__(self, hidden_layer_sizes=(100,), activation="relu",
-                 algorithm='adam', alpha=0.0001,
+                 solver='adam', alpha=0.0001,
                  batch_size='auto', learning_rate="constant",
                  learning_rate_init=0.001,
                  power_t=0.5, max_iter=200, shuffle=True,
@@ -1233,7 +1221,7 @@ def __init__(self, hidden_layer_sizes=(100,), activation="relu",
 
         sup = super(MLPRegressor, self)
         sup.__init__(hidden_layer_sizes=hidden_layer_sizes,
-                     activation=activation, algorithm=algorithm, alpha=alpha,
+                     activation=activation, solver=solver, alpha=alpha,
                      batch_size=batch_size, learning_rate=learning_rate,
                      learning_rate_init=learning_rate_init, power_t=power_t,
                      max_iter=max_iter, loss='squared_loss', shuffle=shuffle,
@@ -1258,7 +1246,7 @@ def predict(self, X):
             The predicted values.
         """
         check_is_fitted(self, "coefs_")
-        y_pred = self._decision_scores(X)
+        y_pred = self._predict(X)
         if y_pred.shape[1] == 1:
             return y_pred.ravel()
         return y_pred
diff --git a/sklearn/neural_network/tests/test_mlp.py b/sklearn/neural_network/tests/test_mlp.py
index e4f080058173..b8552246ceef 100644
--- a/sklearn/neural_network/tests/test_mlp.py
+++ b/sklearn/neural_network/tests/test_mlp.py
@@ -51,7 +51,7 @@
 
 
 def test_alpha():
-    # Test that larger alpha yields weights closer to zero"""
+    # Test that larger alpha yields weights closer to zero
     X = X_digits_binary[:100]
     y = y_digits_binary[:100]
 
@@ -71,16 +71,15 @@ def test_alpha():
 
 
 def test_fit():
-    # Test that the algorithm solution is equal to a worked out example."""
+    # Test that the algorithm solution is equal to a worked out example.
     X = np.array([[0.6, 0.8, 0.7]])
     y = np.array([0])
-    mlp = MLPClassifier(algorithm='sgd', learning_rate_init=0.1, alpha=0.1,
+    mlp = MLPClassifier(solver='sgd', learning_rate_init=0.1, alpha=0.1,
                         activation='logistic', random_state=1, max_iter=1,
                         hidden_layer_sizes=2, momentum=0)
     # set weights
     mlp.coefs_ = [0] * 2
     mlp.intercepts_ = [0] * 2
-    mlp.classes_ = [0, 1]
     mlp.n_outputs_ = 1
     mlp.coefs_[0] = np.array([[0.1, 0.2], [0.3, 0.1], [0.5, 0]])
     mlp.coefs_[1] = np.array([[0.1], [0.2]])
@@ -89,8 +88,6 @@ def test_fit():
     mlp._coef_grads = [] * 2
     mlp._intercept_grads = [] * 2
 
-    mlp.label_binarizer_.y_type_ = 'binary'
-
     # Initialize parameters
     mlp.n_iter_ = 0
     mlp.learning_rate_ = 0.1
@@ -160,7 +157,8 @@ def test_fit():
     #            0.7 * -0.002244 + 0.09626) = 0.572
     #  o1 = h * W2 + b21 = 0.677 * 0.04706 +
     #             0.572 * 0.154089 + 0.9235 = 1.043
-    assert_almost_equal(mlp.decision_function(X), 1.043, decimal=3)
+    #  prob = sigmoid(o1) = 0.739
+    assert_almost_equal(mlp.predict_proba(X)[0, 1], 0.739, decimal=3)
 
 
 def test_gradient():
@@ -178,7 +176,7 @@ def test_gradient():
 
         for activation in ACTIVATION_TYPES:
             mlp = MLPClassifier(activation=activation, hidden_layer_sizes=10,
-                                algorithm='l-bfgs', alpha=1e-5,
+                                solver='lbgfs', alpha=1e-5,
                                 learning_rate_init=0.2, max_iter=1,
                                 random_state=1)
             mlp.fit(X, y)
@@ -237,7 +235,7 @@ def test_lbfgs_classification():
         expected_shape_dtype = (X_test.shape[0], y_train.dtype.kind)
 
         for activation in ACTIVATION_TYPES:
-            mlp = MLPClassifier(algorithm='l-bfgs', hidden_layer_sizes=50,
+            mlp = MLPClassifier(solver='lbgfs', hidden_layer_sizes=50,
                                 max_iter=150, shuffle=True, random_state=1,
                                 activation=activation)
             mlp.fit(X_train, y_train)
@@ -248,11 +246,11 @@ def test_lbfgs_classification():
 
 
 def test_lbfgs_regression():
-    # Test lbfgs on the boston dataset, a regression problems."""
+    # Test lbfgs on the boston dataset, a regression problems.
     X = Xboston
     y = yboston
     for activation in ACTIVATION_TYPES:
-        mlp = MLPRegressor(algorithm='l-bfgs', hidden_layer_sizes=50,
+        mlp = MLPRegressor(solver='lbgfs', hidden_layer_sizes=50,
                            max_iter=150, shuffle=True, random_state=1,
                            activation=activation)
         mlp.fit(X, y)
@@ -264,11 +262,11 @@ def test_lbfgs_regression():
 
 
 def test_learning_rate_warmstart():
-    # Test that warm_start reuses past solution."""
+    # Tests that warm_start reuse past solutions.
     X = [[3, 2], [1, 6], [5, 6], [-2, -4]]
     y = [1, 1, 1, 0]
     for learning_rate in ["invscaling", "constant"]:
-        mlp = MLPClassifier(algorithm='sgd', hidden_layer_sizes=4,
+        mlp = MLPClassifier(solver='sgd', hidden_layer_sizes=4,
                             learning_rate=learning_rate, max_iter=1,
                             power_t=0.25, warm_start=True)
         with ignore_warnings(category=ConvergenceWarning):
@@ -285,18 +283,18 @@ def test_learning_rate_warmstart():
 
 
 def test_multilabel_classification():
-    # Test that multi-label classification works as expected."""
+    # Test that multi-label classification works as expected.
     # test fit method
     X, y = make_multilabel_classification(n_samples=50, random_state=0,
                                           return_indicator=True)
-    mlp = MLPClassifier(algorithm='l-bfgs', hidden_layer_sizes=50, alpha=1e-5,
+    mlp = MLPClassifier(solver='lbgfs', hidden_layer_sizes=50, alpha=1e-5,
                         max_iter=150, random_state=0, activation='logistic',
                         learning_rate_init=0.2)
     mlp.fit(X, y)
     assert_equal(mlp.score(X, y), 1)
 
     # test partial fit method
-    mlp = MLPClassifier(algorithm='sgd', hidden_layer_sizes=50, max_iter=150,
+    mlp = MLPClassifier(solver='sgd', hidden_layer_sizes=50, max_iter=150,
                         random_state=0, activation='logistic', alpha=1e-5,
                         learning_rate_init=0.2)
     for i in range(100):
@@ -305,19 +303,19 @@ def test_multilabel_classification():
 
 
 def test_multioutput_regression():
-    # Test that multi-output regression works as expected"""
+    # Test that multi-output regression works as expected
     X, y = make_regression(n_samples=200, n_targets=5)
-    mlp = MLPRegressor(algorithm='l-bfgs', hidden_layer_sizes=50, max_iter=200,
+    mlp = MLPRegressor(solver='lbgfs', hidden_layer_sizes=50, max_iter=200,
                        random_state=1)
     mlp.fit(X, y)
     assert_greater(mlp.score(X, y), 0.9)
 
 
 def test_partial_fit_classes_error():
-    # Tests that passing different classes to partial_fit raises an error"""
+    # Tests that passing different classes to partial_fit raises an error
     X = [[3, 2]]
     y = [0]
-    clf = MLPClassifier(algorithm='sgd')
+    clf = MLPClassifier(solver='sgd')
     clf.partial_fit(X, y, classes=[0, 1])
     assert_raises(ValueError, clf.partial_fit, X, y, classes=[1, 2])
 
@@ -329,13 +327,13 @@ def test_partial_fit_classification():
     for X, y in classification_datasets:
         X = X
         y = y
-        mlp = MLPClassifier(algorithm='sgd', max_iter=100, random_state=1,
+        mlp = MLPClassifier(solver='sgd', max_iter=100, random_state=1,
                             tol=0, alpha=1e-5, learning_rate_init=0.2)
 
         with ignore_warnings(category=ConvergenceWarning):
             mlp.fit(X, y)
         pred1 = mlp.predict(X)
-        mlp = MLPClassifier(algorithm='sgd', random_state=1, alpha=1e-5,
+        mlp = MLPClassifier(solver='sgd', random_state=1, alpha=1e-5,
                             learning_rate_init=0.2)
         for i in range(100):
             mlp.partial_fit(X, y, classes=np.unique(y))
@@ -344,6 +342,17 @@ def test_partial_fit_classification():
         assert_greater(mlp.score(X, y), 0.95)
 
 
+def test_partial_fit_unseen_classes():
+    # Non regression test for bug 6994
+    # Tests for labeling errors in partial fit
+
+    clf = MLPClassifier(random_state=0)
+    clf.partial_fit([[1], [2], [3]], ["a", "b", "c"],
+                    classes=["a", "b", "c", "d"])
+    clf.partial_fit([[4]], ["d"])
+    assert_greater(clf.score([[1], [2], [3], [4]], ["a", "b", "c", "d"]), 0)
+
+
 def test_partial_fit_regression():
     # Test partial_fit on regression.
     # `partial_fit` should yield the same results as 'fit' for regression.
@@ -351,14 +360,14 @@ def test_partial_fit_regression():
     y = yboston
 
     for momentum in [0, .9]:
-        mlp = MLPRegressor(algorithm='sgd', max_iter=100, activation='relu',
+        mlp = MLPRegressor(solver='sgd', max_iter=100, activation='relu',
                            random_state=1, learning_rate_init=0.01,
                            batch_size=X.shape[0], momentum=momentum)
         with warnings.catch_warnings(record=True):
             # catch convergence warning
             mlp.fit(X, y)
         pred1 = mlp.predict(X)
-        mlp = MLPRegressor(algorithm='sgd', activation='relu',
+        mlp = MLPRegressor(solver='sgd', activation='relu',
                            learning_rate_init=0.01, random_state=1,
                            batch_size=X.shape[0], momentum=momentum)
         for i in range(100):
@@ -371,23 +380,20 @@ def test_partial_fit_regression():
 
 
 def test_partial_fit_errors():
-    # Test partial_fit error handling."""
+    # Test partial_fit error handling.
     X = [[3, 2], [1, 6]]
     y = [1, 0]
 
     # no classes passed
     assert_raises(ValueError,
-                  MLPClassifier(
-                      algorithm='sgd').partial_fit,
-                  X, y,
-                  classes=[2])
+                  MLPClassifier(solver='sgd').partial_fit, X, y, classes=[2])
 
-    # l-bfgs doesn't support partial_fit
-    assert_false(hasattr(MLPClassifier(algorithm='l-bfgs'), 'partial_fit'))
+    # lbgfs doesn't support partial_fit
+    assert_false(hasattr(MLPClassifier(solver='lbgfs'), 'partial_fit'))
 
 
 def test_params_errors():
-    # Test that invalid parameters raise value error"""
+    # Test that invalid parameters raise value error
     X = [[3, 2], [1, 6]]
     y = [1, 0]
     clf = MLPClassifier
@@ -397,14 +403,25 @@ def test_params_errors():
     assert_raises(ValueError, clf(shuffle='true').fit, X, y)
     assert_raises(ValueError, clf(alpha=-1).fit, X, y)
     assert_raises(ValueError, clf(learning_rate_init=-1).fit, X, y)
-
-    assert_raises(ValueError, clf(algorithm='hadoken').fit, X, y)
+    assert_raises(ValueError, clf(momentum=2).fit, X, y)
+    assert_raises(ValueError, clf(momentum=-0.5).fit, X, y)
+    assert_raises(ValueError, clf(nesterovs_momentum='invalid').fit, X, y)
+    assert_raises(ValueError, clf(early_stopping='invalid').fit, X, y)
+    assert_raises(ValueError, clf(validation_fraction=1).fit, X, y)
+    assert_raises(ValueError, clf(validation_fraction=-0.5).fit, X, y)
+    assert_raises(ValueError, clf(beta_1=1).fit, X, y)
+    assert_raises(ValueError, clf(beta_1=-0.5).fit, X, y)
+    assert_raises(ValueError, clf(beta_2=1).fit, X, y)
+    assert_raises(ValueError, clf(beta_2=-0.5).fit, X, y)
+    assert_raises(ValueError, clf(epsilon=-0.5).fit, X, y)
+
+    assert_raises(ValueError, clf(solver='hadoken').fit, X, y)
     assert_raises(ValueError, clf(learning_rate='converge').fit, X, y)
     assert_raises(ValueError, clf(activation='cloak').fit, X, y)
 
 
 def test_predict_proba_binary():
-    # Test that predict_proba works as expected for binary class."""
+    # Test that predict_proba works as expected for binary class.
     X = X_digits_binary[:50]
     y = y_digits_binary[:50]
 
@@ -426,8 +443,8 @@ def test_predict_proba_binary():
     assert_equal(roc_auc_score(y, y_proba[:, 1]), 1.0)
 
 
-def test_predict_proba_multi():
-    # Test that predict_proba works as expected for multi class."""
+def test_predict_proba_multiclass():
+    # Test that predict_proba works as expected for multi class.
     X = X_digits_multi[:10]
     y = y_digits_multi[:10]
 
@@ -447,17 +464,41 @@ def test_predict_proba_multi():
     assert_array_equal(y_log_proba, np.log(y_proba))
 
 
+def test_predict_proba_multilabel():
+    # Test that predict_proba works as expected for multilabel.
+    # Multilabel should not use softmax which makes probabilities sum to 1
+    X, Y = make_multilabel_classification(n_samples=50, random_state=0,
+                                          return_indicator=True)
+    n_samples, n_classes = Y.shape
+
+    clf = MLPClassifier(solver='lbgfs', hidden_layer_sizes=30,
+                        random_state=0)
+    clf.fit(X, Y)
+    y_proba = clf.predict_proba(X)
+
+    assert_equal(y_proba.shape, (n_samples, n_classes))
+    assert_array_equal(y_proba > 0.5, Y)
+
+    y_log_proba = clf.predict_log_proba(X)
+    proba_max = y_proba.argmax(axis=1)
+    proba_log_max = y_log_proba.argmax(axis=1)
+
+    assert_greater((y_proba.sum(1) - 1).dot(y_proba.sum(1) - 1), 1e-10)
+    assert_array_equal(proba_max, proba_log_max)
+    assert_array_equal(y_log_proba, np.log(y_proba))
+
+
 def test_sparse_matrices():
-    # Test that sparse and dense input matrices output the same results."""
+    # Test that sparse and dense input matrices output the same results.
     X = X_digits_binary[:50]
     y = y_digits_binary[:50]
     X_sparse = csr_matrix(X)
-    mlp = MLPClassifier(random_state=1, hidden_layer_sizes=15)
-    with ignore_warnings(category=ConvergenceWarning):
-        mlp.fit(X, y)
-        pred1 = mlp.decision_function(X)
-        mlp.fit(X_sparse, y)
-        pred2 = mlp.decision_function(X_sparse)
+    mlp = MLPClassifier(solver='lbgfs', hidden_layer_sizes=15,
+                        random_state=1)
+    mlp.fit(X, y)
+    pred1 = mlp.predict(X)
+    mlp.fit(X_sparse, y)
+    pred2 = mlp.predict(X_sparse)
     assert_almost_equal(pred1, pred2)
     pred1 = mlp.predict(X)
     pred2 = mlp.predict(X_sparse)
@@ -466,10 +507,10 @@ def test_sparse_matrices():
 
 def test_tolerance():
     # Test tolerance.
-    # It should force the algorithm to exit the loop when it converges.
+    # It should force the solver to exit the loop when it converges.
     X = [[3, 2], [1, 6]]
     y = [1, 0]
-    clf = MLPClassifier(tol=0.5, max_iter=3000, algorithm='sgd')
+    clf = MLPClassifier(tol=0.5, max_iter=3000, solver='sgd')
     clf.fit(X, y)
     assert_greater(clf.max_iter, clf.n_iter_)
 
@@ -478,7 +519,7 @@ def test_verbose_sgd():
     # Test verbose.
     X = [[3, 2], [1, 6]]
     y = [1, 0]
-    clf = MLPClassifier(algorithm='sgd', max_iter=2, verbose=10,
+    clf = MLPClassifier(solver='sgd', max_iter=2, verbose=10,
                         hidden_layer_sizes=2)
     old_stdout = sys.stdout
     sys.stdout = output = StringIO()
@@ -495,7 +536,7 @@ def test_early_stopping():
     X = X_digits_binary[:100]
     y = y_digits_binary[:100]
     tol = 0.2
-    clf = MLPClassifier(tol=tol, max_iter=3000, algorithm='sgd',
+    clf = MLPClassifier(tol=tol, max_iter=3000, solver='sgd',
                         early_stopping=True)
     clf.fit(X, y)
     assert_greater(clf.max_iter, clf.n_iter_)
@@ -510,7 +551,7 @@ def test_early_stopping():
 def test_adaptive_learning_rate():
     X = [[3, 2], [1, 6]]
     y = [1, 0]
-    clf = MLPClassifier(tol=0.5, max_iter=3000, algorithm='sgd',
+    clf = MLPClassifier(tol=0.5, max_iter=3000, solver='sgd',
                         learning_rate='adaptive')
     clf.fit(X, y)
     assert_greater(clf.max_iter, clf.n_iter_)
diff --git a/sklearn/preprocessing/_function_transformer.py b/sklearn/preprocessing/_function_transformer.py
index e24029918d48..bf6199b00457 100644
--- a/sklearn/preprocessing/_function_transformer.py
+++ b/sklearn/preprocessing/_function_transformer.py
@@ -23,6 +23,8 @@ class FunctionTransformer(BaseEstimator, TransformerMixin):
 
     .. versionadded:: 0.17
 
+    Read more in the :ref:`User Guide <function_transformer>`.
+
     Parameters
     ----------
     func : callable, optional default=None
diff --git a/sklearn/tests/test_base.py b/sklearn/tests/test_base.py
index 1c5c9bb7447a..d0df21c78046 100644
--- a/sklearn/tests/test_base.py
+++ b/sklearn/tests/test_base.py
@@ -324,6 +324,11 @@ def __getstate__(self):
         return self.__dict__
 
 
+class TreeBadVersion(DecisionTreeClassifier):
+    def __getstate__(self):
+        return dict(self.__dict__.items(), _sklearn_version="something")
+
+
 def test_pickle_version_warning():
     # check that warnings are raised when unpickling in a different version
 
@@ -335,9 +340,9 @@ def test_pickle_version_warning():
     assert_no_warnings(pickle.loads, tree_pickle)
 
     # check that warning is raised on different version
-    tree_pickle_other = tree_pickle.replace(sklearn.__version__.encode(),
-                                            b"something")
-    message = ("Trying to unpickle estimator DecisionTreeClassifier from "
+    tree = TreeBadVersion().fit(iris.data, iris.target)
+    tree_pickle_other = pickle.dumps(tree)
+    message = ("Trying to unpickle estimator TreeBadVersion from "
                "version {0} when using version {1}. This might lead to "
                "breaking code or invalid results. "
                "Use at your own risk.".format("something",
@@ -351,7 +356,7 @@ def test_pickle_version_warning():
     tree_pickle_noversion = pickle.dumps(tree)
     assert_false(b"version" in tree_pickle_noversion)
     message = message.replace("something", "pre-0.18")
-    message = message.replace("DecisionTreeClassifier", "TreeNoVersion")
+    message = message.replace("TreeBadVersion", "TreeNoVersion")
     # check we got the warning about using pre-0.18 pickle
     assert_warns_message(UserWarning, message, pickle.loads,
                          tree_pickle_noversion)
diff --git a/sklearn/tests/test_isotonic.py b/sklearn/tests/test_isotonic.py
index 5da86bc35f83..7abadaf562d9 100644
--- a/sklearn/tests/test_isotonic.py
+++ b/sklearn/tests/test_isotonic.py
@@ -80,6 +80,10 @@ def test_isotonic_regression():
     y_ = np.array([3, 6, 6, 8, 8, 8, 10])
     assert_array_equal(y_, isotonic_regression(y))
 
+    y = np.array([10, 0, 2])
+    y_ = np.array([4, 4, 4])
+    assert_array_equal(y_, isotonic_regression(y))
+
     x = np.arange(len(y))
     ir = IsotonicRegression(y_min=0., y_max=1.)
     ir.fit(x, y)
diff --git a/sklearn/tests/test_metaestimators.py b/sklearn/tests/test_metaestimators.py
index 4c6ace3d3aeb..f0f30cb91ae7 100644
--- a/sklearn/tests/test_metaestimators.py
+++ b/sklearn/tests/test_metaestimators.py
@@ -39,7 +39,8 @@ def __init__(self, name, construct, skip_methods=(),
                   skip_methods=['transform', 'inverse_transform', 'score']),
     DelegatorData('BaggingClassifier', BaggingClassifier,
                   skip_methods=['transform', 'inverse_transform', 'score',
-                                'predict_proba', 'predict_log_proba', 'predict'])
+                                'predict_proba', 'predict_log_proba',
+                                'predict'])
 ]
 
 
diff --git a/sklearn/tree/export.py b/sklearn/tree/export.py
index 92cdab8f27d2..5d67b4fabef1 100644
--- a/sklearn/tree/export.py
+++ b/sklearn/tree/export.py
@@ -62,9 +62,11 @@ def _color_brew(n):
 
 
 class Sentinel:
-    __repr__ = lambda x: '"tree.dot"'
+    def __repr__():
+        return '"tree.dot"'
 SENTINEL = Sentinel()
 
+
 def export_graphviz(decision_tree, out_file=SENTINEL, max_depth=None,
                     feature_names=None, class_names=None, label='all',
                     filled=False, leaves_parallel=False, impurity=True,
diff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py
index 672886cca723..8c117ae696ec 100644
--- a/sklearn/utils/estimator_checks.py
+++ b/sklearn/utils/estimator_checks.py
@@ -894,7 +894,7 @@ def check_clustering(name, Alg):
     pred = alg.labels_
     assert_greater(adjusted_rand_score(pred, y), 0.4)
     # fit another time with ``fit_predict`` and compare results
-    if name is 'SpectralClustering':
+    if name == 'SpectralClustering':
         # there is no way to make Spectral clustering deterministic :(
         return
     set_random_state(alg)
diff --git a/sklearn/utils/extmath.py b/sklearn/utils/extmath.py
index cd10e2c0687d..1857a27adfad 100644
--- a/sklearn/utils/extmath.py
+++ b/sklearn/utils/extmath.py
@@ -349,7 +349,7 @@ def randomized_svd(M, n_components, n_oversamples=10, n_iter='auto',
     n_random = n_components + n_oversamples
     n_samples, n_features = M.shape
 
-    if n_iter is 'auto':
+    if n_iter == 'auto':
         # Checks if the number of iterations is explicitely specified
         # Adjust n_iter. 7 was found a good compromise for PCA. See #5299
         n_iter = 7 if n_components < .1 * min(M.shape) else 4
diff --git a/sklearn/utils/metaestimators.py b/sklearn/utils/metaestimators.py
index 9850ea50ea8a..346064448b00 100644
--- a/sklearn/utils/metaestimators.py
+++ b/sklearn/utils/metaestimators.py
@@ -14,16 +14,22 @@ class _IffHasAttrDescriptor(object):
     """Implements a conditional property using the descriptor protocol.
 
     Using this class to create a decorator will raise an ``AttributeError``
-    if the ``attribute_name`` is not present on the base object.
+    if none of the delegates (specified in ``delegate_names``) is an attribute
+    of the base object or the first found delegate does not have an attribute
+    ``attribute_name``.
 
-    This allows ducktyping of the decorated method based on ``attribute_name``.
+    This allows ducktyping of the decorated method based on
+    ``delegate.attribute_name``. Here ``delegate`` is the first item in
+    ``delegate_names`` for which ``hasattr(object, delegate) is True``.
 
     See https://docs.python.org/3/howto/descriptor.html for an explanation of
     descriptors.
     """
-    def __init__(self, fn, attribute_name):
+    def __init__(self, fn, delegate_names, attribute_name):
         self.fn = fn
-        self.get_attribute = attrgetter(attribute_name)
+        self.delegate_names = delegate_names
+        self.attribute_name = attribute_name
+
         # update the docstring of the descriptor
         update_wrapper(self, fn)
 
@@ -32,7 +38,17 @@ def __get__(self, obj, type=None):
         if obj is not None:
             # delegate only on instances, not the classes.
             # this is to allow access to the docstrings.
-            self.get_attribute(obj)
+            for delegate_name in self.delegate_names:
+                try:
+                    delegate = attrgetter(delegate_name)(obj)
+                except AttributeError:
+                    continue
+                else:
+                    getattr(delegate, self.attribute_name)
+                    break
+            else:
+                attrgetter(self.delegate_names[-1])(obj)
+
         # lambda, but not partial, allows help() to work with update_wrapper
         out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)
         # update the docstring of the returned function
@@ -46,27 +62,18 @@ def if_delegate_has_method(delegate):
     This enables ducktyping by hasattr returning True according to the
     sub-estimator.
 
-    >>> from sklearn.utils.metaestimators import if_delegate_has_method
-    >>>
-    >>>
-    >>> class MetaEst(object):
-    ...     def __init__(self, sub_est):
-    ...         self.sub_est = sub_est
-    ...
-    ...     @if_delegate_has_method(delegate='sub_est')
-    ...     def predict(self, X):
-    ...         return self.sub_est.predict(X)
-    ...
-    >>> class HasPredict(object):
-    ...     def predict(self, X):
-    ...         return X.sum(axis=1)
-    ...
-    >>> class HasNoPredict(object):
-    ...     pass
-    ...
-    >>> hasattr(MetaEst(HasPredict()), 'predict')
-    True
-    >>> hasattr(MetaEst(HasNoPredict()), 'predict')
-    False
+    Parameters
+    ----------
+    delegate : string, list of strings or tuple of strings
+        Name of the sub-estimator that can be accessed as an attribute of the
+        base object. If a list or a tuple of names are provided, the first
+        sub-estimator that is an attribute of the base object  will be used.
+
     """
-    return lambda fn: _IffHasAttrDescriptor(fn, '%s.%s' % (delegate, fn.__name__))
+    if isinstance(delegate, list):
+        delegate = tuple(delegate)
+    if not isinstance(delegate, tuple):
+        delegate = (delegate,)
+
+    return lambda fn: _IffHasAttrDescriptor(fn, delegate,
+                                            attribute_name=fn.__name__)
diff --git a/sklearn/utils/tests/test_metaestimators.py b/sklearn/utils/tests/test_metaestimators.py
index cb1f46ef80eb..d73c67d0d198 100644
--- a/sklearn/utils/tests/test_metaestimators.py
+++ b/sklearn/utils/tests/test_metaestimators.py
@@ -1,5 +1,5 @@
+from nose.tools import assert_true, assert_false
 from sklearn.utils.metaestimators import if_delegate_has_method
-from nose.tools import assert_true
 
 
 class Prefix(object):
@@ -24,3 +24,57 @@ def test_delegated_docstring():
                 in str(MockMetaEstimator.func.__doc__))
     assert_true("This is a mock delegated function"
                 in str(MockMetaEstimator().func.__doc__))
+
+
+class MetaEst(object):
+    """A mock meta estimator"""
+    def __init__(self, sub_est, better_sub_est=None):
+        self.sub_est = sub_est
+        self.better_sub_est = better_sub_est
+
+    @if_delegate_has_method(delegate='sub_est')
+    def predict(self):
+        pass
+
+
+class MetaEstTestTuple(MetaEst):
+    """A mock meta estimator to test passing a tuple of delegates"""
+
+    @if_delegate_has_method(delegate=('sub_est', 'better_sub_est'))
+    def predict(self):
+        pass
+
+
+class MetaEstTestList(MetaEst):
+    """A mock meta estimator to test passing a list of delegates"""
+
+    @if_delegate_has_method(delegate=['sub_est', 'better_sub_est'])
+    def predict(self):
+        pass
+
+
+class HasPredict(object):
+    """A mock sub-estimator with predict method"""
+
+    def predict(self):
+        pass
+
+
+class HasNoPredict(object):
+    """A mock sub-estimator with no predict method"""
+    pass
+
+
+def test_if_delegate_has_method():
+    assert_true(hasattr(MetaEst(HasPredict()), 'predict'))
+    assert_false(hasattr(MetaEst(HasNoPredict()), 'predict'))
+    assert_false(
+        hasattr(MetaEstTestTuple(HasNoPredict(), HasNoPredict()), 'predict'))
+    assert_true(
+        hasattr(MetaEstTestTuple(HasPredict(), HasNoPredict()), 'predict'))
+    assert_false(
+        hasattr(MetaEstTestTuple(HasNoPredict(), HasPredict()), 'predict'))
+    assert_false(
+        hasattr(MetaEstTestList(HasNoPredict(), HasPredict()), 'predict'))
+    assert_true(
+        hasattr(MetaEstTestList(HasPredict(), HasPredict()), 'predict'))
