diff --git a/.travis.yml b/.travis.yml
index 9842f1392833..5b5733f71b6e 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -42,6 +42,12 @@ env:
     - DISTRIB="conda" PYTHON_VERSION="3.5" INSTALL_MKL="true"
       NUMPY_VERSION="1.10.4" SCIPY_VERSION="0.17.0" CYTHON_VERSION="0.23.4"
       CACHED_BUILD_DIR="$HOME/sklearn_build_latest"
+    # flake8 linting on diff wrt common ancestor with upstream/master
+    - RUN_FLAKE8="true" SKIP_TESTS="true"
+      DISTRIB="conda" PYTHON_VERSION="3.5" INSTALL_MKL="true"
+      NUMPY_VERSION="1.10.4" SCIPY_VERSION="0.17.0" CYTHON_VERSION="0.23.4"
+      CACHED_BUILD_DIR="$HOME/dummy"
+
 
 matrix:
   allow_failures:
diff --git a/Makefile b/Makefile
index c6a2cfa0049e..20bc4f582e12 100644
--- a/Makefile
+++ b/Makefile
@@ -68,3 +68,6 @@ doc-noplot: inplace
 code-analysis:
 	flake8 sklearn | grep -v __init__ | grep -v external
 	pylint -E -i y sklearn/ -d E1103,E0611,E1101
+
+flake8-diff:
+	./build_tools/travis/flake8_diff.sh
diff --git a/build_tools/travis/flake8_diff.sh b/build_tools/travis/flake8_diff.sh
new file mode 100755
index 000000000000..525a16c91363
--- /dev/null
+++ b/build_tools/travis/flake8_diff.sh
@@ -0,0 +1,100 @@
+#!/bin/bash
+
+# This script is used in Travis to check that PRs do not add obvious
+# flake8 violations. It relies on two things:
+#   - find common ancestor between branch and
+#     scikit-learn/scikit-learn remote
+#   - run flake8 --diff on the diff between the branch and the common
+#     ancestor
+#
+# Additional features:
+#   - the line numbers in Travis match the local branch on the PR
+#     author machine.
+#   - ./build_tools/travis/flake8_diff.sh can be run locally for quick
+#     turn-around
+
+set -e
+# pipefail is necessary to propagate exit codes
+set -o pipefail
+
+PROJECT=scikit-learn/scikit-learn
+PROJECT_URL=https://github.com/$PROJECT.git
+
+echo "Remotes:"
+git remote --verbose
+
+# Find the remote with the project name (upstream in most cases)
+REMOTE=$(git remote -v | grep $PROJECT | cut -f1 | head -1)
+
+# Add a temporary remote if needed. For example this is necessary when
+# Travis is configured to run in a fork. In this case 'origin' is the
+# fork and not the reference repo we want to diff against.
+if [[ -z "$REMOTE" ]]; then
+    TMP_REMOTE=tmp_reference_upstream
+    REMOTE=$TMP_REMOTE
+    git remote add $REMOTE $PROJECT_URL
+fi
+
+if [[ "$TRAVIS" == "true" ]]; then
+    if [[ "$TRAVIS_PULL_REQUEST" == "false" ]]
+    then
+        # Travis does the git clone with a limited depth (50 at the time of
+        # writing). This may not be enough to find the common ancestor with
+        # $REMOTE/master so we unshallow the git checkout
+        git fetch --unshallow || echo "Unshallowing the git checkout failed"
+    else
+        # We want to fetch the code as it is in the PR branch and not
+        # the result of the merge into master. This way line numbers
+        # reported by Travis will match with the local code.
+        BRANCH_NAME=travis_pr_$TRAVIS_PULL_REQUEST
+        git fetch $REMOTE pull/$TRAVIS_PULL_REQUEST/head:$BRANCH_NAME
+        git checkout $BRANCH_NAME
+    fi
+fi
+
+
+echo -e '\nLast 2 commits:'
+echo '--------------------------------------------------------------------------------'
+git log -2 --pretty=short
+
+git fetch $REMOTE master
+REMOTE_MASTER_REF="$REMOTE/master"
+
+# Find common ancestor between HEAD and remotes/$REMOTE/master
+COMMIT=$(git merge-base @ $REMOTE_MASTER_REF) || \
+    echo "No common ancestor found for $(git show @ -q) and $(git show $REMOTE_MASTER_REF -q)"
+
+if [[ -n "$TMP_REMOTE" ]]; then
+    git remote remove $TMP_REMOTE
+fi
+
+if [ -z "$COMMIT" ]; then
+    exit 1
+fi
+
+echo -e "\nCommon ancestor between HEAD and $REMOTE_MASTER_REF is:"
+echo '--------------------------------------------------------------------------------'
+git show --no-patch $COMMIT
+
+echo -e '\nRunning flake8 on the diff in the range'\
+     "$(git rev-parse --short $COMMIT)..$(git rev-parse --short @)" \
+     "($(git rev-list $COMMIT.. | wc -l) commit(s)):"
+echo '--------------------------------------------------------------------------------'
+
+# We ignore files from sklearn/externals. Unfortunately there is no
+# way to do it with flake8 directly (the --exclude does not seem to
+# work with --diff). We could use the exclude magic in the git pathspec
+# ':!sklearn/externals' but it is only available on git 1.9 and Travis
+# uses git 1.8.
+# We need the following command to exit with 0 hence the echo in case
+# there is no match
+MODIFIED_FILES=$(git diff --name-only $COMMIT | grep -v 'sklearn/externals' || echo "no_match")
+
+if [[ "$MODIFIED_FILES" == "no_match" ]]; then
+    echo "No file outside sklearn/externals has been modified"
+else
+    # Conservative approach: diff without context so that code that
+    # was not changed does not create failures
+    git diff --unified=0 $COMMIT -- $MODIFIED_FILES | flake8 --diff --show-source
+fi
+echo -e "No problem detected by flake8\n"
diff --git a/build_tools/travis/install.sh b/build_tools/travis/install.sh
index 96faa65d1ebb..b5825d080fa8 100755
--- a/build_tools/travis/install.sh
+++ b/build_tools/travis/install.sh
@@ -53,7 +53,7 @@ if [[ "$DISTRIB" == "conda" ]]; then
     if [[ "$INSTALL_MKL" == "true" ]]; then
         conda create -n testenv --yes python=$PYTHON_VERSION pip nose \
             numpy=$NUMPY_VERSION scipy=$SCIPY_VERSION numpy scipy \
-            cython=$CYTHON_VERSION libgfortran mkl
+            cython=$CYTHON_VERSION libgfortran mkl flake8
     else
         conda create -n testenv --yes python=$PYTHON_VERSION pip nose \
             numpy=$NUMPY_VERSION scipy=$SCIPY_VERSION cython=$CYTHON_VERSION \
@@ -95,18 +95,26 @@ if [[ "$COVERAGE" == "true" ]]; then
     pip install coverage coveralls
 fi
 
-if [ ! -d "$CACHED_BUILD_DIR" ]; then
-    mkdir -p $CACHED_BUILD_DIR
-fi
+if [[ "$SKIP_TESTS" == "true" ]]; then
+    echo "No need to build scikit-learn when not running the tests"
+else
+    if [ ! -d "$CACHED_BUILD_DIR" ]; then
+        mkdir -p $CACHED_BUILD_DIR
+    fi
 
-rsync -av --exclude '.git/' --exclude='testvenv/' \
-      $TRAVIS_BUILD_DIR $CACHED_BUILD_DIR
+    rsync -av --exclude '.git/' --exclude='testvenv/' \
+          $TRAVIS_BUILD_DIR $CACHED_BUILD_DIR
 
-cd $CACHED_BUILD_DIR/scikit-learn
+    cd $CACHED_BUILD_DIR/scikit-learn
 
-# Build scikit-learn in the install.sh script to collapse the verbose
-# build output in the travis output when it succeeds.
-python --version
-python -c "import numpy; print('numpy %s' % numpy.__version__)"
-python -c "import scipy; print('scipy %s' % scipy.__version__)"
-python setup.py develop
+    # Build scikit-learn in the install.sh script to collapse the verbose
+    # build output in the travis output when it succeeds.
+    python --version
+    python -c "import numpy; print('numpy %s' % numpy.__version__)"
+    python -c "import scipy; print('scipy %s' % scipy.__version__)"
+    python setup.py develop
+fi
+
+if [[ "$RUN_FLAKE8" == "true" ]]; then
+    conda install flake8
+fi
diff --git a/build_tools/travis/test_script.sh b/build_tools/travis/test_script.sh
index cad92e56d4c0..f1b80433b617 100755
--- a/build_tools/travis/test_script.sh
+++ b/build_tools/travis/test_script.sh
@@ -8,32 +8,42 @@
 
 set -e
 
-# Get into a temp directory to run test from the installed scikit learn and
-# check if we do not leave artifacts
-mkdir -p $TEST_DIR
-# We need the setup.cfg for the nose settings
-cp setup.cfg $TEST_DIR
-cd $TEST_DIR
-
 python --version
 python -c "import numpy; print('numpy %s' % numpy.__version__)"
 python -c "import scipy; print('scipy %s' % scipy.__version__)"
 python -c "import multiprocessing as mp; print('%d CPUs' % mp.cpu_count())"
 
-# Skip tests that require large downloads over the network to save bandwidth
-# usage as travis workers are stateless and therefore traditional local
-# disk caching does not work.
-export SKLEARN_SKIP_NETWORK_TESTS=1
-
-if [[ "$COVERAGE" == "true" ]]; then
-   nosetests -s --with-coverage --with-timer --timer-top-n 20 sklearn
-else
-   nosetests -s --with-timer --timer-top-n 20 sklearn
+run_tests() {
+    # Get into a temp directory to run test from the installed scikit learn and
+    # check if we do not leave artifacts
+    mkdir -p $TEST_DIR
+    # We need the setup.cfg for the nose settings
+    cp setup.cfg $TEST_DIR
+    cd $TEST_DIR
+
+    # Skip tests that require large downloads over the network to save bandwidth
+    # usage as travis workers are stateless and therefore traditional local
+    # disk caching does not work.
+    export SKLEARN_SKIP_NETWORK_TESTS=1
+
+    if [[ "$COVERAGE" == "true" ]]; then
+        nosetests -s --with-coverage --with-timer --timer-top-n 20 sklearn
+    else
+        nosetests -s --with-timer --timer-top-n 20 sklearn
+    fi
+
+    # Is directory still empty ?
+    ls -ltra
+
+    # Test doc
+    cd $CACHED_BUILD_DIR/scikit-learn
+    make test-doc test-sphinxext
+}
+
+if [[ "$RUN_FLAKE8" == "true" ]]; then
+    source build_tools/travis/flake8_diff.sh
 fi
 
-# Is directory still empty ?
-ls -ltra
-
-# Test doc
-cd $CACHED_BUILD_DIR/scikit-learn
-make test-doc test-sphinxext
+if [[ "$SKIP_TESTS" != "true" ]]; then
+    run_tests
+fi
diff --git a/doc/about.rst b/doc/about.rst
index 37a08d16b68d..6d6fc1afa6ca 100644
--- a/doc/about.rst
+++ b/doc/about.rst
@@ -17,31 +17,31 @@ Citing scikit-learn
 If you use scikit-learn in a scientific publication, we would appreciate
 citations to the following paper:
 
- `Scikit-learn: Machine Learning in Python
- <http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html>`_, Pedregosa
- *et al.*, JMLR 12, pp. 2825-2830, 2011.
-
- Bibtex entry::
-
-   @article{scikit-learn,
-    title={Scikit-learn: Machine Learning in {P}ython},
-    author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
-            and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
-            and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
-            Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
-    journal={Journal of Machine Learning Research},
-    volume={12},
-    pages={2825--2830},
-    year={2011}
-   }
+  `Scikit-learn: Machine Learning in Python
+  <http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html>`_, Pedregosa
+  *et al.*, JMLR 12, pp. 2825-2830, 2011.
+
+  Bibtex entry::
+
+    @article{scikit-learn,
+     title={Scikit-learn: Machine Learning in {P}ython},
+     author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
+             and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
+             and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
+             Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
+     journal={Journal of Machine Learning Research},
+     volume={12},
+     pages={2825--2830},
+     year={2011}
+    }
 
 If you want to cite scikit-learn for its API or design, you may also want to consider the
 following paper:
 
-`API design for machine learning software: experiences from the scikit-learn
-project <http://arxiv.org/abs/1309.0238>`_, Buitinck *et al.*, 2013.
+  `API design for machine learning software: experiences from the scikit-learn
+  project <http://arxiv.org/abs/1309.0238>`_, Buitinck *et al.*, 2013.
 
-Bibtex entry::
+  Bibtex entry::
 
     @inproceedings{sklearn_api,
       author    = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and
@@ -102,11 +102,13 @@ program.
 - 2012 - `Vlad Niculae`_, Immanuel Bayer.
 - 2013 - Kemal Eren, Nicolas Trésegnie
 - 2014 - Hamzeh Alsalhi, Issam Laradji, Maheshakya Wijewardena, Manoj Kumar.
+- 2015 - `Raghav R V <https://github.com/raghavrv>`_, Wei Xue
+- 2016 - `Nelson Liu <http://nelsonliu.me>`_, `YenChen Lin <http://yclin.me>`_
 
 It also provided funding for sprints and events around scikit-learn. If
 you would like to participate in the next Google Summer of code
 program, please see `this page
-<https://github.com/scikit-learn/scikit-learn/wiki/SummerOfCode>`_
+<https://github.com/scikit-learn/scikit-learn/wiki/SummerOfCode>`_.
 
 The `NeuroDebian <http://neuro.debian.net>`_ project providing `Debian
 <http://www.debian.org>`_ packaging and contributions is supported by
diff --git a/doc/conf.py b/doc/conf.py
index 57d385a881aa..9c75922b2a50 100644
--- a/doc/conf.py
+++ b/doc/conf.py
@@ -32,11 +32,20 @@
 # extensions coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
 extensions = [
     'sphinx.ext.autodoc', 'sphinx.ext.autosummary',
-    'sphinx.ext.pngmath', 'numpy_ext.numpydoc',
+    'numpy_ext.numpydoc',
     'sphinx.ext.linkcode', 'sphinx.ext.doctest',
     'sphinx_gallery.gen_gallery',
 ]
 
+# pngmath / imgmath compatibility layer for different sphinx versions
+import sphinx
+from distutils.version import LooseVersion
+if LooseVersion(sphinx.__version__) < LooseVersion('1.4'):
+    extensions.append('sphinx.ext.pngmath')
+else:
+    extensions.append('sphinx.ext.imgmath')
+
+
 autodoc_default_flags = ['members', 'inherited-members']
 
 # Add any paths that contain templates here, relative to this directory.
diff --git a/doc/datasets/rcv1_fixture.py b/doc/datasets/rcv1_fixture.py
index c409f2f937a6..75d93d7ec568 100644
--- a/doc/datasets/rcv1_fixture.py
+++ b/doc/datasets/rcv1_fixture.py
@@ -1,7 +1,7 @@
 """Fixture module to skip the datasets loading when offline
 
 The RCV1 data is rather large and some CI workers such as travis are
-stateless hence will not cache the dataset as regular sklearn users would do.
+stateless hence will not cache the dataset as regular scikit-learn users would do.
 
 The following will skip the execution of the rcv1.rst doctests
 if the proper environment variable is configured (see the source code of
diff --git a/doc/developers/advanced_installation.rst b/doc/developers/advanced_installation.rst
index c086a3deb39b..9029f5991afa 100644
--- a/doc/developers/advanced_installation.rst
+++ b/doc/developers/advanced_installation.rst
@@ -61,7 +61,7 @@ installing build dependencies
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 installing from source requires you to have installed the scikit-learn runtime
-dependencies, python development headers and a working c/c++ compiler.
+dependencies, python development headers and a working C/C++ compiler.
 under debian-based operating systems, which include ubuntu, if you have
 python 2 you can install all these requirements by issuing::
 
@@ -257,7 +257,7 @@ or::
 building on windows
 -------------------
 
-to build scikit-learn on windows you need a working c/c++ compiler in
+to build scikit-learn on windows you need a working C/C++ compiler in
 addition to numpy, scipy and setuptools.
 
 picking the right compiler depends on the version of python (2 or 3)
@@ -364,7 +364,7 @@ bleeding edge
 =============
 
 see section :ref:`git_repo` on how to get the development version. then follow
-the previous instructions to build from source depending on your platform. 
+the previous instructions to build from source depending on your platform.
 You will also require Cython >=0.23 in order to build the development version.
 
 
diff --git a/doc/developers/contributing.rst b/doc/developers/contributing.rst
index c2b75d6630c0..9a4e9a917210 100644
--- a/doc/developers/contributing.rst
+++ b/doc/developers/contributing.rst
@@ -157,7 +157,10 @@ Contributing pull requests
 It is recommended to check that your contribution complies with the following
 rules before submitting a pull request:
 
-    * Follow the `coding-guidelines`_ (see below).
+    * Follow the `coding-guidelines`_ (see below). To make sure that
+      your PR does not add PEP8 violations you can run
+      `./build_tools/travis/flake8_diff.sh` or `make flake8-diff` on a
+      Unix-like system.
 
     * When applicable, use the validation tools and other code in the
       ``sklearn.utils`` submodule.  A list of utility routines available
@@ -871,9 +874,9 @@ an integer called ``n_iter``.
 Rolling your own estimator
 ==========================
 If you want to implement a new estimator that is scikit-learn-compatible,
-whether it is just for you or for contributing it to sklearn, there are several
-internals of scikit-learn that you should be aware of in addition to the
-sklearn API outlined above. You can check whether your estimator
+whether it is just for you or for contributing it to scikit-learn, there are
+several internals of scikit-learn that you should be aware of in addition to
+the scikit-learn API outlined above. You can check whether your estimator
 adheres to the scikit-learn interface and standards by running
 :func:`utils.estimator_checks.check_estimator` on the class::
 
@@ -929,7 +932,7 @@ E.g., below is a custom classifier. For more information on this example, see
 
 get_params and set_params
 -------------------------
-All sklearn estimator have ``get_params`` and ``set_params`` functions.
+All scikit-learn estimators have ``get_params`` and ``set_params`` functions.
 The ``get_params`` function takes no arguments and returns a dict of the
 ``__init__`` parameters of the estimator, together with their values.
 It must take one keyword argument, ``deep``,
diff --git a/doc/faq.rst b/doc/faq.rst
index eaf674d70df3..2b911cf0093b 100644
--- a/doc/faq.rst
+++ b/doc/faq.rst
@@ -250,12 +250,14 @@ You can find more default on the new start methods in the `multiprocessing
 documentation <https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods>`_.
 
 
-Why is there no support for deep learning / Will there be support for deep learning in scikit-learn?
-----------------------------------------------------------------------------------------------------
-Deep learning requires a rich vocabulary to define an architecture and the
-use of GPUs for efficient computing. However, neither of these fit within
-the design constraints of scikit-learn. As a result, deep learning is
-currently out of scope for what scikit-learn seeks to achieve.
+Why is there no support for deep or reinforcement learning / Will there be support for deep or reinforcement learning in scikit-learn?
+--------------------------------------------------------------------------------------------------------------------------------------
+Deep learning and reinforcement learning both require a rich vocabulary to
+define an architecture, with deep learning additionally requiring
+GPUs for efficient computing. However, neither of these fit within
+the design constraints of scikit-learn; as a result, deep learning
+and reinforcement learning are currently out of scope for what
+scikit-learn seeks to achieve.
 
 
 Why is my pull request not getting any attention?
diff --git a/doc/install.rst b/doc/install.rst
index 0b58c0b6e28a..f2b02ba864fb 100644
--- a/doc/install.rst
+++ b/doc/install.rst
@@ -28,15 +28,22 @@ or ``conda``::
 
     conda install scikit-learn
 
-**We don't recommend installing scipy or numpy using pip on linux**,
-as this will involve a lengthy build-process with many dependencies.
-Without careful configuration, building numpy yourself can lead to an installation
-that is much slower than it should be. 
+**We don't recommend installing scipy or numpy using pip on Linux**,
+as this will involve a lengthy build process with many dependencies.
+Without careful configuration, building numpy yourself is likely to lead to an
+installation that is much slower than it should be.
 If you are using Linux, consider using your package manager to install
 scikit-learn. It is usually the easiest way, but might not provide the newest
 version.
 If you haven't already installed numpy and scipy and can't install them via
-your operation system, it is recommended to use a third party distribution.
+your operating system, we recommended using a third-party distribution.
+
+If you must install scikit-learn and its dependencies with pip, you can install
+it as ``scikit-learn[alldeps]``. We strongly recommend against doing this
+unless you are familiar with how to correctly build numpy and scipy. The most
+common use case for this is in a ``requirements.txt`` file used as part of an
+automated build process for a PaaS application or a Docker image. This option
+is not intended for manual installation from the command line.
 
 Third-party Distributions
 ==========================
diff --git a/doc/modules/classes.rst b/doc/modules/classes.rst
index f58ca98fd9db..d1d8eec4dd4e 100644
--- a/doc/modules/classes.rst
+++ b/doc/modules/classes.rst
@@ -174,6 +174,7 @@ Splitter Classes
    model_selection.LabelShuffleSplit
    model_selection.StratifiedShuffleSplit
    model_selection.PredefinedSplit
+   model_selection.TimeSeriesSplit
 
 Splitter Functions
 ------------------
@@ -953,8 +954,8 @@ See the :ref:`metrics` section of the user guide for further details.
    :template: class.rst
 
    mixture.GaussianMixture
+   mixture.BayesianGaussianMixture
    mixture.DPGMM
-   mixture.VBGMM
 
 
 .. _multiclass_ref:
diff --git a/doc/modules/cross_validation.rst b/doc/modules/cross_validation.rst
index d2d542cb791b..872844cd0cc2 100644
--- a/doc/modules/cross_validation.rst
+++ b/doc/modules/cross_validation.rst
@@ -137,7 +137,7 @@ validation iterator instead, for instance::
 
   >>> from sklearn.model_selection import ShuffleSplit
   >>> n_samples = iris.data.shape[0]
-  >>> cv = ShuffleSplit(n_iter=3, test_size=0.3, random_state=0)
+  >>> cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=0)
   >>> cross_val_score(clf, iris.data, iris.target, cv=cv)
   ...                                                     # doctest: +ELLIPSIS
   array([ 0.97...,  0.97...,  1.        ])
@@ -224,7 +224,7 @@ Example of 2-fold cross-validation on a dataset with 4 samples::
   >>> from sklearn.model_selection import KFold
 
   >>> X = ["a", "b", "c", "d"]
-  >>> kf = KFold(n_folds=2)
+  >>> kf = KFold(n_splits=2)
   >>> for train, test in kf.split(X):
   ...     print("%s %s" % (train, test))
   [2 3] [0 1]
@@ -253,7 +253,7 @@ two slightly unbalanced classes::
 
   >>> X = np.ones(10)
   >>> y = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]
-  >>> skf = StratifiedKFold(n_folds=3)
+  >>> skf = StratifiedKFold(n_splits=3)
   >>> for train, test in skf.split(X, y):
   ...     print("%s %s" % (train, test))
   [2 3 6 7 8 9] [0 1 4 5]
@@ -278,7 +278,7 @@ Imagine you have three subjects, each with an associated number from 1 to 3::
   >>> y = ["a", "b", "b", "b", "c", "c", "c", "d", "d", "d"]
   >>> labels = [1, 1, 1, 2, 2, 2, 3, 3, 3, 3]
 
-  >>> lkf = LabelKFold(n_folds=3)
+  >>> lkf = LabelKFold(n_splits=3)
   >>> for train, test in lkf.split(X, y, labels):
   ...     print("%s %s" % (train, test))
   [0 1 2 3 4 5] [6 7 8 9]
@@ -454,7 +454,7 @@ Here is a usage example::
 
   >>> from sklearn.model_selection import ShuffleSplit
   >>> X = np.arange(5)
-  >>> ss = ShuffleSplit(n_iter=3, test_size=0.25,
+  >>> ss = ShuffleSplit(n_splits=3, test_size=0.25,
   ...     random_state=0)
   >>> for train_index, test_index in ss.split(X):
   ...     print("%s %s" % (train_index, test_index))
@@ -485,7 +485,7 @@ Here is a usage example::
   >>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001]
   >>> y = ["a", "b", "b", "b", "c", "c", "c", "a"]
   >>> labels = [1, 1, 2, 2, 3, 3, 4, 4]
-  >>> lss = LabelShuffleSplit(n_iter=4, test_size=0.5, random_state=0)
+  >>> lss = LabelShuffleSplit(n_splits=4, test_size=0.5, random_state=0)
   >>> for train, test in lss.split(X, y, labels):
   ...     print("%s %s" % (train, test))
   ...
@@ -521,6 +521,50 @@ See also
 stratified splits, *i.e* which creates splits by preserving the same
 percentage for each target class as in the complete set.
 
+Cross validation of time series data
+====================================
+
+Time series data is characterised by the correlation between observations 
+that are near in time (*autocorrelation*). However, classical 
+cross-validation techniques such as :class:`KFold` and 
+:class:`ShuffleSplit` assume the samples are independent and 
+identically distributed, and would result in unreasonable correlation 
+between training and testing instances (yielding poor estimates of 
+generalisation error) on time series data. Therefore, it is very important 
+to evaluate our model for time series data on the "future" observations 
+least like those that are used to train the model. To achieve this, one 
+solution is provided by :class:`TimeSeriesSplit`.
+
+
+TimeSeriesSplit
+-----------------------
+
+:class:`TimeSeriesSplit` is a variation of *k-fold* which 
+returns first :math:`k` folds as train set and the :math:`(k+1)` th 
+fold as test set. Note that unlike standard cross-validation methods, 
+successive training sets are supersets of those that come before them.
+Also, it adds all surplus data to the first training partition, which
+is always used to train the model.
+
+This class can be used to cross-validate time series data samples 
+that are observed at fixed time intervals.
+
+Example of 3-split time series cross-validation on a dataset with 6 samples::
+
+  >>> from sklearn.model_selection import TimeSeriesSplit
+
+  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])
+  >>> y = np.array([1, 2, 3, 4, 5, 6])
+  >>> tscv = TimeSeriesSplit(n_splits=3)
+  >>> print(tscv)  # doctest: +NORMALIZE_WHITESPACE
+  TimeSeriesSplit(n_splits=3)
+  >>> for train, test in tscv.split(X):
+  ...     print("%s %s" % (train, test))
+  [0 1 2] [3]
+  [0 1 2 3] [4]
+  [0 1 2 3 4] [5]
+
+
 A note on shuffling
 ===================
 
diff --git a/doc/modules/ensemble.rst b/doc/modules/ensemble.rst
index e33296f8728b..887e0400ca59 100644
--- a/doc/modules/ensemble.rst
+++ b/doc/modules/ensemble.rst
@@ -464,7 +464,7 @@ The number of weak learners (i.e. regression trees) is controlled by the paramet
 .. note::
 
    Classification with more than 2 classes requires the induction
-   of ``n_classes`` regression trees at each at each iteration,
+   of ``n_classes`` regression trees at each iteration,
    thus, the total number of induced trees equals
    ``n_classes * n_estimators``. For datasets with a large number
    of classes we strongly recommend to use
diff --git a/doc/modules/feature_extraction.rst b/doc/modules/feature_extraction.rst
index f4f81d1e2208..4995177705c1 100644
--- a/doc/modules/feature_extraction.rst
+++ b/doc/modules/feature_extraction.rst
@@ -398,18 +398,49 @@ suitable for usage by a classifier it is very common to use the tf–idf
 transform.
 
 Tf means **term-frequency** while tf–idf means term-frequency times
-**inverse document-frequency**. This was originally a term weighting
-scheme developed for information retrieval (as a ranking function
-for search engines results), that has also found good use in document
-classification and clustering.
+**inverse document-frequency**:
+:math:`\text{tf-idf(t,d)}=\text{tf(t,d)} \times \text{idf(t)}`.
+
+Using the ``TfidfTransformer``'s default settings,
+``TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)``
+the term frequency, the number of times a term occurs in a given document,
+is multiplied with idf component, which is computed as
+
+:math:`\text{idf}(t) = log{\frac{1 + n_d}{1+\text{df}(d,t)}} + 1`,
+
+where :math:`n_d` is the total number of documents, and :math:`\text{df}(d,t)`
+is the number of documents that contain term :math:`t`. The resulting tf-idf
+vectors are then normalized by the Euclidean norm:
+
+:math:`v_{norm} = \frac{v}{||v||_2} = \frac{v}{\sqrt{v{_1}^2 +
+v{_2}^2 + \dots + v{_n}^2}}`.
+
+This was originally a term weighting scheme developed for information retrieval
+(as a ranking function for search engines results) that has also found good
+use in document classification and clustering.
+
+The following sections contain further explanations and examples that
+illustrate how the tf-idfs are computed exactly and how the tf-idfs
+computed in scikit-learn's :class:`TfidfTransformer`
+and :class:`TfidfVectorizer` differ slightly from the standard textbook
+notation that defines the idf as
+
+:math:`\text{idf}(t) = log{\frac{n_d}{1+\text{df}(d,t)}}.`
+
+
+In the :class:`TfidfTransformer` and :class:`TfidfVectorizer`
+with ``smooth_idf=False``, the
+"1" count is added to the idf instead of the idf's denominator:
+
+:math:`\text{idf}(t) = log{\frac{n_d}{\text{df}(d,t)}} + 1`
 
 This normalization is implemented by the :class:`TfidfTransformer`
 class::
 
   >>> from sklearn.feature_extraction.text import TfidfTransformer
-  >>> transformer = TfidfTransformer()
+  >>> transformer = TfidfTransformer(smooth_idf=False)
   >>> transformer   # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
-  TfidfTransformer(norm=...'l2', smooth_idf=True, sublinear_tf=False,
+  TfidfTransformer(norm=...'l2', smooth_idf=False, sublinear_tf=False,
                    use_idf=True)
 
 Again please see the :ref:`reference documentation
@@ -433,14 +464,74 @@ content of the documents::
       with 9 stored elements in Compressed Sparse ... format>
 
   >>> tfidf.toarray()                        # doctest: +ELLIPSIS
-  array([[ 0.85...,  0.  ...,  0.52...],
-         [ 1.  ...,  0.  ...,  0.  ...],
-         [ 1.  ...,  0.  ...,  0.  ...],
-         [ 1.  ...,  0.  ...,  0.  ...],
-         [ 0.55...,  0.83...,  0.  ...],
-         [ 0.63...,  0.  ...,  0.77...]])
-
-Each row is normalized to have unit euclidean norm. The weights of each
+  array([[ 0.81940995,  0.        ,  0.57320793],
+         [ 1.        ,  0.        ,  0.        ],
+         [ 1.        ,  0.        ,  0.        ],
+         [ 1.        ,  0.        ,  0.        ],
+         [ 0.47330339,  0.88089948,  0.        ],
+         [ 0.58149261,  0.        ,  0.81355169]])
+
+Each row is normalized to have unit Euclidean norm:
+
+:math:`v_{norm} = \frac{v}{||v||_2} = \frac{v}{\sqrt{v{_1}^2 +
+v{_2}^2 + \dots + v{_n}^2}}`
+
+For example, we can compute the tf-idf of the first term in the first
+document in the `counts` array as follows:
+
+:math:`n_{d, {\text{term1}}} = 6`
+
+:math:`\text{df}(d, t)_{\text{term1}} = 6`
+
+:math:`\text{idf}(d, t)_{\text{term1}} =
+log \frac{n_d}{\text{df}(d, t)} + 1 = log(1)+1 = 1`
+
+:math:`\text{tf-idf}_{\text{term1}} = \text{tf} \times \text{idf} = 3 \times 1 = 3`
+
+Now, if we repeat this computation for the remaining 2 terms in the document,
+we get
+
+:math:`\text{tf-idf}_{\text{term2}} = 0 \times log(6/1)+1 = 0`
+
+:math:`\text{tf-idf}_{\text{term3}} = 1 \times log(6/2)+1 \approx 2.0986`
+
+and the vector of raw tf-idfs:
+
+:math:`\text{tf-idf}_raw = [3, 0, 2.0986].`
+
+
+Then, applying the Euclidean (L2) norm, we obtain the following tf-idfs
+for document 1:
+
+:math:`\frac{[3, 0, 2.0986]}{\sqrt{\big(3^2 + 0^2 + 2.0986^2\big)}}
+= [ 0.819,  0,  0.573].`
+
+Furthermore, the default parameter ``smooth_idf=True`` adds "1" to the numerator
+and  denominator as if an extra document was seen containing every term in the
+collection exactly once, which prevents zero divisions:
+
+:math:`\text{idf}(t) = log{\frac{1 + n_d}{1+\text{df}(d,t)}} + 1`
+
+Using this modification, the tf-idf of the third term in document 1 changes to
+1.8473:
+
+:math:`\text{tf-idf}_{\text{term3}} = 1 \times log(7/3)+1 \approx 1.8473`
+
+And the L2-normalized tf-idf changes to
+
+:math:`\frac{[3, 0, 1.8473]}{\sqrt{\big(3^2 + 0^2 + 1.8473^2\big)}}
+= [0.8515, 0, 0.5243]`::
+
+  >>> transformer = TfidfTransformer()
+  >>> transformer.fit_transform(counts).toarray()
+  array([[ 0.85151335,  0.        ,  0.52433293],
+         [ 1.        ,  0.        ,  0.        ],
+         [ 1.        ,  0.        ,  0.        ],
+         [ 1.        ,  0.        ,  0.        ],
+         [ 0.55422893,  0.83236428,  0.        ],
+         [ 0.63035731,  0.        ,  0.77630514]])
+
+The weights of each
 feature computed by the ``fit`` method call are stored in a model
 attribute::
 
@@ -448,6 +539,8 @@ attribute::
   array([ 1. ...,  2.25...,  1.84...])
 
 
+
+
 As tf–idf is very often used for text features, there is also another
 class called :class:`TfidfVectorizer` that combines all the options of
 :class:`CountVectorizer` and :class:`TfidfTransformer` in a single model::
diff --git a/doc/modules/gaussian_process.rst b/doc/modules/gaussian_process.rst
index 52211a154d8d..fb408c4acd71 100644
--- a/doc/modules/gaussian_process.rst
+++ b/doc/modules/gaussian_process.rst
@@ -66,7 +66,7 @@ WhiteKernel component into the kernel, which can estimate the global noise
 level from the data (see example below).
 
 The implementation is based on Algorithm 2.1 of [RW2006]_. In addition to
-the API of standard sklearn estimators, GaussianProcessRegressor:
+the API of standard scikit-learn estimators, GaussianProcessRegressor:
 
 * allows prediction without prior fitting (based on the GP prior)
 
@@ -164,7 +164,7 @@ than just predicting the mean.
 GPR on Mauna Loa CO2 data
 -------------------------
 
-This example is based on Section 5.4.3 of [RW2006]_. 
+This example is based on Section 5.4.3 of [RW2006]_.
 It illustrates an example of complex kernel engineering and
 hyperparameter optimization using gradient ascent on the
 log-marginal-likelihood. The data consists of the monthly average atmospheric
@@ -602,11 +602,11 @@ References
 ----------
 
     * `[RW2006]
-      <http://www.gaussianprocess.org/gpml/chapters/>`_ 
-      **Gaussian Processes for Machine Learning**, 
-      Carl Eduard Rasmussen and Christopher K.I. Williams, MIT Press 2006. 
-      Link to an official complete PDF version of the book 
-      `here <http://www.gaussianprocess.org/gpml/chapters/RW.pdf>`_ . 
+      <http://www.gaussianprocess.org/gpml/chapters/>`_
+      **Gaussian Processes for Machine Learning**,
+      Carl Eduard Rasmussen and Christopher K.I. Williams, MIT Press 2006.
+      Link to an official complete PDF version of the book
+      `here <http://www.gaussianprocess.org/gpml/chapters/RW.pdf>`_ .
 
 .. currentmodule:: sklearn.gaussian_process
 
@@ -616,9 +616,9 @@ References
 Legacy Gaussian Processes
 =========================
 
-In this section, the implementation of Gaussian processes used in sklearn until
-release 0.16.1 is described. Note that this implementation is deprecated and
-will be removed in version 0.18.
+In this section, the implementation of Gaussian processes used in scikit-learn
+until release 0.16.1 is described. Note that this implementation is deprecated
+and will be removed in version 0.18.
 
 An introductory regression example
 ----------------------------------
diff --git a/doc/modules/linear_model.rst b/doc/modules/linear_model.rst
index f52f18309f17..0293cc04a997 100644
--- a/doc/modules/linear_model.rst
+++ b/doc/modules/linear_model.rst
@@ -43,10 +43,10 @@ and will store the coefficients :math:`w` of the linear model in its
 ``coef_`` member::
 
     >>> from sklearn import linear_model
-    >>> clf = linear_model.LinearRegression()
-    >>> clf.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
+    >>> reg = linear_model.LinearRegression()
+    >>> reg.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
     LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
-    >>> clf.coef_
+    >>> reg.coef_
     array([ 0.5,  0.5])
 
 However, coefficient estimates for Ordinary Least Squares rely on the
@@ -101,13 +101,13 @@ arrays X, y and will store the coefficients :math:`w` of the linear model in
 its ``coef_`` member::
 
     >>> from sklearn import linear_model
-    >>> clf = linear_model.Ridge (alpha = .5)
-    >>> clf.fit ([[0, 0], [0, 0], [1, 1]], [0, .1, 1]) # doctest: +NORMALIZE_WHITESPACE
+    >>> reg = linear_model.Ridge (alpha = .5)
+    >>> reg.fit ([[0, 0], [0, 0], [1, 1]], [0, .1, 1]) # doctest: +NORMALIZE_WHITESPACE
     Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None,
           normalize=False, random_state=None, solver='auto', tol=0.001)
-    >>> clf.coef_
+    >>> reg.coef_
     array([ 0.34545455,  0.34545455])
-    >>> clf.intercept_ #doctest: +ELLIPSIS
+    >>> reg.intercept_ #doctest: +ELLIPSIS
     0.13636...
 
 
@@ -138,11 +138,11 @@ as GridSearchCV except that it defaults to Generalized Cross-Validation
 (GCV), an efficient form of leave-one-out cross-validation::
 
     >>> from sklearn import linear_model
-    >>> clf = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])
-    >>> clf.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])       # doctest: +SKIP
+    >>> reg = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])
+    >>> reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])       # doctest: +SKIP
     RidgeCV(alphas=[0.1, 1.0, 10.0], cv=None, fit_intercept=True, scoring=None,
         normalize=False)
-    >>> clf.alpha_                                      # doctest: +SKIP
+    >>> reg.alpha_                                      # doctest: +SKIP
     0.1
 
 .. topic:: References
@@ -182,12 +182,12 @@ the algorithm to fit the coefficients. See :ref:`least_angle_regression`
 for another implementation::
 
     >>> from sklearn import linear_model
-    >>> clf = linear_model.Lasso(alpha = 0.1)
-    >>> clf.fit([[0, 0], [1, 1]], [0, 1])
+    >>> reg = linear_model.Lasso(alpha = 0.1)
+    >>> reg.fit([[0, 0], [1, 1]], [0, 1])
     Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,
        normalize=False, positive=False, precompute=False, random_state=None,
        selection='cyclic', tol=0.0001, warm_start=False)
-    >>> clf.predict([[1, 1]])
+    >>> reg.predict([[1, 1]])
     array([ 0.8])
 
 Also useful for lower-level tasks is the function :func:`lasso_path` that
@@ -441,12 +441,12 @@ function of the norm of its coefficients.
 ::
 
    >>> from sklearn import linear_model
-   >>> clf = linear_model.LassoLars(alpha=.1)
-   >>> clf.fit([[0, 0], [1, 1]], [0, 1])  # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE
+   >>> reg = linear_model.LassoLars(alpha=.1)
+   >>> reg.fit([[0, 0], [1, 1]], [0, 1])  # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE
    LassoLars(alpha=0.1, copy_X=True, eps=..., fit_intercept=True,
         fit_path=True, max_iter=500, normalize=True, positive=False,
         precompute='auto', verbose=False)
-   >>> clf.coef_    # doctest: +ELLIPSIS
+   >>> reg.coef_    # doctest: +ELLIPSIS
    array([ 0.717157...,  0.        ])
 
 .. topic:: Examples:
@@ -604,21 +604,21 @@ Bayesian Ridge Regression is used for regression::
     >>> from sklearn import linear_model
     >>> X = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]]
     >>> Y = [0., 1., 2., 3.]
-    >>> clf = linear_model.BayesianRidge()
-    >>> clf.fit(X, Y)
+    >>> reg = linear_model.BayesianRidge()
+    >>> reg.fit(X, Y)
     BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,
            fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,
            normalize=False, tol=0.001, verbose=False)
 
 After being fitted, the model can then be used to predict new values::
 
-    >>> clf.predict ([[1, 0.]])
+    >>> reg.predict ([[1, 0.]])
     array([ 0.50000013])
 
 
 The weights :math:`w` of the model can be access::
 
-    >>> clf.coef_
+    >>> reg.coef_
     array([ 0.49999993,  0.49999993])
 
 Due to the Bayesian framework, the weights found are slightly different to the
@@ -1233,12 +1233,19 @@ This way, we can solve the XOR problem with a linear classifier::
     >>> import numpy as np
     >>> X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
     >>> y = X[:, 0] ^ X[:, 1]
-    >>> X = PolynomialFeatures(interaction_only=True).fit_transform(X)
+    >>> y
+    array([0, 1, 1, 0])
+    >>> X = PolynomialFeatures(interaction_only=True).fit_transform(X).astype(int)
     >>> X
-    array([[ 1.,  0.,  0.,  0.],
-           [ 1.,  0.,  1.,  0.],
-           [ 1.,  1.,  0.,  0.],
-           [ 1.,  1.,  1.,  1.]])
+    array([[1, 0, 0, 0],
+           [1, 0, 1, 0],
+           [1, 1, 0, 0],
+           [1, 1, 1, 1]])
     >>> clf = Perceptron(fit_intercept=False, n_iter=10, shuffle=False).fit(X, y)
+
+And the classifier "predictions" are perfect::
+
+    >>> clf.predict(X)
+    array([0, 1, 1, 0])
     >>> clf.score(X, y)
     1.0
diff --git a/doc/modules/manifold.rst b/doc/modules/manifold.rst
index 4fc05bb05d23..3c003e0d0cb5 100644
--- a/doc/modules/manifold.rst
+++ b/doc/modules/manifold.rst
@@ -59,10 +59,10 @@ interesting structure within the data will be lost.
 
 To address this concern, a number of supervised and unsupervised linear
 dimensionality reduction frameworks have been designed, such as Principal
-Component Analysis (PCA), Independent Component Analysis, Linear 
-Discriminant Analysis, and others.  These algorithms define specific 
+Component Analysis (PCA), Independent Component Analysis, Linear
+Discriminant Analysis, and others.  These algorithms define specific
 rubrics to choose an "interesting" linear projection of the data.
-These methods can be powerful, but often miss important non-linear 
+These methods can be powerful, but often miss important non-linear
 structure in the data.
 
 
@@ -91,7 +91,7 @@ from the data itself, without the use of predetermined classifications.
     * See :ref:`sphx_glr_auto_examples_manifold_plot_compare_methods.py` for an example of
       dimensionality reduction on a toy "S-curve" dataset.
 
-The manifold learning implementations available in sklearn are
+The manifold learning implementations available in scikit-learn are
 summarized below
 
 .. _isomap:
@@ -121,13 +121,13 @@ The Isomap algorithm comprises three stages:
    nearest neighbors of :math:`N` points in :math:`D` dimensions.
 
 2. **Shortest-path graph search.**  The most efficient known algorithms
-   for this are *Dijkstra's Algorithm*, which is approximately 
+   for this are *Dijkstra's Algorithm*, which is approximately
    :math:`O[N^2(k + \log(N))]`, or the *Floyd-Warshall algorithm*, which
    is :math:`O[N^3]`.  The algorithm can be selected by the user with
    the ``path_method`` keyword of ``Isomap``.  If unspecified, the code
    attempts to choose the best algorithm for the input data.
 
-3. **Partial eigenvalue decomposition.**  The embedding is encoded in the 
+3. **Partial eigenvalue decomposition.**  The embedding is encoded in the
    eigenvectors corresponding to the :math:`d` largest eigenvalues of the
    :math:`N \times N` isomap kernel.  For a dense solver, the cost is
    approximately :math:`O[d N^2]`.  This cost can often be improved using
@@ -191,7 +191,7 @@ The overall complexity of standard LLE is
 * :math:`d` : output dimension
 
 .. topic:: References:
-   
+
    * `"Nonlinear dimensionality reduction by locally linear embedding"
      <http://www.sciencemag.org/content/290/5500/2323.full>`_
      Roweis, S. & Saul, L.  Science 290:2323 (2000)
@@ -221,7 +221,7 @@ It requires ``n_neighbors > n_components``.
    :target: ../auto_examples/manifold/plot_lle_digits.html
    :align: center
    :scale: 50
-   
+
 Complexity
 ----------
 
@@ -232,7 +232,7 @@ The MLLE algorithm comprises three stages:
 2. **Weight Matrix Construction**. Approximately
    :math:`O[D N k^3] + O[N (k-D) k^2]`.  The first term is exactly equivalent
    to that of standard LLE.  The second term has to do with constructing the
-   weight matrix from multiple weights.  In practice, the added cost of 
+   weight matrix from multiple weights.  In practice, the added cost of
    constructing the MLLE weight matrix is relatively small compared to the
    cost of steps 1 and 3.
 
@@ -247,7 +247,7 @@ The overall complexity of MLLE is
 * :math:`d` : output dimension
 
 .. topic:: References:
-     
+
    * `"MLLE: Modified Locally Linear Embedding Using Multiple Weights"
      <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382>`_
      Zhang, Z. & Wang, J.
@@ -271,7 +271,7 @@ It requires ``n_neighbors > n_components * (n_components + 3) / 2``.
    :target: ../auto_examples/manifold/plot_lle_digits.html
    :align: center
    :scale: 50
-   
+
 Complexity
 ----------
 
@@ -308,10 +308,10 @@ Spectral Embedding
 Spectral Embedding (also known as Laplacian Eigenmaps) is one method
 to calculate non-linear embedding. It finds a low dimensional representation
 of the data using a spectral decomposition of the graph Laplacian.
-The graph generated can be considered as a discrete approximation of the 
-low dimensional manifold in the high dimensional space. Minimization of a 
-cost function based on the graph ensures that points close to each other on 
-the manifold are mapped close to each other in the low dimensional space, 
+The graph generated can be considered as a discrete approximation of the
+low dimensional manifold in the high dimensional space. Minimization of a
+cost function based on the graph ensures that points close to each other on
+the manifold are mapped close to each other in the low dimensional space,
 preserving local distances. Spectral embedding can be  performed with the
 function :func:`spectral_embedding` or its object-oriented counterpart
 :class:`SpectralEmbedding`.
@@ -326,9 +326,9 @@ The Spectral Embedding algorithm comprises three stages:
 
 2. **Graph Laplacian Construction**. unnormalized Graph Laplacian
    is constructed as :math:`L = D - A` for and normalized one as
-   :math:`L = D^{-\frac{1}{2}} (D - A) D^{-\frac{1}{2}}`.  
+   :math:`L = D^{-\frac{1}{2}} (D - A) D^{-\frac{1}{2}}`.
 
-3. **Partial Eigenvalue Decomposition**. Eigenvalue decomposition is 
+3. **Partial Eigenvalue Decomposition**. Eigenvalue decomposition is
    done on graph Laplacian
 
 The overall complexity of spectral embedding is
@@ -342,7 +342,7 @@ The overall complexity of spectral embedding is
 .. topic:: References:
 
    * `"Laplacian Eigenmaps for Dimensionality Reduction
-     and Data Representation" 
+     and Data Representation"
      <http://web.cse.ohio-state.edu/~mbelkin/papers/LEM_NC_03.pdf>`_
      M. Belkin, P. Niyogi, Neural Computation, June 2003; 15 (6):1373-1396
 
@@ -354,7 +354,7 @@ Though not technically a variant of LLE, Local tangent space alignment (LTSA)
 is algorithmically similar enough to LLE that it can be put in this category.
 Rather than focusing on preserving neighborhood distances as in LLE, LTSA
 seeks to characterize the local geometry at each neighborhood via its
-tangent space, and performs a global optimization to align these local 
+tangent space, and performs a global optimization to align these local
 tangent spaces to learn the embedding.  LTSA can be performed with function
 :func:`locally_linear_embedding` or its object-oriented counterpart
 :class:`LocallyLinearEmbedding`, with the keyword ``method = 'ltsa'``.
@@ -421,7 +421,7 @@ space and the similarities/dissimilarities.
    :target: ../auto_examples/manifold/plot_lle_digits.html
    :align: center
    :scale: 50
- 
+
 
 Let :math:`S` be the similarity matrix, and :math:`X` the coordinates of the
 :math:`n` input points. Disparities :math:`\hat{d}_{ij}` are transformation of
@@ -456,7 +456,7 @@ order to avoid that, the disparities :math:`\hat{d}_{ij}` are normalized.
    :target: ../auto_examples/manifold/plot_mds.html
    :align: center
    :scale: 60
-  
+
 
 .. topic:: References:
 
@@ -499,7 +499,7 @@ probabilities in the original space and the embedded space will be minimized
 by gradient descent. Note that the KL divergence is not convex, i.e.
 multiple restarts with different initializations will end up in local minima
 of the KL divergence. Hence, it is sometimes useful to try different seeds
-and select the embedding with the lowest KL divergence. 
+and select the embedding with the lowest KL divergence.
 
 The disadvantages to using t-SNE are roughly:
 
@@ -552,7 +552,7 @@ divergence will increase during optimization. More tips can be found in
 Laurens van der Maaten's FAQ (see references). The last parameter, angle,
 is a tradeoff between performance and accuracy. Larger angles imply that we
 can approximate larger regions by a single point,leading to better speed
-but less accurate results. 
+but less accurate results.
 
 Barnes-Hut t-SNE
 ----------------
@@ -560,8 +560,8 @@ Barnes-Hut t-SNE
 The Barnes-Hut t-SNE that has been implemented here is usually much slower than
 other manifold learning algorithms. The optimization is quite difficult
 and the computation of the gradient is :math:`O[d N log(N)]`, where :math:`d`
-is the number of output dimensions and :math:`N` is the number of samples. The 
-Barnes-Hut method improves on the exact method where t-SNE complexity is 
+is the number of output dimensions and :math:`N` is the number of samples. The
+Barnes-Hut method improves on the exact method where t-SNE complexity is
 :math:`O[d N^2]`, but has several other notable differences:
 
 * The Barnes-Hut implementation only works when the target dimensionality is 3
diff --git a/doc/modules/mixture.rst b/doc/modules/mixture.rst
index 9979d36c93a6..5e3c2c448de7 100644
--- a/doc/modules/mixture.rst
+++ b/doc/modules/mixture.rst
@@ -133,40 +133,13 @@ parameters to maximize the likelihood of the data given those
 assignments. Repeating this process is guaranteed to always converge
 to a local optimum.
 
-.. _vbgmm:
+.. _bgmm:
 
-VBGMM: variational Gaussian mixtures
-====================================
+Bayesian Gaussian Mixture
+=========================
 
-The :class:`VBGMM` object implements a variant of the Gaussian mixture
-model with :ref:`variational inference <variational_inference>` algorithms.
-
-Pros and cons of class :class:`VBGMM`: variational inference
-------------------------------------------------------------
-
-Pros
-.....
-
-:Regularization: due to the incorporation of prior information,
-   variational solutions have less pathological special cases than
-   expectation-maximization solutions. One can then use full
-   covariance matrices in high dimensions or in cases where some
-   components might be centered around a single point without
-   risking divergence.
-
-Cons
-.....
-
-:Bias: to regularize a model one has to add biases. The
-   variational algorithm will bias all the means towards the origin
-   (part of the prior information adds a "ghost point" in the origin
-   to every mixture component) and it will bias the covariances to
-   be more spherical. It will also, depending on the concentration
-   parameter, bias the cluster structure either towards uniformity
-   or towards a rich-get-richer scenario.
-
-:Hyperparameters: this algorithm needs an extra hyperparameter
-   that might need experimental tuning via cross-validation.
+The :class:`BayesianGaussianMixture` object implements a variant of the Gaussian
+mixture model with variational inference algorithms.
 
 .. _variational_inference:
 
@@ -175,7 +148,7 @@ Estimation algorithm: variational inference
 
 Variational inference is an extension of expectation-maximization that
 maximizes a lower bound on model evidence (including
-priors) instead of data likelihood.  The principle behind
+priors) instead of data likelihood. The principle behind
 variational methods is the same as expectation-maximization (that is
 both are iterative algorithms that alternate between finding the
 probabilities for each point to be generated by each mixture and
@@ -188,13 +161,54 @@ much so as to render usage unpractical.
 
 Due to its Bayesian nature, the variational algorithm needs more
 hyper-parameters than expectation-maximization, the most
-important of these being the concentration parameter ``alpha``. Specifying
-a high value of alpha leads more often to uniformly-sized mixture
+important of these being the concentration parameter ``dirichlet_concentration_prior``. Specifying
+a high value of prior of the dirichlet concentration leads more often to uniformly-sized mixture
 components, while specifying small (between 0 and 1) values will lead
 to some mixture components getting almost all the points while most
 mixture components will be centered on just a few of the remaining
 points.
 
+.. figure:: ../auto_examples/mixture/images/sphx_glr_plot_bayesian_gaussian_mixture_001.png
+   :target: ../auto_examples/mixture/plot_bayesian_gaussian_mixture.html
+   :align: center
+   :scale: 50%
+
+.. topic:: Examples:
+
+    * See :ref:`plot_bayesian_gaussian_mixture.py` for a comparaison of
+      the results of the ``BayesianGaussianMixture`` for different values
+      of the parameter ``dirichlet_concentration_prior``.
+
+Pros and cons of variational inference with :class:BayesianGaussianMixture
+--------------------------------------------------------------------------
+
+Pros
+.....
+
+:Regularization: due to the incorporation of prior information,
+   variational solutions have less pathological special cases than
+   expectation-maximization solutions.
+
+:Automatic selection: when `dirichlet_concentration_prior` is small enough and
+`n_components` is larger than what is found necessary by the model, the
+Variational Bayesian mixture model has a natural tendency to set some mixture
+weights values close to zero. This makes it possible to let the model choose a
+suitable number of effective components automatically.
+
+Cons
+.....
+
+:Bias: to regularize a model one has to add biases. The
+   variational algorithm will bias all the means towards the origin
+   (part of the prior information adds a "ghost point" in the origin
+   to every mixture component) and it will bias the covariances to
+   be more spherical. It will also, depending on the concentration
+   parameter, bias the cluster structure either towards uniformity
+   or towards a rich-get-richer scenario.
+
+:Hyperparameters: this algorithm needs an extra hyperparameter
+   that might need experimental tuning via cross-validation.
+
 .. _dpgmm:
 
 DPGMM: Infinite Gaussian mixtures
diff --git a/doc/modules/model_persistence.rst b/doc/modules/model_persistence.rst
index 8f8bc4a6206c..29080ed65fc6 100644
--- a/doc/modules/model_persistence.rst
+++ b/doc/modules/model_persistence.rst
@@ -63,9 +63,12 @@ Security & maintainability limitations
 pickle (and joblib by extension), has some issues regarding maintainability
 and security. Because of this,
 
-* Never unpickle untrusted data
-* Models saved in one version of scikit-learn might not load in another
-  version.
+* Never unpickle untrusted data as it could lead to malicious code being 
+  executed upon loading.
+* While models saved using one version of scikit-learn might load in 
+  other versions, this is entirely unsupported and inadvisable. It should 
+  also be kept in mind that operations performed on such data could give
+  different and unexpected results.
 
 In order to rebuild a similar model with future versions of scikit-learn,
 additional metadata should be saved along the pickled model:
diff --git a/doc/modules/naive_bayes.rst b/doc/modules/naive_bayes.rst
index bcb6dd9fb644..7d83ba38d1e7 100644
--- a/doc/modules/naive_bayes.rst
+++ b/doc/modules/naive_bayes.rst
@@ -187,8 +187,8 @@ for which the full training set might not fit in memory. To handle this case,
 :class:`MultinomialNB`, :class:`BernoulliNB`, and :class:`GaussianNB`
 expose a ``partial_fit`` method that can be used
 incrementally as done with other classifiers as demonstrated in
-:ref:`sphx_glr_auto_examples_applications_plot_out_of_core_classification.py`. Both discrete
-classifiers support sample weighting; :class:`GaussianNB` does not.
+:ref:`sphx_glr_auto_examples_applications_plot_out_of_core_classification.py`. All naive Bayes
+classifiers support sample weighting.
 
 Contrary to the ``fit`` method, the first call to ``partial_fit`` needs to be
 passed the list of all the expected class labels.
diff --git a/doc/modules/pipeline.rst b/doc/modules/pipeline.rst
index 344d3faa19aa..ddbcc022d78c 100644
--- a/doc/modules/pipeline.rst
+++ b/doc/modules/pipeline.rst
@@ -37,17 +37,16 @@ is an estimator object::
     >>> from sklearn.pipeline import Pipeline
     >>> from sklearn.svm import SVC
     >>> from sklearn.decomposition import PCA
-    >>> estimators = [('reduce_dim', PCA()), ('svm', SVC())]
-    >>> clf = Pipeline(estimators)
-    >>> clf # doctest: +NORMALIZE_WHITESPACE
+    >>> estimators = [('reduce_dim', PCA()), ('clf', SVC())]
+    >>> pipe = Pipeline(estimators)
+    >>> pipe # doctest: +NORMALIZE_WHITESPACE
     Pipeline(steps=[('reduce_dim', PCA(copy=True, iterated_power=4,
     n_components=None, random_state=None, svd_solver='auto', tol=0.0,
-    whiten=False)), ('svm', SVC(C=1.0, cache_size=200, class_weight=None,
+    whiten=False)), ('clf', SVC(C=1.0, cache_size=200, class_weight=None,
     coef0=0.0, decision_function_shape=None, degree=3, gamma='auto',
     kernel='rbf', max_iter=-1, probability=False, random_state=None,
     shrinking=True, tol=0.001, verbose=False))])
 
-
 The utility function :func:`make_pipeline` is a shorthand
 for constructing pipelines;
 it takes a variable number of estimators and returns a pipeline,
@@ -64,23 +63,23 @@ filling in the names automatically::
 
 The estimators of a pipeline are stored as a list in the ``steps`` attribute::
 
-    >>> clf.steps[0]
+    >>> pipe.steps[0]
     ('reduce_dim', PCA(copy=True, iterated_power=4, n_components=None, random_state=None,
       svd_solver='auto', tol=0.0, whiten=False))
 
 and as a ``dict`` in ``named_steps``::
 
-    >>> clf.named_steps['reduce_dim']
+    >>> pipe.named_steps['reduce_dim']
     PCA(copy=True, iterated_power=4, n_components=None, random_state=None,
       svd_solver='auto', tol=0.0, whiten=False)
 
 Parameters of the estimators in the pipeline can be accessed using the
 ``<estimator>__<parameter>`` syntax::
 
-    >>> clf.set_params(svm__C=10) # doctest: +NORMALIZE_WHITESPACE
+    >>> pipe.set_params(clf__C=10) # doctest: +NORMALIZE_WHITESPACE
     Pipeline(steps=[('reduce_dim', PCA(copy=True, iterated_power=4,
         n_components=None, random_state=None, svd_solver='auto', tol=0.0,
-        whiten=False)), ('svm', SVC(C=10, cache_size=200, class_weight=None,
+        whiten=False)), ('clf', SVC(C=10, cache_size=200, class_weight=None,
         coef0=0.0, decision_function_shape=None, degree=3, gamma='auto',
         kernel='rbf', max_iter=-1, probability=False, random_state=None,
         shrinking=True, tol=0.001, verbose=False))])
@@ -90,9 +89,17 @@ This is particularly important for doing grid searches::
 
     >>> from sklearn.model_selection import GridSearchCV
     >>> params = dict(reduce_dim__n_components=[2, 5, 10],
-    ...               svm__C=[0.1, 10, 100])
-    >>> grid_search = GridSearchCV(clf, param_grid=params)
+    ...               clf__C=[0.1, 10, 100])
+    >>> grid_search = GridSearchCV(pipe, param_grid=params)
+
+Individual steps may also be replaced as parameters, and non-final steps may be
+ignored by setting them to ``None``::
 
+    >>> from sklearn.linear_model import LogisticRegression
+    >>> params = dict(reduce_dim=[None, PCA(5), PCA(10)],
+    ...               clf=[SVC(), LogisticRegression()],
+    ...               clf__C=[0.1, 10, 100])
+    >>> grid_search = GridSearchCV(pipe, param_grid=params)
 
 .. topic:: Examples:
 
@@ -172,6 +179,15 @@ Like pipelines, feature unions have a shorthand constructor called
 :func:`make_union` that does not require explicit naming of the components.
 
 
+Like ``Pipeline``, individual steps may be replaced using ``set_params``,
+and ignored by setting to ``None``::
+
+    >>> combined.set_params(kernel_pca=None) # doctest: +NORMALIZE_WHITESPACE
+    FeatureUnion(n_jobs=1, transformer_list=[('linear_pca', PCA(copy=True,
+          iterated_power=4, n_components=None, random_state=None,
+          svd_solver='auto', tol=0.0, whiten=False)), ('kernel_pca', None)],
+        transformer_weights=None)
+
 .. topic:: Examples:
 
  * :ref:`sphx_glr_auto_examples_feature_stacker.py`
diff --git a/doc/modules/preprocessing.rst b/doc/modules/preprocessing.rst
index ac607228c01e..22e4a930bb25 100644
--- a/doc/modules/preprocessing.rst
+++ b/doc/modules/preprocessing.rst
@@ -16,7 +16,7 @@ Standardization, or mean removal and variance scaling
 =====================================================
 
 **Standardization** of datasets is a **common requirement for many
-machine learning estimators** implemented in the scikit; they might behave
+machine learning estimators** implemented in scikit-learn; they might behave
 badly if the individual features do not more or less look like standard
 normally distributed data: Gaussian with **zero mean and unit variance**.
 
@@ -187,7 +187,7 @@ sparse inputs, especially if features are on different scales.
 :class:`MaxAbsScaler`  and :func:`maxabs_scale` were specifically designed
 for scaling sparse data, and are the recommended way to go about this.
 However, :func:`scale` and :class:`StandardScaler` can accept ``scipy.sparse``
-matrices  as input, as long as ``with_centering=False`` is explicitly passed
+matrices  as input, as long as ``with_mean=False`` is explicitly passed
 to the constructor. Otherwise a ``ValueError`` will be raised as
 silently centering would break the sparsity and would often crash the
 execution by allocating excessive amounts of memory unintentionally.
@@ -445,49 +445,28 @@ values, either using the mean, the median or the most frequent value of
 the row or column in which the missing values are located. This class
 also allows for different missing values encodings.
 
-Imputing missing values ordinarily discards the information of which values
-were missing. Setting ``add_indicator_features=True`` allows the knowledge of
-which features were imputed to be exploited by a downstream estimator
-by adding features that indicate which elements have been imputed.
-
 The following snippet demonstrates how to replace missing values,
 encoded as ``np.nan``, using the mean value of the columns (axis 0)
-that contain the missing values. In case there is a feature which has
-all missing features, it is discarded when transformed. Also if the
-indicator matrix is requested (``add_indicator_features=True``),
-then the shape of the transformed input is 
-``(n_samples, n_features_new + len(imputed_features_))`` ::
+that contain the missing values::
 
     >>> import numpy as np
     >>> from sklearn.preprocessing import Imputer
     >>> imp = Imputer(missing_values='NaN', strategy='mean', axis=0)
-    >>> imp.fit([[1, 2], [np.nan, 3], [7, 6]])  # doctest: +NORMALIZE_WHITESPACE
-    Imputer(add_indicator_features=False, axis=0, copy=True, missing_values='NaN',
-        strategy='mean', verbose=0)
+    >>> imp.fit([[1, 2], [np.nan, 3], [7, 6]])
+    Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)
     >>> X = [[np.nan, 2], [6, np.nan], [7, 6]]
     >>> print(imp.transform(X))                           # doctest: +ELLIPSIS
     [[ 4.          2.        ]
      [ 6.          3.666...]
      [ 7.          6.        ]]
-    >>> imp_with_in = Imputer(missing_values='NaN', strategy='mean', axis=0,add_indicator_features=True)
-    >>> imp_with_in.fit([[1, 2], [np.nan, 3], [7, 6]])
-    Imputer(add_indicator_features=True, axis=0, copy=True, missing_values='NaN',
-        strategy='mean', verbose=0)
-    >>> print(imp_with_in.transform(X))                           # doctest: +ELLIPSIS
-    [[ 4.          2.          1.          0.        ]
-     [ 6.          3.66666667  0.          1.        ]
-     [ 7.          6.          0.          0.        ]]
-    >>> print(imp_with_in.imputed_features_)
-    [0 1]
 
 The :class:`Imputer` class also supports sparse matrices::
 
     >>> import scipy.sparse as sp
     >>> X = sp.csc_matrix([[1, 2], [0, 3], [7, 6]])
     >>> imp = Imputer(missing_values=0, strategy='mean', axis=0)
-    >>> imp.fit(X) # doctest: +NORMALIZE_WHITESPACE
-    Imputer(add_indicator_features=False, axis=0, copy=True, missing_values=0,
-        strategy='mean', verbose=0)
+    >>> imp.fit(X)
+    Imputer(axis=0, copy=True, missing_values=0, strategy='mean', verbose=0)
     >>> X_test = sp.csc_matrix([[0, 2], [6, 0], [7, 6]])
     >>> print(imp.transform(X_test))                      # doctest: +ELLIPSIS
     [[ 4.          2.        ]
diff --git a/doc/related_projects.rst b/doc/related_projects.rst
index d2c3ebbfcaa9..149a8ff632aa 100644
--- a/doc/related_projects.rst
+++ b/doc/related_projects.rst
@@ -114,6 +114,7 @@ and tasks.
 
 - `multiisotonic <https://github.com/alexfields/multiisotonic>`_ Isotonic regression on multidimensional features.
 
+- `spherecluster <https://github.com/clara-labs/spherecluster>`_ Spherical K-means and mixture of von Mises Fisher clustering routines for data on the unit hypersphere.
 
 Statistical learning with Python
 --------------------------------
diff --git a/doc/themes/scikit-learn/static/nature.css_t b/doc/themes/scikit-learn/static/nature.css_t
index 5e4c00783db7..c9c228fc7b7e 100644
--- a/doc/themes/scikit-learn/static/nature.css_t
+++ b/doc/themes/scikit-learn/static/nature.css_t
@@ -116,6 +116,7 @@ div.navbar div.nav-icon {
         background: #ff9c34;
         padding: 5px 10px;
         border-radius: 5px;
+        z-index: 10;
     }
     div.navbar ul li {
         display: none;
@@ -137,6 +138,7 @@ div.navbar div.nav-icon {
         top: 10px;
         margin-right: 10px;
         background: #ff9c34;
+        z-index: 9;
     }
     div.navbar.responsive > ul li {
         display: flex;
@@ -1186,6 +1188,7 @@ div.navbar ul.dropdown-menu li a.btn {
   vertical-align: middle;
   padding: 0;
   padding-left: 4px;
+  font-size: 8px;
 }
 
 .navbar .btn-group .btn {
@@ -1197,10 +1200,6 @@ div.navbar ul.dropdown-menu li a.btn {
   box-shadow: inset 0 0px 0 rgba(255,255,255,.2), 0 0px 0px rgba(0,0,0,.05);
 }
 
-a.btn.dropdown-toggle,  a.btn.dropdown-toggle:hover{
-  vertical-align: baseline;
-}
-
 li#other-versions {
   position: absolute;
   left: inherit;
@@ -1268,7 +1267,7 @@ li#other-versions {
   color: #fff;
   vertical-align: middle;
   border-top-color: rgb(255, 255, 255);
-  padding-bottom: 8px;
+  padding-bottom: 5px;
 }
 
 .navbar .dropdown-menu .divider {
diff --git a/doc/tutorial/statistical_inference/model_selection.rst b/doc/tutorial/statistical_inference/model_selection.rst
index e61749b00c19..ef3568ffb0bc 100644
--- a/doc/tutorial/statistical_inference/model_selection.rst
+++ b/doc/tutorial/statistical_inference/model_selection.rst
@@ -61,7 +61,7 @@ This example shows an example usage of the ``split`` method.
 
     >>> from sklearn.model_selection import KFold, cross_val_score
     >>> X = ["a", "a", "b", "c", "c", "c"]
-    >>> k_fold = KFold(n_folds=3)
+    >>> k_fold = KFold(n_splits=3)
     >>> for train_indices, test_indices in k_fold.split(X):
     ...      print('Train: %s | test: %s' % (train_indices, test_indices))
     Train: [2 3 4 5] | test: [0 1]
@@ -70,7 +70,7 @@ This example shows an example usage of the ``split`` method.
 
 The cross-validation can then be performed easily::
 
-    >>> kfold = KFold(n_folds=3)
+    >>> kfold = KFold(n_splits=3)
     >>> [svc.fit(X_digits[train], y_digits[train]).score(X_digits[test], y_digits[test])
     ...          for train, test in k_fold.split(X_digits)]
     [0.93489148580968284, 0.95659432387312182, 0.93989983305509184]
@@ -106,11 +106,11 @@ scoring method.
 
    *
 
-    - :class:`KFold` **(n_folds, shuffle, random_state)**
+    - :class:`KFold` **(n_splits, shuffle, random_state)**
 
     - :class:`StratifiedKFold` **(n_iter, test_size, train_size, random_state)**
 
-    - :class:`LabelKFold` **(n_folds, shuffle, random_state)**
+    - :class:`LabelKFold` **(n_splits, shuffle, random_state)**
 
 
    *
@@ -207,7 +207,7 @@ Grid-search
 
 .. currentmodule:: sklearn.model_selection
 
-The sklearn provides an object that, given data, computes the score
+scikit-learn provides an object that, given data, computes the score
 during the fit of an estimator on a parameter grid and chooses the
 parameters to maximize the cross-validation score. This object takes an
 estimator during the construction and exposes an estimator API::
@@ -257,9 +257,9 @@ Cross-validated estimators
 ----------------------------
 
 Cross-validation to set a parameter can be done more efficiently on an
-algorithm-by-algorithm basis. This is why for certain estimators the
-sklearn exposes :ref:`cross_validation` estimators that set their parameter
-automatically by cross-validation::
+algorithm-by-algorithm basis. This is why, for certain estimators,
+scikit-learn exposes :ref:`cross_validation` estimators that set their
+parameter automatically by cross-validation::
 
     >>> from sklearn import linear_model, datasets
     >>> lasso = linear_model.LassoCV()
diff --git a/doc/tutorial/text_analytics/working_with_text_data.rst b/doc/tutorial/text_analytics/working_with_text_data.rst
index 633e1d994c16..e1aa2d50fca4 100644
--- a/doc/tutorial/text_analytics/working_with_text_data.rst
+++ b/doc/tutorial/text_analytics/working_with_text_data.rst
@@ -212,7 +212,7 @@ dictionary of features and transform documents to feature vectors::
   >>> X_train_counts.shape
   (2257, 35788)
 
-:class:`CountVectorizer` supports counts of N-grams of words or consequective characters.
+:class:`CountVectorizer` supports counts of N-grams of words or consecutive characters.
 Once fitted, the vectorizer has built a dictionary of feature indices::
 
   >>> count_vect.vocabulary_.get(u'algorithm')
diff --git a/doc/tutorial/text_analytics/working_with_text_data_fixture.py b/doc/tutorial/text_analytics/working_with_text_data_fixture.py
index c10f5c4f4d14..d620986d0715 100644
--- a/doc/tutorial/text_analytics/working_with_text_data_fixture.py
+++ b/doc/tutorial/text_analytics/working_with_text_data_fixture.py
@@ -1,7 +1,7 @@
 """Fixture module to skip the datasets loading when offline
 
 The 20 newsgroups data is rather large and some CI workers such as travis are
-stateless hence will not cache the dataset as regular sklearn users would do.
+stateless hence will not cache the dataset as regular scikit-learn users would.
 
 The following will skip the execution of the working_with_text_data.rst doctests
 if the proper environment variable is configured (see the source code of
diff --git a/doc/whats_new.rst b/doc/whats_new.rst
index a4154de9bd27..8471a8bec1b2 100644
--- a/doc/whats_new.rst
+++ b/doc/whats_new.rst
@@ -62,6 +62,17 @@ Model Selection Enhancements and API Changes
     the corresponding parameter is not applicable. Additionally a list of all
     the parameter dicts are stored at ``results_['params']``.
 
+  - **Parameters ``n_folds`` and ``n_iter`` renamed to ``n_splits``**
+
+    Some parameter names have changed:
+    The ``n_folds`` parameter in :class:`model_selection.KFold`,
+    :class:`model_selection.LabelKFold`, and
+    :class:`model_selection.StratifiedKFold` is now renamed to ``n_splits``.
+    The ``n_iter`` parameter in :class:`model_selection.ShuffleSplit`,
+    :class:`model_selection.LabelShuffleSplit`,
+    and :class:`model_selection.StratifiedShuffleSplit` is now renamed
+    to ``n_splits``.
+
 
 New features
 ............
@@ -130,6 +141,12 @@ New features
      <https://github.com/scikit-learn/scikit-learn/pull/6954>`_) by `Nelson
      Liu`_
 
+   - Added new cross-validation splitter
+     :class:`model_selection.TimeSeriesSplit` to handle time series data.
+     (`#6586
+     <https://github.com/scikit-learn/scikit-learn/pull/6586>`_) by `YenChen
+     Lin`_
+
 Enhancements
 ............
 
@@ -230,13 +247,51 @@ Enhancements
      (`#6846 <https://github.com/scikit-learn/scikit-learn/pull/6846>`_)
      By `Sebastian Säger`_ and `YenChen Lin`_.
 
-   - Added new return type ``(data, target)`` : tuple option to
-     :func:`load_iris` dataset, 
-     (`#7049 <https://github.com/scikit-learn/scikit-learn/pull/7049>`_)
+   - Added parameter ``return_X_y`` and return type ``(data, target) : tuple`` option to
+     :func:`load_iris` dataset
+     `#7049 <https://github.com/scikit-learn/scikit-learn/pull/7049>`_,
      :func:`load_breast_cancer` dataset
-     (`#7152 <https://github.com/scikit-learn/scikit-learn/pull/7152>`_) by
+     `#7152 <https://github.com/scikit-learn/scikit-learn/pull/7152>`_,
+     :func:`load_digits` dataset,
+     :func:`load_diabetes` dataset,
+     :func:`load_linnerud` dataset,
+     :func:`load_boston` dataset
+     `#7154 <https://github.com/scikit-learn/scikit-learn/pull/7154>`_ by
      `Manvendra Singh`_.
 
+   - :class:`RobustScaler` now accepts ``quantile_range`` parameter.
+     (`#5929 <https://github.com/scikit-learn/scikit-learn/pull/5929>`_)
+     By `Konstantin Podshumok`_.
+
+   - The memory footprint is reduced (sometimes greatly) for :class:`BaseBagging`
+     and classes that inherit from it, i.e, :class:`BaggingClassifier`,
+     :class:`BaggingRegressor`, and :class:`IsolationForest`, by dynamically
+     generating attribute ``estimators_samples_`` only when it is needed.
+     By `David Staub`_.
+
+   - :class:`linear_model.ElasticNet` and :class:`linear_model.Lasso`
+     now works with ``np.float32`` input data without converting it
+     into ``np.float64``. This allows to reduce the memory
+     consumption.
+     (`#6913 <https://github.com/scikit-learn/scikit-learn/pull/6913>`_)
+     By `YenChen Lin`_.
+
+   - Added ``labels`` flag to :class:`metrics.log_loss` to to explicitly provide
+     the labels when the number of classes in ``y_true`` and ``y_pred`` differ.
+     (`#7239 <https://github.com/scikit-learn/scikit-learn/pull/7239/>`_)
+     by `Hong Guangguo`_ with help from `Mads Jensen`_ and `Nelson Liu`_.
+
+   - Added ``n_jobs`` and ``sample_weights`` parameters for :class:`VotingClassifier`
+     to fit underlying estimators in parallel.
+     (`#5805 <https://github.com/scikit-learn/scikit-learn/pull/5805>`_)
+     By `Ibraim Ganiev`_.
+
+   - Added support for substituting or disabling :class:`pipeline.Pipeline`
+     and :class:`pipeline.FeatureUnion` components using the ``set_params``
+     interface that powers :mod:`sklearn.grid_search`.
+     See :ref:`example_plot_compare_reduction.py`. By `Joel Nothman`_ and
+     `Robert McGibbon`_.
+
 Bug fixes
 .........
 
@@ -304,7 +359,7 @@ Bug fixes
     - Fix a bug where some formats of ``scipy.sparse`` matrix, and estimators
       with them as parameters, could not be passed to :func:`base.clone`.
       By `Loic Esteve`_.
-      
+
     - Fix bug in :class:`neighbors.RadiusNeighborsClassifier` where an error
       occurred when there were outliers being labelled and a weight function
       specified (`#6902
@@ -320,6 +375,17 @@ Bug fixes
       (`#7101 <https://github.com/scikit-learn/scikit-learn/pull/7101>`_)
       By `Ibraim Ganiev`_.
 
+    - Fix sparse input support in :func:`silhouette_score` as well as example
+      examples/text/document_clustering.py. By `YenChen Lin`_.
+
+    - :func:`_transform_selected` now always passes a copy of `X` to transform
+      function when `copy=True` (`#7194
+      <https://github.com/scikit-learn/scikit-learn/issues/7194>`_). By `Caio
+      Oliveira <https://github.com/caioaao>`_.
+
+    - Fix :class:`linear_model.ElasticNet` sparse decision function to match
+    output with dense in the multioutput case.
+
 API changes summary
 -------------------
 
@@ -336,10 +402,19 @@ API changes summary
    - Access to public attributes ``.X_`` and ``.y_`` has been deprecated in
      :class:`isotonic.IsotonicRegression`. By `Jonathan Arfa`_.
 
+   - The old :class:`VBGMM` is deprecated in favor of the new
+     :class:`BayesianGaussianMixture`. The new class solves the computational
+     problems of the old class and computes the Variational Bayesian Gaussian
+     mixture faster than before.
+     Ref :ref:`b` for more information.
+     (`#6651 <https://github.com/scikit-learn/scikit-learn/pull/6651>`_) by
+     `Wei Xue`_ and `Thierry Guillemot`_.
+
    - The old :class:`GMM` is deprecated in favor of the new
      :class:`GaussianMixture`. The new class computes the Gaussian mixture
      faster than before and some of computational problems have been solved.
-     By `Wei Xue`_ and `Thierry Guillemot`_.
+     (`#6666 <https://github.com/scikit-learn/scikit-learn/pull/6666>`_) by
+     `Wei Xue`_ and `Thierry Guillemot`_.
 
    - The ``grid_scores_`` attribute of :class:`model_selection.GridSearchCV`
      and :class:`model_selection.RandomizedSearchCV` is deprecated in favor of
@@ -348,6 +423,17 @@ API changes summary
      (`#6697 <https://github.com/scikit-learn/scikit-learn/pull/6697>`_) by
      `Raghav R V`_.
 
+   - The parameters ``n_iter`` or ``n_folds`` in old CV splitters are replaced
+     by the new parameter ``n_splits`` since it can provide a consistent
+     and unambiguous interface to represent the number of train-test splits.
+     (`#7187 <https://github.com/scikit-learn/scikit-learn/pull/7187>`_)
+     by `YenChen Lin`_.
+
+    - ``classes`` parameter was renamed to ``labels`` in
+      :func:`metrics.classification.hamming_loss`.
+      (`#7260 <https://github.com/scikit-learn/scikit-learn/pull/7260>`_) by
+      `Sebastián Vanrell`_.
+
 
 .. currentmodule:: sklearn
 
@@ -4322,3 +4408,13 @@ David Huard, Dave Morrill, Ed Schofield, Travis Oliphant, Pearu Peterson.
 .. _Manvendra Singh: https://github.com/manu-chroma
 
 .. _Ibraim Ganiev: https://github.com/olologin
+
+.. _Konstantin Podshumok: https://github.com/podshumok
+
+.. _David Staub: https://github.com/staubda
+
+.. _Hong Guangguo: https://github.com/hongguangguo
+
+.. _Mads Jensen: https://github.com/indianajensen
+
+.. _Sebastián Vanrell: https://github.com/srvanrell
diff --git a/examples/applications/plot_outlier_detection_housing.py b/examples/applications/plot_outlier_detection_housing.py
index 8b7d6b5418f1..41c697e2e2d2 100644
--- a/examples/applications/plot_outlier_detection_housing.py
+++ b/examples/applications/plot_outlier_detection_housing.py
@@ -19,7 +19,8 @@
 able to focus on the main mode of the data distribution, it sticks to the
 assumption that the data should be Gaussian distributed, yielding some biased
 estimation of the data structure, but yet accurate to some extent.
-The One-Class SVM algorithm
+The One-Class SVM does not assume any parametric form of the data distribution
+and can therefore model the complex shape of the data much better.
 
 First example
 -------------
diff --git a/examples/ensemble/plot_gradient_boosting_oob.py b/examples/ensemble/plot_gradient_boosting_oob.py
index 39e623f261cc..dfae1ad9b8a9 100644
--- a/examples/ensemble/plot_gradient_boosting_oob.py
+++ b/examples/ensemble/plot_gradient_boosting_oob.py
@@ -74,14 +74,14 @@ def heldout_score(clf, X_test, y_test):
     return score
 
 
-def cv_estimate(n_folds=3):
-    cv = KFold(n_folds=n_folds)
+def cv_estimate(n_splits=3):
+    cv = KFold(n_splits=n_splits)
     cv_clf = ensemble.GradientBoostingClassifier(**params)
     val_scores = np.zeros((n_estimators,), dtype=np.float64)
     for train, test in cv.split(X_train, y_train):
         cv_clf.fit(X_train[train], y_train[train])
         val_scores += heldout_score(cv_clf, X_train[test], y_train[test])
-    val_scores /= n_folds
+    val_scores /= n_splits
     return val_scores
 
 
diff --git a/examples/feature_stacker.py b/examples/feature_stacker.py
index 4ce574aa36bc..d1d32dfb9e2f 100644
--- a/examples/feature_stacker.py
+++ b/examples/feature_stacker.py
@@ -30,7 +30,7 @@
 
 X, y = iris.data, iris.target
 
-# This dataset is way to high-dimensional. Better do PCA:
+# This dataset is way too high-dimensional. Better do PCA:
 pca = PCA(n_components=2)
 
 # Maybe some original features where good, too?
diff --git a/examples/hetero_feature_union.py b/examples/hetero_feature_union.py
index 0ec565a8f543..0af24a3c05c3 100644
--- a/examples/hetero_feature_union.py
+++ b/examples/hetero_feature_union.py
@@ -51,7 +51,7 @@ class ItemSelector(BaseEstimator, TransformerMixin):
 
     >> len(data[key]) == n_samples
 
-    Please note that this is the opposite convention to sklearn feature
+    Please note that this is the opposite convention to scikit-learn feature
     matrixes (where the first index corresponds to sample).
 
     ItemSelector only requires that the collection implement getitem
diff --git a/examples/missing_values.py b/examples/missing_values.py
index 14de3f4e38ad..8a0895f9a589 100644
--- a/examples/missing_values.py
+++ b/examples/missing_values.py
@@ -8,11 +8,6 @@
 Imputing does not always improve the predictions, so please check via cross-validation.
 Sometimes dropping rows or using marker values is more effective.
 
-In this example, we artificially mark some of the elements in complete
-dataset as missing. Then we estimate performance using the complete dataset,
-dataset without the missing samples, after imputation without the indicator
-matrix and imputation with the indicator matrix for the missing values.
-
 Missing values can be replaced by the mean, the median or the most frequent
 value using the ``strategy`` hyper-parameter.
 The median is a more robust estimator for data with high magnitude variables
@@ -20,10 +15,9 @@
 
 Script output::
 
-  Score with the complete dataset = 0.56
+  Score with the entire dataset = 0.56
   Score without the samples containing missing values = 0.48
   Score after imputation of the missing values = 0.55
-  Score after imputation with indicator features = 0.57
 
 In this case, imputing helps the classifier get close to the original score.
   
@@ -46,11 +40,11 @@
 # Estimate the score on the entire dataset, with no missing values
 estimator = RandomForestRegressor(random_state=0, n_estimators=100)
 score = cross_val_score(estimator, X_full, y_full).mean()
-print("Score with the complete dataset = %.2f" % score)
+print("Score with the entire dataset = %.2f" % score)
 
 # Add missing values in 75% of the lines
 missing_rate = 0.75
-n_missing_samples = int(n_samples * missing_rate)
+n_missing_samples = np.floor(n_samples * missing_rate)
 missing_samples = np.hstack((np.zeros(n_samples - n_missing_samples,
                                       dtype=np.bool),
                              np.ones(n_missing_samples,
@@ -76,12 +70,3 @@
                                                        n_estimators=100))])
 score = cross_val_score(estimator, X_missing, y_missing).mean()
 print("Score after imputation of the missing values = %.2f" % score)
-
-# Estimate score after imputation of the missing values with indicator matrix
-estimator = Pipeline([("imputer", Imputer(missing_values=0,
-                                          strategy="mean",
-                                          axis=0, add_indicator_features=True)),
-                      ("forest", RandomForestRegressor(random_state=0,
-                                                       n_estimators=100))])
-score = cross_val_score(estimator, X_missing, y_missing).mean()
-print("Score after imputation with indicator features = %.2f" % score)
diff --git a/examples/mixture/plot_bayesian_gaussian_mixture.py b/examples/mixture/plot_bayesian_gaussian_mixture.py
new file mode 100644
index 000000000000..9efea3f04c7f
--- /dev/null
+++ b/examples/mixture/plot_bayesian_gaussian_mixture.py
@@ -0,0 +1,114 @@
+"""
+======================================================
+Bayesian Gaussian Mixture Concentration Prior Analysis
+======================================================
+
+Plot the resulting ellipsoids of a mixture of three Gaussians with
+variational Bayesian Gaussian Mixture for three different values on the
+prior the dirichlet concentration.
+
+For all models, the Variationnal Bayesian Gaussian Mixture adapts its number of
+mixture automatically. The parameter `dirichlet_concentration_prior` has a
+direct link with the resulting number of components. Specifying a high value of
+`dirichlet_concentration_prior` leads more often to uniformly-sized mixture
+components, while specifying small (under 0.1) values will lead to some mixture
+components getting almost all the points while most mixture components will be
+centered on just a few of the remaining points.
+"""
+# Author: Thierry Guillemot <thierry.guillemot.work@gmail.com>
+# License: BSD 3 clause
+
+import numpy as np
+import matplotlib as mpl
+import matplotlib.pyplot as plt
+import matplotlib.gridspec as gridspec
+
+from sklearn.mixture import BayesianGaussianMixture
+
+print(__doc__)
+
+
+def plot_ellipses(ax, weights, means, covars):
+    for n in range(means.shape[0]):
+        v, w = np.linalg.eigh(covars[n][:2, :2])
+        u = w[0] / np.linalg.norm(w[0])
+        angle = np.arctan2(u[1], u[0])
+        angle = 180 * angle / np.pi  # convert to degrees
+        v = 2 * np.sqrt(2) * np.sqrt(v)
+        ell = mpl.patches.Ellipse(means[n, :2], v[0], v[1], 180 + angle)
+        ell.set_clip_box(ax.bbox)
+        ell.set_alpha(weights[n])
+        ax.add_artist(ell)
+
+
+def plot_results(ax1, ax2, estimator, dirichlet_concentration_prior, X, y, plot_title=False):
+    estimator.dirichlet_concentration_prior = dirichlet_concentration_prior
+    estimator.fit(X)
+    ax1.set_title("Bayesian Gaussian Mixture for "
+                  r"$dc_0=%.1e$" % dirichlet_concentration_prior)
+    # ax1.axis('equal')
+    ax1.scatter(X[:, 0], X[:, 1], s=5, marker='o', color=colors[y], alpha=0.8)
+    ax1.set_xlim(-2., 2.)
+    ax1.set_ylim(-3., 3.)
+    ax1.set_xticks(())
+    ax1.set_yticks(())
+    plot_ellipses(ax1, estimator.weights_, estimator.means_,
+                  estimator.covariances_)
+
+    ax2.get_xaxis().set_tick_params(direction='out')
+    ax2.yaxis.grid(True, alpha=0.7)
+    for k, w in enumerate(estimator.weights_):
+        ax2.bar(k - .45, w, width=0.9, color='royalblue', zorder=3)
+        ax2.text(k, w + 0.007, "%.1f%%" % (w * 100.),
+                 horizontalalignment='center')
+    ax2.set_xlim(-.6, 2 * n_components - .4)
+    ax2.set_ylim(0., 1.1)
+    ax2.tick_params(axis='y', which='both', left='off',
+                    right='off', labelleft='off')
+    ax2.tick_params(axis='x', which='both', top='off')
+
+    if plot_title:
+        ax1.set_ylabel('Estimated Mixtures')
+        ax2.set_ylabel('Weight of each component')
+
+# Parameters
+random_state = 2
+n_components, n_features = 3, 2
+colors = np.array(['mediumseagreen', 'royalblue', 'r', 'gold',
+                   'orchid', 'indigo', 'darkcyan', 'tomato'])
+dirichlet_concentration_prior = np.logspace(-3, 3, 3)
+covars = np.array([[[.7, .0], [.0, .1]],
+                   [[.5, .0], [.0, .1]],
+                   [[.5, .0], [.0, .1]]])
+samples = np.array([200, 500, 200])
+means = np.array([[.0, -.70],
+                  [.0, .0],
+                  [.0, .70]])
+
+
+# Here we put beta_prior to 0.8 to minimize the influence of the prior for this
+# dataset
+estimator = BayesianGaussianMixture(n_components=2 * n_components,
+                                    init_params='random', max_iter=1500,
+                                    mean_precision_prior=.8, tol=1e-9,
+                                    random_state=random_state)
+
+# Generate data
+rng = np.random.RandomState(random_state)
+X = np.vstack([
+    rng.multivariate_normal(means[j], covars[j], samples[j])
+    for j in range(n_components)])
+y = np.concatenate([j * np.ones(samples[j], dtype=int)
+                    for j in range(n_components)])
+
+# Plot Results
+plt.figure(figsize=(4.7 * 3, 8))
+plt.subplots_adjust(bottom=.04, top=0.95, hspace=.05, wspace=.05,
+                    left=.03, right=.97)
+
+gs = gridspec.GridSpec(3, len(dirichlet_concentration_prior))
+for k, dc in enumerate(dirichlet_concentration_prior):
+    plot_results(plt.subplot(gs[0:2, k]), plt.subplot(gs[2, k]),
+                 estimator, dc, X, y, plot_title=k == 0)
+
+plt.show()
diff --git a/examples/mixture/plot_gmm_covariances.py b/examples/mixture/plot_gmm_covariances.py
index dbd5be50f93e..2b4cd88642a9 100644
--- a/examples/mixture/plot_gmm_covariances.py
+++ b/examples/mixture/plot_gmm_covariances.py
@@ -69,7 +69,7 @@ def make_ellipses(gmm, ax):
 
 # Break up the dataset into non-overlapping training (75%) and testing
 # (25%) sets.
-skf = StratifiedKFold(n_folds=4)
+skf = StratifiedKFold(n_splits=4)
 # Only take the first fold.
 train_index, test_index = next(iter(skf.split(iris.data, iris.target)))
 
diff --git a/examples/model_selection/plot_learning_curve.py b/examples/model_selection/plot_learning_curve.py
index 505ceb827338..cb8cd87a7803 100644
--- a/examples/model_selection/plot_learning_curve.py
+++ b/examples/model_selection/plot_learning_curve.py
@@ -101,14 +101,14 @@ def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,
 title = "Learning Curves (Naive Bayes)"
 # Cross validation with 100 iterations to get smoother mean test and train
 # score curves, each time with 20% data randomly selected as a validation set.
-cv = ShuffleSplit(n_iter=100, test_size=0.2, random_state=0)
+cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)
 
 estimator = GaussianNB()
 plot_learning_curve(estimator, title, X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=4)
 
 title = "Learning Curves (SVM, RBF kernel, $\gamma=0.001$)"
 # SVC is more expensive so we do a lower number of CV iterations:
-cv = ShuffleSplit(n_iter=10, test_size=0.2, random_state=0)
+cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)
 estimator = SVC(gamma=0.001)
 plot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=4)
 
diff --git a/examples/model_selection/plot_roc_crossval.py b/examples/model_selection/plot_roc_crossval.py
index b208fc3d6907..6678dcb1af8b 100644
--- a/examples/model_selection/plot_roc_crossval.py
+++ b/examples/model_selection/plot_roc_crossval.py
@@ -58,7 +58,7 @@
 # Classification and ROC analysis
 
 # Run classifier with cross-validation and plot ROC curves
-cv = StratifiedKFold(n_folds=6)
+cv = StratifiedKFold(n_splits=6)
 classifier = svm.SVC(kernel='linear', probability=True,
                      random_state=random_state)
 
diff --git a/examples/plot_compare_reduction.py b/examples/plot_compare_reduction.py
new file mode 100644
index 000000000000..0f578722ee68
--- /dev/null
+++ b/examples/plot_compare_reduction.py
@@ -0,0 +1,75 @@
+#!/usr/bin/python
+# -*- coding: utf-8 -*-
+"""
+=================================================================
+Selecting dimensionality reduction with Pipeline and GridSearchCV
+=================================================================
+
+This example constructs a pipeline that does dimensionality
+reduction followed by prediction with a support vector
+classifier. It demonstrates the use of GridSearchCV and
+Pipeline to optimize over different classes of estimators in a
+single CV run -- unsupervised PCA and NMF dimensionality
+reductions are compared to univariate feature selection during
+the grid search.
+"""
+# Authors: Robert McGibbon, Joel Nothman
+
+from __future__ import print_function, division
+
+import numpy as np
+import matplotlib.pyplot as plt
+from sklearn.datasets import load_digits
+from sklearn.model_selection import GridSearchCV
+from sklearn.pipeline import Pipeline
+from sklearn.svm import LinearSVC
+from sklearn.decomposition import PCA, NMF
+from sklearn.feature_selection import SelectKBest, chi2
+
+print(__doc__)
+
+pipe = Pipeline([
+    ('reduce_dim', PCA()),
+    ('classify', LinearSVC())
+])
+
+N_FEATURES_OPTIONS = [2, 4, 8]
+C_OPTIONS = [1, 10, 100, 1000]
+param_grid = [
+    {
+        'reduce_dim': [PCA(iterated_power=7), NMF()],
+        'reduce_dim__n_components': N_FEATURES_OPTIONS,
+        'classify__C': C_OPTIONS
+    },
+    {
+        'reduce_dim': [SelectKBest(chi2)],
+        'reduce_dim__k': N_FEATURES_OPTIONS,
+        'classify__C': C_OPTIONS
+    },
+]
+reducer_labels = ['PCA', 'NMF', 'KBest(chi2)']
+
+grid = GridSearchCV(pipe, cv=3, n_jobs=2, param_grid=param_grid)
+digits = load_digits()
+grid.fit(digits.data, digits.target)
+
+mean_scores = np.array(grid.results_['test_mean_score'])
+# scores are in the order of param_grid iteration, which is alphabetical
+mean_scores = mean_scores.reshape(len(C_OPTIONS), -1, len(N_FEATURES_OPTIONS))
+# select score for best C
+mean_scores = mean_scores.max(axis=0)
+bar_offsets = (np.arange(len(N_FEATURES_OPTIONS)) *
+               (len(reducer_labels) + 1) + .5)
+
+plt.figure()
+COLORS = 'bgrcmyk'
+for i, (label, reducer_scores) in enumerate(zip(reducer_labels, mean_scores)):
+    plt.bar(bar_offsets + i, reducer_scores, label=label, color=COLORS[i])
+
+plt.title("Comparing feature reduction techniques")
+plt.xlabel('Reduced number of features')
+plt.xticks(bar_offsets + len(reducer_labels) / 2, N_FEATURES_OPTIONS)
+plt.ylabel('Digit classification accuracy')
+plt.ylim((0, 1))
+plt.legend(loc='upper left')
+plt.show()
diff --git a/examples/svm/plot_rbf_parameters.py b/examples/svm/plot_rbf_parameters.py
index abbac81b18a0..b71d6b22dc7c 100644
--- a/examples/svm/plot_rbf_parameters.py
+++ b/examples/svm/plot_rbf_parameters.py
@@ -59,7 +59,7 @@
 
 We should also note that small differences in scores results from the random
 splits of the cross-validation procedure. Those spurious variations can be
-smoothed out by increasing the number of CV iterations ``n_iter`` at the
+smoothed out by increasing the number of CV iterations ``n_splits`` at the
 expense of compute time. Increasing the value number of ``C_range`` and
 ``gamma_range`` steps will increase the resolution of the hyper-parameter heat
 map.
@@ -128,7 +128,7 @@ def __call__(self, value, clip=None):
 C_range = np.logspace(-2, 10, 13)
 gamma_range = np.logspace(-9, 3, 13)
 param_grid = dict(gamma=gamma_range, C=C_range)
-cv = StratifiedShuffleSplit(n_iter=5, test_size=0.2, random_state=42)
+cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)
 grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)
 grid.fit(X, y)
 
diff --git a/examples/svm/plot_svm_scale_c.py b/examples/svm/plot_svm_scale_c.py
index 996a0190e943..09934c2f5d85 100644
--- a/examples/svm/plot_svm_scale_c.py
+++ b/examples/svm/plot_svm_scale_c.py
@@ -128,8 +128,8 @@
         # To get nice curve, we need a large number of iterations to
         # reduce the variance
         grid = GridSearchCV(clf, refit=False, param_grid=param_grid,
-                            cv=ShuffleSplit(train_size=train_size, n_iter=250,
-                                            random_state=1))
+                            cv=ShuffleSplit(train_size=train_size,
+                                            n_splits=250, random_state=1))
         grid.fit(X, y)
         scores = grid.results_['test_mean_score']
 
diff --git a/examples/tree/unveil_tree_structure.py b/examples/tree/plot_unveil_tree_structure.py
similarity index 100%
rename from examples/tree/unveil_tree_structure.py
rename to examples/tree/plot_unveil_tree_structure.py
diff --git a/setup.py b/setup.py
index ba5b3d31be75..3c32655c51de 100755
--- a/setup.py
+++ b/setup.py
@@ -42,6 +42,10 @@
 
 VERSION = sklearn.__version__
 
+SCIPY_MIN_VERSION = '0.9'
+NUMPY_MIN_VERSION = '1.6.1'
+
+
 # Optional setuptools features
 # We need to import setuptools early, if we want setuptools features,
 # as it monkey-patches the 'setup' function
@@ -58,6 +62,12 @@
     extra_setuptools_args = dict(
         zip_safe=False,  # the package can run out of an .egg file
         include_package_data=True,
+        extras_require={
+            'alldeps': (
+                'numpy >= {0}'.format(NUMPY_MIN_VERSION),
+                'scipy >= {0}'.format(SCIPY_MIN_VERSION),
+            ),
+        },
     )
 else:
     extra_setuptools_args = dict()
@@ -131,10 +141,6 @@ def configuration(parent_package='', top_path=None):
     return config
 
 
-scipy_min_version = '0.9'
-numpy_min_version = '1.6.1'
-
-
 def get_scipy_status():
     """
     Returns a dictionary containing a boolean specifying whether SciPy
@@ -146,7 +152,7 @@ def get_scipy_status():
         import scipy
         scipy_version = scipy.__version__
         scipy_status['up_to_date'] = parse_version(
-            scipy_version) >= parse_version(scipy_min_version)
+            scipy_version) >= parse_version(SCIPY_MIN_VERSION)
         scipy_status['version'] = scipy_version
     except ImportError:
         scipy_status['up_to_date'] = False
@@ -165,7 +171,7 @@ def get_numpy_status():
         import numpy
         numpy_version = numpy.__version__
         numpy_status['up_to_date'] = parse_version(
-            numpy_version) >= parse_version(numpy_min_version)
+            numpy_version) >= parse_version(NUMPY_MIN_VERSION)
         numpy_status['version'] = numpy_version
     except ImportError:
         numpy_status['up_to_date'] = False
@@ -210,8 +216,8 @@ def setup_package():
                                  'Programming Language :: Python :: 2.6',
                                  'Programming Language :: Python :: 2.7',
                                  'Programming Language :: Python :: 3',
-                                 'Programming Language :: Python :: 3.3',
                                  'Programming Language :: Python :: 3.4',
+                                 'Programming Language :: Python :: 3.5',
                                  ],
                     cmdclass=cmdclass,
                     **extra_setuptools_args)
@@ -236,10 +242,10 @@ def setup_package():
     else:
         numpy_status = get_numpy_status()
         numpy_req_str = "scikit-learn requires NumPy >= {0}.\n".format(
-            numpy_min_version)
+            NUMPY_MIN_VERSION)
         scipy_status = get_scipy_status()
         scipy_req_str = "scikit-learn requires SciPy >= {0}.\n".format(
-            scipy_min_version)
+            SCIPY_MIN_VERSION)
 
         instructions = ("Installation instructions are available on the "
                         "scikit-learn website: "
diff --git a/sklearn/cluster/k_means_.py b/sklearn/cluster/k_means_.py
index 6e2a84b2d98c..584c1c2cac80 100644
--- a/sklearn/cluster/k_means_.py
+++ b/sklearn/cluster/k_means_.py
@@ -792,6 +792,22 @@ class KMeans(BaseEstimator, ClusterMixin, TransformerMixin):
     inertia_ : float
         Sum of distances of samples to their closest cluster center.
 
+    Examples
+    --------
+
+    >>> from sklearn.cluster import KMeans
+    >>> import numpy as np
+    >>> X = np.array([[1, 2], [1, 4], [1, 0],
+    ...               [4, 2], [4, 4], [4, 0]])
+    >>> kmeans = KMeans(n_clusters=2, random_state=0).fit(X)
+    >>> kmeans.labels_
+    array([0, 0, 0, 1, 1, 1], dtype=int32)
+    >>> kmeans.predict([[0, 0], [4, 4]])
+    array([0, 1], dtype=int32)
+    >>> kmeans.cluster_centers_
+    array([[ 1.,  2.],
+           [ 4.,  2.]])
+
     See also
     --------
 
@@ -844,8 +860,7 @@ def _check_fit_data(self, X):
         return X
 
     def _check_test_data(self, X):
-        X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES,
-                        warn_on_dtype=True)
+        X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES)
         n_samples, n_features = X.shape
         expected_n_features = self.cluster_centers_.shape[1]
         if not n_features == expected_n_features:
diff --git a/sklearn/cluster/tests/test_k_means.py b/sklearn/cluster/tests/test_k_means.py
index 9ee3ef616bf0..4b23ab9cc167 100644
--- a/sklearn/cluster/tests/test_k_means.py
+++ b/sklearn/cluster/tests/test_k_means.py
@@ -44,15 +44,6 @@
 X_csr = sp.csr_matrix(X)
 
 
-def test_kmeans_dtype():
-    rnd = np.random.RandomState(0)
-    X = rnd.normal(size=(40, 2))
-    X = (X * 10).astype(np.uint8)
-    km = KMeans(n_init=1).fit(X)
-    pred_x = assert_warns(DataConversionWarning, km.predict, X)
-    assert_array_equal(km.labels_, pred_x)
-
-
 def test_elkan_results():
     rnd = np.random.RandomState(0)
     X_normal = rnd.normal(size=(50, 10))
@@ -669,7 +660,7 @@ def test_int_input():
 
         expected_labels = [0, 1, 1, 0, 0, 1]
         scores = np.array([v_measure_score(expected_labels, km.labels_)
-                        for km in fitted_models])
+                           for km in fitted_models])
         assert_array_equal(scores, np.ones(scores.shape[0]))
 
 
@@ -793,24 +784,28 @@ def test_float_precision():
                 if is_sparse:
                     X_test = sp.csr_matrix(X_csr, dtype=dtype)
                 else:
-                    X_test = dtype(X)
+                    X_test = X.astype(dtype)
                 estimator.fit(X_test)
-                # dtype of cluster centers has to be the dtype of the input data
+                # dtype of cluster centers has to be the dtype of the input
+                # data
                 assert_equal(estimator.cluster_centers_.dtype, dtype)
                 inertia[dtype] = estimator.inertia_
                 X_new[dtype] = estimator.transform(X_test)
                 centers[dtype] = estimator.cluster_centers_
-                # make sure predictions correspond to the correct label
-                assert_equal(estimator.predict(X_test[0]), estimator.labels_[0])
+                # ensure the extracted row is a 2d array
+                assert_equal(estimator.predict(X_test[:1]),
+                             estimator.labels_[0])
                 if hasattr(estimator, 'partial_fit'):
                     estimator.partial_fit(X_test[0:3])
-                    # dtype of cluster centers has to stay the same after partial_fit
+                    # dtype of cluster centers has to stay the same after
+                    # partial_fit
                     assert_equal(estimator.cluster_centers_.dtype, dtype)
 
             # compare arrays with low precision since the difference between
-            # 32 and 64 bit sometimes makes a difference up to the 4th decimal place
+            # 32 and 64 bit sometimes makes a difference up to the 4th decimal
+            # place
             assert_array_almost_equal(inertia[np.float32], inertia[np.float64],
-                                    decimal=4)
+                                      decimal=4)
             assert_array_almost_equal(X_new[np.float32], X_new[np.float64],
                                       decimal=4)
             assert_array_almost_equal(centers[np.float32], centers[np.float64],
@@ -818,14 +813,14 @@ def test_float_precision():
 
 
 def test_KMeans_init_centers():
-    # This test is used to check KMeans won't mutate the user provided input array silently
-    # even if input data and init centers have the same type
+    # This test is used to check KMeans won't mutate the user provided input
+    # array silently even if input data and init centers have the same type
     X_small = np.array([[1.1, 1.1], [-7.5, -7.5], [-1.1, -1.1], [7.5, 7.5]])
     init_centers = np.array([[0.0, 0.0], [5.0, 5.0], [-5.0, -5.0]])
     for dtype in [np.int32, np.int64, np.float32, np.float64]:
         X_test = dtype(X_small)
         init_centers_test = dtype(init_centers)
         assert_array_equal(init_centers, init_centers_test)
-        km = KMeans(init=init_centers_test, n_clusters=3)
+        km = KMeans(init=init_centers_test, n_clusters=3, n_init=1)
         km.fit(X_test)
         assert_equal(False, np.may_share_memory(km.cluster_centers_, init_centers))
diff --git a/sklearn/covariance/tests/test_graph_lasso.py b/sklearn/covariance/tests/test_graph_lasso.py
index f8da99ce3a5f..bc2c8339da21 100644
--- a/sklearn/covariance/tests/test_graph_lasso.py
+++ b/sklearn/covariance/tests/test_graph_lasso.py
@@ -61,8 +61,8 @@ def test_graph_lasso(random_state=0):
 
 def test_graph_lasso_iris():
     # Hard-coded solution from R glasso package for alpha=1.0
-    # The iris datasets in R and sklearn do not match in a few places, these
-    # values are for the sklearn version
+    # The iris datasets in R and scikit-learn do not match in a few places,
+    # these values are for the scikit-learn version.
     cov_R = np.array([
         [0.68112222, 0.0, 0.2651911, 0.02467558],
         [0.00, 0.1867507, 0.0, 0.00],
diff --git a/sklearn/cross_validation.py b/sklearn/cross_validation.py
index e23d1040b12c..508b0460ec15 100644
--- a/sklearn/cross_validation.py
+++ b/sklearn/cross_validation.py
@@ -1853,14 +1853,9 @@ def train_test_split(*arrays, **options):
     Parameters
     ----------
     *arrays : sequence of indexables with same length / shape[0]
-
-        allowed inputs are lists, numpy arrays, scipy-sparse
+        Allowed inputs are lists, numpy arrays, scipy-sparse
         matrices or pandas dataframes.
 
-        .. versionadded:: 0.16
-            preserves input type instead of always casting to numpy array.
-
-
     test_size : float, int, or None (default is None)
         If float, should be between 0.0 and 1.0 and represent the
         proportion of the dataset to include in the test split. If
@@ -1890,7 +1885,9 @@ def train_test_split(*arrays, **options):
         List containing train-test split of inputs.
 
         .. versionadded:: 0.16
-            Output type is the same as the input type.
+            If the input is sparse, the output will be a
+            ``scipy.sparse.csr_matrix``. Else, output type is the same as the
+            input type.
 
     Examples
     --------
diff --git a/sklearn/datasets/base.py b/sklearn/datasets/base.py
index 3d6107268b13..b5c7b4cd4fe7 100644
--- a/sklearn/datasets/base.py
+++ b/sklearn/datasets/base.py
@@ -264,7 +264,7 @@ def load_iris(return_X_y=False):
         If True, returns ``(data, target)`` instead of a Bunch object.
         See below for more information about the `data` and `target` object.
 
-    .. versionadded:: 0.18
+        .. versionadded:: 0.18
 
     Returns
     -------
@@ -277,7 +277,7 @@ def load_iris(return_X_y=False):
 
     (data, target) : tuple if ``return_X_y`` is True
 
-    .. versionadded:: 0.18
+        .. versionadded:: 0.18
 
     Examples
     --------
@@ -338,7 +338,7 @@ def load_breast_cancer(return_X_y=False):
         If True, returns ``(data, target)`` instead of a Bunch object.
         See below for more information about the `data` and `target` object.
 
-    .. versionadded:: 0.18
+        .. versionadded:: 0.18
 
     Returns
     -------
@@ -351,7 +351,7 @@ def load_breast_cancer(return_X_y=False):
 
     (data, target) : tuple if ``return_X_y`` is True
 
-    .. versionadded:: 0.18
+        .. versionadded:: 0.18
 
     The copy of UCI ML Breast Cancer Wisconsin (Diagnostic) dataset is
     downloaded from:
@@ -411,7 +411,7 @@ def load_breast_cancer(return_X_y=False):
                  feature_names=feature_names)
 
 
-def load_digits(n_class=10):
+def load_digits(n_class=10, return_X_y=False):
     """Load and return the digits dataset (classification).
 
     Each datapoint is a 8x8 image of a digit.
@@ -431,6 +431,12 @@ def load_digits(n_class=10):
     n_class : integer, between 0 and 10, optional (default=10)
         The number of classes to return.
 
+    return_X_y : boolean, default=False.
+        If True, returns ``(data, target)`` instead of a Bunch object.
+        See below for more information about the `data` and `target` object.
+
+        .. versionadded:: 0.18
+
     Returns
     -------
     data : Bunch
@@ -440,6 +446,10 @@ def load_digits(n_class=10):
         sample, 'target_names', the meaning of the labels, and 'DESCR',
         the full description of the dataset.
 
+    (data, target) : tuple if ``return_X_y`` is True
+
+        .. versionadded:: 0.18
+
     Examples
     --------
     To load the data and visualize the images::
@@ -458,7 +468,7 @@ def load_digits(n_class=10):
                       delimiter=',')
     with open(join(module_path, 'descr', 'digits.rst')) as f:
         descr = f.read()
-    target = data[:, -1]
+    target = data[:, -1].astype(np.int)
     flat_data = data[:, :-1]
     images = flat_data.view()
     images.shape = (-1, 8, 8)
@@ -468,14 +478,17 @@ def load_digits(n_class=10):
         flat_data, target = flat_data[idx], target[idx]
         images = images[idx]
 
+    if return_X_y:
+        return flat_data, target
+
     return Bunch(data=flat_data,
-                 target=target.astype(np.int),
+                 target=target,
                  target_names=np.arange(10),
                  images=images,
                  DESCR=descr)
 
 
-def load_diabetes():
+def load_diabetes(return_X_y=False):
     """Load and return the diabetes dataset (regression).
 
     ==============      ==================
@@ -487,20 +500,36 @@ def load_diabetes():
 
     Read more in the :ref:`User Guide <datasets>`.
 
+    Parameters
+    ----------
+    return_X_y : boolean, default=False.
+        If True, returns ``(data, target)`` instead of a Bunch object.
+        See below for more information about the `data` and `target` object.
+
+        .. versionadded:: 0.18
+
     Returns
     -------
     data : Bunch
         Dictionary-like object, the interesting attributes are:
         'data', the data to learn and 'target', the regression target for each
         sample.
+
+    (data, target) : tuple if ``return_X_y`` is True
+
+        .. versionadded:: 0.18    
     """
     base_dir = join(dirname(__file__), 'data')
     data = np.loadtxt(join(base_dir, 'diabetes_data.csv.gz'))
     target = np.loadtxt(join(base_dir, 'diabetes_target.csv.gz'))
+    
+    if return_X_y:
+        return data, target
+
     return Bunch(data=data, target=target)
 
 
-def load_linnerud():
+def load_linnerud(return_X_y=False):
     """Load and return the linnerud dataset (multivariate regression).
 
     Samples total: 20
@@ -508,6 +537,14 @@ def load_linnerud():
     Features: integer
     Targets: integer
 
+    Parameters
+    ----------
+    return_X_y : boolean, default=False.
+        If True, returns ``(data, target)`` instead of a Bunch object.
+        See below for more information about the `data` and `target` object.
+
+        .. versionadded:: 0.18
+
     Returns
     -------
     data : Bunch
@@ -515,6 +552,10 @@ def load_linnerud():
         'targets', the two multivariate datasets, with 'data' corresponding to
         the exercise and 'targets' corresponding to the physiological
         measurements, as well as 'feature_names' and 'target_names'.
+    
+    (data, target) : tuple if ``return_X_y`` is True
+
+        .. versionadded:: 0.18
     """
     base_dir = join(dirname(__file__), 'data/')
     # Read data
@@ -529,13 +570,16 @@ def load_linnerud():
     with open(dirname(__file__) + '/descr/linnerud.rst') as f:
         descr = f.read()
 
+    if return_X_y:
+        return data_exercise, data_physiological
+
     return Bunch(data=data_exercise, feature_names=header_exercise,
                  target=data_physiological,
                  target_names=header_physiological,
                  DESCR=descr)
 
 
-def load_boston():
+def load_boston(return_X_y=False):
     """Load and return the boston house-prices dataset (regression).
 
     ==============     ==============
@@ -545,6 +589,14 @@ def load_boston():
     Targets             real 5. - 50.
     ==============     ==============
 
+    Parameters
+    ----------
+    return_X_y : boolean, default=False.
+        If True, returns ``(data, target)`` instead of a Bunch object.
+        See below for more information about the `data` and `target` object.
+
+        .. versionadded:: 0.18
+
     Returns
     -------
     data : Bunch
@@ -552,6 +604,10 @@ def load_boston():
         'data', the data to learn, 'target', the regression targets,
         and 'DESCR', the full description of the dataset.
 
+    (data, target) : tuple if ``return_X_y`` is True
+
+        .. versionadded:: 0.18    
+
     Examples
     --------
     >>> from sklearn.datasets import load_boston
@@ -580,6 +636,9 @@ def load_boston():
             data[i] = np.asarray(d[:-1], dtype=np.float64)
             target[i] = np.asarray(d[-1], dtype=np.float64)
 
+    if return_X_y:
+        return data, target
+
     return Bunch(data=data,
                  target=target,
                  # last column is target value
diff --git a/sklearn/datasets/mldata.py b/sklearn/datasets/mldata.py
index 3768435fe489..1ab3edea91bd 100644
--- a/sklearn/datasets/mldata.py
+++ b/sklearn/datasets/mldata.py
@@ -103,7 +103,7 @@ def fetch_mldata(dataname, target_name='label', data_name='data',
     (150, 4)
 
     Load the 'leukemia' dataset from mldata.org, which needs to be transposed
-    to respects the sklearn axes convention:
+    to respects the scikit-learn axes convention:
 
     >>> leuk = fetch_mldata('leukemia', transpose_data=True,
     ...                     data_home=test_data_home)
@@ -205,7 +205,7 @@ def fetch_mldata(dataname, target_name='label', data_name='data',
             del dataset[col_names[1]]
             dataset['data'] = matlab_dict[col_names[1]]
 
-    # set axes to sklearn conventions
+    # set axes to scikit-learn conventions
     if transpose_data:
         dataset['data'] = dataset['data'].T
     if 'target' in dataset:
diff --git a/sklearn/datasets/tests/test_base.py b/sklearn/datasets/tests/test_base.py
index 6be9b399e122..523d5fcd8c46 100644
--- a/sklearn/datasets/tests/test_base.py
+++ b/sklearn/datasets/tests/test_base.py
@@ -128,6 +128,13 @@ def test_load_digits():
     assert_equal(digits.data.shape, (1797, 64))
     assert_equal(numpy.unique(digits.target).size, 10)
 
+    # test return_X_y option
+    X_y_tuple = load_digits(return_X_y=True)
+    bunch = load_digits()
+    assert_true(isinstance(X_y_tuple, tuple))
+    assert_array_equal(X_y_tuple[0], bunch.data)
+    assert_array_equal(X_y_tuple[1], bunch.target)
+
 
 def test_load_digits_n_class_lt_10():
     digits = load_digits(9)
@@ -165,6 +172,13 @@ def test_load_diabetes():
     assert_equal(res.data.shape, (442, 10))
     assert_true(res.target.size, 442)
 
+    # test return_X_y option
+    X_y_tuple = load_diabetes(return_X_y=True)
+    bunch = load_diabetes()
+    assert_true(isinstance(X_y_tuple, tuple))
+    assert_array_equal(X_y_tuple[0], bunch.data)
+    assert_array_equal(X_y_tuple[1], bunch.target)
+
 
 def test_load_linnerud():
     res = load_linnerud()
@@ -173,6 +187,12 @@ def test_load_linnerud():
     assert_equal(len(res.target_names), 3)
     assert_true(res.DESCR)
 
+    # test return_X_y option
+    X_y_tuple = load_linnerud(return_X_y=True)
+    bunch = load_linnerud()
+    assert_true(isinstance(X_y_tuple, tuple))
+    assert_array_equal(X_y_tuple[0], bunch.data)
+    assert_array_equal(X_y_tuple[1], bunch.target)
 
 def test_load_iris():
     res = load_iris()
@@ -211,6 +231,12 @@ def test_load_boston():
     assert_equal(res.feature_names.size, 13)
     assert_true(res.DESCR)
 
+    # test return_X_y option
+    X_y_tuple = load_boston(return_X_y=True)
+    bunch = load_boston()
+    assert_true(isinstance(X_y_tuple, tuple))
+    assert_array_equal(X_y_tuple[0], bunch.data)
+    assert_array_equal(X_y_tuple[1], bunch.target)
 
 def test_loads_dumps_bunch():
     bunch = Bunch(x="x")
diff --git a/sklearn/decomposition/nmf.py b/sklearn/decomposition/nmf.py
index f1b7e106262a..43a368050e10 100644
--- a/sklearn/decomposition/nmf.py
+++ b/sklearn/decomposition/nmf.py
@@ -29,6 +29,9 @@
 from .cdnmf_fast import _update_cdnmf_fast
 
 
+INTEGER_TYPES = (numbers.Integral, np.integer)
+
+
 def safe_vstack(Xs):
     if any(sp.issparse(X) for X in Xs):
         return sp.vstack(Xs)
@@ -746,10 +749,10 @@ def non_negative_factorization(X, W=None, H=None, n_components=None,
     if n_components is None:
         n_components = n_features
 
-    if not isinstance(n_components, numbers.Integral) or n_components <= 0:
+    if not isinstance(n_components, INTEGER_TYPES) or n_components <= 0:
         raise ValueError("Number of components must be a positive integer;"
                          " got (n_components=%r)" % n_components)
-    if not isinstance(max_iter, numbers.Integral) or max_iter < 0:
+    if not isinstance(max_iter, INTEGER_TYPES) or max_iter < 0:
         raise ValueError("Maximum number of iterations must be a positive integer;"
                          " got (max_iter=%r)" % max_iter)
     if not isinstance(tol, numbers.Number) or tol < 0:
@@ -1108,7 +1111,7 @@ def transform(self, X):
 
     def inverse_transform(self, W):
         """Transform data back to its original space.
-        
+
         Parameters
         ----------
         W: {array-like, sparse matrix}, shape (n_samples, n_components)
diff --git a/sklearn/decomposition/tests/test_dict_learning.py b/sklearn/decomposition/tests/test_dict_learning.py
index 65e3fc99d174..b7ed5c470349 100644
--- a/sklearn/decomposition/tests/test_dict_learning.py
+++ b/sklearn/decomposition/tests/test_dict_learning.py
@@ -1,4 +1,7 @@
 import numpy as np
+
+from sklearn.exceptions import ConvergenceWarning
+
 from sklearn.utils import check_array
 
 from sklearn.utils.testing import assert_array_almost_equal
@@ -66,9 +69,12 @@ def test_dict_learning_lassocd_readonly_data():
     n_components = 12
     with TempMemmap(X) as X_read_only:
         dico = DictionaryLearning(n_components, transform_algorithm='lasso_cd',
-                                  transform_alpha=0.001, random_state=0, n_jobs=-1)
-        code = dico.fit(X_read_only).transform(X_read_only)
-        assert_array_almost_equal(np.dot(code, dico.components_), X_read_only, decimal=2)
+                                  transform_alpha=0.001, random_state=0,
+                                  n_jobs=-1)
+        with ignore_warnings(category=ConvergenceWarning):
+            code = dico.fit(X_read_only).transform(X_read_only)
+        assert_array_almost_equal(np.dot(code, dico.components_), X_read_only,
+                                  decimal=2)
 
 
 def test_dict_learning_nonzero_coefs():
diff --git a/sklearn/ensemble/bagging.py b/sklearn/ensemble/bagging.py
index 235af59240ff..c73b4d50d6c2 100644
--- a/sklearn/ensemble/bagging.py
+++ b/sklearn/ensemble/bagging.py
@@ -20,6 +20,7 @@
 from ..utils import check_random_state, check_X_y, check_array, column_or_1d
 from ..utils.random import sample_without_replacement
 from ..utils.validation import has_fit_parameter, check_is_fitted
+from ..utils import indices_to_mask
 from ..utils.fixes import bincount
 from ..utils.metaestimators import if_delegate_has_method
 from ..utils.multiclass import check_classification_targets
@@ -33,21 +34,41 @@
 MAX_INT = np.iinfo(np.int32).max
 
 
+def _generate_indices(random_state, bootstrap, n_population, n_samples):
+    """Draw randomly sampled indices."""
+    # Draw sample indices
+    if bootstrap:
+        indices = random_state.randint(0, n_population, n_samples)
+    else:
+        indices = sample_without_replacement(n_population, n_samples,
+                                             random_state=random_state)
+
+    return indices
+
+
+def _generate_bagging_indices(random_state, bootstrap_features,
+                              bootstrap_samples, n_features, n_samples,
+                              max_features, max_samples):
+    """Randomly draw feature and sample indices."""
+    # Get valid random state
+    random_state = check_random_state(random_state)
+
+    # Draw indices
+    feature_indices = _generate_indices(random_state, bootstrap_features,
+                                        n_features, max_features)
+    sample_indices = _generate_indices(random_state, bootstrap_samples,
+                                       n_samples, max_samples)
+
+    return feature_indices, sample_indices
+
+
 def _parallel_build_estimators(n_estimators, ensemble, X, y, sample_weight,
-                               max_samples, seeds, total_n_estimators, verbose):
+                               seeds, total_n_estimators, verbose):
     """Private function used to build a batch of estimators within a job."""
     # Retrieve settings
     n_samples, n_features = X.shape
-    max_features = ensemble.max_features
-
-    if (not isinstance(max_samples, (numbers.Integral, np.integer)) and
-            (0.0 < max_samples <= 1.0)):
-        max_samples = int(max_samples * n_samples)
-
-    if (not isinstance(max_features, (numbers.Integral, np.integer)) and
-            (0.0 < max_features <= 1.0)):
-        max_features = int(max_features * n_features)
-
+    max_features = ensemble._max_features
+    max_samples = ensemble._max_samples
     bootstrap = ensemble.bootstrap
     bootstrap_features = ensemble.bootstrap_features
     support_sample_weight = has_fit_parameter(ensemble.base_estimator_,
@@ -57,7 +78,6 @@ def _parallel_build_estimators(n_estimators, ensemble, X, y, sample_weight,
 
     # Build estimators
     estimators = []
-    estimators_samples = []
     estimators_features = []
 
     for i in range(n_estimators):
@@ -65,22 +85,20 @@ def _parallel_build_estimators(n_estimators, ensemble, X, y, sample_weight,
             print("Building estimator %d of %d for this parallel run (total %d)..." %
                   (i + 1, n_estimators, total_n_estimators))
 
-        random_state = check_random_state(seeds[i])
-        seed = random_state.randint(MAX_INT)
+        random_state = np.random.RandomState(seeds[i])
         estimator = ensemble._make_estimator(append=False)
 
-        try:  # Not all estimator accept a random_state
-            estimator.set_params(random_state=seed)
+        try:  # Not all estimators accept a random_state
+            estimator.set_params(random_state=seeds[i])
         except ValueError:
             pass
 
-        # Draw features
-        if bootstrap_features:
-            features = random_state.randint(0, n_features, max_features)
-        else:
-            features = sample_without_replacement(n_features,
-                                                  max_features,
-                                                  random_state=random_state)
+        # Draw random feature, sample indices
+        features, indices = _generate_bagging_indices(random_state,
+                                                      bootstrap_features,
+                                                      bootstrap, n_features,
+                                                      n_samples, max_features,
+                                                      max_samples)
 
         # Draw samples, using sample weights, and then fit
         if support_sample_weight:
@@ -90,40 +108,22 @@ def _parallel_build_estimators(n_estimators, ensemble, X, y, sample_weight,
                 curr_sample_weight = sample_weight.copy()
 
             if bootstrap:
-                indices = random_state.randint(0, n_samples, max_samples)
                 sample_counts = bincount(indices, minlength=n_samples)
                 curr_sample_weight *= sample_counts
-
             else:
-                not_indices = sample_without_replacement(
-                    n_samples,
-                    n_samples - max_samples,
-                    random_state=random_state)
-
-                curr_sample_weight[not_indices] = 0
+                not_indices_mask = ~indices_to_mask(indices, n_samples)
+                curr_sample_weight[not_indices_mask] = 0
 
             estimator.fit(X[:, features], y, sample_weight=curr_sample_weight)
-            samples = curr_sample_weight > 0.
 
         # Draw samples, using a mask, and then fit
         else:
-            if bootstrap:
-                indices = random_state.randint(0, n_samples, max_samples)
-            else:
-                indices = sample_without_replacement(n_samples,
-                                                     max_samples,
-                                                     random_state=random_state)
-
-            sample_counts = bincount(indices, minlength=n_samples)
-
             estimator.fit((X[indices])[:, features], y[indices])
-            samples = sample_counts > 0.
 
         estimators.append(estimator)
-        estimators_samples.append(samples)
         estimators_features.append(features)
 
-    return estimators, estimators_samples, estimators_features
+    return estimators, estimators_features
 
 
 def _parallel_predict_proba(estimators, estimators_features, X, n_classes):
@@ -251,7 +251,7 @@ def fit(self, X, y, sample_weight=None):
         """
         return self._fit(X, y, self.max_samples, sample_weight=sample_weight)
 
-    def _fit(self, X, y, max_samples, max_depth=None, sample_weight=None):
+    def _fit(self, X, y, max_samples=None, max_depth=None, sample_weight=None):
         """Build a Bagging ensemble of estimators from the training
            set (X, y).
 
@@ -289,6 +289,7 @@ def _fit(self, X, y, max_samples, max_depth=None, sample_weight=None):
 
         # Remap output
         n_samples, self.n_features_ = X.shape
+        self._n_samples = n_samples
         y = self._validate_y(y)
 
         # Check parameters
@@ -297,13 +298,19 @@ def _fit(self, X, y, max_samples, max_depth=None, sample_weight=None):
         if max_depth is not None:
             self.base_estimator_.max_depth = max_depth
 
-        # if max_samples is float:
-        if not isinstance(max_samples, (numbers.Integral, np.integer)):
+        # Validate max_samples
+        if max_samples is None:
+            max_samples = self.max_samples
+        elif not isinstance(max_samples, (numbers.Integral, np.integer)):
             max_samples = int(max_samples * X.shape[0])
 
         if not (0 < max_samples <= X.shape[0]):
             raise ValueError("max_samples must be in (0, n_samples]")
 
+        # Store validated integer row sampling value
+        self._max_samples = max_samples
+
+        # Validate max_features
         if isinstance(self.max_features, (numbers.Integral, np.integer)):
             max_features = self.max_features
         else:  # float
@@ -312,6 +319,10 @@ def _fit(self, X, y, max_samples, max_depth=None, sample_weight=None):
         if not (0 < max_features <= self.n_features_):
             raise ValueError("max_features must be in (0, n_features]")
 
+        # Store validated integer feature sampling value
+        self._max_features = max_features
+
+        # Other checks
         if not self.bootstrap and self.oob_score:
             raise ValueError("Out of bag estimation only available"
                              " if bootstrap=True")
@@ -326,7 +337,6 @@ def _fit(self, X, y, max_samples, max_depth=None, sample_weight=None):
         if not self.warm_start or len(self.estimators_) == 0:
             # Free allocated memory, if any
             self.estimators_ = []
-            self.estimators_samples_ = []
             self.estimators_features_ = []
 
         n_more_estimators = self.n_estimators - len(self.estimators_)
@@ -352,6 +362,7 @@ def _fit(self, X, y, max_samples, max_depth=None, sample_weight=None):
             random_state.randint(MAX_INT, size=len(self.estimators_))
 
         seeds = random_state.randint(MAX_INT, size=n_more_estimators)
+        self._seeds = seeds
 
         all_results = Parallel(n_jobs=n_jobs, verbose=self.verbose)(
             delayed(_parallel_build_estimators)(
@@ -360,7 +371,6 @@ def _fit(self, X, y, max_samples, max_depth=None, sample_weight=None):
                 X,
                 y,
                 sample_weight,
-                max_samples,
                 seeds[starts[i]:starts[i + 1]],
                 total_n_estimators,
                 verbose=self.verbose)
@@ -369,10 +379,8 @@ def _fit(self, X, y, max_samples, max_depth=None, sample_weight=None):
         # Reduce
         self.estimators_ += list(itertools.chain.from_iterable(
             t[0] for t in all_results))
-        self.estimators_samples_ += list(itertools.chain.from_iterable(
-            t[1] for t in all_results))
         self.estimators_features_ += list(itertools.chain.from_iterable(
-            t[2] for t in all_results))
+            t[1] for t in all_results))
 
         if self.oob_score:
             self._set_oob_score(X, y)
@@ -387,6 +395,38 @@ def _validate_y(self, y):
         # Default implementation
         return column_or_1d(y, warn=True)
 
+    def _get_estimators_indices(self):
+        # Get drawn indices along both sample and feature axes
+        for seed in self._seeds:
+            # Operations accessing random_state must be performed identically
+            # to those in `_parallel_build_estimators()`
+            random_state = np.random.RandomState(seed)
+            feature_indices, sample_indices = _generate_bagging_indices(
+                random_state, self.bootstrap_features, self.bootstrap,
+                self.n_features_, self._n_samples, self._max_features,
+                self._max_samples)
+
+            yield feature_indices, sample_indices
+
+    @property
+    def estimators_samples_(self):
+        """The subset of drawn samples for each base estimator.
+
+        Returns a dynamically generated list of boolean masks identifying
+        the samples used for for fitting each member of the ensemble, i.e.,
+        the in-bag samples.
+
+        Note: the list is re-created at each call to the property in order
+        to reduce the object memory footprint by not storing the sampling
+        data. Thus fetching the property may be slower than expected.
+        """
+        sample_masks = []
+        for _, sample_indices in self._get_estimators_indices():
+            mask = indices_to_mask(sample_indices, self._n_samples)
+            sample_masks.append(mask)
+
+        return sample_masks
+
 
 class BaggingClassifier(BaseBagging, ClassifierMixin):
     """A Bagging classifier.
@@ -470,7 +510,7 @@ class BaggingClassifier(BaseBagging, ClassifierMixin):
 
     estimators_samples_ : list of arrays
         The subset of drawn samples (i.e., the in-bag samples) for each base
-        estimator.
+        estimator. Each subset is defined by a boolean mask.
 
     estimators_features_ : list of arrays
         The subset of drawn features for each base estimator.
@@ -538,17 +578,17 @@ def _validate_estimator(self):
             default=DecisionTreeClassifier())
 
     def _set_oob_score(self, X, y):
+        n_samples = y.shape[0]
         n_classes_ = self.n_classes_
         classes_ = self.classes_
-        n_samples = y.shape[0]
 
         predictions = np.zeros((n_samples, n_classes_))
 
         for estimator, samples, features in zip(self.estimators_,
                                                 self.estimators_samples_,
                                                 self.estimators_features_):
-            mask = np.ones(n_samples, dtype=np.bool)
-            mask[samples] = False
+            # Create mask for OOB samples
+            mask = ~samples
 
             if hasattr(estimator, "predict_proba"):
                 predictions[mask, :] += estimator.predict_proba(
@@ -833,7 +873,7 @@ class BaggingRegressor(BaseBagging, RegressorMixin):
 
     estimators_samples_ : list of arrays
         The subset of drawn samples (i.e., the in-bag samples) for each base
-        estimator.
+        estimator. Each subset is defined by a boolean mask.
 
     estimators_features_ : list of arrays
         The subset of drawn features for each base estimator.
@@ -940,8 +980,8 @@ def _set_oob_score(self, X, y):
         for estimator, samples, features in zip(self.estimators_,
                                                 self.estimators_samples_,
                                                 self.estimators_features_):
-            mask = np.ones(n_samples, dtype=np.bool)
-            mask[samples] = False
+            # Create mask for OOB samples
+            mask = ~samples
 
             predictions[mask] += estimator.predict((X[mask, :])[:, features])
             n_predictions[mask] += 1
diff --git a/sklearn/ensemble/forest.py b/sklearn/ensemble/forest.py
index 731885a1d414..4d4f04bc1240 100644
--- a/sklearn/ensemble/forest.py
+++ b/sklearn/ensemble/forest.py
@@ -160,9 +160,9 @@ def apply(self, X):
         Parameters
         ----------
         X : array-like or sparse matrix, shape = [n_samples, n_features]
-            The input samples. Internally, it will be converted to
-            ``dtype=np.float32`` and if a sparse matrix is provided
-            to a sparse ``csr_matrix``.
+            The input samples. Internally, its dtype will be converted to
+            ``dtype=np.float32``. If a sparse matrix is provided, it will be
+            converted into a sparse ``csr_matrix``.
 
         Returns
         -------
@@ -184,9 +184,9 @@ def decision_path(self, X):
         Parameters
         ----------
         X : array-like or sparse matrix, shape = [n_samples, n_features]
-            The input samples. Internally, it will be converted to
-            ``dtype=np.float32`` and if a sparse matrix is provided
-            to a sparse ``csr_matrix``.
+            The input samples. Internally, its dtype will be converted to
+            ``dtype=np.float32``. If a sparse matrix is provided, it will be
+            converted into a sparse ``csr_matrix``.
 
         Returns
         -------
@@ -217,9 +217,9 @@ def fit(self, X, y, sample_weight=None):
         Parameters
         ----------
         X : array-like or sparse matrix of shape = [n_samples, n_features]
-            The training input samples. Internally, it will be converted to
-            ``dtype=np.float32`` and if a sparse matrix is provided
-            to a sparse ``csc_matrix``.
+            The training input samples. Internally, its dtype will be converted to
+            ``dtype=np.float32``. If a sparse matrix is provided, it will be
+            converted into a sparse ``csc_matrix``.
 
         y : array-like, shape = [n_samples] or [n_samples, n_outputs]
             The target values (class labels in classification, real numbers in
@@ -516,9 +516,9 @@ def predict(self, X):
         Parameters
         ----------
         X : array-like or sparse matrix of shape = [n_samples, n_features]
-            The input samples. Internally, it will be converted to
-            ``dtype=np.float32`` and if a sparse matrix is provided
-            to a sparse ``csr_matrix``.
+            The input samples. Internally, its dtype will be converted to
+            ``dtype=np.float32``. If a sparse matrix is provided, it will be
+            converted into a sparse ``csr_matrix``.
 
         Returns
         -------
@@ -552,9 +552,9 @@ class in a leaf.
         Parameters
         ----------
         X : array-like or sparse matrix of shape = [n_samples, n_features]
-            The input samples. Internally, it will be converted to
-            ``dtype=np.float32`` and if a sparse matrix is provided
-            to a sparse ``csr_matrix``.
+            The input samples. Internally, its dtype will be converted to
+            ``dtype=np.float32``. If a sparse matrix is provided, it will be
+            converted into a sparse ``csr_matrix``.
 
         Returns
         -------
@@ -605,9 +605,9 @@ def predict_log_proba(self, X):
         Parameters
         ----------
         X : array-like or sparse matrix of shape = [n_samples, n_features]
-            The input samples. Internally, it will be converted to
-            ``dtype=np.float32`` and if a sparse matrix is provided
-            to a sparse ``csr_matrix``.
+            The input samples. Internally, its dtype will be converted to
+            ``dtype=np.float32``. If a sparse matrix is provided, it will be
+            converted into a sparse ``csr_matrix``.
 
         Returns
         -------
@@ -666,9 +666,9 @@ def predict(self, X):
         Parameters
         ----------
         X : array-like or sparse matrix of shape = [n_samples, n_features]
-            The input samples. Internally, it will be converted to
-            ``dtype=np.float32`` and if a sparse matrix is provided
-            to a sparse ``csr_matrix``.
+            The input samples. Internally, its dtype will be converted to
+            ``dtype=np.float32``. If a sparse matrix is provided, it will be
+            converted into a sparse ``csr_matrix``.
 
         Returns
         -------
@@ -777,7 +777,6 @@ class RandomForestClassifier(ForestClassifier):
         The maximum depth of the tree. If None, then nodes are expanded until
         all leaves are pure or until all leaves contain less than
         min_samples_split samples.
-        Ignored if ``max_leaf_nodes`` is not None.
 
     min_samples_split : int, float, optional (default=2)
         The minimum number of samples required to split an internal node:
@@ -803,7 +802,6 @@ class RandomForestClassifier(ForestClassifier):
         Grow trees with ``max_leaf_nodes`` in best-first fashion.
         Best nodes are defined as relative reduction in impurity.
         If None then unlimited number of leaf nodes.
-        If not None then ``max_depth`` will be ignored.
 
     min_impurity_split : float, optional (default=1e-7)
         Threshold for early stopping in tree growth. A node will split
@@ -984,7 +982,6 @@ class RandomForestRegressor(ForestRegressor):
         The maximum depth of the tree. If None, then nodes are expanded until
         all leaves are pure or until all leaves contain less than
         min_samples_split samples.
-        Ignored if ``max_leaf_nodes`` is not None.
 
     min_samples_split : int, float, optional (default=2)
         The minimum number of samples required to split an internal node:
@@ -1010,7 +1007,6 @@ class RandomForestRegressor(ForestRegressor):
         Grow trees with ``max_leaf_nodes`` in best-first fashion.
         Best nodes are defined as relative reduction in impurity.
         If None then unlimited number of leaf nodes.
-        If not None then ``max_depth`` will be ignored.
 
     min_impurity_split : float, optional (default=1e-7)
         Threshold for early stopping in tree growth. A node will split
@@ -1151,7 +1147,6 @@ class ExtraTreesClassifier(ForestClassifier):
         The maximum depth of the tree. If None, then nodes are expanded until
         all leaves are pure or until all leaves contain less than
         min_samples_split samples.
-        Ignored if ``max_leaf_nodes`` is not None.
 
     min_samples_split : int, float, optional (default=2)
         The minimum number of samples required to split an internal node:
@@ -1177,7 +1172,6 @@ class ExtraTreesClassifier(ForestClassifier):
         Grow trees with ``max_leaf_nodes`` in best-first fashion.
         Best nodes are defined as relative reduction in impurity.
         If None then unlimited number of leaf nodes.
-        If not None then ``max_depth`` will be ignored.
 
     min_impurity_split : float, optional (default=1e-7)
         Threshold for early stopping in tree growth. A node will split
@@ -1357,7 +1351,6 @@ class ExtraTreesRegressor(ForestRegressor):
         The maximum depth of the tree. If None, then nodes are expanded until
         all leaves are pure or until all leaves contain less than
         min_samples_split samples.
-        Ignored if ``max_leaf_nodes`` is not None.
 
     min_samples_split : int, float, optional (default=2)
         The minimum number of samples required to split an internal node:
@@ -1383,7 +1376,6 @@ class ExtraTreesRegressor(ForestRegressor):
         Grow trees with ``max_leaf_nodes`` in best-first fashion.
         Best nodes are defined as relative reduction in impurity.
         If None then unlimited number of leaf nodes.
-        If not None then ``max_depth`` will be ignored.
 
     min_impurity_split : float, optional (default=1e-7)
         Threshold for early stopping in tree growth. A node will split
@@ -1510,7 +1502,6 @@ class RandomTreesEmbedding(BaseForest):
         The maximum depth of each tree. If None, then nodes are expanded until
         all leaves are pure or until all leaves contain less than
         min_samples_split samples.
-        Ignored if ``max_leaf_nodes`` is not None.
 
     min_samples_split : int, float, optional (default=2)
         The minimum number of samples required to split an internal node:
@@ -1536,7 +1527,6 @@ class RandomTreesEmbedding(BaseForest):
         Grow trees with ``max_leaf_nodes`` in best-first fashion.
         Best nodes are defined as relative reduction in impurity.
         If None then unlimited number of leaf nodes.
-        If not None then ``max_depth`` will be ignored.
 
     min_impurity_split : float, optional (default=1e-7)
         Threshold for early stopping in tree growth. A node will split
diff --git a/sklearn/ensemble/gradient_boosting.py b/sklearn/ensemble/gradient_boosting.py
index e74ed8bbcb83..4ea8ef8e4e77 100644
--- a/sklearn/ensemble/gradient_boosting.py
+++ b/sklearn/ensemble/gradient_boosting.py
@@ -1234,9 +1234,9 @@ def apply(self, X):
         Parameters
         ----------
         X : array-like or sparse matrix, shape = [n_samples, n_features]
-            The input samples. Internally, it will be converted to
-            ``dtype=np.float32`` and if a sparse matrix is provided
-            to a sparse ``csr_matrix``.
+            The input samples. Internally, its dtype will be converted to
+            ``dtype=np.float32``. If a sparse matrix is provided, it will
+            be converted to a sparse ``csr_matrix``.
 
         Returns
         -------
@@ -1296,7 +1296,6 @@ class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):
         depth limits the number of nodes in the tree. Tune this parameter
         for best performance; the best value depends on the interaction
         of the input variables.
-        Ignored if ``max_leaf_nodes`` is not None.
 
     criterion : string, optional (default="friedman_mse")
         The function to measure the quality of a split. Supported criteria
@@ -1359,7 +1358,6 @@ class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):
         Grow trees with ``max_leaf_nodes`` in best-first fashion.
         Best nodes are defined as relative reduction in impurity.
         If None then unlimited number of leaf nodes.
-        If not None then ``max_depth`` will be ignored.
 
     min_impurity_split : float, optional (default=1e-7)
         Threshold for early stopping in tree growth. A node will split
@@ -1661,7 +1659,6 @@ class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):
         depth limits the number of nodes in the tree. Tune this parameter
         for best performance; the best value depends on the interaction
         of the input variables.
-        Ignored if ``max_leaf_nodes`` is not None.
 
     criterion : string, optional (default="friedman_mse")
         The function to measure the quality of a split. Supported criteria
@@ -1868,9 +1865,9 @@ def apply(self, X):
         Parameters
         ----------
         X : array-like or sparse matrix, shape = [n_samples, n_features]
-            The input samples. Internally, it will be converted to
-            ``dtype=np.float32`` and if a sparse matrix is provided
-            to a sparse ``csr_matrix``.
+            The input samples. Internally, its dtype will be converted to
+            ``dtype=np.float32``. If a sparse matrix is provided, it will
+            be converted to a sparse ``csr_matrix``.
 
         Returns
         -------
diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py
index b79cc2e28ce0..05ea557b58bd 100644
--- a/sklearn/ensemble/iforest.py
+++ b/sklearn/ensemble/iforest.py
@@ -19,6 +19,8 @@
 
 __all__ = ["IsolationForest"]
 
+INTEGER_TYPES = (numbers.Integral, np.integer)
+
 
 class IsolationForest(BaseBagging):
     """Isolation Forest Algorithm
@@ -66,8 +68,8 @@ class IsolationForest(BaseBagging):
             - If float, then draw `max_features * X.shape[1]` features.
 
     bootstrap : boolean, optional (default=False)
-        If True, individual trees are fit on random subsets of the training 
-        data sampled with replacement. If False, sampling without replacement 
+        If True, individual trees are fit on random subsets of the training
+        data sampled with replacement. If False, sampling without replacement
         is performed.
 
     n_jobs : integer, optional (default=1)
@@ -170,7 +172,7 @@ def fit(self, X, y=None, sample_weight=None):
                                  'Valid choices are: "auto", int or'
                                  'float' % self.max_samples)
 
-        elif isinstance(self.max_samples, numbers.Integral):
+        elif isinstance(self.max_samples, INTEGER_TYPES):
             if self.max_samples > n_samples:
                 warn("max_samples (%s) is greater than the "
                      "total number of samples (%s). max_samples "
@@ -181,7 +183,8 @@ def fit(self, X, y=None, sample_weight=None):
                 max_samples = self.max_samples
         else:  # float
             if not (0. < self.max_samples <= 1.):
-                raise ValueError("max_samples must be in (0, 1]")
+                raise ValueError("max_samples must be in (0, 1], got %r"
+                                 % self.max_samples)
             max_samples = int(self.max_samples * X.shape[0])
 
         self.max_samples_ = max_samples
@@ -280,7 +283,7 @@ def _average_path_length(n_samples_leaf):
     average_path_length : array, same shape as n_samples_leaf
 
     """
-    if isinstance(n_samples_leaf, numbers.Integral):
+    if isinstance(n_samples_leaf, INTEGER_TYPES):
         if n_samples_leaf <= 1:
             return 1.
         else:
diff --git a/sklearn/ensemble/tests/test_bagging.py b/sklearn/ensemble/tests/test_bagging.py
index 5d85713a7621..70e44eb2824a 100644
--- a/sklearn/ensemble/tests/test_bagging.py
+++ b/sklearn/ensemble/tests/test_bagging.py
@@ -663,3 +663,61 @@ def test_oob_score_removed_on_warm_start():
     clf.fit(X, y)
 
     assert_raises(AttributeError, getattr, clf, "oob_score_")
+
+
+def test_oob_score_consistency():
+    # Make sure OOB scores are identical when random_state, estimator, and 
+    # training data are fixed and fitting is done twice
+    X, y = make_hastie_10_2(n_samples=200, random_state=1)
+    bagging = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5,
+                                max_features=0.5, oob_score=True,
+                                random_state=1)
+    assert_equal(bagging.fit(X, y).oob_score_, bagging.fit(X, y).oob_score_)
+
+
+def test_estimators_samples():
+    # Check that format of estimators_samples_ is correct and that results
+    # generated at fit time can be identically reproduced at a later time
+    # using data saved in object attributes.
+    X, y = make_hastie_10_2(n_samples=200, random_state=1)
+    bagging = BaggingClassifier(LogisticRegression(), max_samples=0.5,
+                                max_features=0.5, random_state=1,
+                                bootstrap=False)
+    bagging.fit(X, y)
+
+    # Get relevant attributes
+    estimators_samples = bagging.estimators_samples_
+    estimators_features = bagging.estimators_features_
+    estimators = bagging.estimators_
+
+    # Test for correct formatting
+    assert_equal(len(estimators_samples), len(estimators))
+    assert_equal(len(estimators_samples[0]), len(X))
+    assert_equal(estimators_samples[0].dtype.kind, 'b')
+
+    # Re-fit single estimator to test for consistent sampling
+    estimator_index = 0
+    estimator_samples = estimators_samples[estimator_index]
+    estimator_features = estimators_features[estimator_index]
+    estimator = estimators[estimator_index]
+
+    X_train = (X[estimator_samples])[:, estimator_features]
+    y_train = y[estimator_samples]
+
+    orig_coefs = estimator.coef_
+    estimator.fit(X_train, y_train)
+    new_coefs = estimator.coef_
+
+    assert_array_almost_equal(orig_coefs, new_coefs)
+
+
+def test_max_samples_consistency():
+    # Make sure validated max_samples and original max_samples are identical
+    # when valid integer max_samples supplied by user
+    max_samples = 100
+    X, y = make_hastie_10_2(n_samples=2*max_samples, random_state=1)
+    bagging = BaggingClassifier(KNeighborsClassifier(),
+                                max_samples=max_samples,
+                                max_features=0.5, random_state=1)
+    bagging.fit(X, y)
+    assert_equal(bagging._max_samples, max_samples)
diff --git a/sklearn/ensemble/tests/test_forest.py b/sklearn/ensemble/tests/test_forest.py
index 489ba40689d3..5ff4cf851f2d 100644
--- a/sklearn/ensemble/tests/test_forest.py
+++ b/sklearn/ensemble/tests/test_forest.py
@@ -147,7 +147,7 @@ def check_boston_criterion(name, criterion):
                           random_state=1)
     clf.fit(boston.data, boston.target)
     score = clf.score(boston.data, boston.target)
-    assert_greater(score, 0.95, "Failed with max_features=None, criterion %s "
+    assert_greater(score, 0.94, "Failed with max_features=None, criterion %s "
                                 "and score = %f" % (criterion, score))
 
     clf = ForestRegressor(n_estimators=5, criterion=criterion,
diff --git a/sklearn/ensemble/tests/test_iforest.py b/sklearn/ensemble/tests/test_iforest.py
index 00812dd1332e..a9aac308578d 100644
--- a/sklearn/ensemble/tests/test_iforest.py
+++ b/sklearn/ensemble/tests/test_iforest.py
@@ -193,3 +193,10 @@ def test_iforest_works():
     # assert detect outliers:
     assert_greater(np.min(decision_func[-2:]), np.max(decision_func[:-2]))
     assert_array_equal(pred, 6 * [1] + 2 * [-1])
+
+
+def test_max_samples_consistency():
+    # Make sure validated max_samples in iforest and BaseBagging are identical
+    X = iris.data
+    clf = IsolationForest().fit(X)
+    assert_equal(clf.max_samples_, clf._max_samples)
diff --git a/sklearn/ensemble/tests/test_voting_classifier.py b/sklearn/ensemble/tests/test_voting_classifier.py
index bc08d3a8d2c3..f3a5274ab3f4 100644
--- a/sklearn/ensemble/tests/test_voting_classifier.py
+++ b/sklearn/ensemble/tests/test_voting_classifier.py
@@ -1,7 +1,7 @@
-"""Testing for the boost module (sklearn.ensemble.boost)."""
+"""Testing for the VotingClassifier"""
 
 import numpy as np
-from sklearn.utils.testing import assert_almost_equal
+from sklearn.utils.testing import assert_almost_equal, assert_array_equal
 from sklearn.utils.testing import assert_equal
 from sklearn.utils.testing import assert_raise_message
 from sklearn.exceptions import NotFittedError
@@ -15,6 +15,7 @@
 from sklearn.datasets import make_multilabel_classification
 from sklearn.svm import SVC
 from sklearn.multiclass import OneVsRestClassifier
+from sklearn.neighbors import KNeighborsClassifier
 
 
 # Load the iris dataset and randomly permute it
@@ -207,3 +208,53 @@ def test_gridsearch():
 
     grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)
     grid.fit(iris.data, iris.target)
+
+
+def test_parallel_predict():
+    """Check parallel backend of VotingClassifier on toy dataset."""
+    clf1 = LogisticRegression(random_state=123)
+    clf2 = RandomForestClassifier(random_state=123)
+    clf3 = GaussianNB()
+    X = np.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])
+    y = np.array([1, 1, 2, 2])
+
+    eclf1 = VotingClassifier(estimators=[
+        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],
+        voting='soft',
+        n_jobs=1).fit(X, y)
+    eclf2 = VotingClassifier(estimators=[
+        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],
+        voting='soft',
+        n_jobs=2).fit(X, y)
+
+    assert_array_equal(eclf1.predict(X), eclf2.predict(X))
+    assert_array_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))
+
+
+def test_sample_weight():
+    """Tests sample_weight parameter of VotingClassifier"""
+    clf1 = LogisticRegression(random_state=123)
+    clf2 = RandomForestClassifier(random_state=123)
+    clf3 = SVC(probability=True, random_state=123)
+    eclf1 = VotingClassifier(estimators=[
+        ('lr', clf1), ('rf', clf2), ('svc', clf3)],
+        voting='soft').fit(X, y, sample_weight=np.ones((len(y),)))
+    eclf2 = VotingClassifier(estimators=[
+        ('lr', clf1), ('rf', clf2), ('svc', clf3)],
+        voting='soft').fit(X, y)
+    assert_array_equal(eclf1.predict(X), eclf2.predict(X))
+    assert_array_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))
+
+    sample_weight = np.random.RandomState(123).uniform(size=(len(y),))
+    eclf3 = VotingClassifier(estimators=[('lr', clf1)], voting='soft')
+    eclf3.fit(X, y, sample_weight)
+    clf1.fit(X, y, sample_weight)
+    assert_array_equal(eclf3.predict(X), clf1.predict(X))
+    assert_array_equal(eclf3.predict_proba(X), clf1.predict_proba(X))
+
+    clf4 = KNeighborsClassifier()
+    eclf3 = VotingClassifier(estimators=[
+        ('lr', clf1), ('svc', clf3), ('knn', clf4)],
+        voting='soft')
+    msg = ('Underlying estimator \'knn\' does not support sample weights.')
+    assert_raise_message(ValueError, msg, eclf3.fit, X, y, sample_weight)
diff --git a/sklearn/ensemble/voting_classifier.py b/sklearn/ensemble/voting_classifier.py
index d77a40c3718b..40e21d56270d 100644
--- a/sklearn/ensemble/voting_classifier.py
+++ b/sklearn/ensemble/voting_classifier.py
@@ -19,7 +19,17 @@
 from ..base import clone
 from ..preprocessing import LabelEncoder
 from ..externals import six
-from ..utils.validation import check_is_fitted
+from ..externals.joblib import Parallel, delayed
+from ..utils.validation import has_fit_parameter, check_is_fitted
+
+
+def _parallel_fit_estimator(estimator, X, y, sample_weight):
+    """Private function used to fit an estimator within a job."""
+    if sample_weight is not None:
+        estimator.fit(X, y, sample_weight)
+    else:
+        estimator.fit(X, y)
+    return estimator
 
 
 class VotingClassifier(BaseEstimator, ClassifierMixin, TransformerMixin):
@@ -47,6 +57,10 @@ class VotingClassifier(BaseEstimator, ClassifierMixin, TransformerMixin):
         predicted class labels (`hard` voting) or class probabilities
         before averaging (`soft` voting). Uses uniform weights if `None`.
 
+    n_jobs : int, optional (default=1)
+        The number of jobs to run in parallel for ``fit``.
+        If -1, then the number of jobs is set to the number of cores.
+
     Attributes
     ----------
     estimators_ : list of classifiers
@@ -86,14 +100,14 @@ class VotingClassifier(BaseEstimator, ClassifierMixin, TransformerMixin):
     >>>
     """
 
-    def __init__(self, estimators, voting='hard', weights=None):
-
+    def __init__(self, estimators, voting='hard', weights=None, n_jobs=1):
         self.estimators = estimators
         self.named_estimators = dict(estimators)
         self.voting = voting
         self.weights = weights
+        self.n_jobs = n_jobs
 
-    def fit(self, X, y):
+    def fit(self, X, y, sample_weight=None):
         """ Fit the estimators.
 
         Parameters
@@ -105,6 +119,11 @@ def fit(self, X, y):
         y : array-like, shape = [n_samples]
             Target values.
 
+        sample_weight : array-like, shape = [n_samples] or None
+            Sample weights. If None, then samples are equally weighted.
+            Note that this is supported only if all underlying estimators
+            support sample weights.
+
         Returns
         -------
         self : object
@@ -127,14 +146,23 @@ def fit(self, X, y):
                              '; got %d weights, %d estimators'
                              % (len(self.weights), len(self.estimators)))
 
+        if sample_weight is not None:
+            for name, step in self.estimators:
+                if not has_fit_parameter(step, 'sample_weight'):
+                    raise ValueError('Underlying estimator \'%s\' does not support'
+                                     ' sample weights.' % name)
+
         self.le_ = LabelEncoder()
         self.le_.fit(y)
         self.classes_ = self.le_.classes_
         self.estimators_ = []
 
-        for name, clf in self.estimators:
-            fitted_clf = clone(clf).fit(X, self.le_.transform(y))
-            self.estimators_.append(fitted_clf)
+        transformed_y = self.le_.transform(y)
+
+        self.estimators_ = Parallel(n_jobs=self.n_jobs)(
+                delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,
+                    sample_weight)
+                    for _, clf in self.estimators)
 
         return self
 
@@ -214,7 +242,7 @@ def transform(self, X):
           array-like = [n_classifiers, n_samples, n_classes]
             Class probabilities calculated by each classifier.
         If `voting='hard'`:
-          array-like = [n_classifiers, n_samples]
+          array-like = [n_samples, n_classifiers]
             Class labels predicted by each classifier.
         """
         check_is_fitted(self, 'estimators_')
diff --git a/sklearn/externals/joblib/__init__.py b/sklearn/externals/joblib/__init__.py
index 970ccd6c362c..43000626b2cf 100644
--- a/sklearn/externals/joblib/__init__.py
+++ b/sklearn/externals/joblib/__init__.py
@@ -115,7 +115,7 @@
 # Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
 # 'X.Y.dev0' is the canonical version of 'X.Y.dev'
 #
-__version__ = '0.10.0'
+__version__ = '0.10.2'
 
 
 from .memory import Memory, MemorizedResult
diff --git a/sklearn/externals/joblib/_memory_helpers.py b/sklearn/externals/joblib/_memory_helpers.py
index 857ad29d79ad..08a90de96d28 100644
--- a/sklearn/externals/joblib/_memory_helpers.py
+++ b/sklearn/externals/joblib/_memory_helpers.py
@@ -102,4 +102,4 @@ def open_py_source(filename):
         buffer.seek(0)
         text = TextIOWrapper(buffer, encoding, line_buffering=True)
         text.mode = 'r'
-        return text
+        return text
\ No newline at end of file
diff --git a/sklearn/externals/joblib/_parallel_backends.py b/sklearn/externals/joblib/_parallel_backends.py
index e281bd5b69c7..e9325830479c 100644
--- a/sklearn/externals/joblib/_parallel_backends.py
+++ b/sklearn/externals/joblib/_parallel_backends.py
@@ -25,9 +25,9 @@ class ParallelBackendBase(with_metaclass(ABCMeta)):
     def effective_n_jobs(self, n_jobs):
         """Determine the number of jobs that can actually run in parallel
 
-        n_jobs is the is the number of workers requested by the callers.
-        Passing n_jobs=-1 means requesting all available workers for instance
-        matching the number of CPU cores on the worker host(s).
+        n_jobs is the number of workers requested by the callers. Passing
+        n_jobs=-1 means requesting all available workers for instance matching
+        the number of CPU cores on the worker host(s).
 
         This method should return a guesstimate of the number of workers that
         can actually perform work concurrently. The primary use case is to make
diff --git a/sklearn/externals/joblib/format_stack.py b/sklearn/externals/joblib/format_stack.py
index ad28a86c2496..91eabeb0e1ad 100644
--- a/sklearn/externals/joblib/format_stack.py
+++ b/sklearn/externals/joblib/format_stack.py
@@ -273,8 +273,10 @@ def linereader(file=file, lnum=[lnum], getline=linecache.getline):
             # enclosing scope.
             for token in generate_tokens(linereader):
                 tokeneater(*token)
-        except (IndexError, UnicodeDecodeError):
+        except (IndexError, UnicodeDecodeError, SyntaxError):
             # signals exit of tokenizer
+            # SyntaxError can happen when trying to tokenize
+            # a compiled (e.g. .so or .pyd) extension
             pass
         except tokenize.TokenError as msg:
             _m = ("An unexpected error occurred while tokenizing input file %s\n"
diff --git a/sklearn/externals/joblib/func_inspect.py b/sklearn/externals/joblib/func_inspect.py
index 9fb67f024165..ad5a548d38de 100644
--- a/sklearn/externals/joblib/func_inspect.py
+++ b/sklearn/externals/joblib/func_inspect.py
@@ -50,8 +50,7 @@ def get_func_code(func):
             line_no = 1
             if source_file.startswith('<doctest '):
                 source_file, line_no = re.match(
-                            '\<doctest (.*\.rst)\[(.*)\]\>',
-                            source_file).groups()
+                    '\<doctest (.*\.rst)\[(.*)\]\>', source_file).groups()
                 line_no = int(line_no)
                 source_file = '<doctest %s>' % source_file
             return source_code, source_file, line_no
@@ -312,7 +311,7 @@ def filter_args(func, ignore_lst, args=(), kwargs=dict()):
                              "function %s"
                              % (item,
                                 _signature_str(name, arg_spec))
-            )
+                             )
     # XXX: Return a sorted list of pairs?
     return arg_dict
 
@@ -352,4 +351,4 @@ def format_call(func, args, kwargs, object_name="Memory"):
                                           path, signature)
     return msg
     # XXX: Not using logging framework
-    #self.debug(msg)
+    # self.debug(msg)
diff --git a/sklearn/externals/joblib/numpy_pickle_utils.py b/sklearn/externals/joblib/numpy_pickle_utils.py
index ee879a691247..6f471073e672 100644
--- a/sklearn/externals/joblib/numpy_pickle_utils.py
+++ b/sklearn/externals/joblib/numpy_pickle_utils.py
@@ -14,7 +14,7 @@
 import contextlib
 from contextlib import closing
 
-from ._compat import PY3_OR_LATER, PY26, PY27
+from ._compat import PY3_OR_LATER, PY26, PY27, _basestring
 
 try:
     from threading import RLock
@@ -299,7 +299,7 @@ def __init__(self, filename, mode="rb", compresslevel=9):
         else:
             raise ValueError("Invalid mode: %r" % (mode,))
 
-        if isinstance(filename, (str, bytes)):
+        if isinstance(filename, _basestring):
             self._fp = open(filename, mode)
             self._closefp = True
             self._mode = mode_code
diff --git a/sklearn/externals/joblib/parallel.py b/sklearn/externals/joblib/parallel.py
index 43913f397972..74bd18c4fb25 100644
--- a/sklearn/externals/joblib/parallel.py
+++ b/sklearn/externals/joblib/parallel.py
@@ -32,6 +32,10 @@
 from ._compat import _basestring
 from .func_inspect import getfullargspec
 
+# Make sure that those two classes are part of the public joblib.parallel API
+# so that 3rd party backend implementers can import them from here.
+from ._parallel_backends import AutoBatchingMixin  # noqa
+from ._parallel_backends import ParallelBackendBase  # noqa
 
 BACKENDS = {
     'multiprocessing': MultiprocessingBackend,
diff --git a/sklearn/feature_extraction/image.py b/sklearn/feature_extraction/image.py
index 7b9c5c94e0cd..f4bfd7e53389 100644
--- a/sklearn/feature_extraction/image.py
+++ b/sklearn/feature_extraction/image.py
@@ -152,8 +152,8 @@ def img_to_graph(img, mask=None, return_as=sparse.coo_matrix, dtype=None):
 
     Notes
     -----
-    For sklearn versions 0.14.1 and prior, return_as=np.ndarray was handled
-    by returning a dense np.matrix instance.  Going forward, np.ndarray
+    For scikit-learn versions 0.14.1 and prior, return_as=np.ndarray was
+    handled by returning a dense np.matrix instance.  Going forward, np.ndarray
     returns an np.ndarray, as expected.
 
     For compatibility, user code relying on this method should wrap its
@@ -188,8 +188,8 @@ def grid_to_graph(n_x, n_y, n_z=1, mask=None, return_as=sparse.coo_matrix,
 
     Notes
     -----
-    For sklearn versions 0.14.1 and prior, return_as=np.ndarray was handled
-    by returning a dense np.matrix instance.  Going forward, np.ndarray
+    For scikit-learn versions 0.14.1 and prior, return_as=np.ndarray was
+    handled by returning a dense np.matrix instance.  Going forward, np.ndarray
     returns an np.ndarray, as expected.
 
     For compatibility, user code relying on this method should wrap its
diff --git a/sklearn/feature_extraction/text.py b/sklearn/feature_extraction/text.py
index 97d12c4fb94e..d291cc54d60b 100644
--- a/sklearn/feature_extraction/text.py
+++ b/sklearn/feature_extraction/text.py
@@ -920,15 +920,32 @@ class TfidfTransformer(BaseEstimator, TransformerMixin):
     informative than features that occur in a small fraction of the training
     corpus.
 
-    The actual formula used for tf-idf is tf * (idf + 1) = tf + tf * idf,
-    instead of tf * idf. The effect of this is that terms with zero idf, i.e.
-    that occur in all documents of a training set, will not be entirely
-    ignored. The formulas used to compute tf and idf depend on parameter
-    settings that correspond to the SMART notation used in IR, as follows:
-
-    Tf is "n" (natural) by default, "l" (logarithmic) when sublinear_tf=True.
+    The formula that is used to compute the tf-idf of term t is
+    tf-idf(d, t) = tf(t) * idf(d, t), and the idf is computed as
+    idf(d, t) = log [ n / df(d, t) ] + 1 (if ``smooth_idf=False``),
+    where n is the total number of documents and df(d, t) is the
+    document frequency; the document frequency is the number of documents d
+    that contain term t. The effect of adding "1" to the idf in the equation
+    above is that terms with zero idf, i.e., terms  that occur in all documents
+    in a training set, will not be entirely ignored.
+    (Note that the idf formula above differs from the standard
+    textbook notation that defines the idf as
+    idf(d, t) = log [ n / (df(d, t) + 1) ]).
+
+    If ``smooth_idf=True`` (the default), the constant "1" is added to the
+    numerator and denominator of the idf as if an extra document was seen
+    containing every term in the collection exactly once, which prevents
+    zero divisions: idf(d, t) = log [ (1 + n) / 1 + df(d, t) ] + 1.
+
+    Furthermore, the formulas used to compute tf and idf depend
+    on parameter settings that correspond to the SMART notation used in IR
+    as follows:
+
+    Tf is "n" (natural) by default, "l" (logarithmic) when
+    ``sublinear_tf=True``.
     Idf is "t" when use_idf is given, "n" (none) otherwise.
-    Normalization is "c" (cosine) when norm='l2', "n" (none) when norm=None.
+    Normalization is "c" (cosine) when ``norm='l2'``, "n" (none)
+    when ``norm=None``.
 
     Read more in the :ref:`User Guide <text_feature_extraction>`.
 
diff --git a/sklearn/gaussian_process/gpr.py b/sklearn/gaussian_process/gpr.py
index e04f2a6798b2..24ff1b058ae8 100644
--- a/sklearn/gaussian_process/gpr.py
+++ b/sklearn/gaussian_process/gpr.py
@@ -23,7 +23,8 @@ class GaussianProcessRegressor(BaseEstimator, RegressorMixin):
     The implementation is based on Algorithm 2.1 of Gaussian Processes
     for Machine Learning (GPML) by Rasmussen and Williams.
 
-    In addition to standard sklearn estimator API, GaussianProcessRegressor:
+    In addition to standard scikit-learn estimator API,
+    GaussianProcessRegressor:
 
        * allows prediction without prior fitting (based on the GP prior)
        * provides an additional method sample_y(X), which evaluates samples
diff --git a/sklearn/gaussian_process/kernels.py b/sklearn/gaussian_process/kernels.py
index 251bc10e2d3e..7b5aa005ad8f 100644
--- a/sklearn/gaussian_process/kernels.py
+++ b/sklearn/gaussian_process/kernels.py
@@ -33,6 +33,17 @@
 from sklearn.externals.funcsigs import signature
 
 
+def _check_length_scale(X, length_scale):
+    length_scale = np.squeeze(length_scale).astype(float)
+    if np.ndim(length_scale) > 1:
+        raise ValueError("length_scale cannot be of dimension greater than 1")
+    if np.ndim(length_scale) == 1 and X.shape[1] != length_scale.shape[0]:
+        raise ValueError("Anisotropic kernel must have the same number of "
+                         "dimensions as data (%d!=%d)"
+                         % (length_scale.shape[0], X.shape[1]))
+    return length_scale
+
+
 class Hyperparameter(namedtuple('Hyperparameter',
                                 ('name', 'value_type', 'bounds',
                                  'n_elements', 'fixed'))):
@@ -92,6 +103,15 @@ def __new__(cls, name, value_type, bounds, n_elements=1, fixed=None):
         return super(Hyperparameter, cls).__new__(
             cls, name, value_type, bounds, n_elements, fixed)
 
+    # This is mainly a testing utility to check that two hyperparameters
+    # are equal.
+    def __eq__(self, other):
+        return (self.name == other.name and
+                self.value_type == other.value_type and
+                np.all(self.bounds == other.bounds) and
+                self.n_elements == other.n_elements and
+                self.fixed == other.fixed)
+
 
 class Kernel(six.with_metaclass(ABCMeta)):
     """Base class for all kernels."""
@@ -187,9 +207,9 @@ def n_dims(self):
     def hyperparameters(self):
         """Returns a list of all hyperparameter specifications."""
         r = []
-        for attr, value in sorted(self.__dict__.items()):
+        for attr in dir(self):
             if attr.startswith("hyperparameter_"):
-                r.append(value)
+                r.append(getattr(self, attr))
         return r
 
     @property
@@ -207,9 +227,10 @@ def theta(self):
             The non-fixed, log-transformed hyperparameters of the kernel
         """
         theta = []
+        params = self.get_params()
         for hyperparameter in self.hyperparameters:
             if not hyperparameter.fixed:
-                theta.append(getattr(self, hyperparameter.name))
+                theta.append(params[hyperparameter.name])
         if len(theta) > 0:
             return np.log(np.hstack(theta))
         else:
@@ -224,23 +245,25 @@ def theta(self, theta):
         theta : array, shape (n_dims,)
             The non-fixed, log-transformed hyperparameters of the kernel
         """
+        params = self.get_params()
         i = 0
         for hyperparameter in self.hyperparameters:
             if hyperparameter.fixed:
                 continue
             if hyperparameter.n_elements > 1:
                 # vector-valued parameter
-                setattr(self, hyperparameter.name,
-                        np.exp(theta[i:i + hyperparameter.n_elements]))
+                params[hyperparameter.name] = np.exp(
+                    theta[i:i + hyperparameter.n_elements])
                 i += hyperparameter.n_elements
             else:
-                setattr(self, hyperparameter.name, np.exp(theta[i]))
+                params[hyperparameter.name] = np.exp(theta[i])
                 i += 1
 
         if i != len(theta):
             raise ValueError("theta has not the correct number of entries."
                              " Should be %d; given are %d"
                              % (i, len(theta)))
+        self.set_params(**params)
 
     @property
     def bounds(self):
@@ -910,8 +933,10 @@ def __init__(self, constant_value=1.0, constant_value_bounds=(1e-5, 1e5)):
         self.constant_value = constant_value
         self.constant_value_bounds = constant_value_bounds
 
-        self.hyperparameter_constant_value = \
-            Hyperparameter("constant_value", "numeric", constant_value_bounds)
+    @property
+    def hyperparameter_constant_value(self):
+        return Hyperparameter(
+            "constant_value", "numeric", self.constant_value_bounds)
 
     def __call__(self, X, Y=None, eval_gradient=False):
         """Return the kernel k(X, Y) and optionally its gradient.
@@ -999,8 +1024,10 @@ def __init__(self, noise_level=1.0, noise_level_bounds=(1e-5, 1e5)):
         self.noise_level = noise_level
         self.noise_level_bounds = noise_level_bounds
 
-        self.hyperparameter_noise_level = \
-            Hyperparameter("noise_level", "numeric", noise_level_bounds)
+    @property
+    def hyperparameter_noise_level(self):
+        return Hyperparameter(
+            "noise_level", "numeric", self.noise_level_bounds)
 
     def __call__(self, X, Y=None, eval_gradient=False):
         """Return the kernel k(X, Y) and optionally its gradient.
@@ -1095,25 +1122,21 @@ class RBF(StationaryKernelMixin, NormalizedKernelMixin, Kernel):
         The lower and upper bound on length_scale
     """
     def __init__(self, length_scale=1.0, length_scale_bounds=(1e-5, 1e5)):
-        if np.iterable(length_scale):
-            if len(length_scale) > 1:
-                self.anisotropic = True
-                self.length_scale = np.asarray(length_scale, dtype=np.float)
-            else:
-                self.anisotropic = False
-                self.length_scale = float(length_scale[0])
-        else:
-            self.anisotropic = False
-            self.length_scale = float(length_scale)
+        self.length_scale = length_scale
         self.length_scale_bounds = length_scale_bounds
 
-        if self.anisotropic:  # anisotropic length_scale
-            self.hyperparameter_length_scale = \
-                Hyperparameter("length_scale", "numeric", length_scale_bounds,
-                               len(length_scale))
-        else:
-            self.hyperparameter_length_scale = \
-                Hyperparameter("length_scale", "numeric", length_scale_bounds)
+    @property
+    def anisotropic(self):
+        return np.iterable(self.length_scale) and len(self.length_scale) > 1
+
+    @property
+    def hyperparameter_length_scale(self):
+        if self.anisotropic:
+            return Hyperparameter("length_scale", "numeric",
+                                  self.length_scale_bounds,
+                                  len(self.length_scale))
+        return Hyperparameter(
+            "length_scale", "numeric", self.length_scale_bounds)
 
     def __call__(self, X, Y=None, eval_gradient=False):
         """Return the kernel k(X, Y) and optionally its gradient.
@@ -1142,13 +1165,9 @@ def __call__(self, X, Y=None, eval_gradient=False):
             is True.
         """
         X = np.atleast_2d(X)
-        if self.anisotropic and X.shape[1] != self.length_scale.shape[0]:
-            raise Exception("Anisotropic kernel must have the same number of "
-                            "dimensions as data (%d!=%d)"
-                            % (self.length_scale.shape[0], X.shape[1]))
-
+        length_scale = _check_length_scale(X, self.length_scale)
         if Y is None:
-            dists = pdist(X / self.length_scale, metric='sqeuclidean')
+            dists = pdist(X / length_scale, metric='sqeuclidean')
             K = np.exp(-.5 * dists)
             # convert from upper-triangular matrix to square matrix
             K = squareform(K)
@@ -1157,7 +1176,7 @@ def __call__(self, X, Y=None, eval_gradient=False):
             if eval_gradient:
                 raise ValueError(
                     "Gradient can only be evaluated when Y is None.")
-            dists = cdist(X / self.length_scale, Y / self.length_scale,
+            dists = cdist(X / length_scale, Y / length_scale,
                           metric='sqeuclidean')
             K = np.exp(-.5 * dists)
 
@@ -1165,19 +1184,16 @@ def __call__(self, X, Y=None, eval_gradient=False):
             if self.hyperparameter_length_scale.fixed:
                 # Hyperparameter l kept fixed
                 return K, np.empty((X.shape[0], X.shape[0], 0))
-            elif not self.anisotropic or self.length_scale.shape[0] == 1:
+            elif not self.anisotropic or length_scale.shape[0] == 1:
                 K_gradient = \
                     (K * squareform(dists))[:, :, np.newaxis]
                 return K, K_gradient
             elif self.anisotropic:
                 # We need to recompute the pairwise dimension-wise distances
                 K_gradient = (X[:, np.newaxis, :] - X[np.newaxis, :, :]) ** 2 \
-                    / (self.length_scale ** 2)
+                    / (length_scale ** 2)
                 K_gradient *= K[..., np.newaxis]
                 return K, K_gradient
-            else:
-                raise Exception("Anisotropic kernels require that the number "
-                                "of length scales and features match.")
         else:
             return K
 
@@ -1188,7 +1204,7 @@ def __repr__(self):
                                                    self.length_scale)))
         else:  # isotropic
             return "{0}(length_scale={1:.3g})".format(
-                self.__class__.__name__, self.length_scale)
+                self.__class__.__name__, np.ravel(self.length_scale)[0])
 
 
 class Matern(RBF):
@@ -1259,18 +1275,14 @@ def __call__(self, X, Y=None, eval_gradient=False):
             is True.
         """
         X = np.atleast_2d(X)
-        if self.anisotropic and X.shape[1] != self.length_scale.shape[0]:
-            raise Exception("Anisotropic kernel must have the same number of "
-                            "dimensions as data (%d!=%d)"
-                            % (self.length_scale.shape[0], X.shape[1]))
-
+        length_scale = _check_length_scale(X, self.length_scale)
         if Y is None:
-            dists = pdist(X / self.length_scale, metric='euclidean')
+            dists = pdist(X / length_scale, metric='euclidean')
         else:
             if eval_gradient:
                 raise ValueError(
                     "Gradient can only be evaluated when Y is None.")
-            dists = cdist(X / self.length_scale, Y / self.length_scale,
+            dists = cdist(X / length_scale, Y / length_scale,
                           metric='euclidean')
 
         if self.nu == 0.5:
@@ -1303,7 +1315,7 @@ def __call__(self, X, Y=None, eval_gradient=False):
             # We need to recompute the pairwise dimension-wise distances
             if self.anisotropic:
                 D = (X[:, np.newaxis, :] - X[np.newaxis, :, :])**2 \
-                    / (self.length_scale ** 2)
+                    / (length_scale ** 2)
             else:
                 D = squareform(dists**2)[:, :, np.newaxis]
 
@@ -1336,9 +1348,10 @@ def __repr__(self):
                 self.__class__.__name__,
                 ", ".join(map("{0:.3g}".format, self.length_scale)),
                 self.nu)
-        else:  # isotropic
+        else:
             return "{0}(length_scale={1:.3g}, nu={2:.3g})".format(
-                self.__class__.__name__, self.length_scale, self.nu)
+                self.__class__.__name__, np.ravel(self.length_scale)[0],
+                self.nu)
 
 
 class RationalQuadratic(StationaryKernelMixin, NormalizedKernelMixin, Kernel):
@@ -1373,10 +1386,14 @@ def __init__(self, length_scale=1.0, alpha=1.0,
         self.length_scale_bounds = length_scale_bounds
         self.alpha_bounds = alpha_bounds
 
-        self.hyperparameter_length_scale = \
-            Hyperparameter("length_scale", "numeric", length_scale_bounds)
-        self.hyperparameter_alpha = \
-            Hyperparameter("alpha", "numeric", alpha_bounds)
+    @property
+    def hyperparameter_length_scale(self):
+        return Hyperparameter(
+            "length_scale", "numeric", self.length_scale_bounds)
+
+    @property
+    def hyperparameter_alpha(self):
+        return Hyperparameter("alpha", "numeric", self.alpha_bounds)
 
     def __call__(self, X, Y=None, eval_gradient=False):
         """Return the kernel k(X, Y) and optionally its gradient.
@@ -1478,10 +1495,15 @@ def __init__(self, length_scale=1.0, periodicity=1.0,
         self.length_scale_bounds = length_scale_bounds
         self.periodicity_bounds = periodicity_bounds
 
-        self.hyperparameter_length_scale = \
-            Hyperparameter("length_scale", "numeric", length_scale_bounds)
-        self.hyperparameter_periodicity = \
-            Hyperparameter("periodicity", "numeric", periodicity_bounds)
+    @property
+    def hyperparameter_length_scale(self):
+        return Hyperparameter(
+            "length_scale", "numeric", self.length_scale_bounds)
+
+    @property
+    def hyperparameter_periodicity(self):
+        return Hyperparameter(
+            "periodicity", "numeric", self.periodicity_bounds)
 
     def __call__(self, X, Y=None, eval_gradient=False):
         """Return the kernel k(X, Y) and optionally its gradient.
@@ -1579,8 +1601,9 @@ def __init__(self, sigma_0=1.0, sigma_0_bounds=(1e-5, 1e5)):
         self.sigma_0 = sigma_0
         self.sigma_0_bounds = sigma_0_bounds
 
-        self.hyperparameter_sigma_0 = \
-            Hyperparameter("sigma_0", "numeric", sigma_0_bounds)
+    @property
+    def hyperparameter_sigma_0(self):
+        return Hyperparameter("sigma_0", "numeric", self.sigma_0_bounds)
 
     def __call__(self, X, Y=None, eval_gradient=False):
         """Return the kernel k(X, Y) and optionally its gradient.
@@ -1707,15 +1730,12 @@ def __init__(self, gamma=1.0, gamma_bounds=(1e-5, 1e5), metric="linear",
                  pairwise_kernels_kwargs=None):
         self.gamma = gamma
         self.gamma_bounds = gamma_bounds
-
-        self.hyperparameter_gamma = \
-            Hyperparameter("gamma", "numeric", gamma_bounds)
-
         self.metric = metric
-        if pairwise_kernels_kwargs is not None:
-            self.pairwise_kernels_kwargs = pairwise_kernels_kwargs
-        else:
-            self.pairwise_kernels_kwargs = {}
+        self.pairwise_kernels_kwargs = pairwise_kernels_kwargs
+
+    @property
+    def hyperparameter_gamma(self):
+        return Hyperparameter("gamma", "numeric", self.gamma_bounds)
 
     def __call__(self, X, Y=None, eval_gradient=False):
         """Return the kernel k(X, Y) and optionally its gradient.
@@ -1743,10 +1763,14 @@ def __call__(self, X, Y=None, eval_gradient=False):
             hyperparameter of the kernel. Only returned when eval_gradient
             is True.
         """
+        pairwise_kernels_kwargs = self.pairwise_kernels_kwargs
+        if self.pairwise_kernels_kwargs is None:
+            pairwise_kernels_kwargs = {}
+
         X = np.atleast_2d(X)
         K = pairwise_kernels(X, Y, metric=self.metric, gamma=self.gamma,
                              filter_params=True,
-                             **self.pairwise_kernels_kwargs)
+                             **pairwise_kernels_kwargs)
         if eval_gradient:
             if self.hyperparameter_gamma.fixed:
                 return K, np.empty((X.shape[0], X.shape[0], 0))
@@ -1755,7 +1779,7 @@ def __call__(self, X, Y=None, eval_gradient=False):
                 def f(gamma):  # helper function
                     return pairwise_kernels(
                         X, Y, metric=self.metric, gamma=np.exp(gamma),
-                        filter_params=True, **self.pairwise_kernels_kwargs)
+                        filter_params=True, **pairwise_kernels_kwargs)
                 return K, _approx_fprime(self.theta, f, 1e-10)
         else:
             return K
diff --git a/sklearn/gaussian_process/tests/test_kernels.py b/sklearn/gaussian_process/tests/test_kernels.py
index d69513be4365..116fad8ddaf9 100644
--- a/sklearn/gaussian_process/tests/test_kernels.py
+++ b/sklearn/gaussian_process/tests/test_kernels.py
@@ -41,7 +41,8 @@
            4.0 * Matern(length_scale=[0.5, 0.5], nu=2.5),
            RationalQuadratic(length_scale=0.5, alpha=1.5),
            ExpSineSquared(length_scale=0.5, periodicity=1.5),
-           DotProduct(sigma_0=2.0), DotProduct(sigma_0=2.0) ** 2]
+           DotProduct(sigma_0=2.0), DotProduct(sigma_0=2.0) ** 2,
+           RBF(length_scale=[2.0]), Matern(length_scale=[2.0])]
 for metric in PAIRWISE_KERNEL_FUNCTIONS:
     if metric in ["additive_chi2", "chi2"]:
         continue
@@ -183,36 +184,55 @@ def test_kernel_stationary():
         assert_almost_equal(K[0, 0], np.diag(K))
 
 
+def check_hyperparameters_equal(kernel1, kernel2):
+    """Check that hyperparameters of two kernels are equal"""
+    for attr in set(dir(kernel1) + dir(kernel2)):
+        if attr.startswith("hyperparameter_"):
+            attr_value1 = getattr(kernel1, attr)
+            attr_value2 = getattr(kernel2, attr)
+            assert_equal(attr_value1, attr_value2)
+
+
 def test_kernel_clone():
     """ Test that sklearn's clone works correctly on kernels. """
+    bounds = (1e-5, 1e5)
     for kernel in kernels:
         kernel_cloned = clone(kernel)
 
+        # XXX: Should this be fixed?
+        # This differs from the sklearn's estimators equality check.
         assert_equal(kernel, kernel_cloned)
         assert_not_equal(id(kernel), id(kernel_cloned))
-        for attr in kernel.__dict__.keys():
-            attr_value = getattr(kernel, attr)
-            attr_value_cloned = getattr(kernel_cloned, attr)
-            if attr.startswith("hyperparameter_"):
-                assert_equal(attr_value.name, attr_value_cloned.name)
-                assert_equal(attr_value.value_type,
-                             attr_value_cloned.value_type)
-                assert_array_equal(attr_value.bounds,
-                                   attr_value_cloned.bounds)
-                assert_equal(attr_value.n_elements,
-                             attr_value_cloned.n_elements)
-            elif np.iterable(attr_value):
-                for i in range(len(attr_value)):
-                    if np.iterable(attr_value[i]):
-                        assert_array_equal(attr_value[i],
-                                           attr_value_cloned[i])
-                    else:
-                        assert_equal(attr_value[i], attr_value_cloned[i])
+
+        # Check that all constructor parameters are equal.
+        assert_equal(kernel.get_params(), kernel_cloned.get_params())
+
+        # Check that all hyperparameters are equal.
+        yield check_hyperparameters_equal, kernel, kernel_cloned
+
+        # This test is to verify that using set_params does not
+        # break clone on kernels.
+        # This used to break because in kernels such as the RBF, non-trivial
+        # logic that modified the length scale used to be in the constructor
+        # See https://github.com/scikit-learn/scikit-learn/issues/6961
+        # for more details.
+        params = kernel.get_params()
+        # RationalQuadratic kernel is isotropic.
+        isotropic_kernels = (ExpSineSquared, RationalQuadratic)
+        if 'length_scale' in params and not isinstance(kernel, isotropic_kernels):
+            length_scale = params['length_scale']
+            if np.iterable(length_scale):
+                params['length_scale'] = length_scale[0]
+                params['length_scale_bounds'] = bounds
             else:
-                assert_equal(attr_value, attr_value_cloned)
-            if not isinstance(attr_value, Hashable):
-                # modifiable attributes must not be identical
-                assert_not_equal(id(attr_value), id(attr_value_cloned))
+                params['length_scale'] = [length_scale] * 2
+                params['length_scale_bounds'] = bounds * 2
+            kernel_cloned.set_params(**params)
+            kernel_cloned_clone = clone(kernel_cloned)
+            assert_equal(kernel_cloned_clone.get_params(),
+                         kernel_cloned.get_params())
+            assert_not_equal(id(kernel_cloned_clone), id(kernel_cloned))
+            yield check_hyperparameters_equal, kernel_cloned, kernel_cloned_clone
 
 
 def test_matern_kernel():
@@ -285,3 +305,10 @@ def test_set_get_params():
                 kernel.set_params(**{hyperparameter.name: value})
                 assert_almost_equal(np.exp(kernel.theta[index]), value)
                 index += 1
+
+
+def test_repr_kernels():
+    """Smoke-test for repr in kernels."""
+
+    for kernel in kernels:
+        repr(kernel)
diff --git a/sklearn/grid_search.py b/sklearn/grid_search.py
index 1072408e02c6..292062b15454 100644
--- a/sklearn/grid_search.py
+++ b/sklearn/grid_search.py
@@ -326,17 +326,18 @@ def _check_param_grid(param_grid):
         param_grid = [param_grid]
 
     for p in param_grid:
-        for v in p.values():
+        for name, v in p.items():
             if isinstance(v, np.ndarray) and v.ndim > 1:
                 raise ValueError("Parameter array should be one-dimensional.")
 
             check = [isinstance(v, k) for k in (list, tuple, np.ndarray)]
             if True not in check:
-                raise ValueError("Parameter values should be a list.")
+                raise ValueError("Parameter values for parameter ({0}) need "
+                                 "to be a sequence.".format(name))
 
             if len(v) == 0:
-                raise ValueError("Parameter values should be a non-empty "
-                                 "list.")
+                raise ValueError("Parameter values for parameter ({0}) need "
+                                 "to be a non-empty sequence.".format(name))
 
 
 class _CVScoreTuple (namedtuple('_CVScoreTuple',
diff --git a/sklearn/linear_model/cd_fast.pyx b/sklearn/linear_model/cd_fast.pyx
index 847ef1e98cb4..63916c61f7b3 100644
--- a/sklearn/linear_model/cd_fast.pyx
+++ b/sklearn/linear_model/cd_fast.pyx
@@ -13,10 +13,16 @@ import numpy.linalg as linalg
 
 cimport cython
 from cpython cimport bool
+from cython cimport floating
 import warnings
 
 ctypedef np.float64_t DOUBLE
 ctypedef np.uint32_t UINT32_t
+ctypedef floating (*DOT)(int N, floating *X, int incX, floating *Y,
+                         int incY) nogil
+ctypedef void (*AXPY)(int N, floating alpha, floating *X, int incX,
+                      floating *Y, int incY) nogil
+ctypedef floating (*ASUM)(int N, floating *X, int incX) nogil
 
 np.import_array()
 
@@ -42,13 +48,13 @@ cdef inline UINT32_t rand_int(UINT32_t end, UINT32_t* random_state) nogil:
     return our_rand_r(random_state) % end
 
 
-cdef inline double fmax(double x, double y) nogil:
+cdef inline floating fmax(floating x, floating y) nogil:
     if x > y:
         return x
     return y
 
 
-cdef inline double fsign(double f) nogil:
+cdef inline floating fsign(floating f) nogil:
     if f == 0:
         return 0
     elif f > 0:
@@ -57,11 +63,11 @@ cdef inline double fsign(double f) nogil:
         return -1.0
 
 
-cdef double abs_max(int n, double* a) nogil:
+cdef floating abs_max(int n, floating* a) nogil:
     """np.max(np.abs(a))"""
     cdef int i
-    cdef double m = fabs(a[0])
-    cdef double d
+    cdef floating m = fabs(a[0])
+    cdef floating d
     for i in range(1, n):
         d = fabs(a[i])
         if d > m:
@@ -69,11 +75,11 @@ cdef double abs_max(int n, double* a) nogil:
     return m
 
 
-cdef double max(int n, double* a) nogil:
+cdef floating max(int n, floating* a) nogil:
     """np.max(a)"""
     cdef int i
-    cdef double m = a[0]
-    cdef double d
+    cdef floating m = a[0]
+    cdef floating d
     for i in range(1, n):
         d = a[i]
         if d > m:
@@ -81,11 +87,11 @@ cdef double max(int n, double* a) nogil:
     return m
 
 
-cdef double diff_abs_max(int n, double* a, double* b) nogil:
+cdef floating diff_abs_max(int n, floating* a, floating* b) nogil:
     """np.max(np.abs(a - b))"""
     cdef int i
-    cdef double m = fabs(a[0] - b[0])
-    cdef double d
+    cdef floating m = fabs(a[0] - b[0])
+    cdef floating d
     for i in range(1, n):
         d = fabs(a[i] - b[i])
         if d > m:
@@ -105,29 +111,35 @@ cdef extern from "cblas.h":
 
     void daxpy "cblas_daxpy"(int N, double alpha, double *X, int incX,
                              double *Y, int incY) nogil
+    void saxpy "cblas_saxpy"(int N, float alpha, float *X, int incX,
+                             float *Y, int incY) nogil
     double ddot "cblas_ddot"(int N, double *X, int incX, double *Y, int incY
                              ) nogil
+    float sdot "cblas_sdot"(int N, float *X, int incX, float *Y,
+                            int incY) nogil
     double dasum "cblas_dasum"(int N, double *X, int incX) nogil
+    float sasum "cblas_sasum"(int N, float *X, int incX) nogil
     void dger "cblas_dger"(CBLAS_ORDER Order, int M, int N, double alpha,
-                double *X, int incX, double *Y, int incY, double *A, int lda) nogil
-    void dgemv "cblas_dgemv"(CBLAS_ORDER Order,
-                      CBLAS_TRANSPOSE TransA, int M, int N,
-                      double alpha, double *A, int lda,
-                      double *X, int incX, double beta,
-                      double *Y, int incY) nogil
+                           double *X, int incX, double *Y, int incY,
+                           double *A, int lda) nogil
+    void dgemv "cblas_dgemv"(CBLAS_ORDER Order, CBLAS_TRANSPOSE TransA,
+                             int M, int N, double alpha, double *A, int lda,
+                             double *X, int incX, double beta,
+                             double *Y, int incY) nogil
     double dnrm2 "cblas_dnrm2"(int N, double *X, int incX) nogil
-    void dcopy "cblas_dcopy"(int N, double *X, int incX, double *Y, int incY) nogil
+    void dcopy "cblas_dcopy"(int N, double *X, int incX, double *Y,
+                             int incY) nogil
     void dscal "cblas_dscal"(int N, double alpha, double *X, int incX) nogil
 
 
 @cython.boundscheck(False)
 @cython.wraparound(False)
 @cython.cdivision(True)
-def enet_coordinate_descent(np.ndarray[DOUBLE, ndim=1] w,
-                            double alpha, double beta,
-                            np.ndarray[DOUBLE, ndim=2, mode='fortran'] X,
-                            np.ndarray[DOUBLE, ndim=1, mode='c'] y,
-                            int max_iter, double tol,
+def enet_coordinate_descent(np.ndarray[floating, ndim=1] w,
+                            floating alpha, floating beta,
+                            np.ndarray[floating, ndim=2, mode='fortran'] X,
+                            np.ndarray[floating, ndim=1, mode='c'] y,
+                            int max_iter, floating tol,
                             object rng, bint random=0, bint positive=0):
     """Cython version of the coordinate descent algorithm
         for Elastic-Net regression
@@ -138,31 +150,49 @@ def enet_coordinate_descent(np.ndarray[DOUBLE, ndim=1] w,
 
     """
 
+    # fused types version of BLAS functions
+    cdef DOT dot
+    cdef AXPY axpy
+    cdef ASUM asum
+
+    if floating is float:
+        dtype = np.float32
+        dot = sdot
+        axpy = saxpy
+        asum = sasum
+    else:
+        dtype = np.float64
+        dot = ddot
+        axpy = daxpy
+        asum = dasum
+
     # get the data information into easy vars
     cdef unsigned int n_samples = X.shape[0]
     cdef unsigned int n_features = X.shape[1]
 
     # get the number of tasks indirectly, using strides
-    cdef unsigned int n_tasks = y.strides[0] / sizeof(DOUBLE)
+    cdef unsigned int n_tasks = y.strides[0] / sizeof(floating)
 
     # compute norms of the columns of X
-    cdef np.ndarray[DOUBLE, ndim=1] norm_cols_X = (X**2).sum(axis=0)
+    cdef np.ndarray[floating, ndim=1] norm_cols_X = (X**2).sum(axis=0)
 
     # initial value of the residuals
-    cdef np.ndarray[DOUBLE, ndim=1] R = np.empty(n_samples)
-
-    cdef np.ndarray[DOUBLE, ndim=1] XtA = np.empty(n_features)
-    cdef double tmp
-    cdef double w_ii
-    cdef double d_w_max
-    cdef double w_max
-    cdef double d_w_ii
-    cdef double gap = tol + 1.0
-    cdef double d_w_tol = tol
-    cdef double dual_norm_XtA
-    cdef double R_norm2
-    cdef double w_norm2
-    cdef double l1_norm
+    cdef np.ndarray[floating, ndim=1] R = np.empty(n_samples, dtype=dtype)
+    cdef np.ndarray[floating, ndim=1] XtA = np.empty(n_features, dtype=dtype)
+
+    cdef floating tmp
+    cdef floating w_ii
+    cdef floating d_w_max
+    cdef floating w_max
+    cdef floating d_w_ii
+    cdef floating gap = tol + 1.0
+    cdef floating d_w_tol = tol
+    cdef floating dual_norm_XtA
+    cdef floating R_norm2
+    cdef floating w_norm2
+    cdef floating l1_norm
+    cdef floating const
+    cdef floating A_norm2
     cdef unsigned int ii
     cdef unsigned int i
     cdef unsigned int n_iter = 0
@@ -170,6 +200,12 @@ def enet_coordinate_descent(np.ndarray[DOUBLE, ndim=1] w,
     cdef UINT32_t rand_r_state_seed = rng.randint(0, RAND_R_MAX)
     cdef UINT32_t* rand_r_state = &rand_r_state_seed
 
+    cdef floating *X_data = <floating*> X.data
+    cdef floating *y_data = <floating*> y.data
+    cdef floating *w_data = <floating*> w.data
+    cdef floating *R_data = <floating*> R.data
+    cdef floating *XtA_data = <floating*> XtA.data
+
     if alpha == 0:
         warnings.warn("Coordinate descent with alpha=0 may lead to unexpected"
             " results and is discouraged.")
@@ -177,13 +213,10 @@ def enet_coordinate_descent(np.ndarray[DOUBLE, ndim=1] w,
     with nogil:
         # R = y - np.dot(X, w)
         for i in range(n_samples):
-            R[i] = y[i] - ddot(n_features,
-                               <DOUBLE*>(X.data + i * sizeof(DOUBLE)),
-                               n_samples, <DOUBLE*>w.data, 1)
+            R[i] = y[i] - dot(n_features, &X_data[i], n_samples, w_data, 1)
 
         # tol *= np.dot(y, y)
-        tol *= ddot(n_samples, <DOUBLE*>y.data, n_tasks,
-                    <DOUBLE*>y.data, n_tasks)
+        tol *= dot(n_samples, y_data, n_tasks, y_data, n_tasks)
 
         for n_iter in range(max_iter):
             w_max = 0.0
@@ -201,14 +234,11 @@ def enet_coordinate_descent(np.ndarray[DOUBLE, ndim=1] w,
 
                 if w_ii != 0.0:
                     # R += w_ii * X[:,ii]
-                    daxpy(n_samples, w_ii,
-                          <DOUBLE*>(X.data + ii * n_samples * sizeof(DOUBLE)),
-                          1, <DOUBLE*>R.data, 1)
+                    axpy(n_samples, w_ii, &X_data[ii * n_samples], 1,
+                         R_data, 1)
 
                 # tmp = (X[:,ii]*R).sum()
-                tmp = ddot(n_samples,
-                           <DOUBLE*>(X.data + ii * n_samples * sizeof(DOUBLE)),
-                           1, <DOUBLE*>R.data, 1)
+                tmp = dot(n_samples, &X_data[ii * n_samples], 1, R_data, 1)
 
                 if positive and tmp < 0:
                     w[ii] = 0.0
@@ -218,9 +248,8 @@ def enet_coordinate_descent(np.ndarray[DOUBLE, ndim=1] w,
 
                 if w[ii] != 0.0:
                     # R -=  w[ii] * X[:,ii] # Update residual
-                    daxpy(n_samples, -w[ii],
-                          <DOUBLE*>(X.data + ii * n_samples * sizeof(DOUBLE)),
-                          1, <DOUBLE*>R.data, 1)
+                    axpy(n_samples, -w[ii], &X_data[ii * n_samples], 1,
+                         R_data, 1)
 
                 # update the maximum absolute coefficient update
                 d_w_ii = fabs(w[ii] - w_ii)
@@ -230,32 +259,28 @@ def enet_coordinate_descent(np.ndarray[DOUBLE, ndim=1] w,
                 if fabs(w[ii]) > w_max:
                     w_max = fabs(w[ii])
 
-            if (w_max == 0.0
-                    or d_w_max / w_max < d_w_tol
-                    or n_iter == max_iter - 1):
+            if (w_max == 0.0 or
+                d_w_max / w_max < d_w_tol or
+                n_iter == max_iter - 1):
                 # the biggest coordinate update of this iteration was smaller
                 # than the tolerance: check the duality gap as ultimate
                 # stopping criterion
 
                 # XtA = np.dot(X.T, R) - beta * w
                 for i in range(n_features):
-                    XtA[i] = ddot(
-                        n_samples,
-                        <DOUBLE*>(X.data + i * n_samples *sizeof(DOUBLE)),
-                        1, <DOUBLE*>R.data, 1) - beta * w[i]
+                    XtA[i] = dot(n_samples, &X_data[i * n_samples],
+                                 1, R_data, 1) - beta * w[i]
 
                 if positive:
-                    dual_norm_XtA = max(n_features, <DOUBLE*>XtA.data)
+                    dual_norm_XtA = max(n_features, XtA_data)
                 else:
-                    dual_norm_XtA = abs_max(n_features, <DOUBLE*>XtA.data)
+                    dual_norm_XtA = abs_max(n_features, XtA_data)
 
                 # R_norm2 = np.dot(R, R)
-                R_norm2 = ddot(n_samples, <DOUBLE*>R.data, 1,
-                               <DOUBLE*>R.data, 1)
+                R_norm2 = dot(n_samples, R_data, 1, R_data, 1)
 
                 # w_norm2 = np.dot(w, w)
-                w_norm2 = ddot(n_features, <DOUBLE*>w.data, 1,
-                               <DOUBLE*>w.data, 1)
+                w_norm2 = dot(n_features, w_data, 1, w_data, 1)
 
                 if (dual_norm_XtA > alpha):
                     const = alpha / dual_norm_XtA
@@ -265,33 +290,30 @@ def enet_coordinate_descent(np.ndarray[DOUBLE, ndim=1] w,
                     const = 1.0
                     gap = R_norm2
 
-                l1_norm = dasum(n_features, <DOUBLE*>w.data, 1)
+                l1_norm = asum(n_features, w_data, 1)
 
                 # np.dot(R.T, y)
-                gap += (alpha * l1_norm - const * ddot(
-                            n_samples,
-                            <DOUBLE*>R.data, 1,
-                            <DOUBLE*>y.data, n_tasks)
+                gap += (alpha * l1_norm
+                        - const * dot(n_samples, R_data, 1, y_data, n_tasks)
                         + 0.5 * beta * (1 + const ** 2) * (w_norm2))
 
                 if gap < tol:
                     # return if we reached desired tolerance
                     break
-
     return w, gap, tol, n_iter + 1
 
 
 @cython.boundscheck(False)
 @cython.wraparound(False)
 @cython.cdivision(True)
-def sparse_enet_coordinate_descent(double[:] w,
-                            double alpha, double beta,
-                            np.ndarray[double, ndim=1, mode='c'] X_data,
+def sparse_enet_coordinate_descent(floating [:] w,
+                            floating alpha, floating beta,
+                            np.ndarray[floating, ndim=1, mode='c'] X_data,
                             np.ndarray[int, ndim=1, mode='c'] X_indices,
                             np.ndarray[int, ndim=1, mode='c'] X_indptr,
-                            np.ndarray[double, ndim=1] y,
-                            double[:] X_mean, int max_iter,
-                            double tol, object rng, bint random=0,
+                            np.ndarray[floating, ndim=1] y,
+                            floating[:] X_mean, int max_iter,
+                            floating tol, object rng, bint random=0,
                             bint positive=0):
     """Cython version of the coordinate descent algorithm for Elastic-Net
 
@@ -307,30 +329,54 @@ def sparse_enet_coordinate_descent(double[:] w,
 
     # compute norms of the columns of X
     cdef unsigned int ii
-    cdef double[:] norm_cols_X = np.zeros(n_features, np.float64)
+    cdef floating[:] norm_cols_X
 
     cdef unsigned int startptr = X_indptr[0]
     cdef unsigned int endptr
 
     # get the number of tasks indirectly, using strides
-    cdef unsigned int n_tasks = y.strides[0] / sizeof(DOUBLE)
+    cdef unsigned int n_tasks
 
     # initial value of the residuals
-    cdef double[:] R = y.copy()
+    cdef floating[:] R = y.copy()
 
-    cdef double[:] X_T_R = np.zeros(n_features)
-    cdef double[:] XtA = np.zeros(n_features)
+    cdef floating[:] X_T_R
+    cdef floating[:] XtA
 
-    cdef double tmp
-    cdef double w_ii
-    cdef double d_w_max
-    cdef double w_max
-    cdef double d_w_ii
-    cdef double X_mean_ii
-    cdef double R_sum = 0.0
-    cdef double normalize_sum
-    cdef double gap = tol + 1.0
-    cdef double d_w_tol = tol
+    # fused types version of BLAS functions
+    cdef DOT dot
+    cdef ASUM asum
+
+    if floating is float:
+        dtype = np.float32
+        n_tasks = y.strides[0] / sizeof(float)
+        dot = sdot
+        asum = sasum
+    else:
+        dtype = np.float64
+        n_tasks = y.strides[0] / sizeof(DOUBLE)
+        dot = ddot
+        asum = dasum
+
+    norm_cols_X = np.zeros(n_features, dtype=dtype)
+    X_T_R = np.zeros(n_features, dtype=dtype)
+    XtA = np.zeros(n_features, dtype=dtype)
+
+    cdef floating tmp
+    cdef floating w_ii
+    cdef floating d_w_max
+    cdef floating w_max
+    cdef floating d_w_ii
+    cdef floating X_mean_ii
+    cdef floating R_sum = 0.0
+    cdef floating R_norm2
+    cdef floating w_norm2
+    cdef floating A_norm2
+    cdef floating l1_norm
+    cdef floating normalize_sum
+    cdef floating gap = tol + 1.0
+    cdef floating d_w_tol = tol
+    cdef floating dual_norm_XtA
     cdef unsigned int jj
     cdef unsigned int n_iter = 0
     cdef unsigned int f_iter
@@ -363,7 +409,7 @@ def sparse_enet_coordinate_descent(double[:] w,
             startptr = endptr
 
         # tol *= np.dot(y, y)
-        tol *= ddot(n_samples, <DOUBLE*>&y[0], 1, <DOUBLE*>&y[0], 1)
+        tol *= dot(n_samples, &y[0], 1, &y[0], 1)
 
         for n_iter in range(max_iter):
 
@@ -451,10 +497,10 @@ def sparse_enet_coordinate_descent(double[:] w,
                     dual_norm_XtA = abs_max(n_features, &XtA[0])
 
                 # R_norm2 = np.dot(R, R)
-                R_norm2 = ddot(n_samples, <DOUBLE*>&R[0], 1, <DOUBLE*>&R[0], 1)
+                R_norm2 = dot(n_samples, &R[0], 1, &R[0], 1)
 
                 # w_norm2 = np.dot(w, w)
-                w_norm2 = ddot(n_features, <DOUBLE*>&w[0], 1, <DOUBLE*>&w[0], 1)
+                w_norm2 = dot(n_features, &w[0], 1, &w[0], 1)
                 if (dual_norm_XtA > alpha):
                     const = alpha / dual_norm_XtA
                     A_norm2 = R_norm2 * const**2
@@ -463,13 +509,12 @@ def sparse_enet_coordinate_descent(double[:] w,
                     const = 1.0
                     gap = R_norm2
 
-                l1_norm = dasum(n_features, <DOUBLE*>&w[0], 1)
+                l1_norm = asum(n_features, &w[0], 1)
 
-                # The expression inside ddot is equivalent to np.dot(R.T, y)
-                gap += (alpha * l1_norm - const * ddot(
+                gap += (alpha * l1_norm - const * dot(
                             n_samples,
-                            <DOUBLE*>&R[0], 1,
-                            <DOUBLE*>&y[0], n_tasks
+                            &R[0], 1,
+                            &y[0], n_tasks
                             )
                         + 0.5 * beta * (1 + const ** 2) * w_norm2)
 
@@ -483,11 +528,11 @@ def sparse_enet_coordinate_descent(double[:] w,
 @cython.boundscheck(False)
 @cython.wraparound(False)
 @cython.cdivision(True)
-def enet_coordinate_descent_gram(double[:] w, double alpha, double beta,
-                                 np.ndarray[double, ndim=2, mode='c'] Q,
-                                 np.ndarray[double, ndim=1, mode='c'] q,
-                                 np.ndarray[double, ndim=1] y,
-                                 int max_iter, double tol, object rng,
+def enet_coordinate_descent_gram(floating[:] w, floating alpha, floating beta,
+                                 np.ndarray[floating, ndim=2, mode='c'] Q,
+                                 np.ndarray[floating, ndim=1, mode='c'] q,
+                                 np.ndarray[floating, ndim=1] y,
+                                 int max_iter, floating tol, object rng,
                                  bint random=0, bint positive=0):
     """Cython version of the coordinate descent algorithm
         for Elastic-Net regression
@@ -501,34 +546,52 @@ def enet_coordinate_descent_gram(double[:] w, double alpha, double beta,
         q = X^T y
     """
 
+    # fused types version of BLAS functions
+    cdef DOT dot
+    cdef AXPY axpy
+    cdef ASUM asum
+
+    if floating is float:
+        dtype = np.float32
+        dot = sdot
+        axpy = saxpy
+        asum = sasum
+    else:
+        dtype = np.float64
+        dot = ddot
+        axpy = daxpy
+        asum = dasum
+
     # get the data information into easy vars
     cdef unsigned int n_samples = y.shape[0]
     cdef unsigned int n_features = Q.shape[0]
 
     # initial value "Q w" which will be kept of up to date in the iterations
-    cdef double[:] H = np.dot(Q, w)
-
-    cdef double[:] XtA = np.zeros(n_features)
-    cdef double tmp
-    cdef double w_ii
-    cdef double d_w_max
-    cdef double w_max
-    cdef double d_w_ii
-    cdef double gap = tol + 1.0
-    cdef double d_w_tol = tol
-    cdef double dual_norm_XtA
+    cdef floating[:] H = np.dot(Q, w)
+
+    cdef floating[:] XtA = np.zeros(n_features, dtype=dtype)
+    cdef floating tmp
+    cdef floating w_ii
+    cdef floating d_w_max
+    cdef floating w_max
+    cdef floating d_w_ii
+    cdef floating q_dot_w
+    cdef floating w_norm2
+    cdef floating gap = tol + 1.0
+    cdef floating d_w_tol = tol
+    cdef floating dual_norm_XtA
     cdef unsigned int ii
     cdef unsigned int n_iter = 0
     cdef unsigned int f_iter
     cdef UINT32_t rand_r_state_seed = rng.randint(0, RAND_R_MAX)
     cdef UINT32_t* rand_r_state = &rand_r_state_seed
 
-    cdef double y_norm2 = np.dot(y, y)
-    cdef double* w_ptr = <double*>&w[0]
-    cdef double* Q_ptr = &Q[0, 0]
-    cdef double* q_ptr = <double*>q.data
-    cdef double* H_ptr = &H[0]
-    cdef double* XtA_ptr = &XtA[0]
+    cdef floating y_norm2 = np.dot(y, y)
+    cdef floating* w_ptr = <floating*>&w[0]
+    cdef floating* Q_ptr = &Q[0, 0]
+    cdef floating* q_ptr = <floating*>q.data
+    cdef floating* H_ptr = &H[0]
+    cdef floating* XtA_ptr = &XtA[0]
     tol = tol * y_norm2
 
     if alpha == 0:
@@ -552,8 +615,8 @@ def enet_coordinate_descent_gram(double[:] w, double alpha, double beta,
 
                 if w_ii != 0.0:
                     # H -= w_ii * Q[ii]
-                    daxpy(n_features, -w_ii, Q_ptr + ii * n_features, 1,
-                          H_ptr, 1)
+                    axpy(n_features, -w_ii, Q_ptr + ii * n_features, 1,
+                         H_ptr, 1)
 
                 tmp = q[ii] - H[ii]
 
@@ -565,8 +628,8 @@ def enet_coordinate_descent_gram(double[:] w, double alpha, double beta,
 
                 if w[ii] != 0.0:
                     # H +=  w[ii] * Q[ii] # Update H = X.T X w
-                    daxpy(n_features, w[ii], Q_ptr + ii * n_features, 1,
-                          H_ptr, 1)
+                    axpy(n_features, w[ii], Q_ptr + ii * n_features, 1,
+                         H_ptr, 1)
 
                 # update the maximum absolute coefficient update
                 d_w_ii = fabs(w[ii] - w_ii)
@@ -582,7 +645,7 @@ def enet_coordinate_descent_gram(double[:] w, double alpha, double beta,
                 # criterion
 
                 # q_dot_w = np.dot(w, q)
-                q_dot_w = ddot(n_features, w_ptr, 1, q_ptr, 1)
+                q_dot_w = dot(n_features, w_ptr, 1, q_ptr, 1)
 
                 for ii in range(n_features):
                     XtA[ii] = q[ii] - H[ii] - beta * w[ii]
@@ -598,7 +661,7 @@ def enet_coordinate_descent_gram(double[:] w, double alpha, double beta,
                 R_norm2 = y_norm2 + tmp - 2.0 * q_dot_w
 
                 # w_norm2 = np.dot(w, w)
-                w_norm2 = ddot(n_features, &w[0], 1, &w[0], 1)
+                w_norm2 = dot(n_features, &w[0], 1, &w[0], 1)
 
                 if (dual_norm_XtA > alpha):
                     const = alpha / dual_norm_XtA
@@ -609,7 +672,7 @@ def enet_coordinate_descent_gram(double[:] w, double alpha, double beta,
                     gap = R_norm2
 
                 # The call to dasum is equivalent to the L1 norm of w
-                gap += (alpha * dasum(n_features, &w[0], 1) -
+                gap += (alpha * asum(n_features, &w[0], 1) -
                         const * y_norm2 +  const * q_dot_w +
                         0.5 * beta * (1 + const ** 2) * w_norm2)
 
diff --git a/sklearn/linear_model/coordinate_descent.py b/sklearn/linear_model/coordinate_descent.py
index 74c2b45f7ebe..5002a8af4440 100644
--- a/sklearn/linear_model/coordinate_descent.py
+++ b/sklearn/linear_model/coordinate_descent.py
@@ -65,14 +65,14 @@ def _alpha_grid(X, y, Xy=None, l1_ratio=1.0, fit_intercept=True,
         Whether to fit an intercept or not
 
     normalize : boolean, optional, default False
-        If True, the regressors X will be normalized before regression.
-        This parameter is ignored when `fit_intercept` is set to False.
+        If ``True``, the regressors X will be normalized before regression.
+        This parameter is ignored when ``fit_intercept`` is set to ``False``.
         When the regressors are normalized, note that this makes the
         hyperparameters learnt more robust and almost independent of the number
         of samples. The same property is not valid for standardized data.
         However, if you wish to standardize, please use
-        `preprocessing.StandardScaler` before calling `fit` on an estimator
-        with `normalize=False`.
+        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator
+        with ``normalize=False``.
 
     copy_X : boolean, optional, default True
         If ``True``, X will be copied; else, it may be overwritten.
@@ -372,16 +372,18 @@ def enet_path(X, y, l1_ratio=0.5, eps=1e-3, n_alphas=100, alphas=None,
     ElasticNet
     ElasticNetCV
     """
-    # We expect X and y to be already float64 Fortran ordered when bypassing
+    # We expect X and y to be already Fortran ordered when bypassing
     # checks
     if check_input:
-        X = check_array(X, 'csc', dtype=np.float64, order='F', copy=copy_X)
-        y = check_array(y, 'csc', dtype=np.float64, order='F', copy=False,
+        X = check_array(X, 'csc', dtype=[np.float64, np.float32],
+                        order='F', copy=copy_X)
+        y = check_array(y, 'csc', dtype=X.dtype.type, order='F', copy=False,
                         ensure_2d=False)
         if Xy is not None:
             # Xy should be a 1d contiguous array or a 2D C ordered array
-            Xy = check_array(Xy, dtype=np.float64, order='C', copy=False,
+            Xy = check_array(Xy, dtype=X.dtype.type, order='C', copy=False,
                              ensure_2d=False)
+
     n_samples, n_features = X.shape
 
     multi_output = False
@@ -395,8 +397,9 @@ def enet_path(X, y, l1_ratio=0.5, eps=1e-3, n_alphas=100, alphas=None,
             # As sparse matrices are not actually centered we need this
             # to be passed to the CD solver.
             X_sparse_scaling = params['X_offset'] / params['X_scale']
+            X_sparse_scaling = np.asarray(X_sparse_scaling, dtype=X.dtype)
         else:
-            X_sparse_scaling = np.zeros(n_features)
+            X_sparse_scaling = np.zeros(n_features, dtype=X.dtype)
 
     # X should be normalized and fit already if function is called
     # from ElasticNet.fit
@@ -426,15 +429,15 @@ def enet_path(X, y, l1_ratio=0.5, eps=1e-3, n_alphas=100, alphas=None,
     random = (selection == 'random')
 
     if not multi_output:
-        coefs = np.empty((n_features, n_alphas), dtype=np.float64)
+        coefs = np.empty((n_features, n_alphas), dtype=X.dtype)
     else:
         coefs = np.empty((n_outputs, n_features, n_alphas),
-                         dtype=np.float64)
+                         dtype=X.dtype)
 
     if coef_init is None:
-        coef_ = np.asfortranarray(np.zeros(coefs.shape[:-1]))
+        coef_ = np.asfortranarray(np.zeros(coefs.shape[:-1], dtype=X.dtype))
     else:
-        coef_ = np.asfortranarray(coef_init)
+        coef_ = np.asfortranarray(coef_init, dtype=X.dtype)
 
     for i, alpha in enumerate(alphas):
         l1_reg = alpha * l1_ratio * n_samples
@@ -470,7 +473,9 @@ def enet_path(X, y, l1_ratio=0.5, eps=1e-3, n_alphas=100, alphas=None,
         if dual_gap_ > eps_:
             warnings.warn('Objective did not converge.' +
                           ' You might want' +
-                          ' to increase the number of iterations',
+                          ' to increase the number of iterations.' +
+                          ' Fitting data with very small alpha' +
+                          ' may cause precision problems.',
                           ConvergenceWarning)
 
         if verbose:
@@ -517,14 +522,13 @@ class ElasticNet(LinearModel, RegressorMixin):
 
     Parameters
     ----------
-    alpha : float
-        Constant that multiplies the penalty terms. Defaults to 1.0
+    alpha : float, optional
+        Constant that multiplies the penalty terms. Defaults to 1.0.
         See the notes for the exact mathematical meaning of this
-        parameter.
-        ``alpha = 0`` is equivalent to an ordinary least square, solved
+        parameter.``alpha = 0`` is equivalent to an ordinary least square, solved
         by the :class:`LinearRegression` object. For numerical
-        reasons, using ``alpha = 0`` with the Lasso object is not advised
-        and you should prefer the LinearRegression object.
+        reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.
+        Given this, you should use the :class:`LinearRegression` object.
 
     l1_ratio : float
         The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For
@@ -537,14 +541,14 @@ class ElasticNet(LinearModel, RegressorMixin):
         data is assumed to be already centered.
 
     normalize : boolean, optional, default False
-        If True, the regressors X will be normalized before regression.
-        This parameter is ignored when `fit_intercept` is set to False.
+        If ``True``, the regressors X will be normalized before regression.
+        This parameter is ignored when ``fit_intercept`` is set to ``False``.
         When the regressors are normalized, note that this makes the
         hyperparameters learnt more robust and almost independent of the number
         of samples. The same property is not valid for standardized data.
         However, if you wish to standardize, please use
-        `preprocessing.StandardScaler` before calling `fit` on an estimator
-        with `normalize=False`.
+        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator
+        with ``normalize=False``.
 
     precompute : True | False | array-like
         Whether to use a precomputed Gram matrix to speed up
@@ -664,16 +668,16 @@ def fit(self, X, y, check_input=True):
             raise ValueError('precompute should be one of True, False or'
                              ' array-like. Got %r' % self.precompute)
 
-        # We expect X and y to be already float64 Fortran ordered arrays
+        # We expect X and y to be float64 or float32 Fortran ordered arrays
         # when bypassing checks
         if check_input:
-            y = np.asarray(y, dtype=np.float64)
-            X, y = check_X_y(X, y, accept_sparse='csc', dtype=np.float64,
-                             order='F',
+            X, y = check_X_y(X, y, accept_sparse='csc',
+                             order='F', dtype=[np.float64, np.float32],
                              copy=self.copy_X and self.fit_intercept,
                              multi_output=True, y_numeric=True)
-            y = check_array(y, dtype=np.float64, order='F', copy=False,
+            y = check_array(y, order='F', copy=False, dtype=X.dtype.type,
                             ensure_2d=False)
+
         X, y, X_offset, y_offset, X_scale, precompute, Xy = \
             _pre_fit(X, y, None, self.precompute, self.normalize,
                      self.fit_intercept, copy=False)
@@ -689,14 +693,14 @@ def fit(self, X, y, check_input=True):
             raise ValueError("selection should be either random or cyclic.")
 
         if not self.warm_start or self.coef_ is None:
-            coef_ = np.zeros((n_targets, n_features), dtype=np.float64,
+            coef_ = np.zeros((n_targets, n_features), dtype=X.dtype,
                              order='F')
         else:
             coef_ = self.coef_
             if coef_.ndim == 1:
                 coef_ = coef_[np.newaxis, :]
 
-        dual_gaps_ = np.zeros(n_targets, dtype=np.float64)
+        dual_gaps_ = np.zeros(n_targets, dtype=X.dtype)
         self.n_iter_ = []
 
         for k in xrange(n_targets):
@@ -726,12 +730,15 @@ def fit(self, X, y, check_input=True):
         self.coef_, self.dual_gap_ = map(np.squeeze, [coef_, dual_gaps_])
         self._set_intercept(X_offset, y_offset, X_scale)
 
+        # workaround since _set_intercept will cast self.coef_ into float64
+        self.coef_ = np.asarray(self.coef_, dtype=X.dtype)
+
         # return self for chaining fit and predict calls
         return self
 
     @property
     def sparse_coef_(self):
-        """ sparse representation of the fitted coef """
+        """ sparse representation of the fitted ``coef_`` """
         return sparse.csr_matrix(self.coef_)
 
     @deprecated(" and will be removed in 0.19")
@@ -763,9 +770,8 @@ def _decision_function(self, X):
         """
         check_is_fitted(self, 'n_iter_')
         if sparse.isspmatrix(X):
-            return np.ravel(safe_sparse_dot(self.coef_, X.T,
-                                            dense_output=True) +
-                            self.intercept_)
+            return safe_sparse_dot(X, self.coef_.T,
+                                   dense_output=True) + self.intercept_
         else:
             return super(ElasticNet, self)._decision_function(X)
 
@@ -791,8 +797,8 @@ class Lasso(ElasticNet):
         Constant that multiplies the L1 term. Defaults to 1.0.
         ``alpha = 0`` is equivalent to an ordinary least square, solved
         by the :class:`LinearRegression` object. For numerical
-        reasons, using ``alpha = 0`` is with the Lasso object is not advised
-        and you should prefer the LinearRegression object.
+        reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.
+        Given this, you should use the :class:`LinearRegression` object.
 
     fit_intercept : boolean
         whether to calculate the intercept for this model. If set
@@ -800,14 +806,14 @@ class Lasso(ElasticNet):
         (e.g. data is expected to be already centered).
 
     normalize : boolean, optional, default False
-        If True, the regressors X will be normalized before regression.
-        This parameter is ignored when `fit_intercept` is set to False.
+        If ``True``, the regressors X will be normalized before regression.
+        This parameter is ignored when ``fit_intercept`` is set to ``False``.
         When the regressors are normalized, note that this makes the
         hyperparameters learnt more robust and almost independent of the number
         of samples. The same property is not valid for standardized data.
         However, if you wish to standardize, please use
-        `preprocessing.StandardScaler` before calling `fit` on an estimator
-        with `normalize=False`.
+        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator
+        with ``normalize=False``.
 
     copy_X : boolean, optional, default True
         If ``True``, X will be copied; else, it may be overwritten.
@@ -1286,14 +1292,14 @@ class LassoCV(LinearModelCV, RegressorMixin):
         (e.g. data is expected to be already centered).
 
     normalize : boolean, optional, default False
-        If True, the regressors X will be normalized before regression.
-        This parameter is ignored when `fit_intercept` is set to False.
+        If ``True``, the regressors X will be normalized before regression.
+        This parameter is ignored when ``fit_intercept`` is set to ``False``.
         When the regressors are normalized, note that this makes the
         hyperparameters learnt more robust and almost independent of the number
         of samples. The same property is not valid for standardized data.
         However, if you wish to standardize, please use
-        `preprocessing.StandardScaler` before calling `fit` on an estimator
-        with `normalize=False`.
+        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator
+        with ``normalize=False``.
 
     copy_X : boolean, optional, default True
         If ``True``, X will be copied; else, it may be overwritten.
@@ -1440,14 +1446,14 @@ class ElasticNetCV(LinearModelCV, RegressorMixin):
         (e.g. data is expected to be already centered).
 
     normalize : boolean, optional, default False
-        If True, the regressors X will be normalized before regression.
-        This parameter is ignored when `fit_intercept` is set to False.
+        If ``True``, the regressors X will be normalized before regression.
+        This parameter is ignored when ``fit_intercept`` is set to ``False``.
         When the regressors are normalized, note that this makes the
         hyperparameters learnt more robust and almost independent of the number
         of samples. The same property is not valid for standardized data.
         However, if you wish to standardize, please use
-        `preprocessing.StandardScaler` before calling `fit` on an estimator
-        with `normalize=False`.
+        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator
+        with ``normalize=False``.
 
     copy_X : boolean, optional, default True
         If ``True``, X will be copied; else, it may be overwritten.
@@ -1572,14 +1578,14 @@ class MultiTaskElasticNet(Lasso):
         (e.g. data is expected to be already centered).
 
     normalize : boolean, optional, default False
-        If True, the regressors X will be normalized before regression.
-        This parameter is ignored when `fit_intercept` is set to False.
+        If ``True``, the regressors X will be normalized before regression.
+        This parameter is ignored when ``fit_intercept`` is set to ``False``.
         When the regressors are normalized, note that this makes the
         hyperparameters learnt more robust and almost independent of the number
         of samples. The same property is not valid for standardized data.
         However, if you wish to standardize, please use
-        `preprocessing.StandardScaler` before calling `fit` on an estimator
-        with `normalize=False`.
+        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator
+        with ``normalize=False``.
 
     copy_X : boolean, optional, default True
         If ``True``, X will be copied; else, it may be overwritten.
@@ -1758,14 +1764,14 @@ class MultiTaskLasso(MultiTaskElasticNet):
         (e.g. data is expected to be already centered).
 
     normalize : boolean, optional, default False
-        If True, the regressors X will be normalized before regression.
-        This parameter is ignored when `fit_intercept` is set to False.
+        If ``True``, the regressors X will be normalized before regression.
+        This parameter is ignored when ``fit_intercept`` is set to ``False``.
         When the regressors are normalized, note that this makes the
         hyperparameters learnt more robust and almost independent of the number
         of samples. The same property is not valid for standardized data.
         However, if you wish to standardize, please use
-        `preprocessing.StandardScaler` before calling `fit` on an estimator
-        with `normalize=False`.
+        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator
+        with ``normalize=False``.
 
     copy_X : boolean, optional, default True
         If ``True``, X will be copied; else, it may be overwritten.
@@ -1895,14 +1901,14 @@ class MultiTaskElasticNetCV(LinearModelCV, RegressorMixin):
         (e.g. data is expected to be already centered).
 
     normalize : boolean, optional, default False
-        If True, the regressors X will be normalized before regression.
-        This parameter is ignored when `fit_intercept` is set to False.
+        If ``True``, the regressors X will be normalized before regression.
+        This parameter is ignored when ``fit_intercept`` is set to ``False``.
         When the regressors are normalized, note that this makes the
         hyperparameters learnt more robust and almost independent of the number
         of samples. The same property is not valid for standardized data.
         However, if you wish to standardize, please use
-        `preprocessing.StandardScaler` before calling `fit` on an estimator
-        with `normalize=False`.
+        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator
+        with ``normalize=False``.
 
     copy_X : boolean, optional, default True
         If ``True``, X will be copied; else, it may be overwritten.
@@ -2060,14 +2066,14 @@ class MultiTaskLassoCV(LinearModelCV, RegressorMixin):
         (e.g. data is expected to be already centered).
 
     normalize : boolean, optional, default False
-        If True, the regressors X will be normalized before regression.
-        This parameter is ignored when `fit_intercept` is set to False.
+        If ``True``, the regressors X will be normalized before regression.
+        This parameter is ignored when ``fit_intercept`` is set to ``False``.
         When the regressors are normalized, note that this makes the
         hyperparameters learnt more robust and almost independent of the number
         of samples. The same property is not valid for standardized data.
         However, if you wish to standardize, please use
-        `preprocessing.StandardScaler` before calling `fit` on an estimator
-        with `normalize=False`.
+        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator
+        with ``normalize=False``.
 
     copy_X : boolean, optional, default True
         If ``True``, X will be copied; else, it may be overwritten.
diff --git a/sklearn/linear_model/huber.py b/sklearn/linear_model/huber.py
index 28e318212d28..66b8478ab72e 100644
--- a/sklearn/linear_model/huber.py
+++ b/sklearn/linear_model/huber.py
@@ -245,12 +245,15 @@ def fit(self, X, y, sample_weight=None):
                 parameters = np.zeros(X.shape[1] + 2)
             else:
                 parameters = np.zeros(X.shape[1] + 1)
+            # Make sure to initialize the scale parameter to a strictly
+            # positive value:
+            parameters[-1] = 1
 
         # Sigma or the scale factor should be non-negative.
         # Setting it to be zero might cause undefined bounds hence we set it
         # to a value close to zero.
         bounds = np.tile([-np.inf, np.inf], (parameters.shape[0], 1))
-        bounds[-1][0] = 1e-12
+        bounds[-1][0] = np.finfo(np.float64).eps * 10
 
         # Type Error caused in old versions of SciPy because of no
         # maxiter argument ( <= 0.9).
@@ -258,14 +261,17 @@ def fit(self, X, y, sample_weight=None):
             parameters, f, dict_ = optimize.fmin_l_bfgs_b(
                 _huber_loss_and_gradient, parameters,
                 args=(X, y, self.epsilon, self.alpha, sample_weight),
-                maxiter=self.max_iter, tol=self.tol, bounds=bounds,
+                maxiter=self.max_iter, pgtol=self.tol, bounds=bounds,
                 iprint=0)
         except TypeError:
             parameters, f, dict_ = optimize.fmin_l_bfgs_b(
                 _huber_loss_and_gradient, parameters,
                 args=(X, y, self.epsilon, self.alpha, sample_weight),
                 bounds=bounds)
-
+        if dict_['warnflag'] == 2:
+            raise ValueError("HuberRegressor convergence failed:"
+                             " l-BFGS-b solver terminated with %s"
+                             % dict_['task'].decode('ascii'))
         self.n_iter_ = dict_.get('nit', None)
         self.scale_ = parameters[-1]
         if self.fit_intercept:
diff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py
index f1b730e38ce8..007bdedd72cb 100644
--- a/sklearn/linear_model/logistic.py
+++ b/sklearn/linear_model/logistic.py
@@ -1142,7 +1142,7 @@ def fit(self, X, y, sample_weight=None):
         Parameters
         ----------
         X : {array-like, sparse matrix}, shape (n_samples, n_features)
-            Training vector, where n_samples in the number of samples and
+            Training vector, where n_samples is the number of samples and
             n_features is the number of features.
 
         y : array-like, shape (n_samples,)
@@ -1525,7 +1525,7 @@ def fit(self, X, y, sample_weight=None):
         Parameters
         ----------
         X : {array-like, sparse matrix}, shape (n_samples, n_features)
-            Training vector, where n_samples in the number of samples and
+            Training vector, where n_samples is the number of samples and
             n_features is the number of features.
 
         y : array-like, shape (n_samples,)
diff --git a/sklearn/linear_model/stochastic_gradient.py b/sklearn/linear_model/stochastic_gradient.py
index f6a95b2ea3eb..232ca90a77a5 100644
--- a/sklearn/linear_model/stochastic_gradient.py
+++ b/sklearn/linear_model/stochastic_gradient.py
@@ -635,9 +635,11 @@ class SGDClassifier(BaseSGDClassifier, _LearntSelectorMixin):
 
     learning_rate : string, optional
         The learning rate schedule:
-        constant: eta = eta0
-        optimal: eta = 1.0 / (alpha * (t + t0)) [default]
-        invscaling: eta = eta0 / pow(t, power_t)
+
+        - 'constant': eta = eta0
+        - 'optimal': eta = 1.0 / (alpha * (t + t0)) [default]
+        - 'invscaling': eta = eta0 / pow(t, power_t)
+
         where t0 is chosen by a heuristic proposed by Leon Bottou.
 
     eta0 : double
@@ -666,7 +668,8 @@ class SGDClassifier(BaseSGDClassifier, _LearntSelectorMixin):
         When set to True, computes the averaged SGD weights and stores the
         result in the ``coef_`` attribute. If set to an int greater than 1,
         averaging will begin once the total number of samples seen reaches
-        average. So average=10 will begin averaging after seeing 10 samples.
+        average. So ``average=10`` will begin averaging after seeing 10
+        samples.
 
     Attributes
     ----------
@@ -731,7 +734,10 @@ def predict_proba(self):
         Elkan.
 
         Binary probability estimates for loss="modified_huber" are given by
-        (clip(decision_function(X), -1, 1) + 1) / 2.
+        (clip(decision_function(X), -1, 1) + 1) / 2. For other loss functions
+        it is necessary to perform proper probability calibration by wrapping
+        the classifier with
+        :class:`sklearn.calibration.CalibratedClassifierCV` instead.
 
         Parameters
         ----------
@@ -1162,10 +1168,13 @@ class SGDRegressor(BaseSGDRegressor, _LearntSelectorMixin):
         and the correct label are ignored if they are less than this threshold.
 
     learning_rate : string, optional
-        The learning rate:
-        constant: eta = eta0
-        optimal: eta = 1.0/(alpha * t)
-        invscaling: eta = eta0 / pow(t, power_t) [default]
+        The learning rate schedule:
+
+        - 'constant': eta = eta0
+        - 'optimal': eta = 1.0 / (alpha * (t + t0)) [default]
+        - 'invscaling': eta = eta0 / pow(t, power_t)
+
+        where t0 is chosen by a heuristic proposed by Leon Bottou.
 
     eta0 : double, optional
         The initial learning rate [default 0.01].
@@ -1181,7 +1190,7 @@ class SGDRegressor(BaseSGDRegressor, _LearntSelectorMixin):
         When set to True, computes the averaged SGD weights and stores the
         result in the ``coef_`` attribute. If set to an int greater than 1,
         averaging will begin once the total number of samples seen reaches
-        average. So ``average=10 will`` begin averaging after seeing 10
+        average. So ``average=10`` will begin averaging after seeing 10
         samples.
 
     Attributes
diff --git a/sklearn/linear_model/tests/test_coordinate_descent.py b/sklearn/linear_model/tests/test_coordinate_descent.py
index 918180ce1891..41d687b6b383 100644
--- a/sklearn/linear_model/tests/test_coordinate_descent.py
+++ b/sklearn/linear_model/tests/test_coordinate_descent.py
@@ -670,3 +670,45 @@ def test_lasso_non_float_y():
         clf_float = model(fit_intercept=False)
         clf_float.fit(X, y_float)
         assert_array_equal(clf.coef_, clf_float.coef_)
+
+
+def test_enet_float_precision():
+    # Generate dataset
+    X, y, X_test, y_test = build_dataset(n_samples=20, n_features=10)
+    # Here we have a small number of iterations, and thus the
+    # ElasticNet might not converge. This is to speed up tests
+
+    for normalize in [True, False]:
+        for fit_intercept in [True, False]:
+            coef = {}
+            intercept = {}
+            for dtype in [np.float64, np.float32]:
+                clf = ElasticNet(alpha=0.5, max_iter=100, precompute=False,
+                                 fit_intercept=fit_intercept,
+                                 normalize=normalize)
+
+                X = dtype(X)
+                y = dtype(y)
+                ignore_warnings(clf.fit)(X, y)
+
+                coef[dtype] = clf.coef_
+                intercept[dtype] = clf.intercept_
+
+                assert_equal(clf.coef_.dtype, dtype)
+
+                # test precompute Gram array
+                Gram = X.T.dot(X)
+                clf_precompute = ElasticNet(alpha=0.5, max_iter=100,
+                                            precompute=Gram,
+                                            fit_intercept=fit_intercept,
+                                            normalize=normalize)
+                ignore_warnings(clf_precompute.fit)(X, y)
+                assert_array_almost_equal(clf.coef_, clf_precompute.coef_)
+                assert_array_almost_equal(clf.intercept_,
+                                          clf_precompute.intercept_)
+
+            assert_array_almost_equal(coef[np.float32], coef[np.float64],
+                                      decimal=4)
+            assert_array_almost_equal(intercept[np.float32],
+                                      intercept[np.float64],
+                                      decimal=4)
diff --git a/sklearn/linear_model/tests/test_huber.py b/sklearn/linear_model/tests/test_huber.py
index 5068b56d3aec..9431e96f7410 100644
--- a/sklearn/linear_model/tests/test_huber.py
+++ b/sklearn/linear_model/tests/test_huber.py
@@ -9,6 +9,7 @@
 from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_greater
+from sklearn.utils.testing import assert_false
 
 from sklearn.datasets import make_regression
 from sklearn.linear_model import (
@@ -64,14 +65,22 @@ def test_huber_sample_weights():
     # Test sample_weights implementation in HuberRegressor"""
 
     X, y = make_regression_with_outliers()
-    huber = HuberRegressor(fit_intercept=True, alpha=0.1)
+    huber = HuberRegressor(fit_intercept=True)
     huber.fit(X, y)
     huber_coef = huber.coef_
     huber_intercept = huber.intercept_
 
+    # Rescale coefs before comparing with assert_array_almost_equal to make sure
+    # that the number of decimal places used is somewhat insensitive to the
+    # amplitude of the coefficients and therefore to the scale of the data
+    # and the regularization parameter
+    scale = max(np.mean(np.abs(huber.coef_)),
+                np.mean(np.abs(huber.intercept_)))
+
     huber.fit(X, y, sample_weight=np.ones(y.shape[0]))
-    assert_array_almost_equal(huber.coef_, huber_coef)
-    assert_array_almost_equal(huber.intercept_, huber_intercept)
+    assert_array_almost_equal(huber.coef_ / scale, huber_coef / scale)
+    assert_array_almost_equal(huber.intercept_ / scale,
+                              huber_intercept / scale)
 
     X, y = make_regression_with_outliers(n_samples=5, n_features=20)
     X_new = np.vstack((X, np.vstack((X[1], X[1], X[3]))))
@@ -79,15 +88,21 @@ def test_huber_sample_weights():
     huber.fit(X_new, y_new)
     huber_coef = huber.coef_
     huber_intercept = huber.intercept_
-    huber.fit(X, y, sample_weight=[1, 3, 1, 2, 1])
-    assert_array_almost_equal(huber.coef_, huber_coef, 3)
-    assert_array_almost_equal(huber.intercept_, huber_intercept, 3)
+    sample_weight = np.ones(X.shape[0])
+    sample_weight[1] = 3
+    sample_weight[3] = 2
+    huber.fit(X, y, sample_weight=sample_weight)
+
+    assert_array_almost_equal(huber.coef_ / scale, huber_coef / scale)
+    assert_array_almost_equal(huber.intercept_ / scale,
+                              huber_intercept / scale)
 
     # Test sparse implementation with sample weights.
     X_csr = sparse.csr_matrix(X)
-    huber_sparse = HuberRegressor(fit_intercept=True, alpha=0.1)
-    huber_sparse.fit(X_csr, y, sample_weight=[1, 3, 1, 2, 1])
-    assert_array_almost_equal(huber_sparse.coef_, huber_coef, 3)
+    huber_sparse = HuberRegressor(fit_intercept=True)
+    huber_sparse.fit(X_csr, y, sample_weight=sample_weight)
+    assert_array_almost_equal(huber_sparse.coef_ / scale,
+                              huber_coef / scale)
 
 
 def test_huber_sparse():
@@ -99,31 +114,31 @@ def test_huber_sparse():
     huber_sparse = HuberRegressor(fit_intercept=True, alpha=0.1)
     huber_sparse.fit(X_csr, y)
     assert_array_almost_equal(huber_sparse.coef_, huber.coef_)
+    assert_array_equal(huber.outliers_, huber_sparse.outliers_)
 
 
 def test_huber_scaling_invariant():
     """Test that outliers filtering is scaling independent."""
     rng = np.random.RandomState(0)
     X, y = make_regression_with_outliers()
-    huber = HuberRegressor(fit_intercept=False, alpha=0.0, max_iter=100,
-                           epsilon=1.35)
+    huber = HuberRegressor(fit_intercept=False, alpha=0.0, max_iter=100)
     huber.fit(X, y)
     n_outliers_mask_1 = huber.outliers_
+    assert_false(np.all(n_outliers_mask_1))
 
     huber.fit(X, 2. * y)
     n_outliers_mask_2 = huber.outliers_
+    assert_array_equal(n_outliers_mask_2, n_outliers_mask_1)
 
     huber.fit(2. * X, 2. * y)
     n_outliers_mask_3 = huber.outliers_
-
-    assert_array_equal(n_outliers_mask_2, n_outliers_mask_1)
     assert_array_equal(n_outliers_mask_3, n_outliers_mask_1)
 
 
 def test_huber_and_sgd_same_results():
     """Test they should converge to same coefficients for same parameters"""
 
-    X, y = make_regression_with_outliers(n_samples=5, n_features=2)
+    X, y = make_regression_with_outliers(n_samples=10, n_features=2)
 
     # Fit once to find out the scale parameter. Scale down X and y by scale
     # so that the scale parameter is optimized to 1.0
@@ -136,7 +151,7 @@ def test_huber_and_sgd_same_results():
     assert_almost_equal(huber.scale_, 1.0, 3)
 
     sgdreg = SGDRegressor(
-        alpha=0.0, loss="huber", shuffle=True, random_state=0, n_iter=1000000,
+        alpha=0.0, loss="huber", shuffle=True, random_state=0, n_iter=10000,
         fit_intercept=False, epsilon=1.35)
     sgdreg.fit(X_scale, y_scale)
     assert_array_almost_equal(huber.coef_, sgdreg.coef_, 1)
@@ -155,9 +170,8 @@ def test_huber_warm_start():
     assert_array_almost_equal(huber_warm.coef_, huber_warm_coef, 1)
 
     # No n_iter_ in old SciPy (<=0.9)
-    # And as said above, the first iteration seems to be run anyway.
     if huber_warm.n_iter_ is not None:
-        assert_equal(1, huber_warm.n_iter_)
+        assert_equal(0, huber_warm.n_iter_)
 
 
 def test_huber_better_r2_score():
diff --git a/sklearn/linear_model/tests/test_sparse_coordinate_descent.py b/sklearn/linear_model/tests/test_sparse_coordinate_descent.py
index 3be4cb833791..6b4c09d9742e 100644
--- a/sklearn/linear_model/tests/test_sparse_coordinate_descent.py
+++ b/sklearn/linear_model/tests/test_sparse_coordinate_descent.py
@@ -267,3 +267,27 @@ def test_same_output_sparse_dense_lasso_and_enet_cv():
         assert_almost_equal(clfs.intercept_, clfd.intercept_, 7)
         assert_array_almost_equal(clfs.mse_path_, clfd.mse_path_)
         assert_array_almost_equal(clfs.alphas_, clfd.alphas_)
+
+
+def test_same_multiple_output_sparse_dense():
+    for normalize in [True, False]:
+        l = ElasticNet(normalize=normalize)
+        X = [[0, 1, 2, 3, 4],
+             [0, 2, 5, 8, 11],
+             [9, 10, 11, 12, 13],
+             [10, 11, 12, 13, 14]]
+        y = [[1, 2, 3, 4, 5],
+             [1, 3, 6, 9, 12],
+             [10, 11, 12, 13, 14],
+             [11, 12, 13, 14, 15]]
+        ignore_warnings(l.fit)(X, y)
+        sample = np.array([1, 2, 3, 4, 5]).reshape(1, -1)
+        predict_dense = l.predict(sample)
+
+        l_sp = ElasticNet(normalize=normalize)
+        X_sp = sp.coo_matrix(X)
+        ignore_warnings(l_sp.fit)(X_sp, y)
+        sample_sparse = sp.coo_matrix(sample)
+        predict_sparse = l_sp.predict(sample_sparse)
+
+        assert_array_almost_equal(predict_sparse, predict_dense)
diff --git a/sklearn/manifold/t_sne.py b/sklearn/manifold/t_sne.py
index 90643a5deae8..f5bc6ea9bbd1 100644
--- a/sklearn/manifold/t_sne.py
+++ b/sklearn/manifold/t_sne.py
@@ -23,6 +23,7 @@
 from . import _utils
 from . import _barnes_hut_tsne
 from ..utils.fixes import astype
+from ..externals.six import string_types
 
 
 MACHINE_EPSILON = np.finfo(np.double).eps
@@ -567,8 +568,9 @@ class TSNE(BaseEstimator):
         the distance between them. The default is "euclidean" which is
         interpreted as squared euclidean distance.
 
-    init : string, optional (default: "random")
-        Initialization of embedding. Possible options are 'random' and 'pca'.
+    init : string or numpy array, optional (default: "random")
+        Initialization of embedding. Possible options are 'random', 'pca',
+        and a numpy array of shape (n_samples, n_components).
         PCA initialization cannot be used with precomputed distances and is
         usually more globally stable than random initialization.
 
@@ -643,8 +645,10 @@ def __init__(self, n_components=2, perplexity=30.0,
                  n_iter_without_progress=30, min_grad_norm=1e-7,
                  metric="euclidean", init="random", verbose=0,
                  random_state=None, method='barnes_hut', angle=0.5):
-        if init not in ["pca", "random"] or isinstance(init, np.ndarray):
-            msg = "'init' must be 'pca', 'random' or a NumPy array"
+        if not ((isinstance(init, string_types) and
+                init in ["pca", "random"]) or
+                isinstance(init, np.ndarray)):
+            msg = "'init' must be 'pca', 'random', or a numpy array"
             raise ValueError(msg)
         self.n_components = n_components
         self.perplexity = perplexity
@@ -707,7 +711,7 @@ def _fit(self, X, skip_num_points=0):
             raise ValueError("n_iter should be at least 200")
 
         if self.metric == "precomputed":
-            if self.init == 'pca':
+            if isinstance(self.init, string_types) and self.init == 'pca':
                 raise ValueError("The parameter init=\"pca\" cannot be used "
                                  "with metric=\"precomputed\".")
             if X.shape[0] != X.shape[1]:
@@ -763,12 +767,12 @@ def _fit(self, X, skip_num_points=0):
         assert np.all(P <= 1), ("All probabilities should be less "
                                 "or then equal to one")
 
-        if self.init == 'pca':
+        if isinstance(self.init, np.ndarray):
+            X_embedded = self.init
+        elif self.init == 'pca':
             pca = PCA(n_components=self.n_components, svd_solver='randomized',
                       random_state=random_state)
             X_embedded = pca.fit_transform(X)
-        elif isinstance(self.init, np.ndarray):
-            X_embedded = self.init
         elif self.init == 'random':
             X_embedded = None
         else:
diff --git a/sklearn/manifold/tests/test_locally_linear.py b/sklearn/manifold/tests/test_locally_linear.py
index 0ef5b193d246..d1052967bc88 100644
--- a/sklearn/manifold/tests/test_locally_linear.py
+++ b/sklearn/manifold/tests/test_locally_linear.py
@@ -14,7 +14,7 @@
 eigen_solvers = ['dense', 'arpack']
 
 
-#----------------------------------------------------------------------
+# ----------------------------------------------------------------------
 # Test utility routines
 def test_barycenter_kneighbors_graph():
     X = np.array([[0, 1], [1.01, 1.], [2, 0]])
@@ -33,7 +33,7 @@ def test_barycenter_kneighbors_graph():
     assert_less(linalg.norm(pred - X) / X.shape[0], 1)
 
 
-#----------------------------------------------------------------------
+# ----------------------------------------------------------------------
 # Test LLE by computing the reconstruction error on some manifolds.
 
 def test_lle_simple_grid():
diff --git a/sklearn/manifold/tests/test_t_sne.py b/sklearn/manifold/tests/test_t_sne.py
index 84377fae565c..41aefdc20331 100644
--- a/sklearn/manifold/tests/test_t_sne.py
+++ b/sklearn/manifold/tests/test_t_sne.py
@@ -7,6 +7,7 @@
 from sklearn.utils.testing import assert_less_equal
 from sklearn.utils.testing import assert_equal
 from sklearn.utils.testing import assert_almost_equal
+from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_less
 from sklearn.utils.testing import assert_raises_regexp
@@ -196,10 +197,13 @@ def test_gradient():
 
     P = _joint_probabilities(distances, desired_perplexity=25.0,
                              verbose=0)
-    fun = lambda params: _kl_divergence(params, P, alpha, n_samples,
-                                        n_components)[0]
-    grad = lambda params: _kl_divergence(params, P, alpha, n_samples,
-                                         n_components)[1]
+
+    def fun(params):
+        return _kl_divergence(params, P, alpha, n_samples, n_components)[0]
+
+    def grad(params):
+        return _kl_divergence(params, P, alpha, n_samples, n_components)[1]
+
     assert_almost_equal(check_grad(fun, grad, X_embedded.ravel()), 0.0,
                         decimal=5)
 
@@ -305,11 +309,25 @@ def test_non_square_precomputed_distances():
 
 
 def test_init_not_available():
-    # 'init' must be 'pca' or 'random'.
-    m = "'init' must be 'pca', 'random' or a NumPy array"
+    # 'init' must be 'pca', 'random', or numpy array.
+    m = "'init' must be 'pca', 'random', or a numpy array"
     assert_raises_regexp(ValueError, m, TSNE, init="not available")
 
 
+def test_init_ndarray():
+    # Initialize TSNE with ndarray and test fit
+    tsne = TSNE(init=np.zeros((100, 2)))
+    X_embedded = tsne.fit_transform(np.ones((100, 5)))
+    assert_array_equal(np.zeros((100, 2)), X_embedded)
+
+
+def test_init_ndarray_precomputed():
+    # Initialize TSNE with ndarray and metric 'precomputed'
+    # Make sure no FutureWarning is thrown from _fit
+    tsne = TSNE(init=np.zeros((100, 2)), metric="precomputed")
+    tsne.fit(np.zeros((100, 100)))
+
+
 def test_distance_not_available():
     # 'metric' must be valid.
     tsne = TSNE(metric="not available")
diff --git a/sklearn/metrics/classification.py b/sklearn/metrics/classification.py
index b90c4a067db4..50d3d3b7523f 100644
--- a/sklearn/metrics/classification.py
+++ b/sklearn/metrics/classification.py
@@ -30,6 +30,7 @@
 
 from ..preprocessing import LabelBinarizer, label_binarize
 from ..preprocessing import LabelEncoder
+from ..utils import assert_all_finite
 from ..utils import check_array
 from ..utils import check_consistent_length
 from ..utils import column_or_1d
@@ -666,7 +667,8 @@ def f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary',
 
     References
     ----------
-    .. [1] `Wikipedia entry for the F1-score <https://en.wikipedia.org/wiki/F1_score>`_
+    .. [1] `Wikipedia entry for the F1-score
+           <https://en.wikipedia.org/wiki/F1_score>`_
 
     Examples
     --------
@@ -1451,7 +1453,8 @@ class 2       1.00      0.67      0.80         3
     return report
 
 
-def hamming_loss(y_true, y_pred, classes=None, sample_weight=None):
+def hamming_loss(y_true, y_pred, labels=None, sample_weight=None,
+                 classes=None):
     """Compute the average Hamming loss.
 
     The Hamming loss is the fraction of labels that are incorrectly predicted.
@@ -1466,12 +1469,19 @@ def hamming_loss(y_true, y_pred, classes=None, sample_weight=None):
     y_pred : 1d array-like, or label indicator array / sparse matrix
         Predicted labels, as returned by a classifier.
 
-    classes : array, shape = [n_labels], optional
-        Integer array of labels.
+    labels : array, shape = [n_labels], optional (default=None)
+        Integer array of labels. If not provided, labels will be inferred
+        from y_true and y_pred.
+
+        .. versionadded:: 0.18
 
     sample_weight : array-like of shape = [n_samples], optional
         Sample weights.
 
+    classes : array, shape = [n_labels], optional
+        (deprecated) Integer array of labels. This parameter has been
+         renamed to ``labels`` in version 0.18 and will be removed in 0.20.
+
     Returns
     -------
     loss : float or int,
@@ -1519,12 +1529,17 @@ def hamming_loss(y_true, y_pred, classes=None, sample_weight=None):
     >>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))
     0.75
     """
+    if classes is not None:
+        warnings.warn("'classes' was renamed to 'labels' in version 0.18 and "
+                      "will be removed in 0.20.", DeprecationWarning)
+        labels = classes
+
     y_type, y_true, y_pred = _check_targets(y_true, y_pred)
 
-    if classes is None:
-        classes = unique_labels(y_true, y_pred)
+    if labels is None:
+        labels = unique_labels(y_true, y_pred)
     else:
-        classes = np.asarray(classes)
+        labels = np.asarray(labels)
 
     if sample_weight is None:
         weight_average = 1.
@@ -1535,7 +1550,7 @@ def hamming_loss(y_true, y_pred, classes=None, sample_weight=None):
         n_differences = count_nonzero(y_true - y_pred,
                                       sample_weight=sample_weight)
         return (n_differences /
-                (y_true.shape[0] * len(classes) * weight_average))
+                (y_true.shape[0] * len(labels) * weight_average))
 
     elif y_type in ["binary", "multiclass"]:
         return _weighted_sum(y_true != y_pred, sample_weight, normalize=True)
@@ -1543,13 +1558,15 @@ def hamming_loss(y_true, y_pred, classes=None, sample_weight=None):
         raise ValueError("{0} is not supported".format(y_type))
 
 
-def log_loss(y_true, y_pred, eps=1e-15, normalize=True, sample_weight=None):
+def log_loss(y_true, y_pred, eps=1e-15, normalize=True, sample_weight=None,
+             labels=None):
     """Log loss, aka logistic loss or cross-entropy loss.
 
     This is the loss function used in (multinomial) logistic regression
     and extensions of it such as neural networks, defined as the negative
     log-likelihood of the true labels given a probabilistic classifier's
-    predictions. For a single sample with true label yt in {0,1} and
+    predictions. The log loss is only defined for two or more labels.
+    For a single sample with true label yt in {0,1} and
     estimated probability yp that yt = 1, the log loss is
 
         -log P(yt|yp) = -(yt log(yp) + (1 - yt) log(1 - yp))
@@ -1561,9 +1578,13 @@ def log_loss(y_true, y_pred, eps=1e-15, normalize=True, sample_weight=None):
     y_true : array-like or label indicator matrix
         Ground truth (correct) labels for n_samples samples.
 
-    y_pred : array-like of float, shape = (n_samples, n_classes)
+    y_pred : array-like of float, shape = (n_samples, n_classes) or (n_samples,)
         Predicted probabilities, as returned by a classifier's
-        predict_proba method.
+        predict_proba method. If ``y_pred.shape = (n_samples,)``
+        the probabilities provided are assumed to be that of the
+        positive class. The labels in ``y_pred`` are assumed to be
+        ordered alphabetically, as done by
+        :class:`preprocessing.LabelBinarizer`.
 
     eps : float
         Log loss is undefined for p=0 or p=1, so probabilities are
@@ -1576,6 +1597,12 @@ def log_loss(y_true, y_pred, eps=1e-15, normalize=True, sample_weight=None):
     sample_weight : array-like of shape = [n_samples], optional
         Sample weights.
 
+    labels : array-like, optional (default=None)
+        If not provided, labels will be inferred from y_true. If ``labels``
+        is ``None`` and ``y_pred`` has shape (n_samples,) the labels are
+        assumed to be binary and are inferred from ``y_true``.
+        .. versionadded:: 0.18
+
     Returns
     -------
     loss : float
@@ -1595,37 +1622,61 @@ def log_loss(y_true, y_pred, eps=1e-15, normalize=True, sample_weight=None):
     -----
     The logarithm used is the natural logarithm (base-e).
     """
+    y_pred = check_array(y_pred, ensure_2d=False)
+    check_consistent_length(y_pred, y_true)
+
     lb = LabelBinarizer()
-    T = lb.fit_transform(y_true)
-    if T.shape[1] == 1:
-        T = np.append(1 - T, T, axis=1)
 
-    y_pred = check_array(y_pred, ensure_2d=False)
-    # Clipping
-    Y = np.clip(y_pred, eps, 1 - eps)
+    if labels is not None:
+        lb.fit(labels)
+    else:
+        lb.fit(y_true)
+
+    if len(lb.classes_) == 1:
+        if labels is None:
+            raise ValueError('y_true contains only one label ({0}). Please '
+                             'provide the true labels explicitly through the '
+                             'labels argument.'.format(lb.classes_[0]))
+        else:
+            raise ValueError('The labels array needs to contain at least two '
+                             'labels for log_loss, '
+                             'got {0}.'.format(lb.classes_))
 
-    # This happens in cases when elements in y_pred have type "str".
-    if not isinstance(Y, np.ndarray):
-        raise ValueError("y_pred should be an array of floats.")
+    transformed_labels = lb.transform(y_true)
+
+    if transformed_labels.shape[1] == 1:
+        transformed_labels = np.append(1 - transformed_labels,
+                                       transformed_labels, axis=1)
+
+    # Clipping
+    y_pred = np.clip(y_pred, eps, 1 - eps)
 
     # If y_pred is of single dimension, assume y_true to be binary
     # and then check.
-    if Y.ndim == 1:
-        Y = Y[:, np.newaxis]
-    if Y.shape[1] == 1:
-        Y = np.append(1 - Y, Y, axis=1)
+    if y_pred.ndim == 1:
+        y_pred = y_pred[:, np.newaxis]
+    if y_pred.shape[1] == 1:
+        y_pred = np.append(1 - y_pred, y_pred, axis=1)
 
     # Check if dimensions are consistent.
-    check_consistent_length(T, Y)
-    T = check_array(T)
-    Y = check_array(Y)
-    if T.shape[1] != Y.shape[1]:
-        raise ValueError("y_true and y_pred have different number of classes "
-                         "%d, %d" % (T.shape[1], Y.shape[1]))
+    transformed_labels = check_array(transformed_labels)
+    if len(lb.classes_) != y_pred.shape[1]:
+        if labels is None:
+            raise ValueError("y_true and y_pred contain different number of "
+                             "classes {0}, {1}. Please provide the true "
+                             "labels explicitly through the labels argument. "
+                             "Classes found in "
+                             "y_true: {2}".format(transformed_labels.shape[1],
+                                                  y_pred.shape[1],
+                                                  lb.classes_))
+        else:
+            raise ValueError('The number of classes in labels is different '
+                             'from that in y_pred. Classes found in '
+                             'labels: {0}'.format(lb.classes_))
 
     # Renormalize
-    Y /= Y.sum(axis=1)[:, np.newaxis]
-    loss = -(T * np.log(Y)).sum(axis=1)
+    y_pred /= y_pred.sum(axis=1)[:, np.newaxis]
+    loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)
 
     return _weighted_sum(loss, sample_weight, normalize)
 
@@ -1843,6 +1894,9 @@ def brier_score_loss(y_true, y_prob, sample_weight=None, pos_label=None):
     """
     y_true = column_or_1d(y_true)
     y_prob = column_or_1d(y_prob)
+    assert_all_finite(y_true)
+    assert_all_finite(y_prob)
+
     if pos_label is None:
         pos_label = y_true.max()
     y_true = np.array(y_true == pos_label, int)
diff --git a/sklearn/metrics/cluster/expected_mutual_info_fast.pyx b/sklearn/metrics/cluster/expected_mutual_info_fast.pyx
index 2ef60701d6f1..d0c08be8d238 100644
--- a/sklearn/metrics/cluster/expected_mutual_info_fast.pyx
+++ b/sklearn/metrics/cluster/expected_mutual_info_fast.pyx
@@ -13,21 +13,21 @@ cimport cython
 from sklearn.utils.lgamma cimport lgamma
 
 np.import_array()
-
+ctypedef np.float64_t DOUBLE
 
 @cython.boundscheck(False)
 @cython.wraparound(False)
 def expected_mutual_information(contingency, int n_samples):
     """Calculate the expected mutual information for two labelings."""
     cdef int R, C
-    cdef float N, gln_N, emi, term2, term3, gln
-    cdef np.ndarray[double] gln_a, gln_b, gln_Na, gln_Nb, gln_nij, log_Nnij
-    cdef np.ndarray[double] nijs, term1
-    cdef np.ndarray[double, ndim=2] log_ab_outer
+    cdef DOUBLE N, gln_N, emi, term2, term3, gln
+    cdef np.ndarray[DOUBLE] gln_a, gln_b, gln_Na, gln_Nb, gln_nij, log_Nnij
+    cdef np.ndarray[DOUBLE] nijs, term1
+    cdef np.ndarray[DOUBLE, ndim=2] log_ab_outer
     cdef np.ndarray[np.int32_t] a, b
     #cdef np.ndarray[int, ndim=2] start, end
     R, C = contingency.shape
-    N = float(n_samples)
+    N = <DOUBLE>n_samples
     a = np.sum(contingency, axis=1).astype(np.int32)
     b = np.sum(contingency, axis=0).astype(np.int32)
     # There are three major terms to the EMI equation, which are multiplied to
diff --git a/sklearn/metrics/cluster/tests/test_unsupervised.py b/sklearn/metrics/cluster/tests/test_unsupervised.py
index e1718e604d29..04bdf8b6f60f 100644
--- a/sklearn/metrics/cluster/tests/test_unsupervised.py
+++ b/sklearn/metrics/cluster/tests/test_unsupervised.py
@@ -1,4 +1,5 @@
 import numpy as np
+import scipy.sparse as sp
 from scipy.sparse import csr_matrix
 
 from sklearn import datasets
@@ -7,6 +8,7 @@
 from sklearn.utils.testing import assert_equal
 from sklearn.utils.testing import assert_raises_regexp
 from sklearn.utils.testing import assert_raise_message
+from sklearn.utils.testing import assert_greater
 from sklearn.metrics.cluster import silhouette_score
 from sklearn.metrics.cluster import calinski_harabaz_score
 from sklearn.metrics import pairwise_distances
@@ -15,31 +17,43 @@
 def test_silhouette():
     # Tests the Silhouette Coefficient.
     dataset = datasets.load_iris()
-    X = dataset.data
+    X_dense = dataset.data
+    X_csr = csr_matrix(X_dense)
+    X_dok = sp.dok_matrix(X_dense)
+    X_lil = sp.lil_matrix(X_dense)
     y = dataset.target
-    D = pairwise_distances(X, metric='euclidean')
-    # Given that the actual labels are used, we can assume that S would be
-    # positive.
-    silhouette = silhouette_score(D, y, metric='precomputed')
-    assert(silhouette > 0)
-    # Test without calculating D
-    silhouette_metric = silhouette_score(X, y, metric='euclidean')
-    assert_almost_equal(silhouette, silhouette_metric)
-    # Test with sampling
-    silhouette = silhouette_score(D, y, metric='precomputed',
-                                  sample_size=int(X.shape[0] / 2),
-                                  random_state=0)
-    silhouette_metric = silhouette_score(X, y, metric='euclidean',
-                                         sample_size=int(X.shape[0] / 2),
-                                         random_state=0)
-    assert(silhouette > 0)
-    assert(silhouette_metric > 0)
-    assert_almost_equal(silhouette_metric, silhouette)
-    # Test with sparse X
-    X_sparse = csr_matrix(X)
-    D = pairwise_distances(X_sparse, metric='euclidean')
-    silhouette = silhouette_score(D, y, metric='precomputed')
-    assert(silhouette > 0)
+
+    for X in [X_dense, X_csr, X_dok, X_lil]:
+        D = pairwise_distances(X, metric='euclidean')
+        # Given that the actual labels are used, we can assume that S would be
+        # positive.
+        score_precomputed = silhouette_score(D, y, metric='precomputed')
+        assert_greater(score_precomputed, 0)
+        # Test without calculating D
+        score_euclidean = silhouette_score(X, y, metric='euclidean')
+        assert_almost_equal(score_precomputed, score_euclidean)
+
+        if X is X_dense:
+            score_dense_without_sampling = score_precomputed
+        else:
+            assert_almost_equal(score_euclidean,
+                                score_dense_without_sampling)
+
+        # Test with sampling
+        score_precomputed = silhouette_score(D, y, metric='precomputed',
+                                             sample_size=int(X.shape[0] / 2),
+                                             random_state=0)
+        score_euclidean = silhouette_score(X, y, metric='euclidean',
+                                           sample_size=int(X.shape[0] / 2),
+                                           random_state=0)
+        assert_greater(score_precomputed, 0)
+        assert_greater(score_euclidean, 0)
+        assert_almost_equal(score_euclidean, score_precomputed)
+
+        if X is X_dense:
+            score_dense_with_sampling = score_precomputed
+        else:
+            assert_almost_equal(score_euclidean, score_dense_with_sampling)
 
 
 def test_no_nan():
diff --git a/sklearn/metrics/cluster/unsupervised.py b/sklearn/metrics/cluster/unsupervised.py
index ce9e35487b1b..c1cca77ded2e 100644
--- a/sklearn/metrics/cluster/unsupervised.py
+++ b/sklearn/metrics/cluster/unsupervised.py
@@ -88,7 +88,7 @@ def silhouette_score(X, labels, metric='euclidean', sample_size=None,
            <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_
 
     """
-    X, labels = check_X_y(X, labels)
+    X, labels = check_X_y(X, labels, accept_sparse=['csc', 'csr'])
     le = LabelEncoder()
     labels = le.fit_transform(labels)
     n_labels = len(le.classes_)
diff --git a/sklearn/metrics/pairwise.py b/sklearn/metrics/pairwise.py
index d23bc707241d..c058e2096729 100644
--- a/sklearn/metrics/pairwise.py
+++ b/sklearn/metrics/pairwise.py
@@ -893,7 +893,7 @@ def cosine_similarity(X, Y=None, dense_output=True):
         ``False``, the output is sparse if both input arrays are sparse.
 
         .. versionadded:: 0.17
-           parameter *dense_output* for sparse output.
+           parameter ``dense_output`` for dense output.
 
     Returns
     -------
diff --git a/sklearn/metrics/pairwise_fast.pyx b/sklearn/metrics/pairwise_fast.pyx
index d26572691a16..09d39bd8a126 100644
--- a/sklearn/metrics/pairwise_fast.pyx
+++ b/sklearn/metrics/pairwise_fast.pyx
@@ -14,8 +14,8 @@ cimport numpy as np
 cdef extern from "cblas.h":
     double cblas_dasum(int, const double *, int) nogil
 
-ctypedef float [:, :] float_array_2d_t 
-ctypedef double [:, :] double_array_2d_t 
+ctypedef float [:, :] float_array_2d_t
+ctypedef double [:, :] double_array_2d_t
 
 cdef fused floating1d:
     float[::1]
diff --git a/sklearn/metrics/ranking.py b/sklearn/metrics/ranking.py
index da75465597f6..03bee6e5064f 100644
--- a/sklearn/metrics/ranking.py
+++ b/sklearn/metrics/ranking.py
@@ -23,6 +23,7 @@
 import numpy as np
 from scipy.sparse import csr_matrix
 
+from ..utils import assert_all_finite
 from ..utils import check_consistent_length
 from ..utils import column_or_1d, check_array
 from ..utils.multiclass import type_of_target
@@ -296,6 +297,9 @@ def _binary_clf_curve(y_true, y_score, pos_label=None, sample_weight=None):
     check_consistent_length(y_true, y_score)
     y_true = column_or_1d(y_true)
     y_score = column_or_1d(y_score)
+    assert_all_finite(y_true)
+    assert_all_finite(y_score)
+
     if sample_weight is not None:
         sample_weight = column_or_1d(sample_weight)
 
diff --git a/sklearn/metrics/tests/test_classification.py b/sklearn/metrics/tests/test_classification.py
index e578964b600f..576c3a362b29 100644
--- a/sklearn/metrics/tests/test_classification.py
+++ b/sklearn/metrics/tests/test_classification.py
@@ -45,7 +45,6 @@
 from sklearn.metrics import zero_one_loss
 from sklearn.metrics import brier_score_loss
 
-
 from sklearn.metrics.classification import _check_targets
 from sklearn.exceptions import UndefinedMetricWarning
 
@@ -768,6 +767,7 @@ def test_multilabel_hamming_loss():
     assert_equal(hamming_loss(y1, np.zeros_like(y1), sample_weight=w), 2. / 3)
     # sp_hamming only works with 1-D arrays
     assert_equal(hamming_loss(y1[0], y2[0]), sp_hamming(y1[0], y2[0]))
+    assert_warns(DeprecationWarning, hamming_loss, y1, y2, classes=[0, 1])
 
 
 def test_multilabel_jaccard_similarity_score():
@@ -1384,6 +1384,32 @@ def test_log_loss():
     loss = log_loss(y_true, y_pred)
     assert_almost_equal(loss, 1.0383217, decimal=6)
 
+    # test labels option
+
+    y_true = [2, 2]
+    y_pred = [[0.2, 0.7], [0.6, 0.5]]
+    y_score = np.array([[0.1, 0.9], [0.1, 0.9]])
+    error_str = ('y_true contains only one label (2). Please provide '
+                 'the true labels explicitly through the labels argument.')
+    assert_raise_message(ValueError, error_str, log_loss, y_true, y_pred)
+
+    y_pred = [[0.2, 0.7], [0.6, 0.5], [0.2, 0.3]]
+    error_str = ('Found input variables with inconsistent numbers of samples: '
+                 '[3, 2]')
+    assert_raise_message(ValueError, error_str, log_loss, y_true, y_pred)
+
+    # works when the labels argument is used
+
+    true_log_loss = -np.mean(np.log(y_score[:, 1]))
+    calculated_log_loss = log_loss(y_true, y_score, labels=[1, 2])
+    assert_almost_equal(calculated_log_loss, true_log_loss)
+
+    # ensure labels work when len(np.unique(y_true)) != y_pred.shape[1]
+    y_true = [1, 2, 2]
+    y_score2 = [[0.2, 0.7, 0.3], [0.6, 0.5, 0.3], [0.3, 0.9, 0.1]]
+    loss = log_loss(y_true, y_score2, labels=[1, 2, 3])
+    assert_almost_equal(loss, 1.0630345, decimal=6)
+
 
 def test_log_loss_pandas_input():
     # case when input is a pandas series and dataframe gh-5715
diff --git a/sklearn/metrics/tests/test_common.py b/sklearn/metrics/tests/test_common.py
index 699eb501c416..4974d29356be 100644
--- a/sklearn/metrics/tests/test_common.py
+++ b/sklearn/metrics/tests/test_common.py
@@ -19,6 +19,7 @@
 from sklearn.utils.testing import assert_greater
 from sklearn.utils.testing import assert_not_equal
 from sklearn.utils.testing import assert_raises
+from sklearn.utils.testing import assert_raise_message
 from sklearn.utils.testing import assert_true
 from sklearn.utils.testing import ignore_warnings
 
@@ -257,6 +258,8 @@
 METRICS_WITH_LABELS = [
     "confusion_matrix",
 
+    "hamming_loss",
+
     "precision_score", "recall_score", "f1_score", "f2_score", "f0.5_score",
 
     "weighted_f0.5_score", "weighted_f1_score", "weighted_f2_score",
@@ -608,6 +611,29 @@ def test_invariance_string_vs_numbers_labels():
             assert_raises(ValueError, metric, y1_str.astype('O'), y2)
 
 
+def test_inf_nan_input():
+    invalids =[([0, 1], [np.inf, np.inf]),
+               ([0, 1], [np.nan, np.nan]),
+               ([0, 1], [np.nan, np.inf])]
+
+    METRICS = dict()
+    METRICS.update(THRESHOLDED_METRICS)
+    METRICS.update(REGRESSION_METRICS)
+
+    for metric in METRICS.values():
+        for y_true, y_score in invalids:
+            assert_raise_message(ValueError,
+                                 "contains NaN, infinity",
+                                 metric, y_true, y_score)
+
+    # Classification metrics all raise a mixed input exception
+    for metric in CLASSIFICATION_METRICS.values():
+        for y_true, y_score in invalids:
+            assert_raise_message(ValueError,
+                                 "Can't handle mix of binary and continuous",
+                                 metric, y_true, y_score)
+
+
 @ignore_warnings
 def check_single_sample(name):
     # Non-regression test: scores should work with a single sample.
diff --git a/sklearn/metrics/tests/test_pairwise.py b/sklearn/metrics/tests/test_pairwise.py
index 530dc6068a32..01863166953d 100644
--- a/sklearn/metrics/tests/test_pairwise.py
+++ b/sklearn/metrics/tests/test_pairwise.py
@@ -61,7 +61,8 @@ def test_pairwise_distances():
     Y_tuples = tuple([tuple([v for v in row]) for row in Y])
     S2 = pairwise_distances(X_tuples, Y_tuples, metric="euclidean")
     assert_array_almost_equal(S, S2)
-    # "cityblock" uses sklearn metric, cityblock (function) is scipy.spatial.
+    # "cityblock" uses scikit-learn metric, cityblock (function) is
+    # scipy.spatial.
     S = pairwise_distances(X, metric="cityblock")
     S2 = pairwise_distances(X, metric=cityblock)
     assert_equal(S.shape[0], S.shape[1])
@@ -78,7 +79,8 @@ def test_pairwise_distances():
     S3 = manhattan_distances(X, Y, size_threshold=10)
     assert_array_almost_equal(S, S3)
     # Test cosine as a string metric versus cosine callable
-    # "cosine" uses sklearn metric, cosine (function) is scipy.spatial
+    # The string "cosine" uses sklearn.metric,
+    # while the function cosine is scipy.spatial
     S = pairwise_distances(X, Y, metric="cosine")
     S2 = pairwise_distances(X, Y, metric=cosine)
     assert_equal(S.shape[0], X.shape[0])
@@ -330,7 +332,7 @@ def test_pairwise_distances_argmin_min():
     assert_equal(type(Dsp), np.ndarray)
     assert_equal(type(Esp), np.ndarray)
 
-    # Non-euclidean sklearn metric
+    # Non-euclidean scikit-learn metric
     D, E = pairwise_distances_argmin_min(X, Y, metric="manhattan")
     D2 = pairwise_distances_argmin(X, Y, metric="manhattan")
     assert_array_almost_equal(D, [0, 1])
diff --git a/sklearn/mixture/__init__.py b/sklearn/mixture/__init__.py
index 8269ec4a31d9..3622518352ca 100644
--- a/sklearn/mixture/__init__.py
+++ b/sklearn/mixture/__init__.py
@@ -8,6 +8,7 @@
 from .dpgmm import DPGMM, VBGMM
 
 from .gaussian_mixture import GaussianMixture
+from .bayesian_mixture import BayesianGaussianMixture
 
 
 __all__ = ['DPGMM',
@@ -17,4 +18,5 @@
            'distribute_covar_matrix_to_match_covariance_type',
            'log_multivariate_normal_density',
            'sample_gaussian',
-           'GaussianMixture']
+           'GaussianMixture',
+           'BayesianGaussianMixture']
diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py
index 3b4a04353401..062d40322885 100644
--- a/sklearn/mixture/base.py
+++ b/sklearn/mixture/base.py
@@ -2,6 +2,7 @@
 
 # Author: Wei Xue <xuewei4d@gmail.com>
 # Modified by Thierry Guillemot <thierry.guillemot.work@gmail.com>
+# License: BSD 3 clause
 
 from __future__ import print_function
 
@@ -136,7 +137,7 @@ def _initialize_parameters(self, X):
         ----------
         X : array-like, shape  (n_samples, n_features)
         """
-        n_samples = X.shape[0]
+        n_samples, _ = X.shape
         random_state = check_random_state(self.random_state)
 
         if self.init_params == 'kmeans':
@@ -145,7 +146,7 @@ def _initialize_parameters(self, X):
                                    random_state=random_state).fit(X).labels_
             resp[np.arange(n_samples), label] = 1
         elif self.init_params == 'random':
-            resp = random_state.rand(X.shape[0], self.n_components)
+            resp = random_state.rand(n_samples, self.n_components)
             resp /= resp.sum(axis=1)[:, np.newaxis]
         else:
             raise ValueError("Unimplemented initialization method '%s'"
@@ -191,32 +192,36 @@ def fit(self, X, y=None):
         do_init = not(self.warm_start and hasattr(self, 'converged_'))
         n_init = self.n_init if do_init else 1
 
-        max_log_likelihood = -np.infty
+        max_lower_bound = -np.infty
         self.converged_ = False
 
+        n_samples, _ = X.shape
         for init in range(n_init):
             self._print_verbose_msg_init_beg(init)
 
             if do_init:
                 self._initialize_parameters(X)
-            current_log_likelihood, resp = self._e_step(X)
+                self.lower_bound_ = -np.infty
 
             for n_iter in range(self.max_iter):
-                prev_log_likelihood = current_log_likelihood
+                prev_lower_bound = self.lower_bound_
 
-                self._m_step(X, resp)
-                current_log_likelihood, resp = self._e_step(X)
-                change = current_log_likelihood - prev_log_likelihood
+                log_prob_norm, log_resp = self._e_step(X)
+                self._m_step(X, log_resp)
+                self.lower_bound_ = self._compute_lower_bound(
+                    log_resp, log_prob_norm)
+
+                change = self.lower_bound_ - prev_lower_bound
                 self._print_verbose_msg_iter_end(n_iter, change)
 
                 if abs(change) < self.tol:
                     self.converged_ = True
                     break
 
-            self._print_verbose_msg_init_end(current_log_likelihood)
+            self._print_verbose_msg_init_end(self.lower_bound_)
 
-            if current_log_likelihood > max_log_likelihood:
-                max_log_likelihood = current_log_likelihood
+            if self.lower_bound_ > max_lower_bound:
+                max_lower_bound = self.lower_bound_
                 best_params = self._get_parameters()
                 best_n_iter = n_iter
 
@@ -232,7 +237,6 @@ def fit(self, X, y=None):
 
         return self
 
-    @abstractmethod
     def _e_step(self, X):
         """E step.
 
@@ -242,21 +246,27 @@ def _e_step(self, X):
 
         Returns
         -------
-        log-likelihood : scalar
+        log_prob_norm : array, shape (n_samples,)
+            Logarithm of the probability of each sample in X.
 
-        responsibility : array, shape (n_samples, n_components)
+        log_responsibility : array, shape (n_samples, n_components)
+            Logarithm of the posterior probabilities (or responsibilities) of
+            the point of each sample in X.
         """
-        pass
+        log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
+        return np.mean(log_prob_norm), log_resp
 
     @abstractmethod
-    def _m_step(self, X, resp):
+    def _m_step(self, X, log_resp):
         """M step.
 
         Parameters
         ----------
         X : array-like, shape (n_samples, n_features)
 
-        resp : array-like, shape (n_samples, n_components)
+        log_resp : array-like, shape (n_samples, n_components)
+            Logarithm of the posterior probabilities (or responsibilities) of
+            the point of each sample in X.
         """
         pass
 
@@ -342,7 +352,7 @@ def predict_proba(self, X):
         """
         self._check_is_fitted()
         X = _check_X(X, None, self.means_.shape[1])
-        _, _, log_resp = self._estimate_log_prob_resp(X)
+        _, log_resp = self._estimate_log_prob_resp(X)
         return np.exp(log_resp)
 
     def _estimate_weighted_log_prob(self, X):
@@ -400,9 +410,6 @@ def _estimate_log_prob_resp(self, X):
         log_prob_norm : array, shape (n_samples,)
             log p(X)
 
-        log_prob : array, shape (n_samples, n_components)
-            log p(X|Z) + log weights
-
         log_responsibilities : array, shape (n_samples, n_components)
             logarithm of the responsibilities
         """
@@ -411,7 +418,7 @@ def _estimate_log_prob_resp(self, X):
         with np.errstate(under='ignore'):
             # ignore underflow
             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
-        return log_prob_norm, weighted_log_prob, log_resp
+        return log_prob_norm, log_resp
 
     def _print_verbose_msg_init_beg(self, n_init):
         """Print verbose message on initialization."""
diff --git a/sklearn/mixture/bayesian_mixture.py b/sklearn/mixture/bayesian_mixture.py
new file mode 100644
index 000000000000..2b5d4458c879
--- /dev/null
+++ b/sklearn/mixture/bayesian_mixture.py
@@ -0,0 +1,688 @@
+"""Bayesian Gaussian Mixture Model."""
+# Author: Wei Xue <xuewei4d@gmail.com>
+#         Thierry Guillemot <thierry.guillemot.work@gmail.com>
+# License: BSD 3 clause
+
+import math
+import numpy as np
+from scipy.special import digamma, gammaln
+
+from .base import BaseMixture, _check_shape
+from .gaussian_mixture import _check_precision_matrix
+from .gaussian_mixture import _check_precision_positivity
+from .gaussian_mixture import _compute_log_det_cholesky
+from .gaussian_mixture import _compute_precision_cholesky
+from .gaussian_mixture import _estimate_gaussian_parameters
+from .gaussian_mixture import _estimate_log_gaussian_prob
+from ..utils import check_array
+from ..utils.validation import check_is_fitted
+
+
+def _log_dirichlet_norm(dirichlet_concentration):
+    """Compute the log of the Dirichlet distribution normalization term.
+
+    Parameters
+    ----------
+    dirichlet_concentration : array-like, shape (n_samples,)
+        The parameters values of the Dirichlet distribution.
+
+    Returns
+    -------
+    log_dirichlet_norm : float
+        The log normalization of the Dirichlet distribution.
+    """
+    return (gammaln(np.sum(dirichlet_concentration)) -
+            np.sum(gammaln(dirichlet_concentration)))
+
+
+def _log_wishart_norm(degrees_of_freedom, log_det_precisions_chol, n_features):
+    """Compute the log of the Wishart distribution normalization term.
+
+    Parameters
+    ----------
+    degrees_of_freedom : array-like, shape (n_components,)
+        The number of degrees of freedom on the covariance Wishart
+        distributions.
+
+    log_det_precision_chol : array-like, shape (n_components,)
+         The determinant of the precision matrix for each component.
+
+    n_features : int
+        The number of features.
+
+    Return
+    ------
+    log_wishart_norm : array-like, shape (n_components,)
+        The log normalization of the Wishart distribution.
+    """
+    # To simplify the computation we have removed the np.log(np.pi) term
+    return -(degrees_of_freedom * log_det_precisions_chol +
+             degrees_of_freedom * n_features * .5 * math.log(2.) +
+             np.sum(gammaln(.5 * (degrees_of_freedom -
+                                  np.arange(n_features)[:, np.newaxis])), 0))
+
+
+class BayesianGaussianMixture(BaseMixture):
+    """Variational estimation of a Gaussian mixture.
+
+    This class allows to infer an approximate posterior distribution over the
+    parameters of a Gaussian mixture distribution. The effective number of
+    components can be inferred from the data.
+
+    Read more in the :ref:`User Guide <bgmm>`.
+
+    Parameters
+    ----------
+    n_components : int, defaults to 1.
+        The number of mixture components. Depending on the data and the value
+        of the `dirichlet_concentration_prior` the model can decide to not use
+        all the components by setting some component `weights_` to values very
+        close to zero. The number of effective components is therefore smaller
+        than n_components.
+
+    covariance_type : {'full', 'tied', 'diag', 'spherical'}, defaults to 'full'.
+        String describing the type of covariance parameters to use.
+        Must be one of::
+        'full' (each component has its own general covariance matrix).
+        'tied' (all components share the same general covariance matrix),
+        'diag' (each component has its own diagonal covariance matrix),
+        'spherical' (each component has its own single variance),
+
+    tol : float, defaults to 1e-3.
+        The convergence threshold. EM iterations will stop when the
+        lower bound average gain on the likelihood (of the training data with
+        respect to the model) is below this threshold.
+
+    reg_covar : float, defaults to 1e-6.
+        Non-negative regularization added to the diagonal of covariance.
+        Allows to assure that the covariance matrices are all positive.
+
+    max_iter : int, defaults to 100.
+        The number of EM iterations to perform.
+
+    n_init : int, defaults to 1.
+        The number of initializations to perform. The result with the highest
+        lower bound value on the likelihood is kept.
+
+    init_params : {'kmeans', 'random'}, defaults to 'kmeans'.
+        The method used to initialize the weights, the means and the
+        covariances.
+        Must be one of::
+        'kmeans' : responsibilities are initialized using kmeans.
+        'random' : responsibilities are initialized randomly.
+
+    dirichlet_concentration_prior : float | None, optional.
+        The dirichlet concentration of each component on the weight
+        distribution (Dirichlet). The higher concentration puts more mass in
+        the center and will lead to more components being active, while a lower
+        concentration parameter will lead to more mass at the edge of the
+        mixture weights simplex. The value of the parameter must be greater
+        than 0. If it is None, it's set to `1. / n_components`.
+
+    mean_precision_prior : float | None, optional.
+        The precision prior on the mean distribution (Gaussian).
+        Controls the extend to where means can be placed. Smaller
+        values concentrates the means of each clusters around `mean_prior`.
+        The value of the parameter must be greater than 0.
+        If it is None, it's set to 1.
+
+    mean_prior : array-like, shape (`n_features`,), optional
+        The prior on the mean distribution (Gaussian).
+        If it is None, it's set to the mean of X.
+
+    degrees_of_freedom_prior : float | None, optional.
+        The prior of the number of degrees of freedom on the covariance
+        distributions (Wishart). If it is None, it's set to `n_features`.
+
+    covariance_prior : float or array-like, optional
+        The prior on the covariance distribution (Wishart).
+        If it is None, the emiprical covariance prior is initialized using the
+        covariance of X. The shape depends on `covariance_type`::
+            (`n_features`, `n_features`) if 'full',
+            (`n_features`, `n_features`) if 'tied',
+            (`n_features`)               if 'diag',
+            float                        if 'spherical'
+
+    random_state: RandomState or an int seed, defaults to None.
+        A random number generator instance.
+
+    warm_start : bool, default to False.
+        If 'warm_start' is True, the solution of the last fitting is used as
+        initialization for the next call of fit(). This can speed up
+        convergence when fit is called several time on similar problems.
+
+    verbose : int, default to 0.
+        Enable verbose output. If 1 then it prints the current
+        initialization and each iteration step. If greater than 1 then
+        it prints also the log probability and the time needed
+        for each step.
+
+    Attributes
+    ----------
+    weights_ : array-like, shape (`n_components`,)
+        The weights of each mixture components.
+
+    means_ : array-like, shape (`n_components`, `n_features`)
+        The mean of each mixture component.
+
+    covariances_ : array-like
+        The covariance of each mixture component.
+        The shape depends on `covariance_type`::
+            (n_components,)                        if 'spherical',
+            (n_features, n_features)               if 'tied',
+            (n_components, n_features)             if 'diag',
+            (n_components, n_features, n_features) if 'full'
+
+    precisions_ : array-like
+        The precision matrices for each component in the mixture. A precision
+        matrix is the inverse of a covariance matrix. A covariance matrix is
+        symmetric positive definite so the mixture of Gaussian can be
+        equivalently parameterized by the precision matrices. Storing the
+        precision matrices instead of the covariance matrices makes it more
+        efficient to compute the log-likelihood of new samples at test time.
+        The shape depends on `covariance_type`::
+            (n_components,)                        if 'spherical',
+            (n_features, n_features)               if 'tied',
+            (n_components, n_features)             if 'diag',
+            (n_components, n_features, n_features) if 'full'
+
+    precisions_cholesky_ : array-like
+        The cholesky decomposition of the precision matrices of each mixture
+        component. A precision matrix is the inverse of a covariance matrix.
+        A covariance matrix is symmetric positive definite so the mixture of
+        Gaussian can be equivalently parameterized by the precision matrices.
+        Storing the precision matrices instead of the covariance matrices makes
+        it more efficient to compute the log-likelihood of new samples at test
+        time. The shape depends on `covariance_type`::
+            (n_components,)                        if 'spherical',
+            (n_features, n_features)               if 'tied',
+            (n_components, n_features)             if 'diag',
+            (n_components, n_features, n_features) if 'full'
+
+    converged_ : bool
+        True when convergence was reached in fit(), False otherwise.
+
+    n_iter_ : int
+        Number of step used by the best fit of inference to reach the
+        convergence.
+
+    lower_bound_ : float
+        Lower bound value on the likelihood (of the training data with
+        respect to the model) of the best fit of inference.
+
+    dirichlet_concentration_prior_ : float
+        The dirichlet concentration of each component on the weight
+        distribution (Dirichlet). The higher concentration puts more mass in
+        the center and will lead to more components being active, while a lower
+        concentration parameter will lead to more mass at the edge of the
+        simplex.
+
+    dirichlet_concentration_ : array-like, shape (`n_components`, )
+        The dirichlet concentration of each component on the weight
+        distribution (Dirichlet).
+
+    mean_precision_prior : float
+        The precision prior on the mean distribution (Gaussian).
+        Controls the extend to where means can be placed.
+        Smaller values concentrates the means of each clusters around
+        `mean_prior`.
+
+    mean_precision_ : array-like, shape (`n_components`, )
+        The precision of each components on the mean distribution (Gaussian).
+
+    means_prior_ : array-like, shape (`n_features`,)
+        The prior on the mean distribution (Gaussian).
+
+    degrees_of_freedom_prior_ : float
+        The prior of the number of degrees of freedom on the covariance
+        distributions (Wishart).
+
+    degrees_of_freedom_ : array-like, shape (`n_components`,)
+        The number of degrees of freedom of each components in the model.
+
+    covariance_prior_ : float or array-like
+        The prior on the covariance distribution (Wishart).
+        The shape depends on `covariance_type`::
+            (`n_features`, `n_features`) if 'full',
+            (`n_features`, `n_features`) if 'tied',
+            (`n_features`)               if 'diag',
+            float                        if 'spherical'
+
+    See Also
+    --------
+    GaussianMixture : Finite Gaussian mixture fit with EM.
+    """
+
+    def __init__(self, n_components=1, covariance_type='full', tol=1e-3,
+                 reg_covar=1e-6, max_iter=100, n_init=1, init_params='kmeans',
+                 dirichlet_concentration_prior=None,
+                 mean_precision_prior=None, mean_prior=None,
+                 degrees_of_freedom_prior=None, covariance_prior=None,
+                 random_state=None, warm_start=False, verbose=0,
+                 verbose_interval=20):
+        super(BayesianGaussianMixture, self).__init__(
+            n_components=n_components, tol=tol, reg_covar=reg_covar,
+            max_iter=max_iter, n_init=n_init, init_params=init_params,
+            random_state=random_state, warm_start=warm_start,
+            verbose=verbose, verbose_interval=verbose_interval)
+
+        self.covariance_type = covariance_type
+        self.dirichlet_concentration_prior = dirichlet_concentration_prior
+        self.mean_precision_prior = mean_precision_prior
+        self.mean_prior = mean_prior
+        self.degrees_of_freedom_prior = degrees_of_freedom_prior
+        self.covariance_prior = covariance_prior
+
+    def _check_parameters(self, X):
+        """Check that the parameters are well defined.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+        """
+        if self.covariance_type not in ['spherical', 'tied', 'diag', 'full']:
+            raise ValueError("Invalid value for 'covariance_type': %s "
+                             "'covariance_type' should be in "
+                             "['spherical', 'tied', 'diag', 'full']"
+                             % self.covariance_type)
+        self._check_weights_parameters()
+        self._check_means_parameters(X)
+        self._check_precision_parameters(X)
+        self._checkcovariance_prior_parameter(X)
+
+    def _check_weights_parameters(self):
+        """Check the parameter of the Dirichlet distribution."""
+        if self.dirichlet_concentration_prior is None:
+            self.dirichlet_concentration_prior_ = 1. / self.n_components
+        elif self.dirichlet_concentration_prior > 0.:
+            self.dirichlet_concentration_prior_ = (
+                self.dirichlet_concentration_prior)
+        else:
+            raise ValueError("The parameter 'dirichlet_concentration_prior' "
+                             "should be greater than 0., but got %.3f."
+                             % self.dirichlet_concentration_prior)
+
+    def _check_means_parameters(self, X):
+        """Check the parameters of the Gaussian distribution.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+        """
+        _, n_features = X.shape
+
+        if self.mean_precision_prior is None:
+            self.mean_precision_prior_ = 1.
+        elif self.mean_precision_prior > 0.:
+            self.mean_precision_prior_ = self.mean_precision_prior
+        else:
+            raise ValueError("The parameter 'mean_precision_prior' should be "
+                             "greater than 0., but got %.3f."
+                             % self.mean_precision_prior)
+
+        if self.mean_prior is None:
+            self.mean_prior_ = X.mean(axis=0)
+        else:
+            self.mean_prior_ = check_array(self.mean_prior,
+                                           dtype=[np.float64, np.float32],
+                                           ensure_2d=False)
+            _check_shape(self.mean_prior_, (n_features, ), 'means')
+
+    def _check_precision_parameters(self, X):
+        """Check the prior parameters of the precision distribution.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+        """
+        _, n_features = X.shape
+
+        if self.degrees_of_freedom_prior is None:
+            self.degrees_of_freedom_prior_ = n_features
+        elif self.degrees_of_freedom_prior > n_features - 1.:
+            self.degrees_of_freedom_prior_ = self.degrees_of_freedom_prior
+        else:
+            raise ValueError("The parameter 'degrees_of_freedom_prior' "
+                             "should be greater than %d, but got %.3f."
+                             % (n_features - 1, self.degrees_of_freedom_prior))
+
+    def _checkcovariance_prior_parameter(self, X):
+        """Check the `covariance_prior_`.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+        """
+        _, n_features = X.shape
+
+        if self.covariance_prior is None:
+            self.covariance_prior_ = {
+                'full': np.atleast_2d(np.cov(X.T)),
+                'tied': np.atleast_2d(np.cov(X.T)),
+                'diag': np.var(X, axis=0, ddof=1),
+                'spherical': np.var(X, axis=0, ddof=1).mean()
+            }[self.covariance_type]
+
+        elif self.covariance_type in ['full', 'tied']:
+            self.covariance_prior_ = check_array(
+                self.covariance_prior, dtype=[np.float64, np.float32],
+                ensure_2d=False)
+            _check_shape(self.covariance_prior_, (n_features, n_features),
+                         '%s covariance_prior' % self.covariance_type)
+            _check_precision_matrix(self.covariance_prior_,
+                                    self.covariance_type)
+        elif self.covariance_type == 'diag':
+            self.covariance_prior_ = check_array(
+                self.covariance_prior, dtype=[np.float64, np.float32],
+                ensure_2d=False)
+            _check_shape(self.covariance_prior_, (n_features,),
+                         '%s covariance_prior' % self.covariance_type)
+            _check_precision_positivity(self.covariance_prior_,
+                                        self.covariance_type)
+        # spherical case
+        elif self.covariance_prior > 0.:
+            self.covariance_prior_ = self.covariance_prior
+        else:
+            raise ValueError("The parameter 'spherical covariance_prior' "
+                             "should be greater than 0., but got %.3f."
+                             % self.covariance_prior)
+
+    def _initialize(self, X, resp):
+        """Initialization of the mixture parameters.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+
+        resp : array-like, shape (n_samples, n_components)
+        """
+        nk, xk, sk = _estimate_gaussian_parameters(X, resp, self.reg_covar,
+                                                   self.covariance_type)
+
+        self._estimate_weights(nk)
+        self._estimate_means(nk, xk)
+        self._estimate_precisions(nk, xk, sk)
+
+    def _estimate_weights(self, nk):
+        """Estimate the parameters of the Dirichlet distribution.
+
+        Parameters
+        ----------
+        nk : array-like, shape (n_components,)
+        """
+        self.dirichlet_concentration_ = (
+            self.dirichlet_concentration_prior_ + nk)
+
+    def _estimate_means(self, nk, xk):
+        """Estimate the parameters of the Gaussian distribution.
+
+        Parameters
+        ----------
+        nk : array-like, shape (n_components,)
+
+        xk : array-like, shape (n_components, n_features)
+        """
+        self.mean_precision_ = self.mean_precision_prior_ + nk
+        self.means_ = ((self.mean_precision_prior_ * self.mean_prior_ +
+                        nk[:, np.newaxis] * xk) /
+                       self.mean_precision_[:, np.newaxis])
+
+    def _estimate_precisions(self, nk, xk, sk):
+        """Estimate the precisions parameters of the precision distribution.
+
+        Parameters
+        ----------
+        nk : array-like, shape (n_components,)
+
+        xk : array-like, shape (n_components, n_features)
+
+        sk : array-like
+            The shape depends of `covariance_type`:
+            'full' : (n_components, n_features, n_features)
+            'tied' : (n_features, n_features)
+            'diag' : (n_components, n_features)
+            'spherical' : (n_components,)
+        """
+        {"full": self._estimate_wishart_full,
+         "tied": self._estimate_wishart_tied,
+         "diag": self._estimate_wishart_diag,
+         "spherical": self._estimate_wishart_spherical
+         }[self.covariance_type](nk, xk, sk)
+
+        self.precisions_cholesky_ = _compute_precision_cholesky(
+            self.covariances_, self.covariance_type)
+
+    def _estimate_wishart_full(self, nk, xk, sk):
+        """Estimate the full Wishart distribution parameters.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+
+        nk : array-like, shape (n_components,)
+
+        xk : array-like, shape (n_components, n_features)
+
+        sk : array-like, shape (n_components, n_features, n_features)
+        """
+        _, n_features = xk.shape
+
+        # Warning : in some Bishop book, there is a typo on the formula 10.63
+        # `degrees_of_freedom_k = degrees_of_freedom_0 + Nk` is
+        # the correct formula
+        self.degrees_of_freedom_ = self.degrees_of_freedom_prior_ + nk
+
+        self.covariances_ = np.empty((self.n_components, n_features,
+                                      n_features))
+
+        for k in range(self.n_components):
+            diff = xk[k] - self.mean_prior_
+            self.covariances_[k] = (self.covariance_prior_ + nk[k] * sk[k] +
+                                    nk[k] * self.mean_precision_prior_ /
+                                    self.mean_precision_[k] * np.outer(diff,
+                                                                       diff))
+
+        # Contrary to the original bishop book, we normalize the covariances
+        self.covariances_ /= (
+            self.degrees_of_freedom_[:, np.newaxis, np.newaxis])
+
+    def _estimate_wishart_tied(self, nk, xk, sk):
+        """Estimate the tied Wishart distribution parameters.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+
+        nk : array-like, shape (n_components,)
+
+        xk : array-like, shape (n_components, n_features)
+
+        sk : array-like, shape (n_features, n_features)
+        """
+        _, n_features = xk.shape
+
+        # Warning : in some Bishop book, there is a typo on the formula 10.63
+        # `degrees_of_freedom_k = degrees_of_freedom_0 + Nk`
+        # is the correct formula
+        self.degrees_of_freedom_ = (
+            self.degrees_of_freedom_prior_ + nk.sum() / self.n_components)
+
+        diff = xk - self.mean_prior_
+        self.covariances_ = (
+            self.covariance_prior_ + sk * nk.sum() / self.n_components +
+            self.mean_precision_prior_ / self.n_components * np.dot(
+                (nk / self.mean_precision_) * diff.T, diff))
+
+        # Contrary to the original bishop book, we normalize the covariances
+        self.covariances_ /= self.degrees_of_freedom_
+
+    def _estimate_wishart_diag(self, nk, xk, sk):
+        """Estimate the diag Wishart distribution parameters.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+
+        nk : array-like, shape (n_components,)
+
+        xk : array-like, shape (n_components, n_features)
+
+        sk : array-like, shape (n_components, n_features)
+        """
+        _, n_features = xk.shape
+
+        # Warning : in some Bishop book, there is a typo on the formula 10.63
+        # `degrees_of_freedom_k = degrees_of_freedom_0 + Nk`
+        # is the correct formula
+        self.degrees_of_freedom_ = self.degrees_of_freedom_prior_ + nk
+
+        diff = xk - self.mean_prior_
+        self.covariances_ = (
+            self.covariance_prior_ + nk[:, np.newaxis] * (
+                sk + (self.mean_precision_prior_ /
+                      self.mean_precision_)[:, np.newaxis] * np.square(diff)))
+
+        # Contrary to the original bishop book, we normalize the covariances
+        self.covariances_ /= self.degrees_of_freedom_[:, np.newaxis]
+
+    def _estimate_wishart_spherical(self, nk, xk, sk):
+        """Estimate the spherical Wishart distribution parameters.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+
+        nk : array-like, shape (n_components,)
+
+        xk : array-like, shape (n_components, n_features)
+
+        sk : array-like, shape (n_components,)
+        """
+        _, n_features = xk.shape
+
+        # Warning : in some Bishop book, there is a typo on the formula 10.63
+        # `degrees_of_freedom_k = degrees_of_freedom_0 + Nk`
+        # is the correct formula
+        self.degrees_of_freedom_ = self.degrees_of_freedom_prior_ + nk
+
+        diff = xk - self.mean_prior_
+        self.covariances_ = (
+            self.covariance_prior_ + nk * (
+                sk + self.mean_precision_prior_ / self.mean_precision_ *
+                np.mean(np.square(diff), 1)))
+
+        # Contrary to the original bishop book, we normalize the covariances
+        self.covariances_ /= self.degrees_of_freedom_
+
+    def _check_is_fitted(self):
+        check_is_fitted(self, ['dirichlet_concentration_', 'mean_precision_',
+                               'means_', 'degrees_of_freedom_',
+                               'covariances_', 'precisions_',
+                               'precisions_cholesky_'])
+
+    def _m_step(self, X, log_resp):
+        """M step.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+
+        log_resp : array-like, shape (n_samples, n_components)
+            Logarithm of the posterior probabilities (or responsibilities) of
+            the point of each sample in X.
+        """
+        n_samples, _ = X.shape
+
+        nk, xk, sk = _estimate_gaussian_parameters(
+            X, np.exp(log_resp), self.reg_covar, self.covariance_type)
+        self._estimate_weights(nk)
+        self._estimate_means(nk, xk)
+        self._estimate_precisions(nk, xk, sk)
+
+    def _estimate_log_weights(self):
+        return (digamma(self.dirichlet_concentration_) -
+                digamma(np.sum(self.dirichlet_concentration_)))
+
+    def _estimate_log_prob(self, X):
+        _, n_features = X.shape
+        # We remove `n_features * np.log(self.degrees_of_freedom_)` because
+        # the precision matrix is normalized
+        log_gauss = (_estimate_log_gaussian_prob(
+            X, self.means_, self.precisions_cholesky_, self.covariance_type) -
+            .5 * n_features * np.log(self.degrees_of_freedom_))
+
+        log_lambda = n_features * np.log(2.) + np.sum(digamma(
+            .5 * (self.degrees_of_freedom_ -
+                  np.arange(0, n_features)[:, np.newaxis])), 0)
+
+        return log_gauss + .5 * (log_lambda -
+                                 n_features / self.mean_precision_)
+
+    def _compute_lower_bound(self, log_resp, log_prob_norm):
+        """Estimate the lower bound of the model.
+
+        The lower bound on the likelihood (of the training data with respect to
+        the model) is used to detect the convergence and has to decrease at
+        each iteration.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+
+        log_resp : array, shape (n_samples, n_components)
+            Logarithm of the posterior probabilities (or responsibilities) of
+            the point of each sample in X.
+
+        log_prob_norm : float
+            Logarithm of the probability of each sample in X.
+
+        Returns
+        -------
+        lower_bound : float
+        """
+        # Contrary to the original formula, we have done some simplification
+        # and removed all the constant terms.
+        n_features, = self.mean_prior_.shape
+
+        # We removed `.5 * n_features * np.log(self.degrees_of_freedom_)`
+        # because the precision matrix is normalized.
+        log_det_precisions_chol = (_compute_log_det_cholesky(
+            self.precisions_cholesky_, self.covariance_type, n_features) -
+            .5 * n_features * np.log(self.degrees_of_freedom_))
+
+        if self.covariance_type == 'tied':
+            log_wishart = self.n_components * np.float64(_log_wishart_norm(
+                self.degrees_of_freedom_, log_det_precisions_chol, n_features))
+        else:
+            log_wishart = np.sum(_log_wishart_norm(
+                self.degrees_of_freedom_, log_det_precisions_chol, n_features))
+
+        return (-np.sum(np.exp(log_resp) * log_resp) - log_wishart -
+                _log_dirichlet_norm(self.dirichlet_concentration_) -
+                0.5 * n_features * np.sum(np.log(self.mean_precision_)))
+
+    def _get_parameters(self):
+        return (self.dirichlet_concentration_,
+                self.mean_precision_, self.means_,
+                self.degrees_of_freedom_, self.covariances_,
+                self.precisions_cholesky_)
+
+    def _set_parameters(self, params):
+        (self.dirichlet_concentration_, self.mean_precision_, self.means_,
+         self.degrees_of_freedom_, self.covariances_,
+         self.precisions_cholesky_) = params
+
+        # Attributes computation
+        self. weights_ = (self.dirichlet_concentration_ /
+                          np.sum(self.dirichlet_concentration_))
+
+        if self.covariance_type == 'full':
+            self.precisions_ = np.array([
+                np.dot(prec_chol, prec_chol.T)
+                for prec_chol in self.precisions_cholesky_])
+
+        elif self.covariance_type == 'tied':
+            self.precisions_ = np.dot(self.precisions_cholesky_,
+                                      self.precisions_cholesky_.T)
+        else:
+            self.precisions_ = self.precisions_cholesky_ ** 2
diff --git a/sklearn/mixture/dpgmm.py b/sklearn/mixture/dpgmm.py
index 700b63fdad31..ee2bd6491116 100644
--- a/sklearn/mixture/dpgmm.py
+++ b/sklearn/mixture/dpgmm.py
@@ -631,8 +631,8 @@ def __init__(self, n_components=1, covariance_type='diag', alpha=1.0,
 
 
 @deprecated("The VBGMM class is not working correctly and it's better "
-            "to not use it. VBGMM is deprecated in 0.18 and "
-            "will be removed in 0.20.")
+            "to use sklearn.mixture.BayesianGaussianMixture class instead. "
+            "VBGMM is deprecated in 0.18 and will be removed in 0.20.")
 class VBGMM(_DPGMMBase):
     """Variational Inference for the Gaussian Mixture Model
 
@@ -725,7 +725,36 @@ def __init__(self, n_components=1, covariance_type='diag', alpha=1.0,
             n_components, covariance_type, random_state=random_state,
             tol=tol, verbose=verbose, min_covar=min_covar,
             n_iter=n_iter, params=params, init_params=init_params)
-        self.alpha = float(alpha) / n_components
+        self.alpha = alpha
+
+    def _fit(self, X, y=None):
+        """Estimate model parameters with the variational algorithm.
+
+        For a full derivation and description of the algorithm see
+        doc/modules/dp-derivation.rst
+        or
+        http://scikit-learn.org/stable/modules/dp-derivation.html
+
+        A initialization step is performed before entering the EM
+        algorithm. If you want to avoid this step, set the keyword
+        argument init_params to the empty string '' when creating
+        the object. Likewise, if you just would like to do an
+        initialization, set n_iter=0.
+
+        Parameters
+        ----------
+        X : array_like, shape (n, n_features)
+            List of n_features-dimensional data points.  Each row
+            corresponds to a single data point.
+
+        Returns
+        -------
+        responsibilities : array, shape (n_samples, n_components)
+            Posterior probabilities of each mixture component for each
+            observation.
+        """
+        self.alpha_ = float(self.alpha) / self.n_components
+        return super(VBGMM, self)._fit(X, y)
 
     def score_samples(self, X):
         """Return the likelihood of the data under the model.
@@ -772,10 +801,10 @@ def score_samples(self, X):
 
     def _update_concentration(self, z):
         for i in range(self.n_components):
-            self.gamma_[i] = self.alpha + np.sum(z.T[i])
+            self.gamma_[i] = self.alpha_ + np.sum(z.T[i])
 
     def _initialize_gamma(self):
-        self.gamma_ = self.alpha * np.ones(self.n_components)
+        self.gamma_ = self.alpha_ * np.ones(self.n_components)
 
     def _bound_proportions(self, z):
         logprior = 0.
@@ -789,10 +818,10 @@ def _bound_proportions(self, z):
     def _bound_concentration(self):
         logprior = 0.
         logprior = gammaln(np.sum(self.gamma_)) - gammaln(self.n_components
-                                                          * self.alpha)
-        logprior -= np.sum(gammaln(self.gamma_) - gammaln(self.alpha))
+                                                          * self.alpha_)
+        logprior -= np.sum(gammaln(self.gamma_) - gammaln(self.alpha_))
         sg = digamma(np.sum(self.gamma_))
-        logprior += np.sum((self.gamma_ - self.alpha)
+        logprior += np.sum((self.gamma_ - self.alpha_)
                            * (digamma(self.gamma_) - sg))
         return logprior
 
diff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py
index 84d36defd48f..749af6c82fd6 100644
--- a/sklearn/mixture/gaussian_mixture.py
+++ b/sklearn/mixture/gaussian_mixture.py
@@ -2,6 +2,7 @@
 
 # Author: Wei Xue <xuewei4d@gmail.com>
 # Modified by Thierry Guillemot <thierry.guillemot.work@gmail.com>
+# License: BSD 3 clause
 
 import numpy as np
 
@@ -11,6 +12,7 @@
 from ..externals.six.moves import zip
 from ..utils import check_array
 from ..utils.validation import check_is_fitted
+from ..utils.extmath import row_norms
 
 
 ###############################################################################
@@ -187,11 +189,10 @@ def _estimate_gaussian_covariances_tied(resp, X, nk, means, reg_covar):
     covariance : array, shape (n_features, n_features)
         The tied covariance matrix of the components.
     """
-    n_samples, _ = X.shape
     avg_X2 = np.dot(X.T, X)
     avg_means2 = np.dot(nk * means.T, means)
     covariance = avg_X2 - avg_means2
-    covariance /= n_samples
+    covariance /= nk.sum()
     covariance.flat[::len(covariance) + 1] += reg_covar
     return covariance
 
@@ -304,8 +305,9 @@ def _compute_precision_cholesky(covariances, covariance_type):
         components. The shape depends of the covariance_type.
     """
     estimate_precision_error_message = (
-        "The algorithm has diverged because of too few samples per "
-        "components. Try to decrease the number of components, "
+        "Fitting the mixture model failed because some components have "
+        "ill-defined empirical covariance (for instance caused by singleton "
+        "or collapsed samples). Try to decrease the number of components, "
         "or increase reg_covar.")
 
     if covariance_type in 'full':
@@ -336,62 +338,48 @@ def _compute_precision_cholesky(covariances, covariance_type):
 
 ###############################################################################
 # Gaussian mixture probability estimators
-def _estimate_log_gaussian_prob_full(X, means, precisions_chol):
-    """Estimate the log Gaussian probability for 'full' precision.
+def _compute_log_det_cholesky(matrix_chol, covariance_type, n_features):
+    """Compute the log-det of the cholesky decomposition of matrices.
 
     Parameters
     ----------
-    X : array-like, shape (n_samples, n_features)
+    matrix_chol : array-like,
+        Cholesky decompositions of the matrices.
+        'full' : shape of (n_components, n_features, n_features)
+        'tied' : shape of (n_features, n_features)
+        'diag' : shape of (n_components, n_features)
+        'spherical' : shape of (n_components,)
 
-    means : array-like, shape (n_components, n_features)
+    covariance_type : {'full', 'tied', 'diag', 'spherical'}
 
-    precisions_chol : array-like, shape (n_components, n_features, n_features)
-        Cholesky decompositions of the precision matrices.
+    n_features : int
+        Number of features.
 
     Returns
     -------
-    log_prob : array, shape (n_samples, n_components)
+    log_det_precision_chol : array-like, shape (n_components,)
+        The determinant of the precision matrix for each component.
     """
-    n_samples, n_features = X.shape
-    n_components, _ = means.shape
-    log_prob = np.empty((n_samples, n_components))
-    for k, (mu, prec_chol) in enumerate(zip(means, precisions_chol)):
-        log_det = -2. * np.sum(np.log(np.diagonal(prec_chol)))
-        y = np.dot(X - mu, prec_chol)
-        log_prob[:, k] = -.5 * (n_features * np.log(2. * np.pi) + log_det +
-                                np.sum(np.square(y), axis=1))
-    return log_prob
+    if covariance_type == 'full':
+        n_components, _, _ = matrix_chol.shape
+        log_det_chol = (np.sum(np.log(
+            matrix_chol.reshape(
+                n_components, -1)[:, ::n_features + 1]), 1))
 
+    elif covariance_type == 'tied':
+        log_det_chol = (np.sum(np.log(np.diag(matrix_chol))))
 
-def _estimate_log_gaussian_prob_tied(X, means, precision_chol):
-    """Estimate the log Gaussian probability for 'tied' precision.
+    elif covariance_type == 'diag':
+        log_det_chol = (np.sum(np.log(matrix_chol), axis=1))
 
-    Parameters
-    ----------
-    X : array-like, shape (n_samples, n_features)
+    else:
+        log_det_chol = n_features * (np.log(matrix_chol))
 
-    means : array-like, shape (n_components, n_features)
+    return log_det_chol
 
-    precision_chol : array-like, shape (n_features, n_features)
-        Cholesky decomposition of the precision matrix.
-
-    Returns
-    -------
-    log_prob : array-like, shape (n_samples, n_components)
-    """
-    n_samples, n_features = X.shape
-    n_components, _ = means.shape
-    log_prob = np.empty((n_samples, n_components))
-    log_det = -2. * np.sum(np.log(np.diagonal(precision_chol)))
-    for k, mu in enumerate(means):
-        y = np.dot(X - mu, precision_chol)
-        log_prob[:, k] = np.sum(np.square(y), axis=1)
-    log_prob = -.5 * (n_features * np.log(2. * np.pi) + log_det + log_prob)
-    return log_prob
 
-
-def _estimate_log_gaussian_prob_diag(X, means, precisions_chol):
-    """Estimate the log Gaussian probability for 'diag' precision.
+def _estimate_log_gaussian_prob(X, means, precisions_chol, covariance_type):
+    """Estimate the log Gaussian probability.
 
     Parameters
     ----------
@@ -399,47 +387,49 @@ def _estimate_log_gaussian_prob_diag(X, means, precisions_chol):
 
     means : array-like, shape (n_components, n_features)
 
-    precisions_chol : array-like, shape (n_components, n_features)
+    precisions_chol : array-like,
         Cholesky decompositions of the precision matrices.
+        'full' : shape of (n_components, n_features, n_features)
+        'tied' : shape of (n_features, n_features)
+        'diag' : shape of (n_components, n_features)
+        'spherical' : shape of (n_components,)
 
-    Returns
-    -------
-    log_prob : array-like, shape (n_samples, n_components)
-    """
-    n_samples, n_features = X.shape
-    precisions = precisions_chol ** 2
-    log_prob = -.5 * (n_features * np.log(2. * np.pi) -
-                      np.sum(np.log(precisions), 1) +
-                      np.sum((means ** 2 * precisions), 1) -
-                      2. * np.dot(X, (means * precisions).T) +
-                      np.dot(X ** 2, precisions.T))
-    return log_prob
-
-
-def _estimate_log_gaussian_prob_spherical(X, means, precisions_chol):
-    """Estimate the log Gaussian probability for 'spherical' precision.
-
-    Parameters
-    ----------
-    X : array-like, shape (n_samples, n_features)
-
-    means : array-like, shape (n_components, n_features)
-
-    precisions_chol : array-like, shape (n_components, )
-        Cholesky decompositions of the precision matrices.
+    covariance_type : {'full', 'tied', 'diag', 'spherical'}
 
     Returns
     -------
-    log_prob : array-like, shape (n_samples, n_components)
+    log_prob : array, shape (n_samples, n_components)
     """
     n_samples, n_features = X.shape
-    precisions = precisions_chol ** 2
-    log_prob = -.5 * (n_features * np.log(2 * np.pi) -
-                      n_features * np.log(precisions) +
-                      np.sum(means ** 2, 1) * precisions -
-                      2 * np.dot(X, means.T * precisions) +
-                      np.outer(np.sum(X ** 2, axis=1), precisions))
-    return log_prob
+    n_components, _ = means.shape
+    # det(precision_chol) is half of det(precision)
+    log_det = _compute_log_det_cholesky(
+        precisions_chol, covariance_type, n_features)
+
+    if covariance_type == 'full':
+        log_prob = np.empty((n_samples, n_components))
+        for k, (mu, prec_chol) in enumerate(zip(means, precisions_chol)):
+            y = np.dot(X, prec_chol) - np.dot(mu, prec_chol)
+            log_prob[:, k] = np.sum(np.square(y), axis=1)
+
+    elif covariance_type == 'tied':
+        log_prob = np.empty((n_samples, n_components))
+        for k, mu in enumerate(means):
+            y = np.dot(X, precisions_chol) - np.dot(mu, precisions_chol)
+            log_prob[:, k] = np.sum(np.square(y), axis=1)
+
+    elif covariance_type == 'diag':
+        precisions = precisions_chol ** 2
+        log_prob = (np.sum((means ** 2 * precisions), 1) -
+                    2. * np.dot(X, (means * precisions).T) +
+                    np.dot(X ** 2, precisions.T))
+
+    elif covariance_type == 'spherical':
+        precisions = precisions_chol ** 2
+        log_prob = (np.sum(means ** 2, 1) * precisions -
+                    2 * np.dot(X, means.T * precisions) +
+                    np.outer(row_norms(X, squared=True), precisions))
+    return -.5 * (n_features * np.log(2 * np.pi) + log_prob) + log_det
 
 
 class GaussianMixture(BaseMixture):
@@ -465,7 +455,7 @@ class GaussianMixture(BaseMixture):
 
     tol : float, defaults to 1e-3.
         The convergence threshold. EM iterations will stop when the
-        log_likelihood average gain is below this threshold.
+        lower bound average gain is below this threshold.
 
     reg_covar : float, defaults to 0.
         Non-negative regularization added to the diagonal of covariance.
@@ -475,7 +465,7 @@ class GaussianMixture(BaseMixture):
         The number of EM iterations to perform.
 
     n_init : int, defaults to 1.
-        The number of initializations to perform. The best results is kept.
+        The number of initializations to perform. The best results are kept.
 
     init_params : {'kmeans', 'random'}, defaults to 'kmeans'.
         The method used to initialize the weights, the means and the
@@ -563,6 +553,14 @@ class GaussianMixture(BaseMixture):
 
     n_iter_ : int
         Number of step used by the best fit of EM to reach the convergence.
+
+    lower_bound_ : float
+        Log-likelihood of the best fit of EM.
+
+    See Also
+    --------
+    BayesianGaussianMixture : Finite gaussian mixture model fit with a
+        variational algorithm.
     """
 
     def __init__(self, n_components=1, covariance_type='full', tol=1e-3,
@@ -637,38 +635,45 @@ def _initialize(self, X, resp):
         else:
             self.precisions_cholesky_ = self.precisions_init
 
-    def _e_step(self, X):
-        log_prob_norm, _, log_resp = self._estimate_log_prob_resp(X)
-        return np.mean(log_prob_norm), np.exp(log_resp)
+    def _m_step(self, X, log_resp):
+        """M step.
 
-    def _m_step(self, X, resp):
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+
+        log_resp : array-like, shape (n_samples, n_components)
+            Logarithm of the posterior probabilities (or responsibilities) of
+            the point of each sample in X.
+        """
         n_samples, _ = X.shape
         self.weights_, self.means_, self.covariances_ = (
-            _estimate_gaussian_parameters(X, resp, self.reg_covar,
+            _estimate_gaussian_parameters(X, np.exp(log_resp), self.reg_covar,
                                           self.covariance_type))
         self.weights_ /= n_samples
         self.precisions_cholesky_ = _compute_precision_cholesky(
             self.covariances_, self.covariance_type)
 
     def _estimate_log_prob(self, X):
-        return {"full": _estimate_log_gaussian_prob_full,
-                "tied": _estimate_log_gaussian_prob_tied,
-                "diag": _estimate_log_gaussian_prob_diag,
-                "spherical": _estimate_log_gaussian_prob_spherical
-                }[self.covariance_type](X, self.means_,
-                                        self.precisions_cholesky_)
+        return _estimate_log_gaussian_prob(
+            X, self.means_, self.precisions_cholesky_, self.covariance_type)
 
     def _estimate_log_weights(self):
         return np.log(self.weights_)
 
+    def _compute_lower_bound(self, _, log_prob_norm):
+        return log_prob_norm
+
     def _check_is_fitted(self):
         check_is_fitted(self, ['weights_', 'means_', 'precisions_cholesky_'])
 
     def _get_parameters(self):
-        return self.weights_, self.means_, self.precisions_cholesky_
+        return (self.weights_, self.means_, self.covariances_,
+                self.precisions_cholesky_)
 
     def _set_parameters(self, params):
-        self.weights_, self.means_, self.precisions_cholesky_ = params
+        (self.weights_, self.means_, self.covariances_,
+         self.precisions_cholesky_) = params
 
         # Attributes computation
         _, n_features = self.means_.shape
diff --git a/sklearn/mixture/gmm.py b/sklearn/mixture/gmm.py
index 034bad562f1f..e5c8734c770b 100644
--- a/sklearn/mixture/gmm.py
+++ b/sklearn/mixture/gmm.py
@@ -682,7 +682,7 @@ def _log_multivariate_normal_density_spherical(X, means, covars):
     cv = covars.copy()
     if covars.ndim == 1:
         cv = cv[:, np.newaxis]
-    if covars.shape[1] == 1:
+    if cv.shape[1] == 1:
         cv = np.tile(cv, (1, X.shape[-1]))
     return _log_multivariate_normal_density_diag(X, means, cv)
 
diff --git a/sklearn/mixture/tests/test_bayesian_mixture.py b/sklearn/mixture/tests/test_bayesian_mixture.py
new file mode 100644
index 000000000000..3003a9444669
--- /dev/null
+++ b/sklearn/mixture/tests/test_bayesian_mixture.py
@@ -0,0 +1,359 @@
+# Author: Wei Xue <xuewei4d@gmail.com>
+#         Thierry Guillemot <thierry.guillemot.work@gmail.com>
+# License: BSD 3 clause
+
+import numpy as np
+from scipy.special import gammaln
+
+from sklearn.utils.testing import assert_raise_message
+from sklearn.utils.testing import assert_almost_equal
+
+from sklearn.mixture.bayesian_mixture import _log_dirichlet_norm
+from sklearn.mixture.bayesian_mixture import _log_wishart_norm
+
+from sklearn.mixture import BayesianGaussianMixture
+
+from sklearn.mixture.tests.test_gaussian_mixture import RandomData
+from sklearn.exceptions import ConvergenceWarning
+from sklearn.utils.testing import assert_greater_equal, ignore_warnings
+
+
+COVARIANCE_TYPE = ['full', 'tied', 'diag', 'spherical']
+
+
+def test_log_dirichlet_norm():
+    rng = np.random.RandomState(0)
+
+    dirichlet_concentration = rng.rand(2)
+    expected_norm = (gammaln(np.sum(dirichlet_concentration)) -
+                     np.sum(gammaln(dirichlet_concentration)))
+    predected_norm = _log_dirichlet_norm(dirichlet_concentration)
+
+    assert_almost_equal(expected_norm, predected_norm)
+
+
+def test_log_wishart_norm():
+    rng = np.random.RandomState(0)
+
+    n_components, n_features = 5, 2
+    degrees_of_freedom = np.abs(rng.rand(n_components)) + 1.
+    log_det_precisions_chol = n_features * np.log(range(2, 2 + n_components))
+
+    expected_norm = np.empty(5)
+    for k, (degrees_of_freedom_k, log_det_k) in enumerate(
+            zip(degrees_of_freedom, log_det_precisions_chol)):
+        expected_norm[k] = -(
+            degrees_of_freedom_k * (log_det_k + .5 * n_features * np.log(2.)) +
+            np.sum(gammaln(.5 * (degrees_of_freedom_k -
+                                 np.arange(0, n_features)[:, np.newaxis])), 0))
+    predected_norm = _log_wishart_norm(degrees_of_freedom,
+                                       log_det_precisions_chol, n_features)
+
+    assert_almost_equal(expected_norm, predected_norm)
+
+
+def test_bayesian_mixture_covariance_type():
+    rng = np.random.RandomState(0)
+    n_samples, n_features = 10, 2
+    X = rng.rand(n_samples, n_features)
+
+    covariance_type = 'bad_covariance_type'
+    bgmm = BayesianGaussianMixture(covariance_type=covariance_type,
+                                   random_state=rng)
+    assert_raise_message(ValueError,
+                         "Invalid value for 'covariance_type': %s "
+                         "'covariance_type' should be in "
+                         "['spherical', 'tied', 'diag', 'full']"
+                         % covariance_type,
+                         bgmm.fit, X)
+
+
+def test_bayesian_mixture_weights_prior_initialisation():
+    rng = np.random.RandomState(0)
+    n_samples, n_components, n_features = 10, 5, 2
+    X = rng.rand(n_samples, n_features)
+
+    # Check raise message for a bad value of dirichlet_concentration_prior
+    bad_dirichlet_concentration_prior_ = 0.
+    bgmm = BayesianGaussianMixture(
+        dirichlet_concentration_prior=bad_dirichlet_concentration_prior_,
+        random_state=0)
+    assert_raise_message(ValueError,
+                         "The parameter 'dirichlet_concentration_prior' "
+                         "should be greater than 0., but got %.3f."
+                         % bad_dirichlet_concentration_prior_,
+                         bgmm.fit, X)
+
+    # Check correct init for a given value of dirichlet_concentration_prior
+    dirichlet_concentration_prior = rng.rand()
+    bgmm = BayesianGaussianMixture(
+        dirichlet_concentration_prior=dirichlet_concentration_prior,
+        random_state=rng).fit(X)
+    assert_almost_equal(dirichlet_concentration_prior,
+                        bgmm.dirichlet_concentration_prior_)
+
+    # Check correct init for the default value of dirichlet_concentration_prior
+    bgmm = BayesianGaussianMixture(n_components=n_components,
+                                   random_state=rng).fit(X)
+    assert_almost_equal(1. / n_components, bgmm.dirichlet_concentration_prior_)
+
+
+def test_bayesian_mixture_means_prior_initialisation():
+    rng = np.random.RandomState(0)
+    n_samples, n_components, n_features = 10, 3, 2
+    X = rng.rand(n_samples, n_features)
+
+    # Check raise message for a bad value of mean_precision_prior
+    bad_mean_precision_prior_ = 0.
+    bgmm = BayesianGaussianMixture(
+        mean_precision_prior=bad_mean_precision_prior_,
+        random_state=rng)
+    assert_raise_message(ValueError,
+                         "The parameter 'mean_precision_prior' should be "
+                         "greater than 0., but got %.3f."
+                         % bad_mean_precision_prior_,
+                         bgmm.fit, X)
+
+    # Check correct init for a given value of mean_precision_prior
+    mean_precision_prior = rng.rand()
+    bgmm = BayesianGaussianMixture(
+        mean_precision_prior=mean_precision_prior,
+        random_state=rng).fit(X)
+    assert_almost_equal(mean_precision_prior, bgmm.mean_precision_prior_)
+
+    # Check correct init for the default value of mean_precision_prior
+    bgmm = BayesianGaussianMixture(random_state=rng).fit(X)
+    assert_almost_equal(1., bgmm.mean_precision_prior_)
+
+    # Check raise message for a bad shape of mean_prior
+    mean_prior = rng.rand(n_features + 1)
+    bgmm = BayesianGaussianMixture(n_components=n_components,
+                                   mean_prior=mean_prior,
+                                   random_state=rng)
+    assert_raise_message(ValueError,
+                         "The parameter 'means' should have the shape of ",
+                         bgmm.fit, X)
+
+    # Check correct init for a given value of mean_prior
+    mean_prior = rng.rand(n_features)
+    bgmm = BayesianGaussianMixture(n_components=n_components,
+                                   mean_prior=mean_prior,
+                                   random_state=rng).fit(X)
+    assert_almost_equal(mean_prior, bgmm.mean_prior_)
+
+    # Check correct init for the default value of bemean_priorta
+    bgmm = BayesianGaussianMixture(n_components=n_components,
+                                   random_state=rng).fit(X)
+    assert_almost_equal(X.mean(axis=0), bgmm.mean_prior_)
+
+
+def test_bayesian_mixture_precisions_prior_initialisation():
+    rng = np.random.RandomState(0)
+    n_samples, n_features = 10, 2
+    X = rng.rand(n_samples, n_features)
+
+    # Check raise message for a bad value of degrees_of_freedom_prior
+    bad_degrees_of_freedom_prior_ = n_features - 1.
+    bgmm = BayesianGaussianMixture(
+        degrees_of_freedom_prior=bad_degrees_of_freedom_prior_,
+        random_state=rng)
+    assert_raise_message(ValueError,
+                         "The parameter 'degrees_of_freedom_prior' should be "
+                         "greater than %d, but got %.3f."
+                         % (n_features - 1, bad_degrees_of_freedom_prior_),
+                         bgmm.fit, X)
+
+    # Check correct init for a given value of degrees_of_freedom_prior
+    degrees_of_freedom_prior = rng.rand() + n_features - 1.
+    bgmm = BayesianGaussianMixture(
+        degrees_of_freedom_prior=degrees_of_freedom_prior,
+        random_state=rng).fit(X)
+    assert_almost_equal(degrees_of_freedom_prior,
+                        bgmm.degrees_of_freedom_prior_)
+
+    # Check correct init for the default value of degrees_of_freedom_prior
+    degrees_of_freedom_prior_default = n_features
+    bgmm = BayesianGaussianMixture(
+        degrees_of_freedom_prior=degrees_of_freedom_prior_default,
+        random_state=rng).fit(X)
+    assert_almost_equal(degrees_of_freedom_prior_default,
+                        bgmm.degrees_of_freedom_prior_)
+
+    # Check correct init for a given value of covariance_prior
+    covariance_prior = {
+        'full': np.cov(X.T, bias=1) + 10,
+        'tied': np.cov(X.T, bias=1) + 5,
+        'diag': np.diag(np.atleast_2d(np.cov(X.T, bias=1))) + 3,
+        'spherical': rng.rand()}
+
+    bgmm = BayesianGaussianMixture(random_state=rng)
+    for cov_type in ['full', 'tied', 'diag', 'spherical']:
+        bgmm.covariance_type = cov_type
+        bgmm.covariance_prior = covariance_prior[cov_type]
+        bgmm.fit(X)
+        assert_almost_equal(covariance_prior[cov_type],
+                            bgmm.covariance_prior_)
+
+    # Check raise message for a bad spherical value of covariance_prior
+    bad_covariance_prior_ = -1.
+    bgmm = BayesianGaussianMixture(covariance_type='spherical',
+                                   covariance_prior=bad_covariance_prior_,
+                                   random_state=rng)
+    assert_raise_message(ValueError,
+                         "The parameter 'spherical covariance_prior' "
+                         "should be greater than 0., but got %.3f."
+                         % bad_covariance_prior_,
+                         bgmm.fit, X)
+
+    # Check correct init for the default value of covariance_prior
+    covariance_prior_default = {
+        'full': np.atleast_2d(np.cov(X.T)),
+        'tied': np.atleast_2d(np.cov(X.T)),
+        'diag': np.var(X, axis=0, ddof=1),
+        'spherical': np.var(X, axis=0, ddof=1).mean()}
+
+    bgmm = BayesianGaussianMixture(random_state=0)
+    for cov_type in ['full', 'tied', 'diag', 'spherical']:
+        bgmm.covariance_type = cov_type
+        bgmm.fit(X)
+        assert_almost_equal(covariance_prior_default[cov_type],
+                            bgmm.covariance_prior_)
+
+
+def test_bayesian_mixture_check_is_fitted():
+    rng = np.random.RandomState(0)
+    n_samples, n_features = 10, 2
+
+    # Check raise message
+    bgmm = BayesianGaussianMixture(random_state=rng)
+    X = rng.rand(n_samples, n_features)
+    assert_raise_message(ValueError,
+                         'This BayesianGaussianMixture instance is not '
+                         'fitted yet.', bgmm.score, X)
+
+
+def test_bayesian_mixture_weights():
+    rng = np.random.RandomState(0)
+    n_samples, n_features = 10, 2
+
+    X = rng.rand(n_samples, n_features)
+    bgmm = BayesianGaussianMixture(random_state=rng).fit(X)
+
+    # Check the weights values
+    expected_weights = (bgmm.dirichlet_concentration_ /
+                        np.sum(bgmm.dirichlet_concentration_))
+    predected_weights = bgmm.weights_
+
+    assert_almost_equal(expected_weights, predected_weights)
+
+    # Check the weights sum = 1
+    assert_almost_equal(np.sum(bgmm.weights_), 1.0)
+
+
+@ignore_warnings(category=ConvergenceWarning)
+def test_monotonic_likelihood():
+    # We check that each step of the each step of variational inference without
+    # regularization improve monotonically the training set of the bound
+    rng = np.random.RandomState(0)
+    rand_data = RandomData(rng, scale=7)
+    n_components = rand_data.n_components
+
+    for covar_type in COVARIANCE_TYPE:
+        X = rand_data.X[covar_type]
+        bgmm = BayesianGaussianMixture(n_components=2 * n_components,
+                                       covariance_type=covar_type,
+                                       warm_start=True, max_iter=1,
+                                       random_state=rng, tol=1e-4)
+        current_lower_bound = -np.infty
+        # Do one training iteration at a time so we can make sure that the
+        # training log likelihood increases after each iteration.
+        for _ in range(500):
+            prev_lower_bound = current_lower_bound
+            current_lower_bound = bgmm.fit(X).lower_bound_
+            assert_greater_equal(current_lower_bound, prev_lower_bound)
+
+            if bgmm.converged_:
+                break
+        assert(bgmm.converged_)
+
+
+def test_compare_covar_type():
+    # We can compare the 'full' precision with the other cov_type if we apply
+    # 1 iter of the M-step (done during _initialize_parameters).
+    rng = np.random.RandomState(0)
+    rand_data = RandomData(rng, scale=7)
+    X = rand_data.X['full']
+    n_components = rand_data.n_components
+    # Computation of the full_covariance
+    bgmm = BayesianGaussianMixture(n_components=2 * n_components,
+                                   covariance_type='full',
+                                   max_iter=1, random_state=0, tol=1e-7)
+    bgmm._check_initial_parameters(X)
+    bgmm._initialize_parameters(X)
+    full_covariances = (bgmm.covariances_ *
+                        bgmm.degrees_of_freedom_[:, np.newaxis, np.newaxis])
+
+    # Check tied_covariance = mean(full_covariances, 0)
+    bgmm = BayesianGaussianMixture(n_components=2 * n_components,
+                                   covariance_type='tied',
+                                   max_iter=1, random_state=0, tol=1e-7)
+    bgmm._check_initial_parameters(X)
+    bgmm._initialize_parameters(X)
+
+    tied_covariance = bgmm.covariances_ * bgmm.degrees_of_freedom_
+    assert_almost_equal(tied_covariance, np.mean(full_covariances, 0))
+
+    # Check diag_covariance = diag(full_covariances)
+    bgmm = BayesianGaussianMixture(n_components=2 * n_components,
+                                   covariance_type='diag',
+                                   max_iter=1, random_state=0, tol=1e-7)
+    bgmm._check_initial_parameters(X)
+    bgmm._initialize_parameters(X)
+
+    diag_covariances = (bgmm.covariances_ *
+                        bgmm.degrees_of_freedom_[:, np.newaxis])
+    assert_almost_equal(diag_covariances,
+                        np.array([np.diag(cov) for cov in full_covariances]))
+
+    # Check spherical_covariance = np.mean(diag_covariances, 0)
+    bgmm = BayesianGaussianMixture(n_components=2 * n_components,
+                                   covariance_type='spherical',
+                                   max_iter=1, random_state=0, tol=1e-7)
+    bgmm._check_initial_parameters(X)
+    bgmm._initialize_parameters(X)
+
+    spherical_covariances = bgmm.covariances_ * bgmm.degrees_of_freedom_
+    assert_almost_equal(spherical_covariances, np.mean(diag_covariances, 1))
+
+
+@ignore_warnings(category=ConvergenceWarning)
+def test_check_covariance_precision():
+    # We check that the dot product of the covariance and the precision
+    # matrices is identity.
+    rng = np.random.RandomState(0)
+    rand_data = RandomData(rng, scale=7)
+    n_components, n_features = 2 * rand_data.n_components, 2
+
+    # Computation of the full_covariance
+    bgmm = BayesianGaussianMixture(n_components=n_components,
+                                   max_iter=100, random_state=rng, tol=1e-3,
+                                   reg_covar=0)
+    for covar_type in COVARIANCE_TYPE:
+        bgmm.covariance_type = covar_type
+        bgmm.fit(rand_data.X[covar_type])
+
+        if covar_type == 'full':
+            for covar, precision in zip(bgmm.covariances_, bgmm.precisions_):
+                assert_almost_equal(np.dot(covar, precision),
+                                    np.eye(n_features))
+        elif covar_type == 'tied':
+            assert_almost_equal(np.dot(bgmm.covariances_, bgmm.precisions_),
+                                np.eye(n_features))
+
+        elif covar_type == 'diag':
+            assert_almost_equal(bgmm.covariances_ * bgmm.precisions_,
+                                np.ones((n_components, n_features)))
+
+        else:
+            assert_almost_equal(bgmm.covariances_ * bgmm.precisions_,
+                                np.ones(n_components))
diff --git a/sklearn/mixture/tests/test_dpgmm.py b/sklearn/mixture/tests/test_dpgmm.py
index c636403db767..363d31bc29f4 100644
--- a/sklearn/mixture/tests/test_dpgmm.py
+++ b/sklearn/mixture/tests/test_dpgmm.py
@@ -183,10 +183,11 @@ class TestDPGMMWithFullCovars(unittest.TestCase, DPGMMTester):
 
 
 def test_VBGMM_deprecation():
-    assert_warns_message(
-        DeprecationWarning,
-        "The VBGMM class is not working correctly and it's better to not use "
-        "it. VBGMM is deprecated in 0.18 and will be removed in 0.20.", VBGMM)
+    assert_warns_message(DeprecationWarning, "The VBGMM class is not working "
+                         "correctly and it's better to use "
+                         "sklearn.mixture.BayesianGaussianMixture class "
+                         "instead. VBGMM is deprecated in 0.18 and will be "
+                         "removed in 0.20.", VBGMM)
 
 
 class VBGMMTester(GMMTester):
@@ -216,3 +217,12 @@ class TestVBGMMWithTiedCovars(unittest.TestCase, VBGMMTester):
 class TestVBGMMWithFullCovars(unittest.TestCase, VBGMMTester):
     covariance_type = 'full'
     setUp = GMMTester._setUp
+
+
+def test_vbgmm_no_modify_alpha():
+    alpha = 2.
+    n_components = 3
+    X, y = make_blobs(random_state=1)
+    vbgmm = VBGMM(n_components=n_components, alpha=alpha, n_iter=1)
+    assert_equal(vbgmm.alpha, alpha)
+    assert_equal(vbgmm.fit(X).alpha_, float(alpha) / n_components)
diff --git a/sklearn/mixture/tests/test_gaussian_mixture.py b/sklearn/mixture/tests/test_gaussian_mixture.py
index 9e4070fd7531..194248ac7a0d 100644
--- a/sklearn/mixture/tests/test_gaussian_mixture.py
+++ b/sklearn/mixture/tests/test_gaussian_mixture.py
@@ -1,3 +1,7 @@
+# Author: Wei Xue <xuewei4d@gmail.com>
+#         Thierry Guillemot <thierry.guillemot.work@gmail.com>
+# License: BSD 3 clauseimport warnings
+
 import sys
 import warnings
 
@@ -14,8 +18,9 @@
     _estimate_gaussian_covariances_full,
     _estimate_gaussian_covariances_tied,
     _estimate_gaussian_covariances_diag,
-    _estimate_gaussian_covariances_spherical,
-    _compute_precision_cholesky)
+    _estimate_gaussian_covariances_spherical)
+from sklearn.mixture.gaussian_mixture import _compute_precision_cholesky
+from sklearn.mixture.gaussian_mixture import _compute_log_det_cholesky
 from sklearn.exceptions import ConvergenceWarning, NotFittedError
 from sklearn.utils.extmath import fast_logdet
 from sklearn.utils.testing import assert_allclose
@@ -385,6 +390,7 @@ def test_suffstat_sk_tied():
     precs_est = linalg.inv(covars_pred_tied)
     assert_array_almost_equal(precs_est, precs_pred)
 
+
 def test_suffstat_sk_diag():
     # test against 'full' case
     rng = np.random.RandomState(0)
@@ -432,6 +438,29 @@ def test_gaussian_suffstat_sk_spherical():
                                                   'spherical')
     assert_almost_equal(covars_pred_spherical, 1. / precs_chol_pred ** 2)
 
+
+def test_compute_log_det_cholesky():
+    n_features = 2
+    rand_data = RandomData(np.random.RandomState(0))
+
+    for covar_type in COVARIANCE_TYPE:
+        covariance = rand_data.covariances[covar_type]
+
+        if covar_type == 'full':
+            predected_det = np.array([linalg.det(cov) for cov in covariance])
+        elif covar_type == 'tied':
+            predected_det = linalg.det(covariance)
+        elif covar_type == 'diag':
+            predected_det = np.array([np.prod(cov) for cov in covariance])
+        elif covar_type == 'spherical':
+            predected_det = covariance ** n_features
+
+        # We compute the cholesky decomposition of the covariance matrix
+        expected_det = _compute_log_det_cholesky(_compute_precision_cholesky(
+            covariance, covar_type), covar_type, n_features=n_features)
+        assert_array_almost_equal(expected_det, - .5 * np.log(predected_det))
+
+
 def _naive_lmvnpdf_diag(X, means, covars):
     resp = np.empty((len(X), len(means)))
     stds = np.sqrt(covars)
@@ -441,11 +470,7 @@ def _naive_lmvnpdf_diag(X, means, covars):
 
 
 def test_gaussian_mixture_log_probabilities():
-    from sklearn.mixture.gaussian_mixture import (
-        _estimate_log_gaussian_prob_full,
-        _estimate_log_gaussian_prob_tied,
-        _estimate_log_gaussian_prob_diag,
-        _estimate_log_gaussian_prob_spherical)
+    from sklearn.mixture.gaussian_mixture import _estimate_log_gaussian_prob
 
     # test aginst with _naive_lmvnpdf_diag
     rng = np.random.RandomState(0)
@@ -462,12 +487,12 @@ def test_gaussian_mixture_log_probabilities():
     # full covariances
     precs_full = np.array([np.diag(1. / np.sqrt(x)) for x in covars_diag])
 
-    log_prob = _estimate_log_gaussian_prob_full(X, means, precs_full)
+    log_prob = _estimate_log_gaussian_prob(X, means, precs_full, 'full')
     assert_array_almost_equal(log_prob, log_prob_naive)
 
     # diag covariances
     precs_chol_diag = 1. / np.sqrt(covars_diag)
-    log_prob = _estimate_log_gaussian_prob_diag(X, means, precs_chol_diag)
+    log_prob = _estimate_log_gaussian_prob(X, means, precs_chol_diag, 'diag')
     assert_array_almost_equal(log_prob, log_prob_naive)
 
     # tied
@@ -476,7 +501,7 @@ def test_gaussian_mixture_log_probabilities():
 
     log_prob_naive = _naive_lmvnpdf_diag(X, means,
                                          [covars_tied] * n_components)
-    log_prob = _estimate_log_gaussian_prob_tied(X, means, precs_tied)
+    log_prob = _estimate_log_gaussian_prob(X, means, precs_tied, 'tied')
 
     assert_array_almost_equal(log_prob, log_prob_naive)
 
@@ -486,7 +511,8 @@ def test_gaussian_mixture_log_probabilities():
     log_prob_naive = _naive_lmvnpdf_diag(X, means,
                                          [[k] * n_features for k in
                                           covars_spherical])
-    log_prob = _estimate_log_gaussian_prob_spherical(X, means, precs_spherical)
+    log_prob = _estimate_log_gaussian_prob(X, means,
+                                           precs_spherical, 'spherical')
     assert_array_almost_equal(log_prob, log_prob_naive)
 
 # skip tests on weighted_log_probabilities, log_weights
@@ -723,11 +749,11 @@ def test_warm_start():
     X = rng.rand(n_samples, n_features)
 
     # Assert the warm_start give the same result for the same number of iter
-    g = GaussianMixture(n_components=n_components, n_init=1,
-                        max_iter=2, reg_covar=0, random_state=random_state,
+    g = GaussianMixture(n_components=n_components, n_init=1, max_iter=2,
+                        reg_covar=0, random_state=random_state,
                         warm_start=False)
-    h = GaussianMixture(n_components=n_components, n_init=1,
-                        max_iter=1, reg_covar=0, random_state=random_state,
+    h = GaussianMixture(n_components=n_components, n_init=1, max_iter=1,
+                        reg_covar=0, random_state=random_state,
                         warm_start=True)
 
     with warnings.catch_warnings():
@@ -826,7 +852,7 @@ def test_monotonic_likelihood():
             warnings.simplefilter("ignore", ConvergenceWarning)
             # Do one training iteration at a time so we can make sure that the
             # training log likelihood increases after each iteration.
-            for _ in range(300):
+            for _ in range(600):
                 prev_log_likelihood = current_log_likelihood
                 try:
                     current_log_likelihood = gmm.fit(X).score(X)
@@ -838,6 +864,8 @@ def test_monotonic_likelihood():
                 if gmm.converged_:
                     break
 
+            assert_true(gmm.converged_)
+
 
 def test_regularisation():
     # We train the GaussianMixture on degenerate data by defining two clusters
@@ -855,10 +883,12 @@ def test_regularisation():
         with warnings.catch_warnings():
             warnings.simplefilter("ignore", RuntimeWarning)
             assert_raise_message(ValueError,
-                                 "The algorithm has diverged because of too "
-                                 "few samples per components. "
-                                 "Try to decrease the number of components, "
-                                 "or increase reg_covar.", gmm.fit, X)
+                                 "Fitting the mixture model failed because "
+                                 "some components have ill-defined empirical "
+                                 "covariance (for instance caused by "
+                                 "singleton or collapsed samples). Try to "
+                                 "decrease the number of components, or "
+                                 "increase reg_covar.", gmm.fit, X)
 
             gmm.set_params(reg_covar=1e-6).fit(X)
 
@@ -871,14 +901,14 @@ def test_property():
     for covar_type in COVARIANCE_TYPE:
         X = rand_data.X[covar_type]
         gmm = GaussianMixture(n_components=n_components,
-                              covariance_type=covar_type, random_state=rng)
+                              covariance_type=covar_type, random_state=rng,
+                              n_init=5)
         gmm.fit(X)
-        print(covar_type)
-        if covar_type is 'full':
+        if covar_type == 'full':
             for prec, covar in zip(gmm.precisions_, gmm.covariances_):
 
                 assert_array_almost_equal(linalg.inv(prec), covar)
-        elif covar_type is 'tied':
+        elif covar_type == 'tied':
             assert_array_almost_equal(linalg.inv(gmm.precisions_),
                                       gmm.covariances_)
         else:
diff --git a/sklearn/model_selection/__init__.py b/sklearn/model_selection/__init__.py
index caf0fe70ff49..c2acf500aff5 100644
--- a/sklearn/model_selection/__init__.py
+++ b/sklearn/model_selection/__init__.py
@@ -2,6 +2,7 @@
 from ._split import KFold
 from ._split import LabelKFold
 from ._split import StratifiedKFold
+from ._split import TimeSeriesSplit
 from ._split import LeaveOneLabelOut
 from ._split import LeaveOneOut
 from ._split import LeavePLabelOut
@@ -27,6 +28,7 @@
 
 __all__ = ('BaseCrossValidator',
            'GridSearchCV',
+           'TimeSeriesSplit',
            'KFold',
            'LabelKFold',
            'LabelShuffleSplit',
diff --git a/sklearn/model_selection/_search.py b/sklearn/model_selection/_search.py
index df0f721d109d..be3509770141 100644
--- a/sklearn/model_selection/_search.py
+++ b/sklearn/model_selection/_search.py
@@ -24,6 +24,7 @@
 from ..base import MetaEstimatorMixin
 from ._split import check_cv
 from ._validation import _fit_and_score
+from ..exceptions import NotFittedError
 from ..externals.joblib import Parallel, delayed
 from ..externals import six
 from ..utils import check_random_state
@@ -327,17 +328,18 @@ def _check_param_grid(param_grid):
         param_grid = [param_grid]
 
     for p in param_grid:
-        for v in p.values():
+        for name, v in p.items():
             if isinstance(v, np.ndarray) and v.ndim > 1:
                 raise ValueError("Parameter array should be one-dimensional.")
 
             check = [isinstance(v, k) for k in (list, tuple, np.ndarray)]
             if True not in check:
-                raise ValueError("Parameter values should be a list.")
+                raise ValueError("Parameter values for parameter ({0}) need "
+                                 "to be a sequence.".format(name))
 
             if len(v) == 0:
-                raise ValueError("Parameter values should be a non-empty "
-                                 "list.")
+                raise ValueError("Parameter values for parameter ({0}) need "
+                                 "to be a non-empty sequence.".format(name))
 
 
 # XXX Remove in 0.20
@@ -414,6 +416,15 @@ def score(self, X, y=None):
                              % self.best_estimator_)
         return self.scorer_(self.best_estimator_, X, y)
 
+    def _check_is_fitted(self, method_name):
+        if not self.refit:
+            raise NotFittedError(('This GridSearchCV instance was initialized '
+                                  'with refit=False. %s is '
+                                  'available only after refitting on the best '
+                                  'parameters. ') % method_name)
+        else:
+            check_is_fitted(self, 'best_estimator_')
+
     @if_delegate_has_method(delegate='estimator')
     def predict(self, X):
         """Call predict on the estimator with the best found parameters.
@@ -428,6 +439,7 @@ def predict(self, X):
             underlying estimator.
 
         """
+        self._check_is_fitted('predict')
         return self.best_estimator_.predict(X)
 
     @if_delegate_has_method(delegate='estimator')
@@ -444,6 +456,7 @@ def predict_proba(self, X):
             underlying estimator.
 
         """
+        self._check_is_fitted('predict_proba')
         return self.best_estimator_.predict_proba(X)
 
     @if_delegate_has_method(delegate='estimator')
@@ -460,6 +473,7 @@ def predict_log_proba(self, X):
             underlying estimator.
 
         """
+        self._check_is_fitted('predict_log_proba')
         return self.best_estimator_.predict_log_proba(X)
 
     @if_delegate_has_method(delegate='estimator')
@@ -476,6 +490,7 @@ def decision_function(self, X):
             underlying estimator.
 
         """
+        self._check_is_fitted('decision_function')
         return self.best_estimator_.decision_function(X)
 
     @if_delegate_has_method(delegate='estimator')
@@ -492,11 +507,12 @@ def transform(self, X):
             underlying estimator.
 
         """
+        self._check_is_fitted('transform')
         return self.best_estimator_.transform(X)
 
     @if_delegate_has_method(delegate='estimator')
     def inverse_transform(self, Xt):
-        """Call inverse_transform on the estimator with the best found parameters.
+        """Call inverse_transform on the estimator with the best found params.
 
         Only available if the underlying estimator implements
         ``inverse_transform`` and ``refit=True``.
@@ -508,6 +524,7 @@ def inverse_transform(self, Xt):
             underlying estimator.
 
         """
+        self._check_is_fitted('inverse_transform')
         return self.best_estimator_.transform(Xt)
 
     def _fit(self, X, y, labels, parameter_iterable):
diff --git a/sklearn/model_selection/_split.py b/sklearn/model_selection/_split.py
index da841fd8dca2..5989edd30b10 100644
--- a/sklearn/model_selection/_split.py
+++ b/sklearn/model_selection/_split.py
@@ -5,7 +5,7 @@
 
 # Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
 #         Gael Varoquaux <gael.varoquaux@normalesup.org>,
-#         Olivier Girsel <olivier.grisel@ensta.org>
+#         Olivier Grisel <olivier.grisel@ensta.org>
 #         Raghav R V <rvraghav93@gmail.com>
 # License: BSD 3 clause
 
@@ -122,7 +122,7 @@ class LeaveOneOut(BaseCrossValidator):
     sample is used once as a test set (singleton) while the remaining
     samples form the training set.
 
-    Note: ``LeaveOneOut()`` is equivalent to ``KFold(n_folds=n)`` and
+    Note: ``LeaveOneOut()`` is equivalent to ``KFold(n_splits=n)`` and
     ``LeavePOut(p=1)`` where ``n`` is the number of samples.
 
     Due to the high number of test sets (which is the same as the
@@ -197,7 +197,7 @@ class LeavePOut(BaseCrossValidator):
     samples form the training set in each iteration.
 
     Note: ``LeavePOut(p)`` is NOT equivalent to
-    ``KFold(n_folds=n_samples // p)`` which creates non-overlapping test sets.
+    ``KFold(n_splits=n_samples // p)`` which creates non-overlapping test sets.
 
     Due to the high number of iterations which grows combinatorically with the
     number of samples this cross-validation method can be very costly. For
@@ -261,27 +261,27 @@ def get_n_splits(self, X, y=None, labels=None):
 
 
 class _BaseKFold(with_metaclass(ABCMeta, BaseCrossValidator)):
-    """Base class for KFold and StratifiedKFold"""
+    """Base class for KFold, LabelKFold, and StratifiedKFold"""
 
     @abstractmethod
-    def __init__(self, n_folds, shuffle, random_state):
-        if not isinstance(n_folds, numbers.Integral):
+    def __init__(self, n_splits, shuffle, random_state):
+        if not isinstance(n_splits, numbers.Integral):
             raise ValueError('The number of folds must be of Integral type. '
                              '%s of type %s was passed.'
-                             % (n_folds, type(n_folds)))
-        n_folds = int(n_folds)
+                             % (n_splits, type(n_splits)))
+        n_splits = int(n_splits)
 
-        if n_folds <= 1:
+        if n_splits <= 1:
             raise ValueError(
                 "k-fold cross-validation requires at least one"
-                " train/test split by setting n_folds=2 or more,"
-                " got n_folds={0}.".format(n_folds))
+                " train/test split by setting n_splits=2 or more,"
+                " got n_splits={0}.".format(n_splits))
 
         if not isinstance(shuffle, bool):
             raise TypeError("shuffle must be True or False;"
                             " got {0}".format(shuffle))
 
-        self.n_folds = n_folds
+        self.n_splits = n_splits
         self.shuffle = shuffle
         self.random_state = random_state
 
@@ -311,10 +311,10 @@ def split(self, X, y=None, labels=None):
         """
         X, y, labels = indexable(X, y, labels)
         n_samples = _num_samples(X)
-        if self.n_folds > n_samples:
+        if self.n_splits > n_samples:
             raise ValueError(
-                ("Cannot have number of folds n_folds={0} greater"
-                 " than the number of samples: {1}.").format(self.n_folds,
+                ("Cannot have number of splits n_splits={0} greater"
+                 " than the number of samples: {1}.").format(self.n_splits,
                                                              n_samples))
 
         for train, test in super(_BaseKFold, self).split(X, y, labels):
@@ -339,7 +339,7 @@ def get_n_splits(self, X=None, y=None, labels=None):
         n_splits : int
             Returns the number of splitting iterations in the cross-validator.
         """
-        return self.n_folds
+        return self.n_splits
 
 
 class KFold(_BaseKFold):
@@ -355,7 +355,7 @@ class KFold(_BaseKFold):
 
     Parameters
     ----------
-    n_folds : int, default=3
+    n_splits : int, default=3
         Number of folds. Must be at least 2.
 
     shuffle : boolean, optional
@@ -370,11 +370,11 @@ class KFold(_BaseKFold):
     >>> from sklearn.model_selection import KFold
     >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
     >>> y = np.array([1, 2, 3, 4])
-    >>> kf = KFold(n_folds=2)
+    >>> kf = KFold(n_splits=2)
     >>> kf.get_n_splits(X)
     2
     >>> print(kf)  # doctest: +NORMALIZE_WHITESPACE
-    KFold(n_folds=2, random_state=None, shuffle=False)
+    KFold(n_splits=2, random_state=None, shuffle=False)
     >>> for train_index, test_index in kf.split(X):
     ...    print("TRAIN:", train_index, "TEST:", test_index)
     ...    X_train, X_test = X[train_index], X[test_index]
@@ -384,9 +384,9 @@ class KFold(_BaseKFold):
 
     Notes
     -----
-    The first ``n_samples % n_folds`` folds have size
-    ``n_samples // n_folds + 1``, other folds have size
-    ``n_samples // n_folds``, where ``n_samples`` is the number of samples.
+    The first ``n_samples % n_splits`` folds have size
+    ``n_samples // n_splits + 1``, other folds have size
+    ``n_samples // n_splits``, where ``n_samples`` is the number of samples.
 
     See also
     --------
@@ -398,9 +398,9 @@ class KFold(_BaseKFold):
     LabelKFold: K-fold iterator variant with non-overlapping labels.
     """
 
-    def __init__(self, n_folds=3, shuffle=False,
+    def __init__(self, n_splits=3, shuffle=False,
                  random_state=None):
-        super(KFold, self).__init__(n_folds, shuffle, random_state)
+        super(KFold, self).__init__(n_splits, shuffle, random_state)
 
     def _iter_test_indices(self, X, y=None, labels=None):
         n_samples = _num_samples(X)
@@ -408,9 +408,9 @@ def _iter_test_indices(self, X, y=None, labels=None):
         if self.shuffle:
             check_random_state(self.random_state).shuffle(indices)
 
-        n_folds = self.n_folds
-        fold_sizes = (n_samples // n_folds) * np.ones(n_folds, dtype=np.int)
-        fold_sizes[:n_samples % n_folds] += 1
+        n_splits = self.n_splits
+        fold_sizes = (n_samples // n_splits) * np.ones(n_splits, dtype=np.int)
+        fold_sizes[:n_samples % n_splits] += 1
         current = 0
         for fold_size in fold_sizes:
             start, stop = current, current + fold_size
@@ -429,7 +429,7 @@ class LabelKFold(_BaseKFold):
 
     Parameters
     ----------
-    n_folds : int, default=3
+    n_splits : int, default=3
         Number of folds. Must be at least 2.
 
     Examples
@@ -438,11 +438,11 @@ class LabelKFold(_BaseKFold):
     >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
     >>> y = np.array([1, 2, 3, 4])
     >>> labels = np.array([0, 0, 2, 2])
-    >>> label_kfold = LabelKFold(n_folds=2)
+    >>> label_kfold = LabelKFold(n_splits=2)
     >>> label_kfold.get_n_splits(X, y, labels)
     2
     >>> print(label_kfold)
-    LabelKFold(n_folds=2)
+    LabelKFold(n_splits=2)
     >>> for train_index, test_index in label_kfold.split(X, y, labels):
     ...     print("TRAIN:", train_index, "TEST:", test_index)
     ...     X_train, X_test = X[train_index], X[test_index]
@@ -464,8 +464,8 @@ class LabelKFold(_BaseKFold):
         For splitting the data according to explicit domain-specific
         stratification of the dataset.
     """
-    def __init__(self, n_folds=3):
-        super(LabelKFold, self).__init__(n_folds, shuffle=False,
+    def __init__(self, n_splits=3):
+        super(LabelKFold, self).__init__(n_splits, shuffle=False,
                                          random_state=None)
 
     def _iter_test_indices(self, X, y, labels):
@@ -475,10 +475,10 @@ def _iter_test_indices(self, X, y, labels):
         unique_labels, labels = np.unique(labels, return_inverse=True)
         n_labels = len(unique_labels)
 
-        if self.n_folds > n_labels:
-            raise ValueError("Cannot have number of folds n_folds=%d greater"
+        if self.n_splits > n_labels:
+            raise ValueError("Cannot have number of splits n_splits=%d greater"
                              " than the number of labels: %d."
-                             % (self.n_folds, n_labels))
+                             % (self.n_splits, n_labels))
 
         # Weight labels by their number of occurrences
         n_samples_per_label = np.bincount(labels)
@@ -488,7 +488,7 @@ def _iter_test_indices(self, X, y, labels):
         n_samples_per_label = n_samples_per_label[indices]
 
         # Total weight of each fold
-        n_samples_per_fold = np.zeros(self.n_folds)
+        n_samples_per_fold = np.zeros(self.n_splits)
 
         # Mapping from label index to fold index
         label_to_fold = np.zeros(len(unique_labels))
@@ -501,7 +501,7 @@ def _iter_test_indices(self, X, y, labels):
 
         indices = label_to_fold[labels]
 
-        for f in range(self.n_folds):
+        for f in range(self.n_splits):
             yield np.where(indices == f)[0]
 
 
@@ -518,7 +518,7 @@ class StratifiedKFold(_BaseKFold):
 
     Parameters
     ----------
-    n_folds : int, default=3
+    n_splits : int, default=3
         Number of folds. Must be at least 2.
 
     shuffle : boolean, optional
@@ -534,11 +534,11 @@ class StratifiedKFold(_BaseKFold):
     >>> from sklearn.model_selection import StratifiedKFold
     >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
     >>> y = np.array([0, 0, 1, 1])
-    >>> skf = StratifiedKFold(n_folds=2)
+    >>> skf = StratifiedKFold(n_splits=2)
     >>> skf.get_n_splits(X, y)
     2
     >>> print(skf)  # doctest: +NORMALIZE_WHITESPACE
-    StratifiedKFold(n_folds=2, random_state=None, shuffle=False)
+    StratifiedKFold(n_splits=2, random_state=None, shuffle=False)
     >>> for train_index, test_index in skf.split(X, y):
     ...    print("TRAIN:", train_index, "TEST:", test_index)
     ...    X_train, X_test = X[train_index], X[test_index]
@@ -548,13 +548,13 @@ class StratifiedKFold(_BaseKFold):
 
     Notes
     -----
-    All the folds have size ``trunc(n_samples / n_folds)``, the last one has
+    All the folds have size ``trunc(n_samples / n_splits)``, the last one has
     the complementary.
 
     """
 
-    def __init__(self, n_folds=3, shuffle=False, random_state=None):
-        super(StratifiedKFold, self).__init__(n_folds, shuffle, random_state)
+    def __init__(self, n_splits=3, shuffle=False, random_state=None):
+        super(StratifiedKFold, self).__init__(n_splits, shuffle, random_state)
 
     def _make_test_folds(self, X, y=None, labels=None):
         if self.shuffle:
@@ -566,26 +566,26 @@ def _make_test_folds(self, X, y=None, labels=None):
         unique_y, y_inversed = np.unique(y, return_inverse=True)
         y_counts = bincount(y_inversed)
         min_labels = np.min(y_counts)
-        if np.all(self.n_folds > y_counts):
+        if np.all(self.n_splits > y_counts):
             raise ValueError("All the n_labels for individual classes"
-                             " are less than %d folds."
-                             % (self.n_folds))
-        if self.n_folds > min_labels:
+                             " are less than n_splits=%d."
+                             % (self.n_splits))
+        if self.n_splits > min_labels:
             warnings.warn(("The least populated class in y has only %d"
                            " members, which is too few. The minimum"
                            " number of labels for any class cannot"
-                           " be less than n_folds=%d."
-                           % (min_labels, self.n_folds)), Warning)
+                           " be less than n_splits=%d."
+                           % (min_labels, self.n_splits)), Warning)
 
         # pre-assign each sample to a test fold index using individual KFold
         # splitting strategies for each class so as to respect the balance of
         # classes
         # NOTE: Passing the data corresponding to ith class say X[y==class_i]
         # will break when the data is not 100% stratifiable for all classes.
-        # So we pass np.zeroes(max(c, n_folds)) as data to the KFold
+        # So we pass np.zeroes(max(c, n_splits)) as data to the KFold
         per_cls_cvs = [
-            KFold(self.n_folds, shuffle=self.shuffle,
-                  random_state=rng).split(np.zeros(max(count, self.n_folds)))
+            KFold(self.n_splits, shuffle=self.shuffle,
+                  random_state=rng).split(np.zeros(max(count, self.n_splits)))
             for count in y_counts]
 
         test_folds = np.zeros(n_samples, dtype=np.int)
@@ -593,7 +593,7 @@ def _make_test_folds(self, X, y=None, labels=None):
             for cls, (_, test_split) in zip(unique_y, per_cls_splits):
                 cls_test_folds = test_folds[y == cls]
                 # the test split can be too big because we used
-                # KFold(...).split(X[:max(c, n_folds)]) when data is not 100%
+                # KFold(...).split(X[:max(c, n_splits)]) when data is not 100%
                 # stratifiable for all the classes
                 # (we use a warning instead of raising an exception)
                 # If this is the case, let's trim it:
@@ -605,7 +605,7 @@ def _make_test_folds(self, X, y=None, labels=None):
 
     def _iter_test_masks(self, X, y=None, labels=None):
         test_folds = self._make_test_folds(X, y)
-        for i in range(self.n_folds):
+        for i in range(self.n_splits):
             yield test_folds == i
 
     def split(self, X, y, labels=None):
@@ -634,6 +634,99 @@ def split(self, X, y, labels=None):
         """
         return super(StratifiedKFold, self).split(X, y, labels)
 
+
+class TimeSeriesSplit(_BaseKFold):
+    """Time Series cross-validator
+
+    Provides train/test indices to split time series data samples
+    that are observed at fixed time intervals, in train/test sets.
+    In each split, test indices must be higher than before, and thus shuffling
+    in cross validator is inappropriate.
+
+    This cross-validation object is a variation of :class:`KFold`.
+    In the kth split, it returns first k folds as train set and the
+    (k+1)th fold as test set.
+
+    Note that unlike standard cross-validation methods, successive
+    training sets are supersets of those that come before them.
+
+    Read more in the :ref:`User Guide <cross_validation>`.
+
+    Parameters
+    ----------
+    n_splits : int, default=3
+        Number of splits. Must be at least 1.
+
+    Examples
+    --------
+    >>> from sklearn.model_selection import TimeSeriesSplit
+    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
+    >>> y = np.array([1, 2, 3, 4])
+    >>> tscv = TimeSeriesSplit(n_splits=3)
+    >>> print(tscv)  # doctest: +NORMALIZE_WHITESPACE
+    TimeSeriesSplit(n_splits=3)
+    >>> for train_index, test_index in tscv.split(X):
+    ...    print("TRAIN:", train_index, "TEST:", test_index)
+    ...    X_train, X_test = X[train_index], X[test_index]
+    ...    y_train, y_test = y[train_index], y[test_index]
+    TRAIN: [0] TEST: [1]
+    TRAIN: [0 1] TEST: [2]
+    TRAIN: [0 1 2] TEST: [3]
+
+    Notes
+    -----
+    The training set has size ``i * n_samples // (n_splits + 1)
+    + n_samples % (n_splits + 1)`` in the ``i``th split,
+    with a test set of size ``n_samples//(n_splits + 1)``,
+    where ``n_samples`` is the number of samples.
+    """
+    def __init__(self, n_splits=3):
+        super(TimeSeriesSplit, self).__init__(n_splits,
+                                              shuffle=False,
+                                              random_state=None)
+
+    def split(self, X, y=None, labels=None):
+        """Generate indices to split data into training and test set.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+            Training data, where n_samples is the number of samples
+            and n_features is the number of features.
+
+        y : array-like, shape (n_samples,)
+            The target variable for supervised learning problems.
+
+        labels : array-like, with shape (n_samples,), optional
+            Group labels for the samples used while splitting the dataset into
+            train/test set.
+
+        Returns
+        -------
+        train : ndarray
+            The training set indices for that split.
+
+        test : ndarray
+            The testing set indices for that split.
+        """
+        X, y, labels = indexable(X, y, labels)
+        n_samples = _num_samples(X)
+        n_splits = self.n_splits
+        n_folds = n_splits + 1
+        if n_folds > n_samples:
+            raise ValueError(
+                ("Cannot have number of folds ={0} greater"
+                 " than the number of samples: {1}.").format(n_folds,
+                                                             n_samples))
+        indices = np.arange(n_samples)
+        test_size = (n_samples // n_folds)
+        test_starts = range(test_size + n_samples % n_folds,
+                            n_samples, test_size)
+        for test_start in test_starts:
+            yield (indices[:test_start],
+                   indices[test_start:test_start + test_size])
+
+
 class LeaveOneLabelOut(BaseCrossValidator):
     """Leave One Label Out cross-validator
 
@@ -803,10 +896,10 @@ def get_n_splits(self, X, y, labels):
 class BaseShuffleSplit(with_metaclass(ABCMeta)):
     """Base class for ShuffleSplit and StratifiedShuffleSplit"""
 
-    def __init__(self, n_iter=10, test_size=0.1, train_size=None,
+    def __init__(self, n_splits=10, test_size=0.1, train_size=None,
                  random_state=None):
         _validate_shuffle_split_init(test_size, train_size)
-        self.n_iter = n_iter
+        self.n_splits = n_splits
         self.test_size = test_size
         self.train_size = train_size
         self.random_state = random_state
@@ -862,7 +955,7 @@ def get_n_splits(self, X=None, y=None, labels=None):
         n_splits : int
             Returns the number of splitting iterations in the cross-validator.
         """
-        return self.n_iter
+        return self.n_splits
 
     def __repr__(self):
         return _build_repr(self)
@@ -881,7 +974,7 @@ class ShuffleSplit(BaseShuffleSplit):
 
     Parameters
     ----------
-    n_iter : int (default 10)
+    n_splits : int (default 10)
         Number of re-shuffling & splitting iterations.
 
     test_size : float, int, or None, default 0.1
@@ -904,18 +997,18 @@ class ShuffleSplit(BaseShuffleSplit):
     >>> from sklearn.model_selection import ShuffleSplit
     >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
     >>> y = np.array([1, 2, 1, 2])
-    >>> rs = ShuffleSplit(n_iter=3, test_size=.25, random_state=0)
+    >>> rs = ShuffleSplit(n_splits=3, test_size=.25, random_state=0)
     >>> rs.get_n_splits(X)
     3
     >>> print(rs)
-    ShuffleSplit(n_iter=3, random_state=0, test_size=0.25, train_size=None)
+    ShuffleSplit(n_splits=3, random_state=0, test_size=0.25, train_size=None)
     >>> for train_index, test_index in rs.split(X):
     ...    print("TRAIN:", train_index, "TEST:", test_index)
     ...  # doctest: +ELLIPSIS
     TRAIN: [3 1 0] TEST: [2]
     TRAIN: [2 1 3] TEST: [0]
     TRAIN: [0 2 1] TEST: [3]
-    >>> rs = ShuffleSplit(n_iter=3, train_size=0.5, test_size=.25,
+    >>> rs = ShuffleSplit(n_splits=3, train_size=0.5, test_size=.25,
     ...                   random_state=0)
     >>> for train_index, test_index in rs.split(X):
     ...    print("TRAIN:", train_index, "TEST:", test_index)
@@ -930,7 +1023,7 @@ def _iter_indices(self, X, y=None, labels=None):
         n_train, n_test = _validate_shuffle_split(n_samples, self.test_size,
                                                   self.train_size)
         rng = check_random_state(self.random_state)
-        for i in range(self.n_iter):
+        for i in range(self.n_splits):
             # random partition
             permutation = rng.permutation(n_samples)
             ind_test = permutation[:n_test]
@@ -955,7 +1048,7 @@ class LabelShuffleSplit(ShuffleSplit):
 
     For example, a less computationally intensive alternative to
     ``LeavePLabelOut(p=10)`` would be
-    ``LabelShuffleSplit(test_size=10, n_iter=100)``.
+    ``LabelShuffleSplit(test_size=10, n_splits=100)``.
 
     Note: The parameters ``test_size`` and ``train_size`` refer to labels, and
     not to samples, as in ShuffleSplit.
@@ -963,7 +1056,7 @@ class LabelShuffleSplit(ShuffleSplit):
 
     Parameters
     ----------
-    n_iter : int (default 5)
+    n_splits : int (default 5)
         Number of re-shuffling & splitting iterations.
 
     test_size : float (default 0.2), int, or None
@@ -982,10 +1075,10 @@ class LabelShuffleSplit(ShuffleSplit):
         Pseudo-random number generator state used for random sampling.
     '''
 
-    def __init__(self, n_iter=5, test_size=0.2, train_size=None,
+    def __init__(self, n_splits=5, test_size=0.2, train_size=None,
                  random_state=None):
         super(LabelShuffleSplit, self).__init__(
-            n_iter=n_iter,
+            n_splits=n_splits,
             test_size=test_size,
             train_size=train_size,
             random_state=random_state)
@@ -1022,7 +1115,7 @@ class StratifiedShuffleSplit(BaseShuffleSplit):
 
     Parameters
     ----------
-    n_iter : int (default 10)
+    n_splits : int (default 10)
         Number of re-shuffling & splitting iterations.
 
     test_size : float (default 0.1), int, or None
@@ -1045,11 +1138,11 @@ class StratifiedShuffleSplit(BaseShuffleSplit):
     >>> from sklearn.model_selection import StratifiedShuffleSplit
     >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
     >>> y = np.array([0, 0, 1, 1])
-    >>> sss = StratifiedShuffleSplit(n_iter=3, test_size=0.5, random_state=0)
+    >>> sss = StratifiedShuffleSplit(n_splits=3, test_size=0.5, random_state=0)
     >>> sss.get_n_splits(X, y)
     3
     >>> print(sss)       # doctest: +ELLIPSIS
-    StratifiedShuffleSplit(n_iter=3, random_state=0, ...)
+    StratifiedShuffleSplit(n_splits=3, random_state=0, ...)
     >>> for train_index, test_index in sss.split(X, y):
     ...    print("TRAIN:", train_index, "TEST:", test_index)
     ...    X_train, X_test = X[train_index], X[test_index]
@@ -1059,10 +1152,10 @@ class StratifiedShuffleSplit(BaseShuffleSplit):
     TRAIN: [0 2] TEST: [3 1]
     """
 
-    def __init__(self, n_iter=10, test_size=0.1, train_size=None,
+    def __init__(self, n_splits=10, test_size=0.1, train_size=None,
                  random_state=None):
         super(StratifiedShuffleSplit, self).__init__(
-            n_iter, test_size, train_size, random_state)
+            n_splits, test_size, train_size, random_state)
 
     def _iter_indices(self, X, y, labels=None):
         n_samples = _num_samples(X)
@@ -1093,7 +1186,7 @@ def _iter_indices(self, X, y, labels=None):
         t_i = np.minimum(class_counts - n_i,
                          np.round(n_test * p_i).astype(int))
 
-        for _ in range(self.n_iter):
+        for _ in range(self.n_splits):
             train = []
             test = []
 
@@ -1436,13 +1529,9 @@ def train_test_split(*arrays, **options):
     Parameters
     ----------
     *arrays : sequence of indexables with same length / shape[0]
-
-        allowed inputs are lists, numpy arrays, scipy-sparse
+        Allowed inputs are lists, numpy arrays, scipy-sparse
         matrices or pandas dataframes.
 
-        .. versionadded:: 0.16
-            preserves input type instead of always casting to numpy array.
-
     test_size : float, int, or None (default is None)
         If float, should be between 0.0 and 1.0 and represent the
         proportion of the dataset to include in the test split. If
@@ -1469,7 +1558,9 @@ def train_test_split(*arrays, **options):
         List containing train-test split of inputs.
 
         .. versionadded:: 0.16
-            Output type is the same as the input type.
+            If the input is sparse, the output will be a
+            ``scipy.sparse.csr_matrix``. Else, output type is the same as the
+            input type.
 
     Examples
     --------
diff --git a/sklearn/model_selection/tests/test_search.py b/sklearn/model_selection/tests/test_search.py
index c3365bd3a7e6..b5abd8d873a5 100644
--- a/sklearn/model_selection/tests/test_search.py
+++ b/sklearn/model_selection/tests/test_search.py
@@ -28,6 +28,7 @@
 
 from sklearn.externals.six.moves import zip
 from sklearn.base import BaseEstimator
+from sklearn.exceptions import NotFittedError
 from sklearn.datasets import make_classification
 from sklearn.datasets import make_blobs
 from sklearn.datasets import make_multilabel_classification
@@ -73,8 +74,10 @@ def predict(self, T):
         return T.shape[0]
 
     predict_proba = predict
+    predict_log_proba = predict
     decision_function = predict
     transform = predict
+    inverse_transform = predict
 
     def score(self, X=None, Y=None):
         if self.foo_param > 1:
@@ -165,6 +168,22 @@ def test_grid_search():
     assert_raises(ValueError, grid_search.fit, X, y)
 
 
+def test_grid_search_incorrect_param_grid():
+    clf = MockClassifier()
+    assert_raise_message(
+        ValueError,
+        "Parameter values for parameter (C) need to be a sequence.",
+        GridSearchCV, clf, {'C': 1})
+
+
+def test_grid_search_param_grid_includes_sequence_of_a_zero_length():
+    clf = MockClassifier()
+    assert_raise_message(
+        ValueError,
+        "Parameter values for parameter (C) need to be a non-empty sequence.",
+        GridSearchCV, clf, {'C': []})
+
+
 @ignore_warnings
 def test_grid_search_no_score():
     # Test grid-search on classifier that has no score function.
@@ -268,6 +287,14 @@ def test_no_refit():
                 hasattr(grid_search, "best_index_") and
                 hasattr(grid_search, "best_params_"))
 
+    # Make sure the predict/transform etc fns raise meaningfull error msg
+    for fn_name in ('predict', 'predict_proba', 'predict_log_proba',
+                    'transform', 'inverse_transform'):
+        assert_raise_message(NotFittedError,
+                             ('refit=False. %s is available only after '
+                              'refitting on the best parameters' % fn_name),
+                             getattr(grid_search, fn_name), X)
+
 
 def test_grid_search_error():
     # Test that grid search will capture errors on data with different length
@@ -454,7 +481,7 @@ def test_X_as_list():
     y = np.array([0] * 5 + [1] * 5)
 
     clf = CheckingClassifier(check_X=lambda x: isinstance(x, list))
-    cv = KFold(n_folds=3)
+    cv = KFold(n_splits=3)
     grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, cv=cv)
     grid_search.fit(X.tolist(), y).score(X, y)
     assert_true(hasattr(grid_search, "results_"))
@@ -466,7 +493,7 @@ def test_y_as_list():
     y = np.array([0] * 5 + [1] * 5)
 
     clf = CheckingClassifier(check_y=lambda x: isinstance(x, list))
-    cv = KFold(n_folds=3)
+    cv = KFold(n_splits=3)
     grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, cv=cv)
     grid_search.fit(X, y.tolist()).score(X, y)
     assert_true(hasattr(grid_search, "results_"))
@@ -597,14 +624,14 @@ def test_grid_search_results():
     X, y = make_classification(n_samples=50, n_features=4,
                                random_state=42)
 
-    n_folds = 3
+    n_splits = 3
     n_grid_points = 6
     params = [dict(kernel=['rbf', ], C=[1, 10], gamma=[0.1, 1]),
               dict(kernel=['poly', ], degree=[1, 2])]
-    grid_search = GridSearchCV(SVC(), cv=n_folds, iid=False,
+    grid_search = GridSearchCV(SVC(), cv=n_splits, iid=False,
                                param_grid=params)
     grid_search.fit(X, y)
-    grid_search_iid = GridSearchCV(SVC(), cv=n_folds, iid=True,
+    grid_search_iid = GridSearchCV(SVC(), cv=n_splits, iid=True,
                                    param_grid=params)
     grid_search_iid.fit(X, y)
 
@@ -645,14 +672,15 @@ def test_random_search_results():
     # scipy.stats dists now supports `seed` but we still support scipy 0.12
     # which doesn't support the seed. Hence the assertions in the test for
     # random_search alone should not depend on randomization.
-    n_folds = 3
+    n_splits = 3
     n_search_iter = 30
     params = dict(C=expon(scale=10), gamma=expon(scale=0.1))
-    random_search = RandomizedSearchCV(SVC(), n_iter=n_search_iter, cv=n_folds,
+    random_search = RandomizedSearchCV(SVC(), n_iter=n_search_iter,
+                                       cv=n_splits,
                                        iid=False, param_distributions=params)
     random_search.fit(X, y)
     random_search_iid = RandomizedSearchCV(SVC(), n_iter=n_search_iter,
-                                           cv=n_folds, iid=True,
+                                           cv=n_splits, iid=True,
                                            param_distributions=params)
     random_search_iid.fit(X, y)
 
@@ -779,22 +807,22 @@ def test_search_results_none_param():
 
 def test_grid_search_correct_score_results():
     # test that correct scores are used
-    n_folds = 3
+    n_splits = 3
     clf = LinearSVC(random_state=0)
     X, y = make_blobs(random_state=0, centers=2)
     Cs = [.1, 1, 10]
     for score in ['f1', 'roc_auc']:
-        grid_search = GridSearchCV(clf, {'C': Cs}, scoring=score, cv=n_folds)
+        grid_search = GridSearchCV(clf, {'C': Cs}, scoring=score, cv=n_splits)
         results = grid_search.fit(X, y).results_
 
         # Test scorer names
         result_keys = list(results.keys())
         expected_keys = (("test_mean_score", "test_rank_score") +
                          tuple("test_split%d_score" % cv_i
-                               for cv_i in range(n_folds)))
+                               for cv_i in range(n_splits)))
         assert_true(all(in1d(expected_keys, result_keys)))
 
-        cv = StratifiedKFold(n_folds=n_folds)
+        cv = StratifiedKFold(n_splits=n_splits)
         n_splits = grid_search.n_splits_
         for candidate_i, C in enumerate(Cs):
             clf.set_params(C=C)
diff --git a/sklearn/model_selection/tests/test_split.py b/sklearn/model_selection/tests/test_split.py
index 69749f8e4c0a..d28148efe695 100644
--- a/sklearn/model_selection/tests/test_split.py
+++ b/sklearn/model_selection/tests/test_split.py
@@ -3,7 +3,7 @@
 import warnings
 
 import numpy as np
-from scipy.sparse import coo_matrix
+from scipy.sparse import coo_matrix, csc_matrix, csr_matrix
 from scipy import stats
 from scipy.misc import comb
 from itertools import combinations
@@ -30,6 +30,7 @@
 from sklearn.model_selection import KFold
 from sklearn.model_selection import StratifiedKFold
 from sklearn.model_selection import LabelKFold
+from sklearn.model_selection import TimeSeriesSplit
 from sklearn.model_selection import LeaveOneOut
 from sklearn.model_selection import LeaveOneLabelOut
 from sklearn.model_selection import LeavePOut
@@ -132,9 +133,9 @@ def get_params(self, deep=False):
 def test_cross_validator_with_default_params():
     n_samples = 4
     n_unique_labels = 4
-    n_folds = 2
+    n_splits = 2
     p = 2
-    n_iter = 10  # (the default value)
+    n_shuffle_splits = 10  # (the default value)
 
     X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
     X_1d = np.array([1, 2, 3, 4])
@@ -142,8 +143,8 @@ def test_cross_validator_with_default_params():
     labels = np.array([1, 2, 3, 4])
     loo = LeaveOneOut()
     lpo = LeavePOut(p)
-    kf = KFold(n_folds)
-    skf = StratifiedKFold(n_folds)
+    kf = KFold(n_splits)
+    skf = StratifiedKFold(n_splits)
     lolo = LeaveOneLabelOut()
     lopo = LeavePLabelOut(p)
     ss = ShuffleSplit(random_state=0)
@@ -151,23 +152,24 @@ def test_cross_validator_with_default_params():
 
     loo_repr = "LeaveOneOut()"
     lpo_repr = "LeavePOut(p=2)"
-    kf_repr = "KFold(n_folds=2, random_state=None, shuffle=False)"
-    skf_repr = "StratifiedKFold(n_folds=2, random_state=None, shuffle=False)"
+    kf_repr = "KFold(n_splits=2, random_state=None, shuffle=False)"
+    skf_repr = "StratifiedKFold(n_splits=2, random_state=None, shuffle=False)"
     lolo_repr = "LeaveOneLabelOut()"
     lopo_repr = "LeavePLabelOut(n_labels=2)"
-    ss_repr = ("ShuffleSplit(n_iter=10, random_state=0, test_size=0.1, "
+    ss_repr = ("ShuffleSplit(n_splits=10, random_state=0, test_size=0.1, "
                "train_size=None)")
     ps_repr = "PredefinedSplit(test_fold=array([1, 1, 2, 2]))"
 
-    n_splits = [n_samples, comb(n_samples, p), n_folds, n_folds,
-                n_unique_labels, comb(n_unique_labels, p), n_iter, 2]
+    n_splits_expected = [n_samples, comb(n_samples, p), n_splits, n_splits,
+                         n_unique_labels, comb(n_unique_labels, p),
+                         n_shuffle_splits, 2]
 
     for i, (cv, cv_repr) in enumerate(zip(
             [loo, lpo, kf, skf, lolo, lopo, ss, ps],
             [loo_repr, lpo_repr, kf_repr, skf_repr, lolo_repr, lopo_repr,
              ss_repr, ps_repr])):
         # Test if get_n_splits works correctly
-        assert_equal(n_splits[i], cv.get_n_splits(X, y, labels))
+        assert_equal(n_splits_expected[i], cv.get_n_splits(X, y, labels))
 
         # Test if the cross-validator works as expected even if
         # the data is 1d
@@ -194,13 +196,13 @@ def check_valid_split(train, test, n_samples=None):
         assert_equal(train.union(test), set(range(n_samples)))
 
 
-def check_cv_coverage(cv, X, y, labels, expected_n_iter=None):
+def check_cv_coverage(cv, X, y, labels, expected_n_splits=None):
     n_samples = _num_samples(X)
     # Check that a all the samples appear at least once in a test fold
-    if expected_n_iter is not None:
-        assert_equal(cv.get_n_splits(X, y, labels), expected_n_iter)
+    if expected_n_splits is not None:
+        assert_equal(cv.get_n_splits(X, y, labels), expected_n_splits)
     else:
-        expected_n_iter = cv.get_n_splits(X, y, labels)
+        expected_n_splits = cv.get_n_splits(X, y, labels)
 
     collected_test_samples = set()
     iterations = 0
@@ -210,7 +212,7 @@ def check_cv_coverage(cv, X, y, labels, expected_n_iter=None):
         collected_test_samples.update(test)
 
     # Check that the accumulated test samples cover the whole dataset
-    assert_equal(iterations, expected_n_iter)
+    assert_equal(iterations, expected_n_splits)
     if n_samples is not None:
         assert_equal(collected_test_samples, set(range(n_samples)))
 
@@ -234,10 +236,10 @@ def test_kfold_valueerrors():
     # side of the split at each split
     with warnings.catch_warnings():
         warnings.simplefilter("ignore")
-        check_cv_coverage(skf_3, X2, y, labels=None, expected_n_iter=3)
+        check_cv_coverage(skf_3, X2, y, labels=None, expected_n_splits=3)
 
     # Check that errors are raised if all n_labels for individual
-    # classes are less than n_folds.
+    # classes are less than n_splits.
     y = np.array([3, 3, -1, -1, 2])
 
     assert_raises(ValueError, next, skf_3.split(X2, y))
@@ -252,27 +254,27 @@ def test_kfold_valueerrors():
     assert_raise_message(ValueError, error_string,
                          StratifiedKFold, 1)
 
-    # When n_folds is not integer:
+    # When n_splits is not integer:
     assert_raises(ValueError, KFold, 1.5)
     assert_raises(ValueError, KFold, 2.0)
     assert_raises(ValueError, StratifiedKFold, 1.5)
     assert_raises(ValueError, StratifiedKFold, 2.0)
 
     # When shuffle is not  a bool:
-    assert_raises(TypeError, KFold, n_folds=4, shuffle=None)
+    assert_raises(TypeError, KFold, n_splits=4, shuffle=None)
 
 
 def test_kfold_indices():
     # Check all indices are returned in the test folds
     X1 = np.ones(18)
     kf = KFold(3)
-    check_cv_coverage(kf, X1, y=None, labels=None, expected_n_iter=3)
+    check_cv_coverage(kf, X1, y=None, labels=None, expected_n_splits=3)
 
     # Check all indices are returned in the test folds even when equal-sized
     # folds are not possible
     X2 = np.ones(17)
     kf = KFold(3)
-    check_cv_coverage(kf, X2, y=None, labels=None, expected_n_iter=3)
+    check_cv_coverage(kf, X2, y=None, labels=None, expected_n_splits=3)
 
     # Check if get_n_splits returns the number of folds
     assert_equal(5, KFold(5).get_n_splits(X2))
@@ -441,7 +443,7 @@ def test_shuffle_stratifiedkfold():
     for (_, test0), (_, test1) in zip(kf0.split(X_40, y),
                                       kf1.split(X_40, y)):
         assert_not_equal(set(test0), set(test1))
-    check_cv_coverage(kf0, X_40, y, labels=None, expected_n_iter=5)
+    check_cv_coverage(kf0, X_40, y, labels=None, expected_n_splits=5)
 
 
 def test_kfold_can_detect_dependent_samples_on_digits():  # see #2372
@@ -456,9 +458,9 @@ def test_kfold_can_detect_dependent_samples_on_digits():  # see #2372
     X, y = digits.data[:600], digits.target[:600]
     model = SVC(C=10, gamma=0.005)
 
-    n_folds = 3
+    n_splits = 3
 
-    cv = KFold(n_folds=n_folds, shuffle=False)
+    cv = KFold(n_splits=n_splits, shuffle=False)
     mean_score = cross_val_score(model, X, y, cv=cv).mean()
     assert_greater(0.92, mean_score)
     assert_greater(mean_score, 0.80)
@@ -467,11 +469,11 @@ def test_kfold_can_detect_dependent_samples_on_digits():  # see #2372
     # overfitting of the model with regards to the writing style of the authors
     # by yielding a seriously overestimated score:
 
-    cv = KFold(n_folds, shuffle=True, random_state=0)
+    cv = KFold(n_splits, shuffle=True, random_state=0)
     mean_score = cross_val_score(model, X, y, cv=cv).mean()
     assert_greater(mean_score, 0.92)
 
-    cv = KFold(n_folds, shuffle=True, random_state=1)
+    cv = KFold(n_splits, shuffle=True, random_state=1)
     mean_score = cross_val_score(model, X, y, cv=cv).mean()
     assert_greater(mean_score, 0.92)
 
@@ -482,7 +484,7 @@ def test_kfold_can_detect_dependent_samples_on_digits():  # see #2372
     # the estimated mean score is close to the score measured with
     # non-shuffled KFold
 
-    cv = StratifiedKFold(n_folds)
+    cv = StratifiedKFold(n_splits)
     mean_score = cross_val_score(model, X, y, cv=cv).mean()
     assert_greater(0.93, mean_score)
     assert_greater(mean_score, 0.80)
@@ -562,7 +564,7 @@ def test_stratified_shuffle_split_even():
     # Test the StratifiedShuffleSplit, indices are drawn with a
     # equal chance
     n_folds = 5
-    n_iter = 1000
+    n_splits = 1000
 
     def assert_counts_are_ok(idx_counts, p):
         # Here we test that the distribution of the counts
@@ -577,19 +579,19 @@ def assert_counts_are_ok(idx_counts, p):
 
     for n_samples in (6, 22):
         labels = np.array((n_samples // 2) * [0, 1])
-        splits = StratifiedShuffleSplit(n_iter=n_iter,
+        splits = StratifiedShuffleSplit(n_splits=n_splits,
                                         test_size=1. / n_folds,
                                         random_state=0)
 
         train_counts = [0] * n_samples
         test_counts = [0] * n_samples
-        n_splits = 0
+        n_splits_actual = 0
         for train, test in splits.split(X=np.ones(n_samples), y=labels):
-            n_splits += 1
+            n_splits_actual += 1
             for counter, ids in [(train_counts, train), (test_counts, test)]:
                 for id in ids:
                     counter[id] += 1
-        assert_equal(n_splits, n_iter)
+        assert_equal(n_splits_actual, n_splits)
 
         n_train, n_test = _validate_shuffle_split(n_samples,
                                                   test_size=1./n_folds,
@@ -616,10 +618,10 @@ def test_stratified_shuffle_split_overlap_train_test_bug():
     y = [0, 1, 2, 3] * 3 + [4, 5] * 5
     X = np.ones_like(y)
 
-    splits = StratifiedShuffleSplit(n_iter=1,
-                                    test_size=0.5, random_state=0)
+    sss = StratifiedShuffleSplit(n_splits=1,
+                                 test_size=0.5, random_state=0)
 
-    train, test = next(iter(splits.split(X=X, y=y)))
+    train, test = next(iter(sss.split(X=X, y=y)))
 
     assert_array_equal(np.intersect1d(train, test), [])
 
@@ -653,15 +655,15 @@ def test_label_shuffle_split():
 
     for l in labels:
         X = y = np.ones(len(l))
-        n_iter = 6
+        n_splits = 6
         test_size = 1./3
-        slo = LabelShuffleSplit(n_iter, test_size=test_size, random_state=0)
+        slo = LabelShuffleSplit(n_splits, test_size=test_size, random_state=0)
 
         # Make sure the repr works
         repr(slo)
 
         # Test that the length is correct
-        assert_equal(slo.get_n_splits(X, y, labels=l), n_iter)
+        assert_equal(slo.get_n_splits(X, y, labels=l), n_splits)
 
         l_unique = np.unique(l)
 
@@ -782,6 +784,18 @@ def train_test_split_pandas():
         assert_true(isinstance(X_test, InputFeatureType))
 
 
+def train_test_split_sparse():
+    # check that train_test_split converts scipy sparse matrices
+    # to csr, as stated in the documentation
+    X = np.arange(100).reshape((10, 10))
+    sparse_types = [csr_matrix, csc_matrix, coo_matrix]
+    for InputFeatureType in sparse_types:
+        X_s = InputFeatureType(X)
+        X_train, X_test = train_test_split(X_s)
+        assert_true(isinstance(X_train, csr_matrix))
+        assert_true(isinstance(X_test, csr_matrix))
+
+
 def train_test_split_mock_pandas():
     # X mock dataframe
     X_df = MockDataFrame(X)
@@ -906,7 +920,7 @@ def test_label_kfold():
     # Parameters of the test
     n_labels = 15
     n_samples = 1000
-    n_folds = 5
+    n_splits = 5
 
     X = y = np.ones(n_samples)
 
@@ -914,12 +928,12 @@ def test_label_kfold():
     tolerance = 0.05 * n_samples  # 5 percent error allowed
     labels = rng.randint(0, n_labels, n_samples)
 
-    ideal_n_labels_per_fold = n_samples // n_folds
+    ideal_n_labels_per_fold = n_samples // n_splits
 
     len(np.unique(labels))
     # Get the test fold indices from the test set indices of each fold
     folds = np.zeros(n_samples)
-    lkf = LabelKFold(n_folds=n_folds)
+    lkf = LabelKFold(n_splits=n_splits)
     for i, (_, test) in enumerate(lkf.split(X, y, labels)):
         folds[test] = i
 
@@ -949,9 +963,9 @@ def test_label_kfold():
 
     n_labels = len(np.unique(labels))
     n_samples = len(labels)
-    n_folds = 5
+    n_splits = 5
     tolerance = 0.05 * n_samples  # 5 percent error allowed
-    ideal_n_labels_per_fold = n_samples // n_folds
+    ideal_n_labels_per_fold = n_samples // n_splits
 
     X = y = np.ones(n_samples)
 
@@ -980,8 +994,46 @@ def test_label_kfold():
     # Should fail if there are more folds than labels
     labels = np.array([1, 1, 1, 2, 2])
     X = y = np.ones(len(labels))
+    assert_raises_regexp(ValueError, "Cannot have number of splits.*greater",
+                         next, LabelKFold(n_splits=3).split(X, y, labels))
+
+
+def test_time_series_cv():
+    X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14]]
+
+    # Should fail if there are more folds than samples
     assert_raises_regexp(ValueError, "Cannot have number of folds.*greater",
-                         next, LabelKFold(n_folds=3).split(X, y, labels))
+                         next,
+                         TimeSeriesSplit(n_splits=7).split(X))
+
+    tscv = TimeSeriesSplit(2)
+
+    # Manually check that Time Series CV preserves the data
+    # ordering on toy datasets
+    splits = tscv.split(X[:-1])
+    train, test = next(splits)
+    assert_array_equal(train, [0, 1])
+    assert_array_equal(test, [2, 3])
+
+    train, test = next(splits)
+    assert_array_equal(train, [0, 1, 2, 3])
+    assert_array_equal(test, [4, 5])
+
+    splits = TimeSeriesSplit(2).split(X)
+
+    train, test = next(splits)
+    assert_array_equal(train, [0, 1, 2])
+    assert_array_equal(test, [3, 4])
+
+    train, test = next(splits)
+    assert_array_equal(train, [0, 1, 2, 3, 4])
+    assert_array_equal(test, [5, 6])
+
+    # Check get_n_splits returns the correct number of splits
+    splits = TimeSeriesSplit(2).split(X)
+    n_splits_actual = len(list(splits))
+    assert_equal(n_splits_actual, tscv.get_n_splits())
+    assert_equal(n_splits_actual, 2)
 
 
 def test_nested_cv():
@@ -992,7 +1044,7 @@ def test_nested_cv():
     labels = rng.randint(0, 5, 15)
 
     cvs = [LeaveOneLabelOut(), LeaveOneOut(), LabelKFold(), StratifiedKFold(),
-           StratifiedShuffleSplit(n_iter=3, random_state=0)]
+           StratifiedShuffleSplit(n_splits=3, random_state=0)]
 
     for inner_cv, outer_cv in combinations_with_replacement(cvs, 2):
         gs = GridSearchCV(Ridge(), param_grid={'alpha': [1, .1]},
diff --git a/sklearn/model_selection/tests/test_validation.py b/sklearn/model_selection/tests/test_validation.py
index 2e694fd45e59..62a86000562f 100644
--- a/sklearn/model_selection/tests/test_validation.py
+++ b/sklearn/model_selection/tests/test_validation.py
@@ -136,7 +136,8 @@ def _is_training_data(self, X):
 X = np.ones((10, 2))
 X_sparse = coo_matrix(X)
 y = np.array([0, 0, 1, 1, 2, 2, 3, 3, 4, 4])
-# The number of samples per class needs to be > n_folds, for StratifiedKFold(3)
+# The number of samples per class needs to be > n_splits,
+# for StratifiedKFold(n_splits=3)
 y2 = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 3])
 
 
@@ -701,7 +702,7 @@ def test_learning_curve_with_boolean_indices():
                                n_redundant=0, n_classes=2,
                                n_clusters_per_class=1, random_state=0)
     estimator = MockImprovingEstimator(20)
-    cv = KFold(n_folds=3)
+    cv = KFold(n_splits=3)
     train_sizes, train_scores, test_scores = learning_curve(
         estimator, X, y, cv=cv, train_sizes=np.linspace(0.1, 1.0, 10))
     assert_array_equal(train_sizes, np.linspace(2, 20, 10))
diff --git a/sklearn/naive_bayes.py b/sklearn/naive_bayes.py
index 9e3e0fb6f434..6b0623843cec 100644
--- a/sklearn/naive_bayes.py
+++ b/sklearn/naive_bayes.py
@@ -388,7 +388,7 @@ def _partial_fit(self, X, y, classes=None, _refit=False,
         if not np.all(unique_y_in_classes):
             raise ValueError("The target label(s) %s in y do not exist in the "
                              "initial classes %s" %
-                             (y[~unique_y_in_classes], classes))
+                             (unique_y[~unique_y_in_classes], classes))
 
         for y_i in unique_y:
             i = classes.searchsorted(y_i)
diff --git a/sklearn/neural_network/_base.py b/sklearn/neural_network/_base.py
index 225ab6dfb792..9bbaba6d7ee1 100644
--- a/sklearn/neural_network/_base.py
+++ b/sklearn/neural_network/_base.py
@@ -99,64 +99,78 @@ def softmax(X):
                'relu': relu, 'softmax': softmax}
 
 
-def inplace_logistic_derivative(Z):
-    """Compute the derivative of the logistic function given output value
-    from logistic function
+def inplace_identity_derivative(Z, delta):
+    """Apply the derivative of the identity function: do nothing.
+
+    Parameters
+    ----------
+    Z : {array-like, sparse matrix}, shape (n_samples, n_features)
+        The data which was output from the identity activation function during
+        the forward pass.
+
+    delta : {array-like}, shape (n_samples, n_features)
+         The backpropagated error signal to be modified inplace.
+    """
+    # Nothing to do
+
+
+def inplace_logistic_derivative(Z, delta):
+    """Apply the derivative of the logistic sigmoid function.
 
     It exploits the fact that the derivative is a simple function of the output
-    value from logistic function
+    value from logistic function.
 
     Parameters
     ----------
     Z : {array-like, sparse matrix}, shape (n_samples, n_features)
-        The input data which is output from logistic function
+        The data which was output from the logistic activation function during
+        the forward pass.
 
-    Returns
-    -------
-    Z_new : {array-like, sparse matrix}, shape (n_samples, n_features)
-        The transformed data.
+    delta : {array-like}, shape (n_samples, n_features)
+         The backpropagated error signal to be modified inplace.
     """
-    return Z * (1 - Z)
+    delta *= Z
+    delta *= (1 - Z)
 
 
-def inplace_tanh_derivative(Z):
-    """Compute the derivative of the hyperbolic tan function given output value
-    from hyperbolic tan
+def inplace_tanh_derivative(Z, delta):
+    """Apply the derivative of the hyperbolic tanh function.
 
     It exploits the fact that the derivative is a simple function of the output
-    value from hyperbolic tan
+    value from hyperbolic tangent.
 
     Parameters
     ----------
     Z : {array-like, sparse matrix}, shape (n_samples, n_features)
-        The input data which is output from hyperbolic tan function
+        The data which was output from the hyperbolic tangent activation
+        function during the forward pass.
 
-    Returns
-    -------
-    Z_new : {array-like, sparse matrix}, shape (n_samples, n_features)
-        The transformed data.
+    delta : {array-like}, shape (n_samples, n_features)
+         The backpropagated error signal to be modified inplace.
     """
-    return 1 - (Z ** 2)
+    delta *= (1 - Z ** 2)
 
 
-def inplace_relu_derivative(Z):
-    """Compute the derivative of the rectified linear unit function given output
-    value from relu
+def inplace_relu_derivative(Z, delta):
+    """Apply the derivative of the relu function.
+
+    It exploits the fact that the derivative is a simple function of the output
+    value from rectified linear units activation function.
 
     Parameters
     ----------
     Z : {array-like, sparse matrix}, shape (n_samples, n_features)
-        The input data which is output from some relu
+        The data which was output from the rectified linear units activation
+        function during the forward pass.
 
-    Returns
-    -------
-    Z_new : {array-like, sparse matrix}, shape (n_samples, n_features)
-        The transformed data.
+    delta : {array-like}, shape (n_samples, n_features)
+         The backpropagated error signal to be modified inplace.
     """
-    return (Z > 0).astype(Z.dtype)
+    delta[Z == 0] = 0
 
 
-DERIVATIVES = {'tanh': inplace_tanh_derivative,
+DERIVATIVES = {'identity': inplace_identity_derivative,
+               'tanh': inplace_tanh_derivative,
                'logistic': inplace_logistic_derivative,
                'relu': inplace_relu_derivative}
 
diff --git a/sklearn/neural_network/multilayer_perceptron.py b/sklearn/neural_network/multilayer_perceptron.py
index c4a22e5ef7eb..72562aafce02 100644
--- a/sklearn/neural_network/multilayer_perceptron.py
+++ b/sklearn/neural_network/multilayer_perceptron.py
@@ -235,8 +235,7 @@ def _backprop(self, X, y, activations, deltas, coef_grads,
         # combinations of output activation and loss function:
         # sigmoid and binary cross entropy, softmax and categorical cross
         # entropy, and identity with squared loss
-        diff = y - activations[-1]
-        deltas[last] = -diff
+        deltas[last] = activations[-1] - y
 
         # Compute gradient for the last layer
         coef_grads, intercept_grads = self._compute_loss_grad(
@@ -245,8 +244,8 @@ def _backprop(self, X, y, activations, deltas, coef_grads,
         # Iterate over the hidden layers
         for i in range(self.n_layers_ - 2, 0, -1):
             deltas[i - 1] = safe_sparse_dot(deltas[i], self.coefs_[i].T)
-            derivative = DERIVATIVES[self.activation]
-            deltas[i - 1] *= derivative(activations[i])
+            inplace_derivative = DERIVATIVES[self.activation]
+            inplace_derivative(activations[i], deltas[i - 1])
 
             coef_grads, intercept_grads = self._compute_loss_grad(
                 i - 1, n_samples, activations, deltas, coef_grads,
@@ -302,9 +301,7 @@ def _init_coef(self, fan_in, fan_out, rng):
             # Use the initialization method recommended by
             # Glorot et al.
             init_bound = np.sqrt(2. / (fan_in + fan_out))
-        elif self.activation == 'tanh':
-            init_bound = np.sqrt(6. / (fan_in + fan_out))
-        elif self.activation == 'relu':
+        elif self.activation in ('identity', 'tanh', 'relu'):
             init_bound = np.sqrt(6. / (fan_in + fan_out))
         else:
             # this was caught earlier, just to make sure
@@ -414,7 +411,7 @@ def _validate_hyperparameters(self):
             raise ValueError("epsilon must be > 0, got %s." % self.epsilon)
 
         # raise ValueError if not registered
-        supported_activations = ['logistic', 'tanh', 'relu']
+        supported_activations = ('identity', 'logistic', 'tanh', 'relu')
         if self.activation not in supported_activations:
             raise ValueError("The activation '%s' is not supported. Supported "
                              "activations are %s." % (self.activation,
@@ -688,9 +685,12 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
         The ith element represents the number of neurons in the ith
         hidden layer.
 
-    activation : {'logistic', 'tanh', 'relu'}, default 'relu'
+    activation : {'identity', 'logistic', 'tanh', 'relu'}, default 'relu'
         Activation function for the hidden layer.
 
+        - 'identity', no-op activation, useful to implement linear bottleneck,
+          returns f(x) = x
+
         - 'logistic', the logistic sigmoid function,
           returns f(x) = 1 / (1 + exp(-x)).
 
@@ -728,20 +728,20 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
     learning_rate : {'constant', 'invscaling', 'adaptive'}, default 'constant'
         Learning rate schedule for weight updates.
 
-        -'constant', is a constant learning rate given by
-         'learning_rate_init'.
+        - 'constant' is a constant learning rate given by
+          'learning_rate_init'.
 
-        -'invscaling' gradually decreases the learning rate ``learning_rate_`` at
-          each time step 't' using an inverse scaling exponent of 'power_t'.
+        - 'invscaling' gradually decreases the learning rate ``learning_rate_``
+          at each time step 't' using an inverse scaling exponent of 'power_t'.
           effective_learning_rate = learning_rate_init / pow(t, power_t)
 
-        -'adaptive', keeps the learning rate constant to
-         'learning_rate_init' as long as training loss keeps decreasing.
-         Each time two consecutive epochs fail to decrease training loss by at
-         least tol, or fail to increase validation score by at least tol if
-         'early_stopping' is on, the current learning rate is divided by 5.
+        - 'adaptive' keeps the learning rate constant to
+          'learning_rate_init' as long as training loss keeps decreasing.
+          Each time two consecutive epochs fail to decrease training loss by at
+          least tol, or fail to increase validation score by at least tol if
+          'early_stopping' is on, the current learning rate is divided by 5.
 
-         Only used when algorithm='sgd'.
+        Only used when ``algorithm='sgd'``.
 
     max_iter : int, optional, default 200
         Maximum number of iterations. The algorithm iterates until convergence
@@ -1042,9 +1042,12 @@ class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):
         The ith element represents the number of neurons in the ith
         hidden layer.
 
-    activation : {'logistic', 'tanh', 'relu'}, default 'relu'
+    activation : {'identity', 'logistic', 'tanh', 'relu'}, default 'relu'
         Activation function for the hidden layer.
 
+        - 'identity', no-op activation, useful to implement linear bottleneck,
+          returns f(x) = x
+
         - 'logistic', the logistic sigmoid function,
           returns f(x) = 1 / (1 + exp(-x)).
 
@@ -1082,20 +1085,20 @@ class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):
     learning_rate : {'constant', 'invscaling', 'adaptive'}, default 'constant'
         Learning rate schedule for weight updates.
 
-        -'constant', is a constant learning rate given by
-         'learning_rate_init'.
+        - 'constant' is a constant learning rate given by
+          'learning_rate_init'.
 
-        -'invscaling' gradually decreases the learning rate ``learning_rate_`` at
-          each time step 't' using an inverse scaling exponent of 'power_t'.
+        - 'invscaling' gradually decreases the learning rate ``learning_rate_``
+          at each time step 't' using an inverse scaling exponent of 'power_t'.
           effective_learning_rate = learning_rate_init / pow(t, power_t)
 
-        -'adaptive', keeps the learning rate constant to
-         'learning_rate_init' as long as training loss keeps decreasing.
-         Each time two consecutive epochs fail to decrease training loss by at
-         least tol, or fail to increase validation score by at least tol if
-         'early_stopping' is on, the current learning rate is divided by 5.
+        - 'adaptive' keeps the learning rate constant to
+          'learning_rate_init' as long as training loss keeps decreasing.
+          Each time two consecutive epochs fail to decrease training loss by at
+          least tol, or fail to increase validation score by at least tol if
+          'early_stopping' is on, the current learning rate is divided by 5.
 
-         Only used when algorithm='sgd'.
+        Only used when algorithm='sgd'.
 
     max_iter : int, optional, default 200
         Maximum number of iterations. The algorithm iterates until convergence
diff --git a/sklearn/neural_network/rbm.py b/sklearn/neural_network/rbm.py
index da872c8b893c..bdbc04e6be84 100644
--- a/sklearn/neural_network/rbm.py
+++ b/sklearn/neural_network/rbm.py
@@ -243,7 +243,7 @@ def partial_fit(self, X, y=None):
                     0.01,
                     (self.n_components, X.shape[1])
                 ),
-                order='fortran')
+                order='F')
         if not hasattr(self, 'intercept_hidden_'):
             self.intercept_hidden_ = np.zeros(self.n_components, )
         if not hasattr(self, 'intercept_visible_'):
@@ -340,7 +340,7 @@ def fit(self, X, y=None):
 
         self.components_ = np.asarray(
             rng.normal(0, 0.01, (self.n_components, X.shape[1])),
-            order='fortran')
+            order='F')
         self.intercept_hidden_ = np.zeros(self.n_components, )
         self.intercept_visible_ = np.zeros(X.shape[1], )
         self.h_samples_ = np.zeros((self.batch_size, self.n_components))
diff --git a/sklearn/neural_network/tests/test_mlp.py b/sklearn/neural_network/tests/test_mlp.py
index bdf79ba1973d..e4f080058173 100644
--- a/sklearn/neural_network/tests/test_mlp.py
+++ b/sklearn/neural_network/tests/test_mlp.py
@@ -14,6 +14,7 @@
 
 from sklearn.datasets import load_digits, load_boston
 from sklearn.datasets import make_regression, make_multilabel_classification
+from sklearn.exceptions import ConvergenceWarning
 from sklearn.externals.six.moves import cStringIO as StringIO
 from sklearn.metrics import roc_auc_score
 from sklearn.neural_network import MLPClassifier
@@ -22,12 +23,12 @@
 from sklearn.preprocessing import StandardScaler, MinMaxScaler
 from scipy.sparse import csr_matrix
 from sklearn.utils.testing import (assert_raises, assert_greater, assert_equal,
-                                   assert_false)
+                                   assert_false, ignore_warnings)
 
 
 np.seterr(all='warn')
 
-ACTIVATION_TYPES = ["logistic", "tanh", "relu"]
+ACTIVATION_TYPES = ["identity", "logistic", "tanh", "relu"]
 
 digits_dataset_multi = load_digits(n_class=3)
 
@@ -60,7 +61,8 @@ def test_alpha():
 
     for alpha in alpha_values:
         mlp = MLPClassifier(hidden_layer_sizes=10, alpha=alpha, random_state=1)
-        mlp.fit(X, y)
+        with ignore_warnings(category=ConvergenceWarning):
+            mlp.fit(X, y)
         alpha_vectors.append(np.array([absolute_sum(mlp.coefs_[0]),
                                        absolute_sum(mlp.coefs_[1])]))
 
@@ -254,21 +256,26 @@ def test_lbfgs_regression():
                            max_iter=150, shuffle=True, random_state=1,
                            activation=activation)
         mlp.fit(X, y)
-        assert_greater(mlp.score(X, y), 0.95)
+        if activation == 'identity':
+            assert_greater(mlp.score(X, y), 0.84)
+        else:
+            # Non linear models perform much better than linear bottleneck:
+            assert_greater(mlp.score(X, y), 0.95)
 
 
 def test_learning_rate_warmstart():
-    # Tests that warm_start reuses past solution."""
+    # Test that warm_start reuses past solution."""
     X = [[3, 2], [1, 6], [5, 6], [-2, -4]]
     y = [1, 1, 1, 0]
     for learning_rate in ["invscaling", "constant"]:
         mlp = MLPClassifier(algorithm='sgd', hidden_layer_sizes=4,
                             learning_rate=learning_rate, max_iter=1,
                             power_t=0.25, warm_start=True)
-        mlp.fit(X, y)
-        prev_eta = mlp._optimizer.learning_rate
-        mlp.fit(X, y)
-        post_eta = mlp._optimizer.learning_rate
+        with ignore_warnings(category=ConvergenceWarning):
+            mlp.fit(X, y)
+            prev_eta = mlp._optimizer.learning_rate
+            mlp.fit(X, y)
+            post_eta = mlp._optimizer.learning_rate
 
         if learning_rate == 'constant':
             assert_equal(prev_eta, post_eta)
@@ -317,7 +324,7 @@ def test_partial_fit_classes_error():
 
 def test_partial_fit_classification():
     # Test partial_fit on classification.
-    # `partial_fit` should yield the same results as 'fit'for binary and
+    # `partial_fit` should yield the same results as 'fit' for binary and
     # multi-class classification.
     for X, y in classification_datasets:
         X = X
@@ -325,7 +332,8 @@ def test_partial_fit_classification():
         mlp = MLPClassifier(algorithm='sgd', max_iter=100, random_state=1,
                             tol=0, alpha=1e-5, learning_rate_init=0.2)
 
-        mlp.fit(X, y)
+        with ignore_warnings(category=ConvergenceWarning):
+            mlp.fit(X, y)
         pred1 = mlp.predict(X)
         mlp = MLPClassifier(algorithm='sgd', random_state=1, alpha=1e-5,
                             learning_rate_init=0.2)
@@ -401,7 +409,8 @@ def test_predict_proba_binary():
     y = y_digits_binary[:50]
 
     clf = MLPClassifier(hidden_layer_sizes=5)
-    clf.fit(X, y)
+    with ignore_warnings(category=ConvergenceWarning):
+        clf.fit(X, y)
     y_proba = clf.predict_proba(X)
     y_log_proba = clf.predict_log_proba(X)
 
@@ -423,7 +432,8 @@ def test_predict_proba_multi():
     y = y_digits_multi[:10]
 
     clf = MLPClassifier(hidden_layer_sizes=5)
-    clf.fit(X, y)
+    with ignore_warnings(category=ConvergenceWarning):
+        clf.fit(X, y)
     y_proba = clf.predict_proba(X)
     y_log_proba = clf.predict_log_proba(X)
 
@@ -443,10 +453,11 @@ def test_sparse_matrices():
     y = y_digits_binary[:50]
     X_sparse = csr_matrix(X)
     mlp = MLPClassifier(random_state=1, hidden_layer_sizes=15)
-    mlp.fit(X, y)
-    pred1 = mlp.decision_function(X)
-    mlp.fit(X_sparse, y)
-    pred2 = mlp.decision_function(X_sparse)
+    with ignore_warnings(category=ConvergenceWarning):
+        mlp.fit(X, y)
+        pred1 = mlp.decision_function(X)
+        mlp.fit(X_sparse, y)
+        pred2 = mlp.decision_function(X_sparse)
     assert_almost_equal(pred1, pred2)
     pred1 = mlp.predict(X)
     pred2 = mlp.predict(X_sparse)
@@ -458,7 +469,7 @@ def test_tolerance():
     # It should force the algorithm to exit the loop when it converges.
     X = [[3, 2], [1, 6]]
     y = [1, 0]
-    clf = MLPClassifier(tol=0.5, max_iter=3000, algorithm='sgd', verbose=10)
+    clf = MLPClassifier(tol=0.5, max_iter=3000, algorithm='sgd')
     clf.fit(X, y)
     assert_greater(clf.max_iter, clf.n_iter_)
 
@@ -472,7 +483,8 @@ def test_verbose_sgd():
     old_stdout = sys.stdout
     sys.stdout = output = StringIO()
 
-    clf.fit(X, y)
+    with ignore_warnings(category=ConvergenceWarning):
+        clf.fit(X, y)
     clf.partial_fit(X, y)
 
     sys.stdout = old_stdout
@@ -499,7 +511,7 @@ def test_adaptive_learning_rate():
     X = [[3, 2], [1, 6]]
     y = [1, 0]
     clf = MLPClassifier(tol=0.5, max_iter=3000, algorithm='sgd',
-                        learning_rate='adaptive', verbose=10)
+                        learning_rate='adaptive')
     clf.fit(X, y)
     assert_greater(clf.max_iter, clf.n_iter_)
     assert_greater(1e-6, clf._optimizer.learning_rate)
diff --git a/sklearn/pipeline.py b/sklearn/pipeline.py
index 87770947386d..91e4fef0ec4d 100644
--- a/sklearn/pipeline.py
+++ b/sklearn/pipeline.py
@@ -11,6 +11,7 @@
 
 from collections import defaultdict
 from warnings import warn
+from abc import ABCMeta, abstractmethod
 
 import numpy as np
 from scipy import sparse
@@ -20,12 +21,69 @@
 from .externals import six
 from .utils import tosequence
 from .utils.metaestimators import if_delegate_has_method
-from .externals.six import iteritems
 
 __all__ = ['Pipeline', 'FeatureUnion']
 
 
-class Pipeline(BaseEstimator):
+class _BasePipeline(six.with_metaclass(ABCMeta, BaseEstimator)):
+    """Handles parameter management for classifiers composed of named steps.
+    """
+
+    @abstractmethod
+    def __init__(self):
+        pass
+
+    def _replace_step(self, steps_attr, name, new_val):
+        # assumes `name` is a valid step name
+        new_steps = getattr(self, steps_attr)[:]
+        for i, (step_name, _) in enumerate(new_steps):
+            if step_name == name:
+                new_steps[i] = (name, new_val)
+                break
+        setattr(self, steps_attr, new_steps)
+
+    def _get_params(self, steps_attr, deep=True):
+        out = super(_BasePipeline, self).get_params(deep=False)
+        if not deep:
+            return out
+        steps = getattr(self, steps_attr)
+        out.update(steps)
+        for name, estimator in steps:
+            if estimator is None:
+                continue
+            for key, value in six.iteritems(estimator.get_params(deep=True)):
+                out['%s__%s' % (name, key)] = value
+        return out
+
+    def _set_params(self, steps_attr, **params):
+        # Ensure strict ordering of parameter setting:
+        # 1. All steps
+        if steps_attr in params:
+            setattr(self, steps_attr, params.pop(steps_attr))
+        # 2. Step replacement
+        step_names, _ = zip(*getattr(self, steps_attr))
+        for name in list(six.iterkeys(params)):
+            if '__' not in name and name in step_names:
+                self._replace_step(steps_attr, name, params.pop(name))
+        # 3. Step parameters and other initilisation arguments
+        super(_BasePipeline, self).set_params(**params)
+        return self
+
+    def _validate_names(self, names):
+        if len(set(names)) != len(names):
+            raise ValueError('Names provided are not unique: '
+                             '{0!r}'.format(list(names)))
+        invalid_names = set(names).intersection(self.get_params(deep=False))
+        if invalid_names:
+            raise ValueError('Step names conflict with constructor arguments: '
+                             '{0!r}'.format(sorted(invalid_names)))
+        invalid_names = [name for name in names if '__' in name]
+        if invalid_names:
+            raise ValueError('Step names must not contain __: got '
+                             '{0!r}'.format(invalid_names))
+
+
+class Pipeline(_BasePipeline):
     """Pipeline of transforms with a final estimator.
 
     Sequentially apply a list of transforms and a final estimator.
@@ -37,6 +95,9 @@ class Pipeline(BaseEstimator):
     cross-validated together while setting different parameters.
     For this, it enables setting parameters of the various steps using their
     names and the parameter name separated by a '__', as in the example below.
+    A step's estimator may be replaced entirely by setting the parameter
+    with its name to another estimator, or a transformer removed by setting
+    to None.
 
     Read more in the :ref:`User Guide <pipeline>`.
 
@@ -87,44 +148,67 @@ class Pipeline(BaseEstimator):
     # BaseEstimator interface
 
     def __init__(self, steps):
-        names, estimators = zip(*steps)
-        if len(dict(steps)) != len(steps):
-            raise ValueError("Provided step names are not unique: %s"
-                             % (names,))
-
         # shallow copy of steps
         self.steps = tosequence(steps)
-        transforms = estimators[:-1]
+        self._validate_steps()
+
+    def get_params(self, deep=True):
+        """Get parameters for this estimator.
+
+        Parameters
+        ----------
+        deep: boolean, optional
+            If True, will return the parameters for this estimator and
+            contained subobjects that are estimators.
+
+        Returns
+        -------
+        params : mapping of string to any
+            Parameter names mapped to their values.
+        """
+        return self._get_params('steps', deep=deep)
+
+    def set_params(self, **kwargs):
+        """Set the parameters of this estimator.
+
+        Valid parameter keys can be listed with ``get_params()``.
+
+        Returns
+        -------
+        self
+        """
+        self._set_params('steps', **kwargs)
+        return self
+
+    def _validate_steps(self):
+        names, estimators = zip(*self.steps)
+
+        # validate names
+        self._validate_names(names)
+
+        # validate estimators
+        transformers = estimators[:-1]
         estimator = estimators[-1]
 
-        for t in transforms:
+        for t in transformers:
+            if t is None:
+                continue
             if (not (hasattr(t, "fit") or hasattr(t, "fit_transform")) or not
                     hasattr(t, "transform")):
-                raise TypeError("All intermediate steps of the chain should "
-                                "be transforms and implement fit and transform"
-                                " '%s' (type %s) doesn't)" % (t, type(t)))
-
-        if not hasattr(estimator, "fit"):
-            raise TypeError("Last step of chain should implement fit "
-                            "'%s' (type %s) doesn't)"
+                raise TypeError("All intermediate steps should be "
+                                "transformers and implement fit and transform."
+                                " '%s' (type %s) doesn't" % (t, type(t)))
+
+        # We allow last estimator to be None as an identity transformation
+        if estimator is not None and not hasattr(estimator, "fit"):
+            raise TypeError("Last step of Pipeline should implement fit. "
+                            "'%s' (type %s) doesn't"
                             % (estimator, type(estimator)))
 
     @property
     def _estimator_type(self):
         return self.steps[-1][1]._estimator_type
 
-    def get_params(self, deep=True):
-        if not deep:
-            return super(Pipeline, self).get_params(deep=False)
-        else:
-            out = self.named_steps
-            for name, step in six.iteritems(self.named_steps):
-                for key, value in six.iteritems(step.get_params(deep=True)):
-                    out['%s__%s' % (name, key)] = value
-
-            out.update(super(Pipeline, self).get_params(deep=False))
-            return out
-
     @property
     def named_steps(self):
         return dict(self.steps)
@@ -135,22 +219,30 @@ def _final_estimator(self):
 
     # Estimator interface
 
-    def _pre_transform(self, X, y=None, **fit_params):
-        fit_params_steps = dict((step, {}) for step, _ in self.steps)
+    def _fit(self, X, y=None, **fit_params):
+        self._validate_steps()
+        fit_params_steps = dict((name, {}) for name, step in self.steps
+                                if step is not None)
         for pname, pval in six.iteritems(fit_params):
             step, param = pname.split('__', 1)
             fit_params_steps[step][param] = pval
         Xt = X
         for name, transform in self.steps[:-1]:
-            if hasattr(transform, "fit_transform"):
+            if transform is None:
+                pass
+            elif hasattr(transform, "fit_transform"):
                 Xt = transform.fit_transform(Xt, y, **fit_params_steps[name])
             else:
                 Xt = transform.fit(Xt, y, **fit_params_steps[name]) \
                               .transform(Xt)
+        if self._final_estimator is None:
+            return Xt, {}
         return Xt, fit_params_steps[self.steps[-1][0]]
 
     def fit(self, X, y=None, **fit_params):
-        """Fit all the transforms one after the other and transform the
+        """Fit the model
+
+        Fit all the transforms one after the other and transform the
         data, then fit the transformed data using the final estimator.
 
         Parameters
@@ -158,17 +250,31 @@ def fit(self, X, y=None, **fit_params):
         X : iterable
             Training data. Must fulfill input requirements of first step of the
             pipeline.
+
         y : iterable, default=None
             Training targets. Must fulfill label requirements for all steps of
             the pipeline.
+
+        **fit_params : dict of string -> object
+            Parameters passed to the ``fit`` method of each step, where
+            each parameter name is prefixed such that parameter ``p`` for step
+            ``s`` has key ``s__p``.
+
+        Returns
+        -------
+        self : Pipeline
+            This estimator
         """
-        Xt, fit_params = self._pre_transform(X, y, **fit_params)
-        self.steps[-1][-1].fit(Xt, y, **fit_params)
+        Xt, fit_params = self._fit(X, y, **fit_params)
+        if self._final_estimator is not None:
+            self._final_estimator.fit(Xt, y, **fit_params)
         return self
 
     def fit_transform(self, X, y=None, **fit_params):
-        """Fit all the transforms one after the other and transform the
-        data, then use fit_transform on transformed data using the final
+        """Fit the model and transform with the final estimator
+
+        Fits all the transforms one after the other and transforms the
+        data, then uses fit_transform on transformed data with the final
         estimator.
 
         Parameters
@@ -180,28 +286,44 @@ def fit_transform(self, X, y=None, **fit_params):
         y : iterable, default=None
             Training targets. Must fulfill label requirements for all steps of
             the pipeline.
+
+        **fit_params : dict of string -> object
+            Parameters passed to the ``fit`` method of each step, where
+            each parameter name is prefixed such that parameter ``p`` for step
+            ``s`` has key ``s__p``.
+
+        Returns
+        -------
+        Xt : array-like, shape = [n_samples, n_transformed_features]
+            Transformed samples
         """
-        Xt, fit_params = self._pre_transform(X, y, **fit_params)
-        if hasattr(self.steps[-1][-1], 'fit_transform'):
-            return self.steps[-1][-1].fit_transform(Xt, y, **fit_params)
+        last_step = self._final_estimator
+        Xt, fit_params = self._fit(X, y, **fit_params)
+        if hasattr(last_step, 'fit_transform'):
+            return last_step.fit_transform(Xt, y, **fit_params)
+        elif last_step is None:
+            return Xt
         else:
-            return self.steps[-1][-1].fit(Xt, y, **fit_params).transform(Xt)
+            return last_step.fit(Xt, y, **fit_params).transform(Xt)
 
     @if_delegate_has_method(delegate='_final_estimator')
     def predict(self, X):
-        """Applies transforms to the data, and the predict method of the
-        final estimator. Valid only if the final estimator implements
-        predict.
+        """Apply transforms to the data, and predict with the final estimator
 
         Parameters
         ----------
         X : iterable
             Data to predict on. Must fulfill input requirements of first step
             of the pipeline.
+
+        Returns
+        -------
+        y_pred : array-like
         """
         Xt = X
         for name, transform in self.steps[:-1]:
-            Xt = transform.transform(Xt)
+            if transform is not None:
+                Xt = transform.transform(Xt)
         return self.steps[-1][-1].predict(Xt)
 
     @if_delegate_has_method(delegate='_final_estimator')
@@ -217,122 +339,172 @@ def fit_predict(self, X, y=None, **fit_params):
         X : iterable
             Training data. Must fulfill input requirements of first step of
             the pipeline.
+
         y : iterable, default=None
             Training targets. Must fulfill label requirements for all steps
             of the pipeline.
+
+        **fit_params : dict of string -> object
+            Parameters passed to the ``fit`` method of each step, where
+            each parameter name is prefixed such that parameter ``p`` for step
+            ``s`` has key ``s__p``.
+
+        Returns
+        -------
+        y_pred : array-like
         """
-        Xt, fit_params = self._pre_transform(X, y, **fit_params)
+        Xt = X
+        for name, transform in self.steps[:-1]:
+            if transform is not None:
+                Xt = transform.transform(Xt)
         return self.steps[-1][-1].fit_predict(Xt, y, **fit_params)
 
     @if_delegate_has_method(delegate='_final_estimator')
     def predict_proba(self, X):
-        """Applies transforms to the data, and the predict_proba method of the
-        final estimator. Valid only if the final estimator implements
-        predict_proba.
+        """Apply transforms, and predict_proba of the final estimator
 
         Parameters
         ----------
         X : iterable
             Data to predict on. Must fulfill input requirements of first step
             of the pipeline.
+
+        Returns
+        -------
+        y_proba : array-like, shape = [n_samples, n_classes]
         """
         Xt = X
         for name, transform in self.steps[:-1]:
-            Xt = transform.transform(Xt)
+            if transform is not None:
+                Xt = transform.transform(Xt)
         return self.steps[-1][-1].predict_proba(Xt)
 
     @if_delegate_has_method(delegate='_final_estimator')
     def decision_function(self, X):
-        """Applies transforms to the data, and the decision_function method of
-        the final estimator. Valid only if the final estimator implements
-        decision_function.
+        """Apply transforms, and decision_function of the final estimator
 
         Parameters
         ----------
         X : iterable
             Data to predict on. Must fulfill input requirements of first step
             of the pipeline.
+
+        Returns
+        -------
+        y_score : array-like, shape = [n_samples, n_classes]
         """
         Xt = X
         for name, transform in self.steps[:-1]:
-            Xt = transform.transform(Xt)
+            if transform is not None:
+                Xt = transform.transform(Xt)
         return self.steps[-1][-1].decision_function(Xt)
 
     @if_delegate_has_method(delegate='_final_estimator')
     def predict_log_proba(self, X):
-        """Applies transforms to the data, and the predict_log_proba method of
-        the final estimator. Valid only if the final estimator implements
-        predict_log_proba.
+        """Apply transforms, and predict_log_proba of the final estimator
 
         Parameters
         ----------
         X : iterable
             Data to predict on. Must fulfill input requirements of first step
             of the pipeline.
+
+        Returns
+        -------
+        y_score : array-like, shape = [n_samples, n_classes]
         """
         Xt = X
         for name, transform in self.steps[:-1]:
-            Xt = transform.transform(Xt)
+            if transform is not None:
+                Xt = transform.transform(Xt)
         return self.steps[-1][-1].predict_log_proba(Xt)
 
-    @if_delegate_has_method(delegate='_final_estimator')
-    def transform(self, X):
-        """Applies transforms to the data, and the transform method of the
-        final estimator. Valid only if the final estimator implements
-        transform.
+    @property
+    def transform(self):
+        """Apply transforms, and transform with the final estimator
+
+        This also works where final estimator is ``None``: all prior
+        transformations are applied.
 
         Parameters
         ----------
         X : iterable
-            Data to predict on. Must fulfill input requirements of first step
+            Data to transform. Must fulfill input requirements of first step
             of the pipeline.
+
+        Returns
+        -------
+        Xt : array-like, shape = [n_samples, n_transformed_features]
         """
+        # _final_estimator is None or has transform, otherwise attribute error
+        if self._final_estimator is not None:
+            self._final_estimator.transform
+        return self._transform
+
+    def _transform(self, X):
         Xt = X
         for name, transform in self.steps:
-            Xt = transform.transform(Xt)
+            if transform is not None:
+                Xt = transform.transform(Xt)
         return Xt
 
-    @if_delegate_has_method(delegate='_final_estimator')
-    def inverse_transform(self, X):
-        """Applies inverse transform to the data.
-        Starts with the last step of the pipeline and applies
-        ``inverse_transform`` in inverse order of the pipeline steps.
-        Valid only if all steps of the pipeline implement inverse_transform.
+    @property
+    def inverse_transform(self):
+        """Apply inverse transformations in reverse order
+
+        All estimators in the pipeline must support ``inverse_transform``.
 
         Parameters
         ----------
-        X : iterable
-            Data to inverse transform. Must fulfill output requirements of the
-            last step of the pipeline.
+        Xt : array-like, shape = [n_samples, n_transformed_features]
+            Data samples, where ``n_samples`` is the number of samples and
+            ``n_features`` is the number of features. Must fulfill
+            input requirements of last step of pipeline's
+            ``inverse_transform`` method.
+
+        Returns
+        -------
+        Xt : array-like, shape = [n_samples, n_features]
         """
-        if X.ndim == 1:
+        # raise AttributeError if necessary for hasattr behaviour
+        for name, transform in self.steps:
+            if transform is not None:
+                transform.inverse_transform
+        return self._inverse_transform
+
+    def _inverse_transform(self, X):
+        if hasattr(X, 'ndim') and X.ndim == 1:
             warn("From version 0.19, a 1d X will not be reshaped in"
                  " pipeline.inverse_transform any more.", FutureWarning)
             X = X[None, :]
         Xt = X
-        for name, step in self.steps[::-1]:
-            Xt = step.inverse_transform(Xt)
+        for name, transform in self.steps[::-1]:
+            if transform is not None:
+                Xt = transform.inverse_transform(Xt)
         return Xt
 
     @if_delegate_has_method(delegate='_final_estimator')
     def score(self, X, y=None):
-        """Applies transforms to the data, and the score method of the
-        final estimator. Valid only if the final estimator implements
-        score.
+        """Apply transforms, and score with the final estimator
 
         Parameters
         ----------
         X : iterable
-            Data to score. Must fulfill input requirements of first step of the
-            pipeline.
+            Data to predict on. Must fulfill input requirements of first step
+            of the pipeline.
 
         y : iterable, default=None
             Targets used for scoring. Must fulfill label requirements for all
             steps of the pipeline.
+
+        Returns
+        -------
+        score : float
         """
         Xt = X
         for name, transform in self.steps[:-1]:
-            Xt = transform.transform(Xt)
+            if transform is not None:
+                Xt = transform.transform(Xt)
         return self.steps[-1][-1].score(Xt, y)
 
     @property
@@ -377,7 +549,8 @@ def make_pipeline(*steps):
     --------
     >>> from sklearn.naive_bayes import GaussianNB
     >>> from sklearn.preprocessing import StandardScaler
-    >>> make_pipeline(StandardScaler(), GaussianNB(priors=None))    # doctest: +NORMALIZE_WHITESPACE
+    >>> make_pipeline(StandardScaler(), GaussianNB(priors=None))
+    ...     # doctest: +NORMALIZE_WHITESPACE
     Pipeline(steps=[('standardscaler',
                      StandardScaler(copy=True, with_mean=True, with_std=True)),
                     ('gaussiannb', GaussianNB(priors=None))])
@@ -393,38 +566,38 @@ def _fit_one_transformer(transformer, X, y):
     return transformer.fit(X, y)
 
 
-def _transform_one(transformer, name, X, transformer_weights):
-    if transformer_weights is not None and name in transformer_weights:
-        # if we have a weight for this transformer, multiply output
-        return transformer.transform(X) * transformer_weights[name]
-    return transformer.transform(X)
+def _transform_one(transformer, name, weight, X):
+    res = transformer.transform(X)
+    # if we have a weight for this transformer, multiply output
+    if weight is None:
+        return res
+    return res * weight
 
 
-def _fit_transform_one(transformer, name, X, y, transformer_weights,
+def _fit_transform_one(transformer, name, weight, X, y,
                        **fit_params):
-    if transformer_weights is not None and name in transformer_weights:
-        # if we have a weight for this transformer, multiply output
-        if hasattr(transformer, 'fit_transform'):
-            X_transformed = transformer.fit_transform(X, y, **fit_params)
-            return X_transformed * transformer_weights[name], transformer
-        else:
-            X_transformed = transformer.fit(X, y, **fit_params).transform(X)
-            return X_transformed * transformer_weights[name], transformer
     if hasattr(transformer, 'fit_transform'):
-        X_transformed = transformer.fit_transform(X, y, **fit_params)
-        return X_transformed, transformer
+        res = transformer.fit_transform(X, y, **fit_params)
     else:
-        X_transformed = transformer.fit(X, y, **fit_params).transform(X)
-        return X_transformed, transformer
+        res = transformer.fit(X, y, **fit_params).transform(X)
+    # if we have a weight for this transformer, multiply output
+    if weight is None:
+        return res, transformer
+    return res * weight, transformer
 
 
-class FeatureUnion(BaseEstimator, TransformerMixin):
+class FeatureUnion(_BasePipeline, TransformerMixin):
     """Concatenates results of multiple transformer objects.
 
     This estimator applies a list of transformer objects in parallel to the
     input data, then concatenates the results. This is useful to combine
     several feature extraction mechanisms into a single transformer.
 
+    Parameters of the transformers may be set using its name and the parameter
+    name separated by a '__'. A transformer may be replaced entirely by
+    setting the parameter with its name to another transformer,
+    or removed by setting to ``None``.
+
     Read more in the :ref:`User Guide <feature_union>`.
 
     Parameters
@@ -442,9 +615,62 @@ class FeatureUnion(BaseEstimator, TransformerMixin):
 
     """
     def __init__(self, transformer_list, n_jobs=1, transformer_weights=None):
-        self.transformer_list = transformer_list
+        self.transformer_list = tosequence(transformer_list)
         self.n_jobs = n_jobs
         self.transformer_weights = transformer_weights
+        self._validate_transformers()
+
+    def get_params(self, deep=True):
+        """Get parameters for this estimator.
+
+        Parameters
+        ----------
+        deep: boolean, optional
+            If True, will return the parameters for this estimator and
+            contained subobjects that are estimators.
+
+        Returns
+        -------
+        params : mapping of string to any
+            Parameter names mapped to their values.
+        """
+        return self._get_params('transformer_list', deep=deep)
+
+    def set_params(self, **kwargs):
+        """Set the parameters of this estimator.
+
+        Valid parameter keys can be listed with ``get_params()``.
+
+        Returns
+        -------
+        self
+        """
+        self._set_params('transformer_list', **kwargs)
+        return self
+
+    def _validate_transformers(self):
+        names, transformers = zip(*self.transformer_list)
+
+        # validate names
+        self._validate_names(names)
+
+        # validate estimators
+        for t in transformers:
+            if t is None:
+                continue
+            if (not (hasattr(t, "fit") or hasattr(t, "fit_transform")) or not
+                    hasattr(t, "transform")):
+                raise TypeError("All estimators should implement fit and "
+                                "transform. '%s' (type %s) doesn't" %
+                                (t, type(t)))
+
+    def _iter(self):
+        """Generate (name, est, weight) tuples excluding None transformers
+        """
+        get_weight = (self.transformer_weights or {}).get
+        return ((name, trans, get_weight(name))
+                for name, trans in self.transformer_list
+                if trans is not None)
 
     def get_feature_names(self):
         """Get feature names from all transformers.
@@ -455,10 +681,11 @@ def get_feature_names(self):
             Names of the features produced by transform.
         """
         feature_names = []
-        for name, trans in self.transformer_list:
+        for name, trans, weight in self._iter():
             if not hasattr(trans, 'get_feature_names'):
-                raise AttributeError("Transformer %s does not provide"
-                                     " get_feature_names." % str(name))
+                raise AttributeError("Transformer %s (type %s) does not "
+                                     "provide get_feature_names."
+                                     % (str(name), type(trans).__name__))
             feature_names.extend([name + "__" + f for f in
                                   trans.get_feature_names()])
         return feature_names
@@ -468,35 +695,50 @@ def fit(self, X, y=None):
 
         Parameters
         ----------
-        X : array-like or sparse matrix, shape (n_samples, n_features)
+        X : iterable or array-like, depending on transformers
             Input data, used to fit transformers.
+
+        y : array-like, shape (n_samples, ...), optional
+            Targets for supervised learning.
+
+        Returns
+        -------
+        self : FeatureUnion
+            This estimator
         """
+        self._validate_transformers()
         transformers = Parallel(n_jobs=self.n_jobs)(
             delayed(_fit_one_transformer)(trans, X, y)
-            for name, trans in self.transformer_list)
+            for _, trans, _ in self._iter())
         self._update_transformer_list(transformers)
         return self
 
     def fit_transform(self, X, y=None, **fit_params):
-        """Fit all transformers using X, transform the data and concatenate
-        results.
+        """Fit all transformers, transform the data and concatenate results.
 
         Parameters
         ----------
-        X : array-like or sparse matrix, shape (n_samples, n_features)
+        X : iterable or array-like, depending on transformers
             Input data to be transformed.
 
+        y : array-like, shape (n_samples, ...), optional
+            Targets for supervised learning.
+
         Returns
         -------
         X_t : array-like or sparse matrix, shape (n_samples, sum_n_components)
             hstack of results of transformers. sum_n_components is the
             sum of n_components (output dimension) over transformers.
         """
+        self._validate_transformers()
         result = Parallel(n_jobs=self.n_jobs)(
-            delayed(_fit_transform_one)(trans, name, X, y,
-                                        self.transformer_weights, **fit_params)
-            for name, trans in self.transformer_list)
+            delayed(_fit_transform_one)(trans, name, weight, X, y,
+                                        **fit_params)
+            for name, trans, weight in self._iter())
 
+        if not result:
+            # All transformers are None
+            return np.zeros((X.shape[0], 0))
         Xs, transformers = zip(*result)
         self._update_transformer_list(transformers)
         if any(sparse.issparse(f) for f in Xs):
@@ -510,7 +752,7 @@ def transform(self, X):
 
         Parameters
         ----------
-        X : array-like or sparse matrix, shape (n_samples, n_features)
+        X : iterable or array-like, depending on transformers
             Input data to be transformed.
 
         Returns
@@ -520,29 +762,22 @@ def transform(self, X):
             sum of n_components (output dimension) over transformers.
         """
         Xs = Parallel(n_jobs=self.n_jobs)(
-            delayed(_transform_one)(trans, name, X, self.transformer_weights)
-            for name, trans in self.transformer_list)
+            delayed(_transform_one)(trans, name, weight, X)
+            for name, trans, weight in self._iter())
+        if not Xs:
+            # All transformers are None
+            return np.zeros((X.shape[0], 0))
         if any(sparse.issparse(f) for f in Xs):
             Xs = sparse.hstack(Xs).tocsr()
         else:
             Xs = np.hstack(Xs)
         return Xs
 
-    def get_params(self, deep=True):
-        if not deep:
-            return super(FeatureUnion, self).get_params(deep=False)
-        else:
-            out = dict(self.transformer_list)
-            for name, trans in self.transformer_list:
-                for key, value in iteritems(trans.get_params(deep=True)):
-                    out['%s__%s' % (name, key)] = value
-            out.update(super(FeatureUnion, self).get_params(deep=False))
-            return out
-
     def _update_transformer_list(self, transformers):
+        transformers = iter(transformers)
         self.transformer_list[:] = [
-            (name, new)
-            for ((name, old), new) in zip(self.transformer_list, transformers)
+            (name, None if old is None else next(transformers))
+            for name, old in self.transformer_list
         ]
 
 
diff --git a/sklearn/preprocessing/data.py b/sklearn/preprocessing/data.py
index 4b2592067c55..1c3d8db58027 100644
--- a/sklearn/preprocessing/data.py
+++ b/sklearn/preprocessing/data.py
@@ -123,7 +123,7 @@ def scale(X, axis=0, with_mean=True, with_std=True, copy=True):
     --------
     StandardScaler: Performs scaling to unit variance using the``Transformer`` API
         (e.g. as part of a preprocessing :class:`sklearn.pipeline.Pipeline`).
-    """
+    """  # noqa
     X = check_array(X, accept_sparse='csc', copy=copy, ensure_2d=False,
                     warn_on_dtype=True, estimator='the scale function',
                     dtype=FLOAT_DTYPES)
@@ -398,7 +398,8 @@ def minmax_scale(X, feature_range=(0, 1), axis=0, copy=True):
     Read more in the :ref:`User Guide <preprocessing_scaler>`.
 
     .. versionadded:: 0.17
-       *minmax_scale* function interface to :class:`sklearn.preprocessing.MinMaxScaler`.
+       *minmax_scale* function interface
+       to :class:`sklearn.preprocessing.MinMaxScaler`.
 
     Parameters
     ----------
@@ -417,7 +418,7 @@ def minmax_scale(X, feature_range=(0, 1), axis=0, copy=True):
     --------
     MinMaxScaler: Performs scaling to a given range using the``Transformer`` API
         (e.g. as part of a preprocessing :class:`sklearn.pipeline.Pipeline`).
-    """
+    """  # noqa
     # To allow retro-compatibility, we handle here the case of 1D-input
     # From 0.17, 1D-input are deprecated in scaler objects
     # Although, we want to allow the users to keep calling this function
@@ -515,7 +516,7 @@ class StandardScaler(BaseEstimator, TransformerMixin):
 
     :class:`sklearn.decomposition.PCA`
         Further removes the linear correlation across features with 'whiten=True'.
-    """
+    """  # noqa
 
     def __init__(self, copy=True, with_mean=True, with_std=True):
         self.with_mean = with_mean
@@ -523,7 +524,8 @@ def __init__(self, copy=True, with_mean=True, with_std=True):
         self.copy = copy
 
     @property
-    @deprecated("Attribute ``std_`` will be removed in 0.19. Use ``scale_`` instead")
+    @deprecated("Attribute ``std_`` will be removed in 0.19. "
+                "Use ``scale_`` instead")
     def std_(self):
         return self.scale_
 
@@ -865,7 +867,7 @@ def maxabs_scale(X, axis=0, copy=True):
     --------
     MaxAbsScaler: Performs scaling to the [-1, 1] range using the``Transformer`` API
         (e.g. as part of a preprocessing :class:`sklearn.pipeline.Pipeline`).
-    """
+    """  # noqa
     # To allow retro-compatibility, we handle here the case of 1D-input
     # From 0.17, 1D-input are deprecated in scaler objects
     # Although, we want to allow the users to keep calling this function
@@ -897,8 +899,9 @@ class RobustScaler(BaseEstimator, TransformerMixin):
     """Scale features using statistics that are robust to outliers.
 
     This Scaler removes the median and scales the data according to
-    the Interquartile Range (IQR). The IQR is the range between the 1st
-    quartile (25th quantile) and the 3rd quartile (75th quantile).
+    the quantile range (defaults to IQR: Interquartile Range).
+    The IQR is the range between the 1st quartile (25th quantile)
+    and the 3rd quartile (75th quantile).
 
     Centering and scaling happen independently on each feature (or each
     sample, depending on the `axis` argument) by computing the relevant
@@ -928,6 +931,12 @@ class RobustScaler(BaseEstimator, TransformerMixin):
     with_scaling : boolean, True by default
         If True, scale the data to interquartile range.
 
+    quantile_range : tuple (q_min, q_max), 0.0 < q_min < q_max < 100.0
+        Default: (25.0, 75.0) = (1st quantile, 3rd quantile) = IQR
+        Quantile range used to calculate scale_
+
+        .. versionadded:: 0.18
+
     copy : boolean, optional, default is True
         If False, try to avoid a copy and do inplace scaling instead.
         This is not guaranteed to always work inplace; e.g. if the data is
@@ -950,7 +959,8 @@ class RobustScaler(BaseEstimator, TransformerMixin):
     robust_scale: Equivalent function without the object oriented API.
 
     :class:`sklearn.decomposition.PCA`
-        Further removes the linear correlation across features with 'whiten=True'.
+        Further removes the linear correlation across features with
+        'whiten=True'.
 
     Notes
     -----
@@ -960,9 +970,11 @@ class RobustScaler(BaseEstimator, TransformerMixin):
     https://en.wikipedia.org/wiki/Interquartile_range
     """
 
-    def __init__(self, with_centering=True, with_scaling=True, copy=True):
+    def __init__(self, with_centering=True, with_scaling=True,
+                 quantile_range=(25.0, 75.0), copy=True):
         self.with_centering = with_centering
         self.with_scaling = with_scaling
+        self.quantile_range = quantile_range
         self.copy = copy
 
     def _check_array(self, X, copy):
@@ -998,7 +1010,12 @@ def fit(self, X, y=None):
             self.center_ = np.median(X, axis=0)
 
         if self.with_scaling:
-            q = np.percentile(X, (25, 75), axis=0)
+            q_min, q_max = self.quantile_range
+            if not 0 <= q_min <= q_max <= 100:
+                raise ValueError("Invalid quantile range: %s" %
+                                 str(self.quantile_range))
+
+            q = np.percentile(X, self.quantile_range, axis=0)
             self.scale_ = (q[1] - q[0])
             self.scale_ = _handle_zeros_in_scale(self.scale_, copy=False)
         return self
@@ -1056,7 +1073,8 @@ def inverse_transform(self, X):
         return X
 
 
-def robust_scale(X, axis=0, with_centering=True, with_scaling=True, copy=True):
+def robust_scale(X, axis=0, with_centering=True, with_scaling=True,
+                 quantile_range=(25.0, 75.0), copy=True):
     """Standardize a dataset along any axis
 
     Center to the median and component wise scale
@@ -1081,6 +1099,12 @@ def robust_scale(X, axis=0, with_centering=True, with_scaling=True, copy=True):
         If True, scale the data to unit variance (or equivalently,
         unit standard deviation).
 
+    quantile_range : tuple (q_min, q_max), 0.0 < q_min < q_max < 100.0
+        Default: (25.0, 75.0) = (1st quantile, 3rd quantile) = IQR
+        Quantile range used to calculate scale_
+
+        .. versionadded:: 0.18
+
     copy : boolean, optional, default is True
         set to False to perform inplace row normalization and avoid a
         copy (if the input is already a numpy array or a scipy.sparse
@@ -1105,7 +1129,7 @@ def robust_scale(X, axis=0, with_centering=True, with_scaling=True, copy=True):
         (e.g. as part of a preprocessing :class:`sklearn.pipeline.Pipeline`).
     """
     s = RobustScaler(with_centering=with_centering, with_scaling=with_scaling,
-                     copy=copy)
+                     quantile_range=quantile_range, copy=copy)
     if axis == 0:
         return s.fit_transform(X)
     else:
@@ -1670,11 +1694,11 @@ def _transform_selected(X, transform, selected="all", copy=True):
     -------
     X : array or sparse matrix, shape=(n_samples, n_features_new)
     """
+    X = check_array(X, accept_sparse='csc', copy=copy, dtype=FLOAT_DTYPES)
+
     if isinstance(selected, six.string_types) and selected == "all":
         return transform(X)
 
-    X = check_array(X, accept_sparse='csc', copy=copy, dtype=FLOAT_DTYPES)
-
     if len(selected) == 0:
         return X
 
@@ -1724,7 +1748,8 @@ class OneHotEncoder(BaseEstimator, TransformerMixin):
         - int : number of categorical values per feature.
                 Each feature value should be in ``range(n_values)``
         - array : ``n_values[i]`` is the number of categorical values in
-                  ``X[:, i]``. Each feature value should be in ``range(n_values[i])``
+                  ``X[:, i]``. Each feature value should be
+                  in ``range(n_values[i])``
 
     categorical_features: "all" or array of indices or mask
         Specify what features are treated as categorical.
@@ -1816,7 +1841,7 @@ def _fit_transform(self, X):
             raise ValueError("X needs to contain only non-negative integers.")
         n_samples, n_features = X.shape
         if (isinstance(self.n_values, six.string_types) and
-            self.n_values == 'auto'):
+                self.n_values == 'auto'):
             n_values = np.max(X, axis=0) + 1
         elif isinstance(self.n_values, numbers.Integral):
             if (np.max(X, axis=0) >= self.n_values).any():
@@ -1849,7 +1874,7 @@ def _fit_transform(self, X):
                                 dtype=self.dtype).tocsr()
 
         if (isinstance(self.n_values, six.string_types) and
-            self.n_values == 'auto'):
+                self.n_values == 'auto'):
             mask = np.array(out.sum(axis=0)).ravel() != 0
             active_features = np.where(mask)[0]
             out = out[:, active_features]
@@ -1901,7 +1926,7 @@ def _transform(self, X):
                                 shape=(n_samples, indices[-1]),
                                 dtype=self.dtype).tocsr()
         if (isinstance(self.n_values, six.string_types) and
-            self.n_values == 'auto'):
+                self.n_values == 'auto'):
             out = out[:, self.active_features_]
 
         return out if self.sparse else out.toarray()
diff --git a/sklearn/preprocessing/imputation.py b/sklearn/preprocessing/imputation.py
index db8913b97f0f..ec401ee1675e 100644
--- a/sklearn/preprocessing/imputation.py
+++ b/sklearn/preprocessing/imputation.py
@@ -10,7 +10,6 @@
 
 from ..base import BaseEstimator, TransformerMixin
 from ..utils import check_array
-from ..utils import safe_mask
 from ..utils.fixes import astype
 from ..utils.sparsefuncs import _get_median
 from ..utils.validation import check_is_fitted
@@ -103,21 +102,11 @@ class Imputer(BaseEstimator, TransformerMixin):
         - If `axis=0` and X is encoded as a CSR matrix;
         - If `axis=1` and X is encoded as a CSC matrix.
 
-    add_indicator_features : boolean, optional (default=False)
-        If True, the transformed ``X`` will have binary indicator features
-        appended. These correspond to input features with at least one
-        missing value marking which elements have been imputed.
-
     Attributes
     ----------
     statistics_ : array of shape (n_features,)
         The imputation fill value for each feature if axis == 0.
 
-    imputed_features_ : array of shape (n_features_with_missing, )
-        The input features which have been imputed during transform.
-        The size of this attribute will be the number of features with
-        at least one missing value (and fewer than all in the axis=0 case).
-
     Notes
     -----
     - When ``axis=0``, columns which only contained missing values at `fit`
@@ -127,13 +116,12 @@ class Imputer(BaseEstimator, TransformerMixin):
       contain missing values).
     """
     def __init__(self, missing_values="NaN", strategy="mean",
-                 axis=0, verbose=0, copy=True, add_indicator_features=False):
+                 axis=0, verbose=0, copy=True):
         self.missing_values = missing_values
         self.strategy = strategy
         self.axis = axis
         self.verbose = verbose
         self.copy = copy
-        self.add_indicator_features = add_indicator_features
 
     def fit(self, X, y=None):
         """Fit the imputer on X.
@@ -311,74 +299,13 @@ def _dense_fit(self, X, strategy, missing_values, axis):
 
             return most_frequent
 
-    def _sparse_transform(self, X, valid_stats, valid_idx):
-        """transformer on sparse data."""
-        mask = _get_mask(X.data, self.missing_values)
-        indexes = np.repeat(np.arange(len(X.indptr) - 1, dtype=np.int),
-                            np.diff(X.indptr))[mask]
-
-        X.data[mask] = astype(valid_stats[indexes], X.dtype,
-                              copy=False)
-
-        mask_matrix = X.__class__((mask, X.indices.copy(),
-                                  X.indptr.copy()), shape=X.shape,
-                                  dtype=X.dtype)
-        mask_matrix.eliminate_zeros()  # removes explicit False entries
-        features_with_missing_values = mask_matrix.sum(axis=0).A.nonzero()[1]
-        features_mask = safe_mask(mask_matrix, features_with_missing_values)
-        imputed_mask = mask_matrix[:, features_mask]
-        if self.axis == 0:
-            self.imputed_features_ = valid_idx[features_with_missing_values]
-        else:
-            self.imputed_features_ = features_with_missing_values
-
-        if self.add_indicator_features:
-            X = sparse.hstack((X, imputed_mask))
-
-        return X
-
-    def _dense_transform(self, X, valid_stats, valid_idx):
-        """transformer on dense data."""
-        mask = _get_mask(X, self.missing_values)
-        n_missing = np.sum(mask, axis=self.axis)
-        values = np.repeat(valid_stats, n_missing)
-
-        if self.axis == 0:
-            coordinates = np.where(mask.transpose())[::-1]
-        else:
-            coordinates = mask
-
-        X[coordinates] = values
-
-        features_with_missing_values = np.where(np.any
-                                                (mask, axis=0))[0]
-        imputed_mask = mask[:, features_with_missing_values]
-        if self.axis == 0:
-            self.imputed_features_ = valid_idx[features_with_missing_values]
-        else:
-            self.imputed_features_ = features_with_missing_values
-
-        if self.add_indicator_features:
-            X = np.hstack((X, imputed_mask))
-
-        return X
-
     def transform(self, X):
         """Impute all missing values in X.
 
         Parameters
         ----------
-        X : {array-like, sparse matrix}, shape = (n_samples, n_features)
+        X : {array-like, sparse matrix}, shape = [n_samples, n_features]
             The input data to complete.
-
-        Return
-        ------
-        X_new : {array-like, sparse matrix},
-                Transformed array.
-                shape (n_samples, n_features_new) when
-                ``add_indicator_features`` is False,
-                shape (n_samples, n_features_new + len(imputed_features_)
-                when ``add_indicator_features`` is True.
         """
         if self.axis == 0:
             check_is_fitted(self, 'statistics_')
@@ -410,27 +337,39 @@ def transform(self, X):
         invalid_mask = np.isnan(statistics)
         valid_mask = np.logical_not(invalid_mask)
         valid_statistics = statistics[valid_mask]
-        valid_idx = np.where(valid_mask)[0]
+        valid_statistics_indexes = np.where(valid_mask)[0]
         missing = np.arange(X.shape[not self.axis])[invalid_mask]
 
         if self.axis == 0 and invalid_mask.any():
             if self.verbose:
                 warnings.warn("Deleting features without "
                               "observed values: %s" % missing)
-            X = X[:, valid_idx]
+            X = X[:, valid_statistics_indexes]
         elif self.axis == 1 and invalid_mask.any():
             raise ValueError("Some rows only contain "
                              "missing values: %s" % missing)
 
         # Do actual imputation
         if sparse.issparse(X) and self.missing_values != 0:
-            # sparse matrix and missing values is not zero
-            X = self._sparse_transform(X, valid_statistics, valid_idx)
+            mask = _get_mask(X.data, self.missing_values)
+            indexes = np.repeat(np.arange(len(X.indptr) - 1, dtype=np.int),
+                                np.diff(X.indptr))[mask]
+
+            X.data[mask] = astype(valid_statistics[indexes], X.dtype,
+                                  copy=False)
         else:
-            # sparse with zero as missing value and dense matrix
             if sparse.issparse(X):
                 X = X.toarray()
 
-            X = self._dense_transform(X, valid_statistics, valid_idx)
+            mask = _get_mask(X, self.missing_values)
+            n_missing = np.sum(mask, axis=self.axis)
+            values = np.repeat(valid_statistics, n_missing)
+
+            if self.axis == 0:
+                coordinates = np.where(mask.transpose())[::-1]
+            else:
+                coordinates = mask
+
+            X[coordinates] = values
 
         return X
diff --git a/sklearn/preprocessing/tests/test_data.py b/sklearn/preprocessing/tests/test_data.py
index 5d81a24358d0..39360ebdd779 100644
--- a/sklearn/preprocessing/tests/test_data.py
+++ b/sklearn/preprocessing/tests/test_data.py
@@ -202,7 +202,7 @@ def test_scale_1d():
 
 @skip_if_32bit
 def test_standard_scaler_numerical_stability():
-    """Test numerical stability of scaling"""
+    # Test numerical stability of scaling
     # np.log(1e-5) is taken because of its floating point representation
     # was empirically found to cause numerical problems with np.mean & np.std.
 
@@ -802,7 +802,7 @@ def test_scale_input_finiteness_validation():
 
 
 def test_robust_scaler_2d_arrays():
-    """Test robust scaling of 2d array along first axis"""
+    # Test robust scaling of 2d array along first axis
     rng = np.random.RandomState(0)
     X = rng.randn(4, 5)
     X[:, 0] = 0.0  # first feature is always of zero
@@ -840,6 +840,32 @@ def test_robust_scaler_iris():
     assert_array_almost_equal(iqr, 1)
 
 
+def test_robust_scaler_iris_quantiles():
+    X = iris.data
+    scaler = RobustScaler(quantile_range=(10, 90))
+    X_trans = scaler.fit_transform(X)
+    assert_array_almost_equal(np.median(X_trans, axis=0), 0)
+    X_trans_inv = scaler.inverse_transform(X_trans)
+    assert_array_almost_equal(X, X_trans_inv)
+    q = np.percentile(X_trans, q=(10, 90), axis=0)
+    q_range = q[1] - q[0]
+    assert_array_almost_equal(q_range, 1)
+
+
+def test_robust_scaler_invalid_range():
+    for range_ in [
+        (-1, 90),
+        (-2, -3),
+        (10, 101),
+        (100.5, 101),
+        (90, 50),
+    ]:
+        scaler = RobustScaler(quantile_range=range_)
+
+        assert_raises_regex(ValueError, 'Invalid quantile range: \(',
+                            scaler.fit, iris.data)
+
+
 def test_scale_function_without_centering():
     rng = np.random.RandomState(42)
     X = rng.randn(4, 5)
@@ -884,7 +910,7 @@ def test_robust_scale_axis1():
 
 
 def test_robust_scaler_zero_variance_features():
-    """Check RobustScaler on toy data with zero variance features"""
+    # Check RobustScaler on toy data with zero variance features
     X = [[0., 1., +0.5],
          [0., 1., -0.1],
          [0., 1., +1.1]]
@@ -917,7 +943,7 @@ def test_robust_scaler_zero_variance_features():
 
 
 def test_maxabs_scaler_zero_variance_features():
-    """Check MaxAbsScaler on toy data with zero variance features"""
+    # Check MaxAbsScaler on toy data with zero variance features
     X = [[0., 1., +0.5],
          [0., 1., -0.3],
          [0., 1., +1.5],
@@ -1374,9 +1400,9 @@ def test_center_kernel():
 
 
 def test_cv_pipeline_precomputed():
-    """Cross-validate a regression on four coplanar points with the same
-    value. Use precomputed kernel to ensure Pipeline with KernelCenterer
-    is treated as a _pairwise operation."""
+    # Cross-validate a regression on four coplanar points with the same
+    # value. Use precomputed kernel to ensure Pipeline with KernelCenterer
+    # is treated as a _pairwise operation.
     X = np.array([[3, 0, 0], [0, 3, 0], [0, 0, 3], [1, 1, 1]])
     y_true = np.ones((4,))
     K = X.dot(X.T)
@@ -1535,6 +1561,23 @@ def test_transform_selected():
     _check_transform_selected(X, X, [False, False, False])
 
 
+def test_transform_selected_copy_arg():
+    # transformer that alters X
+    def _mutating_transformer(X):
+        X[0, 0] = X[0, 0] + 1
+        return X
+
+    original_X = np.asarray([[1, 2], [3, 4]])
+    expected_Xtr = [[2, 2], [3, 4]]
+
+    X = original_X.copy()
+    Xtr = _transform_selected(X, _mutating_transformer, copy=True,
+                              selected='all')
+
+    assert_array_equal(toarray(X), toarray(original_X))
+    assert_array_equal(toarray(Xtr), expected_Xtr)
+
+
 def _run_one_hot(X, X2, cat):
     enc = OneHotEncoder(categorical_features=cat)
     Xtr = enc.fit_transform(X)
diff --git a/sklearn/preprocessing/tests/test_imputation.py b/sklearn/preprocessing/tests/test_imputation.py
index d7c6bc24aca7..028f5603d1bb 100644
--- a/sklearn/preprocessing/tests/test_imputation.py
+++ b/sklearn/preprocessing/tests/test_imputation.py
@@ -2,7 +2,6 @@
 import numpy as np
 from scipy import sparse
 
-from sklearn.base import clone
 from sklearn.utils.testing import assert_equal
 from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_raises
@@ -359,50 +358,3 @@ def test_imputation_copy():
 
     # Note: If X is sparse and if missing_values=0, then a (dense) copy of X is
     # made, even if copy=False.
-
-
-def check_indicator(X, expected_imputed_features, axis):
-    n_samples, n_features = X.shape
-    imputer = Imputer(missing_values=-1, strategy='mean', axis=axis)
-    imputer_with_in = clone(imputer).set_params(add_indicator_features=True)
-    Xt = imputer.fit_transform(X)
-    Xt_with_in = imputer_with_in.fit_transform(X)
-    imputed_features_mask = X[:, expected_imputed_features] == -1
-    n_features_new = Xt.shape[1]
-    n_imputed_features = len(imputer_with_in.imputed_features_)
-    assert_array_equal(imputer.imputed_features_, expected_imputed_features)
-    assert_array_equal(imputer_with_in.imputed_features_,
-                       expected_imputed_features)
-    assert_equal(Xt_with_in.shape,
-                 (n_samples, n_features_new + n_imputed_features))
-    assert_array_equal(Xt_with_in, np.hstack((Xt, imputed_features_mask)))
-    imputer_with_in = clone(imputer).set_params(add_indicator_features=True)
-    assert_array_equal(Xt_with_in,
-                       imputer_with_in.fit_transform(sparse.csc_matrix(X)).A)
-    assert_array_equal(Xt_with_in,
-                       imputer_with_in.fit_transform(sparse.csr_matrix(X)).A)
-
-
-def test_indicator_features():
-    # one feature with all missng values
-    X = np.array([
-       [-1,  -1,   2,   3],
-       [4,  -1,   6,  -1],
-       [8,  -1,  10,  11],
-       [12,  -1,  -1,  15],
-       [16,  -1,  18,  19]
-    ])
-    check_indicator(X, np.array([0, 2, 3]), axis=0)
-    check_indicator(X, np.array([0, 1, 2, 3]), axis=1)
-
-    # one feature with all missing values and one with no missing value
-    # when axis=0 the feature gets discarded
-    X = np.array([
-       [-1,  -1,   1,   3],
-       [4,  -1,   0,  -1],
-       [8,  -1,   1,  0],
-       [0,  -1,   0,  15],
-       [16,  -1,   1,  19]
-    ])
-    check_indicator(X, np.array([0, 3]), axis=0)
-    check_indicator(X, np.array([0, 1, 3]), axis=1)
diff --git a/sklearn/src/cblas/ATL_dsrefdot.c b/sklearn/src/cblas/ATL_dsrefdot.c
new file mode 100644
index 000000000000..442e51a08e20
--- /dev/null
+++ b/sklearn/src/cblas/ATL_dsrefdot.c
@@ -0,0 +1,141 @@
+/* ---------------------------------------------------------------------
+ *
+ * -- Automatically Tuned Linear Algebra Software (ATLAS)
+ *    (C) Copyright 2000 All Rights Reserved
+ *
+ * -- ATLAS routine -- Version 3.2 -- December 25, 2000
+ *
+ * Author         : Antoine P. Petitet
+ * Originally developed at the University of Tennessee,
+ * Innovative Computing Laboratory, Knoxville TN, 37996-1301, USA.
+ *
+ * ---------------------------------------------------------------------
+ *
+ * -- Copyright notice and Licensing terms:
+ *
+ *  Redistribution  and  use in  source and binary forms, with or without
+ *  modification, are  permitted provided  that the following  conditions
+ *  are met:
+ *
+ * 1. Redistributions  of  source  code  must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce  the above copyright
+ *    notice,  this list of conditions, and the  following disclaimer in
+ *    the documentation and/or other materials provided with the distri-
+ *    bution.
+ * 3. The name of the University,  the ATLAS group,  or the names of its
+ *    contributors  may not be used to endorse or promote products deri-
+ *    ved from this software without specific written permission.
+ *
+ * -- Disclaimer:
+ *
+ * THIS  SOFTWARE  IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES,  INCLUDING,  BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE UNIVERSITY
+ * OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,  INDIRECT, INCIDENTAL, SPE-
+ * CIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+ * TO,  PROCUREMENT  OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA,
+ * OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEO-
+ * RY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT  (IN-
+ * CLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * ---------------------------------------------------------------------
+ */
+/*
+ * Include files
+ */
+#include "atlas_refmisc.h"
+#include "atlas_reflevel1.h"
+
+double ATL_dsrefdot
+(
+   const int                  N,
+   const float                * X,
+   const int                  INCX,
+   const float                * Y,
+   const int                  INCY
+)
+{
+/*
+ * Purpose
+ * =======
+ *
+ * ATL_dsrefdot  returns the dot product x^T * y of two n-vectors x and
+ * y.  The result is internally computed using double precision arithme-
+ * tic.
+ *
+ * Arguments
+ * =========
+ *
+ * N       (input)                       const int
+ *         On entry, N specifies the length of the vector x. N  must  be
+ *         at least zero. Unchanged on exit.
+ *
+ * X       (input)                       const float *
+ *         On entry,  X  points to the  first entry to be accessed of an
+ *         incremented array of size equal to or greater than
+ *            ( 1 + ( n - 1 ) * abs( INCX ) ) * sizeof(   float   ),
+ *         that contains the vector x. Unchanged on exit.
+ *
+ * INCX    (input)                       const int
+ *         On entry, INCX specifies the increment for the elements of X.
+ *         INCX must not be zero. Unchanged on exit.
+ *
+ * Y       (input)                       const float *
+ *         On entry,  Y  points to the  first entry to be accessed of an
+ *         incremented array of size equal to or greater than
+ *            ( 1 + ( n - 1 ) * abs( INCY ) ) * sizeof(   float   ),
+ *         that contains the vector y. Unchanged on exit.
+ *
+ * INCY    (input)                       const int
+ *         On entry, INCY specifies the increment for the elements of Y.
+ *         INCY must not be zero. Unchanged on exit.
+ *
+ * ---------------------------------------------------------------------
+ */
+/*
+ * .. Local Variables ..
+ */
+   register double            dot = ATL_dZERO, x0, x1, x2, x3, y0, y1, y2, y3;
+   float                      * StX;
+   register int               i;
+   int                        nu;
+   const int                  incX2 = 2 * INCX, incY2 = 2 * INCY,
+                              incX3 = 3 * INCX, incY3 = 3 * INCY,
+                              incX4 = 4 * INCX, incY4 = 4 * INCY;
+/* ..
+ * .. Executable Statements ..
+ *
+ */
+   if( N > 0 )
+   {
+      if( ( nu = ( N >> 2 ) << 2 ) != 0 )
+      {
+         StX = (float *)X + nu * INCX;
+
+         do
+         {
+            x0 = (double)(*X);       y0 = (double)(*Y);
+            x1 = (double)(X[INCX ]); y1 = (double)(Y[INCY ]);
+            x2 = (double)(X[incX2]); y2 = (double)(Y[incY2]);
+            x3 = (double)(X[incX3]); y3 = (double)(Y[incY3]);
+            dot += x0 * y0; dot += x1 * y1; dot += x2 * y2; dot += x3 * y3;
+            X  += incX4;
+            Y  += incY4;
+
+         } while( X != StX );
+      }
+
+      for( i = N - nu; i != 0; i-- )
+      {
+         x0 = (double)(*X); y0 = (double)(*Y); dot += x0 * y0;
+         X += INCX; Y += INCY;
+      }
+   }
+   return( dot );
+/*
+ * End of ATL_dsrefdot
+ */
+}
diff --git a/sklearn/src/cblas/ATL_srefasum.c b/sklearn/src/cblas/ATL_srefasum.c
new file mode 100644
index 000000000000..aec26caf011a
--- /dev/null
+++ b/sklearn/src/cblas/ATL_srefasum.c
@@ -0,0 +1,133 @@
+/* ---------------------------------------------------------------------
+ *
+ * -- Automatically Tuned Linear Algebra Software (ATLAS)
+ *    (C) Copyright 2000 All Rights Reserved
+ *
+ * -- ATLAS routine -- Version 3.9.24 -- December 25, 2000
+ *
+ * Author         : Antoine P. Petitet
+ * Originally developed at the University of Tennessee,
+ * Innovative Computing Laboratory, Knoxville TN, 37996-1301, USA.
+ *
+ * ---------------------------------------------------------------------
+ *
+ * -- Copyright notice and Licensing terms:
+ *
+ *  Redistribution  and  use in  source and binary forms, with or without
+ *  modification, are  permitted provided  that the following  conditions
+ *  are met:
+ *
+ * 1. Redistributions  of  source  code  must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce  the above copyright
+ *    notice,  this list of conditions, and the  following disclaimer in
+ *    the documentation and/or other materials provided with the distri-
+ *    bution.
+ * 3. The name of the University,  the ATLAS group,  or the names of its
+ *    contributors  may not be used to endorse or promote products deri-
+ *    ved from this software without specific written permission.
+ *
+ * -- Disclaimer:
+ *
+ * THIS  SOFTWARE  IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES,  INCLUDING,  BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE UNIVERSITY
+ * OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,  INDIRECT, INCIDENTAL, SPE-
+ * CIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+ * TO,  PROCUREMENT  OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA,
+ * OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEO-
+ * RY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT  (IN-
+ * CLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * ---------------------------------------------------------------------
+ */
+/*
+ * Include files
+ */
+#include "atlas_refmisc.h"
+#include "atlas_reflevel1.h"
+
+float ATL_srefasum
+(
+   const int                  N,
+   const float                * X,
+   const int                  INCX
+)
+{
+/*
+ * Purpose
+ * =======
+ *
+ * ATL_srefasum   returns the sum of absolute values of the entries of a
+ * vector x.
+ *
+ * Arguments
+ * =========
+ *
+ * N       (input)                       const int
+ *         On entry, N specifies the length of the vector x. N  must  be
+ *         at least zero. Unchanged on exit.
+ *
+ * X       (input)                       const float *
+ *         On entry,  X  points to the  first entry to be accessed of an
+ *         incremented array of size equal to or greater than
+ *            ( 1 + ( n - 1 ) * abs( INCX ) ) * sizeof(   float   ),
+ *         that contains the vector x. Unchanged on exit.
+ *
+ * INCX    (input)                       const int
+ *         On entry, INCX specifies the increment for the elements of X.
+ *         INCX must not be zero. Unchanged on exit.
+ *
+ * ---------------------------------------------------------------------
+ */
+/*
+ * .. Local Variables ..
+ */
+   register float             sum = ATL_sZERO, x0, x1, x2, x3,
+                              x4, x5, x6, x7;
+   float                      * StX;
+   register int               i;
+   int                        nu;
+   const int                  incX2 = 2 * INCX, incX3 = 3 * INCX,
+                              incX4 = 4 * INCX, incX5 = 5 * INCX,
+                              incX6 = 6 * INCX, incX7 = 7 * INCX,
+                              incX8 = 8 * INCX;
+/* ..
+ * .. Executable Statements ..
+ *
+ */
+   if( ( N > 0 ) && ( INCX >= 1 ) )
+   {
+      if( ( nu = ( N >> 3 ) << 3 ) != 0 )
+      {
+         StX = (float *)X + nu * INCX;
+
+         do
+         {
+            x0 = (*X);     x4 = X[incX4]; x1 = X[INCX ]; x5 = X[incX5];
+            x2 = X[incX2]; x6 = X[incX6]; x3 = X[incX3]; x7 = X[incX7];
+
+            sum += Msabs( x0 ); sum += Msabs( x4 );
+            sum += Msabs( x1 ); sum += Msabs( x3 );
+            sum += Msabs( x2 ); sum += Msabs( x6 );
+            sum += Msabs( x5 ); sum += Msabs( x7 );
+
+            X  += incX8;
+
+         } while( X != StX );
+      }
+
+      for( i = N - nu; i != 0; i-- )
+      {
+         x0   = (*X);
+         sum += Msabs( x0 );
+         X   += INCX;
+      }
+   }
+   return( sum );
+/*
+ * End of ATL_srefasum
+ */
+}
diff --git a/sklearn/src/cblas/cblas_sasum.c b/sklearn/src/cblas/cblas_sasum.c
new file mode 100644
index 000000000000..439707ba021f
--- /dev/null
+++ b/sklearn/src/cblas/cblas_sasum.c
@@ -0,0 +1,44 @@
+/*
+ *             Automatically Tuned Linear Algebra Software v3.10.2
+ *                    (C) Copyright 1999 R. Clint Whaley
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *   1. Redistributions of source code must retain the above copyright
+ *      notice, this list of conditions and the following disclaimer.
+ *   2. Redistributions in binary form must reproduce the above copyright
+ *      notice, this list of conditions, and the following disclaimer in the
+ *      documentation and/or other materials provided with the distribution.
+ *   3. The name of the ATLAS group or the names of its contributers may
+ *      not be used to endorse or promote products derived from this
+ *      software without specific written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
+ * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE ATLAS GROUP OR ITS CONTRIBUTORS
+ * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ *
+ */
+
+#define SREAL
+#include "atlas_misc.h"
+#ifdef ATL_USEPTHREADS
+   #include "atlas_ptalias1.h"
+#endif
+#include "atlas_level1.h"
+#include "cblas.h"
+
+float cblas_sasum(const int N, const float *X, const int incX)
+{
+   if (N > 0 && incX > 0)
+      return(ATL_sasum(N, X, incX));
+   return(0.0f);
+}
diff --git a/sklearn/src/cblas/cblas_saxpy.c b/sklearn/src/cblas/cblas_saxpy.c
new file mode 100644
index 000000000000..19600a53a512
--- /dev/null
+++ b/sklearn/src/cblas/cblas_saxpy.c
@@ -0,0 +1,156 @@
+/* ---------------------------------------------------------------------
+ *
+ * -- Automatically Tuned Linear Algebra Software (ATLAS)
+ *    (C) Copyright 2000 All Rights Reserved
+ *
+ * -- ATLAS routine -- Version 3.9.24 -- December 25, 2000
+ *
+ * Author         : Antoine P. Petitet
+ * Originally developed at the University of Tennessee,
+ * Innovative Computing Laboratory, Knoxville TN, 37996-1301, USA.
+ *
+ * ---------------------------------------------------------------------
+ *
+ * -- Copyright notice and Licensing terms:
+ *
+ *  Redistribution  and  use in  source and binary forms, with or without
+ *  modification, are  permitted provided  that the following  conditions
+ *  are met:
+ *
+ * 1. Redistributions  of  source  code  must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce  the above copyright
+ *    notice,  this list of conditions, and the  following disclaimer in
+ *    the documentation and/or other materials provided with the distri-
+ *    bution.
+ * 3. The name of the University,  the ATLAS group,  or the names of its
+ *    contributors  may not be used to endorse or promote products deri-
+ *    ved from this software without specific written permission.
+ *
+ * -- Disclaimer:
+ *
+ * THIS  SOFTWARE  IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES,  INCLUDING,  BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE UNIVERSITY
+ * OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,  INDIRECT, INCIDENTAL, SPE-
+ * CIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+ * TO,  PROCUREMENT  OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA,
+ * OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEO-
+ * RY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT  (IN-
+ * CLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * ---------------------------------------------------------------------
+ */
+/*
+ * Include files
+ */
+#include "atlas_refmisc.h"
+
+void cblas_saxpy
+(
+   const int                  N,
+   const float                ALPHA,
+   const float                * X,
+   const int                  INCX,
+   float                      * Y,
+   const int                  INCY
+)
+{
+/*
+ * Purpose
+ * =======
+ *
+ * ATL_srefaxpy performs the following operation:
+ *
+ *    y := y + alpha * x,
+ *
+ * where alpha is a scalar and x and y are two n-vectors.
+ *
+ * Arguments
+ * =========
+ *
+ * N       (input)                       const int
+ *         On entry, N specifies the length of the vector x. N  must  be
+ *         at least zero. Unchanged on exit.
+ *
+ * ALPHA   (input)                       const float
+ *         On entry, ALPHA specifies the scalar alpha.   When  ALPHA  is
+ *         supplied as zero, then the entries of the incremented array X
+ *         need not be set on input. Unchanged on exit.
+ *
+ * X       (input)                       const float *
+ *         On entry,  X  points to the  first entry to be accessed of an
+ *         incremented array of size equal to or greater than
+ *            ( 1 + ( n - 1 ) * abs( INCX ) ) * sizeof(   float   ),
+ *         that contains the vector x. Unchanged on exit.
+ *
+ * INCX    (input)                       const int
+ *         On entry, INCX specifies the increment for the elements of X.
+ *         INCX must not be zero. Unchanged on exit.
+ *
+ * Y       (input/output)                float *
+ *         On entry,  Y  points to the  first entry to be accessed of an
+ *         incremented array of size equal to or greater than
+ *            ( 1 + ( n - 1 ) * abs( INCY ) ) * sizeof(   float   ),
+ *         that contains the vector y.  On exit,  the entries of the in-
+ *         cremented array  Y are updated with the scaled entries of the
+ *         incremented array  X.
+ *
+ * INCY    (input)                       const int
+ *         On entry, INCY specifies the increment for the elements of Y.
+ *         INCY must not be zero. Unchanged on exit.
+ *
+ * ---------------------------------------------------------------------
+ */
+/*
+ * .. Local Variables ..
+ */
+   register const float       alpha = ALPHA;
+   register float             x0, x1, x2, x3, y0, y1, y2, y3;
+   float                      * StX;
+   register int               i;
+   int                        nu;
+   const int                  incX2 = 2 * INCX, incY2 = 2 * INCY,
+                              incX3 = 3 * INCX, incY3 = 3 * INCY,
+                              incX4 = 4 * INCX, incY4 = 4 * INCY;
+/* ..
+ * .. Executable Statements ..
+ *
+ */
+   if( ( N > 0 ) && ( alpha != ATL_sZERO ) )
+   {
+      if( ( nu = ( N >> 2 ) << 2 ) != 0 )
+      {
+         StX = (float *)X + nu * INCX;
+
+         do
+         {
+            x0 = (*X);     y0 = (*Y);     x1 = X[INCX ]; y1 = Y[INCY ];
+            x2 = X[incX2]; y2 = Y[incY2]; x3 = X[incX3]; y3 = Y[incY3];
+
+            *Y       = y0 + alpha * x0; Y[INCY ] = y1 + alpha * x1;
+            Y[incY2] = y2 + alpha * x2; Y[incY3] = y3 + alpha * x3;
+
+            X  += incX4;
+            Y  += incY4;
+
+         } while( X != StX );
+      }
+
+      for( i = N - nu; i != 0; i-- )
+      {
+         x0  = (*X);
+         y0  = (*Y);
+
+         *Y  = y0 + alpha * x0;
+
+         X  += INCX;
+         Y  += INCY;
+      }
+   }
+/*
+ * End of ATL_srefaxpy
+ */
+}
diff --git a/sklearn/svm/tests/test_sparse.py b/sklearn/svm/tests/test_sparse.py
index e4b6c30c88ba..278e7b4e19dd 100644
--- a/sklearn/svm/tests/test_sparse.py
+++ b/sklearn/svm/tests/test_sparse.py
@@ -10,7 +10,8 @@
 from sklearn.svm.tests import test_svm
 from sklearn.exceptions import ConvergenceWarning
 from sklearn.utils.extmath import safe_sparse_dot
-from sklearn.utils.testing import assert_warns, assert_raise_message
+from sklearn.utils.testing import (assert_warns, assert_raise_message,
+                                   ignore_warnings)
 
 # test sample 1
 X = np.array([[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]])
@@ -334,7 +335,9 @@ def test_timeout():
 
 def test_consistent_proba():
     a = svm.SVC(probability=True, max_iter=1, random_state=0)
-    proba_1 = a.fit(X, Y).predict_proba(X)
+    with ignore_warnings(category=ConvergenceWarning):
+        proba_1 = a.fit(X, Y).predict_proba(X)
     a = svm.SVC(probability=True, max_iter=1, random_state=0)
-    proba_2 = a.fit(X, Y).predict_proba(X)
+    with ignore_warnings(category=ConvergenceWarning):
+        proba_2 = a.fit(X, Y).predict_proba(X)
     assert_array_almost_equal(proba_1, proba_2)
diff --git a/sklearn/tests/test_base.py b/sklearn/tests/test_base.py
index 6f4be0dcc8ab..3770168a0b18 100644
--- a/sklearn/tests/test_base.py
+++ b/sklearn/tests/test_base.py
@@ -73,7 +73,7 @@ def predict(self, X=None):
 
 
 class VargEstimator(BaseEstimator):
-    """Sklearn estimators shouldn't have vargs."""
+    """scikit-learn estimators shouldn't have vargs."""
     def __init__(self, *vargs):
         pass
 
diff --git a/sklearn/tests/test_cross_validation.py b/sklearn/tests/test_cross_validation.py
index 60962aa0dffd..0e03cad783e5 100644
--- a/sklearn/tests/test_cross_validation.py
+++ b/sklearn/tests/test_cross_validation.py
@@ -7,6 +7,7 @@
 from scipy.sparse import csr_matrix
 from scipy import stats
 
+from sklearn.exceptions import ConvergenceWarning
 from sklearn.utils.testing import assert_true
 from sklearn.utils.testing import assert_false
 from sklearn.utils.testing import assert_equal
@@ -1167,14 +1168,16 @@ def test_cross_val_predict_input_types():
     assert_equal(predictions.shape, (10,))
 
     # test with multioutput y
-    predictions = cval.cross_val_predict(clf, X_sparse, X)
+    with ignore_warnings(category=ConvergenceWarning):
+        predictions = cval.cross_val_predict(clf, X_sparse, X)
     assert_equal(predictions.shape, (10, 2))
 
     predictions = cval.cross_val_predict(clf, X_sparse, y)
     assert_array_equal(predictions.shape, (10,))
 
     # test with multioutput y
-    predictions = cval.cross_val_predict(clf, X_sparse, X)
+    with ignore_warnings(category=ConvergenceWarning):
+        predictions = cval.cross_val_predict(clf, X_sparse, X)
     assert_array_equal(predictions.shape, (10, 2))
 
     # test with X and y as list
diff --git a/sklearn/tests/test_pipeline.py b/sklearn/tests/test_pipeline.py
index a8c5fff4efe8..518c966169a2 100644
--- a/sklearn/tests/test_pipeline.py
+++ b/sklearn/tests/test_pipeline.py
@@ -14,8 +14,9 @@
 from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_warns_message
+from sklearn.utils.testing import assert_dict_equal
 
-from sklearn.base import clone
+from sklearn.base import clone, BaseEstimator
 from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline, make_union
 from sklearn.svm import SVC
 from sklearn.linear_model import LogisticRegression
@@ -38,7 +39,7 @@
 )
 
 
-class IncorrectT(object):
+class NoFit(object):
     """Small class to test parameter dispatching.
     """
 
@@ -47,7 +48,7 @@ def __init__(self, a=None, b=None):
         self.b = b
 
 
-class T(IncorrectT):
+class NoTrans(NoFit):
 
     def fit(self, X, y):
         return self
@@ -60,8 +61,12 @@ def set_params(self, **params):
         return self
 
 
-class TransfT(T):
+class NoInvTransf(NoTrans):
+    def transform(self, X, y=None):
+        return X
+
 
+class Transf(NoInvTransf):
     def transform(self, X, y=None):
         return X
 
@@ -69,7 +74,29 @@ def inverse_transform(self, X):
         return X
 
 
-class FitParamT(object):
+class Mult(BaseEstimator):
+    def __init__(self, mult=1):
+        self.mult = mult
+
+    def fit(self, X, y):
+        return self
+
+    def transform(self, X):
+        return np.asarray(X) * self.mult
+
+    def inverse_transform(self, X):
+        return np.asarray(X) / self.mult
+
+    def predict(self, X):
+        return (np.asarray(X) * self.mult).sum(axis=1)
+
+    predict_proba = predict_log_proba = decision_function = predict
+
+    def score(self, X, y=None):
+        return np.sum(X)
+
+
+class FitParamT(BaseEstimator):
     """Mock classifier
     """
 
@@ -88,9 +115,12 @@ def test_pipeline_init():
     assert_raises(TypeError, Pipeline)
     # Check that we can't instantiate pipelines with objects without fit
     # method
-    pipe = assert_raises(TypeError, Pipeline, [('svc', IncorrectT)])
+    assert_raises_regex(TypeError,
+                        'Last step of Pipeline should implement fit. '
+                        '.*NoFit.*',
+                        Pipeline, [('clf', NoFit())])
     # Smoke test with only an estimator
-    clf = T()
+    clf = NoTrans()
     pipe = Pipeline([('svc', clf)])
     assert_equal(pipe.get_params(deep=True),
                  dict(svc__a=None, svc__b=None, svc=clf,
@@ -108,8 +138,12 @@ def test_pipeline_init():
     filter1 = SelectKBest(f_classif)
     pipe = Pipeline([('anova', filter1), ('svc', clf)])
 
-    # Check that we can't use the same stage name twice
-    assert_raises(ValueError, Pipeline, [('svc', SVC()), ('svc', SVC())])
+    # Check that we can't instantiate with non-transformers on the way
+    # Note that NoTrans implements fit, but not transform
+    assert_raises_regex(TypeError,
+                        'All intermediate steps should be transformers'
+                        '.*\\bNoTrans\\b.*',
+                        Pipeline, [('t', NoTrans()), ('svc', clf)])
 
     # Check that params are set
     pipe.set_params(svc__C=0.1)
@@ -160,7 +194,7 @@ def test_pipeline_methods_anova():
 
 def test_pipeline_fit_params():
     # Test that the pipeline can take fit parameters
-    pipe = Pipeline([('transf', TransfT()), ('clf', FitParamT())])
+    pipe = Pipeline([('transf', Transf()), ('clf', FitParamT())])
     pipe.fit(X=None, y=None, clf__should_succeed=True)
     # classifier should return True
     assert_true(pipe.predict(None))
@@ -297,17 +331,24 @@ def test_feature_union():
     assert_equal(fs.fit_transform(X, y).shape, (X.shape[0], 4))
 
     # test it works with transformers missing fit_transform
-    fs = FeatureUnion([("mock", TransfT()), ("svd", svd), ("select", select)])
+    fs = FeatureUnion([("mock", Transf()), ("svd", svd), ("select", select)])
     X_transformed = fs.fit_transform(X, y)
     assert_equal(X_transformed.shape, (X.shape[0], 8))
 
+    # test error if some elements do not support transform
+    assert_raises_regex(TypeError,
+                        'All estimators should implement fit and '
+                        'transform.*\\bNoTrans\\b',
+                        FeatureUnion,
+                        [("transform", Transf()), ("no_transform", NoTrans())])
+
 
 def test_make_union():
     pca = PCA(svd_solver='full')
-    mock = TransfT()
+    mock = Transf()
     fu = make_union(pca, mock)
     names, transformers = zip(*fu.transformer_list)
-    assert_equal(names, ("pca", "transft"))
+    assert_equal(names, ("pca", "transf"))
     assert_equal(transformers, (pca, mock))
 
 
@@ -336,28 +377,149 @@ def test_pipeline_fit_transform():
     iris = load_iris()
     X = iris.data
     y = iris.target
-    transft = TransfT()
-    pipeline = Pipeline([('mock', transft)])
+    transf = Transf()
+    pipeline = Pipeline([('mock', transf)])
 
     # test fit_transform:
     X_trans = pipeline.fit_transform(X, y)
-    X_trans2 = transft.fit(X, y).transform(X)
+    X_trans2 = transf.fit(X, y).transform(X)
     assert_array_almost_equal(X_trans, X_trans2)
 
 
-def test_make_pipeline():
-    t1 = TransfT()
-    t2 = TransfT()
+def test_set_pipeline_steps():
+    transf1 = Transf()
+    transf2 = Transf()
+    pipeline = Pipeline([('mock', transf1)])
+    assert_true(pipeline.named_steps['mock'] is transf1)
+
+    # Directly setting attr
+    pipeline.steps = [('mock2', transf2)]
+    assert_true('mock' not in pipeline.named_steps)
+    assert_true(pipeline.named_steps['mock2'] is transf2)
+    assert_equal([('mock2', transf2)], pipeline.steps)
+
+    # Using set_params
+    pipeline.set_params(steps=[('mock', transf1)])
+    assert_equal([('mock', transf1)], pipeline.steps)
+
+    # Using set_params to replace single step
+    pipeline.set_params(mock=transf2)
+    assert_equal([('mock', transf2)], pipeline.steps)
+
+    # With invalid data
+    pipeline.set_params(steps=[('junk', ())])
+    assert_raises(TypeError, pipeline.fit, [[1]], [1])
+    assert_raises(TypeError, pipeline.fit_transform, [[1]], [1])
+
+
+def test_set_pipeline_step_none():
+    # Test setting Pipeline steps to None
+    X = np.array([[1]])
+    y = np.array([1])
+    mult2 = Mult(mult=2)
+    mult3 = Mult(mult=3)
+    mult5 = Mult(mult=5)
+
+    def make():
+        return Pipeline([('m2', mult2), ('m3', mult3), ('last', mult5)])
+
+    pipeline = make()
+
+    exp = 2 * 3 * 5
+    assert_array_equal([[exp]], pipeline.fit_transform(X, y))
+    assert_array_equal([exp], pipeline.fit(X).predict(X))
+    assert_array_equal(X, pipeline.inverse_transform([[exp]]))
+
+    pipeline.set_params(m3=None)
+    exp = 2 * 5
+    assert_array_equal([[exp]], pipeline.fit_transform(X, y))
+    assert_array_equal([exp], pipeline.fit(X).predict(X))
+    assert_array_equal(X, pipeline.inverse_transform([[exp]]))
+    assert_dict_equal(pipeline.get_params(deep=True),
+                      {'steps': pipeline.steps,
+                       'm2': mult2,
+                       'm3': None,
+                       'last': mult5,
+                       'm2__mult': 2,
+                       'last__mult': 5,
+                       })
+
+    pipeline.set_params(m2=None)
+    exp = 5
+    assert_array_equal([[exp]], pipeline.fit_transform(X, y))
+    assert_array_equal([exp], pipeline.fit(X).predict(X))
+    assert_array_equal(X, pipeline.inverse_transform([[exp]]))
+
+    # for other methods, ensure no AttributeErrors on None:
+    other_methods = ['predict_proba', 'predict_log_proba',
+                     'decision_function', 'transform', 'score']
+    for method in other_methods:
+        getattr(pipeline, method)(X)
+
+    pipeline.set_params(m2=mult2)
+    exp = 2 * 5
+    assert_array_equal([[exp]], pipeline.fit_transform(X, y))
+    assert_array_equal([exp], pipeline.fit(X).predict(X))
+    assert_array_equal(X, pipeline.inverse_transform([[exp]]))
+
+    pipeline = make()
+    pipeline.set_params(last=None)
+    # mult2 and mult3 are active
+    exp = 6
+    assert_array_equal([[exp]], pipeline.fit(X, y).transform(X))
+    assert_array_equal([[exp]], pipeline.fit_transform(X, y))
+    assert_array_equal(X, pipeline.inverse_transform([[exp]]))
+    assert_raise_message(AttributeError,
+                         "'NoneType' object has no attribute 'predict'",
+                         getattr, pipeline, 'predict')
+
+    # Check None step at construction time
+    exp = 2 * 5
+    pipeline = Pipeline([('m2', mult2), ('m3', None), ('last', mult5)])
+    assert_array_equal([[exp]], pipeline.fit_transform(X, y))
+    assert_array_equal([exp], pipeline.fit(X).predict(X))
+    assert_array_equal(X, pipeline.inverse_transform([[exp]]))
+
+
+def test_pipeline_ducktyping():
+    pipeline = make_pipeline(Mult(5))
+    pipeline.predict
+    pipeline.transform
+    pipeline.inverse_transform
+
+    pipeline = make_pipeline(Transf())
+    assert_false(hasattr(pipeline, 'predict'))
+    pipeline.transform
+    pipeline.inverse_transform
+
+    pipeline = make_pipeline(None)
+    assert_false(hasattr(pipeline, 'predict'))
+    pipeline.transform
+    pipeline.inverse_transform
+
+    pipeline = make_pipeline(Transf(), NoInvTransf())
+    assert_false(hasattr(pipeline, 'predict'))
+    pipeline.transform
+    assert_false(hasattr(pipeline, 'inverse_transform'))
+
+    pipeline = make_pipeline(NoInvTransf(), Transf())
+    assert_false(hasattr(pipeline, 'predict'))
+    pipeline.transform
+    assert_false(hasattr(pipeline, 'inverse_transform'))
 
+
+def test_make_pipeline():
+    t1 = Transf()
+    t2 = Transf()
     pipe = make_pipeline(t1, t2)
     assert_true(isinstance(pipe, Pipeline))
-    assert_equal(pipe.steps[0][0], "transft-1")
-    assert_equal(pipe.steps[1][0], "transft-2")
+    assert_equal(pipe.steps[0][0], "transf-1")
+    assert_equal(pipe.steps[1][0], "transf-2")
 
     pipe = make_pipeline(t1, t2, FitParamT())
     assert_true(isinstance(pipe, Pipeline))
-    assert_equal(pipe.steps[0][0], "transft-1")
-    assert_equal(pipe.steps[1][0], "transft-2")
+    assert_equal(pipe.steps[0][0], "transf-1")
+    assert_equal(pipe.steps[1][0], "transf-2")
     assert_equal(pipe.steps[2][0], "fitparamt")
 
 
@@ -378,7 +540,7 @@ def test_feature_union_weights():
                       transformer_weights={"pca": 10})
     X_fit_transformed = fs.fit_transform(X, y)
     # test it works with transformers missing fit_transform
-    fs = FeatureUnion([("mock", TransfT()), ("pca", pca), ("select", select)],
+    fs = FeatureUnion([("mock", Transf()), ("pca", pca), ("select", select)],
                       transformer_weights={"mock": 10})
     X_fit_transformed_wo_method = fs.fit_transform(X, y)
     # check against expected result
@@ -450,6 +612,11 @@ def test_feature_union_feature_names():
         assert_true("chars__" in feat or "words__" in feat)
     assert_equal(len(feature_names), 35)
 
+    ft = FeatureUnion([("tr1", Transf())]).fit([[1]])
+    assert_raise_message(AttributeError,
+                         'Transformer tr1 (type Transf) does not provide '
+                         'get_feature_names', ft.get_feature_names)
+
 
 def test_classes_property():
     iris = load_iris()
@@ -467,8 +634,96 @@ def test_classes_property():
 
 
 def test_X1d_inverse_transform():
-    transformer = TransfT()
+    transformer = Transf()
     pipeline = make_pipeline(transformer)
     X = np.ones(10)
     msg = "1d X will not be reshaped in pipeline.inverse_transform"
     assert_warns_message(FutureWarning, msg, pipeline.inverse_transform, X)
+
+
+def test_set_feature_union_steps():
+    mult2 = Mult(2)
+    mult2.get_feature_names = lambda: ['x2']
+    mult3 = Mult(3)
+    mult3.get_feature_names = lambda: ['x3']
+    mult5 = Mult(5)
+    mult5.get_feature_names = lambda: ['x5']
+
+    ft = FeatureUnion([('m2', mult2), ('m3', mult3)])
+    assert_array_equal([[2, 3]], ft.transform(np.asarray([[1]])))
+    assert_equal(['m2__x2', 'm3__x3'], ft.get_feature_names())
+
+    # Directly setting attr
+    ft.transformer_list = [('m5', mult5)]
+    assert_array_equal([[5]], ft.transform(np.asarray([[1]])))
+    assert_equal(['m5__x5'], ft.get_feature_names())
+
+    # Using set_params
+    ft.set_params(transformer_list=[('mock', mult3)])
+    assert_array_equal([[3]], ft.transform(np.asarray([[1]])))
+    assert_equal(['mock__x3'], ft.get_feature_names())
+
+    # Using set_params to replace single step
+    ft.set_params(mock=mult5)
+    assert_array_equal([[5]], ft.transform(np.asarray([[1]])))
+    assert_equal(['mock__x5'], ft.get_feature_names())
+
+
+def test_set_feature_union_step_none():
+    mult2 = Mult(2)
+    mult2.get_feature_names = lambda: ['x2']
+    mult3 = Mult(3)
+    mult3.get_feature_names = lambda: ['x3']
+    X = np.asarray([[1]])
+
+    ft = FeatureUnion([('m2', mult2), ('m3', mult3)])
+    assert_array_equal([[2, 3]], ft.fit(X).transform(X))
+    assert_array_equal([[2, 3]], ft.fit_transform(X))
+    assert_equal(['m2__x2', 'm3__x3'], ft.get_feature_names())
+
+    ft.set_params(m2=None)
+    assert_array_equal([[3]], ft.fit(X).transform(X))
+    assert_array_equal([[3]], ft.fit_transform(X))
+    assert_equal(['m3__x3'], ft.get_feature_names())
+
+    ft.set_params(m3=None)
+    assert_array_equal([[]], ft.fit(X).transform(X))
+    assert_array_equal([[]], ft.fit_transform(X))
+    assert_equal([], ft.get_feature_names())
+
+    # check we can change back
+    ft.set_params(m3=mult3)
+    assert_array_equal([[3]], ft.fit(X).transform(X))
+
+
+def test_step_name_validation():
+    bad_steps1 = [('a__q', Mult(2)), ('b', Mult(3))]
+    bad_steps2 = [('a', Mult(2)), ('a', Mult(3))]
+    for cls, param in [(Pipeline, 'steps'),
+                       (FeatureUnion, 'transformer_list')]:
+        # we validate in construction (despite scikit-learn convention)
+        bad_steps3 = [('a', Mult(2)), (param, Mult(3))]
+        for bad_steps, message in [
+            (bad_steps1, "Step names must not contain __: got ['a__q']"),
+            (bad_steps2, "Names provided are not unique: ['a', 'a']"),
+            (bad_steps3, "Step names conflict with constructor "
+                         "arguments: ['%s']" % param),
+        ]:
+            # three ways to make invalid:
+            # - construction
+            assert_raise_message(ValueError, message, cls,
+                                 **{param: bad_steps})
+
+            # - setattr
+            est = cls(**{param: [('a', Mult(1))]})
+            setattr(est, param, bad_steps)
+            assert_raise_message(ValueError, message, est.fit, [[1]], [1])
+            assert_raise_message(ValueError, message, est.fit_transform,
+                                 [[1]], [1])
+
+            # - set_params
+            est = cls(**{param: [('a', Mult(1))]})
+            est.set_params(**{param: bad_steps})
+            assert_raise_message(ValueError, message, est.fit, [[1]], [1])
+            assert_raise_message(ValueError, message, est.fit_transform,
+                                 [[1]], [1])
diff --git a/sklearn/tree/_tree.pyx b/sklearn/tree/_tree.pyx
index f3db4a197580..64e664f09eba 100644
--- a/sklearn/tree/_tree.pyx
+++ b/sklearn/tree/_tree.pyx
@@ -285,8 +285,6 @@ cdef class BestFirstTreeBuilder(TreeBuilder):
 
     The best node to expand is given by the node at the frontier that has the
     highest impurity improvement.
-
-    NOTE: this TreeBuilder will ignore ``tree.max_depth`` .
     """
     cdef SIZE_t max_leaf_nodes
 
diff --git a/sklearn/tree/tests/test_tree.py b/sklearn/tree/tests/test_tree.py
index 761fa2cb40f4..61a010274b93 100644
--- a/sklearn/tree/tests/test_tree.py
+++ b/sklearn/tree/tests/test_tree.py
@@ -4,7 +4,7 @@
 import pickle
 from functools import partial
 from itertools import product
-import platform
+import struct
 
 import numpy as np
 from scipy.sparse import csc_matrix
@@ -1121,7 +1121,7 @@ def test_realloc():
 
 
 def test_huge_allocations():
-    n_bits = int(platform.architecture()[0].rstrip('bit'))
+    n_bits = 8 * struct.calcsize("P")
 
     X = np.random.randn(10, 2)
     y = np.random.randint(0, 2, 10)
diff --git a/sklearn/tree/tree.py b/sklearn/tree/tree.py
index 91db1a162607..1ec678931752 100644
--- a/sklearn/tree/tree.py
+++ b/sklearn/tree/tree.py
@@ -566,7 +566,6 @@ class DecisionTreeClassifier(BaseDecisionTree, ClassifierMixin):
         The maximum depth of the tree. If None, then nodes are expanded until
         all leaves are pure or until all leaves contain less than
         min_samples_split samples.
-        Ignored if ``max_leaf_nodes`` is not None.
 
     min_samples_split : int, float, optional (default=2)
         The minimum number of samples required to split an internal node:
@@ -592,7 +591,6 @@ class DecisionTreeClassifier(BaseDecisionTree, ClassifierMixin):
         Grow a tree with ``max_leaf_nodes`` in best-first fashion.
         Best nodes are defined as relative reduction in impurity.
         If None then unlimited number of leaf nodes.
-        If not None then ``max_depth`` will be ignored.
 
     class_weight : dict, list of dicts, "balanced" or None, optional (default=None)
         Weights associated with classes in the form ``{class_label: weight}``.
@@ -832,7 +830,6 @@ class DecisionTreeRegressor(BaseDecisionTree, RegressorMixin):
         The maximum depth of the tree. If None, then nodes are expanded until
         all leaves are pure or until all leaves contain less than
         min_samples_split samples.
-        Ignored if ``max_leaf_nodes`` is not None.
 
     min_samples_split : int, float, optional (default=2)
         The minimum number of samples required to split an internal node:
@@ -858,7 +855,6 @@ class DecisionTreeRegressor(BaseDecisionTree, RegressorMixin):
         Grow a tree with ``max_leaf_nodes`` in best-first fashion.
         Best nodes are defined as relative reduction in impurity.
         If None then unlimited number of leaf nodes.
-        If not None then ``max_depth`` will be ignored.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
diff --git a/sklearn/utils/__init__.py b/sklearn/utils/__init__.py
index 4d62e4f39418..21e9e0693d25 100644
--- a/sklearn/utils/__init__.py
+++ b/sklearn/utils/__init__.py
@@ -32,7 +32,7 @@ class ConvergenceWarning(_ConvergenceWarning):
            "compute_class_weight", "compute_sample_weight",
            "column_or_1d", "safe_indexing",
            "check_consistent_length", "check_X_y", 'indexable',
-           "check_symmetric"]
+           "check_symmetric", "indices_to_mask"]
 
 
 def safe_mask(X, mask):
@@ -419,3 +419,27 @@ def tosequence(x):
         return x
     else:
         return list(x)
+
+
+def indices_to_mask(indices, mask_length):
+    """Convert list of indices to boolean mask.
+
+    Parameters
+    ----------
+    indices : list-like
+        List of integers treated as indices.
+    mask_length : int
+        Length of boolean mask to be generated.
+
+    Returns
+    -------
+    mask : 1d boolean nd-array
+        Boolean array that is True where indices are present, else False.
+    """
+    if mask_length <= np.max(indices):
+        raise ValueError("mask_length must be greater than max(indices)")
+
+    mask = np.zeros(mask_length, dtype=np.bool)
+    mask[indices] = True
+
+    return mask
diff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py
index aaf174906f96..db27550d973b 100644
--- a/sklearn/utils/estimator_checks.py
+++ b/sklearn/utils/estimator_checks.py
@@ -220,7 +220,7 @@ def _yield_all_checks(name, Estimator):
 
 
 def check_estimator(Estimator):
-    """Check if estimator adheres to sklearn conventions.
+    """Check if estimator adheres to scikit-learn conventions.
 
     This estimator will run an extensive test-suite for input validation,
     shapes, etc.
@@ -1529,6 +1529,9 @@ def __init__(self):
         def fit(self, X, y):
             return self
 
+        def transform(self, X):
+            return X
+
     if name in ('FeatureUnion', 'Pipeline'):
         e = estimator([('clf', T())])
 
diff --git a/sklearn/utils/metaestimators.py b/sklearn/utils/metaestimators.py
index 330e3790691b..c77249fbe97f 100644
--- a/sklearn/utils/metaestimators.py
+++ b/sklearn/utils/metaestimators.py
@@ -83,5 +83,6 @@ def if_fitted_delegate_has_method(delegate, *args, **kwargs):
         base object.
 
     """
-    fn = lambda obj: check_is_fitted(obj, *args, **kwargs)
+    def fn(obj):
+        return check_is_fitted(obj, *args, **kwargs)
     return if_delegate_has_method(delegate, fn_check_obj=fn)
diff --git a/sklearn/utils/mocking.py b/sklearn/utils/mocking.py
index 1fd9e03d3d83..507f08c8e7c1 100644
--- a/sklearn/utils/mocking.py
+++ b/sklearn/utils/mocking.py
@@ -26,7 +26,7 @@ def __init__(self, array):
     def __len__(self):
         return len(self.array)
 
-    def __array__(self):
+    def __array__(self, dtype=None):
         # Pandas data frames also are array-like: we want to make sure that
         # input validation in cross-validation does not try to call that
         # method.
diff --git a/sklearn/utils/testing.py b/sklearn/utils/testing.py
index 1035c4e7b9a2..2eeead9711bb 100644
--- a/sklearn/utils/testing.py
+++ b/sklearn/utils/testing.py
@@ -54,6 +54,11 @@
 from nose.tools import assert_false
 from nose.tools import assert_raises
 from nose.tools import raises
+try:
+    from nose.tools import assert_dict_equal
+except ImportError:
+    # Not in old versions of nose, but is only for formatting anyway
+    assert_dict_equal = assert_equal
 from nose import SkipTest
 from nose import with_setup
 
diff --git a/sklearn/utils/tests/test_multiclass.py b/sklearn/utils/tests/test_multiclass.py
index f6c45e1015d7..d103a177d716 100644
--- a/sklearn/utils/tests/test_multiclass.py
+++ b/sklearn/utils/tests/test_multiclass.py
@@ -36,7 +36,7 @@ class NotAnArray(object):
     def __init__(self, data):
         self.data = data
 
-    def __array__(self):
+    def __array__(self, dtype=None):
         return self.data
 
 
diff --git a/sklearn/utils/tests/test_validation.py b/sklearn/utils/tests/test_validation.py
index d577864fb709..a6268b08d1a8 100644
--- a/sklearn/utils/tests/test_validation.py
+++ b/sklearn/utils/tests/test_validation.py
@@ -236,6 +236,16 @@ def test_check_array_pandas_dtype_object_conversion():
     assert_equal(check_array(X_df, ensure_2d=False).dtype.kind, "f")
 
 
+def test_check_array_on_mock_dataframe():
+    arr = np.array([[0.2, 0.7], [0.6, 0.5], [0.4, 0.1], [0.7, 0.2]])
+    mock_df = MockDataFrame(arr)
+    checked_arr = check_array(mock_df)
+    assert_equal(checked_arr.dtype,
+                 arr.dtype)
+    checked_arr = check_array(mock_df, dtype=np.float32)
+    assert_equal(checked_arr.dtype, np.dtype(np.float32))
+
+
 def test_check_array_dtype_stability():
     # test that lists with ints don't get converted to floats
     X = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
diff --git a/sklearn/utils/validation.py b/sklearn/utils/validation.py
index c7444968aa74..deb98eef8503 100644
--- a/sklearn/utils/validation.py
+++ b/sklearn/utils/validation.py
@@ -174,10 +174,11 @@ def check_consistent_length(*arrays):
         Objects that will be checked for consistent length.
     """
 
-    uniques = np.unique([_num_samples(X) for X in arrays if X is not None])
+    lengths = [_num_samples(X) for X in arrays if X is not None]
+    uniques = np.unique(lengths)
     if len(uniques) > 1:
-        raise ValueError("Found arrays with inconsistent numbers of samples: "
-                         "%s" % str(uniques))
+        raise ValueError("Found input variables with inconsistent numbers of"
+                         " samples: %r" % [int(l) for l in lengths])
 
 
 def indexable(*iterables):
