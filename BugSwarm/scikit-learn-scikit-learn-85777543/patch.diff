diff --git a/continuous_integration/appveyor/requirements.txt b/continuous_integration/appveyor/requirements.txt
index 3de7bc2797..b8c41fc03d 100755
--- a/continuous_integration/appveyor/requirements.txt
+++ b/continuous_integration/appveyor/requirements.txt
@@ -2,7 +2,7 @@
 # Those wheels were collected from http://www.lfd.uci.edu/~gohlke/pythonlibs/
 # This is a temporary solution. As soon as numpy and scipy provide official
 # wheel for windows we ca delete this --find-links line.
---find-links http://28daf2247a33ed269873-7b1aad3fab3cc330e1fd9d109892382a.r6.cf2.rackcdn.com/index.html
+--find-links http://28daf2247a33ed269873-7b1aad3fab3cc330e1fd9d109892382a.r6.cf2.rackcdn.com/
 
 # fix the versions of numpy to force the use of numpy and scipy to use the whl
 # of the rackspace folder instead of trying to install from more recent
diff --git a/examples/ensemble/plot_feature_transformation.py b/examples/ensemble/plot_feature_transformation.py
index 7b6941846a..1d5b927e48 100755
--- a/examples/ensemble/plot_feature_transformation.py
+++ b/examples/ensemble/plot_feature_transformation.py
@@ -34,10 +34,10 @@
 from sklearn.linear_model import LogisticRegression
 from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,
                               GradientBoostingClassifier)
-from sklearn.feature_selection import SelectFromModel
 from sklearn.preprocessing import OneHotEncoder
 from sklearn.cross_validation import train_test_split
 from sklearn.metrics import roc_curve
+from sklearn.pipeline import make_pipeline
 
 n_estimator = 10
 X, y = make_classification(n_samples=80000)
@@ -51,13 +51,13 @@
                                                             test_size=0.5)
 
 # Unsupervised transformation based on totally random trees
-rt = RandomTreesEmbedding(max_depth=3, n_estimators=n_estimator)
-rt_lm = LogisticRegression()
-rt.fit(X_train, y_train)
-rt_lm.fit(SelectFromModel(rt, prefit=True).transform(X_train_lr), y_train_lr)
+rt = RandomTreesEmbedding(max_depth=3, n_estimators=n_estimator,
+	random_state=0)
 
-y_pred_rt = rt_lm.predict_proba(
-	SelectFromModel(rt, prefit=True).transform(X_test))[:, 1]
+rt_lm = LogisticRegression()
+pipeline = make_pipeline(rt, rt_lm)
+pipeline.fit(X_train, y_train)
+y_pred_rt = pipeline.predict_proba(X_test)[:, 1]
 fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_test, y_pred_rt)
 
 # Supervised transformation based on random forests
diff --git a/examples/ensemble/plot_random_forest_embedding.py b/examples/ensemble/plot_random_forest_embedding.py
index eef04ac333..ba6329d729 100755
--- a/examples/ensemble/plot_random_forest_embedding.py
+++ b/examples/ensemble/plot_random_forest_embedding.py
@@ -30,7 +30,6 @@
 from sklearn.datasets import make_circles
 from sklearn.ensemble import RandomTreesEmbedding, ExtraTreesClassifier
 from sklearn.decomposition import TruncatedSVD
-from sklearn.feature_selection import SelectFromModel
 from sklearn.naive_bayes import BernoulliNB
 
 # make a synthetic dataset
@@ -38,9 +37,7 @@
 
 # use RandomTreesEmbedding to transform data
 hasher = RandomTreesEmbedding(n_estimators=10, random_state=0, max_depth=3)
-hasher.fit(X)
-model = SelectFromModel(hasher, prefit=True)
-X_transformed = model.transform(X)
+X_transformed = hasher.fit_transform(X)
 
 # Visualize result using PCA
 pca = TruncatedSVD(n_components=2)
diff --git a/examples/linear_model/plot_theilsen.py b/examples/linear_model/plot_theilsen.py
index fc0ba571cc..bb9b093721 100755
--- a/examples/linear_model/plot_theilsen.py
+++ b/examples/linear_model/plot_theilsen.py
@@ -48,6 +48,8 @@
 estimators = [('OLS', LinearRegression()),
               ('Theil-Sen', TheilSenRegressor(random_state=42)),
               ('RANSAC', RANSACRegressor(random_state=42)), ]
+colors = {'OLS':'green', 'Theil-Sen':'blue', 'RANSAC':'red'}
+linestyle = {'OLS':'-', 'Theil-Sen':':', 'RANSAC':'--'}
 
 ##############################################################################
 # Outliers only in the y direction
@@ -71,7 +73,7 @@
     estimator.fit(X, y)
     elapsed_time = time.time() - t0
     y_pred = estimator.predict(line_x.reshape(2, 1))
-    plt.plot(line_x, y_pred,
+    plt.plot(line_x, y_pred, color = colors[name], linestyle = linestyle[name],
              label='%s (fit time: %.2fs)' % (name, elapsed_time))
 
 plt.axis('tight')
@@ -100,7 +102,7 @@
     estimator.fit(X, y)
     elapsed_time = time.time() - t0
     y_pred = estimator.predict(line_x.reshape(2, 1))
-    plt.plot(line_x, y_pred,
+    plt.plot(line_x, y_pred, color = colors[name], linestyle = linestyle[name],
              label='%s (fit time: %.2fs)' % (name, elapsed_time))
 
 plt.axis('tight')
diff --git a/sklearn/cluster/affinity_propagation_.py b/sklearn/cluster/affinity_propagation_.py
index 799ff15489..3a5d725408 100755
--- a/sklearn/cluster/affinity_propagation_.py
+++ b/sklearn/cluster/affinity_propagation_.py
@@ -19,7 +19,7 @@
 
 
 def affinity_propagation(S, preference=None, convergence_iter=15, max_iter=200,
-                         damping=0.9, copy=True, verbose=False,
+                         damping=None, copy=True, verbose=False,
                          return_n_iter=False):
     """Perform Affinity Propagation Clustering of data
 
@@ -47,7 +47,7 @@ def affinity_propagation(S, preference=None, convergence_iter=15, max_iter=200,
     max_iter : int, optional, default: 200
         Maximum number of iterations
 
-    damping : float, optional, default: 0.9
+    damping : float, optional, default: 0.5
         Damping factor between 0.5 and 1.
 
     copy : boolean, optional, default: True
@@ -82,6 +82,12 @@ def affinity_propagation(S, preference=None, convergence_iter=15, max_iter=200,
     Brendan J. Frey and Delbert Dueck, "Clustering by Passing Messages
     Between Data Points", Science Feb. 2007
     """
+    if damping is None:
+        warnings.warn("damping=0.5 has been deprecated "
+                      "in favor of 0.9",
+                      DeprecationWarning)
+        damping = 0.5
+    
     S = as_float_array(S, copy=copy)
     n_samples = S.shape[0]
 
@@ -204,7 +210,7 @@ class AffinityPropagation(BaseEstimator, ClusterMixin):
 
     Parameters
     ----------
-    damping : float, optional, default: 0.9
+    damping : float, optional, default: 0.5
         Damping factor between 0.5 and 1.
 
     convergence_iter : int, optional, default: 15
@@ -224,11 +230,12 @@ class AffinityPropagation(BaseEstimator, ClusterMixin):
         preferences value. If the preferences are not passed as arguments,
         they will be set to the median of the input similarities.
 
-    affinity : string, optional, default=``euclidean``
-        Which affinity to use. At the moment ``precomputed`` and
-        ``euclidean`` are supported. ``euclidean`` uses the
-        negative euclidean distance between points.
-
+    affinity : string, optional, default=``sqeuclidean``
+        Which affinity to use. At the moment ``precomputed``, ``euclidean``,
+        and ``sqeuclidean`` are supported. ``sqeuclidean`` and ``euclidean``
+        use the negative squared euclidean distances between points.
+        ``euclidean`` will mean non-squared distances in future versions.
+        
     verbose : boolean, optional, default: False
         Whether to be verbose.
 
@@ -264,8 +271,8 @@ class AffinityPropagation(BaseEstimator, ClusterMixin):
     Between Data Points", Science Feb. 2007
     """
 
-    def __init__(self, damping=.9, max_iter=200, convergence_iter=15,
-                 copy=True, preference=None, affinity='euclidean',
+    def __init__(self, damping=None, max_iter=200, convergence_iter=15,
+                 copy=True, preference=None, affinity='sqeuclidean',
                  verbose=False):
 
         self.damping = damping
@@ -295,21 +302,25 @@ def fit(self, X, y=None):
         if self.affinity == "precomputed":
             self.affinity_matrix_ = X
         elif self.affinity == "euclidean":
+            warnings.warn("Using squared distances. In the future please use " 
+                          "sqeuclidean for squared and euclidean for "
+                          "for non-squared distances.",
+                          DeprecationWarning)
             self.affinity_matrix_ = -euclidean_distances(X, squared=False)
+        elif self.affinity == "sqeuclidean":
+            self.affinity_matrix_ = -euclidean_distances(X, squared=True)
         else:
             raise ValueError("Affinity must be 'precomputed' or "
                              "'euclidean'. Got %s instead"
                              % str(self.affinity))
 
-        with warnings.catch_warnings():
-            warnings.simplefilter('always', ConvergenceWarning)
-            self.cluster_centers_indices_, self.labels_, self.n_iter_ = \
-                affinity_propagation(
-                    self.affinity_matrix_, self.preference,
-                    max_iter=self.max_iter,
-                    convergence_iter=self.convergence_iter,
-                    damping=self.damping,
-                    copy=self.copy, verbose=self.verbose, return_n_iter=True)
+        self.cluster_centers_indices_, self.labels_, self.n_iter_ = \
+            affinity_propagation(
+                self.affinity_matrix_, self.preference,
+                max_iter=self.max_iter,
+                convergence_iter=self.convergence_iter,
+                damping=self.damping,
+                copy=self.copy, verbose=self.verbose, return_n_iter=True)
 
         if self.affinity != "precomputed":
             self.cluster_centers_ = X[self.cluster_centers_indices_].copy()
diff --git a/sklearn/cluster/tests/test_affinity_propagation.py b/sklearn/cluster/tests/test_affinity_propagation.py
index be76d597ee..20bc6e09c4 100755
--- a/sklearn/cluster/tests/test_affinity_propagation.py
+++ b/sklearn/cluster/tests/test_affinity_propagation.py
@@ -24,7 +24,7 @@
 def test_affinity_propagation():
     # Affinity Propagation algorithm
     # Compute similarities
-    S = -euclidean_distances(X, squared=False)
+    S = -euclidean_distances(X, squared=True)
     preference = np.median(S) * 10
     # Compute Affinity Propagation
     cluster_centers_indices, labels = affinity_propagation(
@@ -67,13 +67,14 @@ def test_affinity_propagation_predict():
     labels2 = af.predict(X)
     assert_array_equal(labels, labels2)
 
+def test_affinity_propagation_deprecation_warning():
+    # Test DeprecationWarning for parameter 'euclidean'
+    af = AffinityPropagation(affinity="euclidean", damping=0.9)
+    assert_warns(DeprecationWarning, af.fit, X)
     
-def test_affinity_propagation_convergence_warning():
-    # Test warning in AffinityPropagation.fit
-    af = AffinityPropagation(affinity="euclidean", max_iter=1)
-    assert_warns(ConvergenceWarning, af.fit, X)
-    assert_raises(ValueError, af.predict, X)
-    
+    # Test DeprecationWarning for default damping=0.5
+    af = AffinityPropagation()
+    assert_warns(DeprecationWarning, af.fit, X)
     
 def test_affinity_propagation_predict_error():
     # Test exception in AffinityPropagation.predict
diff --git a/sklearn/tests/test_discriminant_analysis.py b/sklearn/tests/test_discriminant_analysis.py
index b00289472f..f704f4b427 100755
--- a/sklearn/tests/test_discriminant_analysis.py
+++ b/sklearn/tests/test_discriminant_analysis.py
@@ -1,10 +1,6 @@
-try:
-    # Python 2 compat
-    reload
-except NameError:
-    # Regular Python 3+ import
-    from importlib import reload
+import sys
 import numpy as np
+from nose import SkipTest
 
 from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_array_almost_equal
@@ -22,6 +18,15 @@
 from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
 
 
+# import reload
+version = sys.version_info
+if version[0] == 3:
+    # Python 3+ import for reload. Builtin in Python2
+    if version[1] == 3:
+        reload = None
+    from importlib import reload
+
+
 # Data is just 6 separable points in the plane
 X = np.array([[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]], dtype='f')
 y = np.array([1, 1, 1, 2, 2, 2])
@@ -298,6 +303,9 @@ def test_qda_regularization():
 
 
 def test_deprecated_lda_qda_deprecation():
+    if reload is None:
+        SkipTest("Can't reload module on Python3.3")
+
     def import_lda_module():
         import sklearn.lda
         # ensure that we trigger DeprecationWarning even if the sklearn.lda
diff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py
index e87da57554..3c141e5eb8 100755
--- a/sklearn/utils/estimator_checks.py
+++ b/sklearn/utils/estimator_checks.py
@@ -66,7 +66,7 @@
 # _LearntSelectorMixin is removed.
 DEPRECATED_TRANSFORM = [
     "RandomForestClassifier", "RandomForestRegressor", "ExtraTreesClassifier",
-    "ExtraTreesRegressor", "RandomTreesEmbedding", "DecisionTreeClassifier",
+    "ExtraTreesRegressor", "DecisionTreeClassifier",
     "DecisionTreeRegressor", "ExtraTreeClassifier", "ExtraTreeRegressor",
     "LinearSVC", "SGDClassifier", "SGDRegressor", "Perceptron",
     "LogisticRegression", "LogisticRegressionCV",
