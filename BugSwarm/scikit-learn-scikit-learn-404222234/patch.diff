diff --git a/.circleci/config.yml b/.circleci/config.yml
index 7ad3e5427b..fcc43270c6 100755
--- a/.circleci/config.yml
+++ b/.circleci/config.yml
@@ -91,4 +91,3 @@ workflows:
       - deploy:
           requires:
             - python3
-            - python2
diff --git a/doc/datasets/index.rst b/doc/datasets/index.rst
index d8eabfaabd..babdf065aa 100755
--- a/doc/datasets/index.rst
+++ b/doc/datasets/index.rst
@@ -154,7 +154,7 @@ They can be loaded using the following functions:
 
 .. include:: ./kddcup99.rst
 
-.. _generated_datasets:
+.. _sample_generators:
 
 Generated datasets
 ==================
diff --git a/doc/developers/contributing.rst b/doc/developers/contributing.rst
index 854dbe4442..7892654971 100755
--- a/doc/developers/contributing.rst
+++ b/doc/developers/contributing.rst
@@ -41,7 +41,7 @@ ticket to the
 also welcome to post feature requests or pull requests.
 
 
-=======
+==================
 Ways to contribute
 ==================
 
diff --git a/doc/faq.rst b/doc/faq.rst
index 8d5a6f4f4d..85ec39e45b 100755
--- a/doc/faq.rst
+++ b/doc/faq.rst
@@ -362,7 +362,7 @@ of a single numeric dtype. These do not explicitly represent categorical
 variables at present. Thus, unlike R's data.frames or pandas.DataFrame, we
 require explicit conversion of categorical features to numeric values, as
 discussed in :ref:`preprocessing_categorical_features`.
-See also :ref:`sphx_glr_auto_examples_compose_column_transformer_mixed_types.py` for an
+See also :ref:`sphx_glr_auto_examples_compose_plot_column_transformer_mixed_types.py` for an
 example of working with heterogeneous (e.g. categorical and numeric) data.
 
 Why does Scikit-learn not directly work with, for example, pandas.DataFrame?
diff --git a/doc/modules/compose.rst b/doc/modules/compose.rst
index ebb879182b..8f303ed585 100755
--- a/doc/modules/compose.rst
+++ b/doc/modules/compose.rst
@@ -500,5 +500,5 @@ above example would be::
 
 .. topic:: Examples:
 
- * :ref:`sphx_glr_auto_examples_compose_column_transformer.py`
- * :ref:`sphx_glr_auto_examples_compose_column_transformer_mixed_types.py`
+ * :ref:`sphx_glr_auto_examples_compose_plot_column_transformer.py`
+ * :ref:`sphx_glr_auto_examples_compose_plot_column_transformer_mixed_types.py`
diff --git a/doc/modules/preprocessing.rst b/doc/modules/preprocessing.rst
index 5d3979ead6..f70a05cf37 100755
--- a/doc/modules/preprocessing.rst
+++ b/doc/modules/preprocessing.rst
@@ -582,9 +582,9 @@ on a k-means clustering procedure performed on each feature independently.
 
 .. topic:: Examples:
 
-  * :ref:`sphx_glr_auto_examples_plot_discretization.py`
-  * :ref:`sphx_glr_auto_examples_plot_discretization_classification.py`
-  * :ref:`sphx_glr_auto_examples_plot_discretization_strategies.py`
+  * :ref:`sphx_glr_auto_examples_preprocessing_plot_discretization.py`
+  * :ref:`sphx_glr_auto_examples_preprocessing_plot_discretization_classification.py`
+  * :ref:`sphx_glr_auto_examples_preprocessing_plot_discretization_strategies.py`
 
 .. _preprocessing_binarization:
 
diff --git a/doc/whats_new/v0.20.rst b/doc/whats_new/v0.20.rst
index 0df0635d57..1c4e8a3597 100755
--- a/doc/whats_new/v0.20.rst
+++ b/doc/whats_new/v0.20.rst
@@ -137,7 +137,7 @@ Preprocessing
 - Added :class:`compose.ColumnTransformer`, which allows to apply
   different transformers to different columns of arrays or pandas
   DataFrames. :issue:`9012` by `Andreas MÃ¼ller`_ and `Joris Van den Bossche`_,
-  and :issue:`11315` by :user:`Thomas Fan <thomasjpfan>`_.
+  and :issue:`11315` by :user:`Thomas Fan <thomasjpfan>`.
 
 - Added :class:`preprocessing.PowerTransformer`, which implements the Box-Cox
   power transformation, allowing users to map data from any distribution to a
@@ -208,7 +208,7 @@ Misc
 
 - An environment variable to use the site joblib instead of the vendored
   one was added (:ref:`environment_variable`).
-  :issue:`11166`by `Gael Varoquaux`_
+  :issue:`11166` by `Gael Varoquaux`_
 
 Enhancements
 ............
diff --git a/sklearn/cluster/_k_means.pyx b/sklearn/cluster/_k_means.pyx
index e8800ee792..66fd620a90 100755
--- a/sklearn/cluster/_k_means.pyx
+++ b/sklearn/cluster/_k_means.pyx
@@ -20,9 +20,6 @@ from sklearn.utils.sparsefuncs_fast import assign_rows_csr
 ctypedef np.float64_t DOUBLE
 ctypedef np.int32_t INT
 
-ctypedef floating (*DOT)(int N, floating *X, int incX, floating *Y,
-                         int incY)
-
 cdef extern from "cblas.h":
     double ddot "cblas_ddot"(int N, double *X, int incX, double *Y, int incY)
     float sdot "cblas_sdot"(int N, float *X, int incX, float *Y, int incY)
@@ -58,7 +55,6 @@ cpdef DOUBLE _assign_labels_array(np.ndarray[floating, ndim=2] X,
         DOUBLE inertia = 0.0
         DOUBLE min_dist
         DOUBLE dist
-        DOT dot
 
     if floating is float:
         center_squared_norms = np.zeros(n_clusters, dtype=np.float32)
@@ -130,7 +126,6 @@ cpdef DOUBLE _assign_labels_csr(X, np.ndarray[floating, ndim=1] sample_weight,
         DOUBLE inertia = 0.0
         DOUBLE min_dist
         DOUBLE dist
-        DOT dot
 
     if floating is float:
         center_squared_norms = np.zeros(n_clusters, dtype=np.float32)
diff --git a/sklearn/cluster/affinity_propagation_.py b/sklearn/cluster/affinity_propagation_.py
index 307a4fde6f..2e646746ee 100755
--- a/sklearn/cluster/affinity_propagation_.py
+++ b/sklearn/cluster/affinity_propagation_.py
@@ -290,6 +290,24 @@ class AffinityPropagation(BaseEstimator, ClusterMixin):
     n_iter_ : int
         Number of iterations taken to converge.
 
+    Examples
+    --------
+    >>> from sklearn.cluster import AffinityPropagation
+    >>> import numpy as np
+    >>> X = np.array([[1, 2], [1, 4], [1, 0],
+    ...               [4, 2], [4, 4], [4, 0]])
+    >>> clustering = AffinityPropagation().fit(X)
+    >>> clustering # doctest: +NORMALIZE_WHITESPACE
+    AffinityPropagation(affinity='euclidean', convergence_iter=15, copy=True,
+              damping=0.5, max_iter=200, preference=None, verbose=False)
+    >>> clustering.labels_
+    array([0, 0, 0, 1, 1, 1])
+    >>> clustering.predict([[0, 0], [4, 4]])
+    array([0, 1])
+    >>> clustering.cluster_centers_
+    array([[1, 2],
+           [4, 2]])
+
     Notes
     -----
     For an example, see :ref:`examples/cluster/plot_affinity_propagation.py
diff --git a/sklearn/cluster/hierarchical.py b/sklearn/cluster/hierarchical.py
index f630fa990b..db1b2c36df 100755
--- a/sklearn/cluster/hierarchical.py
+++ b/sklearn/cluster/hierarchical.py
@@ -776,7 +776,8 @@ def fit(self, X, y=None):
         -------
         self
         """
-        if self.pooling_func != 'deprecated':
+        if (self.pooling_func != 'deprecated' and
+                not isinstance(self, AgglomerationTransform)):
             warnings.warn('Agglomerative "pooling_func" parameter is not used.'
                           ' It has been deprecated in version 0.20 and will be'
                           'removed in 0.22', DeprecationWarning)
diff --git a/sklearn/cluster/tests/test_feature_agglomeration.py b/sklearn/cluster/tests/test_feature_agglomeration.py
index 98d5dfc4b7..5c992109ff 100755
--- a/sklearn/cluster/tests/test_feature_agglomeration.py
+++ b/sklearn/cluster/tests/test_feature_agglomeration.py
@@ -4,7 +4,7 @@
 # Authors: Sergul Aydore 2017
 import numpy as np
 from sklearn.cluster import FeatureAgglomeration
-from sklearn.utils.testing import assert_true
+from sklearn.utils.testing import assert_true, assert_no_warnings
 from sklearn.utils.testing import assert_array_almost_equal
 
 
@@ -16,8 +16,8 @@ def test_feature_agglomeration():
                                       pooling_func=np.mean)
     agglo_median = FeatureAgglomeration(n_clusters=n_clusters,
                                         pooling_func=np.median)
-    agglo_mean.fit(X)
-    agglo_median.fit(X)
+    assert_no_warnings(agglo_mean.fit, X)
+    assert_no_warnings(agglo_median.fit, X)
     assert_true(np.size(np.unique(agglo_mean.labels_)) == n_clusters)
     assert_true(np.size(np.unique(agglo_median.labels_)) == n_clusters)
     assert_true(np.size(agglo_mean.labels_) == X.shape[1])
diff --git a/sklearn/datasets/descr/wine_data.rst b/sklearn/datasets/descr/wine_data.rst
index f43e652413..9d506b4ab7 100755
--- a/sklearn/datasets/descr/wine_data.rst
+++ b/sklearn/datasets/descr/wine_data.rst
@@ -21,6 +21,7 @@ Wine recognition dataset
  		- Hue
  		- OD280/OD315 of diluted wines
  		- Proline
+
     - class:
             - class_0
             - class_1
@@ -91,4 +92,4 @@ School of Information and Computer Science.
   "THE CLASSIFICATION PERFORMANCE OF RDA" 
   Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of 
   Mathematics and Statistics, James Cook University of North Queensland. 
-  (Also submitted to Journal of Chemometrics).
\ No newline at end of file
+  (Also submitted to Journal of Chemometrics).
diff --git a/sklearn/decomposition/pca.py b/sklearn/decomposition/pca.py
index a070f887d1..db183af45a 100755
--- a/sklearn/decomposition/pca.py
+++ b/sklearn/decomposition/pca.py
@@ -129,7 +129,7 @@ class PCA(_BasePCA):
 
             n_components == min(n_samples, n_features)
 
-        If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka\'s
+        If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's
         MLE is used to guess the dimension. Use of ``n_components == 'mle'``
         will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.
 
diff --git a/sklearn/discriminant_analysis.py b/sklearn/discriminant_analysis.py
index 96f3421456..1ed28c975a 100755
--- a/sklearn/discriminant_analysis.py
+++ b/sklearn/discriminant_analysis.py
@@ -622,9 +622,9 @@ def __init__(self, priors=None, reg_param=0., store_covariance=False,
         self.tol = tol
 
     @property
-    @deprecated("Attribute covariances_ was deprecated in version"
+    @deprecated("Attribute ``covariances_`` was deprecated in version"
                 " 0.19 and will be removed in 0.21. Use "
-                "covariance_ instead")
+                "``covariance_`` instead")
     def covariances_(self):
         return self.covariance_
 
diff --git a/sklearn/feature_extraction/text.py b/sklearn/feature_extraction/text.py
index df664d5381..b12b4ac7d6 100755
--- a/sklearn/feature_extraction/text.py
+++ b/sklearn/feature_extraction/text.py
@@ -481,42 +481,17 @@ class HashingVectorizer(BaseEstimator, VectorizerMixin, TransformerMixin):
 
     Examples
     --------
-    >>> from sklearn.feature_extraction.text import CountVectorizer
+    >>> from sklearn.feature_extraction.text import HashingVectorizer
     >>> corpus = [
     ...     'This is the first document.',
-    ...     'This is the second second document.',
-    ...     'And the third one.',
+    ...     'This document is the second document.',
+    ...     'And this is the third one.',
     ...     'Is this the first document?',
     ... ]
-    >>> vectorizer = HashingVectorizer()
-    >>> print(vectorizer._get_hasher())  # doctest: +NORMALIZE_WHITESPACE
-    FeatureHasher(alternate_sign=True, dtype=<class 'numpy.float64'>,
-           input_type='string', n_features=1048576, non_negative=False)
+    >>> vectorizer = HashingVectorizer(n_features=2**4)
     >>> X = vectorizer.fit_transform(corpus)
-    >>> print(X.toarray())  # doctest: +NORMALIZE_WHITESPACE
-    [[ 0.  0.  0. ...,  0.  0.  0.]
-     [ 0.  0.  0. ...,  0.  0.  0.]
-     [ 0.  0.  0. ...,  0.  0.  0.]
-     [ 0.  0.  0. ...,  0.  0.  0.]]
-    >>> print(X.shape)
-    (4, 1048576)
-    >>> vectorizer = HashingVectorizer(n_features=2**3)
-    >>> print(vectorizer._get_hasher())  # doctest: +NORMALIZE_WHITESPACE
-    FeatureHasher(alternate_sign=True, dtype=<class 'numpy.float64'>,
-           input_type='string', n_features=8, non_negative=False)
-    >>> X = vectorizer.fit_transform(corpus)
-    >>> print(X.toarray())  # doctest: +NORMALIZE_WHITESPACE
-    [[-0.89442719  0.          0.          0.          0.          0.4472136
-       0.          0.        ]
-     [-0.40824829  0.          0.          0.81649658  0.          0.40824829
-       0.          0.        ]
-     [ 0.          0.          0.          0.         -0.70710678  0.
-      -0.70710678  0.        ]
-     [-0.89442719  0.          0.          0.          0.          0.4472136
-       0.          0.        ]]
     >>> print(X.shape)
-    (4, 8)
-
+    (4, 16)
 
     See also
     --------
@@ -784,26 +759,21 @@ class CountVectorizer(BaseEstimator, VectorizerMixin):
 
         This is only available if no vocabulary was given.
 
+    Examples
+    --------
     >>> from sklearn.feature_extraction.text import TfidfVectorizer
     >>> corpus = [
     ...     'This is the first document.',
-    ...     'This is the second second document.',
-    ...     'And the third one.',
+    ...     'This document is the second document.',
+    ...     'And this is the third one.',
     ...     'Is this the first document?',
     ... ]
     >>> vectorizer = TfidfVectorizer()
     >>> X = vectorizer.fit_transform(corpus)
     >>> print(vectorizer.get_feature_names())
     ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']
-    >>> print(X.toarray())  # doctest: +NORMALIZE_WHITESPACE
-    [[ 0.          0.43877674  0.54197657  0.43877674  0.          0.
-       0.35872874  0.          0.43877674]
-     [ 0.          0.27230147  0.          0.27230147  0.          0.85322574
-       0.22262429  0.          0.27230147]
-     [ 0.55280532  0.          0.          0.          0.55280532  0.
-       0.28847675  0.55280532  0.        ]
-     [ 0.          0.43877674  0.54197657  0.43877674  0.          0.
-       0.35872874  0.          0.43877674]]
+    >>> print(X.shape)
+    (4, 9)
 
     See also
     --------
@@ -1441,8 +1411,8 @@ class TfidfVectorizer(CountVectorizer):
     >>> from sklearn.feature_extraction.text import CountVectorizer
     >>> corpus = [
     ...     'This is the first document.',
-    ...     'This is the second second document.',
-    ...     'And the third one.',
+    ...     'This document is the second document.',
+    ...     'And this is the third one.',
     ...     'Is this the first document?',
     ... ]
     >>> vectorizer = CountVectorizer()
@@ -1451,8 +1421,8 @@ class TfidfVectorizer(CountVectorizer):
     ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']
     >>> print(X.toarray())  # doctest: +NORMALIZE_WHITESPACE
     [[0 1 1 1 0 0 1 0 1]
-     [0 1 0 1 0 2 1 0 1]
-     [1 0 0 0 1 0 1 1 0]
+     [0 2 0 1 0 1 1 0 1]
+     [1 0 0 1 1 0 1 1 1]
      [0 1 1 1 0 0 1 0 1]]
 
     See also
diff --git a/sklearn/kernel_approximation.py b/sklearn/kernel_approximation.py
index a0720a8568..4044a8f6a9 100755
--- a/sklearn/kernel_approximation.py
+++ b/sklearn/kernel_approximation.py
@@ -459,6 +459,25 @@ class Nystroem(BaseEstimator, TransformerMixin):
         Normalization matrix needed for embedding.
         Square root of the kernel matrix on ``components_``.
 
+    Examples
+    --------
+    >>> from sklearn import datasets, svm
+    >>> from sklearn.kernel_approximation import Nystroem
+    >>> digits = datasets.load_digits(n_class=9)
+    >>> data = digits.data / 16.
+    >>> clf = svm.LinearSVC()
+    >>> feature_map_nystroem = Nystroem(gamma=.2,
+    ...                                 random_state=1,
+    ...                                 n_components=300)
+    >>> data_transformed = feature_map_nystroem.fit_transform(data)
+    >>> clf.fit(data_transformed, digits.target)
+    ... # doctest: +NORMALIZE_WHITESPACE
+    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
+         intercept_scaling=1, loss='squared_hinge', max_iter=1000,
+         multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
+         verbose=0)
+    >>> clf.score(data_transformed, digits.target) # doctest: +ELLIPSIS
+    0.9987...
 
     References
     ----------
diff --git a/sklearn/linear_model/cd_fast.pyx b/sklearn/linear_model/cd_fast.pyx
index 4fbfe8b248..a51d1bdbdb 100755
--- a/sklearn/linear_model/cd_fast.pyx
+++ b/sklearn/linear_model/cd_fast.pyx
@@ -18,11 +18,6 @@ import warnings
 
 ctypedef np.float64_t DOUBLE
 ctypedef np.uint32_t UINT32_t
-ctypedef floating (*DOT)(int N, floating *X, int incX, floating *Y,
-                         int incY) nogil
-ctypedef void (*AXPY)(int N, floating alpha, floating *X, int incX,
-                      floating *Y, int incY) nogil
-ctypedef floating (*ASUM)(int N, floating *X, int incX) nogil
 
 np.import_array()
 
@@ -162,10 +157,6 @@ def enet_coordinate_descent(np.ndarray[floating, ndim=1] w,
     """
 
     # fused types version of BLAS functions
-    cdef DOT dot
-    cdef AXPY axpy
-    cdef ASUM asum
-
     if floating is float:
         dtype = np.float32
         dot = sdot
@@ -355,9 +346,6 @@ def sparse_enet_coordinate_descent(floating [:] w,
     cdef floating[:] XtA
 
     # fused types version of BLAS functions
-    cdef DOT dot
-    cdef ASUM asum
-
     if floating is float:
         dtype = np.float32
         n_tasks = y.strides[0] / sizeof(float)
@@ -559,10 +547,6 @@ def enet_coordinate_descent_gram(floating[:] w, floating alpha, floating beta,
     """
 
     # fused types version of BLAS functions
-    cdef DOT dot
-    cdef AXPY axpy
-    cdef ASUM asum
-
     if floating is float:
         dtype = np.float32
         dot = sdot
@@ -713,10 +697,6 @@ def enet_coordinate_descent_multi_task(floating[::1, :] W, floating l1_reg,
 
     """
     # fused types version of BLAS functions
-    cdef DOT dot
-    cdef AXPY axpy
-    cdef ASUM asum
-
     if floating is float:
         dtype = np.float32
         dot = sdot
diff --git a/sklearn/preprocessing/_discretization.py b/sklearn/preprocessing/_discretization.py
index d51a2b96c4..1cbe0d09b7 100755
--- a/sklearn/preprocessing/_discretization.py
+++ b/sklearn/preprocessing/_discretization.py
@@ -67,7 +67,7 @@ class KBinsDiscretizer(BaseEstimator, TransformerMixin):
         will have ``n_bins_[i] == 0``.
 
     bin_edges_ : array of arrays, shape (n_features, )
-        The edges of each bin. Contain arrays of varying shapes (n_bins_, ).
+        The edges of each bin. Contain arrays of varying shapes ``(n_bins_, )``
         Ignored features will have empty arrays.
 
     Examples
diff --git a/sklearn/preprocessing/_encoders.py b/sklearn/preprocessing/_encoders.py
index f3bd4b9ffb..4bd11012b0 100755
--- a/sklearn/preprocessing/_encoders.py
+++ b/sklearn/preprocessing/_encoders.py
@@ -198,24 +198,24 @@ class OneHotEncoder(_BaseEncoder):
         in the training set. Only available when n_values is ``'auto'``.
 
         .. deprecated:: 0.20
-            The `active_features_` attribute was deprecated in version
+            The ``active_features_`` attribute was deprecated in version
             0.20 and will be removed in 0.22.
 
     feature_indices_ : array of shape (n_features,)
         Indices to feature ranges.
         Feature ``i`` in the original data is mapped to features
         from ``feature_indices_[i]`` to ``feature_indices_[i+1]``
-        (and then potentially masked by `active_features_` afterwards)
+        (and then potentially masked by ``active_features_`` afterwards)
 
         .. deprecated:: 0.20
-            The `feature_indices_` attribute was deprecated in version
+            The ``feature_indices_`` attribute was deprecated in version
             0.20 and will be removed in 0.22.
 
     n_values_ : array of shape (n_features,)
         Maximum number of values per feature.
 
         .. deprecated:: 0.20
-            The `n_values_` attribute was deprecated in version
+            The ``n_values_`` attribute was deprecated in version
             0.20 and will be removed in 0.22.
 
     Examples
@@ -269,21 +269,21 @@ def __init__(self, n_values=None, categorical_features=None,
     # Deprecated attributes
 
     @property
-    @deprecated("The 'active_features_' attribute was deprecated in version "
+    @deprecated("The ``active_features_`` attribute was deprecated in version "
                 "0.20 and will be removed 0.22.")
     def active_features_(self):
         check_is_fitted(self, 'categories_')
         return self._active_features_
 
     @property
-    @deprecated("The 'feature_indices_' attribute was deprecated in version "
+    @deprecated("The ``feature_indices_`` attribute was deprecated in version "
                 "0.20 and will be removed 0.22.")
     def feature_indices_(self):
         check_is_fitted(self, 'categories_')
         return self._feature_indices_
 
     @property
-    @deprecated("The 'n_values_' attribute was deprecated in version "
+    @deprecated("The ``n_values_`` attribute was deprecated in version "
                 "0.20 and will be removed 0.22.")
     def n_values_(self):
         check_is_fitted(self, 'categories_')
diff --git a/sklearn/tests/test_discriminant_analysis.py b/sklearn/tests/test_discriminant_analysis.py
index 8eb5da1908..6e509949b0 100755
--- a/sklearn/tests/test_discriminant_analysis.py
+++ b/sklearn/tests/test_discriminant_analysis.py
@@ -324,9 +324,9 @@ def test_qda_deprecation():
                          "removed in 0.21.", clf.fit, X, y)
 
     # check that covariance_ (and covariances_ with warning) is stored
-    assert_warns_message(DeprecationWarning, "Attribute covariances_ was "
+    assert_warns_message(DeprecationWarning, "Attribute ``covariances_`` was "
                          "deprecated in version 0.19 and will be removed "
-                         "in 0.21. Use covariance_ instead", getattr, clf,
+                         "in 0.21. Use ``covariance_`` instead", getattr, clf,
                          'covariances_')
 
 
