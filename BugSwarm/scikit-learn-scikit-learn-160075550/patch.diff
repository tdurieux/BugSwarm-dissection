diff --git a/benchmarks/bench_isotonic.py b/benchmarks/bench_isotonic.py
index e201d89800..0a4fb6de44 100755
--- a/benchmarks/bench_isotonic.py
+++ b/benchmarks/bench_isotonic.py
@@ -22,8 +22,8 @@
 
 
 def generate_perturbed_logarithm_dataset(size):
-    return np.random.randint(-50, 50, size=n) \
-        + 50. * np.log(1 + np.arange(n))
+    return (np.random.randint(-50, 50, size=size) +
+            50. * np.log(1 + np.arange(size)))
 
 
 def generate_logistic_dataset(size):
@@ -31,9 +31,17 @@ def generate_logistic_dataset(size):
     return np.random.random(size=size) < 1.0 / (1.0 + np.exp(-X))
 
 
+def generate_pathological_dataset(size):
+    # Triggers O(n^2) complexity on the original implementation.
+    return np.r_[np.arange(size),
+                 np.arange(-(size - 1), size),
+                 np.arange(-(size - 1), 1)]
+
+
 DATASET_GENERATORS = {
     'perturbed_logarithm': generate_perturbed_logarithm_dataset,
-    'logistic': generate_logistic_dataset
+    'logistic': generate_logistic_dataset,
+    'pathological': generate_pathological_dataset,
 }
 
 
@@ -53,6 +61,8 @@ def bench_isotonic_regression(Y):
 if __name__ == '__main__':
     parser = argparse.ArgumentParser(
         description="Isotonic Regression benchmark tool")
+    parser.add_argument('--seed', type=int,
+                        help="RNG seed")
     parser.add_argument('--iterations', type=int, required=True,
                         help="Number of iterations to average timings over "
                         "for each problem size")
@@ -67,6 +77,8 @@ def bench_isotonic_regression(Y):
 
     args = parser.parse_args()
 
+    np.random.seed(args.seed)
+
     timings = []
     for exponent in range(args.log_min_problem_size,
                           args.log_max_problem_size):
diff --git a/build_tools/travis/flake8_diff.sh b/build_tools/travis/flake8_diff.sh
index 9f639c5734..20fb3d0a1b 100755
--- a/build_tools/travis/flake8_diff.sh
+++ b/build_tools/travis/flake8_diff.sh
@@ -38,10 +38,20 @@ fi
 if [[ "$TRAVIS" == "true" ]]; then
     if [[ "$TRAVIS_PULL_REQUEST" == "false" ]]
     then
-        # Travis does the git clone with a limited depth (50 at the time of
-        # writing). This may not be enough to find the common ancestor with
-        # $REMOTE/master so we unshallow the git checkout
-        git fetch --unshallow || echo "Unshallowing the git checkout failed"
+        # In main repo, using TRAVIS_COMMIT_RANGE to test the commits
+        # that were pushed into a branch
+        if [[ "$PROJECT" == "$TRAVIS_REPO_SLUG" ]]; then
+            if [[ -z "$TRAVIS_COMMIT_RANGE" ]]; then
+                echo "New branch, no commit range from Travis so passing this test by convention"
+                exit 0
+            fi
+            COMMIT_RANGE=$TRAVIS_COMMIT_RANGE
+        else
+            # Travis does the git clone with a limited depth (50 at the time of
+            # writing). This may not be enough to find the common ancestor with
+            # $REMOTE/master so we unshallow the git checkout
+            git fetch --unshallow || echo "Unshallowing the git checkout failed"
+        fi
     else
         # We want to fetch the code as it is in the PR branch and not
         # the result of the merge into master. This way line numbers
@@ -57,28 +67,36 @@ echo -e '\nLast 2 commits:'
 echo '--------------------------------------------------------------------------------'
 git log -2 --pretty=short
 
-git fetch $REMOTE master
-REMOTE_MASTER_REF="$REMOTE/master"
+# If not using the commit range from Travis we need to find the common
+# ancestor between HEAD and $REMOTE/master
+if [[ -z "$COMMIT_RANGE" ]]; then
+    REMOTE_MASTER_REF="$REMOTE/master"
+    # Make sure that $REMOTE_MASTER_REF is a valid reference
+    git fetch $REMOTE master:refs/$REMOTE_MASTER_REF
 
-# Find common ancestor between HEAD and remotes/$REMOTE/master
-COMMIT=$(git merge-base @ $REMOTE_MASTER_REF) || \
-    echo "No common ancestor found for $(git show @ -q) and $(git show $REMOTE_MASTER_REF -q)"
+    COMMIT=$(git merge-base @ $REMOTE_MASTER_REF) || \
+        echo "No common ancestor found for $(git show @ -q) and $(git show $REMOTE_MASTER_REF -q)"
 
-if [[ -n "$TMP_REMOTE" ]]; then
-    git remote remove $TMP_REMOTE
-fi
+    if [[ -n "$TMP_REMOTE" ]]; then
+        git remote remove $TMP_REMOTE
+    fi
 
-if [ -z "$COMMIT" ]; then
-    exit 1
-fi
+    if [ -z "$COMMIT" ]; then
+        exit 1
+    fi
 
-echo -e "\nCommon ancestor between HEAD and $REMOTE_MASTER_REF is:"
-echo '--------------------------------------------------------------------------------'
-git show --no-patch $COMMIT
+    echo -e "\nCommon ancestor between HEAD and $REMOTE_MASTER_REF is:"
+    echo '--------------------------------------------------------------------------------'
+    git show --no-patch $COMMIT
+
+    COMMIT_RANGE="$(git rev-parse --short $COMMIT)..$(git rev-parse --short @)"
+
+else
+    echo "Got the commit range from Travis: $COMMIT_RANGE"
+fi
 
-echo -e '\nRunning flake8 on the diff in the range'\
-     "$(git rev-parse --short $COMMIT)..$(git rev-parse --short @)" \
-     "($(git rev-list $COMMIT.. | wc -l) commit(s)):"
+echo -e '\nRunning flake8 on the diff in the range' "$COMMIT_RANGE" \
+     "($(git rev-list $COMMIT_RANGE | wc -l) commit(s)):"
 echo '--------------------------------------------------------------------------------'
 
 # We ignore files from sklearn/externals. Unfortunately there is no
@@ -88,10 +106,11 @@ echo '--------------------------------------------------------------------------
 # uses git 1.8.
 # We need the following command to exit with 0 hence the echo in case
 # there is no match
-MODIFIED_FILES=$(git diff --name-only $COMMIT | grep -v 'sklearn/externals' || echo "no_match")
+MODIFIED_FILES=$(git diff --name-only $COMMIT_RANGE | grep -v 'sklearn/externals' | \
+                     grep -v 'doc/sphinxext/sphinx_gallery' || echo "no_match")
 
 if [[ "$MODIFIED_FILES" == "no_match" ]]; then
-    echo "No file outside sklearn/externals has been modified"
+    echo "No file outside sklearn/externals and doc/sphinxext/sphinx_gallery has been modified"
 else
     # Conservative approach: diff without context so that code that
     # was not changed does not create failures
diff --git a/doc/modules/clustering.rst b/doc/modules/clustering.rst
index 27a3178d58..9f67b38856 100755
--- a/doc/modules/clustering.rst
+++ b/doc/modules/clustering.rst
@@ -718,7 +718,7 @@ More formally, we define a core sample as being a sample in the dataset such
 that there exist ``min_samples`` other samples within a distance of
 ``eps``, which are defined as *neighbors* of the core sample. This tells
 us that the core sample is in a dense area of the vector space. A cluster
-is a set of core samples, that can be built by recursively by taking a core
+is a set of core samples that can be built by recursively taking a core
 sample, finding all of its neighbors that are core samples, finding all of
 *their* neighbors that are core samples, and so on. A cluster also has a
 set of non-core samples, which are samples that are neighbors of a core sample
diff --git a/doc/sphinxext/sphinx_gallery/__init__.py b/doc/sphinxext/sphinx_gallery/__init__.py
index c5ff4151ee..94a22d34ce 100755
--- a/doc/sphinxext/sphinx_gallery/__init__.py
+++ b/doc/sphinxext/sphinx_gallery/__init__.py
@@ -5,7 +5,7 @@
 
 """
 import os
-__version__ = '0.1.2'
+__version__ = '0.1.4'
 
 
 def glr_path_static():
diff --git a/doc/sphinxext/sphinx_gallery/_static/gallery.css b/doc/sphinxext/sphinx_gallery/_static/gallery.css
index dd599b402d..ac10f8bd69 100755
--- a/doc/sphinxext/sphinx_gallery/_static/gallery.css
+++ b/doc/sphinxext/sphinx_gallery/_static/gallery.css
@@ -1,8 +1,8 @@
 /*
-Sphinx-Gallery is has compatible CSS to fix default sphinx themes
+Sphinx-Gallery has compatible CSS to fix default sphinx themes
 Tested for Sphinx 1.3.1 for all themes: default, alabaster, sphinxdoc,
 scrolls, agogo, traditional, nature, haiku, pyramid
-Tested for Read the docs theme 0.1.7 */
+Tested for Read the Docs theme 0.1.7 */
 .sphx-glr-thumbcontainer {
   background: #fff;
   border: solid #fff 1px;
@@ -103,16 +103,33 @@ thumbnail with its default link Background color */
 blockquote.sphx-glr-script-out {
   margin-left: 0pt;
 }
-.sphx-glr-download {
+div.sphx-glr-download {
+  display: inline-block;
+  margin: 1em auto 1ex 2ex;
+}
+
+div.sphx-glr-download a {
   background-color: #ffc;
-  border: 1px solid #c2c22d;
+  background-image: linear-gradient(to bottom, #FFC, #d5d57e);
   border-radius: 4px;
-  margin: 1em auto 1ex auto;
+  border: 1px solid #c2c22d;
+  color: #000;
+  display: inline-block;
+  font-weight: bold;
   max-width: 45ex;
   padding: 1ex;
+  text-align: center;
+}
+
+div.sphx-glr-download code.download {
+  display: inline-block;
 }
-.sphx-glr-download a {
-  color: #4b4600;
+
+div.sphx-glr-download a:hover {
+  box-shadow: inset 0 1px 0 rgba(255,255,255,.1), 0 1px 5px rgba(0,0,0,.25);
+  text-decoration: none;
+  background-image: none;
+  background-color: #d5d57e;
 }
 
 ul.sphx-glr-horizontal {
@@ -125,3 +142,12 @@ ul.sphx-glr-horizontal li {
 ul.sphx-glr-horizontal img {
   height: auto !important;
 }
+
+p.sphx-glr-signature a.reference.external {
+  background-color: #EBECED;
+  -moz-border-radius: 5px;
+  -webkit-border-radius: 5px;
+  border-radius: 5px;
+  padding: 3px;
+  font-size: 75%;
+}
diff --git a/doc/sphinxext/sphinx_gallery/backreferences.py b/doc/sphinxext/sphinx_gallery/backreferences.py
index 67aaca966a..4fe579c6ad 100755
--- a/doc/sphinxext/sphinx_gallery/backreferences.py
+++ b/doc/sphinxext/sphinx_gallery/backreferences.py
@@ -75,7 +75,7 @@ def get_short_module_name(module_name, obj_name):
         short_name = '.'.join(parts[:i])
         try:
             exec('from %s import %s' % (short_name, obj_name))
-        except ImportError:
+        except Exception:  # libraries can throw all sorts of exceptions...
             # get the last working module name
             short_name = '.'.join(parts[:(i + 1)])
             break
diff --git a/doc/sphinxext/sphinx_gallery/docs_resolv.py b/doc/sphinxext/sphinx_gallery/docs_resolv.py
index fb596fdb1f..0675a131d5 100755
--- a/doc/sphinxext/sphinx_gallery/docs_resolv.py
+++ b/doc/sphinxext/sphinx_gallery/docs_resolv.py
@@ -13,18 +13,18 @@
 
 # Try Python 2 first, otherwise load from Python 3
 try:
-    from StringIO import StringIO
     import cPickle as pickle
     import urllib2 as urllib
     from urllib2 import HTTPError, URLError
 except ImportError:
-    from io import StringIO
     import pickle
     import urllib.request
     import urllib.error
     import urllib.parse
     from urllib.error import HTTPError, URLError
 
+from io import StringIO
+
 
 def _get_data(url):
     """Helper function to get data over http or from a local file"""
diff --git a/doc/sphinxext/sphinx_gallery/downloads.py b/doc/sphinxext/sphinx_gallery/downloads.py
new file mode 100755
index 0000000000..d67ad54e0d
--- /dev/null
+++ b/doc/sphinxext/sphinx_gallery/downloads.py
@@ -0,0 +1,113 @@
+# -*- coding: utf-8 -*-
+r"""
+Utilities for downloadable items
+================================
+
+"""
+# Author: Óscar Nájera
+# License: 3-clause BSD
+
+from __future__ import absolute_import, division, print_function
+
+import os
+import zipfile
+
+CODE_DOWNLOAD = """
+\n.. container:: sphx-glr-download
+
+    :download:`Download Python source code: {0} <{0}>`\n
+
+\n.. container:: sphx-glr-download
+
+    :download:`Download Jupyter notebook: {1} <{1}>`\n"""
+
+CODE_ZIP_DOWNLOAD = """
+\n.. container:: sphx-glr-download
+
+    :download:`Download all examples in Python source code: {0} </{1}>`\n
+
+\n.. container:: sphx-glr-download
+
+    :download:`Download all examples in Jupyter notebook files: {2} </{3}>`\n"""
+
+
+def python_zip(file_list, gallery_path, extension='.py'):
+    """Stores all files in file_list into an zip file
+
+    Parameters
+    ----------
+    file_list : list of strings
+        Holds all the file names to be included in zip file
+    gallery_path : string
+        path to where the zipfile is stored
+    extension : str
+        '.py' or '.ipynb' In order to deal with downloads of python
+        sources and jupyter notebooks the file extension from files in
+        file_list will be removed and replace with the value of this
+        variable while generating the zip file
+    Returns
+    -------
+    zipname : string
+        zip file name, written as `target_dir_{python,jupyter}.zip`
+        depending on the extension
+    """
+    zipname = gallery_path.replace(os.path.sep, '_')
+    zipname += '_python' if extension == '.py' else '_jupyter'
+    zipname = os.path.join(gallery_path, zipname + '.zip')
+
+    zipf = zipfile.ZipFile(zipname, mode='w')
+    for fname in file_list:
+        file_src = os.path.splitext(fname)[0] + extension
+        zipf.write(file_src)
+    zipf.close()
+
+    return zipname
+
+
+def list_downloadable_sources(target_dir):
+    """Returns a list of python source files is target_dir
+
+    Parameters
+    ----------
+    target_dir : string
+        path to the directory where python source file are
+    Returns
+    -------
+    list
+        list of paths to all Python source files in `target_dir`
+    """
+    return [os.path.join(target_dir, fname)
+            for fname in os.listdir(target_dir)
+            if fname.endswith('.py')]
+
+
+def generate_zipfiles(gallery_dir):
+    """
+    Collects all Python source files and Jupyter notebooks in
+    gallery_dir and makes zipfiles of them
+
+    Parameters
+    ----------
+    gallery_dir : string
+        path of the gallery to collect downloadable sources
+
+    Return
+    ------
+    download_rst: string
+        RestructuredText to include download buttons to the generated files
+    """
+
+    listdir = list_downloadable_sources(gallery_dir)
+    for directory in sorted(os.listdir(gallery_dir)):
+        if os.path.isdir(os.path.join(gallery_dir, directory)):
+            target_dir = os.path.join(gallery_dir, directory)
+            listdir.extend(list_downloadable_sources(target_dir))
+
+    py_zipfile = python_zip(listdir, gallery_dir)
+    jy_zipfile = python_zip(listdir, gallery_dir, ".ipynb")
+
+    dw_rst = CODE_ZIP_DOWNLOAD.format(os.path.basename(py_zipfile),
+                                      py_zipfile,
+                                      os.path.basename(jy_zipfile),
+                                      jy_zipfile)
+    return dw_rst
diff --git a/doc/sphinxext/sphinx_gallery/gen_gallery.py b/doc/sphinxext/sphinx_gallery/gen_gallery.py
index 5baf6f6301..352a718797 100755
--- a/doc/sphinxext/sphinx_gallery/gen_gallery.py
+++ b/doc/sphinxext/sphinx_gallery/gen_gallery.py
@@ -12,11 +12,28 @@
 
 
 from __future__ import division, print_function, absolute_import
+import copy
 import re
 import os
 from . import glr_path_static
-from .gen_rst import generate_dir_rst
+from .gen_rst import generate_dir_rst, SPHX_GLR_SIG
 from .docs_resolv import embed_code_links
+from .downloads import generate_zipfiles
+
+DEFAULT_GALLERY_CONF = {
+    'filename_pattern': re.escape(os.sep) + 'plot',
+    'examples_dirs': os.path.join('..', 'examples'),
+    'gallery_dirs': 'auto_examples',
+    'mod_example_dir': os.path.join('modules', 'generated'),
+    'doc_module': (),
+    'reference_url': {},
+    # build options
+    'plot_gallery': True,
+    'download_all_examples': True,
+    'abort_on_example_error': False,
+    'failing_examples': {},
+    'expected_failing_examples': set(),
+}
 
 
 def clean_gallery_out(build_dir):
@@ -55,9 +72,11 @@ def generate_gallery_rst(app):
     except TypeError:
         plot_gallery = bool(app.builder.config.plot_gallery)
 
+    gallery_conf = copy.deepcopy(DEFAULT_GALLERY_CONF)
     gallery_conf.update(app.config.sphinx_gallery_conf)
     gallery_conf.update(plot_gallery=plot_gallery)
-    gallery_conf.update(abort_on_example_error=app.builder.config.abort_on_example_error)
+    gallery_conf.update(
+        abort_on_example_error=app.builder.config.abort_on_example_error)
 
     # this assures I can call the config in other places
     app.config.sphinx_gallery_conf = gallery_conf
@@ -77,6 +96,11 @@ def generate_gallery_rst(app):
                                        app.builder.srcdir)
     seen_backrefs = set()
 
+    computation_times = []
+
+    # cd to the appropriate directory regardless of sphinx configuration
+    working_dir = os.getcwd()
+    os.chdir(app.builder.srcdir)
     for examples_dir, gallery_dir in zip(examples_dirs, gallery_dirs):
         examples_dir = os.path.relpath(examples_dir,
                                        app.builder.srcdir)
@@ -86,22 +110,44 @@ def generate_gallery_rst(app):
         for workdir in [examples_dir, gallery_dir, mod_examples_dir]:
             if not os.path.exists(workdir):
                 os.makedirs(workdir)
-
         # we create an index.rst with all examples
         fhindex = open(os.path.join(gallery_dir, 'index.rst'), 'w')
         # Here we don't use an os.walk, but we recurse only twice: flat is
         # better than nested.
-        fhindex.write(generate_dir_rst(examples_dir, gallery_dir, gallery_conf,
-                                       seen_backrefs))
+        this_fhindex, this_computation_times = \
+            generate_dir_rst(examples_dir, gallery_dir, gallery_conf,
+                             seen_backrefs)
+
+        computation_times += this_computation_times
+
+        fhindex.write(this_fhindex)
         for directory in sorted(os.listdir(examples_dir)):
             if os.path.isdir(os.path.join(examples_dir, directory)):
                 src_dir = os.path.join(examples_dir, directory)
                 target_dir = os.path.join(gallery_dir, directory)
-                fhindex.write(generate_dir_rst(src_dir, target_dir,
-                                               gallery_conf,
-                                               seen_backrefs))
+                this_fhindex, this_computation_times = \
+                    generate_dir_rst(src_dir, target_dir, gallery_conf,
+                                     seen_backrefs)
+                fhindex.write(this_fhindex)
+                computation_times += this_computation_times
+
+        if gallery_conf['download_all_examples']:
+            download_fhindex = generate_zipfiles(gallery_dir)
+            fhindex.write(download_fhindex)
+
+        fhindex.write(SPHX_GLR_SIG)
         fhindex.flush()
 
+    # Back to initial directory
+    os.chdir(working_dir)
+
+    print("Computation time summary:")
+    for time_elapsed, fname in sorted(computation_times)[::-1]:
+        if time_elapsed is not None:
+            print("\t- %s : %.2g sec" % (fname, time_elapsed))
+        else:
+            print("\t- %s : not run" % fname)
+
 
 def touch_empty_backreferences(app, what, name, obj, options, lines):
     """Generate empty back-reference example files
@@ -110,7 +156,8 @@ def touch_empty_backreferences(app, what, name, obj, options, lines):
     examples for a class / module that is being parsed by autodoc"""
 
     examples_path = os.path.join(app.srcdir,
-                                 app.config.sphinx_gallery_conf["mod_example_dir"],
+                                 app.config.sphinx_gallery_conf[
+                                     "mod_example_dir"],
                                  "%s.examples" % name)
 
     if not os.path.exists(examples_path):
@@ -118,21 +165,68 @@ def touch_empty_backreferences(app, what, name, obj, options, lines):
         open(examples_path, 'w').close()
 
 
-gallery_conf = {
-    'filename_pattern': re.escape(os.sep) + 'plot',
-    'examples_dirs': '../examples',
-    'gallery_dirs': 'auto_examples',
-    'mod_example_dir': os.path.join('modules', 'generated'),
-    'doc_module': (),
-    'reference_url': {},
-}
+def sumarize_failing_examples(app, exception):
+    """Collects the list of falling examples during build and prints them with the traceback
+
+    Raises ValueError if there where failing examples
+    """
+    if exception is not None:
+        return
+
+    # Under no-plot Examples are not run so nothing to summarize
+    if not app.config.sphinx_gallery_conf['plot_gallery']:
+        return
+
+    gallery_conf = app.config.sphinx_gallery_conf
+    failing_examples = set(gallery_conf['failing_examples'])
+    expected_failing_examples = set(gallery_conf['expected_failing_examples'])
+
+    examples_expected_to_fail = failing_examples.intersection(
+        expected_failing_examples)
+    expected_fail_msg = []
+    if examples_expected_to_fail:
+        expected_fail_msg.append("Examples failing as expected:")
+        for fail_example in examples_expected_to_fail:
+            expected_fail_msg.append(fail_example + ' failed leaving traceback:\n' +
+                                     gallery_conf['failing_examples'][fail_example] + '\n')
+        print("\n".join(expected_fail_msg))
+
+    examples_not_expected_to_fail = failing_examples.difference(
+        expected_failing_examples)
+    fail_msgs = []
+    if examples_not_expected_to_fail:
+        fail_msgs.append("Unexpected failing examples:")
+        for fail_example in examples_not_expected_to_fail:
+            fail_msgs.append(fail_example + ' failed leaving traceback:\n' +
+                             gallery_conf['failing_examples'][fail_example] + '\n')
+
+    examples_not_expected_to_pass = expected_failing_examples.difference(
+        failing_examples)
+    if examples_not_expected_to_pass:
+        fail_msgs.append("Examples expected to fail, but not failling:\n" +
+                         "Please remove this examples from\n" +
+                         "sphinx_gallery_conf['expected_failing_examples']\n" +
+                         "in your conf.py file"
+                         "\n".join(examples_not_expected_to_pass))
+
+    if fail_msgs:
+        raise ValueError("Here is a summary of the problems encountered when "
+                         "running the examples\n\n" + "\n".join(fail_msgs) +
+                         "\n" + "-" * 79)
+
+
+def get_default_config_value(key):
+    def default_getter(conf):
+        return conf['sphinx_gallery_conf'].get(key, DEFAULT_GALLERY_CONF[key])
+    return default_getter
 
 
 def setup(app):
     """Setup sphinx-gallery sphinx extension"""
-    app.add_config_value('plot_gallery', True, 'html')
-    app.add_config_value('abort_on_example_error', False, 'html')
-    app.add_config_value('sphinx_gallery_conf', gallery_conf, 'html')
+    app.add_config_value('sphinx_gallery_conf', DEFAULT_GALLERY_CONF, 'html')
+    for key in ['plot_gallery', 'abort_on_example_error']:
+        app.add_config_value(key, get_default_config_value(key), 'html')
+
     app.add_stylesheet('gallery.css')
 
     if 'sphinx.ext.autodoc' in app._extensions:
@@ -140,6 +234,7 @@ def setup(app):
 
     app.connect('builder-inited', generate_gallery_rst)
 
+    app.connect('build-finished', sumarize_failing_examples)
     app.connect('build-finished', embed_code_links)
 
 
diff --git a/doc/sphinxext/sphinx_gallery/gen_rst.py b/doc/sphinxext/sphinx_gallery/gen_rst.py
index cb02c55094..329f4623b9 100755
--- a/doc/sphinxext/sphinx_gallery/gen_rst.py
+++ b/doc/sphinxext/sphinx_gallery/gen_rst.py
@@ -12,9 +12,12 @@
 Files that generate images should start with 'plot'
 
 """
+# Don't use unicode_literals here (be explicit with u"..." instead) otherwise
+# tricky errors come up with exec(code_blocks, ...) calls
 from __future__ import division, print_function, absolute_import
 from time import time
 import ast
+import codecs
 import hashlib
 import os
 import re
@@ -24,6 +27,8 @@
 import traceback
 import warnings
 
+from .downloads import CODE_DOWNLOAD
+
 
 # Try Python 2 first, otherwise load from Python 3
 from textwrap import dedent
@@ -48,10 +53,7 @@ def prefixed_lines():
                 yield (prefix + line if predicate(line) else line)
         return ''.join(prefixed_lines())
 
-try:
-    from StringIO import StringIO
-except ImportError:
-    from io import StringIO
+from io import StringIO
 
 try:
     # make sure that the Agg backend is set before importing any
@@ -72,6 +74,7 @@ def prefixed_lines():
     basestring
 except NameError:
     basestring = str
+    unicode = str
 
 
 ###############################################################################
@@ -92,17 +95,21 @@ def flush(self):
         self.file1.flush()
         self.file2.flush()
 
+    # When called from a local terminal seaborn needs it in Python3
+    def isatty(self):
+        self.file1.isatty()
 
-###############################################################################
-CODE_DOWNLOAD = """**Total running time of the script:**
-({0:.0f} minutes {1:.3f} seconds)\n\n
-\n.. container:: sphx-glr-download
 
-    **Download Python source code:** :download:`{2} <{2}>`\n
-\n.. container:: sphx-glr-download
+class MixedEncodingStringIO(StringIO):
+    """Helper when both ASCII and unicode strings will be written"""
+
+    def write(self, data):
+        if not isinstance(data, unicode):
+            data = data.decode('utf-8')
+        StringIO.write(self, data)
 
-    **Download IPython notebook:** :download:`{3} <{3}>`\n"""
 
+###############################################################################
 # The following strings are used when we have several pictures: we use
 # an html div tag that our CSS uses to turn the lists into horizontal
 # lists.
@@ -124,11 +131,17 @@ def flush(self):
 """
 
 
-CODE_OUTPUT = """.. rst-class:: sphx-glr-script-out
+# This one could contain unicode
+CODE_OUTPUT = u""".. rst-class:: sphx-glr-script-out
 
  Out::
 
-  {0}\n"""
+{0}\n"""
+
+
+SPHX_GLR_SIG = """\n.. rst-class:: sphx-glr-signature
+
+    `Generated by Sphinx-Gallery <http://sphinx-gallery.readthedocs.io>`_\n"""
 
 
 def get_docstring_and_rest(filename):
@@ -143,7 +156,10 @@ def get_docstring_and_rest(filename):
     rest: str
         `filename` content without the docstring
     """
-    with open(filename) as f:
+    # can't use codecs.open(filename, 'r', 'utf-8') here b/c ast doesn't
+    # seem to work with unicode strings in Python2.7
+    # "SyntaxError: encoding declaration in Unicode string"
+    with open(filename, 'rb') as f:
         content = f.read()
 
     node = ast.parse(content)
@@ -154,9 +170,11 @@ def get_docstring_and_rest(filename):
        isinstance(node.body[0].value, ast.Str):
         docstring_node = node.body[0]
         docstring = docstring_node.value.s
+        if hasattr(docstring, 'decode'):  # python2.7
+            docstring = docstring.decode('utf-8')
         # This get the content of the file after the docstring last line
         # Note: 'maxsplit' argument is not a keyword argument in python2
-        rest = content.split('\n', docstring_node.lineno)[-1]
+        rest = content.decode('utf-8').split('\n', docstring_node.lineno)[-1]
         return docstring, rest
     else:
         raise ValueError(('Could not find docstring in file "{0}". '
@@ -174,7 +192,6 @@ def split_code_and_text_blocks(source_file):
         and content string of block.
     """
     docstring, rest_of_content = get_docstring_and_rest(source_file)
-
     blocks = [('text', docstring)]
 
     pattern = re.compile(
@@ -187,7 +204,7 @@ def split_code_and_text_blocks(source_file):
         code_block_content = rest_of_content[pos_so_far:match_start_pos]
         text_content = match.group('text_content')
         sub_pat = re.compile('^#', flags=re.M)
-        text_block_content = dedent(re.sub(sub_pat, '', text_content))
+        text_block_content = dedent(re.sub(sub_pat, '', text_content)).lstrip()
         if code_block_content.strip():
             blocks.append(('code', code_block_content))
         if text_block_content.strip():
@@ -213,7 +230,24 @@ def text2string(content):
     try:
         return ast.literal_eval(content) + '\n'
     except Exception:
-        return content
+        return content + '\n'
+
+
+def extract_thumbnail_number(text):
+    """ Pull out the thumbnail image number specified in the docstring. """
+
+    # check whether the user has specified a specific thumbnail image
+    pattr = re.compile(
+        r"^\s*#\s*sphinx_gallery_thumbnail_number\s*=\s*([0-9]+)\s*$", flags=re.MULTILINE)
+    match = pattr.search(text)
+
+    if match is None:
+        # by default, use the first figure created
+        thumbnail_number = 1
+    else:
+        thumbnail_number = int(match.groups()[0])
+
+    return thumbnail_number
 
 
 def extract_intro(filename):
@@ -250,35 +284,19 @@ def get_md5sum(src_file):
     return src_md5
 
 
-def check_md5sum_change(src_file):
-    """Returns True if src_file has a different md5sum"""
+def md5sum_is_current(src_file):
+    """Returns True if src_file has the same md5 hash as the one stored on disk"""
 
     src_md5 = get_md5sum(src_file)
 
     src_md5_file = src_file + '.md5'
-    src_file_changed = True
     if os.path.exists(src_md5_file):
         with open(src_md5_file, 'r') as file_checksum:
             ref_md5 = file_checksum.read()
-        if src_md5 == ref_md5:
-            src_file_changed = False
-
-    if src_file_changed:
-        with open(src_md5_file, 'w') as file_checksum:
-            file_checksum.write(src_md5)
-
-    return src_file_changed
 
+        return src_md5 == ref_md5
 
-def _plots_are_current(src_file, image_file):
-    """Test existence of image file and no change in md5sum of
-    example"""
-
-    first_image_file = image_file.format(1)
-    has_image = os.path.exists(first_image_file)
-    src_file_changed = check_md5sum_change(src_file)
-
-    return has_image and not src_file_changed
+    return False
 
 
 def save_figures(image_path, fig_count, gallery_conf):
@@ -290,10 +308,15 @@ def save_figures(image_path, fig_count, gallery_conf):
         Path where plots are saved (format string which accepts figure number)
     fig_count : int
         Previous figure number count. Figure number add from this number
+    gallery_conf : dict
+        Contains the configuration of Sphinx-Gallery
 
     Returns
     -------
-    list of strings containing the full path to each figure
+    figure_list : list of str
+        strings containing the full path to each figure
+    images_rst : str
+        rst code to embed the images in the document
     """
     figure_list = []
 
@@ -317,9 +340,9 @@ def save_figures(image_path, fig_count, gallery_conf):
     if gallery_conf.get('find_mayavi_figures', False):
         from mayavi import mlab
         e = mlab.get_engine()
-        last_matplotlib_fig_num = len(figure_list)
+        last_matplotlib_fig_num = fig_count + len(figure_list)
         total_fig_num = last_matplotlib_fig_num + len(e.scenes)
-        mayavi_fig_nums = range(last_matplotlib_fig_num, total_fig_num)
+        mayavi_fig_nums = range(last_matplotlib_fig_num + 1, total_fig_num + 1)
 
         for scene, mayavi_fig_num in zip(e.scenes, mayavi_fig_nums):
             current_fig = image_path.format(mayavi_fig_num)
@@ -329,7 +352,18 @@ def save_figures(image_path, fig_count, gallery_conf):
             figure_list.append(current_fig)
         mlab.close(all=True)
 
-    return figure_list
+    # Depending on whether we have one or more figures, we're using a
+    # horizontal list or a single rst call to 'image'.
+    images_rst = ""
+    if len(figure_list) == 1:
+        figure_name = figure_list[0]
+        images_rst = SINGLE_IMAGE % figure_name.lstrip('/')
+    elif len(figure_list) > 1:
+        images_rst = HLIST_HEADER
+        for figure_name in figure_list:
+            images_rst += HLIST_IMAGE_TEMPLATE % figure_name.lstrip('/')
+
+    return figure_list, images_rst
 
 
 def scale_image(in_fname, out_fname, max_width, max_height):
@@ -377,18 +411,28 @@ def scale_image(in_fname, out_fname, max_width, max_height):
                           generated images')
 
 
-def save_thumbnail(image_path, base_image_name, gallery_conf):
+def save_thumbnail(image_path_template, src_file, gallery_conf):
     """Save the thumbnail image"""
-    first_image_file = image_path.format(1)
-    thumb_dir = os.path.join(os.path.dirname(first_image_file), 'thumb')
+    # read specification of the figure to display as thumbnail from main text
+    _, content = get_docstring_and_rest(src_file)
+    thumbnail_number = extract_thumbnail_number(content)
+    thumbnail_image_path = image_path_template.format(thumbnail_number)
+
+    thumb_dir = os.path.join(os.path.dirname(thumbnail_image_path), 'thumb')
     if not os.path.exists(thumb_dir):
         os.makedirs(thumb_dir)
 
+    base_image_name = os.path.splitext(os.path.basename(src_file))[0]
     thumb_file = os.path.join(thumb_dir,
                               'sphx_glr_%s_thumb.png' % base_image_name)
 
-    if os.path.exists(first_image_file):
-        scale_image(first_image_file, thumb_file, 400, 280)
+    if src_file in gallery_conf['failing_examples']:
+        broken_img = os.path.join(glr_path_static(), 'broken_example.png')
+        scale_image(broken_img, thumb_file, 200, 140)
+
+    elif os.path.exists(thumbnail_image_path):
+        scale_image(thumbnail_image_path, thumb_file, 400, 280)
+
     elif not os.path.exists(thumb_file):
         # create something to replace the thumbnail
         default_thumb_file = os.path.join(glr_path_static(), 'no_image.png')
@@ -405,17 +449,20 @@ def generate_dir_rst(src_dir, target_dir, gallery_conf, seen_backrefs):
               src_dir)
         print('Skipping this directory')
         print(80 * '_')
-        return ""  # because string is an expected return type
+        return "", []  # because string is an expected return type
 
     fhindex = open(os.path.join(src_dir, 'README.txt')).read()
+
     if not os.path.exists(target_dir):
         os.makedirs(target_dir)
     sorted_listdir = [fname for fname in sorted(os.listdir(src_dir))
                       if fname.endswith('.py')]
     entries_text = []
+    computation_times = []
     for fname in sorted_listdir:
-        amount_of_code = generate_file_rst(fname, target_dir, src_dir,
-                                           gallery_conf)
+        amount_of_code, time_elapsed = \
+            generate_file_rst(fname, target_dir, src_dir, gallery_conf)
+        computation_times.append((time_elapsed, fname))
         new_fname = os.path.join(src_dir, fname)
         intro = extract_intro(new_fname)
         write_backreferences(seen_backrefs, gallery_conf,
@@ -438,95 +485,119 @@ def generate_dir_rst(src_dir, target_dir, gallery_conf, seen_backrefs):
     fhindex += """.. raw:: html\n
     <div style='clear:both'></div>\n\n"""
 
-    return fhindex
+    return fhindex, computation_times
 
 
-def execute_script(code_block, example_globals, image_path, fig_count,
-                   src_file, gallery_conf):
+def execute_code_block(code_block, example_globals,
+                       block_vars, gallery_conf):
     """Executes the code block of the example file"""
     time_elapsed = 0
     stdout = ''
 
-    # We need to execute the code
-    print('plotting code blocks in %s' % src_file)
+    # If example is not suitable to run, skip executing its blocks
+    if not block_vars['execute_script']:
+        return stdout, time_elapsed
 
     plt.close('all')
     cwd = os.getcwd()
     # Redirect output to stdout and
     orig_stdout = sys.stdout
+    src_file = block_vars['src_file']
 
     try:
         # First cd in the original example dir, so that any file
         # created by the example get created in this directory
         os.chdir(os.path.dirname(src_file))
-        my_buffer = StringIO()
+        my_buffer = MixedEncodingStringIO()
         my_stdout = Tee(sys.stdout, my_buffer)
         sys.stdout = my_stdout
 
         t_start = time()
+        # don't use unicode_literals at the top of this file or you get
+        # nasty errors here on Py2.7
         exec(code_block, example_globals)
         time_elapsed = time() - t_start
 
         sys.stdout = orig_stdout
 
         my_stdout = my_buffer.getvalue().strip().expandtabs()
+        # raise RuntimeError
         if my_stdout:
-            stdout = CODE_OUTPUT.format(indent(my_stdout, ' ' * 4))
+            stdout = CODE_OUTPUT.format(indent(my_stdout, u' ' * 4))
         os.chdir(cwd)
-        figure_list = save_figures(image_path, fig_count, gallery_conf)
-
-        # Depending on whether we have one or more figures, we're using a
-        # horizontal list or a single rst call to 'image'.
-        image_list = ""
-        if len(figure_list) == 1:
-            figure_name = figure_list[0]
-            image_list = SINGLE_IMAGE % figure_name.lstrip('/')
-        elif len(figure_list) > 1:
-            image_list = HLIST_HEADER
-            for figure_name in figure_list:
-                image_list += HLIST_IMAGE_TEMPLATE % figure_name.lstrip('/')
+        fig_list, images_rst = save_figures(
+            block_vars['image_path'], block_vars['fig_count'], gallery_conf)
+        fig_num = len(fig_list)
 
     except Exception:
         formatted_exception = traceback.format_exc()
 
-        print(80 * '_')
-        print('%s is not compiling:' % src_file)
-        print(formatted_exception)
-        print(80 * '_')
-
-        figure_list = []
-        image_list = codestr2rst(formatted_exception, lang='pytb')
+        fail_example_warning = 80 * '_' + '\n' + \
+            '%s failed to execute correctly:' % src_file + \
+            formatted_exception + 80 * '_' + '\n'
+        warnings.warn(fail_example_warning)
 
-        # Overrides the output thumbnail in the gallery for easy identification
-        broken_img = os.path.join(glr_path_static(), 'broken_example.png')
-        shutil.copyfile(broken_img, os.path.join(cwd, image_path.format(1)))
-        fig_count += 1  # raise count to avoid overwriting image
+        fig_num = 0
+        images_rst = codestr2rst(formatted_exception, lang='pytb')
 
         # Breaks build on first example error
-
+        # XXX This check can break during testing e.g. if you uncomment the
+        # `raise RuntimeError` by the `my_stdout` call, maybe use `.get()`?
         if gallery_conf['abort_on_example_error']:
             raise
+        # Stores failing file
+        gallery_conf['failing_examples'][src_file] = formatted_exception
+        block_vars['execute_script'] = False
 
     finally:
         os.chdir(cwd)
         sys.stdout = orig_stdout
 
-    print(" - time elapsed : %.2g sec" % time_elapsed)
-    code_output = "\n{0}\n\n{1}\n\n".format(image_list, stdout)
+    code_output = u"\n{0}\n\n{1}\n\n".format(images_rst, stdout)
+    block_vars['fig_count'] += fig_num
+
+    return code_output, time_elapsed
+
 
-    return code_output, time_elapsed, fig_count + len(figure_list)
+def clean_modules():
+    """Remove "unload" seaborn from the name space
+
+    After a script is executed it can load a variety of setting that one
+    does not want to influence in other examples in the gallery."""
+
+    # Horrible code to 'unload' seaborn, so that it resets
+    # its default when is load
+    # Python does not support unloading of modules
+    # https://bugs.python.org/issue9072
+    for module in list(sys.modules.keys()):
+        if 'seaborn' in module:
+            del sys.modules[module]
+
+    # Reset Matplotlib to default
+    plt.rcdefaults()
 
 
 def generate_file_rst(fname, target_dir, src_dir, gallery_conf):
-    """ Generate the rst file for a given example.
+    """Generate the rst file for a given example.
 
-        Returns the amout of code (in characters) of the corresponding
-        files.
+    Returns
+    -------
+    amount_of_code : int
+        character count of the corresponding python script in file
+    time_elapsed : float
+        seconds required to run the script
     """
 
     src_file = os.path.join(src_dir, fname)
     example_file = os.path.join(target_dir, fname)
     shutil.copyfile(src_file, example_file)
+    script_blocks = split_code_and_text_blocks(src_file)
+    amount_of_code = sum([len(bcontent)
+                          for blabel, bcontent in script_blocks
+                          if blabel == 'code'])
+
+    if md5sum_is_current(example_file):
+        return amount_of_code, 0
 
     image_dir = os.path.join(target_dir, 'images')
     if not os.path.exists(image_dir):
@@ -534,82 +605,80 @@ def generate_file_rst(fname, target_dir, src_dir, gallery_conf):
 
     base_image_name = os.path.splitext(fname)[0]
     image_fname = 'sphx_glr_' + base_image_name + '_{0:03}.png'
-    image_path = os.path.join(image_dir, image_fname)
-
-    script_blocks = split_code_and_text_blocks(example_file)
-
-    amount_of_code = sum([len(bcontent)
-                          for blabel, bcontent in script_blocks
-                          if blabel == 'code'])
-
-    if _plots_are_current(example_file, image_path):
-        return amount_of_code
-
-    time_elapsed = 0
+    image_path_template = os.path.join(image_dir, image_fname)
 
     ref_fname = example_file.replace(os.path.sep, '_')
     example_rst = """\n\n.. _sphx_glr_{0}:\n\n""".format(ref_fname)
     example_nb = Notebook(fname, target_dir)
 
     filename_pattern = gallery_conf.get('filename_pattern')
-    if re.search(filename_pattern, src_file) and gallery_conf['plot_gallery']:
-        example_globals = {
-            # A lot of examples contains 'print(__doc__)' for example in
-            # scikit-learn so that running the example prints some useful
-            # information. Because the docstring has been separated from
-            # the code blocks in sphinx-gallery, __doc__ is actually
-            # __builtin__.__doc__ in the execution context and we do not
-            # want to print it
-            '__doc__': '',
-            # Examples may contain if __name__ == '__main__' guards
-            # for in example scikit-learn if the example uses multiprocessing
-            '__name__': '__main__'}
-
-        fig_count = 0
-        # A simple example has two blocks: one for the
-        # example introduction/explanation and one for the code
-        is_example_notebook_like = len(script_blocks) > 2
-        for blabel, bcontent in script_blocks:
-            if blabel == 'code':
-                code_output, rtime, fig_count = execute_script(bcontent,
-                                                               example_globals,
-                                                               image_path,
-                                                               fig_count,
-                                                               src_file,
-                                                               gallery_conf)
-
-                time_elapsed += rtime
-                example_nb.add_code_cell(bcontent)
-
-                if is_example_notebook_like:
-                    example_rst += codestr2rst(bcontent) + '\n'
-                    example_rst += code_output
-                else:
-                    example_rst += code_output
-                    if 'sphx-glr-script-out' in code_output:
-                        # Add some vertical space after output
-                        example_rst += "\n\n|\n\n"
-                    example_rst += codestr2rst(bcontent) + '\n'
-
-            else:
-                example_rst += text2string(bcontent) + '\n'
-                example_nb.add_markdown_cell(text2string(bcontent))
-    else:
-        for blabel, bcontent in script_blocks:
-            if blabel == 'code':
+    execute_script = re.search(filename_pattern, src_file) and gallery_conf[
+        'plot_gallery']
+    example_globals = {
+        # A lot of examples contains 'print(__doc__)' for example in
+        # scikit-learn so that running the example prints some useful
+        # information. Because the docstring has been separated from
+        # the code blocks in sphinx-gallery, __doc__ is actually
+        # __builtin__.__doc__ in the execution context and we do not
+        # want to print it
+        '__doc__': '',
+        # Examples may contain if __name__ == '__main__' guards
+        # for in example scikit-learn if the example uses multiprocessing
+        '__name__': '__main__',
+    }
+
+    # A simple example has two blocks: one for the
+    # example introduction/explanation and one for the code
+    is_example_notebook_like = len(script_blocks) > 2
+    time_elapsed = 0
+    block_vars = {'execute_script': execute_script, 'fig_count': 0,
+                  'image_path': image_path_template, 'src_file': src_file}
+    print('Executing file %s' % src_file)
+    for blabel, bcontent in script_blocks:
+        if blabel == 'code':
+            code_output, rtime = execute_code_block(bcontent,
+                                                    example_globals,
+                                                    block_vars,
+                                                    gallery_conf)
+
+            time_elapsed += rtime
+            example_nb.add_code_cell(bcontent)
+
+            if is_example_notebook_like:
                 example_rst += codestr2rst(bcontent) + '\n'
-                example_nb.add_code_cell(bcontent)
+                example_rst += code_output
             else:
-                example_rst += bcontent + '\n'
-                example_nb.add_markdown_cell(text2string(bcontent))
+                example_rst += code_output
+                if 'sphx-glr-script-out' in code_output:
+                    # Add some vertical space after output
+                    example_rst += "\n\n|\n\n"
+                example_rst += codestr2rst(bcontent) + '\n'
+
+        else:
+            example_rst += text2string(bcontent) + '\n'
+            example_nb.add_markdown_cell(text2string(bcontent))
 
-    save_thumbnail(image_path, base_image_name, gallery_conf)
+    clean_modules()
+
+    # Writes md5 checksum if example has build correctly
+    # not failed and was initially meant to run(no-plot shall not cache md5sum)
+    if block_vars['execute_script']:
+        with open(example_file + '.md5', 'w') as file_checksum:
+            file_checksum.write(get_md5sum(example_file))
+
+    save_thumbnail(image_path_template, src_file, gallery_conf)
 
     time_m, time_s = divmod(time_elapsed, 60)
     example_nb.save_file()
-    with open(os.path.join(target_dir, base_image_name + '.rst'), 'w') as f:
-        example_rst += CODE_DOWNLOAD.format(time_m, time_s, fname,
-                                            example_nb.file_name)
+    with codecs.open(os.path.join(target_dir, base_image_name + '.rst'),
+                     mode='w', encoding='utf-8') as f:
+        example_rst += "**Total running time of the script:**" \
+                       " ({0: .0f} minutes {1: .3f} seconds)\n\n".format(
+                           time_m, time_s)
+        example_rst += CODE_DOWNLOAD.format(fname, example_nb.file_name)
+        example_rst += SPHX_GLR_SIG
         f.write(example_rst)
 
-    return amount_of_code
+    print("{0} ran in : {1:.2g} seconds\n".format(src_file, time_elapsed))
+
+    return amount_of_code, time_elapsed
diff --git a/doc/sphinxext/sphinx_gallery/notebook.py b/doc/sphinxext/sphinx_gallery/notebook.py
index c0ee5cd80b..fc0fccfb6b 100755
--- a/doc/sphinxext/sphinx_gallery/notebook.py
+++ b/doc/sphinxext/sphinx_gallery/notebook.py
@@ -4,18 +4,20 @@
 Parser for Jupyter notebooks
 ============================
 
-Class that holds the Ipython notebook information
+Class that holds the Jupyter notebook information
 
 """
 # Author: Óscar Nájera
 # License: 3-clause BSD
 
 from __future__ import division, absolute_import, print_function
+from functools import partial
 import json
 import os
 import re
 import sys
 
+
 def ipy_notebook_skeleton():
     """Returns a dictionary with the elements of a Jupyter notebook"""
     py_version = sys.version_info
@@ -46,25 +48,58 @@ def ipy_notebook_skeleton():
     return notebook_skeleton
 
 
+def directive_fun(match, directive):
+    """Helper to fill in directives"""
+    directive_to_alert = dict(note="info", warning="danger")
+    return ('<div class="alert alert-{0}"><h4>{1}</h4><p>{2}</p></div>'
+            .format(directive_to_alert[directive], directive.capitalize(),
+                    match.group(1).strip()))
+
+
 def rst2md(text):
     """Converts the RST text from the examples docstrigs and comments
-    into markdown text for the IPython notebooks"""
+    into markdown text for the Jupyter notebooks"""
 
     top_heading = re.compile(r'^=+$\s^([\w\s-]+)^=+$', flags=re.M)
     text = re.sub(top_heading, r'# \1', text)
 
     math_eq = re.compile(r'^\.\. math::((?:.+)?(?:\n+^  .+)*)', flags=re.M)
     text = re.sub(math_eq,
-                  lambda match: r'$${0}$$'.format(match.group(1).strip()),
+                  lambda match: r'\begin{{align}}{0}\end{{align}}'.format(
+                      match.group(1).strip()),
                   text)
-    inline_math = re.compile(r':math:`(.+)`')
+    inline_math = re.compile(r':math:`(.+?)`', re.DOTALL)
     text = re.sub(inline_math, r'$\1$', text)
 
+    directives = ('warning', 'note')
+    for directive in directives:
+        directive_re = re.compile(r'^\.\. %s::((?:.+)?(?:\n+^  .+)*)'
+                                  % directive, flags=re.M)
+        text = re.sub(directive_re,
+                      partial(directive_fun, directive=directive), text)
+
+    links = re.compile(r'^ *\.\. _.*:.*$\n', flags=re.M)
+    text = re.sub(links, '', text)
+
+    refs = re.compile(r':ref:`')
+    text = re.sub(refs, '`', text)
+
+    contents = re.compile(r'^\s*\.\. contents::.*$(\n +:\S+: *$)*\n',
+                          flags=re.M)
+    text = re.sub(contents, '', text)
+
+    images = re.compile(
+        r'^\.\. image::(.*$)(?:\n *:alt:(.*$)\n)?(?: +:\S+:.*$\n)*',
+        flags=re.M)
+    text = re.sub(
+        images, lambda match: '![{1}]({0})\n'.format(
+            match.group(1).strip(), (match.group(2) or '').strip()), text)
+
     return text
 
 
 class Notebook(object):
-    """Ipython notebook object
+    """Jupyter notebook object
 
     Constructs the file cell-by-cell and writes it at the end"""
 
diff --git a/doc/themes/scikit-learn/layout.html b/doc/themes/scikit-learn/layout.html
index 6ed1db7688..d6b377a30a 100755
--- a/doc/themes/scikit-learn/layout.html
+++ b/doc/themes/scikit-learn/layout.html
@@ -12,7 +12,6 @@
     :license: BSD
 #}
 {% extends "basic/layout.html" %}
-{% set css_files = css_files + ["_static/gallery.css"] %}
 
 {% block htmltitle %}
   {{ super() }}
diff --git a/doc/themes/scikit-learn/static/gallery.css b/doc/themes/scikit-learn/static/gallery.css
deleted file mode 100755
index 27dce45618..0000000000
--- a/doc/themes/scikit-learn/static/gallery.css
+++ /dev/null
@@ -1,73 +0,0 @@
-/******** For the gallery **************/
-
-/* ------- Fix maximum size of thumbnails in example gallery -------------- */
-div.thumbnailContainer img {
-    max-width: 160px;
-    max-height: 160px;
-    /*display: block;*/
-    margin: auto auto auto -8px;
-    display: inline;
-}
-
-div.thumbnailContainer div.figure {
-    float: left;
-    margin: 10px 11px 7em 11px;
-    -webkit-border-radius: 10px;
-    -moz-border-radius: 10px;
-    border-radius: 10px;
-    border: 2px solid #FFF;
-    background-color: #FFF;
-    width: 150px;
-    height: 100px;
-    -webkit-background-size: 150px 100px;
-    -moz-background-size: 150px 100px;
-}
-
-*[tooltip] {
-    position: relative;
-    float: left;
-}
-
-*[tooltip]:hover:after{
-    background: rgba(0,0,0,.8);
-    border-radius: 5px;
-    color: white;
-    content: attr(tooltip);
-    left: 95%;
-    padding: 5px 15px;
-    position: absolute;
-    z-index: 98;
-    width: 220px;
-    bottom: 52%
-}
-
-
-*[tooltip]:hover:before {
-    content: "";
-    position: absolute;
-    z-index: 99;
-    border: solid;
-    border-color: #333 transparent;
-    border-width: 18px 0px 0px 20px;
-    left: 85%;
-    bottom: 58%
-}
-
-div.thumbnailContainer {
-    box-shadow: none;
-    border: solid white 1px;
-    padding-top: 5px;
-}
-
-div.thumbnailContainer:hover {
-    box-shadow: 0 0 15px rgba(142, 176, 202, 0.5);
-    border: solid #B4DDFC 1px;
-}
-
-div.thumbnailContainer div p {
-    margin: 0 0 .1em 0;
-}
-
-div.thumbnailContainer div a {
-    display: block; /* To have a better hover behavior */
-}
diff --git a/doc/themes/scikit-learn/static/nature.css_t b/doc/themes/scikit-learn/static/nature.css_t
index c9c228fc7b..e7ec024ea9 100755
--- a/doc/themes/scikit-learn/static/nature.css_t
+++ b/doc/themes/scikit-learn/static/nature.css_t
@@ -129,7 +129,7 @@ div.navbar div.nav-icon {
         margin-left: 6px;
     }
     div.navbar.responsive > ul li.btn-li + li {
-        margin-top: -5px;   
+        margin-top: -5px;
     }
     div.navbar.responsive > ul {
         visiblity: visible;
@@ -777,6 +777,11 @@ div.sprint-wrapper:hover {
 
 /*-----------------------The Examples Gallery-------------------------------*/
 
+div.sphx-glr-download code.download {
+  display: inline-block;
+  white-space: normal;
+}
+
 /* ------- Zoom plots to make them fit in layout -------------------------- */
 div.body img.align-center {
     max-width:805px;
diff --git a/doc/whats_new.rst b/doc/whats_new.rst
index 301215d2f2..8db58b884d 100755
--- a/doc/whats_new.rst
+++ b/doc/whats_new.rst
@@ -340,6 +340,12 @@ Enhancements
      (`#7419 <https://github.com/scikit-learn/scikit-learn/pull/7419>_`)
      By `Gregory Stupp`_ and `Joel Nothman`_.
 
+   - Isotonic regression (:mod:`isotonic`) now uses a better algorithm to avoid
+     `O(n^2)` behavior in pathological cases, and is also generally faster
+     (`#6601 <https://github.com/scikit-learn/scikit-learn/pull/6691>`).
+     By `Antony Lee`_.
+
+
 Bug fixes
 .........
 
diff --git a/sklearn/_isotonic.pyx b/sklearn/_isotonic.pyx
index e3ee2a5483..1cec075fc6 100755
--- a/sklearn/_isotonic.pyx
+++ b/sklearn/_isotonic.pyx
@@ -1,4 +1,4 @@
-# Author: Nelle Varoquaux, Andrew Tulloch
+# Author: Nelle Varoquaux, Andrew Tulloch, Antony Lee
 
 # Uses the pool adjacent violators algorithm (PAVA), with the
 # enhancement of searching for the longest decreasing subsequence to
@@ -10,73 +10,67 @@ cimport cython
 
 ctypedef np.float64_t DOUBLE
 
-np.import_array()
-
 
 @cython.boundscheck(False)
 @cython.wraparound(False)
 @cython.cdivision(True)
-def _isotonic_regression(np.ndarray[DOUBLE, ndim=1] y,
-                         np.ndarray[DOUBLE, ndim=1] weight,
-                         np.ndarray[DOUBLE, ndim=1] solution):
+def _inplace_contiguous_isotonic_regression(DOUBLE[::1] y, DOUBLE[::1] w):
     cdef:
-        DOUBLE numerator, denominator, ratio
-        Py_ssize_t i, pooled, n, k
-
-    n = y.shape[0]
-    # The algorithm proceeds by iteratively updating the solution
-    # array.
+        Py_ssize_t n = y.shape[0], i, k
+        DOUBLE prev_y, sum_wy, sum_w
+        Py_ssize_t[::1] target = np.arange(n, dtype=np.intp)
 
-    # TODO - should we just pass in a pre-copied solution
-    # array and mutate that?
-    for i in range(n):
-        solution[i] = y[i]
+    # target describes a list of blocks.  At any time, if [i..j] (inclusive) is
+    # an active block, then target[i] := j and target[j] := i.
 
-    if n <= 1:
-        return solution
+    # For "active" indices (block starts):
+    # w[i] := sum{w_orig[j], j=[i..target[i]]}
+    # y[i] := sum{y_orig[j]*w_orig[j], j=[i..target[i]]} / w[i]
 
-    n -= 1
-    while 1:
-        # repeat until there are no more adjacent violators.
+    with nogil:
         i = 0
-        pooled = 0
         while i < n:
-            k = i
-            while k < n and solution[k] >= solution[k + 1]:
-                k += 1
-            if solution[i] != solution[k]:
-                # solution[i:k + 1] is a decreasing subsequence, so
-                # replace each point in the subsequence with the
-                # weighted average of the subsequence.
-
-                # TODO: explore replacing each subsequence with a
-                # _single_ weighted point, and reconstruct the whole
-                # sequence from the sequence of collapsed points.
-                # Theoretically should reduce running time, though
-                # initial experiments weren't promising.
-                numerator = 0.0
-                denominator = 0.0
-                for j in range(i, k + 1):
-                    numerator += solution[j] * weight[j]
-                    denominator += weight[j]
-                ratio = numerator / denominator
-                for j in range(i, k + 1):
-                    solution[j] = ratio
-                pooled = 1
-            i = k + 1
-        # Check for convergence
-        if pooled == 0:
-            break
-
-    return solution
+            k = target[i] + 1
+            if k == n:
+                break
+            if y[i] < y[k]:
+                i = k
+                continue
+            sum_wy = w[i] * y[i]
+            sum_w = w[i]
+            while True:
+                # We are within a decreasing subsequence.
+                prev_y = y[k]
+                sum_wy += w[k] * y[k]
+                sum_w += w[k]
+                k = target[k] + 1
+                if k == n or prev_y < y[k]:
+                    # Non-singleton decreasing subsequence is finished,
+                    # update first entry.
+                    y[i] = sum_wy / sum_w
+                    w[i] = sum_w
+                    target[i] = k - 1
+                    target[k - 1] = i
+                    if i > 0:
+                        # Backtrack if we can.  This makes the algorithm
+                        # single-pass and ensures O(n) complexity.
+                        i = target[i - 1]
+                    # Otherwise, restart from the same point.
+                    break
+        # Reconstruct the solution.
+        i = 0
+        while i < n:
+            k = target[i] + 1
+            y[i + 1 : k] = y[i]
+            i = k
 
 
 @cython.boundscheck(False)
 @cython.wraparound(False)
 @cython.cdivision(True)
 def _make_unique(np.ndarray[dtype=np.float64_t] X,
-                  np.ndarray[dtype=np.float64_t] y,
-                  np.ndarray[dtype=np.float64_t] sample_weights):
+                 np.ndarray[dtype=np.float64_t] y,
+                 np.ndarray[dtype=np.float64_t] sample_weights):
     """Average targets for duplicate X, drop duplicates.
 
     Aggregates duplicate X values into a single X value where
diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py
index fdd1f852c6..1b31562b46 100755
--- a/sklearn/decomposition/kernel_pca.py
+++ b/sklearn/decomposition/kernel_pca.py
@@ -34,15 +34,15 @@ class KernelPCA(BaseEstimator, TransformerMixin):
     degree : int, default=3
         Degree for poly kernels. Ignored by other kernels.
 
-    gamma : float, optional
-        Kernel coefficient for rbf and poly kernels. Default: 1/n_features.
-        Ignored by other kernels.
+    gamma : float, default=1/n_features
+        Kernel coefficient for rbf and poly kernels. Ignored by other
+        kernels.
 
-    coef0 : float, optional
+    coef0 : float, default=1
         Independent term in poly and sigmoid kernels.
         Ignored by other kernels.
 
-    kernel_params : mapping of string to any, optional
+    kernel_params : mapping of string to any, default=None
         Parameters (keyword arguments) and values for kernel passed as
         callable object. Ignored by other kernels.
 
@@ -54,18 +54,18 @@ class KernelPCA(BaseEstimator, TransformerMixin):
         Learn the inverse transform for non-precomputed kernels.
         (i.e. learn to find the pre-image of a point)
 
-    eigen_solver : string ['auto'|'dense'|'arpack']
+    eigen_solver : string ['auto'|'dense'|'arpack'], default='auto'
         Select eigensolver to use. If n_components is much less than
         the number of training samples, arpack may be more efficient
         than the dense eigensolver.
 
     tol : float, default=0
-        Convergence tolerance for arpack. If zero, optimal value will be
-        chosen by arpack.
+        Convergence tolerance for arpack.
+        If 0, optimal value will be chosen by arpack.
 
     max_iter : int, default=None
-        Maximum number of iterations for arpack. If None, optimal value will
-        be chosen by arpack.
+        Maximum number of iterations for arpack.
+        If None, optimal value will be chosen by arpack.
 
     remove_zero_eig : boolean, default=False
         If True, then all components with zero eigenvalues are removed, so
@@ -99,15 +99,14 @@ class KernelPCA(BaseEstimator, TransformerMixin):
         `remove_zero_eig` are not set, then all components are stored.
 
     dual_coef_ : array, (n_samples, n_features)
-        Inverse transform matrix. If `fit_inverse_transform=False`,
-        ``dual_coef_`` is not present.
+        Inverse transform matrix. Set if `fit_inverse_transform` is True.
 
     X_transformed_fit_ : array, (n_samples, n_components)
         Projection of the fitted data on the kernel principal components.
 
     X_fit_ : (n_samples, n_features)
         The data used to fit the model. If `copy_X=False`, then `X_fit_` is
-        a reference.
+        a reference. This attribute is used for the calls to transform.
 
     References
     ----------
diff --git a/sklearn/feature_extraction/text.py b/sklearn/feature_extraction/text.py
index d291cc54d6..3bc2e8cb0c 100755
--- a/sklearn/feature_extraction/text.py
+++ b/sklearn/feature_extraction/text.py
@@ -685,9 +685,11 @@ def _sort_features(self, X, vocabulary):
         sorted_features = sorted(six.iteritems(vocabulary))
         map_index = np.empty(len(sorted_features), dtype=np.int32)
         for new_val, (term, old_val) in enumerate(sorted_features):
-            map_index[new_val] = old_val
             vocabulary[term] = new_val
-        return X[:, map_index]
+            map_index[old_val] = new_val
+
+        X.indices = map_index.take(X.indices, mode='clip')
+        return X
 
     def _limit_features(self, X, vocabulary, high=None, low=None,
                         limit=None):
@@ -741,16 +743,25 @@ def _count_vocab(self, raw_documents, fixed_vocab):
             vocabulary.default_factory = vocabulary.__len__
 
         analyze = self.build_analyzer()
-        j_indices = _make_int_array()
+        j_indices = []
         indptr = _make_int_array()
+        values = _make_int_array()
         indptr.append(0)
         for doc in raw_documents:
+            feature_counter = {}
             for feature in analyze(doc):
                 try:
-                    j_indices.append(vocabulary[feature])
+                    feature_idx = vocabulary[feature]
+                    if feature_idx not in feature_counter:
+                        feature_counter[feature_idx] = 1
+                    else:
+                        feature_counter[feature_idx] += 1
                 except KeyError:
                     # Ignore out-of-vocabulary items for fixed_vocab=True
                     continue
+
+            j_indices.extend(feature_counter.keys())
+            values.extend(feature_counter.values())
             indptr.append(len(j_indices))
 
         if not fixed_vocab:
@@ -760,14 +771,14 @@ def _count_vocab(self, raw_documents, fixed_vocab):
                 raise ValueError("empty vocabulary; perhaps the documents only"
                                  " contain stop words")
 
-        j_indices = frombuffer_empty(j_indices, dtype=np.intc)
+        j_indices = np.asarray(j_indices, dtype=np.intc)
         indptr = np.frombuffer(indptr, dtype=np.intc)
-        values = np.ones(len(j_indices))
+        values = frombuffer_empty(values, dtype=np.intc)
 
         X = sp.csr_matrix((values, j_indices, indptr),
                           shape=(len(indptr) - 1, len(vocabulary)),
                           dtype=self.dtype)
-        X.sum_duplicates()
+        X.sort_indices()
         return vocabulary, X
 
     def fit(self, raw_documents, y=None):
diff --git a/sklearn/isotonic.py b/sklearn/isotonic.py
index 0585438e87..62f9f0300b 100755
--- a/sklearn/isotonic.py
+++ b/sklearn/isotonic.py
@@ -10,7 +10,7 @@
 from .utils import as_float_array, check_array, check_consistent_length
 from .utils import deprecated
 from .utils.fixes import astype
-from ._isotonic import _isotonic_regression, _make_unique
+from ._isotonic import _inplace_contiguous_isotonic_regression, _make_unique
 import warnings
 import math
 
@@ -120,28 +120,22 @@ def isotonic_regression(y, sample_weight=None, y_min=None, y_max=None,
     "Active set algorithms for isotonic regression; A unifying framework"
     by Michael J. Best and Nilotpal Chakravarti, section 3.
     """
-    y = np.asarray(y, dtype=np.float64)
+    order = np.s_[:] if increasing else np.s_[::-1]
+    y = np.array(y[order], dtype=np.float64)
     if sample_weight is None:
-        sample_weight = np.ones(len(y), dtype=y.dtype)
+        sample_weight = np.ones(len(y), dtype=np.float64)
     else:
-        sample_weight = np.asarray(sample_weight, dtype=np.float64)
-    if not increasing:
-        y = y[::-1]
-        sample_weight = sample_weight[::-1]
-
-    solution = np.empty(len(y))
-    y_ = _isotonic_regression(y, sample_weight, solution)
-    if not increasing:
-        y_ = y_[::-1]
+        sample_weight = np.array(sample_weight[order], dtype=np.float64)
 
+    _inplace_contiguous_isotonic_regression(y, sample_weight)
     if y_min is not None or y_max is not None:
         # Older versions of np.clip don't accept None as a bound, so use np.inf
         if y_min is None:
             y_min = -np.inf
         if y_max is None:
             y_max = np.inf
-        np.clip(y_, y_min, y_max, y_)
-    return y_
+        np.clip(y, y_min, y_max, y)
+    return y[order]
 
 
 class IsotonicRegression(BaseEstimator, TransformerMixin, RegressorMixin):
diff --git a/sklearn/tests/test_isotonic.py b/sklearn/tests/test_isotonic.py
index 5da86bc35f..7abadaf562 100755
--- a/sklearn/tests/test_isotonic.py
+++ b/sklearn/tests/test_isotonic.py
@@ -80,6 +80,10 @@ def test_isotonic_regression():
     y_ = np.array([3, 6, 6, 8, 8, 8, 10])
     assert_array_equal(y_, isotonic_regression(y))
 
+    y = np.array([10, 0, 2])
+    y_ = np.array([4, 4, 4])
+    assert_array_equal(y_, isotonic_regression(y))
+
     x = np.arange(len(y))
     ir = IsotonicRegression(y_min=0., y_max=1.)
     ir.fit(x, y)
diff --git a/sklearn/tree/export.py b/sklearn/tree/export.py
index 92cdab8f27..5d67b4fabe 100755
--- a/sklearn/tree/export.py
+++ b/sklearn/tree/export.py
@@ -62,9 +62,11 @@ def _color_brew(n):
 
 
 class Sentinel:
-    __repr__ = lambda x: '"tree.dot"'
+    def __repr__():
+        return '"tree.dot"'
 SENTINEL = Sentinel()
 
+
 def export_graphviz(decision_tree, out_file=SENTINEL, max_depth=None,
                     feature_names=None, class_names=None, label='all',
                     filled=False, leaves_parallel=False, impurity=True,
