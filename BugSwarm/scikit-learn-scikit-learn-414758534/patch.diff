diff --git a/.travis.yml b/.travis.yml
index 89ab1a3805..7196296a38 100755
--- a/.travis.yml
+++ b/.travis.yml
@@ -38,13 +38,13 @@ matrix:
            NUMPY_VERSION="1.10.4" SCIPY_VERSION="0.16.1" CYTHON_VERSION="0.25.2"
            PILLOW_VERSION="4.0.0" COVERAGE=true
       if: type != cron
-    # This environment tests the newest supported Anaconda release.
+    # This environment tests the latest available dependencies.
     # It runs tests requiring pandas and PyAMG.
     # It also runs with the site joblib instead of the vendored copy of joblib.
-    - env: DISTRIB="conda" PYTHON_VERSION="3.6.2" INSTALL_MKL="true"
-           NUMPY_VERSION="1.14.2" SCIPY_VERSION="1.0.0" PANDAS_VERSION="0.20.3"
-           CYTHON_VERSION="0.26.1" PYAMG_VERSION="3.3.2" PILLOW_VERSION="4.3.0"
-           JOBLIB_VERSION="0.12" COVERAGE=true
+    - env: DISTRIB="conda" PYTHON_VERSION="*" INSTALL_MKL="true"
+           NUMPY_VERSION="*" SCIPY_VERSION="*" PANDAS_VERSION="*"
+           CYTHON_VERSION="*" PYAMG_VERSION="*" PILLOW_VERSION="*"
+           JOBLIB_VERSION="*" COVERAGE=true
            CHECK_PYTEST_SOFT_DEPENDENCY="true" TEST_DOCSTRINGS="true"
            SKLEARN_SITE_JOBLIB=1
       if: type != cron
diff --git a/AUTHORS.rst b/AUTHORS.rst
deleted file mode 100755
index 48427fc0a2..0000000000
--- a/AUTHORS.rst
+++ /dev/null
@@ -1,75 +0,0 @@
-.. -*- mode: rst -*-
-
-
-This is a community effort, and as such many people have contributed
-to it over the years.
-
-History
--------
-
-This project was started in 2007 as a Google Summer of Code project by
-David Cournapeau. Later that year, Matthieu Brucher started work on
-this project as part of his thesis.
-
-In 2010 Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort and Vincent
-Michel of INRIA took leadership of the project and made the first public
-release, February the 1st 2010. Since then, several releases have appeared
-following a ~3 month cycle, and a thriving international community has
-been leading the development.
-
-People
-------
-
-The following people have been core contributors to scikit-learn's development and maintenance:
-
-.. hlist::
-
-  * `Mathieu Blondel <http://mblondel.org>`_
-  * `Matthieu Brucher <http://matt.eifelle.com/>`_
-  * Lars Buitinck
-  * David Cournapeau
-  * `Noel Dawe <http://noel.dawe.me>`_
-  * Vincent Dubourg
-  * Edouard Duchesnay
-  * `Tom Dupré la Tour <https://github.com/TomDLT>`_
-  * Alexander Fabisch
-  * `Virgile Fritsch <https://team.inria.fr/parietal/vfritsch/>`_
-  * `Satra Ghosh <http://www.mit.edu/~satra>`_
-  * `Angel Soler Gollonet <http://webylimonada.com>`_
-  * Chris Filo Gorgolewski
-  * `Alexandre Gramfort <http://alexandre.gramfort.net>`_
-  * `Olivier Grisel <https://twitter.com/ogrisel>`_
-  * `Jaques Grobler <https://github.com/jaquesgrobler>`_
-  * `Yaroslav Halchenko <http://www.onerussian.com/>`_
-  * `Brian Holt <http://personal.ee.surrey.ac.uk/Personal/B.Holt/>`_
-  * `Arnaud Joly <http://www.ajoly.org>`_
-  * Thouis (Ray) Jones
-  * `Kyle Kastner <http://kastnerkyle.github.io>`_
-  * `Manoj Kumar <https://manojbits.wordpress.com>`_
-  * Robert Layton
-  * `Guillaume Lemaitre <https://github.com/glemaitre>`_
-  * `Wei Li <http://kuantkid.github.io/>`_
-  * Paolo Losi
-  * `Gilles Louppe <http://glouppe.github.io/>`_
-  * `Jan Hendrik Metzen <https://github.com/jmetzen>`_
-  * Vincent Michel
-  * Jarrod Millman
-  * `Andreas Müller <http://peekaboo-vision.blogspot.com>`_ (release manager)
-  * `Vlad Niculae <http://vene.ro>`_
-  * `Joel Nothman <http://joelnothman.com>`_
-  * `Alexandre Passos <http://atpassos.posterous.com>`_
-  * `Fabian Pedregosa <http://fa.bianp.net/blog/>`_
-  * `Peter Prettenhofer <https://sites.google.com/site/peterprettenhofer/>`_
-  * `Hanmin Qin <https://github.com/qinhanmin2014>`_
-  * Bertrand Thirion
-  * `Joris Van den Bossche <https://github.com/jorisvandenbossche>`_
-  * `Jake VanderPlas <http://staff.washington.edu/jakevdp/>`_
-  * Nelle Varoquaux
-  * `Gael Varoquaux <http://gael-varoquaux.info/>`_
-  * Ron Weiss
-  * `Roman Yurchak <https://github.com/rth>`_
-
-Please do not email the authors directly to ask for assistance or report issues.
-Instead, please see `What's the best way to ask questions about scikit-learn
-<http://scikit-learn.org/stable/faq.html#what-s-the-best-way-to-get-help-on-scikit-learn-usage>`_
-in the FAQ.
diff --git a/MANIFEST.in b/MANIFEST.in
index ed0ca0e872..db605f55f7 100755
--- a/MANIFEST.in
+++ b/MANIFEST.in
@@ -2,7 +2,7 @@ include *.rst
 recursive-include doc *
 recursive-include examples *
 recursive-include sklearn *.c *.h *.pyx *.pxd *.pxi
-recursive-include sklearn/datasets *.csv *.csv.gz *.rst *.jpg *.txt
+recursive-include sklearn/datasets *.csv *.csv.gz *.rst *.jpg *.txt *.arff.gz *.json.gz
 include COPYING
 include AUTHORS.rst
 include README.rst
diff --git a/Makefile b/Makefile
index ac242b12e5..c2da6ec5ce 100755
--- a/Makefile
+++ b/Makefile
@@ -24,7 +24,7 @@ inplace:
 	$(PYTHON) setup.py build_ext -i
 
 test-code: in
-	$(PYTEST) --showlocals -v sklearn
+	$(PYTEST) --showlocals -v sklearn --durations=20
 test-sphinxext:
 	$(PYTEST) --showlocals -v doc/sphinxext/
 test-doc:
diff --git a/README.rst b/README.rst
index eb1957686a..fa2ef793b9 100755
--- a/README.rst
+++ b/README.rst
@@ -120,7 +120,7 @@ Testing
 ~~~~~~~
 
 After installation, you can launch the test suite from outside the
-source directory (you will need to have the ``pytest`` package installed)::
+source directory (you will need to have ``pytest`` >= 3.3.0 installed)::
 
     pytest sklearn
 
diff --git a/build_tools/Makefile b/build_tools/Makefile
new file mode 100755
index 0000000000..68162733b4
--- /dev/null
+++ b/build_tools/Makefile
@@ -0,0 +1,4 @@
+# Makefile for maintenance tools
+
+authors:
+	python generate_authors_table.py > ../doc/authors.rst
diff --git a/build_tools/circle/push_doc.sh b/build_tools/circle/push_doc.sh
index 2ab1cff451..cb87a84548 100755
--- a/build_tools/circle/push_doc.sh
+++ b/build_tools/circle/push_doc.sh
@@ -4,7 +4,7 @@
 # The behavior of the script is controlled by environment variable defined
 # in the circle.yml in the top level folder of the project.
 
-set -e
+set -ex
 
 if [ -z $CIRCLE_PROJECT_USERNAME ];
 then USERNAME="sklearn-ci";
@@ -38,11 +38,23 @@ if [ ! -d $DOC_REPO ];
 then git clone --depth 1 --no-checkout "git@github.com:scikit-learn/"$DOC_REPO".git";
 fi
 cd $DOC_REPO
-git config core.sparseCheckout true
+
+# check if it's a new branch
+
 echo $dir > .git/info/sparse-checkout
-git checkout $CIRCLE_BRANCH
-git reset --hard origin/$CIRCLE_BRANCH
-git rm -rf $dir/ && rm -rf $dir/
+if ! git show HEAD:$dir >/dev/null
+then
+	# directory does not exist. Need to make it so sparse checkout works
+	mkdir $dir
+	touch $dir/index.html
+	git add $dir
+fi
+git checkout master
+git reset --hard origin/master
+if [ -d $dir ]
+then
+	git rm -rf $dir/ && rm -rf $dir/
+fi
 cp -R $GENERATED_DOC_DIR $dir
 git config user.email "olivier.grisel+sklearn-ci@gmail.com"
 git config user.name $USERNAME
diff --git a/build_tools/generate_authors_table.py b/build_tools/generate_authors_table.py
new file mode 100755
index 0000000000..ea37964733
--- /dev/null
+++ b/build_tools/generate_authors_table.py
@@ -0,0 +1,117 @@
+"""
+This script generates an html table of contributors, with names and avatars.
+The list is generated from scikit-learn's teams on GitHub, plus a small number
+of hard-coded contributors.
+
+The table should be updated for each new inclusion in the teams.
+Generating the table requires admin rights.
+"""
+from __future__ import print_function
+
+import sys
+import requests
+import getpass
+
+try:
+    # With authentication: up to 5000 requests per hour.
+    print("user:", file=sys.stderr)
+    user = input()
+    passwd = getpass.getpass()
+    auth = (user, passwd)
+except IndexError:
+    # Without authentication: up to 60 requests per hour.
+    auth = None
+
+ROW_SIZE = 7
+LOGO_URL = 'https://avatars2.githubusercontent.com/u/365630?v=4'
+
+
+def group_iterable(iterable, size):
+    """Group iterable into lines"""
+    group = []
+    for element in iterable:
+        group.append(element)
+        if len(group) == size:
+            yield group
+            group = []
+    if len(group) != 0:
+        yield group
+
+
+def get_contributors():
+    """Get the list of contributor profiles. Require admin rights."""
+    # get members of scikit-learn teams on GitHub
+    members = []
+    for team in [11523, 33471]:
+        for page in [1, 2]:  # 30 per page
+            members.extend(requests.get(
+                "https://api.github.com/teams/%d/members?page=%d"
+                % (team, page), auth=auth).json())
+
+    # keep only the logins
+    logins = [c['login'] for c in members]
+    # add missing contributors with GitHub accounts
+    logins.extend(['dubourg', 'jarrodmillman', 'mbrucher', 'thouis'])
+    # add missing contributors without GitHub accounts
+    logins.extend(['Angel Soler Gollonet'])
+    # remove duplicate
+    logins = set(logins)
+    # remove CI
+    logins.remove('sklearn-ci')
+
+    # get profiles from GitHub
+    profiles = [get_profile(login) for login in logins]
+    # sort by last name
+    profiles = sorted(profiles, key=key)
+
+    return profiles
+
+
+def get_profile(login):
+    """Get the GitHub profile from login"""
+    profile = requests.get("https://api.github.com/users/%s" % login,
+                           auth=auth).json()
+    if 'name' not in profile:
+        # default profile if the login does not exist
+        return dict(name=login, avatar_url=LOGO_URL, html_url="")
+    else:
+        if profile["name"] is None:
+            profile["name"] = profile["login"]
+
+        # fix missing names
+        missing_names = {'bthirion': 'Bertrand Thirion',
+                         'dubourg': 'Vincent Dubourg',
+                         'Duchesnay': 'Edouard Duchesnay',
+                         'Lars': 'Lars Buitinck',
+                         'MechCoder': 'Manoj Kumar'}
+        if profile["name"] in missing_names:
+            profile["name"] = missing_names[profile["name"]]
+        return profile
+
+
+def key(profile):
+    """Get the last name in lower case"""
+    return profile["name"].split(' ')[-1].lower()
+
+
+contributors = get_contributors()
+
+print(".. raw :: html\n")
+print("    <!-- Generated by gen_authors.py -->")
+print("    <table>")
+print("    <col style='width:%d%%' span='%d'>"
+      % (int(100 / ROW_SIZE), ROW_SIZE))
+print("    <style>")
+print("      img.avatar {border-radius: 10px;}")
+print("      td {vertical-align: top;}")
+print("    </style>")
+for row in group_iterable(contributors, size=ROW_SIZE):
+    print("    <tr>")
+    for contributor in row:
+        print("    <td>")
+        print("    <a href='%s'><img src='%s' class='avatar' /></a> <br />"
+              % (contributor["html_url"], contributor["avatar_url"]))
+        print("    <p>%s</p>" % contributor["name"])
+        print("    </td>")
+    print("    </tr>")
+print("    </table>")
diff --git a/build_tools/travis/install.sh b/build_tools/travis/install.sh
index d41e746a1a..b15e76ea39 100755
--- a/build_tools/travis/install.sh
+++ b/build_tools/travis/install.sh
@@ -84,7 +84,11 @@ elif [[ "$DISTRIB" == "ubuntu" ]]; then
     # and scipy
     virtualenv --system-site-packages testvenv
     source testvenv/bin/activate
-    pip install pytest pytest-cov cython==$CYTHON_VERSION
+    # FIXME: Importing scipy.sparse with numpy 1.8.2 and scipy 0.13.3 produces
+    # a deprecation warning and the test suite fails on such warnings.
+    # To test these numpy/scipy versions, we use pytest<3.8 as it has
+    # a known limitation/bug of not capturing warnings during test collection.
+    pip install pytest==3.7.4 pytest-cov cython==$CYTHON_VERSION
 
 elif [[ "$DISTRIB" == "scipy-dev" ]]; then
     make_conda python=3.7
diff --git a/conftest.py b/conftest.py
index 621097bfc4..f175661165 100755
--- a/conftest.py
+++ b/conftest.py
@@ -11,6 +11,19 @@
 import pytest
 from _pytest.doctest import DoctestItem
 
+from sklearn.utils.fixes import PY3_OR_LATER
+
+PYTEST_MIN_VERSION = '3.3.0'
+
+if LooseVersion(pytest.__version__) < PYTEST_MIN_VERSION:
+    raise('Your version of pytest is too old, you should have at least '
+          'pytest >= {} installed.'.format(PYTEST_MIN_VERSION))
+
+
+def pytest_addoption(parser):
+    parser.addoption("--skip-network", action="store_true", default=False,
+                     help="skip network tests")
+
 
 def pytest_collection_modifyitems(config, items):
 
@@ -22,19 +35,30 @@ def pytest_collection_modifyitems(config, items):
             if item.name == 'sklearn.feature_extraction.hashing.FeatureHasher':
                 item.add_marker(skip_marker)
 
+    # Skip tests which require internet if the flag is provided
+    if config.getoption("--skip-network"):
+        skip_network = pytest.mark.skip(
+            reason="test requires internet connectivity")
+        for item in items:
+            if "network" in item.keywords:
+                item.add_marker(skip_network)
+
     # numpy changed the str/repr formatting of numpy arrays in 1.14. We want to
-    # run doctests only for numpy >= 1.14.
-    skip_doctests = True
+    # run doctests only for numpy >= 1.14. We want to skip the doctest for
+    # python 2 due to unicode.
+    skip_doctests = False
+    if not PY3_OR_LATER:
+        skip_doctests = True
     try:
         import numpy as np
-        if LooseVersion(np.__version__) >= LooseVersion('1.14'):
-            skip_doctests = False
+        if LooseVersion(np.__version__) < LooseVersion('1.14'):
+            skip_doctests = True
     except ImportError:
         pass
 
     if skip_doctests:
         skip_marker = pytest.mark.skip(
-            reason='doctests are only run for numpy >= 1.14')
+            reason='doctests are only run for numpy >= 1.14 and python >= 3')
 
         for item in items:
             if isinstance(item, DoctestItem):
diff --git a/doc/about.rst b/doc/about.rst
index 90295b96fb..218b0ad897 100755
--- a/doc/about.rst
+++ b/doc/about.rst
@@ -1,7 +1,31 @@
 About us
 ========
 
-.. include:: ../AUTHORS.rst
+History
+-------
+
+This project was started in 2007 as a Google Summer of Code project by
+David Cournapeau. Later that year, Matthieu Brucher started work on
+this project as part of his thesis.
+
+In 2010 Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort and Vincent
+Michel of INRIA took leadership of the project and made the first public
+release, February the 1st 2010. Since then, several releases have appeared
+following a ~3 month cycle, and a thriving international community has
+been leading the development.
+
+Authors
+-------
+
+The following people have been core contributors to scikit-learn's development
+and maintenance:
+
+.. include:: authors.rst
+
+Please do not email the authors directly to ask for assistance or report issues.
+Instead, please see `What's the best way to ask questions about scikit-learn
+<http://scikit-learn.org/stable/faq.html#what-s-the-best-way-to-get-help-on-scikit-learn-usage>`_
+in the FAQ.
 
 .. seealso::
 
diff --git a/doc/authors.rst b/doc/authors.rst
new file mode 100755
index 0000000000..0210dff4be
--- /dev/null
+++ b/doc/authors.rst
@@ -0,0 +1,220 @@
+.. raw :: html
+
+    <!-- Generated by gen_authors.py -->
+    <table>
+    <col style='width:14%' span='7'>
+    <style>
+      img.avatar {border-radius: 10px;}
+      td {vertical-align: top;}
+    </style>
+    <tr>
+    <td>
+    <a href='https://github.com/mblondel'><img src='https://avatars2.githubusercontent.com/u/233706?v=4' class='avatar' /></a> <br />
+    <p>Mathieu Blondel</p>
+    </td>
+    <td>
+    <a href='https://github.com/jorisvandenbossche'><img src='https://avatars2.githubusercontent.com/u/1020496?v=4' class='avatar' /></a> <br />
+    <p>Joris Van den Bossche</p>
+    </td>
+    <td>
+    <a href='https://github.com/mbrucher'><img src='https://avatars1.githubusercontent.com/u/321752?v=4' class='avatar' /></a> <br />
+    <p>Matthieu Brucher</p>
+    </td>
+    <td>
+    <a href='https://github.com/larsmans'><img src='https://avatars1.githubusercontent.com/u/335383?v=4' class='avatar' /></a> <br />
+    <p>Lars Buitinck</p>
+    </td>
+    <td>
+    <a href='https://github.com/cournape'><img src='https://avatars1.githubusercontent.com/u/25111?v=4' class='avatar' /></a> <br />
+    <p>David Cournapeau</p>
+    </td>
+    <td>
+    <a href='https://github.com/ndawe'><img src='https://avatars1.githubusercontent.com/u/202816?v=4' class='avatar' /></a> <br />
+    <p>Noel Dawe</p>
+    </td>
+    <td>
+    <a href='https://github.com/lucidfrontier45'><img src='https://avatars2.githubusercontent.com/u/655305?v=4' class='avatar' /></a> <br />
+    <p>Shiqiao Du</p>
+    </td>
+    </tr>
+    <tr>
+    <td>
+    <a href='https://github.com/dubourg'><img src='https://avatars0.githubusercontent.com/u/401766?v=4' class='avatar' /></a> <br />
+    <p>Vincent Dubourg</p>
+    </td>
+    <td>
+    <a href='https://github.com/duchesnay'><img src='https://avatars1.githubusercontent.com/u/344402?v=4' class='avatar' /></a> <br />
+    <p>Edouard Duchesnay</p>
+    </td>
+    <td>
+    <a href='https://github.com/lesteve'><img src='https://avatars1.githubusercontent.com/u/1680079?v=4' class='avatar' /></a> <br />
+    <p>Loïc Estève</p>
+    </td>
+    <td>
+    <a href='https://github.com/AlexanderFabisch'><img src='https://avatars1.githubusercontent.com/u/869592?v=4' class='avatar' /></a> <br />
+    <p>Alexander Fabisch</p>
+    </td>
+    <td>
+    <a href='https://github.com/VirgileFritsch'><img src='https://avatars3.githubusercontent.com/u/263280?v=4' class='avatar' /></a> <br />
+    <p>Virgile Fritsch</p>
+    </td>
+    <td>
+    <a href='https://github.com/satra'><img src='https://avatars2.githubusercontent.com/u/184063?v=4' class='avatar' /></a> <br />
+    <p>Satrajit Ghosh</p>
+    </td>
+    <td>
+    <a href=''><img src='https://avatars2.githubusercontent.com/u/365630?v=4' class='avatar' /></a> <br />
+    <p>Angel Soler Gollonet</p>
+    </td>
+    </tr>
+    <tr>
+    <td>
+    <a href='https://github.com/chrisfilo'><img src='https://avatars2.githubusercontent.com/u/238759?v=4' class='avatar' /></a> <br />
+    <p>Chris Filo Gorgolewski</p>
+    </td>
+    <td>
+    <a href='https://github.com/agramfort'><img src='https://avatars2.githubusercontent.com/u/161052?v=4' class='avatar' /></a> <br />
+    <p>Alexandre Gramfort</p>
+    </td>
+    <td>
+    <a href='https://github.com/ogrisel'><img src='https://avatars0.githubusercontent.com/u/89061?v=4' class='avatar' /></a> <br />
+    <p>Olivier Grisel</p>
+    </td>
+    <td>
+    <a href='https://github.com/jaquesgrobler'><img src='https://avatars3.githubusercontent.com/u/1378870?v=4' class='avatar' /></a> <br />
+    <p>Jaques Grobler</p>
+    </td>
+    <td>
+    <a href='https://github.com/yarikoptic'><img src='https://avatars3.githubusercontent.com/u/39889?v=4' class='avatar' /></a> <br />
+    <p>Yaroslav Halchenko</p>
+    </td>
+    <td>
+    <a href='https://github.com/bdholt1'><img src='https://avatars0.githubusercontent.com/u/937444?v=4' class='avatar' /></a> <br />
+    <p>Brian Holt</p>
+    </td>
+    <td>
+    <a href='https://github.com/arjoly'><img src='https://avatars0.githubusercontent.com/u/1274722?v=4' class='avatar' /></a> <br />
+    <p>Arnaud Joly</p>
+    </td>
+    </tr>
+    <tr>
+    <td>
+    <a href='https://github.com/thouis'><img src='https://avatars1.githubusercontent.com/u/473043?v=4' class='avatar' /></a> <br />
+    <p>Thouis (Ray) Jones</p>
+    </td>
+    <td>
+    <a href='https://github.com/kastnerkyle'><img src='https://avatars2.githubusercontent.com/u/1563421?v=4' class='avatar' /></a> <br />
+    <p>Kyle Kastner</p>
+    </td>
+    <td>
+    <a href='https://github.com/MechCoder'><img src='https://avatars3.githubusercontent.com/u/1867024?v=4' class='avatar' /></a> <br />
+    <p>Manoj Kumar</p>
+    </td>
+    <td>
+    <a href='https://github.com/robertlayton'><img src='https://avatars2.githubusercontent.com/u/800543?v=4' class='avatar' /></a> <br />
+    <p>Robert Layton</p>
+    </td>
+    <td>
+    <a href='https://github.com/glemaitre'><img src='https://avatars2.githubusercontent.com/u/7454015?v=4' class='avatar' /></a> <br />
+    <p>Guillaume Lemaitre</p>
+    </td>
+    <td>
+    <a href='https://github.com/weilinear'><img src='https://avatars0.githubusercontent.com/u/2232328?v=4' class='avatar' /></a> <br />
+    <p>Wei Li</p>
+    </td>
+    <td>
+    <a href='https://github.com/paolo-losi'><img src='https://avatars1.githubusercontent.com/u/264906?v=4' class='avatar' /></a> <br />
+    <p>Paolo Losi</p>
+    </td>
+    </tr>
+    <tr>
+    <td>
+    <a href='https://github.com/glouppe'><img src='https://avatars3.githubusercontent.com/u/477771?v=4' class='avatar' /></a> <br />
+    <p>Gilles Louppe</p>
+    </td>
+    <td>
+    <a href='https://github.com/jmetzen'><img src='https://avatars1.githubusercontent.com/u/1116263?v=4' class='avatar' /></a> <br />
+    <p>Jan Hendrik Metzen</p>
+    </td>
+    <td>
+    <a href='https://github.com/vmichel'><img src='https://avatars1.githubusercontent.com/u/295195?v=4' class='avatar' /></a> <br />
+    <p>Vincent Michel</p>
+    </td>
+    <td>
+    <a href='https://github.com/jarrodmillman'><img src='https://avatars1.githubusercontent.com/u/123428?v=4' class='avatar' /></a> <br />
+    <p>Jarrod Millman</p>
+    </td>
+    <td>
+    <a href='https://github.com/amueller'><img src='https://avatars3.githubusercontent.com/u/449558?v=4' class='avatar' /></a> <br />
+    <p>Andreas Mueller</p>
+    </td>
+    <td>
+    <a href='https://github.com/vene'><img src='https://avatars0.githubusercontent.com/u/241745?v=4' class='avatar' /></a> <br />
+    <p>Vlad Niculae</p>
+    </td>
+    <td>
+    <a href='https://github.com/jnothman'><img src='https://avatars2.githubusercontent.com/u/78827?v=4' class='avatar' /></a> <br />
+    <p>Joel Nothman</p>
+    </td>
+    </tr>
+    <tr>
+    <td>
+    <a href='https://github.com/alextp'><img src='https://avatars0.githubusercontent.com/u/5061?v=4' class='avatar' /></a> <br />
+    <p>Alexandre Passos</p>
+    </td>
+    <td>
+    <a href='https://github.com/fabianp'><img src='https://avatars3.githubusercontent.com/u/277639?v=4' class='avatar' /></a> <br />
+    <p>Fabian Pedregosa</p>
+    </td>
+    <td>
+    <a href='https://github.com/pprett'><img src='https://avatars0.githubusercontent.com/u/111730?v=4' class='avatar' /></a> <br />
+    <p>Peter Prettenhofer</p>
+    </td>
+    <td>
+    <a href='https://github.com/qinhanmin2014'><img src='https://avatars2.githubusercontent.com/u/12003569?v=4' class='avatar' /></a> <br />
+    <p>Hanmin Qin</p>
+    </td>
+    <td>
+    <a href='https://github.com/raghavrv'><img src='https://avatars3.githubusercontent.com/u/9487348?v=4' class='avatar' /></a> <br />
+    <p>(Venkat) Raghav, Rajagopalan</p>
+    </td>
+    <td>
+    <a href='https://github.com/jmschrei'><img src='https://avatars2.githubusercontent.com/u/3916816?v=4' class='avatar' /></a> <br />
+    <p>Jacob Schreiber</p>
+    </td>
+    <td>
+    <a href='https://github.com/bthirion'><img src='https://avatars1.githubusercontent.com/u/234454?v=4' class='avatar' /></a> <br />
+    <p>Bertrand Thirion</p>
+    </td>
+    </tr>
+    <tr>
+    <td>
+    <a href='https://github.com/TomDLT'><img src='https://avatars2.githubusercontent.com/u/11065596?v=4' class='avatar' /></a> <br />
+    <p>Tom Dupré la Tour</p>
+    </td>
+    <td>
+    <a href='https://github.com/jakevdp'><img src='https://avatars0.githubusercontent.com/u/781659?v=4' class='avatar' /></a> <br />
+    <p>Jake Vanderplas</p>
+    </td>
+    <td>
+    <a href='https://github.com/NelleV'><img src='https://avatars0.githubusercontent.com/u/184798?v=4' class='avatar' /></a> <br />
+    <p>Nelle Varoquaux</p>
+    </td>
+    <td>
+    <a href='https://github.com/GaelVaroquaux'><img src='https://avatars3.githubusercontent.com/u/208217?v=4' class='avatar' /></a> <br />
+    <p>Gael Varoquaux</p>
+    </td>
+    <td>
+    <a href='https://github.com/dwf'><img src='https://avatars1.githubusercontent.com/u/60206?v=4' class='avatar' /></a> <br />
+    <p>David Warde-Farley</p>
+    </td>
+    <td>
+    <a href='https://github.com/ronw'><img src='https://avatars2.githubusercontent.com/u/113819?v=4' class='avatar' /></a> <br />
+    <p>Ron Weiss</p>
+    </td>
+    <td>
+    <a href='https://github.com/rth'><img src='https://avatars0.githubusercontent.com/u/630936?v=4' class='avatar' /></a> <br />
+    <p>Roman Yurchak</p>
+    </td>
+    </tr>
+    </table>
diff --git a/doc/conf.py b/doc/conf.py
index fac3b9fc04..e0dc4c6f4a 100755
--- a/doc/conf.py
+++ b/doc/conf.py
@@ -237,6 +237,7 @@
     'scipy': ('https://docs.scipy.org/doc/scipy/reference', None),
     'matplotlib': ('https://matplotlib.org/', None),
     'pandas': ('https://pandas.pydata.org/pandas-docs/stable/', None),
+    'joblib': ('https://joblib.readthedocs.io/en/latest/', None),
 }
 
 sphinx_gallery_conf = {
diff --git a/doc/datasets/index.rst b/doc/datasets/index.rst
index 2f061aabe8..e0640916fb 100755
--- a/doc/datasets/index.rst
+++ b/doc/datasets/index.rst
@@ -356,83 +356,148 @@ features::
 
     >>> import numpy as np
     >>> import os
-    >>> import tempfile
-    >>> # Create a temporary folder for the data fetcher
-    >>> custom_data_home = tempfile.mkdtemp()
-    >>> os.makedirs(os.path.join(custom_data_home, 'mldata'))
 
+.. _openml:
 
-.. _mldata:
-
-Downloading datasets from the mldata.org repository
+Downloading datasets from the openml.org repository
 ---------------------------------------------------
 
-`mldata.org <http://mldata.org>`_ is a public repository for machine learning
-data, supported by the `PASCAL network <http://www.pascal-network.org>`_ .
-
-The ``sklearn.datasets`` package is able to directly download data
-sets from the repository using the function
-:func:`sklearn.datasets.fetch_mldata`.
-
-For example, to download the MNIST digit recognition database::
-
-  >>> from sklearn.datasets import fetch_mldata
-  >>> mnist = fetch_mldata('MNIST original', data_home=custom_data_home)
-
-The MNIST database contains a total of 70000 examples of handwritten digits
-of size 28x28 pixels, labeled from 0 to 9::
-
-  >>> mnist.data.shape
-  (70000, 784)
-  >>> mnist.target.shape
-  (70000,)
-  >>> np.unique(mnist.target)
-  array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])
-
-After the first download, the dataset is cached locally in the path
-specified by the ``data_home`` keyword argument, which defaults to
-``~/scikit_learn_data/``::
-
-  >>> os.listdir(os.path.join(custom_data_home, 'mldata'))
-  ['mnist-original.mat']
-
-Data sets in `mldata.org <http://mldata.org>`_ do not adhere to a strict
-naming or formatting convention. :func:`sklearn.datasets.fetch_mldata` is
-able to make sense of the most common cases, but allows to tailor the
-defaults to individual datasets:
-
-* The data arrays in `mldata.org <http://mldata.org>`_ are most often
-  shaped as ``(n_features, n_samples)``. This is the opposite of the
-  ``scikit-learn`` convention, so :func:`sklearn.datasets.fetch_mldata`
-  transposes the matrix by default. The ``transpose_data`` keyword controls
-  this behavior::
-
-    >>> iris = fetch_mldata('iris', data_home=custom_data_home)
-    >>> iris.data.shape
-    (150, 4)
-    >>> iris = fetch_mldata('iris', transpose_data=False,
-    ...                     data_home=custom_data_home)
-    >>> iris.data.shape
-    (4, 150)
-
-* For datasets with multiple columns, :func:`sklearn.datasets.fetch_mldata`
-  tries to identify the target and data columns and rename them to ``target``
-  and ``data``. This is done by looking for arrays named ``label`` and
-  ``data`` in the dataset, and failing that by choosing the first array to be
-  ``target`` and the second to be ``data``. This behavior can be changed with
-  the ``target_name`` and ``data_name`` keywords, setting them to a specific
-  name or index number (the name and order of the columns in the datasets
-  can be found at its `mldata.org <http://mldata.org>`_ under the tab "Data"::
-
-    >>> iris2 = fetch_mldata('datasets-UCI iris', target_name=1, data_name=0,
-    ...                      data_home=custom_data_home)
-    >>> iris3 = fetch_mldata('datasets-UCI iris', target_name='class',
-    ...                      data_name='double0', data_home=custom_data_home)
-
-
-..
-    >>> import shutil
-    >>> shutil.rmtree(custom_data_home)
+`openml.org <https://openml.org>`_ is a public repository for machine learning
+data and experiments, that allows everybody to upload open datasets.
+
+The ``sklearn.datasets`` package is able to download datasets
+from the repository using the function
+:func:`sklearn.datasets.fetch_openml`.
+
+For example, to download a dataset of gene expressions in mice brains::
+
+  >>> from sklearn.datasets import fetch_openml
+  >>> mice = fetch_openml(name='miceprotein', version=4)
+
+To fully specify a dataset, you need to provide a name and a version, though
+the version is optional, see :ref:`openml_versions` below.
+The dataset contains a total of 1080 examples belonging to 8 different
+classes::
+
+  >>> mice.data.shape
+  (1080, 77)
+  >>> mice.target.shape
+  (1080,)
+  >>> np.unique(mice.target) # doctest: +NORMALIZE_WHITESPACE
+  array(['c-CS-m', 'c-CS-s', 'c-SC-m', 'c-SC-s', 't-CS-m', 't-CS-s', 't-SC-m', 't-SC-s'], dtype=object)
+
+You can get more information on the dataset by looking at the ``DESCR``
+and ``details`` attributes::
+
+  >>> print(mice.DESCR) # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS +SKIP
+  **Author**: Clara Higuera, Katheleen J. Gardiner, Krzysztof J. Cios
+  **Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression) - 2015
+  **Please cite**: Higuera C, Gardiner KJ, Cios KJ (2015) Self-Organizing
+  Feature Maps Identify Proteins Critical to Learning in a Mouse Model of Down
+  Syndrome. PLoS ONE 10(6): e0129126...
+
+  >>> mice.details # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS +SKIP
+  {'id': '40966', 'name': 'MiceProtein', 'version': '4', 'format': 'ARFF',
+  'upload_date': '2017-11-08T16:00:15', 'licence': 'Public',
+  'url': 'https://www.openml.org/data/v1/download/17928620/MiceProtein.arff',
+  'file_id': '17928620', 'default_target_attribute': 'class',
+  'row_id_attribute': 'MouseID',
+  'ignore_attribute': ['Genotype', 'Treatment', 'Behavior'],
+  'tag': ['OpenML-CC18', 'study_135', 'study_98', 'study_99'],
+  'visibility': 'public', 'status': 'active',
+  'md5_checksum': '3c479a6885bfa0438971388283a1ce32'}
+
+
+The ``DESCR`` contains a free-text description of the data, while ``details``
+contains a dictionary of meta-data stored by openml, like the dataset id.
+For more details, see the `OpenML documentation
+<https://docs.openml.org/#data>`_ The ``data_id`` of the mice protein dataset
+is 40966, and you can use this (or the name) to get more information on the
+dataset on the openml website::
+
+  >>> mice.url
+  'https://www.openml.org/d/40966'
+
+The ``data_id`` also uniquely identifies a dataset from OpenML::
+
+  >>> mice = fetch_openml(data_id=40966)
+  >>> mice.details # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS +SKIP
+  {'id': '4550', 'name': 'MiceProtein', 'version': '1', 'format': 'ARFF',
+  'creator': ...,
+  'upload_date': '2016-02-17T14:32:49', 'licence': 'Public', 'url':
+  'https://www.openml.org/data/v1/download/1804243/MiceProtein.ARFF', 'file_id':
+  '1804243', 'default_target_attribute': 'class', 'citation': 'Higuera C,
+  Gardiner KJ, Cios KJ (2015) Self-Organizing Feature Maps Identify Proteins
+  Critical to Learning in a Mouse Model of Down Syndrome. PLoS ONE 10(6):
+  e0129126. [Web Link] journal.pone.0129126', 'tag': ['OpenML100', 'study_14',
+  'study_34'], 'visibility': 'public', 'status': 'active', 'md5_checksum':
+  '3c479a6885bfa0438971388283a1ce32'}
+
+.. _openml_versions:
+
+Dataset Versions
+~~~~~~~~~~~~~~~~
+
+A dataset is uniquely specified by its ``data_id``, but not necessarily by its
+name. Several different "versions" of a dataset with the same name can exist
+which can contain entirely different datasets.
+If a particular version of a dataset has been found to contain significant
+issues, it might be deactivated. Using a name to specify a dataset will yield
+the earliest version of a dataset that is still active. That means that
+``fetch_openml(name="miceprotein")`` can yield different results at different
+times if earlier versions become inactive.
+You can see that the dataset with ``data_id`` 40966 that we fetched above is
+the version 1 of the "miceprotein" dataset::
+
+  >>> mice.details['version']  #doctest: +SKIP
+  '1'
+
+In fact, this dataset only has one version. The iris dataset on the other hand
+has multiple versions::
+
+  >>> iris = fetch_openml(name="iris")
+  >>> iris.details['version']  #doctest: +SKIP
+  '1'
+  >>> iris.details['id']  #doctest: +SKIP
+  '61'
+
+  >>> iris_61 = fetch_openml(data_id=61)
+  >>> iris_61.details['version']
+  '1'
+  >>> iris_61.details['id']
+  '61'
+
+  >>> iris_969 = fetch_openml(data_id=969)
+  >>> iris_969.details['version']
+  '3'
+  >>> iris_969.details['id']
+  '969'
+
+Specifying the dataset by the name "iris" yields the lowest version, version 1,
+with the ``data_id`` 61. To make sure you always get this exact dataset, it is
+safest to specify it by the dataset ``data_id``. The other dataset, with
+``data_id`` 969, is version 3 (version 2 has become inactive), and contains a
+binarized version of the data::
+
+  >>> np.unique(iris_969.target)
+  array(['N', 'P'], dtype=object)
+
+You can also specify both the name and the version, which also uniquely
+identifies the dataset::
+
+  >>> iris_version_3 = fetch_openml(name="iris", version=3)
+  >>> iris_version_3.details['version']
+  '3'
+  >>> iris_version_3.details['id']
+  '969'
+
+
+.. topic:: References:
+
+ * Vanschoren, van Rijn, Bischl and Torgo
+   `"OpenML: networked science in machine learning"
+   <https://arxiv.org/pdf/1407.7722.pdf>`_,
+   ACM SIGKDD Explorations Newsletter, 15(2), 49-60, 2014.
 
 .. _external_datasets:
 
diff --git a/doc/developers/advanced_installation.rst b/doc/developers/advanced_installation.rst
index 720c11ed98..e146363d0a 100755
--- a/doc/developers/advanced_installation.rst
+++ b/doc/developers/advanced_installation.rst
@@ -50,7 +50,9 @@ Building Scikit-learn also requires
 
 Running tests requires
 
-- pytest
+.. |PytestMinVersion| replace:: 3.3.0
+
+- pytest >=\ |PytestMinVersion|
 
 Some tests also require `pandas <https://pandas.pydata.org>`_.
 
@@ -276,9 +278,8 @@ Testing
 Testing scikit-learn once installed
 -----------------------------------
 
-Testing requires having the `pytest
-<https://docs.pytest.org>`_ library. Some tests also require having
-`pandas <https://pandas.pydata.org/>` installed.
+Testing requires having `pytest <https://docs.pytest.org>`_ >=\ |PytestMinVersion|\ .
+Some tests also require having `pandas <https://pandas.pydata.org/>` installed.
 After installation, the package can be tested by executing *from outside* the
 source directory::
 
diff --git a/doc/developers/contributing.rst b/doc/developers/contributing.rst
index 4888d9205d..31f5e5ef84 100755
--- a/doc/developers/contributing.rst
+++ b/doc/developers/contributing.rst
@@ -79,6 +79,7 @@ link to it from your website, or simply star to say "I use it":
    * `joblib <https://github.com/joblib/joblib/issues>`__
    * `sphinx-gallery <https://github.com/sphinx-gallery/sphinx-gallery/issues>`__
    * `numpydoc <https://github.com/numpy/numpydoc/issues>`__
+   * `liac-arff <https://github.com/renatopp/liac-arff>`__
 
    and larger projects:
 
diff --git a/doc/developers/maintainer.rst b/doc/developers/maintainer.rst
index d0d0db8a04..a3309abcfb 100755
--- a/doc/developers/maintainer.rst
+++ b/doc/developers/maintainer.rst
@@ -1,8 +1,17 @@
 Maintainer / core-developer information
 ========================================
 
+Before a release
+----------------
+
+1. Update authors table::
+
+    $ cd build_tools; make authors; cd ..
+
+   and commit.
+
 Making a release
-------------------
+----------------
 For more information see https://github.com/scikit-learn/scikit-learn/wiki/How-to-make-a-release
 
 
diff --git a/doc/faq.rst b/doc/faq.rst
index bef75f58e1..f6e557ef74 100755
--- a/doc/faq.rst
+++ b/doc/faq.rst
@@ -222,7 +222,10 @@ DBSCAN with Levenshtein distances::
     array([[0],
            [1],
            [2]])
-    >>> dbscan(X, metric=lev_metric, eps=5, min_samples=2)  # doctest: +SKIP
+    >>> # We need to specify algoritum='brute' as the default assumes
+    >>> # a continuous feature space.
+    >>> dbscan(X, metric=lev_metric, eps=5, min_samples=2, algorithm='brute')
+    ... # doctest: +SKIP
     ([0, 1], array([ 0,  0, -1]))
 
 (This uses the third-party edit distance package ``leven``.)
@@ -364,6 +367,7 @@ See also :ref:`sphx_glr_auto_examples_compose_plot_column_transformer_mixed_type
 example of working with heterogeneous (e.g. categorical and numeric) data.
 
 Why does Scikit-learn not directly work with, for example, pandas.DataFrame?
+----------------------------------------------------------------------------
 
 The homogeneous NumPy and SciPy data objects currently expected are most
 efficient to process for most operations. Extensive work would also be needed
diff --git a/doc/glossary.rst b/doc/glossary.rst
index 8a2628e849..4d93696c3a 100755
--- a/doc/glossary.rst
+++ b/doc/glossary.rst
@@ -1479,8 +1479,15 @@ functions or non-estimator constructors.
 
         ``n_jobs`` is an int, specifying the maximum number of concurrently
         running jobs.  If set to -1, all CPUs are used. If 1 is given, no
-        parallel computing code is used at all.  For n_jobs below -1, (n_cpus +
-        1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used.
+        joblib level parallelism is used at all, which is useful for
+        debugging. Even with ``n_jobs = 1``, parallelism may occur due to
+        numerical processing libraries (see :ref:`FAQ <faq_mkl_threading>`).
+        For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for
+        ``n_jobs = -2``, all CPUs but one are used.
+
+        ``n_jobs=None`` means *unset*; it will generally be interpreted as
+        ``n_jobs=1``, unless the current :class:`joblib.Parallel` backend
+        context specifies otherwise.
 
         The use of ``n_jobs``-based parallelism in estimators varies:
 
diff --git a/doc/index.rst b/doc/index.rst
index 0de085b1d7..d97d28f701 100755
--- a/doc/index.rst
+++ b/doc/index.rst
@@ -209,12 +209,10 @@
                     </li>
                     <li><strong>Scikit-learn 0.21 will drop support for Python 2.7 and Python 3.4.</strong>
                     </li>
-                    <li><em>July 2018.</em> scikit-learn 0.20 is available for download (<a href="whats_new.html#version-0-20">Changelog</a>).
+                    <li><em>September 2018.</em> scikit-learn 0.20 is available for download (<a href="whats_new.html#version-0-20">Changelog</a>).
                     </li>
                     <li><em>July 2018.</em> scikit-learn 0.19.2 is available for download (<a href="whats_new.html#version-0-19">Changelog</a>).
                     </li>
-                    <li><em>October 2017.</em> scikit-learn 0.19.1 is available for download (<a href="whats_new.html#version-0-19">Changelog</a>).
-                    </li>
                     <li><em>July 2017.</em> scikit-learn 0.19.0 is available for download (<a href="whats_new/v0.19.html#version-0-19">Changelog</a>).
                     </li>
                     <li><em>June 2017.</em> scikit-learn 0.18.2 is available for download (<a href="whats_new/v0.18.html#version-0-18-2">Changelog</a>).
diff --git a/doc/install.rst b/doc/install.rst
index 7dbb2287c4..bb6b67af3e 100755
--- a/doc/install.rst
+++ b/doc/install.rst
@@ -78,7 +78,7 @@ Canopy and Anaconda for all supported platforms
 
 `Canopy
 <https://www.enthought.com/products/canopy>`_ and `Anaconda
-<https://www.continuum.io/downloads>`_ both ship a recent
+<https://www.anaconda.com/download>`_ both ship a recent
 version of scikit-learn, in addition to a large set of scientific python
 library for Windows, Mac OSX and Linux.
 
diff --git a/doc/modules/classes.rst b/doc/modules/classes.rst
index 44ff2ff0ae..57ccfb5cff 100755
--- a/doc/modules/classes.rst
+++ b/doc/modules/classes.rst
@@ -257,8 +257,8 @@ Loaders
    datasets.fetch_kddcup99
    datasets.fetch_lfw_pairs
    datasets.fetch_lfw_people
-   datasets.fetch_mldata
    datasets.fetch_olivetti_faces
+   datasets.fetch_openml
    datasets.fetch_rcv1
    datasets.fetch_species_distributions
    datasets.get_data_home
@@ -1512,6 +1512,7 @@ To be removed in 0.22
    :template: deprecated_function.rst
 
    covariance.graph_lasso
+   datasets.fetch_mldata
 
 
 To be removed in 0.21
diff --git a/doc/modules/clustering.rst b/doc/modules/clustering.rst
index 968a66e67f..1f8210f35f 100755
--- a/doc/modules/clustering.rst
+++ b/doc/modules/clustering.rst
@@ -838,9 +838,9 @@ algorithm builds a *reachability* graph, which assigns each sample both a
 ``reachability_`` distance, and a spot within the cluster ``ordering_``
 attribute; these two attributes are assigned when the model is fitted, and are
 used to determine cluster membership. If OPTICS is run with the default value
-of *inf* set for ``max_bound``, then DBSCAN style cluster extraction can be
+of *inf* set for ``max_eps``, then DBSCAN style cluster extraction can be
 performed in linear time for any given ``eps`` value using the
-``extract_dbscan`` method. Setting ``max_bound`` to a lower value will result
+``extract_dbscan`` method. Setting ``max_eps`` to a lower value will result
 in shorter run times, and can be thought of as the maximum cluster object size
 (in diameter) that OPTICS will be able to extract.
 
@@ -892,10 +892,10 @@ larger parent cluster.
     shorter run time than OPTICS; however, for repeated runs at varying ``eps``
     values, a single run of OPTICS may require less cumulative runtime than
     DBSCAN. It is also important to note that OPTICS output can be unstable at
-    ``eps`` values very close to the initial ``max_bound`` value. OPTICS seems
+    ``eps`` values very close to the initial ``max_eps`` value. OPTICS seems
     to produce near identical results to DBSCAN provided that ``eps`` passed to
     ``extract_dbscan`` is a half order of magnitude less than the inital
-    ``max_bound`` that was used to fit; using a value close to ``max_bound``
+    ``max_eps`` that was used to fit; using a value close to ``max_eps``
     will throw a warning, and using a value larger will result in an exception. 
 
 .. topic:: Computational Complexity
@@ -909,7 +909,7 @@ larger parent cluster.
     multithreaded, and has better algorithmic runtime complexity than OPTICS--
     at the cost of worse memory scaling. For extremely large datasets that
     exhaust system memory using HDBSCAN, OPTICS will maintain *n* (as opposed
-    to *n^2* memory scaling); however, tuning of the ``max_bound`` parameter
+    to *n^2* memory scaling); however, tuning of the ``max_eps`` parameter
     will likely need to be used to give a solution in a reasonable amount of
     wall time.
 
diff --git a/doc/modules/compose.rst b/doc/modules/compose.rst
index 5a291bfaeb..663ca40b8c 100755
--- a/doc/modules/compose.rst
+++ b/doc/modules/compose.rst
@@ -353,13 +353,13 @@ Like pipelines, feature unions have a shorthand constructor called
 
 
 Like ``Pipeline``, individual steps may be replaced using ``set_params``,
-and ignored by setting to ``None``::
+and ignored by setting to ``'drop'``::
 
-    >>> combined.set_params(kernel_pca=None)
+    >>> combined.set_params(kernel_pca='drop')
     ... # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS
     FeatureUnion(n_jobs=None,
                  transformer_list=[('linear_pca', PCA(copy=True,...)),
-                                   ('kernel_pca', None)],
+                                   ('kernel_pca', 'drop')],
                  transformer_weights=None)
 
 .. topic:: Examples:
diff --git a/doc/modules/ensemble.rst b/doc/modules/ensemble.rst
index 85fd8a30ba..5399f13dbc 100755
--- a/doc/modules/ensemble.rst
+++ b/doc/modules/ensemble.rst
@@ -218,7 +218,7 @@ setting ``oob_score=True``.
     The size of the model with the default parameters is :math:`O( M * N * log (N) )`,
     where :math:`M` is the number of trees and :math:`N` is the number of samples.
     In order to reduce the size of the model, you can change these parameters:
-    ``min_samples_split``, ``min_samples_leaf``, ``max_leaf_nodes`` and ``max_depth``.
+    ``min_samples_split``, ``max_leaf_nodes``, ``max_depth`` and ``min_samples_leaf``.
 
 Parallelization
 ---------------
@@ -260,11 +260,16 @@ respect to the predictability of the target variable. Features used at
 the top of the tree contribute to the final prediction decision of a
 larger fraction of the input samples. The **expected fraction of the
 samples** they contribute to can thus be used as an estimate of the
-**relative importance of the features**.
+**relative importance of the features**. In scikit-learn, the fraction of
+samples a feature contributes to is combined with the decrease in impurity
+from splitting them to create a normalized estimate of the predictive power
+of that feature.
 
-By **averaging** those expected activity rates over several randomized
+By **averaging** the estimates of predictive ability over several randomized
 trees one can **reduce the variance** of such an estimate and use it
-for feature selection.
+for feature selection. This is known as the mean decrease in impurity, or MDI.
+Refer to [L2014]_ for more information on MDI and feature importance
+evaluation with Random Forests.
 
 The following example shows a color-coded representation of the relative
 importances of each individual pixel for a face recognition task using
@@ -288,6 +293,12 @@ to the prediction function.
 
 .. _random_trees_embedding:
 
+.. topic:: References
+
+ .. [L2014] G. Louppe,
+         "Understanding Random Forests: From Theory to Practice",
+         PhD Thesis, U. of Liege, 2014.
+
 Totally Random Trees Embedding
 ------------------------------
 
@@ -383,8 +394,7 @@ the final combination. By default, weak learners are decision stumps. Different
 weak learners can be specified through the ``base_estimator`` parameter.
 The main parameters to tune to obtain good results are ``n_estimators`` and
 the complexity of the base estimators (e.g., its depth ``max_depth`` or
-minimum required number of samples at a leaf ``min_samples_leaf`` in case of
-decision trees).
+minimum required number of samples to consider a split ``min_samples_split``).
 
 .. topic:: Examples:
 
@@ -964,7 +974,8 @@ The following example shows how to fit the majority rule classifier::
    >>> iris = datasets.load_iris()
    >>> X, y = iris.data[:, 1:3], iris.target
 
-   >>> clf1 = LogisticRegression(random_state=1)
+   >>> clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial',
+   ...                           random_state=1)
    >>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)
    >>> clf3 = GaussianNB()
 
@@ -973,10 +984,10 @@ The following example shows how to fit the majority rule classifier::
    >>> for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):
    ...     scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')
    ...     print("Accuracy: %0.2f (+/- %0.2f) [%s]" % (scores.mean(), scores.std(), label))
-   Accuracy: 0.90 (+/- 0.05) [Logistic Regression]
+   Accuracy: 0.95 (+/- 0.04) [Logistic Regression]
    Accuracy: 0.94 (+/- 0.04) [Random Forest]
    Accuracy: 0.91 (+/- 0.04) [naive Bayes]
-   Accuracy: 0.95 (+/- 0.05) [Ensemble]
+   Accuracy: 0.95 (+/- 0.04) [Ensemble]
 
 
 Weighted Average Probabilities (Soft Voting)
@@ -1049,7 +1060,8 @@ The `VotingClassifier` can also be used together with `GridSearch` in order
 to tune the hyperparameters of the individual estimators::
 
    >>> from sklearn.model_selection import GridSearchCV
-   >>> clf1 = LogisticRegression(random_state=1)
+   >>> clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial',
+   ...                           random_state=1)
    >>> clf2 = RandomForestClassifier(random_state=1)
    >>> clf3 = GaussianNB()
    >>> eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')
diff --git a/doc/modules/feature_extraction.rst b/doc/modules/feature_extraction.rst
index 97001f37fb..b3867373cb 100755
--- a/doc/modules/feature_extraction.rst
+++ b/doc/modules/feature_extraction.rst
@@ -919,7 +919,7 @@ concepts may not map one-to-one onto Lucene concepts.)
 
 To make the preprocessor, tokenizer and analyzers aware of the model
 parameters it is possible to derive from the class and override the
-``build_preprocessor``, ``build_tokenizer``` and ``build_analyzer``
+``build_preprocessor``, ``build_tokenizer`` and ``build_analyzer``
 factory methods instead of passing custom functions.
 
 Some tips and tricks:
diff --git a/doc/modules/lda_qda.rst b/doc/modules/lda_qda.rst
index 3d45dd78f3..e1dfb0c03e 100755
--- a/doc/modules/lda_qda.rst
+++ b/doc/modules/lda_qda.rst
@@ -15,7 +15,7 @@ surface, respectively.
 
 These classifiers are attractive because they have closed-form solutions that
 can be easily computed, are inherently multiclass, have proven to work well in
-practice and have no hyperparameters to tune.
+practice, and have no hyperparameters to tune.
 
 .. |ldaqda| image:: ../auto_examples/classification/images/sphx_glr_plot_lda_qda_001.png
         :target: ../auto_examples/classification/plot_lda_qda.html
@@ -43,7 +43,7 @@ linear subspace consisting of the directions which maximize the separation
 between classes (in a precise sense discussed in the mathematics section
 below). The dimension of the output is necessarily less than the number of
 classes, so this is, in general, a rather strong dimensionality reduction, and
-only makes senses in a multiclass setting.
+only makes sense in a multiclass setting.
 
 This is implemented in
 :func:`discriminant_analysis.LinearDiscriminantAnalysis.transform`. The desired
@@ -70,10 +70,10 @@ the class conditional distribution of the data :math:`P(X|y=k)` for each class
 and we select the class :math:`k` which maximizes this conditional probability.
 
 More specifically, for linear and quadratic discriminant analysis,
-:math:`P(X|y)` is modelled as a multivariate Gaussian distribution with
+:math:`P(X|y)` is modeled as a multivariate Gaussian distribution with
 density:
 
-.. math:: P(X | y=k) = \frac{1}{(2\pi)^{d/2} |\Sigma_k|^{1/2}}\exp\left(-\frac{1}{2} (X-\mu_k)^t \Sigma_k^{-1} (X-\mu_k)\right) 
+.. math:: P(X | y=k) = \frac{1}{(2\pi)^{d/2} |\Sigma_k|^{1/2}}\exp\left(-\frac{1}{2} (X-\mu_k)^t \Sigma_k^{-1} (X-\mu_k)\right)
 
 where :math:`d` is the number of features.
 
@@ -85,7 +85,7 @@ matrices, or by a regularized estimator: see the section on shrinkage below).
 
 In the case of LDA, the Gaussians for each class are assumed to share the same
 covariance matrix: :math:`\Sigma_k = \Sigma` for all :math:`k`. This leads to
-linear decision surfaces between, as can be seen by comparing the
+linear decision surfaces, which can be seen by comparing the
 log-probability ratios :math:`\log[P(y=k | X) / P(y=l | X)]`:
 
 .. math::
@@ -127,7 +127,7 @@ classifier, there is a dimensionality reduction by linear projection onto a
 :math:`K-1` dimensional space.
 
 We can reduce the dimension even more, to a chosen :math:`L`, by projecting
-onto the linear subspace :math:`H_L` which maximize the variance of the
+onto the linear subspace :math:`H_L` which maximizes the variance of the
 :math:`\mu^*_k` after projection (in effect, we are doing a form of PCA for the
 transformed class means :math:`\mu^*_k`). This :math:`L` corresponds to the
 ``n_components`` parameter used in the
diff --git a/doc/modules/linear_model.rst b/doc/modules/linear_model.rst
index 968cc663f9..ab6b299483 100755
--- a/doc/modules/linear_model.rst
+++ b/doc/modules/linear_model.rst
@@ -775,15 +775,20 @@ The "saga" solver [7]_ is a variant of "sag" that also supports the
 non-smooth `penalty="l1"` option. This is therefore the solver of choice
 for sparse multinomial logistic regression.
 
-In a nutshell, one may choose the solver with the following rules:
-
-=================================  =====================================
-Case                               Solver
-=================================  =====================================
-L1 penalty                         "liblinear" or "saga"
-Multinomial loss                   "lbfgs", "sag", "saga" or "newton-cg"
-Very Large dataset (`n_samples`)   "sag" or "saga"
-=================================  =====================================
+In a nutshell, the following table summarizes the solvers characteristics:
+
+============================   ===========  =======  ===========  =====  ======
+solver                         'liblinear'  'lbfgs'  'newton-cg'  'sag'  'saga'
+============================   ===========  =======  ===========  =====  ======
+Multinomial + L2 penalty       no           yes      yes          yes    yes
+OVR + L2 penalty               yes          yes      yes          yes    yes
+Multinomial + L1 penalty       no           no       no           no     yes
+OVR + L1 penalty               yes          no       no           no     yes
+============================   ===========  =======  ===========  =====  ======
+Penalize the intercept (bad)   yes          no       no           no     no
+Faster for large datasets      no           no       no           yes    yes
+Robust to unscaled datasets    yes          yes      yes          no     no
+============================   ===========  =======  ===========  =====  ======
 
 The "saga" solver is often the best choice. The "liblinear" solver is
 used by default for historical reasons.
diff --git a/doc/modules/model_persistence.rst b/doc/modules/model_persistence.rst
index f5173e5d9f..ccf5755c1c 100755
--- a/doc/modules/model_persistence.rst
+++ b/doc/modules/model_persistence.rst
@@ -35,7 +35,7 @@ persistence model, namely `pickle <https://docs.python.org/2/library/pickle.html
   >>> y[0]
   0
 
-In the specific case of scikit-learn, it may be more interesting to use
+In the specific case of scikit-learn, it may be better to use
 joblib's replacement of pickle (``joblib.dump`` & ``joblib.load``),
 which is more efficient on objects that carry large numpy arrays internally as
 is often the case for fitted scikit-learn estimators, but can only pickle to the
diff --git a/doc/modules/neural_networks_unsupervised.rst b/doc/modules/neural_networks_unsupervised.rst
index 262eba614c..914ef3d1f0 100755
--- a/doc/modules/neural_networks_unsupervised.rst
+++ b/doc/modules/neural_networks_unsupervised.rst
@@ -59,8 +59,8 @@ The energy function measures the quality of a joint assignment:
 
 .. math:: 
 
-   E(\mathbf{v}, \mathbf{h}) = \sum_i \sum_j w_{ij}v_ih_j + \sum_i b_iv_i
-     + \sum_j c_jh_j
+   E(\mathbf{v}, \mathbf{h}) = -\sum_i \sum_j w_{ij}v_ih_j - \sum_i b_iv_i
+     - \sum_j c_jh_j
 
 In the formula above, :math:`\mathbf{b}` and :math:`\mathbf{c}` are the
 intercept vectors for the visible and hidden layers, respectively. The
diff --git a/doc/modules/outlier_detection.rst b/doc/modules/outlier_detection.rst
index 6f7f658521..3482d4246c 100755
--- a/doc/modules/outlier_detection.rst
+++ b/doc/modules/outlier_detection.rst
@@ -8,9 +8,9 @@ Novelty and Outlier Detection
 
 Many applications require being able to decide whether a new observation
 belongs to the same distribution as existing observations (it is an
-`inlier`), or should be considered as different (it is an outlier).
+*inlier*), or should be considered as different (it is an *outlier*).
 Often, this ability is used to clean real data sets. Two important
-distinction must be made:
+distinctions must be made:
 
 :outlier detection:
   The training data contains outliers which are defined as observations that
@@ -35,7 +35,7 @@ a low density region of the training data, considered as normal in this
 context.
 
 The scikit-learn project provides a set of machine learning tools that
-can be used both for novelty or outliers detection. This strategy is
+can be used both for novelty or outlier detection. This strategy is
 implemented with objects learning in an unsupervised way from the data::
 
     estimator.fit(X_train)
@@ -77,6 +77,18 @@ not available.
   The scores of abnormality of the training samples are always accessible
   through the ``negative_outlier_factor_`` attribute.
 
+The behavior of :class:`neighbors.LocalOutlierFactor` is summarized in the
+following table.
+
+===================== ================================ =====================
+Method                Outlier detection                Novelty detection
+===================== ================================ =====================
+``fit_predict``       OK                               Not available
+``predict``           Not available                    Use only on new data
+``decision_function`` Not available                    Use only on new data
+``score_samples``     Use ``negative_outlier_factor_`` Use only on new data
+===================== ================================ =====================
+
 
 Overview of outlier detection methods
 =====================================
@@ -162,7 +174,7 @@ Outlier Detection
 
 Outlier detection is similar to novelty detection in the sense that
 the goal is to separate a core of regular observations from some
-polluting ones, called "outliers". Yet, in the case of outlier
+polluting ones, called *outliers*. Yet, in the case of outlier
 detection, we don't have a clean data set representing the population
 of regular observations that can be used to train any tool.
 
@@ -306,10 +318,10 @@ This strategy is illustrated below.
 .. topic:: Examples:
 
    * See :ref:`sphx_glr_auto_examples_neighbors_plot_lof_outlier_detection.py`
-   for an illustration of the use of :class:`neighbors.LocalOutlierFactor`.
+     for an illustration of the use of :class:`neighbors.LocalOutlierFactor`.
 
    * See :ref:`sphx_glr_auto_examples_plot_anomaly_comparison.py` for a
-   comparison with other anomaly detection methods.
+     comparison with other anomaly detection methods.
 
 .. topic:: References:
 
@@ -341,19 +353,7 @@ Note that ``fit_predict`` is not available in this case.
   The scores of abnormality of the training samples are always accessible
   through the ``negative_outlier_factor_`` attribute.
 
-The behavior of LOF is summarized in the following table.
-
-====================  ================================  =====================
-Method                Outlier detection                 Novelty detection
-====================  ================================  =====================
-`fit_predict`         OK                                Not available
-`predict`             Not available                     Use only on test data
-`decision_function`   Not available                     Use only on test data
-`score_samples`       Use `negative_outlier_factor_`    Use only on test data
-====================  ================================  =====================
-
-
-This strategy is illustrated below.
+Novelty detection with Local Outlier Factor is illustrated below.
 
   .. figure:: ../auto_examples/neighbors/images/sphx_glr_plot_lof_novelty_detection_001.png
      :target: ../auto_examples/neighbors/sphx_glr_plot_lof_novelty_detection.html
diff --git a/doc/modules/svm.rst b/doc/modules/svm.rst
index bd065c14f7..4429dd8b13 100755
--- a/doc/modules/svm.rst
+++ b/doc/modules/svm.rst
@@ -164,11 +164,12 @@ Each row of the coefficients corresponds to one of the ``n_class`` many
 order of the "one" class.
 
 In the case of "one-vs-one" :class:`SVC`, the layout of the attributes
-is a little more involved. In the case of having a linear kernel,
-The layout of ``coef_`` and ``intercept_`` is similar to the one
-described for :class:`LinearSVC` described above, except that the shape of
-``coef_`` is ``[n_class * (n_class - 1) / 2, n_features]``, corresponding to as
-many binary classifiers. The order for classes
+is a little more involved. In the case of having a linear kernel, the
+attributes ``coef_`` and ``intercept_`` have the shape
+``[n_class * (n_class - 1) / 2, n_features]`` and
+``[n_class * (n_class - 1) / 2]`` respectively. This is similar to the
+layout for :class:`LinearSVC` described above, with each row now corresponding
+to a binary classifier. The order for classes
 0 to n is "0 vs 1", "0 vs 2" , ... "0 vs n", "1 vs 2", "1 vs 3", "1 vs n", . .
 . "n-1 vs n".
 
diff --git a/doc/modules/tree.rst b/doc/modules/tree.rst
index 91b58b031b..86f8b2f6fa 100755
--- a/doc/modules/tree.rst
+++ b/doc/modules/tree.rst
@@ -330,15 +330,17 @@ Tips on practical use
     for each additional level the tree grows to.  Use ``max_depth`` to control
     the size of the tree to prevent overfitting.
 
-  * Use ``min_samples_split`` or ``min_samples_leaf`` to control the number of
-    samples at a leaf node.  A very small number will usually mean the tree
-    will overfit, whereas a large number will prevent the tree from learning
-    the data. Try ``min_samples_leaf=5`` as an initial value. If the sample size
-    varies greatly, a float number can be used as percentage in these two parameters.
-    The main difference between the two is that ``min_samples_leaf`` guarantees
-    a minimum number of samples in a leaf, while ``min_samples_split`` can
-    create arbitrary small leaves, though ``min_samples_split`` is more common
-    in the literature.
+  * Use ``min_samples_split`` or ``min_samples_leaf`` to ensure that multiple
+    samples inform every decision in the tree, by controlling which splits will
+    be considered. A very small number will usually mean the tree will overfit,
+    whereas a large number will prevent the tree from learning the data. Try
+    ``min_samples_leaf=5`` as an initial value. If the sample size varies
+    greatly, a float number can be used as percentage in these two parameters.
+    While ``min_samples_split`` can create arbitrarily small leaves,
+    ``min_samples_leaf`` guarantees that each leaf has a minimum size, avoiding
+    low-variance, over-fit leaf nodes in regression problems.  For
+    classification with few classes, ``min_samples_leaf=1`` is often the best
+    choice.
 
   * Balance your dataset before training to prevent the tree from being biased
     toward the classes that are dominant. Class balancing can be done by
diff --git a/doc/related_projects.rst b/doc/related_projects.rst
index 9e5d5a32c0..ce5f5c24db 100755
--- a/doc/related_projects.rst
+++ b/doc/related_projects.rst
@@ -183,7 +183,10 @@ and tasks.
 
 - `multiisotonic <https://github.com/alexfields/multiisotonic>`_ Isotonic
   regression on multidimensional features.
-  
+
+- `scikit-multilearn <https://scikit.ml>`_ Multi-label classification with 
+  focus on label space manipulation.
+
 - `seglearn <https://github.com/dmbee/seglearn>`_ Time series and sequence 
   learning using sliding window segmentation.
 
diff --git a/doc/themes/scikit-learn/layout.html b/doc/themes/scikit-learn/layout.html
index 79ddd08093..21136856aa 100755
--- a/doc/themes/scikit-learn/layout.html
+++ b/doc/themes/scikit-learn/layout.html
@@ -340,17 +340,13 @@ <h2>Machine Learning in Python</h2>
      </div>
 
     {% if theme_google_analytics|tobool %}
-    <script type="text/javascript">
-      var _gaq = _gaq || [];
-      _gaq.push(['_setAccount', 'UA-22606712-2']);
-      _gaq.push(['_trackPageview']);
-
-      (function() {
-        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
-        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
-        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
-      })();
+    <script>
+        window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
+        ga('create', 'UA-22606712-2', 'auto');
+        ga('set', 'anonymizeIp', true);
+        ga('send', 'pageview');
     </script>
+    <script async src='https://www.google-analytics.com/analytics.js'></script>
     {% endif %}
     <script>
       (function() {
diff --git a/doc/tutorial/basic/tutorial.rst b/doc/tutorial/basic/tutorial.rst
index 781495df99..18189ee385 100755
--- a/doc/tutorial/basic/tutorial.rst
+++ b/doc/tutorial/basic/tutorial.rst
@@ -318,8 +318,8 @@ Refitting and updating parameters
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 Hyper-parameters of an estimator can be updated after it has been constructed
-via the :func:`sklearn.pipeline.Pipeline.set_params` method. Calling ``fit()``
-more than once will overwrite what was learned by any previous ``fit()``::
+via the :term:`set_params()<set_params>` method. Calling ``fit()`` more than
+once will overwrite what was learned by any previous ``fit()``::
 
   >>> import numpy as np
   >>> from sklearn.svm import SVC
@@ -346,9 +346,10 @@ more than once will overwrite what was learned by any previous ``fit()``::
   >>> clf.predict(X_test)
   array([1, 0, 1, 1, 0])
 
-Here, the default kernel ``rbf`` is first changed to ``linear`` after the
-estimator has been constructed via ``SVC()``, and changed back to ``rbf`` to
-refit the estimator and to make a second prediction.
+Here, the default kernel ``rbf`` is first changed to ``linear`` via
+:func:`SVC.set_params()<sklearn.svm.SVC.set_params>` after the estimator has
+been constructed, and changed back to ``rbf`` to refit the estimator and to
+make a second prediction.
 
 Multiclass vs. multilabel fitting
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
diff --git a/doc/tutorial/statistical_inference/supervised_learning.rst b/doc/tutorial/statistical_inference/supervised_learning.rst
index e60751b7f6..2c12f1038c 100755
--- a/doc/tutorial/statistical_inference/supervised_learning.rst
+++ b/doc/tutorial/statistical_inference/supervised_learning.rst
@@ -374,12 +374,13 @@ function or **logistic** function:
 
 ::
 
-    >>> logistic = linear_model.LogisticRegression(C=1e5)
-    >>> logistic.fit(iris_X_train, iris_y_train)
+    >>> log = linear_model.LogisticRegression(solver='lbfgs', C=1e5,
+    ...                                       multi_class='multinomial')
+    >>> log.fit(iris_X_train, iris_y_train)  # doctest: +NORMALIZE_WHITESPACE
     LogisticRegression(C=100000.0, class_weight=None, dual=False,
-              fit_intercept=True, intercept_scaling=1, max_iter=100,
-              multi_class='ovr', n_jobs=None, penalty='l2', random_state=None,
-              solver='liblinear', tol=0.0001, verbose=0, warm_start=False)
+        fit_intercept=True, intercept_scaling=1, max_iter=100,
+        multi_class='multinomial', n_jobs=None, penalty='l2', random_state=None,
+        solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)
 
 This is known as :class:`LogisticRegression`.
 
diff --git a/doc/tutorial/text_analytics/working_with_text_data.rst b/doc/tutorial/text_analytics/working_with_text_data.rst
index 32a218bc28..589d83006f 100755
--- a/doc/tutorial/text_analytics/working_with_text_data.rst
+++ b/doc/tutorial/text_analytics/working_with_text_data.rst
@@ -441,7 +441,7 @@ parameter combinations in parallel with the ``n_jobs`` parameter. If we give
 this parameter a value of ``-1``, grid search will detect how many cores
 are installed and use them all::
 
-  >>> gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)
+  >>> gs_clf = GridSearchCV(text_clf, parameters, cv=5, iid=False, n_jobs=-1)
 
 The grid search instance behaves like a normal ``scikit-learn``
 model. Let's perform the search on a smaller subset of the training data
@@ -465,7 +465,7 @@ mean score and the parameters setting corresponding to that score::
   ...
   clf__alpha: 0.001
   tfidf__use_idf: True
-  vect__ngram_range: (1, 1)
+  vect__ngram_range: (1, 2)
 
 A more detailed summary of the search is available at ``gs_clf.cv_results_``.
 
diff --git a/doc/whats_new.rst b/doc/whats_new.rst
index 0e7345836f..03cbcd3ed3 100755
--- a/doc/whats_new.rst
+++ b/doc/whats_new.rst
@@ -11,8 +11,8 @@ Release notes for current and recent releases are detailed on this page, with
 **Tip:** `Subscribe to scikit-learn releases <https://libraries.io/pypi/scikit-learn>`__
 on libraries.io to be notified when new versions are released.
 
+.. include:: whats_new/v0.21.rst
 .. include:: whats_new/v0.20.rst
-.. include:: whats_new/v0.19.rst
 
 .. _previous_releases_whats_new:
 
@@ -21,6 +21,7 @@ Previous Releases
 .. toctree::
     :maxdepth: 1
 
+    Version 0.19 <whats_new/v0.19.rst>
     Version 0.18 <whats_new/v0.18.rst>
     Version 0.17 <whats_new/v0.17.rst>
     Version 0.16 <whats_new/v0.16.rst>
diff --git a/doc/whats_new/v0.20.rst b/doc/whats_new/v0.20.rst
index cca201c3ab..e5c9bf76cd 100755
--- a/doc/whats_new/v0.20.rst
+++ b/doc/whats_new/v0.20.rst
@@ -4,8 +4,10 @@
 
 .. _changes_0_20:
 
-Version 0.20 (under development)
-================================
+Version 0.20.0
+==============
+
+**September, 2018**
 
 This release packs in a mountain of bug fixes, features and enhancements for
 the Scikit-learn library, and improvements to the documentation and examples.
@@ -62,6 +64,7 @@ occurs due to changes in the modelling logic (bug fixes or enhancements), or in
 random sampling procedures.
 
 - :class:`cluster.DBSCAN` (bug fix)
+- :class:`cluster.MeanShift` (bug fix)
 - :class:`decomposition.IncrementalPCA` in Python 2 (bug fix)
 - :class:`decomposition.SparsePCA` (bug fix)
 - :class:`ensemble.GradientBoostingClassifier` (bug fix affecting feature importances)
@@ -88,6 +91,15 @@ Details are listed in the changelog below.
 (While we are trying to better inform users by providing this information, we
 cannot assure that this list is complete.)
 
+Known Major Bugs
+----------------
+
+* :issue:`11924`: :class:`LogisticRegressionCV` with `solver='lbfgs'` and
+  `multi_class='multinomial'` may be non-deterministic or otherwise broken on
+  macOS. This appears to be the case on Travis CI servers, but has not been
+  confirmed on personal MacBooks! This issue has been present in previous
+  releases.
+
 Changelog
 ---------
 
@@ -97,10 +109,6 @@ Support for Python 3.3 has been officially dropped.
 :mod:`sklearn.cluster`
 ......................
 
-- |MajorFeature| A new clustering algorithm: :class:`cluster.OPTICS`: an
-  algoritm related to :class:`cluster.DBSCAN`, that has hyperparameters easier
-  to set and tat scales better, by :user:`Shane <espg>`.
-
 - |MajorFeature| :class:`cluster.AgglomerativeClustering` now supports Single
   Linkage clustering via ``linkage='single'``. :issue:`9372` by :user:`Leland
   McInnes <lmcinnes>` and :user:`Steve Astels <sastels>`.
@@ -118,7 +126,7 @@ Support for Python 3.3 has been officially dropped.
   regardless of ``algorithm``.
   :issue:`8003` by :user:`Joël Billaud <recamshak>`.
 
-- |Enhancement| :class:`cluster.KMeans` now gives a warning, if the number of
+- |Enhancement| :class:`cluster.KMeans` now gives a warning if the number of
   distinct clusters found is smaller than ``n_clusters``. This may occur when
   the number of distinct points in the data set is actually smaller than the
   number of cluster one is looking for.
@@ -137,14 +145,19 @@ Support for Python 3.3 has been officially dropped.
   and :user:`Devansh D. <devanshdalal>`.
 
 - |Fix| Fixed a bug in :func:`cluster.k_means_elkan` where the returned
-  `iteration` was 1 less than the correct value. Also added the missing
-  `n_iter_` attribute in the docstring of :class:`cluster.KMeans`.
+  ``iteration`` was 1 less than the correct value. Also added the missing
+  ``n_iter_`` attribute in the docstring of :class:`cluster.KMeans`.
   :issue:`11353` by :user:`Jeremie du Boisberranger <jeremiedbb>`.
 
 - |Fix| Fixed a bug in :class:`cluster.DBSCAN` with precomputed sparse neighbors
   graph, which would add explicitly zeros on the diagonal even when already
   present. :issue:`10482` by `Tom Dupre la Tour`_.
 
+- |Fix| Fixed a bug in :func:`cluster.mean_shift` where the assigned labels
+  were not deterministic if there were multiple clusters with the same
+  intensities.
+  :issue:`11901` by :user:`Adrin Jalali <adrinjalali>`.
+
 - |API| Deprecate ``pooling_func`` unused parameter in
   :class:`cluster.AgglomerativeClustering`.
   :issue:`9875` by :user:`Kumar Ashutosh <thechargedneutron>`.
@@ -166,9 +179,13 @@ Support for Python 3.3 has been officially dropped.
   by `Andreas Müller`_ and :user:`Guillaume Lemaitre <glemaitre>`.
 
 
+
 :mod:`sklearn.covariance`
 .........................
 
+- |Efficiency| Runtime improvements to :class:`covariance.GraphicalLasso`.
+  :issue:`9858` by :user:`Steven Brown <stevendbrown>`.
+
 - |API| The :func:`covariance.graph_lasso`,
   :class:`covariance.GraphLasso` and :class:`covariance.GraphLassoCV` have been
   renamed to :func:`covariance.graphical_lasso`,
@@ -180,8 +197,13 @@ Support for Python 3.3 has been officially dropped.
 :mod:`sklearn.datasets`
 .......................
 
+- |MajorFeature| Added :func:`datasets.fetch_openml` to fetch datasets from
+  `OpenML <http://openml.org>`_. OpenML is a free, open data sharing platform
+  and will be used instead of mldata as it provides better service availability.
+  :issue:`9908` by `Andreas Müller`_ and :user:`Jan N. van Rijn <janvanrijn>`.
+
 - |Feature| In :func:`datasets.make_blobs`, one can now pass a list to the
-  `n_samples` parameter to indicate the number of samples to generate per
+  ``n_samples`` parameter to indicate the number of samples to generate per
   cluster. :issue:`8617` by :user:`Maskani Filali Mohamed <maskani-moh>` and
   :user:`Konstantinos Katrioplas <kkatrio>`.
 
@@ -189,8 +211,11 @@ Support for Python 3.3 has been officially dropped.
   :issue:`9101` by :user:`alex-33 <alex-33>`
   and :user:`Maskani Filali Mohamed <maskani-moh>`.
 
+- |Feature| ``return_X_y`` parameter has been added to several dataset loaders.
+  :issue:`10774` by :user:`Chris Catalfo <ccatalfo>`.
+
 - |Fix| Fixed a bug in :func:`datasets.load_boston` which had a wrong data
-  point. :issue:`10801` by :user:`Takeshi Yoshizawa <tarcusx>`.
+  point. :issue:`10795` by :user:`Takeshi Yoshizawa <tarcusx>`.
 
 - |Fix| Fixed a bug in :func:`datasets.load_iris` which had two wrong data points.
   :issue:`11082` by :user:`Sadhana Srinivasan <rotuna>`
@@ -200,9 +225,12 @@ Support for Python 3.3 has been officially dropped.
   properly shuffled. :issue:`9731` by `Nicolas Goix`_.
 
 - |Fix| Fixed a bug in :func:`datasets.make_circles`, where no odd number of
-  data points could be generated. :issue:`10037` by :user:`Christian Braune
+  data points could be generated. :issue:`10045` by :user:`Christian Braune
   <christianbraune79>`.
 
+- |API| Deprecated :func:`sklearn.datasets.fetch_mldata` to be removed in
+  version 0.22. mldata.org is no longer operational. Until removal it will
+  remain possible to load cached datasets. :issue:`11466` by `Joel Nothman`_.
 
 :mod:`sklearn.decomposition`
 ............................
@@ -219,6 +247,9 @@ Support for Python 3.3 has been officially dropped.
   value is for backward compatibility and should not be used. :issue:`11585`
   by :user:`Ivan Panico <FollowKenny>`.
 
+- |Efficiency| Efficiency improvements in :func:`decomposition.dict_learning`.
+  :issue:`11420` and others by :user:`John Kirkham <jakirkham>`.
+
 - |Fix| Fix for uninformative error in :class:`decomposition.IncrementalPCA`:
   now an error is raised if the number of components is larger than the
   chosen batch size. The ``n_components=None`` case was adapted accordingly.
@@ -231,8 +262,8 @@ Support for Python 3.3 has been officially dropped.
 
 - |Fix| In :class:`decomposition.PCA` selecting a n_components parameter greater
   than the number of samples now raises an error. Similarly, the
-  ``n_components=None`` case now selects the minimum of n_samples and
-  n_features.
+  ``n_components=None`` case now selects the minimum of ``n_samples`` and
+  ``n_features``.
   :issue:`8484` by :user:`Wally Gauze <wallygauze>`.
 
 - |Fix| Fixed a bug in :class:`decomposition.PCA` where users will get
@@ -240,8 +271,11 @@ Support for Python 3.3 has been officially dropped.
   versions.
   :issue:`9886` by :user:`Hanmin Qin <qinhanmin2014>`.
 
+- |Fix| Fixed an underflow in calculating KL-divergence for
+  :class:`decomposition.NMF` :issue:`10142` by `Tom Dupre la Tour`_.
+
 - |Fix| Fixed a bug in :class:`decomposition.SparseCoder` when running OMP
-  sparse coding in parallel using readonly memory mapped datastructures.
+  sparse coding in parallel using read-only memory mapped datastructures.
   :issue:`5956` by :user:`Vighnesh Birodkar <vighneshbirodkar>` and
   :user:`Olivier Grisel <ogrisel>`.
 
@@ -251,7 +285,7 @@ Support for Python 3.3 has been officially dropped.
 
 - |Efficiency| Memory usage improvement for :func:`_class_means` and
   :func:`_class_cov` in :mod:`discriminant_analysis`. :issue:`10898` by
-  :user:`Nanxin Chen <bobchennan>`.`
+  :user:`Nanxin Chen <bobchennan>`.
 
 
 :mod:`sklearn.dummy`
@@ -264,6 +298,10 @@ Support for Python 3.3 has been officially dropped.
   only require X to be an object with finite length or shape. :issue:`9832` by
   :user:`Vrishank Bhardwaj <vrishank97>`.
 
+- |Feature| :class:`dummy.DummyClassifier` and :class:`dummy.DummyRegressor`
+  can now be scored without supplying test samples.
+  :issue:`11951` by :user:`Rüdiger Busche <JarnoRFB>`.
+
 
 :mod:`sklearn.ensemble`
 .......................
@@ -278,7 +316,7 @@ Support for Python 3.3 has been officially dropped.
   via ``n_iter_no_change``, ``validation_fraction`` and ``tol``. :issue:`7071`
   by `Raghav RV`_
 
-- |Feature| Add `named_estimators_` parameter in
+- |Feature| Added ``named_estimators_`` parameter in
   :class:`ensemble.VotingClassifier` to access fitted estimators.
   :issue:`9157` by :user:`Herilalaina Rakotoarison <herilalaina>`.
 
@@ -311,7 +349,7 @@ Support for Python 3.3 has been officially dropped.
   :issue:`9524` by :user:`Guillaume Lemaitre <glemaitre>`.
 
 - |Fix| :class:`ensemble.BaseBagging` where one could not deterministically
-  reproduce ``fit`` result usinbg the object attributes when ``random_state``
+  reproduce ``fit`` result using the object attributes when ``random_state``
   is set. :issue:`9723` by :user:`Guillaume Lemaitre <glemaitre>`.
 
 
@@ -322,10 +360,14 @@ Support for Python 3.3 has been officially dropped.
   :class:`feature_extraction.text.CountVectorizer` initialized with a
   vocabulary. :issue:`10908` by :user:`Mohamed Maskani <maskani-moh>`.
 
+- |Enhancement| ``idf_`` can now be set on a
+  :class:`feature_extraction.text.TfidfTransformer`.
+  :issue:`10899` by :user:`Sergey Melderis <serega>`.
+
 - |Fix| Fixed a bug in :func:`feature_extraction.image.extract_patches_2d` which
   would throw an exception if ``max_patches`` was greater than or equal to the
   number of all possible patches rather than simply returning the number of
-  possible patches. :issue:`10100` by :user:`Varun Agrawal <varunagrawal>`
+  possible patches. :issue:`10101` by :user:`Varun Agrawal <varunagrawal>`
 
 - |Fix| Fixed a bug in :class:`feature_extraction.text.CountVectorizer`,
   :class:`feature_extraction.text.TfidfVectorizer`,
@@ -354,6 +396,9 @@ Support for Python 3.3 has been officially dropped.
   :class:`feature_selection.RFECV` to bound evaluated features counts.
   :issue:`11293` by :user:`Brent Yi <brentyi>`.
 
+- |Feature| :class:`feature_selection.RFECV`'s fit method now supports
+  :term:`groups`.  :issue:`9656` by :user:`Adam Greenhall <adamgreenhall>`.
+
 - |Fix| Fixed computation of ``n_features_to_compute`` for edge case with tied
   CV scores in :class:`feature_selection.RFECV`.
   :issue:`9222` by :user:`Nick Hoh <nickypie>`.
@@ -391,7 +436,7 @@ Support for Python 3.3 has been officially dropped.
 - |Fix| Fixed a bug in :class:`isotonic.IsotonicRegression` which incorrectly
   combined weights when fitting a model to data involving points with
   identical X values.
-  :issue:`9432` by :user:`Dallas Card <dallascard>`
+  :issue:`9484` by :user:`Dallas Card <dallascard>`
 
 
 :mod:`sklearn.linear_model`
@@ -410,7 +455,7 @@ Support for Python 3.3 has been officially dropped.
 
 - |Feature| Add `sample_weight` parameter to the fit method of
   :class:`linear_model.BayesianRidge` for weighted linear regression.
-  :issue:`10111` by :user:`Peter St. John <pstjohn>`.
+  :issue:`10112` by :user:`Peter St. John <pstjohn>`.
 
 - |Fix| Fixed a bug in :func:`logistic.logistic_regression_path` to ensure
   that the returned coefficients are correct when ``multiclass='multinomial'``.
@@ -430,7 +475,7 @@ Support for Python 3.3 has been officially dropped.
 
 - |Fix| Fixed a bug in :class:`linear_model.LogisticRegressionCV` where the
   'ovr' strategy was always used to compute cross-validation scores in the
-  multiclass setting, even if 'multinomial' was set.
+  multiclass setting, even if ``'multinomial'`` was set.
   :issue:`8720` by :user:`William de Vazelhes <wdevazelhes>`.
 
 - |Fix| Fixed a bug in :class:`linear_model.OrthogonalMatchingPursuit` that was
@@ -441,6 +486,11 @@ Support for Python 3.3 has been officially dropped.
   incorrectly updated estimates for the standard deviation and the
   coefficients. :issue:`10153` by :user:`Jörg Döpfert <jdoepfert>`.
 
+- |Fix| Fixed a bug in :class:`linear_model.ARDRegression` and
+  :class:`linear_model.BayesianRidge` which caused NaN predictions when fitted
+  with a constant target.
+  :issue:`10095` by :user:`Jörg Döpfert <jdoepfert>`.
+
 - |Fix| Fixed a bug in :class:`linear_model.RidgeClassifierCV` where
   the parameter ``store_cv_values`` was not implemented though
   it was documented in ``cv_values`` as a way to set up the storage
@@ -456,12 +506,12 @@ Support for Python 3.3 has been officially dropped.
   :issue:`10687` by :user:`Martin Hahn <martin-hahn>`.
 
 - |Fix| Fixed a bug in :func:`sklearn.linear_model.LogisticRegression` where the
-  multi_class='multinomial' with binary output with warm_start = True
+  ``multi_class='multinomial'`` with binary output ``with warm_start=True``
   :issue:`10836` by :user:`Aishwarya Srinivasan <aishgrt1>`.
 
 - |Fix| Fixed a bug in :class:`linear_model.RidgeCV` where using integer
   ``alphas`` raised an error.
-  :issue:`10393` by :user:`Mabel Villalba-Jiménez <mabelvj>`.
+  :issue:`10397` by :user:`Mabel Villalba-Jiménez <mabelvj>`.
 
 - |Fix| Fixed condition triggering gap computation in
   :class:`linear_model.Lasso` and :class:`linear_model.ElasticNet` when working
@@ -472,7 +522,7 @@ Support for Python 3.3 has been officially dropped.
   :class:`linear_model.PassiveAggressiveClassifier`,
   :class:`linear_model.PassiveAggressiveRegressor` and
   :class:`linear_model.Perceptron`, where the stopping criterion was stopping
-  the algorithm before convergence. A parameter `n_iter_no_change` was added
+  the algorithm before convergence. A parameter ``n_iter_no_change`` was added
   and set by default to 5. Previous behavior is equivalent to setting the
   parameter to 1. :issue:`9043` by `Tom Dupre la Tour`_.
 
@@ -481,6 +531,12 @@ Support for Python 3.3 has been officially dropped.
   ValueError.
   :issue:`11327` by :user:`Karan Dhingra <kdhingra307>` and `Joel Nothman`_.
 
+- |API| The default values of the ``solver`` and ``multi_class`` parameters of
+  :class:`linear_model.LogisticRegression` will change respectively from
+  ``'liblinear'`` and ``'ovr'`` in version 0.20 to ``'lbfgs'`` and
+  ``'auto'`` in version 0.22. A FutureWarning is raised when the default
+  values are used. :issue:`11905` by `Tom Dupre la Tour`_ and `Joel Nothman`_.
+
 - |API| Deprecate ``positive=True`` option in :class:`linear_model.Lars` as
   the underlying implementation is broken. Use :class:`linear_model.Lasso`
   instead. :issue:`9837` by `Alexandre Gramfort`_.
@@ -505,12 +561,18 @@ Support for Python 3.3 has been officially dropped.
 
 - |Feature| :func:`manifold.t_sne.trustworthiness` accepts metrics other than
   Euclidean. :issue:`9775` by :user:`William de Vazelhes <wdevazelhes>`.
-  
+
 - |Feature| :class:`manifold.Isomap`, :class:`manifold.TSNE`, and
   :class:`manifold.SpectralEmbedding` now accept precomputed sparse
   neighbors graph as input. :issue:`10482` by `Tom Dupre la Tour`_ and
   :user:`Kumar Ashutosh <thechargedneutron>`.
 
+- |Fix| Fixed a bug in :func:`manifold.spectral_embedding` where the
+  normalization of the spectrum was using a division instead of a
+  multiplication. :issue:`8129` by :user:`Jan Margeta <jmargeta>`,
+  :user:`Guillaume Lemaitre <glemaitre>`, and :user:`Devansh D.
+  <devanshdalal>`.
+
 - |API| |Feature| Deprecate ``precomputed`` parameter in function
   :func:`manifold.t_sne.trustworthiness`. Instead, the new parameter ``metric``
   should be used with any compatible metric including 'precomputed', in which
@@ -524,7 +586,7 @@ Support for Python 3.3 has been officially dropped.
   'precomputed', in which case the input matrix ``X`` should be a matrix of
   pairwise distances or squared distances. :issue:`9775` by
   :user:`William de Vazelhes <wdevazelhes>`.
-  
+
 - |API| Deprecate ``training_data_`` unused attribute in
   :class:`manifold.Isomap`. :issue:`10482` by `Tom Dupre la Tour`_.
 
@@ -540,9 +602,9 @@ Support for Python 3.3 has been officially dropped.
   a corresponding ``'balanced_accuracy'`` scorer for binary and multiclass
   classification. :issue:`8066` by :user:`xyguo` and :user:`Aman Dalmia
   <dalmia>`, and :issue:`10587` by `Joel Nothman`_.
-  
+
 - |Feature| Partial AUC is available via ``max_fpr`` parameter in
-  :func:`metrics.roc_auc_score`. :issue:`3273` by
+  :func:`metrics.roc_auc_score`. :issue:`3840` by
   :user:`Alexander Niederbühl <Alexander-N>`.
 
 - |Feature| A scorer based on :func:`metrics.brier_score_loss` is also
@@ -561,7 +623,7 @@ Support for Python 3.3 has been officially dropped.
 
 - |Feature| :func:`metrics.classification_report` now reports all applicable averages on
   the given data, including micro, macro and weighted average as well as samples
-  average for multilabel data. :issue:`11679` by :user:`Alexander Pacha <apacha>`. 
+  average for multilabel data. :issue:`11679` by :user:`Alexander Pacha <apacha>`.
 
 - |Feature| :func:`metrics.average_precision_score` now supports binary
   ``y_true`` other than ``{0, 1}`` or ``{-1, 1}`` through ``pos_label``
@@ -698,6 +760,11 @@ Support for Python 3.3 has been officially dropped.
   group-based CV strategies. :issue:`9085` by :user:`Laurent Direr <ldirer>`
   and `Andreas Müller`_.
 
+- |API| The default value of the ``error_score`` parameter in
+  :class:`model_selection.GridSearchCV` and
+  :class:`model_selection.RandomizedSearchCV` will change to ``np.NaN`` in
+  version 0.22. :issue:`10677` by :user:`Kirill Zhdanovich <Zhdanovich>`.
+
 - |API| Changed ValueError exception raised in
   :class:`model_selection.ParameterSampler` to a UserWarning for case where the
   class is instantiated with a greater value of ``n_iter`` than the total space
@@ -748,13 +815,13 @@ Support for Python 3.3 has been officially dropped.
 - |Efficiency| :class:`neighbors.RadiusNeighborsRegressor` and
   :class:`neighbors.RadiusNeighborsClassifier` are now
   parallelized according to ``n_jobs`` regardless of ``algorithm``.
-  :issue:`8003` by :user:`Joël Billaud <recamshak>`.
+  :issue:`10887` by :user:`Joël Billaud <recamshak>`.
 
 - |Efficiency| :mod:`Nearest neighbors <neighbors>` query methods are now more
   memory efficient when ``algorithm='brute'``.
   :issue:`11136` by `Joel Nothman`_ and :user:`Aman Dalmia <dalmia>`.
 
-- |Feature| Add `sample_weight` parameter to the fit method of
+- |Feature| Add ``sample_weight`` parameter to the fit method of
   :class:`neighbors.KernelDensity` to enable weighting in kernel density
   estimation.
   :issue:`4394` by :user:`Samuel O. Ronsin <samronsin>`.
@@ -767,7 +834,7 @@ Support for Python 3.3 has been officially dropped.
   ``score_samples``. By default, ``novelty`` is set to ``False``, and only
   the ``fit_predict`` method is avaiable.
   By :user:`Albert Thomas <albertcthomas>`.
-  
+
 - |Feature| :class:`neighbors.KNeighborsClassifier`,
   :class:`neighbors.KNeighborsRegressor`,
   :class:`neighbors.RadiusNeighborsClassifier`,
@@ -791,8 +858,8 @@ Support for Python 3.3 has been officially dropped.
   faster construction and querying times.
   :issue:`11556` by :user:`Jake VanderPlas <jakevdp>`
 
-- |Fix| Fixed a bug in `neighbors.KDTree` and `neighbors.BallTree` where
-  pickled tree objects would change their type to the super class `BinaryTree`.
+- |Fix| Fixed a bug in :class:`neighbors.KDTree` and :class:`neighbors.BallTree` where
+  pickled tree objects would change their type to the super class :class:`BinaryTree`.
   :issue:`11774` by :user:`Nicolas Hug <NicolasHug>`.
 
 
@@ -825,6 +892,9 @@ Support for Python 3.3 has been officially dropped.
   parameters such as ``return_std`` in a pipeline with caution.
   :issue:`9304` by :user:`Breno Freitas <brenolf>`.
 
+- |API| :class:`pipeline.FeatureUnion` now supports ``'drop'`` as a transformer
+  to drop features. :issue:`11144` by :user:`thomasjpfan`.
+
 
 :mod:`sklearn.preprocessing`
 ............................
@@ -909,7 +979,7 @@ Support for Python 3.3 has been officially dropped.
   :issue:`11042` by :user:`Daniel Morales <DanielMorales9>`.
 
 - |Fix| Fix ``fit`` and ``partial_fit`` in
-  :class:`preprocessing.StandardScaler` in the rare case when `with_mean=False`
+  :class:`preprocessing.StandardScaler` in the rare case when ``with_mean=False``
   and `with_std=False` which was crashing by calling ``fit`` more than once and
   giving inconsistent results for ``mean_`` whether the input was a sparse or a
   dense matrix. ``mean_`` will be set to ``None`` with both sparse and dense
@@ -959,7 +1029,7 @@ Support for Python 3.3 has been officially dropped.
   unicode in Python2, the ``predict_proba`` method was raising an
   unexpected TypeError given dense inputs.
   :issue:`10412` by :user:`Jiongyan Zhang <qmick>`.
-  
+
 - |API| Deprecate ``random_state`` parameter in :class:`svm.OneClassSVM` as
   the underlying implementation is not random.
   :issue:`9497` by :user:`Albert Thomas <albertcthomas>`.
@@ -974,6 +1044,11 @@ Support for Python 3.3 has been officially dropped.
 :mod:`sklearn.tree`
 ...................
 
+- |Enhancement| Although private (and hence not assured API stability),
+  :class:`tree._criterion.ClassificationCriterion` and
+  :class:`tree._criterion.RegressionCriterion` may now be cimported and
+  extended. :issue:`10325` by :user:`Camil Staps <camilstaps>`.
+
 - |Fix| Fixed a bug in :class:`tree.BaseDecisionTree` with `splitter="best"`
   where split threshold could become infinite when values in X were
   near infinite. :issue:`10536` by :user:`Jonathan Ohayon <Johayon>`.
@@ -1055,6 +1130,19 @@ Miscellaneous
   :func:`metrics.pairwise_distances_chunked`. See :ref:`working_memory`.
   :issue:`10280` by `Joel Nothman`_ and :user:`Aman Dalmia <dalmia>`.
 
+- |Feature| The version of :mod:`joblib` bundled with Scikit-learn is now 0.12.
+  This uses a new default multiprocessing implementation, named `loky
+  <https://github.com/tomMoral/loky>`_. While this may incur some memory and
+  communication overhead, it should provide greater cross-platform stability
+  than relying on Python standard library multiprocessing. :issue:`11741` by
+  the Joblib developers, especially :user:`Thomas Moreau <tomMoral>` and
+  `Olivier Grisel`_.
+
+- |Feature| An environment variable to use the site joblib instead of the
+  vendored one was added (:ref:`environment_variable`). The main API of joblib
+  is now exposed in :mod:`sklearn.utils`.
+  :issue:`11166` by `Gael Varoquaux`_.
+
 - |Feature| Add almost complete PyPy 3 support. Known unsupported
   functionalities are :func:`datasets.load_svmlight_file`,
   :class:`feature_extraction.FeatureHasher` and
@@ -1062,11 +1150,6 @@ Miscellaneous
   PyPy3-v5.10+, Numpy 1.14.0+, and scipy 1.1.0+ are required.
   :issue:`11010` by :user:`Ronan Lamy <rlamy>` and `Roman Yurchak`_.
 
-- |Feature| An environment variable to use the site joblib instead of the
-  vendored one was added (:ref:`environment_variable`). The main API of joblib
-  is now exposed in :mod:`sklearn.utils`.
-  :issue:`11166` by `Gael Varoquaux`_.
-
 - |Feature| A utility method :func:`sklearn.show_versions()` was added to
   print out information relevant for debugging. It includes the user system,
   the Python executable, the version of the main libraries and BLAS binding
@@ -1076,11 +1159,31 @@ Miscellaneous
   a wrapped estimator and its parameter. :issue:`9999` by :user:`Marcus Voss
   <marcus-voss>` and `Joel Nothman`_.
 
+- |Fix| Fixed a bug where calling :func:`sklearn.base.clone` was not thread
+  safe and could result in a "pop from empty list" error. :issue:`9569`
+  by `Andreas Müller`_.
+
+- |API| The default value of ``n_jobs`` is changed from ``1`` to ``None`` in
+  all related functions and classes. ``n_jobs=None`` means ``unset``. It will
+  generally be interpreted as ``n_jobs=1``, unless the current
+  ``joblib.Parallel`` backend context specifies otherwise (See
+  :term:`Glossary <n_jobs>` for additional information). Note that this change
+  happens immediately (i.e., without a deprecation cycle).
+  :issue:`11741` by `Olivier Grisel`_.
+
 Changes to estimator checks
 ---------------------------
 
 These changes mostly affect library developers.
 
+- Checks for transformers now apply if the estimator implements
+  :term:`transform`, regardless of whether it inherits from
+  :class:`sklearn.base.TransformerMixin`. :issue:`10474` by `Joel Nothman`_.
+
+- Classifiers are now checked for consistency between :term:`decision_function`
+  and categorical predictions.
+  :issue:`10500` by :user:`Narine Kokhlikyan <NarineK>`.
+
 - Allow tests in :func:`utils.estimator_checks.check_estimator` to test functions
   that accept pairwise data.
   :issue:`9701` by :user:`Kyle Johnson <gkjohns>`
@@ -1099,8 +1202,8 @@ These changes mostly affect library developers.
 
 - Add ``check_methods_subset_invariance`` to
   :func:`~utils.estimator_checks.check_estimator`, which checks that
-  estimator methods are invariant if applied to a data subset. :issue:`10420`
-  by :user:`Jonathan Ohayon <Johayon>`
+  estimator methods are invariant if applied to a data subset.
+  :issue:`10428` by :user:`Jonathan Ohayon <Johayon>`
 
 - Add tests in :func:`utils.estimator_checks.check_estimator` to check that an
   estimator can handle read-only memmap input data. :issue:`10663` by
@@ -1109,3 +1212,7 @@ These changes mostly affect library developers.
 - ``check_sample_weights_pandas_series`` now uses 8 rather than 6 samples
   to accommodate for the default number of clusters in :class:`cluster.KMeans`.
   :issue:`10933` by :user:`Johannes Hansen <jnhansen>`.
+
+- Estimators are now checked for whether ``sample_weight=None`` equates to
+  ``sample_weight=np.ones(...)``.
+  :issue:`11558` by :user:`Sergul Aydore <sergulaydore>`.
diff --git a/doc/whats_new/v0.21.rst b/doc/whats_new/v0.21.rst
new file mode 100755
index 0000000000..03440502ae
--- /dev/null
+++ b/doc/whats_new/v0.21.rst
@@ -0,0 +1,57 @@
+.. include:: _contributors.rst
+
+.. currentmodule:: sklearn
+
+.. _changes_0_21:
+
+Version 0.21.0
+==============
+
+**In development**
+
+Changed models
+--------------
+
+The following estimators and functions, when fit with the same data and
+parameters, may produce different models from the previous version. This often
+occurs due to changes in the modelling logic (bug fixes or enhancements), or in
+random sampling procedures.
+
+- please add class and reason here (see version 0.20 what's new)
+
+Details are listed in the changelog below.
+
+(While we are trying to better inform users by providing this information, we
+cannot assure that this list is complete.)
+
+Changelog
+---------
+
+Support for Python 3.4 and below has been officially dropped.
+
+..
+    See version doc/whats_new/v0.20.rst for structure. Entries should be
+    prefixed with one of the labels: |MajorFeature|, |Feature|, |Efficiency|,
+    |Enhancement|, |Fix| or |API|. They should be under a heading for the
+    relevant module (or *Multiple Modules* or *Miscellaneous*), and within each
+    section should be ordered according to the label ordering above. Entries
+    should end with: :issue:`123456` by :user:`Joe Bloggs <joeongithub>`.
+
+- An entry goes here
+- An entry goes here
+
+:mod:`sklearn.cluster`
+......................
+
+- |MajorFeature| A new clustering algorithm: :class:`cluster.OPTICS`: an
+  algoritm related to :class:`cluster.DBSCAN`, that has hyperparameters easier
+  to set and that scales better, by :user:`Shane <espg>` and
+  :user:`Adrin Jalali <adrinjalali>`.
+
+Multiple modules
+................
+
+Changes to estimator checks
+---------------------------
+
+These changes mostly affect library developers.
diff --git a/examples/applications/plot_face_recognition.py b/examples/applications/plot_face_recognition.py
index 13a38d13bc..dce3df1d3e 100755
--- a/examples/applications/plot_face_recognition.py
+++ b/examples/applications/plot_face_recognition.py
@@ -108,7 +108,8 @@
 t0 = time()
 param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],
               'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }
-clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)
+clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'),
+                   param_grid, cv=5)
 clf = clf.fit(X_train_pca, y_train)
 print("done in %0.3fs" % (time() - t0))
 print("Best estimator found by grid search:")
diff --git a/examples/applications/plot_prediction_latency.py b/examples/applications/plot_prediction_latency.py
index f5a3b51735..6eac023b71 100755
--- a/examples/applications/plot_prediction_latency.py
+++ b/examples/applications/plot_prediction_latency.py
@@ -26,7 +26,6 @@
 
 from sklearn.preprocessing import StandardScaler
 from sklearn.model_selection import train_test_split
-from scipy.stats import scoreatpercentile
 from sklearn.datasets.samples_generator import make_regression
 from sklearn.ensemble.forest import RandomForestRegressor
 from sklearn.linear_model.ridge import Ridge
@@ -50,7 +49,7 @@ def atomic_benchmark_estimator(estimator, X_test, verbose=False):
         estimator.predict(instance)
         runtimes[i] = time.time() - start
     if verbose:
-        print("atomic_benchmark runtimes:", min(runtimes), scoreatpercentile(
+        print("atomic_benchmark runtimes:", min(runtimes), np.percentile(
             runtimes, 50), max(runtimes))
     return runtimes
 
@@ -65,7 +64,7 @@ def bulk_benchmark_estimator(estimator, X_test, n_bulk_repeats, verbose):
         runtimes[i] = time.time() - start
     runtimes = np.array(list(map(lambda x: x / float(n_instances), runtimes)))
     if verbose:
-        print("bulk_benchmark runtimes:", min(runtimes), scoreatpercentile(
+        print("bulk_benchmark runtimes:", min(runtimes), np.percentile(
             runtimes, 50), max(runtimes))
     return runtimes
 
@@ -207,8 +206,8 @@ def n_feature_influence(estimators, n_train, n_test, n_features, percentile):
             estimator.fit(X_train, y_train)
             gc.collect()
             runtimes = bulk_benchmark_estimator(estimator, X_test, 30, False)
-            percentiles[cls_name][n] = 1e6 * scoreatpercentile(runtimes,
-                                                               percentile)
+            percentiles[cls_name][n] = 1e6 * np.percentile(runtimes,
+                                                           percentile)
     return percentiles
 
 
diff --git a/examples/applications/plot_stock_market.py b/examples/applications/plot_stock_market.py
index 379efb8e4d..e2edc25b7e 100755
--- a/examples/applications/plot_stock_market.py
+++ b/examples/applications/plot_stock_market.py
@@ -166,7 +166,7 @@
 
 # #############################################################################
 # Learn a graphical structure from the correlations
-edge_model = covariance.GraphicalLassoCV()
+edge_model = covariance.GraphicalLassoCV(cv=5)
 
 # standardize the time series: using correlations rather than covariance
 # is more efficient for structure recovery
diff --git a/examples/cluster/plot_optics.py b/examples/cluster/plot_optics.py
index 19fd683ddd..ed4a742b99 100755
--- a/examples/cluster/plot_optics.py
+++ b/examples/cluster/plot_optics.py
@@ -68,8 +68,8 @@
     Rk = reachability[labels == k]
     ax1.plot(Xk, Rk, c, alpha=0.3)
 ax1.plot(space[labels == -1], reachability[labels == -1], 'k.', alpha=0.3)
-ax1.plot(space, np.full_like(space, 0.75), 'k-', alpha=0.5)
-ax1.plot(space, np.full_like(space, 0.25), 'k-.', alpha=0.5)
+ax1.plot(space, np.full_like(space, 0.75, dtype=float), 'k-', alpha=0.5)
+ax1.plot(space, np.full_like(space, 0.25, dtype=float), 'k-.', alpha=0.5)
 ax1.set_ylabel('Reachability (epsilon distance)')
 ax1.set_title('Reachability Plot')
 
diff --git a/examples/compose/plot_column_transformer_mixed_types.py b/examples/compose/plot_column_transformer_mixed_types.py
index 64f1a3c88d..73ee27f83a 100755
--- a/examples/compose/plot_column_transformer_mixed_types.py
+++ b/examples/compose/plot_column_transformer_mixed_types.py
@@ -61,13 +61,12 @@
 categorical_features = ['embarked', 'sex', 'pclass']
 categorical_transformer = Pipeline(steps=[
     ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
-    ('onehot', OneHotEncoder(sparse=False, handle_unknown='ignore'))])
+    ('onehot', OneHotEncoder(handle_unknown='ignore'))])
 
 preprocessor = ColumnTransformer(
     transformers=[
         ('num', numeric_transformer, numeric_features),
-        ('cat', categorical_transformer, categorical_features)],
-    remainder='drop')
+        ('cat', categorical_transformer, categorical_features)])
 
 # Append classifier to preprocessing pipeline.
 # Now we have a full prediction pipeline.
@@ -77,8 +76,7 @@
 X = data.drop('survived', axis=1)
 y = data['survived']
 
-X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
-                                                    shuffle=True)
+X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
 
 clf.fit(X_train, y_train)
 print("model score: %.3f" % clf.score(X_test, y_test))
diff --git a/examples/compose/plot_digits_pipe.py b/examples/compose/plot_digits_pipe.py
index b95d2847ad..2352abba45 100755
--- a/examples/compose/plot_digits_pipe.py
+++ b/examples/compose/plot_digits_pipe.py
@@ -54,7 +54,7 @@
 # Parameters of pipelines can be set using ‘__’ separated parameter names:
 estimator = GridSearchCV(pipe,
                          dict(pca__n_components=n_components,
-                              logistic__C=Cs))
+                              logistic__C=Cs), cv=5)
 estimator.fit(X_digits, y_digits)
 
 plt.axvline(estimator.best_estimator_.named_steps['pca'].n_components,
diff --git a/examples/compose/plot_feature_union.py b/examples/compose/plot_feature_union.py
index 4798617f40..56d1e320c4 100755
--- a/examples/compose/plot_feature_union.py
+++ b/examples/compose/plot_feature_union.py
@@ -55,6 +55,6 @@
                   features__univ_select__k=[1, 2],
                   svm__C=[0.1, 1, 10])
 
-grid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)
+grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, verbose=10)
 grid_search.fit(X, y)
 print(grid_search.best_estimator_)
diff --git a/examples/covariance/plot_covariance_estimation.py b/examples/covariance/plot_covariance_estimation.py
index d33b77d68a..acbe567c53 100755
--- a/examples/covariance/plot_covariance_estimation.py
+++ b/examples/covariance/plot_covariance_estimation.py
@@ -83,7 +83,7 @@
 
 # GridSearch for an optimal shrinkage coefficient
 tuned_parameters = [{'shrinkage': shrinkages}]
-cv = GridSearchCV(ShrunkCovariance(), tuned_parameters)
+cv = GridSearchCV(ShrunkCovariance(), tuned_parameters, cv=5)
 cv.fit(X_train)
 
 # Ledoit-Wolf optimal shrinkage coefficient estimate
diff --git a/examples/covariance/plot_sparse_cov.py b/examples/covariance/plot_sparse_cov.py
index 313d2544c7..a2009bb330 100755
--- a/examples/covariance/plot_sparse_cov.py
+++ b/examples/covariance/plot_sparse_cov.py
@@ -83,7 +83,7 @@
 # Estimate the covariance
 emp_cov = np.dot(X.T, X) / n_samples
 
-model = GraphicalLassoCV()
+model = GraphicalLassoCV(cv=5)
 model.fit(X)
 cov_ = model.covariance_
 prec_ = model.precision_
diff --git a/examples/decomposition/plot_pca_vs_fa_model_selection.py b/examples/decomposition/plot_pca_vs_fa_model_selection.py
index b858434d91..9d395f70c3 100755
--- a/examples/decomposition/plot_pca_vs_fa_model_selection.py
+++ b/examples/decomposition/plot_pca_vs_fa_model_selection.py
@@ -69,20 +69,20 @@ def compute_scores(X):
     for n in n_components:
         pca.n_components = n
         fa.n_components = n
-        pca_scores.append(np.mean(cross_val_score(pca, X)))
-        fa_scores.append(np.mean(cross_val_score(fa, X)))
+        pca_scores.append(np.mean(cross_val_score(pca, X, cv=5)))
+        fa_scores.append(np.mean(cross_val_score(fa, X, cv=5)))
 
     return pca_scores, fa_scores
 
 
 def shrunk_cov_score(X):
     shrinkages = np.logspace(-2, 0, 30)
-    cv = GridSearchCV(ShrunkCovariance(), {'shrinkage': shrinkages})
-    return np.mean(cross_val_score(cv.fit(X).best_estimator_, X))
+    cv = GridSearchCV(ShrunkCovariance(), {'shrinkage': shrinkages}, cv=5)
+    return np.mean(cross_val_score(cv.fit(X).best_estimator_, X, cv=5))
 
 
 def lw_score(X):
-    return np.mean(cross_val_score(LedoitWolf(), X))
+    return np.mean(cross_val_score(LedoitWolf(), X, cv=5))
 
 
 for X, title in [(X_homo, 'Homoscedastic Noise'),
diff --git a/examples/exercises/plot_cv_diabetes.py b/examples/exercises/plot_cv_diabetes.py
index 76b0d81b89..d68fd21bd7 100755
--- a/examples/exercises/plot_cv_diabetes.py
+++ b/examples/exercises/plot_cv_diabetes.py
@@ -29,7 +29,7 @@
 alphas = np.logspace(-4, -0.5, 30)
 
 tuned_parameters = [{'alpha': alphas}]
-n_folds = 3
+n_folds = 5
 
 clf = GridSearchCV(lasso, tuned_parameters, cv=n_folds, refit=False)
 clf.fit(X, y)
@@ -60,7 +60,7 @@
 # performs cross-validation on the training data it receives).
 # We use external cross-validation to see how much the automatically obtained
 # alphas differ across different cross-validation folds.
-lasso_cv = LassoCV(alphas=alphas, random_state=0)
+lasso_cv = LassoCV(alphas=alphas, cv=5, random_state=0)
 k_fold = KFold(3)
 
 print("Answer to the bonus question:",
diff --git a/examples/exercises/plot_cv_digits.py b/examples/exercises/plot_cv_digits.py
index a68f92afbd..f51bcc7e02 100755
--- a/examples/exercises/plot_cv_digits.py
+++ b/examples/exercises/plot_cv_digits.py
@@ -26,7 +26,7 @@
 scores_std = list()
 for C in C_s:
     svc.C = C
-    this_scores = cross_val_score(svc, X, y, n_jobs=1)
+    this_scores = cross_val_score(svc, X, y, cv=5, n_jobs=1)
     scores.append(np.mean(this_scores))
     scores_std.append(np.std(this_scores))
 
diff --git a/examples/feature_selection/plot_select_from_model_boston.py b/examples/feature_selection/plot_select_from_model_boston.py
index 17ef6d6bd0..400a736942 100755
--- a/examples/feature_selection/plot_select_from_model_boston.py
+++ b/examples/feature_selection/plot_select_from_model_boston.py
@@ -23,7 +23,7 @@
 X, y = boston['data'], boston['target']
 
 # We use the base estimator LassoCV since the L1 norm promotes sparsity of features.
-clf = LassoCV()
+clf = LassoCV(cv=5)
 
 # Set a minimum threshold of 0.25
 sfm = SelectFromModel(clf, threshold=0.25)
diff --git a/examples/gaussian_process/plot_gpr_co2.py b/examples/gaussian_process/plot_gpr_co2.py
index b0b271a364..4c438ce821 100755
--- a/examples/gaussian_process/plot_gpr_co2.py
+++ b/examples/gaussian_process/plot_gpr_co2.py
@@ -8,7 +8,7 @@
 hyperparameter optimization using gradient ascent on the
 log-marginal-likelihood. The data consists of the monthly average atmospheric
 CO2 concentrations (in parts per million by volume (ppmv)) collected at the
-Mauna Loa Observatory in Hawaii, between 1958 and 1997. The objective is to
+Mauna Loa Observatory in Hawaii, between 1958 and 2001. The objective is to
 model the CO2 concentration as a function of the time t.
 
 The kernel is composed of several terms that are responsible for explaining
@@ -57,24 +57,55 @@
 explained by the model. The figure shows also that the model makes very
 confident predictions until around 2015.
 """
-print(__doc__)
-
 # Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
 #
 # License: BSD 3 clause
 
+from __future__ import division, print_function
+
 import numpy as np
 
 from matplotlib import pyplot as plt
-
+from sklearn.datasets import fetch_openml
 from sklearn.gaussian_process import GaussianProcessRegressor
 from sklearn.gaussian_process.kernels \
     import RBF, WhiteKernel, RationalQuadratic, ExpSineSquared
-from sklearn.datasets import fetch_mldata
+try:
+    from urllib.request import urlopen
+except ImportError:
+    # Python 2
+    from urllib2 import urlopen
+
+print(__doc__)
+
+
+def load_mauna_loa_atmospheric_co2():
+    ml_data = fetch_openml(data_id=41187)
+    months = []
+    ppmv_sums = []
+    counts = []
+
+    y = ml_data.data[:, 0]
+    m = ml_data.data[:, 1]
+    month_float = y + (m - 1) / 12
+    ppmvs = ml_data.target
+
+    for month, ppmv in zip(month_float, ppmvs):
+        if not months or month != months[-1]:
+            months.append(month)
+            ppmv_sums.append(ppmv)
+            counts.append(1)
+        else:
+            # aggregate monthly sum to produce average
+            ppmv_sums[-1] += ppmv
+            counts[-1] += 1
+
+    months = np.asarray(months).reshape(-1, 1)
+    avg_ppmvs = np.asarray(ppmv_sums) / counts
+    return months, avg_ppmvs
+
 
-data = fetch_mldata('mauna-loa-atmospheric-co2').data
-X = data[:, [1]]
-y = data[:, 0]
+X, y = load_mauna_loa_atmospheric_co2()
 
 # Kernel with parameters given in GPML book
 k1 = 66.0**2 * RBF(length_scale=67.0)  # long term smooth rising trend
diff --git a/examples/gaussian_process/plot_gpr_prior_posterior.py b/examples/gaussian_process/plot_gpr_prior_posterior.py
index 47163f77de..85d18041ad 100755
--- a/examples/gaussian_process/plot_gpr_prior_posterior.py
+++ b/examples/gaussian_process/plot_gpr_prior_posterior.py
@@ -29,7 +29,7 @@
                                 length_scale_bounds=(0.1, 10.0),
                                 periodicity_bounds=(1.0, 10.0)),
            ConstantKernel(0.1, (0.01, 10.0))
-               * (DotProduct(sigma_0=1.0, sigma_0_bounds=(0.0, 10.0)) ** 2),
+               * (DotProduct(sigma_0=1.0, sigma_0_bounds=(0.1, 10.0)) ** 2),
            1.0 * Matern(length_scale=1.0, length_scale_bounds=(1e-1, 10.0),
                         nu=1.5)]
 
diff --git a/examples/linear_model/plot_omp.py b/examples/linear_model/plot_omp.py
index f07b7d7233..8a3b52fc58 100755
--- a/examples/linear_model/plot_omp.py
+++ b/examples/linear_model/plot_omp.py
@@ -67,7 +67,7 @@
 
 # plot the noisy reconstruction with number of non-zeros set by CV
 ##################################################################
-omp_cv = OrthogonalMatchingPursuitCV()
+omp_cv = OrthogonalMatchingPursuitCV(cv=5)
 omp_cv.fit(X, y_noisy)
 coef = omp_cv.coef_
 idx_r, = coef.nonzero()
diff --git a/examples/linear_model/plot_sgd_early_stopping.py b/examples/linear_model/plot_sgd_early_stopping.py
index 31ce61f39d..4076fa5f6b 100755
--- a/examples/linear_model/plot_sgd_early_stopping.py
+++ b/examples/linear_model/plot_sgd_early_stopping.py
@@ -47,7 +47,7 @@
 import matplotlib.pyplot as plt
 
 from sklearn import linear_model
-from sklearn.datasets import fetch_mldata
+from sklearn.datasets import fetch_openml
 from sklearn.model_selection import train_test_split
 from sklearn.utils.testing import ignore_warnings
 from sklearn.exceptions import ConvergenceWarning
@@ -56,9 +56,10 @@
 print(__doc__)
 
 
-def load_mnist(n_samples=None, class_0=0, class_1=8):
+def load_mnist(n_samples=None, class_0='0', class_1='8'):
     """Load MNIST, select two classes, shuffle and return only n_samples."""
-    mnist = fetch_mldata('MNIST original')
+    # Load data from http://openml.org/d/554
+    mnist = fetch_openml('mnist_784', version=1)
 
     # take only two classes for binary classification
     mask = np.logical_or(mnist.target == class_0, mnist.target == class_1)
diff --git a/examples/linear_model/plot_sparse_logistic_regression_mnist.py b/examples/linear_model/plot_sparse_logistic_regression_mnist.py
index 5610f471b5..523f5683a5 100755
--- a/examples/linear_model/plot_sparse_logistic_regression_mnist.py
+++ b/examples/linear_model/plot_sparse_logistic_regression_mnist.py
@@ -20,7 +20,7 @@
 import matplotlib.pyplot as plt
 import numpy as np
 
-from sklearn.datasets import fetch_mldata
+from sklearn.datasets import fetch_openml
 from sklearn.linear_model import LogisticRegression
 from sklearn.model_selection import train_test_split
 from sklearn.preprocessing import StandardScaler
@@ -35,9 +35,9 @@
 t0 = time.time()
 train_samples = 5000
 
-mnist = fetch_mldata('MNIST original')
-X = mnist.data.astype('float64')
-y = mnist.target
+# Load data from https://www.openml.org/d/554
+X, y = fetch_openml('mnist_784', version=1, return_X_y=True)
+
 random_state = check_random_state(0)
 permutation = random_state.permutation(X.shape[0])
 X = X[permutation]
diff --git a/examples/mixture/plot_gmm_selection.py b/examples/mixture/plot_gmm_selection.py
index 3ccaba5262..0acee7366a 100755
--- a/examples/mixture/plot_gmm_selection.py
+++ b/examples/mixture/plot_gmm_selection.py
@@ -57,6 +57,7 @@
 bars = []
 
 # Plot the BIC scores
+plt.figure(figsize=(8, 6))
 spl = plt.subplot(2, 1, 1)
 for i, (cv_type, color) in enumerate(zip(cv_types, color_iter)):
     xpos = np.array(n_components_range) + .2 * (i - 2)
diff --git a/examples/model_selection/grid_search_text_feature_extraction.py b/examples/model_selection/grid_search_text_feature_extraction.py
index c3bd054c99..c220a43ed8 100755
--- a/examples/model_selection/grid_search_text_feature_extraction.py
+++ b/examples/model_selection/grid_search_text_feature_extraction.py
@@ -113,7 +113,8 @@
 
     # find the best parameters for both the feature extraction and the
     # classifier
-    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)
+    grid_search = GridSearchCV(pipeline, parameters, cv=5,
+                               n_jobs=-1, verbose=1)
 
     print("Performing grid search...")
     print("pipeline:", [name for name, _ in pipeline.steps])
diff --git a/examples/model_selection/plot_learning_curve.py b/examples/model_selection/plot_learning_curve.py
index 6e022ebe27..4d86c323f5 100755
--- a/examples/model_selection/plot_learning_curve.py
+++ b/examples/model_selection/plot_learning_curve.py
@@ -25,7 +25,7 @@
 
 
 def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,
-                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):
+                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):
     """
     Generate a simple plot of the test and training learning curve.
 
@@ -63,8 +63,11 @@ def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validators that can be used here.
 
-    n_jobs : integer, optional
-        Number of jobs to run in parallel (default 1).
+    n_jobs : int or None, optional (default=None)
+        Number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     train_sizes : array-like, shape (n_ticks,), dtype float or int
         Relative or absolute numbers of training examples that will be used to
diff --git a/examples/model_selection/plot_multi_metric_evaluation.py b/examples/model_selection/plot_multi_metric_evaluation.py
index eaec4c8d18..e1b5cbb303 100755
--- a/examples/model_selection/plot_multi_metric_evaluation.py
+++ b/examples/model_selection/plot_multi_metric_evaluation.py
@@ -43,7 +43,7 @@
 # Setting refit='AUC', refits an estimator on the whole dataset with the
 # parameter setting that has the best cross-validated AUC score.
 # That estimator is made available at ``gs.best_estimator_`` along with
-# parameters like ``gs.best_score_``, ``gs.best_parameters_`` and
+# parameters like ``gs.best_score_``, ``gs.best_params_`` and
 # ``gs.best_index_``
 gs = GridSearchCV(DecisionTreeClassifier(random_state=42),
                   param_grid={'min_samples_split': range(2, 403, 10)},
diff --git a/examples/model_selection/plot_randomized_search.py b/examples/model_selection/plot_randomized_search.py
index bac6495090..2429c92e26 100755
--- a/examples/model_selection/plot_randomized_search.py
+++ b/examples/model_selection/plot_randomized_search.py
@@ -55,14 +55,13 @@ def report(results, n_top=3):
 param_dist = {"max_depth": [3, None],
               "max_features": sp_randint(1, 11),
               "min_samples_split": sp_randint(2, 11),
-              "min_samples_leaf": sp_randint(1, 11),
               "bootstrap": [True, False],
               "criterion": ["gini", "entropy"]}
 
 # run randomized search
 n_iter_search = 20
 random_search = RandomizedSearchCV(clf, param_distributions=param_dist,
-                                   n_iter=n_iter_search)
+                                   n_iter=n_iter_search, cv=5)
 
 start = time()
 random_search.fit(X, y)
@@ -74,12 +73,11 @@ def report(results, n_top=3):
 param_grid = {"max_depth": [3, None],
               "max_features": [1, 3, 10],
               "min_samples_split": [2, 3, 10],
-              "min_samples_leaf": [1, 3, 10],
               "bootstrap": [True, False],
               "criterion": ["gini", "entropy"]}
 
 # run grid search
-grid_search = GridSearchCV(clf, param_grid=param_grid)
+grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5)
 start = time()
 grid_search.fit(X, y)
 
diff --git a/examples/multioutput/plot_classifier_chain_yeast.py b/examples/multioutput/plot_classifier_chain_yeast.py
index 6a90e14dfc..cb3a5085e3 100755
--- a/examples/multioutput/plot_classifier_chain_yeast.py
+++ b/examples/multioutput/plot_classifier_chain_yeast.py
@@ -32,24 +32,23 @@
 with randomly ordered chains).
 """
 
-print(__doc__)
-
 # Author: Adam Kleczewski
 # License: BSD 3 clause
 
 import numpy as np
 import matplotlib.pyplot as plt
+from sklearn.datasets import fetch_openml
 from sklearn.multioutput import ClassifierChain
 from sklearn.model_selection import train_test_split
 from sklearn.multiclass import OneVsRestClassifier
 from sklearn.metrics import jaccard_similarity_score
 from sklearn.linear_model import LogisticRegression
-from sklearn.datasets import fetch_mldata
 
-# Load a multi-label dataset
-yeast = fetch_mldata('yeast')
-X = yeast['data']
-Y = yeast['target'].transpose().toarray()
+print(__doc__)
+
+# Load a multi-label dataset from https://www.openml.org/d/40597
+X, Y = fetch_openml('yeast', version=4, return_X_y=True)
+Y = Y == 'TRUE'
 X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2,
                                                     random_state=0)
 
diff --git a/examples/neighbors/plot_digits_kde_sampling.py b/examples/neighbors/plot_digits_kde_sampling.py
index 8367d16b95..ca44c96f13 100755
--- a/examples/neighbors/plot_digits_kde_sampling.py
+++ b/examples/neighbors/plot_digits_kde_sampling.py
@@ -27,7 +27,7 @@
 
 # use grid search cross-validation to optimize the bandwidth
 params = {'bandwidth': np.logspace(-1, 1, 20)}
-grid = GridSearchCV(KernelDensity(), params)
+grid = GridSearchCV(KernelDensity(), params, cv=5)
 grid.fit(data)
 
 print("best bandwidth: {0}".format(grid.best_estimator_.bandwidth))
diff --git a/examples/neural_networks/plot_mnist_filters.py b/examples/neural_networks/plot_mnist_filters.py
index 6c3b8b2284..408e555443 100755
--- a/examples/neural_networks/plot_mnist_filters.py
+++ b/examples/neural_networks/plot_mnist_filters.py
@@ -20,15 +20,17 @@
 for a very short time. Training longer would result in weights with a much
 smoother spatial appearance.
 """
-print(__doc__)
-
 import matplotlib.pyplot as plt
-from sklearn.datasets import fetch_mldata
+from sklearn.datasets import fetch_openml
 from sklearn.neural_network import MLPClassifier
 
-mnist = fetch_mldata("MNIST original")
+print(__doc__)
+
+# Load data from https://www.openml.org/d/554
+X, y = fetch_openml('mnist_784', version=1, return_X_y=True)
+X = X / 255.
+
 # rescale the data, use the traditional train/test split
-X, y = mnist.data / 255., mnist.target
 X_train, X_test = X[:60000], X[60000:]
 y_train, y_test = y[:60000], y[60000:]
 
diff --git a/examples/plot_missing_values.py b/examples/plot_missing_values.py
index 777120053a..755943fb55 100755
--- a/examples/plot_missing_values.py
+++ b/examples/plot_missing_values.py
@@ -39,7 +39,7 @@ def get_results(dataset):
     # Estimate the score on the entire dataset, with no missing values
     estimator = RandomForestRegressor(random_state=0, n_estimators=100)
     full_scores = cross_val_score(estimator, X_full, y_full,
-                                  scoring='neg_mean_squared_error')
+                                  scoring='neg_mean_squared_error', cv=5)
 
     # Add missing values in 75% of the lines
     missing_rate = 0.75
@@ -57,7 +57,8 @@ def get_results(dataset):
     y_missing = y_full.copy()
     estimator = RandomForestRegressor(random_state=0, n_estimators=100)
     zero_impute_scores = cross_val_score(estimator, X_missing, y_missing,
-                                         scoring='neg_mean_squared_error')
+                                         scoring='neg_mean_squared_error',
+                                         cv=5)
 
     # Estimate the score after imputation (mean strategy) of the missing values
     X_missing = X_full.copy()
@@ -68,7 +69,8 @@ def get_results(dataset):
                    MissingIndicator(missing_values=0)),
         RandomForestRegressor(random_state=0, n_estimators=100))
     mean_impute_scores = cross_val_score(estimator, X_missing, y_missing,
-                                         scoring='neg_mean_squared_error')
+                                         scoring='neg_mean_squared_error',
+                                         cv=5)
 
 
     return ((full_scores.mean(), full_scores.std()),
diff --git a/examples/svm/plot_svm_anova.py b/examples/svm/plot_svm_anova.py
index 45599f31f5..08f9fddf71 100755
--- a/examples/svm/plot_svm_anova.py
+++ b/examples/svm/plot_svm_anova.py
@@ -45,7 +45,7 @@
 for percentile in percentiles:
     clf.set_params(anova__percentile=percentile)
     # Compute cross-validation score using 1 CPU
-    this_scores = cross_val_score(clf, X, y, n_jobs=1)
+    this_scores = cross_val_score(clf, X, y, cv=5, n_jobs=1)
     score_means.append(this_scores.mean())
     score_stds.append(this_scores.std())
 
diff --git a/examples/svm/plot_weighted_samples.py b/examples/svm/plot_weighted_samples.py
index 9cdb2dcb49..0549da7a38 100755
--- a/examples/svm/plot_weighted_samples.py
+++ b/examples/svm/plot_weighted_samples.py
@@ -45,7 +45,7 @@ def plot_decision_function(classifier, sample_weight, axis, title):
 sample_weight_last_ten[15:] *= 5
 sample_weight_last_ten[9] *= 15
 
-# for reference, first fit without class weights
+# for reference, first fit without sample weights
 
 # fit the model
 clf_weights = svm.SVC(gamma=1)
diff --git a/setup.cfg b/setup.cfg
index 09c5c9829a..93aca4a44f 100755
--- a/setup.cfg
+++ b/setup.cfg
@@ -5,6 +5,10 @@ test = pytest
 # disable-pytest-warnings should be removed once we rewrite tests
 # using yield with parametrize
 addopts =
+    --ignore build_tools
+    --ignore benchmarks
+    --ignore doc
+    --ignore examples
     --doctest-modules
     --disable-pytest-warnings
     -rs
diff --git a/sklearn/__init__.py b/sklearn/__init__.py
index 1d7cd2ef92..aafc8a34b2 100755
--- a/sklearn/__init__.py
+++ b/sklearn/__init__.py
@@ -44,7 +44,7 @@
 # Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
 # 'X.Y.dev0' is the canonical version of 'X.Y.dev'
 #
-__version__ = '0.20.dev0'
+__version__ = '0.21.dev0'
 
 
 try:
diff --git a/sklearn/base.py b/sklearn/base.py
index d75adb06d6..56ffb18bf8 100755
--- a/sklearn/base.py
+++ b/sklearn/base.py
@@ -342,9 +342,12 @@ def fit_predict(self, X, y=None):
         X : ndarray, shape (n_samples, n_features)
             Input data.
 
+        y : Ignored
+            not used, present for API consistency by convention.
+
         Returns
         -------
-        y : ndarray, shape (n_samples,)
+        labels : ndarray, shape (n_samples,)
             cluster labels
         """
         # non-optimized default implementation; override when a better
@@ -494,6 +497,9 @@ def fit_predict(self, X, y=None):
         X : ndarray, shape (n_samples, n_features)
             Input data.
 
+        y : Ignored
+            not used, present for API consistency by convention.
+
         Returns
         -------
         y : ndarray, shape (n_samples,)
diff --git a/sklearn/cluster/_optics_inner.pyx b/sklearn/cluster/_optics_inner.pyx
index 24e8619078..0c1b056bb1 100755
--- a/sklearn/cluster/_optics_inner.pyx
+++ b/sklearn/cluster/_optics_inner.pyx
@@ -5,6 +5,13 @@ cimport cython
 ctypedef np.float64_t DTYPE_t
 ctypedef np.int_t DTYPE
 
+# as defined in PEP485 (python3.5)
+cdef inline isclose(double a, 
+                    double b,
+                    double rel_tol=1e-09,
+                    double abs_tol=0.0):
+    return abs(a-b) <= max(rel_tol * max(abs(a), abs(b)), abs_tol)
+
 @cython.boundscheck(False)
 @cython.wraparound(False)
 # Checks for smallest reachability distance
@@ -24,7 +31,7 @@ cpdef quick_scan(double[:] rdists, double[:] dists):
             rdist = rdists[i]
             dist = dists[i]
             idx = i
-        if rdists[i] == rdist:
+        elif isclose(rdists[i], rdist):
             if dists[i] < dist:
                 dist = dists[i]
                 idx = i
diff --git a/sklearn/cluster/bicluster.py b/sklearn/cluster/bicluster.py
index 7425e17fcd..8bbf735312 100755
--- a/sklearn/cluster/bicluster.py
+++ b/sklearn/cluster/bicluster.py
@@ -228,15 +228,14 @@ class SpectralCoclustering(BaseSpectral):
         chosen and the algorithm runs once. Otherwise, the algorithm
         is run for each initialization and the best solution chosen.
 
-    n_jobs : int, optional, default: 1
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. This works by breaking
         down the pairwise matrix into n_jobs even slices and computing them in
         parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For n_jobs below -1,
-        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
-        are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None (default)
         Used for randomizing the singular value decomposition and the k-means
@@ -375,15 +374,14 @@ class SpectralBiclustering(BaseSpectral):
         chosen and the algorithm runs once. Otherwise, the algorithm
         is run for each initialization and the best solution chosen.
 
-    n_jobs : int, optional, default: 1
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. This works by breaking
         down the pairwise matrix into n_jobs even slices and computing them in
         parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For n_jobs below -1,
-        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
-        are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None (default)
         Used for randomizing the singular value decomposition and the k-means
diff --git a/sklearn/cluster/dbscan_.py b/sklearn/cluster/dbscan_.py
index e50f3cf89f..cd17a7132d 100755
--- a/sklearn/cluster/dbscan_.py
+++ b/sklearn/cluster/dbscan_.py
@@ -77,9 +77,11 @@ def dbscan(X, eps=0.5, min_samples=5, metric='minkowski', metric_params=None,
         weight may inhibit its eps-neighbor from being core.
         Note that weights are absolute, and default to 1.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Returns
     -------
@@ -232,9 +234,11 @@ class DBSCAN(BaseEstimator, ClusterMixin):
         The power of the Minkowski metric to be used to calculate distance
         between points.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
diff --git a/sklearn/cluster/k_means_.py b/sklearn/cluster/k_means_.py
index 444d8fca29..5fbe8810e5 100755
--- a/sklearn/cluster/k_means_.py
+++ b/sklearn/cluster/k_means_.py
@@ -261,14 +261,13 @@ def k_means(X, n_clusters, sample_weight=None, init='k-means++',
         the data mean, in this case it will also not ensure that data is
         C-contiguous which may cause a significant slowdown.
 
-    n_jobs : int
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. This works by computing
         each of the n_init runs in parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For n_jobs below -1,
-        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
-        are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     algorithm : "auto", "full" or "elkan", default="auto"
         K-means algorithm to use. The classical EM-style algorithm is "full".
@@ -834,14 +833,13 @@ class KMeans(BaseEstimator, ClusterMixin, TransformerMixin):
         the data mean, in this case it will also not ensure that data is
         C-contiguous which may cause a significant slowdown.
 
-    n_jobs : int
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. This works by computing
         each of the n_init runs in parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For n_jobs below -1,
-        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
-        are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     algorithm : "auto", "full" or "elkan", default="auto"
         K-means algorithm to use. The classical EM-style algorithm is "full".
@@ -950,6 +948,7 @@ def fit(self, X, y=None, sample_weight=None):
             copy if the given data is not C-contiguous.
 
         y : Ignored
+            not used, present here for API consistency by convention.
 
         sample_weight : array-like, shape (n_samples,), optional
             The weights for each observation in X. If None, all observations
@@ -981,6 +980,7 @@ def fit_predict(self, X, y=None, sample_weight=None):
             New data to transform.
 
         y : Ignored
+            not used, present here for API consistency by convention.
 
         sample_weight : array-like, shape (n_samples,), optional
             The weights for each observation in X. If None, all observations
@@ -1004,6 +1004,7 @@ def fit_transform(self, X, y=None, sample_weight=None):
             New data to transform.
 
         y : Ignored
+            not used, present here for API consistency by convention.
 
         sample_weight : array-like, shape (n_samples,), optional
             The weights for each observation in X. If None, all observations
@@ -1083,6 +1084,7 @@ def score(self, X, y=None, sample_weight=None):
             New data.
 
         y : Ignored
+            not used, present here for API consistency by convention.
 
         sample_weight : array-like, shape (n_samples,), optional
             The weights for each observation in X. If None, all observations
@@ -1475,6 +1477,7 @@ def fit(self, X, y=None, sample_weight=None):
             if the given data is not C-contiguous.
 
         y : Ignored
+            not used, present here for API consistency by convention.
 
         sample_weight : array-like, shape (n_samples,), optional
             The weights for each observation in X. If None, all observations
@@ -1656,6 +1659,7 @@ def partial_fit(self, X, y=None, sample_weight=None):
             X will be copied if it is not C-contiguous.
 
         y : Ignored
+            not used, present here for API consistency by convention.
 
         sample_weight : array-like, shape (n_samples,), optional
             The weights for each observation in X. If None, all observations
diff --git a/sklearn/cluster/mean_shift_.py b/sklearn/cluster/mean_shift_.py
index 04d94ee419..800c85c365 100755
--- a/sklearn/cluster/mean_shift_.py
+++ b/sklearn/cluster/mean_shift_.py
@@ -53,9 +53,11 @@ def estimate_bandwidth(X, quantile=0.3, n_samples=None, random_state=0,
         deterministic.
         See :term:`Glossary <random_state>`.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Returns
     -------
@@ -152,14 +154,13 @@ def mean_shift(X, bandwidth=None, seeds=None, bin_seeding=False,
         Maximum number of iterations, per seed point before the clustering
         operation terminates (for that seed point), if has not converged yet.
 
-    n_jobs : int
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. This works by computing
         each of the n_init runs in parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For n_jobs below -1,
-        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
-        are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
         .. versionadded:: 0.17
            Parallel Execution using *n_jobs*.
@@ -214,8 +215,10 @@ def mean_shift(X, bandwidth=None, seeds=None, bin_seeding=False,
     # If the distance between two kernels is less than the bandwidth,
     # then we have to remove one because it is a duplicate. Remove the
     # one with fewer points.
+
     sorted_by_intensity = sorted(center_intensity_dict.items(),
-                                 key=lambda tup: tup[1], reverse=True)
+                                 key=lambda tup: (tup[1], tup[0]),
+                                 reverse=True)
     sorted_centers = np.array([tup[0] for tup in sorted_by_intensity])
     unique = np.ones(len(sorted_centers), dtype=np.bool)
     nbrs = NearestNeighbors(radius=bandwidth,
@@ -334,14 +337,13 @@ class MeanShift(BaseEstimator, ClusterMixin):
         not within any kernel. Orphans are assigned to the nearest kernel.
         If false, then orphans are given cluster label -1.
 
-    n_jobs : int
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. This works by computing
         each of the n_init runs in parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For n_jobs below -1,
-        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
-        are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -359,9 +361,9 @@ class MeanShift(BaseEstimator, ClusterMixin):
     ...               [4, 7], [3, 5], [3, 6]])
     >>> clustering = MeanShift(bandwidth=2).fit(X)
     >>> clustering.labels_
-    array([0, 0, 0, 1, 1, 1])
+    array([1, 1, 1, 0, 0, 0])
     >>> clustering.predict([[0, 0], [5, 5]])
-    array([0, 1])
+    array([1, 0])
     >>> clustering # doctest: +NORMALIZE_WHITESPACE
     MeanShift(bandwidth=2, bin_seeding=False, cluster_all=True, min_bin_freq=1,
          n_jobs=None, seeds=None)
diff --git a/sklearn/cluster/optics_.py b/sklearn/cluster/optics_.py
index b623a15f53..19b6a79f45 100755
--- a/sklearn/cluster/optics_.py
+++ b/sklearn/cluster/optics_.py
@@ -21,18 +21,19 @@
 from ._optics_inner import quick_scan
 
 
-def optics(X, min_samples=5, max_bound=np.inf, metric='euclidean',
+def optics(X, min_samples=5, max_eps=np.inf, metric='euclidean',
            p=2, metric_params=None, maxima_ratio=.75,
            rejection_ratio=.7, similarity_threshold=0.4,
-           significant_min=.003, min_cluster_size_ratio=.005,
+           significant_min=.003, min_cluster_size=.005,
            min_maxima_ratio=0.001, algorithm='ball_tree',
            leaf_size=30, n_jobs=None):
     """Perform OPTICS clustering from vector array
 
     OPTICS: Ordering Points To Identify the Clustering Structure
-    Equivalent to DBSCAN, finds core sample of high density and expands
+    Closely related to DBSCAN, finds core sample of high density and expands
     clusters from them. Unlike DBSCAN, keeps cluster hierarchy for a variable
-    neighborhood radius. Optimized for usage on large point datasets.
+    neighborhood radius. Better suited for usage on large point datasets than
+    the current sklearn implementation of DBSCAN.
 
     Read more in the :ref:`User Guide <optics>`.
 
@@ -41,23 +42,41 @@ def optics(X, min_samples=5, max_bound=np.inf, metric='euclidean',
     X : array, shape (n_samples, n_features)
         The data.
 
-    min_samples : int
+    min_samples : int (default=5)
         The number of samples in a neighborhood for a point to be considered
         as a core point.
 
-    max_bound : float, optional
+    max_eps : float, optional (default=np.inf)
         The maximum distance between two samples for them to be considered
-        as in the same neighborhood. This is also the largest object size
-        expected within the dataset. Default value of "np.inf" will identify
-        clusters across all scales; reducing `max_bound` will result in
+        as in the same neighborhood. Default value of "np.inf" will identify
+        clusters across all scales; reducing `max_eps` will result in
         shorter run times.
 
-    metric : string or callable, optional
-        The distance metric to use for neighborhood lookups. Default is
-        "minkowski". Other options include "euclidean", "manhattan",
-        "chebyshev", "haversine", "seuclidean", "hamming", "canberra",
-        and "braycurtis". The "wminkowski" and "mahalanobis" metrics are
-        also valid with an additional argument.
+    metric : string or callable, optional (default='euclidean')
+        metric to use for distance computation. Any metric from scikit-learn
+        or scipy.spatial.distance can be used.
+
+        If metric is a callable function, it is called on each
+        pair of instances (rows) and the resulting value recorded. The callable
+        should take two arrays as input and return one value indicating the
+        distance between them. This works for Scipy's metrics, but is less
+        efficient than passing the metric name as a string.
+
+        Distance matrices are not supported.
+
+        Valid values for metric are:
+
+        - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',
+          'manhattan']
+
+        - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',
+          'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',
+          'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',
+          'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',
+          'yule']
+
+        See the documentation for scipy.spatial.distance for details on these
+        metrics.
 
     p : integer, optional (default=2)
         Parameter for the Minkowski metric from
@@ -68,20 +87,20 @@ def optics(X, min_samples=5, max_bound=np.inf, metric='euclidean',
     metric_params : dict, optional (default=None)
         Additional keyword arguments for the metric function.
 
-    maxima_ratio : float, optional
+    maxima_ratio : float, optional (default=.75)
         The maximum ratio we allow of average height of clusters on the
         right and left to the local maxima in question. The higher the
         ratio, the more generous the algorithm is to preserving local
         minima, and the more cuts the resulting tree will have.
 
-    rejection_ratio : float, optional
+    rejection_ratio : float, optional (default=.7)
         Adjusts the fitness of the clustering. When the maxima_ratio is
         exceeded, determine which of the clusters to the left and right to
         reject based on rejection_ratio. Higher values will result in points
         being more readily classified as noise; conversely, lower values will
         result in more points being clustered.
 
-    similarity_threshold : float, optional
+    similarity_threshold : float, optional (default=.4)
         Used to check if nodes can be moved up one level, that is, if the
         new cluster created is too "similar" to its parent, given the
         similarity threshold. Similarity can be determined by 1) the size
@@ -91,19 +110,23 @@ def optics(X, min_samples=5, max_bound=np.inf, metric='euclidean',
         node. A lower value for the similarity threshold means less levels
         in the tree.
 
-    significant_min : float, optional
+    significant_min : float, optional (default=.003)
         Sets a lower threshold on how small a significant maxima can be.
 
-    min_cluster_size_ratio : float, optional
-        Minimum percentage of dataset expected for cluster membership.
+    min_cluster_size : int > 1 or float between 0 and 1 (default=0.005)
+        Minimum number of samples in an OPTICS cluster, expressed as an
+        absolute number or a fraction of the number of samples (rounded
+        to be at least 2).
 
-    min_maxima_ratio : float, optional
+    min_maxima_ratio : float, optional (default=.001)
         Used to determine neighborhood size for minimum cluster membership.
+        Each local maxima should be a largest value in a neighborhood
+        of the `size min_maxima_ratio * len(X)` from left and right.
 
     algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional
         Algorithm used to compute the nearest neighbors:
 
-        - 'ball_tree' will use :class:`BallTree`
+        - 'ball_tree' will use :class:`BallTree` (default)
         - 'kd_tree' will use :class:`KDTree`
         - 'brute' will use a brute-force search.
         - 'auto' will attempt to decide the most appropriate algorithm
@@ -118,9 +141,11 @@ def optics(X, min_samples=5, max_bound=np.inf, metric='euclidean',
         required to store the tree. The optimal value depends on the
         nature of the problem.
 
-    n_jobs : int, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Returns
     -------
@@ -145,10 +170,10 @@ def optics(X, min_samples=5, max_bound=np.inf, metric='euclidean',
     Record 28, no. 2 (1999): 49-60.
     """
 
-    clust = OPTICS(min_samples, max_bound, metric, p, metric_params,
+    clust = OPTICS(min_samples, max_eps, metric, p, metric_params,
                    maxima_ratio, rejection_ratio,
                    similarity_threshold, significant_min,
-                   min_cluster_size_ratio, min_maxima_ratio,
+                   min_cluster_size, min_maxima_ratio,
                    algorithm, leaf_size, n_jobs)
     clust.fit(X)
     return clust.core_sample_indices_, clust.labels_
@@ -158,31 +183,50 @@ class OPTICS(BaseEstimator, ClusterMixin):
     """Estimate clustering structure from vector array
 
     OPTICS: Ordering Points To Identify the Clustering Structure
-    Equivalent to DBSCAN, finds core sample of high density and expands
+    Closely related to DBSCAN, finds core sample of high density and expands
     clusters from them. Unlike DBSCAN, keeps cluster hierarchy for a variable
-    neighborhood radius. Optimized for usage on large point datasets.
+    neighborhood radius. Better suited for usage on large point datasets than
+    the current sklearn implementation of DBSCAN.
 
     Read more in the :ref:`User Guide <optics>`.
 
     Parameters
     ----------
-    min_samples : int
+    min_samples : int (default=5)
         The number of samples in a neighborhood for a point to be considered
         as a core point.
 
-    max_bound : float, optional
+    max_eps : float, optional (default=np.inf)
         The maximum distance between two samples for them to be considered
-        as in the same neighborhood. This is also the largest object size
-        expected within the dataset. Default value of "np.inf" will identify
-        clusters across all scales; reducing `max_bound` will result in
+        as in the same neighborhood. Default value of "np.inf" will identify
+        clusters across all scales; reducing `max_eps` will result in
         shorter run times.
 
-    metric : string or callable, optional
-        The distance metric to use for neighborhood lookups. Default is
-        "minkowski". Other options include "euclidean", "manhattan",
-        "chebyshev", "haversine", "seuclidean", "hamming", "canberra",
-        and "braycurtis". The "wminkowski" and "mahalanobis" metrics are
-        also valid with an additional argument.
+    metric : string or callable, optional (default='euclidean')
+        metric to use for distance computation. Any metric from scikit-learn
+        or scipy.spatial.distance can be used.
+
+        If metric is a callable function, it is called on each
+        pair of instances (rows) and the resulting value recorded. The callable
+        should take two arrays as input and return one value indicating the
+        distance between them. This works for Scipy's metrics, but is less
+        efficient than passing the metric name as a string.
+
+        Distance matrices are not supported.
+
+        Valid values for metric are:
+
+        - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',
+          'manhattan']
+
+        - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',
+          'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',
+          'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',
+          'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',
+          'yule']
+
+        See the documentation for scipy.spatial.distance for details on these
+        metrics.
 
     p : integer, optional (default=2)
         Parameter for the Minkowski metric from
@@ -193,20 +237,20 @@ class OPTICS(BaseEstimator, ClusterMixin):
     metric_params : dict, optional (default=None)
         Additional keyword arguments for the metric function.
 
-    maxima_ratio : float, optional
+    maxima_ratio : float, optional (default=.75)
         The maximum ratio we allow of average height of clusters on the
         right and left to the local maxima in question. The higher the
         ratio, the more generous the algorithm is to preserving local
         minima, and the more cuts the resulting tree will have.
 
-    rejection_ratio : float, optional
+    rejection_ratio : float, optional (default=.7)
         Adjusts the fitness of the clustering. When the maxima_ratio is
         exceeded, determine which of the clusters to the left and right to
         reject based on rejection_ratio. Higher values will result in points
         being more readily classified as noise; conversely, lower values will
         result in more points being clustered.
 
-    similarity_threshold : float, optional
+    similarity_threshold : float, optional (default=.4)
         Used to check if nodes can be moved up one level, that is, if the
         new cluster created is too "similar" to its parent, given the
         similarity threshold. Similarity can be determined by 1) the size
@@ -216,19 +260,23 @@ class OPTICS(BaseEstimator, ClusterMixin):
         node. A lower value for the similarity threshold means less levels
         in the tree.
 
-    significant_min : float, optional
+    significant_min : float, optional (default=.003)
         Sets a lower threshold on how small a significant maxima can be.
 
-    min_cluster_size_ratio : float, optional
-        Minimum percentage of dataset expected for cluster membership.
+    min_cluster_size : int > 1 or float between 0 and 1 (default=0.005)
+        Minimum number of samples in an OPTICS cluster, expressed as an
+        absolute number or a fraction of the number of samples (rounded
+        to be at least 2).
 
-    min_maxima_ratio : float, optional
+    min_maxima_ratio : float, optional (default=.001)
         Used to determine neighborhood size for minimum cluster membership.
+        Each local maxima should be a largest value in a neighborhood
+        of the `size min_maxima_ratio * len(X)` from left and right.
 
     algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional
         Algorithm used to compute the nearest neighbors:
 
-        - 'ball_tree' will use :class:`BallTree`
+        - 'ball_tree' will use :class:`BallTree` (default)
         - 'kd_tree' will use :class:`KDTree`
         - 'brute' will use a brute-force search.
         - 'auto' will attempt to decide the most appropriate algorithm
@@ -243,9 +291,11 @@ class OPTICS(BaseEstimator, ClusterMixin):
         required to store the tree. The optimal value depends on the
         nature of the problem.
 
-    n_jobs : int, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -280,20 +330,20 @@ class OPTICS(BaseEstimator, ClusterMixin):
     Record 28, no. 2 (1999): 49-60.
     """
 
-    def __init__(self, min_samples=5, max_bound=np.inf, metric='euclidean',
+    def __init__(self, min_samples=5, max_eps=np.inf, metric='euclidean',
                  p=2, metric_params=None, maxima_ratio=.75,
                  rejection_ratio=.7, similarity_threshold=0.4,
-                 significant_min=.003, min_cluster_size_ratio=.005,
+                 significant_min=.003, min_cluster_size=.005,
                  min_maxima_ratio=0.001, algorithm='ball_tree',
                  leaf_size=30, n_jobs=None):
 
-        self.max_bound = max_bound
+        self.max_eps = max_eps
         self.min_samples = min_samples
         self.maxima_ratio = maxima_ratio
         self.rejection_ratio = rejection_ratio
         self.similarity_threshold = similarity_threshold
         self.significant_min = significant_min
-        self.min_cluster_size_ratio = min_cluster_size_ratio
+        self.min_cluster_size = min_cluster_size
         self.min_maxima_ratio = min_maxima_ratio
         self.algorithm = algorithm
         self.metric = metric
@@ -306,7 +356,7 @@ def fit(self, X, y=None):
         """Perform OPTICS clustering
 
         Extracts an ordered list of points and reachability distances, and
-        performs initial clustering using `max_bound` distance specified at
+        performs initial clustering using `max_eps` distance specified at
         OPTICS object instantiation.
 
         Parameters
@@ -324,22 +374,31 @@ def fit(self, X, y=None):
         X = check_array(X, dtype=np.float)
 
         n_samples = len(X)
+
+        if self.min_samples > n_samples:
+            raise ValueError("Number of training samples (n_samples=%d) must "
+                             "be greater than min_samples (min_samples=%d) "
+                             "used for clustering." %
+                             (n_samples, self.min_samples))
+
+        if self.min_cluster_size <= 0 or (self.min_cluster_size !=
+                                          int(self.min_cluster_size)
+                                          and self.min_cluster_size > 1):
+            raise ValueError('min_cluster_size must be a positive integer or '
+                             'a float between 0 and 1. Got %r' %
+                             self.min_cluster_size)
+        elif self.min_cluster_size > n_samples:
+            raise ValueError('min_cluster_size must be no greater than the '
+                             'number of samples (%d). Got %d' %
+                             (n_samples, self.min_cluster_size))
+
         # Start all points as 'unprocessed' ##
-        self._processed = np.zeros((n_samples, 1), dtype=bool)
         self.reachability_ = np.empty(n_samples)
         self.reachability_.fill(np.inf)
         self.core_distances_ = np.empty(n_samples)
         self.core_distances_.fill(np.nan)
         # Start all points as noise ##
         self.labels_ = np.full(n_samples, -1, dtype=int)
-        self.ordering_ = []
-
-        # Check for valid n_samples relative to min_samples
-        if self.min_samples > n_samples:
-            raise ValueError("Number of training samples (n_samples=%d) must "
-                             "be greater than min_samples (min_samples=%d) "
-                             "used for clustering." %
-                             (n_samples, self.min_samples))
 
         nbrs = NearestNeighbors(n_neighbors=self.min_samples,
                                 algorithm=self.algorithm,
@@ -351,11 +410,7 @@ def fit(self, X, y=None):
         self.core_distances_[:] = nbrs.kneighbors(X,
                                                   self.min_samples)[0][:, -1]
 
-        # Main OPTICS loop. Not parallelizable. The order that entries are
-        # written to the 'ordering_' list is important!
-        for point in range(n_samples):
-            if not self._processed[point]:
-                self._expand_cluster_order(point, X, nbrs)
+        self.ordering_ = self._calculate_optics_order(X, nbrs)
 
         indices_, self.labels_ = _extract_optics(self.ordering_,
                                                  self.reachability_,
@@ -363,66 +418,75 @@ def fit(self, X, y=None):
                                                  self.rejection_ratio,
                                                  self.similarity_threshold,
                                                  self.significant_min,
-                                                 self.min_cluster_size_ratio,
+                                                 self.min_cluster_size,
                                                  self.min_maxima_ratio)
         self.core_sample_indices_ = indices_
-        self.n_clusters_ = np.max(self.labels_)
         return self
 
-    # OPTICS helper functions; these should not be public #
-
-    def _expand_cluster_order(self, point, X, nbrs):
-        # As above, not parallelizable. Parallelizing would allow items in
-        # the 'unprocessed' list to switch to 'processed'
-        if self.core_distances_[point] <= self.max_bound:
-            while not self._processed[point]:
-                self._processed[point] = True
-                self.ordering_.append(point)
-                point = self._set_reach_dist(point, X, nbrs)
-        else:  # For very noisy points
-            self.ordering_.append(point)
-            self._processed[point] = True
-
-    def _set_reach_dist(self, point_index, X, nbrs):
-        P = np.array(X[point_index]).reshape(1, -1)
-        indices = nbrs.radius_neighbors(P, radius=self.max_bound,
+    # OPTICS helper functions
+
+    def _calculate_optics_order(self, X, nbrs):
+        # Main OPTICS loop. Not parallelizable. The order that entries are
+        # written to the 'ordering_' list is important!
+        processed = np.zeros(X.shape[0], dtype=bool)
+        ordering = np.zeros(X.shape[0], dtype=int)
+        ordering_idx = 0
+        for point in range(X.shape[0]):
+            if processed[point]:
+                continue
+            if self.core_distances_[point] <= self.max_eps:
+                while not processed[point]:
+                    processed[point] = True
+                    ordering[ordering_idx] = point
+                    ordering_idx += 1
+                    point = self._set_reach_dist(point, processed, X, nbrs)
+            else:  # For very noisy points
+                ordering[ordering_idx] = point
+                ordering_idx += 1
+                processed[point] = True
+        return ordering
+
+    def _set_reach_dist(self, point_index, processed, X, nbrs):
+        P = X[point_index:point_index + 1]
+        indices = nbrs.radius_neighbors(P, radius=self.max_eps,
                                         return_distance=False)[0]
 
         # Getting indices of neighbors that have not been processed
-        unproc = np.compress((~np.take(self._processed, indices)).ravel(),
+        unproc = np.compress((~np.take(processed, indices)).ravel(),
                              indices, axis=0)
         # Keep n_jobs = 1 in the following lines...please
-        if len(unproc) > 0:
+        if not unproc.size:
+            # Everything is already processed. Return to main loop
+            return point_index
+
+        if self.metric == 'precomputed':
+            dists = X[point_index, unproc]
+        else:
             dists = pairwise_distances(P, np.take(X, unproc, axis=0),
                                        self.metric, n_jobs=None).ravel()
 
-            rdists = np.maximum(dists, self.core_distances_[point_index])
-            new_reach = np.minimum(np.take(self.reachability_, unproc), rdists)
-            self.reachability_[unproc] = new_reach
+        rdists = np.maximum(dists, self.core_distances_[point_index])
+        new_reach = np.minimum(np.take(self.reachability_, unproc), rdists)
+        self.reachability_[unproc] = new_reach
 
-        # Checks to see if everything is already processed;
-        # if so, return control to main loop
-        if unproc.size > 0:
-            # Define return order based on reachability distance
-            return(unproc[quick_scan(np.take(self.reachability_, unproc),
-                                     dists)])
-        else:
-            return point_index
+        # Define return order based on reachability distance
+        return (unproc[quick_scan(np.take(self.reachability_, unproc),
+                                  dists)])
 
     def extract_dbscan(self, eps):
         """Performs DBSCAN extraction for an arbitrary epsilon.
 
-        Extraction runs in linear time. Note that if the `max_bound` OPTICS
+        Extraction runs in linear time. Note that if the `max_eps` OPTICS
         parameter was set to < inf for extracting reachability and ordering
         arrays, DBSCAN extractions will be unstable for `eps` values close to
-        `max_bound`. Setting `eps` < (`max_bound` / 5.0) will guarantee
+        `max_eps`. Setting `eps` < (`max_eps` / 5.0) will guarantee
         extraction parity with DBSCAN.
 
         Parameters
         ----------
         eps : float or int, required
-            DBSCAN `eps` parameter. Must be set to < `max_bound`. Equivalence
-            with DBSCAN algorithm is achieved if `eps` is < (`max_bound` / 5)
+            DBSCAN `eps` parameter. Must be set to < `max_eps`. Equivalence
+            with DBSCAN algorithm is achieved if `eps` is < (`max_eps` / 5)
 
         Returns
         -------
@@ -434,14 +498,14 @@ def extract_dbscan(self, eps):
         """
         check_is_fitted(self, 'reachability_')
 
-        if eps > self.max_bound:
+        if eps > self.max_eps:
             raise ValueError('Specify an epsilon smaller than %s. Got %s.'
-                             % (self.max_bound, eps))
+                             % (self.max_eps, eps))
 
-        if eps * 5.0 > (self.max_bound * 1.05):
+        if eps * 5.0 > (self.max_eps * 1.05):
             warnings.warn(
-                "Warning, max_bound (%s) is close to eps (%s): "
-                "Output may be unstable." % (self.max_bound, eps),
+                "Warning, max_eps (%s) is close to eps (%s): "
+                "Output may be unstable." % (self.max_eps, eps),
                 RuntimeWarning, stacklevel=2)
         # Stability warning is documented in _extract_dbscan method...
 
@@ -486,7 +550,7 @@ def _extract_dbscan(ordering, core_distances, reachability, eps):
 
 def _extract_optics(ordering, reachability, maxima_ratio=.75,
                     rejection_ratio=.7, similarity_threshold=0.4,
-                    significant_min=.003, min_cluster_size_ratio=.005,
+                    significant_min=.003, min_cluster_size=.005,
                     min_maxima_ratio=0.001):
     """Performs automatic cluster extraction for variable density data.
 
@@ -524,8 +588,10 @@ def _extract_optics(ordering, reachability, maxima_ratio=.75,
     significant_min : float, optional
         Sets a lower threshold on how small a significant maxima can be.
 
-    min_cluster_size_ratio : float, optional
-        Minimum percentage of dataset expected for cluster membership.
+    min_cluster_size : int > 1 or float between 0 and 1
+        Minimum number of samples in an OPTICS cluster, expressed as an
+        absolute number or a fraction of the number of samples (rounded
+        to be at least 2).
 
     min_maxima_ratio : float, optional
         Used to determine neighborhood size for minimum cluster membership.
@@ -545,7 +611,7 @@ def _extract_optics(ordering, reachability, maxima_ratio=.75,
     root_node = _automatic_cluster(reachability_plot, ordering,
                                    maxima_ratio, rejection_ratio,
                                    similarity_threshold, significant_min,
-                                   min_cluster_size_ratio, min_maxima_ratio)
+                                   min_cluster_size, min_maxima_ratio)
     leaves = _get_leaves(root_node, [])
     # Start cluster id's at 0
     clustid = 0
@@ -564,7 +630,7 @@ def _extract_optics(ordering, reachability, maxima_ratio=.75,
 def _automatic_cluster(reachability_plot, ordering,
                        maxima_ratio, rejection_ratio,
                        similarity_threshold, significant_min,
-                       min_cluster_size_ratio, min_maxima_ratio):
+                       min_cluster_size, min_maxima_ratio):
     """Converts reachability plot to cluster tree and returns root node.
 
     Parameters
@@ -576,13 +642,10 @@ def _automatic_cluster(reachability_plot, ordering,
     """
 
     min_neighborhood_size = 2
-    min_cluster_size = int(min_cluster_size_ratio * len(ordering))
+    if min_cluster_size <= 1:
+        min_cluster_size = max(2, min_cluster_size * len(ordering))
     neighborhood_size = int(min_maxima_ratio * len(ordering))
 
-    # Should this check for < min_samples? Should this be public?
-    if min_cluster_size < 5:
-        min_cluster_size = 5
-
     # Again, should this check < min_samples, should the parameter be public?
     if neighborhood_size < min_neighborhood_size:
         neighborhood_size = min_neighborhood_size
@@ -608,12 +671,6 @@ def __init__(self, points, start, end, parent_node):
         self.children = []
         self.split_point = -1
 
-    def assign_split_point(self, split_point):
-        self.split_point = split_point
-
-    def add_child(self, child):
-        self.children.append(child)
-
 
 def _is_local_maxima(index, reachability_plot, neighborhood_size):
     right_idx = slice(index + 1, index + neighborhood_size + 1)
@@ -657,7 +714,7 @@ def _cluster_tree(node, parent_node, local_maxima_points,
 
     # take largest local maximum as possible separation between clusters
     s = local_maxima_points[0]
-    node.assign_split_point(s)
+    node.split_point = s
     local_maxima_points = local_maxima_points[1:]
 
     # create two new nodes and add to list of nodes
@@ -679,12 +736,8 @@ def _cluster_tree(node, parent_node, local_maxima_points,
     node_list.append((node_2, local_max_2))
 
     if reachability_plot[s] < significant_min:
-        node.assign_split_point(-1)
+        node.split_point = -1
         # if split_point is not significant, ignore this split and continue
-        _cluster_tree(node, parent_node, local_maxima_points,
-                      reachability_plot, reachability_ordering,
-                      min_cluster_size, maxima_ratio, rejection_ratio,
-                      similarity_threshold, significant_min)
         return
 
     # only check a certain ratio of points in the child
@@ -711,7 +764,7 @@ def _cluster_tree(node, parent_node, local_maxima_points,
                 (avg_reach2 / reachability_plot[s]) >= rejection_ratio):
             # since split_point is not significant,
             # ignore this split and continue (reject both child nodes)
-            node.assign_split_point(-1)
+            node.split_point = -1
             _cluster_tree(node, parent_node, local_maxima_points,
                           reachability_plot, reachability_ordering,
                           min_cluster_size, maxima_ratio, rejection_ratio,
@@ -729,7 +782,7 @@ def _cluster_tree(node, parent_node, local_maxima_points,
         node_list.remove((node_2, local_max_2))
     if not node_list:
         # parent_node will be a leaf
-        node.assign_split_point(-1)
+        node.split_point = -1
         return
 
     # Check if nodes can be moved up one level - the new cluster created
@@ -744,13 +797,13 @@ def _cluster_tree(node, parent_node, local_maxima_points,
 
     for nl in node_list:
         if bypass_node == 1:
-            parent_node.add_child(nl[0])
+            parent_node.children.append(nl[0])
             _cluster_tree(nl[0], parent_node, nl[1],
                           reachability_plot, reachability_ordering,
                           min_cluster_size, maxima_ratio, rejection_ratio,
                           similarity_threshold, significant_min)
         else:
-            node.add_child(nl[0])
+            node.children.append(nl[0])
             _cluster_tree(nl[0], node, nl[1], reachability_plot,
                           reachability_ordering, min_cluster_size,
                           maxima_ratio, rejection_ratio,
diff --git a/sklearn/cluster/spectral.py b/sklearn/cluster/spectral.py
index 6e9290eb3d..31a2046dbf 100755
--- a/sklearn/cluster/spectral.py
+++ b/sklearn/cluster/spectral.py
@@ -358,9 +358,11 @@ class SpectralClustering(BaseEstimator, ClusterMixin):
         Parameters (keyword arguments) and values for kernel passed as
         callable object. Ignored by other kernels.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
diff --git a/sklearn/cluster/tests/test_k_means.py b/sklearn/cluster/tests/test_k_means.py
index 7935e7134d..5994c770db 100755
--- a/sklearn/cluster/tests/test_k_means.py
+++ b/sklearn/cluster/tests/test_k_means.py
@@ -885,7 +885,7 @@ def test_sparse_validate_centers():
     # Test that a ValueError is raised for validate_center_shape
     classifier = KMeans(n_clusters=3, init=centers, n_init=1)
 
-    msg = "The shape of the initial centers \(\(4L?, 4L?\)\) " \
+    msg = r"The shape of the initial centers \(\(4L?, 4L?\)\) " \
           "does not match the number of clusters 3"
     assert_raises_regex(ValueError, msg, classifier.fit, X)
 
@@ -969,7 +969,7 @@ def test_sample_weight_length():
     # check that an error is raised when passing sample weights
     # with an incompatible shape
     km = KMeans(n_clusters=n_clusters, random_state=42)
-    assert_raises_regex(ValueError, 'len\(sample_weight\)', km.fit, X,
+    assert_raises_regex(ValueError, r'len\(sample_weight\)', km.fit, X,
                         sample_weight=np.ones(2))
 
 
diff --git a/sklearn/cluster/tests/test_mean_shift.py b/sklearn/cluster/tests/test_mean_shift.py
index 1d6940a947..441f822cdb 100755
--- a/sklearn/cluster/tests/test_mean_shift.py
+++ b/sklearn/cluster/tests/test_mean_shift.py
@@ -101,6 +101,18 @@ def test_unfitted():
     assert_false(hasattr(ms, "labels_"))
 
 
+def test_cluster_intensity_tie():
+    X = np.array([[1, 1], [2, 1], [1, 0],
+                  [4, 7], [3, 5], [3, 6]])
+    c1 = MeanShift(bandwidth=2).fit(X)
+
+    X = np.array([[4, 7], [3, 5], [3, 6],
+                  [1, 1], [2, 1], [1, 0]])
+    c2 = MeanShift(bandwidth=2).fit(X)
+    assert_array_equal(c1.labels_, [1, 1, 1, 0, 0, 0])
+    assert_array_equal(c2.labels_, [0, 0, 0, 1, 1, 1])
+
+
 def test_bin_seeds():
     # Test the bin seeding technique which can be used in the mean shift
     # algorithm
diff --git a/sklearn/cluster/tests/test_optics.py b/sklearn/cluster/tests/test_optics.py
index 597785083c..1215746faa 100755
--- a/sklearn/cluster/tests/test_optics.py
+++ b/sklearn/cluster/tests/test_optics.py
@@ -2,6 +2,7 @@
 #          Amy X. Zhang <axz@mit.edu>
 # License: BSD 3 clause
 
+from __future__ import print_function, division
 import numpy as np
 import pytest
 
@@ -10,15 +11,28 @@
 from sklearn.cluster.optics_ import _TreeNode, _cluster_tree
 from sklearn.cluster.optics_ import _find_local_maxima
 from sklearn.metrics.cluster import contingency_matrix
+from sklearn.metrics.pairwise import pairwise_distances
 from sklearn.cluster.dbscan_ import DBSCAN
 from sklearn.utils.testing import assert_equal, assert_warns
-from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_raise_message
+from sklearn.utils.testing import assert_allclose
+from sklearn.utils import _IS_32BIT
 
 from sklearn.cluster.tests.common import generate_clustered_data
 
 
+rng = np.random.RandomState(0)
+n_points_per_cluster = 250
+C1 = [-5, -2] + .8 * rng.randn(n_points_per_cluster, 2)
+C2 = [4, -1] + .1 * rng.randn(n_points_per_cluster, 2)
+C3 = [1, -2] + .2 * rng.randn(n_points_per_cluster, 2)
+C4 = [-2, 3] + .3 * rng.randn(n_points_per_cluster, 2)
+C5 = [3, -2] + 1.6 * rng.randn(n_points_per_cluster, 2)
+C6 = [5, 6] + 2 * rng.randn(n_points_per_cluster, 2)
+X = np.vstack((C1, C2, C3, C4, C5, C6))
+
+
 def test_correct_number_of_clusters():
     # in 'auto' mode
 
@@ -26,12 +40,30 @@ def test_correct_number_of_clusters():
     X = generate_clustered_data(n_clusters=n_clusters)
     # Parameters chosen specifically for this task.
     # Compute OPTICS
-    clust = OPTICS(max_bound=5.0 * 6.0, min_samples=4, metric='euclidean')
+    clust = OPTICS(max_eps=5.0 * 6.0, min_samples=4, metric='euclidean')
     clust.fit(X)
     # number of clusters, ignoring noise if present
     n_clusters_1 = len(set(clust.labels_)) - int(-1 in clust.labels_)
     assert_equal(n_clusters_1, n_clusters)
 
+    # check attribute types and sizes
+    assert clust.core_sample_indices_.ndim == 1
+    assert clust.core_sample_indices_.size > 0
+    assert clust.core_sample_indices_.dtype.kind == 'i'
+
+    assert clust.labels_.shape == (len(X),)
+    assert clust.labels_.dtype.kind == 'i'
+
+    assert clust.reachability_.shape == (len(X),)
+    assert clust.reachability_.dtype.kind == 'f'
+
+    assert clust.core_distances_.shape == (len(X),)
+    assert clust.core_distances_.dtype.kind == 'f'
+
+    assert clust.ordering_.shape == (len(X),)
+    assert clust.ordering_.dtype.kind == 'i'
+    assert set(clust.ordering_) == set(range(len(X)))
+
 
 def test_minimum_number_of_sample_check():
     # test that we check a minimum number of samples
@@ -40,7 +72,7 @@ def test_minimum_number_of_sample_check():
 
     # Compute OPTICS
     X = [[1, 1]]
-    clust = OPTICS(max_bound=5.0 * 0.3, min_samples=10)
+    clust = OPTICS(max_eps=5.0 * 0.3, min_samples=10)
 
     # Run the fit
     assert_raise_message(ValueError, msg, clust.fit, X)
@@ -50,7 +82,7 @@ def test_empty_extract():
     # Test extract where fit() has not yet been run.
     msg = ("This OPTICS instance is not fitted yet. Call 'fit' with "
            "appropriate arguments before using this method.")
-    clust = OPTICS(max_bound=5.0 * 0.3, min_samples=10)
+    clust = OPTICS(max_eps=5.0 * 0.3, min_samples=10)
     assert_raise_message(ValueError, msg, clust.extract_dbscan, 0.01)
 
 
@@ -62,7 +94,7 @@ def test_bad_extract():
                                 cluster_std=0.4, random_state=0)
 
     # Compute OPTICS
-    clust = OPTICS(max_bound=5.0 * 0.003, min_samples=10)
+    clust = OPTICS(max_eps=5.0 * 0.003, min_samples=10)
     clust2 = clust.fit(X)
     assert_raise_message(ValueError, msg, clust2.extract_dbscan, 0.3)
 
@@ -75,7 +107,7 @@ def test_close_extract():
                                 cluster_std=0.4, random_state=0)
 
     # Compute OPTICS
-    clust = OPTICS(max_bound=1.0, min_samples=10)
+    clust = OPTICS(max_eps=1.0, min_samples=10)
     clust3 = clust.fit(X)
     # check warning when centers are passed
     assert_warns(RuntimeWarning, clust3.extract_dbscan, .3)
@@ -116,27 +148,36 @@ def test_dbscan_optics_parity(eps, min_samples):
 
 def test_auto_extract_hier():
     # Tests auto extraction gets correct # of clusters with varying density
+    clust = OPTICS(min_samples=9).fit(X)
+    assert_equal(len(set(clust.labels_)), 6)
 
-    # Generate sample data
-    rng = np.random.RandomState(0)
-    n_points_per_cluster = 250
 
-    C1 = [-5, -2] + .8 * rng.randn(n_points_per_cluster, 2)
-    C2 = [4, -1] + .1 * rng.randn(n_points_per_cluster, 2)
-    C3 = [1, -2] + .2 * rng.randn(n_points_per_cluster, 2)
-    C4 = [-2, 3] + .3 * rng.randn(n_points_per_cluster, 2)
-    C5 = [3, -2] + 1.6 * rng.randn(n_points_per_cluster, 2)
-    C6 = [5, 6] + 2 * rng.randn(n_points_per_cluster, 2)
-    X = np.vstack((C1, C2, C3, C4, C5, C6))
+# try arbitrary minimum sizes
+@pytest.mark.parametrize('min_cluster_size', range(2, X.shape[0] // 10, 23))
+def test_min_cluster_size(min_cluster_size):
+    redX = X[::10]  # reduce for speed
+    clust = OPTICS(min_samples=9, min_cluster_size=min_cluster_size).fit(redX)
+    cluster_sizes = np.bincount(clust.labels_[clust.labels_ != -1])
+    if cluster_sizes.size:
+        assert min(cluster_sizes) >= min_cluster_size
+    # check behaviour is the same when min_cluster_size is a fraction
+    clust_frac = OPTICS(min_samples=9,
+                        min_cluster_size=min_cluster_size / redX.shape[0])
+    clust_frac.fit(redX)
+    assert_array_equal(clust.labels_, clust_frac.labels_)
 
-    # Compute OPTICS
 
-    clust = OPTICS(min_samples=9)
+@pytest.mark.parametrize('min_cluster_size', [0, -1, 1.1, 2.2])
+def test_min_cluster_size_invalid(min_cluster_size):
+    clust = OPTICS(min_cluster_size=min_cluster_size)
+    with pytest.raises(ValueError, match="must be a positive integer or a "):
+        clust.fit(X)
 
-    # Run the fit
-    clust.fit(X)
 
-    assert_equal(len(set(clust.labels_)), 6)
+def test_min_cluster_size_invalid2():
+    clust = OPTICS(min_cluster_size=len(X) + 1)
+    with pytest.raises(ValueError, match="must be no greater than the "):
+        clust.fit(X)
 
 
 @pytest.mark.parametrize("reach, n_child, members", [
@@ -168,23 +209,7 @@ def test_cluster_sigmin_pruning(reach, n_child, members):
 def test_reach_dists():
     # Tests against known extraction array
 
-    rng = np.random.RandomState(0)
-    n_points_per_cluster = 250
-
-    C1 = [-5, -2] + .8 * rng.randn(n_points_per_cluster, 2)
-    C2 = [4, -1] + .1 * rng.randn(n_points_per_cluster, 2)
-    C3 = [1, -2] + .2 * rng.randn(n_points_per_cluster, 2)
-    C4 = [-2, 3] + .3 * rng.randn(n_points_per_cluster, 2)
-    C5 = [3, -2] + 1.6 * rng.randn(n_points_per_cluster, 2)
-    C6 = [5, 6] + 2 * rng.randn(n_points_per_cluster, 2)
-    X = np.vstack((C1, C2, C3, C4, C5, C6))
-
-    # Compute OPTICS
-
-    clust = OPTICS(min_samples=10, metric='minkowski')
-
-    # Run the fit
-    clust.fit(X)
+    clust = OPTICS(min_samples=10, metric='minkowski').fit(X)
 
     # Expected values, matches 'RD' results from:
     # http://chemometria.us.edu.pl/download/optics.py
@@ -405,4 +430,22 @@ def test_reach_dists():
          1.364861, 0.459580, 1.025370, 0.980304, 0.607592, 0.533907, 1.134650,
          0.446161, 0.629962]
 
-    assert_array_almost_equal(clust.reachability_, np.array(v))
+    # FIXME: known failure in 32bit Linux; numerical imprecision results in
+    # different ordering in quick_scan
+    if _IS_32BIT:  # pragma: no cover
+        assert_allclose(clust.reachability_, np.array(v), rtol=1e-2)
+    else:
+        # we compare to truncated decimals, so use atol
+        assert_allclose(clust.reachability_, np.array(v), atol=1e-5)
+
+
+def test_precomputed_dists():
+    redX = X[::10]
+    dists = pairwise_distances(redX, metric='euclidean')
+    clust1 = OPTICS(min_samples=10, algorithm='brute',
+                    metric='precomputed').fit(dists)
+    clust2 = OPTICS(min_samples=10, algorithm='brute',
+                    metric='euclidean').fit(redX)
+
+    assert_allclose(clust1.reachability_, clust2.reachability_)
+    assert_array_equal(clust1.labels_, clust2.labels_)
diff --git a/sklearn/compose/_column_transformer.py b/sklearn/compose/_column_transformer.py
index 13f38563ac..e09d2d09d7 100755
--- a/sklearn/compose/_column_transformer.py
+++ b/sklearn/compose/_column_transformer.py
@@ -93,8 +93,11 @@ class ColumnTransformer(_BaseComposition, TransformerMixin):
         the stacked result will be sparse or dense, respectively, and this
         keyword will be ignored.
 
-    n_jobs : int, optional
-        Number of jobs to run in parallel (default 1).
+    n_jobs : int or None, optional (default=None)
+        Number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     transformer_weights : dict, optional
         Multiplicative weights for features per transformer. The output of the
@@ -666,8 +669,11 @@ def make_column_transformer(*transformers, **kwargs):
         non-specified columns will use the ``remainder`` estimator. The
         estimator must support `fit` and `transform`.
 
-    n_jobs : int, optional
-        Number of jobs to run in parallel (default 1).
+    n_jobs : int or None, optional (default=None)
+        Number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Returns
     -------
diff --git a/sklearn/covariance/elliptic_envelope.py b/sklearn/covariance/elliptic_envelope.py
index 7ea17b65aa..90e5a0f6d6 100755
--- a/sklearn/covariance/elliptic_envelope.py
+++ b/sklearn/covariance/elliptic_envelope.py
@@ -103,11 +103,13 @@ def fit(self, X, y=None):
         ----------
         X : numpy array or sparse matrix, shape (n_samples, n_features).
             Training data
-        y : (ignored)
+
+        y : Ignored
+            not used, present for API consistency by convention.
+
         """
         super(EllipticEnvelope, self).fit(X)
-        self.offset_ = sp.stats.scoreatpercentile(
-            -self.dist_, 100. * self.contamination)
+        self.offset_ = np.percentile(-self.dist_, 100. * self.contamination)
         return self
 
     def decision_function(self, X, raw_values=None):
diff --git a/sklearn/covariance/graph_lasso_.py b/sklearn/covariance/graph_lasso_.py
index a116d9919f..b10e3c7f3f 100755
--- a/sklearn/covariance/graph_lasso_.py
+++ b/sklearn/covariance/graph_lasso_.py
@@ -520,8 +520,11 @@ class GraphicalLassoCV(GraphicalLasso):
         than number of samples. Elsewhere prefer cd which is more numerically
         stable.
 
-    n_jobs : int, optional
-        number of jobs to run in parallel (default 1).
+    n_jobs : int or None, optional (default=None)
+        number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : boolean, optional
         If verbose is True, the objective function and duality gap are
@@ -927,8 +930,11 @@ class GraphLassoCV(GraphicalLassoCV):
         than number of samples. Elsewhere prefer cd which is more numerically
         stable.
 
-    n_jobs : int, optional
-        number of jobs to run in parallel (default 1).
+    n_jobs : int or None, optional (default=None)
+        number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : boolean, optional
         If verbose is True, the objective function and duality gap are
diff --git a/sklearn/datasets/__init__.py b/sklearn/datasets/__init__.py
index c43c0c4758..c7d78e6334 100755
--- a/sklearn/datasets/__init__.py
+++ b/sklearn/datasets/__init__.py
@@ -23,6 +23,7 @@
 from .twenty_newsgroups import fetch_20newsgroups
 from .twenty_newsgroups import fetch_20newsgroups_vectorized
 from .mldata import fetch_mldata, mldata_filename
+from .openml import fetch_openml
 from .samples_generator import make_classification
 from .samples_generator import make_multilabel_classification
 from .samples_generator import make_hastie_10_2
@@ -65,6 +66,7 @@
            'fetch_covtype',
            'fetch_rcv1',
            'fetch_kddcup99',
+           'fetch_openml',
            'get_data_home',
            'load_boston',
            'load_diabetes',
diff --git a/sklearn/datasets/descr/iris.rst b/sklearn/datasets/descr/iris.rst
index a35edc728c..e05206454d 100755
--- a/sklearn/datasets/descr/iris.rst
+++ b/sklearn/datasets/descr/iris.rst
@@ -25,7 +25,7 @@ Iris plants dataset
     sepal length:   4.3  7.9   5.84   0.83    0.7826
     sepal width:    2.0  4.4   3.05   0.43   -0.4194
     petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)
-    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)
+    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)
     ============== ==== ==== ======= ===== ====================
 
     :Missing Attribute Values: None
diff --git a/sklearn/datasets/descr/linnerud.rst b/sklearn/datasets/descr/linnerud.rst
index 848ee193e1..5585b50a7e 100755
--- a/sklearn/datasets/descr/linnerud.rst
+++ b/sklearn/datasets/descr/linnerud.rst
@@ -11,12 +11,12 @@ Linnerrud dataset
 
 The Linnerud dataset constains two small dataset:
 
-- *exercise*: A list containing the following components: exercise data with
-  20 observations on 3 exercise variables: Weight, Waist and Pulse.
+- *physiological* - CSV containing 20 observations on 3 exercise variables:
+   Weight, Waist and Pulse.
 
-- *physiological*: Data frame with 20 observations on 3 physiological variables:
+- *exercise* - CSV containing 20 observations on 3 physiological variables:
    Chins, Situps and Jumps.
 
 .. topic:: References
 
-  * Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic.
\ No newline at end of file
+  * Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic.
diff --git a/sklearn/datasets/mlcomp.py b/sklearn/datasets/mlcomp.py
index 169df6e551..9adb7bbc1c 100755
--- a/sklearn/datasets/mlcomp.py
+++ b/sklearn/datasets/mlcomp.py
@@ -24,7 +24,7 @@ def _load_document_classification(dataset_path, metadata, set_=None, **kwargs):
             "in March 2017, the load_mlcomp function was deprecated "
             "in version 0.19 and will be removed in 0.21.")
 def load_mlcomp(name_or_id, set_="raw", mlcomp_root=None, **kwargs):
-    """Load a datasets as downloaded from http://mlcomp.org
+    r"""Load a datasets as downloaded from http://mlcomp.org
 
     Read more in the :ref:`User Guide <datasets>`.
 
diff --git a/sklearn/datasets/mldata.py b/sklearn/datasets/mldata.py
index 1416208584..5948d04a8b 100755
--- a/sklearn/datasets/mldata.py
+++ b/sklearn/datasets/mldata.py
@@ -25,13 +25,19 @@
 
 from .base import get_data_home
 from ..utils import Bunch
+from ..utils import deprecated
 
 MLDATA_BASE_URL = "http://mldata.org/repository/data/download/matlab/%s"
 
 
+@deprecated('mldata_filename was deprecated in version 0.20 and will be '
+            'removed in version 0.22')
 def mldata_filename(dataname):
     """Convert a raw name for a data set in a mldata.org filename.
 
+    .. deprecated:: 0.20
+        Will be removed in version 0.22
+
     Parameters
     ----------
     dataname : str
@@ -46,10 +52,14 @@ def mldata_filename(dataname):
     return re.sub(r'[().]', '', dataname)
 
 
+@deprecated('fetch_mldata was deprecated in version 0.20 and will be removed '
+            'in version 0.22')
 def fetch_mldata(dataname, target_name='label', data_name='data',
                  transpose_data=True, data_home=None):
     """Fetch an mldata.org data set
 
+    mldata.org is no longer operational.
+
     If the file does not exist yet, it is downloaded from mldata.org .
 
     mldata.org does not have an enforced convention for storing data or
@@ -70,6 +80,9 @@ def fetch_mldata(dataname, target_name='label', data_name='data',
     mldata.org data sets may have multiple columns, which are stored in the
     Bunch object with their original name.
 
+    .. deprecated:: 0.20
+        Will be removed in version 0.22
+
     Parameters
     ----------
 
@@ -99,40 +112,6 @@ def fetch_mldata(dataname, target_name='label', data_name='data',
         'data', the data to learn, 'target', the classification labels,
         'DESCR', the full description of the dataset, and
         'COL_NAMES', the original names of the dataset columns.
-
-    Examples
-    --------
-    Load the 'iris' dataset from mldata.org:
-
-    >>> from sklearn.datasets.mldata import fetch_mldata
-    >>> import tempfile
-    >>> test_data_home = tempfile.mkdtemp()
-
-    >>> iris = fetch_mldata('iris', data_home=test_data_home)
-    >>> iris.target.shape
-    (150,)
-    >>> iris.data.shape
-    (150, 4)
-
-    Load the 'leukemia' dataset from mldata.org, which needs to be transposed
-    to respects the scikit-learn axes convention:
-
-    >>> leuk = fetch_mldata('leukemia', transpose_data=True,
-    ...                     data_home=test_data_home)
-    >>> leuk.data.shape
-    (72, 7129)
-
-    Load an alternative 'iris' dataset, which has different names for the
-    columns:
-
-    >>> iris2 = fetch_mldata('datasets-UCI iris', target_name=1,
-    ...                      data_name=0, data_home=test_data_home)
-    >>> iris3 = fetch_mldata('datasets-UCI iris',
-    ...                      target_name='class', data_name='double0',
-    ...                      data_home=test_data_home)
-
-    >>> import shutil
-    >>> shutil.rmtree(test_data_home)
     """
 
     # normalize dataset name
diff --git a/sklearn/datasets/openml.py b/sklearn/datasets/openml.py
new file mode 100755
index 0000000000..d667cb3699
--- /dev/null
+++ b/sklearn/datasets/openml.py
@@ -0,0 +1,588 @@
+import gzip
+import json
+import os
+import shutil
+from os.path import join
+from warnings import warn
+
+try:
+    # Python 3+
+    from urllib.request import urlopen, Request
+except ImportError:
+    # Python 2
+    from urllib2 import urlopen, Request
+
+
+import numpy as np
+import scipy.sparse
+
+from sklearn.externals import _arff
+from .base import get_data_home
+from ..externals.six import string_types, PY2, BytesIO
+from ..externals.six.moves.urllib.error import HTTPError
+from ..utils import Bunch
+
+__all__ = ['fetch_openml']
+
+_OPENML_PREFIX = "https://openml.org/"
+_SEARCH_NAME = "api/v1/json/data/list/data_name/{}/limit/2"
+_DATA_INFO = "api/v1/json/data/{}"
+_DATA_FEATURES = "api/v1/json/data/features/{}"
+_DATA_FILE = "data/v1/download/{}"
+
+
+def _open_openml_url(openml_path, data_home):
+    """
+    Returns a resource from OpenML.org. Caches it to data_home if required.
+
+    Parameters
+    ----------
+    openml_path : str
+        OpenML URL that will be accessed. This will be prefixes with
+        _OPENML_PREFIX
+
+    data_home : str
+        Directory to which the files will be cached. If None, no caching will
+        be applied.
+
+    Returns
+    -------
+    result : stream
+        A stream to the OpenML resource
+    """
+    req = Request(_OPENML_PREFIX + openml_path)
+    req.add_header('Accept-encoding', 'gzip')
+    fsrc = urlopen(req)
+    is_gzip = fsrc.info().get('Content-Encoding', '') == 'gzip'
+
+    if data_home is None:
+        if is_gzip:
+            if PY2:
+                fsrc = BytesIO(fsrc.read())
+            return gzip.GzipFile(fileobj=fsrc, mode='rb')
+        return fsrc
+
+    local_path = os.path.join(data_home, 'openml.org', openml_path + ".gz")
+    if not os.path.exists(local_path):
+        try:
+            os.makedirs(os.path.dirname(local_path))
+        except OSError:
+            # potentially, the directory has been created already
+            pass
+
+        try:
+            with open(local_path, 'wb') as fdst:
+                shutil.copyfileobj(fsrc, fdst)
+                fsrc.close()
+        except Exception:
+            os.unlink(local_path)
+            raise
+    # XXX: unnecessary decompression on first access
+    if is_gzip:
+        return gzip.GzipFile(local_path, 'rb')
+    return fsrc
+
+
+def _get_json_content_from_openml_api(url, error_message, raise_if_error,
+                                      data_home):
+    """
+    Loads json data from the openml api
+
+    Parameters
+    ----------
+    url : str
+        The URL to load from. Should be an official OpenML endpoint
+
+    error_message : str or None
+        The error message to raise if an acceptable OpenML error is thrown
+        (acceptable error is, e.g., data id not found. Other errors, like 404's
+        will throw the native error message)
+
+    raise_if_error : bool
+        Whether to raise an error if OpenML returns an acceptable error (e.g.,
+        date not found). If this argument is set to False, a None is returned
+        in case of acceptable errors. Note that all other errors (e.g., 404)
+        will still be raised as normal.
+
+    data_home : str or None
+        Location to cache the response. None if no cache is required.
+
+    Returns
+    -------
+    json_data : json or None
+        the json result from the OpenML server if the call was successful;
+        None otherwise iff raise_if_error was set to False and the error was
+        ``acceptable``
+    """
+    data_found = True
+    try:
+        response = _open_openml_url(url, data_home)
+    except HTTPError as error:
+        # 412 is an OpenML specific error code, indicating a generic error
+        # (e.g., data not found)
+        if error.code == 412:
+            data_found = False
+        else:
+            raise error
+    if not data_found:
+        # not in except for nicer traceback
+        if raise_if_error:
+            raise ValueError(error_message)
+        else:
+            return None
+    json_data = json.loads(response.read().decode("utf-8"))
+    response.close()
+    return json_data
+
+
+def _split_sparse_columns(arff_data, include_columns):
+    """
+    obtains several columns from sparse arff representation. Additionally, the
+    column indices are re-labelled, given the columns that are not included.
+    (e.g., when including [1, 2, 3], the columns will be relabelled to
+    [0, 1, 2])
+
+    Parameters
+    ----------
+    arff_data : tuple
+        A tuple of three lists of equal size; first list indicating the value,
+        second the x coordinate and the third the y coordinate.
+
+    include_columns : list
+        A list of columns to include.
+
+    Returns
+    -------
+    arff_data_new : tuple
+        Subset of arff data with only the include columns indicated by the
+        include_columns argument.
+    """
+    arff_data_new = (list(), list(), list())
+    reindexed_columns = {column_idx: array_idx for array_idx, column_idx
+                         in enumerate(include_columns)}
+    for val, row_idx, col_idx in zip(arff_data[0], arff_data[1], arff_data[2]):
+        if col_idx in include_columns:
+            arff_data_new[0].append(val)
+            arff_data_new[1].append(row_idx)
+            arff_data_new[2].append(reindexed_columns[col_idx])
+    return arff_data_new
+
+
+def _sparse_data_to_array(arff_data, include_columns):
+    # turns the sparse data back into an array (can't use toarray() function,
+    # as this does only work on numeric data)
+    num_obs = max(arff_data[1]) + 1
+    y_shape = (num_obs, len(include_columns))
+    reindexed_columns = {column_idx: array_idx for array_idx, column_idx
+                         in enumerate(include_columns)}
+    # TODO: improve for efficiency
+    y = np.empty(y_shape, dtype=np.float64)
+    for val, row_idx, col_idx in zip(arff_data[0], arff_data[1], arff_data[2]):
+        if col_idx in include_columns:
+            y[row_idx, reindexed_columns[col_idx]] = val
+    return y
+
+
+def _convert_arff_data(arff_data, col_slice_x, col_slice_y):
+    """
+    converts the arff object into the appropriate matrix type (np.array or
+    scipy.sparse.csr_matrix) based on the 'data part' (i.e., in the
+    liac-arff dict, the object from the 'data' key)
+
+    Parameters
+    ----------
+    arff_data : list or dict
+        as obtained from liac-arff object
+
+    col_slice_x : list
+        The column indices that are sliced from the original array to return
+        as X data
+
+    col_slice_y : list
+        The column indices that are sliced from the original array to return
+        as y data
+
+    Returns
+    -------
+    X : np.array or scipy.sparse.csr_matrix
+    y : np.array
+    """
+    if isinstance(arff_data, list):
+        data = np.array(arff_data, dtype=np.float64)
+        X = np.array(data[:, col_slice_x], dtype=np.float64)
+        y = np.array(data[:, col_slice_y], dtype=np.float64)
+        return X, y
+    elif isinstance(arff_data, tuple):
+        arff_data_X = _split_sparse_columns(arff_data, col_slice_x)
+        num_obs = max(arff_data[1]) + 1
+        X_shape = (num_obs, len(col_slice_x))
+        X = scipy.sparse.coo_matrix(
+            (arff_data_X[0], (arff_data_X[1], arff_data_X[2])),
+            shape=X_shape, dtype=np.float64)
+        X = X.tocsr()
+        y = _sparse_data_to_array(arff_data, col_slice_y)
+        return X, y
+    else:
+        # This should never happen
+        raise ValueError('Unexpected Data Type obtained from arff.')
+
+
+def _get_data_info_by_name(name, version, data_home):
+    """
+    Utilizes the openml dataset listing api to find a dataset by
+    name/version
+    OpenML api function:
+    https://www.openml.org/api_docs#!/data/get_data_list_data_name_data_name
+
+    Parameters
+    ----------
+    name : str
+        name of the dataset
+
+    version : int or str
+        If version is an integer, the exact name/version will be obtained from
+        OpenML. If version is a string (value: "active") it will take the first
+        version from OpenML that is annotated as active. Any other string
+        values except "active" are treated as integer.
+
+    data_home : str or None
+        Location to cache the response. None if no cache is required.
+
+    Returns
+    -------
+    first_dataset : json
+        json representation of the first dataset object that adhired to the
+        search criteria
+
+    """
+    if version == "active":
+        # situation in which we return the oldest active version
+        url = _SEARCH_NAME.format(name) + "/status/active/"
+        error_msg = "No active dataset {} found.".format(name)
+        json_data = _get_json_content_from_openml_api(url, error_msg, True,
+                                                      data_home)
+        res = json_data['data']['dataset']
+        if len(res) > 1:
+            warn("Multiple active versions of the dataset matching the name"
+                 " {name} exist. Versions may be fundamentally different, "
+                 "returning version"
+                 " {version}.".format(name=name, version=res[0]['version']))
+        return res[0]
+
+    # an integer version has been provided
+    url = (_SEARCH_NAME + "/data_version/{}").format(name, version)
+    json_data = _get_json_content_from_openml_api(url, None, False,
+                                                  data_home)
+    if json_data is None:
+        # we can do this in 1 function call if OpenML does not require the
+        # specification of the dataset status (i.e., return datasets with a
+        # given name / version regardless of active, deactivated, etc. )
+        # TODO: feature request OpenML.
+        url += "/status/deactivated"
+        error_msg = "Dataset {} with version {} not found.".format(name,
+                                                                   version)
+        json_data = _get_json_content_from_openml_api(url, error_msg, True,
+                                                      data_home)
+
+    return json_data['data']['dataset'][0]
+
+
+def _get_data_description_by_id(data_id, data_home):
+    # OpenML API function: https://www.openml.org/api_docs#!/data/get_data_id
+    url = _DATA_INFO.format(data_id)
+    error_message = "Dataset with data_id {} not found.".format(data_id)
+    json_data = _get_json_content_from_openml_api(url, error_message, True,
+                                                  data_home)
+    return json_data['data_set_description']
+
+
+def _get_data_features(data_id, data_home):
+    # OpenML function:
+    # https://www.openml.org/api_docs#!/data/get_data_features_id
+    url = _DATA_FEATURES.format(data_id)
+    error_message = "Dataset with data_id {} not found.".format(data_id)
+    json_data = _get_json_content_from_openml_api(url, error_message, True,
+                                                  data_home)
+    return json_data['data_features']['feature']
+
+
+def _download_data_arff(file_id, sparse, data_home, encode_nominal=True):
+    # Accesses an ARFF file on the OpenML server. Documentation:
+    # https://www.openml.org/api_data_docs#!/data/get_download_id
+    # encode_nominal argument is to ensure unit testing, do not alter in
+    # production!
+    url = _DATA_FILE.format(file_id)
+    response = _open_openml_url(url, data_home)
+    if sparse is True:
+        return_type = _arff.COO
+    else:
+        return_type = _arff.DENSE
+
+    if PY2:
+        arff_file = _arff.load(response.read(), encode_nominal=encode_nominal,
+                               return_type=return_type, )
+    else:
+        arff_file = _arff.loads(response.read().decode('utf-8'),
+                                encode_nominal=encode_nominal,
+                                return_type=return_type)
+    response.close()
+    return arff_file
+
+
+def _verify_target_data_type(features_dict, target_columns):
+    # verifies the data type of the y array in case there are multiple targets
+    # (throws an error if these targets do not comply with sklearn support)
+    if not isinstance(target_columns, list):
+        raise ValueError('target_column should be list, '
+                         'got: %s' % type(target_columns))
+    found_types = set()
+    for target_column in target_columns:
+        if target_column not in features_dict:
+            raise KeyError('Could not find target_column={}')
+        if features_dict[target_column]['data_type'] == "numeric":
+            found_types.add(np.float64)
+        else:
+            found_types.add(object)
+
+        # note: we compare to a string, not boolean
+        if features_dict[target_column]['is_ignore'] == 'true':
+            warn('target_column={} has flag is_ignore.'.format(
+                target_column))
+        if features_dict[target_column]['is_row_identifier'] == 'true':
+            warn('target_column={} has flag is_row_identifier.'.format(
+                target_column))
+    if len(found_types) > 1:
+        raise ValueError('Can only handle homogeneous multi-target datasets, '
+                         'i.e., all targets are either numeric or '
+                         'categorical.')
+
+
+def fetch_openml(name=None, version='active', data_id=None, data_home=None,
+                 target_column='default-target', cache=True, return_X_y=False):
+    """Fetch dataset from openml by name or dataset id.
+
+    Datasets are uniquely identified by either an integer ID or by a
+    combination of name and version (i.e. there might be multiple
+    versions of the 'iris' dataset). Please give either name or data_id
+    (not both). In case a name is given, a version can also be
+    provided.
+
+    Read more in the :ref:`User Guide <openml>`.
+
+    .. note:: EXPERIMENTAL
+
+        The API is experimental in version 0.20 (particularly the return value
+        structure), and might have small backward-incompatible changes in
+        future releases.
+
+    Parameters
+    ----------
+    name : str or None
+        String identifier of the dataset. Note that OpenML can have multiple
+        datasets with the same name.
+
+    version : integer or 'active', default='active'
+        Version of the dataset. Can only be provided if also ``name`` is given.
+        If 'active' the oldest version that's still active is used. Since
+        there may be more than one active version of a dataset, and those
+        versions may fundamentally be different from one another, setting an
+        exact version is highly recommended.
+
+    data_id : int or None
+        OpenML ID of the dataset. The most specific way of retrieving a
+        dataset. If data_id is not given, name (and potential version) are
+        used to obtain a dataset.
+
+    data_home : string or None, default None
+        Specify another download and cache folder for the data sets. By default
+        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.
+
+    target_column : string, list or None, default 'default-target'
+        Specify the column name in the data to use as target. If
+        'default-target', the standard target column a stored on the server
+        is used. If ``None``, all columns are returned as data and the
+        target is ``None``. If list (of strings), all columns with these names
+        are returned as multi-target (Note: not all scikit-learn classifiers
+        can handle all types of multi-output combinations)
+
+    cache : boolean, default=True
+        Whether to cache downloaded datasets using joblib.
+
+    return_X_y : boolean, default=False.
+        If True, returns ``(data, target)`` instead of a Bunch object. See
+        below for more information about the `data` and `target` objects.
+
+    Returns
+    -------
+
+    data : Bunch
+        Dictionary-like object, with attributes:
+
+        data : np.array or scipy.sparse.csr_matrix of floats
+            The feature matrix. Categorical features are encoded as ordinals.
+        target : np.array
+            The regression target or classification labels, if applicable.
+            Dtype is float if numeric, and object if categorical.
+        DESCR : str
+            The full description of the dataset
+        feature_names : list
+            The names of the dataset columns
+        categories : dict
+            Maps each categorical feature name to a list of values, such
+            that the value encoded as i is ith in the list.
+        details : dict
+            More metadata from OpenML
+
+    (data, target) : tuple if ``return_X_y`` is True
+
+        .. note:: EXPERIMENTAL
+
+            This interface is **experimental** as at version 0.20 and
+            subsequent releases may change attributes without notice
+            (although there should only be minor changes to ``data``
+            and ``target``).
+
+        Missing values in the 'data' are represented as NaN's. Missing values
+        in 'target' are represented as NaN's (numerical target) or None
+        (categorical target)
+    """
+    data_home = get_data_home(data_home=data_home)
+    data_home = join(data_home, 'openml')
+    if cache is False:
+        # no caching will be applied
+        data_home = None
+
+    # check valid function arguments. data_id XOR (name, version) should be
+    # provided
+    if name is not None:
+        # OpenML is case-insensitive, but the caching mechanism is not
+        # convert all data names (str) to lower case
+        name = name.lower()
+        if data_id is not None:
+            raise ValueError(
+                "Dataset data_id={} and name={} passed, but you can only "
+                "specify a numeric data_id or a name, not "
+                "both.".format(data_id, name))
+        data_info = _get_data_info_by_name(name, version, data_home)
+        data_id = data_info['did']
+    elif data_id is not None:
+        # from the previous if statement, it is given that name is None
+        if version is not "active":
+            raise ValueError(
+                "Dataset data_id={} and version={} passed, but you can only "
+                "specify a numeric data_id or a version, not "
+                "both.".format(data_id, name))
+    else:
+        raise ValueError(
+            "Neither name nor data_id are provided. Please provide name or "
+            "data_id.")
+
+    data_description = _get_data_description_by_id(data_id, data_home)
+    if data_description['status'] != "active":
+        warn("Version {} of dataset {} is inactive, meaning that issues have "
+             "been found in the dataset. Try using a newer version from "
+             "this URL: {}".format(
+                data_description['version'],
+                data_description['name'],
+                data_description['url']))
+
+    # download data features, meta-info about column types
+    features_list = _get_data_features(data_id, data_home)
+
+    for feature in features_list:
+        if 'true' in (feature['is_ignore'], feature['is_row_identifier']):
+            continue
+        if feature['data_type'] == 'string':
+            raise ValueError('STRING attributes are not yet supported')
+
+    if target_column == "default-target":
+        # determines the default target based on the data feature results
+        # (which is currently more reliable than the data description;
+        # see issue: https://github.com/openml/OpenML/issues/768)
+        target_column = [feature['name'] for feature in features_list
+                         if feature['is_target'] == 'true']
+    elif isinstance(target_column, string_types):
+        # for code-simplicity, make target_column by default a list
+        target_column = [target_column]
+    elif target_column is None:
+        target_column = []
+    elif not isinstance(target_column, list):
+        raise TypeError("Did not recognize type of target_column"
+                        "Should be six.string_type, list or None. Got: "
+                        "{}".format(type(target_column)))
+    data_columns = [feature['name'] for feature in features_list
+                    if (feature['name'] not in target_column and
+                        feature['is_ignore'] != 'true' and
+                        feature['is_row_identifier'] != 'true')]
+
+    # prepare which columns and data types should be returned for the X and y
+    features_dict = {feature['name']: feature for feature in features_list}
+
+    # XXX: col_slice_y should be all nominal or all numeric
+    _verify_target_data_type(features_dict, target_column)
+
+    col_slice_y = [int(features_dict[col_name]['index'])
+                   for col_name in target_column]
+
+    col_slice_x = [int(features_dict[col_name]['index'])
+                   for col_name in data_columns]
+    for col_idx in col_slice_y:
+        feat = features_list[col_idx]
+        nr_missing = int(feat['number_of_missing_values'])
+        if nr_missing > 0:
+            raise ValueError('Target column {} has {} missing values. '
+                             'Missing values are not supported for target '
+                             'columns. '.format(feat['name'], nr_missing))
+
+    # determine arff encoding to return
+    return_sparse = False
+    if data_description['format'].lower() == 'sparse_arff':
+        return_sparse = True
+
+    # obtain the data
+    arff = _download_data_arff(data_description['file_id'], return_sparse,
+                               data_home)
+    arff_data = arff['data']
+    nominal_attributes = {k: v for k, v in arff['attributes']
+                          if isinstance(v, list)}
+    for feature in features_list:
+        if 'true' in (feature['is_row_identifier'],
+                      feature['is_ignore']) and (feature['name'] not in
+                                                 target_column):
+            del nominal_attributes[feature['name']]
+    X, y = _convert_arff_data(arff_data, col_slice_x, col_slice_y)
+
+    is_classification = {col_name in nominal_attributes
+                         for col_name in target_column}
+    if not is_classification:
+        # No target
+        pass
+    elif all(is_classification):
+        y = np.hstack([np.take(np.asarray(nominal_attributes.pop(col_name),
+                                          dtype='O'),
+                               y[:, i:i+1].astype(int))
+                       for i, col_name in enumerate(target_column)])
+    elif any(is_classification):
+        raise ValueError('Mix of nominal and non-nominal targets is not '
+                         'currently supported')
+
+    description = u"{}\n\nDownloaded from openml.org.".format(
+        data_description.pop('description'))
+
+    # reshape y back to 1-D array, if there is only 1 target column; back
+    # to None if there are not target columns
+    if y.shape[1] == 1:
+        y = y.reshape((-1,))
+    elif y.shape[1] == 0:
+        y = None
+
+    if return_X_y:
+        return X, y
+
+    bunch = Bunch(
+        data=X, target=y, feature_names=data_columns,
+        DESCR=description, details=data_description,
+        categories=nominal_attributes,
+        url="https://www.openml.org/d/{}".format(data_id))
+
+    return bunch
diff --git a/sklearn/datasets/tests/data/openml/2/api-v1-json-data-2.json.gz b/sklearn/datasets/tests/data/openml/2/api-v1-json-data-2.json.gz
new file mode 100755
index 0000000000..22dfb6ff61
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/2/api-v1-json-data-2.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/2/api-v1-json-data-features-2.json.gz b/sklearn/datasets/tests/data/openml/2/api-v1-json-data-features-2.json.gz
new file mode 100755
index 0000000000..cb3d275009
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/2/api-v1-json-data-features-2.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/2/api-v1-json-data-list-data_name-anneal-limit-2-data_version-1.json.gz b/sklearn/datasets/tests/data/openml/2/api-v1-json-data-list-data_name-anneal-limit-2-data_version-1.json.gz
new file mode 100755
index 0000000000..a95a8131dd
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/2/api-v1-json-data-list-data_name-anneal-limit-2-data_version-1.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/2/api-v1-json-data-list-data_name-anneal-limit-2-status-active-.json.gz b/sklearn/datasets/tests/data/openml/2/api-v1-json-data-list-data_name-anneal-limit-2-status-active-.json.gz
new file mode 100755
index 0000000000..e85c1b5ff9
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/2/api-v1-json-data-list-data_name-anneal-limit-2-status-active-.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/2/data-v1-download-1666876.arff.gz b/sklearn/datasets/tests/data/openml/2/data-v1-download-1666876.arff.gz
new file mode 100755
index 0000000000..cdf3254add
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/2/data-v1-download-1666876.arff.gz differ
diff --git a/sklearn/datasets/tests/data/openml/292/api-v1-json-data-292.json.gz b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-292.json.gz
new file mode 100755
index 0000000000..888140f92b
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-292.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/292/api-v1-json-data-40981.json.gz b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-40981.json.gz
new file mode 100755
index 0000000000..888140f92b
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-40981.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/292/api-v1-json-data-features-292.json.gz b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-features-292.json.gz
new file mode 100755
index 0000000000..29016cc36b
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-features-292.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/292/api-v1-json-data-features-40981.json.gz b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-features-40981.json.gz
new file mode 100755
index 0000000000..29016cc36b
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-features-40981.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/292/api-v1-json-data-list-data_name-australian-limit-2-data_version-1-status-deactivated.json.gz b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-list-data_name-australian-limit-2-data_version-1-status-deactivated.json.gz
new file mode 100755
index 0000000000..8cb61626e1
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-list-data_name-australian-limit-2-data_version-1-status-deactivated.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/292/api-v1-json-data-list-data_name-australian-limit-2-data_version-1.json.gz b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-list-data_name-australian-limit-2-data_version-1.json.gz
new file mode 100755
index 0000000000..0e2c4395f1
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-list-data_name-australian-limit-2-data_version-1.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/292/api-v1-json-data-list-data_name-australian-limit-2-status-active-.json.gz b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-list-data_name-australian-limit-2-status-active-.json.gz
new file mode 100755
index 0000000000..b91949b9e4
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-list-data_name-australian-limit-2-status-active-.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/292/data-v1-download-49822.arff.gz b/sklearn/datasets/tests/data/openml/292/data-v1-download-49822.arff.gz
new file mode 100755
index 0000000000..6821829e1e
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/292/data-v1-download-49822.arff.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-40589.json.gz b/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-40589.json.gz
new file mode 100755
index 0000000000..9c71553ce5
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-40589.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-features-40589.json.gz b/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-features-40589.json.gz
new file mode 100755
index 0000000000..155460906a
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-features-40589.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-list-data_name-emotions-limit-2-data_version-3.json.gz b/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-list-data_name-emotions-limit-2-data_version-3.json.gz
new file mode 100755
index 0000000000..01e6648a91
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-list-data_name-emotions-limit-2-data_version-3.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-list-data_name-emotions-limit-2-status-active-.json.gz b/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-list-data_name-emotions-limit-2-status-active-.json.gz
new file mode 100755
index 0000000000..0fc8d5ba1f
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-list-data_name-emotions-limit-2-status-active-.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40589/data-v1-download-4644182.arff.gz b/sklearn/datasets/tests/data/openml/40589/data-v1-download-4644182.arff.gz
new file mode 100755
index 0000000000..96ed11d969
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40589/data-v1-download-4644182.arff.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-40675.json.gz b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-40675.json.gz
new file mode 100755
index 0000000000..42b876f0a4
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-40675.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-features-40675.json.gz b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-features-40675.json.gz
new file mode 100755
index 0000000000..2d5c6f8a30
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-features-40675.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-list-data_name-glass2-limit-2-data_version-1-status-deactivated.json.gz b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-list-data_name-glass2-limit-2-data_version-1-status-deactivated.json.gz
new file mode 100755
index 0000000000..f038de4196
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-list-data_name-glass2-limit-2-data_version-1-status-deactivated.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-list-data_name-glass2-limit-2-data_version-1.json.gz b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-list-data_name-glass2-limit-2-data_version-1.json.gz
new file mode 100755
index 0000000000..df1665b1db
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-list-data_name-glass2-limit-2-data_version-1.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-list-data_name-glass2-limit-2-status-active-.json.gz b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-list-data_name-glass2-limit-2-status-active-.json.gz
new file mode 100755
index 0000000000..ff46d678f6
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-list-data_name-glass2-limit-2-status-active-.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40675/data-v1-download-4965250.arff.gz b/sklearn/datasets/tests/data/openml/40675/data-v1-download-4965250.arff.gz
new file mode 100755
index 0000000000..c59c3b769e
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40675/data-v1-download-4965250.arff.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40945/api-v1-json-data-40945.json.gz b/sklearn/datasets/tests/data/openml/40945/api-v1-json-data-40945.json.gz
new file mode 100755
index 0000000000..aaafa4a2de
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40945/api-v1-json-data-40945.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40945/api-v1-json-data-features-40945.json.gz b/sklearn/datasets/tests/data/openml/40945/api-v1-json-data-features-40945.json.gz
new file mode 100755
index 0000000000..24cb46957f
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40945/api-v1-json-data-features-40945.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-40966.json.gz b/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-40966.json.gz
new file mode 100755
index 0000000000..02b25d717f
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-40966.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-features-40966.json.gz b/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-features-40966.json.gz
new file mode 100755
index 0000000000..a372f9a7be
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-features-40966.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-list-data_name-miceprotein-limit-2-data_version-4.json.gz b/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-list-data_name-miceprotein-limit-2-data_version-4.json.gz
new file mode 100755
index 0000000000..0931e0b2da
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-list-data_name-miceprotein-limit-2-data_version-4.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-list-data_name-miceprotein-limit-2-status-active-.json.gz b/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-list-data_name-miceprotein-limit-2-status-active-.json.gz
new file mode 100755
index 0000000000..190571cb65
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-list-data_name-miceprotein-limit-2-status-active-.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40966/data-v1-download-17928620.arff.gz b/sklearn/datasets/tests/data/openml/40966/data-v1-download-17928620.arff.gz
new file mode 100755
index 0000000000..43ec977bf6
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40966/data-v1-download-17928620.arff.gz differ
diff --git a/sklearn/datasets/tests/data/openml/561/api-v1-json-data-561.json.gz b/sklearn/datasets/tests/data/openml/561/api-v1-json-data-561.json.gz
new file mode 100755
index 0000000000..e4df6060ca
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/561/api-v1-json-data-561.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/561/api-v1-json-data-features-561.json.gz b/sklearn/datasets/tests/data/openml/561/api-v1-json-data-features-561.json.gz
new file mode 100755
index 0000000000..54a3ab6a7a
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/561/api-v1-json-data-features-561.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/561/api-v1-json-data-list-data_name-cpu-limit-2-data_version-1.json.gz b/sklearn/datasets/tests/data/openml/561/api-v1-json-data-list-data_name-cpu-limit-2-data_version-1.json.gz
new file mode 100755
index 0000000000..872c5a8205
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/561/api-v1-json-data-list-data_name-cpu-limit-2-data_version-1.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/561/api-v1-json-data-list-data_name-cpu-limit-2-status-active-.json.gz b/sklearn/datasets/tests/data/openml/561/api-v1-json-data-list-data_name-cpu-limit-2-status-active-.json.gz
new file mode 100755
index 0000000000..99a631470e
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/561/api-v1-json-data-list-data_name-cpu-limit-2-status-active-.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/561/data-v1-download-52739.arff.gz b/sklearn/datasets/tests/data/openml/561/data-v1-download-52739.arff.gz
new file mode 100755
index 0000000000..eeb088c224
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/561/data-v1-download-52739.arff.gz differ
diff --git a/sklearn/datasets/tests/data/openml/61/api-v1-json-data-61.json.gz b/sklearn/datasets/tests/data/openml/61/api-v1-json-data-61.json.gz
new file mode 100755
index 0000000000..83c3ececcf
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/61/api-v1-json-data-61.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/61/api-v1-json-data-features-61.json.gz b/sklearn/datasets/tests/data/openml/61/api-v1-json-data-features-61.json.gz
new file mode 100755
index 0000000000..6df4cf0dad
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/61/api-v1-json-data-features-61.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/61/api-v1-json-data-list-data_name-iris-limit-2-data_version-1.json.gz b/sklearn/datasets/tests/data/openml/61/api-v1-json-data-list-data_name-iris-limit-2-data_version-1.json.gz
new file mode 100755
index 0000000000..71b0c876ad
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/61/api-v1-json-data-list-data_name-iris-limit-2-data_version-1.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/61/api-v1-json-data-list-data_name-iris-limit-2-status-active-.json.gz b/sklearn/datasets/tests/data/openml/61/api-v1-json-data-list-data_name-iris-limit-2-status-active-.json.gz
new file mode 100755
index 0000000000..7ea17070fb
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/61/api-v1-json-data-list-data_name-iris-limit-2-status-active-.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/61/data-v1-download-61.arff.gz b/sklearn/datasets/tests/data/openml/61/data-v1-download-61.arff.gz
new file mode 100755
index 0000000000..b05dadf99f
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/61/data-v1-download-61.arff.gz differ
diff --git a/sklearn/datasets/tests/test_mldata.py b/sklearn/datasets/tests/test_mldata.py
index 65e10a8781..be0d994e9b 100755
--- a/sklearn/datasets/tests/test_mldata.py
+++ b/sklearn/datasets/tests/test_mldata.py
@@ -13,6 +13,7 @@
 from sklearn.utils.testing import assert_equal
 from sklearn.utils.testing import assert_raises
 from sklearn.utils.testing import assert_array_equal
+from sklearn.utils.testing import assert_warns
 
 import pytest
 
@@ -26,6 +27,7 @@ def tmpdata(tmpdir_factory):
     shutil.rmtree(str(tmpdir))
 
 
+@pytest.mark.filterwarnings('ignore::DeprecationWarning')
 def test_mldata_filename():
     cases = [('datasets-UCI iris', 'datasets-uci-iris'),
              ('news20.binary', 'news20binary'),
@@ -36,6 +38,7 @@ def test_mldata_filename():
         assert_equal(mldata_filename(name), desired)
 
 
+@pytest.mark.filterwarnings('ignore::DeprecationWarning')
 def test_download(tmpdata):
     """Test that fetch_mldata is able to download and cache a data set."""
     _urlopen_ref = datasets.mldata.urlopen
@@ -46,7 +49,8 @@ def test_download(tmpdata):
         },
     })
     try:
-        mock = fetch_mldata('mock', data_home=tmpdata)
+        mock = assert_warns(DeprecationWarning, fetch_mldata,
+                            'mock', data_home=tmpdata)
         for n in ["COL_NAMES", "DESCR", "target", "data"]:
             assert_in(n, mock)
 
@@ -54,11 +58,13 @@ def test_download(tmpdata):
         assert_equal(mock.data.shape, (150, 4))
 
         assert_raises(datasets.mldata.HTTPError,
+                      assert_warns, DeprecationWarning,
                       fetch_mldata, 'not_existing_name')
     finally:
         datasets.mldata.urlopen = _urlopen_ref
 
 
+@pytest.mark.filterwarnings('ignore::DeprecationWarning')
 def test_fetch_one_column(tmpdata):
     _urlopen_ref = datasets.mldata.urlopen
     try:
@@ -82,6 +88,7 @@ def test_fetch_one_column(tmpdata):
         datasets.mldata.urlopen = _urlopen_ref
 
 
+@pytest.mark.filterwarnings('ignore::DeprecationWarning')
 def test_fetch_multiple_column(tmpdata):
     _urlopen_ref = datasets.mldata.urlopen
     try:
diff --git a/sklearn/datasets/tests/test_openml.py b/sklearn/datasets/tests/test_openml.py
new file mode 100755
index 0000000000..cf9cfcdc81
--- /dev/null
+++ b/sklearn/datasets/tests/test_openml.py
@@ -0,0 +1,596 @@
+"""Test the openml loader.
+"""
+import gzip
+import json
+import numpy as np
+import os
+import re
+import scipy.sparse
+import sklearn
+import pytest
+
+from sklearn.datasets import fetch_openml
+from sklearn.datasets.openml import (_open_openml_url,
+                                     _get_data_description_by_id,
+                                     _download_data_arff)
+from sklearn.utils.testing import (assert_warns_message,
+                                   assert_raise_message)
+from sklearn.externals.six import string_types
+from sklearn.externals.six.moves.urllib.error import HTTPError
+from sklearn.datasets.tests.test_common import check_return_X_y
+from functools import partial
+
+
+currdir = os.path.dirname(os.path.abspath(__file__))
+# if True, urlopen will be monkey patched to only use local files
+test_offline = True
+
+
+def _test_features_list(data_id):
+    # XXX Test is intended to verify/ensure correct decoding behavior
+    # Not usable with sparse data or datasets that have columns marked as
+    # {row_identifier, ignore}
+    def decode_column(data_bunch, col_idx):
+        col_name = data_bunch.feature_names[col_idx]
+        if col_name in data_bunch.categories:
+            # XXX: This would be faster with np.take, although it does not
+            # handle missing values fast (also not with mode='wrap')
+            cat = data_bunch.categories[col_name]
+            result = [cat[idx] if 0 <= idx < len(cat) else None for idx in
+                      data_bunch.data[:, col_idx].astype(int)]
+            return np.array(result, dtype='O')
+        else:
+            # non-nominal attribute
+            return data_bunch.data[:, col_idx]
+
+    data_bunch = fetch_openml(data_id=data_id, cache=False, target_column=None)
+
+    # also obtain decoded arff
+    data_description = _get_data_description_by_id(data_id, None)
+    sparse = data_description['format'].lower() == 'sparse_arff'
+    if sparse is True:
+        raise ValueError('This test is not intended for sparse data, to keep '
+                         'code relatively simple')
+    data_arff = _download_data_arff(data_description['file_id'],
+                                    sparse, None, False)
+    data_downloaded = np.array(data_arff['data'], dtype='O')
+
+    for i in range(len(data_bunch.feature_names)):
+        # XXX: Test per column, as this makes it easier to avoid problems with
+        # missing values
+
+        np.testing.assert_array_equal(data_downloaded[:, i],
+                                      decode_column(data_bunch, i))
+
+
+def _fetch_dataset_from_openml(data_id, data_name, data_version,
+                               target_column,
+                               expected_observations, expected_features,
+                               expected_missing,
+                               expected_data_dtype, expected_target_dtype,
+                               expect_sparse, compare_default_target):
+    # fetches a dataset in three various ways from OpenML, using the
+    # fetch_openml function, and does various checks on the validity of the
+    # result. Note that this function can be mocked (by invoking
+    # _monkey_patch_webbased_functions before invoking this function)
+    data_by_name_id = fetch_openml(name=data_name, version=data_version,
+                                   cache=False)
+    assert int(data_by_name_id.details['id']) == data_id
+
+    fetch_openml(name=data_name, cache=False)
+    # without specifying the version, there is no guarantee that the data id
+    # will be the same
+
+    # fetch with dataset id
+    data_by_id = fetch_openml(data_id=data_id, cache=False,
+                              target_column=target_column)
+    assert data_by_id.details['name'] == data_name
+    assert data_by_id.data.shape == (expected_observations, expected_features)
+    if isinstance(target_column, str):
+        # single target, so target is vector
+        assert data_by_id.target.shape == (expected_observations, )
+    elif isinstance(target_column, list):
+        # multi target, so target is array
+        assert data_by_id.target.shape == (expected_observations,
+                                           len(target_column))
+    assert data_by_id.data.dtype == np.float64
+    assert data_by_id.target.dtype == expected_target_dtype
+    assert len(data_by_id.feature_names) == expected_features
+    for feature in data_by_id.feature_names:
+        assert isinstance(feature, string_types)
+
+    # TODO: pass in a list of expected nominal features
+    for feature, categories in data_by_id.categories.items():
+        feature_idx = data_by_id.feature_names.index(feature)
+        values = np.unique(data_by_id.data[:, feature_idx])
+        values = values[np.isfinite(values)]
+        assert set(values) <= set(range(len(categories)))
+
+    if compare_default_target:
+        # check whether the data by id and data by id target are equal
+        data_by_id_default = fetch_openml(data_id=data_id, cache=False)
+        if data_by_id.data.dtype == np.float64:
+            np.testing.assert_allclose(data_by_id.data,
+                                       data_by_id_default.data)
+        else:
+            assert np.array_equal(data_by_id.data, data_by_id_default.data)
+        if data_by_id.target.dtype == np.float64:
+            np.testing.assert_allclose(data_by_id.target,
+                                       data_by_id_default.target)
+        else:
+            assert np.array_equal(data_by_id.target, data_by_id_default.target)
+
+    if expect_sparse:
+        assert isinstance(data_by_id.data, scipy.sparse.csr_matrix)
+    else:
+        assert isinstance(data_by_id.data, np.ndarray)
+        # np.isnan doesn't work on CSR matrix
+        assert (np.count_nonzero(np.isnan(data_by_id.data)) ==
+                expected_missing)
+
+    # test return_X_y option
+    fetch_func = partial(fetch_openml, data_id=data_id, cache=False,
+                         target_column=target_column)
+    check_return_X_y(data_by_id, fetch_func)
+    return data_by_id
+
+
+def _monkey_patch_webbased_functions(context,
+                                     data_id,
+                                     gzip_response):
+    url_prefix_data_description = "https://openml.org/api/v1/json/data/"
+    url_prefix_data_features = "https://openml.org/api/v1/json/data/features/"
+    url_prefix_download_data = "https://openml.org/data/v1/"
+    url_prefix_data_list = "https://openml.org/api/v1/json/data/list/"
+
+    path_suffix = '.gz'
+    read_fn = gzip.open
+
+    class MockHTTPResponse(object):
+        def __init__(self, data, is_gzip):
+            self.data = data
+            self.is_gzip = is_gzip
+
+        def read(self, amt=-1):
+            return self.data.read(amt)
+
+        def tell(self):
+            return self.data.tell()
+
+        def seek(self, pos, whence=0):
+            return self.data.seek(pos, whence)
+
+        def close(self):
+            self.data.close()
+
+        def info(self):
+            if self.is_gzip:
+                return {'Content-Encoding': 'gzip'}
+            return {}
+
+    def _file_name(url, suffix):
+        return (re.sub(r'\W', '-', url[len("https://openml.org/"):])
+                + suffix + path_suffix)
+
+    def _mock_urlopen_data_description(url, has_gzip_header):
+        assert url.startswith(url_prefix_data_description)
+
+        path = os.path.join(currdir, 'data', 'openml', str(data_id),
+                            _file_name(url, '.json'))
+
+        if has_gzip_header and gzip_response:
+            fp = open(path, 'rb')
+            return MockHTTPResponse(fp, True)
+        else:
+            fp = read_fn(path, 'rb')
+            return MockHTTPResponse(fp, False)
+
+    def _mock_urlopen_data_features(url, has_gzip_header):
+        assert url.startswith(url_prefix_data_features)
+        path = os.path.join(currdir, 'data', 'openml', str(data_id),
+                            _file_name(url, '.json'))
+        if has_gzip_header and gzip_response:
+            fp = open(path, 'rb')
+            return MockHTTPResponse(fp, True)
+        else:
+            fp = read_fn(path, 'rb')
+            return MockHTTPResponse(fp, False)
+
+    def _mock_urlopen_download_data(url, has_gzip_header):
+        assert (url.startswith(url_prefix_download_data))
+
+        path = os.path.join(currdir, 'data', 'openml', str(data_id),
+                            _file_name(url, '.arff'))
+
+        if has_gzip_header and gzip_response:
+            fp = open(path, 'rb')
+            return MockHTTPResponse(fp, True)
+        else:
+            fp = read_fn(path, 'rb')
+            return MockHTTPResponse(fp, False)
+
+    def _mock_urlopen_data_list(url, has_gzip_header):
+        assert url.startswith(url_prefix_data_list)
+
+        json_file_path = os.path.join(currdir, 'data', 'openml',
+                                      str(data_id), _file_name(url, '.json'))
+        # load the file itself, to simulate a http error
+        json_data = json.loads(read_fn(json_file_path, 'rb').
+                               read().decode('utf-8'))
+        if 'error' in json_data:
+            raise HTTPError(url=None, code=412,
+                            msg='Simulated mock error',
+                            hdrs=None, fp=None)
+
+        if has_gzip_header:
+            fp = open(json_file_path, 'rb')
+            return MockHTTPResponse(fp, True)
+        else:
+            fp = read_fn(json_file_path, 'rb')
+            return MockHTTPResponse(fp, False)
+
+    def _mock_urlopen(request):
+        url = request.get_full_url()
+        has_gzip_header = request.get_header('Accept-encoding') == "gzip"
+        if url.startswith(url_prefix_data_list):
+            return _mock_urlopen_data_list(url, has_gzip_header)
+        elif url.startswith(url_prefix_data_features):
+            return _mock_urlopen_data_features(url, has_gzip_header)
+        elif url.startswith(url_prefix_download_data):
+            return _mock_urlopen_download_data(url, has_gzip_header)
+        elif url.startswith(url_prefix_data_description):
+            return _mock_urlopen_data_description(url, has_gzip_header)
+        else:
+            raise ValueError('Unknown mocking URL pattern: %s' % url)
+
+    # XXX: Global variable
+    if test_offline:
+        context.setattr(sklearn.datasets.openml, 'urlopen', _mock_urlopen)
+
+
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_fetch_openml_iris(monkeypatch, gzip_response):
+    # classification dataset with numeric only columns
+    data_id = 61
+    data_name = 'iris'
+    data_version = 1
+    target_column = 'class'
+    expected_observations = 150
+    expected_features = 4
+    expected_missing = 0
+
+    _monkey_patch_webbased_functions(monkeypatch, data_id, gzip_response)
+    assert_warns_message(
+        UserWarning,
+        "Multiple active versions of the dataset matching the name"
+        " iris exist. Versions may be fundamentally different, "
+        "returning version 1.",
+        _fetch_dataset_from_openml,
+        **{'data_id': data_id, 'data_name': data_name,
+           'data_version': data_version,
+           'target_column': target_column,
+           'expected_observations': expected_observations,
+           'expected_features': expected_features,
+           'expected_missing': expected_missing,
+           'expect_sparse': False,
+           'expected_data_dtype': np.float64,
+           'expected_target_dtype': object,
+           'compare_default_target': True}
+    )
+
+
+def test_decode_iris(monkeypatch):
+    data_id = 61
+    _monkey_patch_webbased_functions(monkeypatch, data_id, False)
+    _test_features_list(data_id)
+
+
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_fetch_openml_iris_multitarget(monkeypatch, gzip_response):
+    # classification dataset with numeric only columns
+    data_id = 61
+    data_name = 'iris'
+    data_version = 1
+    target_column = ['sepallength', 'sepalwidth']
+    expected_observations = 150
+    expected_features = 3
+    expected_missing = 0
+
+    _monkey_patch_webbased_functions(monkeypatch, data_id, gzip_response)
+    _fetch_dataset_from_openml(data_id, data_name, data_version, target_column,
+                               expected_observations, expected_features,
+                               expected_missing,
+                               object, np.float64, expect_sparse=False,
+                               compare_default_target=False)
+
+
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_fetch_openml_anneal(monkeypatch, gzip_response):
+    # classification dataset with numeric and categorical columns
+    data_id = 2
+    data_name = 'anneal'
+    data_version = 1
+    target_column = 'class'
+    # Not all original instances included for space reasons
+    expected_observations = 11
+    expected_features = 38
+    expected_missing = 267
+    _monkey_patch_webbased_functions(monkeypatch, data_id, gzip_response)
+    _fetch_dataset_from_openml(data_id, data_name, data_version, target_column,
+                               expected_observations, expected_features,
+                               expected_missing,
+                               object, object, expect_sparse=False,
+                               compare_default_target=True)
+
+
+def test_decode_anneal(monkeypatch):
+    data_id = 2
+    _monkey_patch_webbased_functions(monkeypatch, data_id, False)
+    _test_features_list(data_id)
+
+
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_fetch_openml_anneal_multitarget(monkeypatch, gzip_response):
+    # classification dataset with numeric and categorical columns
+    data_id = 2
+    data_name = 'anneal'
+    data_version = 1
+    target_column = ['class', 'product-type', 'shape']
+    # Not all original instances included for space reasons
+    expected_observations = 11
+    expected_features = 36
+    expected_missing = 267
+    _monkey_patch_webbased_functions(monkeypatch, data_id, gzip_response)
+    _fetch_dataset_from_openml(data_id, data_name, data_version, target_column,
+                               expected_observations, expected_features,
+                               expected_missing,
+                               object, object, expect_sparse=False,
+                               compare_default_target=False)
+
+
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_fetch_openml_cpu(monkeypatch, gzip_response):
+    # regression dataset with numeric and categorical columns
+    data_id = 561
+    data_name = 'cpu'
+    data_version = 1
+    target_column = 'class'
+    expected_observations = 209
+    expected_features = 7
+    expected_missing = 0
+    _monkey_patch_webbased_functions(monkeypatch, data_id, gzip_response)
+    _fetch_dataset_from_openml(data_id, data_name, data_version, target_column,
+                               expected_observations, expected_features,
+                               expected_missing,
+                               object, np.float64, expect_sparse=False,
+                               compare_default_target=True)
+
+
+def test_decode_cpu(monkeypatch):
+    data_id = 561
+    _monkey_patch_webbased_functions(monkeypatch, data_id, False)
+    _test_features_list(data_id)
+
+
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_fetch_openml_australian(monkeypatch, gzip_response):
+    # sparse dataset
+    # Australian is the only sparse dataset that is reasonably small
+    # as it is inactive, we need to catch the warning. Due to mocking
+    # framework, it is not deactivated in our tests
+    data_id = 292
+    data_name = 'Australian'
+    data_version = 1
+    target_column = 'Y'
+    # Not all original instances included for space reasons
+    expected_observations = 85
+    expected_features = 14
+    expected_missing = 0
+    _monkey_patch_webbased_functions(monkeypatch, data_id, gzip_response)
+    assert_warns_message(
+        UserWarning,
+        "Version 1 of dataset Australian is inactive,",
+        _fetch_dataset_from_openml,
+        **{'data_id': data_id, 'data_name': data_name,
+           'data_version': data_version,
+           'target_column': target_column,
+           'expected_observations': expected_observations,
+           'expected_features': expected_features,
+           'expected_missing': expected_missing,
+           'expect_sparse': True,
+           'expected_data_dtype': np.float64,
+           'expected_target_dtype': object,
+           'compare_default_target': False}  # numpy specific check
+    )
+
+
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_fetch_openml_miceprotein(monkeypatch, gzip_response):
+    # JvR: very important check, as this dataset defined several row ids
+    # and ignore attributes. Note that data_features json has 82 attributes,
+    # and row id (1), ignore attributes (3) have been removed (and target is
+    # stored in data.target)
+    data_id = 40966
+    data_name = 'MiceProtein'
+    data_version = 4
+    target_column = 'class'
+    # Not all original instances included for space reasons
+    expected_observations = 7
+    expected_features = 77
+    expected_missing = 7
+    _monkey_patch_webbased_functions(monkeypatch, data_id, gzip_response)
+    _fetch_dataset_from_openml(data_id, data_name, data_version, target_column,
+                               expected_observations, expected_features,
+                               expected_missing,
+                               np.float64, object, expect_sparse=False,
+                               compare_default_target=True)
+
+
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_fetch_openml_emotions(monkeypatch, gzip_response):
+    # classification dataset with multiple targets (natively)
+    data_id = 40589
+    data_name = 'emotions'
+    data_version = 3
+    target_column = ['amazed.suprised', 'happy.pleased', 'relaxing.calm',
+                     'quiet.still', 'sad.lonely', 'angry.aggresive']
+    expected_observations = 13
+    expected_features = 72
+    expected_missing = 0
+    _monkey_patch_webbased_functions(monkeypatch, data_id, gzip_response)
+
+    _fetch_dataset_from_openml(data_id, data_name, data_version, target_column,
+                               expected_observations, expected_features,
+                               expected_missing,
+                               np.float64, object, expect_sparse=False,
+                               compare_default_target=True)
+
+
+def test_decode_emotions(monkeypatch):
+    data_id = 40589
+    _monkey_patch_webbased_functions(monkeypatch, data_id, False)
+    _test_features_list(data_id)
+
+
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_open_openml_url_cache(monkeypatch, gzip_response):
+    data_id = 61
+
+    _monkey_patch_webbased_functions(
+        monkeypatch, data_id, gzip_response)
+    openml_path = sklearn.datasets.openml._DATA_FILE.format(data_id)
+    test_directory = os.path.join(os.path.expanduser('~'), 'scikit_learn_data')
+    # first fill the cache
+    response1 = _open_openml_url(openml_path, test_directory)
+    # assert file exists
+    location = os.path.join(test_directory, 'openml.org', openml_path + '.gz')
+    assert os.path.isfile(location)
+    # redownload, to utilize cache
+    response2 = _open_openml_url(openml_path, test_directory)
+    assert response1.read() == response2.read()
+
+
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_fetch_openml_notarget(monkeypatch, gzip_response):
+    data_id = 61
+    target_column = None
+    expected_observations = 150
+    expected_features = 5
+
+    _monkey_patch_webbased_functions(monkeypatch, data_id, gzip_response)
+    data = fetch_openml(data_id=data_id, target_column=target_column,
+                        cache=False)
+    assert data.data.shape == (expected_observations, expected_features)
+    assert data.target is None
+
+
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_fetch_openml_inactive(monkeypatch, gzip_response):
+    # fetch inactive dataset by id
+    data_id = 40675
+    _monkey_patch_webbased_functions(monkeypatch, data_id, gzip_response)
+    glas2 = assert_warns_message(
+        UserWarning, "Version 1 of dataset glass2 is inactive,", fetch_openml,
+        data_id=data_id, cache=False)
+    # fetch inactive dataset by name and version
+    assert glas2.data.shape == (163, 9)
+    glas2_by_version = assert_warns_message(
+        UserWarning, "Version 1 of dataset glass2 is inactive,", fetch_openml,
+        data_id=None, name="glass2", version=1, cache=False)
+    assert int(glas2_by_version.details['id']) == data_id
+
+
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_fetch_nonexiting(monkeypatch, gzip_response):
+    # there is no active version of glass2
+    data_id = 40675
+    _monkey_patch_webbased_functions(monkeypatch, data_id, gzip_response)
+    # Note that we only want to search by name (not data id)
+    assert_raise_message(ValueError, "No active dataset glass2 found",
+                         fetch_openml, name='glass2', cache=False)
+
+
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_raises_illegal_multitarget(monkeypatch, gzip_response):
+    data_id = 61
+    targets = ['sepalwidth', 'class']
+    _monkey_patch_webbased_functions(monkeypatch, data_id, gzip_response)
+    # Note that we only want to search by name (not data id)
+    assert_raise_message(ValueError,
+                         "Can only handle homogeneous multi-target datasets,",
+                         fetch_openml, data_id=data_id,
+                         target_column=targets, cache=False)
+
+
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_warn_ignore_attribute(monkeypatch, gzip_response):
+    data_id = 40966
+    expected_row_id_msg = "target_column={} has flag is_row_identifier."
+    expected_ignore_msg = "target_column={} has flag is_ignore."
+    _monkey_patch_webbased_functions(monkeypatch, data_id, gzip_response)
+    # single column test
+    assert_warns_message(UserWarning, expected_row_id_msg.format('MouseID'),
+                         fetch_openml, data_id=data_id,
+                         target_column='MouseID',
+                         cache=False)
+    assert_warns_message(UserWarning, expected_ignore_msg.format('Genotype'),
+                         fetch_openml, data_id=data_id,
+                         target_column='Genotype',
+                         cache=False)
+    # multi column test
+    assert_warns_message(UserWarning, expected_row_id_msg.format('MouseID'),
+                         fetch_openml, data_id=data_id,
+                         target_column=['MouseID', 'class'],
+                         cache=False)
+    assert_warns_message(UserWarning, expected_ignore_msg.format('Genotype'),
+                         fetch_openml, data_id=data_id,
+                         target_column=['Genotype', 'class'],
+                         cache=False)
+
+
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_string_attribute(monkeypatch, gzip_response):
+    data_id = 40945
+    _monkey_patch_webbased_functions(monkeypatch, data_id, gzip_response)
+    # single column test
+    assert_raise_message(ValueError,
+                         'STRING attributes are not yet supported',
+                         fetch_openml, data_id=data_id, cache=False)
+
+
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_illegal_column(monkeypatch, gzip_response):
+    data_id = 61
+    _monkey_patch_webbased_functions(monkeypatch, data_id, gzip_response)
+    assert_raise_message(KeyError, "Could not find target_column=",
+                         fetch_openml, data_id=data_id,
+                         target_column='undefined', cache=False)
+
+    assert_raise_message(KeyError, "Could not find target_column=",
+                         fetch_openml, data_id=data_id,
+                         target_column=['undefined', 'class'],
+                         cache=False)
+
+
+@pytest.mark.parametrize('gzip_response', [True, False])
+def test_fetch_openml_raises_missing_values_target(monkeypatch, gzip_response):
+    data_id = 2
+    _monkey_patch_webbased_functions(monkeypatch, data_id, gzip_response)
+    assert_raise_message(ValueError, "Target column ",
+                         fetch_openml, data_id=data_id, target_column='family')
+
+
+def test_fetch_openml_raises_illegal_argument():
+    assert_raise_message(ValueError, "Dataset data_id=",
+                         fetch_openml, data_id=-1, name="name")
+
+    assert_raise_message(ValueError, "Dataset data_id=",
+                         fetch_openml, data_id=-1, name=None,
+                         version="version")
+
+    assert_raise_message(ValueError, "Dataset data_id=",
+                         fetch_openml, data_id=-1, name="name",
+                         version="version")
+
+    assert_raise_message(ValueError, "Neither name nor data_id are provided. "
+                         "Please provide name or data_id.", fetch_openml)
diff --git a/sklearn/decomposition/dict_learning.py b/sklearn/decomposition/dict_learning.py
index b0cf682812..d050949f13 100755
--- a/sklearn/decomposition/dict_learning.py
+++ b/sklearn/decomposition/dict_learning.py
@@ -8,11 +8,10 @@
 import sys
 import itertools
 
-from math import sqrt, ceil
+from math import ceil
 
 import numpy as np
 from scipy import linalg
-from numpy.lib.stride_tricks import as_strided
 
 from ..base import BaseEstimator, TransformerMixin
 from ..utils import Parallel, delayed, effective_n_jobs
@@ -246,8 +245,11 @@ def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars',
     max_iter : int, 1000 by default
         Maximum number of iterations to perform if `algorithm='lasso_cd'`.
 
-    n_jobs : int, optional
+    n_jobs : int or None, optional (default=None)
         Number of parallel jobs to run.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     check_input : boolean, optional
         If False, the input arrays X and dictionary will not be checked.
@@ -376,6 +378,7 @@ def _update_dict(dictionary, Y, code, verbose=False, return_r2=False,
     # Get BLAS functions
     gemm, = linalg.get_blas_funcs(('gemm',), (dictionary, code, Y))
     ger, = linalg.get_blas_funcs(('ger',), (dictionary, code))
+    nrm2, = linalg.get_blas_funcs(('nrm2',), (dictionary,))
     # Residuals, computed with BLAS for speed and efficiency
     # R <- -1.0 * U * V^T + 1.0 * Y
     # Outputs R as Fortran array for efficiency
@@ -383,12 +386,13 @@ def _update_dict(dictionary, Y, code, verbose=False, return_r2=False,
     for k in range(n_components):
         # R <- 1.0 * U_k * V_k^T + R
         R = ger(1.0, dictionary[:, k], code[k, :], a=R, overwrite_a=True)
-        dictionary[:, k] = np.dot(R, code[k, :].T)
+        dictionary[:, k] = np.dot(R, code[k, :])
         if positive:
             np.clip(dictionary[:, k], 0, None, out=dictionary[:, k])
         # Scale k'th atom
-        atom_norm_square = np.dot(dictionary[:, k], dictionary[:, k])
-        if atom_norm_square < 1e-20:
+        # (U_k * U_k) ** 0.5
+        atom_norm = nrm2(dictionary[:, k])
+        if atom_norm < 1e-10:
             if verbose == 1:
                 sys.stdout.write("+")
                 sys.stdout.flush()
@@ -399,20 +403,15 @@ def _update_dict(dictionary, Y, code, verbose=False, return_r2=False,
                 np.clip(dictionary[:, k], 0, None, out=dictionary[:, k])
             # Setting corresponding coefs to 0
             code[k, :] = 0.0
-            dictionary[:, k] /= sqrt(np.dot(dictionary[:, k],
-                                            dictionary[:, k]))
+            # (U_k * U_k) ** 0.5
+            atom_norm = nrm2(dictionary[:, k])
+            dictionary[:, k] /= atom_norm
         else:
-            dictionary[:, k] /= sqrt(atom_norm_square)
+            dictionary[:, k] /= atom_norm
             # R <- -1.0 * U_k * V_k^T + R
             R = ger(-1.0, dictionary[:, k], code[k, :], a=R, overwrite_a=True)
     if return_r2:
-        R **= 2
-        # R is fortran-ordered. For numpy version < 1.6, sum does not
-        # follow the quick striding first, and is thus inefficient on
-        # fortran ordered data. We take a flat view of the data with no
-        # striding
-        R = as_strided(R, shape=(R.size, ), strides=(R.dtype.itemsize,))
-        R = np.sum(R)
+        R = nrm2(R) ** 2.0
         return dictionary, R
     return dictionary
 
@@ -459,8 +458,11 @@ def dict_learning(X, n_components, alpha, max_iter=100, tol=1e-8,
         Lasso solution (linear_model.Lasso). Lars will be faster if
         the estimated components are sparse.
 
-    n_jobs : int,
-        Number of parallel jobs to run, or -1 to autodetect.
+    n_jobs : int or None, optional (default=None)
+        Number of parallel jobs to run.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     dict_init : array of shape (n_components, n_features),
         Initial value for the dictionary for warm restart scenarios.
@@ -654,8 +656,11 @@ def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,
     shuffle : boolean,
         Whether to shuffle the data before splitting it in batches.
 
-    n_jobs : int,
-        Number of parallel jobs to run, or -1 to autodetect.
+    n_jobs : int or None, optional (default=None)
+        Number of parallel jobs to run.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     method : {'lars', 'cd'}
         lars: uses the least angle regression method to solve the lasso problem
@@ -949,8 +954,11 @@ class SparseCoder(BaseEstimator, SparseCodingMixin):
         its negative part and its positive part. This can improve the
         performance of downstream classifiers.
 
-    n_jobs : int,
-        number of parallel jobs to run
+    n_jobs : int or None, optional (default=None)
+        Number of parallel jobs to run.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     positive_code : bool
         Whether to enforce positivity when finding the code.
@@ -1069,8 +1077,11 @@ class DictionaryLearning(BaseEstimator, SparseCodingMixin):
         the reconstruction error targeted. In this case, it overrides
         `n_nonzero_coefs`.
 
-    n_jobs : int,
-        number of parallel jobs to run
+    n_jobs : int or None, optional (default=None)
+        Number of parallel jobs to run.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     code_init : array of shape (n_samples, n_components),
         initial value for the code, for warm restart
@@ -1220,8 +1231,11 @@ class MiniBatchDictionaryLearning(BaseEstimator, SparseCodingMixin):
         Lasso solution (linear_model.Lasso). Lars will be faster if
         the estimated components are sparse.
 
-    n_jobs : int,
-        number of parallel jobs to run
+    n_jobs : int or None, optional (default=None)
+        Number of parallel jobs to run.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     batch_size : int,
         number of samples in each mini-batch
diff --git a/sklearn/decomposition/factor_analysis.py b/sklearn/decomposition/factor_analysis.py
index 481a5e2322..eea477937e 100755
--- a/sklearn/decomposition/factor_analysis.py
+++ b/sklearn/decomposition/factor_analysis.py
@@ -108,6 +108,16 @@ class FactorAnalysis(BaseEstimator, TransformerMixin):
     n_iter_ : int
         Number of iterations run.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.decomposition import FactorAnalysis
+    >>> X, _ = load_digits(return_X_y=True)
+    >>> transformer = FactorAnalysis(n_components=7, random_state=0)
+    >>> X_transformed = transformer.fit_transform(X)
+    >>> X_transformed.shape
+    (1797, 7)
+
     References
     ----------
     .. David Barber, Bayesian Reasoning and Machine Learning,
diff --git a/sklearn/decomposition/fastica_.py b/sklearn/decomposition/fastica_.py
index f2484672ab..f64d4787b3 100755
--- a/sklearn/decomposition/fastica_.py
+++ b/sklearn/decomposition/fastica_.py
@@ -443,6 +443,17 @@ def my_g(x):
         maximum number of iterations run across all components. Else
         they are just the number of iterations taken to converge.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.decomposition import FastICA
+    >>> X, _ = load_digits(return_X_y=True)
+    >>> transformer = FastICA(n_components=7,
+    ...         random_state=0)
+    >>> X_transformed = transformer.fit_transform(X)
+    >>> X_transformed.shape
+    (1797, 7)
+
     Notes
     -----
     Implementation based on
diff --git a/sklearn/decomposition/incremental_pca.py b/sklearn/decomposition/incremental_pca.py
index 72f1326c58..05e6693051 100755
--- a/sklearn/decomposition/incremental_pca.py
+++ b/sklearn/decomposition/incremental_pca.py
@@ -100,6 +100,20 @@ class IncrementalPCA(_BasePCA):
         The number of samples processed by the estimator. Will be reset on
         new calls to fit, but increments across ``partial_fit`` calls.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.decomposition import IncrementalPCA
+    >>> X, _ = load_digits(return_X_y=True)
+    >>> transformer = IncrementalPCA(n_components=7, batch_size=200)
+    >>> # either partially fit on smaller batches of data
+    >>> transformer.partial_fit(X[:100, :])
+    IncrementalPCA(batch_size=200, copy=True, n_components=7, whiten=False)
+    >>> # or let the fit function itself divide the data into batches
+    >>> X_transformed = transformer.fit_transform(X)
+    >>> X_transformed.shape
+    (1797, 7)
+
     Notes
     -----
     Implements the incremental PCA model from:
diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py
index 60b93ae3d6..133717e13f 100755
--- a/sklearn/decomposition/kernel_pca.py
+++ b/sklearn/decomposition/kernel_pca.py
@@ -89,9 +89,11 @@ class KernelPCA(BaseEstimator, TransformerMixin):
 
         .. versionadded:: 0.18
 
-    n_jobs : int, default=1
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run.
-        If `-1`, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
         .. versionadded:: 0.18
 
@@ -118,6 +120,16 @@ class KernelPCA(BaseEstimator, TransformerMixin):
         The data used to fit the model. If `copy_X=False`, then `X_fit_` is
         a reference. This attribute is used for the calls to transform.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.decomposition import KernelPCA
+    >>> X, _ = load_digits(return_X_y=True)
+    >>> transformer = KernelPCA(n_components=7, kernel='linear')
+    >>> X_transformed = transformer.fit_transform(X)
+    >>> X_transformed.shape
+    (1797, 7)
+
     References
     ----------
     Kernel PCA was introduced in:
diff --git a/sklearn/decomposition/nmf.py b/sklearn/decomposition/nmf.py
index 990d31bf2c..0617a1797f 100755
--- a/sklearn/decomposition/nmf.py
+++ b/sklearn/decomposition/nmf.py
@@ -880,7 +880,7 @@ def non_negative_factorization(X, W=None, H=None, n_components=None,
 
     init :  None | 'random' | 'nndsvd' | 'nndsvda' | 'nndsvdar' | 'custom'
         Method used to initialize the procedure.
-        Default: 'nndsvd' if n_components < n_features, otherwise random.
+        Default: 'random'.
         Valid options:
 
         - 'random': non-negative random matrices, scaled with:
diff --git a/sklearn/decomposition/online_lda.py b/sklearn/decomposition/online_lda.py
index f9fa165eb0..5b48ea1a26 100755
--- a/sklearn/decomposition/online_lda.py
+++ b/sklearn/decomposition/online_lda.py
@@ -215,9 +215,11 @@ class LatentDirichletAllocation(BaseEstimator, TransformerMixin):
         Max number of iterations for updating document topic distribution in
         the E-step.
 
-    n_jobs : int, optional (default=1)
-        The number of jobs to use in the E-step. If -1, all CPUs are used. For
-        ``n_jobs`` below -1, (n_cpus + 1 + n_jobs) are used.
+    n_jobs : int or None, optional (default=None)
+        The number of jobs to use in the E-step.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : int, optional (default=0)
         Verbosity level.
@@ -250,6 +252,22 @@ class LatentDirichletAllocation(BaseEstimator, TransformerMixin):
     n_iter_ : int
         Number of passes over the dataset.
 
+    Examples
+    --------
+    >>> from sklearn.decomposition import LatentDirichletAllocation
+    >>> from sklearn.datasets import make_multilabel_classification
+    >>> # This produces a feature matrix of token counts, similar to what
+    >>> # CountVectorizer would produce on text.
+    >>> X, _ = make_multilabel_classification(random_state=0)
+    >>> lda = LatentDirichletAllocation(n_components=5,
+    ...     random_state=0)
+    >>> lda.fit(X) # doctest: +ELLIPSIS
+    LatentDirichletAllocation(...)
+    >>> # get topics for some given samples:
+    >>> lda.transform(X[-2:])
+    array([[0.00360392, 0.25499205, 0.0036211 , 0.64236448, 0.09541846],
+           [0.15297572, 0.00362644, 0.44412786, 0.39568399, 0.003586  ]])
+
     References
     ----------
     [1] "Online Learning for Latent Dirichlet Allocation", Matthew D. Hoffman,
diff --git a/sklearn/decomposition/sparse_pca.py b/sklearn/decomposition/sparse_pca.py
index f34dcbf376..95c9ab8960 100755
--- a/sklearn/decomposition/sparse_pca.py
+++ b/sklearn/decomposition/sparse_pca.py
@@ -48,8 +48,11 @@ class SparsePCA(BaseEstimator, TransformerMixin):
         Lasso solution (linear_model.Lasso). Lars will be faster if
         the estimated components are sparse.
 
-    n_jobs : int,
+    n_jobs : int or None, optional (default=None)
         Number of parallel jobs to run.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     U_init : array of shape (n_samples, n_components),
         Initial values for the loadings for warm restart scenarios.
@@ -96,6 +99,24 @@ class SparsePCA(BaseEstimator, TransformerMixin):
         Per-feature empirical mean, estimated from the training set.
         Equal to ``X.mean(axis=0)``.
 
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.datasets import make_friedman1
+    >>> from sklearn.decomposition import SparsePCA
+    >>> X, _ = make_friedman1(n_samples=200, n_features=30, random_state=0)
+    >>> transformer = SparsePCA(n_components=5,
+    ...         normalize_components=True,
+    ...         random_state=0)
+    >>> transformer.fit(X) # doctest: +ELLIPSIS
+    SparsePCA(...)
+    >>> X_transformed = transformer.transform(X)
+    >>> X_transformed.shape
+    (200, 5)
+    >>> # most values in the components_ are zero (sparsity)
+    >>> np.mean(transformer.components_ == 0) # doctest: +ELLIPSIS
+    0.9666...
+
     See also
     --------
     PCA
@@ -269,8 +290,11 @@ class MiniBatchSparsePCA(SparsePCA):
     shuffle : boolean,
         whether to shuffle the data before splitting it in batches
 
-    n_jobs : int,
-        number of parallel jobs to run, or -1 to autodetect.
+    n_jobs : int or None, optional (default=None)
+        Number of parallel jobs to run.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     method : {'lars', 'cd'}
         lars: uses the least angle regression method to solve the lasso problem
@@ -312,6 +336,25 @@ class MiniBatchSparsePCA(SparsePCA):
         Per-feature empirical mean, estimated from the training set.
         Equal to ``X.mean(axis=0)``.
 
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.datasets import make_friedman1
+    >>> from sklearn.decomposition import MiniBatchSparsePCA
+    >>> X, _ = make_friedman1(n_samples=200, n_features=30, random_state=0)
+    >>> transformer = MiniBatchSparsePCA(n_components=5,
+    ...         batch_size=50,
+    ...         normalize_components=True,
+    ...         random_state=0)
+    >>> transformer.fit(X) # doctest: +ELLIPSIS
+    MiniBatchSparsePCA(...)
+    >>> X_transformed = transformer.transform(X)
+    >>> X_transformed.shape
+    (200, 5)
+    >>> # most values in the components_ are zero (sparsity)
+    >>> np.mean(transformer.components_ == 0)
+    0.94
+
     See also
     --------
     PCA
diff --git a/sklearn/dummy.py b/sklearn/dummy.py
index f2c8664131..2fac84fd7b 100755
--- a/sklearn/dummy.py
+++ b/sklearn/dummy.py
@@ -105,9 +105,11 @@ def fit(self, X, y, sample_weight=None):
         -------
         self : object
         """
-        if self.strategy not in ("most_frequent", "stratified", "uniform",
-                                 "constant", "prior"):
-            raise ValueError("Unknown strategy type.")
+        allowed_strategies = ("most_frequent", "stratified", "uniform",
+                              "constant", "prior")
+        if self.strategy not in allowed_strategies:
+            raise ValueError("Unknown strategy type: %s, expected one of %s."
+                             % (self.strategy, allowed_strategies))
 
         if self.strategy == "uniform" and sp.issparse(y):
             y = y.toarray()
@@ -318,6 +320,37 @@ def predict_log_proba(self, X):
         else:
             return [np.log(p) for p in proba]
 
+    def score(self, X, y, sample_weight=None):
+        """Returns the mean accuracy on the given test data and labels.
+
+        In multi-label classification, this is the subset accuracy
+        which is a harsh metric since you require for each sample that
+        each label set be correctly predicted.
+
+        Parameters
+        ----------
+        X : {array-like, None}
+            Test samples with shape = (n_samples, n_features) or
+            None. Passing None as test samples gives the same result
+            as passing real test samples, since DummyClassifier
+            operates independently of the sampled observations.
+
+        y : array-like, shape = (n_samples) or (n_samples, n_outputs)
+            True labels for X.
+
+        sample_weight : array-like, shape = [n_samples], optional
+            Sample weights.
+
+        Returns
+        -------
+        score : float
+            Mean accuracy of self.predict(X) wrt. y.
+
+        """
+        if X is None:
+            X = np.zeros(shape=(len(y), 1))
+        return super(DummyClassifier, self).score(X, y, sample_weight)
+
 
 class DummyRegressor(BaseEstimator, RegressorMixin):
     """
@@ -386,10 +419,10 @@ def fit(self, X, y, sample_weight=None):
         -------
         self : object
         """
-        if self.strategy not in ("mean", "median", "quantile", "constant"):
-            raise ValueError("Unknown strategy type: %s, expected "
-                             "'mean', 'median', 'quantile' or 'constant'"
-                             % self.strategy)
+        allowed_strategies = ("mean", "median", "quantile", "constant")
+        if self.strategy not in allowed_strategies:
+            raise ValueError("Unknown strategy type: %s, expected one of %s."
+                             % (self.strategy, allowed_strategies))
 
         y = check_array(y, ensure_2d=False)
         if len(y) == 0:
@@ -478,3 +511,41 @@ def predict(self, X, return_std=False):
             y_std = np.ravel(y_std)
 
         return (y, y_std) if return_std else y
+
+    def score(self, X, y, sample_weight=None):
+        """Returns the coefficient of determination R^2 of the prediction.
+
+        The coefficient R^2 is defined as (1 - u/v), where u is the residual
+        sum of squares ((y_true - y_pred) ** 2).sum() and v is the total
+        sum of squares ((y_true - y_true.mean()) ** 2).sum().
+        The best possible score is 1.0 and it can be negative (because the
+        model can be arbitrarily worse). A constant model that always
+        predicts the expected value of y, disregarding the input features,
+        would get a R^2 score of 0.0.
+
+        Parameters
+        ----------
+        X : {array-like, None}
+            Test samples with shape = (n_samples, n_features) or None.
+            For some estimators this may be a
+            precomputed kernel matrix instead, shape = (n_samples,
+            n_samples_fitted], where n_samples_fitted is the number of
+            samples used in the fitting for the estimator.
+            Passing None as test samples gives the same result
+            as passing real test samples, since DummyRegressor
+            operates independently of the sampled observations.
+
+        y : array-like, shape = (n_samples) or (n_samples, n_outputs)
+            True values for X.
+
+        sample_weight : array-like, shape = [n_samples], optional
+            Sample weights.
+
+        Returns
+        -------
+        score : float
+            R^2 of self.predict(X) wrt. y.
+        """
+        if X is None:
+            X = np.zeros(shape=(len(y), 1))
+        return super(DummyRegressor, self).score(X, y, sample_weight)
diff --git a/sklearn/ensemble/bagging.py b/sklearn/ensemble/bagging.py
index c019bad82e..51dce324a0 100755
--- a/sklearn/ensemble/bagging.py
+++ b/sklearn/ensemble/bagging.py
@@ -484,9 +484,11 @@ class BaggingClassifier(BaseBagging, ClassifierMixin):
         .. versionadded:: 0.17
            *warm_start* constructor parameter.
 
-    n_jobs : int, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of jobs to run in parallel for both `fit` and `predict`.
-        If -1, then the number of jobs is set to the number of cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
@@ -860,9 +862,11 @@ class BaggingRegressor(BaseBagging, RegressorMixin):
         and add more estimators to the ensemble, otherwise, just fit
         a whole new ensemble. See :term:`the Glossary <warm_start>`.
 
-    n_jobs : int, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of jobs to run in parallel for both `fit` and `predict`.
-        If -1, then the number of jobs is set to the number of cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
diff --git a/sklearn/ensemble/forest.py b/sklearn/ensemble/forest.py
index fb2d3025bf..542f7ca804 100755
--- a/sklearn/ensemble/forest.py
+++ b/sklearn/ensemble/forest.py
@@ -793,7 +793,11 @@ class RandomForestClassifier(ForestClassifier):
            Added float values for fractions.
 
     min_samples_leaf : int, float, optional (default=1)
-        The minimum number of samples required to be at a leaf node:
+        The minimum number of samples required to be at a leaf node.
+        A split point at any depth will only be considered if it leaves at
+        least ``min_samples_leaf`` training samples in each of the left and
+        right branches.  This may have the effect of smoothing the model,
+        especially in regression.
 
         - If int, then consider `min_samples_leaf` as the minimum number.
         - If float, then `min_samples_leaf` is a fraction and
@@ -863,9 +867,11 @@ class RandomForestClassifier(ForestClassifier):
         Whether to use out-of-bag samples to estimate
         the generalization accuracy.
 
-    n_jobs : integer, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of jobs to run in parallel for both `fit` and `predict`.
-        If -1, then the number of jobs is set to the number of cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
@@ -1076,7 +1082,11 @@ class RandomForestRegressor(ForestRegressor):
            Added float values for fractions.
 
     min_samples_leaf : int, float, optional (default=1)
-        The minimum number of samples required to be at a leaf node:
+        The minimum number of samples required to be at a leaf node.
+        A split point at any depth will only be considered if it leaves at
+        least ``min_samples_leaf`` training samples in each of the left and
+        right branches.  This may have the effect of smoothing the model,
+        especially in regression.
 
         - If int, then consider `min_samples_leaf` as the minimum number.
         - If float, then `min_samples_leaf` is a fraction and
@@ -1146,9 +1156,11 @@ class RandomForestRegressor(ForestRegressor):
         whether to use out-of-bag samples to estimate
         the R^2 on unseen data.
 
-    n_jobs : integer, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of jobs to run in parallel for both `fit` and `predict`.
-        If -1, then the number of jobs is set to the number of cores.
+        `None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
@@ -1220,11 +1232,18 @@ class RandomForestRegressor(ForestRegressor):
     search of the best split. To obtain a deterministic behaviour during
     fitting, ``random_state`` has to be fixed.
 
+    The default value ``max_features="auto"`` uses ``n_features`` 
+    rather than ``n_features / 3``. The latter was originally suggested in
+    [1], whereas the former was more recently justified empirically in [2].
+
     References
     ----------
 
     .. [1] L. Breiman, "Random Forests", Machine Learning, 45(1), 5-32, 2001.
 
+    .. [2] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized 
+           trees", Machine Learning, 63(1), 3-42, 2006.
+
     See also
     --------
     DecisionTreeRegressor, ExtraTreesRegressor
@@ -1312,7 +1331,11 @@ class ExtraTreesClassifier(ForestClassifier):
            Added float values for fractions.
 
     min_samples_leaf : int, float, optional (default=1)
-        The minimum number of samples required to be at a leaf node:
+        The minimum number of samples required to be at a leaf node.
+        A split point at any depth will only be considered if it leaves at
+        least ``min_samples_leaf`` training samples in each of the left and
+        right branches.  This may have the effect of smoothing the model,
+        especially in regression.
 
         - If int, then consider `min_samples_leaf` as the minimum number.
         - If float, then `min_samples_leaf` is a fraction and
@@ -1382,9 +1405,11 @@ class ExtraTreesClassifier(ForestClassifier):
         Whether to use out-of-bag samples to estimate
         the generalization accuracy.
 
-    n_jobs : integer, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of jobs to run in parallel for both `fit` and `predict`.
-        If -1, then the number of jobs is set to the number of cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
@@ -1467,8 +1492,8 @@ class labels (multi-output problem).
     References
     ----------
 
-    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized trees",
-           Machine Learning, 63(1), 3-42, 2006.
+    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized 
+           trees", Machine Learning, 63(1), 3-42, 2006.
 
     See also
     --------
@@ -1566,7 +1591,11 @@ class ExtraTreesRegressor(ForestRegressor):
            Added float values for fractions.
 
     min_samples_leaf : int, float, optional (default=1)
-        The minimum number of samples required to be at a leaf node:
+        The minimum number of samples required to be at a leaf node.
+        A split point at any depth will only be considered if it leaves at
+        least ``min_samples_leaf`` training samples in each of the left and
+        right branches.  This may have the effect of smoothing the model,
+        especially in regression.
 
         - If int, then consider `min_samples_leaf` as the minimum number.
         - If float, then `min_samples_leaf` is a fraction and
@@ -1635,9 +1664,11 @@ class ExtraTreesRegressor(ForestRegressor):
     oob_score : bool, optional (default=False)
         Whether to use out-of-bag samples to estimate the R^2 on unseen data.
 
-    n_jobs : integer, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of jobs to run in parallel for both `fit` and `predict`.
-        If -1, then the number of jobs is set to the number of cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
@@ -1776,7 +1807,11 @@ class RandomTreesEmbedding(BaseForest):
            Added float values for fractions.
 
     min_samples_leaf : int, float, optional (default=1)
-        The minimum number of samples required to be at a leaf node:
+        The minimum number of samples required to be at a leaf node.
+        A split point at any depth will only be considered if it leaves at
+        least ``min_samples_leaf`` training samples in each of the left and
+        right branches.  This may have the effect of smoothing the model,
+        especially in regression.
 
         - If int, then consider `min_samples_leaf` as the minimum number.
         - If float, then `min_samples_leaf` is a fraction and
@@ -1827,9 +1862,11 @@ class RandomTreesEmbedding(BaseForest):
         Whether or not to return a sparse CSR matrix, as default behavior,
         or to return a dense array compatible with dense pipeline operators.
 
-    n_jobs : integer, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of jobs to run in parallel for both `fit` and `predict`.
-        If -1, then the number of jobs is set to the number of cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
diff --git a/sklearn/ensemble/gradient_boosting.py b/sklearn/ensemble/gradient_boosting.py
index c76a5722c5..6ae4f6fd1b 100755
--- a/sklearn/ensemble/gradient_boosting.py
+++ b/sklearn/ensemble/gradient_boosting.py
@@ -38,7 +38,6 @@
 import numbers
 import numpy as np
 
-from scipy import stats
 from scipy.sparse import csc_matrix
 from scipy.sparse import csr_matrix
 from scipy.sparse import issparse
@@ -91,7 +90,7 @@ def fit(self, X, y, sample_weight=None):
             Individual weights for each sample
         """
         if sample_weight is None:
-            self.quantile = stats.scoreatpercentile(y, self.alpha * 100.0)
+            self.quantile = np.percentile(y, self.alpha * 100.0)
         else:
             self.quantile = _weighted_percentile(y, sample_weight,
                                                  self.alpha * 100.0)
@@ -608,7 +607,7 @@ def __call__(self, y, pred, sample_weight=None):
         gamma = self.gamma
         if gamma is None:
             if sample_weight is None:
-                gamma = stats.scoreatpercentile(np.abs(diff), self.alpha * 100)
+                gamma = np.percentile(np.abs(diff), self.alpha * 100)
             else:
                 gamma = _weighted_percentile(np.abs(diff), sample_weight, self.alpha * 100)
 
@@ -641,7 +640,7 @@ def negative_gradient(self, y, pred, sample_weight=None, **kargs):
         pred = pred.ravel()
         diff = y - pred
         if sample_weight is None:
-            gamma = stats.scoreatpercentile(np.abs(diff), self.alpha * 100)
+            gamma = np.percentile(np.abs(diff), self.alpha * 100)
         else:
             gamma = _weighted_percentile(np.abs(diff), sample_weight, self.alpha * 100)
         gamma_mask = np.abs(diff) <= gamma
@@ -1190,22 +1189,14 @@ def _fit_stage(self, i, X, y, y_pred, sample_weight, sample_mask,
                 # no inplace multiplication!
                 sample_weight = sample_weight * sample_mask.astype(np.float64)
 
-            if X_csc is not None:
-                tree.fit(X_csc, residual, sample_weight=sample_weight,
-                         check_input=False, X_idx_sorted=X_idx_sorted)
-            else:
-                tree.fit(X, residual, sample_weight=sample_weight,
-                         check_input=False, X_idx_sorted=X_idx_sorted)
+            X = X_csr if X_csr is not None else X
+            tree.fit(X, residual, sample_weight=sample_weight,
+                     check_input=False, X_idx_sorted=X_idx_sorted)
 
             # update tree leaves
-            if X_csr is not None:
-                loss.update_terminal_regions(tree.tree_, X_csr, y, residual, y_pred,
-                                             sample_weight, sample_mask,
-                                             self.learning_rate, k=k)
-            else:
-                loss.update_terminal_regions(tree.tree_, X, y, residual, y_pred,
-                                             sample_weight, sample_mask,
-                                             self.learning_rate, k=k)
+            loss.update_terminal_regions(tree.tree_, X, y, residual, y_pred,
+                                         sample_weight, sample_mask,
+                                         self.learning_rate, k=k)
 
             # add tree to ensemble
             self.estimators_[i, k] = tree
@@ -1366,9 +1357,10 @@ def fit(self, X, y, sample_weight=None, monitor=None):
 
         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
-            Training vectors, where n_samples is the number of samples
-            and n_features is the number of features.
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
+            The input samples. Internally, it will be converted to
+            ``dtype=np.float32`` and if a sparse matrix is provided
+            to a sparse ``csr_matrix``.
 
         y : array-like, shape (n_samples,)
             Target values (strings or integers in classification, real numbers
@@ -1747,7 +1739,11 @@ class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):
            Added float values for fractions.
 
     min_samples_leaf : int, float, optional (default=1)
-        The minimum number of samples required to be at a leaf node:
+        The minimum number of samples required to be at a leaf node.
+        A split point at any depth will only be considered if it leaves at
+        least ``min_samples_leaf`` training samples in each of the left and
+        right branches.  This may have the effect of smoothing the model,
+        especially in regression.
 
         - If int, then consider `min_samples_leaf` as the minimum number.
         - If float, then `min_samples_leaf` is a fraction and
@@ -2202,7 +2198,11 @@ class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):
            Added float values for fractions.
 
     min_samples_leaf : int, float, optional (default=1)
-        The minimum number of samples required to be at a leaf node:
+        The minimum number of samples required to be at a leaf node.
+        A split point at any depth will only be considered if it leaves at
+        least ``min_samples_leaf`` training samples in each of the left and
+        right branches.  This may have the effect of smoothing the model,
+        especially in regression.
 
         - If int, then consider `min_samples_leaf` as the minimum number.
         - If float, then `min_samples_leaf` is a fraction and
diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py
index 46635124ed..72d1d206f4 100755
--- a/sklearn/ensemble/iforest.py
+++ b/sklearn/ensemble/iforest.py
@@ -5,7 +5,6 @@
 from __future__ import division
 
 import numpy as np
-import scipy as sp
 import warnings
 from warnings import warn
 from sklearn.utils.fixes import euler_gamma
@@ -85,9 +84,11 @@ class IsolationForest(BaseBagging, OutlierMixin):
         data sampled with replacement. If False, sampling without replacement
         is performed.
 
-    n_jobs : integer, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of jobs to run in parallel for both `fit` and `predict`.
-        If -1, then the number of jobs is set to the number of cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     behaviour : str, default='old'
         Behaviour of the ``decision_function`` which can be either 'old' or
@@ -134,7 +135,7 @@ class IsolationForest(BaseBagging, OutlierMixin):
     offset_ : float
         Offset used to define the decision function from the raw scores.
         We have the relation: ``decision_function = score_samples - offset_``.
-        Assuming behaviour == 'new', offset_ is defined as follows.
+        Assuming behaviour == 'new', ``offset_`` is defined as follows.
         When the contamination parameter is set to "auto", the offset is equal
         to -0.5 as the scores of inliers are close to 0 and the scores of
         outliers are close to -1. When a contamination parameter different
@@ -142,7 +143,7 @@ class IsolationForest(BaseBagging, OutlierMixin):
         the expected number of outliers (samples with decision function < 0)
         in training.
         Assuming the behaviour parameter is set to 'old', we always have
-        offset_ = -0.5, making the decision function independent from the
+        ``offset_ = -0.5``, making the decision function independent from the
         contamination parameter.
 
     References
@@ -199,6 +200,9 @@ def fit(self, X, y=None, sample_weight=None):
         sample_weight : array-like, shape = [n_samples] or None
             Sample weights. If None, then samples are equally weighted.
 
+        y : Ignored
+            not used, present for API consistency by convention.
+
         Returns
         -------
         self : object
@@ -267,8 +271,8 @@ def fit(self, X, y=None, sample_weight=None):
                                  "'auto' when behaviour == 'old'.")
 
             self.offset_ = -0.5
-            self._threshold_ = sp.stats.scoreatpercentile(
-                self.decision_function(X), 100. * self._contamination)
+            self._threshold_ = np.percentile(self.decision_function(X),
+                                             100. * self._contamination)
 
             return self
 
@@ -281,8 +285,8 @@ def fit(self, X, y=None, sample_weight=None):
 
         # else, define offset_ wrt contamination parameter, so that the
         # threshold_ attribute is implicitly 0 and is not needed anymore:
-        self.offset_ = sp.stats.scoreatpercentile(
-            self.score_samples(X), 100. * self._contamination)
+        self.offset_ = np.percentile(self.score_samples(X),
+                                     100. * self._contamination)
 
         return self
 
diff --git a/sklearn/ensemble/partial_dependence.py b/sklearn/ensemble/partial_dependence.py
index 6e4f0ed641..f8d5ca7f24 100755
--- a/sklearn/ensemble/partial_dependence.py
+++ b/sklearn/ensemble/partial_dependence.py
@@ -203,9 +203,10 @@ def plot_partial_dependence(gbrt, X, features, feature_names=None,
     percentiles : (low, high), default=(0.05, 0.95)
         The lower and upper percentile used to create the extreme values
         for the PDP axes.
-    n_jobs : int
-        The number of CPUs to use to compute the PDs. -1 means 'all CPUs'.
-        Defaults to 1.
+    n_jobs : int or None, optional (default=None)
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
     verbose : int
         Verbose output during PD computations. Defaults to 0.
     ax : Matplotlib axis object, default None
diff --git a/sklearn/ensemble/tests/test_bagging.py b/sklearn/ensemble/tests/test_bagging.py
index 505ec2f17b..608df3dc43 100755
--- a/sklearn/ensemble/tests/test_bagging.py
+++ b/sklearn/ensemble/tests/test_bagging.py
@@ -293,6 +293,8 @@ def test_bootstrap_features():
         assert_greater(boston.data.shape[1], np.unique(features).shape[0])
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_probability():
     # Predict probabilities.
     rng = check_random_state(0)
@@ -712,6 +714,8 @@ def test_oob_score_consistency():
     assert_equal(bagging.fit(X, y).oob_score_, bagging.fit(X, y).oob_score_)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_estimators_samples():
     # Check that format of estimators_samples_ is correct and that results
     # generated at fit time can be identically reproduced at a later time
@@ -748,6 +752,8 @@ def test_estimators_samples():
     assert_array_almost_equal(orig_coefs, new_coefs)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_estimators_samples_deterministic():
     # This test is a regression test to check that with a random step
     # (e.g. SparseRandomProjection) and a given random state, the results
diff --git a/sklearn/ensemble/tests/test_gradient_boosting.py b/sklearn/ensemble/tests/test_gradient_boosting.py
index 6f7654c7d6..e407ca8ef2 100755
--- a/sklearn/ensemble/tests/test_gradient_boosting.py
+++ b/sklearn/ensemble/tests/test_gradient_boosting.py
@@ -452,6 +452,7 @@ def test_max_feature_regression():
     assert_true(deviance < 0.5, "GB failed with deviance %.4f" % deviance)
 
 
+@pytest.mark.network
 def test_feature_importance_regression():
     """Test that Gini importance is calculated correctly.
 
diff --git a/sklearn/ensemble/tests/test_voting_classifier.py b/sklearn/ensemble/tests/test_voting_classifier.py
index f5bfdbd101..16de82e661 100755
--- a/sklearn/ensemble/tests/test_voting_classifier.py
+++ b/sklearn/ensemble/tests/test_voting_classifier.py
@@ -28,6 +28,8 @@
 X, y = iris.data[:, 1:3], iris.target
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_estimator_init():
     eclf = VotingClassifier(estimators=[])
     msg = ('Invalid `estimators` attribute, `estimators` should be'
@@ -59,6 +61,8 @@ def test_estimator_init():
     assert_raise_message(ValueError, msg, eclf.fit, X, y)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_predictproba_hardvoting():
     eclf = VotingClassifier(estimators=[('lr1', LogisticRegression()),
                                         ('lr2', LogisticRegression())],
@@ -67,6 +71,8 @@ def test_predictproba_hardvoting():
     assert_raise_message(AttributeError, msg, eclf.predict_proba, X)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_notfitted():
     eclf = VotingClassifier(estimators=[('lr1', LogisticRegression()),
                                         ('lr2', LogisticRegression())],
@@ -76,6 +82,8 @@ def test_notfitted():
     assert_raise_message(NotFittedError, msg, eclf.predict_proba, X)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_majority_label_iris():
     """Check classification by majority label on dataset iris."""
@@ -92,7 +100,8 @@ def test_majority_label_iris():
 @pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_tie_situation():
     """Check voting classifier selects smaller class label in tie situation."""
-    clf1 = LogisticRegression(random_state=123)
+    clf1 = LogisticRegression(random_state=123, multi_class='ovr',
+                              solver='liblinear')
     clf2 = RandomForestClassifier(random_state=123)
     eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)],
                             voting='hard')
@@ -101,6 +110,8 @@ def test_tie_situation():
     assert_equal(eclf.fit(X, y).predict(X)[73], 1)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_weights_iris():
     """Check classification by average probabilities on dataset iris."""
@@ -115,6 +126,8 @@ def test_weights_iris():
     assert_almost_equal(scores.mean(), 0.93, decimal=2)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_predict_on_toy_problem():
     """Manually check predicted class labels for toy dataset."""
@@ -148,6 +161,8 @@ def test_predict_on_toy_problem():
     assert_equal(all(eclf.fit(X, y).predict(X)), all([1, 1, 1, 2, 2, 2]))
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_predict_proba_on_toy_problem():
     """Calculate predicted probabilities on toy dataset."""
@@ -188,18 +203,14 @@ def test_predict_proba_on_toy_problem():
     assert_almost_equal(t21, eclf_res[2][1], decimal=1)
     assert_almost_equal(t31, eclf_res[3][1], decimal=1)
 
-    try:
+    with pytest.raises(
+            AttributeError,
+            match="predict_proba is not available when voting='hard'"):
         eclf = VotingClassifier(estimators=[
                                 ('lr', clf1), ('rf', clf2), ('gnb', clf3)],
                                 voting='hard')
         eclf.fit(X, y).predict_proba(X)
 
-    except AttributeError:
-        pass
-    else:
-        raise AssertionError('AttributeError for voting == "hard"'
-                             ' and with predict_proba not raised')
-
 
 def test_multilabel():
     """Check if error is raised for multilabel classification."""
@@ -216,6 +227,8 @@ def test_multilabel():
         return
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_gridsearch():
     """Check GridSearch support."""
@@ -234,6 +247,8 @@ def test_gridsearch():
     grid.fit(iris.data, iris.target)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_parallel_fit():
     """Check parallel backend of VotingClassifier on toy dataset."""
@@ -256,6 +271,8 @@ def test_parallel_fit():
     assert_array_almost_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_sample_weight():
     """Tests sample_weight parameter of VotingClassifier"""
@@ -300,6 +317,8 @@ def fit(self, X, y, *args, **sample_weight):
     eclf.fit(X, y, sample_weight=np.ones((len(y),)))
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_set_params():
     """set_params should be able to set estimators"""
@@ -335,6 +354,8 @@ def test_set_params():
                  eclf1.get_params()["lr"].get_params()['C'])
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_set_estimator_none():
     """VotingClassifier set_params should be able to set estimators as None"""
@@ -390,6 +411,8 @@ def test_set_estimator_none():
     assert_array_equal(eclf2.transform(X1), np.array([[0], [1]]))
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_estimator_weights_format():
     # Test estimator weights inputs as list and array
@@ -408,6 +431,8 @@ def test_estimator_weights_format():
     assert_array_almost_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_transform():
     """Check transform method of VotingClassifier on toy dataset."""
diff --git a/sklearn/ensemble/voting_classifier.py b/sklearn/ensemble/voting_classifier.py
index e585947f11..da08a163f3 100755
--- a/sklearn/ensemble/voting_classifier.py
+++ b/sklearn/ensemble/voting_classifier.py
@@ -59,9 +59,11 @@ class VotingClassifier(_BaseComposition, ClassifierMixin, TransformerMixin):
         predicted class labels (`hard` voting) or class probabilities
         before averaging (`soft` voting). Uses uniform weights if `None`.
 
-    n_jobs : int, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of jobs to run in parallel for ``fit``.
-        If -1, then the number of jobs is set to the number of cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     flatten_transform : bool, optional (default=None)
         Affects shape of transform output only when voting='soft'
@@ -90,7 +92,8 @@ class VotingClassifier(_BaseComposition, ClassifierMixin, TransformerMixin):
     >>> from sklearn.linear_model import LogisticRegression
     >>> from sklearn.naive_bayes import GaussianNB
     >>> from sklearn.ensemble import RandomForestClassifier, VotingClassifier
-    >>> clf1 = LogisticRegression(random_state=1)
+    >>> clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial',
+    ...                           random_state=1)
     >>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)
     >>> clf3 = GaussianNB()
     >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
diff --git a/sklearn/externals/_arff.py b/sklearn/externals/_arff.py
new file mode 100755
index 0000000000..eaec6083d0
--- /dev/null
+++ b/sklearn/externals/_arff.py
@@ -0,0 +1,1059 @@
+# -*- coding: utf-8 -*-
+# =============================================================================
+# Federal University of Rio Grande do Sul (UFRGS)
+# Connectionist Artificial Intelligence Laboratory (LIAC)
+# Renato de Pontes Pereira - rppereira@inf.ufrgs.br
+# =============================================================================
+# Copyright (c) 2011 Renato de Pontes Pereira, renato.ppontes at gmail dot com
+#
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
+#
+# The above copyright notice and this permission notice shall be included in
+# all copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+# SOFTWARE.
+# =============================================================================
+
+'''
+The liac-arff module implements functions to read and write ARFF files in
+Python. It was created in the Connectionist Artificial Intelligence Laboratory
+(LIAC), which takes place at the Federal University of Rio Grande do Sul 
+(UFRGS), in Brazil.
+
+ARFF (Attribute-Relation File Format) is an file format specially created for
+describe datasets which are commonly used for machine learning experiments and
+softwares. This file format was created to be used in Weka, the best 
+representative software for machine learning automated experiments.
+
+An ARFF file can be divided into two sections: header and data. The Header 
+describes the metadata of the dataset, including a general description of the 
+dataset, its name and its attributes. The source below is an example of a 
+header section in a XOR dataset::
+
+    % 
+    % XOR Dataset
+    % 
+    % Created by Renato Pereira
+    %            rppereira@inf.ufrgs.br
+    %            http://inf.ufrgs.br/~rppereira
+    % 
+    % 
+    @RELATION XOR
+
+    @ATTRIBUTE input1 REAL
+    @ATTRIBUTE input2 REAL
+    @ATTRIBUTE y REAL
+
+The Data section of an ARFF file describes the observations of the dataset, in 
+the case of XOR dataset::
+
+    @DATA
+    0.0,0.0,0.0
+    0.0,1.0,1.0
+    1.0,0.0,1.0
+    1.0,1.0,0.0
+    % 
+    % 
+    % 
+
+Notice that several lines are starting with an ``%`` symbol, denoting a 
+comment, thus, lines with ``%`` at the beginning will be ignored, except by the
+description part at the beginning of the file. The declarations ``@RELATION``, 
+``@ATTRIBUTE``, and ``@DATA`` are all case insensitive and obligatory.
+
+For more information and details about the ARFF file description, consult
+http://www.cs.waikato.ac.nz/~ml/weka/arff.html
+
+
+ARFF Files in Python
+~~~~~~~~~~~~~~~~~~~~
+
+This module uses built-ins python objects to represent a deserialized ARFF 
+file. A dictionary is used as the container of the data and metadata of ARFF,
+and have the following keys:
+
+- **description**: (OPTIONAL) a string with the description of the dataset.
+- **relation**: (OBLIGATORY) a string with the name of the dataset.
+- **attributes**: (OBLIGATORY) a list of attributes with the following 
+  template::
+
+    (attribute_name, attribute_type)
+
+  the attribute_name is a string, and attribute_type must be an string
+  or a list of strings.
+- **data**: (OBLIGATORY) a list of data instances. Each data instance must be 
+  a list with values, depending on the attributes.
+
+The above keys must follow the case which were described, i.e., the keys are 
+case sensitive. The attribute type ``attribute_type`` must be one of these 
+strings (they are not case sensitive): ``NUMERIC``, ``INTEGER``, ``REAL`` or 
+``STRING``. For nominal attributes, the ``atribute_type`` must be a list of 
+strings.
+
+In this format, the XOR dataset presented above can be represented as a python 
+object as::
+
+    xor_dataset = {
+        'description': 'XOR Dataset',
+        'relation': 'XOR',
+        'attributes': [
+            ('input1', 'REAL'),
+            ('input2', 'REAL'),
+            ('y', 'REAL'),
+        ],
+        'data': [
+            [0.0, 0.0, 0.0],
+            [0.0, 1.0, 1.0],
+            [1.0, 0.0, 1.0],
+            [1.0, 1.0, 0.0]
+        ]
+    }
+
+
+Features
+~~~~~~~~
+
+This module provides several features, including:
+
+- Read and write ARFF files using python built-in structures, such dictionaries
+  and lists;
+- Supports `scipy.sparse.coo <http://docs.scipy
+  .org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html#scipy.sparse.coo_matrix>`_
+  and lists of dictionaries as used by SVMLight
+- Supports the following attribute types: NUMERIC, REAL, INTEGER, STRING, and
+  NOMINAL;
+- Has an interface similar to other built-in modules such as ``json``, or 
+  ``zipfile``;
+- Supports read and write the descriptions of files;
+- Supports missing values and names with spaces;
+- Supports unicode values and names;
+- Fully compatible with Python 2.7+, Python 3.3+, pypy and pypy3;
+- Under `MIT License <http://opensource.org/licenses/MIT>`_
+
+'''
+__author__ = 'Renato de Pontes Pereira, Matthias Feurer, Joel Nothman'
+__author_email__ = ('renato.ppontes@gmail.com, '
+                    'feurerm@informatik.uni-freiburg.de, '
+                    'joel.nothman@gmail.com')
+__version__ = '2.3.1'
+
+import re
+import sys
+import csv
+
+# CONSTANTS ===================================================================
+_SIMPLE_TYPES = ['NUMERIC', 'REAL', 'INTEGER', 'STRING']
+
+_TK_DESCRIPTION = '%'
+_TK_COMMENT     = '%'
+_TK_RELATION    = '@RELATION'
+_TK_ATTRIBUTE   = '@ATTRIBUTE'
+_TK_DATA        = '@DATA'
+
+_RE_RELATION     = re.compile(r'^([^\{\}%,\s]*|\".*\"|\'.*\')$', re.UNICODE)
+_RE_ATTRIBUTE    = re.compile(r'^(\".*\"|\'.*\'|[^\{\}%,\s]*)\s+(.+)$', re.UNICODE)
+_RE_TYPE_NOMINAL = re.compile(r'^\{\s*((\".*\"|\'.*\'|\S*)\s*,\s*)*(\".*\"|\'.*\'|\S*)\s*\}$', re.UNICODE)
+_RE_QUOTE_CHARS = re.compile(r'["\'\\ \t%,]')
+_RE_ESCAPE_CHARS = re.compile(r'(?=["\'\\%])')  # don't need to capture anything
+_RE_SPARSE_LINE = re.compile(r'^\{.*\}$')
+_RE_NONTRIVIAL_DATA = re.compile('["\'{}\\s]')
+
+
+def _build_re_values():
+    quoted_re = r'''
+                    "      # open quote followed by zero or more of:
+                    (?:
+                        (?<!\\)    # no additional backslash
+                        (?:\\\\)*  # maybe escaped backslashes
+                        \\"        # escaped quote
+                    |
+                        \\[^"]     # escaping a non-quote
+                    |
+                        [^"\\]     # non-quote char
+                    )*
+                    "      # close quote
+                    '''
+    # a value is surrounded by " or by ' or contains no quotables
+    value_re = r'''(?:
+        %s|          # a value may be surrounded by "
+        %s|          # or by '
+        [^,\s"'{}]+  # or may contain no characters requiring quoting
+        )''' % (quoted_re,
+                quoted_re.replace('"', "'"))
+
+    # This captures (value, error) groups. Because empty values are allowed,
+    # we cannot just look for empty values to handle syntax errors.
+    # We presume the line has had ',' prepended...
+    dense = re.compile(r'''(?x)
+        ,                # may follow ','
+        \s*
+        ((?=,)|$|%(value_re)s)  # empty or value
+        |
+        (\S.*)           # error
+        ''' % {'value_re': value_re})
+
+    # This captures (key, value) groups and will have an empty key/value
+    # in case of syntax errors.
+    # It does not ensure that the line starts with '{' or ends with '}'.
+    sparse = re.compile(r'''(?x)
+        (?:^\s*\{|,)   # may follow ',', or '{' at line start
+        \s*
+        (\d+)          # attribute key
+        \s+
+        (%(value_re)s) # value
+        |
+        (?!}\s*$)      # not an error if it's }$
+        (?!^\s*{\s*}\s*$)  # not an error if it's ^{}$
+        \S.*           # error
+        ''' % {'value_re': value_re})
+    return dense, sparse
+
+
+_RE_DENSE_VALUES, _RE_SPARSE_KEY_VALUES = _build_re_values()
+
+
+def _unquote(v):
+    if v[:1] in ('"', "'"):
+        return re.sub(r'\\(.)', r'\1', v[1:-1])
+    elif v in ('?', ''):
+        return None
+    else:
+        return v
+
+
+def _parse_values(s):
+    '''(INTERNAL) Split a line into a list of values'''
+    if not _RE_NONTRIVIAL_DATA.search(s):
+        # Fast path for trivial cases (unfortunately we have to handle missing
+        # values because of the empty string case :(.)
+        return [None if s in ('?', '') else s
+                for s in next(csv.reader([s]))]
+
+    # _RE_DENSE_VALUES tokenizes despite quoting, whitespace, etc.
+    values, errors = zip(*_RE_DENSE_VALUES.findall(',' + s))
+    if not any(errors):
+        return [_unquote(v) for v in values]
+    if _RE_SPARSE_LINE.match(s):
+        try:
+            return {int(k): _unquote(v)
+                    for k, v in _RE_SPARSE_KEY_VALUES.findall(s)}
+        except ValueError as exc:
+            # an ARFF syntax error in sparse data
+            for match in _RE_SPARSE_KEY_VALUES.finditer(s):
+                if not match.group(1):
+                    raise BadLayout('Error parsing %r' % match.group())
+            raise BadLayout('Unknown parsing error')
+    else:
+        # an ARFF syntax error
+        for match in _RE_DENSE_VALUES.finditer(s):
+            if match.group(2):
+                raise BadLayout('Error parsing %r' % match.group())
+        raise BadLayout('Unknown parsing error')
+
+
+DENSE = 0   # Constant value representing a dense matrix
+COO = 1     # Constant value representing a sparse matrix in coordinate format
+LOD = 2     # Constant value representing a sparse matrix in list of
+            # dictionaries format
+_SUPPORTED_DATA_STRUCTURES = [DENSE, COO, LOD]
+
+# =============================================================================
+
+# COMPATIBILITY WITH PYTHON 3 =================================================
+PY3 = sys.version_info[0] == 3
+if PY3:
+    unicode = str
+    basestring = str
+    xrange = range
+# COMPABILITY WITH PYTHON 2 ===================================================
+# =============================================================================
+PY2 = sys.version_info[0] == 2
+if PY2:
+    from itertools import izip as zip
+
+# EXCEPTIONS ==================================================================
+class ArffException(Exception):
+    message = None
+
+    def __init__(self):
+        self.line = -1
+
+    def __str__(self):
+        return self.message%self.line
+
+class BadRelationFormat(ArffException):
+    '''Error raised when the relation declaration is in an invalid format.'''
+    message = 'Bad @RELATION format, at line %d.'
+
+class BadAttributeFormat(ArffException):
+    '''Error raised when some attribute declaration is in an invalid format.'''
+    message = 'Bad @ATTRIBUTE format, at line %d.'
+
+class BadDataFormat(ArffException):
+    '''Error raised when some data instance is in an invalid format.'''
+    def __init__(self, value):
+        super(BadDataFormat, self).__init__()
+        self.message = (
+            'Bad @DATA instance format in line %d: ' +
+            ('%s' % value)
+        )
+
+class BadAttributeType(ArffException):
+    '''Error raised when some invalid type is provided into the attribute 
+    declaration.'''
+    message = 'Bad @ATTRIBUTE type, at line %d.'
+
+class BadAttributeName(ArffException):
+    '''Error raised when an attribute name is provided twice the attribute
+    declaration.'''
+
+    def __init__(self, value, value2):
+        super(BadAttributeName, self).__init__()
+        self.message = (
+            ('Bad @ATTRIBUTE name %s at line' % value) +
+            ' %d, this name is already in use in line' +
+            (' %d.' % value2)
+        )
+
+class BadNominalValue(ArffException):
+    '''Error raised when a value in used in some data instance but is not 
+    declared into it respective attribute declaration.'''
+
+    def __init__(self, value):
+        super(BadNominalValue, self).__init__()
+        self.message = (
+            ('Data value %s not found in nominal declaration, ' % value)
+            + 'at line %d.'
+        )
+
+class BadNominalFormatting(ArffException):
+    '''Error raised when a nominal value with space is not properly quoted.'''
+    def __init__(self, value):
+        super(BadNominalFormatting, self).__init__()
+        self.message = (
+            ('Nominal data value "%s" not properly quoted in line ' % value) +
+            '%d.'
+        )
+
+class BadNumericalValue(ArffException):
+    '''Error raised when and invalid numerical value is used in some data 
+    instance.'''
+    message = 'Invalid numerical value, at line %d.'
+
+class BadStringValue(ArffException):
+    '''Error raise when a string contains space but is not quoted.'''
+    message = 'Invalid string value at line %d.'
+
+class BadLayout(ArffException):
+    '''Error raised when the layout of the ARFF file has something wrong.'''
+    message = 'Invalid layout of the ARFF file, at line %d.'
+
+    def __init__(self, msg=''):
+        super(BadLayout, self).__init__()
+        if msg:
+            self.message = BadLayout.message + ' ' + msg.replace('%', '%%')
+
+class BadObject(ArffException):
+    '''Error raised when the object representing the ARFF file has something 
+    wrong.'''
+
+    def __str__(self):
+        return 'Invalid object.'
+
+class BadObject(ArffException):
+    '''Error raised when the object representing the ARFF file has something 
+    wrong.'''
+    def __init__(self, msg=''):
+        self.msg = msg
+
+    def __str__(self):
+        return '%s'%self.msg
+# =============================================================================
+
+# INTERNAL ====================================================================
+def encode_string(s):
+    if _RE_QUOTE_CHARS.search(s):
+        return u"'%s'" % _RE_ESCAPE_CHARS.sub(r'\\', s)
+    return s
+
+
+class EncodedNominalConversor(object):
+    def __init__(self, values):
+        self.values = {v: i for i, v in enumerate(values)}
+        self.values[0] = 0
+
+    def __call__(self, value):
+        try:
+            return self.values[value]
+        except KeyError:
+            raise BadNominalValue(value)
+
+
+class NominalConversor(object):
+    def __init__(self, values):
+        self.values = set(values)
+        self.zero_value = values[0]
+
+    def __call__(self, value):
+        if value not in self.values:
+            if value == 0:
+                # Sparse decode
+                # See issue #52: nominals should take their first value when
+                # unspecified in a sparse matrix. Naturally, this is consistent
+                # with EncodedNominalConversor.
+                return self.zero_value
+            raise BadNominalValue(value)
+        return unicode(value)
+
+
+class Data(object):
+    '''Internal helper class to allow for different matrix types without
+    making the code a huge collection of if statements.'''
+    def __init__(self):
+        self.data = []
+
+    def decode_data(self, s, conversors):
+        values = _parse_values(s)
+
+        if isinstance(values, dict):
+            if max(values) >= len(conversors):
+                raise BadDataFormat(s)
+            # XXX: int 0 is used for implicit values, not '0'
+            values = [values[i] if i in values else 0 for i in
+                      xrange(len(conversors))]
+        else:
+            if len(values) != len(conversors):
+                raise BadDataFormat(s)
+
+        self.data.append(self._decode_values(values, conversors))
+
+    @staticmethod
+    def _decode_values(values, conversors):
+        try:
+            values = [None if value is None else conversor(value)
+                      for conversor, value
+                      in zip(conversors, values)]
+        except ValueError as exc:
+            if 'float: ' in str(exc):
+                raise BadNumericalValue()
+        return values
+
+    def _tuplify_sparse_data(self, x):
+        if len(x) != 2:
+            raise BadDataFormat(x)
+        return (int(x[0].strip('"').strip("'")), x[1])
+
+    def encode_data(self, data, attributes):
+        '''(INTERNAL) Encodes a line of data.
+
+        Data instances follow the csv format, i.e, attribute values are
+        delimited by commas. After converted from csv.
+
+        :param data: a list of values.
+        :param attributes: a list of attributes. Used to check if data is valid.
+        :return: a string with the encoded data line.
+        '''
+        current_row = 0
+
+        for inst in data:
+            if len(inst) != len(attributes):
+                raise BadObject(
+                    'Instance %d has %d attributes, expected %d' %
+                     (current_row, len(inst), len(attributes))
+                )
+
+            new_data = []
+            for value in inst:
+                if value is None or value == u'' or value != value:
+                    s = '?'
+                else:
+                    s = encode_string(unicode(value))
+                new_data.append(s)
+
+            current_row += 1
+            yield u','.join(new_data)
+
+class COOData(Data):
+    def __init__(self):
+        self.data = ([], [], [])
+        self._current_num_data_points = 0
+
+    def decode_data(self, s, conversors):
+        values = _parse_values(s)
+
+        if not isinstance(values, dict):
+            raise BadLayout()
+        if not values:
+            self._current_num_data_points += 1
+            return
+        col, values = zip(*sorted(values.items()))
+        try:
+            values = [value if value is None else conversors[key](value)
+                      for key, value in zip(col, values)]
+        except ValueError as exc:
+            if 'float: ' in str(exc):
+                raise BadNumericalValue()
+            raise
+        except IndexError:
+            # conversor out of range
+            raise BadDataFormat(s)
+        self.data[0].extend(values)
+        self.data[1].extend([self._current_num_data_points] * len(values))
+        self.data[2].extend(col)
+
+        self._current_num_data_points += 1
+
+    def encode_data(self, data, attributes):
+        num_attributes = len(attributes)
+        new_data = []
+        current_row = 0
+
+        row = data.row
+        col = data.col
+        data = data.data
+
+        # Check if the rows are sorted
+        if not all(row[i] <= row[i + 1] for i in xrange(len(row) - 1)):
+            raise ValueError("liac-arff can only output COO matrices with "
+                             "sorted rows.")
+
+        for v, col, row in zip(data, col, row):
+            if row > current_row:
+                # Add empty rows if necessary
+                while current_row < row:
+                    yield " ".join([u"{", u','.join(new_data), u"}"])
+                    new_data = []
+                    current_row += 1
+
+            if col >= num_attributes:
+                raise BadObject(
+                    'Instance %d has at least %d attributes, expected %d' %
+                    (current_row, col + 1, num_attributes)
+                )
+
+            if v is None or v == u'' or v != v:
+                s = '?'
+            else:
+                s = encode_string(unicode(v))
+            new_data.append("%d %s" % (col, s))
+
+        yield " ".join([u"{", u','.join(new_data), u"}"])
+
+class LODData(Data):
+    def __init__(self):
+        self.data = []
+
+    def decode_data(self, s, conversors):
+        values = _parse_values(s)
+        n_conversors = len(conversors)
+
+        if not isinstance(values, dict):
+            raise BadLayout()
+        try:
+            self.data.append({key: None if value is None else conversors[key](value)
+                              for key, value in values.items()})
+        except ValueError as exc:
+            if 'float: ' in str(exc):
+                raise BadNumericalValue()
+            raise
+        except IndexError:
+            # conversor out of range
+            raise BadDataFormat(s)
+
+    def encode_data(self, data, attributes):
+        current_row = 0
+
+        num_attributes = len(attributes)
+        for row in data:
+            new_data = []
+
+            if len(row) > 0 and max(row) >= num_attributes:
+                raise BadObject(
+                    'Instance %d has %d attributes, expected %d' %
+                    (current_row, max(row) + 1, num_attributes)
+                )
+
+            for col in sorted(row):
+                v = row[col]
+                if v is None or v == u'' or v != v:
+                    s = '?'
+                else:
+                    s = encode_string(unicode(v))
+                new_data.append("%d %s" % (col, s))
+
+            current_row += 1
+            yield " ".join([u"{", u','.join(new_data), u"}"])
+
+def _get_data_object_for_decoding(matrix_type):
+    if matrix_type == DENSE:
+        return Data()
+    elif matrix_type == COO:
+        return COOData()
+    elif matrix_type == LOD:
+        return LODData()
+    else:
+        raise ValueError("Matrix type %s not supported." % str(matrix_type))
+
+def _get_data_object_for_encoding(matrix):
+    # Probably a scipy.sparse
+    if hasattr(matrix, 'format'):
+        if matrix.format == 'coo':
+            return COOData()
+        else:
+            raise ValueError('Cannot guess matrix format!')
+    elif isinstance(matrix[0], dict):
+        return LODData()
+    else:
+        return Data()
+
+# =============================================================================
+
+# ADVANCED INTERFACE ==========================================================
+class ArffDecoder(object):
+    '''An ARFF decoder.'''
+
+    def __init__(self):
+        '''Constructor.'''
+        self._conversors = []
+        self._current_line = 0
+
+    def _decode_comment(self, s):
+        '''(INTERNAL) Decodes a comment line.
+
+        Comments are single line strings starting, obligatorily, with the ``%``
+        character, and can have any symbol, including whitespaces or special
+        characters.
+
+        This method must receive a normalized string, i.e., a string without
+        padding, including the "\r\n" characters. 
+
+        :param s: a normalized string.
+        :return: a string with the decoded comment.
+        '''
+        res = re.sub(r'^\%( )?', '', s)
+        return res
+
+    def _decode_relation(self, s):
+        '''(INTERNAL) Decodes a relation line.
+
+        The relation declaration is a line with the format ``@RELATION 
+        <relation-name>``, where ``relation-name`` is a string. The string must
+        start with alphabetic character and must be quoted if the name includes
+        spaces, otherwise this method will raise a `BadRelationFormat` exception.
+
+        This method must receive a normalized string, i.e., a string without
+        padding, including the "\r\n" characters. 
+
+        :param s: a normalized string.
+        :return: a string with the decoded relation name.
+        '''
+        _, v = s.split(' ', 1)
+        v = v.strip()
+
+        if not _RE_RELATION.match(v):
+            raise BadRelationFormat()
+
+        res = unicode(v.strip('"\''))
+        return res
+
+    def _decode_attribute(self, s):
+        '''(INTERNAL) Decodes an attribute line.
+
+        The attribute is the most complex declaration in an arff file. All 
+        attributes must follow the template::
+
+             @attribute <attribute-name> <datatype>
+
+        where ``attribute-name`` is a string, quoted if the name contains any 
+        whitespace, and ``datatype`` can be:
+
+        - Numerical attributes as ``NUMERIC``, ``INTEGER`` or ``REAL``.
+        - Strings as ``STRING``.
+        - Dates (NOT IMPLEMENTED).
+        - Nominal attributes with format:
+
+            {<nominal-name1>, <nominal-name2>, <nominal-name3>, ...} 
+
+        The nominal names follow the rules for the attribute names, i.e., they
+        must be quoted if the name contains whitespaces.
+
+        This method must receive a normalized string, i.e., a string without
+        padding, including the "\r\n" characters. 
+
+        :param s: a normalized string.
+        :return: a tuple (ATTRIBUTE_NAME, TYPE_OR_VALUES).
+        '''
+        _, v = s.split(' ', 1)
+        v = v.strip()
+
+        # Verify the general structure of declaration
+        m = _RE_ATTRIBUTE.match(v)
+        if not m:
+            raise BadAttributeFormat()
+
+        # Extracts the raw name and type
+        name, type_ = m.groups()
+
+        # Extracts the final name
+        name = unicode(name.strip('"\''))
+
+        # Extracts the final type
+        if _RE_TYPE_NOMINAL.match(type_):
+            try:
+                type_ = _parse_values(type_.strip('{} '))
+            except Exception:
+                raise BadAttributeType()
+            if isinstance(type_, dict):
+                raise BadAttributeType()
+
+        else:
+            # If not nominal, verify the type name
+            type_ = unicode(type_).upper()
+            if type_ not in ['NUMERIC', 'REAL', 'INTEGER', 'STRING']:
+                raise BadAttributeType()
+
+        return (name, type_)
+
+    def _decode(self, s, encode_nominal=False, matrix_type=DENSE):
+        '''Do the job the ``encode``.'''
+
+        # Make sure this method is idempotent
+        self._current_line = 0
+
+        # If string, convert to a list of lines
+        if isinstance(s, basestring):
+            s = s.strip('\r\n ').replace('\r\n', '\n').split('\n')
+
+        # Create the return object
+        obj = {
+            u'description': u'',
+            u'relation': u'',
+            u'attributes': [],
+            u'data': []
+        }
+        attribute_names = {}
+
+        # Create the data helper object
+        data = _get_data_object_for_decoding(matrix_type)
+
+        # Read all lines
+        STATE = _TK_DESCRIPTION
+        for row in s:
+            self._current_line += 1
+            # Ignore empty lines
+            row = row.strip(' \r\n')
+            if not row: continue
+
+            u_row = row.upper()
+
+            # DESCRIPTION -----------------------------------------------------
+            if u_row.startswith(_TK_DESCRIPTION) and STATE == _TK_DESCRIPTION:
+                obj['description'] += self._decode_comment(row) + '\n'
+            # -----------------------------------------------------------------
+
+            # RELATION --------------------------------------------------------
+            elif u_row.startswith(_TK_RELATION):
+                if STATE != _TK_DESCRIPTION:
+                    raise BadLayout()
+
+                STATE = _TK_RELATION
+                obj['relation'] = self._decode_relation(row)
+            # -----------------------------------------------------------------
+
+            # ATTRIBUTE -------------------------------------------------------
+            elif u_row.startswith(_TK_ATTRIBUTE):
+                if STATE != _TK_RELATION and STATE != _TK_ATTRIBUTE:
+                    raise BadLayout()
+
+                STATE = _TK_ATTRIBUTE
+
+                attr = self._decode_attribute(row)
+                if attr[0] in attribute_names:
+                    raise BadAttributeName(attr[0], attribute_names[attr[0]])
+                else:
+                    attribute_names[attr[0]] = self._current_line
+                obj['attributes'].append(attr)
+
+                if isinstance(attr[1], (list, tuple)):
+                    if encode_nominal:
+                        conversor = EncodedNominalConversor(attr[1])
+                    else:
+                        conversor = NominalConversor(attr[1])
+                else:
+                    CONVERSOR_MAP = {'STRING': unicode,
+                                     'INTEGER': lambda x: int(float(x)),
+                                     'NUMERIC': float,
+                                     'REAL': float}
+                    conversor = CONVERSOR_MAP[attr[1]]
+
+                self._conversors.append(conversor)
+            # -----------------------------------------------------------------
+
+            # DATA ------------------------------------------------------------
+            elif u_row.startswith(_TK_DATA):
+                if STATE != _TK_ATTRIBUTE:
+                    raise BadLayout()
+
+                STATE = _TK_DATA
+            # -----------------------------------------------------------------
+
+            # COMMENT ---------------------------------------------------------
+            elif u_row.startswith(_TK_COMMENT):
+                pass
+            # -----------------------------------------------------------------
+
+            # DATA INSTANCES --------------------------------------------------
+            elif STATE == _TK_DATA:
+                data.decode_data(row, self._conversors)
+            # -----------------------------------------------------------------
+
+            # UNKNOWN INFORMATION ---------------------------------------------
+            else:
+                raise BadLayout()
+            # -----------------------------------------------------------------
+
+        # Alter the data object
+        obj['data'] = data.data
+        if obj['description'].endswith('\n'):
+            obj['description'] = obj['description'][:-1]
+
+        return obj
+
+    def decode(self, s, encode_nominal=False, return_type=DENSE):
+        '''Returns the Python representation of a given ARFF file.
+
+        When a file object is passed as an argument, this method reads lines
+        iteratively, avoiding to load unnecessary information to the memory.
+
+        :param s: a string or file object with the ARFF file.
+        :param encode_nominal: boolean, if True perform a label encoding
+            while reading the .arff file.
+        :param return_type: determines the data structure used to store the
+            dataset. Can be one of `arff.DENSE`, `arff.COO` and `arff.LOD`.
+            Consult the section on `working with sparse data`_
+        '''
+        try:
+            return self._decode(s, encode_nominal=encode_nominal,
+                                matrix_type=return_type)
+        except ArffException as e:
+            e.line = self._current_line
+            raise e
+
+
+class ArffEncoder(object):
+    '''An ARFF encoder.'''
+
+    def _encode_comment(self, s=''):
+        '''(INTERNAL) Encodes a comment line.
+
+        Comments are single line strings starting, obligatorily, with the ``%``
+        character, and can have any symbol, including whitespaces or special
+        characters.
+
+        If ``s`` is None, this method will simply return an empty comment.
+
+        :param s: (OPTIONAL) string.
+        :return: a string with the encoded comment line.
+        '''
+        if s:
+            return u'%s %s'%(_TK_COMMENT, s)
+        else:
+            return u'%s' % _TK_COMMENT
+
+    def _encode_relation(self, name):
+        '''(INTERNAL) Decodes a relation line.
+
+        The relation declaration is a line with the format ``@RELATION 
+        <relation-name>``, where ``relation-name`` is a string. 
+
+        :param name: a string.
+        :return: a string with the encoded relation declaration.
+        '''
+        for char in ' %{},':
+            if char in name:
+                name = '"%s"'%name
+                break
+
+        return u'%s %s'%(_TK_RELATION, name)
+
+    def _encode_attribute(self, name, type_):
+        '''(INTERNAL) Encodes an attribute line.
+
+        The attribute follow the template::
+
+             @attribute <attribute-name> <datatype>
+
+        where ``attribute-name`` is a string, and ``datatype`` can be:
+
+        - Numerical attributes as ``NUMERIC``, ``INTEGER`` or ``REAL``.
+        - Strings as ``STRING``.
+        - Dates (NOT IMPLEMENTED).
+        - Nominal attributes with format:
+
+            {<nominal-name1>, <nominal-name2>, <nominal-name3>, ...} 
+
+        This method must receive a the name of the attribute and its type, if
+        the attribute type is nominal, ``type`` must be a list of values.
+
+        :param name: a string.
+        :param type_: a string or a list of string.
+        :return: a string with the encoded attribute declaration.
+        '''
+        for char in ' %{},':
+            if char in name:
+                name = '"%s"'%name
+                break
+
+        if isinstance(type_, (tuple, list)):
+            type_tmp = []
+            for i in range(len(type_)):
+                type_tmp.append(u'%s' % encode_string(type_[i]))
+            type_ = u'{%s}'%(u', '.join(type_tmp))
+
+        return u'%s %s %s'%(_TK_ATTRIBUTE, name, type_)
+
+    def encode(self, obj):
+        '''Encodes a given object to an ARFF file.
+
+        :param obj: the object containing the ARFF information.
+        :return: the ARFF file as an unicode string.
+        '''
+        data = [row for row in self.iter_encode(obj)]
+
+        return u'\n'.join(data)
+
+    def iter_encode(self, obj):
+        '''The iterative version of `arff.ArffEncoder.encode`.
+
+        This encodes iteratively a given object and return, one-by-one, the 
+        lines of the ARFF file.
+
+        :param obj: the object containing the ARFF information.
+        :return: (yields) the ARFF file as unicode strings.
+        '''
+        # DESCRIPTION
+        if obj.get('description', None):
+            for row in obj['description'].split('\n'):
+                yield self._encode_comment(row)
+
+        # RELATION
+        if not obj.get('relation'):
+            raise BadObject('Relation name not found or with invalid value.')
+
+        yield self._encode_relation(obj['relation'])
+        yield u''
+
+        # ATTRIBUTES
+        if not obj.get('attributes'):
+            raise BadObject('Attributes not found.')
+
+        attribute_names = set()
+        for attr in obj['attributes']:
+            # Verify for bad object format
+            if not isinstance(attr, (tuple, list)) or \
+               len(attr) != 2 or \
+               not isinstance(attr[0], basestring):
+                raise BadObject('Invalid attribute declaration "%s"'%str(attr))
+
+            if isinstance(attr[1], basestring):
+                # Verify for invalid types
+                if attr[1] not in _SIMPLE_TYPES:
+                    raise BadObject('Invalid attribute type "%s"'%str(attr))
+
+            # Verify for bad object format
+            elif not isinstance(attr[1], (tuple, list)):
+                raise BadObject('Invalid attribute type "%s"'%str(attr))
+
+            # Verify attribute name is not used twice
+            if attr[0] in attribute_names:
+                raise BadObject('Trying to use attribute name "%s" for the '
+                                'second time.' % str(attr[0]))
+            else:
+                attribute_names.add(attr[0])
+
+            yield self._encode_attribute(attr[0], attr[1])
+        yield u''
+        attributes = obj['attributes']
+
+        # DATA
+        yield _TK_DATA
+        if 'data' in obj:
+            data = _get_data_object_for_encoding(obj.get('data'))
+            for line in data.encode_data(obj.get('data'), attributes):
+                yield line
+
+        yield u''
+
+# =============================================================================
+
+# BASIC INTERFACE =============================================================
+def load(fp, encode_nominal=False, return_type=DENSE):
+    '''Load a file-like object containing the ARFF document and convert it into
+    a Python object. 
+
+    :param fp: a file-like object.
+    :param encode_nominal: boolean, if True perform a label encoding
+        while reading the .arff file.
+    :param return_type: determines the data structure used to store the
+        dataset. Can be one of `arff.DENSE`, `arff.COO` and `arff.LOD`.
+        Consult the section on `working with sparse data`_
+    :return: a dictionary.
+     '''
+    decoder = ArffDecoder()
+    return decoder.decode(fp, encode_nominal=encode_nominal,
+                          return_type=return_type)
+
+def loads(s, encode_nominal=False, return_type=DENSE):
+    '''Convert a string instance containing the ARFF document into a Python
+    object.
+
+    :param s: a string object.
+    :param encode_nominal: boolean, if True perform a label encoding
+        while reading the .arff file.
+    :param return_type: determines the data structure used to store the
+        dataset. Can be one of `arff.DENSE`, `arff.COO` and `arff.LOD`.
+        Consult the section on `working with sparse data`_
+    :return: a dictionary.
+    '''
+    decoder = ArffDecoder()
+    return decoder.decode(s, encode_nominal=encode_nominal,
+                          return_type=return_type)
+
+def dump(obj, fp):
+    '''Serialize an object representing the ARFF document to a given file-like 
+    object.
+
+    :param obj: a dictionary.
+    :param fp: a file-like object.
+    '''
+    encoder = ArffEncoder()
+    generator = encoder.iter_encode(obj)
+
+    last_row = next(generator)
+    for row in generator:
+        fp.write(last_row + u'\n')
+        last_row = row
+    fp.write(last_row)
+
+    return fp
+
+def dumps(obj):
+    '''Serialize an object representing the ARFF document, returning a string.
+
+    :param obj: a dictionary.
+    :return: a string with the ARFF document.
+    '''
+    encoder = ArffEncoder()
+    return encoder.encode(obj)
+# =============================================================================
diff --git a/sklearn/externals/copy_joblib.sh b/sklearn/externals/copy_joblib.sh
index 8784132977..f2c4ab3ed3 100755
--- a/sklearn/externals/copy_joblib.sh
+++ b/sklearn/externals/copy_joblib.sh
@@ -11,7 +11,7 @@ else
         JOBLIB=$1
 fi
 
-pip install $JOBLIB --target $INSTALL_FOLDER
+pip install --no-cache $JOBLIB --target $INSTALL_FOLDER
 cp -r $INSTALL_FOLDER/joblib joblib
 rm -rf $INSTALL_FOLDER
 
diff --git a/sklearn/externals/joblib/__init__.py b/sklearn/externals/joblib/__init__.py
index 1b5938350e..0d008b5605 100755
--- a/sklearn/externals/joblib/__init__.py
+++ b/sklearn/externals/joblib/__init__.py
@@ -12,7 +12,7 @@
 
 
     ==================== ===============================================
-    **Documentation:**       http://pythonhosted.org/joblib
+    **Documentation:**       https://joblib.readthedocs.io
 
     **Download:**            http://pypi.python.org/pypi/joblib#downloads
 
@@ -106,7 +106,7 @@
 # Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
 # 'X.Y.dev0' is the canonical version of 'X.Y.dev'
 #
-__version__ = '0.12.2'
+__version__ = '0.12.5'
 
 
 from .memory import Memory, MemorizedResult, register_store_backend
diff --git a/sklearn/externals/joblib/_multiprocessing_helpers.py b/sklearn/externals/joblib/_multiprocessing_helpers.py
index 4111a26f22..be642b869f 100755
--- a/sklearn/externals/joblib/_multiprocessing_helpers.py
+++ b/sklearn/externals/joblib/_multiprocessing_helpers.py
@@ -4,6 +4,7 @@
 circular dependencies (for instance for the assert_spawning name).
 """
 import os
+import sys
 import warnings
 
 
@@ -21,7 +22,16 @@
 #            issue a warning if not
 if mp is not None:
     try:
-        _sem = mp.Semaphore()
+        # Use the spawn context
+        if sys.version_info < (3, 3):
+            Semaphore = mp.Semaphore
+        else:
+            # Using mp.Semaphore has a border effect and set the default
+            # backend for multiprocessing. To avoid that, we use the 'spawn'
+            # context which is available on all supported platforms.
+            ctx = mp.get_context('spawn')
+            Semaphore = ctx.Semaphore
+        _sem = Semaphore()
         del _sem  # cleanup
     except (ImportError, OSError) as e:
         mp = None
diff --git a/sklearn/externals/joblib/_parallel_backends.py b/sklearn/externals/joblib/_parallel_backends.py
index 85312abec6..c78750667e 100755
--- a/sklearn/externals/joblib/_parallel_backends.py
+++ b/sklearn/externals/joblib/_parallel_backends.py
@@ -130,7 +130,6 @@ def get_nested_backend(self):
         else:
             return ThreadingBackend(nesting_level=nesting_level)
 
-
     @contextlib.contextmanager
     def retrieval_context(self):
         """Context manager to manage an execution context.
@@ -348,7 +347,8 @@ def configure(self, n_jobs=1, parallel=None, **backend_args):
         n_jobs = self.effective_n_jobs(n_jobs)
         if n_jobs == 1:
             # Avoid unnecessary overhead and use sequential backend instead.
-            raise FallbackToBackend(SequentialBackend())
+            raise FallbackToBackend(
+                SequentialBackend(nesting_level=self.nesting_level))
         self.parallel = parallel
         self._n_jobs = n_jobs
         return n_jobs
@@ -421,7 +421,8 @@ def configure(self, n_jobs=1, parallel=None, prefer=None, require=None,
         """Build a process or thread pool and return the number of workers"""
         n_jobs = self.effective_n_jobs(n_jobs)
         if n_jobs == 1:
-            raise FallbackToBackend(SequentialBackend())
+            raise FallbackToBackend(
+                SequentialBackend(nesting_level=self.nesting_level))
 
         already_forked = int(os.environ.get(self.JOBLIB_SPAWNED_PROCESS, 0))
         if already_forked:
@@ -462,7 +463,8 @@ def configure(self, n_jobs=1, parallel=None, prefer=None, require=None,
         """Build a process executor and return the number of workers"""
         n_jobs = self.effective_n_jobs(n_jobs)
         if n_jobs == 1:
-            raise FallbackToBackend(SequentialBackend())
+            raise FallbackToBackend(
+                SequentialBackend(nesting_level=self.nesting_level))
 
         self._workers = get_memmapping_executor(
             n_jobs, timeout=idle_worker_timeout,
diff --git a/sklearn/externals/joblib/_store_backends.py b/sklearn/externals/joblib/_store_backends.py
index 027fb9f9f7..9196f0a774 100755
--- a/sklearn/externals/joblib/_store_backends.py
+++ b/sklearn/externals/joblib/_store_backends.py
@@ -35,6 +35,8 @@ class StoreBackendBase(with_metaclass(ABCMeta)):
     """Helper Abstract Base Class which defines all methods that
        a StorageBackend must implement."""
 
+    location = None
+
     @abstractmethod
     def _open_item(self, f, mode):
         """Opens an item on the store and return a file-like object.
@@ -327,7 +329,8 @@ def _concurrency_safe_write(self, to_write, filename, write_func):
 
     def __repr__(self):
         """Printable representation of the store location."""
-        return self.location
+        return '{class_name}(location="{location}")'.format(
+            class_name=self.__class__.__name__, location=self.location)
 
 
 class FileSystemStoreBackend(StoreBackendBase, StoreBackendMixin):
@@ -384,11 +387,13 @@ def get_items(self):
 
         return items
 
-    def configure(self, location, verbose=1, backend_options={}):
+    def configure(self, location, verbose=1, backend_options=None):
         """Configure the store backend.
 
         For this backend, valid store options are 'compress' and 'mmap_mode'
         """
+        if backend_options is None:
+            backend_options = {}
 
         # setup location directory
         self.location = location
@@ -396,17 +401,15 @@ def configure(self, location, verbose=1, backend_options={}):
             mkdirp(self.location)
 
         # item can be stored compressed for faster I/O
-        self.compress = backend_options['compress']
+        self.compress = backend_options.get('compress', False)
 
         # FileSystemStoreBackend can be used with mmap_mode options under
         # certain conditions.
-        mmap_mode = None
-        if 'mmap_mode' in backend_options:
-            mmap_mode = backend_options['mmap_mode']
-            if self.compress and mmap_mode is not None:
-                warnings.warn('Compressed items cannot be memmapped in a '
-                              'filesystem store. Option will be ignored.',
-                              stacklevel=2)
+        mmap_mode = backend_options.get('mmap_mode')
+        if self.compress and mmap_mode is not None:
+            warnings.warn('Compressed items cannot be memmapped in a '
+                          'filesystem store. Option will be ignored.',
+                          stacklevel=2)
 
         self.mmap_mode = mmap_mode
         self.verbose = verbose
diff --git a/sklearn/externals/joblib/externals/cloudpickle/__init__.py b/sklearn/externals/joblib/externals/cloudpickle/__init__.py
index c8c8fa208a..8004dcde0b 100755
--- a/sklearn/externals/joblib/externals/cloudpickle/__init__.py
+++ b/sklearn/externals/joblib/externals/cloudpickle/__init__.py
@@ -2,4 +2,4 @@
 
 from .cloudpickle import *
 
-__version__ = '0.5.2'
+__version__ = '0.5.6'
diff --git a/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py b/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py
index e5aab0591f..842723539d 100755
--- a/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py
+++ b/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py
@@ -163,7 +163,7 @@ def cell_set(cell, value):
     )(value)
 
 
-#relevant opcodes
+# relevant opcodes
 STORE_GLOBAL = opcode.opmap['STORE_GLOBAL']
 DELETE_GLOBAL = opcode.opmap['DELETE_GLOBAL']
 LOAD_GLOBAL = opcode.opmap['LOAD_GLOBAL']
@@ -173,7 +173,7 @@ def cell_set(cell, value):
 
 
 def islambda(func):
-    return getattr(func,'__name__') == '<lambda>'
+    return getattr(func, '__name__') == '<lambda>'
 
 
 _BUILTIN_TYPE_NAMES = {}
@@ -270,24 +270,19 @@ def dump(self, obj):
             if 'recursion' in e.args[0]:
                 msg = """Could not pickle object as excessively deep recursion required."""
                 raise pickle.PicklingError(msg)
+            else:
+                raise
 
     def save_memoryview(self, obj):
         self.save(obj.tobytes())
+
     dispatch[memoryview] = save_memoryview
 
     if not PY3:
         def save_buffer(self, obj):
             self.save(str(obj))
-        dispatch[buffer] = save_buffer
 
-    def save_unsupported(self, obj):
-        raise pickle.PicklingError("Cannot pickle objects of type %s" % type(obj))
-    dispatch[types.GeneratorType] = save_unsupported
-
-    # itertools objects do not pickle!
-    for v in itertools.__dict__.values():
-        if type(v) is type:
-            dispatch[v] = save_unsupported
+        dispatch[buffer] = save_buffer  # noqa: F821 'buffer' was removed in Python 3
 
     def save_module(self, obj):
         """
@@ -309,6 +304,7 @@ def save_module(self, obj):
             self.save_reduce(dynamic_subimport, (obj.__name__, vars(obj)), obj=obj)
         else:
             self.save_reduce(subimport, (obj.__name__,), obj=obj)
+
     dispatch[types.ModuleType] = save_module
 
     def save_codeobject(self, obj):
@@ -329,6 +325,7 @@ def save_codeobject(self, obj):
                 obj.co_firstlineno, obj.co_lnotab, obj.co_freevars, obj.co_cellvars
             )
         self.save_reduce(types.CodeType, args, obj=obj)
+
     dispatch[types.CodeType] = save_codeobject
 
     def save_function(self, obj, name=None):
@@ -337,7 +334,13 @@ def save_function(self, obj, name=None):
         Determines what kind of function obj is (e.g. lambda, defined at
         interactive prompt, etc) and handles the pickling appropriately.
         """
-        if obj in _BUILTIN_TYPE_CONSTRUCTORS:
+        try:
+            should_special_case = obj in _BUILTIN_TYPE_CONSTRUCTORS
+        except TypeError:
+            # Methods of builtin types aren't hashable in python 2.
+            should_special_case = False
+
+        if should_special_case:
             # We keep a special-cased cache of built-in type constructors at
             # global scope, because these functions are structured very
             # differently in different python versions and implementations (for
@@ -420,6 +423,7 @@ def save_function(self, obj, name=None):
         else:
             write(pickle.GLOBAL + modname + '\n' + name + '\n')
             self.memoize(obj)
+
     dispatch[types.FunctionType] = save_function
 
     def _save_subimports(self, code, top_level_dependencies):
@@ -427,19 +431,22 @@ def _save_subimports(self, code, top_level_dependencies):
         Ensure de-pickler imports any package child-modules that
         are needed by the function
         """
+
         # check if any known dependency is an imported package
         for x in top_level_dependencies:
             if isinstance(x, types.ModuleType) and hasattr(x, '__package__') and x.__package__:
                 # check if the package has any currently loaded sub-imports
                 prefix = x.__name__ + '.'
-                for name, module in sys.modules.items():
+                # A concurrent thread could mutate sys.modules,
+                # make sure we iterate over a copy to avoid exceptions
+                for name in list(sys.modules):
                     # Older versions of pytest will add a "None" module to sys.modules.
                     if name is not None and name.startswith(prefix):
                         # check whether the function can address the sub-module
                         tokens = set(name[len(prefix):].split('.'))
                         if not tokens - set(code.co_names):
                             # ensure unpickler executes this import
-                            self.save(module)
+                            self.save(sys.modules[name])
                             # then discards the reference to it
                             self.write(pickle.POP)
 
@@ -454,6 +461,15 @@ def save_dynamic_class(self, obj):
         clsdict = dict(obj.__dict__)  # copy dict proxy to a dict
         clsdict.pop('__weakref__', None)
 
+        # For ABCMeta in python3.7+, remove _abc_impl as it is not picklable.
+        # This is a fix which breaks the cache but this only makes the first
+        # calls to issubclass slower.
+        if "_abc_impl" in clsdict:
+            import abc
+            (registry, _, _, _) = abc._get_dump(obj)
+            clsdict["_abc_impl"] = [subclass_weakref()
+                                    for subclass_weakref in registry]
+
         # On PyPy, __doc__ is a readonly attribute, so we need to include it in
         # the initial skeleton class.  This is safe because we know that the
         # doc can't participate in a cycle with the original class.
@@ -545,9 +561,13 @@ def save_function_tuple(self, func):
             'globals': f_globals,
             'defaults': defaults,
             'dict': dct,
-            'module': func.__module__,
             'closure_values': closure_values,
+            'module': func.__module__,
+            'name': func.__name__,
+            'doc': func.__doc__,
         }
+        if hasattr(func, '__annotations__'):
+            state['annotations'] = func.__annotations__
         if hasattr(func, '__qualname__'):
             state['qualname'] = func.__qualname__
         save(state)
@@ -572,8 +592,7 @@ def extract_code_globals(cls, co):
                 # PyPy "builtin-code" object
                 out_names = set()
             else:
-                out_names = set(names[oparg]
-                                for op, oparg in _walk_global_ops(co))
+                out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}
 
                 # see if nested function have any global refs
                 if co.co_consts:
@@ -614,7 +633,16 @@ def extract_func_data(self, func):
         # save the dict
         dct = func.__dict__
 
-        base_globals = self.globals_ref.get(id(func.__globals__), {})
+        base_globals = self.globals_ref.get(id(func.__globals__), None)
+        if base_globals is None:
+            # For functions defined in a well behaved module use
+            # vars(func.__module__) for base_globals. This is necessary to
+            # share the global variables across multiple pickled functions from
+            # this module.
+            if hasattr(func, '__module__') and func.__module__ is not None:
+                base_globals = func.__module__
+            else:
+                base_globals = {}
         self.globals_ref[id(func.__globals__)] = base_globals
 
         return (code, f_globals, defaults, closure, dct, base_globals)
@@ -623,6 +651,7 @@ def save_builtin_function(self, obj):
         if obj.__module__ == "__builtin__":
             return self.save_global(obj)
         return self.save_function(obj)
+
     dispatch[types.BuiltinFunctionType] = save_builtin_function
 
     def save_global(self, obj, name=None, pack=struct.pack):
@@ -661,7 +690,8 @@ def save_instancemethod(self, obj):
                 self.save_reduce(types.MethodType, (obj.__func__, obj.__self__), obj=obj)
             else:
                 self.save_reduce(types.MethodType, (obj.__func__, obj.__self__, obj.__self__.__class__),
-                         obj=obj)
+                                 obj=obj)
+
     dispatch[types.MethodType] = save_instancemethod
 
     def save_inst(self, obj):
@@ -715,11 +745,13 @@ def save_inst(self, obj):
     def save_property(self, obj):
         # properties not correctly saved in python
         self.save_reduce(property, (obj.fget, obj.fset, obj.fdel, obj.__doc__), obj=obj)
+
     dispatch[property] = save_property
 
     def save_classmethod(self, obj):
         orig_func = obj.__func__
         self.save_reduce(type(obj), (orig_func,), obj=obj)
+
     dispatch[classmethod] = save_classmethod
     dispatch[staticmethod] = save_classmethod
 
@@ -730,7 +762,7 @@ def __getitem__(self, item):
                 return item
         items = obj(Dummy())
         if not isinstance(items, tuple):
-            items = (items, )
+            items = (items,)
         return self.save_reduce(operator.itemgetter, items)
 
     if type(operator.itemgetter) is type:
@@ -761,16 +793,16 @@ def __getattribute__(self, item):
     def save_file(self, obj):
         """Save a file"""
         try:
-            import StringIO as pystringIO #we can't use cStringIO as it lacks the name attribute
+            import StringIO as pystringIO  # we can't use cStringIO as it lacks the name attribute
         except ImportError:
             import io as pystringIO
 
-        if not hasattr(obj, 'name') or  not hasattr(obj, 'mode'):
+        if not hasattr(obj, 'name') or not hasattr(obj, 'mode'):
             raise pickle.PicklingError("Cannot pickle files that do not map to an actual file")
         if obj is sys.stdout:
-            return self.save_reduce(getattr, (sys,'stdout'), obj=obj)
+            return self.save_reduce(getattr, (sys, 'stdout'), obj=obj)
         if obj is sys.stderr:
-            return self.save_reduce(getattr, (sys,'stderr'), obj=obj)
+            return self.save_reduce(getattr, (sys, 'stderr'), obj=obj)
         if obj is sys.stdin:
             raise pickle.PicklingError("Cannot pickle standard input")
         if obj.closed:
@@ -805,10 +837,10 @@ def save_ellipsis(self, obj):
     def save_not_implemented(self, obj):
         self.save_reduce(_gen_not_implemented, ())
 
-    if PY3:
-        dispatch[io.TextIOWrapper] = save_file
-    else:
+    try:               # Python 2
         dispatch[file] = save_file
+    except NameError:  # Python 3
+        dispatch[io.TextIOWrapper] = save_file
 
     dispatch[type(Ellipsis)] = save_ellipsis
     dispatch[type(NotImplemented)] = save_not_implemented
@@ -823,6 +855,11 @@ def save_logger(self, obj):
 
     dispatch[logging.Logger] = save_logger
 
+    def save_root_logger(self, obj):
+        self.save_reduce(logging.getLogger, (), obj=obj)
+
+    dispatch[logging.RootLogger] = save_root_logger
+
     """Special functions for Add-on libraries"""
     def inject_addons(self):
         """Plug in system. Register additional pickling functions if modules already loaded"""
@@ -898,7 +935,6 @@ def subimport(name):
 def dynamic_subimport(name, vars):
     mod = imp.new_module(name)
     mod.__dict__.update(vars)
-    sys.modules[name] = mod
     return mod
 
 
@@ -928,7 +964,7 @@ def _modules_to_main(modList):
         if type(modname) is str:
             try:
                 mod = __import__(modname)
-            except Exception as e:
+            except Exception:
                 sys.stderr.write('warning: could not import %s\n.  '
                                  'Your function may unexpectedly error due to this import failing;'
                                  'A version mismatch is likely.  Specific error was:\n' % modname)
@@ -937,7 +973,7 @@ def _modules_to_main(modList):
                 setattr(main, mod.__name__, mod)
 
 
-#object generators:
+# object generators:
 def _genpartial(func, args, kwds):
     if not args:
         args = ()
@@ -945,9 +981,11 @@ def _genpartial(func, args, kwds):
         kwds = {}
     return partial(func, *args, **kwds)
 
+
 def _gen_ellipsis():
     return Ellipsis
 
+
 def _gen_not_implemented():
     return NotImplemented
 
@@ -1008,9 +1046,19 @@ def _fill_function(*args):
     else:
         raise ValueError('Unexpected _fill_value arguments: %r' % (args,))
 
-    func.__globals__.update(state['globals'])
+    # Only set global variables that do not exist.
+    for k, v in state['globals'].items():
+        if k not in func.__globals__:
+            func.__globals__[k] = v
+
     func.__defaults__ = state['defaults']
     func.__dict__ = state['dict']
+    if 'annotations' in state:
+        func.__annotations__ = state['annotations']
+    if 'doc' in state:
+        func.__doc__  = state['doc']
+    if 'name' in state:
+        func.__name__ = state['name']
     if 'module' in state:
         func.__module__ = state['module']
     if 'qualname' in state:
@@ -1041,6 +1089,14 @@ def _make_skel_func(code, cell_count, base_globals=None):
     """
     if base_globals is None:
         base_globals = {}
+    elif isinstance(base_globals, str):
+        if sys.modules.get(base_globals, None) is not None:
+            # this checks if we can import the previous environment the object
+            # lived in
+            base_globals = vars(sys.modules[base_globals])
+        else:
+            base_globals = {}
+
     base_globals['__builtins__'] = __builtins__
 
     closure = (
@@ -1056,15 +1112,23 @@ def _rehydrate_skeleton_class(skeleton_class, class_dict):
 
     See CloudPickler.save_dynamic_class for more info.
     """
+    registry = None
     for attrname, attr in class_dict.items():
-        setattr(skeleton_class, attrname, attr)
+        if attrname == "_abc_impl":
+            registry = attr
+        else:
+            setattr(skeleton_class, attrname, attr)
+    if registry is not None:
+        for subclass in registry:
+            skeleton_class.register(subclass)
+
     return skeleton_class
 
 
 def _find_module(mod_name):
     """
     Iterate over each part instead of calling imp.find_module directly.
-    This function is able to find submodules (e.g. sickit.tree)
+    This function is able to find submodules (e.g. scikit.tree)
     """
     path = None
     for part in mod_name.split('.'):
@@ -1075,9 +1139,11 @@ def _find_module(mod_name):
             file.close()
     return path, description
 
+
 """Constructors for 3rd party libraries
 Note: These can never be renamed due to client compatibility issues"""
 
+
 def _getobject(modname, attribute):
     mod = __import__(modname, fromlist=[attribute])
     return mod.__dict__[attribute]
diff --git a/sklearn/externals/joblib/externals/loky/__init__.py b/sklearn/externals/joblib/externals/loky/__init__.py
index 6480423cff..4f68645458 100755
--- a/sklearn/externals/joblib/externals/loky/__init__.py
+++ b/sklearn/externals/joblib/externals/loky/__init__.py
@@ -3,10 +3,20 @@
 :class:`ProcessPoolExecutor` and a function :func:`get_reusable_executor` which
 hide the pool management under the hood.
 """
-from .reusable_executor import get_reusable_executor  # noqa: F401
-from .process_executor import ProcessPoolExecutor  # noqa: F401
-from .process_executor import BrokenProcessPool  # noqa: F401
+from ._base import Executor, Future
+from ._base import wait, as_completed
+from ._base import TimeoutError, CancelledError
+from ._base import ALL_COMPLETED, FIRST_COMPLETED, FIRST_EXCEPTION
 
-from .backend.context import cpu_count  # noqa: F401
+from .backend.context import cpu_count
+from .reusable_executor import get_reusable_executor
+from .process_executor import BrokenProcessPool, ProcessPoolExecutor
 
-__version__ = '2.2.0'
+
+__all__ = ["get_reusable_executor", "cpu_count", "wait", "as_completed",
+           "Future", "Executor", "ProcessPoolExecutor",
+           "BrokenProcessPool", "CancelledError", "TimeoutError",
+           "FIRST_COMPLETED", "FIRST_EXCEPTION", "ALL_COMPLETED", ]
+
+
+__version__ = '2.3.1'
diff --git a/sklearn/externals/joblib/externals/loky/_base.py b/sklearn/externals/joblib/externals/loky/_base.py
index ff4ac92cf4..92422bbf3f 100755
--- a/sklearn/externals/joblib/externals/loky/_base.py
+++ b/sklearn/externals/joblib/externals/loky/_base.py
@@ -11,46 +11,58 @@
 # Licensed to PSF under a Contributor Agreement.
 
 import sys
-import collections
+import time
 import logging
 import threading
-import time
+import collections
+
+
+if sys.version_info[:2] >= (3, 3):
+
+    from concurrent.futures import wait, as_completed
+    from concurrent.futures import TimeoutError, CancelledError
+    from concurrent.futures import Executor, Future as _BaseFuture
+
+    from concurrent.futures import FIRST_EXCEPTION
+    from concurrent.futures import ALL_COMPLETED, FIRST_COMPLETED
+
+    from concurrent.futures._base import LOGGER
+    from concurrent.futures._base import PENDING, RUNNING, CANCELLED
+    from concurrent.futures._base import CANCELLED_AND_NOTIFIED, FINISHED
+else:
 
-FIRST_COMPLETED = 'FIRST_COMPLETED'
-FIRST_EXCEPTION = 'FIRST_EXCEPTION'
-ALL_COMPLETED = 'ALL_COMPLETED'
-_AS_COMPLETED = '_AS_COMPLETED'
-
-# Possible future states (for internal use by the futures package).
-PENDING = 'PENDING'
-RUNNING = 'RUNNING'
-# The future was cancelled by the user...
-CANCELLED = 'CANCELLED'
-# ...and _Waiter.add_cancelled() was called by a worker.
-CANCELLED_AND_NOTIFIED = 'CANCELLED_AND_NOTIFIED'
-FINISHED = 'FINISHED'
-
-_FUTURE_STATES = [
-    PENDING,
-    RUNNING,
-    CANCELLED,
-    CANCELLED_AND_NOTIFIED,
-    FINISHED
-]
-
-_STATE_TO_DESCRIPTION_MAP = {
-    PENDING: "pending",
-    RUNNING: "running",
-    CANCELLED: "cancelled",
-    CANCELLED_AND_NOTIFIED: "cancelled",
-    FINISHED: "finished"
-}
-
-# Logger for internal use by the futures package.
-LOGGER = logging.getLogger("concurrent.futures")
-
-
-if sys.version_info[:2] < (3, 3):
+    FIRST_COMPLETED = 'FIRST_COMPLETED'
+    FIRST_EXCEPTION = 'FIRST_EXCEPTION'
+    ALL_COMPLETED = 'ALL_COMPLETED'
+    _AS_COMPLETED = '_AS_COMPLETED'
+
+    # Possible future states (for internal use by the futures package).
+    PENDING = 'PENDING'
+    RUNNING = 'RUNNING'
+    # The future was cancelled by the user...
+    CANCELLED = 'CANCELLED'
+    # ...and _Waiter.add_cancelled() was called by a worker.
+    CANCELLED_AND_NOTIFIED = 'CANCELLED_AND_NOTIFIED'
+    FINISHED = 'FINISHED'
+
+    _FUTURE_STATES = [
+        PENDING,
+        RUNNING,
+        CANCELLED,
+        CANCELLED_AND_NOTIFIED,
+        FINISHED
+    ]
+
+    _STATE_TO_DESCRIPTION_MAP = {
+        PENDING: "pending",
+        RUNNING: "running",
+        CANCELLED: "cancelled",
+        CANCELLED_AND_NOTIFIED: "cancelled",
+        FINISHED: "finished"
+    }
+
+    # Logger for internal use by the futures package.
+    LOGGER = logging.getLogger("concurrent.futures")
 
     class Error(Exception):
         """Base class for all future-related exceptions."""
@@ -63,548 +75,553 @@ class CancelledError(Error):
     class TimeoutError(Error):
         """The operation exceeded the given deadline."""
         pass
-else:
-    from concurrent.futures import CancelledError, TimeoutError
 
+    class _Waiter(object):
+        """Provides the event that wait() and as_completed() block on."""
+        def __init__(self):
+            self.event = threading.Event()
+            self.finished_futures = []
 
-class _Waiter(object):
-    """Provides the event that wait() and as_completed() block on."""
-    def __init__(self):
-        self.event = threading.Event()
-        self.finished_futures = []
+        def add_result(self, future):
+            self.finished_futures.append(future)
 
-    def add_result(self, future):
-        self.finished_futures.append(future)
+        def add_exception(self, future):
+            self.finished_futures.append(future)
 
-    def add_exception(self, future):
-        self.finished_futures.append(future)
+        def add_cancelled(self, future):
+            self.finished_futures.append(future)
 
-    def add_cancelled(self, future):
-        self.finished_futures.append(future)
+    class _AsCompletedWaiter(_Waiter):
+        """Used by as_completed()."""
 
+        def __init__(self):
+            super(_AsCompletedWaiter, self).__init__()
+            self.lock = threading.Lock()
 
-class _AsCompletedWaiter(_Waiter):
-    """Used by as_completed()."""
+        def add_result(self, future):
+            with self.lock:
+                super(_AsCompletedWaiter, self).add_result(future)
+                self.event.set()
 
-    def __init__(self):
-        super(_AsCompletedWaiter, self).__init__()
-        self.lock = threading.Lock()
+        def add_exception(self, future):
+            with self.lock:
+                super(_AsCompletedWaiter, self).add_exception(future)
+                self.event.set()
 
-    def add_result(self, future):
-        with self.lock:
-            super(_AsCompletedWaiter, self).add_result(future)
-            self.event.set()
+        def add_cancelled(self, future):
+            with self.lock:
+                super(_AsCompletedWaiter, self).add_cancelled(future)
+                self.event.set()
 
-    def add_exception(self, future):
-        with self.lock:
-            super(_AsCompletedWaiter, self).add_exception(future)
-            self.event.set()
+    class _FirstCompletedWaiter(_Waiter):
+        """Used by wait(return_when=FIRST_COMPLETED)."""
 
-    def add_cancelled(self, future):
-        with self.lock:
-            super(_AsCompletedWaiter, self).add_cancelled(future)
+        def add_result(self, future):
+            super(_FirstCompletedWaiter, self).add_result(future)
             self.event.set()
 
+        def add_exception(self, future):
+            super(_FirstCompletedWaiter, self).add_exception(future)
+            self.event.set()
 
-class _FirstCompletedWaiter(_Waiter):
-    """Used by wait(return_when=FIRST_COMPLETED)."""
-
-    def add_result(self, future):
-        super(_FirstCompletedWaiter, self).add_result(future)
-        self.event.set()
-
-    def add_exception(self, future):
-        super(_FirstCompletedWaiter, self).add_exception(future)
-        self.event.set()
+        def add_cancelled(self, future):
+            super(_FirstCompletedWaiter, self).add_cancelled(future)
+            self.event.set()
 
-    def add_cancelled(self, future):
-        super(_FirstCompletedWaiter, self).add_cancelled(future)
-        self.event.set()
+    class _AllCompletedWaiter(_Waiter):
+        """Used by wait(return_when=FIRST_EXCEPTION and ALL_COMPLETED)."""
 
+        def __init__(self, num_pending_calls, stop_on_exception):
+            self.num_pending_calls = num_pending_calls
+            self.stop_on_exception = stop_on_exception
+            self.lock = threading.Lock()
+            super(_AllCompletedWaiter, self).__init__()
 
-class _AllCompletedWaiter(_Waiter):
-    """Used by wait(return_when=FIRST_EXCEPTION and ALL_COMPLETED)."""
+        def _decrement_pending_calls(self):
+            with self.lock:
+                self.num_pending_calls -= 1
+                if not self.num_pending_calls:
+                    self.event.set()
 
-    def __init__(self, num_pending_calls, stop_on_exception):
-        self.num_pending_calls = num_pending_calls
-        self.stop_on_exception = stop_on_exception
-        self.lock = threading.Lock()
-        super(_AllCompletedWaiter, self).__init__()
+        def add_result(self, future):
+            super(_AllCompletedWaiter, self).add_result(future)
+            self._decrement_pending_calls()
 
-    def _decrement_pending_calls(self):
-        with self.lock:
-            self.num_pending_calls -= 1
-            if not self.num_pending_calls:
+        def add_exception(self, future):
+            super(_AllCompletedWaiter, self).add_exception(future)
+            if self.stop_on_exception:
                 self.event.set()
+            else:
+                self._decrement_pending_calls()
 
-    def add_result(self, future):
-        super(_AllCompletedWaiter, self).add_result(future)
-        self._decrement_pending_calls()
-
-    def add_exception(self, future):
-        super(_AllCompletedWaiter, self).add_exception(future)
-        if self.stop_on_exception:
-            self.event.set()
-        else:
+        def add_cancelled(self, future):
+            super(_AllCompletedWaiter, self).add_cancelled(future)
             self._decrement_pending_calls()
 
-    def add_cancelled(self, future):
-        super(_AllCompletedWaiter, self).add_cancelled(future)
-        self._decrement_pending_calls()
-
-
-class _AcquireFutures(object):
-    """A context manager that does an ordered acquire of Future conditions."""
-
-    def __init__(self, futures):
-        self.futures = sorted(futures, key=id)
-
-    def __enter__(self):
-        for future in self.futures:
-            future._condition.acquire()
+    class _AcquireFutures(object):
+        """A context manager that does an ordered acquire of Future conditions.
+        """
 
-    def __exit__(self, *args):
-        for future in self.futures:
-            future._condition.release()
+        def __init__(self, futures):
+            self.futures = sorted(futures, key=id)
 
+        def __enter__(self):
+            for future in self.futures:
+                future._condition.acquire()
 
-def _create_and_install_waiters(fs, return_when):
-    if return_when == _AS_COMPLETED:
-        waiter = _AsCompletedWaiter()
-    elif return_when == FIRST_COMPLETED:
-        waiter = _FirstCompletedWaiter()
-    else:
-        pending_count = sum(
-                f._state not in [CANCELLED_AND_NOTIFIED, FINISHED] for f in fs)
+        def __exit__(self, *args):
+            for future in self.futures:
+                future._condition.release()
 
-        if return_when == FIRST_EXCEPTION:
-            waiter = _AllCompletedWaiter(pending_count, stop_on_exception=True)
-        elif return_when == ALL_COMPLETED:
-            waiter = _AllCompletedWaiter(pending_count,
-                                         stop_on_exception=False)
+    def _create_and_install_waiters(fs, return_when):
+        if return_when == _AS_COMPLETED:
+            waiter = _AsCompletedWaiter()
+        elif return_when == FIRST_COMPLETED:
+            waiter = _FirstCompletedWaiter()
         else:
-            raise ValueError("Invalid return condition: %r" % return_when)
-
-    for f in fs:
-        f._waiters.append(waiter)
-
-    return waiter
-
-
-def as_completed(fs, timeout=None):
-    """An iterator over the given futures that yields each as it completes.
-
-    Args:
-        fs: The sequence of Futures (possibly created by different Executors)
-            to iterate over.
-        timeout: The maximum number of seconds to wait. If None, then there
-            is no limit on the wait time.
-
-    Returns:
-        An iterator that yields the given Futures as they complete (finished or
-        cancelled). If any given Futures are duplicated, they will be returned
-        once.
-
-    Raises:
-        TimeoutError: If the entire result iterator could not be generated
-            before the given timeout.
-    """
-    if timeout is not None:
-        end_time = timeout + time.time()
-
-    fs = set(fs)
-    with _AcquireFutures(fs):
-        finished = set(
-                f for f in fs
-                if f._state in [CANCELLED_AND_NOTIFIED, FINISHED])
-        pending = fs - finished
-        waiter = _create_and_install_waiters(fs, _AS_COMPLETED)
-
-    try:
-        for future in finished:
-            yield future
-
-        while pending:
-            if timeout is None:
-                wait_timeout = None
+            pending_count = sum(
+                    f._state not in [CANCELLED_AND_NOTIFIED, FINISHED]
+                    for f in fs)
+
+            if return_when == FIRST_EXCEPTION:
+                waiter = _AllCompletedWaiter(pending_count,
+                                             stop_on_exception=True)
+            elif return_when == ALL_COMPLETED:
+                waiter = _AllCompletedWaiter(pending_count,
+                                             stop_on_exception=False)
             else:
-                wait_timeout = end_time - time.time()
-                if wait_timeout < 0:
-                    raise TimeoutError('%d (of %d) futures unfinished' % (
-                        len(pending), len(fs)))
-
-            waiter.event.wait(wait_timeout)
-
-            with waiter.lock:
-                finished = waiter.finished_futures
-                waiter.finished_futures = []
-                waiter.event.clear()
+                raise ValueError("Invalid return condition: %r" % return_when)
 
-            for future in finished:
-                yield future
-                pending.remove(future)
-
-    finally:
         for f in fs:
-            with f._condition:
-                f._waiters.remove(waiter)
-
-
-DoneAndNotDoneFutures = collections.namedtuple(
-        'DoneAndNotDoneFutures', 'done not_done')
-
-
-def wait(fs, timeout=None, return_when=ALL_COMPLETED):
-    """Wait for the futures in the given sequence to complete.
-
-    Args:
-        fs: The sequence of Futures (possibly created by different Executors)
-            to wait upon.
-        timeout: The maximum number of seconds to wait. If None, then there
-            is no limit on the wait time.
-        return_when: Indicates when this function should return. The options
-            are:
-
-            FIRST_COMPLETED - Return when any future finishes or is
-                              cancelled.
-            FIRST_EXCEPTION - Return when any future finishes by raising an
-                              exception. If no future raises an exception
-                              then it is equivalent to ALL_COMPLETED.
-            ALL_COMPLETED -   Return when all futures finish or are cancelled.
-
-    Returns:
-        A named 2-tuple of sets. The first set, named 'done', contains the
-        futures that completed (is finished or cancelled) before the wait
-        completed. The second set, named 'not_done', contains uncompleted
-        futures.
-    """
-    with _AcquireFutures(fs):
-        done = set(f for f in fs
-                   if f._state in [CANCELLED_AND_NOTIFIED, FINISHED])
-        not_done = set(fs) - done
-
-        if (return_when == FIRST_COMPLETED) and done:
-            return DoneAndNotDoneFutures(done, not_done)
-        elif (return_when == FIRST_EXCEPTION) and done:
-            if any(f for f in done
-                   if not f.cancelled() and f.exception() is not None):
-                return DoneAndNotDoneFutures(done, not_done)
-
-        if len(done) == len(fs):
-            return DoneAndNotDoneFutures(done, not_done)
-
-        waiter = _create_and_install_waiters(fs, return_when)
-
-    waiter.event.wait(timeout)
-    for f in fs:
-        with f._condition:
-            f._waiters.remove(waiter)
-
-    done.update(waiter.finished_futures)
-    return DoneAndNotDoneFutures(done, set(fs) - done)
-
-
-class Future(object):
-    """Represents the result of an asynchronous computation."""
-
-    def __init__(self):
-        """Initializes the future. Should not be called by clients."""
-        self._condition = threading.Condition()
-        self._state = PENDING
-        self._result = None
-        self._exception = None
-        self._waiters = []
-        self._done_callbacks = []
-
-    def _invoke_callbacks(self):
-        for callback in self._done_callbacks:
-            try:
-                callback(self)
-            except BaseException:
-                LOGGER.exception('exception calling callback for %r', self)
-
-    def __repr__(self):
-        with self._condition:
-            if self._state == FINISHED:
-                if self._exception:
-                    return '<%s at %#x state=%s raised %s>' % (
-                        self.__class__.__name__,
-                        id(self),
-                        _STATE_TO_DESCRIPTION_MAP[self._state],
-                        self._exception.__class__.__name__)
-                else:
-                    return '<%s at %#x state=%s returned %s>' % (
-                        self.__class__.__name__,
-                        id(self),
-                        _STATE_TO_DESCRIPTION_MAP[self._state],
-                        self._result.__class__.__name__)
-            return '<%s at %#x state=%s>' % (
-                    self.__class__.__name__,
-                    id(self),
-                   _STATE_TO_DESCRIPTION_MAP[self._state])
-
-    def cancel(self):
-        """Cancel the future if possible.
-
-        Returns True if the future was cancelled, False otherwise. A future
-        cannot be cancelled if it is running or has already completed.
-        """
-        with self._condition:
-            if self._state in [RUNNING, FINISHED]:
-                return False
-
-            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
-                return True
-
-            self._state = CANCELLED
-            self._condition.notify_all()
-
-        self._invoke_callbacks()
-        return True
-
-    def cancelled(self):
-        """Return True if the future was cancelled."""
-        with self._condition:
-            return self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]
-
-    def running(self):
-        """Return True if the future is currently executing."""
-        with self._condition:
-            return self._state == RUNNING
-
-    def done(self):
-        """Return True of the future was cancelled or finished executing."""
-        with self._condition:
-            return self._state in [CANCELLED, CANCELLED_AND_NOTIFIED, FINISHED]
-
-    def __get_result(self):
-        if self._exception:
-            raise self._exception
-        else:
-            return self._result
-
-    def add_done_callback(self, fn):
-        """Attaches a callable that will be called when the future finishes.
-
-        Args:
-            fn: A callable that will be called with this future as its only
-                argument when the future completes or is cancelled. The
-                callable will always be called by a thread in the same process
-                in which it was added. If the future has already completed or
-                been cancelled then the callable will be called immediately.
-                These callables are called in the order that they were added.
-        """
-        with self._condition:
-            if self._state not in [CANCELLED, CANCELLED_AND_NOTIFIED,
-                                   FINISHED]:
-                self._done_callbacks.append(fn)
-                return
-        fn(self)
-
-    def result(self, timeout=None):
-        """Return the result of the call that the future represents.
-
-        Args:
-            timeout: The number of seconds to wait for the result if the future
-                isn't done. If None, then there is no limit on the wait time.
+            f._waiters.append(waiter)
 
-        Returns:
-            The result of the call that the future represents.
-
-        Raises:
-            CancelledError: If the future was cancelled.
-            TimeoutError: If the future didn't finish executing before the
-                given timeout.
-            Exception: If the call raised then that exception will be raised.
-        """
-        with self._condition:
-            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
-                raise CancelledError()
-            elif self._state == FINISHED:
-                return self.__get_result()
-
-            self._condition.wait(timeout)
-
-            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
-                raise CancelledError()
-            elif self._state == FINISHED:
-                return self.__get_result()
-            else:
-                raise TimeoutError()
+        return waiter
 
-    def exception(self, timeout=None):
-        """Return the exception raised by the call that the future represents.
+    def as_completed(fs, timeout=None):
+        """An iterator over the given futures that yields each as it completes.
 
         Args:
-            timeout: The number of seconds to wait for the exception if the
-                future isn't done. If None, then there is no limit on the wait
-                time.
+            fs: The sequence of Futures (possibly created by different
+                Executors) to iterate over.
+            timeout: The maximum number of seconds to wait. If None, then there
+                is no limit on the wait time.
 
         Returns:
-            The exception raised by the call that the future represents or None
-            if the call completed without raising.
+            An iterator that yields the given Futures as they complete
+            (finished or cancelled). If any given Futures are duplicated, they
+            will be returned once.
 
         Raises:
-            CancelledError: If the future was cancelled.
-            TimeoutError: If the future didn't finish executing before the
-                given timeout.
+            TimeoutError: If the entire result iterator could not be generated
+                before the given timeout.
         """
+        if timeout is not None:
+            end_time = timeout + time.time()
 
-        with self._condition:
-            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
-                raise CancelledError()
-            elif self._state == FINISHED:
-                return self._exception
-
-            self._condition.wait(timeout)
-
-            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
-                raise CancelledError()
-            elif self._state == FINISHED:
-                return self._exception
-            else:
-                raise TimeoutError()
-
-    # The following methods should only be used by Executors and in tests.
-    def set_running_or_notify_cancel(self):
-        """Mark the future as running or process any cancel notifications.
-
-        Should only be used by Executor implementations and unit tests.
-
-        If the future has been cancelled (cancel() was called and returned
-        True) then any threads waiting on the future completing (though calls
-        to as_completed() or wait()) are notified and False is returned.
-
-        If the future was not cancelled then it is put in the running state
-        (future calls to running() will return True) and True is returned.
-
-        This method should be called by Executor implementations before
-        executing the work associated with this future. If this method returns
-        False then the work should not be executed.
+        fs = set(fs)
+        with _AcquireFutures(fs):
+            finished = set(
+                    f for f in fs
+                    if f._state in [CANCELLED_AND_NOTIFIED, FINISHED])
+            pending = fs - finished
+            waiter = _create_and_install_waiters(fs, _AS_COMPLETED)
 
-        Returns:
-            False if the Future was cancelled, True otherwise.
+        try:
+            for future in finished:
+                yield future
 
-        Raises:
-            RuntimeError: if this method was already called or if set_result()
-                or set_exception() was called.
-        """
-        with self._condition:
-            if self._state == CANCELLED:
-                self._state = CANCELLED_AND_NOTIFIED
-                for waiter in self._waiters:
-                    waiter.add_cancelled(self)
-                # self._condition.notify_all() is not necessary because
-                # self.cancel() triggers a notification.
-                return False
-            elif self._state == PENDING:
-                self._state = RUNNING
-                return True
-            else:
-                LOGGER.critical('Future %s in unexpected state: %s',
-                                id(self),
-                                self._state)
-                raise RuntimeError('Future in unexpected state')
+            while pending:
+                if timeout is None:
+                    wait_timeout = None
+                else:
+                    wait_timeout = end_time - time.time()
+                    if wait_timeout < 0:
+                        raise TimeoutError('%d (of %d) futures unfinished' % (
+                            len(pending), len(fs)))
 
-    def set_result(self, result):
-        """Sets the return value of work associated with the future.
+                waiter.event.wait(wait_timeout)
 
-        Should only be used by Executor implementations and unit tests.
-        """
-        with self._condition:
-            self._result = result
-            self._state = FINISHED
-            for waiter in self._waiters:
-                waiter.add_result(self)
-            self._condition.notify_all()
-        self._invoke_callbacks()
-
-    def set_exception(self, exception):
-        """Sets the result of the future as being the given exception.
-
-        Should only be used by Executor implementations and unit tests.
-        """
-        with self._condition:
-            self._exception = exception
-            self._state = FINISHED
-            for waiter in self._waiters:
-                waiter.add_exception(self)
-            self._condition.notify_all()
-        self._invoke_callbacks()
+                with waiter.lock:
+                    finished = waiter.finished_futures
+                    waiter.finished_futures = []
+                    waiter.event.clear()
 
+                for future in finished:
+                    yield future
+                    pending.remove(future)
 
-class Executor(object):
-    """This is an abstract base class for concrete asynchronous executors."""
+        finally:
+            for f in fs:
+                with f._condition:
+                    f._waiters.remove(waiter)
 
-    def submit(self, fn, *args, **kwargs):
-        """Submits a callable to be executed with the given arguments.
+    DoneAndNotDoneFutures = collections.namedtuple(
+            'DoneAndNotDoneFutures', 'done not_done')
 
-        Schedules the callable to be executed as fn(*args, **kwargs) and
-        returns a Future instance representing the execution of the callable.
-
-        Returns:
-            A Future representing the given call.
-        """
-        raise NotImplementedError()
-
-    def map(self, fn, *iterables, **kwargs):
-        """Returns an iterator equivalent to map(fn, iter).
+    def wait(fs, timeout=None, return_when=ALL_COMPLETED):
+        """Wait for the futures in the given sequence to complete.
 
         Args:
-            fn: A callable that will take as many arguments as there are
-                passed iterables.
+            fs: The sequence of Futures (possibly created by different
+                Executors) to wait upon.
             timeout: The maximum number of seconds to wait. If None, then there
                 is no limit on the wait time.
-            chunksize: The size of the chunks the iterable will be broken into
-                before being passed to a child process. This argument is only
-                used by ProcessPoolExecutor; it is ignored by
-                ThreadPoolExecutor.
+            return_when: Indicates when this function should return. The
+                options are:
 
-        Returns:
-            An iterator equivalent to: map(func, *iterables) but the calls may
-            be evaluated out-of-order.
+                FIRST_COMPLETED - Return when any future finishes or is
+                                cancelled.
+                FIRST_EXCEPTION - Return when any future finishes by raising an
+                                exception. If no future raises an exception
+                                then it is equivalent to ALL_COMPLETED.
+                ALL_COMPLETED -   Return when all futures finish or are
+                                cancelled.
 
-        Raises:
-            TimeoutError: If the entire result iterator could not be generated
-                before the given timeout.
-            Exception: If fn(*args) raises for any values.
+        Returns:
+            A named 2-tuple of sets. The first set, named 'done', contains the
+            futures that completed (is finished or cancelled) before the wait
+            completed. The second set, named 'not_done', contains uncompleted
+            futures.
         """
-        timeout = kwargs.get('timeout')
-        if timeout is not None:
-            end_time = timeout + time.time()
+        with _AcquireFutures(fs):
+            done = set(f for f in fs
+                       if f._state in [CANCELLED_AND_NOTIFIED, FINISHED])
+            not_done = set(fs) - done
 
-        fs = [self.submit(fn, *args) for args in zip(*iterables)]
+            if (return_when == FIRST_COMPLETED) and done:
+                return DoneAndNotDoneFutures(done, not_done)
+            elif (return_when == FIRST_EXCEPTION) and done:
+                if any(f for f in done
+                       if not f.cancelled() and f.exception() is not None):
+                    return DoneAndNotDoneFutures(done, not_done)
 
-        # Yield must be hidden in closure so that the futures are submitted
-        # before the first iterator value is required.
-        def result_iterator():
-            try:
-                for future in fs:
-                    if timeout is None:
-                        yield future.result()
-                    else:
-                        yield future.result(end_time - time.time())
-            finally:
-                for future in fs:
-                    future.cancel()
-        return result_iterator()
+            if len(done) == len(fs):
+                return DoneAndNotDoneFutures(done, not_done)
 
-    def shutdown(self, wait=True):
-        """Clean-up the resources associated with the Executor.
+            waiter = _create_and_install_waiters(fs, return_when)
 
-        It is safe to call this method several times. Otherwise, no other
-        methods can be called after this one.
+        waiter.event.wait(timeout)
+        for f in fs:
+            with f._condition:
+                f._waiters.remove(waiter)
 
-        Args:
-            wait: If True then shutdown will not return until all running
-                futures have finished executing and the resources used by the
-                executor have been reclaimed.
-        """
-        pass
+        done.update(waiter.finished_futures)
+        return DoneAndNotDoneFutures(done, set(fs) - done)
+
+    class _BaseFuture(object):
+        """Represents the result of an asynchronous computation."""
+
+        def __init__(self):
+            """Initializes the future. Should not be called by clients."""
+            self._condition = threading.Condition()
+            self._state = PENDING
+            self._result = None
+            self._exception = None
+            self._waiters = []
+            self._done_callbacks = []
+
+        def __repr__(self):
+            with self._condition:
+                if self._state == FINISHED:
+                    if self._exception:
+                        return '<%s at %#x state=%s raised %s>' % (
+                            self.__class__.__name__,
+                            id(self),
+                            _STATE_TO_DESCRIPTION_MAP[self._state],
+                            self._exception.__class__.__name__)
+                    else:
+                        return '<%s at %#x state=%s returned %s>' % (
+                            self.__class__.__name__,
+                            id(self),
+                            _STATE_TO_DESCRIPTION_MAP[self._state],
+                            self._result.__class__.__name__)
+                return '<%s at %#x state=%s>' % (
+                        self.__class__.__name__,
+                        id(self),
+                        _STATE_TO_DESCRIPTION_MAP[self._state])
+
+        def cancel(self):
+            """Cancel the future if possible.
+
+            Returns True if the future was cancelled, False otherwise. A future
+            cannot be cancelled if it is running or has already completed.
+            """
+            with self._condition:
+                if self._state in [RUNNING, FINISHED]:
+                    return False
+
+                if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
+                    return True
+
+                self._state = CANCELLED
+                self._condition.notify_all()
+
+            self._invoke_callbacks()
+            return True
+
+        def cancelled(self):
+            """Return True if the future was cancelled."""
+            with self._condition:
+                return self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]
+
+        def running(self):
+            """Return True if the future is currently executing."""
+            with self._condition:
+                return self._state == RUNNING
+
+        def done(self):
+            """Return True of the future was cancelled or finished executing.
+            """
+            with self._condition:
+                return self._state in [CANCELLED, CANCELLED_AND_NOTIFIED,
+                                       FINISHED]
+
+        def __get_result(self):
+            if self._exception:
+                raise self._exception
+            else:
+                return self._result
+
+        def add_done_callback(self, fn):
+            """Attaches a callable that will be called when the future finishes.
+
+            Args:
+                fn: A callable that will be called with this future as its only
+                    argument when the future completes or is cancelled. The
+                    callable will always be called by a thread in the same
+                    process in which it was added. If the future has already
+                    completed or been cancelled then the callable will be
+                    called immediately. These callables are called in the order
+                    that they were added.
+            """
+            with self._condition:
+                if self._state not in [CANCELLED, CANCELLED_AND_NOTIFIED,
+                                       FINISHED]:
+                    self._done_callbacks.append(fn)
+                    return
+            fn(self)
+
+        def result(self, timeout=None):
+            """Return the result of the call that the future represents.
+
+            Args:
+                timeout: The number of seconds to wait for the result if the
+                    future isn't done. If None, then there is no limit on the
+                    wait time.
+
+            Returns:
+                The result of the call that the future represents.
+
+            Raises:
+                CancelledError: If the future was cancelled.
+                TimeoutError: If the future didn't finish executing before the
+                    given timeout.
+                Exception: If the call raised then that exception will be
+                raised.
+            """
+            with self._condition:
+                if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
+                    raise CancelledError()
+                elif self._state == FINISHED:
+                    return self.__get_result()
+
+                self._condition.wait(timeout)
+
+                if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
+                    raise CancelledError()
+                elif self._state == FINISHED:
+                    return self.__get_result()
+                else:
+                    raise TimeoutError()
+
+        def exception(self, timeout=None):
+            """Return the exception raised by the call that the future
+            represents.
+
+            Args:
+                timeout: The number of seconds to wait for the exception if the
+                    future isn't done. If None, then there is no limit on the
+                    wait time.
+
+            Returns:
+                The exception raised by the call that the future represents or
+                None if the call completed without raising.
+
+            Raises:
+                CancelledError: If the future was cancelled.
+                TimeoutError: If the future didn't finish executing before the
+                    given timeout.
+            """
+
+            with self._condition:
+                if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
+                    raise CancelledError()
+                elif self._state == FINISHED:
+                    return self._exception
+
+                self._condition.wait(timeout)
+
+                if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
+                    raise CancelledError()
+                elif self._state == FINISHED:
+                    return self._exception
+                else:
+                    raise TimeoutError()
+
+        # The following methods should only be used by Executors and in tests.
+        def set_running_or_notify_cancel(self):
+            """Mark the future as running or process any cancel notifications.
+
+            Should only be used by Executor implementations and unit tests.
+
+            If the future has been cancelled (cancel() was called and returned
+            True) then any threads waiting on the future completing (though
+            calls to as_completed() or wait()) are notified and False is
+            returned.
+
+            If the future was not cancelled then it is put in the running state
+            (future calls to running() will return True) and True is returned.
+
+            This method should be called by Executor implementations before
+            executing the work associated with this future. If this method
+            returns False then the work should not be executed.
+
+            Returns:
+                False if the Future was cancelled, True otherwise.
+
+            Raises:
+                RuntimeError: if this method was already called or if
+                    set_result() or set_exception() was called.
+            """
+            with self._condition:
+                if self._state == CANCELLED:
+                    self._state = CANCELLED_AND_NOTIFIED
+                    for waiter in self._waiters:
+                        waiter.add_cancelled(self)
+                    # self._condition.notify_all() is not necessary because
+                    # self.cancel() triggers a notification.
+                    return False
+                elif self._state == PENDING:
+                    self._state = RUNNING
+                    return True
+                else:
+                    LOGGER.critical('Future %s in unexpected state: %s',
+                                    id(self),
+                                    self._state)
+                    raise RuntimeError('Future in unexpected state')
+
+        def set_result(self, result):
+            """Sets the return value of work associated with the future.
+
+            Should only be used by Executor implementations and unit tests.
+            """
+            with self._condition:
+                self._result = result
+                self._state = FINISHED
+                for waiter in self._waiters:
+                    waiter.add_result(self)
+                self._condition.notify_all()
+            self._invoke_callbacks()
+
+        def set_exception(self, exception):
+            """Sets the result of the future as being the given exception.
+
+            Should only be used by Executor implementations and unit tests.
+            """
+            with self._condition:
+                self._exception = exception
+                self._state = FINISHED
+                for waiter in self._waiters:
+                    waiter.add_exception(self)
+                self._condition.notify_all()
+            self._invoke_callbacks()
 
-    def __enter__(self):
-        return self
+    class Executor(object):
+        """This is an abstract base class for concrete asynchronous executors.
+        """
 
-    def __exit__(self, exc_type, exc_val, exc_tb):
-        self.shutdown(wait=True)
-        return False
+        def submit(self, fn, *args, **kwargs):
+            """Submits a callable to be executed with the given arguments.
+
+            Schedules the callable to be executed as fn(*args, **kwargs) and
+            returns a Future instance representing the execution of the
+            callable.
+
+            Returns:
+                A Future representing the given call.
+            """
+            raise NotImplementedError()
+
+        def map(self, fn, *iterables, **kwargs):
+            """Returns an iterator equivalent to map(fn, iter).
+
+            Args:
+                fn: A callable that will take as many arguments as there are
+                    passed iterables.
+                timeout: The maximum number of seconds to wait. If None, then
+                    there is no limit on the wait time.
+                chunksize: The size of the chunks the iterable will be broken
+                    into before being passed to a child process. This argument
+                    is only used by ProcessPoolExecutor; it is ignored by
+                    ThreadPoolExecutor.
+
+            Returns:
+                An iterator equivalent to: map(func, *iterables) but the calls
+                may be evaluated out-of-order.
+
+            Raises:
+                TimeoutError: If the entire result iterator could not be
+                    generated before the given timeout.
+                Exception: If fn(*args) raises for any values.
+            """
+            timeout = kwargs.get('timeout')
+            if timeout is not None:
+                end_time = timeout + time.time()
+
+            fs = [self.submit(fn, *args) for args in zip(*iterables)]
+
+            # Yield must be hidden in closure so that the futures are submitted
+            # before the first iterator value is required.
+            def result_iterator():
+                try:
+                    for future in fs:
+                        if timeout is None:
+                            yield future.result()
+                        else:
+                            yield future.result(end_time - time.time())
+                finally:
+                    for future in fs:
+                        future.cancel()
+            return result_iterator()
+
+        def shutdown(self, wait=True):
+            """Clean-up the resources associated with the Executor.
+
+            It is safe to call this method several times. Otherwise, no other
+            methods can be called after this one.
+
+            Args:
+                wait: If True then shutdown will not return until all running
+                    futures have finished executing and the resources used by
+                    the executor have been reclaimed.
+            """
+            pass
+
+        def __enter__(self):
+            return self
+
+        def __exit__(self, exc_type, exc_val, exc_tb):
+            self.shutdown(wait=True)
+            return False
+
+
+# To make loky._base.Future instances awaitable  by concurrent.futures.wait,
+# derive our custom Future class from _BaseFuture. _invoke_callback is the only
+# modification made to this class in loky.
+class Future(_BaseFuture):
+    def _invoke_callbacks(self):
+        for callback in self._done_callbacks:
+            try:
+                callback(self)
+            except BaseException:
+                LOGGER.exception('exception calling callback for %r', self)
diff --git a/sklearn/externals/joblib/externals/loky/backend/compat.py b/sklearn/externals/joblib/externals/loky/backend/compat.py
index 6366b23d9f..729c77c7d9 100755
--- a/sklearn/externals/joblib/externals/loky/backend/compat.py
+++ b/sklearn/externals/joblib/externals/loky/backend/compat.py
@@ -9,10 +9,10 @@
 
 if sys.version_info[:2] >= (3, 3):
     import queue
-    from _pickle import PicklingError
 else:
     import Queue as queue
-    from pickle import PicklingError
+
+from pickle import PicklingError
 
 if sys.version_info >= (3, 4):
     from multiprocessing.process import BaseProcess
diff --git a/sklearn/externals/joblib/externals/loky/backend/context.py b/sklearn/externals/joblib/externals/loky/backend/context.py
index 52df5589cb..0f744c5918 100755
--- a/sklearn/externals/joblib/externals/loky/backend/context.py
+++ b/sklearn/externals/joblib/externals/loky/backend/context.py
@@ -18,25 +18,41 @@
 import multiprocessing as mp
 
 
-from .process import LokyProcess
+from .process import LokyProcess, LokyInitMainProcess
+
+START_METHODS = ['loky', 'loky_init_main']
+_DEFAULT_START_METHOD = None
 
 if sys.version_info[:2] >= (3, 4):
-    from multiprocessing import get_context as get_mp_context
+    from multiprocessing import get_context as mp_get_context
     from multiprocessing.context import assert_spawning, set_spawning_popen
     from multiprocessing.context import get_spawning_popen, BaseContext
 
-    def get_context(method="loky"):
+    START_METHODS += ['spawn']
+    if sys.platform != 'win32':
+        START_METHODS += ['fork', 'forkserver']
+
+    def get_context(method=None):
+        # Try to overload the default context
+        method = method or _DEFAULT_START_METHOD or "loky"
         if method == "fork":
-            warnings.warn("`fork` start method should not be used with `loky` "
-                          "as it does not respect POSIX. Try using `spawn` or "
-                          "`loky` instead.", UserWarning)
-        return get_mp_context(method)
+            # If 'fork' is explicitly requested, warn user about potential
+            # issues.
+            warnings.warn("`fork` start method should not be used with "
+                          "`loky` as it does not respect POSIX. Try using "
+                          "`spawn` or `loky` instead.", UserWarning)
+        try:
+            context = mp_get_context(method)
+        except ValueError:
+            raise ValueError("Unknown context '{}'. Value should be in {}."
+                             .format(method, START_METHODS))
+
+        return context
 
 else:
-    METHODS = ['loky', 'loky_init_main']
     if sys.platform != 'win32':
         import threading
-        # Mecanism to check that the current thread is spawning a child process
+        # Mechanism to check that the current thread is spawning a process
         _tls = threading.local()
         popen_attr = 'spawning_popen'
     else:
@@ -59,14 +75,30 @@ def assert_spawning(obj):
                 ' through inheritance' % type(obj).__name__
             )
 
-    def get_context(method="loky"):
+    def get_context(method=None):
+        method = method or _DEFAULT_START_METHOD or 'loky'
         if method == "loky":
             return LokyContext()
         elif method == "loky_init_main":
             return LokyInitMainContext()
         else:
-            raise ValueError("Method {} is not implemented. The available "
-                             "methods are {}".format(method, METHODS))
+            raise ValueError("Unknown context '{}'. Value should be in {}."
+                             .format(method, START_METHODS))
+
+
+def set_start_method(method, force=False):
+    global _DEFAULT_START_METHOD
+    if _DEFAULT_START_METHOD is not None and not force:
+        raise RuntimeError('context has already been set')
+    assert method is None or method in START_METHODS, (
+        "'{}' is not a valid start_method. It should be in {}"
+        .format(method, START_METHODS))
+
+    _DEFAULT_START_METHOD = method
+
+
+def get_start_method():
+    return _DEFAULT_START_METHOD
 
 
 def cpu_count():
@@ -74,12 +106,13 @@ def cpu_count():
 
     The returned number of CPUs accounts for:
      * the number of CPUs in the system, as given by
-       ``multiprocessing.cpu_count``
+       ``multiprocessing.cpu_count``;
      * the CPU affinity settings of the current process
-       (available with Python 3.4+ on some Unix systems)
-     * CFS scheduler CPU bandwidth limit
-       (available on Linux only)
-    and is given as the minimum of these three constraints.
+       (available with Python 3.4+ on some Unix systems);
+     * CFS scheduler CPU bandwidth limit (available on Linux only, typically
+       set by docker and similar container orchestration systems);
+     * the value of the LOKY_MAX_CPU_COUNT environment variable if defined.
+    and is given as the minimum of these constraints.
     It is also always larger or equal to 1.
     """
     import math
@@ -109,10 +142,15 @@ def cpu_count():
             cfs_period_us = int(fh.read())
 
         if cfs_quota_us > 0 and cfs_period_us > 0:
-            cpu_count_cfs = math.ceil(cfs_quota_us / cfs_period_us)
-            cpu_count_cfs = max(cpu_count_cfs, 1)
+            # Make sure this quantity is an int as math.ceil returns a
+            # float in python2.7. (See issue #165)
+            cpu_count_cfs = int(math.ceil(cfs_quota_us / cfs_period_us))
 
-    return min(cpu_count_mp, cpu_count_affinity, cpu_count_cfs)
+    # User defined soft-limit passed as an loky specific environment variable.
+    cpu_count_loky = int(os.environ.get('LOKY_MAX_CPU_COUNT', cpu_count_mp))
+    aggregate_cpu_count = min(cpu_count_mp, cpu_count_affinity, cpu_count_cfs,
+                              cpu_count_loky)
+    return max(aggregate_cpu_count, 1)
 
 
 class LokyContext(BaseContext):
@@ -138,7 +176,7 @@ def get_context(self):
             return self
 
         def get_start_method(self):
-            return "loky"
+            return self._name
 
         def Pipe(self, duplex=True):
             '''Returns two connection object connected by a pipe'''
@@ -216,12 +254,12 @@ class LokyInitMainContext(LokyContext):
     For more details, see the end of the following section of python doc
     https://docs.python.org/3/library/multiprocessing.html#multiprocessing-programming
     """
-    def Process(self, *args, **kwargs):
-        kwargs.pop('init_main_module', True)
-        return LokyProcess(*args, init_main_module=True, **kwargs)
+    _name = 'loky_init_main'
+    Process = LokyInitMainProcess
 
 
 if sys.version_info > (3, 4):
     """Register loky context so it works with multiprocessing.get_context"""
-    mp.context._concrete_contexts['loky'] = LokyContext()
+    ctx_loky = LokyContext()
+    mp.context._concrete_contexts['loky'] = ctx_loky
     mp.context._concrete_contexts['loky_init_main'] = LokyInitMainContext()
diff --git a/sklearn/externals/joblib/externals/loky/backend/popen_loky_posix.py b/sklearn/externals/joblib/externals/loky/backend/popen_loky_posix.py
index 729c7c71fe..35a5907d21 100755
--- a/sklearn/externals/joblib/externals/loky/backend/popen_loky_posix.py
+++ b/sklearn/externals/joblib/externals/loky/backend/popen_loky_posix.py
@@ -127,7 +127,8 @@ def _launch(self, process_obj):
             set_spawning_popen(self)
             try:
                 prep_data = spawn.get_preparation_data(
-                    process_obj._name, process_obj.init_main_module)
+                    process_obj._name,
+                    getattr(process_obj, "init_main_module", True))
                 reduction.dump(prep_data, fp)
                 reduction.dump(process_obj, fp)
 
@@ -150,9 +151,10 @@ def _launch(self, process_obj):
                     cmd_python += ['--semaphore',
                                    str(reduction._mk_inheritable(tracker_fd))]
                 self._fds.extend([child_r, child_w, tracker_fd])
-                util.debug("launch python with cmd:\n%s" % cmd_python)
                 from .fork_exec import fork_exec
                 pid = fork_exec(cmd_python, self._fds)
+                util.debug("launched python with pid {} and cmd:\n{}"
+                           .format(pid, cmd_python))
                 self.sentinel = parent_r
 
                 method = 'getbuffer'
diff --git a/sklearn/externals/joblib/externals/loky/backend/popen_loky_win32.py b/sklearn/externals/joblib/externals/loky/backend/popen_loky_win32.py
index 1f3e909e78..dccf04bf65 100755
--- a/sklearn/externals/joblib/externals/loky/backend/popen_loky_win32.py
+++ b/sklearn/externals/joblib/externals/loky/backend/popen_loky_win32.py
@@ -1,10 +1,11 @@
 import os
 import sys
+from pickle import load
+from multiprocessing import process, util
 
-from .context import get_spawning_popen, set_spawning_popen
 from . import spawn
 from . import reduction
-from multiprocessing import util
+from .context import get_spawning_popen, set_spawning_popen
 
 if sys.platform == "win32":
     # Avoid import error by code introspection tools such as test runners
@@ -42,7 +43,7 @@ class Popen(_Popen):
 
     def __init__(self, process_obj):
         prep_data = spawn.get_preparation_data(
-            process_obj._name, process_obj.init_main_module)
+            process_obj._name, getattr(process_obj, "init_main_module", True))
 
         # read end of pipe will be "stolen" by the child process
         # -- see spawn_main() in spawn.py.
@@ -50,8 +51,7 @@ def __init__(self, process_obj):
         if sys.version_info[:2] > (3, 3):
             wfd = msvcrt.open_osfhandle(wfd, 0)
 
-        cmd = spawn.get_command_line(parent_pid=os.getpid(),
-                                     pipe_handle=rhandle)
+        cmd = get_command_line(parent_pid=os.getpid(), pipe_handle=rhandle)
         cmd = ' '.join('"%s"' % x for x in cmd)
 
         try:
@@ -64,7 +64,7 @@ def __init__(self, process_obj):
                         None, None, inherit, 0,
                         None, None, None)
                     _winapi.CloseHandle(ht)
-                except:
+                except BaseException as e:
                     _winapi.CloseHandle(rhandle)
                     raise
 
@@ -97,3 +97,54 @@ def __init__(self, process_obj):
     def duplicate_for_child(self, handle):
         assert self is get_spawning_popen()
         return reduction.duplicate(handle, self.sentinel)
+
+
+if sys.version_info[:2] >= (3, 4):
+    from multiprocessing.spawn import get_command_line
+else:
+    # compatibility for python2.7. Duplicate here the code from
+    # multiprocessing.forking.main to call our prepare function and correctly
+    # set the default start_methods in loky.
+
+    def get_command_line(pipe_handle, **kwds):
+        '''
+        Returns prefix of command line used for spawning a child process
+        '''
+        if getattr(sys, 'frozen', False):
+            return ([sys.executable, '--multiprocessing-fork', pipe_handle])
+        else:
+            prog = 'from sklearn.externals.joblib.externals.loky.backend.popen_loky_win32 import main; main()'
+            opts = util._args_from_interpreter_flags()
+            return [spawn.get_executable()] + opts + [
+                '-c', prog, '--multiprocessing-fork', pipe_handle]
+
+    def is_forking(argv):
+        '''
+        Return whether commandline indicates we are forking
+        '''
+        if len(argv) >= 2 and argv[1] == '--multiprocessing-fork':
+            assert len(argv) == 3
+            return True
+        else:
+            return False
+
+    def main():
+        '''
+        Run code specified by data received over pipe
+        '''
+        assert is_forking(sys.argv)
+
+        handle = int(sys.argv[-1])
+        fd = msvcrt.open_osfhandle(handle, os.O_RDONLY)
+        from_parent = os.fdopen(fd, 'rb')
+
+        process.current_process()._inheriting = True
+        preparation_data = load(from_parent)
+        spawn.prepare(preparation_data)
+        self = load(from_parent)
+        process.current_process()._inheriting = False
+
+        from_parent.close()
+
+        exitcode = self._bootstrap()
+        exit(exitcode)
diff --git a/sklearn/externals/joblib/externals/loky/backend/process.py b/sklearn/externals/joblib/externals/loky/backend/process.py
index 401a46fa4f..f6a00c90e3 100755
--- a/sklearn/externals/joblib/externals/loky/backend/process.py
+++ b/sklearn/externals/joblib/externals/loky/backend/process.py
@@ -73,6 +73,21 @@ def authkey(self, authkey):
             '''
             self._authkey = AuthenticationKey(authkey)
 
+        def _bootstrap(self):
+            from .context import set_start_method
+            set_start_method(self._start_method)
+            super(LokyProcess, self)._bootstrap()
+
+
+class LokyInitMainProcess(LokyProcess):
+    _start_method = 'loky_init_main'
+
+    def __init__(self, group=None, target=None, name=None, args=(),
+                 kwargs={}, daemon=None):
+        super(LokyInitMainProcess, self).__init__(
+            group=group, target=target, name=name, args=args, kwargs=kwargs,
+            daemon=daemon, init_main_module=True)
+
 
 #
 # We subclass bytes to avoid accidental transmission of auth keys over network
diff --git a/sklearn/externals/joblib/externals/loky/backend/queues.py b/sklearn/externals/joblib/externals/loky/backend/queues.py
index f640151687..04f080f3e1 100755
--- a/sklearn/externals/joblib/externals/loky/backend/queues.py
+++ b/sklearn/externals/joblib/externals/loky/backend/queues.py
@@ -147,16 +147,18 @@ def _feed(buffer, notempty, send_bytes, writelock, close, reducers,
                             return
 
                         # serialize the data before acquiring the lock
-                        obj = CustomizableLokyPickler.dumps(
+                        obj_ = CustomizableLokyPickler.dumps(
                             obj, reducers=reducers)
                         if wacquire is None:
-                            send_bytes(obj)
+                            send_bytes(obj_)
                         else:
                             wacquire()
                             try:
-                                send_bytes(obj)
+                                send_bytes(obj_)
                             finally:
                                 wrelease()
+                        # Remove references early to avoid leaking memory
+                        del obj, obj_
                 except IndexError:
                     pass
             except BaseException as e:
diff --git a/sklearn/externals/joblib/externals/loky/backend/reduction.py b/sklearn/externals/joblib/externals/loky/backend/reduction.py
index 20eb581cbf..b621a92930 100755
--- a/sklearn/externals/joblib/externals/loky/backend/reduction.py
+++ b/sklearn/externals/joblib/externals/loky/backend/reduction.py
@@ -181,12 +181,13 @@ def h(cls):
 register(type(_C.h), _reduce_method)
 
 
-def _reduce_method_descriptor(m):
-    return getattr, (m.__objclass__, m.__name__)
+if not hasattr(sys, "pypy_version_info"):
+    # PyPy uses functions instead of method_descriptors and wrapper_descriptors
+    def _reduce_method_descriptor(m):
+        return getattr, (m.__objclass__, m.__name__)
 
-
-register(type(list.append), _reduce_method_descriptor)
-register(type(int.__add__), _reduce_method_descriptor)
+    register(type(list.append), _reduce_method_descriptor)
+    register(type(int.__add__), _reduce_method_descriptor)
 
 
 # Make partial func pickable
diff --git a/sklearn/externals/joblib/externals/loky/backend/spawn.py b/sklearn/externals/joblib/externals/loky/backend/spawn.py
index a7e57b884c..d92d189ddc 100755
--- a/sklearn/externals/joblib/externals/loky/backend/spawn.py
+++ b/sklearn/externals/joblib/externals/loky/backend/spawn.py
@@ -10,9 +10,10 @@
 import sys
 import runpy
 import types
-import multiprocessing as mp
 from multiprocessing import process, util
 
+from sklearn.externals.joblib.externals.loky.backend import context
+
 
 if sys.platform != 'win32':
     WINEXE = False
@@ -26,21 +27,6 @@
 else:
     _python_exe = sys.executable
 
-if sys.version_info[:2] < (3, 4):
-    def get_command_line(pipe_handle, **kwds):
-        '''
-        Returns prefix of command line used for spawning a child process
-        '''
-        if getattr(sys, 'frozen', False):
-            return ([sys.executable, '--multiprocessing-fork', pipe_handle])
-        else:
-            prog = 'from multiprocessing.forking import main; main()'
-            opts = util._args_from_interpreter_flags()
-            return [_python_exe] + opts + [
-                '-c', prog, '--multiprocessing-fork', pipe_handle]
-else:
-    from multiprocessing.spawn import get_command_line
-
 
 def get_executable():
     return _python_exe
@@ -108,7 +94,7 @@ def get_preparation_data(name, init_main_module=True):
         main_module = sys.modules['__main__']
         try:
             main_mod_name = getattr(main_module.__spec__, "name", None)
-        except:
+        except BaseException:
             main_mod_name = None
         if main_mod_name is not None:
             d['init_main_from_name'] = main_mod_name
@@ -165,9 +151,6 @@ def prepare(data):
     if 'orig_dir' in data:
         process.ORIGINAL_DIR = data['orig_dir']
 
-    if hasattr(mp, 'set_start_method'):
-        mp.set_start_method('loky', force=True)
-
     if 'tacker_pid' in data:
         from . import semaphore_tracker
         semaphore_tracker._semaphore_tracker._pid = data["tracker_pid"]
diff --git a/sklearn/externals/joblib/externals/loky/backend/utils.py b/sklearn/externals/joblib/externals/loky/backend/utils.py
index d54098a816..db21c84e02 100755
--- a/sklearn/externals/joblib/externals/loky/backend/utils.py
+++ b/sklearn/externals/joblib/externals/loky/backend/utils.py
@@ -30,16 +30,14 @@ def _recursive_terminate_with_psutil(process, retries=5):
     except psutil.NoSuchProcess:
         return
 
-    for child in children:
+    # Kill the children in reverse order to avoid killing the parents before
+    # the children in cases where there are more processes nested.
+    for child in children[::-1]:
         try:
-            child.terminate()
+            child.kill()
         except psutil.NoSuchProcess:
             pass
 
-    gone, still_alive = psutil.wait_procs(children, timeout=5)
-    for child_process in still_alive:
-        child_process.kill()
-
     process.terminate()
     process.join()
 
diff --git a/sklearn/externals/joblib/externals/loky/cloudpickle_wrapper.py b/sklearn/externals/joblib/externals/loky/cloudpickle_wrapper.py
new file mode 100755
index 0000000000..6b387e75f1
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/cloudpickle_wrapper.py
@@ -0,0 +1,53 @@
+import os
+import inspect
+from functools import partial
+
+from .backend import LOKY_PICKLER
+
+try:
+    from cloudpickle import dumps, loads
+    cloudpickle = True
+except ImportError:
+    cloudpickle = False
+
+if not LOKY_PICKLER and cloudpickle:
+    wrap_cache = dict()
+
+    class CloudpickledObjectWrapper(object):
+        def __init__(self, obj):
+            self.pickled_obj = dumps(obj)
+
+        def __reduce__(self):
+            return loads, (self.pickled_obj,)
+
+    def _wrap_non_picklable_objects(obj):
+        need_wrap = "__main__" in getattr(obj, "__module__", "")
+        if isinstance(obj, partial):
+            return partial(
+                _wrap_non_picklable_objects(obj.func),
+                *[_wrap_non_picklable_objects(a) for a in obj.args],
+                **{k: _wrap_non_picklable_objects(v)
+                   for k, v in obj.keywords.items()}
+            )
+        if callable(obj):
+            # Need wrap if the object is a function defined in a local scope of
+            # another function.
+            func_code = getattr(obj, "__code__", "")
+            need_wrap |= getattr(func_code, "co_flags", 0) & inspect.CO_NESTED
+
+            # Need wrap if the obj is a lambda expression
+            func_name = getattr(obj, "__name__", "")
+            need_wrap |= "<lambda>" in func_name
+
+        if not need_wrap:
+            return obj
+
+        wrapped_obj = wrap_cache.get(obj)
+        if wrapped_obj is None:
+            wrapped_obj = CloudpickledObjectWrapper(obj)
+            wrap_cache[obj] = wrapped_obj
+        return wrapped_obj
+
+else:
+    def _wrap_non_picklable_objects(obj):
+        return obj
diff --git a/sklearn/externals/joblib/externals/loky/process_executor.py b/sklearn/externals/joblib/externals/loky/process_executor.py
index 6868f52706..cfdd37abce 100755
--- a/sklearn/externals/joblib/externals/loky/process_executor.py
+++ b/sklearn/externals/joblib/externals/loky/process_executor.py
@@ -60,18 +60,18 @@
 
 
 import os
+import gc
 import sys
-import types
+import struct
 import weakref
 import warnings
 import itertools
 import traceback
 import threading
+from time import time
 import multiprocessing as mp
 from functools import partial
 from pickle import PicklingError
-from time import time
-import gc
 
 from . import _base
 from .backend import get_context
@@ -80,6 +80,7 @@
 from .backend.context import cpu_count
 from .backend.queues import Queue, SimpleQueue, Full
 from .backend.utils import recursive_terminate
+from .cloudpickle_wrapper import _wrap_non_picklable_objects
 
 try:
     from concurrent.futures.process import BrokenProcessPool as _BPPException
@@ -115,14 +116,16 @@
 MAX_DEPTH = int(os.environ.get("LOKY_MAX_DEPTH", 10))
 _CURRENT_DEPTH = 0
 
-# Minimum time interval between two consecutive memory usage checks.
-_MEMORY_CHECK_DELAY = 1.
+# Minimum time interval between two consecutive memory leak protection checks.
+_MEMORY_LEAK_CHECK_DELAY = 1.
 
 # Number of bytes of memory usage allowed over the reference process size.
 _MAX_MEMORY_LEAK_SIZE = int(1e8)
 
+
 try:
     from psutil import Process
+    _USE_PSUTIL = True
 
     def _get_memory_usage(pid, force_gc=False):
         if force_gc:
@@ -131,7 +134,7 @@ def _get_memory_usage(pid, force_gc=False):
         return Process(pid).memory_info().rss
 
 except ImportError:
-    _get_memory_usage = None
+    _USE_PSUTIL = False
 
 
 class _ThreadWakeup:
@@ -263,35 +266,16 @@ def __repr__(self):
         return "CallItem({}, {}, {}, {})".format(
             self.work_id, self.fn, self.args, self.kwargs)
 
-    try:
-        # If cloudpickle is present on the system, use it to pickle the
-        # function. This permits to use interactive terminal for loky calls.
-        # TODO: Add option to deactivate, as it increases pickling time.
-        from .backend import LOKY_PICKLER
-        assert LOKY_PICKLER is None or LOKY_PICKLER == ""
-
-        import cloudpickle  # noqa: F401
-
-        def __getstate__(self):
-            from cloudpickle import dumps
-            if isinstance(self.fn, (types.FunctionType,
-                                    types.LambdaType,
-                                    partial)):
-                cp = True
-                fn = dumps(self.fn)
-            else:
-                cp = False
-                fn = self.fn
-            return (self.work_id, self.args, self.kwargs, fn, cp)
+    def __getstate__(self):
+        return (
+            self.work_id,
+            _wrap_non_picklable_objects(self.fn),
+            [_wrap_non_picklable_objects(a) for a in self.args],
+            {k: _wrap_non_picklable_objects(a) for k, a in self.kwargs.items()}
+        )
 
-        def __setstate__(self, state):
-            self.work_id, self.args, self.kwargs, self.fn, cp = state
-            if cp:
-                from cloudpickle import loads
-                self.fn = loads(self.fn)
-
-    except (ImportError, AssertionError) as e:
-        pass
+    def __setstate__(self, state):
+        self.work_id, self.fn, self.args, self.kwargs = state
 
 
 class _SafeQueue(Queue):
@@ -305,12 +289,17 @@ def __init__(self, max_size=0, ctx=None, pending_work_items=None,
 
     def _on_queue_feeder_error(self, e, obj):
         if isinstance(obj, _CallItem):
-            # fromat traceback only on python3
-            pickling_error = PicklingError(
-                "Could not pickle the task to send it to the workers.")
+            # format traceback only works on python3
+            if isinstance(e, struct.error):
+                raised_error = RuntimeError(
+                    "The task could not be sent to the workers as it is too "
+                    "large for `send_bytes`.")
+            else:
+                raised_error = PicklingError(
+                    "Could not pickle the task to send it to the workers.")
             tb = traceback.format_exception(
                 type(e), e, getattr(e, "__traceback__", None))
-            pickling_error.__cause__ = _RemoteTraceback(
+            raised_error.__cause__ = _RemoteTraceback(
                 '\n"""\n{}"""'.format(''.join(tb)))
             work_item = self.pending_work_items.pop(obj.work_id, None)
             self.running_work_items.remove(obj.work_id)
@@ -318,7 +307,7 @@ def _on_queue_feeder_error(self, e, obj):
             # case, the queue_manager_thread fails all work_items with
             # BrokenProcessPool
             if work_item is not None:
-                work_item.future.set_exception(pickling_error)
+                work_item.future.set_exception(raised_error)
                 del work_item
             self.thread_wakeup.wakeup()
         else:
@@ -394,8 +383,8 @@ def _process_worker(call_queue, result_queue, initializer, initargs,
     # set the global _CURRENT_DEPTH mechanism to limit recursive call
     global _CURRENT_DEPTH
     _CURRENT_DEPTH = current_depth
-    _REFERENCE_PROCESS_SIZE = None
-    _LAST_MEMORY_CHECK = None
+    _process_reference_size = None
+    _last_memory_leak_check = None
     pid = os.getpid()
 
     mp.util.debug('Worker started with timeout=%s' % timeout)
@@ -414,7 +403,13 @@ def _process_worker(call_queue, result_queue, initializer, initargs,
                 mp.util.info("Could not acquire processes_management_lock")
                 continue
         except BaseException as e:
-            traceback.print_exc()
+            previous_tb = traceback.format_exc()
+            try:
+                result_queue.put(_RemoteTraceback(previous_tb))
+            except BaseException:
+                # If we cannot format correctly the exception, at least print
+                # the traceback.
+                print(previous_tb)
             sys.exit(1)
         if call_item is None:
             # Notify queue management thread about clean worker shutdown
@@ -428,21 +423,22 @@ def _process_worker(call_queue, result_queue, initializer, initargs,
             result_queue.put(_ResultItem(call_item.work_id, exception=exc))
         else:
             _sendback_result(result_queue, call_item.work_id, result=r)
+            del r
 
         # Free the resource as soon as possible, to avoid holding onto
         # open files or shared memory that is not needed anymore
         del call_item
 
-        if _get_memory_usage is not None:
-            if _REFERENCE_PROCESS_SIZE is None:
+        if _USE_PSUTIL:
+            if _process_reference_size is None:
                 # Make reference measurement after the first call
-                _REFERENCE_PROCESS_SIZE = _get_memory_usage(pid, force_gc=True)
-                _LAST_MEMORY_CHECK = time()
+                _process_reference_size = _get_memory_usage(pid, force_gc=True)
+                _last_memory_leak_check = time()
                 continue
-            if time() - _LAST_MEMORY_CHECK > _MEMORY_CHECK_DELAY:
+            if time() - _last_memory_leak_check > _MEMORY_LEAK_CHECK_DELAY:
                 mem_usage = _get_memory_usage(pid)
-                _LAST_MEMORY_CHECK = time()
-                if mem_usage - _REFERENCE_PROCESS_SIZE < _MAX_MEMORY_LEAK_SIZE:
+                _last_memory_leak_check = time()
+                if mem_usage - _process_reference_size < _MAX_MEMORY_LEAK_SIZE:
                     # Memory usage stays within bounds: everything is fine.
                     continue
 
@@ -450,8 +446,8 @@ def _process_worker(call_queue, result_queue, initializer, initargs,
                 # after a forced garbage collection to break any reference
                 # cycles.
                 mem_usage = _get_memory_usage(pid, force_gc=True)
-                _LAST_MEMORY_CHECK = time()
-                if mem_usage - _REFERENCE_PROCESS_SIZE < _MAX_MEMORY_LEAK_SIZE:
+                _last_memory_leak_check = time()
+                if mem_usage - _process_reference_size < _MAX_MEMORY_LEAK_SIZE:
                     # The GC managed to free the memory: everything is fine.
                     continue
 
@@ -461,6 +457,14 @@ def _process_worker(call_queue, result_queue, initializer, initargs,
                 result_queue.put(pid)
                 with worker_exit_lock:
                     return
+        else:
+            # if psutil is not installed, trigger gc.collect events
+            # regularly to limit potential memory leaks due to reference cycles
+            if ((_last_memory_leak_check is None) or
+                    (time() - _last_memory_leak_check >
+                     _MEMORY_LEAK_CHECK_DELAY)):
+                gc.collect()
+                _last_memory_leak_check = time()
 
 
 def _add_call_item_to_queue(pending_work_items,
@@ -612,31 +616,41 @@ def shutdown_all_workers():
         worker_sentinels = [p.sentinel for p in processes.values()]
         ready = wait(readers + worker_sentinels)
 
-        broken = ("A process in the executor was terminated abruptly", None)
+        broken = ("A worker process managed by the executor was unexpectedly "
+                  "terminated. This could be caused by a segmentation fault "
+                  "while calling the function or by an excessive memory usage "
+                  "causing the Operating System to kill the worker.", None,
+                  TerminatedWorkerError)
         if result_reader in ready:
             try:
                 result_item = result_reader.recv()
                 broken = None
+                if isinstance(result_item, _RemoteTraceback):
+                    broken = ("A task has failed to un-serialize. Please "
+                              "ensure that the arguments of the function are "
+                              "all picklable.", result_item.tb,
+                              BrokenProcessPool)
             except BaseException as e:
                 tb = getattr(e, "__traceback__", None)
                 if tb is None:
                     _, _, tb = sys.exc_info()
-                broken = ("A result has failed to un-serialize",
-                          traceback.format_exception(type(e), e, tb))
+                broken = ("A result has failed to un-serialize. Please "
+                          "ensure that the objects returned by the function "
+                          "are always picklable.",
+                          traceback.format_exception(type(e), e, tb),
+                          BrokenProcessPool)
         elif wakeup_reader in ready:
             broken = None
             result_item = None
         thread_wakeup.clear()
-        if broken:
-            msg, cause = broken
-            # Mark the process pool broken so that submits fail right now.
-            executor_flags.flag_as_broken(
-                msg + ", the pool is not usable anymore.")
-            bpe = BrokenProcessPool(
-                msg + " while the future was running or pending.")
-            if cause is not None:
+        if broken is not None:
+            msg, cause_tb, exc_type = broken
+            bpe = exc_type(msg)
+            if cause_tb is not None:
                 bpe.__cause__ = _RemoteTraceback(
-                    "\n'''\n{}'''".format(''.join(cause)))
+                    "\n'''\n{}'''".format(''.join(cause_tb)))
+            # Mark the process pool broken so that submits fail right now.
+            executor_flags.flag_as_broken(bpe)
 
             # All futures in flight must be marked failed
             for work_id, work_item in pending_work_items.items():
@@ -799,6 +813,15 @@ class LokyRecursionError(RuntimeError):
 
 
 class BrokenProcessPool(_BPPException):
+    """
+    Raised when the executor is broken while a future was in the running state.
+    The cause can an error raised when unpickling the task in the worker
+    process or when unpickling the result value in the parent process. It can
+    also be caused by a worker process being terminated unexpectedly.
+    """
+
+
+class TerminatedWorkerError(BrokenProcessPool):
     """
     Raised when a process in a ProcessPoolExecutor terminated abruptly
     while a future was in the running state.
@@ -861,8 +884,8 @@ def __init__(self, max_workers=None, job_reducers=None,
 
         if initializer is not None and not callable(initializer):
             raise TypeError("initializer must be a callable")
-        self._initializer = initializer
-        self._initargs = initargs
+        self._initializer = _wrap_non_picklable_objects(initializer)
+        self._initargs = [_wrap_non_picklable_objects(a) for a in initargs]
 
         _check_max_depth(self._context)
 
@@ -989,8 +1012,8 @@ def _ensure_executor_running(self):
 
     def submit(self, fn, *args, **kwargs):
         with self._flags.shutdown_lock:
-            if self._flags.broken:
-                raise BrokenProcessPool(self._flags.broken)
+            if self._flags.broken is not None:
+                raise self._flags.broken
             if self._flags.shutdown:
                 raise ShutdownExecutorError(
                     'cannot schedule new futures after shutdown')
diff --git a/sklearn/externals/joblib/func_inspect.py b/sklearn/externals/joblib/func_inspect.py
index e9320b43f1..d4d2670943 100755
--- a/sklearn/externals/joblib/func_inspect.py
+++ b/sklearn/externals/joblib/func_inspect.py
@@ -275,7 +275,7 @@ def filter_args(func, ignore_lst, args=(), kwargs=dict()):
         else:
             position = arg_position - len(arg_names)
             if arg_name in kwargs:
-                arg_dict[arg_name] = kwargs.pop(arg_name)
+                arg_dict[arg_name] = kwargs[arg_name]
             else:
                 try:
                     arg_dict[arg_name] = arg_defaults[position]
diff --git a/sklearn/externals/joblib/memory.py b/sklearn/externals/joblib/memory.py
index 91204af01f..e31ba2edb7 100755
--- a/sklearn/externals/joblib/memory.py
+++ b/sklearn/externals/joblib/memory.py
@@ -96,8 +96,11 @@ def register_store_backend(backend_name, backend):
     _STORE_BACKENDS[backend_name] = backend
 
 
-def _store_backend_factory(backend, location, verbose=0, backend_options={}):
+def _store_backend_factory(backend, location, verbose=0, backend_options=None):
     """Return the correct store object for the given location."""
+    if backend_options is None:
+        backend_options = {}
+
     if isinstance(location, StoreBackendBase):
         return location
     elif isinstance(location, _basestring):
@@ -207,8 +210,11 @@ class MemorizedResult(Logger):
     def __init__(self, location, func, args_id, backend='local',
                  mmap_mode=None, verbose=0, timestamp=None, metadata=None):
         Logger.__init__(self)
-        self.func = func
         self.func_id = _build_func_identifier(func)
+        if isinstance(func, _basestring):
+            self.func = func
+        else:
+            self.func = self.func_id
         self.args_id = args_id
         self.store_backend = _store_backend_factory(backend, location,
                                                     verbose=verbose)
@@ -252,14 +258,15 @@ def __repr__(self):
         return ('{class_name}(location="{location}", func="{func}", '
                 'args_id="{args_id}")'
                 .format(class_name=self.__class__.__name__,
-                        location=self.store_backend,
+                        location=self.store_backend.location,
                         func=self.func,
                         args_id=self.args_id
                         ))
-    def __reduce__(self):
-        return (self.__class__,
-                (self.store_backend, self.func, self.args_id),
-                {'mmap_mode': self.mmap_mode})
+
+    def __getstate__(self):
+        state = self.__dict__.copy()
+        state['timestamp'] = None
+        return state
 
 
 class NotMemorizedResult(object):
@@ -324,9 +331,6 @@ def __call__(self, *args, **kwargs):
     def call_and_shelve(self, *args, **kwargs):
         return NotMemorizedResult(self.func(*args, **kwargs))
 
-    def __reduce__(self):
-        return (self.__class__, (self.func,))
-
     def __repr__(self):
         return '{0}(func={1})'.format(self.__class__.__name__, self.func)
 
@@ -386,6 +390,7 @@ def __init__(self, func, location, backend='local', ignore=None,
         self.mmap_mode = mmap_mode
         self.compress = compress
         self.func = func
+
         if ignore is None:
             ignore = []
         self.ignore = ignore
@@ -421,28 +426,44 @@ def __init__(self, func, location, backend='local', ignore=None,
             doc = func.__doc__
         self.__doc__ = 'Memoized version of %s' % doc
 
-    def _cached_call(self, args, kwargs):
+    def _cached_call(self, args, kwargs, shelving=False):
         """Call wrapped function and cache result, or read cache if available.
 
         This function returns the wrapped function output and some metadata.
 
+        Arguments:
+        ----------
+
+        args, kwargs: list and dict
+            input arguments for wrapped function
+
+        shelving: bool
+            True when called via the call_and_shelve function.
+
+
         Returns
         -------
-        output: value or tuple
-            what is returned by wrapped function
+        output: value or tuple or None
+            Output of the wrapped function.
+            If shelving is True and the call has been already cached,
+            output is None.
 
         argument_hash: string
-            hash of function arguments
+            Hash of function arguments.
 
         metadata: dict
-            some metadata about wrapped function call (see _persist_input())
+            Some metadata about wrapped function call (see _persist_input()).
         """
-        # Compare the function code with the previous to see if the
-        # function code has changed
         func_id, args_id = self._get_output_identifiers(*args, **kwargs)
         metadata = None
         msg = None
+
+        # Wether or not the memorized function must be called
+        must_call = False
+
         # FIXME: The statements below should be try/excepted
+        # Compare the function code with the previous to see if the
+        # function code has changed
         if not (self._check_previous_func_code(stacklevel=4) and
                 self.store_backend.contains_item([func_id, args_id])):
             if self._verbose > 10:
@@ -452,16 +473,7 @@ def _cached_call(self, args, kwargs):
                           .format(name, args_id,
                                   self.store_backend.
                                   get_cached_func_info([func_id])['location']))
-            out, metadata = self.call(*args, **kwargs)
-            if self.mmap_mode is not None:
-                # Memmap the output at the first call to be consistent with
-                # later calls
-                if self._verbose:
-                    msg = _format_load_msg(func_id, args_id,
-                                           timestamp=self.timestamp,
-                                           metadata=metadata)
-                out = self.store_backend.load_item([func_id, args_id], msg=msg,
-                                                   verbose=self._verbose)
+            must_call = True
         else:
             try:
                 t0 = time.time()
@@ -469,8 +481,16 @@ def _cached_call(self, args, kwargs):
                     msg = _format_load_msg(func_id, args_id,
                                            timestamp=self.timestamp,
                                            metadata=metadata)
-                out = self.store_backend.load_item([func_id, args_id], msg=msg,
-                                                   verbose=self._verbose)
+
+                if not shelving:
+                    # When shelving, we do not need to load the output
+                    out = self.store_backend.load_item(
+                        [func_id, args_id],
+                        msg=msg,
+                        verbose=self._verbose)
+                else:
+                    out = None
+
                 if self._verbose > 4:
                     t = time.time() - t0
                     _, name = get_func_name(self.func)
@@ -482,8 +502,19 @@ def _cached_call(self, args, kwargs):
                 self.warn('Exception while loading results for '
                           '{}\n {}'.format(signature, traceback.format_exc()))
 
-                out, metadata = self.call(*args, **kwargs)
-                args_id = None
+                must_call = True
+
+        if must_call:
+            out, metadata = self.call(*args, **kwargs)
+            if self.mmap_mode is not None:
+                # Memmap the output at the first call to be consistent with
+                # later calls
+                if self._verbose:
+                    msg = _format_load_msg(func_id, args_id,
+                                           timestamp=self.timestamp,
+                                           metadata=metadata)
+                out = self.store_backend.load_item([func_id, args_id], msg=msg,
+                                                   verbose=self._verbose)
 
         return (out, args_id, metadata)
 
@@ -502,7 +533,7 @@ def call_and_shelve(self, *args, **kwargs):
             class "NotMemorizedResult" is used when there is no cache
             activated (e.g. location=None in Memory).
         """
-        _, args_id, metadata = self._cached_call(args, kwargs)
+        _, args_id, metadata = self._cached_call(args, kwargs, shelving=True)
         return MemorizedResult(self.store_backend, self.func, args_id,
                                metadata=metadata, verbose=self._verbose - 1,
                                timestamp=self.timestamp)
@@ -510,13 +541,13 @@ class "NotMemorizedResult" is used when there is no cache
     def __call__(self, *args, **kwargs):
         return self._cached_call(args, kwargs)[0]
 
-    def __reduce__(self):
+    def __getstate__(self):
         """ We don't store the timestamp when pickling, to avoid the hash
             depending from it.
-            In addition, when unpickling, we run the __init__
         """
-        return (self.__class__, (self.func, self.store_backend, self.ignore,
-                self.mmap_mode, self.compress, self._verbose))
+        state = self.__dict__.copy()
+        state['timestamp'] = None
+        return state
 
     # ------------------------------------------------------------------------
     # Private interface
@@ -744,9 +775,10 @@ def _persist_input(self, duration, args, kwargs, this_duration_limit=0.5):
     # ------------------------------------------------------------------------
 
     def __repr__(self):
-        return ("{0}(func={1}, location={2})".format(self.__class__.__name__,
-                                                     self.func,
-                                                     self.store_backend,))
+        return '{class_name}(func={func}, location={location})'.format(
+            class_name=self.__class__.__name__,
+            func=self.func,
+            location=self.store_backend.location,)
 
 
 ###############################################################################
@@ -809,7 +841,7 @@ class Memory(Logger):
 
     def __init__(self, location=None, backend='local', cachedir=None,
                  mmap_mode=None, compress=False, verbose=1, bytes_limit=None,
-                 backend_options={}):
+                 backend_options=None):
         # XXX: Bad explanation of the None value of cachedir
         Logger.__init__(self)
         self._verbose = verbose
@@ -817,6 +849,11 @@ def __init__(self, location=None, backend='local', cachedir=None,
         self.timestamp = time.time()
         self.bytes_limit = bytes_limit
         self.backend = backend
+        self.compress = compress
+        if backend_options is None:
+            backend_options = {}
+        self.backend_options = backend_options
+
         if compress and mmap_mode is not None:
             warnings.warn('Compressed results cannot be memmapped',
                           stacklevel=2)
@@ -896,9 +933,11 @@ def cache(self, func=None, ignore=None, verbose=None, mmap_mode=False):
             mmap_mode = self.mmap_mode
         if isinstance(func, MemorizedFunc):
             func = func.func
-        return MemorizedFunc(func, self.store_backend, mmap_mode=mmap_mode,
-                             ignore=ignore, verbose=verbose,
-                             timestamp=self.timestamp)
+        return MemorizedFunc(func, location=self.store_backend,
+                             backend=self.backend,
+                             ignore=ignore, mmap_mode=mmap_mode,
+                             compress=self.compress,
+                             verbose=verbose, timestamp=self.timestamp)
 
     def clear(self, warn=True):
         """ Erase the complete cache directory.
@@ -931,19 +970,15 @@ def eval(self, func, *args, **kwargs):
     # ------------------------------------------------------------------------
 
     def __repr__(self):
-        return '{0}(location={1})'.format(
-            self.__class__.__name__, (repr(None) if self.store_backend is None
-                                      else repr(self.store_backend)))
+        return '{class_name}(location={location})'.format(
+            class_name=self.__class__.__name__,
+            location=(None if self.store_backend is None
+                      else self.store_backend.location))
 
-    def __reduce__(self):
+    def __getstate__(self):
         """ We don't store the timestamp when pickling, to avoid the hash
             depending from it.
-            In addition, when unpickling, we run the __init__
         """
-        # We need to remove 'joblib' from the end of cachedir
-        location = (repr(self.store_backend)[:-7]
-                    if self.store_backend is not None else None)
-        compress = self.store_backend.compress \
-            if self.store_backend is not None else False
-        return (self.__class__, (location, self.backend, self.mmap_mode,
-                                 compress, self._verbose))
+        state = self.__dict__.copy()
+        state['timestamp'] = None
+        return state
diff --git a/sklearn/externals/joblib/numpy_pickle.py b/sklearn/externals/joblib/numpy_pickle.py
index 496429f50f..bae0df31fa 100755
--- a/sklearn/externals/joblib/numpy_pickle.py
+++ b/sklearn/externals/joblib/numpy_pickle.py
@@ -435,7 +435,9 @@ def dump(value, filename, compress=0, protocol=None, cache_size=None):
     else:
         compress_level = compress
 
-    if compress_method == 'lz4' and lz4 is None:
+    # LZ4 compression is only supported and installation checked with
+    # python 3+.
+    if compress_method == 'lz4' and lz4 is None and PY3_OR_LATER:
         raise ValueError(LZ4_NOT_INSTALLED_ERROR)
 
     if (compress_level is not None and
diff --git a/sklearn/externals/joblib/parallel.py b/sklearn/externals/joblib/parallel.py
index 8be0ed2c6b..6cca94cde6 100755
--- a/sklearn/externals/joblib/parallel.py
+++ b/sklearn/externals/joblib/parallel.py
@@ -196,11 +196,11 @@ def unregister(self):
 # to set an environment variable to switch the default start method from
 # 'fork' to 'forkserver' or 'spawn' to avoid this issue albeit at the cost
 # of causing semantic changes and some additional pool instantiation overhead.
+DEFAULT_MP_CONTEXT = None
 if hasattr(mp, 'get_context'):
     method = os.environ.get('JOBLIB_START_METHOD', '').strip() or None
-    DEFAULT_MP_CONTEXT = mp.get_context(method=method)
-else:
-    DEFAULT_MP_CONTEXT = None
+    if method is not None:
+        DEFAULT_MP_CONTEXT = mp.get_context(method=method)
 
 
 class CloudpickledObjectWrapper(object):
@@ -679,6 +679,8 @@ def __init__(self, n_jobs=None, backend=None, verbose=0, timeout=None,
         )
         if DEFAULT_MP_CONTEXT is not None:
             self._backend_args['context'] = DEFAULT_MP_CONTEXT
+        elif hasattr(mp, "get_context"):
+            self._backend_args['context'] = mp.get_context()
 
         if backend is None:
             backend = active_backend
diff --git a/sklearn/feature_selection/from_model.py b/sklearn/feature_selection/from_model.py
index f6f9c2776b..3e2efdbeb1 100755
--- a/sklearn/feature_selection/from_model.py
+++ b/sklearn/feature_selection/from_model.py
@@ -16,12 +16,13 @@ def _get_feature_importances(estimator, norm_order=1):
     """Retrieve or aggregate feature importances from estimator"""
     importances = getattr(estimator, "feature_importances_", None)
 
-    if importances is None and hasattr(estimator, "coef_"):
+    coef_ = getattr(estimator, "coef_", None)
+    if importances is None and coef_ is not None:
         if estimator.coef_.ndim == 1:
-            importances = np.abs(estimator.coef_)
+            importances = np.abs(coef_)
 
         else:
-            importances = np.linalg.norm(estimator.coef_, axis=0,
+            importances = np.linalg.norm(coef_, axis=0,
                                          ord=norm_order)
 
     elif importances is None:
diff --git a/sklearn/feature_selection/rfe.py b/sklearn/feature_selection/rfe.py
index b6df8ad962..6aa4c10174 100755
--- a/sklearn/feature_selection/rfe.py
+++ b/sklearn/feature_selection/rfe.py
@@ -377,10 +377,11 @@ class RFECV(RFE, MetaEstimatorMixin):
     verbose : int, (default=0)
         Controls verbosity of output.
 
-    n_jobs : int, (default=1)
+    n_jobs : int or None, optional (default=None)
         Number of cores to run in parallel while fitting across folds.
-        Defaults to 1 core. If ``n_jobs=-1``, then number of jobs is set
-        to number of cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
diff --git a/sklearn/feature_selection/tests/test_from_model.py b/sklearn/feature_selection/tests/test_from_model.py
index e6bb76c5e1..47e62eb8e7 100755
--- a/sklearn/feature_selection/tests/test_from_model.py
+++ b/sklearn/feature_selection/tests/test_from_model.py
@@ -8,7 +8,6 @@
 from sklearn.utils.testing import assert_greater
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_array_equal
-from sklearn.utils.testing import assert_almost_equal
 from sklearn.utils.testing import assert_allclose
 from sklearn.utils.testing import assert_raises
 from sklearn.utils.testing import skip_if_32bit
@@ -178,6 +177,8 @@ def test_feature_importances():
         assert_array_almost_equal(X_new, X[:, feature_mask])
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_sample_weight():
     # Ensure sample weights are passed to underlying estimator
     X, y = datasets.make_classification(
@@ -214,6 +215,8 @@ def test_coef_default_threshold():
     assert_array_almost_equal(X_new, X[:, mask])
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @skip_if_32bit
 def test_2d_coef():
     X, y = datasets.make_classification(
diff --git a/sklearn/feature_selection/univariate_selection.py b/sklearn/feature_selection/univariate_selection.py
index 3a553370e0..6534667339 100755
--- a/sklearn/feature_selection/univariate_selection.py
+++ b/sklearn/feature_selection/univariate_selection.py
@@ -438,8 +438,7 @@ def _get_support_mask(self):
             return np.zeros(len(self.scores_), dtype=np.bool)
 
         scores = _clean_nans(self.scores_)
-        threshold = stats.scoreatpercentile(scores,
-                                            100 - self.percentile)
+        threshold = np.percentile(scores, 100 - self.percentile)
         mask = scores > threshold
         ties = np.where(scores == threshold)[0]
         if len(ties):
diff --git a/sklearn/gaussian_process/gpc.py b/sklearn/gaussian_process/gpc.py
index 2389a2c5ef..bca6bc506d 100755
--- a/sklearn/gaussian_process/gpc.py
+++ b/sklearn/gaussian_process/gpc.py
@@ -534,11 +534,11 @@ def optimizer(obj_func, initial_theta, bounds):
         Note that "one_vs_one" does not support predicting probability
         estimates.
 
-    n_jobs : int, optional, default: 1
-        The number of jobs to use for the computation. If -1 all CPUs are used.
-        If 1 is given, no parallel computing code is used at all, which is
-        useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are
-        used. Thus for n_jobs = -2, all CPUs but one are used.
+    n_jobs : int or None, optional (default=None)
+        The number of jobs to use for the computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -558,6 +558,21 @@ def optimizer(obj_func, initial_theta, bounds):
     n_classes_ : int
         The number of classes in the training data
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_iris
+    >>> from sklearn.gaussian_process import GaussianProcessClassifier
+    >>> from sklearn.gaussian_process.kernels import RBF
+    >>> X, y = load_iris(return_X_y=True)
+    >>> kernel = 1.0 * RBF(1.0)
+    >>> gpc = GaussianProcessClassifier(kernel=kernel,
+    ...         random_state=0).fit(X, y)
+    >>> gpc.score(X, y) # doctest: +ELLIPSIS
+    0.9866...
+    >>> gpc.predict_proba(X[:2,:])
+    array([[0.83548752, 0.03228706, 0.13222543],
+           [0.79064206, 0.06525643, 0.14410151]])
+
     .. versionadded:: 0.18
     """
     def __init__(self, kernel=None, optimizer="fmin_l_bfgs_b",
diff --git a/sklearn/gaussian_process/gpr.py b/sklearn/gaussian_process/gpr.py
index 27043c46dd..ac2c0a46b6 100755
--- a/sklearn/gaussian_process/gpr.py
+++ b/sklearn/gaussian_process/gpr.py
@@ -132,6 +132,20 @@ def optimizer(obj_func, initial_theta, bounds):
     log_marginal_likelihood_value_ : float
         The log-marginal-likelihood of ``self.kernel_.theta``
 
+    Examples
+    --------
+    >>> from sklearn.datasets import make_friedman2
+    >>> from sklearn.gaussian_process import GaussianProcessRegressor
+    >>> from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel
+    >>> X, y = make_friedman2(n_samples=500, noise=0, random_state=0)
+    >>> kernel = DotProduct() + WhiteKernel()
+    >>> gpr = GaussianProcessRegressor(kernel=kernel,
+    ...         random_state=0).fit(X, y)
+    >>> gpr.score(X, y) # doctest: +ELLIPSIS
+    0.3680...
+    >>> gpr.predict(X[:2,:], return_std=True) # doctest: +ELLIPSIS
+    (array([653.0..., 592.1...]), array([316.6..., 316.6...]))
+
     """
     def __init__(self, kernel=None, alpha=1e-10,
                  optimizer="fmin_l_bfgs_b", n_restarts_optimizer=0,
diff --git a/sklearn/linear_model/base.py b/sklearn/linear_model/base.py
index e626c84f4c..29734a2135 100755
--- a/sklearn/linear_model/base.py
+++ b/sklearn/linear_model/base.py
@@ -381,10 +381,12 @@ class LinearRegression(LinearModel, RegressorMixin):
     copy_X : boolean, optional, default True
         If True, X will be copied; else, it may be overwritten.
 
-    n_jobs : int, optional, default 1
-        The number of jobs to use for the computation.
-        If -1 all CPUs are used. This will only provide speedup for
-        n_targets > 1 and sufficient large problems.
+    n_jobs : int or None, optional (default=None)
+        The number of jobs to use for the computation. This will only provide
+        speedup for n_targets > 1 and sufficient large problems.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -397,6 +399,23 @@ class LinearRegression(LinearModel, RegressorMixin):
     intercept_ : array
         Independent term in the linear model.
 
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.linear_model import LinearRegression
+    >>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])
+    >>> # y = 1 * x_0 + 2 * x_1 + 3
+    >>> y = np.dot(X, np.array([1, 2])) + 3
+    >>> reg = LinearRegression().fit(X, y)
+    >>> reg.score(X, y)
+    1.0
+    >>> reg.coef_
+    array([1., 2.])
+    >>> reg.intercept_ # doctest: +ELLIPSIS
+    3.0000...
+    >>> reg.predict(np.array([[3, 5]]))
+    array([16.])
+
     Notes
     -----
     From the implementation point of view, this is just plain Ordinary
diff --git a/sklearn/linear_model/cd_fast.pyx b/sklearn/linear_model/cd_fast.pyx
index d923b01d14..c75ad0f667 100755
--- a/sklearn/linear_model/cd_fast.pyx
+++ b/sklearn/linear_model/cd_fast.pyx
@@ -141,10 +141,10 @@ cdef extern from "cblas.h":
 @cython.boundscheck(False)
 @cython.wraparound(False)
 @cython.cdivision(True)
-def enet_coordinate_descent(np.ndarray[floating, ndim=1, mode='c'] w,
+def enet_coordinate_descent(floating[::1] w,
                             floating alpha, floating beta,
-                            np.ndarray[floating, ndim=2, mode='fortran'] X,
-                            np.ndarray[floating, ndim=1, mode='c'] y,
+                            floating[::1, :] X,
+                            floating[::1] y,
                             int max_iter, floating tol,
                             object rng, bint random=0, bint positive=0):
     """Cython version of the coordinate descent algorithm
@@ -159,25 +159,29 @@ def enet_coordinate_descent(np.ndarray[floating, ndim=1, mode='c'] w,
     # fused types version of BLAS functions
     if floating is float:
         dtype = np.float32
+        gemv = sgemv
         dot = sdot
         axpy = saxpy
         asum = sasum
+        copy = scopy
     else:
         dtype = np.float64
+        gemv = dgemv
         dot = ddot
         axpy = daxpy
         asum = dasum
+        copy = dcopy
 
     # get the data information into easy vars
     cdef unsigned int n_samples = X.shape[0]
     cdef unsigned int n_features = X.shape[1]
 
     # compute norms of the columns of X
-    cdef np.ndarray[floating, ndim=1] norm_cols_X = (X**2).sum(axis=0)
+    cdef floating[::1] norm_cols_X = np.square(X).sum(axis=0)
 
     # initial value of the residuals
-    cdef np.ndarray[floating, ndim=1] R = np.empty(n_samples, dtype=dtype)
-    cdef np.ndarray[floating, ndim=1] XtA = np.empty(n_features, dtype=dtype)
+    cdef floating[::1] R = np.empty(n_samples, dtype=dtype)
+    cdef floating[::1] XtA = np.empty(n_features, dtype=dtype)
 
     cdef floating tmp
     cdef floating w_ii
@@ -199,23 +203,20 @@ def enet_coordinate_descent(np.ndarray[floating, ndim=1, mode='c'] w,
     cdef UINT32_t rand_r_state_seed = rng.randint(0, RAND_R_MAX)
     cdef UINT32_t* rand_r_state = &rand_r_state_seed
 
-    cdef floating *X_data = <floating*> X.data
-    cdef floating *y_data = <floating*> y.data
-    cdef floating *w_data = <floating*> w.data
-    cdef floating *R_data = <floating*> R.data
-    cdef floating *XtA_data = <floating*> XtA.data
-
     if alpha == 0 and beta == 0:
         warnings.warn("Coordinate descent with no regularization may lead to unexpected"
             " results and is discouraged.")
 
     with nogil:
         # R = y - np.dot(X, w)
-        for i in range(n_samples):
-            R[i] = y[i] - dot(n_features, &X_data[i], n_samples, w_data, 1)
+        copy(n_samples, &y[0], 1, &R[0], 1)
+        gemv(CblasColMajor, CblasNoTrans,
+             n_samples, n_features, -1.0, &X[0, 0], n_samples,
+             &w[0], 1,
+             1.0, &R[0], 1)
 
         # tol *= np.dot(y, y)
-        tol *= dot(n_samples, y_data, 1, y_data, 1)
+        tol *= dot(n_samples, &y[0], 1, &y[0], 1)
 
         for n_iter in range(max_iter):
             w_max = 0.0
@@ -233,11 +234,10 @@ def enet_coordinate_descent(np.ndarray[floating, ndim=1, mode='c'] w,
 
                 if w_ii != 0.0:
                     # R += w_ii * X[:,ii]
-                    axpy(n_samples, w_ii, &X_data[ii * n_samples], 1,
-                         R_data, 1)
+                    axpy(n_samples, w_ii, &X[0, ii], 1, &R[0], 1)
 
                 # tmp = (X[:,ii]*R).sum()
-                tmp = dot(n_samples, &X_data[ii * n_samples], 1, R_data, 1)
+                tmp = dot(n_samples, &X[0, ii], 1, &R[0], 1)
 
                 if positive and tmp < 0:
                     w[ii] = 0.0
@@ -247,16 +247,13 @@ def enet_coordinate_descent(np.ndarray[floating, ndim=1, mode='c'] w,
 
                 if w[ii] != 0.0:
                     # R -=  w[ii] * X[:,ii] # Update residual
-                    axpy(n_samples, -w[ii], &X_data[ii * n_samples], 1,
-                         R_data, 1)
+                    axpy(n_samples, -w[ii], &X[0, ii], 1, &R[0], 1)
 
                 # update the maximum absolute coefficient update
                 d_w_ii = fabs(w[ii] - w_ii)
-                if d_w_ii > d_w_max:
-                    d_w_max = d_w_ii
+                d_w_max = fmax(d_w_max, d_w_ii)
 
-                if fabs(w[ii]) > w_max:
-                    w_max = fabs(w[ii])
+                w_max = fmax(w_max, fabs(w[ii]))
 
             if (w_max == 0.0 or
                 d_w_max / w_max < d_w_tol or
@@ -267,19 +264,19 @@ def enet_coordinate_descent(np.ndarray[floating, ndim=1, mode='c'] w,
 
                 # XtA = np.dot(X.T, R) - beta * w
                 for i in range(n_features):
-                    XtA[i] = dot(n_samples, &X_data[i * n_samples],
-                                 1, R_data, 1) - beta * w[i]
+                    XtA[i] = (dot(n_samples, &X[0, i], 1, &R[0], 1)
+                              - beta * w[i])
 
                 if positive:
-                    dual_norm_XtA = max(n_features, XtA_data)
+                    dual_norm_XtA = max(n_features, &XtA[0])
                 else:
-                    dual_norm_XtA = abs_max(n_features, XtA_data)
+                    dual_norm_XtA = abs_max(n_features, &XtA[0])
 
                 # R_norm2 = np.dot(R, R)
-                R_norm2 = dot(n_samples, R_data, 1, R_data, 1)
+                R_norm2 = dot(n_samples, &R[0], 1, &R[0], 1)
 
                 # w_norm2 = np.dot(w, w)
-                w_norm2 = dot(n_features, w_data, 1, w_data, 1)
+                w_norm2 = dot(n_features, &w[0], 1, &w[0], 1)
 
                 if (dual_norm_XtA > alpha):
                     const = alpha / dual_norm_XtA
@@ -289,11 +286,11 @@ def enet_coordinate_descent(np.ndarray[floating, ndim=1, mode='c'] w,
                     const = 1.0
                     gap = R_norm2
 
-                l1_norm = asum(n_features, w_data, 1)
+                l1_norm = asum(n_features, &w[0], 1)
 
                 # np.dot(R.T, y)
                 gap += (alpha * l1_norm
-                        - const * dot(n_samples, R_data, 1, y_data, 1)
+                        - const * dot(n_samples, &R[0], 1, &y[0], 1)
                         + 0.5 * beta * (1 + const ** 2) * (w_norm2))
 
                 if gap < tol:
diff --git a/sklearn/linear_model/coordinate_descent.py b/sklearn/linear_model/coordinate_descent.py
index 09736927be..2d0723944b 100755
--- a/sklearn/linear_model/coordinate_descent.py
+++ b/sklearn/linear_model/coordinate_descent.py
@@ -1320,9 +1320,11 @@ class LassoCV(LinearModelCV, RegressorMixin):
     verbose : bool or integer
         Amount of verbosity.
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the cross validation. If ``-1``, use
-        all the CPUs.
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the cross validation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     positive : bool, optional
         If positive, restrict regression coefficients to be positive
@@ -1366,6 +1368,17 @@ class LassoCV(LinearModelCV, RegressorMixin):
         number of iterations run by the coordinate descent solver to reach
         the specified tolerance for the optimal alpha.
 
+    Examples
+    --------
+    >>> from sklearn.linear_model import LassoCV
+    >>> from sklearn.datasets import make_regression
+    >>> X, y = make_regression(noise=4, random_state=0)
+    >>> reg = LassoCV(cv=5, random_state=0).fit(X, y)
+    >>> reg.score(X, y) # doctest: +ELLIPSIS
+    0.9993...
+    >>> reg.predict(X[:1,])
+    array([-78.4951...])
+
     Notes
     -----
     For an example, see
@@ -1480,9 +1493,11 @@ class ElasticNetCV(LinearModelCV, RegressorMixin):
     verbose : bool or integer
         Amount of verbosity.
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the cross validation. If ``-1``, use
-        all the CPUs.
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the cross validation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     positive : bool, optional
         When set to ``True``, forces the coefficients to be positive.
@@ -2014,10 +2029,12 @@ class MultiTaskElasticNetCV(LinearModelCV, RegressorMixin):
     verbose : bool or integer
         Amount of verbosity.
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the cross validation. If ``-1``, use
-        all the CPUs. Note that this is used only if multiple values for
-        l1_ratio are given.
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the cross validation. Note that this is
+        used only if multiple values for l1_ratio are given.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional, default None
         The seed of the pseudo random number generator that selects a random
@@ -2186,10 +2203,12 @@ class MultiTaskLassoCV(LinearModelCV, RegressorMixin):
     verbose : bool or integer
         Amount of verbosity.
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the cross validation. If ``-1``, use
-        all the CPUs. Note that this is used only if multiple values for
-        l1_ratio are given.
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the cross validation. Note that this is
+        used only if multiple values for l1_ratio are given.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional, default None
         The seed of the pseudo random number generator that selects a random
@@ -2227,6 +2246,19 @@ class MultiTaskLassoCV(LinearModelCV, RegressorMixin):
         number of iterations run by the coordinate descent solver to reach
         the specified tolerance for the optimal alpha.
 
+    Examples
+    --------
+    >>> from sklearn.linear_model import MultiTaskLassoCV
+    >>> from sklearn.datasets import make_regression
+    >>> X, y = make_regression(n_targets=2, noise=4, random_state=0)
+    >>> reg = MultiTaskLassoCV(cv=5, random_state=0).fit(X, y)
+    >>> reg.score(X, y) # doctest: +ELLIPSIS
+    0.9994...
+    >>> reg.alpha_
+    0.5713...
+    >>> reg.predict(X[:1,])
+    array([[153.7971...,  94.9015...]])
+
     See also
     --------
     MultiTaskElasticNet
diff --git a/sklearn/linear_model/huber.py b/sklearn/linear_model/huber.py
index b6f4658ea5..3270b5d221 100755
--- a/sklearn/linear_model/huber.py
+++ b/sklearn/linear_model/huber.py
@@ -192,6 +192,29 @@ class HuberRegressor(LinearModel, RegressorMixin, BaseEstimator):
         A boolean mask which is set to True where the samples are identified
         as outliers.
 
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.linear_model import HuberRegressor, LinearRegression
+    >>> from sklearn.datasets import make_regression
+    >>> np.random.seed(0)
+    >>> X, y, coef = make_regression(
+    ...     n_samples=200, n_features=2, noise=4.0, coef=True, random_state=0)
+    >>> X[:4] = np.random.uniform(10, 20, (4, 2))
+    >>> y[:4] = np.random.uniform(10, 20, 4)
+    >>> huber = HuberRegressor().fit(X, y)
+    >>> huber.score(X, y) # doctest: +ELLIPSIS
+    -7.284608623514573
+    >>> huber.predict(X[:1,])
+    array([806.7200...])
+    >>> linear = LinearRegression().fit(X, y)
+    >>> print("True coefficients:", coef)
+    True coefficients: [20.4923...  34.1698...]
+    >>> print("Huber coefficients:", huber.coef_)
+    Huber coefficients: [17.7906... 31.0106...]
+    >>> print("Linear Regression coefficients:", linear.coef_)
+    Linear Regression coefficients: [-1.9221...  7.0226...]
+
     References
     ----------
     .. [1] Peter J. Huber, Elvezio M. Ronchetti, Robust Statistics
diff --git a/sklearn/linear_model/least_angle.py b/sklearn/linear_model/least_angle.py
index dd47030308..ce13b99b6a 100755
--- a/sklearn/linear_model/least_angle.py
+++ b/sklearn/linear_model/least_angle.py
@@ -1022,9 +1022,11 @@ class LarsCV(Lars):
         The maximum number of points on the path used to compute the
         residuals in the cross-validation
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the cross validation. If ``-1``, use
-        all the CPUs
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the cross validation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     eps : float, optional
         The machine-precision regularization in the computation of the
@@ -1068,6 +1070,19 @@ class LarsCV(Lars):
     n_iter_ : array-like or int
         the number of iterations run by Lars with the optimal alpha.
 
+    Examples
+    --------
+    >>> from sklearn.linear_model import LarsCV
+    >>> from sklearn.datasets import make_regression
+    >>> X, y = make_regression(n_samples=200, noise=4.0, random_state=0)
+    >>> reg = LarsCV(cv=5).fit(X, y)
+    >>> reg.score(X, y) # doctest: +ELLIPSIS
+    0.9996...
+    >>> reg.alpha_
+    0.0254...
+    >>> reg.predict(X[:1,])
+    array([154.0842...])
+
     See also
     --------
     lars_path, LassoLars, LassoLarsCV
@@ -1235,9 +1250,11 @@ class LassoLarsCV(LarsCV):
         The maximum number of points on the path used to compute the
         residuals in the cross-validation
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the cross validation. If ``-1``, use
-        all the CPUs
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the cross validation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     eps : float, optional
         The machine-precision regularization in the computation of the
@@ -1286,6 +1303,19 @@ class LassoLarsCV(LarsCV):
     n_iter_ : array-like or int
         the number of iterations run by Lars with the optimal alpha.
 
+    Examples
+    --------
+    >>> from sklearn.linear_model import LassoLarsCV
+    >>> from sklearn.datasets import make_regression
+    >>> X, y = make_regression(noise=4.0, random_state=0)
+    >>> reg = LassoLarsCV(cv=5).fit(X, y)
+    >>> reg.score(X, y) # doctest: +ELLIPSIS
+    0.9992...
+    >>> reg.alpha_
+    0.0484...
+    >>> reg.predict(X[:1,])
+    array([-77.8723...])
+
     Notes
     -----
 
diff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py
index 855d097050..01a4f78ab0 100755
--- a/sklearn/linear_model/logistic.py
+++ b/sklearn/linear_model/logistic.py
@@ -32,7 +32,7 @@
 from ..exceptions import (NotFittedError, ConvergenceWarning,
                           ChangedBehaviorWarning)
 from ..utils.multiclass import check_classification_targets
-from ..utils import Parallel, delayed
+from ..utils import Parallel, delayed, effective_n_jobs
 from ..model_selection import check_cv
 from ..externals import six
 from ..metrics import get_scorer
@@ -424,35 +424,61 @@ def hessp(v):
     return grad, hessp
 
 
-def _check_solver_option(solver, multi_class, penalty, dual):
-    if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:
-        raise ValueError("Logistic Regression supports only liblinear, "
-                         "newton-cg, lbfgs, sag and saga solvers, got %s"
-                         % solver)
-
-    if multi_class not in ['multinomial', 'ovr']:
-        raise ValueError("multi_class should be either multinomial or "
-                         "ovr, got %s" % multi_class)
-
+def _check_solver(solver, penalty, dual):
+    if solver == 'warn':
+        solver = 'liblinear'
+        warnings.warn("Default solver will be changed to 'lbfgs' in 0.22. "
+                      "Specify a solver to silence this warning.",
+                      FutureWarning)
+
+    all_solvers = ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']
+    if solver not in all_solvers:
+        raise ValueError("Logistic Regression supports only solvers in %s, got"
+                         " %s." % (all_solvers, solver))
+
+    all_penalties = ['l1', 'l2']
+    if penalty not in all_penalties:
+        raise ValueError("Logistic Regression supports only penalties in %s,"
+                         " got %s." % (all_penalties, penalty))
+
+    if solver not in ['liblinear', 'saga'] and penalty != 'l2':
+        raise ValueError("Solver %s supports only l2 penalties, "
+                         "got %s penalty." % (solver, penalty))
+    if solver != 'liblinear' and dual:
+        raise ValueError("Solver %s supports only "
+                         "dual=False, got dual=%s" % (solver, dual))
+    return solver
+
+
+def _check_multi_class(multi_class, solver, n_classes):
+    if multi_class == 'warn':
+        multi_class = 'ovr'
+        if n_classes > 2:
+            warnings.warn("Default multi_class will be changed to 'auto' in"
+                          " 0.22. Specify the multi_class option to silence "
+                          "this warning.", FutureWarning)
+    if multi_class == 'auto':
+        if solver == 'liblinear':
+            multi_class = 'ovr'
+        elif n_classes > 2:
+            multi_class = 'multinomial'
+        else:
+            multi_class = 'ovr'
+    if multi_class not in ('multinomial', 'ovr'):
+        raise ValueError("multi_class should be 'multinomial', 'ovr' or "
+                         "'auto'. Got %s." % multi_class)
     if multi_class == 'multinomial' and solver == 'liblinear':
         raise ValueError("Solver %s does not support "
                          "a multinomial backend." % solver)
+    return multi_class
 
-    if solver not in ['liblinear', 'saga']:
-        if penalty != 'l2':
-            raise ValueError("Solver %s supports only l2 penalties, "
-                             "got %s penalty." % (solver, penalty))
-    if solver != 'liblinear':
-        if dual:
-            raise ValueError("Solver %s supports only "
-                             "dual=False, got dual=%s" % (solver, dual))
 
 
 def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
                              max_iter=100, tol=1e-4, verbose=0,
                              solver='lbfgs', coef=None,
                              class_weight=None, dual=False, penalty='l2',
-                             intercept_scaling=1., multi_class='ovr',
+                             intercept_scaling=1., multi_class='warn',
                              random_state=None, check_input=True,
                              max_squared_sum=None, sample_weight=None):
     """Compute a Logistic Regression model for a list of regularization
@@ -471,7 +497,7 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
     X : array-like or sparse matrix, shape (n_samples, n_features)
         Input data.
 
-    y : array-like, shape (n_samples,)
+    y : array-like, shape (n_samples,) or (n_samples, n_targets)
         Input data, target values.
 
     pos_class : int, None
@@ -540,12 +566,18 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
         To lessen the effect of regularization on synthetic feature weight
         (and therefore on the intercept) intercept_scaling has to be increased.
 
-    multi_class : str, {'ovr', 'multinomial'}
-        Multiclass option can be either 'ovr' or 'multinomial'. If the option
-        chosen is 'ovr', then a binary problem is fit for each label. Else
-        the loss minimised is the multinomial loss fit across
-        the entire probability distribution. Does not work for 'liblinear'
-        solver.
+    multi_class : str, {'ovr', 'multinomial', 'auto'}, default: 'ovr'
+        If the option chosen is 'ovr', then a binary problem is fit for each
+        label. For 'multinomial' the loss minimised is the multinomial loss fit
+        across the entire probability distribution, *even when the data is
+        binary*. 'multinomial' is unavailable when solver='liblinear'.
+        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',
+        and otherwise selects 'multinomial'.
+
+        .. versionadded:: 0.18
+           Stochastic Average Gradient descent solver for 'multinomial' case.
+        .. versionchanged:: 0.20
+            Default will change from 'ovr' to 'auto' in 0.22.
 
     random_state : int, RandomState instance or None, optional, default None
         The seed of the pseudo random number generator to use when shuffling
@@ -593,7 +625,7 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
     if isinstance(Cs, numbers.Integral):
         Cs = np.logspace(-4, 4, Cs)
 
-    _check_solver_option(solver, multi_class, penalty, dual)
+    solver = _check_solver(solver, penalty, dual)
 
     # Preprocessing.
     if check_input:
@@ -602,9 +634,11 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
         y = check_array(y, ensure_2d=False, dtype=None)
         check_consistent_length(X, y)
     _, n_features = X.shape
+
     classes = np.unique(y)
     random_state = check_random_state(random_state)
 
+    multi_class = _check_multi_class(multi_class, solver, len(classes))
     if pos_class is None and multi_class != 'multinomial':
         if (classes.size > 2):
             raise ValueError('To fit OvR, use the pos_class argument')
@@ -761,8 +795,9 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
                              "'newton-cg', 'sag'}, got '%s' instead" % solver)
 
         if multi_class == 'multinomial':
-            multi_w0 = np.reshape(w0, (classes.size, -1))
-            if classes.size == 2:
+            n_classes = max(2, classes.size)
+            multi_w0 = np.reshape(w0, (n_classes, -1))
+            if n_classes == 2:
                 multi_w0 = multi_w0[1][np.newaxis, :]
             coefs.append(multi_w0.copy())
         else:
@@ -779,7 +814,7 @@ def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,
                           max_iter=100, tol=1e-4, class_weight=None,
                           verbose=0, solver='lbfgs', penalty='l2',
                           dual=False, intercept_scaling=1.,
-                          multi_class='ovr', random_state=None,
+                          multi_class='warn', random_state=None,
                           max_squared_sum=None, sample_weight=None):
     """Computes scores across logistic_regression_path
 
@@ -864,11 +899,10 @@ def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,
         (and therefore on the intercept) intercept_scaling has to be increased.
 
     multi_class : str, {'ovr', 'multinomial'}
-        Multiclass option can be either 'ovr' or 'multinomial'. If the option
-        chosen is 'ovr', then a binary problem is fit for each label. Else
-        the loss minimised is the multinomial loss fit across
-        the entire probability distribution. Does not work for 'liblinear'
-        solver.
+        If the option chosen is 'ovr', then a binary problem is fit for each
+        label. For 'multinomial' the loss minimised is the multinomial loss fit
+        across the entire probability distribution, *even when the data is
+        binary*. 'multinomial' is unavailable when solver='liblinear'.
 
     random_state : int, RandomState instance or None, optional, default None
         The seed of the pseudo random number generator to use when shuffling
@@ -903,8 +937,6 @@ def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,
     n_iter : array, shape(n_cs,)
         Actual number of iteration for each Cs.
     """
-    _check_solver_option(solver, multi_class, penalty, dual)
-
     X_train = X[train]
     X_test = X[test]
     y_train = y[train]
@@ -925,7 +957,7 @@ def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,
         check_input=False, max_squared_sum=max_squared_sum,
         sample_weight=sample_weight)
 
-    log_reg = LogisticRegression(multi_class=multi_class)
+    log_reg = LogisticRegression(solver=solver, multi_class=multi_class)
 
     # The score method of Logistic Regression has a classes_ attribute.
     if multi_class == 'ovr':
@@ -1046,7 +1078,7 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,
         'liblinear'.
 
     solver : str, {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, \
-             default: 'liblinear'
+             default: 'liblinear'.
 
         Algorithm to use in the optimization problem.
 
@@ -1066,20 +1098,25 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,
            Stochastic Average Gradient descent solver.
         .. versionadded:: 0.19
            SAGA solver.
+        .. versionchanged:: 0.20
+            Default will change from 'liblinear' to 'lbfgs' in 0.22.
 
     max_iter : int, default: 100
         Useful only for the newton-cg, sag and lbfgs solvers.
         Maximum number of iterations taken for the solvers to converge.
 
-    multi_class : str, {'ovr', 'multinomial'}, default: 'ovr'
-        Multiclass option can be either 'ovr' or 'multinomial'. If the option
-        chosen is 'ovr', then a binary problem is fit for each label. Else
-        the loss minimised is the multinomial loss fit across
-        the entire probability distribution. Does not work for 'liblinear'
-        solver.
+    multi_class : str, {'ovr', 'multinomial', 'auto'}, default: 'ovr'
+        If the option chosen is 'ovr', then a binary problem is fit for each
+        label. For 'multinomial' the loss minimised is the multinomial loss fit
+        across the entire probability distribution, *even when the data is
+        binary*. 'multinomial' is unavailable when solver='liblinear'.
+        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',
+        and otherwise selects 'multinomial'.
 
         .. versionadded:: 0.18
            Stochastic Average Gradient descent solver for 'multinomial' case.
+        .. versionchanged:: 0.20
+            Default will change from 'ovr' to 'auto' in 0.22.
 
     verbose : int, default: 0
         For the liblinear and lbfgs solvers set verbose to any positive
@@ -1093,11 +1130,13 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,
         .. versionadded:: 0.17
            *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.
 
-    n_jobs : int, default: 1
+    n_jobs : int or None, optional (default=None)
         Number of CPU cores used when parallelizing over classes if
         multi_class='ovr'". This parameter is ignored when the ``solver`` is
         set to 'liblinear' regardless of whether 'multi_class' is specified or
-        not. If given a value of -1, all cores are used.
+        not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`
+        context. ``-1`` means using all processors.
+        See :term:`Glossary <n_jobs>` for more details.
 
     Attributes
     ----------
@@ -1133,20 +1172,20 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,
     >>> from sklearn.datasets import load_iris
     >>> from sklearn.linear_model import LogisticRegression
     >>> X, y = load_iris(return_X_y=True)
-    >>> clf = LogisticRegression(random_state=0).fit(X, y)
+    >>> clf = LogisticRegression(random_state=0, solver='lbfgs',
+    ...                          multi_class='multinomial').fit(X, y)
     >>> clf.predict(X[:2, :])
     array([0, 0])
     >>> clf.predict_proba(X[:2, :]) # doctest: +ELLIPSIS
-    array([[8.78...e-01, 1.21...e-01, 1.079...e-05],
-           [7.97...e-01, 2.02...e-01, 3.029...e-05]])
+    array([[9.8...e-01, 1.8...e-02, 1.4...e-08],
+           [9.7...e-01, 2.8...e-02, ...e-08]])
     >>> clf.score(X, y)
-    0.96
+    0.97...
 
     See also
     --------
     SGDClassifier : incrementally trained logistic regression (when given
         the parameter ``loss="log"``).
-    sklearn.svm.LinearSVC : learns SVM models using the same algorithm.
     LogisticRegressionCV : Logistic regression with built-in cross validation
 
     Notes
@@ -1183,8 +1222,8 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,
 
     def __init__(self, penalty='l2', dual=False, tol=1e-4, C=1.0,
                  fit_intercept=True, intercept_scaling=1, class_weight=None,
-                 random_state=None, solver='liblinear', max_iter=100,
-                 multi_class='ovr', verbose=0, warm_start=False, n_jobs=None):
+                 random_state=None, solver='warn', max_iter=100,
+                 multi_class='warn', verbose=0, warm_start=False, n_jobs=None):
 
         self.penalty = penalty
         self.dual = dual
@@ -1210,7 +1249,7 @@ def fit(self, X, y, sample_weight=None):
             Training vector, where n_samples is the number of samples and
             n_features is the number of features.
 
-        y : array-like, shape (n_samples,)
+        y : array-like, shape (n_samples,) or (n_samples, n_targets)
             Target vector relative to X.
 
         sample_weight : array-like, shape (n_samples,) optional
@@ -1234,25 +1273,27 @@ def fit(self, X, y, sample_weight=None):
             raise ValueError("Tolerance for stopping criteria must be "
                              "positive; got (tol=%r)" % self.tol)
 
-        if self.solver in ['newton-cg']:
+        solver = _check_solver(self.solver, self.penalty, self.dual)
+
+        if solver in ['newton-cg']:
             _dtype = [np.float64, np.float32]
         else:
             _dtype = np.float64
 
         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order="C",
-                         accept_large_sparse=self.solver != 'liblinear')
+                         accept_large_sparse=solver != 'liblinear')
         check_classification_targets(y)
         self.classes_ = np.unique(y)
         n_samples, n_features = X.shape
 
-        _check_solver_option(self.solver, self.multi_class, self.penalty,
-                             self.dual)
+        multi_class = _check_multi_class(self.multi_class, solver,
+                                         len(self.classes_))
 
-        if self.solver == 'liblinear':
-            if self.n_jobs != 1:
+        if solver == 'liblinear':
+            if effective_n_jobs(self.n_jobs) != 1:
                 warnings.warn("'n_jobs' > 1 does not have any effect when"
                               " 'solver' is set to 'liblinear'. Got 'n_jobs'"
-                              " = {}.".format(self.n_jobs))
+                              " = {}.".format(effective_n_jobs(self.n_jobs)))
             self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                 X, y, self.C, self.fit_intercept, self.intercept_scaling,
                 self.class_weight, self.penalty, self.dual, self.verbose,
@@ -1261,7 +1302,7 @@ def fit(self, X, y, sample_weight=None):
             self.n_iter_ = np.array([n_iter_])
             return self
 
-        if self.solver in ['sag', 'saga']:
+        if solver in ['sag', 'saga']:
             max_squared_sum = row_norms(X, squared=True).max()
         else:
             max_squared_sum = None
@@ -1290,7 +1331,7 @@ def fit(self, X, y, sample_weight=None):
         self.intercept_ = np.zeros(n_classes)
 
         # Hack so that we iterate only once for the multinomial case.
-        if self.multi_class == 'multinomial':
+        if multi_class == 'multinomial':
             classes_ = [None]
             warm_start_coef = [warm_start_coef]
         if warm_start_coef is None:
@@ -1300,7 +1341,7 @@ def fit(self, X, y, sample_weight=None):
 
         # The SAG solver releases the GIL so it's more efficient to use
         # threads for this solver.
-        if self.solver in ['sag', 'saga']:
+        if solver in ['sag', 'saga']:
             prefer = 'threads'
         else:
             prefer = 'processes'
@@ -1308,8 +1349,8 @@ def fit(self, X, y, sample_weight=None):
                                prefer=prefer)(
             path_func(X, y, pos_class=class_, Cs=[self.C],
                       fit_intercept=self.fit_intercept, tol=self.tol,
-                      verbose=self.verbose, solver=self.solver,
-                      multi_class=self.multi_class, max_iter=self.max_iter,
+                      verbose=self.verbose, solver=solver,
+                      multi_class=multi_class, max_iter=self.max_iter,
                       class_weight=self.class_weight, check_input=False,
                       random_state=self.random_state, coef=warm_start_coef_,
                       penalty=self.penalty,
@@ -1320,7 +1361,7 @@ def fit(self, X, y, sample_weight=None):
         fold_coefs_, _, n_iter_ = zip(*fold_coefs_)
         self.n_iter_ = np.asarray(n_iter_, dtype=np.int32)[:, 0]
 
-        if self.multi_class == 'multinomial':
+        if multi_class == 'multinomial':
             self.coef_ = fold_coefs_[0][0]
         else:
             self.coef_ = np.asarray(fold_coefs_)
@@ -1358,7 +1399,11 @@ def predict_proba(self, X):
         """
         if not hasattr(self, "coef_"):
             raise NotFittedError("Call fit before prediction")
-        if self.multi_class == "ovr":
+
+        ovr = (self.multi_class in ["ovr", "warn"] or
+               (self.multi_class == 'auto' and (self.classes_.size <= 2 or
+                                                self.solver == 'liblinear')))
+        if ovr:
             return super(LogisticRegression, self)._predict_proba_lr(X)
         else:
             decision = self.decision_function(X)
@@ -1452,7 +1497,7 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,
         default scoring option used is 'accuracy'.
 
     solver : str, {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, \
-             default: 'lbfgs'
+             default: 'lbfgs'.
 
         Algorithm to use in the optimization problem.
 
@@ -1495,9 +1540,11 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,
         .. versionadded:: 0.17
            class_weight == 'balanced'
 
-    n_jobs : int, optional
-        Number of CPU cores used during the cross-validation loop. If given
-        a value of -1, all cores are used.
+    n_jobs : int or None, optional (default=None)
+        Number of CPU cores used during the cross-validation loop.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : int
         For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any
@@ -1523,15 +1570,18 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,
         To lessen the effect of regularization on synthetic feature weight
         (and therefore on the intercept) intercept_scaling has to be increased.
 
-    multi_class : str, {'ovr', 'multinomial'}
-        Multiclass option can be either 'ovr' or 'multinomial'. If the option
-        chosen is 'ovr', then a binary problem is fit for each label. Else
-        the loss minimised is the multinomial loss fit across
-        the entire probability distribution. Does not work for 'liblinear'
-        solver.
+    multi_class : str, {'ovr', 'multinomial', 'auto'}, default: 'ovr'
+        If the option chosen is 'ovr', then a binary problem is fit for each
+        label. For 'multinomial' the loss minimised is the multinomial loss fit
+        across the entire probability distribution, *even when the data is
+        binary*. 'multinomial' is unavailable when solver='liblinear'.
+        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',
+        and otherwise selects 'multinomial'.
 
         .. versionadded:: 0.18
            Stochastic Average Gradient descent solver for 'multinomial' case.
+        .. versionchanged:: 0.20
+            Default will change from 'ovr' to 'auto' in 0.22.
 
     random_state : int, RandomState instance or None, optional, default None
         If int, random_state is the seed used by the random number generator;
@@ -1591,25 +1641,24 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,
     >>> from sklearn.datasets import load_iris
     >>> from sklearn.linear_model import LogisticRegressionCV
     >>> X, y = load_iris(return_X_y=True)
-    >>> clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)
+    >>> clf = LogisticRegressionCV(cv=5, random_state=0,
+    ...                            multi_class='multinomial').fit(X, y)
     >>> clf.predict(X[:2, :])
     array([0, 0])
-    >>> clf.predict_proba(X[:2, :]) # doctest: +ELLIPSIS
-    array([[8.72...e-01, 1.27...e-01, 5.50...e-14],
-           [6.76...e-01, 3.23...e-01, 2.11...e-13]])
+    >>> clf.predict_proba(X[:2, :]).shape
+    (2, 3)
     >>> clf.score(X, y) # doctest: +ELLIPSIS
-    0.9266...
+    0.98...
 
     See also
     --------
     LogisticRegression
 
     """
-
     def __init__(self, Cs=10, fit_intercept=True, cv='warn', dual=False,
                  penalty='l2', scoring=None, solver='lbfgs', tol=1e-4,
                  max_iter=100, class_weight=None, n_jobs=None, verbose=0,
-                 refit=True, intercept_scaling=1., multi_class='ovr',
+                 refit=True, intercept_scaling=1., multi_class='warn',
                  random_state=None):
         self.Cs = Cs
         self.fit_intercept = fit_intercept
@@ -1637,7 +1686,7 @@ def fit(self, X, y, sample_weight=None):
             Training vector, where n_samples is the number of samples and
             n_features is the number of features.
 
-        y : array-like, shape (n_samples,)
+        y : array-like, shape (n_samples,) or (n_samples, n_targets)
             Target vector relative to X.
 
         sample_weight : array-like, shape (n_samples,) optional
@@ -1648,8 +1697,7 @@ def fit(self, X, y, sample_weight=None):
         -------
         self : object
         """
-        _check_solver_option(self.solver, self.multi_class, self.penalty,
-                             self.dual)
+        solver = _check_solver(self.solver, self.penalty, self.dual)
 
         if not isinstance(self.max_iter, numbers.Number) or self.max_iter < 0:
             raise ValueError("Maximum number of iteration must be positive;"
@@ -1660,7 +1708,7 @@ def fit(self, X, y, sample_weight=None):
 
         X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64,
                          order="C",
-                         accept_large_sparse=self.solver != 'liblinear')
+                         accept_large_sparse=solver != 'liblinear')
         check_classification_targets(y)
 
         class_weight = self.class_weight
@@ -1676,7 +1724,10 @@ def fit(self, X, y, sample_weight=None):
         classes = self.classes_ = label_encoder.classes_
         encoded_labels = label_encoder.transform(label_encoder.classes_)
 
-        if self.solver in ['sag', 'saga']:
+        multi_class = _check_multi_class(self.multi_class, solver,
+                                         len(classes))
+
+        if solver in ['sag', 'saga']:
             max_squared_sum = row_norms(X, squared=True).max()
         else:
             max_squared_sum = None
@@ -1702,7 +1753,7 @@ def fit(self, X, y, sample_weight=None):
 
         # We need this hack to iterate only once over labels, in the case of
         # multi_class = multinomial, without changing the value of the labels.
-        if self.multi_class == 'multinomial':
+        if multi_class == 'multinomial':
             iter_encoded_labels = iter_classes = [None]
         else:
             iter_encoded_labels = encoded_labels
@@ -1727,10 +1778,10 @@ def fit(self, X, y, sample_weight=None):
                                prefer=prefer)(
             path_func(X, y, train, test, pos_class=label, Cs=self.Cs,
                       fit_intercept=self.fit_intercept, penalty=self.penalty,
-                      dual=self.dual, solver=self.solver, tol=self.tol,
+                      dual=self.dual, solver=solver, tol=self.tol,
                       max_iter=self.max_iter, verbose=self.verbose,
                       class_weight=class_weight, scoring=self.scoring,
-                      multi_class=self.multi_class,
+                      multi_class=multi_class,
                       intercept_scaling=self.intercept_scaling,
                       random_state=self.random_state,
                       max_squared_sum=max_squared_sum,
@@ -1739,7 +1790,7 @@ def fit(self, X, y, sample_weight=None):
             for label in iter_encoded_labels
             for train, test in folds)
 
-        if self.multi_class == 'multinomial':
+        if multi_class == 'multinomial':
             multi_coefs_paths, Cs, multi_scores, n_iter_ = zip(*fold_coefs_)
             multi_coefs_paths = np.asarray(multi_coefs_paths)
             multi_scores = np.asarray(multi_scores)
@@ -1776,14 +1827,14 @@ def fit(self, X, y, sample_weight=None):
         self.intercept_ = np.zeros(n_classes)
 
         # hack to iterate only once for multinomial case.
-        if self.multi_class == 'multinomial':
+        if multi_class == 'multinomial':
             scores = multi_scores
             coefs_paths = multi_coefs_paths
 
         for index, (cls, encoded_label) in enumerate(
                 zip(iter_classes, iter_encoded_labels)):
 
-            if self.multi_class == 'ovr':
+            if multi_class == 'ovr':
                 # The scores_ / coefs_paths_ dict have unencoded class
                 # labels as their keys
                 scores = self.scores_[cls]
@@ -1794,7 +1845,7 @@ def fit(self, X, y, sample_weight=None):
 
                 C_ = self.Cs_[best_index]
                 self.C_.append(C_)
-                if self.multi_class == 'multinomial':
+                if multi_class == 'multinomial':
                     coef_init = np.mean(coefs_paths[:, best_index, :, :],
                                         axis=0)
                 else:
@@ -1803,12 +1854,12 @@ def fit(self, X, y, sample_weight=None):
                 # Note that y is label encoded and hence pos_class must be
                 # the encoded label / None (for 'multinomial')
                 w, _, _ = logistic_regression_path(
-                    X, y, pos_class=encoded_label, Cs=[C_], solver=self.solver,
+                    X, y, pos_class=encoded_label, Cs=[C_], solver=solver,
                     fit_intercept=self.fit_intercept, coef=coef_init,
                     max_iter=self.max_iter, tol=self.tol,
                     penalty=self.penalty,
                     class_weight=class_weight,
-                    multi_class=self.multi_class,
+                    multi_class=multi_class,
                     verbose=max(0, self.verbose - 1),
                     random_state=self.random_state,
                     check_input=False, max_squared_sum=max_squared_sum,
@@ -1823,7 +1874,7 @@ def fit(self, X, y, sample_weight=None):
                              for i in range(len(folds))], axis=0)
                 self.C_.append(np.mean(self.Cs_[best_indices]))
 
-            if self.multi_class == 'multinomial':
+            if multi_class == 'multinomial':
                 self.C_ = np.tile(self.C_, n_classes)
                 self.coef_ = w[:, :X.shape[1]]
                 if self.fit_intercept:
diff --git a/sklearn/linear_model/omp.py b/sklearn/linear_model/omp.py
index 084336bbb1..c304c0f341 100755
--- a/sklearn/linear_model/omp.py
+++ b/sklearn/linear_model/omp.py
@@ -583,6 +583,17 @@ class OrthogonalMatchingPursuit(LinearModel, RegressorMixin):
     n_iter_ : int or array-like
         Number of active features across every target.
 
+    Examples
+    --------
+    >>> from sklearn.linear_model import OrthogonalMatchingPursuit
+    >>> from sklearn.datasets import make_regression
+    >>> X, y = make_regression(noise=4, random_state=0)
+    >>> reg = OrthogonalMatchingPursuit().fit(X, y)
+    >>> reg.score(X, y) # doctest: +ELLIPSIS
+    0.9991...
+    >>> reg.predict(X[:1,])
+    array([-78.3854...])
+
     Notes
     -----
     Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,
@@ -789,9 +800,11 @@ class OrthogonalMatchingPursuitCV(LinearModel, RegressorMixin):
             ``cv`` default value if None will change from 3-fold to 5-fold
             in v0.22.
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the cross validation. If ``-1``, use
-        all the CPUs
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the cross validation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : boolean or integer, optional
         Sets the verbosity amount
@@ -812,6 +825,20 @@ class OrthogonalMatchingPursuitCV(LinearModel, RegressorMixin):
         Number of active features across every target for the model refit with
         the best hyperparameters got by cross-validating across all folds.
 
+    Examples
+    --------
+    >>> from sklearn.linear_model import OrthogonalMatchingPursuitCV
+    >>> from sklearn.datasets import make_regression
+    >>> X, y = make_regression(n_features=100, n_informative=10,
+    ...                        noise=4, random_state=0)
+    >>> reg = OrthogonalMatchingPursuitCV(cv=5).fit(X, y)
+    >>> reg.score(X, y) # doctest: +ELLIPSIS
+    0.9991...
+    >>> reg.n_nonzero_coefs_
+    10
+    >>> reg.predict(X[:1,])
+    array([-78.3854...])
+
     See also
     --------
     orthogonal_mp
diff --git a/sklearn/linear_model/passive_aggressive.py b/sklearn/linear_model/passive_aggressive.py
index 7a04f77481..22f1c0fbba 100755
--- a/sklearn/linear_model/passive_aggressive.py
+++ b/sklearn/linear_model/passive_aggressive.py
@@ -68,10 +68,12 @@ class PassiveAggressiveClassifier(BaseSGDClassifier):
         hinge: equivalent to PA-I in the reference paper.
         squared_hinge: equivalent to PA-II in the reference paper.
 
-    n_jobs : integer, optional
+    n_jobs : int or None, optional (default=None)
         The number of CPUs to use to do the OVA (One Versus All, for
-        multi-class problems) computation. -1 means 'all CPUs'. Defaults
-        to 1.
+        multi-class problems) computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional, default=None
         The seed of the pseudo random number generator to use when shuffling
diff --git a/sklearn/linear_model/perceptron.py b/sklearn/linear_model/perceptron.py
index 3e24be23c1..1bc06f4f17 100755
--- a/sklearn/linear_model/perceptron.py
+++ b/sklearn/linear_model/perceptron.py
@@ -47,10 +47,12 @@ class Perceptron(BaseSGDClassifier):
     eta0 : double
         Constant by which the updates are multiplied. Defaults to 1.
 
-    n_jobs : integer, optional
+    n_jobs : int or None, optional (default=None)
         The number of CPUs to use to do the OVA (One Versus All, for
-        multi-class problems) computation. -1 means 'all CPUs'. Defaults
-        to 1.
+        multi-class problems) computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional, default None
         The seed of the pseudo random number generator to use when shuffling
@@ -123,6 +125,20 @@ class Perceptron(BaseSGDClassifier):
     ``Perceptron()`` is equivalent to `SGDClassifier(loss="perceptron",
     eta0=1, learning_rate="constant", penalty=None)`.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.linear_model import Perceptron
+    >>> X, y = load_digits(return_X_y=True)
+    >>> clf = Perceptron(tol=1e-3, random_state=0)
+    >>> clf.fit(X, y)
+    Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,
+          fit_intercept=True, max_iter=None, n_iter=None, n_iter_no_change=5,
+          n_jobs=None, penalty=None, random_state=0, shuffle=True, tol=0.001,
+          validation_fraction=0.1, verbose=0, warm_start=False)
+    >>> clf.score(X, y) # doctest: +ELLIPSIS
+    0.946...
+
     See also
     --------
 
diff --git a/sklearn/linear_model/randomized_l1.py b/sklearn/linear_model/randomized_l1.py
index 0a28305185..40ebe3c578 100755
--- a/sklearn/linear_model/randomized_l1.py
+++ b/sklearn/linear_model/randomized_l1.py
@@ -257,9 +257,11 @@ class RandomizedLasso(BaseRandomizedLinearModel):
         If None, the random number generator is the RandomState instance used
         by `np.random`.
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the resampling. If '-1', use
-        all the CPUs
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the resampling.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     pre_dispatch : int, or string, optional
         Controls the number of jobs that get dispatched during parallel
@@ -380,7 +382,8 @@ def _randomized_logistic(X, y, weights, mask, C=1., verbose=False,
     for this_C, this_scores in zip(C, scores.T):
         # XXX : would be great to do it with a warm_start ...
         clf = LogisticRegression(C=this_C, tol=tol, penalty='l1', dual=False,
-                                 fit_intercept=fit_intercept)
+                                 fit_intercept=fit_intercept,
+                                 solver='liblinear', multi_class='ovr')
         clf.fit(X, y)
         this_scores[:] = np.any(
             np.abs(clf.coef_) > 10 * np.finfo(np.float).eps, axis=0)
@@ -451,9 +454,11 @@ class RandomizedLogisticRegression(BaseRandomizedLinearModel):
         If None, the random number generator is the RandomState instance used
         by `np.random`.
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the resampling. If '-1', use
-        all the CPUs
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the resampling.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     pre_dispatch : int, or string, optional
         Controls the number of jobs that get dispatched during parallel
@@ -608,9 +613,11 @@ def lasso_stability_path(X, y, scaling=0.5, random_state=None,
     eps : float, optional
         Smallest value of alpha / alpha_max considered
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the resampling. If '-1', use
-        all the CPUs
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the resampling.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : boolean or integer, optional
         Sets the verbosity amount
diff --git a/sklearn/linear_model/ransac.py b/sklearn/linear_model/ransac.py
index 9dcd044d1f..f929533e87 100755
--- a/sklearn/linear_model/ransac.py
+++ b/sklearn/linear_model/ransac.py
@@ -186,6 +186,18 @@ class RANSACRegressor(BaseEstimator, MetaEstimatorMixin, RegressorMixin):
 
         .. versionadded:: 0.19
 
+    Examples
+    --------
+    >>> from sklearn.linear_model import RANSACRegressor
+    >>> from sklearn.datasets import make_regression
+    >>> X, y = make_regression(
+    ...     n_samples=200, n_features=2, noise=4.0, random_state=0)
+    >>> reg = RANSACRegressor(random_state=0).fit(X, y)
+    >>> reg.score(X, y) # doctest: +ELLIPSIS
+    0.9885...
+    >>> reg.predict(X[:1,])
+    array([-31.9417...])
+
     References
     ----------
     .. [1] https://en.wikipedia.org/wiki/RANSAC
diff --git a/sklearn/linear_model/sag.py b/sklearn/linear_model/sag.py
index 06a72d47b4..3e8861f26d 100755
--- a/sklearn/linear_model/sag.py
+++ b/sklearn/linear_model/sag.py
@@ -212,13 +212,15 @@ def sag_solver(X, y, sample_weight=None, loss='log', alpha=1., beta=0.,
 
     >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
     >>> y = np.array([1, 1, 2, 2])
-    >>> clf = linear_model.LogisticRegression(solver='sag')
+    >>> clf = linear_model.LogisticRegression(
+    ...     solver='sag', multi_class='multinomial')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     LogisticRegression(C=1.0, class_weight=None, dual=False,
         fit_intercept=True, intercept_scaling=1, max_iter=100,
-        multi_class='ovr', n_jobs=None, penalty='l2', random_state=None,
-        solver='sag', tol=0.0001, verbose=0, warm_start=False)
+        multi_class='multinomial', n_jobs=None, penalty='l2',
+        random_state=None, solver='sag', tol=0.0001, verbose=0,
+        warm_start=False)
 
     References
     ----------
diff --git a/sklearn/linear_model/stochastic_gradient.py b/sklearn/linear_model/stochastic_gradient.py
index a4c5938cb1..5e253003a2 100755
--- a/sklearn/linear_model/stochastic_gradient.py
+++ b/sklearn/linear_model/stochastic_gradient.py
@@ -806,10 +806,12 @@ class SGDClassifier(BaseSGDClassifier):
         For epsilon-insensitive, any differences between the current prediction
         and the correct label are ignored if they are less than this threshold.
 
-    n_jobs : integer, optional
+    n_jobs : int or None, optional (default=None)
         The number of CPUs to use to do the OVA (One Versus All, for
-        multi-class problems) computation. -1 means 'all CPUs'. Defaults
-        to 1.
+        multi-class problems) computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional (default=None)
         The seed of the pseudo random number generator to use when shuffling
@@ -843,7 +845,7 @@ class SGDClassifier(BaseSGDClassifier):
         The exponent for inverse scaling learning rate [default 0.5].
 
     early_stopping : bool, default=False
-        Whether to use early stopping to terminate training when validation.
+        Whether to use early stopping to terminate training when validation
         score is not improving. If set to True, it will automatically set aside
         a fraction of training data as validation and terminate training when
         validation score is not improving by at least tol for
@@ -1452,7 +1454,7 @@ class SGDRegressor(BaseSGDRegressor):
         The exponent for inverse scaling learning rate [default 0.5].
 
     early_stopping : bool, default=False
-        Whether to use early stopping to terminate training when validation.
+        Whether to use early stopping to terminate training when validation
         score is not improving. If set to True, it will automatically set aside
         a fraction of training data as validation and terminate training when
         validation score is not improving by at least tol for
diff --git a/sklearn/linear_model/tests/test_logistic.py b/sklearn/linear_model/tests/test_logistic.py
index 9fc6d6ec5f..4195405b86 100755
--- a/sklearn/linear_model/tests/test_logistic.py
+++ b/sklearn/linear_model/tests/test_logistic.py
@@ -1,15 +1,18 @@
+import os
+import sys
 import numpy as np
 import scipy.sparse as sp
 from scipy import linalg, optimize, sparse
 
 import pytest
 
+from sklearn.base import clone
 from sklearn.datasets import load_iris, make_classification
 from sklearn.metrics import log_loss
 from sklearn.metrics.scorer import get_scorer
 from sklearn.model_selection import StratifiedKFold
 from sklearn.preprocessing import LabelEncoder
-from sklearn.utils import compute_class_weight
+from sklearn.utils import compute_class_weight, _IS_32BIT
 from sklearn.utils.testing import assert_almost_equal
 from sklearn.utils.testing import assert_allclose
 from sklearn.utils.testing import assert_array_almost_equal
@@ -22,6 +25,7 @@
 from sklearn.utils.testing import assert_warns
 from sklearn.utils.testing import ignore_warnings
 from sklearn.utils.testing import assert_warns_message
+from sklearn.utils.testing import assert_no_warnings
 
 from sklearn.exceptions import ConvergenceWarning
 from sklearn.exceptions import ChangedBehaviorWarning
@@ -57,6 +61,8 @@ def check_predictions(clf, X, y):
     assert_array_equal(probabilities.argmax(axis=1), y)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_predict_2_classes():
     # Simple sanity check on a 2 classes dataset
     # Make sure it predicts the correct result on simple datasets.
@@ -72,6 +78,7 @@ def test_predict_2_classes():
                                          random_state=0), X_sp, Y1)
 
 
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_error():
     # Test for appropriate exception on errors
     msg = "Penalty term must be positive"
@@ -95,6 +102,7 @@ def test_error():
         assert_raise_message(ValueError, msg, LR(max_iter="test").fit, X, Y1)
 
 
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_logistic_cv_mock_scorer():
 
     class MockScorer(object):
@@ -130,7 +138,7 @@ def __call__(self, model, X, y, sample_weight=None):
 
 
 def test_logistic_cv_score_does_not_warn_by_default():
-    lr = LogisticRegressionCV(cv=2)
+    lr = LogisticRegressionCV(cv=2, multi_class='ovr')
     lr.fit(X, Y1)
 
     with pytest.warns(None) as record:
@@ -142,7 +150,7 @@ def test_lr_liblinear_warning():
     n_samples, n_features = iris.data.shape
     target = iris.target_names[iris.target]
 
-    lr = LogisticRegression(solver='liblinear', n_jobs=2)
+    lr = LogisticRegression(solver='liblinear', multi_class='ovr', n_jobs=2)
     assert_warns_message(UserWarning,
                          "'n_jobs' > 1 does not have any effect when"
                          " 'solver' is set to 'liblinear'. Got 'n_jobs'"
@@ -150,6 +158,8 @@ def test_lr_liblinear_warning():
                          lr.fit, iris.data, target)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_predict_3_classes():
     check_predictions(LogisticRegression(C=10), X, Y2)
     check_predictions(LogisticRegression(C=10), X_sp, Y2)
@@ -164,7 +174,8 @@ def test_predict_iris():
     # Test that both multinomial and OvR solvers handle
     # multiclass data correctly and give good accuracy
     # score (>0.95) for the training data.
-    for clf in [LogisticRegression(C=len(iris.data)),
+    for clf in [LogisticRegression(C=len(iris.data), solver='liblinear',
+                                   multi_class='ovr'),
                 LogisticRegression(C=len(iris.data), solver='lbfgs',
                                    multi_class='multinomial'),
                 LogisticRegression(C=len(iris.data), solver='newton-cg',
@@ -198,12 +209,13 @@ def test_multinomial_validation(solver):
 def test_check_solver_option(LR):
     X, y = iris.data, iris.target
 
-    msg = ('Logistic Regression supports only liblinear, newton-cg, '
-           'lbfgs, sag and saga solvers, got wrong_name')
-    lr = LR(solver="wrong_name")
+    msg = ("Logistic Regression supports only solvers in ['liblinear', "
+           "'newton-cg', 'lbfgs', 'sag', 'saga'], got wrong_name.")
+    lr = LR(solver="wrong_name", multi_class="ovr")
     assert_raise_message(ValueError, msg, lr.fit, X, y)
 
-    msg = "multi_class should be either multinomial or ovr, got wrong_name"
+    msg = ("multi_class should be 'multinomial', 'ovr' or 'auto'. "
+           "Got wrong_name")
     lr = LR(solver='newton-cg', multi_class="wrong_name")
     assert_raise_message(ValueError, msg, lr.fit, X, y)
 
@@ -216,15 +228,40 @@ def test_check_solver_option(LR):
     for solver in ['newton-cg', 'lbfgs', 'sag']:
         msg = ("Solver %s supports only l2 penalties, got l1 penalty." %
                solver)
-        lr = LR(solver=solver, penalty='l1')
+        lr = LR(solver=solver, penalty='l1', multi_class='ovr')
         assert_raise_message(ValueError, msg, lr.fit, X, y)
     for solver in ['newton-cg', 'lbfgs', 'sag', 'saga']:
         msg = ("Solver %s supports only dual=False, got dual=True" %
                solver)
-        lr = LR(solver=solver, dual=True)
+        lr = LR(solver=solver, dual=True, multi_class='ovr')
         assert_raise_message(ValueError, msg, lr.fit, X, y)
 
 
+@pytest.mark.parametrize('model, params, warn_solver',
+                         [(LogisticRegression, {}, True),
+                          (LogisticRegressionCV, {'cv': 5}, False)])
+def test_logistic_regression_warnings(model, params, warn_solver):
+    clf_solver_warning = model(multi_class='ovr', **params)
+    clf_multi_class_warning = model(solver='lbfgs', **params)
+    clf_no_warnings = model(solver='lbfgs', multi_class='ovr', **params)
+
+    solver_warning_msg = "Default solver will be changed to 'lbfgs'"
+    multi_class_warning_msg = "Default multi_class will be changed to 'auto"
+
+    if warn_solver:
+        assert_warns_message(FutureWarning, solver_warning_msg,
+                             clf_solver_warning.fit, iris.data, iris.target)
+    else:
+        assert_no_warnings(clf_no_warnings.fit, iris.data, iris.target)
+
+    assert_warns_message(FutureWarning, multi_class_warning_msg,
+                         clf_multi_class_warning.fit, iris.data, iris.target)
+    # But no warning when binary target:
+    assert_no_warnings(clf_multi_class_warning.fit,
+                       iris.data, iris.target == 0)
+    assert_no_warnings(clf_no_warnings.fit, iris.data, iris.target)
+
+
 @pytest.mark.parametrize('solver', ['lbfgs', 'newton-cg', 'sag', 'saga'])
 def test_multinomial_binary(solver):
     # Test multinomial LR on a binary problem.
@@ -259,11 +296,13 @@ def test_multinomial_binary_probabilities():
 
     expected_proba_class_1 = (np.exp(decision) /
                               (np.exp(decision) + np.exp(-decision)))
-    expected_proba = np.c_[1-expected_proba_class_1, expected_proba_class_1]
+    expected_proba = np.c_[1 - expected_proba_class_1, expected_proba_class_1]
 
     assert_almost_equal(proba, expected_proba)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_sparsify():
     # Test sparsify and densify members.
     n_samples, n_features = iris.data.shape
@@ -287,6 +326,8 @@ def test_sparsify():
     assert_array_almost_equal(pred_d_d, pred_d_s)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_inconsistent_input():
     # Test that an exception is raised on inconsistent input
     rng = np.random.RandomState(0)
@@ -305,6 +346,8 @@ def test_inconsistent_input():
                   rng.random_sample((3, 12)))
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_write_parameters():
     # Test that we can write to coef_ and intercept_
     clf = LogisticRegression(random_state=0)
@@ -314,6 +357,8 @@ def test_write_parameters():
     assert_array_almost_equal(clf.decision_function(X), 0)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_nan():
     # Test proper NaN handling.
     # Regression test for Issue #252: fit used to go into an infinite loop.
@@ -336,12 +381,11 @@ def test_consistency_path():
     for solver in ['sag', 'saga']:
         coefs, Cs, _ = f(logistic_regression_path)(
             X, y, Cs=Cs, fit_intercept=False, tol=1e-5, solver=solver,
-            max_iter=1000,
-            random_state=0)
+            max_iter=1000, multi_class='ovr', random_state=0)
         for i, C in enumerate(Cs):
             lr = LogisticRegression(C=C, fit_intercept=False, tol=1e-5,
-                                    solver=solver,
-                                    random_state=0)
+                                    solver=solver, multi_class='ovr',
+                                    random_state=0, max_iter=1000)
             lr.fit(X, y)
             lr_coef = lr.coef_.ravel()
             assert_array_almost_equal(lr_coef, coefs[i], decimal=4,
@@ -352,9 +396,10 @@ def test_consistency_path():
         Cs = [1e3]
         coefs, Cs, _ = f(logistic_regression_path)(
             X, y, Cs=Cs, fit_intercept=True, tol=1e-6, solver=solver,
-            intercept_scaling=10000., random_state=0)
+            intercept_scaling=10000., random_state=0, multi_class='ovr')
         lr = LogisticRegression(C=Cs[0], fit_intercept=True, tol=1e-4,
-                                intercept_scaling=10000., random_state=0)
+                                intercept_scaling=10000., random_state=0,
+                                multi_class='ovr', solver=solver)
         lr.fit(X, y)
         lr_coef = np.concatenate([lr.coef_.ravel(), lr.intercept_])
         assert_array_almost_equal(lr_coef, coefs[0], decimal=4,
@@ -373,11 +418,14 @@ def test_logistic_regression_path_convergence_fail():
 def test_liblinear_dual_random_state():
     # random_state is relevant for liblinear solver only if dual=True
     X, y = make_classification(n_samples=20, random_state=0)
-    lr1 = LogisticRegression(random_state=0, dual=True, max_iter=1, tol=1e-15)
+    lr1 = LogisticRegression(random_state=0, dual=True, max_iter=1, tol=1e-15,
+                             solver='liblinear', multi_class='ovr')
     lr1.fit(X, y)
-    lr2 = LogisticRegression(random_state=0, dual=True, max_iter=1, tol=1e-15)
+    lr2 = LogisticRegression(random_state=0, dual=True, max_iter=1, tol=1e-15,
+                             solver='liblinear', multi_class='ovr')
     lr2.fit(X, y)
-    lr3 = LogisticRegression(random_state=8, dual=True, max_iter=1, tol=1e-15)
+    lr3 = LogisticRegression(random_state=8, dual=True, max_iter=1, tol=1e-15,
+                             solver='liblinear', multi_class='ovr')
     lr3.fit(X, y)
 
     # same result for same random state
@@ -477,9 +525,10 @@ def test_logistic_cv():
     X_ref -= X_ref.mean()
     X_ref /= X_ref.std()
     lr_cv = LogisticRegressionCV(Cs=[1.], fit_intercept=False,
-                                 solver='liblinear')
+                                 solver='liblinear', multi_class='ovr')
     lr_cv.fit(X_ref, y)
-    lr = LogisticRegression(C=1., fit_intercept=False)
+    lr = LogisticRegression(C=1., fit_intercept=False,
+                            solver='liblinear', multi_class='ovr')
     lr.fit(X_ref, y)
     assert_array_almost_equal(lr.coef_, lr_cv.coef_)
 
@@ -568,6 +617,7 @@ def test_multinomial_logistic_regression_string_inputs():
     assert_equal(sorted(np.unique(lr_cv_str.predict(X_ref))), ['bar', 'baz'])
 
 
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_logistic_cv_sparse():
     X, y = make_classification(n_samples=50, n_features=5,
@@ -630,11 +680,11 @@ def test_ovr_multinomial_iris():
     precomputed_folds = list(cv.split(train, target))
 
     # Train clf on the original dataset where classes 0 and 1 are separated
-    clf = LogisticRegressionCV(cv=precomputed_folds)
+    clf = LogisticRegressionCV(cv=precomputed_folds, multi_class='ovr')
     clf.fit(train, target)
 
     # Conflate classes 0 and 1 and train clf1 on this modified dataset
-    clf1 = LogisticRegressionCV(cv=precomputed_folds)
+    clf1 = LogisticRegressionCV(cv=precomputed_folds, multi_class='ovr')
     target_copy = target.copy()
     target_copy[target_copy == 0] = 1
     clf1.fit(train, target_copy)
@@ -680,13 +730,12 @@ def test_ovr_multinomial_iris():
 def test_logistic_regression_solvers():
     X, y = make_classification(n_features=10, n_informative=5, random_state=0)
 
-    ncg = LogisticRegression(solver='newton-cg', fit_intercept=False)
-    lbf = LogisticRegression(solver='lbfgs', fit_intercept=False)
-    lib = LogisticRegression(fit_intercept=False)
-    sag = LogisticRegression(solver='sag', fit_intercept=False,
-                             random_state=42)
-    saga = LogisticRegression(solver='saga', fit_intercept=False,
-                              random_state=42)
+    params = dict(fit_intercept=False, random_state=42, multi_class='ovr')
+    ncg = LogisticRegression(solver='newton-cg', **params)
+    lbf = LogisticRegression(solver='lbfgs', **params)
+    lib = LogisticRegression(solver='liblinear', **params)
+    sag = LogisticRegression(solver='sag', **params)
+    saga = LogisticRegression(solver='saga', **params)
     ncg.fit(X, y)
     lbf.fit(X, y)
     sag.fit(X, y)
@@ -708,13 +757,13 @@ def test_logistic_regression_solvers_multiclass():
     X, y = make_classification(n_samples=20, n_features=20, n_informative=10,
                                n_classes=3, random_state=0)
     tol = 1e-7
-    ncg = LogisticRegression(solver='newton-cg', fit_intercept=False, tol=tol)
-    lbf = LogisticRegression(solver='lbfgs', fit_intercept=False, tol=tol)
-    lib = LogisticRegression(fit_intercept=False, tol=tol)
-    sag = LogisticRegression(solver='sag', fit_intercept=False, tol=tol,
-                             max_iter=1000, random_state=42)
-    saga = LogisticRegression(solver='saga', fit_intercept=False, tol=tol,
-                              max_iter=10000, random_state=42)
+    params = dict(fit_intercept=False, tol=tol, random_state=42,
+                  multi_class='ovr')
+    ncg = LogisticRegression(solver='newton-cg', **params)
+    lbf = LogisticRegression(solver='lbfgs', **params)
+    lib = LogisticRegression(solver='liblinear', **params)
+    sag = LogisticRegression(solver='sag', max_iter=1000, **params)
+    saga = LogisticRegression(solver='saga', max_iter=10000, **params)
     ncg.fit(X, y)
     lbf.fit(X, y)
     sag.fit(X, y)
@@ -744,20 +793,25 @@ def test_logistic_regressioncv_class_weights():
 
             clf_lbf = LogisticRegressionCV(solver='lbfgs', Cs=1,
                                            fit_intercept=False,
+                                           multi_class='ovr',
                                            class_weight=class_weight)
             clf_ncg = LogisticRegressionCV(solver='newton-cg', Cs=1,
                                            fit_intercept=False,
+                                           multi_class='ovr',
                                            class_weight=class_weight)
             clf_lib = LogisticRegressionCV(solver='liblinear', Cs=1,
                                            fit_intercept=False,
+                                           multi_class='ovr',
                                            class_weight=class_weight)
             clf_sag = LogisticRegressionCV(solver='sag', Cs=1,
                                            fit_intercept=False,
+                                           multi_class='ovr',
                                            class_weight=class_weight,
                                            tol=1e-5, max_iter=10000,
                                            random_state=0)
             clf_saga = LogisticRegressionCV(solver='saga', Cs=1,
                                             fit_intercept=False,
+                                            multi_class='ovr',
                                             class_weight=class_weight,
                                             tol=1e-5, max_iter=10000,
                                             random_state=0)
@@ -784,27 +838,29 @@ def test_logistic_regression_sample_weights():
         # not passing them at all (default None)
         for solver in ['lbfgs', 'liblinear']:
             clf_sw_none = LR(solver=solver, fit_intercept=False,
-                             random_state=42)
+                             random_state=42, multi_class='ovr')
             clf_sw_none.fit(X, y)
             clf_sw_ones = LR(solver=solver, fit_intercept=False,
-                             random_state=42)
+                             random_state=42, multi_class='ovr')
             clf_sw_ones.fit(X, y, sample_weight=np.ones(y.shape[0]))
             assert_array_almost_equal(
                 clf_sw_none.coef_, clf_sw_ones.coef_, decimal=4)
 
         # Test that sample weights work the same with the lbfgs,
         # newton-cg, and 'sag' solvers
-        clf_sw_lbfgs = LR(solver='lbfgs', fit_intercept=False, random_state=42)
+        clf_sw_lbfgs = LR(solver='lbfgs', fit_intercept=False, random_state=42,
+                          multi_class='ovr')
         clf_sw_lbfgs.fit(X, y, sample_weight=sample_weight)
-        clf_sw_n = LR(solver='newton-cg', fit_intercept=False, random_state=42)
+        clf_sw_n = LR(solver='newton-cg', fit_intercept=False, random_state=42,
+                      multi_class='ovr')
         clf_sw_n.fit(X, y, sample_weight=sample_weight)
         clf_sw_sag = LR(solver='sag', fit_intercept=False, tol=1e-10,
-                        random_state=42)
+                        random_state=42, multi_class='ovr')
         # ignore convergence warning due to small dataset
         with ignore_warnings():
             clf_sw_sag.fit(X, y, sample_weight=sample_weight)
         clf_sw_liblinear = LR(solver='liblinear', fit_intercept=False,
-                              random_state=42)
+                              random_state=42, multi_class='ovr')
         clf_sw_liblinear.fit(X, y, sample_weight=sample_weight)
         assert_array_almost_equal(
             clf_sw_lbfgs.coef_, clf_sw_n.coef_, decimal=4)
@@ -818,9 +874,11 @@ def test_logistic_regression_sample_weights():
         # to be 2 for all instances of class 2
         for solver in ['lbfgs', 'liblinear']:
             clf_cw_12 = LR(solver=solver, fit_intercept=False,
-                           class_weight={0: 1, 1: 2}, random_state=42)
+                           class_weight={0: 1, 1: 2}, random_state=42,
+                           multi_class='ovr')
             clf_cw_12.fit(X, y)
-            clf_sw_12 = LR(solver=solver, fit_intercept=False, random_state=42)
+            clf_sw_12 = LR(solver=solver, fit_intercept=False, random_state=42,
+                           multi_class='ovr')
             clf_sw_12.fit(X, y, sample_weight=sample_weight)
             assert_array_almost_equal(
                 clf_cw_12.coef_, clf_sw_12.coef_, decimal=4)
@@ -829,21 +887,21 @@ def test_logistic_regression_sample_weights():
     # since the patched liblinear code is different.
     clf_cw = LogisticRegression(
         solver="liblinear", fit_intercept=False, class_weight={0: 1, 1: 2},
-        penalty="l1", tol=1e-5, random_state=42)
+        penalty="l1", tol=1e-5, random_state=42, multi_class='ovr')
     clf_cw.fit(X, y)
     clf_sw = LogisticRegression(
         solver="liblinear", fit_intercept=False, penalty="l1", tol=1e-5,
-        random_state=42)
+        random_state=42, multi_class='ovr')
     clf_sw.fit(X, y, sample_weight)
     assert_array_almost_equal(clf_cw.coef_, clf_sw.coef_, decimal=4)
 
     clf_cw = LogisticRegression(
         solver="liblinear", fit_intercept=False, class_weight={0: 1, 1: 2},
-        penalty="l2", dual=True, random_state=42)
+        penalty="l2", dual=True, random_state=42, multi_class='ovr')
     clf_cw.fit(X, y)
     clf_sw = LogisticRegression(
         solver="liblinear", fit_intercept=False, penalty="l2", dual=True,
-        random_state=42)
+        random_state=42, multi_class='ovr')
     clf_sw.fit(X, y, sample_weight)
     assert_array_almost_equal(clf_cw.coef_, clf_sw.coef_, decimal=4)
 
@@ -974,7 +1032,8 @@ def test_liblinear_decision_function_zero():
     # See Issue: https://github.com/scikit-learn/scikit-learn/issues/3600
     # and the PR https://github.com/scikit-learn/scikit-learn/pull/3623
     X, y = make_classification(n_samples=5, n_features=5, random_state=0)
-    clf = LogisticRegression(fit_intercept=False)
+    clf = LogisticRegression(fit_intercept=False, solver='liblinear',
+                             multi_class='ovr')
     clf.fit(X, y)
 
     # Dummy data such that the decision function becomes zero.
@@ -987,10 +1046,11 @@ def test_liblinear_logregcv_sparse():
     # Test LogRegCV with solver='liblinear' works for sparse matrices
 
     X, y = make_classification(n_samples=10, n_features=5, random_state=0)
-    clf = LogisticRegressionCV(solver='liblinear')
+    clf = LogisticRegressionCV(solver='liblinear', multi_class='ovr')
     clf.fit(sparse.csr_matrix(X), y)
 
 
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_saga_sparse():
     # Test LogRegCV with solver='liblinear' works for sparse matrices
@@ -1004,13 +1064,16 @@ def test_logreg_intercept_scaling():
     # Test that the right error message is thrown when intercept_scaling <= 0
 
     for i in [-1, 0]:
-        clf = LogisticRegression(intercept_scaling=i)
+        clf = LogisticRegression(intercept_scaling=i, solver='liblinear',
+                                 multi_class='ovr')
         msg = ('Intercept scaling is %r but needs to be greater than 0.'
                ' To disable fitting an intercept,'
                ' set fit_intercept=False.' % clf.intercept_scaling)
         assert_raise_message(ValueError, msg, clf.fit, X, Y1)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_logreg_intercept_scaling_zero():
     # Test that intercept_scaling is ignored when fit_intercept is False
 
@@ -1031,12 +1094,12 @@ def test_logreg_l1():
     X_constant = np.ones(shape=(n_samples, 2))
     X = np.concatenate((X, X_noise, X_constant), axis=1)
     lr_liblinear = LogisticRegression(penalty="l1", C=1.0, solver='liblinear',
-                                      fit_intercept=False,
+                                      fit_intercept=False, multi_class='ovr',
                                       tol=1e-10)
     lr_liblinear.fit(X, y)
 
     lr_saga = LogisticRegression(penalty="l1", C=1.0, solver='saga',
-                                 fit_intercept=False,
+                                 fit_intercept=False, multi_class='ovr',
                                  max_iter=1000, tol=1e-10)
     lr_saga.fit(X, y)
     assert_array_almost_equal(lr_saga.coef_, lr_liblinear.coef_)
@@ -1062,12 +1125,12 @@ def test_logreg_l1_sparse_data():
     X = sparse.csr_matrix(X)
 
     lr_liblinear = LogisticRegression(penalty="l1", C=1.0, solver='liblinear',
-                                      fit_intercept=False,
+                                      fit_intercept=False, multi_class='ovr',
                                       tol=1e-10)
     lr_liblinear.fit(X, y)
 
     lr_saga = LogisticRegression(penalty="l1", C=1.0, solver='saga',
-                                 fit_intercept=False,
+                                 fit_intercept=False, multi_class='ovr',
                                  max_iter=1000, tol=1e-10)
     lr_saga.fit(X, y)
     assert_array_almost_equal(lr_saga.coef_, lr_liblinear.coef_)
@@ -1078,19 +1141,20 @@ def test_logreg_l1_sparse_data():
 
     # Check that solving on the sparse and dense data yield the same results
     lr_saga_dense = LogisticRegression(penalty="l1", C=1.0, solver='saga',
-                                       fit_intercept=False,
+                                       fit_intercept=False, multi_class='ovr',
                                        max_iter=1000, tol=1e-10)
     lr_saga_dense.fit(X.toarray(), y)
     assert_array_almost_equal(lr_saga.coef_, lr_saga_dense.coef_)
 
 
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_logreg_cv_penalty():
     # Test that the correct penalty is passed to the final fit.
     X, y = make_classification(n_samples=50, n_features=20, random_state=0)
-    lr_cv = LogisticRegressionCV(penalty="l1", Cs=[1.0], solver='liblinear')
+    lr_cv = LogisticRegressionCV(penalty="l1", Cs=[1.0], solver='saga')
     lr_cv.fit(X, y)
-    lr = LogisticRegression(penalty="l1", C=1.0, solver='liblinear')
+    lr = LogisticRegression(penalty="l1", C=1.0, solver='saga')
     lr.fit(X, y)
     assert_equal(np.count_nonzero(lr_cv.coef_), np.count_nonzero(lr.coef_))
 
@@ -1255,7 +1319,8 @@ def test_saga_vs_liblinear():
                 assert_array_almost_equal(saga.coef_, liblinear.coef_, 3)
 
 
-def test_dtype_match():
+@pytest.mark.parametrize('multi_class', ['ovr', 'multinomial'])
+def test_dtype_match(multi_class):
     # Test that np.float32 input data is not cast to np.float64 when possible
 
     X_32 = np.array(X).astype(np.float32)
@@ -1264,28 +1329,33 @@ def test_dtype_match():
     y_64 = np.array(Y1).astype(np.float64)
     X_sparse_32 = sp.csr_matrix(X, dtype=np.float32)
 
-    for solver in ['newton-cg']:
-        for multi_class in ['ovr', 'multinomial']:
+    solver = 'newton-cg'
+
+    # Check type consistency
+    lr_32 = LogisticRegression(solver=solver, multi_class=multi_class,
+                               random_state=42)
+    lr_32.fit(X_32, y_32)
+    assert_equal(lr_32.coef_.dtype, X_32.dtype)
+
+    # check consistency with sparsity
+    lr_32_sparse = LogisticRegression(solver=solver,
+                                      multi_class=multi_class,
+                                      random_state=42)
+    lr_32_sparse.fit(X_sparse_32, y_32)
+    assert_equal(lr_32_sparse.coef_.dtype, X_sparse_32.dtype)
 
-            # Check type consistency
-            lr_32 = LogisticRegression(solver=solver, multi_class=multi_class,
-                                       random_state=42)
-            lr_32.fit(X_32, y_32)
-            assert_equal(lr_32.coef_.dtype, X_32.dtype)
+    # Check accuracy consistency
+    lr_64 = LogisticRegression(solver=solver, multi_class=multi_class,
+                               random_state=42)
+    lr_64.fit(X_64, y_64)
+    assert_equal(lr_64.coef_.dtype, X_64.dtype)
 
-            # check consistency with sparsity
-            lr_32_sparse = LogisticRegression(solver=solver,
-                                              multi_class=multi_class,
-                                              random_state=42)
-            lr_32_sparse.fit(X_sparse_32, y_32)
-            assert_equal(lr_32_sparse.coef_.dtype, X_sparse_32.dtype)
+    rtol = 1e-6
+    if os.name == 'nt' and _IS_32BIT:
+        # FIXME
+        rtol = 1e-2
 
-            # Check accuracy consistency
-            lr_64 = LogisticRegression(solver=solver, multi_class=multi_class,
-                                       random_state=42)
-            lr_64.fit(X_64, y_64)
-            assert_equal(lr_64.coef_.dtype, X_64.dtype)
-            assert_almost_equal(lr_32.coef_, lr_64.coef_.astype(np.float32))
+    assert_allclose(lr_32.coef_, lr_64.coef_.astype(np.float32), rtol=rtol)
 
 
 def test_warm_start_converge_LR():
@@ -1327,3 +1397,50 @@ def test_logistic_regression_path_coefs_multinomial():
         assert_array_almost_equal(coefs[0], coefs[2], decimal=1)
     with pytest.raises(AssertionError):
         assert_array_almost_equal(coefs[1], coefs[2], decimal=1)
+
+
+@pytest.mark.parametrize('est', [LogisticRegression(random_state=0),
+                                 LogisticRegressionCV(random_state=0, cv=3),
+                                 ])
+@pytest.mark.parametrize('solver', ['liblinear', 'lbfgs', 'newton-cg', 'sag',
+                                    'saga'])
+def test_logistic_regression_multi_class_auto(est, solver):
+    # check multi_class='auto' => multi_class='ovr' iff binary y or liblinear
+
+    def fit(X, y, **kw):
+        return clone(est).set_params(**kw).fit(X, y)
+
+    X = iris.data[::10]
+    X2 = iris.data[1::10]
+    y_multi = iris.target[::10]
+    y_bin = y_multi == 0
+    est_auto_bin = fit(X, y_bin, multi_class='auto', solver=solver)
+    est_ovr_bin = fit(X, y_bin, multi_class='ovr', solver=solver)
+    assert np.allclose(est_auto_bin.coef_, est_ovr_bin.coef_)
+    assert np.allclose(est_auto_bin.predict_proba(X2),
+                       est_ovr_bin.predict_proba(X2))
+
+    est_auto_multi = fit(X, y_multi, multi_class='auto', solver=solver)
+    if solver == 'liblinear':
+        est_ovr_multi = fit(X, y_multi, multi_class='ovr', solver=solver)
+        assert np.allclose(est_auto_multi.coef_, est_ovr_multi.coef_)
+        assert np.allclose(est_auto_multi.predict_proba(X2),
+                           est_ovr_multi.predict_proba(X2))
+    else:
+        est_multi_multi = fit(X, y_multi, multi_class='multinomial',
+                              solver=solver)
+        if sys.platform == 'darwin' and solver == 'lbfgs':
+            pytest.xfail('Issue #11924: LogisticRegressionCV(solver="lbfgs", '
+                         'multi_class="multinomial") is nondterministic on '
+                         'MacOS.')  # pragma: no cover
+        assert np.allclose(est_auto_multi.coef_, est_multi_multi.coef_)
+        assert np.allclose(est_auto_multi.predict_proba(X2),
+                           est_multi_multi.predict_proba(X2))
+
+        # Make sure multi_class='ovr' is distinct from ='multinomial'
+        assert not np.allclose(est_auto_bin.coef_,
+                               fit(X, y_bin, multi_class='multinomial',
+                                   solver=solver).coef_)
+        assert not np.allclose(est_auto_bin.coef_,
+                               fit(X, y_multi, multi_class='multinomial',
+                                   solver=solver).coef_)
diff --git a/sklearn/linear_model/tests/test_sag.py b/sklearn/linear_model/tests/test_sag.py
index ca99a81a73..8f4dbc8794 100755
--- a/sklearn/linear_model/tests/test_sag.py
+++ b/sklearn/linear_model/tests/test_sag.py
@@ -247,7 +247,8 @@ def test_classifier_matching():
             n_iter = 300
         clf = LogisticRegression(solver=solver, fit_intercept=fit_intercept,
                                  tol=1e-11, C=1. / alpha / n_samples,
-                                 max_iter=n_iter, random_state=10)
+                                 max_iter=n_iter, random_state=10,
+                                 multi_class='ovr')
         clf.fit(X, y)
 
         weights, intercept = sag_sparse(X, y, step_size, alpha, n_iter=n_iter,
@@ -311,11 +312,12 @@ def test_sag_pobj_matches_logistic_regression():
 
     clf1 = LogisticRegression(solver='sag', fit_intercept=False, tol=.0000001,
                               C=1. / alpha / n_samples, max_iter=max_iter,
-                              random_state=10)
+                              random_state=10, multi_class='ovr')
     clf2 = clone(clf1)
     clf3 = LogisticRegression(fit_intercept=False, tol=.0000001,
                               C=1. / alpha / n_samples, max_iter=max_iter,
-                              random_state=10)
+                              random_state=10, multi_class='ovr',
+                              solver='lbfgs')
 
     clf1.fit(X, y)
     clf2.fit(sp.csr_matrix(X), y)
@@ -507,7 +509,7 @@ def test_sag_classifier_computed_correctly():
 
     clf1 = LogisticRegression(solver='sag', C=1. / alpha / n_samples,
                               max_iter=n_iter, tol=tol, random_state=77,
-                              fit_intercept=fit_intercept)
+                              fit_intercept=fit_intercept, multi_class='ovr')
     clf2 = clone(clf1)
 
     clf1.fit(X, y)
@@ -547,7 +549,7 @@ def test_sag_multiclass_computed_correctly():
 
     clf1 = LogisticRegression(solver='sag', C=1. / alpha / n_samples,
                               max_iter=max_iter, tol=tol, random_state=77,
-                              fit_intercept=fit_intercept)
+                              fit_intercept=fit_intercept, multi_class='ovr')
     clf2 = clone(clf1)
 
     clf1.fit(X, y)
@@ -591,6 +593,7 @@ def test_sag_multiclass_computed_correctly():
         assert_almost_equal(clf2.intercept_[i], intercept2[i], decimal=1)
 
 
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_classifier_results():
     """tests if classifier results match target"""
     alpha = .1
@@ -634,7 +637,7 @@ def test_binary_classifier_class_weight():
     class_weight = {1: .45, -1: .55}
     clf1 = LogisticRegression(solver='sag', C=1. / alpha / n_samples,
                               max_iter=n_iter, tol=tol, random_state=77,
-                              fit_intercept=fit_intercept,
+                              fit_intercept=fit_intercept, multi_class='ovr',
                               class_weight=class_weight)
     clf2 = clone(clf1)
 
@@ -681,7 +684,7 @@ def test_multiclass_classifier_class_weight():
 
     clf1 = LogisticRegression(solver='sag', C=1. / alpha / n_samples,
                               max_iter=max_iter, tol=tol, random_state=77,
-                              fit_intercept=fit_intercept,
+                              fit_intercept=fit_intercept, multi_class='ovr',
                               class_weight=class_weight)
     clf2 = clone(clf1)
     clf1.fit(X, y)
@@ -728,6 +731,7 @@ def test_multiclass_classifier_class_weight():
         assert_almost_equal(clf2.intercept_[i], intercept2[i], decimal=1)
 
 
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_classifier_single_class():
     """tests if ValueError is thrown with only one class"""
     X = [[1, 2], [3, 4]]
@@ -740,6 +744,7 @@ def test_classifier_single_class():
                          X, y)
 
 
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_step_size_alpha_error():
     X = [[0, 0], [0, 0]]
     y = [1, -1]
diff --git a/sklearn/linear_model/theil_sen.py b/sklearn/linear_model/theil_sen.py
index 62cdc36aaf..00ad26d41b 100755
--- a/sklearn/linear_model/theil_sen.py
+++ b/sklearn/linear_model/theil_sen.py
@@ -249,9 +249,11 @@ class TheilSenRegressor(LinearModel, RegressorMixin):
         random number generator; If None, the random number generator is the
         RandomState instance used by `np.random`.
 
-    n_jobs : integer, optional, default 1
-        Number of CPUs to use during the cross validation. If ``-1``, use
-        all the CPUs.
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the cross validation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : boolean, optional, default False
         Verbose mode when fitting the model.
@@ -274,6 +276,18 @@ class TheilSenRegressor(LinearModel, RegressorMixin):
         Number of combinations taken into account from 'n choose k', where n is
         the number of samples and k is the number of subsamples.
 
+    Examples
+    --------
+    >>> from sklearn.linear_model import TheilSenRegressor
+    >>> from sklearn.datasets import make_regression
+    >>> X, y = make_regression(
+    ...     n_samples=200, n_features=2, noise=4.0, random_state=0)
+    >>> reg = TheilSenRegressor(random_state=0).fit(X, y)
+    >>> reg.score(X, y) # doctest: +ELLIPSIS
+    0.9884...
+    >>> reg.predict(X[:1,])
+    array([-31.5871...])
+
     References
     ----------
     - Theil-Sen Estimators in a Multiple Linear Regression Model, 2009
diff --git a/sklearn/manifold/isomap.py b/sklearn/manifold/isomap.py
index 2e760305c8..3399cb9848 100755
--- a/sklearn/manifold/isomap.py
+++ b/sklearn/manifold/isomap.py
@@ -63,9 +63,11 @@ class Isomap(BaseEstimator, TransformerMixin):
         must be square. X may be a sparse matrix, in which case only "nonzero"
         elements may be considered neighbors.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -82,6 +84,18 @@ class Isomap(BaseEstimator, TransformerMixin):
     dist_matrix_ : array-like, shape (n_samples, n_samples)
         Stores the geodesic distance matrix of training data.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.manifold import Isomap
+    >>> X, _ = load_digits(return_X_y=True)
+    >>> X.shape
+    (1797, 64)
+    >>> embedding = Isomap(n_components=2)
+    >>> X_transformed = embedding.fit_transform(X[:100])
+    >>> X_transformed.shape
+    (100, 2)
+
     References
     ----------
 
diff --git a/sklearn/manifold/locally_linear.py b/sklearn/manifold/locally_linear.py
index a30084abd5..c3afdac2cb 100755
--- a/sklearn/manifold/locally_linear.py
+++ b/sklearn/manifold/locally_linear.py
@@ -81,9 +81,11 @@ def barycenter_kneighbors_graph(X, n_neighbors, reg=1e-3, n_jobs=None):
         problem. Only relevant if mode='barycenter'. If None, use the
         default.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Returns
     -------
@@ -253,9 +255,11 @@ def locally_linear_embedding(
         If None, the random number generator is the RandomState instance used
         by `np.random`. Used when ``solver`` == 'arpack'.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Returns
     -------
@@ -582,9 +586,11 @@ class LocallyLinearEmbedding(BaseEstimator, TransformerMixin):
         If None, the random number generator is the RandomState instance used
         by `np.random`. Used when ``eigen_solver`` == 'arpack'.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -598,6 +604,18 @@ class LocallyLinearEmbedding(BaseEstimator, TransformerMixin):
         Stores nearest neighbors instance, including BallTree or KDtree
         if applicable.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.manifold import LocallyLinearEmbedding
+    >>> X, _ = load_digits(return_X_y=True)
+    >>> X.shape
+    (1797, 64)
+    >>> embedding = LocallyLinearEmbedding(n_components=2)
+    >>> X_transformed = embedding.fit_transform(X[:100])
+    >>> X_transformed.shape
+    (100, 2)
+
     References
     ----------
 
diff --git a/sklearn/manifold/mds.py b/sklearn/manifold/mds.py
index 3ef750d4cb..ecfe22af28 100755
--- a/sklearn/manifold/mds.py
+++ b/sklearn/manifold/mds.py
@@ -178,15 +178,14 @@ def smacof(dissimilarities, metric=True, n_components=2, init=None, n_init=8,
         determined by the run with the smallest final stress. If ``init`` is
         provided, this option is overridden and a single run is performed.
 
-    n_jobs : int, optional, default: 1
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. If multiple
         initializations are used (``n_init``), each run of the algorithm is
         computed in parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For ``n_jobs`` below -1,
-        (``n_cpus + 1 + n_jobs``) are used. Thus for ``n_jobs = -2``, all CPUs
-        but one are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     max_iter : int, optional, default: 300
         Maximum number of iterations of the SMACOF algorithm for a single run.
@@ -305,15 +304,14 @@ class MDS(BaseEstimator):
         Relative tolerance with respect to stress at which to declare
         convergence.
 
-    n_jobs : int, optional, default: 1
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. If multiple
         initializations are used (``n_init``), each run of the algorithm is
         computed in parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For ``n_jobs`` below -1,
-        (``n_cpus + 1 + n_jobs``) are used. Thus for ``n_jobs = -2``, all CPUs
-        but one are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional, default: None
         The generator used to initialize the centers.  If int, random_state is
@@ -340,6 +338,17 @@ class MDS(BaseEstimator):
         The final value of the stress (sum of squared distance of the
         disparities and the distances for all constrained points).
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.manifold import MDS
+    >>> X, _ = load_digits(return_X_y=True)
+    >>> X.shape
+    (1797, 64)
+    >>> embedding = MDS(n_components=2)
+    >>> X_transformed = embedding.fit_transform(X[:100])
+    >>> X_transformed.shape
+    (100, 2)
 
     References
     ----------
diff --git a/sklearn/manifold/spectral_embedding_.py b/sklearn/manifold/spectral_embedding_.py
index 5282459667..add532814a 100755
--- a/sklearn/manifold/spectral_embedding_.py
+++ b/sklearn/manifold/spectral_embedding_.py
@@ -382,9 +382,11 @@ class SpectralEmbedding(BaseEstimator):
     n_neighbors : int, default : max(n_samples/10 , 1)
         Number of nearest neighbors for nearest_neighbors graph building.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -395,6 +397,18 @@ class SpectralEmbedding(BaseEstimator):
     affinity_matrix_ : array, shape = (n_samples, n_samples)
         Affinity_matrix constructed from samples or precomputed.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.manifold import SpectralEmbedding
+    >>> X, _ = load_digits(return_X_y=True)
+    >>> X.shape
+    (1797, 64)
+    >>> embedding = SpectralEmbedding(n_components=2)
+    >>> X_transformed = embedding.fit_transform(X[:100])
+    >>> X_transformed.shape
+    (100, 2)
+
     References
     ----------
 
diff --git a/sklearn/manifold/t_sne.py b/sklearn/manifold/t_sne.py
index 05ba21354f..0f097e594f 100755
--- a/sklearn/manifold/t_sne.py
+++ b/sklearn/manifold/t_sne.py
@@ -614,7 +614,7 @@ class TSNE(BaseEstimator):
         Using t-SNE. Journal of Machine Learning Research 9:2579-2605, 2008.
 
     [2] van der Maaten, L.J.P. t-Distributed Stochastic Neighbor Embedding
-        http://homepage.tudelft.nl/19j49/t-SNE.html
+        https://lvdmaaten.github.io/tsne/
 
     [3] L.J.P. van der Maaten. Accelerating t-SNE using Tree-Based Algorithms.
         Journal of Machine Learning Research 15(Oct):3221-3245, 2014.
diff --git a/sklearn/metrics/classification.py b/sklearn/metrics/classification.py
index 29a2115e2e..60f47980d6 100755
--- a/sklearn/metrics/classification.py
+++ b/sklearn/metrics/classification.py
@@ -139,9 +139,9 @@ def accuracy_score(y_true, y_pred, normalize=True, sample_weight=None):
     Returns
     -------
     score : float
-        If ``normalize == True``, return the correctly classified samples
-        (float), else it returns the number of correctly classified samples
-        (int).
+        If ``normalize == True``, return the fraction of correctly
+        classified samples (float), else returns the number of correctly
+        classified samples (int).
 
         The best performance is 1 with ``normalize == True`` and the number
         of samples with ``normalize == False``.
@@ -628,8 +628,9 @@ def f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary',
 
         F1 = 2 * (precision * recall) / (precision + recall)
 
-    In the multi-class and multi-label case, this is the weighted average of
-    the F1 score of each class.
+    In the multi-class and multi-label case, this is the average of
+    the F1 score of each class with weighting depending on the ``average``
+    parameter.
 
     Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.
 
@@ -1402,6 +1403,13 @@ def balanced_accuracy_score(y_true, y_pred, sample_weight=None,
     --------
     recall_score, roc_auc_score
 
+    Notes
+    -----
+    Some literature promotes alternative definitions of balanced accuracy. Our
+    definition is equivalent to :func:`accuracy_score` with class-balanced
+    sample weights, and shares desirable properties with the binary case.
+    See the :ref:`User Guide <balanced_accuracy_score>`.
+
     References
     ----------
     .. [1] Brodersen, K.H.; Ong, C.S.; Stephan, K.E.; Buhmann, J.M. (2010).
diff --git a/sklearn/metrics/pairwise.py b/sklearn/metrics/pairwise.py
index 764a5a9414..dac5241597 100755
--- a/sklearn/metrics/pairwise.py
+++ b/sklearn/metrics/pairwise.py
@@ -1182,15 +1182,14 @@ def pairwise_distances_chunked(X, Y=None, reduce_func=None,
         should take two arrays from X as input and return a value indicating
         the distance between them.
 
-    n_jobs : int
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. This works by breaking
         down the pairwise matrix into n_jobs even slices and computing them in
         parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For n_jobs below -1,
-        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
-        are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     working_memory : int, optional
         The sought maximum memory for temporary distance matrix chunks.
@@ -1345,15 +1344,14 @@ def pairwise_distances(X, Y=None, metric="euclidean", n_jobs=None, **kwds):
         should take two arrays from X as input and return a value indicating
         the distance between them.
 
-    n_jobs : int
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. This works by breaking
         down the pairwise matrix into n_jobs even slices and computing them in
         parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For n_jobs below -1,
-        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
-        are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     **kwds : optional keyword parameters
         Any further parameters are passed directly to the distance function.
@@ -1520,15 +1518,14 @@ def pairwise_kernels(X, Y=None, metric="linear", filter_params=False,
     filter_params : boolean
         Whether to filter invalid parameters or not.
 
-    n_jobs : int
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. This works by breaking
         down the pairwise matrix into n_jobs even slices and computing them in
         parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For n_jobs below -1,
-        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
-        are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     **kwds : optional keyword parameters
         Any further parameters are passed directly to the kernel function.
diff --git a/sklearn/metrics/tests/test_classification.py b/sklearn/metrics/tests/test_classification.py
index 3a91a61263..c07f9d66aa 100755
--- a/sklearn/metrics/tests/test_classification.py
+++ b/sklearn/metrics/tests/test_classification.py
@@ -140,7 +140,14 @@ def test_classification_report_dictionary_output():
         y_true, y_pred, labels=np.arange(len(iris.target_names)),
         target_names=iris.target_names, output_dict=True)
 
-    assert_dict_equal(report, expected_report)
+    # assert the 2 dicts are equal.
+    assert(report.keys() == expected_report.keys())
+    for key in expected_report:
+        assert report[key].keys() == expected_report[key].keys()
+        for metric in expected_report[key]:
+            assert_almost_equal(expected_report[key][metric],
+                                report[key][metric])
+
     assert type(expected_report['setosa']['precision']) == float
     assert type(expected_report['macro avg']['precision']) == float
     assert type(expected_report['setosa']['support']) == int
diff --git a/sklearn/metrics/tests/test_score_objects.py b/sklearn/metrics/tests/test_score_objects.py
index f418a9375d..da04b4215d 100755
--- a/sklearn/metrics/tests/test_score_objects.py
+++ b/sklearn/metrics/tests/test_score_objects.py
@@ -334,6 +334,8 @@ def test_regression_scorers():
     assert_almost_equal(score1, score2)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_thresholded_scorers():
     # Test scorers that take thresholds.
     X, y = make_blobs(random_state=0, centers=2)
@@ -504,6 +506,8 @@ def test_scorer_memmap_input(name):
     check_scorer_memmap(name)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_scoring_is_not_metric():
     assert_raises_regexp(ValueError, 'make_scorer', check_scoring,
                          LogisticRegression(), f1_score)
diff --git a/sklearn/model_selection/_search.py b/sklearn/model_selection/_search.py
index d7bf45c616..b4cd9d068f 100755
--- a/sklearn/model_selection/_search.py
+++ b/sklearn/model_selection/_search.py
@@ -467,7 +467,7 @@ def _check_is_fitted(self, method_name):
                                  'with refit=False. %s is '
                                  'available only after refitting on the best '
                                  'parameters. You can refit an estimator '
-                                 'manually using the ``best_parameters_`` '
+                                 'manually using the ``best_params_`` '
                                  'attribute'
                                  % (type(self).__name__, method_name))
         else:
@@ -821,7 +821,19 @@ def _store(key_name, array, weights=None, splits=False, rank=False):
                                       dtype=np.int)
         iid = self.iid
         if self.iid == 'warn':
-            if len(np.unique(test_sample_counts)) > 1:
+            warn = False
+            for scorer_name in scorers.keys():
+                scores = test_scores[scorer_name].reshape(n_candidates,
+                                                          n_splits)
+                means_weighted = np.average(scores, axis=1,
+                                            weights=test_sample_counts)
+                means_unweighted = np.average(scores, axis=1)
+                if not np.allclose(means_weighted, means_unweighted,
+                                   rtol=1e-4, atol=1e-4):
+                    warn = True
+                    break
+
+            if warn:
                 warnings.warn("The default of the `iid` parameter will change "
                               "from True to False in version 0.22 and will be"
                               " removed in 0.24. This will change numeric"
@@ -903,8 +915,11 @@ class GridSearchCV(BaseSearchCV):
            0.19 and will be removed in version 0.21. Pass fit parameters to
            the ``fit`` method instead.
 
-    n_jobs : int, default=1
+    n_jobs : int or None, optional (default=None)
         Number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     pre_dispatch : int, or string, optional
         Controls the number of jobs that get dispatched during parallel
@@ -969,7 +984,7 @@ class GridSearchCV(BaseSearchCV):
         ``GridSearchCV`` instance.
 
         Also for multiple metric evaluation, the attributes ``best_index_``,
-        ``best_score_`` and ``best_parameters_`` will only be available if
+        ``best_score_`` and ``best_params_`` will only be available if
         ``refit`` is set and all of them will be determined w.r.t this specific
         scorer.
 
@@ -1247,8 +1262,11 @@ class RandomizedSearchCV(BaseSearchCV):
            0.19 and will be removed in version 0.21. Pass fit parameters to
            the ``fit`` method instead.
 
-    n_jobs : int, default=1
+    n_jobs : int or None, optional (default=None)
         Number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     pre_dispatch : int, or string, optional
         Controls the number of jobs that get dispatched during parallel
@@ -1313,7 +1331,7 @@ class RandomizedSearchCV(BaseSearchCV):
         ``RandomizedSearchCV`` instance.
 
         Also for multiple metric evaluation, the attributes ``best_index_``,
-        ``best_score_`` and ``best_parameters_`` will only be available if
+        ``best_score_`` and ``best_params_`` will only be available if
         ``refit`` is set and all of them will be determined w.r.t this specific
         scorer.
 
diff --git a/sklearn/model_selection/_split.py b/sklearn/model_selection/_split.py
index da4d274a64..75c8e5d239 100755
--- a/sklearn/model_selection/_split.py
+++ b/sklearn/model_selection/_split.py
@@ -714,7 +714,7 @@ class TimeSeriesSplit(_BaseKFold):
     Parameters
     ----------
     n_splits : int, default=3
-        Number of splits. Must be at least 1.
+        Number of splits. Must be at least 2.
 
         .. versionchanged:: 0.20
             ``n_splits`` default value will change from 3 to 5 in v0.22.
diff --git a/sklearn/model_selection/_validation.py b/sklearn/model_selection/_validation.py
index f2e1649e24..4ddfc5edac 100755
--- a/sklearn/model_selection/_validation.py
+++ b/sklearn/model_selection/_validation.py
@@ -97,9 +97,11 @@ def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv='warn',
             ``cv`` default value if None will change from 3-fold to 5-fold
             in v0.22.
 
-    n_jobs : integer, optional
-        The number of CPUs to use to do the computation. -1 means
-        'all CPUs'.
+    n_jobs : int or None, optional (default=None)
+        The number of CPUs to use to do the computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : integer, optional
         The verbosity level.
@@ -319,9 +321,11 @@ def cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv='warn',
             ``cv`` default value if None will change from 3-fold to 5-fold
             in v0.22.
 
-    n_jobs : integer, optional
-        The number of CPUs to use to do the computation. -1 means
-        'all CPUs'.
+    n_jobs : int or None, optional (default=None)
+        The number of CPUs to use to do the computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : integer, optional
         The verbosity level.
@@ -692,9 +696,11 @@ def cross_val_predict(estimator, X, y=None, groups=None, cv='warn',
             ``cv`` default value if None will change from 3-fold to 5-fold
             in v0.22.
 
-    n_jobs : integer, optional
-        The number of CPUs to use to do the computation. -1 means
-        'all CPUs'.
+    n_jobs : int or None, optional (default=None)
+        The number of CPUs to use to do the computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : integer, optional
         The verbosity level.
@@ -984,9 +990,11 @@ def permutation_test_score(estimator, X, y, groups=None, cv='warn',
     n_permutations : integer, optional
         Number of times to permute ``y``.
 
-    n_jobs : integer, optional
-        The number of CPUs to use to do the computation. -1 means
-        'all CPUs'.
+    n_jobs : int or None, optional (default=None)
+        The number of CPUs to use to do the computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional (default=0)
         If int, random_state is the seed used by the random number generator;
@@ -1140,8 +1148,11 @@ def learning_curve(estimator, X, y, groups=None,
         If the estimator supports incremental learning, this will be
         used to speed up fitting for different training set sizes.
 
-    n_jobs : integer, optional
-        Number of jobs to run in parallel (default 1).
+    n_jobs : int or None, optional (default=None)
+        Number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     pre_dispatch : integer or string, optional
         Number of predispatched jobs for parallel execution (default is
@@ -1384,8 +1395,11 @@ def validation_curve(estimator, X, y, param_name, param_range, groups=None,
         a scorer callable object / function with signature
         ``scorer(estimator, X, y)``.
 
-    n_jobs : integer, optional
-        Number of jobs to run in parallel (default 1).
+    n_jobs : int or None, optional (default=None)
+        Number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     pre_dispatch : integer or string, optional
         Number of predispatched jobs for parallel execution (default is
diff --git a/sklearn/model_selection/tests/test_search.py b/sklearn/model_selection/tests/test_search.py
index 0409794bf0..916804b384 100755
--- a/sklearn/model_selection/tests/test_search.py
+++ b/sklearn/model_selection/tests/test_search.py
@@ -133,13 +133,13 @@ def assert_grid_iter_equals_getitem(grid):
 
 @pytest.mark.parametrize(
     "input, error_type, error_message",
-    [(0, TypeError, 'Parameter grid is not a dict or a list (0)'),
-     ([{'foo': [0]}, 0], TypeError, 'Parameter grid is not a dict (0)'),
+    [(0, TypeError, r'Parameter grid is not a dict or a list \(0\)'),
+     ([{'foo': [0]}, 0], TypeError, r'Parameter grid is not a dict \(0\)'),
      ({'foo': 0}, TypeError, "Parameter grid value is not iterable "
-      "(key='foo', value=0)")]
+      r"\(key='foo', value=0\)")]
 )
 def test_validate_parameter_grid_input(input, error_type, error_message):
-    with pytest.raises(error_type, message=error_message):
+    with pytest.raises(error_type, match=error_message):
         ParameterGrid(input)
 
 
diff --git a/sklearn/model_selection/tests/test_split.py b/sklearn/model_selection/tests/test_split.py
index 710c194cfc..28286bf240 100755
--- a/sklearn/model_selection/tests/test_split.py
+++ b/sklearn/model_selection/tests/test_split.py
@@ -482,13 +482,10 @@ def test_shuffle_kfold_stratifiedkfold_reproducibility():
                                                 cv.split(*data)):
                 # cv.split(...) returns an array of tuples, each tuple
                 # consisting of an array with train indices and test indices
-                try:
+                with pytest.raises(AssertionError,
+                                   message="The splits for data, are same even"
+                                           " when random state is not set"):
                     np.testing.assert_array_equal(test_a, test_b)
-                except AssertionError:
-                    pass
-                else:
-                    raise AssertionError("The splits for data, are same even "
-                                         "when random state is not set")
 
 
 def test_shuffle_stratifiedkfold():
diff --git a/sklearn/model_selection/tests/test_validation.py b/sklearn/model_selection/tests/test_validation.py
index bb7e736eb3..0d7a05f39d 100755
--- a/sklearn/model_selection/tests/test_validation.py
+++ b/sklearn/model_selection/tests/test_validation.py
@@ -790,6 +790,8 @@ def test_cross_val_score_multilabel():
     assert_almost_equal(score_samples, [1, 1 / 2, 3 / 4, 1 / 2, 1 / 4])
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict():
     boston = load_boston()
@@ -840,6 +842,8 @@ def split(self, X, y=None, groups=None):
                          X, y, method='predict_proba', cv=KFold(2))
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict_decision_function_shape():
     X, y = make_classification(n_classes=2, n_samples=50, random_state=0)
@@ -887,6 +891,8 @@ def test_cross_val_predict_decision_function_shape():
                         cv=KFold(n_splits=3), method='decision_function')
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict_predict_proba_shape():
     X, y = make_classification(n_classes=2, n_samples=50, random_state=0)
@@ -902,6 +908,8 @@ def test_cross_val_predict_predict_proba_shape():
     assert_equal(preds.shape, (150, 3))
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict_predict_log_proba_shape():
     X, y = make_classification(n_classes=2, n_samples=50, random_state=0)
@@ -917,6 +925,8 @@ def test_cross_val_predict_predict_log_proba_shape():
     assert_equal(preds.shape, (150, 3))
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict_input_types():
     iris = load_iris()
@@ -1336,6 +1346,8 @@ def check_cross_val_predict_with_method(est):
         assert_array_equal(predictions, predictions_ystr)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict_with_method():
     check_cross_val_predict_with_method(LogisticRegression())
@@ -1350,6 +1362,8 @@ def test_cross_val_predict_method_checking():
     check_cross_val_predict_with_method(est)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')
 @pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_gridsearchcv_cross_val_predict_with_method():
@@ -1379,11 +1393,13 @@ def get_expected_predictions(X, y, cv, classes, est, method):
     return expected_predictions
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict_class_subset():
 
     X = np.arange(200).reshape(100, 2)
-    y = np.array([x//10 for x in range(100)])
+    y = np.array([x // 10 for x in range(100)])
     classes = 10
 
     kfold3 = KFold(n_splits=3)
diff --git a/sklearn/multiclass.py b/sklearn/multiclass.py
index 61f1f02e40..0361dfa4d4 100755
--- a/sklearn/multiclass.py
+++ b/sklearn/multiclass.py
@@ -157,11 +157,11 @@ class OneVsRestClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):
         An estimator object implementing `fit` and one of `decision_function`
         or `predict_proba`.
 
-    n_jobs : int, optional, default: 1
-        The number of jobs to use for the computation. If -1 all CPUs are used.
-        If 1 is given, no parallel computing code is used at all, which is
-        useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are
-        used. Thus for n_jobs = -2, all CPUs but one are used.
+    n_jobs : int or None, optional (default=None)
+        The number of jobs to use for the computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -456,11 +456,11 @@ class OneVsOneClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):
         An estimator object implementing `fit` and one of `decision_function`
         or `predict_proba`.
 
-    n_jobs : int, optional, default: 1
-        The number of jobs to use for the computation. If -1 all CPUs are used.
-        If 1 is given, no parallel computing code is used at all, which is
-        useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are
-        used. Thus for n_jobs = -2, all CPUs but one are used.
+    n_jobs : int or None, optional (default=None)
+        The number of jobs to use for the computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -662,11 +662,11 @@ class OutputCodeClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):
         random_state is the random number generator; If None, the random number
         generator is the RandomState instance used by `np.random`.
 
-    n_jobs : int, optional, default: 1
-        The number of jobs to use for the computation. If -1 all CPUs are used.
-        If 1 is given, no parallel computing code is used at all, which is
-        useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are
-        used. Thus for n_jobs = -2, all CPUs but one are used.
+    n_jobs : int or None, optional (default=None)
+        The number of jobs to use for the computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
diff --git a/sklearn/multioutput.py b/sklearn/multioutput.py
index dfaef80354..1b0fdd19e1 100755
--- a/sklearn/multioutput.py
+++ b/sklearn/multioutput.py
@@ -209,9 +209,12 @@ class MultiOutputRegressor(MultiOutputEstimator, RegressorMixin):
     estimator : estimator object
         An estimator object implementing `fit` and `predict`.
 
-    n_jobs : int, optional, default=1
-        The number of jobs to run in parallel for `fit`. If -1,
-        then the number of jobs is set to the number of cores.
+    n_jobs : int or None, optional (default=None)
+        The number of jobs to run in parallel for `fit`.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
+
         When individual estimators are fast to train or predict
         using `n_jobs>1` can result in slower performance due
         to the overhead of spawning processes.
@@ -295,13 +298,12 @@ class MultiOutputClassifier(MultiOutputEstimator, ClassifierMixin):
     estimator : estimator object
         An estimator object implementing `fit`, `score` and `predict_proba`.
 
-    n_jobs : int, optional, default=1
-        The number of jobs to use for the computation. If -1 all CPUs are used.
-        If 1 is given, no parallel computing code is used at all, which is
-        useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are
-        used. Thus for n_jobs = -2, all CPUs but one are used.
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation.
         It does each target variable in y in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
diff --git a/sklearn/neighbors/__init__.py b/sklearn/neighbors/__init__.py
index add2f49348..f10cb9993d 100755
--- a/sklearn/neighbors/__init__.py
+++ b/sklearn/neighbors/__init__.py
@@ -15,6 +15,7 @@
 from .kde import KernelDensity
 from .approximate import LSHForest
 from .lof import LocalOutlierFactor
+from .base import VALID_METRICS, VALID_METRICS_SPARSE
 
 __all__ = ['BallTree',
            'DistanceMetric',
@@ -31,4 +32,6 @@
            'radius_neighbors_graph',
            'KernelDensity',
            'LSHForest',
-           'LocalOutlierFactor']
+           'LocalOutlierFactor',
+           'VALID_METRICS',
+           'VALID_METRICS_SPARSE']
diff --git a/sklearn/neighbors/base.py b/sklearn/neighbors/base.py
index 1f54331863..39e6499350 100755
--- a/sklearn/neighbors/base.py
+++ b/sklearn/neighbors/base.py
@@ -326,8 +326,11 @@ def _check_algorithm_metric(self):
                     "kd_tree algorithm does not support callable metric '%s'"
                     % self.metric)
         elif self.metric not in VALID_METRICS[alg_check]:
-            raise ValueError("Metric '%s' not valid for algorithm '%s'"
-                             % (self.metric, self.algorithm))
+            raise ValueError("Metric '%s' not valid. Use "
+                             "sorted(sklearn.neighbors.VALID_METRICS['%s']) "
+                             "to get valid options. "
+                             "Metric can also be a callable function."
+                             % (self.metric, alg_check))
 
         if self.metric_params is not None and 'p' in self.metric_params:
             warnings.warn("Parameter p is found in metric_params. "
@@ -400,9 +403,12 @@ def _fit(self, X):
                               "using brute force")
             if self.effective_metric_ not in VALID_METRICS_SPARSE['brute'] \
                     and not callable(self.effective_metric_):
-
-                raise ValueError("metric '%s' not valid for sparse input"
-                                 % self.effective_metric_)
+                raise ValueError("Metric '%s' not valid for sparse input. "
+                                 "Use sorted(sklearn.neighbors."
+                                 "VALID_METRICS_SPARSE['brute']) "
+                                 "to get valid options. "
+                                 "Metric can also be a callable function."
+                                 % (self.effective_metric_))
             self._fit_X = X.copy()
             self._tree = None
             self._fit_method = 'brute'
diff --git a/sklearn/neighbors/binary_tree.pxi b/sklearn/neighbors/binary_tree.pxi
index 5d618ed494..abb03edb04 100755
--- a/sklearn/neighbors/binary_tree.pxi
+++ b/sklearn/neighbors/binary_tree.pxi
@@ -292,7 +292,7 @@ Additional keywords are passed to the distance metric class.
 
 Attributes
 ----------
-data : np.ndarray
+data : memory view
     The training data
 
 Examples
@@ -486,7 +486,7 @@ cdef DTYPE_t _log_kernel_norm(DTYPE_t h, ITYPE_t d,
     elif kernel == EXPONENTIAL_KERNEL:
         factor = logSn(d - 1) + lgamma(d)
     elif kernel == LINEAR_KERNEL:
-        factor = logVn(d) - np.log1p(d)
+        factor = logVn(d) - log(d + 1.)
     elif kernel == COSINE_KERNEL:
         # this is derived from a chain rule integration
         factor = 0
diff --git a/sklearn/neighbors/classification.py b/sklearn/neighbors/classification.py
index 0cef0f72db..a3be1c62a6 100755
--- a/sklearn/neighbors/classification.py
+++ b/sklearn/neighbors/classification.py
@@ -78,9 +78,11 @@ class KNeighborsClassifier(NeighborsBase, KNeighborsMixin,
     metric_params : dict, optional (default = None)
         Additional keyword arguments for the metric function.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
         Doesn't affect :meth:`fit` method.
 
     Examples
@@ -294,9 +296,11 @@ class RadiusNeighborsClassifier(NeighborsBase, RadiusNeighborsMixin,
     metric_params : dict, optional (default = None)
         Additional keyword arguments for the metric function.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Examples
     --------
diff --git a/sklearn/neighbors/graph.py b/sklearn/neighbors/graph.py
index afb2e7674b..8272c61ce3 100755
--- a/sklearn/neighbors/graph.py
+++ b/sklearn/neighbors/graph.py
@@ -76,9 +76,11 @@ def kneighbors_graph(X, n_neighbors, mode='connectivity', metric='minkowski',
         itself. If 'auto', then True is used for mode='connectivity' and False
         for mode='distance'.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Returns
     -------
@@ -152,9 +154,11 @@ def radius_neighbors_graph(X, radius, mode='connectivity', metric='minkowski',
         itself. If `None`, then True is used for mode='connectivity' and False
         for mode='distance' as this will preserve backwards compatibility.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Returns
     -------
diff --git a/sklearn/neighbors/lof.py b/sklearn/neighbors/lof.py
index 3712c19da0..131b9c72ad 100755
--- a/sklearn/neighbors/lof.py
+++ b/sklearn/neighbors/lof.py
@@ -4,7 +4,6 @@
 
 import numpy as np
 import warnings
-from scipy.stats import scoreatpercentile
 
 from .base import NeighborsBase
 from .base import KNeighborsMixin
@@ -112,15 +111,17 @@ class LocalOutlierFactor(NeighborsBase, KNeighborsMixin, UnsupervisedMixin,
         that you should only use predict, decision_function and score_samples
         on new unseen data and not on the training set.
 
-    n_jobs : int, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
     negative_outlier_factor_ : numpy array, shape (n_samples,)
         The opposite LOF of the training samples. The higher, the more normal.
-        Inliers tend to have a LOF score close to 1 (`negative_outlier_factor_`
+        Inliers tend to have a LOF score close to 1 (``negative_outlier_factor_``
         close to -1), while outliers tend to have a larger LOF score.
 
         The local outlier factor (LOF) of a sample captures its
@@ -169,6 +170,9 @@ def fit_predict(self):
             The query sample or samples to compute the Local Outlier Factor
             w.r.t. to the training samples.
 
+        y : Ignored
+            not used, present for API consistency by convention.
+
         Returns
         -------
         is_inlier : array, shape (n_samples,)
@@ -217,6 +221,9 @@ def fit(self, X, y=None):
             Training data. If array or matrix, shape [n_samples, n_features],
             or [n_samples, n_samples] if metric='precomputed'.
 
+        y : Ignored
+            not used, present for API consistency by convention.
+
         Returns
         -------
         self : object
@@ -261,8 +268,8 @@ def fit(self, X, y=None):
             # inliers score around -1 (the higher, the less abnormal).
             self.offset_ = -1.5
         else:
-            self.offset_ = scoreatpercentile(
-                self.negative_outlier_factor_, 100. * self._contamination)
+            self.offset_ = np.percentile(self.negative_outlier_factor_,
+                                         100. * self._contamination)
 
         return self
 
@@ -402,7 +409,7 @@ def score_samples(self):
         Also, the samples in X are not considered in the neighborhood of any
         point.
         The score_samples on training data is available by considering the
-        the negative_outlier_factor_ attribute.
+        the ``negative_outlier_factor_`` attribute.
 
         Parameters
         ----------
@@ -438,7 +445,7 @@ def _score_samples(self, X):
         Also, the samples in X are not considered in the neighborhood of any
         point.
         The score_samples on training data is available by considering the
-        the negative_outlier_factor_ attribute.
+        the ``negative_outlier_factor_`` attribute.
 
         Parameters
         ----------
diff --git a/sklearn/neighbors/regression.py b/sklearn/neighbors/regression.py
index 47271982fe..5dbc449d6a 100755
--- a/sklearn/neighbors/regression.py
+++ b/sklearn/neighbors/regression.py
@@ -84,9 +84,11 @@ class KNeighborsRegressor(NeighborsBase, KNeighborsMixin,
     metric_params : dict, optional (default = None)
         Additional keyword arguments for the metric function.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
         Doesn't affect :meth:`fit` method.
 
     Examples
@@ -238,9 +240,11 @@ class RadiusNeighborsRegressor(NeighborsBase, RadiusNeighborsMixin,
     metric_params : dict, optional (default = None)
         Additional keyword arguments for the metric function.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Examples
     --------
diff --git a/sklearn/neighbors/tests/test_pipeline.py b/sklearn/neighbors/tests/test_pipeline.py
index 0546a3c7bf..d61a64701d 100755
--- a/sklearn/neighbors/tests/test_pipeline.py
+++ b/sklearn/neighbors/tests/test_pipeline.py
@@ -148,8 +148,10 @@ def test_lof():
     # compare the chained version and the compact version
     est_chain = make_pipeline(
         KNeighborsTransformer(n_neighbors=n_neighbors + 1, mode='distance'),
-        LocalOutlierFactor(metric='precomputed', n_neighbors=n_neighbors))
-    est_compact = LocalOutlierFactor(n_neighbors=n_neighbors)
+        LocalOutlierFactor(metric='precomputed', n_neighbors=n_neighbors,
+                           contamination="auto"))
+    est_compact = LocalOutlierFactor(n_neighbors=n_neighbors,
+                                     contamination="auto")
 
     pred_chain = est_chain.fit_predict(X)
     pred_compact = est_compact.fit_predict(X)
diff --git a/sklearn/neighbors/unsupervised.py b/sklearn/neighbors/unsupervised.py
index 0feb181937..7b1e00f271 100755
--- a/sklearn/neighbors/unsupervised.py
+++ b/sklearn/neighbors/unsupervised.py
@@ -73,9 +73,11 @@ class NearestNeighbors(NeighborsBase, KNeighborsMixin,
     metric_params : dict, optional (default = None)
         Additional keyword arguments for the metric function.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Examples
     --------
diff --git a/sklearn/neural_network/rbm.py b/sklearn/neural_network/rbm.py
index ccf933ed19..c35e8840d2 100755
--- a/sklearn/neural_network/rbm.py
+++ b/sklearn/neural_network/rbm.py
@@ -19,7 +19,6 @@
 from ..utils import check_array
 from ..utils import check_random_state
 from ..utils import gen_even_slices
-from ..utils import issparse
 from ..utils.extmath import safe_sparse_dot
 from ..utils.extmath import log_logistic
 from ..utils.validation import check_is_fitted
@@ -310,7 +309,7 @@ def score_samples(self, X):
         # Randomly corrupt one feature in each sample in v.
         ind = (np.arange(v.shape[0]),
                rng.randint(0, v.shape[1], v.shape[0]))
-        if issparse(v):
+        if sp.issparse(v):
             data = -2 * v[ind] + 1
             v_ = v + sp.csr_matrix((data.A.ravel(), ind), shape=v.shape)
         else:
diff --git a/sklearn/pipeline.py b/sklearn/pipeline.py
index 915c2daf9e..294f69a113 100755
--- a/sklearn/pipeline.py
+++ b/sklearn/pipeline.py
@@ -630,7 +630,7 @@ class FeatureUnion(_BaseComposition, TransformerMixin):
     Parameters of the transformers may be set using its name and the parameter
     name separated by a '__'. A transformer may be replaced entirely by
     setting the parameter with its name to another transformer,
-    or removed by setting to ``None``.
+    or removed by setting to 'drop' or ``None``.
 
     Read more in the :ref:`User Guide <feature_union>`.
 
@@ -640,8 +640,11 @@ class FeatureUnion(_BaseComposition, TransformerMixin):
         List of transformer objects to be applied to the data. The first
         half of each tuple is the name of the transformer.
 
-    n_jobs : int, optional
-        Number of jobs to run in parallel (default 1).
+    n_jobs : int or None, optional (default=None)
+        Number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     transformer_weights : dict, optional
         Multiplicative weights for features per transformer.
@@ -706,7 +709,7 @@ def _validate_transformers(self):
 
         # validate estimators
         for t in transformers:
-            if t is None:
+            if t is None or t == 'drop':
                 continue
             if (not (hasattr(t, "fit") or hasattr(t, "fit_transform")) or not
                     hasattr(t, "transform")):
@@ -716,12 +719,13 @@ def _validate_transformers(self):
 
     def _iter(self):
         """
-        Generate (name, trans, weight) tuples excluding None transformers
+        Generate (name, trans, weight) tuples excluding None and
+        'drop' transformers.
         """
         get_weight = (self.transformer_weights or {}).get
         return ((name, trans, get_weight(name))
                 for name, trans in self.transformer_list
-                if trans is not None)
+                if trans is not None and trans != 'drop')
 
     def get_feature_names(self):
         """Get feature names from all transformers.
@@ -827,10 +831,9 @@ def transform(self, X):
 
     def _update_transformer_list(self, transformers):
         transformers = iter(transformers)
-        self.transformer_list[:] = [
-            (name, None if old is None else next(transformers))
-            for name, old in self.transformer_list
-        ]
+        self.transformer_list[:] = [(name, old if old is None or old == 'drop'
+                                     else next(transformers))
+                                    for name, old in self.transformer_list]
 
 
 def make_union(*transformers, **kwargs):
@@ -844,8 +847,11 @@ def make_union(*transformers, **kwargs):
     ----------
     *transformers : list of estimators
 
-    n_jobs : int, optional
-        Number of jobs to run in parallel (default 1).
+    n_jobs : int or None, optional (default=None)
+        Number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Returns
     -------
diff --git a/sklearn/preprocessing/_encoders.py b/sklearn/preprocessing/_encoders.py
index bd6e10fb62..10324e1706 100755
--- a/sklearn/preprocessing/_encoders.py
+++ b/sklearn/preprocessing/_encoders.py
@@ -10,11 +10,12 @@
 import numpy as np
 from scipy import sparse
 
+from .. import get_config as _get_config
 from ..base import BaseEstimator, TransformerMixin
 from ..externals import six
 from ..utils import check_array
 from ..utils import deprecated
-from ..utils.fixes import _argmax
+from ..utils.fixes import _argmax, _object_dtype_isnan
 from ..utils.validation import check_is_fitted
 
 from .base import _transform_selected
@@ -37,14 +38,30 @@ class _BaseEncoder(BaseEstimator, TransformerMixin):
 
     """
 
-    def _fit(self, X, handle_unknown='error'):
+    def _check_X(self, X):
+        """
+        Perform custom check_array:
+        - convert list of strings to object dtype
+        - check for missing values for object dtype data (check_array does
+          not do that)
 
+        """
         X_temp = check_array(X, dtype=None)
         if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):
             X = check_array(X, dtype=np.object)
         else:
             X = X_temp
 
+        if X.dtype == np.dtype('object'):
+            if not _get_config()['assume_finite']:
+                if _object_dtype_isnan(X).any():
+                    raise ValueError("Input contains NaN")
+
+        return X
+
+    def _fit(self, X, handle_unknown='error'):
+        X = self._check_X(X)
+
         n_samples, n_features = X.shape
 
         if self._categories != 'auto':
@@ -74,12 +91,7 @@ def _fit(self, X, handle_unknown='error'):
             self.categories_.append(cats)
 
     def _transform(self, X, handle_unknown='error'):
-
-        X_temp = check_array(X, dtype=None)
-        if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):
-            X = check_array(X, dtype=np.object)
-        else:
-            X = X_temp
+        X = self._check_X(X)
 
         _, n_features = X.shape
         X_int = np.zeros_like(X, dtype=np.int)
diff --git a/sklearn/preprocessing/tests/test_encoders.py b/sklearn/preprocessing/tests/test_encoders.py
index 9ec16b85df..67169432de 100755
--- a/sklearn/preprocessing/tests/test_encoders.py
+++ b/sklearn/preprocessing/tests/test_encoders.py
@@ -497,6 +497,25 @@ def test_one_hot_encoder_feature_names_unicode():
     assert_array_equal([u'n👍me_c❤t1', u'n👍me_dat2'], feature_names)
 
 
+@pytest.mark.parametrize("X", [np.array([[1, np.nan]]).T,
+                               np.array([['a', np.nan]], dtype=object).T],
+                         ids=['numeric', 'object'])
+@pytest.mark.parametrize("handle_unknown", ['error', 'ignore'])
+def test_one_hot_encoder_raise_missing(X, handle_unknown):
+    ohe = OneHotEncoder(categories='auto', handle_unknown=handle_unknown)
+
+    with pytest.raises(ValueError, match="Input contains NaN"):
+        ohe.fit(X)
+
+    with pytest.raises(ValueError, match="Input contains NaN"):
+        ohe.fit_transform(X)
+
+    ohe.fit(X[:1, :])
+
+    with pytest.raises(ValueError, match="Input contains NaN"):
+        ohe.transform(X)
+
+
 @pytest.mark.parametrize("X", [
     [['abc', 2, 55], ['def', 1, 55]],
     np.array([[10, 2, 55], [20, 1, 55]]),
@@ -524,6 +543,24 @@ def test_ordinal_encoder_inverse():
     assert_raises_regex(ValueError, msg, enc.inverse_transform, X_tr)
 
 
+@pytest.mark.parametrize("X", [np.array([[1, np.nan]]).T,
+                               np.array([['a', np.nan]], dtype=object).T],
+                         ids=['numeric', 'object'])
+def test_ordinal_encoder_raise_missing(X):
+    ohe = OrdinalEncoder()
+
+    with pytest.raises(ValueError, match="Input contains NaN"):
+        ohe.fit(X)
+
+    with pytest.raises(ValueError, match="Input contains NaN"):
+        ohe.fit_transform(X)
+
+    ohe.fit(X[:1, :])
+
+    with pytest.raises(ValueError, match="Input contains NaN"):
+        ohe.transform(X)
+
+
 def test_encoder_dtypes():
     # check that dtypes are preserved when determining categories
     enc = OneHotEncoder(categories='auto')
diff --git a/sklearn/random_projection.py b/sklearn/random_projection.py
index daa3c0243d..4a2edada4c 100755
--- a/sklearn/random_projection.py
+++ b/sklearn/random_projection.py
@@ -150,7 +150,7 @@ def _check_input_size(n_components, n_features):
                          n_components)
     if n_features <= 0:
         raise ValueError("n_features must be strictly positive, got %d" %
-                         n_components)
+                         n_features)
 
 
 def gaussian_random_matrix(n_components, n_features, random_state=None):
@@ -465,6 +465,16 @@ class GaussianRandomProjection(BaseRandomProjection):
     components_ : numpy array of shape [n_components, n_features]
         Random matrix used for the projection.
 
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.random_projection import GaussianRandomProjection
+    >>> X = np.random.rand(100, 10000)
+    >>> transformer = GaussianRandomProjection()
+    >>> X_new = transformer.fit_transform(X)
+    >>> X_new.shape
+    (100, 3947)
+
     See Also
     --------
     SparseRandomProjection
@@ -577,6 +587,20 @@ class SparseRandomProjection(BaseRandomProjection):
     density_ : float in range 0.0 - 1.0
         Concrete density computed from when density = "auto".
 
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.random_projection import SparseRandomProjection
+    >>> np.random.seed(42)
+    >>> X = np.random.rand(100, 10000)
+    >>> transformer = SparseRandomProjection()
+    >>> X_new = transformer.fit_transform(X)
+    >>> X_new.shape
+    (100, 3947)
+    >>> # very few components are non-zero
+    >>> np.mean(transformer.components_ != 0) # doctest: +ELLIPSIS
+    0.0100...
+
     See Also
     --------
     GaussianRandomProjection
diff --git a/sklearn/semi_supervised/label_propagation.py b/sklearn/semi_supervised/label_propagation.py
index 2c3f1ac520..ff32005399 100755
--- a/sklearn/semi_supervised/label_propagation.py
+++ b/sklearn/semi_supervised/label_propagation.py
@@ -101,9 +101,11 @@ class BaseLabelPropagation(six.with_metaclass(ABCMeta, BaseEstimator,
         Convergence tolerance: threshold to consider the system at steady
         state
 
-    n_jobs : int, optional (default = 1)
+   n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
     """
 
     def __init__(self, kernel='rbf', gamma=20, n_neighbors=7,
@@ -334,9 +336,11 @@ class LabelPropagation(BaseLabelPropagation):
         Convergence tolerance: threshold to consider the system at steady
         state
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -452,9 +456,11 @@ class LabelSpreading(BaseLabelPropagation):
       Convergence tolerance: threshold to consider the system at steady
       state
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
diff --git a/sklearn/svm/bounds.py b/sklearn/svm/bounds.py
index f1897e3d8e..3c37f60193 100755
--- a/sklearn/svm/bounds.py
+++ b/sklearn/svm/bounds.py
@@ -32,8 +32,6 @@ def l1_min_c(X, y, loss='squared_hinge', fit_intercept=True,
         Specifies the loss function.
         With 'squared_hinge' it is the squared hinge loss (a.k.a. L2 loss).
         With 'log' it is the loss of logistic regression models.
-        'l2' is accepted as an alias for 'squared_hinge', for backward
-        compatibility reasons, but should not be used in new code.
 
     fit_intercept : bool, default: True
         Specifies if the intercept should be fitted by the model.
@@ -52,7 +50,7 @@ def l1_min_c(X, y, loss='squared_hinge', fit_intercept=True,
         minimum value for C
     """
     if loss not in ('squared_hinge', 'log'):
-        raise ValueError('loss type not in ("squared_hinge", "log", "l2")')
+        raise ValueError('loss type not in ("squared_hinge", "log")')
 
     X = check_array(X, accept_sparse='csc')
     check_consistent_length(X, y)
diff --git a/sklearn/svm/classes.py b/sklearn/svm/classes.py
index 4bb423e790..1028843a9b 100755
--- a/sklearn/svm/classes.py
+++ b/sklearn/svm/classes.py
@@ -841,9 +841,6 @@ class SVR(BaseLibSVM, RegressorMixin):
     intercept_ : array, shape = [1]
         Constants in decision function.
 
-    sample_weight : array-like, shape = [n_samples]
-            Individual weights for each sample
-
     Examples
     --------
     >>> from sklearn.svm import SVR
@@ -1122,6 +1119,9 @@ def fit(self, X, y=None, sample_weight=None, **params):
             Per-sample weights. Rescale C per sample. Higher weights
             force the classifier to put more emphasis on these points.
 
+        y : Ignored
+            not used, present for API consistency by convention.
+
         Returns
         -------
         self : object
diff --git a/sklearn/svm/tests/test_bounds.py b/sklearn/svm/tests/test_bounds.py
index d02c53b05d..fffd7fc787 100755
--- a/sklearn/svm/tests/test_bounds.py
+++ b/sklearn/svm/tests/test_bounds.py
@@ -45,7 +45,8 @@ def check_l1_min_c(X, y, loss, fit_intercept=True, intercept_scaling=None):
     min_c = l1_min_c(X, y, loss, fit_intercept, intercept_scaling)
 
     clf = {
-        'log': LogisticRegression(penalty='l1'),
+        'log': LogisticRegression(penalty='l1', solver='liblinear',
+                                  multi_class='ovr'),
         'squared_hinge': LinearSVC(loss='squared_hinge',
                                    penalty='l1', dual=False),
     }[loss]
diff --git a/sklearn/svm/tests/test_sparse.py b/sklearn/svm/tests/test_sparse.py
index 5fa83050a9..ce14bda1db 100755
--- a/sklearn/svm/tests/test_sparse.py
+++ b/sklearn/svm/tests/test_sparse.py
@@ -1,7 +1,9 @@
+import pytest
+
 import numpy as np
-from scipy import sparse
 from numpy.testing import (assert_array_almost_equal, assert_array_equal,
                            assert_equal)
+from scipy import sparse
 
 from sklearn import datasets, svm, linear_model, base
 from sklearn.datasets import make_classification, load_digits, make_blobs
@@ -10,7 +12,8 @@
 from sklearn.utils.extmath import safe_sparse_dot
 from sklearn.utils.testing import (assert_raises, assert_true, assert_false,
                                    assert_warns, assert_raise_message,
-                                   ignore_warnings)
+                                   ignore_warnings, skip_if_32bit)
+
 
 # test sample 1
 X = np.array([[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]])
@@ -71,6 +74,7 @@ def check_svm_model_equal(dense_svm, sparse_svm, X_train, y_train, X_test):
         assert_raise_message(ValueError, msg, dense_svm.predict, X_test)
 
 
+@skip_if_32bit
 def test_svc():
     """Check that sparse SVC gives the same result as SVC"""
     # many class dataset:
@@ -234,6 +238,8 @@ def test_linearsvc_iris():
     assert_array_equal(pred, sp_clf.predict(iris.data))
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_weight():
     # Test class weights
     X_, y_ = make_classification(n_samples=200, n_features=100,
@@ -265,21 +271,21 @@ def test_sparse_liblinear_intercept_handling():
     test_svm.test_dense_liblinear_intercept_handling(svm.LinearSVC)
 
 
-def test_sparse_oneclasssvm():
+@pytest.mark.parametrize("datasets_index", range(4))
+@pytest.mark.parametrize("kernel", ["linear", "poly", "rbf", "sigmoid"])
+@skip_if_32bit
+def test_sparse_oneclasssvm(datasets_index, kernel):
     # Check that sparse OneClassSVM gives the same result as dense OneClassSVM
     # many class dataset:
     X_blobs, _ = make_blobs(n_samples=100, centers=10, random_state=0)
     X_blobs = sparse.csr_matrix(X_blobs)
-
     datasets = [[X_sp, None, T], [X2_sp, None, T2],
                 [X_blobs[:80], None, X_blobs[80:]],
                 [iris.data, None, iris.data]]
-    kernels = ["linear", "poly", "rbf", "sigmoid"]
-    for dataset in datasets:
-        for kernel in kernels:
-            clf = svm.OneClassSVM(gamma='scale', kernel=kernel)
-            sp_clf = svm.OneClassSVM(gamma='scale', kernel=kernel)
-            check_svm_model_equal(clf, sp_clf, *dataset)
+    dataset = datasets[datasets_index]
+    clf = svm.OneClassSVM(gamma='scale', kernel=kernel)
+    sp_clf = svm.OneClassSVM(gamma='scale', kernel=kernel)
+    check_svm_model_equal(clf, sp_clf, *dataset)
 
 
 def test_sparse_realdata():
diff --git a/sklearn/svm/tests/test_svm.py b/sklearn/svm/tests/test_svm.py
index 6187a08f7b..4a8e4ef735 100755
--- a/sklearn/svm/tests/test_svm.py
+++ b/sklearn/svm/tests/test_svm.py
@@ -5,6 +5,8 @@
 """
 import numpy as np
 import itertools
+import pytest
+
 from numpy.testing import assert_array_equal, assert_array_almost_equal
 from numpy.testing import assert_almost_equal
 from numpy.testing import assert_allclose
@@ -403,6 +405,8 @@ def test_svr_predict():
     assert_array_almost_equal(dec.ravel(), reg.predict(X).ravel())
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_weight():
     # Test class weights
     clf = svm.SVC(gamma='scale', class_weight={1: 0.1})
@@ -442,6 +446,8 @@ def test_sample_weights():
     assert_array_almost_equal(dual_coef_no_weight, clf.dual_coef_)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @ignore_warnings(category=UndefinedMetricWarning)
 def test_auto_weight():
     # Test class weights for imbalanced data
diff --git a/sklearn/tests/test_common.py b/sklearn/tests/test_common.py
index 8e5f020985..c84962ed63 100755
--- a/sklearn/tests/test_common.py
+++ b/sklearn/tests/test_common.py
@@ -72,10 +72,11 @@ def _tested_non_meta_estimators():
 
 
 def _generate_checks_per_estimator(check_generator, estimators):
-    for name, Estimator in estimators:
-        estimator = Estimator()
-        for check in check_generator(name, estimator):
-            yield name, Estimator, check
+    with ignore_warnings(category=(DeprecationWarning, FutureWarning)):
+        for name, Estimator in estimators:
+            estimator = Estimator()
+            for check in check_generator(name, estimator):
+                yield name, Estimator, check
 
 
 def _rename_partial(val):
diff --git a/sklearn/tests/test_dummy.py b/sklearn/tests/test_dummy.py
index 5d955f5101..805c90a7e0 100755
--- a/sklearn/tests/test_dummy.py
+++ b/sklearn/tests/test_dummy.py
@@ -1,5 +1,7 @@
 from __future__ import division
 
+import pytest
+
 import numpy as np
 import scipy.sparse as sp
 
@@ -200,6 +202,45 @@ def test_string_labels():
     assert_array_equal(clf.predict(X), ["paris"] * 5)
 
 
+@pytest.mark.parametrize("y,y_test", [
+    ([2, 1, 1, 1], [2, 2, 1, 1]),
+    (np.array([[2, 2],
+               [1, 1],
+               [1, 1],
+               [1, 1]]),
+     np.array([[2, 2],
+               [2, 2],
+               [1, 1],
+               [1, 1]]))
+])
+def test_classifier_score_with_None(y, y_test):
+    clf = DummyClassifier(strategy="most_frequent")
+    clf.fit(None, y)
+    assert_equal(clf.score(None, y_test), 0.5)
+
+
+@pytest.mark.parametrize("strategy", [
+    "stratified",
+    "most_frequent",
+    "prior",
+    "uniform",
+    "constant"
+])
+def test_classifier_prediction_independent_of_X(strategy):
+    y = [0, 2, 1, 1]
+    X1 = [[0]] * 4
+    clf1 = DummyClassifier(strategy=strategy, random_state=0, constant=0)
+    clf1.fit(X1, y)
+    predictions1 = clf1.predict(X1)
+
+    X2 = [[1]] * 4
+    clf2 = DummyClassifier(strategy=strategy, random_state=0, constant=0)
+    clf2.fit(X2, y)
+    predictions2 = clf2.predict(X2)
+
+    assert_array_equal(predictions1, predictions2)
+
+
 def test_classifier_exceptions():
     clf = DummyClassifier(strategy="unknown")
     assert_raises(ValueError, clf.fit, [], [])
@@ -633,3 +674,39 @@ def test_dummy_regressor_return_std():
     assert_equal(len(y_pred_list), 2)
     # the second element should be all zeros
     assert_array_equal(y_pred_list[1], y_std_expected)
+
+
+@pytest.mark.parametrize("y,y_test", [
+    ([1, 1, 1, 2], [1.25] * 4),
+    (np.array([[2, 2],
+               [1, 1],
+               [1, 1],
+               [1, 1]]),
+     [[1.25, 1.25]] * 4)
+
+])
+def test_regressor_score_with_None(y, y_test):
+    reg = DummyRegressor()
+    reg.fit(None, y)
+    assert_equal(reg.score(None, y_test), 1.0)
+
+
+@pytest.mark.parametrize("strategy", [
+    "mean",
+    "median",
+    "quantile",
+    "constant"
+])
+def test_regressor_prediction_independent_of_X(strategy):
+    y = [0, 2, 1, 1]
+    X1 = [[0]] * 4
+    reg1 = DummyRegressor(strategy=strategy, constant=0, quantile=0.7)
+    reg1.fit(X1, y)
+    predictions1 = reg1.predict(X1)
+
+    X2 = [[1]] * 4
+    reg2 = DummyRegressor(strategy=strategy, constant=0, quantile=0.7)
+    reg2.fit(X2, y)
+    predictions2 = reg2.predict(X2)
+
+    assert_array_equal(predictions1, predictions2)
diff --git a/sklearn/tests/test_multiclass.py b/sklearn/tests/test_multiclass.py
index 08c3b9f01e..130c43b3eb 100755
--- a/sklearn/tests/test_multiclass.py
+++ b/sklearn/tests/test_multiclass.py
@@ -187,6 +187,8 @@ def test_ovr_fit_predict_sparse():
         assert_array_equal(dec_pred, clf_sprs.predict(X_test).toarray())
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_ovr_always_present():
     # Test that ovr works with classes that are always present or absent.
     # Note: tests is the case where _ConstantPredictor is utilised
@@ -244,6 +246,8 @@ def test_ovr_multiclass():
         assert_array_equal(y_pred, [0, 0, 1])
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_ovr_binary():
     # Toy dataset where features correspond directly to labels.
     X = np.array([[0, 0, 5], [0, 5, 0], [3, 0, 0], [0, 0, 6], [6, 0, 0]])
diff --git a/sklearn/tests/test_multioutput.py b/sklearn/tests/test_multioutput.py
index 83e3794d78..1eb5a7e48f 100755
--- a/sklearn/tests/test_multioutput.py
+++ b/sklearn/tests/test_multioutput.py
@@ -1,5 +1,6 @@
 from __future__ import division
 
+import pytest
 import numpy as np
 import scipy.sparse as sp
 
@@ -277,7 +278,8 @@ def test_multiclass_multioutput_estimator_predict_proba():
 
     Y = np.concatenate([y1, y2], axis=1)
 
-    clf = MultiOutputClassifier(LogisticRegression(random_state=seed))
+    clf = MultiOutputClassifier(LogisticRegression(
+        multi_class='ovr', solver='liblinear', random_state=seed))
 
     clf.fit(X, Y)
 
@@ -383,6 +385,8 @@ def test_classifier_chain_fit_and_predict_with_linear_svc():
     assert not hasattr(classifier_chain, 'predict_proba')
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_classifier_chain_fit_and_predict_with_sparse_data():
     # Fit classifier chain with sparse data
     X, Y = generate_multilabel_dataset_with_correlations()
@@ -399,6 +403,8 @@ def test_classifier_chain_fit_and_predict_with_sparse_data():
     assert_array_equal(Y_pred_sparse, Y_pred_dense)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_classifier_chain_vs_independent_models():
     # Verify that an ensemble of classifier chains (each of length
     # N) can achieve a higher Jaccard similarity score than N independent
@@ -421,6 +427,8 @@ def test_classifier_chain_vs_independent_models():
                    jaccard_similarity_score(Y_test, Y_pred_ovr))
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_base_chain_fit_and_predict():
     # Fit base chain and verify predict performance
     X, Y = generate_multilabel_dataset_with_correlations()
@@ -440,6 +448,8 @@ def test_base_chain_fit_and_predict():
     assert isinstance(chains[1], ClassifierMixin)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_base_chain_fit_and_predict_with_sparse_data_and_cv():
     # Fit base chain with sparse data cross_val_predict
     X, Y = generate_multilabel_dataset_with_correlations()
@@ -452,6 +462,8 @@ def test_base_chain_fit_and_predict_with_sparse_data_and_cv():
         assert_equal(Y_pred.shape, Y.shape)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_base_chain_random_order():
     # Fit base chain with random order
     X, Y = generate_multilabel_dataset_with_correlations()
@@ -472,6 +484,8 @@ def test_base_chain_random_order():
             assert_array_almost_equal(est1.coef_, est2.coef_)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_base_chain_crossval_fit_and_predict():
     # Fit chain with cross_val_predict and verify predict
     # performance
diff --git a/sklearn/tests/test_pipeline.py b/sklearn/tests/test_pipeline.py
index a36d3e17e3..6a77d5215d 100755
--- a/sklearn/tests/test_pipeline.py
+++ b/sklearn/tests/test_pipeline.py
@@ -6,6 +6,7 @@
 import shutil
 import time
 
+import pytest
 import numpy as np
 from scipy import sparse
 
@@ -234,6 +235,8 @@ def test_pipeline_init_tuple():
     pipe.score(X)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_pipeline_methods_anova():
     # Test the various methods of the pipeline (anova).
     iris = load_iris()
@@ -784,6 +787,8 @@ def test_feature_union_feature_names():
                          'get_feature_names', ft.get_feature_names)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_classes_property():
     iris = load_iris()
     X = iris.data
@@ -827,7 +832,8 @@ def test_set_feature_union_steps():
     assert_equal(['mock__x5'], ft.get_feature_names())
 
 
-def test_set_feature_union_step_none():
+@pytest.mark.parametrize('drop', ['drop', None])
+def test_set_feature_union_step_drop(drop):
     mult2 = Mult(2)
     mult2.get_feature_names = lambda: ['x2']
     mult3 = Mult(3)
@@ -839,12 +845,12 @@ def test_set_feature_union_step_none():
     assert_array_equal([[2, 3]], ft.fit_transform(X))
     assert_equal(['m2__x2', 'm3__x3'], ft.get_feature_names())
 
-    ft.set_params(m2=None)
+    ft.set_params(m2=drop)
     assert_array_equal([[3]], ft.fit(X).transform(X))
     assert_array_equal([[3]], ft.fit_transform(X))
     assert_equal(['m3__x3'], ft.get_feature_names())
 
-    ft.set_params(m3=None)
+    ft.set_params(m3=drop)
     assert_array_equal([[]], ft.fit(X).transform(X))
     assert_array_equal([[]], ft.fit_transform(X))
     assert_equal([], ft.get_feature_names())
@@ -853,6 +859,12 @@ def test_set_feature_union_step_none():
     ft.set_params(m3=mult3)
     assert_array_equal([[3]], ft.fit(X).transform(X))
 
+    # Check 'drop' step at construction time
+    ft = FeatureUnion([('m2', drop), ('m3', mult3)])
+    assert_array_equal([[3]], ft.fit(X).transform(X))
+    assert_array_equal([[3]], ft.fit_transform(X))
+    assert_equal(['m3__x3'], ft.get_feature_names())
+
 
 def test_step_name_validation():
     bad_steps1 = [('a__q', Mult(2)), ('b', Mult(3))]
@@ -887,6 +899,8 @@ def test_step_name_validation():
                                  [[1]], [1])
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_set_params_nested_pipeline():
     estimator = Pipeline([
         ('a', Pipeline([
diff --git a/sklearn/tests/test_site_joblib.py b/sklearn/tests/test_site_joblib.py
index 7ceb80a281..bffd43cc14 100755
--- a/sklearn/tests/test_site_joblib.py
+++ b/sklearn/tests/test_site_joblib.py
@@ -1,4 +1,6 @@
 import os
+import pytest
+from sklearn import externals
 from sklearn.externals import joblib as joblib_vendored
 from sklearn.utils import Parallel, delayed, Memory, parallel_backend
 
@@ -9,6 +11,11 @@
 
 
 def test_old_pickle(tmpdir):
+    vendored_joblib_home = os.path.dirname(joblib_vendored.__file__)
+    sklearn_externals_home = os.path.dirname(externals.__file__)
+    if not vendored_joblib_home.startswith(sklearn_externals_home):
+        pytest.skip("joblib is physically unvendored (e.g. as in debian)")
+
     # Check that a pickle that references sklearn.external.joblib can load
     f = tmpdir.join('foo.pkl')
     f.write(b'\x80\x02csklearn.externals.joblib.numpy_pickle\nNumpyArrayWrappe'
diff --git a/sklearn/tree/tree.py b/sklearn/tree/tree.py
index 7105a86ce0..9985cee2ee 100755
--- a/sklearn/tree/tree.py
+++ b/sklearn/tree/tree.py
@@ -548,7 +548,11 @@ class DecisionTreeClassifier(BaseDecisionTree, ClassifierMixin):
            Added float values for fractions.
 
     min_samples_leaf : int, float, optional (default=1)
-        The minimum number of samples required to be at a leaf node:
+        The minimum number of samples required to be at a leaf node.
+        A split point at any depth will only be considered if it leaves at
+        least ``min_samples_leaf`` training samples in each of the left and
+        right branches.  This may have the effect of smoothing the model,
+        especially in regression.
 
         - If int, then consider `min_samples_leaf` as the minimum number.
         - If float, then `min_samples_leaf` is a fraction and
@@ -916,7 +920,11 @@ class DecisionTreeRegressor(BaseDecisionTree, RegressorMixin):
            Added float values for fractions.
 
     min_samples_leaf : int, float, optional (default=1)
-        The minimum number of samples required to be at a leaf node:
+        The minimum number of samples required to be at a leaf node.
+        A split point at any depth will only be considered if it leaves at
+        least ``min_samples_leaf`` training samples in each of the left and
+        right branches.  This may have the effect of smoothing the model,
+        especially in regression.
 
         - If int, then consider `min_samples_leaf` as the minimum number.
         - If float, then `min_samples_leaf` is a fraction and
@@ -1174,7 +1182,11 @@ class ExtraTreeClassifier(DecisionTreeClassifier):
            Added float values for fractions.
 
     min_samples_leaf : int, float, optional (default=1)
-        The minimum number of samples required to be at a leaf node:
+        The minimum number of samples required to be at a leaf node.
+        A split point at any depth will only be considered if it leaves at
+        least ``min_samples_leaf`` training samples in each of the left and
+        right branches.  This may have the effect of smoothing the model,
+        especially in regression.
 
         - If int, then consider `min_samples_leaf` as the minimum number.
         - If float, then `min_samples_leaf` is a fraction and
@@ -1358,7 +1370,11 @@ class ExtraTreeRegressor(DecisionTreeRegressor):
            Added float values for fractions.
 
     min_samples_leaf : int, float, optional (default=1)
-        The minimum number of samples required to be at a leaf node:
+        The minimum number of samples required to be at a leaf node.
+        A split point at any depth will only be considered if it leaves at
+        least ``min_samples_leaf`` training samples in each of the left and
+        right branches.  This may have the effect of smoothing the model,
+        especially in regression.
 
         - If int, then consider `min_samples_leaf` as the minimum number.
         - If float, then `min_samples_leaf` is a fraction and
diff --git a/sklearn/utils/__init__.py b/sklearn/utils/__init__.py
index 8af32c145d..4c22752030 100755
--- a/sklearn/utils/__init__.py
+++ b/sklearn/utils/__init__.py
@@ -3,6 +3,7 @@
 """
 import numbers
 import platform
+import struct
 
 import numpy as np
 from scipy.sparse import issparse
@@ -34,6 +35,7 @@
            "register_parallel_backend", "hash", "effective_n_jobs"]
 
 IS_PYPY = platform.python_implementation() == 'PyPy'
+_IS_32BIT = 8 * struct.calcsize("P") == 32
 
 
 class Bunch(dict):
diff --git a/sklearn/utils/_logistic_sigmoid.pyx b/sklearn/utils/_logistic_sigmoid.pyx
index 9c7b8d0a20..58809eb7c1 100755
--- a/sklearn/utils/_logistic_sigmoid.pyx
+++ b/sklearn/utils/_logistic_sigmoid.pyx
@@ -13,9 +13,9 @@ ctypedef np.float64_t DTYPE_t
 cdef DTYPE_t _inner_log_logistic_sigmoid(DTYPE_t x):
     """Log of the logistic sigmoid function log(1 / (1 + e ** -x))"""
     if x > 0:
-        return -np.log1p(exp(-x))
+        return -log(1 + exp(-x))
     else:
-        return x - np.log1p(exp(x))
+        return x - log(1 + exp(x))
 
 
 def _log_logistic_sigmoid(int n_samples, int n_features, 
diff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py
index df67ca59ef..3b48f58243 100755
--- a/sklearn/utils/estimator_checks.py
+++ b/sklearn/utils/estimator_checks.py
@@ -6,7 +6,6 @@
 import traceback
 import pickle
 from copy import deepcopy
-import struct
 from functools import partial
 
 import numpy as np
@@ -14,7 +13,7 @@
 from scipy.stats import rankdata
 
 from sklearn.externals.six.moves import zip
-from sklearn.utils import IS_PYPY
+from sklearn.utils import IS_PYPY, _IS_32BIT
 from sklearn.utils._joblib import hash, Memory
 from sklearn.utils.testing import assert_raises, _get_args
 from sklearn.utils.testing import assert_raises_regex
@@ -404,11 +403,6 @@ def __array__(self, dtype=None):
         return self.data
 
 
-def _is_32bit():
-    """Detect if process is 32bit Python."""
-    return struct.calcsize('P') * 8 == 32
-
-
 def _is_pairwise(estimator):
     """Returns True if estimator has a _pairwise attribute set to True.
 
@@ -945,7 +939,7 @@ def check_transformers_unfitted(name, transformer):
 
 
 def _check_transformer(name, transformer_orig, X, y):
-    if name in ('CCA', 'LocallyLinearEmbedding', 'KernelPCA') and _is_32bit():
+    if name in ('CCA', 'LocallyLinearEmbedding', 'KernelPCA') and _IS_32BIT:
         # Those transformers yield non-deterministic output when executed on
         # a 32bit Python. The same transformers are stable on 64bit Python.
         # FIXME: try to isolate a minimalistic reproduction case only depending
@@ -1023,7 +1017,7 @@ def _check_transformer(name, transformer_orig, X, y):
 
 @ignore_warnings
 def check_pipeline_consistency(name, estimator_orig):
-    if name in ('CCA', 'LocallyLinearEmbedding', 'KernelPCA') and _is_32bit():
+    if name in ('CCA', 'LocallyLinearEmbedding', 'KernelPCA') and _IS_32BIT:
         # Those transformers yield non-deterministic output when executed on
         # a 32bit Python. The same transformers are stable on 64bit Python.
         # FIXME: try to isolate a minimalistic reproduction case only depending
@@ -1547,7 +1541,7 @@ def check_outliers_train(name, estimator_orig, readonly_memmap=True):
     y_scores = estimator.score_samples(X)
     assert y_scores.shape == (n_samples,)
     y_dec = y_scores - estimator.offset_
-    assert_array_equal(y_dec, decision)
+    assert_allclose(y_dec, decision)
 
     # raises error on malformed input for score_samples
     assert_raises(ValueError, estimator.score_samples, X.T)
diff --git a/sklearn/utils/sparsefuncs_fast.pyx b/sklearn/utils/sparsefuncs_fast.pyx
index d49c0683ae..b40b843e94 100755
--- a/sklearn/utils/sparsefuncs_fast.pyx
+++ b/sklearn/utils/sparsefuncs_fast.pyx
@@ -27,7 +27,7 @@ ctypedef np.float64_t DOUBLE
 
 def csr_row_norms(X):
     """L2 norm of each row in CSR matrix X."""
-    if X.dtype != np.float32:
+    if X.dtype not in [np.float32, np.float64]:
         X = X.astype(np.float64)
     return _csr_row_norms(X.data, X.shape, X.indices, X.indptr)
 
@@ -72,7 +72,7 @@ def csr_mean_variance_axis0(X):
         Feature-wise variances
 
     """
-    if X.dtype != np.float32:
+    if X.dtype not in [np.float32, np.float64]:
         X = X.astype(np.float64)
     means, variances, _ =  _csr_mean_variance_axis0(X.data, X.shape[0],
                                                     X.shape[1], X.indices)
@@ -152,7 +152,7 @@ def csc_mean_variance_axis0(X):
         Feature-wise variances
 
     """
-    if X.dtype != np.float32:
+    if X.dtype not in [np.float32, np.float64]:
         X = X.astype(np.float64)
     means, variances, _ = _csc_mean_variance_axis0(X.data, X.shape[0],
                                                    X.shape[1], X.indices,
@@ -260,7 +260,7 @@ def incr_mean_variance_axis0(X, last_mean, last_var, last_n):
     `utils.extmath._batch_mean_variance_update`.
 
     """
-    if X.dtype != np.float32:
+    if X.dtype not in [np.float32, np.float64]:
         X = X.astype(np.float64)
     return _incr_mean_variance_axis0(X.data, X.shape[0], X.shape[1], X.indices,
                                      X.indptr, X.format, last_mean, last_var,
diff --git a/sklearn/utils/stats.py b/sklearn/utils/stats.py
index 43f37bb95a..82b8912b78 100755
--- a/sklearn/utils/stats.py
+++ b/sklearn/utils/stats.py
@@ -22,4 +22,6 @@ def _weighted_percentile(array, sample_weight, percentile=50):
     weight_cdf = stable_cumsum(sample_weight[sorted_idx])
     percentile_idx = np.searchsorted(
         weight_cdf, (percentile / 100.) * weight_cdf[-1])
+    # in rare cases, percentile_idx equals to len(sorted_idx)
+    percentile_idx = np.clip(percentile_idx, 0, len(sorted_idx)-1)
     return array[sorted_idx[percentile_idx]]
diff --git a/sklearn/utils/testing.py b/sklearn/utils/testing.py
index 8dc1188cfe..75b3789619 100755
--- a/sklearn/utils/testing.py
+++ b/sklearn/utils/testing.py
@@ -15,7 +15,6 @@
 import pkgutil
 import warnings
 import sys
-import struct
 import functools
 
 import scipy as sp
@@ -47,7 +46,7 @@
 from sklearn.base import BaseEstimator
 from sklearn.externals import joblib
 from sklearn.utils.fixes import signature
-from sklearn.utils import deprecated, IS_PYPY
+from sklearn.utils import deprecated, IS_PYPY, _IS_32BIT
 
 
 additional_names_in_all = []
@@ -468,9 +467,13 @@ def assert_allclose_dense_sparse(x, y, rtol=1e-07, atol=1e-9, err_msg=''):
                          " not a sparse matrix and an array.")
 
 
+@deprecated('deprecated in version 0.20 to be removed in version 0.22')
 def fake_mldata(columns_dict, dataname, matfile, ordering=None):
     """Create a fake mldata data set.
 
+    .. deprecated:: 0.20
+        Will be removed in version 0.22
+
     Parameters
     ----------
     columns_dict : dict, keys=str, values=ndarray
@@ -508,6 +511,7 @@ def fake_mldata(columns_dict, dataname, matfile, ordering=None):
     scipy.io.savemat(matfile, datasets, oned_as='column')
 
 
+@deprecated('deprecated in version 0.20 to be removed in version 0.22')
 class mock_mldata_urlopen(object):
     """Object that mocks the urlopen function to fake requests to mldata.
 
@@ -515,6 +519,9 @@ class mock_mldata_urlopen(object):
     creates a fake dataset in a StringIO object and returns it. Otherwise, it
     raises an HTTPError.
 
+    .. deprecated:: 0.20
+        Will be removed in version 0.22
+
     Parameters
     ----------
     mock_datasets : dict
@@ -750,7 +757,7 @@ def run_test(*args, **kwargs):
 try:
     import pytest
 
-    skip_if_32bit = pytest.mark.skipif(8 * struct.calcsize("P") == 32,
+    skip_if_32bit = pytest.mark.skipif(_IS_32BIT,
                                        reason='skipped on 32bit platforms')
     skip_travis = pytest.mark.skipif(os.environ.get('TRAVIS') == 'true',
                                      reason='skip on travis')
diff --git a/sklearn/utils/tests/test_class_weight.py b/sklearn/utils/tests/test_class_weight.py
index 1dfedad9bc..c2d03595fb 100755
--- a/sklearn/utils/tests/test_class_weight.py
+++ b/sklearn/utils/tests/test_class_weight.py
@@ -1,4 +1,5 @@
 import numpy as np
+import pytest
 
 from sklearn.linear_model import LogisticRegression
 from sklearn.datasets import make_blobs
@@ -65,6 +66,8 @@ def test_compute_class_weight_dict():
                          classes, y)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_compute_class_weight_invariance():
     # Test that results with class_weight="balanced" is invariant wrt
     # class imbalance if the number of samples is identical.
diff --git a/sklearn/utils/tests/test_validation.py b/sklearn/utils/tests/test_validation.py
index 07c1e9b412..23db332394 100755
--- a/sklearn/utils/tests/test_validation.py
+++ b/sklearn/utils/tests/test_validation.py
@@ -173,7 +173,7 @@ def test_check_array_force_all_finite_valid(value, force_all_finite, retype):
      (np.inf, 'allow-nan', 'Input contains infinity'),
      (np.nan, True, 'Input contains NaN, infinity'),
      (np.nan, 'allow-inf', 'force_all_finite should be a bool or "allow-nan"'),
-     (np.nan, 1, 'force_all_finite should be a bool or "allow-nan"')]
+     (np.nan, 1, 'Input contains NaN, infinity')]
 )
 @pytest.mark.parametrize(
     "retype",
@@ -183,7 +183,7 @@ def test_check_array_force_all_finiteinvalid(value, force_all_finite,
                                              match_msg, retype):
     X = retype(np.arange(4).reshape(2, 2).astype(np.float))
     X[0, 0] = value
-    with pytest.raises(ValueError, message=match_msg):
+    with pytest.raises(ValueError, match=match_msg):
         check_array(X, force_all_finite=force_all_finite,
                     accept_sparse=True)
 
