diff --git a/src/edu/washington/escience/myria/accessmethod/AccessMethod.java b/src/edu/washington/escience/myria/accessmethod/AccessMethod.java
index 732ee61e96..127c124838 100755
--- a/src/edu/washington/escience/myria/accessmethod/AccessMethod.java
+++ b/src/edu/washington/escience/myria/accessmethod/AccessMethod.java
@@ -197,6 +197,16 @@ public abstract void createIndexIfNotExists(
    */
   public abstract void createView(String viewName, String viewDefinition) throws DbException;
 
+  /**
+   * Creates a materialized view
+   *
+   * @param viewName
+   * @param viewDefinition
+   * @throws DbException
+   */
+  public abstract void createMaterializedView(String viewName, String viewDefinition)
+      throws DbException;
+
   /**
    * Executes a command on the underlying database
    *
diff --git a/src/edu/washington/escience/myria/accessmethod/JdbcAccessMethod.java b/src/edu/washington/escience/myria/accessmethod/JdbcAccessMethod.java
index 0ed68e6396..e7ec34a8ec 100755
--- a/src/edu/washington/escience/myria/accessmethod/JdbcAccessMethod.java
+++ b/src/edu/washington/escience/myria/accessmethod/JdbcAccessMethod.java
@@ -627,6 +627,17 @@ public void createView(final String viewName, final String viewDefinition) throw
     }
   }
 
+  @Override
+  public void createMaterializedView(final String viewName, final String viewDefinition)
+      throws DbException {
+    if (jdbcInfo.getDbms().equals(MyriaConstants.STORAGE_SYSTEM_POSTGRESQL)) {
+      createMaterializedViewPostgres(viewName, viewDefinition);
+    } else {
+      throw new UnsupportedOperationException(
+          "create materialized view is not supported in " + jdbcInfo.getDbms() + ", implement me");
+    }
+  }
+
   /**
    * Create a view in postgres if no view with the same name exists
    *
@@ -641,6 +652,21 @@ public void createViewPostgres(final String viewName, final String viewDefinitio
     execute(statement);
   }
 
+  /**
+   * Create a materialized view in postgres if no view with the same name exists
+   *
+   * @param viewName the name of the views
+   * @param viewDefinition the view to be created
+   * @throws DbException if there is an error in the DBMS.
+   */
+  public void createMaterializedViewPostgres(final String viewName, final String viewDefinition)
+      throws DbException {
+    String statement =
+        Joiner.on(' ')
+            .join("CREATE OR REPLACE MATERIALIZED VIEW", viewName, "AS", viewDefinition, ";");
+    execute(statement);
+  }
+
   @Override
   public void runCommand(final String command) throws DbException {
     if (jdbcInfo.getDbms().equals(MyriaConstants.STORAGE_SYSTEM_POSTGRESQL)) {
diff --git a/src/edu/washington/escience/myria/accessmethod/SQLiteAccessMethod.java b/src/edu/washington/escience/myria/accessmethod/SQLiteAccessMethod.java
index 12bc3056cf..4ebde0e56a 100755
--- a/src/edu/washington/escience/myria/accessmethod/SQLiteAccessMethod.java
+++ b/src/edu/washington/escience/myria/accessmethod/SQLiteAccessMethod.java
@@ -498,6 +498,13 @@ public void createView(final String viewName, final String viewDefinition) throw
         "create view is not supported in sqlite yet, implement me");
   }
 
+  @Override
+  public void createMaterializedView(final String viewName, final String viewDefinition)
+      throws DbException {
+    throw new UnsupportedOperationException(
+        "create materialized view is not supported in sqlite yet, implement me");
+  }
+
   @Override
   public void runCommand(final String command) throws DbException {
     throw new UnsupportedOperationException(
diff --git a/src/edu/washington/escience/myria/api/PerfEnforceResource.java b/src/edu/washington/escience/myria/api/PerfEnforceResource.java
index 9701c42595..c7e9be5845 100755
--- a/src/edu/washington/escience/myria/api/PerfEnforceResource.java
+++ b/src/edu/washington/escience/myria/api/PerfEnforceResource.java
@@ -13,6 +13,7 @@
 
 import edu.washington.escience.myria.DbException;
 import edu.washington.escience.myria.perfenforce.PerfEnforceDriver;
+import edu.washington.escience.myria.perfenforce.PerfEnforceException;
 import edu.washington.escience.myria.perfenforce.QueryMetaData;
 
 /**
@@ -30,7 +31,7 @@
   @POST
   @Path("/preparePSLA")
   @Consumes(MediaType.MULTIPART_FORM_DATA)
-  public Response prepareData() {
+  public Response prepareData() throws PerfEnforceException {
     perfenforceDriver.preparePSLA();
     return Response.noContent().build();
   }
@@ -46,7 +47,8 @@ public Response setTier(@FormDataParam("tier") final int queryRuntime) {
   @POST
   @Path("/findSLA")
   @Consumes(MediaType.MULTIPART_FORM_DATA)
-  public Response findSLA(@FormDataParam("querySQL") final String querySQL) throws DbException {
+  public Response findSLA(@FormDataParam("querySQL") final String querySQL)
+      throws PerfEnforceException {
     perfenforceDriver.findSLA(querySQL);
     return Response.noContent().build();
   }
@@ -55,7 +57,7 @@ public Response findSLA(@FormDataParam("querySQL") final String querySQL) throws
   @Path("/recordRealRuntime")
   @Consumes(MediaType.MULTIPART_FORM_DATA)
   public Response recordRealRuntime(@FormDataParam("dataPointRuntime") final Double queryRuntime)
-      throws DbException {
+      throws PerfEnforceException {
     perfenforceDriver.recordRealRuntime(queryRuntime);
     return Response.noContent().build();
   }
diff --git a/src/edu/washington/escience/myria/operator/DbCreateView.java b/src/edu/washington/escience/myria/operator/DbCreateView.java
index 1873e30559..0503c77a9a 100755
--- a/src/edu/washington/escience/myria/operator/DbCreateView.java
+++ b/src/edu/washington/escience/myria/operator/DbCreateView.java
@@ -26,16 +26,19 @@
 
   private final String viewName;
   private final String viewQuery;
+  private final boolean isMaterialized;
 
   public DbCreateView(
       final Operator child,
       final String viewName,
       final String viewQuery,
+      final boolean isMaterialized,
       final ConnectionInfo connectionInfo) {
     super(child);
     this.connectionInfo = connectionInfo;
     this.viewName = viewName;
     this.viewQuery = viewQuery;
+    this.isMaterialized = isMaterialized;
   }
 
   @Override
@@ -49,7 +52,11 @@ protected void init(final ImmutableMap<String, Object> execEnvVars) throws DbExc
     accessMethod = AccessMethod.of(connectionInfo.getDbms(), connectionInfo, false);
 
     /* Add the indexes to the relation */
-    accessMethod.createView(viewName, viewQuery);
+    if (isMaterialized) {
+      accessMethod.createMaterializedView(viewName, viewQuery);
+    } else {
+      accessMethod.createView(viewName, viewQuery);
+    }
   }
 
   @Override
diff --git a/src/edu/washington/escience/myria/parallel/Server.java b/src/edu/washington/escience/myria/parallel/Server.java
index 74e29d04b5..2534a75a4d 100755
--- a/src/edu/washington/escience/myria/parallel/Server.java
+++ b/src/edu/washington/escience/myria/parallel/Server.java
@@ -1182,7 +1182,7 @@ public long createView(
             workerId,
             new SubQueryPlan(
                 new DbCreateView(
-                    EmptyRelation.of(Schema.EMPTY_SCHEMA), viewName, viewDefinition, null)));
+                    EmptyRelation.of(Schema.EMPTY_SCHEMA), viewName, viewDefinition, false, null)));
       }
       ListenableFuture<Query> qf =
           queryManager.submitQuery(
@@ -1203,6 +1203,47 @@ public long createView(
     return queryID;
   }
 
+  /**
+   * Create a materialized view
+   */
+  public long createMaterializedView(
+      final String viewName, final String viewDefinition, final Set<Integer> workers)
+      throws DbException, InterruptedException {
+    long queryID;
+    Set<Integer> actualWorkers = workers;
+    if (workers == null) {
+      actualWorkers = getWorkers().keySet();
+    }
+
+    /* Create the view */
+    try {
+      Map<Integer, SubQueryPlan> workerPlans = new HashMap<>();
+      for (Integer workerId : actualWorkers) {
+        workerPlans.put(
+            workerId,
+            new SubQueryPlan(
+                new DbCreateView(
+                    EmptyRelation.of(Schema.EMPTY_SCHEMA), viewName, viewDefinition, true, null)));
+      }
+      ListenableFuture<Query> qf =
+          queryManager.submitQuery(
+              "create materialized view",
+              "create materialized view",
+              "create materialized view",
+              new SubQueryPlan(new SinkRoot(new EOSSource())),
+              workerPlans);
+      try {
+        queryID = qf.get().getQueryId();
+      } catch (ExecutionException e) {
+        throw new DbException("Error executing query", e.getCause());
+      }
+    } catch (CatalogException e) {
+      throw new DbException(e);
+    }
+
+    return queryID;
+  }
+
   /**
    * Create a function and register it in the catalog
    *
@@ -1304,19 +1345,20 @@ public void executeSQLCommand(
   /**
    * Execute a SQL Statement and retrieve a tuple result
    *
+   * @throws CatalogException
+   * @throws ExecutionException
+   * @throws InterruptedException
+   *
    * @throws IOException
    * @throws DbException
    */
   public String executeSQLCommandSingleRowSingleWorker(
       final String sqlString, final Schema outputSchema, final int workerId)
-      throws IOException, DbException {
-
-    LOGGER.warn("SCHEMA : " + outputSchema.toString());
+      throws InterruptedException, ExecutionException, DbException, CatalogException, IOException {
 
     ByteSink byteSink = new ByteSink();
     TupleWriter writer = new CsvTupleWriter();
 
-    // worker plan
     DbQueryScan scan = new DbQueryScan(sqlString, outputSchema);
     final ExchangePairID operatorId = ExchangePairID.newID();
     CollectProducer producer = new CollectProducer(scan, operatorId, MyriaConstants.MASTER_ID);
@@ -1324,7 +1366,6 @@ public String executeSQLCommandSingleRowSingleWorker(
     Map<Integer, SubQueryPlan> workerPlans = new HashMap<>(workerId);
     workerPlans.put(workerId, workerPlan);
 
-    // master plan
     final CollectConsumer consumer =
         new CollectConsumer(
             outputSchema, operatorId, new HashSet<Integer>(Arrays.asList(workerId)));
@@ -1332,18 +1373,12 @@ public String executeSQLCommandSingleRowSingleWorker(
     final SubQueryPlan masterPlan = new SubQueryPlan(output);
 
     String planString = "execute single worker : " + sqlString;
-    try {
-      queryManager.submitQuery(planString, planString, planString, masterPlan, workerPlans).get();
-    } catch (CatalogException | DbException | InterruptedException | ExecutionException e) {
-      throw new DbException(e);
-    }
+    queryManager.submitQuery(planString, planString, planString, masterPlan, workerPlans).get();
 
     // get query response
     byte[] responseBytes = ((ByteArrayOutputStream) byteSink.getOutputStream()).toByteArray();
-    // LOGGER.warn("BYTES FROM SERVER: " + responseBytes.length);
     String response = new String(responseBytes, Charset.forName("UTF-8"));
     String[] pieces = response.split("\r\n");
-    // LOGGER.warn("RESPONSE FROM SERVER: " + response);
     return pieces[1];
   }
 
diff --git a/src/edu/washington/escience/myria/perfenforce/PerfEnforceDataPreparation.java b/src/edu/washington/escience/myria/perfenforce/PerfEnforceDataPreparation.java
index 67dcb38e53..91eff4f918 100755
--- a/src/edu/washington/escience/myria/perfenforce/PerfEnforceDataPreparation.java
+++ b/src/edu/washington/escience/myria/perfenforce/PerfEnforceDataPreparation.java
@@ -5,7 +5,6 @@
 
 import java.io.BufferedReader;
 import java.io.FileReader;
-import java.io.InputStreamReader;
 import java.io.PrintWriter;
 import java.nio.file.Path;
 import java.nio.file.Paths;
@@ -28,7 +27,6 @@
 import edu.washington.escience.myria.MyriaConstants;
 import edu.washington.escience.myria.RelationKey;
 import edu.washington.escience.myria.Schema;
-import edu.washington.escience.myria.Type;
 import edu.washington.escience.myria.operator.DbInsert;
 import edu.washington.escience.myria.operator.DbQueryScan;
 import edu.washington.escience.myria.operator.EOSSource;
@@ -61,7 +59,8 @@
   /*
    * Ingesting the fact table in a parallel sequence
    */
-  public HashMap<Integer, RelationKey> ingestFact(final TableDescriptionEncoding factTableDesc) {
+  public HashMap<Integer, RelationKey> ingestFact(final TableDescriptionEncoding factTableDesc)
+      throws PerfEnforceException {
     factTableRelationMapper = new HashMap<Integer, RelationKey>();
 
     ArrayList<RelationKey> relationKeysToUnion = new ArrayList<RelationKey>();
@@ -76,97 +75,101 @@
      * the original relationKey name and add it to the catalog. This is what the user will be using on the MyriaL front
      * end.
      */
-    RelationKey relationKeyWithUnion =
-        new RelationKey(
-            factTableDesc.relationKey.getUserName(),
-            factTableDesc.relationKey.getProgramName(),
-            factTableDesc.relationKey.getRelationName() + maxConfig + "_U");
-    server.parallelIngestDataset(
-        relationKeyWithUnion,
-        factTableDesc.schema,
-        factTableDesc.delimiter,
-        null,
-        null,
-        null,
-        factTableDesc.source,
-        maxWorkerRange);
-    relationKeysToUnion.add(relationKeyWithUnion);
-
-    RelationKey relationKeyOriginal =
-        new RelationKey(
-            factTableDesc.relationKey.getUserName(),
-            factTableDesc.relationKey.getProgramName(),
-            factTableDesc.relationKey.getRelationName() + maxConfig);
-    server.createMaterializedView(
-        relationKeyOriginal.toString(MyriaConstants.STORAGE_SYSTEM_POSTGRESQL),
-        PerfEnforceUtils.createUnionQuery(relationKeysToUnion),
-        maxWorkerRange);
-    server.addDatasetToCatalog(relationKeyOriginal, factTableDesc.schema, maxWorkerRange);
-    factTableRelationMapper.put(maxConfig, relationKeyOriginal);
-
-    /*
-     * Iterate and run this for the rest of the workers
-     */
-    Set<Integer> previousWorkerRange = maxWorkerRange;
-    RelationKey previousRelationKey = relationKeyWithUnion;
-    for (int c = 1; c < PerfEnforceDriver.configurations.size(); c++) {
-      // Get the next sequence of workers
-      int currentSize = PerfEnforceDriver.configurations.get(c);
-      Set<Integer> currentWorkerRange = PerfEnforceUtils.getWorkerRangeSet(currentSize);
-      Set<Integer> diff =
-          com.google.common.collect.Sets.difference(previousWorkerRange, currentWorkerRange);
-
-      RelationKey currentRelationKeyToUnion =
+    try {
+      RelationKey relationKeyWithUnion =
           new RelationKey(
               factTableDesc.relationKey.getUserName(),
               factTableDesc.relationKey.getProgramName(),
-              factTableDesc.relationKey.getRelationName() + currentSize + "_U");
-
-      final ExchangePairID shuffleId = ExchangePairID.newID();
-      DbQueryScan scan = new DbQueryScan(previousRelationKey, factTableDesc.schema);
-
-      int[] producingWorkers =
-          PerfEnforceUtils.getRangeInclusiveArray(Collections.min(diff), Collections.max(diff));
-      int[] receivingWorkers =
-          PerfEnforceUtils.getRangeInclusiveArray(1, Collections.max(currentWorkerRange));
-
-      GenericShuffleProducer producer =
-          new GenericShuffleProducer(
-              scan,
-              shuffleId,
-              receivingWorkers,
-              new RoundRobinPartitionFunction(receivingWorkers.length));
-      GenericShuffleConsumer consumer =
-          new GenericShuffleConsumer(factTableDesc.schema, shuffleId, producingWorkers);
-      DbInsert insert = new DbInsert(consumer, currentRelationKeyToUnion, true);
-
-      Map<Integer, RootOperator[]> workerPlans = new HashMap<>(currentSize);
-      for (Integer workerID : producingWorkers) {
-        workerPlans.put(workerID, new RootOperator[] {producer});
-      }
-      for (Integer workerID : receivingWorkers) {
-        workerPlans.put(workerID, new RootOperator[] {insert});
-      }
-
-      server.submitQueryPlan(new SinkRoot(new EOSSource()), workerPlans).get();
-      relationKeysToUnion.add(currentRelationKeyToUnion);
-
-      RelationKey currentConfigRelationKey =
+              factTableDesc.relationKey.getRelationName() + maxConfig + "_U");
+      server.parallelIngestDataset(
+          relationKeyWithUnion,
+          factTableDesc.schema,
+          factTableDesc.delimiter,
+          null,
+          null,
+          null,
+          factTableDesc.source,
+          maxWorkerRange);
+      relationKeysToUnion.add(relationKeyWithUnion);
+
+      RelationKey relationKeyOriginal =
           new RelationKey(
               factTableDesc.relationKey.getUserName(),
               factTableDesc.relationKey.getProgramName(),
-              factTableDesc.relationKey.getRelationName() + currentSize);
+              factTableDesc.relationKey.getRelationName() + maxConfig);
       server.createMaterializedView(
-          currentConfigRelationKey.toString(MyriaConstants.STORAGE_SYSTEM_POSTGRESQL),
+          relationKeyOriginal.toString(MyriaConstants.STORAGE_SYSTEM_POSTGRESQL),
           PerfEnforceUtils.createUnionQuery(relationKeysToUnion),
-          currentWorkerRange);
-      server.addDatasetToCatalog(
-          currentConfigRelationKey, factTableDesc.schema, currentWorkerRange);
-      factTableRelationMapper.put(currentSize, currentConfigRelationKey);
-      previousWorkerRange = currentWorkerRange;
-      previousRelationKey = currentConfigRelationKey;
+          maxWorkerRange);
+      server.addDatasetToCatalog(relationKeyOriginal, factTableDesc.schema, maxWorkerRange);
+      factTableRelationMapper.put(maxConfig, relationKeyOriginal);
+
+      /*
+       * Iterate and run this for the rest of the workers
+       */
+      Set<Integer> previousWorkerRange = maxWorkerRange;
+      RelationKey previousRelationKey = relationKeyWithUnion;
+      for (int c = 1; c < PerfEnforceDriver.configurations.size(); c++) {
+        // Get the next sequence of workers
+        int currentSize = PerfEnforceDriver.configurations.get(c);
+        Set<Integer> currentWorkerRange = PerfEnforceUtils.getWorkerRangeSet(currentSize);
+        Set<Integer> diff =
+            com.google.common.collect.Sets.difference(previousWorkerRange, currentWorkerRange);
+
+        RelationKey currentRelationKeyToUnion =
+            new RelationKey(
+                factTableDesc.relationKey.getUserName(),
+                factTableDesc.relationKey.getProgramName(),
+                factTableDesc.relationKey.getRelationName() + currentSize + "_U");
+
+        final ExchangePairID shuffleId = ExchangePairID.newID();
+        DbQueryScan scan = new DbQueryScan(previousRelationKey, factTableDesc.schema);
+
+        int[] producingWorkers =
+            PerfEnforceUtils.getRangeInclusiveArray(Collections.min(diff), Collections.max(diff));
+        int[] receivingWorkers =
+            PerfEnforceUtils.getRangeInclusiveArray(1, Collections.max(currentWorkerRange));
+
+        GenericShuffleProducer producer =
+            new GenericShuffleProducer(
+                scan,
+                shuffleId,
+                receivingWorkers,
+                new RoundRobinPartitionFunction(receivingWorkers.length));
+        GenericShuffleConsumer consumer =
+            new GenericShuffleConsumer(factTableDesc.schema, shuffleId, producingWorkers);
+        DbInsert insert = new DbInsert(consumer, currentRelationKeyToUnion, true);
+
+        Map<Integer, RootOperator[]> workerPlans = new HashMap<>(currentSize);
+        for (Integer workerID : producingWorkers) {
+          workerPlans.put(workerID, new RootOperator[] {producer});
+        }
+        for (Integer workerID : receivingWorkers) {
+          workerPlans.put(workerID, new RootOperator[] {insert});
+        }
+
+        server.submitQueryPlan(new SinkRoot(new EOSSource()), workerPlans).get();
+        relationKeysToUnion.add(currentRelationKeyToUnion);
+
+        RelationKey currentConfigRelationKey =
+            new RelationKey(
+                factTableDesc.relationKey.getUserName(),
+                factTableDesc.relationKey.getProgramName(),
+                factTableDesc.relationKey.getRelationName() + currentSize);
+        server.createMaterializedView(
+            currentConfigRelationKey.toString(MyriaConstants.STORAGE_SYSTEM_POSTGRESQL),
+            PerfEnforceUtils.createUnionQuery(relationKeysToUnion),
+            currentWorkerRange);
+        server.addDatasetToCatalog(
+            currentConfigRelationKey, factTableDesc.schema, currentWorkerRange);
+        factTableRelationMapper.put(currentSize, currentConfigRelationKey);
+        previousWorkerRange = currentWorkerRange;
+        previousRelationKey = currentConfigRelationKey;
+      }
+      return factTableRelationMapper;
+    } catch (Exception e) {
+      throw new PerfEnforceException("Error while ingesting fact table");
     }
-    return factTableRelationMapper;
   }
 
   /*
@@ -178,46 +181,50 @@ public void ingestDimension(final TableDescriptionEncoding dimTableDesc)
     Set<Integer> totalWorkers =
         PerfEnforceUtils.getWorkerRangeSet(Collections.max(PerfEnforceDriver.configurations));
 
-    server.parallelIngestDataset(
-        dimTableDesc.relationKey,
-        dimTableDesc.schema,
-        dimTableDesc.delimiter,
-        null,
-        null,
-        null,
-        dimTableDesc.source,
-        totalWorkers);
-
-    DbQueryScan dbscan = new DbQueryScan(dimTableDesc.relationKey, dimTableDesc.schema);
-    /*
-     * Is there a better way to broadcast relations?
-     */
-    final ExchangePairID broadcastID = ExchangePairID.newID();
-    int[][] cellPartition = new int[1][];
-    int[] allCells = new int[totalWorkers.size()];
-    for (int i = 0; i < totalWorkers.size(); i++) {
-      allCells[i] = i;
-    }
-    cellPartition[0] = allCells;
-
-    GenericShuffleProducer producer =
-        new GenericShuffleProducer(
-            dbscan,
-            broadcastID,
-            cellPartition,
-            MyriaUtils.integerSetToIntArray(totalWorkers),
-            new FixValuePartitionFunction(0));
-
-    GenericShuffleConsumer consumer =
-        new GenericShuffleConsumer(
-            dimTableDesc.schema, broadcastID, MyriaUtils.integerSetToIntArray(totalWorkers));
-    DbInsert insert = new DbInsert(consumer, dimTableDesc.relationKey, true);
-    Map<Integer, RootOperator[]> workerPlans = new HashMap<>(totalWorkers.size());
-    for (Integer workerID : totalWorkers) {
-      workerPlans.put(workerID, new RootOperator[] {producer, insert});
-    }
+    try {
+      server.parallelIngestDataset(
+          dimTableDesc.relationKey,
+          dimTableDesc.schema,
+          dimTableDesc.delimiter,
+          null,
+          null,
+          null,
+          dimTableDesc.source,
+          totalWorkers);
+
+      DbQueryScan dbscan = new DbQueryScan(dimTableDesc.relationKey, dimTableDesc.schema);
+      /*
+       * Is there a better way to broadcast relations?
+       */
+      final ExchangePairID broadcastID = ExchangePairID.newID();
+      int[][] cellPartition = new int[1][];
+      int[] allCells = new int[totalWorkers.size()];
+      for (int i = 0; i < totalWorkers.size(); i++) {
+        allCells[i] = i;
+      }
+      cellPartition[0] = allCells;
 
-    server.submitQueryPlan(new SinkRoot(new EOSSource()), workerPlans).get();
+      GenericShuffleProducer producer =
+          new GenericShuffleProducer(
+              dbscan,
+              broadcastID,
+              cellPartition,
+              MyriaUtils.integerSetToIntArray(totalWorkers),
+              new FixValuePartitionFunction(0));
+
+      GenericShuffleConsumer consumer =
+          new GenericShuffleConsumer(
+              dimTableDesc.schema, broadcastID, MyriaUtils.integerSetToIntArray(totalWorkers));
+      DbInsert insert = new DbInsert(consumer, dimTableDesc.relationKey, true);
+      Map<Integer, RootOperator[]> workerPlans = new HashMap<>(totalWorkers.size());
+      for (Integer workerID : totalWorkers) {
+        workerPlans.put(workerID, new RootOperator[] {producer, insert});
+      }
+
+      server.submitQueryPlan(new SinkRoot(new EOSSource()), workerPlans).get();
+    } catch (Exception e) {
+      throw new PerfEnforceException("Error ingesting dimension tables");
+    }
   }
 
   /*
@@ -266,34 +273,39 @@ public void postgresStatsAnalyzeTable(final TableDescriptionEncoding t) {
         new HashSet<Integer>(Arrays.asList(1)));
   }
 
-  public void collectSelectivities(final TableDescriptionEncoding t) {
+  public void collectSelectivities(final TableDescriptionEncoding t) throws PerfEnforceException {
     List<StatsTableEncoding> statsList = new ArrayList<StatsTableEncoding>();
-    if (t.type.equals("fact")) {
-      for (Integer currentConfig : PerfEnforceDriver.configurations) {
-        RelationKey factRelationKey = factTableRelationMapper.get(currentConfig);
-        long factTableTupleCount = server.getDatasetStatus(factRelationKey).getNumTuples();
+    try {
+      if (t.type.equals("fact")) {
+        for (Integer currentConfig : PerfEnforceDriver.configurations) {
+          RelationKey factRelationKey = factTableRelationMapper.get(currentConfig);
+          long factTableTupleCount = server.getDatasetStatus(factRelationKey).getNumTuples();
+          statsList.add(
+              runTableRanking(
+                  factRelationKey, factTableTupleCount, currentConfig, t.type, t.keys, t.schema));
+        }
+      } else {
+        RelationKey dimensionTableKey = t.relationKey;
+        long dimensionTableTupleCount = server.getDatasetStatus(dimensionTableKey).getNumTuples();
         statsList.add(
             runTableRanking(
-                factRelationKey, factTableTupleCount, currentConfig, t.type, t.keys, t.schema));
+                dimensionTableKey,
+                dimensionTableTupleCount,
+                Collections.max(PerfEnforceDriver.configurations),
+                t.type,
+                t.keys,
+                t.schema));
       }
-    } else {
-      RelationKey dimensionTableKey = t.relationKey;
-      long dimensionTableTupleCount = server.getDatasetStatus(dimensionTableKey).getNumTuples();
-      statsList.add(
-          runTableRanking(
-              dimensionTableKey,
-              dimensionTableTupleCount,
-              Collections.max(PerfEnforceDriver.configurations),
-              t.type,
-              t.keys,
-              t.schema));
+
+      PrintWriter statsObjectWriter =
+          new PrintWriter(
+              PerfEnforceDriver.configurationPath.resolve("stats.json").toString(), "UTF-8");
+      ObjectMapper mapper = new ObjectMapper();
+      mapper.writeValue(statsObjectWriter, statsList);
+      statsObjectWriter.close();
+    } catch (Exception e) {
+      throw new PerfEnforceException("Error collecting table statistics");
     }
-    PrintWriter statsObjectWriter =
-        new PrintWriter(
-            PerfEnforceDriver.configurationPath.resolve("stats.json").toString(), "UTF-8");
-    ObjectMapper mapper = new ObjectMapper();
-    mapper.writeValue(statsObjectWriter, statsList);
-    statsObjectWriter.close();
   }
 
   /*
@@ -305,7 +317,8 @@ public StatsTableEncoding runTableRanking(
       final int config,
       final String type,
       final Set<Integer> keys,
-      final Schema schema) {
+      final Schema schema)
+      throws PerfEnforceException {
 
     List<String> selectivityKeys = new ArrayList<String>();
     List<Double> selectivityList = Arrays.asList(new Double[] {.001, .01, .1});
@@ -314,100 +327,73 @@ public StatsTableEncoding runTableRanking(
     Schema attributeKeySchema = PerfEnforceUtils.getAttributeKeySchema(keys, schema);
 
     String tableName = relationKey.toString(MyriaConstants.STORAGE_SYSTEM_POSTGRESQL);
+    try {
+      for (int i = 0; i < selectivityList.size(); i++) {
+        String rankingQuery =
+            String.format(
+                "select %s from (select %s, CAST(rank() over (order by %s asc) AS float)/%s as rank from %s) as r where r.rank >= %s LIMIT 1;",
+                attributeKeyString,
+                attributeKeyString,
+                attributeKeyString,
+                tableSize / config,
+                tableName,
+                selectivityList.get(i));
+        String sqlResult =
+            server.executeSQLCommandSingleRowSingleWorker(rankingQuery, attributeKeySchema, 1);
+        selectivityKeys.add(sqlResult);
+      }
 
-    for (int i = 0; i < selectivityList.size(); i++) {
-      String rankingQuery =
-          String.format(
-              "select %s from (select %s, CAST(rank() over (order by %s asc) AS float)/%s as rank from %s) as r where r.rank >= %s LIMIT 1;",
-              attributeKeyString,
-              attributeKeyString,
-              attributeKeyString,
-              tableSize / config,
-              tableName,
-              selectivityList.get(i));
-      String sqlResult =
-          server.executeSQLCommandSingleRowSingleWorker(rankingQuery, attributeKeySchema, 1);
-      selectivityKeys.add(sqlResult);
-    }
+      // HACK: We can't properly "count" tuples for tables that are broadcast
+      long modifiedSize = tableSize;
+      if (type.equalsIgnoreCase("dimension")) {
+        modifiedSize = tableSize / Collections.max(PerfEnforceDriver.configurations);
+      }
 
-    // HACK: We can't properly "count" tuples for tables that are broadcast
-    long modifiedSize = tableSize;
-    if (type.equalsIgnoreCase("dimension")) {
-      modifiedSize = tableSize / Collections.max(PerfEnforceDriver.configurations);
+      return new StatsTableEncoding(tableName, modifiedSize, selectivityKeys);
+    } catch (Exception e) {
+      throw new PerfEnforceException("error running table ranks");
     }
-
-    return new StatsTableEncoding(tableName, modifiedSize, selectivityKeys);
   }
 
-  public void collectFeatures() {
+  public void collectFeaturesFromQueries() throws PerfEnforceException {
 
     for (Integer config : PerfEnforceDriver.configurations) {
       Path workerPath =
           Paths.get(PerfEnforceDriver.configurationPath.toString(), config + "_Workers");
       String currentLine = "";
 
-      PrintWriter featureWriter =
-          new PrintWriter(workerPath.resolve("TESTING.arff").toString(), "UTF-8");
-
-      featureWriter.write("@relation testing \n");
-      featureWriter.write("@attribute numberTables numeric \n");
-      featureWriter.write("@attribute postgesEstCostMin numeric \n");
-      featureWriter.write("@attribute postgesEstCostMax numeric \n");
-      featureWriter.write("@attribute postgesEstNumRows numeric \n");
-      featureWriter.write("@attribute postgesEstWidth numeric \n");
-      featureWriter.write("@attribute numberOfWorkers numeric \n");
-      featureWriter.write("@attribute realTime numeric \n");
-      featureWriter.write("\n");
-      featureWriter.write("@data \n");
-
-      BufferedReader br =
-          new BufferedReader(
-              new FileReader(workerPath.resolve("SQLQueries-Generated.txt").toString()));
-      while ((currentLine = br.readLine()) != null) {
-        currentLine =
-            currentLine.replace(
-                factTableDescription.relationKey.getRelationName(),
-                factTableRelationMapper.get(config).getRelationName());
-        String features = getPostgresFeatures(currentLine, 1);
-        features = features.substring(features.indexOf("cost"));
-        features = features.replace("\"", " ");
-        String[] cmd = {
-          "sh",
-          "-c",
-          "echo \""
-              + features
-              + "\" | sed -e 's/.*cost=//' -e 's/\\.\\./,/' -e 's/ rows=/,/' -e 's/ width=/,/' -e 's/)//'"
-        };
-        ProcessBuilder pb = new ProcessBuilder(cmd);
-        Process p = pb.start();
-
-        BufferedReader input = new BufferedReader(new InputStreamReader(p.getInputStream()));
-        features = input.readLine();
-
-        // add the extra features -- hacky
-        if (currentLine.contains("WHERE")) {
-          String[] tables =
-              currentLine
-                  .substring(currentLine.indexOf("FROM"), currentLine.indexOf("WHERE"))
-                  .split(",");
-          features = tables.length + "," + features;
-        } else {
-          features = "1," + features;
+      try {
+        PrintWriter featureWriter =
+            new PrintWriter(workerPath.resolve("TESTING.arff").toString(), "UTF-8");
+
+        featureWriter.write("@relation testing \n");
+        featureWriter.write("@attribute numberTables numeric \n");
+        featureWriter.write("@attribute postgesEstCostMin numeric \n");
+        featureWriter.write("@attribute postgesEstCostMax numeric \n");
+        featureWriter.write("@attribute postgesEstNumRows numeric \n");
+        featureWriter.write("@attribute postgesEstWidth numeric \n");
+        featureWriter.write("@attribute numberOfWorkers numeric \n");
+        featureWriter.write("@attribute realTime numeric \n");
+        featureWriter.write("\n");
+        featureWriter.write("@data \n");
+
+        BufferedReader br =
+            new BufferedReader(
+                new FileReader(workerPath.resolve("SQLQueries-Generated.txt").toString()));
+        while ((currentLine = br.readLine()) != null) {
+          currentLine =
+              currentLine.replace(
+                  factTableDescription.relationKey.getRelationName(),
+                  factTableRelationMapper.get(config).getRelationName());
+          String explainQuery = "EXPLAIN " + currentLine;
+          String features = PerfEnforceUtils.getMaxFeature(explainQuery, config);
+          featureWriter.write(features + "\n");
         }
-        features += "," + config + ",0";
-        featureWriter.write(features + "\n");
+        featureWriter.close();
+        br.close();
+      } catch (Exception e) {
+        throw new PerfEnforceException("Error creating table features");
       }
-      featureWriter.close();
-      br.close();
     }
   }
-
-  public String getPostgresFeatures(final String query, final int worker) {
-    String explainQuery = "EXPLAIN " + query;
-    String result =
-        server.executeSQLCommandSingleRowSingleWorker(
-            explainQuery, Schema.ofFields("explain", Type.STRING_TYPE), worker);
-
-    return result;
-  }
 }
diff --git a/src/edu/washington/escience/myria/perfenforce/PerfEnforceDriver.java b/src/edu/washington/escience/myria/perfenforce/PerfEnforceDriver.java
index de93237d8e..a4a7d2349f 100755
--- a/src/edu/washington/escience/myria/perfenforce/PerfEnforceDriver.java
+++ b/src/edu/washington/escience/myria/perfenforce/PerfEnforceDriver.java
@@ -6,7 +6,6 @@
 import java.io.BufferedReader;
 import java.io.File;
 import java.io.FileReader;
-import java.io.IOException;
 import java.nio.file.Path;
 import java.util.ArrayList;
 import java.util.Arrays;
@@ -45,27 +44,31 @@
 
   @Inject
   public PerfEnforceDriver(final Path configurationPath) {
-    this.configurationPath = configurationPath;
-    queryCounter = 0;
+    PerfEnforceDriver.configurationPath = configurationPath;
     isDonePSLA = false;
   }
 
   /*
    * Fetch necessary files from S3
    */
-  public void fetchS3Files() throws IOException {
+  public void fetchS3Files() throws PerfEnforceException {
     AmazonS3 s3Client = new AmazonS3Client(new AnonymousAWSCredentials());
     String currentFile = "";
-    BufferedReader bufferedReader =
-        new BufferedReader(new FileReader(configurationPath + "filesToFetch.txt"));
-    while ((currentFile = bufferedReader.readLine()) != null) {
-      Path filePath = configurationPath.resolve(currentFile);
-      s3Client.getObject(
-          new GetObjectRequest("perfenforce", currentFile), new File(filePath.toString()));
+    BufferedReader bufferedReader;
+    try {
+      bufferedReader = new BufferedReader(new FileReader(configurationPath + "filesToFetch.txt"));
+      while ((currentFile = bufferedReader.readLine()) != null) {
+        Path filePath = configurationPath.resolve(currentFile);
+        s3Client.getObject(
+            new GetObjectRequest("perfenforce", currentFile), new File(filePath.toString()));
+      }
+      bufferedReader.close();
+    } catch (Exception e) {
+      throw new PerfEnforceException("Error while fetching files from S3");
     }
   }
 
-  public void preparePSLA() {
+  public void preparePSLA() throws PerfEnforceException {
     fetchS3Files();
 
     PerfEnforceDataPreparation perfenforceDataPrepare = new PerfEnforceDataPreparation();
@@ -85,7 +88,7 @@ public void preparePSLA() {
     PSLAManagerWrapper pslaManager = new PSLAManagerWrapper();
     pslaManager.generateQueries();
 
-    perfenforceDataPrepare.collectFeatures();
+    perfenforceDataPrepare.collectFeaturesFromQueries();
 
     pslaManager.generatePSLA();
     isDonePSLA = true;
@@ -102,11 +105,12 @@ public void setTier(final int tier) {
     perfenforceOnlineLearning = new PerfEnforceOnlineLearning(tier);
   }
 
-  public void findSLA(final String querySQL) {
+  public void findSLA(final String querySQL) throws PerfEnforceException {
     perfenforceOnlineLearning.findSLA(querySQL);
+    perfenforceOnlineLearning.findBestClusterSize();
   }
 
-  public void recordRealRuntime(final Double queryRuntime) {
+  public void recordRealRuntime(final Double queryRuntime) throws PerfEnforceException {
     perfenforceOnlineLearning.recordRealRuntime(queryRuntime);
   }
 
diff --git a/src/edu/washington/escience/myria/perfenforce/PerfEnforceOnlineLearning.java b/src/edu/washington/escience/myria/perfenforce/PerfEnforceOnlineLearning.java
index 140dce936d..5deaff2e2c 100755
--- a/src/edu/washington/escience/myria/perfenforce/PerfEnforceOnlineLearning.java
+++ b/src/edu/washington/escience/myria/perfenforce/PerfEnforceOnlineLearning.java
@@ -8,7 +8,6 @@
 import java.io.FileOutputStream;
 import java.io.FileReader;
 import java.io.FileWriter;
-import java.io.IOException;
 import java.io.InputStreamReader;
 import java.io.PrintWriter;
 import java.nio.channels.FileChannel;
@@ -52,74 +51,79 @@ public PerfEnforceOnlineLearning(final int tier) {
             .toString();
   }
 
-  public void findSLA(final String querySQL) {
+  public void findSLA(final String querySQL) throws PerfEnforceException {
 
     String highestFeatures = PerfEnforceUtils.getMaxFeature(querySQL, clusterSize);
 
-    PrintWriter featureWriter =
-        new PrintWriter(Paths.get(onlineLearningPath, "TESTING.arff").toString(), "UTF-8");
-
-    featureWriter.write("@relation testing \n");
-
-    featureWriter.write("@attribute numberTables numeric \n");
-    featureWriter.write("@attribute postgesEstCostMin numeric \n");
-    featureWriter.write("@attribute postgesEstCostMax numeric \n");
-    featureWriter.write("@attribute postgesEstNumRows numeric \n");
-    featureWriter.write("@attribute postgesEstWidth numeric \n");
-    featureWriter.write("@attribute numberOfWorkers numeric \n");
-    featureWriter.write("@attribute realTime numeric \n");
-
-    featureWriter.write("\n");
-    featureWriter.write("@data \n");
-    featureWriter.write(highestFeatures + "\n");
-    featureWriter.close();
-
-    // predict the runtime
-    String[] cmd = {
-      "java",
-      "-cp",
-      onlineLearningPath + "weka.jar",
-      "weka.classifiers.rules.M5Rules",
-      "-M",
-      "4.0",
-      "-t",
-      onlineLearningPath + "WekaTraining/" + clusterSize + "_Workers/TRAINING.arff",
-      "-T",
-      onlineLearningPath + "TESTING.arff",
-      "-p",
-      "0",
-      "-classifications",
-      " weka.classifiers.evaluation.output.prediction.CSV -file \""
-          + onlineLearningPath
-          + "results.txt"
-          + "\""
-    };
-    ProcessBuilder pb = new ProcessBuilder(cmd);
-
-    Process p = pb.start();
-    p.waitFor();
-
-    String querySLA = "";
-    BufferedReader predictionReader =
-        new BufferedReader(new FileReader(Paths.get(onlineLearningPath, "results.txt").toString()));
-    predictionReader.readLine();
-    querySLA = predictionReader.readLine().split(",")[2];
-    predictionReader.close();
-
-    for (int c : PerfEnforceDriver.configurations) {
-      String maxFeatureForConfiguration = PerfEnforceUtils.getMaxFeature(querySQL, c);
-      FileWriter featureWriterForConfiguration;
-      featureWriterForConfiguration =
-          new FileWriter(
-              Paths.get(onlineLearningPath, "OMLFiles", "features", String.valueOf(c)).toString());
-      featureWriterForConfiguration.write(maxFeatureForConfiguration + '\n');
-      featureWriterForConfiguration.close();
+    try {
+      PrintWriter featureWriter =
+          new PrintWriter(Paths.get(onlineLearningPath, "TESTING.arff").toString(), "UTF-8");
+
+      featureWriter.write("@relation testing \n");
+
+      featureWriter.write("@attribute numberTables numeric \n");
+      featureWriter.write("@attribute postgesEstCostMin numeric \n");
+      featureWriter.write("@attribute postgesEstCostMax numeric \n");
+      featureWriter.write("@attribute postgesEstNumRows numeric \n");
+      featureWriter.write("@attribute postgesEstWidth numeric \n");
+      featureWriter.write("@attribute numberOfWorkers numeric \n");
+      featureWriter.write("@attribute realTime numeric \n");
+
+      featureWriter.write("\n");
+      featureWriter.write("@data \n");
+      featureWriter.write(highestFeatures + "\n");
+      featureWriter.close();
+
+      // predict the runtime
+      String[] cmd = {
+        "java",
+        "-cp",
+        onlineLearningPath + "weka.jar",
+        "weka.classifiers.rules.M5Rules",
+        "-M",
+        "4.0",
+        "-t",
+        onlineLearningPath + "WekaTraining/" + clusterSize + "_Workers/TRAINING.arff",
+        "-T",
+        onlineLearningPath + "TESTING.arff",
+        "-p",
+        "0",
+        "-classifications",
+        " weka.classifiers.evaluation.output.prediction.CSV -file \""
+            + onlineLearningPath
+            + "results.txt"
+            + "\""
+      };
+      ProcessBuilder pb = new ProcessBuilder(cmd);
+
+      Process p = pb.start();
+      p.waitFor();
+
+      String querySLA = "";
+      BufferedReader predictionReader =
+          new BufferedReader(
+              new FileReader(Paths.get(onlineLearningPath, "results.txt").toString()));
+      predictionReader.readLine();
+      querySLA = predictionReader.readLine().split(",")[2];
+      predictionReader.close();
+
+      for (int c : PerfEnforceDriver.configurations) {
+        String maxFeatureForConfiguration = PerfEnforceUtils.getMaxFeature(querySQL, c);
+        FileWriter featureWriterForConfiguration;
+        featureWriterForConfiguration =
+            new FileWriter(
+                Paths.get(onlineLearningPath, "OMLFiles", "features", String.valueOf(c))
+                    .toString());
+        featureWriterForConfiguration.write(maxFeatureForConfiguration + '\n');
+        featureWriterForConfiguration.close();
+      }
+      currentQuery = new QueryMetaData(queryCounter, Double.parseDouble(querySLA));
+    } catch (Exception e) {
+      throw new PerfEnforceException("Error finding SLA");
     }
-    currentQuery = new QueryMetaData(queryCounter, Double.parseDouble(querySLA));
-    findBestClusterSize(currentQuery);
   }
 
-  public void findBestClusterSize(final QueryMetaData query) {
+  public void findBestClusterSize() {
     List<Thread> threadList = new ArrayList<Thread>();
     for (int i = 0; i < PerfEnforceDriver.configurations.size(); i++) {
       final int clusterIndex = i;
@@ -128,7 +132,11 @@ public void findBestClusterSize(final QueryMetaData query) {
               new Runnable() {
                 @Override
                 public void run() {
-                  trainOnlineQueries(clusterIndex, query.id);
+                  try {
+                    trainOnlineQueries(clusterIndex, currentQuery.id);
+                  } catch (PerfEnforceException e) {
+                    e.printStackTrace();
+                  }
                 }
               });
       threadList.add(thread);
@@ -156,10 +164,10 @@ public void run() {
       onlinePrediction = (onlinePrediction < 0) ? 0 : onlinePrediction;
 
       double currentRatio = 0;
-      if (query.slaRuntime == 0) {
+      if (currentQuery.slaRuntime == 0) {
         currentRatio = onlinePrediction / 1;
       } else {
-        currentRatio = onlinePrediction / query.slaRuntime;
+        currentRatio = onlinePrediction / currentQuery.slaRuntime;
       }
       double currentScore = closeToOneScore(currentRatio);
 
@@ -173,7 +181,8 @@ public void run() {
     clusterSize = PerfEnforceDriver.configurations.get(winnerIndex);
   }
 
-  public void trainOnlineQueries(final int clusterIndex, final int queryID) throws IOException {
+  public void trainOnlineQueries(final int clusterIndex, final int queryID)
+      throws PerfEnforceException {
     String MOAFileName = Paths.get(onlineLearningPath, "OMLFiles", "moa.jar").toString();
     String trainingFileName = Paths.get(onlineLearningPath, "OMLFiles", "training.arff").toString();
     String modifiedTrainingFileName =
@@ -182,87 +191,100 @@ public void trainOnlineQueries(final int clusterIndex, final int queryID) throws
     String predictionsFileName =
         Paths.get(onlineLearningPath, "OMLFiles", "predictions" + clusterIndex + ".txt").toString();
 
-    PrintWriter outputWriter = new PrintWriter(modifiedTrainingFileName);
-    outputWriter.close();
-
-    // copy training file to new file
-    FileChannel src = new FileInputStream(trainingFileName).getChannel();
-    FileChannel dest = new FileOutputStream(modifiedTrainingFileName).getChannel();
-    dest.transferFrom(src, 0, src.size());
-    src.close();
-    dest.close();
-
-    // Append all previous data points
-    FileWriter appendDataWriter = new FileWriter(modifiedTrainingFileName, true);
-    for (String s : previousDataPoints) {
-      appendDataWriter.write(s + "\n");
-    }
+    try {
+      PrintWriter outputWriter = new PrintWriter(modifiedTrainingFileName);
+      outputWriter.close();
+
+      // copy training file to new file
+      FileChannel src = new FileInputStream(trainingFileName).getChannel();
+      FileChannel dest = new FileOutputStream(modifiedTrainingFileName).getChannel();
+      dest.transferFrom(src, 0, src.size());
+      src.close();
+      dest.close();
+
+      // Append all previous data points
+      FileWriter appendDataWriter = new FileWriter(modifiedTrainingFileName, true);
+      for (String s : previousDataPoints) {
+        appendDataWriter.write(s + "\n");
+      }
 
-    // Append the current point
-    String newPoint = getQueryFeature(clusterIndex, queryID, 0);
-    appendDataWriter.write(newPoint + "\n");
-    appendDataWriter.close();
+      // Append the current point
+      String newPoint = getQueryFeature(clusterIndex, queryID, 0);
+      appendDataWriter.write(newPoint + "\n");
+      appendDataWriter.close();
 
-    String moaCommand =
-        String.format(
-            "EvaluatePrequentialRegression -l (rules.functions.Perceptron  -d -l %s) -s (ArffFileStream -f %s) -e (WindowRegressionPerformanceEvaluator -w 1) -f 1 -o %s",
-            .04,
-            modifiedTrainingFileName,
-            predictionsFileName);
-    String[] arrayCommand =
-        new String[] {"java", "-classpath", MOAFileName, "moa.DoTask", moaCommand};
+      String moaCommand =
+          String.format(
+              "EvaluatePrequentialRegression -l (rules.functions.Perceptron  -d -l %s) -s (ArffFileStream -f %s) -e (WindowRegressionPerformanceEvaluator -w 1) -f 1 -o %s",
+              .04,
+              modifiedTrainingFileName,
+              predictionsFileName);
+      String[] arrayCommand =
+          new String[] {"java", "-classpath", MOAFileName, "moa.DoTask", moaCommand};
 
-    Process p = Runtime.getRuntime().exec(arrayCommand);
+      Process p = Runtime.getRuntime().exec(arrayCommand);
 
-    BufferedReader reader = new BufferedReader(new InputStreamReader(p.getInputStream()));
-    while ((reader.readLine()) != null) {}
+      BufferedReader reader = new BufferedReader(new InputStreamReader(p.getInputStream()));
+      while ((reader.readLine()) != null) {}
 
-    parsingOnlineFile(clusterIndex, predictionsFileName);
+      parsingOnlineFile(clusterIndex, predictionsFileName);
+    } catch (Exception e) {
+      throw new PerfEnforceException("Error during training");
+    }
   }
 
   public void parsingOnlineFile(final int clusterIndex, final String predictionFileName)
-      throws IOException {
-    BufferedReader streamReader = new BufferedReader(new FileReader(predictionFileName));
-    String currentLine = "";
-    double nextQueryPrediction = 0;
-    while ((currentLine = streamReader.readLine()) != null) {
-      nextQueryPrediction = Double.parseDouble((currentLine.split(",")[0]).split(":")[1]);
+      throws PerfEnforceException {
+    try {
+      BufferedReader streamReader = new BufferedReader(new FileReader(predictionFileName));
+      String currentLine = "";
+      double nextQueryPrediction = 0;
+      while ((currentLine = streamReader.readLine()) != null) {
+        nextQueryPrediction = Double.parseDouble((currentLine.split(",")[0]).split(":")[1]);
+      }
+      streamReader.close();
+      queryPredictions[clusterIndex] = nextQueryPrediction;
+    } catch (Exception e) {
+      throw new PerfEnforceException("Error parsing online predictions file");
     }
-    streamReader.close();
-    queryPredictions[clusterIndex] = nextQueryPrediction;
   }
 
   public String getQueryFeature(
-      final int clusterIndex, final int queryID, final double queryRuntime) throws IOException {
+      final int clusterIndex, final int queryID, final double queryRuntime)
+      throws PerfEnforceException {
     String featureFilePath =
         Paths.get(onlineLearningPath, "OMLFiles", "features", String.valueOf(clusterSize))
             .toString();
 
-    BufferedReader featureReader = new BufferedReader(new FileReader(featureFilePath));
-    for (int i = 0; i < queryID; i++) {
-      featureReader.readLine();
-    }
-    String result = featureReader.readLine();
-    if (queryRuntime != 0) {
-      String[] parts = result.split(",");
-      result =
-          parts[0]
-              + ","
-              + parts[1]
-              + ","
-              + parts[2]
-              + ","
-              + parts[3]
-              + ","
-              + parts[4]
-              + ","
-              + parts[5]
-              + ","
-              + queryRuntime;
-    }
+    try {
+      BufferedReader featureReader = new BufferedReader(new FileReader(featureFilePath));
+      for (int i = 0; i < queryID; i++) {
+        featureReader.readLine();
+      }
+      String result = featureReader.readLine();
+      if (queryRuntime != 0) {
+        String[] parts = result.split(",");
+        result =
+            parts[0]
+                + ","
+                + parts[1]
+                + ","
+                + parts[2]
+                + ","
+                + parts[3]
+                + ","
+                + parts[4]
+                + ","
+                + parts[5]
+                + ","
+                + queryRuntime;
+      }
 
-    featureReader.close();
-    return result;
+      featureReader.close();
+      return result;
+    } catch (Exception e) {
+      throw new PerfEnforceException("Error collecting query feature");
+    }
   }
 
   public double closeToOneScore(final double ratio) {
@@ -273,7 +295,7 @@ public double closeToOneScore(final double ratio) {
     }
   }
 
-  public void recordRealRuntime(final double queryRuntime) {
+  public void recordRealRuntime(final double queryRuntime) throws PerfEnforceException {
     previousDataPoints.add(
         getQueryFeature(
             PerfEnforceDriver.configurations.indexOf(clusterSize), currentQuery.id, queryRuntime));
diff --git a/src/edu/washington/escience/myria/perfenforce/PerfEnforceUtils.java b/src/edu/washington/escience/myria/perfenforce/PerfEnforceUtils.java
index 0113fc3c53..3c73f65388 100755
--- a/src/edu/washington/escience/myria/perfenforce/PerfEnforceUtils.java
+++ b/src/edu/washington/escience/myria/perfenforce/PerfEnforceUtils.java
@@ -12,6 +12,8 @@
 import java.util.List;
 import java.util.Set;
 
+import javax.ws.rs.core.Context;
+
 import com.google.common.base.Charsets;
 import com.google.common.io.Files;
 import com.google.gson.Gson;
@@ -19,6 +21,7 @@
 import edu.washington.escience.myria.RelationKey;
 import edu.washington.escience.myria.Schema;
 import edu.washington.escience.myria.Type;
+import edu.washington.escience.myria.parallel.Server;
 import edu.washington.escience.myria.perfenforce.encoding.TableDescriptionEncoding;
 
 /**
@@ -26,6 +29,8 @@
  */
 public class PerfEnforceUtils {
 
+  @Context private static Server server;
+
   public static List<TableDescriptionEncoding> getTablesOfType(
       final String type, final String configFilePath) throws PerfEnforceException {
     List<TableDescriptionEncoding> listTablesOfType = new ArrayList<TableDescriptionEncoding>();
@@ -105,56 +110,62 @@ public static Schema getAttributeKeySchema(final Set<Integer> keys, final Schema
     return keySchema;
   }
 
-  public static String getMaxFeature(final String sqlQuery, final int configuration) {
-    String explainQuery = "EXPLAIN " + sqlQuery;
-    explainQuery = explainQuery.replace("lineitem", "lineitem" + configuration);
-
-    List<String> featuresList = new ArrayList<String>();
-    String maxFeature = "";
-    double maxCost = Integer.MIN_VALUE;
-
-    for (int w = 1; w <= configuration; w++) {
-      String features = "";
-      features =
-          server.executeSQLCommandSingleRowSingleWorker(
-              explainQuery, Schema.ofFields("explain", Type.STRING_TYPE), w);
-
-      features = features.substring(features.indexOf("cost"));
-      features = features.replace("\"", " ");
-      String[] cmd = {
-        "sh",
-        "-c",
-        "echo \""
-            + features
-            + "\" | sed -e 's/.*cost=//' -e 's/\\.\\./,/' -e 's/ rows=/,/' -e 's/ width=/,/' -e 's/)//'"
-      };
-      ProcessBuilder pb = new ProcessBuilder(cmd);
-      Process p;
-
-      p = pb.start();
-      BufferedReader input = new BufferedReader(new InputStreamReader(p.getInputStream()));
-      features = input.readLine();
-
-      if (sqlQuery.contains("WHERE")) {
-        String[] tables =
-            sqlQuery.substring(sqlQuery.indexOf("FROM"), sqlQuery.indexOf("WHERE")).split(",");
-        features = tables.length + "," + features;
-      } else {
-        features = "1," + features;
-      }
+  public static String getMaxFeature(final String sqlQuery, final int configuration)
+      throws PerfEnforceException {
 
-      features += "," + configuration + ",0";
-      featuresList.add(features);
-    }
+    try {
+      String explainQuery = "EXPLAIN " + sqlQuery;
+      explainQuery = explainQuery.replace("lineitem", "lineitem" + configuration);
+
+      List<String> featuresList = new ArrayList<String>();
+      String maxFeature = "";
+      double maxCost = Integer.MIN_VALUE;
+
+      for (int w = 1; w <= configuration; w++) {
+        String features = "";
+        features =
+            server.executeSQLCommandSingleRowSingleWorker(
+                explainQuery, Schema.ofFields("explain", Type.STRING_TYPE), w);
+
+        features = features.substring(features.indexOf("cost"));
+        features = features.replace("\"", " ");
+        String[] cmd = {
+          "sh",
+          "-c",
+          "echo \""
+              + features
+              + "\" | sed -e 's/.*cost=//' -e 's/\\.\\./,/' -e 's/ rows=/,/' -e 's/ width=/,/' -e 's/)//'"
+        };
+        ProcessBuilder pb = new ProcessBuilder(cmd);
+        Process p;
+
+        p = pb.start();
+        BufferedReader input = new BufferedReader(new InputStreamReader(p.getInputStream()));
+        features = input.readLine();
+
+        if (sqlQuery.contains("WHERE")) {
+          String[] tables =
+              sqlQuery.substring(sqlQuery.indexOf("FROM"), sqlQuery.indexOf("WHERE")).split(",");
+          features = tables.length + "," + features;
+        } else {
+          features = "1," + features;
+        }
 
-    for (String f : featuresList) {
-      double cost = Double.parseDouble(f.split(",")[2]);
-      if (cost > maxCost) {
-        maxCost = cost;
-        maxFeature = f;
+        features += "," + configuration + ",0";
+        featuresList.add(features);
       }
-    }
 
-    return maxFeature;
+      for (String f : featuresList) {
+        double cost = Double.parseDouble(f.split(",")[2]);
+        if (cost > maxCost) {
+          maxCost = cost;
+          maxFeature = f;
+        }
+      }
+
+      return maxFeature;
+    } catch (Exception e) {
+      throw new PerfEnforceException("Error collecting the max feature");
+    }
   }
 }
