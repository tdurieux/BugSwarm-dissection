diff --git a/CONTRIBUTING.md b/CONTRIBUTING.md
index cc59ecbd6d..6f643fc46c 100755
--- a/CONTRIBUTING.md
+++ b/CONTRIBUTING.md
@@ -70,9 +70,19 @@ following rules before you submit a pull request:
    [Utilities for Developers](http://scikit-learn.org/dev/developers/utilities.html#developers-utils)
    page.
 
--  If your pull request addresses an issue, please use the pull request title
-   to describe the issue and mention the issue number in the pull request description. This will make sure a link back to the original issue is
-   created.
+-  Give your pull request a helpful title that summarises what your
+   contribution does. In some cases `Fix <ISSUE TITLE>` is enough.
+   `Fix #<ISSUE NUMBER>` is not enough.
+
+-  Often pull requests resolve one or more other issues (or pull requests).
+   If merging your pull request means that some other issues/PRs should
+   be closed, you should
+   [use keywords to create link to them](https://github.com/blog/1506-closing-issues-via-pull-requests/)
+   (e.g., `Fixes #1234`; multiple issues/PRs are allowed as long as each one
+   is preceded by a keyword). Upon merging, those issues/PRs will
+   automatically be closed by GitHub. If your pull request is simply related
+   to some other issues/PRs, create a link to them without using the keywords
+   (e.g., `See also #1234`).
 
 -  All public methods should have informative docstrings with sample
    usage presented as doctests when appropriate.
@@ -199,13 +209,15 @@ following rules before submitting:
 New contributor tips
 --------------------
 
-A great way to start contributing to scikit-learn is to pick an item
-from the list of [Easy issues](https://github.com/scikit-learn/scikit-learn/issues?labels=Easy)
-in the issue tracker. Resolving these issues allow you to start
-contributing to the project without much prior knowledge. Your
-assistance in this area will be greatly appreciated by the more
-experienced developers as it helps free up their time to concentrate on
-other issues.
+A great way to start contributing to scikit-learn is to pick an item from the
+list of
+[good first issues](https://github.com/scikit-learn/scikit-learn/labels/good%20first%20issue). If
+you have already contributed to scikit-learn look at
+[Easy issues](https://github.com/scikit-learn/scikit-learn/labels/Easy)
+instead. Resolving these issues allow you to start contributing to the project
+without much prior knowledge. Your assistance in this area will be greatly
+appreciated by the more experienced developers as it helps free up their time to
+concentrate on other issues.
 
 Documentation
 -------------
diff --git a/PULL_REQUEST_TEMPLATE.md b/PULL_REQUEST_TEMPLATE.md
index 3321b70332..9db6ade08b 100755
--- a/PULL_REQUEST_TEMPLATE.md
+++ b/PULL_REQUEST_TEMPLATE.md
@@ -1,9 +1,15 @@
 <!--
 Thanks for contributing a pull request! Please ensure you have taken a look at
-the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
+the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
+-->
+
+#### Reference Issues/PRs
+<!--
+Example: Fixes #1234. See also #3456.
+Please use keywords (e.g., Fixes) to create link to the issues or pull requests
+you resolved, so that they will automatically be closed when your pull request
+is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
 -->
-#### Reference Issue
-<!-- Example: Fixes #1234 -->
 
 
 #### What does this implement/fix? Explain your changes.
diff --git a/doc/datasets/index.rst b/doc/datasets/index.rst
index 580d9e3950..bc664609db 100755
--- a/doc/datasets/index.rst
+++ b/doc/datasets/index.rst
@@ -322,6 +322,7 @@ writing data in that format.
     labeled_faces
     covtype
     rcv1
+    kddcup99
 
 
 .. include:: olivetti_faces.rst
@@ -338,6 +339,8 @@ writing data in that format.
 
 .. include:: rcv1.rst
 
+.. include:: kddcup99.rst
+
 .. _boston_house_prices:
 
 .. include:: ../../sklearn/datasets/descr/boston_house_prices.rst
diff --git a/doc/datasets/openml.rst b/doc/datasets/openml.rst
index 8a080fffc1..d82f12fa6e 100755
--- a/doc/datasets/openml.rst
+++ b/doc/datasets/openml.rst
@@ -33,21 +33,21 @@ of size 28x28 pixels, labeled from 0 to 9::
   (1080, 81)
   >>> mice.target.shape
   (1080,)
-  >>> np.unique(mice.target) # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
+  >>> np.unique(mice.target) # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS +SKIP
   array([b"'c-CS-m'", b"'c-CS-s'", b"'c-SC-m'", b"'c-SC-s'", b"'t-CS-m'",
   b"'t-CS-s'", b"'t-SC-m'", b"'t-SC-s'"], dtype='|S8')
 
 You can get more information on the dataset by looking at the ``DESCR``
 and ``details`` attributes::
 
-  >>> print(mice.DESCR) # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
+  >>> print(mice.DESCR) # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS +SKIP
   **Author**: Clara Higuera, Katheleen J. Gardiner, Krzysztof J. Cios  
   **Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression) - 2015   
   **Please cite**: Higuera C, Gardiner KJ, Cios KJ (2015) Self-Organizing
   Feature Maps Identify Proteins Critical to Learning in a Mouse Model of Down
   Syndrome. PLoS ONE 10(6): e0129126...
 
-  >>> mice.details # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
+  >>> mice.details # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS +SKIP
   {'id': '4550', 'name': 'MiceProtein', 'version': '1', 'format': 'ARFF',
   'creator': ...,
   'upload_date': '2016-02-17T14:32:49', 'licence': 'Public', 'url':
@@ -68,7 +68,7 @@ to get more information on the dataset on the openml website: https://www.openml
 The id is also the best way to specify how to fetch a dataset from OpenML::
 
   >>> mice = fetch_openml(4550, data_home=custom_data_home)
-  >>> mice.details # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
+  >>> mice.details # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS +SKIP
   {'id': '4550', 'name': 'MiceProtein', 'version': '1', 'format': 'ARFF',
   'creator': ...,
   'upload_date': '2016-02-17T14:32:49', 'licence': 'Public', 'url':
@@ -93,28 +93,28 @@ if earlier versions become inactive.
 You can see that the dataset with id 4550 that we fetched above is the version 1
 of the "miceprotein" dataset::
 
-  >>> mice.details['version']
+  >>> mice.details['version']  #doctest: +SKIP
   '1'
 
 In fact, this dataset only has one version. The iris dataset on the other hand
 has multiple versions::
 
   >>> iris = fetch_openml("iris", data_home=custom_data_home)
-  >>> iris.details['version']
+  >>> iris.details['version']  #doctest: +SKIP
   '1'
-  >>> iris.details['id']
+  >>> iris.details['id']  #doctest: +SKIP
   '61'
 
   >>> iris_61 = fetch_openml(61, data_home=custom_data_home)
-  >>> iris_61.details['version']
+  >>> iris_61.details['version']  #doctest: +SKIP
   '1'
-  >>> iris_61.details['id']
+  >>> iris_61.details['id']  #doctest: +SKIP
   '61'
 
   >>> iris_969 = fetch_openml(969, data_home=custom_data_home)
-  >>> iris_969.details['version']
+  >>> iris_969.details['version']  #doctest: +SKIP
   '3'
-  >>> iris_969.details['id']
+  >>> iris_969.details['id']  #doctest: +SKIP
   '969'
 
 'Specifying the dataset by the name "iris" yields the lowest version, version 1, with the id 61.
@@ -122,7 +122,7 @@ To make sure you always get this exact dataset, it is safest to specify it by th
 The other dataset, with id 969, is version 3 (version 2 has become inactive), and contains
 a binarized version of the data::
 
-  >>> np.unique(iris_969.target)
+  >>> np.unique(iris_969.target)  #doctest: +SKIP
   array([b'N', b'P'],
         dtype='|S1')
 
diff --git a/doc/developers/contributing.rst b/doc/developers/contributing.rst
index d1d12c5a5c..72e68bc458 100755
--- a/doc/developers/contributing.rst
+++ b/doc/developers/contributing.rst
@@ -218,10 +218,20 @@ rules before submitting a pull request:
       ``sklearn.utils`` submodule.  A list of utility routines available
       for developers can be found in the :ref:`developers-utils` page.
 
-    * If your pull request addresses an issue, please use the title to describe
-      the issue and mention the issue number in the pull request description to
-      ensure a link is created to the original issue.
-
+    * Give your pull request a helpful title that summarises what your
+      contribution does. In some cases "Fix <ISSUE TITLE>" is enough.
+      "Fix #<ISSUE NUMBER>" is not enough.
+
+    * Often pull requests resolve one or more other issues (or pull requests).
+      If merging your pull request means that some other issues/PRs should
+      be closed, you should `use keywords to create link to them
+      <https://github.com/blog/1506-closing-issues-via-pull-requests/>`_
+      (e.g., ``Fixes #1234``; multiple issues/PRs are allowed as long as each
+      one is preceded by a keyword). Upon merging, those issues/PRs will
+      automatically be closed by GitHub. If your pull request is simply
+      related to some other issues/PRs, create a link to them without using
+      the keywords (e.g., ``See also #1234``).
+    
     * All public methods should have informative docstrings with sample
       usage presented as doctests when appropriate.
 
@@ -377,29 +387,37 @@ following rules before submitting:
 Issues for New Contributors
 ---------------------------
 
-New contributors should look for the following tags when looking for issues.
-We strongly recommend that new contributors tackle "easy" issues first: this
-helps the contributor become familiar with the contribution workflow, and
-for the core devs to become acquainted with the contributor; besides which,
-we frequently underestimate how easy an issue is to solve!
+New contributors should look for the following tags when looking for issues.  We
+strongly recommend that new contributors tackle "easy" issues first: this helps
+the contributor become familiar with the contribution workflow, and for the core
+devs to become acquainted with the contributor; besides which, we frequently
+underestimate how easy an issue is to solve!
 
-.. topic:: Easy Tags
+.. topic:: good first issue tag
 
-    A great way to start contributing to scikit-learn is to pick an item from the
-    list of `Easy issues
-    <https://github.com/scikit-learn/scikit-learn/issues?q=is%3Aopen+label%3AEasy+is%3Aissue>`_
+    A great way to start contributing to scikit-learn is to pick an item from
+    the list of `good first issues
+    <https://github.com/scikit-learn/scikit-learn/labels/good%20first%20issue>`_
     in the issue tracker. Resolving these issues allow you to start contributing
-    to the project without much prior knowledge. Your assistance in this area will
-    be greatly appreciated by the more experienced developers as it helps free up
-    their time to concentrate on other issues.
+    to the project without much prior knowledge. If you have already contributed
+    to scikit-learn, you should look at Easy issues instead.
+
+.. topic:: Easy tag
 
-.. topic:: Need Contributor Tags
+    Another great way to contribute to scikit-learn is to pick an item from the
+    list of `Easy issues
+    <https://github.com/scikit-learn/scikit-learn/labels/Easy>`_ in the issue
+    tracker.  Your assistance in this area will be greatly appreciated by the
+    more experienced developers as it helps free up their time to concentrate on
+    other issues.
+
+.. topic:: help wanted tag
 
-    We often use the Need Contributor tag to mark issues regardless of difficulty. Additionally,
-    we use the Need Contributor tag to mark Pull Requests which have been abandoned
+    We often use the help wanted tag to mark issues regardless of difficulty. Additionally,
+    we use the help wanted tag to mark Pull Requests which have been abandoned
     by their original contributor and are available for someone to pick up where the original
-    contributor left off. The list of issues with the Need Contributor tag can be found
-    `here <https://github.com/scikit-learn/scikit-learn/labels/Need%20Contributor>`_ .
+    contributor left off. The list of issues with the help wanted tag can be found
+    `here <https://github.com/scikit-learn/scikit-learn/labels/help%20wanted>`_ .
 
     Note that not all issues which need contributors will have this tag.
 
@@ -553,17 +571,21 @@ should have (at least) one of the following tags:
 :New Feature:
     Feature requests and pull requests implementing a new feature.
 
-There are three other tags to help new contributors:
+There are four other tags to help new contributors:
+
+:good first issue:
+    This issue is ideal for a first contribution to scikit-learn. Ask for help
+    if the formulation is unclear. If you have already contributed to
+    scikit-learn, look at Easy issues instead.
 
 :Easy:
-    This issue can be tackled by anyone, no experience needed.
-    Ask for help if the formulation is unclear.
+    This issue can be tackled without much prior experience.
 
 :Moderate:
     Might need some knowledge of machine learning or the package,
     but is still approachable for someone new to the project.
 
-:Needs Contributor:
+:help wanted:
     This tag marks an issue which currently lacks a contributor or a
     PR that needs another contributor to take over the work. These
     issues can range in difficulty, and may not be approachable
diff --git a/doc/index.rst b/doc/index.rst
index 9aab1c9fca..2df8c8b191 100755
--- a/doc/index.rst
+++ b/doc/index.rst
@@ -207,6 +207,8 @@
                     <li><em>On-going development:</em>
                     <a href="/dev/whats_new.html"><em>What's new</em> (Changelog)</a>
                     </li>
+                    <li><em>October 2017.</em> scikit-learn 0.19.1 is available for download (<a href="whats_new.html#version-0-19">Changelog</a>).
+                    </li>
                     <li><em>July 2017.</em> scikit-learn 0.19.0 is available for download (<a href="whats_new/v0.19.html#version-0-19">Changelog</a>).
                     </li>
                     <li><em>June 2017.</em> scikit-learn 0.18.2 is available for download (<a href="whats_new/v0.18.html#version-0-18-2">Changelog</a>).
diff --git a/doc/modules/classes.rst b/doc/modules/classes.rst
index 76b97dcd35..5af088351d 100755
--- a/doc/modules/classes.rst
+++ b/doc/modules/classes.rst
@@ -26,8 +26,10 @@ Base classes
    :template: class.rst
 
    base.BaseEstimator
+   base.BiclusterMixin
    base.ClassifierMixin
    base.ClusterMixin
+   base.DensityMixin
    base.RegressorMixin
    base.TransformerMixin
 
diff --git a/doc/modules/ensemble.rst b/doc/modules/ensemble.rst
index 56bddcd172..ef46d5a3fe 100755
--- a/doc/modules/ensemble.rst
+++ b/doc/modules/ensemble.rst
@@ -598,7 +598,7 @@ minimize the loss function :math:`L` given the current model
   .. math::
 
     F_m(x) = F_{m-1}(x) + \arg\min_{h} \sum_{i=1}^{n} L(y_i,
-    F_{m-1}(x_i) - h(x))
+    F_{m-1}(x_i) + h(x))
 
 The initial model :math:`F_{0}` is problem specific, for least-squares
 regression one usually chooses the mean of the target values.
@@ -614,7 +614,7 @@ loss function:
 
   .. math::
 
-    F_m(x) = F_{m-1}(x) + \gamma_m \sum_{i=1}^{n} \nabla_F L(y_i,
+    F_m(x) = F_{m-1}(x) - \gamma_m \sum_{i=1}^{n} \nabla_F L(y_i,
     F_{m-1}(x_i))
 
 Where the step length :math:`\gamma_m` is chosen using line search:
diff --git a/doc/modules/manifold.rst b/doc/modules/manifold.rst
index 2586daffa2..76a4914519 100755
--- a/doc/modules/manifold.rst
+++ b/doc/modules/manifold.rst
@@ -533,7 +533,7 @@ the quality of the resulting embedding:
 * maximum number of iterations
 * angle (not used in the exact method)
 
-The perplexity is defined as :math:`k=2^(S)` where :math:`S` is the Shannon
+The perplexity is defined as :math:`k=2^{(S)}` where :math:`S` is the Shannon
 entropy of the conditional probability distribution. The perplexity of a
 :math:`k`-sided die is :math:`k`, so that :math:`k` is effectively the number of
 nearest neighbors t-SNE considers when generating the conditional probabilities.
@@ -598,8 +598,8 @@ where label regions largely overlap. This is a strong clue that this data can
 be well separated by non linear methods that focus on the local structure (e.g.
 an SVM with a Gaussian RBF kernel). However, failing to visualize well
 separated homogeneously labeled groups with t-SNE in 2D does not necessarily
-implie that the data cannot be correctly classified by a supervised model. It
-might be the case that 2 dimensions are not enough low to accurately represents
+imply that the data cannot be correctly classified by a supervised model. It
+might be the case that 2 dimensions are not low enough to accurately represents
 the internal structure of the data.
 
 
diff --git a/doc/whats_new/v0.19.rst b/doc/whats_new/v0.19.rst
index 2fba9b08b4..5b38f409ea 100755
--- a/doc/whats_new/v0.19.rst
+++ b/doc/whats_new/v0.19.rst
@@ -4,6 +4,136 @@
 
 .. _changes_0_19:
 
+Version 0.19.1
+==============
+
+**October 23, 2017**
+
+This is a bug-fix release with some minor documentation improvements and
+enhancements to features released in 0.19.0.
+
+Note there may be minor differences in TSNE output in this release (due to
+:issue:`9623`), in the case where multiple samples have equal distance to some
+sample.
+
+Changelog
+---------
+
+API changes
+...........
+
+- Reverted the addition of ``metrics.ndcg_score`` and ``metrics.dcg_score``
+  which had been merged into version 0.19.0 by error.  The implementations
+  were broken and undocumented.
+
+- ``return_train_score`` which was added to
+  :class:`model_selection.GridSearchCV`,
+  :class:`model_selection.RandomizedSearchCV` and
+  :func:`model_selection.cross_validate` in version 0.19.0 will be changing its
+  default value from True to False in version 0.21.  We found that calculating
+  training score could have a great effect on cross validation runtime in some
+  cases.  Users should explicitly set ``return_train_score`` to False if
+  prediction or scoring functions are slow, resulting in a deleterious effect
+  on CV runtime, or to True if they wish to use the calculated scores.
+  :issue:`9677` by :user:`Kumar Ashutosh <thechargedneutron>` and `Joel
+  Nothman`_.
+
+- ``correlation_models`` and ``regression_models`` from the legacy gaussian
+  processes implementation have been belatedly deprecated. :issue:`9717` by
+  :user:`Kumar Ashutosh <thechargedneutron>`.
+
+Bug fixes
+.........
+
+- Avoid integer overflows in :func:`metrics.matthews_corrcoef`.
+  :issue:`9693` by :user:`Sam Steingold <sam-s>`.
+
+- Fix ValueError in :class:`preprocessing.LabelEncoder` when using
+  ``inverse_transform`` on unseen labels. :issue:`9816` by :user:`Charlie Newey
+  <newey01c>`.
+
+- Fixed a bug in the objective function for :class:`manifold.TSNE` (both exact
+  and with the Barnes-Hut approximation) when ``n_components >= 3``.
+  :issue:`9711` by :user:`goncalo-rodrigues`.
+
+- Fix regression in :func:`model_selection.cross_val_predict` where it
+  raised an error with ``method='predict_proba'`` for some probabilistic
+  classifiers. :issue:`9641` by :user:`James Bourbeau <jrbourbeau>`.
+
+- Fixed a bug where :func:`datasets.make_classification` modified its input
+  ``weights``. :issue:`9865` by :user:`Sachin Kelkar <s4chin>`.
+
+- :class:`model_selection.StratifiedShuffleSplit` now works with multioutput
+  multiclass or multilabel data with more than 1000 columns.  :issue:`9922` by
+  :user:`Charlie Brummitt <crbrummitt>`.
+
+- Fixed a bug with nested and conditional parameter setting, e.g. setting a
+  pipeline step and its parameter at the same time. :issue:`9945` by `Andreas
+  Müller`_ and `Joel Nothman`_.
+
+Regressions in 0.19.0 fixed in 0.19.1:
+
+- Fixed a bug where parallelised prediction in random forests was not
+  thread-safe and could (rarely) result in arbitrary errors. :issue:`9830` by
+  `Joel Nothman`_.
+
+- Fix regression in :func:`model_selection.cross_val_predict` where it no
+  longer accepted ``X`` as a list. :issue:`9600` by :user:`Rasul Kerimov
+  <CoderINusE>`.
+
+- Fixed handling of :func:`cross_val_predict` for binary classification with
+  ``method='decision_function'``. :issue:`9593` by :user:`Reiichiro Nakano
+  <reiinakano>` and core devs.
+
+- Fix regression in :class:`pipeline.Pipeline` where it no longer accepted
+  ``steps`` as a tuple. :issue:`9604` by :user:`Joris Van den Bossche
+  <jorisvandenbossche>`.
+
+- Fix bug where ``n_iter`` was not properly deprecated, leaving ``n_iter``
+  unavailable for interim use in
+  :class:`linear_model.SGDClassifier`, :class:`linear_model.SGDRegressor`,
+  :class:`linear_model.PassiveAggressiveClassifier`,
+  :class:`linear_model.PassiveAggressiveRegressor` and
+  :class:`linear_model.Perceptron`. :issue:`9558` by `Andreas Müller`_.
+
+- Dataset fetchers make sure temporary files are closed before removing them,
+  which caused errors on Windows. :issue:`9847` by :user:`Joan Massich <massich>`.
+
+- Fixed a regression in :class:`manifold.TSNE` where it no longer supported
+  metrics other than 'euclidean' and 'precomputed'. :issue:`9623` by :user:`Oli
+  Blum <oliblum90>`.
+
+Enhancements
+............
+
+- Our test suite and :func:`utils.estimator_checks.check_estimators` can now be
+  run without Nose installed. :issue:`9697` by :user:`Joan Massich <massich>`.
+
+- To improve usability of version 0.19's :class:`pipeline.Pipeline`
+  caching, ``memory`` now allows ``joblib.Memory`` instances.
+  This make use of the new :func:`utils.validation.check_memory` helper.
+  issue:`9584` by :user:`Kumar Ashutosh <thechargedneutron>`
+
+- Some fixes to examples: :issue:`9750`, :issue:`9788`, :issue:`9815`
+
+- Made a FutureWarning in SGD-based estimators less verbose. :issue:`9802` by
+  :user:`Vrishank Bhardwaj <vrishank97>`.
+
+Code and Documentation Contributors
+-----------------------------------
+
+With thanks to:
+
+Joel Nothman, Loic Esteve, Andreas Mueller, Kumar Ashutosh,
+Vrishank Bhardwaj, Hanmin Qin, Rasul Kerimov, James Bourbeau,
+Nagarjuna Kumar, Nathaniel Saul, Olivier Grisel, Roman
+Yurchak, Reiichiro Nakano, Sachin Kelkar, Sam Steingold,
+Yaroslav Halchenko, diegodlh, felix, goncalo-rodrigues,
+jkleint, oliblum90, pasbi, Anthony Gitter, Ben Lawson, Charlie
+Brummitt, Didi Bar-Zev, Gael Varoquaux, Joan Massich, Joris
+Van den Bossche, nielsenmarkus11
+
+
 Version 0.19
 ============
 
diff --git a/doc/whats_new/v0.20.rst b/doc/whats_new/v0.20.rst
index 51d2fab65b..13efcfd6cc 100755
--- a/doc/whats_new/v0.20.rst
+++ b/doc/whats_new/v0.20.rst
@@ -142,3 +142,9 @@ Metrics
   for :func:`metrics.roc_auc_score`. Moreover using ``reorder=True`` can hide bugs
   due to floating point error in the input.
   :issue:`9851` by :user:`Hanmin Qin <qinhanmin2014>`.
+
+Cluster
+
+- Deprecate ``pooling_func`` unused parameter in
+  :class:`cluster.AgglomerativeClustering`. :issue:`9875` by :user:`Kumar Ashutosh
+  <thechargedneutron>`.
diff --git a/examples/applications/plot_stock_market.py b/examples/applications/plot_stock_market.py
index a79b4975e4..75273d1ea7 100755
--- a/examples/applications/plot_stock_market.py
+++ b/examples/applications/plot_stock_market.py
@@ -124,8 +124,8 @@ def quotes_historical_google(symbol, start_date, end_date):
     data = np.genfromtxt(response, delimiter=',', skip_header=1,
                          dtype=dtype, converters=converters,
                          missing_values='-', filling_values=-1)
-    min_date = min(data['date'], default=datetime.min.date())
-    max_date = max(data['date'], default=datetime.max.date())
+    min_date = min(data['date']) if len(data) else datetime.min.date()
+    max_date = max(data['date']) if len(data) else datetime.max.date()
     start_end_diff = (end_date - start_date).days
     min_max_diff = (max_date - min_date).days
     data_is_fine = (
diff --git a/examples/covariance/plot_outlier_detection.py b/examples/covariance/plot_outlier_detection.py
index a680bc35e0..9c697c0471 100755
--- a/examples/covariance/plot_outlier_detection.py
+++ b/examples/covariance/plot_outlier_detection.py
@@ -43,12 +43,15 @@
 
 print(__doc__)
 
-rng = np.random.RandomState(42)
+SEED = 42
+GRID_PRECISION = 100
+
+rng = np.random.RandomState(SEED)
 
 # Example settings
 n_samples = 200
 outliers_fraction = 0.25
-clusters_separation = [0, 1, 2]
+clusters_separation = (0, 1, 2)
 
 # define two outlier detection tools to be compared
 classifiers = {
@@ -63,21 +66,23 @@
         contamination=outliers_fraction)}
 
 # Compare given classifiers under given settings
-xx, yy = np.meshgrid(np.linspace(-7, 7, 100), np.linspace(-7, 7, 100))
-n_inliers = int((1. - outliers_fraction) * n_samples)
+xx, yy = np.meshgrid(np.linspace(-7, 7, GRID_PRECISION),
+                     np.linspace(-7, 7, GRID_PRECISION))
 n_outliers = int(outliers_fraction * n_samples)
+n_inliers = n_samples - n_outliers
 ground_truth = np.ones(n_samples, dtype=int)
 ground_truth[-n_outliers:] = -1
 
 # Fit the problem with varying cluster separation
-for i, offset in enumerate(clusters_separation):
-    np.random.seed(42)
+for _, offset in enumerate(clusters_separation):
+    np.random.seed(SEED)
     # Data generation
     X1 = 0.3 * np.random.randn(n_inliers // 2, 2) - offset
     X2 = 0.3 * np.random.randn(n_inliers // 2, 2) + offset
-    X = np.r_[X1, X2]
+    X = np.concatenate([X1, X2], axis=0)
     # Add outliers
-    X = np.r_[X, np.random.uniform(low=-6, high=6, size=(n_outliers, 2))]
+    X = np.concatenate([X, np.random.uniform(low=-6, high=6,
+                       size=(n_outliers, 2))], axis=0)
 
     # Fit the model
     plt.figure(figsize=(9, 7))
diff --git a/examples/ensemble/plot_forest_iris.py b/examples/ensemble/plot_forest_iris.py
index 73db88d829..81cd54a9bb 100755
--- a/examples/ensemble/plot_forest_iris.py
+++ b/examples/ensemble/plot_forest_iris.py
@@ -107,7 +107,7 @@
         plt.subplot(3, 4, plot_idx)
         if plot_idx <= len(models):
             # Add a title at the top of each column
-            plt.title(model_title)
+            plt.title(model_title, fontsize=9)
 
         # Now plot the decision boundary using a fine mesh as input to a
         # filled contour plot
@@ -154,7 +154,7 @@
                     edgecolor='k', s=20)
         plot_idx += 1  # move on to the next plot in sequence
 
-plt.suptitle("Classifiers on feature subsets of the Iris dataset")
+plt.suptitle("Classifiers on feature subsets of the Iris dataset", fontsize=12)
 plt.axis("tight")
-
+plt.tight_layout(h_pad=0.2, w_pad=0.2, pad=2.5)
 plt.show()
diff --git a/sklearn/base.py b/sklearn/base.py
index d97fe92ccd..b653b7149c 100755
--- a/sklearn/base.py
+++ b/sklearn/base.py
@@ -5,6 +5,7 @@
 
 import copy
 import warnings
+from collections import defaultdict
 
 import numpy as np
 from scipy import sparse
@@ -248,26 +249,24 @@ def set_params(self, **params):
             # Simple optimization to gain speed (inspect is slow)
             return self
         valid_params = self.get_params(deep=True)
-        for key, value in six.iteritems(params):
-            split = key.split('__', 1)
-            if len(split) > 1:
-                # nested objects case
-                name, sub_name = split
-                if name not in valid_params:
-                    raise ValueError('Invalid parameter %s for estimator %s. '
-                                     'Check the list of available parameters '
-                                     'with `estimator.get_params().keys()`.' %
-                                     (name, self))
-                sub_object = valid_params[name]
-                sub_object.set_params(**{sub_name: value})
+
+        nested_params = defaultdict(dict)  # grouped by prefix
+        for key, value in params.items():
+            key, delim, sub_key = key.partition('__')
+            if key not in valid_params:
+                raise ValueError('Invalid parameter %s for estimator %s. '
+                                 'Check the list of available parameters '
+                                 'with `estimator.get_params().keys()`.' %
+                                 (key, self))
+
+            if delim:
+                nested_params[key][sub_key] = value
             else:
-                # simple objects case
-                if key not in valid_params:
-                    raise ValueError('Invalid parameter %s for estimator %s. '
-                                     'Check the list of available parameters '
-                                     'with `estimator.get_params().keys()`.' %
-                                     (key, self.__class__.__name__))
                 setattr(self, key, value)
+
+        for key, sub_params in nested_params.items():
+            valid_params[key].set_params(**sub_params)
+
         return self
 
     def __repr__(self):
diff --git a/sklearn/cluster/hierarchical.py b/sklearn/cluster/hierarchical.py
index c8ead24319..deb0bb5b0c 100755
--- a/sklearn/cluster/hierarchical.py
+++ b/sklearn/cluster/hierarchical.py
@@ -641,10 +641,12 @@ class AgglomerativeClustering(BaseEstimator, ClusterMixin):
         - complete or maximum linkage uses the maximum distances between
           all observations of the two sets.
 
-    pooling_func : callable, default=np.mean
-        This combines the values of agglomerated features into a single
-        value, and should accept an array of shape [M, N] and the keyword
-        argument ``axis=1``, and reduce it to an array of size [M].
+    pooling_func : callable, default='deprecated'
+        Ignored.
+
+        .. deprecated:: 0.20
+            ``pooling_func`` has been deprecated in 0.20 and will be removed
+            in 0.22.
 
     Attributes
     ----------
@@ -670,7 +672,7 @@ class AgglomerativeClustering(BaseEstimator, ClusterMixin):
     def __init__(self, n_clusters=2, affinity="euclidean",
                  memory=None,
                  connectivity=None, compute_full_tree='auto',
-                 linkage='ward', pooling_func=np.mean):
+                 linkage='ward', pooling_func='deprecated'):
         self.n_clusters = n_clusters
         self.memory = memory
         self.connectivity = connectivity
@@ -694,6 +696,10 @@ def fit(self, X, y=None):
         -------
         self
         """
+        if self.pooling_func != 'deprecated':
+            warnings.warn('Agglomerative "pooling_func" parameter is not used.'
+                          ' It has been deprecated in version 0.20 and will be'
+                          'removed in 0.22', DeprecationWarning)
         X = check_array(X, ensure_min_samples=2, estimator=self)
         memory = check_memory(self.memory)
 
@@ -829,6 +835,16 @@ class FeatureAgglomeration(AgglomerativeClustering, AgglomerationTransform):
         are merged to form node `n_features + i`
     """
 
+    def __init__(self, n_clusters=2, affinity="euclidean",
+                 memory=None,
+                 connectivity=None, compute_full_tree='auto',
+                 linkage='ward', pooling_func=np.mean):
+        super(FeatureAgglomeration, self).__init__(
+            n_clusters=n_clusters, memory=memory, connectivity=connectivity,
+            compute_full_tree=compute_full_tree, linkage=linkage,
+            affinity=affinity)
+        self.pooling_func = pooling_func
+
     def fit(self, X, y=None, **params):
         """Fit the hierarchical clustering on the data
 
diff --git a/sklearn/decomposition/truncated_svd.py b/sklearn/decomposition/truncated_svd.py
index 028304672e..726f9162eb 100755
--- a/sklearn/decomposition/truncated_svd.py
+++ b/sklearn/decomposition/truncated_svd.py
@@ -110,7 +110,7 @@ class TruncatedSVD(BaseEstimator, TransformerMixin):
 
     Notes
     -----
-    SVD suffers from a problem called "sign indeterminancy", which means the
+    SVD suffers from a problem called "sign indeterminacy", which means the
     sign of the ``components_`` and the output from transform depend on the
     algorithm and random state. To work around this, fit instances of this
     class to data once, then keep the instance around to do transformations.
diff --git a/sklearn/feature_selection/variance_threshold.py b/sklearn/feature_selection/variance_threshold.py
index 13e1aa7078..c9e018d94a 100755
--- a/sklearn/feature_selection/variance_threshold.py
+++ b/sklearn/feature_selection/variance_threshold.py
@@ -54,7 +54,7 @@ def fit(self, X, y=None):
             Sample vectors from which to compute variances.
 
         y : any
-            Ignored This parameter exists only for compatibility with
+            Ignored. This parameter exists only for compatibility with
             sklearn.pipeline.Pipeline.
 
         Returns
diff --git a/sklearn/gaussian_process/correlation_models.py b/sklearn/gaussian_process/correlation_models.py
index 941f7756fb..3b954e2582 100755
--- a/sklearn/gaussian_process/correlation_models.py
+++ b/sklearn/gaussian_process/correlation_models.py
@@ -14,7 +14,7 @@
 
 
 @deprecated("The function absolute_exponential of correlation_models is "
-            "deprecated in version 0.20 and will be removed in 0.22.")
+            "deprecated in version 0.19.1 and will be removed in 0.22.")
 def absolute_exponential(theta, d):
     """
     Absolute exponential autocorrelation model.
@@ -58,7 +58,7 @@ def absolute_exponential(theta, d):
 
 
 @deprecated("The function squared_exponential of correlation_models is "
-            "deprecated in version 0.20 and will be removed in 0.22.")
+            "deprecated in version 0.19.1 and will be removed in 0.22.")
 def squared_exponential(theta, d):
     """
     Squared exponential correlation model (Radial Basis Function).
@@ -103,7 +103,7 @@ def squared_exponential(theta, d):
 
 
 @deprecated("The function generalized_exponential of correlation_models is "
-            "deprecated in version 0.20 and will be removed in 0.22.")
+            "deprecated in version 0.19.1 and will be removed in 0.22.")
 def generalized_exponential(theta, d):
     """
     Generalized exponential correlation model.
@@ -155,7 +155,7 @@ def generalized_exponential(theta, d):
 
 
 @deprecated("The function pure_nugget of correlation_models is "
-            "deprecated in version 0.20 and will be removed in 0.22.")
+            "deprecated in version 0.19.1 and will be removed in 0.22.")
 def pure_nugget(theta, d):
     """
     Spatial independence correlation model (pure nugget).
@@ -194,7 +194,7 @@ def pure_nugget(theta, d):
 
 
 @deprecated("The function cubic of correlation_models is "
-            "deprecated in version 0.20 and will be removed in 0.22.")
+            "deprecated in version 0.19.1 and will be removed in 0.22.")
 def cubic(theta, d):
     """
     Cubic correlation model::
@@ -246,7 +246,7 @@ def cubic(theta, d):
 
 
 @deprecated("The function linear of correlation_models is "
-            "deprecated in version 0.20 and will be removed in 0.22.")
+            "deprecated in version 0.19.1 and will be removed in 0.22.")
 def linear(theta, d):
     """
     Linear correlation model::
diff --git a/sklearn/gaussian_process/regression_models.py b/sklearn/gaussian_process/regression_models.py
index 7d2152dfc5..b0f7535d11 100755
--- a/sklearn/gaussian_process/regression_models.py
+++ b/sklearn/gaussian_process/regression_models.py
@@ -14,7 +14,7 @@
 
 
 @deprecated("The function constant of regression_models is "
-            "deprecated in version 0.20 and will be removed in 0.22.")
+            "deprecated in version 0.19.1 and will be removed in 0.22.")
 def constant(x):
     """
     Zero order polynomial (constant, p = 1) regression model.
@@ -40,7 +40,7 @@ def constant(x):
 
 
 @deprecated("The function linear of regression_models is "
-            "deprecated in version 0.20 and will be removed in 0.22.")
+            "deprecated in version 0.19.1 and will be removed in 0.22.")
 def linear(x):
     """
     First order polynomial (linear, p = n+1) regression model.
@@ -66,7 +66,7 @@ def linear(x):
 
 
 @deprecated("The function quadratic of regression_models is "
-            "deprecated in version 0.20 and will be removed in 0.22.")
+            "deprecated in version 0.19.1 and will be removed in 0.22.")
 def quadratic(x):
     """
     Second order polynomial (quadratic, p = n*(n-1)/2+n+1) regression model.
diff --git a/sklearn/kernel_ridge.py b/sklearn/kernel_ridge.py
index 3ae1cfac59..308d0661bc 100755
--- a/sklearn/kernel_ridge.py
+++ b/sklearn/kernel_ridge.py
@@ -83,9 +83,9 @@ class KernelRidge(BaseEstimator, RegressorMixin):
 
     See also
     --------
-    Ridge
+    sklearn.linear_model.Ridge:
         Linear ridge regression.
-    SVR
+    sklearn.svm.SVR:
         Support Vector Regression implemented using libsvm.
 
     Examples
diff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py
index 59e6db8457..7c8a8d9ae4 100755
--- a/sklearn/linear_model/logistic.py
+++ b/sklearn/linear_model/logistic.py
@@ -1415,7 +1415,7 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,
         default scoring option used is 'accuracy'.
 
     solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},
-        default: 'liblinear'
+        default: 'lbfgs'
         Algorithm to use in the optimization problem.
 
         - For small datasets, 'liblinear' is a good choice, whereas 'sag' and
diff --git a/sklearn/model_selection/_search.py b/sklearn/model_selection/_search.py
index ebfa1e9bd3..f574b39e89 100755
--- a/sklearn/model_selection/_search.py
+++ b/sklearn/model_selection/_search.py
@@ -36,6 +36,7 @@
 from ..utils.random import sample_without_replacement
 from ..utils.validation import indexable, check_is_fitted
 from ..utils.metaestimators import if_delegate_has_method
+from ..utils.deprecation import DeprecationDict
 from ..metrics.scorer import _check_multimetric_scoring
 from ..metrics.scorer import check_scoring
 
@@ -651,7 +652,9 @@ def fit(self, X, y=None, groups=None, **fit_params):
         if self.return_train_score:
             train_scores = _aggregate_score_dicts(train_score_dicts)
 
-        results = dict()
+        # TODO: replace by a dict in 0.21
+        results = (DeprecationDict() if self.return_train_score == 'warn'
+                   else {})
 
         def _store(key_name, array, weights=None, splits=False, rank=False):
             """A small helper to store the scores/times to the cv_results_"""
@@ -706,9 +709,20 @@ def _store(key_name, array, weights=None, splits=False, rank=False):
                    splits=True, rank=True,
                    weights=test_sample_counts if self.iid else None)
             if self.return_train_score:
+                prev_keys = set(results.keys())
                 _store('train_%s' % scorer_name, train_scores[scorer_name],
                        splits=True)
 
+                if self.return_train_score == 'warn':
+                    for key in set(results.keys()) - prev_keys:
+                        message = (
+                            'You are accessing a training score ({!r}), '
+                            'which will not be available by default '
+                            'any more in 0.21. If you need training scores, '
+                            'please set return_train_score=True').format(key)
+                        # warn on key access
+                        results.add_warning(key, message, FutureWarning)
+
         # For multi-metric evaluation, store the best_index_, best_params_ and
         # best_score_ iff refit is one of the scorer names
         # In single metric evaluation, refit_metric is "score"
@@ -882,10 +896,19 @@ class GridSearchCV(BaseSearchCV):
         FitFailedWarning is raised. This parameter does not affect the refit
         step, which will always raise the error.
 
-    return_train_score : boolean, default=True
-        If ``'False'``, the ``cv_results_`` attribute will not include training
+    return_train_score : boolean, optional
+        If ``False``, the ``cv_results_`` attribute will not include training
         scores.
 
+        Current default is ``'warn'``, which behaves as ``True`` in addition
+        to raising a warning when a training score is looked up.
+        That default will be changed to ``False`` in 0.21.
+        Computing training scores is used to get insights on how different
+        parameter settings impact the overfitting/underfitting trade-off.
+        However computing the scores on the training set can be computationally
+        expensive and is not strictly required to select the parameters that
+        yield the best generalization performance.
+
 
     Examples
     --------
@@ -1044,7 +1067,7 @@ class GridSearchCV(BaseSearchCV):
     def __init__(self, estimator, param_grid, scoring=None, fit_params=None,
                  n_jobs=1, iid=True, refit=True, cv=None, verbose=0,
                  pre_dispatch='2*n_jobs', error_score='raise',
-                 return_train_score=True):
+                 return_train_score="warn"):
         super(GridSearchCV, self).__init__(
             estimator=estimator, scoring=scoring, fit_params=fit_params,
             n_jobs=n_jobs, iid=iid, refit=refit, cv=cv, verbose=verbose,
@@ -1200,10 +1223,19 @@ class RandomizedSearchCV(BaseSearchCV):
         FitFailedWarning is raised. This parameter does not affect the refit
         step, which will always raise the error.
 
-    return_train_score : boolean, default=True
-        If ``'False'``, the ``cv_results_`` attribute will not include training
+    return_train_score : boolean, optional
+        If ``False``, the ``cv_results_`` attribute will not include training
         scores.
 
+        Current default is ``'warn'``, which behaves as ``True`` in addition
+        to raising a warning when a training score is looked up.
+        That default will be changed to ``False`` in 0.21.
+        Computing training scores is used to get insights on how different
+        parameter settings impact the overfitting/underfitting trade-off.
+        However computing the scores on the training set can be computationally
+        expensive and is not strictly required to select the parameters that
+        yield the best generalization performance.
+
     Attributes
     ----------
     cv_results_ : dict of numpy (masked) ndarrays
@@ -1327,7 +1359,7 @@ class RandomizedSearchCV(BaseSearchCV):
     def __init__(self, estimator, param_distributions, n_iter=10, scoring=None,
                  fit_params=None, n_jobs=1, iid=True, refit=True, cv=None,
                  verbose=0, pre_dispatch='2*n_jobs', random_state=None,
-                 error_score='raise', return_train_score=True):
+                 error_score='raise', return_train_score="warn"):
         self.param_distributions = param_distributions
         self.n_iter = n_iter
         self.random_state = random_state
diff --git a/sklearn/model_selection/_validation.py b/sklearn/model_selection/_validation.py
index 798f771534..fdf6fa6912 100755
--- a/sklearn/model_selection/_validation.py
+++ b/sklearn/model_selection/_validation.py
@@ -21,6 +21,7 @@
 
 from ..base import is_classifier, clone
 from ..utils import indexable, check_random_state, safe_indexing
+from ..utils.deprecation import DeprecationDict
 from ..utils.validation import _is_arraylike, _num_samples
 from ..utils.metaestimators import _safe_split
 from ..externals.joblib import Parallel, delayed, logger
@@ -37,7 +38,7 @@
 
 def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv=None,
                    n_jobs=1, verbose=0, fit_params=None,
-                   pre_dispatch='2*n_jobs', return_train_score=True):
+                   pre_dispatch='2*n_jobs', return_train_score="warn"):
     """Evaluate metric(s) by cross-validation and also record fit/score times.
 
     Read more in the :ref:`User Guide <multimetric_cross_validation>`.
@@ -115,9 +116,17 @@ def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv=None,
             - A string, giving an expression as a function of n_jobs,
               as in '2*n_jobs'
 
-    return_train_score : boolean, default True
-        Whether to include train scores in the return dict if ``scoring`` is
-        of multimetric type.
+    return_train_score : boolean, optional
+        Whether to include train scores.
+
+        Current default is ``'warn'``, which behaves as ``True`` in addition
+        to raising a warning when a training score is looked up.
+        That default will be changed to ``False`` in 0.21.
+        Computing training scores is used to get insights on how different
+        parameter settings impact the overfitting/underfitting trade-off.
+        However computing the scores on the training set can be computationally
+        expensive and is not strictly required to select the parameters that
+        yield the best generalization performance.
 
     Returns
     -------
@@ -203,14 +212,24 @@ def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv=None,
         test_scores, fit_times, score_times = zip(*scores)
     test_scores = _aggregate_score_dicts(test_scores)
 
-    ret = dict()
+    # TODO: replace by a dict in 0.21
+    ret = DeprecationDict() if return_train_score == 'warn' else {}
     ret['fit_time'] = np.array(fit_times)
     ret['score_time'] = np.array(score_times)
 
     for name in scorers:
         ret['test_%s' % name] = np.array(test_scores[name])
         if return_train_score:
-            ret['train_%s' % name] = np.array(train_scores[name])
+            key = 'train_%s' % name
+            ret[key] = np.array(train_scores[name])
+            if return_train_score == 'warn':
+                message = (
+                    'You are accessing a training score ({!r}), '
+                    'which will not be available by default '
+                    'any more in 0.21. If you need training scores, '
+                    'please set return_train_score=True').format(key)
+                # warn on key access
+                ret.add_warning(key, message, FutureWarning)
 
     return ret
 
@@ -625,6 +644,15 @@ def cross_val_predict(estimator, X, y=None, groups=None, cv=None, n_jobs=1,
     predictions : ndarray
         This is the result of calling ``method``
 
+    Notes
+    -----
+    In the case that one or more classes are absent in a training portion, a
+    default score needs to be assigned to all instances for that class if
+    ``method`` produces columns per class, as in {'decision_function',
+    'predict_proba', 'predict_log_proba'}.  For ``predict_proba`` this value is
+    0.  In order to ensure finite output, we approximate negative infinity by
+    the minimum finite float value for the dtype in other cases.
+
     Examples
     --------
     >>> from sklearn import datasets, linear_model
@@ -727,12 +755,49 @@ def _fit_and_predict(estimator, X, y, train, test, verbose, fit_params,
     predictions = func(X_test)
     if method in ['decision_function', 'predict_proba', 'predict_log_proba']:
         n_classes = len(set(y))
-        predictions_ = np.zeros((_num_samples(X_test), n_classes))
-        if method == 'decision_function' and len(estimator.classes_) == 2:
-            predictions_[:, estimator.classes_[-1]] = predictions
-        else:
-            predictions_[:, estimator.classes_] = predictions
-        predictions = predictions_
+        if n_classes != len(estimator.classes_):
+            recommendation = (
+                'To fix this, use a cross-validation '
+                'technique resulting in properly '
+                'stratified folds')
+            warnings.warn('Number of classes in training fold ({}) does '
+                          'not match total number of classes ({}). '
+                          'Results may not be appropriate for your use case. '
+                          '{}'.format(len(estimator.classes_),
+                                      n_classes, recommendation),
+                          RuntimeWarning)
+            if method == 'decision_function':
+                if (predictions.ndim == 2 and
+                        predictions.shape[1] != len(estimator.classes_)):
+                    # This handles the case when the shape of predictions
+                    # does not match the number of classes used to train
+                    # it with. This case is found when sklearn.svm.SVC is
+                    # set to `decision_function_shape='ovo'`.
+                    raise ValueError('Output shape {} of {} does not match '
+                                     'number of classes ({}) in fold. '
+                                     'Irregular decision_function outputs '
+                                     'are not currently supported by '
+                                     'cross_val_predict'.format(
+                                        predictions.shape, method,
+                                        len(estimator.classes_),
+                                        recommendation))
+                if len(estimator.classes_) <= 2:
+                    # In this special case, `predictions` contains a 1D array.
+                    raise ValueError('Only {} class/es in training fold, this '
+                                     'is not supported for decision_function '
+                                     'with imbalanced folds. {}'.format(
+                                        len(estimator.classes_),
+                                        recommendation))
+
+            float_min = np.finfo(predictions.dtype).min
+            default_values = {'decision_function': float_min,
+                              'predict_log_proba': float_min,
+                              'predict_proba': 0}
+            predictions_for_all_classes = np.full((_num_samples(predictions),
+                                                   n_classes),
+                                                  default_values[method])
+            predictions_for_all_classes[:, estimator.classes_] = predictions
+            predictions = predictions_for_all_classes
     return predictions, test
 
 
@@ -998,7 +1063,7 @@ def learning_curve(estimator, X, y, groups=None,
         If int, random_state is the seed used by the random number generator;
         If RandomState instance, random_state is the random number generator;
         If None, the random number generator is the RandomState instance used
-        by `np.random`. Used when ``shuffle`` == 'True'.
+        by `np.random`. Used when ``shuffle`` is True.
 
     Returns
     -------
diff --git a/sklearn/model_selection/tests/test_search.py b/sklearn/model_selection/tests/test_search.py
index ee3fe26eed..151f9a2174 100755
--- a/sklearn/model_selection/tests/test_search.py
+++ b/sklearn/model_selection/tests/test_search.py
@@ -19,6 +19,7 @@
 from sklearn.utils.testing import assert_raises
 from sklearn.utils.testing import assert_warns
 from sklearn.utils.testing import assert_warns_message
+from sklearn.utils.testing import assert_no_warnings
 from sklearn.utils.testing import assert_raise_message
 from sklearn.utils.testing import assert_false, assert_true
 from sklearn.utils.testing import assert_array_equal
@@ -332,6 +333,41 @@ def test_grid_search_groups():
         gs.fit(X, y)
 
 
+def test_return_train_score_warn():
+    # Test that warnings are raised. Will be removed in 0.21
+
+    X = np.arange(100).reshape(10, 10)
+    y = np.array([0] * 5 + [1] * 5)
+    grid = {'C': [1, 2]}
+
+    estimators = [GridSearchCV(LinearSVC(random_state=0), grid),
+                  RandomizedSearchCV(LinearSVC(random_state=0), grid,
+                                     n_iter=2)]
+
+    result = {}
+    for estimator in estimators:
+        for val in [True, False, 'warn']:
+            estimator.set_params(return_train_score=val)
+            result[val] = assert_no_warnings(estimator.fit, X, y).cv_results_
+
+    train_keys = ['split0_train_score', 'split1_train_score',
+                  'split2_train_score', 'mean_train_score', 'std_train_score']
+    for key in train_keys:
+        msg = (
+            'You are accessing a training score ({!r}), '
+            'which will not be available by default '
+            'any more in 0.21. If you need training scores, '
+            'please set return_train_score=True').format(key)
+        train_score = assert_warns_message(FutureWarning, msg,
+                                           result['warn'].get, key)
+        assert np.allclose(train_score, result[True][key])
+        assert key not in result[False]
+
+    for key in result['warn']:
+        if key not in train_keys:
+            assert_no_warnings(result['warn'].get, key)
+
+
 def test_classes__property():
     # Test that classes_ property matches best_estimator_.classes_
     X = np.arange(100).reshape(10, 10)
diff --git a/sklearn/model_selection/tests/test_validation.py b/sklearn/model_selection/tests/test_validation.py
index baff762574..ad49385100 100755
--- a/sklearn/model_selection/tests/test_validation.py
+++ b/sklearn/model_selection/tests/test_validation.py
@@ -16,12 +16,14 @@
 from sklearn.utils.testing import assert_almost_equal
 from sklearn.utils.testing import assert_raises
 from sklearn.utils.testing import assert_raise_message
+from sklearn.utils.testing import assert_warns
+from sklearn.utils.testing import assert_warns_message
+from sklearn.utils.testing import assert_no_warnings
 from sklearn.utils.testing import assert_raises_regex
 from sklearn.utils.testing import assert_greater
 from sklearn.utils.testing import assert_less
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_array_equal
-from sklearn.utils.testing import assert_warns
 from sklearn.utils.mocking import CheckingClassifier, MockDataFrame
 
 from sklearn.model_selection import cross_val_score
@@ -42,6 +44,7 @@
 from sklearn.datasets import make_regression
 from sklearn.datasets import load_boston
 from sklearn.datasets import load_iris
+from sklearn.datasets import load_digits
 from sklearn.metrics import explained_variance_score
 from sklearn.metrics import make_scorer
 from sklearn.metrics import accuracy_score
@@ -52,7 +55,7 @@
 from sklearn.metrics.scorer import check_scoring
 
 from sklearn.linear_model import Ridge, LogisticRegression, SGDClassifier
-from sklearn.linear_model import PassiveAggressiveClassifier
+from sklearn.linear_model import PassiveAggressiveClassifier, RidgeClassifier
 from sklearn.neighbors import KNeighborsClassifier
 from sklearn.svm import SVC
 from sklearn.cluster import KMeans
@@ -379,6 +382,28 @@ def test_cross_validate():
         yield check_cross_validate_multi_metric, est, X, y, scores
 
 
+def test_cross_validate_return_train_score_warn():
+    # Test that warnings are raised. Will be removed in 0.21
+
+    X, y = make_classification(random_state=0)
+    estimator = MockClassifier()
+
+    result = {}
+    for val in [False, True, 'warn']:
+        result[val] = assert_no_warnings(cross_validate, estimator, X, y,
+                                         return_train_score=val)
+
+    msg = (
+        'You are accessing a training score ({!r}), '
+        'which will not be available by default '
+        'any more in 0.21. If you need training scores, '
+        'please set return_train_score=True').format('train_score')
+    train_score = assert_warns_message(FutureWarning, msg,
+                                       result['warn'].get, 'train_score')
+    assert np.allclose(train_score, result[True]['train_score'])
+    assert 'train_score' not in result[False]
+
+
 def check_cross_validate_single_metric(clf, X, y, scores):
     (train_mse_scores, test_mse_scores, train_r2_scores,
      test_r2_scores) = scores
@@ -776,6 +801,89 @@ def split(self, X, y=None, groups=None):
 
     assert_raises(ValueError, cross_val_predict, est, X, y, cv=BadCV())
 
+    X, y = load_iris(return_X_y=True)
+
+    warning_message = ('Number of classes in training fold (2) does '
+                       'not match total number of classes (3). '
+                       'Results may not be appropriate for your use case.')
+    assert_warns_message(RuntimeWarning, warning_message,
+                         cross_val_predict, LogisticRegression(),
+                         X, y, method='predict_proba', cv=KFold(2))
+
+
+def test_cross_val_predict_decision_function_shape():
+    X, y = make_classification(n_classes=2, n_samples=50, random_state=0)
+
+    preds = cross_val_predict(LogisticRegression(), X, y,
+                              method='decision_function')
+    assert_equal(preds.shape, (50,))
+
+    X, y = load_iris(return_X_y=True)
+
+    preds = cross_val_predict(LogisticRegression(), X, y,
+                              method='decision_function')
+    assert_equal(preds.shape, (150, 3))
+
+    # This specifically tests imbalanced splits for binary
+    # classification with decision_function. This is only
+    # applicable to classifiers that can be fit on a single
+    # class.
+    X = X[:100]
+    y = y[:100]
+    assert_raise_message(ValueError,
+                         'Only 1 class/es in training fold, this'
+                         ' is not supported for decision_function'
+                         ' with imbalanced folds. To fix '
+                         'this, use a cross-validation technique '
+                         'resulting in properly stratified folds',
+                         cross_val_predict, RidgeClassifier(), X, y,
+                         method='decision_function', cv=KFold(2))
+
+    X, y = load_digits(return_X_y=True)
+    est = SVC(kernel='linear', decision_function_shape='ovo')
+
+    preds = cross_val_predict(est,
+                              X, y,
+                              method='decision_function')
+    assert_equal(preds.shape, (1797, 45))
+
+    ind = np.argsort(y)
+    X, y = X[ind], y[ind]
+    assert_raises_regex(ValueError,
+                        'Output shape \(599L?, 21L?\) of decision_function '
+                        'does not match number of classes \(7\) in fold. '
+                        'Irregular decision_function .*',
+                        cross_val_predict, est, X, y,
+                        cv=KFold(n_splits=3), method='decision_function')
+
+
+def test_cross_val_predict_predict_proba_shape():
+    X, y = make_classification(n_classes=2, n_samples=50, random_state=0)
+
+    preds = cross_val_predict(LogisticRegression(), X, y,
+                              method='predict_proba')
+    assert_equal(preds.shape, (50, 2))
+
+    X, y = load_iris(return_X_y=True)
+
+    preds = cross_val_predict(LogisticRegression(), X, y,
+                              method='predict_proba')
+    assert_equal(preds.shape, (150, 3))
+
+
+def test_cross_val_predict_predict_log_proba_shape():
+    X, y = make_classification(n_classes=2, n_samples=50, random_state=0)
+
+    preds = cross_val_predict(LogisticRegression(), X, y,
+                              method='predict_log_proba')
+    assert_equal(preds.shape, (50, 2))
+
+    X, y = load_iris(return_X_y=True)
+
+    preds = cross_val_predict(LogisticRegression(), X, y,
+                              method='predict_log_proba')
+    assert_equal(preds.shape, (150, 3))
+
 
 def test_cross_val_predict_input_types():
     iris = load_iris()
@@ -1217,11 +1325,12 @@ def get_expected_predictions(X, y, cv, classes, est, method):
         est.fit(X[train], y[train])
         expected_predictions_ = func(X[test])
         # To avoid 2 dimensional indexing
-        exp_pred_test = np.zeros((len(test), classes))
-        if method is 'decision_function' and len(est.classes_) == 2:
-            exp_pred_test[:, est.classes_[-1]] = expected_predictions_
+        if method is 'predict_proba':
+            exp_pred_test = np.zeros((len(test), classes))
         else:
-            exp_pred_test[:, est.classes_] = expected_predictions_
+            exp_pred_test = np.full((len(test), classes),
+                                    np.finfo(expected_predictions.dtype).min)
+        exp_pred_test[:, est.classes_] = expected_predictions_
         expected_predictions[test] = exp_pred_test
 
     return expected_predictions
@@ -1229,9 +1338,9 @@ def get_expected_predictions(X, y, cv, classes, est, method):
 
 def test_cross_val_predict_class_subset():
 
-    X = np.arange(8).reshape(4, 2)
-    y = np.array([0, 0, 1, 2])
-    classes = 3
+    X = np.arange(200).reshape(100, 2)
+    y = np.array([x//10 for x in range(100)])
+    classes = 10
 
     kfold3 = KFold(n_splits=3)
     kfold4 = KFold(n_splits=4)
@@ -1259,7 +1368,7 @@ def test_cross_val_predict_class_subset():
         assert_array_almost_equal(expected_predictions, predictions)
 
         # Testing unordered labels
-        y = [1, 1, -4, 6]
+        y = shuffle(np.repeat(range(10), 10), random_state=0)
         predictions = cross_val_predict(est, X, y, method=method,
                                         cv=kfold3)
         y = le.fit_transform(y)
diff --git a/sklearn/multioutput.py b/sklearn/multioutput.py
index 6c9fbc55f7..5b4389fd0f 100755
--- a/sklearn/multioutput.py
+++ b/sklearn/multioutput.py
@@ -543,11 +543,6 @@ def predict(self, X):
     def predict_proba(self, X):
         """Predict probability estimates.
 
-        By default the inputs to later models in a chain is the binary class
-        predictions not the class probabilities. To use class probabilities
-        as features in subsequent models set the cv property to be one of
-        the allowed values other than None.
-
         Parameters
         ----------
         X : {array-like, sparse matrix}, shape (n_samples, n_features)
diff --git a/sklearn/tests/test_base.py b/sklearn/tests/test_base.py
index 7ad0f20382..580a4e2eca 100755
--- a/sklearn/tests/test_base.py
+++ b/sklearn/tests/test_base.py
@@ -228,6 +228,24 @@ def test_set_params():
     #               bad__stupid_param=True)
 
 
+def test_set_params_passes_all_parameters():
+    # Make sure all parameters are passed together to set_params
+    # of nested estimator. Regression test for #9944
+
+    class TestDecisionTree(DecisionTreeClassifier):
+        def set_params(self, **kwargs):
+            super(TestDecisionTree, self).set_params(**kwargs)
+            # expected_kwargs is in test scope
+            assert kwargs == expected_kwargs
+            return self
+
+    expected_kwargs = {'max_depth': 5, 'min_samples_leaf': 2}
+    for est in [Pipeline([('estimator', TestDecisionTree())]),
+                GridSearchCV(TestDecisionTree(), {})]:
+        est.set_params(estimator__max_depth=5,
+                       estimator__min_samples_leaf=2)
+
+
 def test_score_sample_weight():
 
     rng = np.random.RandomState(0)
diff --git a/sklearn/tests/test_pipeline.py b/sklearn/tests/test_pipeline.py
index d1d62f80e5..ab2108ed69 100755
--- a/sklearn/tests/test_pipeline.py
+++ b/sklearn/tests/test_pipeline.py
@@ -24,10 +24,11 @@
 from sklearn.base import clone, BaseEstimator
 from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline, make_union
 from sklearn.svm import SVC
-from sklearn.linear_model import LogisticRegression
+from sklearn.linear_model import LogisticRegression, Lasso
 from sklearn.linear_model import LinearRegression
 from sklearn.cluster import KMeans
 from sklearn.feature_selection import SelectKBest, f_classif
+from sklearn.dummy import DummyRegressor
 from sklearn.decomposition import PCA, TruncatedSVD
 from sklearn.datasets import load_iris
 from sklearn.preprocessing import StandardScaler
@@ -289,7 +290,7 @@ def test_pipeline_raise_set_params_error():
                  'with `estimator.get_params().keys()`.')
 
     assert_raise_message(ValueError,
-                         error_msg % ('fake', 'Pipeline'),
+                         error_msg % ('fake', pipe),
                          pipe.set_params,
                          fake='nope')
 
@@ -863,6 +864,16 @@ def test_step_name_validation():
                                  [[1]], [1])
 
 
+def test_set_params_nested_pipeline():
+    estimator = Pipeline([
+        ('a', Pipeline([
+            ('b', DummyRegressor())
+        ]))
+    ])
+    estimator.set_params(a__b__alpha=0.001, a__b=Lasso())
+    estimator.set_params(a__steps=[('b', LogisticRegression())], a__b__C=5)
+
+
 def test_pipeline_wrong_memory():
     # Test that an error is raised when memory is not a string or a Memory
     # instance
diff --git a/sklearn/utils/deprecation.py b/sklearn/utils/deprecation.py
index 08530be264..5621f436d9 100755
--- a/sklearn/utils/deprecation.py
+++ b/sklearn/utils/deprecation.py
@@ -2,7 +2,7 @@
 import warnings
 import functools
 
-__all__ = ["deprecated", ]
+__all__ = ["deprecated", "DeprecationDict"]
 
 
 class deprecated(object):
@@ -102,3 +102,33 @@ def _is_deprecated(func):
                                               for c in closures
                      if isinstance(c.cell_contents, str)]))
     return is_deprecated
+
+
+class DeprecationDict(dict):
+    """A dict which raises a warning when some keys are looked up
+
+    Note, this does not raise a warning for __contains__ and iteration.
+
+    It also will raise a warning even after the key has been manually set by
+    the user.
+    """
+    def __init__(self, *args, **kwargs):
+        self._deprecations = {}
+        super(DeprecationDict, self).__init__(*args, **kwargs)
+
+    def __getitem__(self, key):
+        if key in self._deprecations:
+            warn_args, warn_kwargs = self._deprecations[key]
+            warnings.warn(*warn_args, **warn_kwargs)
+        return super(DeprecationDict, self).__getitem__(key)
+
+    def get(self, key, default=None):
+        # dict does not implement it like this, hence it needs to be overridden
+        try:
+            return self[key]
+        except KeyError:
+            return default
+
+    def add_warning(self, key, *args, **kwargs):
+        """Add a warning to be triggered when the specified key is read"""
+        self._deprecations[key] = (args, kwargs)
diff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py
index d6d4a5e5ee..f2166ac916 100755
--- a/sklearn/utils/estimator_checks.py
+++ b/sklearn/utils/estimator_checks.py
@@ -1035,8 +1035,8 @@ def check_clustering(name, clusterer_orig):
     # with lists
     clusterer.fit(X.tolist())
 
-    assert_equal(clusterer.labels_.shape, (n_samples,))
     pred = clusterer.labels_
+    assert_equal(pred.shape, (n_samples,))
     assert_greater(adjusted_rand_score(pred, y), 0.4)
     # fit another time with ``fit_predict`` and compare results
     if name == 'SpectralClustering':
@@ -1047,6 +1047,25 @@ def check_clustering(name, clusterer_orig):
         pred2 = clusterer.fit_predict(X)
     assert_array_equal(pred, pred2)
 
+    # fit_predict(X) and labels_ should be of type int
+    assert_in(pred.dtype, [np.dtype('int32'), np.dtype('int64')])
+    assert_in(pred2.dtype, [np.dtype('int32'), np.dtype('int64')])
+
+    # There should be at least one sample in every cluster. Equivalently
+    # labels_ should contain all the consecutive values between its
+    # min and its max.
+    pred_sorted = np.unique(pred)
+    assert_array_equal(pred_sorted, np.arange(pred_sorted[0],
+                                              pred_sorted[-1] + 1))
+
+    # labels_ should be greater than -1
+    assert_greater_equal(pred_sorted[0], -1)
+    # labels_ should be less than n_clusters - 1
+    if hasattr(clusterer, 'n_clusters'):
+        n_clusters = getattr(clusterer, 'n_clusters')
+        assert_greater_equal(n_clusters - 1, pred_sorted[-1])
+    # else labels_ should be less than max(labels_) which is necessarily true
+
 
 @ignore_warnings(category=DeprecationWarning)
 def check_clusterer_compute_labels_predict(name, clusterer_orig):
diff --git a/sklearn/utils/tests/test_deprecation.py b/sklearn/utils/tests/test_deprecation.py
index e5a1f021cd..d7b3f48c18 100755
--- a/sklearn/utils/tests/test_deprecation.py
+++ b/sklearn/utils/tests/test_deprecation.py
@@ -8,7 +8,9 @@
 from sklearn.utils.deprecation import _is_deprecated
 from sklearn.utils.deprecation import deprecated
 from sklearn.utils.testing import assert_warns_message
+from sklearn.utils.testing import assert_no_warnings
 from sklearn.utils.testing import SkipTest
+from sklearn.utils.deprecation import DeprecationDict
 
 
 @deprecated('qwerty')
@@ -60,3 +62,17 @@ def test_is_deprecated():
 
 def test_pickle():
     pickle.loads(pickle.dumps(mock_function))
+
+
+def test_deprecationdict():
+    dd = DeprecationDict()
+    dd.add_warning('a', 'hello')
+    dd.add_warning('b', 'world', DeprecationWarning)
+    assert 1 == assert_warns_message(UserWarning, 'hello', dd.get, 'a', 1)
+    dd['a'] = 5
+    dd['b'] = 6
+    dd['c'] = 7
+    assert 5 == assert_warns_message(UserWarning, 'hello', dd.__getitem__, 'a')
+    assert 6 == assert_warns_message(DeprecationWarning, 'world',
+                                     dd.__getitem__, 'b')
+    assert 7 == assert_no_warnings(dd.get, 'c')
