diff --git a/sklearn/ensemble/tests/test_iforest.py b/sklearn/ensemble/tests/test_iforest.py
index 8c445a67f0..3e7f241dd8 100755
--- a/sklearn/ensemble/tests/test_iforest.py
+++ b/sklearn/ensemble/tests/test_iforest.py
@@ -202,7 +202,7 @@ def test_max_samples_consistency():
     assert_equal(clf.max_samples_, clf._max_samples)
 
 
-def test_iforest_subsampled_features:
+def test_iforest_subsampled_features():
     rng = check_random_state(0)
     X_train, X_test, y_train, y_test = train_test_split(boston.data[:50],
                                                         boston.target[:50],
diff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py
index 708715a192..565acba536 100755
--- a/sklearn/linear_model/logistic.py
+++ b/sklearn/linear_model/logistic.py
@@ -1369,10 +1369,13 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,
         l2 penalty with liblinear solver. Prefer dual=False when
         n_samples > n_features.
 
-    scoring : callabale
-        Scoring function to use as cross-validation criteria. For a list of
-        scoring functions that can be used, look at :mod:`sklearn.metrics`.
-        The default scoring option used is accuracy_score.
+    scoring : string, callable, or None
+        A string (see model evaluation documentation) or
+        a scorer callable object / function with signature
+        ``scorer(estimator, X, y)``. For a list of scoring functions
+        that can be used, look at :mod:`sklearn.metrics`. The
+        default scoring option used is accuracy_score.
+
 
     solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag'}
         Algorithm to use in the optimization problem.
