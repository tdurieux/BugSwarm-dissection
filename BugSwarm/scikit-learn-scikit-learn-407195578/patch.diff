diff --git a/doc/Makefile b/doc/Makefile
index 557eeaa188..fcb547d14e 100755
--- a/doc/Makefile
+++ b/doc/Makefile
@@ -98,7 +98,7 @@ doctest:
 	      "results in $(BUILDDIR)/doctest/output.txt."
 
 download-data:
-	python -c "from sklearn.datasets.lfw import check_fetch_lfw; check_fetch_lfw()"
+	python -c "from sklearn.datasets.lfw import _check_fetch_lfw; _check_fetch_lfw()"
 
 # Optimize PNG files. Needs OptiPNG. Change the -P argument to the number of
 # cores you have available, so -P 64 if you have a real computer ;)
diff --git a/sklearn/datasets/base.py b/sklearn/datasets/base.py
index f19ee5e586..9060ad5cbe 100755
--- a/sklearn/datasets/base.py
+++ b/sklearn/datasets/base.py
@@ -211,6 +211,9 @@ def load_data(module_path, data_file_name):
 
     Parameters
     ----------
+    module_path : string
+        The module path.
+
     data_file_name : string
         Name of csv file to be loaded from
         module_path/data/data_file_name. For example 'wine_data.csv'.
diff --git a/sklearn/datasets/lfw.py b/sklearn/datasets/lfw.py
index d68024bd6a..b1cd055b1b 100755
--- a/sklearn/datasets/lfw.py
+++ b/sklearn/datasets/lfw.py
@@ -32,6 +32,7 @@
 import numpy as np
 
 from .base import get_data_home, _fetch_remote, RemoteFileMetadata
+from ..utils import deprecated
 from ..utils import Bunch
 from ..utils import Memory
 from ..utils._joblib import __version__ as joblib_version
@@ -80,20 +81,36 @@
 )
 
 
+@deprecated('This function was deprecated in version 0.20 and will be removed '
+            'in 0.22.')
 def scale_face(face):
-    """Scale back to 0-1 range in case of normalization for plotting"""
+    """Scale back to 0-1 range in case of normalization for plotting.
+
+    .. deprecated:: 0.20
+    This function was deprecated in version 0.20 and will be removed in 0.22.
+
+
+    Parameters
+    ----------
+    face : array_like
+        The array to scale
+
+    Returns
+    -------
+    array_like
+        The scaled array
+    """
     scaled = face - face.min()
     scaled /= scaled.max()
     return scaled
 
-
 #
 # Common private utilities for data fetching from the original LFW website
 # local disk caching, and image decoding.
 #
 
 
-def check_fetch_lfw(data_home=None, funneled=True, download_if_missing=True):
+def _check_fetch_lfw(data_home=None, funneled=True, download_if_missing=True):
     """Helper function to download any missing LFW data"""
 
     data_home = get_data_home(data_home=data_home)
@@ -323,7 +340,7 @@ def fetch_lfw_people(data_home=None, funneled=True, resize=0.5,
         .. versionadded:: 0.20
 
     """
-    lfw_home, data_folder_path = check_fetch_lfw(
+    lfw_home, data_folder_path = _check_fetch_lfw(
         data_home=data_home, funneled=funneled,
         download_if_missing=download_if_missing)
     logger.debug('Loading LFW people faces from %s', lfw_home)
@@ -494,7 +511,7 @@ def fetch_lfw_pairs(subset='train', data_home=None, funneled=True, resize=0.5,
         Description of the Labeled Faces in the Wild (LFW) dataset.
 
     """
-    lfw_home, data_folder_path = check_fetch_lfw(
+    lfw_home, data_folder_path = _check_fetch_lfw(
         data_home=data_home, funneled=funneled,
         download_if_missing=download_if_missing)
     logger.debug('Loading %s LFW pairs from %s', subset, lfw_home)
diff --git a/sklearn/datasets/twenty_newsgroups.py b/sklearn/datasets/twenty_newsgroups.py
index 6eed41f0de..38739e6e60 100755
--- a/sklearn/datasets/twenty_newsgroups.py
+++ b/sklearn/datasets/twenty_newsgroups.py
@@ -52,6 +52,7 @@
 from .base import _fetch_remote
 from .base import RemoteFileMetadata
 from ..utils import check_random_state, Bunch
+from ..utils import deprecated
 from ..feature_extraction.text import CountVectorizer
 from ..preprocessing import normalize
 from ..externals import joblib
@@ -71,7 +72,14 @@
 TEST_FOLDER = "20news-bydate-test"
 
 
+@deprecated("Function 'download_20newsgroups' was renamed to "
+            "'_download_20newsgroups' in version 0.20 and will be removed in "
+            "release 0.22.")
 def download_20newsgroups(target_dir, cache_path):
+    return _download_20newsgroups(target_dir, cache_path)
+
+
+def _download_20newsgroups(target_dir, cache_path):
     """Download the 20 newsgroups data and stored it as a zipped pickle."""
     train_path = os.path.join(target_dir, TRAIN_FOLDER)
     test_path = os.path.join(target_dir, TEST_FOLDER)
@@ -101,6 +109,11 @@ def strip_newsgroup_header(text):
     """
     Given text in "news" format, strip the headers, by removing everything
     before the first blank line.
+
+    Parameters
+    ----------
+    text : string
+        The text from which to remove the signature block.
     """
     _before, _blankline, after = text.partition('\n\n')
     return after
@@ -115,6 +128,11 @@ def strip_newsgroup_quoting(text):
     Given text in "news" format, strip lines beginning with the quote
     characters > or |, plus lines that often introduce a quoted section
     (for example, because they contain the string 'writes:'.)
+
+    Parameters
+    ----------
+    text : string
+        The text from which to remove the signature block.
     """
     good_lines = [line for line in text.split('\n')
                   if not _QUOTE_RE.search(line)]
@@ -128,6 +146,11 @@ def strip_newsgroup_footer(text):
     As a rough heuristic, we assume that signatures are set apart by either
     a blank line or a line made of hyphens, and that it is the last such line
     in the file (disregarding blank lines at the end).
+
+    Parameters
+    ----------
+    text : string
+        The text from which to remove the signature block.
     """
     lines = text.strip().split('\n')
     for line_num in range(len(lines) - 1, -1, -1):
@@ -213,8 +236,8 @@ def fetch_20newsgroups(data_home=None, subset='train', categories=None,
         if download_if_missing:
             logger.info("Downloading 20news dataset. "
                         "This may take a few minutes.")
-            cache = download_20newsgroups(target_dir=twenty_home,
-                                          cache_path=cache_path)
+            cache = _download_20newsgroups(target_dir=twenty_home,
+                                           cache_path=cache_path)
         else:
             raise IOError('20Newsgroups dataset not found')
 
diff --git a/sklearn/model_selection/tests/test_search.py b/sklearn/model_selection/tests/test_search.py
index c13cfc9849..9c5e2ec279 100755
--- a/sklearn/model_selection/tests/test_search.py
+++ b/sklearn/model_selection/tests/test_search.py
@@ -1641,7 +1641,8 @@ def check_results(results, gscv):
                                         err_msg='Checking ' + k)
 
     def fit_grid(param_grid):
-        return GridSearchCV(clf, param_grid, cv=5).fit(X, y)
+        return GridSearchCV(clf, param_grid, cv=5,
+                            return_train_score=True).fit(X, y)
 
     class CustomSearchCV(BaseSearchCV):
         def __init__(self, estimator, **kwargs):
@@ -1659,7 +1660,7 @@ def _run_search(self, evaluate):
     clf = DecisionTreeRegressor(random_state=0)
     X, y = make_classification(n_samples=100, n_informative=4,
                                random_state=0)
-    mycv = CustomSearchCV(clf, cv=5).fit(X, y)
+    mycv = CustomSearchCV(clf, cv=5, return_train_score=True).fit(X, y)
     gscv = fit_grid([{'max_depth': [1, 2]},
                      {'min_samples_split': [5, 10]}])
 
diff --git a/sklearn/svm/classes.py b/sklearn/svm/classes.py
index c595e8a12e..4bb423e790 100755
--- a/sklearn/svm/classes.py
+++ b/sklearn/svm/classes.py
@@ -543,7 +543,7 @@ class SVC(BaseSVC):
         non-trivial. See the section about multi-class classification in the
         SVM section of the User Guide for details.
 
-    coef_ : array, shape = [n_class-1, n_features]
+    coef_ : array, shape = [n_class * (n_class-1) / 2, n_features]
         Weights assigned to the features (coefficients in the primal
         problem). This is only available in the case of a linear kernel.
 
@@ -707,7 +707,7 @@ class NuSVC(BaseSVC):
         non-trivial. See the section about multi-class classification in
         the SVM section of the User Guide for details.
 
-    coef_ : array, shape = [n_class-1, n_features]
+    coef_ : array, shape = [n_class * (n_class-1) / 2, n_features]
         Weights assigned to the features (coefficients in the primal
         problem). This is only available in the case of a linear kernel.
 
diff --git a/sklearn/tests/test_docstring_parameters.py b/sklearn/tests/test_docstring_parameters.py
index 704002b26b..648de6b6e6 100755
--- a/sklearn/tests/test_docstring_parameters.py
+++ b/sklearn/tests/test_docstring_parameters.py
@@ -23,20 +23,6 @@
                                                         path=sklearn.__path__)
                       if not ("._" in pckg[1] or ".tests." in pckg[1])])
 
-# TODO Uncomment all modules and fix doc inconsistencies everywhere
-# The list of modules that are not tested for now
-IGNORED_MODULES = (
-    'cluster',
-    'datasets',
-    'model_selection',
-    'multioutput',
-    'setup',
-    # Deprecated modules
-    'cross_validation',
-    'grid_search',
-    'learning_curve',
-)
-
 
 # functions to ignore args / docstring of
 _DOCSTRING_IGNORES = [
@@ -75,8 +61,6 @@ def test_docstring_parameters():
 
     incorrect = []
     for name in PUBLIC_MODULES:
-        if name.startswith('_') or name.split(".")[1] in IGNORED_MODULES:
-            continue
         with warnings.catch_warnings(record=True):
             module = importlib.import_module(name)
         classes = inspect.getmembers(module, inspect.isclass)
diff --git a/sklearn/utils/testing.py b/sklearn/utils/testing.py
index 0a0b13dc2b..8dc1188cfe 100755
--- a/sklearn/utils/testing.py
+++ b/sklearn/utils/testing.py
@@ -940,6 +940,9 @@ def check_docstring_parameters(func, doc=None, ignore=None, class_name=None):
     # Don't check docstring for property-functions
     if inspect.isdatadescriptor(func):
         return incorrect
+    # Don't check docstring for setup / teardown pytest functions
+    if func_name.split('.')[-1] in ('setup_module', 'teardown_module'):
+        return incorrect
     # Dont check estimator_checks module
     if func_name.split('.')[2] == 'estimator_checks':
         return incorrect
