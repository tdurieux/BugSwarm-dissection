diff --git a/doc/modules/classes.rst b/doc/modules/classes.rst
index b4e5a2f3ac..d1d8eec4dd 100755
--- a/doc/modules/classes.rst
+++ b/doc/modules/classes.rst
@@ -954,8 +954,8 @@ See the :ref:`metrics` section of the user guide for further details.
    :template: class.rst
 
    mixture.GaussianMixture
+   mixture.BayesianGaussianMixture
    mixture.DPGMM
-   mixture.VBGMM
 
 
 .. _multiclass_ref:
diff --git a/doc/modules/mixture.rst b/doc/modules/mixture.rst
index 9979d36c93..5e3c2c448d 100755
--- a/doc/modules/mixture.rst
+++ b/doc/modules/mixture.rst
@@ -133,40 +133,13 @@ parameters to maximize the likelihood of the data given those
 assignments. Repeating this process is guaranteed to always converge
 to a local optimum.
 
-.. _vbgmm:
+.. _bgmm:
 
-VBGMM: variational Gaussian mixtures
-====================================
+Bayesian Gaussian Mixture
+=========================
 
-The :class:`VBGMM` object implements a variant of the Gaussian mixture
-model with :ref:`variational inference <variational_inference>` algorithms.
-
-Pros and cons of class :class:`VBGMM`: variational inference
-------------------------------------------------------------
-
-Pros
-.....
-
-:Regularization: due to the incorporation of prior information,
-   variational solutions have less pathological special cases than
-   expectation-maximization solutions. One can then use full
-   covariance matrices in high dimensions or in cases where some
-   components might be centered around a single point without
-   risking divergence.
-
-Cons
-.....
-
-:Bias: to regularize a model one has to add biases. The
-   variational algorithm will bias all the means towards the origin
-   (part of the prior information adds a "ghost point" in the origin
-   to every mixture component) and it will bias the covariances to
-   be more spherical. It will also, depending on the concentration
-   parameter, bias the cluster structure either towards uniformity
-   or towards a rich-get-richer scenario.
-
-:Hyperparameters: this algorithm needs an extra hyperparameter
-   that might need experimental tuning via cross-validation.
+The :class:`BayesianGaussianMixture` object implements a variant of the Gaussian
+mixture model with variational inference algorithms.
 
 .. _variational_inference:
 
@@ -175,7 +148,7 @@ Estimation algorithm: variational inference
 
 Variational inference is an extension of expectation-maximization that
 maximizes a lower bound on model evidence (including
-priors) instead of data likelihood.  The principle behind
+priors) instead of data likelihood. The principle behind
 variational methods is the same as expectation-maximization (that is
 both are iterative algorithms that alternate between finding the
 probabilities for each point to be generated by each mixture and
@@ -188,13 +161,54 @@ much so as to render usage unpractical.
 
 Due to its Bayesian nature, the variational algorithm needs more
 hyper-parameters than expectation-maximization, the most
-important of these being the concentration parameter ``alpha``. Specifying
-a high value of alpha leads more often to uniformly-sized mixture
+important of these being the concentration parameter ``dirichlet_concentration_prior``. Specifying
+a high value of prior of the dirichlet concentration leads more often to uniformly-sized mixture
 components, while specifying small (between 0 and 1) values will lead
 to some mixture components getting almost all the points while most
 mixture components will be centered on just a few of the remaining
 points.
 
+.. figure:: ../auto_examples/mixture/images/sphx_glr_plot_bayesian_gaussian_mixture_001.png
+   :target: ../auto_examples/mixture/plot_bayesian_gaussian_mixture.html
+   :align: center
+   :scale: 50%
+
+.. topic:: Examples:
+
+    * See :ref:`plot_bayesian_gaussian_mixture.py` for a comparaison of
+      the results of the ``BayesianGaussianMixture`` for different values
+      of the parameter ``dirichlet_concentration_prior``.
+
+Pros and cons of variational inference with :class:BayesianGaussianMixture
+--------------------------------------------------------------------------
+
+Pros
+.....
+
+:Regularization: due to the incorporation of prior information,
+   variational solutions have less pathological special cases than
+   expectation-maximization solutions.
+
+:Automatic selection: when `dirichlet_concentration_prior` is small enough and
+`n_components` is larger than what is found necessary by the model, the
+Variational Bayesian mixture model has a natural tendency to set some mixture
+weights values close to zero. This makes it possible to let the model choose a
+suitable number of effective components automatically.
+
+Cons
+.....
+
+:Bias: to regularize a model one has to add biases. The
+   variational algorithm will bias all the means towards the origin
+   (part of the prior information adds a "ghost point" in the origin
+   to every mixture component) and it will bias the covariances to
+   be more spherical. It will also, depending on the concentration
+   parameter, bias the cluster structure either towards uniformity
+   or towards a rich-get-richer scenario.
+
+:Hyperparameters: this algorithm needs an extra hyperparameter
+   that might need experimental tuning via cross-validation.
+
 .. _dpgmm:
 
 DPGMM: Infinite Gaussian mixtures
diff --git a/doc/whats_new.rst b/doc/whats_new.rst
index 8c4aab882d..8471a8bec1 100755
--- a/doc/whats_new.rst
+++ b/doc/whats_new.rst
@@ -64,13 +64,13 @@ Model Selection Enhancements and API Changes
 
   - **Parameters ``n_folds`` and ``n_iter`` renamed to ``n_splits``**
 
-    Some parameter names have changed: 
-    The ``n_folds`` parameter in :class:`model_selection.KFold`, 
-    :class:`model_selection.LabelKFold`, and 
+    Some parameter names have changed:
+    The ``n_folds`` parameter in :class:`model_selection.KFold`,
+    :class:`model_selection.LabelKFold`, and
     :class:`model_selection.StratifiedKFold` is now renamed to ``n_splits``.
     The ``n_iter`` parameter in :class:`model_selection.ShuffleSplit`,
-    :class:`model_selection.LabelShuffleSplit`, 
-    and :class:`model_selection.StratifiedShuffleSplit` is now renamed 
+    :class:`model_selection.LabelShuffleSplit`,
+    and :class:`model_selection.StratifiedShuffleSplit` is now renamed
     to ``n_splits``.
 
 
@@ -141,8 +141,8 @@ New features
      <https://github.com/scikit-learn/scikit-learn/pull/6954>`_) by `Nelson
      Liu`_
 
-   - Added new cross-validation splitter 
-     :class:`model_selection.TimeSeriesSplit` to handle time series data. 
+   - Added new cross-validation splitter
+     :class:`model_selection.TimeSeriesSplit` to handle time series data.
      (`#6586
      <https://github.com/scikit-learn/scikit-learn/pull/6586>`_) by `YenChen
      Lin`_
@@ -402,10 +402,19 @@ API changes summary
    - Access to public attributes ``.X_`` and ``.y_`` has been deprecated in
      :class:`isotonic.IsotonicRegression`. By `Jonathan Arfa`_.
 
+   - The old :class:`VBGMM` is deprecated in favor of the new
+     :class:`BayesianGaussianMixture`. The new class solves the computational
+     problems of the old class and computes the Variational Bayesian Gaussian
+     mixture faster than before.
+     Ref :ref:`b` for more information.
+     (`#6651 <https://github.com/scikit-learn/scikit-learn/pull/6651>`_) by
+     `Wei Xue`_ and `Thierry Guillemot`_.
+
    - The old :class:`GMM` is deprecated in favor of the new
      :class:`GaussianMixture`. The new class computes the Gaussian mixture
      faster than before and some of computational problems have been solved.
-     By `Wei Xue`_ and `Thierry Guillemot`_.
+     (`#6666 <https://github.com/scikit-learn/scikit-learn/pull/6666>`_) by
+     `Wei Xue`_ and `Thierry Guillemot`_.
 
    - The ``grid_scores_`` attribute of :class:`model_selection.GridSearchCV`
      and :class:`model_selection.RandomizedSearchCV` is deprecated in favor of
@@ -415,7 +424,7 @@ API changes summary
      `Raghav R V`_.
 
    - The parameters ``n_iter`` or ``n_folds`` in old CV splitters are replaced
-     by the new parameter ``n_splits`` since it can provide a consistent 
+     by the new parameter ``n_splits`` since it can provide a consistent
      and unambiguous interface to represent the number of train-test splits.
      (`#7187 <https://github.com/scikit-learn/scikit-learn/pull/7187>`_)
      by `YenChen Lin`_.
diff --git a/examples/mixture/plot_bayesian_gaussian_mixture.py b/examples/mixture/plot_bayesian_gaussian_mixture.py
new file mode 100755
index 0000000000..9efea3f04c
--- /dev/null
+++ b/examples/mixture/plot_bayesian_gaussian_mixture.py
@@ -0,0 +1,114 @@
+"""
+======================================================
+Bayesian Gaussian Mixture Concentration Prior Analysis
+======================================================
+
+Plot the resulting ellipsoids of a mixture of three Gaussians with
+variational Bayesian Gaussian Mixture for three different values on the
+prior the dirichlet concentration.
+
+For all models, the Variationnal Bayesian Gaussian Mixture adapts its number of
+mixture automatically. The parameter `dirichlet_concentration_prior` has a
+direct link with the resulting number of components. Specifying a high value of
+`dirichlet_concentration_prior` leads more often to uniformly-sized mixture
+components, while specifying small (under 0.1) values will lead to some mixture
+components getting almost all the points while most mixture components will be
+centered on just a few of the remaining points.
+"""
+# Author: Thierry Guillemot <thierry.guillemot.work@gmail.com>
+# License: BSD 3 clause
+
+import numpy as np
+import matplotlib as mpl
+import matplotlib.pyplot as plt
+import matplotlib.gridspec as gridspec
+
+from sklearn.mixture import BayesianGaussianMixture
+
+print(__doc__)
+
+
+def plot_ellipses(ax, weights, means, covars):
+    for n in range(means.shape[0]):
+        v, w = np.linalg.eigh(covars[n][:2, :2])
+        u = w[0] / np.linalg.norm(w[0])
+        angle = np.arctan2(u[1], u[0])
+        angle = 180 * angle / np.pi  # convert to degrees
+        v = 2 * np.sqrt(2) * np.sqrt(v)
+        ell = mpl.patches.Ellipse(means[n, :2], v[0], v[1], 180 + angle)
+        ell.set_clip_box(ax.bbox)
+        ell.set_alpha(weights[n])
+        ax.add_artist(ell)
+
+
+def plot_results(ax1, ax2, estimator, dirichlet_concentration_prior, X, y, plot_title=False):
+    estimator.dirichlet_concentration_prior = dirichlet_concentration_prior
+    estimator.fit(X)
+    ax1.set_title("Bayesian Gaussian Mixture for "
+                  r"$dc_0=%.1e$" % dirichlet_concentration_prior)
+    # ax1.axis('equal')
+    ax1.scatter(X[:, 0], X[:, 1], s=5, marker='o', color=colors[y], alpha=0.8)
+    ax1.set_xlim(-2., 2.)
+    ax1.set_ylim(-3., 3.)
+    ax1.set_xticks(())
+    ax1.set_yticks(())
+    plot_ellipses(ax1, estimator.weights_, estimator.means_,
+                  estimator.covariances_)
+
+    ax2.get_xaxis().set_tick_params(direction='out')
+    ax2.yaxis.grid(True, alpha=0.7)
+    for k, w in enumerate(estimator.weights_):
+        ax2.bar(k - .45, w, width=0.9, color='royalblue', zorder=3)
+        ax2.text(k, w + 0.007, "%.1f%%" % (w * 100.),
+                 horizontalalignment='center')
+    ax2.set_xlim(-.6, 2 * n_components - .4)
+    ax2.set_ylim(0., 1.1)
+    ax2.tick_params(axis='y', which='both', left='off',
+                    right='off', labelleft='off')
+    ax2.tick_params(axis='x', which='both', top='off')
+
+    if plot_title:
+        ax1.set_ylabel('Estimated Mixtures')
+        ax2.set_ylabel('Weight of each component')
+
+# Parameters
+random_state = 2
+n_components, n_features = 3, 2
+colors = np.array(['mediumseagreen', 'royalblue', 'r', 'gold',
+                   'orchid', 'indigo', 'darkcyan', 'tomato'])
+dirichlet_concentration_prior = np.logspace(-3, 3, 3)
+covars = np.array([[[.7, .0], [.0, .1]],
+                   [[.5, .0], [.0, .1]],
+                   [[.5, .0], [.0, .1]]])
+samples = np.array([200, 500, 200])
+means = np.array([[.0, -.70],
+                  [.0, .0],
+                  [.0, .70]])
+
+
+# Here we put beta_prior to 0.8 to minimize the influence of the prior for this
+# dataset
+estimator = BayesianGaussianMixture(n_components=2 * n_components,
+                                    init_params='random', max_iter=1500,
+                                    mean_precision_prior=.8, tol=1e-9,
+                                    random_state=random_state)
+
+# Generate data
+rng = np.random.RandomState(random_state)
+X = np.vstack([
+    rng.multivariate_normal(means[j], covars[j], samples[j])
+    for j in range(n_components)])
+y = np.concatenate([j * np.ones(samples[j], dtype=int)
+                    for j in range(n_components)])
+
+# Plot Results
+plt.figure(figsize=(4.7 * 3, 8))
+plt.subplots_adjust(bottom=.04, top=0.95, hspace=.05, wspace=.05,
+                    left=.03, right=.97)
+
+gs = gridspec.GridSpec(3, len(dirichlet_concentration_prior))
+for k, dc in enumerate(dirichlet_concentration_prior):
+    plot_results(plt.subplot(gs[0:2, k]), plt.subplot(gs[2, k]),
+                 estimator, dc, X, y, plot_title=k == 0)
+
+plt.show()
diff --git a/sklearn/metrics/tests/test_classification.py b/sklearn/metrics/tests/test_classification.py
index 2edeb65e5b..576c3a362b 100755
--- a/sklearn/metrics/tests/test_classification.py
+++ b/sklearn/metrics/tests/test_classification.py
@@ -1394,8 +1394,8 @@ def test_log_loss():
     assert_raise_message(ValueError, error_str, log_loss, y_true, y_pred)
 
     y_pred = [[0.2, 0.7], [0.6, 0.5], [0.2, 0.3]]
-    error_str = ('Found inconsistent number of samples in input variables: '
-                 ' [3, 2]')
+    error_str = ('Found input variables with inconsistent numbers of samples: '
+                 '[3, 2]')
     assert_raise_message(ValueError, error_str, log_loss, y_true, y_pred)
 
     # works when the labels argument is used
diff --git a/sklearn/mixture/__init__.py b/sklearn/mixture/__init__.py
index 8269ec4a31..3622518352 100755
--- a/sklearn/mixture/__init__.py
+++ b/sklearn/mixture/__init__.py
@@ -8,6 +8,7 @@
 from .dpgmm import DPGMM, VBGMM
 
 from .gaussian_mixture import GaussianMixture
+from .bayesian_mixture import BayesianGaussianMixture
 
 
 __all__ = ['DPGMM',
@@ -17,4 +18,5 @@
            'distribute_covar_matrix_to_match_covariance_type',
            'log_multivariate_normal_density',
            'sample_gaussian',
-           'GaussianMixture']
+           'GaussianMixture',
+           'BayesianGaussianMixture']
diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py
index 282fe51fef..062d403228 100755
--- a/sklearn/mixture/base.py
+++ b/sklearn/mixture/base.py
@@ -237,7 +237,6 @@ def fit(self, X, y=None):
 
         return self
 
-    @abstractmethod
     def _e_step(self, X):
         """E step.
 
@@ -248,12 +247,14 @@ def _e_step(self, X):
         Returns
         -------
         log_prob_norm : array, shape (n_samples,)
-            log p(X)
+            Logarithm of the probability of each sample in X.
 
         log_responsibility : array, shape (n_samples, n_components)
-            logarithm of the responsibilities
+            Logarithm of the posterior probabilities (or responsibilities) of
+            the point of each sample in X.
         """
-        pass
+        log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
+        return np.mean(log_prob_norm), log_resp
 
     @abstractmethod
     def _m_step(self, X, log_resp):
@@ -264,6 +265,8 @@ def _m_step(self, X, log_resp):
         X : array-like, shape (n_samples, n_features)
 
         log_resp : array-like, shape (n_samples, n_components)
+            Logarithm of the posterior probabilities (or responsibilities) of
+            the point of each sample in X.
         """
         pass
 
diff --git a/sklearn/mixture/bayesian_mixture.py b/sklearn/mixture/bayesian_mixture.py
new file mode 100755
index 0000000000..2b5d4458c8
--- /dev/null
+++ b/sklearn/mixture/bayesian_mixture.py
@@ -0,0 +1,688 @@
+"""Bayesian Gaussian Mixture Model."""
+# Author: Wei Xue <xuewei4d@gmail.com>
+#         Thierry Guillemot <thierry.guillemot.work@gmail.com>
+# License: BSD 3 clause
+
+import math
+import numpy as np
+from scipy.special import digamma, gammaln
+
+from .base import BaseMixture, _check_shape
+from .gaussian_mixture import _check_precision_matrix
+from .gaussian_mixture import _check_precision_positivity
+from .gaussian_mixture import _compute_log_det_cholesky
+from .gaussian_mixture import _compute_precision_cholesky
+from .gaussian_mixture import _estimate_gaussian_parameters
+from .gaussian_mixture import _estimate_log_gaussian_prob
+from ..utils import check_array
+from ..utils.validation import check_is_fitted
+
+
+def _log_dirichlet_norm(dirichlet_concentration):
+    """Compute the log of the Dirichlet distribution normalization term.
+
+    Parameters
+    ----------
+    dirichlet_concentration : array-like, shape (n_samples,)
+        The parameters values of the Dirichlet distribution.
+
+    Returns
+    -------
+    log_dirichlet_norm : float
+        The log normalization of the Dirichlet distribution.
+    """
+    return (gammaln(np.sum(dirichlet_concentration)) -
+            np.sum(gammaln(dirichlet_concentration)))
+
+
+def _log_wishart_norm(degrees_of_freedom, log_det_precisions_chol, n_features):
+    """Compute the log of the Wishart distribution normalization term.
+
+    Parameters
+    ----------
+    degrees_of_freedom : array-like, shape (n_components,)
+        The number of degrees of freedom on the covariance Wishart
+        distributions.
+
+    log_det_precision_chol : array-like, shape (n_components,)
+         The determinant of the precision matrix for each component.
+
+    n_features : int
+        The number of features.
+
+    Return
+    ------
+    log_wishart_norm : array-like, shape (n_components,)
+        The log normalization of the Wishart distribution.
+    """
+    # To simplify the computation we have removed the np.log(np.pi) term
+    return -(degrees_of_freedom * log_det_precisions_chol +
+             degrees_of_freedom * n_features * .5 * math.log(2.) +
+             np.sum(gammaln(.5 * (degrees_of_freedom -
+                                  np.arange(n_features)[:, np.newaxis])), 0))
+
+
+class BayesianGaussianMixture(BaseMixture):
+    """Variational estimation of a Gaussian mixture.
+
+    This class allows to infer an approximate posterior distribution over the
+    parameters of a Gaussian mixture distribution. The effective number of
+    components can be inferred from the data.
+
+    Read more in the :ref:`User Guide <bgmm>`.
+
+    Parameters
+    ----------
+    n_components : int, defaults to 1.
+        The number of mixture components. Depending on the data and the value
+        of the `dirichlet_concentration_prior` the model can decide to not use
+        all the components by setting some component `weights_` to values very
+        close to zero. The number of effective components is therefore smaller
+        than n_components.
+
+    covariance_type : {'full', 'tied', 'diag', 'spherical'}, defaults to 'full'.
+        String describing the type of covariance parameters to use.
+        Must be one of::
+        'full' (each component has its own general covariance matrix).
+        'tied' (all components share the same general covariance matrix),
+        'diag' (each component has its own diagonal covariance matrix),
+        'spherical' (each component has its own single variance),
+
+    tol : float, defaults to 1e-3.
+        The convergence threshold. EM iterations will stop when the
+        lower bound average gain on the likelihood (of the training data with
+        respect to the model) is below this threshold.
+
+    reg_covar : float, defaults to 1e-6.
+        Non-negative regularization added to the diagonal of covariance.
+        Allows to assure that the covariance matrices are all positive.
+
+    max_iter : int, defaults to 100.
+        The number of EM iterations to perform.
+
+    n_init : int, defaults to 1.
+        The number of initializations to perform. The result with the highest
+        lower bound value on the likelihood is kept.
+
+    init_params : {'kmeans', 'random'}, defaults to 'kmeans'.
+        The method used to initialize the weights, the means and the
+        covariances.
+        Must be one of::
+        'kmeans' : responsibilities are initialized using kmeans.
+        'random' : responsibilities are initialized randomly.
+
+    dirichlet_concentration_prior : float | None, optional.
+        The dirichlet concentration of each component on the weight
+        distribution (Dirichlet). The higher concentration puts more mass in
+        the center and will lead to more components being active, while a lower
+        concentration parameter will lead to more mass at the edge of the
+        mixture weights simplex. The value of the parameter must be greater
+        than 0. If it is None, it's set to `1. / n_components`.
+
+    mean_precision_prior : float | None, optional.
+        The precision prior on the mean distribution (Gaussian).
+        Controls the extend to where means can be placed. Smaller
+        values concentrates the means of each clusters around `mean_prior`.
+        The value of the parameter must be greater than 0.
+        If it is None, it's set to 1.
+
+    mean_prior : array-like, shape (`n_features`,), optional
+        The prior on the mean distribution (Gaussian).
+        If it is None, it's set to the mean of X.
+
+    degrees_of_freedom_prior : float | None, optional.
+        The prior of the number of degrees of freedom on the covariance
+        distributions (Wishart). If it is None, it's set to `n_features`.
+
+    covariance_prior : float or array-like, optional
+        The prior on the covariance distribution (Wishart).
+        If it is None, the emiprical covariance prior is initialized using the
+        covariance of X. The shape depends on `covariance_type`::
+            (`n_features`, `n_features`) if 'full',
+            (`n_features`, `n_features`) if 'tied',
+            (`n_features`)               if 'diag',
+            float                        if 'spherical'
+
+    random_state: RandomState or an int seed, defaults to None.
+        A random number generator instance.
+
+    warm_start : bool, default to False.
+        If 'warm_start' is True, the solution of the last fitting is used as
+        initialization for the next call of fit(). This can speed up
+        convergence when fit is called several time on similar problems.
+
+    verbose : int, default to 0.
+        Enable verbose output. If 1 then it prints the current
+        initialization and each iteration step. If greater than 1 then
+        it prints also the log probability and the time needed
+        for each step.
+
+    Attributes
+    ----------
+    weights_ : array-like, shape (`n_components`,)
+        The weights of each mixture components.
+
+    means_ : array-like, shape (`n_components`, `n_features`)
+        The mean of each mixture component.
+
+    covariances_ : array-like
+        The covariance of each mixture component.
+        The shape depends on `covariance_type`::
+            (n_components,)                        if 'spherical',
+            (n_features, n_features)               if 'tied',
+            (n_components, n_features)             if 'diag',
+            (n_components, n_features, n_features) if 'full'
+
+    precisions_ : array-like
+        The precision matrices for each component in the mixture. A precision
+        matrix is the inverse of a covariance matrix. A covariance matrix is
+        symmetric positive definite so the mixture of Gaussian can be
+        equivalently parameterized by the precision matrices. Storing the
+        precision matrices instead of the covariance matrices makes it more
+        efficient to compute the log-likelihood of new samples at test time.
+        The shape depends on `covariance_type`::
+            (n_components,)                        if 'spherical',
+            (n_features, n_features)               if 'tied',
+            (n_components, n_features)             if 'diag',
+            (n_components, n_features, n_features) if 'full'
+
+    precisions_cholesky_ : array-like
+        The cholesky decomposition of the precision matrices of each mixture
+        component. A precision matrix is the inverse of a covariance matrix.
+        A covariance matrix is symmetric positive definite so the mixture of
+        Gaussian can be equivalently parameterized by the precision matrices.
+        Storing the precision matrices instead of the covariance matrices makes
+        it more efficient to compute the log-likelihood of new samples at test
+        time. The shape depends on `covariance_type`::
+            (n_components,)                        if 'spherical',
+            (n_features, n_features)               if 'tied',
+            (n_components, n_features)             if 'diag',
+            (n_components, n_features, n_features) if 'full'
+
+    converged_ : bool
+        True when convergence was reached in fit(), False otherwise.
+
+    n_iter_ : int
+        Number of step used by the best fit of inference to reach the
+        convergence.
+
+    lower_bound_ : float
+        Lower bound value on the likelihood (of the training data with
+        respect to the model) of the best fit of inference.
+
+    dirichlet_concentration_prior_ : float
+        The dirichlet concentration of each component on the weight
+        distribution (Dirichlet). The higher concentration puts more mass in
+        the center and will lead to more components being active, while a lower
+        concentration parameter will lead to more mass at the edge of the
+        simplex.
+
+    dirichlet_concentration_ : array-like, shape (`n_components`, )
+        The dirichlet concentration of each component on the weight
+        distribution (Dirichlet).
+
+    mean_precision_prior : float
+        The precision prior on the mean distribution (Gaussian).
+        Controls the extend to where means can be placed.
+        Smaller values concentrates the means of each clusters around
+        `mean_prior`.
+
+    mean_precision_ : array-like, shape (`n_components`, )
+        The precision of each components on the mean distribution (Gaussian).
+
+    means_prior_ : array-like, shape (`n_features`,)
+        The prior on the mean distribution (Gaussian).
+
+    degrees_of_freedom_prior_ : float
+        The prior of the number of degrees of freedom on the covariance
+        distributions (Wishart).
+
+    degrees_of_freedom_ : array-like, shape (`n_components`,)
+        The number of degrees of freedom of each components in the model.
+
+    covariance_prior_ : float or array-like
+        The prior on the covariance distribution (Wishart).
+        The shape depends on `covariance_type`::
+            (`n_features`, `n_features`) if 'full',
+            (`n_features`, `n_features`) if 'tied',
+            (`n_features`)               if 'diag',
+            float                        if 'spherical'
+
+    See Also
+    --------
+    GaussianMixture : Finite Gaussian mixture fit with EM.
+    """
+
+    def __init__(self, n_components=1, covariance_type='full', tol=1e-3,
+                 reg_covar=1e-6, max_iter=100, n_init=1, init_params='kmeans',
+                 dirichlet_concentration_prior=None,
+                 mean_precision_prior=None, mean_prior=None,
+                 degrees_of_freedom_prior=None, covariance_prior=None,
+                 random_state=None, warm_start=False, verbose=0,
+                 verbose_interval=20):
+        super(BayesianGaussianMixture, self).__init__(
+            n_components=n_components, tol=tol, reg_covar=reg_covar,
+            max_iter=max_iter, n_init=n_init, init_params=init_params,
+            random_state=random_state, warm_start=warm_start,
+            verbose=verbose, verbose_interval=verbose_interval)
+
+        self.covariance_type = covariance_type
+        self.dirichlet_concentration_prior = dirichlet_concentration_prior
+        self.mean_precision_prior = mean_precision_prior
+        self.mean_prior = mean_prior
+        self.degrees_of_freedom_prior = degrees_of_freedom_prior
+        self.covariance_prior = covariance_prior
+
+    def _check_parameters(self, X):
+        """Check that the parameters are well defined.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+        """
+        if self.covariance_type not in ['spherical', 'tied', 'diag', 'full']:
+            raise ValueError("Invalid value for 'covariance_type': %s "
+                             "'covariance_type' should be in "
+                             "['spherical', 'tied', 'diag', 'full']"
+                             % self.covariance_type)
+        self._check_weights_parameters()
+        self._check_means_parameters(X)
+        self._check_precision_parameters(X)
+        self._checkcovariance_prior_parameter(X)
+
+    def _check_weights_parameters(self):
+        """Check the parameter of the Dirichlet distribution."""
+        if self.dirichlet_concentration_prior is None:
+            self.dirichlet_concentration_prior_ = 1. / self.n_components
+        elif self.dirichlet_concentration_prior > 0.:
+            self.dirichlet_concentration_prior_ = (
+                self.dirichlet_concentration_prior)
+        else:
+            raise ValueError("The parameter 'dirichlet_concentration_prior' "
+                             "should be greater than 0., but got %.3f."
+                             % self.dirichlet_concentration_prior)
+
+    def _check_means_parameters(self, X):
+        """Check the parameters of the Gaussian distribution.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+        """
+        _, n_features = X.shape
+
+        if self.mean_precision_prior is None:
+            self.mean_precision_prior_ = 1.
+        elif self.mean_precision_prior > 0.:
+            self.mean_precision_prior_ = self.mean_precision_prior
+        else:
+            raise ValueError("The parameter 'mean_precision_prior' should be "
+                             "greater than 0., but got %.3f."
+                             % self.mean_precision_prior)
+
+        if self.mean_prior is None:
+            self.mean_prior_ = X.mean(axis=0)
+        else:
+            self.mean_prior_ = check_array(self.mean_prior,
+                                           dtype=[np.float64, np.float32],
+                                           ensure_2d=False)
+            _check_shape(self.mean_prior_, (n_features, ), 'means')
+
+    def _check_precision_parameters(self, X):
+        """Check the prior parameters of the precision distribution.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+        """
+        _, n_features = X.shape
+
+        if self.degrees_of_freedom_prior is None:
+            self.degrees_of_freedom_prior_ = n_features
+        elif self.degrees_of_freedom_prior > n_features - 1.:
+            self.degrees_of_freedom_prior_ = self.degrees_of_freedom_prior
+        else:
+            raise ValueError("The parameter 'degrees_of_freedom_prior' "
+                             "should be greater than %d, but got %.3f."
+                             % (n_features - 1, self.degrees_of_freedom_prior))
+
+    def _checkcovariance_prior_parameter(self, X):
+        """Check the `covariance_prior_`.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+        """
+        _, n_features = X.shape
+
+        if self.covariance_prior is None:
+            self.covariance_prior_ = {
+                'full': np.atleast_2d(np.cov(X.T)),
+                'tied': np.atleast_2d(np.cov(X.T)),
+                'diag': np.var(X, axis=0, ddof=1),
+                'spherical': np.var(X, axis=0, ddof=1).mean()
+            }[self.covariance_type]
+
+        elif self.covariance_type in ['full', 'tied']:
+            self.covariance_prior_ = check_array(
+                self.covariance_prior, dtype=[np.float64, np.float32],
+                ensure_2d=False)
+            _check_shape(self.covariance_prior_, (n_features, n_features),
+                         '%s covariance_prior' % self.covariance_type)
+            _check_precision_matrix(self.covariance_prior_,
+                                    self.covariance_type)
+        elif self.covariance_type == 'diag':
+            self.covariance_prior_ = check_array(
+                self.covariance_prior, dtype=[np.float64, np.float32],
+                ensure_2d=False)
+            _check_shape(self.covariance_prior_, (n_features,),
+                         '%s covariance_prior' % self.covariance_type)
+            _check_precision_positivity(self.covariance_prior_,
+                                        self.covariance_type)
+        # spherical case
+        elif self.covariance_prior > 0.:
+            self.covariance_prior_ = self.covariance_prior
+        else:
+            raise ValueError("The parameter 'spherical covariance_prior' "
+                             "should be greater than 0., but got %.3f."
+                             % self.covariance_prior)
+
+    def _initialize(self, X, resp):
+        """Initialization of the mixture parameters.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+
+        resp : array-like, shape (n_samples, n_components)
+        """
+        nk, xk, sk = _estimate_gaussian_parameters(X, resp, self.reg_covar,
+                                                   self.covariance_type)
+
+        self._estimate_weights(nk)
+        self._estimate_means(nk, xk)
+        self._estimate_precisions(nk, xk, sk)
+
+    def _estimate_weights(self, nk):
+        """Estimate the parameters of the Dirichlet distribution.
+
+        Parameters
+        ----------
+        nk : array-like, shape (n_components,)
+        """
+        self.dirichlet_concentration_ = (
+            self.dirichlet_concentration_prior_ + nk)
+
+    def _estimate_means(self, nk, xk):
+        """Estimate the parameters of the Gaussian distribution.
+
+        Parameters
+        ----------
+        nk : array-like, shape (n_components,)
+
+        xk : array-like, shape (n_components, n_features)
+        """
+        self.mean_precision_ = self.mean_precision_prior_ + nk
+        self.means_ = ((self.mean_precision_prior_ * self.mean_prior_ +
+                        nk[:, np.newaxis] * xk) /
+                       self.mean_precision_[:, np.newaxis])
+
+    def _estimate_precisions(self, nk, xk, sk):
+        """Estimate the precisions parameters of the precision distribution.
+
+        Parameters
+        ----------
+        nk : array-like, shape (n_components,)
+
+        xk : array-like, shape (n_components, n_features)
+
+        sk : array-like
+            The shape depends of `covariance_type`:
+            'full' : (n_components, n_features, n_features)
+            'tied' : (n_features, n_features)
+            'diag' : (n_components, n_features)
+            'spherical' : (n_components,)
+        """
+        {"full": self._estimate_wishart_full,
+         "tied": self._estimate_wishart_tied,
+         "diag": self._estimate_wishart_diag,
+         "spherical": self._estimate_wishart_spherical
+         }[self.covariance_type](nk, xk, sk)
+
+        self.precisions_cholesky_ = _compute_precision_cholesky(
+            self.covariances_, self.covariance_type)
+
+    def _estimate_wishart_full(self, nk, xk, sk):
+        """Estimate the full Wishart distribution parameters.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+
+        nk : array-like, shape (n_components,)
+
+        xk : array-like, shape (n_components, n_features)
+
+        sk : array-like, shape (n_components, n_features, n_features)
+        """
+        _, n_features = xk.shape
+
+        # Warning : in some Bishop book, there is a typo on the formula 10.63
+        # `degrees_of_freedom_k = degrees_of_freedom_0 + Nk` is
+        # the correct formula
+        self.degrees_of_freedom_ = self.degrees_of_freedom_prior_ + nk
+
+        self.covariances_ = np.empty((self.n_components, n_features,
+                                      n_features))
+
+        for k in range(self.n_components):
+            diff = xk[k] - self.mean_prior_
+            self.covariances_[k] = (self.covariance_prior_ + nk[k] * sk[k] +
+                                    nk[k] * self.mean_precision_prior_ /
+                                    self.mean_precision_[k] * np.outer(diff,
+                                                                       diff))
+
+        # Contrary to the original bishop book, we normalize the covariances
+        self.covariances_ /= (
+            self.degrees_of_freedom_[:, np.newaxis, np.newaxis])
+
+    def _estimate_wishart_tied(self, nk, xk, sk):
+        """Estimate the tied Wishart distribution parameters.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+
+        nk : array-like, shape (n_components,)
+
+        xk : array-like, shape (n_components, n_features)
+
+        sk : array-like, shape (n_features, n_features)
+        """
+        _, n_features = xk.shape
+
+        # Warning : in some Bishop book, there is a typo on the formula 10.63
+        # `degrees_of_freedom_k = degrees_of_freedom_0 + Nk`
+        # is the correct formula
+        self.degrees_of_freedom_ = (
+            self.degrees_of_freedom_prior_ + nk.sum() / self.n_components)
+
+        diff = xk - self.mean_prior_
+        self.covariances_ = (
+            self.covariance_prior_ + sk * nk.sum() / self.n_components +
+            self.mean_precision_prior_ / self.n_components * np.dot(
+                (nk / self.mean_precision_) * diff.T, diff))
+
+        # Contrary to the original bishop book, we normalize the covariances
+        self.covariances_ /= self.degrees_of_freedom_
+
+    def _estimate_wishart_diag(self, nk, xk, sk):
+        """Estimate the diag Wishart distribution parameters.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+
+        nk : array-like, shape (n_components,)
+
+        xk : array-like, shape (n_components, n_features)
+
+        sk : array-like, shape (n_components, n_features)
+        """
+        _, n_features = xk.shape
+
+        # Warning : in some Bishop book, there is a typo on the formula 10.63
+        # `degrees_of_freedom_k = degrees_of_freedom_0 + Nk`
+        # is the correct formula
+        self.degrees_of_freedom_ = self.degrees_of_freedom_prior_ + nk
+
+        diff = xk - self.mean_prior_
+        self.covariances_ = (
+            self.covariance_prior_ + nk[:, np.newaxis] * (
+                sk + (self.mean_precision_prior_ /
+                      self.mean_precision_)[:, np.newaxis] * np.square(diff)))
+
+        # Contrary to the original bishop book, we normalize the covariances
+        self.covariances_ /= self.degrees_of_freedom_[:, np.newaxis]
+
+    def _estimate_wishart_spherical(self, nk, xk, sk):
+        """Estimate the spherical Wishart distribution parameters.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+
+        nk : array-like, shape (n_components,)
+
+        xk : array-like, shape (n_components, n_features)
+
+        sk : array-like, shape (n_components,)
+        """
+        _, n_features = xk.shape
+
+        # Warning : in some Bishop book, there is a typo on the formula 10.63
+        # `degrees_of_freedom_k = degrees_of_freedom_0 + Nk`
+        # is the correct formula
+        self.degrees_of_freedom_ = self.degrees_of_freedom_prior_ + nk
+
+        diff = xk - self.mean_prior_
+        self.covariances_ = (
+            self.covariance_prior_ + nk * (
+                sk + self.mean_precision_prior_ / self.mean_precision_ *
+                np.mean(np.square(diff), 1)))
+
+        # Contrary to the original bishop book, we normalize the covariances
+        self.covariances_ /= self.degrees_of_freedom_
+
+    def _check_is_fitted(self):
+        check_is_fitted(self, ['dirichlet_concentration_', 'mean_precision_',
+                               'means_', 'degrees_of_freedom_',
+                               'covariances_', 'precisions_',
+                               'precisions_cholesky_'])
+
+    def _m_step(self, X, log_resp):
+        """M step.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+
+        log_resp : array-like, shape (n_samples, n_components)
+            Logarithm of the posterior probabilities (or responsibilities) of
+            the point of each sample in X.
+        """
+        n_samples, _ = X.shape
+
+        nk, xk, sk = _estimate_gaussian_parameters(
+            X, np.exp(log_resp), self.reg_covar, self.covariance_type)
+        self._estimate_weights(nk)
+        self._estimate_means(nk, xk)
+        self._estimate_precisions(nk, xk, sk)
+
+    def _estimate_log_weights(self):
+        return (digamma(self.dirichlet_concentration_) -
+                digamma(np.sum(self.dirichlet_concentration_)))
+
+    def _estimate_log_prob(self, X):
+        _, n_features = X.shape
+        # We remove `n_features * np.log(self.degrees_of_freedom_)` because
+        # the precision matrix is normalized
+        log_gauss = (_estimate_log_gaussian_prob(
+            X, self.means_, self.precisions_cholesky_, self.covariance_type) -
+            .5 * n_features * np.log(self.degrees_of_freedom_))
+
+        log_lambda = n_features * np.log(2.) + np.sum(digamma(
+            .5 * (self.degrees_of_freedom_ -
+                  np.arange(0, n_features)[:, np.newaxis])), 0)
+
+        return log_gauss + .5 * (log_lambda -
+                                 n_features / self.mean_precision_)
+
+    def _compute_lower_bound(self, log_resp, log_prob_norm):
+        """Estimate the lower bound of the model.
+
+        The lower bound on the likelihood (of the training data with respect to
+        the model) is used to detect the convergence and has to decrease at
+        each iteration.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+
+        log_resp : array, shape (n_samples, n_components)
+            Logarithm of the posterior probabilities (or responsibilities) of
+            the point of each sample in X.
+
+        log_prob_norm : float
+            Logarithm of the probability of each sample in X.
+
+        Returns
+        -------
+        lower_bound : float
+        """
+        # Contrary to the original formula, we have done some simplification
+        # and removed all the constant terms.
+        n_features, = self.mean_prior_.shape
+
+        # We removed `.5 * n_features * np.log(self.degrees_of_freedom_)`
+        # because the precision matrix is normalized.
+        log_det_precisions_chol = (_compute_log_det_cholesky(
+            self.precisions_cholesky_, self.covariance_type, n_features) -
+            .5 * n_features * np.log(self.degrees_of_freedom_))
+
+        if self.covariance_type == 'tied':
+            log_wishart = self.n_components * np.float64(_log_wishart_norm(
+                self.degrees_of_freedom_, log_det_precisions_chol, n_features))
+        else:
+            log_wishart = np.sum(_log_wishart_norm(
+                self.degrees_of_freedom_, log_det_precisions_chol, n_features))
+
+        return (-np.sum(np.exp(log_resp) * log_resp) - log_wishart -
+                _log_dirichlet_norm(self.dirichlet_concentration_) -
+                0.5 * n_features * np.sum(np.log(self.mean_precision_)))
+
+    def _get_parameters(self):
+        return (self.dirichlet_concentration_,
+                self.mean_precision_, self.means_,
+                self.degrees_of_freedom_, self.covariances_,
+                self.precisions_cholesky_)
+
+    def _set_parameters(self, params):
+        (self.dirichlet_concentration_, self.mean_precision_, self.means_,
+         self.degrees_of_freedom_, self.covariances_,
+         self.precisions_cholesky_) = params
+
+        # Attributes computation
+        self. weights_ = (self.dirichlet_concentration_ /
+                          np.sum(self.dirichlet_concentration_))
+
+        if self.covariance_type == 'full':
+            self.precisions_ = np.array([
+                np.dot(prec_chol, prec_chol.T)
+                for prec_chol in self.precisions_cholesky_])
+
+        elif self.covariance_type == 'tied':
+            self.precisions_ = np.dot(self.precisions_cholesky_,
+                                      self.precisions_cholesky_.T)
+        else:
+            self.precisions_ = self.precisions_cholesky_ ** 2
diff --git a/sklearn/mixture/dpgmm.py b/sklearn/mixture/dpgmm.py
index 11bb2bc473..ee2bd64911 100755
--- a/sklearn/mixture/dpgmm.py
+++ b/sklearn/mixture/dpgmm.py
@@ -631,8 +631,8 @@ def __init__(self, n_components=1, covariance_type='diag', alpha=1.0,
 
 
 @deprecated("The VBGMM class is not working correctly and it's better "
-            "to not use it. VBGMM is deprecated in 0.18 and "
-            "will be removed in 0.20.")
+            "to use sklearn.mixture.BayesianGaussianMixture class instead. "
+            "VBGMM is deprecated in 0.18 and will be removed in 0.20.")
 class VBGMM(_DPGMMBase):
     """Variational Inference for the Gaussian Mixture Model
 
diff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py
index 731d757d77..749af6c82f 100755
--- a/sklearn/mixture/gaussian_mixture.py
+++ b/sklearn/mixture/gaussian_mixture.py
@@ -189,11 +189,10 @@ def _estimate_gaussian_covariances_tied(resp, X, nk, means, reg_covar):
     covariance : array, shape (n_features, n_features)
         The tied covariance matrix of the components.
     """
-    n_samples, _ = X.shape
     avg_X2 = np.dot(X.T, X)
     avg_means2 = np.dot(nk * means.T, means)
     covariance = avg_X2 - avg_means2
-    covariance /= n_samples
+    covariance /= nk.sum()
     covariance.flat[::len(covariance) + 1] += reg_covar
     return covariance
 
@@ -306,8 +305,9 @@ def _compute_precision_cholesky(covariances, covariance_type):
         components. The shape depends of the covariance_type.
     """
     estimate_precision_error_message = (
-        "The algorithm has diverged because of too few samples per "
-        "components. Try to decrease the number of components, "
+        "Fitting the mixture model failed because some components have "
+        "ill-defined empirical covariance (for instance caused by singleton "
+        "or collapsed samples). Try to decrease the number of components, "
         "or increase reg_covar.")
 
     if covariance_type in 'full':
@@ -358,8 +358,7 @@ def _compute_log_det_cholesky(matrix_chol, covariance_type, n_features):
     Returns
     -------
     log_det_precision_chol : array-like, shape (n_components,)
-        The determinant of the cholesky decomposition.
-        matrix.
+        The determinant of the precision matrix for each component.
     """
     if covariance_type == 'full':
         n_components, _, _ = matrix_chol.shape
@@ -456,7 +455,7 @@ class GaussianMixture(BaseMixture):
 
     tol : float, defaults to 1e-3.
         The convergence threshold. EM iterations will stop when the
-        log_likelihood average gain is below this threshold.
+        lower bound average gain is below this threshold.
 
     reg_covar : float, defaults to 0.
         Non-negative regularization added to the diagonal of covariance.
@@ -557,6 +556,11 @@ class GaussianMixture(BaseMixture):
 
     lower_bound_ : float
         Log-likelihood of the best fit of EM.
+
+    See Also
+    --------
+    BayesianGaussianMixture : Finite gaussian mixture model fit with a
+        variational algorithm.
     """
 
     def __init__(self, n_components=1, covariance_type='full', tol=1e-3,
@@ -631,14 +635,20 @@ def _initialize(self, X, resp):
         else:
             self.precisions_cholesky_ = self.precisions_init
 
-    def _e_step(self, X):
-        log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
-        return np.mean(log_prob_norm), np.exp(log_resp)
+    def _m_step(self, X, log_resp):
+        """M step.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
 
-    def _m_step(self, X, resp):
+        log_resp : array-like, shape (n_samples, n_components)
+            Logarithm of the posterior probabilities (or responsibilities) of
+            the point of each sample in X.
+        """
         n_samples, _ = X.shape
         self.weights_, self.means_, self.covariances_ = (
-            _estimate_gaussian_parameters(X, resp, self.reg_covar,
+            _estimate_gaussian_parameters(X, np.exp(log_resp), self.reg_covar,
                                           self.covariance_type))
         self.weights_ /= n_samples
         self.precisions_cholesky_ = _compute_precision_cholesky(
diff --git a/sklearn/mixture/tests/test_bayesian_mixture.py b/sklearn/mixture/tests/test_bayesian_mixture.py
new file mode 100755
index 0000000000..3003a94446
--- /dev/null
+++ b/sklearn/mixture/tests/test_bayesian_mixture.py
@@ -0,0 +1,359 @@
+# Author: Wei Xue <xuewei4d@gmail.com>
+#         Thierry Guillemot <thierry.guillemot.work@gmail.com>
+# License: BSD 3 clause
+
+import numpy as np
+from scipy.special import gammaln
+
+from sklearn.utils.testing import assert_raise_message
+from sklearn.utils.testing import assert_almost_equal
+
+from sklearn.mixture.bayesian_mixture import _log_dirichlet_norm
+from sklearn.mixture.bayesian_mixture import _log_wishart_norm
+
+from sklearn.mixture import BayesianGaussianMixture
+
+from sklearn.mixture.tests.test_gaussian_mixture import RandomData
+from sklearn.exceptions import ConvergenceWarning
+from sklearn.utils.testing import assert_greater_equal, ignore_warnings
+
+
+COVARIANCE_TYPE = ['full', 'tied', 'diag', 'spherical']
+
+
+def test_log_dirichlet_norm():
+    rng = np.random.RandomState(0)
+
+    dirichlet_concentration = rng.rand(2)
+    expected_norm = (gammaln(np.sum(dirichlet_concentration)) -
+                     np.sum(gammaln(dirichlet_concentration)))
+    predected_norm = _log_dirichlet_norm(dirichlet_concentration)
+
+    assert_almost_equal(expected_norm, predected_norm)
+
+
+def test_log_wishart_norm():
+    rng = np.random.RandomState(0)
+
+    n_components, n_features = 5, 2
+    degrees_of_freedom = np.abs(rng.rand(n_components)) + 1.
+    log_det_precisions_chol = n_features * np.log(range(2, 2 + n_components))
+
+    expected_norm = np.empty(5)
+    for k, (degrees_of_freedom_k, log_det_k) in enumerate(
+            zip(degrees_of_freedom, log_det_precisions_chol)):
+        expected_norm[k] = -(
+            degrees_of_freedom_k * (log_det_k + .5 * n_features * np.log(2.)) +
+            np.sum(gammaln(.5 * (degrees_of_freedom_k -
+                                 np.arange(0, n_features)[:, np.newaxis])), 0))
+    predected_norm = _log_wishart_norm(degrees_of_freedom,
+                                       log_det_precisions_chol, n_features)
+
+    assert_almost_equal(expected_norm, predected_norm)
+
+
+def test_bayesian_mixture_covariance_type():
+    rng = np.random.RandomState(0)
+    n_samples, n_features = 10, 2
+    X = rng.rand(n_samples, n_features)
+
+    covariance_type = 'bad_covariance_type'
+    bgmm = BayesianGaussianMixture(covariance_type=covariance_type,
+                                   random_state=rng)
+    assert_raise_message(ValueError,
+                         "Invalid value for 'covariance_type': %s "
+                         "'covariance_type' should be in "
+                         "['spherical', 'tied', 'diag', 'full']"
+                         % covariance_type,
+                         bgmm.fit, X)
+
+
+def test_bayesian_mixture_weights_prior_initialisation():
+    rng = np.random.RandomState(0)
+    n_samples, n_components, n_features = 10, 5, 2
+    X = rng.rand(n_samples, n_features)
+
+    # Check raise message for a bad value of dirichlet_concentration_prior
+    bad_dirichlet_concentration_prior_ = 0.
+    bgmm = BayesianGaussianMixture(
+        dirichlet_concentration_prior=bad_dirichlet_concentration_prior_,
+        random_state=0)
+    assert_raise_message(ValueError,
+                         "The parameter 'dirichlet_concentration_prior' "
+                         "should be greater than 0., but got %.3f."
+                         % bad_dirichlet_concentration_prior_,
+                         bgmm.fit, X)
+
+    # Check correct init for a given value of dirichlet_concentration_prior
+    dirichlet_concentration_prior = rng.rand()
+    bgmm = BayesianGaussianMixture(
+        dirichlet_concentration_prior=dirichlet_concentration_prior,
+        random_state=rng).fit(X)
+    assert_almost_equal(dirichlet_concentration_prior,
+                        bgmm.dirichlet_concentration_prior_)
+
+    # Check correct init for the default value of dirichlet_concentration_prior
+    bgmm = BayesianGaussianMixture(n_components=n_components,
+                                   random_state=rng).fit(X)
+    assert_almost_equal(1. / n_components, bgmm.dirichlet_concentration_prior_)
+
+
+def test_bayesian_mixture_means_prior_initialisation():
+    rng = np.random.RandomState(0)
+    n_samples, n_components, n_features = 10, 3, 2
+    X = rng.rand(n_samples, n_features)
+
+    # Check raise message for a bad value of mean_precision_prior
+    bad_mean_precision_prior_ = 0.
+    bgmm = BayesianGaussianMixture(
+        mean_precision_prior=bad_mean_precision_prior_,
+        random_state=rng)
+    assert_raise_message(ValueError,
+                         "The parameter 'mean_precision_prior' should be "
+                         "greater than 0., but got %.3f."
+                         % bad_mean_precision_prior_,
+                         bgmm.fit, X)
+
+    # Check correct init for a given value of mean_precision_prior
+    mean_precision_prior = rng.rand()
+    bgmm = BayesianGaussianMixture(
+        mean_precision_prior=mean_precision_prior,
+        random_state=rng).fit(X)
+    assert_almost_equal(mean_precision_prior, bgmm.mean_precision_prior_)
+
+    # Check correct init for the default value of mean_precision_prior
+    bgmm = BayesianGaussianMixture(random_state=rng).fit(X)
+    assert_almost_equal(1., bgmm.mean_precision_prior_)
+
+    # Check raise message for a bad shape of mean_prior
+    mean_prior = rng.rand(n_features + 1)
+    bgmm = BayesianGaussianMixture(n_components=n_components,
+                                   mean_prior=mean_prior,
+                                   random_state=rng)
+    assert_raise_message(ValueError,
+                         "The parameter 'means' should have the shape of ",
+                         bgmm.fit, X)
+
+    # Check correct init for a given value of mean_prior
+    mean_prior = rng.rand(n_features)
+    bgmm = BayesianGaussianMixture(n_components=n_components,
+                                   mean_prior=mean_prior,
+                                   random_state=rng).fit(X)
+    assert_almost_equal(mean_prior, bgmm.mean_prior_)
+
+    # Check correct init for the default value of bemean_priorta
+    bgmm = BayesianGaussianMixture(n_components=n_components,
+                                   random_state=rng).fit(X)
+    assert_almost_equal(X.mean(axis=0), bgmm.mean_prior_)
+
+
+def test_bayesian_mixture_precisions_prior_initialisation():
+    rng = np.random.RandomState(0)
+    n_samples, n_features = 10, 2
+    X = rng.rand(n_samples, n_features)
+
+    # Check raise message for a bad value of degrees_of_freedom_prior
+    bad_degrees_of_freedom_prior_ = n_features - 1.
+    bgmm = BayesianGaussianMixture(
+        degrees_of_freedom_prior=bad_degrees_of_freedom_prior_,
+        random_state=rng)
+    assert_raise_message(ValueError,
+                         "The parameter 'degrees_of_freedom_prior' should be "
+                         "greater than %d, but got %.3f."
+                         % (n_features - 1, bad_degrees_of_freedom_prior_),
+                         bgmm.fit, X)
+
+    # Check correct init for a given value of degrees_of_freedom_prior
+    degrees_of_freedom_prior = rng.rand() + n_features - 1.
+    bgmm = BayesianGaussianMixture(
+        degrees_of_freedom_prior=degrees_of_freedom_prior,
+        random_state=rng).fit(X)
+    assert_almost_equal(degrees_of_freedom_prior,
+                        bgmm.degrees_of_freedom_prior_)
+
+    # Check correct init for the default value of degrees_of_freedom_prior
+    degrees_of_freedom_prior_default = n_features
+    bgmm = BayesianGaussianMixture(
+        degrees_of_freedom_prior=degrees_of_freedom_prior_default,
+        random_state=rng).fit(X)
+    assert_almost_equal(degrees_of_freedom_prior_default,
+                        bgmm.degrees_of_freedom_prior_)
+
+    # Check correct init for a given value of covariance_prior
+    covariance_prior = {
+        'full': np.cov(X.T, bias=1) + 10,
+        'tied': np.cov(X.T, bias=1) + 5,
+        'diag': np.diag(np.atleast_2d(np.cov(X.T, bias=1))) + 3,
+        'spherical': rng.rand()}
+
+    bgmm = BayesianGaussianMixture(random_state=rng)
+    for cov_type in ['full', 'tied', 'diag', 'spherical']:
+        bgmm.covariance_type = cov_type
+        bgmm.covariance_prior = covariance_prior[cov_type]
+        bgmm.fit(X)
+        assert_almost_equal(covariance_prior[cov_type],
+                            bgmm.covariance_prior_)
+
+    # Check raise message for a bad spherical value of covariance_prior
+    bad_covariance_prior_ = -1.
+    bgmm = BayesianGaussianMixture(covariance_type='spherical',
+                                   covariance_prior=bad_covariance_prior_,
+                                   random_state=rng)
+    assert_raise_message(ValueError,
+                         "The parameter 'spherical covariance_prior' "
+                         "should be greater than 0., but got %.3f."
+                         % bad_covariance_prior_,
+                         bgmm.fit, X)
+
+    # Check correct init for the default value of covariance_prior
+    covariance_prior_default = {
+        'full': np.atleast_2d(np.cov(X.T)),
+        'tied': np.atleast_2d(np.cov(X.T)),
+        'diag': np.var(X, axis=0, ddof=1),
+        'spherical': np.var(X, axis=0, ddof=1).mean()}
+
+    bgmm = BayesianGaussianMixture(random_state=0)
+    for cov_type in ['full', 'tied', 'diag', 'spherical']:
+        bgmm.covariance_type = cov_type
+        bgmm.fit(X)
+        assert_almost_equal(covariance_prior_default[cov_type],
+                            bgmm.covariance_prior_)
+
+
+def test_bayesian_mixture_check_is_fitted():
+    rng = np.random.RandomState(0)
+    n_samples, n_features = 10, 2
+
+    # Check raise message
+    bgmm = BayesianGaussianMixture(random_state=rng)
+    X = rng.rand(n_samples, n_features)
+    assert_raise_message(ValueError,
+                         'This BayesianGaussianMixture instance is not '
+                         'fitted yet.', bgmm.score, X)
+
+
+def test_bayesian_mixture_weights():
+    rng = np.random.RandomState(0)
+    n_samples, n_features = 10, 2
+
+    X = rng.rand(n_samples, n_features)
+    bgmm = BayesianGaussianMixture(random_state=rng).fit(X)
+
+    # Check the weights values
+    expected_weights = (bgmm.dirichlet_concentration_ /
+                        np.sum(bgmm.dirichlet_concentration_))
+    predected_weights = bgmm.weights_
+
+    assert_almost_equal(expected_weights, predected_weights)
+
+    # Check the weights sum = 1
+    assert_almost_equal(np.sum(bgmm.weights_), 1.0)
+
+
+@ignore_warnings(category=ConvergenceWarning)
+def test_monotonic_likelihood():
+    # We check that each step of the each step of variational inference without
+    # regularization improve monotonically the training set of the bound
+    rng = np.random.RandomState(0)
+    rand_data = RandomData(rng, scale=7)
+    n_components = rand_data.n_components
+
+    for covar_type in COVARIANCE_TYPE:
+        X = rand_data.X[covar_type]
+        bgmm = BayesianGaussianMixture(n_components=2 * n_components,
+                                       covariance_type=covar_type,
+                                       warm_start=True, max_iter=1,
+                                       random_state=rng, tol=1e-4)
+        current_lower_bound = -np.infty
+        # Do one training iteration at a time so we can make sure that the
+        # training log likelihood increases after each iteration.
+        for _ in range(500):
+            prev_lower_bound = current_lower_bound
+            current_lower_bound = bgmm.fit(X).lower_bound_
+            assert_greater_equal(current_lower_bound, prev_lower_bound)
+
+            if bgmm.converged_:
+                break
+        assert(bgmm.converged_)
+
+
+def test_compare_covar_type():
+    # We can compare the 'full' precision with the other cov_type if we apply
+    # 1 iter of the M-step (done during _initialize_parameters).
+    rng = np.random.RandomState(0)
+    rand_data = RandomData(rng, scale=7)
+    X = rand_data.X['full']
+    n_components = rand_data.n_components
+    # Computation of the full_covariance
+    bgmm = BayesianGaussianMixture(n_components=2 * n_components,
+                                   covariance_type='full',
+                                   max_iter=1, random_state=0, tol=1e-7)
+    bgmm._check_initial_parameters(X)
+    bgmm._initialize_parameters(X)
+    full_covariances = (bgmm.covariances_ *
+                        bgmm.degrees_of_freedom_[:, np.newaxis, np.newaxis])
+
+    # Check tied_covariance = mean(full_covariances, 0)
+    bgmm = BayesianGaussianMixture(n_components=2 * n_components,
+                                   covariance_type='tied',
+                                   max_iter=1, random_state=0, tol=1e-7)
+    bgmm._check_initial_parameters(X)
+    bgmm._initialize_parameters(X)
+
+    tied_covariance = bgmm.covariances_ * bgmm.degrees_of_freedom_
+    assert_almost_equal(tied_covariance, np.mean(full_covariances, 0))
+
+    # Check diag_covariance = diag(full_covariances)
+    bgmm = BayesianGaussianMixture(n_components=2 * n_components,
+                                   covariance_type='diag',
+                                   max_iter=1, random_state=0, tol=1e-7)
+    bgmm._check_initial_parameters(X)
+    bgmm._initialize_parameters(X)
+
+    diag_covariances = (bgmm.covariances_ *
+                        bgmm.degrees_of_freedom_[:, np.newaxis])
+    assert_almost_equal(diag_covariances,
+                        np.array([np.diag(cov) for cov in full_covariances]))
+
+    # Check spherical_covariance = np.mean(diag_covariances, 0)
+    bgmm = BayesianGaussianMixture(n_components=2 * n_components,
+                                   covariance_type='spherical',
+                                   max_iter=1, random_state=0, tol=1e-7)
+    bgmm._check_initial_parameters(X)
+    bgmm._initialize_parameters(X)
+
+    spherical_covariances = bgmm.covariances_ * bgmm.degrees_of_freedom_
+    assert_almost_equal(spherical_covariances, np.mean(diag_covariances, 1))
+
+
+@ignore_warnings(category=ConvergenceWarning)
+def test_check_covariance_precision():
+    # We check that the dot product of the covariance and the precision
+    # matrices is identity.
+    rng = np.random.RandomState(0)
+    rand_data = RandomData(rng, scale=7)
+    n_components, n_features = 2 * rand_data.n_components, 2
+
+    # Computation of the full_covariance
+    bgmm = BayesianGaussianMixture(n_components=n_components,
+                                   max_iter=100, random_state=rng, tol=1e-3,
+                                   reg_covar=0)
+    for covar_type in COVARIANCE_TYPE:
+        bgmm.covariance_type = covar_type
+        bgmm.fit(rand_data.X[covar_type])
+
+        if covar_type == 'full':
+            for covar, precision in zip(bgmm.covariances_, bgmm.precisions_):
+                assert_almost_equal(np.dot(covar, precision),
+                                    np.eye(n_features))
+        elif covar_type == 'tied':
+            assert_almost_equal(np.dot(bgmm.covariances_, bgmm.precisions_),
+                                np.eye(n_features))
+
+        elif covar_type == 'diag':
+            assert_almost_equal(bgmm.covariances_ * bgmm.precisions_,
+                                np.ones((n_components, n_features)))
+
+        else:
+            assert_almost_equal(bgmm.covariances_ * bgmm.precisions_,
+                                np.ones(n_components))
diff --git a/sklearn/mixture/tests/test_dpgmm.py b/sklearn/mixture/tests/test_dpgmm.py
index cedf2e90b2..363d31bc29 100755
--- a/sklearn/mixture/tests/test_dpgmm.py
+++ b/sklearn/mixture/tests/test_dpgmm.py
@@ -183,10 +183,11 @@ class TestDPGMMWithFullCovars(unittest.TestCase, DPGMMTester):
 
 
 def test_VBGMM_deprecation():
-    assert_warns_message(
-        DeprecationWarning,
-        "The VBGMM class is not working correctly and it's better to not use "
-        "it. VBGMM is deprecated in 0.18 and will be removed in 0.20.", VBGMM)
+    assert_warns_message(DeprecationWarning, "The VBGMM class is not working "
+                         "correctly and it's better to use "
+                         "sklearn.mixture.BayesianGaussianMixture class "
+                         "instead. VBGMM is deprecated in 0.18 and will be "
+                         "removed in 0.20.", VBGMM)
 
 
 class VBGMMTester(GMMTester):
diff --git a/sklearn/mixture/tests/test_gaussian_mixture.py b/sklearn/mixture/tests/test_gaussian_mixture.py
index fab41933d0..194248ac7a 100755
--- a/sklearn/mixture/tests/test_gaussian_mixture.py
+++ b/sklearn/mixture/tests/test_gaussian_mixture.py
@@ -883,10 +883,12 @@ def test_regularisation():
         with warnings.catch_warnings():
             warnings.simplefilter("ignore", RuntimeWarning)
             assert_raise_message(ValueError,
-                                 "The algorithm has diverged because of too "
-                                 "few samples per components. "
-                                 "Try to decrease the number of components, "
-                                 "or increase reg_covar.", gmm.fit, X)
+                                 "Fitting the mixture model failed because "
+                                 "some components have ill-defined empirical "
+                                 "covariance (for instance caused by "
+                                 "singleton or collapsed samples). Try to "
+                                 "decrease the number of components, or "
+                                 "increase reg_covar.", gmm.fit, X)
 
             gmm.set_params(reg_covar=1e-6).fit(X)
 
diff --git a/sklearn/utils/validation.py b/sklearn/utils/validation.py
index c153bb5dc5..5783ab2ab8 100755
--- a/sklearn/utils/validation.py
+++ b/sklearn/utils/validation.py
@@ -177,8 +177,8 @@ def check_consistent_length(*arrays):
     lengths = [_num_samples(X) for X in arrays if X is not None]
     uniques = np.unique(lengths)
     if len(uniques) > 1:
-        raise ValueError("Found arrays with inconsistent numbers of samples: "
-                         "%r" % lengths)
+        raise ValueError("Found input variables with inconsistent numbers of"
+                         " samples: %r" % lengths)
 
 
 def indexable(*iterables):
