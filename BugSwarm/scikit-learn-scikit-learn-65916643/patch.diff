diff --git a/doc/about.rst b/doc/about.rst
index fb766e65a71a..e3d17113b355 100644
--- a/doc/about.rst
+++ b/doc/about.rst
@@ -10,7 +10,7 @@ About us
 Citing scikit-learn
 -------------------
 
-If you use scikit-learn in scientific publication, we would appreciate
+If you use scikit-learn in a scientific publication, we would appreciate
 citations to the following paper:
 
  `Scikit-learn: Machine Learning in Python
@@ -31,6 +31,27 @@ citations to the following paper:
     year={2011}
    }
 
+If you want to cite scikit-learn for its API or design, you may also want to consider the
+following paper:
+
+`API design for machine learning software: experiences from the scikit-learn
+project <http://arxiv.org/abs/1309.0238>`_, Buitinck *et al.*, 2013.
+
+Bibtex entry::
+
+    @inproceedings{sklearn_api,
+      author    = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and
+                   Fabian Pedregosa and Andreas Mueller and Olivier Grisel and
+                   Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort
+                   and Jaques Grobler and Robert Layton and Jake VanderPlas and
+                   Arnaud Joly and Brian Holt and Ga{\"{e}}l Varoquaux},
+      title     = {{API} design for machine learning software: experiences from the scikit-learn
+                   project},
+      booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
+      year      = {2013},
+      pages = {108--122},
+    }
+
 Artwork
 -------
 
diff --git a/doc/developers/index.rst b/doc/developers/index.rst
index c1c6f1d268a1..eb64e071652c 100644
--- a/doc/developers/index.rst
+++ b/doc/developers/index.rst
@@ -111,8 +111,8 @@ then submit a "pull request" (PR):
 
 Finally, go to the web page of the your fork of the scikit-learn repo,
 and click 'Pull request' to send your changes to the maintainers for review.
-request. This will send an email to the committers, but might also send an
-email to the mailing list in order to get more visibility.
+You may want to consider sending an email to the mailing list for more
+visibility.
 
 .. note::
 
@@ -240,7 +240,7 @@ compromise between mathematical and algorithmic details, and give
 intuition to the reader on what the algorithm does.
 
 Basically, to elaborate on the above, it is best to always
-start with a small paragraph with a hand-waiving explanation of what the
+start with a small paragraph with a hand-waving explanation of what the
 method does to the data. Then, it is very helpful
 to point out why the feature is useful and when it should be used -
 the latter also including "big O"
@@ -265,8 +265,8 @@ opposed to how it works "under the hood".
 Finally, follow the formatting rules below to make it consistently good:
 
     * Add "See also" in docstrings for related classes/functions.
-    
-    * "See also" in docstrings should be one line per reference, 
+
+    * "See also" in docstrings should be one line per reference,
       with a colon and an explanation, for example::
 
         See also
@@ -275,7 +275,7 @@ Finally, follow the formatting rules below to make it consistently good:
         SelectFpr: Select features based on a false positive rate test.
 
     * For unwritten formatting rules, try to follow existing good works:
-    
+
         * For "References" in docstrings, see the Silhouette Coefficient
           (:func:`sklearn.metrics.silhouette_score`).
 
@@ -294,7 +294,7 @@ Testing and improving test coverage
 High-quality `unit testing <http://en.wikipedia.org/wiki/Unit_testing>`_
 is a corner-stone of the scikit-learn development process. For this
 purpose, we use the `nose <http://nose.readthedocs.org/en/latest/>`_
-package. The tests are functions appropriately names, located in `tests`
+package. The tests are functions appropriately named, located in `tests`
 subdirectories, that check the validity of the algorithms and the
 different options of the code.
 
@@ -406,7 +406,7 @@ In addition, we add the following guidelines:
       that is implemented in ``sklearn.foo.bar.baz``,
       the test should import it from ``sklearn.foo``.
 
-    * **Please don't use ``import *`` in any case**. It is considered harmful
+    * **Please don't use** ``import *`` **in any case**. It is considered harmful
       by the `official Python recommendations
       <http://docs.python.org/howto/doanddont.html#from-module-import>`_.
       It makes the code harder to read as the origin of symbols is no
@@ -670,7 +670,7 @@ are always remembered by the estimator.
 Also note that they should not be documented under the "Attributes" section,
 but rather under the "Parameters" section for that estimator.
 
-In addition, **every keyword argument accepted by ``__init__`` should
+In addition, **every keyword argument accepted by** ``__init__`` **should
 correspond to an attribute on the instance**. Scikit-learn relies on this to
 find the relevant attributes to set on an estimator when doing model selection.
 
@@ -802,11 +802,13 @@ E.g., here's a custom classifier::
   ...     """Predicts the majority class of its training data."""
   ...     def __init__(self):
   ...         pass
+  ...
   ...     def fit(self, X, y):
   ...         self.classes_, indices = np.unique(["foo", "bar", "foo"],
   ...                                            return_inverse=True)
   ...         self.majority_ = np.argmax(np.bincount(indices))
   ...         return self
+  ...
   ...     def predict(self, X):
   ...         return np.repeat(self.classes_[self.majority_], len(X))
 
@@ -852,12 +854,12 @@ to apply parameter setting to estimators,
 it is essential that calling ``set_params`` has the same effect
 as setting parameters using the ``__init__`` method.
 The easiest and recommended way to accomplish this is to
-**not do any parameter validation in ``__init__``**.
+**not do any parameter validation in** ``__init__``.
 All logic behind estimator parameters,
 like translating string arguments into functions, should be done in ``fit``.
 
 Also it is expected that parameters with trailing ``_`` are **not to be set
-inside the ``__init__`` method**. All and only the public attributes set by
+inside the** ``__init__`` **method**. All and only the public attributes set by
 fit have a trailing ``_``. As a result the existence of parameters with
 trailing ``_`` is used to check if the estimator has been fitted.
 
diff --git a/doc/developers/utilities.rst b/doc/developers/utilities.rst
index cea51d853207..bc61f0856b45 100644
--- a/doc/developers/utilities.rst
+++ b/doc/developers/utilities.rst
@@ -144,7 +144,7 @@ efficiently process ``scipy.sparse`` data.
 
 - :func:`sparsefuncs.inplace_csr_row_normalize_l1` and
   :func:`sparsefuncs.inplace_csr_row_normalize_l2`: can be used to normalize
-  individual sparse samples to unit l1 or l2 norm as done in
+  individual sparse samples to unit L1 or L2 norm as done in
   :class:`sklearn.preprocessing.Normalizer`.
 
 - :func:`sparsefuncs.inplace_csr_column_scale`: can be used to multiply the
@@ -159,7 +159,8 @@ Graph Routines
 - :func:`graph.single_source_shortest_path_length`:
   (not currently used in scikit-learn)
   Return the shortest path from a single source
-  to all connected nodes on a graph.  Code is adapted from networkx.
+  to all connected nodes on a graph.  Code is adapted from `networkx
+  <https://networkx.github.io/>`_.
   If this is ever needed again, it would be far faster to use a single
   iteration of Dijkstra's algorithm from ``graph_shortest_path``.
 
@@ -169,7 +170,7 @@ Graph Routines
   both dense and sparse connectivity matrices.
 
 - :func:`graph_shortest_path.graph_shortest_path`:
-  (used in :class:``sklearn.manifold.Isomap``)
+  (used in :class:`sklearn.manifold.Isomap`)
   Return the shortest path between all pairs of connected points on a directed
   or undirected graph.  Both the Floyd-Warshall algorithm and Dijkstra's
   algorithm are available.  The algorithm is most efficient when the
@@ -235,7 +236,7 @@ Testing Functions
   requests to mldata.org. Used in tests of :mod:`sklearn.datasets`.
 
 - :func:`testing.all_estimators` : returns a list of all estimators in
-  sklearn to test for consistent behavior and interfaces.
+  scikit-learn to test for consistent behavior and interfaces.
 
 Multiclass and multilabel utility function
 ==========================================
diff --git a/doc/modules/classes.rst b/doc/modules/classes.rst
index ad9054a01cbd..696210ded367 100644
--- a/doc/modules/classes.rst
+++ b/doc/modules/classes.rst
@@ -210,9 +210,7 @@ Loaders
    datasets.load_digits
    datasets.load_files
    datasets.load_iris
-   datasets.load_lfw_pairs
    datasets.fetch_lfw_pairs
-   datasets.load_lfw_people
    datasets.fetch_lfw_people
    datasets.load_linnerud
    datasets.mldata_filename
diff --git a/doc/modules/clustering.rst b/doc/modules/clustering.rst
index bac313d87a5c..a70d1b81267a 100644
--- a/doc/modules/clustering.rst
+++ b/doc/modules/clustering.rst
@@ -879,13 +879,11 @@ classes according to some similarity metric.
 
 .. currentmodule:: sklearn.metrics
 
+.. _adjusted_rand_score:
 
 Adjusted Rand index
 -------------------
 
-Presentation and usage
-~~~~~~~~~~~~~~~~~~~~~~
-
 Given the knowledge of the ground truth class assignments ``labels_true``
 and our clustering algorithm assignments of the same samples
 ``labels_pred``, the **adjusted Rand index** is a function that measures
@@ -1000,13 +998,11 @@ random labelings by defining the adjusted Rand index as follows:
  * `Wikipedia entry for the adjusted Rand index
    <http://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index>`_
 
+.. _mutual_info_score:
 
 Mutual Information based scores
 -------------------------------
 
-Presentation and usage
-~~~~~~~~~~~~~~~~~~~~~~
-
 Given the knowledge of the ground truth class assignments ``labels_true`` and
 our clustering algorithm assignments of the same samples ``labels_pred``, the
 **Mutual Information** is a function that measures the **agreement** of the two
@@ -1168,12 +1164,11 @@ calculated using a similar form to that of the adjusted Rand index:
  * `Wikipedia entry for the Adjusted Mutual Information
    <http://en.wikipedia.org/wiki/Adjusted_Mutual_Information>`_
 
+.. _homogeneity_completeness:
+
 Homogeneity, completeness and V-measure
 ---------------------------------------
 
-Presentation and usage
-~~~~~~~~~~~~~~~~~~~~~~
-
 Given the knowledge of the ground truth class assignments of the samples,
 it is possible to define some intuitive metric using conditional entropy
 analysis.
@@ -1329,9 +1324,6 @@ mean of homogeneity and completeness**:
 Silhouette Coefficient
 ----------------------
 
-Presentation and usage
-~~~~~~~~~~~~~~~~~~~~~~
-
 If the ground truth labels are not known, evaluation must be performed using
 the model itself. The Silhouette Coefficient
 (:func:`sklearn.metrics.silhouette_score`)
diff --git a/doc/modules/covariance.rst b/doc/modules/covariance.rst
index 9da5be0a4083..27c6b3f722e1 100644
--- a/doc/modules/covariance.rst
+++ b/doc/modules/covariance.rst
@@ -248,6 +248,7 @@ paper. It is the same algorithm as in the R ``glasso`` package.
      graphical lasso" <http://biostatistics.oxfordjournals.org/content/9/3/432.short>`_,
      Biostatistics 9, pp 432, 2008
 
+.. _robust_covariance:
 
 Robust Covariance Estimation
 ============================
diff --git a/doc/modules/decomposition.rst b/doc/modules/decomposition.rst
index 6ceccc202fd2..086881898b9a 100644
--- a/doc/modules/decomposition.rst
+++ b/doc/modules/decomposition.rst
@@ -554,9 +554,9 @@ structure of the error covariance :math:`\Psi`:
 * :math:`\Psi = \sigma^2 \mathbf{I}`: This assumption leads to
   the probabilistic model of :class:`PCA`.
 
-* :math:`\Psi = diag(\psi_1, \psi_2, \dots, \psi_n)`: This model is called Factor
-  Analysis, a classical statistical model. The matrix W is sometimes called
-  the "factor loading matrix".
+* :math:`\Psi = diag(\psi_1, \psi_2, \dots, \psi_n)`: This model is called
+  :class:`FactorAnalysis`, a classical statistical model. The matrix W is
+  sometimes called the "factor loading matrix".
 
 Both model essentially estimate a Gaussian with a low-rank covariance matrix.
 Because both models are probabilistic they can be integrated in more complex
diff --git a/doc/modules/ensemble.rst b/doc/modules/ensemble.rst
index 769232ecb7ec..315b23486a1b 100644
--- a/doc/modules/ensemble.rst
+++ b/doc/modules/ensemble.rst
@@ -780,6 +780,8 @@ accessed via the ``feature_importances_`` property::
 
 .. currentmodule:: sklearn.ensemble.partial_dependence
 
+.. _partial_dependence:
+
 Partial dependence
 ..................
 
@@ -989,10 +991,10 @@ calculated as follows:
 ================  ==========    ==========      ==========
 classifier        class 1       class 2         class 3
 ================  ==========    ==========      ==========
-classifier 1	  w1 * 0.2     w1 * 0.5       w1 * 0.3
-classifier 2	  w2 * 0.6     w2 * 0.3       w2 * 0.1
+classifier 1	  w1 * 0.2      w1 * 0.5        w1 * 0.3
+classifier 2	  w2 * 0.6      w2 * 0.3        w2 * 0.1
 classifier 3      w3 * 0.3      w3 * 0.4        w3 * 0.3
-weighted average  0.37	        0.4            0.3
+weighted average  0.37	        0.4             0.3
 ================  ==========    ==========      ==========
 
 Here, the predicted class label is 2, since it has the
@@ -1031,7 +1033,7 @@ Vector Machine, a Decision Tree, and a K-nearest neighbor classifier::
     :scale: 75%
 
 Using the `VotingClassifier` with `GridSearch`
----------------------------------------------
+----------------------------------------------
 
 The `VotingClassifier` can also be used together with `GridSearch` in order
 to tune the hyperparameters of the individual estimators::
diff --git a/doc/modules/feature_extraction.rst b/doc/modules/feature_extraction.rst
index 1971ea7a7c43..9d15cda6f69b 100644
--- a/doc/modules/feature_extraction.rst
+++ b/doc/modules/feature_extraction.rst
@@ -826,6 +826,7 @@ Some tips and tricks:
 Customizing the vectorizer can also be useful when handling Asian languages
 that do not use an explicit word separator such as whitespace.
 
+.. _image_feature_extraction:
 
 Image feature extraction
 ========================
diff --git a/doc/modules/feature_selection.rst b/doc/modules/feature_selection.rst
index 483e74516187..edf2edf78ae9 100644
--- a/doc/modules/feature_selection.rst
+++ b/doc/modules/feature_selection.rst
@@ -13,6 +13,8 @@ improve estimators' accuracy scores or to boost their performance on very
 high-dimensional datasets.
 
 
+.. _variance_threshold:
+
 Removing features with low variance
 ===================================
 
@@ -45,6 +47,8 @@ so we can select using the threshold ``.8 * (1 - .8)``::
 As expected, ``VarianceThreshold`` has removed the first column,
 which has a probability :math:`p = 5/6 > .8` of containing a zero.
 
+.. _univariate_feature_selection:
+
 Univariate feature selection
 ============================
 
@@ -101,6 +105,7 @@ univariate p-values:
 
     :ref:`example_feature_selection_plot_feature_selection.py`
 
+.. _rfe:
 
 Recursive feature elimination
 =============================
diff --git a/doc/modules/grid_search.rst b/doc/modules/grid_search.rst
index 394e8119d20d..eb4a0b10828f 100644
--- a/doc/modules/grid_search.rst
+++ b/doc/modules/grid_search.rst
@@ -72,6 +72,8 @@ evaluated and the best combination is retained.
       classifier (here a linear SVM trained with SGD with either elastic
       net or L2 penalty) using a :class:`pipeline.Pipeline` instance.
 
+.. _randomized_parameter_search:
+
 Randomized Parameter Optimization
 =================================
 While using a grid of parameter settings is currently the most widely used
diff --git a/doc/modules/kernel_approximation.rst b/doc/modules/kernel_approximation.rst
index 7048bf11dcb5..80da38074651 100644
--- a/doc/modules/kernel_approximation.rst
+++ b/doc/modules/kernel_approximation.rst
@@ -43,6 +43,7 @@ kernel function or a precomputed kernel matrix.
 The number of samples used - which is also the dimensionality of the features computed -
 is given by the parameter ``n_components``.
 
+.. _rbf_kernel_approx:
 
 Radial Basis Function Kernel
 ----------------------------
@@ -98,6 +99,7 @@ use of larger feature spaces more efficient.
 
     * :ref:`example_plot_kernel_approximation.py`
 
+.. _additive_chi_kernel_approx:
 
 Additive Chi Squared Kernel
 ---------------------------
@@ -130,6 +132,7 @@ with the approximate feature map provided by :class:`RBFSampler` to yield an app
 feature map for the exponentiated chi squared kernel.
 See the [VZ2010]_ for details and [VVZ2010]_ for combination with the :class:`RBFSampler`.
 
+.. _skewed_chi_kernel_approx:
 
 Skewed Chi Squared Kernel
 -------------------------
diff --git a/doc/modules/linear_model.rst b/doc/modules/linear_model.rst
index 61212a1b0f19..5d1331fa1528 100644
--- a/doc/modules/linear_model.rst
+++ b/doc/modules/linear_model.rst
@@ -266,6 +266,7 @@ They also tend to break when the problem is badly conditioned
 
   * :ref:`example_linear_model_plot_lasso_model_selection.py`
 
+.. _elastic_net:
 
 Elastic Net
 ===========
@@ -486,6 +487,9 @@ previously chosen dictionary elements.
    <http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf>`_,
    S. G. Mallat, Z. Zhang,
 
+
+.. _bayesian_regression:
+
 Bayesian Regression
 ===================
 
@@ -752,6 +756,8 @@ while with ``loss="hinge"`` it fits a linear support vector machine (SVM).
 
  * :ref:`sgd`
 
+.. _perceptron:
+
 Perceptron
 ==========
 
diff --git a/doc/modules/manifold.rst b/doc/modules/manifold.rst
index 5cf546906a72..0a7db76bb746 100644
--- a/doc/modules/manifold.rst
+++ b/doc/modules/manifold.rst
@@ -149,6 +149,7 @@ The overall complexity of Isomap is
      <http://www.sciencemag.org/content/290/5500/2319.full>`_
      Tenenbaum, J.B.; De Silva, V.; & Langford, J.C.  Science 290 (5500)
 
+.. _locally_linear_embedding:
 
 Locally Linear Embedding
 ========================
diff --git a/doc/modules/metrics.rst b/doc/modules/metrics.rst
index 9bba16c7d77f..24179a2cef66 100644
--- a/doc/modules/metrics.rst
+++ b/doc/modules/metrics.rst
@@ -35,6 +35,8 @@ the kernel:
 
 .. currentmodule:: sklearn.metrics.pairwise
 
+.. _cosine_similarity:
+
 Cosine similarity
 -----------------
 :func:`cosine_similarity` computes the L2-normalized dot product of vectors.
@@ -63,6 +65,8 @@ is equivalent to :func:`linear_kernel`, only slower.)
       Information Retrieval. Cambridge University Press.
       http://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html
 
+.. _linear_kernel:
+
 Linear kernel
 -------------
 The function :func:`linear_kernel` computes the linear kernel, that is, a
@@ -73,6 +77,8 @@ If ``x`` and ``y`` are column vectors, their linear kernel is:
 
     k(x, y) = x^\top y
 
+.. _polynomial_kernel:
+
 Polynomial kernel
 -----------------
 The function :func:`polynomial_kernel` computes the degree-d polynomial kernel
@@ -94,6 +100,8 @@ where:
 
 If :math:`c_0 = 0` the kernel is said to be homogeneous.
 
+.. _sigmoid_kernel:
+
 Sigmoid kernel
 --------------
 The function :func:`sigmoid_kernel` computes the sigmoid kernel between two
@@ -111,6 +119,8 @@ where:
     * :math:`\gamma` is known as slope
     * :math:`c_0` is known as intercept
 
+.. _rbf_kernel:
+
 RBF kernel
 ----------
 The function :func:`rbf_kernel` computes the radial basis function (RBF) kernel
@@ -123,6 +133,8 @@ between two vectors. This kernel is defined as:
 where ``x`` and ``y`` are the input vectors. If :math:`\gamma = \sigma^{-2}`
 the kernel is known as the Gaussian kernel of variance :math:`\sigma^2`.
 
+.. _chi2_kernel:
+
 Chi-squared kernel
 ------------------
 The chi-squared kernel is a very popular choice for training non-linear SVMs in
diff --git a/doc/modules/mixture.rst b/doc/modules/mixture.rst
index 2b2560c26114..6970e0b9e8e9 100644
--- a/doc/modules/mixture.rst
+++ b/doc/modules/mixture.rst
@@ -133,6 +133,7 @@ parameters to maximize the likelihood of the data given those
 assignments. Repeating this process is guaranteed to always converge
 to a local optimum. 
 
+.. _vbgmm:
 
 VBGMM classifier: variational Gaussian mixtures
 ================================================
diff --git a/doc/modules/model_evaluation.rst b/doc/modules/model_evaluation.rst
index a889664f37e1..a91494168f09 100644
--- a/doc/modules/model_evaluation.rst
+++ b/doc/modules/model_evaluation.rst
@@ -155,7 +155,7 @@ take several parameters:
   certainties (``needs_threshold=True``).  The default value is
   False.
 
-* any additional parameters, such as ``beta`` in an :func:`f1_score`.
+* any additional parameters, such as ``beta`` or ``labels`` in :func:`f1_score`.
 
 Here is an example of building custom scorers, and of using the
 ``greater_is_better`` parameter::
@@ -307,6 +307,7 @@ array of class labels, multilabel data is specified as an indicator matrix,
 in which cell ``[i, j]`` has value 1 if sample ``i`` has label ``j`` and value
 0 otherwise.
 
+.. _accuracy_score:
 
 Accuracy score
 --------------
@@ -352,6 +353,8 @@ In the multilabel case with binary label indicators: ::
     for an example of accuracy score usage using permutations of
     the dataset.
 
+.. _confusion_matrix:
+
 Confusion matrix
 ----------------
 
@@ -393,6 +396,7 @@ from the :ref:`example_model_selection_plot_confusion_matrix.py` example):
     for an example of using a confusion matrix to classify text
     documents.
 
+.. _classification_report:
 
 Classification report
 ----------------------
@@ -429,6 +433,8 @@ and inferred labels::
     for an example of classification report usage for
     grid search with nested cross-validation.
 
+.. _hamming_loss:
+
 Hamming loss
 -------------
 
@@ -470,6 +476,8 @@ In the multilabel case with binary label indicators: ::
     or superset of the true labels will give a Hamming loss between
     zero and one, exclusive.
 
+.. _jaccard_similarity_score:
+
 Jaccard similarity coefficient score
 -------------------------------------
 
@@ -649,8 +657,9 @@ specified by the ``average`` argument to the
 :func:`fbeta_score`, :func:`precision_recall_fscore_support`,
 :func:`precision_score` and :func:`recall_score` functions, as described
 :ref:`above <average>`. Note that for "micro"-averaging in a multiclass setting
-will produce equal precision, recall and :math:`F`, while "weighted" averaging
-may produce an F-score that is not between precision and recall.
+with all labels included will produce equal precision, recall and :math:`F`,
+while "weighted" averaging may produce an F-score that is not between
+precision and recall.
 
 To make this more explicit, consider the following notation:
 
@@ -701,6 +710,19 @@ Then the metrics are defined as:
   ... # doctest: +ELLIPSIS
   (array([ 0.66...,  0.        ,  0.        ]), array([ 1.,  0.,  0.]), array([ 0.71...,  0.        ,  0.        ]), array([2, 2, 2]...))
 
+For multiclass classification with a "negative class", it is possible to exclude some labels:
+
+  >>> metrics.recall_score(y_true, y_pred, labels=[1, 2], average='micro')
+  ... # excluding 0, no labels were correctly recalled
+  0.0
+
+Similarly, labels not present in the data sample may be accounted for in macro-averaging.
+
+  >>> metrics.precision_score(y_true, y_pred, labels=[0, 1, 2, 3], average='macro')
+  ... # doctest: +ELLIPSIS
+  0.166...
+
+.. _hinge_loss:
 
 Hinge loss
 ----------
@@ -769,6 +791,7 @@ with a svm classifier in a multiclass problem::
   >>> hinge_loss(y_true, pred_decision, labels)  #doctest: +ELLIPSIS
   0.56...
 
+.. _log_loss:
 
 Log loss
 --------
@@ -821,6 +844,7 @@ method.
 The first ``[.9, .1]`` in ``y_pred`` denotes 90% probability that the first
 sample has label 0.  The log loss is non-negative.
 
+.. _matthews_corrcoef:
 
 Matthews correlation coefficient
 ---------------------------------
@@ -1002,6 +1026,8 @@ In multilabel learning, each sample can have any number of ground truth labels
 associated with it. The goal is to give high scores and better rank to
 the ground truth labels.
 
+.. _coverage_error:
+
 Coverage error
 --------------
 
@@ -1034,6 +1060,8 @@ Here is a small example of usage of this function::
     >>> coverage_error(y_true, y_score)
     2.5
 
+.. _label_ranking_average_precision:
+
 Label ranking average precision
 -------------------------------
 
@@ -1075,6 +1103,8 @@ Here is a small example of usage of this function::
     >>> label_ranking_average_precision_score(y_true, y_score) # doctest: +ELLIPSIS
     0.416...
 
+.. _label_ranking_loss:
+
 Ranking loss
 ------------
 
@@ -1144,6 +1174,7 @@ score puts more importance on well explaining the higher variance variables.
 for backward compatibility. This will be changed to ``uniform_average`` in the
 future.
 
+.. _explained_variance_score:
 
 Explained variance score
 -------------------------
@@ -1179,6 +1210,8 @@ function::
     ... # doctest: +ELLIPSIS
     0.990...
 
+.. _mean_absolute_error:
+
 Mean absolute error
 -------------------
 
@@ -1212,6 +1245,7 @@ Here is a small example of usage of the :func:`mean_absolute_error` function::
   ... # doctest: +ELLIPSIS
   0.849...
 
+.. _mean_squared_error:
 
 Mean squared error
 -------------------
@@ -1248,6 +1282,8 @@ function::
     for an example of mean squared error usage to
     evaluate gradient boosting regression.
 
+.. _median_absolute_error:
+
 Median absolute error
 ---------------------
 
@@ -1274,6 +1310,8 @@ function::
   >>> median_absolute_error(y_true, y_pred)
   0.5
 
+.. _r2_score:
+
 R² score, the coefficient of determination
 -------------------------------------------
 
diff --git a/doc/modules/model_persistence.rst b/doc/modules/model_persistence.rst
index e1c9e92f70b5..e6c2f26b3bd1 100644
--- a/doc/modules/model_persistence.rst
+++ b/doc/modules/model_persistence.rst
@@ -22,9 +22,10 @@ persistence model, namely `pickle <http://docs.python.org/library/pickle.html>`_
   >>> iris = datasets.load_iris()
   >>> X, y = iris.data, iris.target
   >>> clf.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE
-  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
-    kernel='rbf', max_iter=-1, probability=False, random_state=None,
-    shrinking=True, tol=0.001, verbose=False)
+  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
+      decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
+      max_iter=-1, probability=False, random_state=None, shrinking=True,
+      tol=0.001, verbose=False)
 
   >>> import pickle
   >>> s = pickle.dumps(clf)
diff --git a/doc/modules/multiclass.rst b/doc/modules/multiclass.rst
index 98b5fc0061ef..bb32592f5848 100644
--- a/doc/modules/multiclass.rst
+++ b/doc/modules/multiclass.rst
@@ -105,6 +105,8 @@ format.
          [1, 1, 1, 1, 1],
          [1, 1, 1, 0, 0]])
 
+.. _ovr_classification:
+
 One-Vs-The-Rest
 ===============
 
@@ -155,6 +157,7 @@ To use this feature, feed the classifier an indicator matrix, in which cell
 
     * :ref:`example_plot_multilabel.py`
 
+.. _ovo_classification:
 
 One-Vs-One
 ==========
@@ -199,6 +202,7 @@ Below is an example of multiclass learning using OvO::
     .. [1] "Pattern Recognition and Machine Learning. Springer",
         Christopher M. Bishop, page 183, (First Edition)
 
+.. _ecoc:
 
 Error-Correcting Output-Codes
 =============================
diff --git a/doc/modules/naive_bayes.rst b/doc/modules/naive_bayes.rst
index 0e60479eddbf..e0c2ea63fe81 100644
--- a/doc/modules/naive_bayes.rst
+++ b/doc/modules/naive_bayes.rst
@@ -74,6 +74,7 @@ it is known to be a bad estimator, so the probability outputs from
    <http://www.cs.unb.ca/profs/hzhang/publications/FLAIRS04ZhangH.pdf>`_
    Proc. FLAIRS.
 
+.. _gaussian_naive_bayes:
 
 Gaussian Naive Bayes
 --------------------
diff --git a/doc/modules/neighbors.rst b/doc/modules/neighbors.rst
index be1a1aeaa40b..dae38b2ebb8a 100644
--- a/doc/modules/neighbors.rst
+++ b/doc/modules/neighbors.rst
@@ -456,6 +456,7 @@ leaf nodes.  The level of this switch can be specified with the parameter
 
 ``leaf_size`` is not referenced for brute force queries.
 
+.. _nearest_centroid_classifier:
 
 Nearest Centroid Classifier
 ===========================
@@ -511,6 +512,8 @@ the model from 0.81 to 0.82.
   * :ref:`example_neighbors_plot_nearest_centroid.py`: an example of
     classification using nearest centroid with different shrink thresholds.
 
+.. _approximate_nearest_neighbors:
+
 Approximate Nearest Neighbors
 =============================
 
diff --git a/doc/modules/pipeline.rst b/doc/modules/pipeline.rst
index 61a0e318da5b..9c8e83030e01 100644
--- a/doc/modules/pipeline.rst
+++ b/doc/modules/pipeline.rst
@@ -42,9 +42,9 @@ is an estimator object::
     >>> clf # doctest: +NORMALIZE_WHITESPACE
     Pipeline(steps=[('reduce_dim', PCA(copy=True, n_components=None,
         whiten=False)), ('svm', SVC(C=1.0, cache_size=200, class_weight=None,
-        coef0=0.0, degree=3, gamma=0.0, kernel='rbf', max_iter=-1,
-        probability=False, random_state=None, shrinking=True, tol=0.001,
-        verbose=False))])
+        coef0=0.0, decision_function_shape=None, degree=3, gamma='auto',
+        kernel='rbf', max_iter=-1, probability=False, random_state=None,
+        shrinking=True, tol=0.001, verbose=False))])
 
 The utility function :func:`make_pipeline` is a shorthand
 for constructing pipelines;
@@ -76,9 +76,9 @@ Parameters of the estimators in the pipeline can be accessed using the
     >>> clf.set_params(svm__C=10) # doctest: +NORMALIZE_WHITESPACE
     Pipeline(steps=[('reduce_dim', PCA(copy=True, n_components=None,
         whiten=False)), ('svm', SVC(C=10, cache_size=200, class_weight=None,
-        coef0=0.0, degree=3, gamma=0.0, kernel='rbf', max_iter=-1,
-        probability=False, random_state=None, shrinking=True, tol=0.001,
-        verbose=False))])
+        coef0=0.0, decision_function_shape=None, degree=3, gamma='auto',
+        kernel='rbf', max_iter=-1, probability=False, random_state=None,
+        shrinking=True, tol=0.001, verbose=False))])
 
 This is particularly important for doing grid searches::
 
diff --git a/doc/modules/preprocessing.rst b/doc/modules/preprocessing.rst
index af325b5f02b2..aeb510e0d989 100644
--- a/doc/modules/preprocessing.rst
+++ b/doc/modules/preprocessing.rst
@@ -196,6 +196,7 @@ data.
     This is very useful for scaling the target / response variables used
     for regression.
 
+.. _kernel_centering:
 
 Centering kernel matrices
 -------------------------
@@ -206,6 +207,7 @@ a :class:`KernelCenterer` can transform the kernel matrix
 so that it contains inner products in the feature space
 defined by :math:`phi` followed by removal of the mean in that space.
 
+.. _preprocessing_normalization:
 
 Normalization
 =============
@@ -267,6 +269,7 @@ The normalizer instance can then be used on sample vectors as any transformer::
   efficient Cython routines. To avoid unnecessary memory copies, it is
   recommended to choose the CSR representation upstream.
 
+.. _preprocessing_binarization:
 
 Binarization
 ============
diff --git a/doc/modules/svm.rst b/doc/modules/svm.rst
index a46871f49e2a..90cd5d09aa5e 100644
--- a/doc/modules/svm.rst
+++ b/doc/modules/svm.rst
@@ -76,9 +76,10 @@ n_features]`` holding the training samples, and an array y of class labels
     >>> y = [0, 1]
     >>> clf = svm.SVC()
     >>> clf.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE
-    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
-    gamma=0.0, kernel='rbf', max_iter=-1, probability=False, random_state=None,
-    shrinking=True, tol=0.001, verbose=False)
+    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
+        decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
+        max_iter=-1, probability=False, random_state=None, shrinking=True,
+        tol=0.001, verbose=False)
 
 After being fitted, the model can then be used to predict new values::
 
@@ -109,18 +110,27 @@ Multi-class classification
 :class:`SVC` and :class:`NuSVC` implement the "one-against-one"
 approach (Knerr et al., 1990) for multi- class classification. If
 ``n_class`` is the number of classes, then ``n_class * (n_class - 1) / 2``
-classifiers are constructed and each one trains data from two classes::
+classifiers are constructed and each one trains data from two classes.
+To provide a consistent interface with other classifiers, the
+``decision_function_shape`` option allows to aggregate the results of the
+"one-against-one" classifiers to a decision function of shape ``(n_samples,
+n_classes)``::
 
     >>> X = [[0], [1], [2], [3]]
     >>> Y = [0, 1, 2, 3]
-    >>> clf = svm.SVC()
+    >>> clf = svm.SVC(decision_function_shape='ovo')
     >>> clf.fit(X, Y) # doctest: +NORMALIZE_WHITESPACE
-    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
-    gamma=0.0, kernel='rbf', max_iter=-1, probability=False, random_state=None,
-    shrinking=True, tol=0.001, verbose=False)
+    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
+        decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',
+        max_iter=-1, probability=False, random_state=None, shrinking=True,
+        tol=0.001, verbose=False)
     >>> dec = clf.decision_function([[1]])
     >>> dec.shape[1] # 4 classes: 4*3/2 = 6
     6
+    >>> clf.decision_function_shape = "ovr"
+    >>> dec = clf.decision_function([[1]])
+    >>> dec.shape[1] # 4 classes
+    4
 
 On the other hand, :class:`LinearSVC` implements "one-vs-the-rest"
 multi-class strategy, thus training n_class models. If there are only
@@ -304,7 +314,7 @@ floating point values instead of integer values::
     >>> y = [0.5, 2.5]
     >>> clf = svm.SVR()
     >>> clf.fit(X, y) # doctest: +NORMALIZE_WHITESPACE
-    SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.0,
+    SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',
         kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)
     >>> clf.predict([[1, 1]])
     array([ 1.5])
@@ -405,7 +415,7 @@ Tips on Practical Use
     approximates the fraction of training errors and support vectors.
 
   * In :class:`SVC`, if data for classification are unbalanced (e.g. many
-    positive and few negative), set ``class_weight='auto'`` and/or try
+    positive and few negative), set ``class_weight='balanced'`` and/or try
     different penalty parameters ``C``.
 
   * The underlying :class:`LinearSVC` implementation uses a random
@@ -503,9 +513,10 @@ test vectors must be provided.
     >>> # linear kernel computation
     >>> gram = np.dot(X, X.T)
     >>> clf.fit(gram, y) # doctest: +NORMALIZE_WHITESPACE
-    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
-    gamma=0.0, kernel='precomputed', max_iter=-1, probability=False,
-    random_state=None, shrinking=True, tol=0.001, verbose=False)
+    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
+        decision_function_shape=None, degree=3, gamma='auto',
+        kernel='precomputed', max_iter=-1, probability=False,
+        random_state=None, shrinking=True, tol=0.001, verbose=False)
     >>> # predict on training examples
     >>> clf.predict(gram)
     array([0, 1])
diff --git a/doc/tutorial/basic/tutorial.rst b/doc/tutorial/basic/tutorial.rst
index e8b1e02e19b1..21aa592ad737 100644
--- a/doc/tutorial/basic/tutorial.rst
+++ b/doc/tutorial/basic/tutorial.rst
@@ -176,9 +176,10 @@ which produces a new array that contains all but
 the last entry of ``digits.data``::
 
   >>> clf.fit(digits.data[:-1], digits.target[:-1])  # doctest: +NORMALIZE_WHITESPACE
-  SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
-    gamma=0.001, kernel='rbf', max_iter=-1, probability=False,
-    random_state=None, shrinking=True, tol=0.001, verbose=False)
+  SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0,
+    decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',
+    max_iter=-1, probability=False, random_state=None, shrinking=True,
+    tol=0.001, verbose=False)
 
 Now you can predict new values, in particular, we can ask to the
 classifier what is the digit of our last image in the ``digits`` dataset,
@@ -214,9 +215,10 @@ persistence model, namely `pickle <http://docs.python.org/library/pickle.html>`_
   >>> iris = datasets.load_iris()
   >>> X, y = iris.data, iris.target
   >>> clf.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE
-  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
-    kernel='rbf', max_iter=-1, probability=False, random_state=None,
-    shrinking=True, tol=0.001, verbose=False)
+  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
+    decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
+    max_iter=-1, probability=False, random_state=None, shrinking=True,
+    tol=0.001, verbose=False)
 
   >>> import pickle
   >>> s = pickle.dumps(clf)
@@ -284,21 +286,22 @@ Regression targets are cast to ``float64``, classification targets are
 maintained::
     >>> from sklearn import datasets
     >>> from sklearn.svm import SVC
-
     >>> iris = datasets.load_iris()
     >>> clf = SVC()
-    >>> clf.fit(iris.data, iris.target)
-    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
-      kernel='rbf', max_iter=-1, probability=False, random_state=None,
-      shrinking=True, tol=0.001, verbose=False)
+    >>> clf.fit(iris.data, iris.target)  # doctest: +NORMALIZE_WHITESPACE
+    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
+      decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
+      max_iter=-1, probability=False, random_state=None, shrinking=True,
+      tol=0.001, verbose=False)
 
     >>> list(clf.predict(iris.data[:3]))
     [0, 0, 0]
 
-    >>> clf.fit(iris.data, iris.target_names[iris.target])
-    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
-      kernel='rbf', max_iter=-1, probability=False, random_state=None,
-      shrinking=True, tol=0.001, verbose=False)
+    >>> clf.fit(iris.data, iris.target_names[iris.target])  # doctest: +NORMALIZE_WHITESPACE
+    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
+      decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
+      max_iter=-1, probability=False, random_state=None, shrinking=True,
+      tol=0.001, verbose=False)
 
     >>> list(clf.predict(iris.data[:3]))  # doctest: +NORMALIZE_WHITESPACE
     ['setosa', 'setosa', 'setosa']
@@ -324,17 +327,19 @@ more than once will overwrite what was learned by any previous ``fit()``::
   >>> X_test = rng.rand(5, 10)
 
   >>> clf = SVC()
-  >>> clf.set_params(kernel='linear').fit(X, y)
-  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
-    kernel='linear', max_iter=-1, probability=False, random_state=None,
-    shrinking=True, tol=0.001, verbose=False)
+  >>> clf.set_params(kernel='linear').fit(X, y)  # doctest: +NORMALIZE_WHITESPACE
+  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
+    decision_function_shape=None, degree=3, gamma='auto', kernel='linear',
+    max_iter=-1, probability=False, random_state=None, shrinking=True,
+    tol=0.001, verbose=False)
   >>> clf.predict(X_test)
   array([1, 0, 1, 1, 0])
 
-  >>> clf.set_params(kernel='rbf').fit(X, y)
-  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
-    kernel='rbf', max_iter=-1, probability=False, random_state=None,
-    shrinking=True, tol=0.001, verbose=False)
+  >>> clf.set_params(kernel='rbf').fit(X, y)  # doctest: +NORMALIZE_WHITESPACE
+  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
+    decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
+    max_iter=-1, probability=False, random_state=None, shrinking=True,
+    tol=0.001, verbose=False)
   >>> clf.predict(X_test)
   array([0, 0, 0, 1, 0])
 
diff --git a/doc/tutorial/statistical_inference/supervised_learning.rst b/doc/tutorial/statistical_inference/supervised_learning.rst
index 901d0409fa6b..17a88be17c38 100644
--- a/doc/tutorial/statistical_inference/supervised_learning.rst
+++ b/doc/tutorial/statistical_inference/supervised_learning.rst
@@ -453,9 +453,10 @@ classification --:class:`SVC` (Support Vector Classification).
     >>> from sklearn import svm
     >>> svc = svm.SVC(kernel='linear')
     >>> svc.fit(iris_X_train, iris_y_train)    # doctest: +NORMALIZE_WHITESPACE
-    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
-      kernel='linear', max_iter=-1, probability=False, random_state=None,
-      shrinking=True, tol=0.001, verbose=False)
+    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
+        decision_function_shape=None, degree=3, gamma='auto', kernel='linear',
+        max_iter=-1, probability=False, random_state=None, shrinking=True,
+        tol=0.001, verbose=False)
 
 
 .. warning:: **Normalizing data**
diff --git a/doc/whats_new.rst b/doc/whats_new.rst
index 9c27ced0ba11..9add06a83473 100644
--- a/doc/whats_new.rst
+++ b/doc/whats_new.rst
@@ -56,6 +56,22 @@ Enhancements
      :class:`linear_model.LogisticRegression`, by avoiding loss computation.
      By `Mathieu Blondel`_ and `Tom Dupre la Tour`_.
 
+   - The ``class_weight="auto"`` heuristic in classifiers supporting 
+     ``class_weight`` was deprecated and replaced by the ``class_weight="balanced"``
+     option, which has a simpler forumlar and interpretation.
+     By Hanna Wallach and `Andreas Müller`_.
+
+   - Added backlinks from the API reference pages to the user guide. By
+     `Andreas Müller`_.
+
+   - The ``labels`` parameter to :func:`sklearn.metrics.f1_score`,
+     :func:`sklearn.metrics.fbeta_score`,
+     :func:`sklearn.metrics.recall_score` and
+     :func:`sklearn.metrics.precision_score` has been extended.
+     It is now possible to ignore one or more labels, such as where
+     a multiclass problem has a majority class to ignore. By `Joel Nothman`_.
+
+
 Bug fixes
 .........
 
@@ -75,6 +91,11 @@ API changes summary
       for retrieving the leaf indices samples are predicted as. By
       `Daniel Galvez`_ and `Gilles Louppe`_.
 
+    - :class:`svm.SVC`` and :class:`svm.NuSVC` now have an ``decision_function_shape``
+      parameter to make their decision function of shape ``(n_samples, n_classes)``
+      by setting ``decision_function_shape='ovr'``. This will be the default behavior
+      starting in 0.19. By `Andreas Müller`_.
+
 .. _changes_0_1_16:
 
 0.16.1
@@ -339,6 +360,7 @@ Enhancements
    - :class:`svm.SVC` fitted on sparse input now implements ``decision_function``.
      By `Rob Zinkov`_ and `Andreas Müller`_.
 
+
 Documentation improvements
 ..........................
 
@@ -462,7 +484,7 @@ Bug fixes
       in GMM. By `Alexis Mignon`_.
 
     - Fixed a error in the computation of conditional probabilities in
-      :class:`naive_bayes.BernoulliNB`. By `Hanna Wallach`_.
+      :class:`naive_bayes.BernoulliNB`. By Hanna Wallach.
 
     - Make the method ``radius_neighbors`` of
       :class:`neighbors.NearestNeighbors` return the samples lying on the
diff --git a/examples/applications/face_recognition.py b/examples/applications/face_recognition.py
index c261b979f901..2422ec67617e 100644
--- a/examples/applications/face_recognition.py
+++ b/examples/applications/face_recognition.py
@@ -105,7 +105,7 @@
 t0 = time()
 param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],
               'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }
-clf = GridSearchCV(SVC(kernel='rbf', class_weight='auto'), param_grid)
+clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)
 clf = clf.fit(X_train_pca, y_train)
 print("done in %0.3fs" % (time() - t0))
 print("Best estimator found by grid search:")
diff --git a/examples/preprocessing/README.txt b/examples/preprocessing/README.txt
index c871e577e36b..3a7784cf4385 100644
--- a/examples/preprocessing/README.txt
+++ b/examples/preprocessing/README.txt
@@ -1,6 +1,6 @@
 .. _preprocessing_examples:
 
 Preprocessing
-------------
+-------------
 
 Examples concerning the :mod:`sklearn.preprocessing` module.
diff --git a/sklearn/base.py b/sklearn/base.py
index 8d1198fb6cd0..f6413df865f1 100644
--- a/sklearn/base.py
+++ b/sklearn/base.py
@@ -11,7 +11,11 @@
 from .externals import six
 
 
-###############################################################################
+class ChangedBehaviorWarning(UserWarning):
+    pass
+
+
+##############################################################################
 def clone(estimator, safe=True):
     """Constructs a new estimator with the same parameters.
 
diff --git a/sklearn/calibration.py b/sklearn/calibration.py
index 884455a28695..6cf2e22f9adb 100644
--- a/sklearn/calibration.py
+++ b/sklearn/calibration.py
@@ -22,7 +22,7 @@
 from .utils.validation import check_is_fitted
 from .isotonic import IsotonicRegression
 from .svm import LinearSVC
-from .cross_validation import _check_cv
+from .cross_validation import check_cv
 from .metrics.classification import _check_binary_probabilistic_predictions
 
 
@@ -37,6 +37,8 @@ class CalibratedClassifierCV(BaseEstimator, ClassifierMixin):
     fitted already and all data is used for calibration. Note that
     data for fitting the classifier and for calibrating it must be disjpint.
 
+    Read more in the :ref:`User Guide <calibration>`.
+
     Parameters
     ----------
     base_estimator : instance BaseEstimator
@@ -139,7 +141,7 @@ def fit(self, X, y, sample_weight=None):
                 calibrated_classifier.fit(X, y)
             self.calibrated_classifiers_.append(calibrated_classifier)
         else:
-            cv = _check_cv(self.cv, X, y, classifier=True)
+            cv = check_cv(self.cv, X, y, classifier=True)
             arg_names = inspect.getargspec(base_estimator.fit)[0]
             estimator_name = type(base_estimator).__name__
             if (sample_weight is not None
@@ -478,6 +480,8 @@ def predict(self, T):
 def calibration_curve(y_true, y_prob, normalize=False, n_bins=5):
     """Compute true and predicted probabilities for a calibration curve.
 
+    Read more in the :ref:`User Guide <calibration>`.
+
     Parameters
     ----------
     y_true : array, shape (n_samples,)
diff --git a/sklearn/cluster/affinity_propagation_.py b/sklearn/cluster/affinity_propagation_.py
index 3f3745b65710..6f3c60780654 100644
--- a/sklearn/cluster/affinity_propagation_.py
+++ b/sklearn/cluster/affinity_propagation_.py
@@ -21,6 +21,8 @@ def affinity_propagation(S, preference=None, convergence_iter=15, max_iter=200,
                          return_n_iter=False):
     """Perform Affinity Propagation Clustering of data
 
+    Read more in the :ref:`User Guide <affinity_propagation>`.
+
     Parameters
     ----------
 
@@ -191,6 +193,8 @@ def affinity_propagation(S, preference=None, convergence_iter=15, max_iter=200,
 class AffinityPropagation(BaseEstimator, ClusterMixin):
     """Perform Affinity Propagation Clustering of data.
 
+    Read more in the :ref:`User Guide <affinity_propagation>`.
+
     Parameters
     ----------
     damping : float, optional, default: 0.5
diff --git a/sklearn/cluster/bicluster.py b/sklearn/cluster/bicluster.py
index 7a32764a6cd1..16a45c94715f 100644
--- a/sklearn/cluster/bicluster.py
+++ b/sklearn/cluster/bicluster.py
@@ -182,6 +182,8 @@ class SpectralCoclustering(BaseSpectral):
 
     Supports sparse matrices, as long as they are nonnegative.
 
+    Read more in the :ref:`User Guide <spectral_coclustering>`.
+
     Parameters
     ----------
     n_clusters : integer, optional, default: 3
@@ -294,6 +296,8 @@ class SpectralBiclustering(BaseSpectral):
     biclusters. The outer product of the corresponding row and column
     label vectors gives this checkerboard structure.
 
+    Read more in the :ref:`User Guide <spectral_biclustering>`.
+
     Parameters
     ----------
     n_clusters : integer or tuple (n_row_clusters, n_column_clusters)
diff --git a/sklearn/cluster/birch.py b/sklearn/cluster/birch.py
index 1da1a747065f..6a4d2b14245e 100644
--- a/sklearn/cluster/birch.py
+++ b/sklearn/cluster/birch.py
@@ -328,6 +328,8 @@ class Birch(BaseEstimator, TransformerMixin, ClusterMixin):
     centroid closest to the new sample. This is done recursively till it
     ends up at the subcluster of the leaf of the tree has the closest centroid.
 
+    Read more in the :ref:`User Guide <birch>`.
+
     Parameters
     ----------
     threshold : float, default 0.5
diff --git a/sklearn/cluster/dbscan_.py b/sklearn/cluster/dbscan_.py
index fb904a18e48b..8a26bbdba7fc 100644
--- a/sklearn/cluster/dbscan_.py
+++ b/sklearn/cluster/dbscan_.py
@@ -26,6 +26,8 @@ def dbscan(X, eps=0.5, min_samples=5, metric='minkowski',
            random_state=None):
     """Perform DBSCAN clustering from vector array or distance matrix.
 
+    Read more in the :ref:`User Guide <dbscan>`.
+
     Parameters
     ----------
     X : array or sparse (CSR) matrix of shape (n_samples, n_features), or \
@@ -148,6 +150,8 @@ class DBSCAN(BaseEstimator, ClusterMixin):
     Finds core samples of high density and expands clusters from them.
     Good for data which contains clusters of similar density.
 
+    Read more in the :ref:`User Guide <dbscan>`.
+
     Parameters
     ----------
     eps : float, optional
diff --git a/sklearn/cluster/hierarchical.py b/sklearn/cluster/hierarchical.py
index bb3f11071387..e580d4e7ae2b 100644
--- a/sklearn/cluster/hierarchical.py
+++ b/sklearn/cluster/hierarchical.py
@@ -98,6 +98,8 @@ def ward_tree(X, connectivity=None, n_components=None, n_clusters=None,
     This is the structured version, that takes into account some topological
     structure between samples.
 
+    Read more in the :ref:`User Guide <hierarchical_clustering>`.
+
     Parameters
     ----------
     X : array, shape (n_samples, n_features)
@@ -311,6 +313,8 @@ def linkage_tree(X, connectivity=None, n_components=None,
     This is the structured version, that takes into account some topological
     structure between samples.
 
+    Read more in the :ref:`User Guide <hierarchical_clustering>`.
+
     Parameters
     ----------
     X : array, shape (n_samples, n_features)
@@ -607,6 +611,8 @@ class AgglomerativeClustering(BaseEstimator, ClusterMixin):
     Recursively merges the pair of clusters that minimally increases
     a given linkage distance.
 
+    Read more in the :ref:`User Guide <hierarchical_clustering>`.
+
     Parameters
     ----------
     n_clusters : int, default=2
@@ -776,6 +782,8 @@ class FeatureAgglomeration(AgglomerativeClustering, AgglomerationTransform):
     Similar to AgglomerativeClustering, but recursively merges features
     instead of samples.
 
+    Read more in the :ref:`User Guide <hierarchical_clustering>`.
+
     Parameters
     ----------
     n_clusters : int, default 2
diff --git a/sklearn/cluster/k_means_.py b/sklearn/cluster/k_means_.py
index ee450ed302ca..7439f592a427 100644
--- a/sklearn/cluster/k_means_.py
+++ b/sklearn/cluster/k_means_.py
@@ -155,6 +155,8 @@ def k_means(X, n_clusters, init='k-means++', precompute_distances='auto',
             return_n_iter=False):
     """K-means clustering algorithm.
 
+    Read more in the :ref:`User Guide <k_means>`.
+
     Parameters
     ----------
     X : array-like or sparse matrix, shape (n_samples, n_features)
@@ -628,6 +630,8 @@ def _init_centroids(X, k, init, random_state=None, x_squared_norms=None,
 class KMeans(BaseEstimator, ClusterMixin, TransformerMixin):
     """K-Means clustering
 
+    Read more in the :ref:`User Guide <k_means>`.
+
     Parameters
     ----------
 
diff --git a/sklearn/cluster/mean_shift_.py b/sklearn/cluster/mean_shift_.py
index 0c2ebb03e75e..bca6bb774ea8 100644
--- a/sklearn/cluster/mean_shift_.py
+++ b/sklearn/cluster/mean_shift_.py
@@ -71,6 +71,8 @@ def mean_shift(X, bandwidth=None, seeds=None, bin_seeding=False,
                max_iterations=None):
     """Perform mean shift clustering of data using a flat kernel.
 
+    Read more in the :ref:`User Guide <mean_shift>`.
+
     Parameters
     ----------
 
@@ -260,6 +262,8 @@ class MeanShift(BaseEstimator, ClusterMixin):
 
     Seeding is performed using a binning technique for scalability.
 
+    Read more in the :ref:`User Guide <mean_shift>`.
+
     Parameters
     ----------
     bandwidth : float, optional
diff --git a/sklearn/cluster/spectral.py b/sklearn/cluster/spectral.py
index 39475929ebd0..4c44ea828fed 100644
--- a/sklearn/cluster/spectral.py
+++ b/sklearn/cluster/spectral.py
@@ -170,6 +170,8 @@ def spectral_clustering(affinity, n_clusters=8, n_components=None,
     If affinity is the adjacency matrix of a graph, this method can be
     used to find normalized graph cuts.
 
+    Read more in the :ref:`User Guide <spectral_clustering>`.
+
     Parameters
     -----------
     affinity : array-like or sparse matrix, shape: (n_samples, n_samples)
@@ -287,6 +289,8 @@ class SpectralClustering(BaseEstimator, ClusterMixin):
     Alternatively, using ``precomputed``, a user-provided affinity
     matrix can be used.
 
+    Read more in the :ref:`User Guide <spectral_clustering>`.
+
     Parameters
     -----------
     n_clusters : integer, optional
diff --git a/sklearn/cluster/tests/test_hierarchical.py b/sklearn/cluster/tests/test_hierarchical.py
index a7e7507704f7..10204ff779d0 100644
--- a/sklearn/cluster/tests/test_hierarchical.py
+++ b/sklearn/cluster/tests/test_hierarchical.py
@@ -496,6 +496,7 @@ def test_n_components():
     for linkage_func in _TREE_BUILDERS.values():
         assert_equal(ignore_warnings(linkage_func)(X, connectivity)[1], 5)
 
+
 def test_agg_n_clusters():
     # Test that an error is raised when n_clusters <= 0
 
@@ -506,7 +507,3 @@ def test_agg_n_clusters():
         msg = ("n_clusters should be an integer greater than 0."
                " %s was provided." % str(agc.n_clusters))
         assert_raise_message(ValueError, msg, agc.fit, X)
-
-if __name__ == '__main__':
-    import nose
-    nose.run(argv=['', __file__])
diff --git a/sklearn/covariance/empirical_covariance_.py b/sklearn/covariance/empirical_covariance_.py
index 9259c3bd111a..f5f4cc10661a 100644
--- a/sklearn/covariance/empirical_covariance_.py
+++ b/sklearn/covariance/empirical_covariance_.py
@@ -85,6 +85,8 @@ def empirical_covariance(X, assume_centered=False):
 class EmpiricalCovariance(BaseEstimator):
     """Maximum likelihood covariance estimator
 
+    Read more in the :ref:`User Guide <covariance>`.
+
     Parameters
     ----------
     store_precision : bool
diff --git a/sklearn/covariance/graph_lasso_.py b/sklearn/covariance/graph_lasso_.py
index 191de3d3e7da..fd94d360013c 100644
--- a/sklearn/covariance/graph_lasso_.py
+++ b/sklearn/covariance/graph_lasso_.py
@@ -21,7 +21,7 @@
 from ..utils.validation import check_random_state, check_array
 from ..linear_model import lars_path
 from ..linear_model import cd_fast
-from ..cross_validation import _check_cv as check_cv, cross_val_score
+from ..cross_validation import check_cv, cross_val_score
 from ..externals.joblib import Parallel, delayed
 import collections
 
@@ -84,6 +84,8 @@ def graph_lasso(emp_cov, alpha, cov_init=None, mode='cd', tol=1e-4,
                 return_n_iter=False):
     """l1-penalized covariance estimator
 
+    Read more in the :ref:`User Guide <sparse_inverse_covariance>`.
+
     Parameters
     ----------
     emp_cov : 2D ndarray, shape (n_features, n_features)
@@ -267,6 +269,8 @@ def graph_lasso(emp_cov, alpha, cov_init=None, mode='cd', tol=1e-4,
 class GraphLasso(EmpiricalCovariance):
     """Sparse inverse covariance estimation with an l1-penalized estimator.
 
+    Read more in the :ref:`User Guide <sparse_inverse_covariance>`.
+
     Parameters
     ----------
     alpha : positive float, default 0.01
@@ -349,6 +353,8 @@ def graph_lasso_path(X, alphas, cov_init=None, X_test=None, mode='cd',
                      tol=1e-4, enet_tol=1e-4, max_iter=100, verbose=False):
     """l1-penalized covariance estimator along a path of decreasing alphas
 
+    Read more in the :ref:`User Guide <sparse_inverse_covariance>`.
+
     Parameters
     ----------
     X : 2D ndarray, shape (n_samples, n_features)
@@ -440,6 +446,8 @@ def graph_lasso_path(X, alphas, cov_init=None, X_test=None, mode='cd',
 class GraphLassoCV(GraphLasso):
     """Sparse inverse covariance w/ cross-validated choice of the l1 penalty
 
+    Read more in the :ref:`User Guide <sparse_inverse_covariance>`.
+
     Parameters
     ----------
     alphas : integer, or list positive float, optional
@@ -488,7 +496,6 @@ class GraphLassoCV(GraphLasso):
         zero.
         If False, data are centered before computation.
 
-
     Attributes
     ----------
     covariance_ : numpy.ndarray, shape (n_features, n_features)
diff --git a/sklearn/covariance/outlier_detection.py b/sklearn/covariance/outlier_detection.py
index 6015e09da9dc..814cc5d4e543 100644
--- a/sklearn/covariance/outlier_detection.py
+++ b/sklearn/covariance/outlier_detection.py
@@ -112,6 +112,8 @@ def threshold(self):
 class EllipticEnvelope(ClassifierMixin, OutlierDetectionMixin, MinCovDet):
     """An object for detecting outliers in a Gaussian distributed dataset.
 
+    Read more in the :ref:`User Guide <outlier_detection>`.
+
     Attributes
     ----------
     `contamination` : float, 0. < contamination < 0.5
diff --git a/sklearn/covariance/robust_covariance.py b/sklearn/covariance/robust_covariance.py
index afacb1fa4a69..9de6b90cd39e 100644
--- a/sklearn/covariance/robust_covariance.py
+++ b/sklearn/covariance/robust_covariance.py
@@ -298,6 +298,8 @@ def fast_mcd(X, support_fraction=None,
              random_state=None):
     """Estimates the Minimum Covariance Determinant matrix.
 
+    Read more in the :ref:`User Guide <robust_covariance>`.
+
     Parameters
     ----------
     X : array-like, shape (n_samples, n_features)
@@ -511,6 +513,8 @@ class MinCovDet(EmpiricalCovariance):
     One should consider projection pursuit methods to deal with multi-modal
     datasets.
 
+    Read more in the :ref:`User Guide <robust_covariance>`.
+
     Parameters
     ----------
     store_precision : bool
diff --git a/sklearn/covariance/shrunk_covariance_.py b/sklearn/covariance/shrunk_covariance_.py
index 477f9ce5c4ce..ae60ac011aba 100644
--- a/sklearn/covariance/shrunk_covariance_.py
+++ b/sklearn/covariance/shrunk_covariance_.py
@@ -27,6 +27,8 @@
 def shrunk_covariance(emp_cov, shrinkage=0.1):
     """Calculates a covariance matrix shrunk on the diagonal
 
+    Read more in the :ref:`User Guide <shrunk_covariance>`.
+
     Parameters
     ----------
     emp_cov : array-like, shape (n_features, n_features)
@@ -64,6 +66,8 @@ def shrunk_covariance(emp_cov, shrinkage=0.1):
 class ShrunkCovariance(EmpiricalCovariance):
     """Covariance estimator with shrinkage
 
+    Read more in the :ref:`User Guide <shrunk_covariance>`.
+
     Parameters
     ----------
     store_precision : boolean, default True
@@ -146,6 +150,8 @@ def fit(self, X, y=None):
 def ledoit_wolf_shrinkage(X, assume_centered=False, block_size=1000):
     """Estimates the shrunk Ledoit-Wolf covariance matrix.
 
+    Read more in the :ref:`User Guide <shrunk_covariance>`.
+
     Parameters
     ----------
     X : array-like, shape (n_samples, n_features)
@@ -235,6 +241,8 @@ def ledoit_wolf_shrinkage(X, assume_centered=False, block_size=1000):
 def ledoit_wolf(X, assume_centered=False, block_size=1000):
     """Estimates the shrunk Ledoit-Wolf covariance matrix.
 
+    Read more in the :ref:`User Guide <shrunk_covariance>`.
+
     Parameters
     ----------
     X : array-like, shape (n_samples, n_features)
@@ -304,6 +312,8 @@ class LedoitWolf(EmpiricalCovariance):
     Covariance Matrices", Ledoit and Wolf, Journal of Multivariate
     Analysis, Volume 88, Issue 2, February 2004, pages 365-411.
 
+    Read more in the :ref:`User Guide <shrunk_covariance>`.
+
     Parameters
     ----------
     store_precision : bool, default=True
@@ -462,6 +472,8 @@ def oas(X, assume_centered=False):
 class OAS(EmpiricalCovariance):
     """Oracle Approximating Shrinkage Estimator
 
+    Read more in the :ref:`User Guide <shrunk_covariance>`.
+
     OAS is a particular form of shrinkage described in
     "Shrinkage Algorithms for MMSE Covariance Estimation"
     Chen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.
diff --git a/sklearn/cross_decomposition/cca_.py b/sklearn/cross_decomposition/cca_.py
index 19ee9cefb9a4..70cd58a2009a 100644
--- a/sklearn/cross_decomposition/cca_.py
+++ b/sklearn/cross_decomposition/cca_.py
@@ -8,6 +8,8 @@ class CCA(_PLS):
 
     CCA inherits from PLS with mode="B" and deflation_mode="canonical".
 
+    Read more in the :ref:`User Guide <cross_decomposition>`.
+
     Parameters
     ----------
     n_components : int, (default 2).
diff --git a/sklearn/cross_decomposition/pls_.py b/sklearn/cross_decomposition/pls_.py
index ee2543d910d5..4c1141697807 100644
--- a/sklearn/cross_decomposition/pls_.py
+++ b/sklearn/cross_decomposition/pls_.py
@@ -448,6 +448,8 @@ class PLSRegression(_PLS):
     This class inherits from _PLS with mode="A", deflation_mode="regression",
     norm_y_weights=False and algorithm="nipals".
 
+    Read more in the :ref:`User Guide <cross_decomposition>`.
+
     Parameters
     ----------
     n_components : int, (default 2)
@@ -569,6 +571,8 @@ class PLSCanonical(_PLS):
     norm_y_weights=True and algorithm="nipals", but svd should provide similar
     results up to numerical errors.
 
+    Read more in the :ref:`User Guide <cross_decomposition>`.
+
     Parameters
     ----------
     scale : boolean, scale data? (default True)
@@ -685,6 +689,8 @@ class PLSSVD(BaseEstimator, TransformerMixin):
     Simply perform a svd on the crosscovariance matrix: X'Y
     There are no iterative deflation here.
 
+    Read more in the :ref:`User Guide <cross_decomposition>`.
+
     Parameters
     ----------
     n_components : int, default 2
diff --git a/sklearn/cross_validation.py b/sklearn/cross_validation.py
index 030d700642e6..fa7c7f210bc0 100644
--- a/sklearn/cross_validation.py
+++ b/sklearn/cross_validation.py
@@ -107,6 +107,8 @@ class LeaveOneOut(_PartitionIterator):
     For large datasets one should favor KFold, StratifiedKFold or
     ShuffleSplit.
 
+    Read more in the :ref:`User Guide <cross_validation>`.
+
     Parameters
     ----------
     n : int
@@ -166,6 +168,8 @@ class LeavePOut(_PartitionIterator):
     number of samples this cross validation method can be very costly. For
     large datasets one should favor KFold, StratifiedKFold or ShuffleSplit.
 
+    Read more in the :ref:`User Guide <cross_validation>`.
+
     Parameters
     ----------
     n : int
@@ -254,6 +258,8 @@ class KFold(_BaseKFold):
     Each fold is then used a validation set once while the k - 1 remaining
     fold form the training set.
 
+    Read more in the :ref:`User Guide <cross_validation>`.
+
     Parameters
     ----------
     n : int
@@ -341,6 +347,8 @@ class StratifiedKFold(_BaseKFold):
     returns stratified folds. The folds are made by preserving
     the percentage of samples for each class.
 
+    Read more in the :ref:`User Guide <cross_validation>`.
+
     Parameters
     ----------
     y : array-like, [n_samples]
@@ -455,6 +463,8 @@ class LeaveOneLabelOut(_PartitionIterator):
     For instance the labels could be the year of collection of the samples
     and thus allow for cross-validation against time-based splits.
 
+    Read more in the :ref:`User Guide <cross_validation>`.
+
     Parameters
     ----------
     labels : array-like of int with shape (n_samples,)
@@ -525,6 +535,8 @@ class LeavePLabelOut(_PartitionIterator):
     ``p`` different values of the labels while the latter uses samples
     all assigned the same labels.
 
+    Read more in the :ref:`User Guide <cross_validation>`.
+
     Parameters
     ----------
     labels : array-like of int with shape (n_samples,)
@@ -624,6 +636,8 @@ class ShuffleSplit(BaseShuffleSplit):
     do not guarantee that all folds will be different, although this is
     still very likely for sizeable datasets.
 
+    Read more in the :ref:`User Guide <cross_validation>`.
+
     Parameters
     ----------
     n : int
@@ -774,6 +788,8 @@ class StratifiedShuffleSplit(BaseShuffleSplit):
     do not guarantee that all folds will be different, although this is
     still very likely for sizeable datasets.
 
+    Read more in the :ref:`User Guide <cross_validation>`.
+
     Parameters
     ----------
     y : array, [n_samples]
@@ -898,6 +914,8 @@ class PredefinedSplit(_PartitionIterator):
     scheme. Each sample can be assigned to at most one test set fold, as
     specified by the user through the ``test_fold`` parameter.
 
+    Read more in the :ref:`User Guide <cross_validation>`.
+
     Parameters
     ----------
     test_fold : "array-like, shape (n_samples,)
@@ -959,6 +977,8 @@ def cross_val_predict(estimator, X, y=None, cv=None, n_jobs=1,
                       verbose=0, fit_params=None, pre_dispatch='2*n_jobs'):
     """Generate cross-validated estimates for each input data point
 
+    Read more in the :ref:`User Guide <cross_validation>`.
+
     Parameters
     ----------
     estimator : estimator object implementing 'fit' and 'predict'
@@ -1013,7 +1033,7 @@ def cross_val_predict(estimator, X, y=None, cv=None, n_jobs=1,
     """
     X, y = indexable(X, y)
 
-    cv = _check_cv(cv, X, y, classifier=is_classifier(estimator))
+    cv = check_cv(cv, X, y, classifier=is_classifier(estimator))
     # We clone the estimator to make sure that all the folds are
     # independent, and that it is pickle-able.
     parallel = Parallel(n_jobs=n_jobs, verbose=verbose,
@@ -1034,6 +1054,8 @@ def cross_val_predict(estimator, X, y=None, cv=None, n_jobs=1,
 def _fit_and_predict(estimator, X, y, train, test, verbose, fit_params):
     """Fit estimator and predict values for a given dataset split.
 
+    Read more in the :ref:`User Guide <cross_validation>`.
+
     Parameters
     ----------
     estimator : estimator object implementing 'fit' and 'predict'
@@ -1110,6 +1132,8 @@ def cross_val_score(estimator, X, y=None, scoring=None, cv=None, n_jobs=1,
                     verbose=0, fit_params=None, pre_dispatch='2*n_jobs'):
     """Evaluate a score by cross-validation
 
+    Read more in the :ref:`User Guide <cross_validation>`.
+
     Parameters
     ----------
     estimator : estimator object implementing 'fit'
@@ -1167,7 +1191,7 @@ def cross_val_score(estimator, X, y=None, scoring=None, cv=None, n_jobs=1,
     """
     X, y = indexable(X, y)
 
-    cv = _check_cv(cv, X, y, classifier=is_classifier(estimator))
+    cv = check_cv(cv, X, y, classifier=is_classifier(estimator))
     scorer = check_scoring(estimator, scoring=scoring)
     # We clone the estimator to make sure that all the folds are
     # independent, and that it is pickle-able.
@@ -1404,11 +1428,6 @@ def check_cv(cv, X=None, y=None, classifier=False):
         The return value is guaranteed to be a cv generator instance, whatever
         the input type.
     """
-    return _check_cv(cv, X=X, y=y, classifier=classifier)
-
-
-def _check_cv(cv, X=None, y=None, classifier=False):
-    # This exists for internal use while indices is being deprecated.
     is_sparse = sp.issparse(X)
     if cv is None:
         cv = 3
@@ -1432,6 +1451,8 @@ def permutation_test_score(estimator, X, y, cv=None,
                            random_state=0, verbose=0, scoring=None):
     """Evaluate the significance of a cross-validated score with permutations
 
+    Read more in the :ref:`User Guide <cross_validation>`.
+
     Parameters
     ----------
     estimator : estimator object implementing 'fit'
@@ -1497,7 +1518,7 @@ def permutation_test_score(estimator, X, y, cv=None,
 
     """
     X, y = indexable(X, y)
-    cv = _check_cv(cv, X, y, classifier=is_classifier(estimator))
+    cv = check_cv(cv, X, y, classifier=is_classifier(estimator))
     scorer = check_scoring(estimator, scoring=scoring)
     random_state = check_random_state(random_state)
 
@@ -1525,6 +1546,8 @@ def train_test_split(*arrays, **options):
     data into a single call for splitting (and optionally subsampling)
     data in a oneliner.
 
+    Read more in the :ref:`User Guide <cross_validation>`.
+
     Parameters
     ----------
     *arrays : sequence of arrays or scipy.sparse matrices with same shape[0]
diff --git a/sklearn/datasets/base.py b/sklearn/datasets/base.py
index a78803db1feb..b515db5ed3b9 100644
--- a/sklearn/datasets/base.py
+++ b/sklearn/datasets/base.py
@@ -129,6 +129,8 @@ def load_files(container_path, description=None, categories=None,
     Similar feature extractors should be built for other kind of unstructured
     data input such as images, audio, video, ...
 
+    Read more in the :ref:`User Guide <datasets>`.
+
     Parameters
     ----------
     container_path : string or unicode
@@ -241,6 +243,8 @@ def load_iris():
     Features            real, positive
     =================   ==============
 
+    Read more in the :ref:`User Guide <datasets>`.
+
     Returns
     -------
     data : Bunch
@@ -299,6 +303,7 @@ def load_digits(n_class=10):
     Features             integers 0-16
     =================   ==============
 
+    Read more in the :ref:`User Guide <datasets>`.
 
     Parameters
     ----------
@@ -359,6 +364,8 @@ def load_diabetes():
     Targets             integer 25 - 346
     ==============      ==================
 
+    Read more in the :ref:`User Guide <datasets>`.
+
     Returns
     -------
     data : Bunch
diff --git a/sklearn/datasets/california_housing.py b/sklearn/datasets/california_housing.py
index f1c1ac28248b..8258bb63c417 100644
--- a/sklearn/datasets/california_housing.py
+++ b/sklearn/datasets/california_housing.py
@@ -50,6 +50,8 @@
 def fetch_california_housing(data_home=None, download_if_missing=True):
     """Loader for the California housing dataset from StatLib.
 
+    Read more in the :ref:`User Guide <datasets>`.
+
     Parameters
     ----------
     data_home : optional, default: None
diff --git a/sklearn/datasets/covtype.py b/sklearn/datasets/covtype.py
index 7c37939b04e9..cc4d111c6036 100644
--- a/sklearn/datasets/covtype.py
+++ b/sklearn/datasets/covtype.py
@@ -45,6 +45,8 @@ def fetch_covtype(data_home=None, download_if_missing=True,
                   random_state=None, shuffle=False):
     """Load the covertype dataset, downloading it if necessary.
 
+    Read more in the :ref:`User Guide <datasets>`.
+
     Parameters
     ----------
     data_home : string, optional
diff --git a/sklearn/datasets/lfw.py b/sklearn/datasets/lfw.py
index 1f0a0c044337..9c27be96bf4e 100644
--- a/sklearn/datasets/lfw.py
+++ b/sklearn/datasets/lfw.py
@@ -26,6 +26,8 @@
 from os import listdir, makedirs, remove
 from os.path import join, exists, isdir
 
+from sklearn.utils import deprecated
+
 import logging
 import numpy as np
 
@@ -362,6 +364,9 @@ def _fetch_lfw_pairs(index_file_path, data_folder_path, slice_=None,
     return pairs, target, np.array(['Different persons', 'Same person'])
 
 
+@deprecated("Function 'load_lfw_people' has been deprecated in 0.17 and will be "
+            "removed in 0.19."
+            "Use fetch_lfw_people(download_if_missing=False) instead.")
 def load_lfw_people(download_if_missing=False, **kwargs):
     """Alias for fetch_lfw_people(download_if_missing=False)
 
@@ -397,6 +402,8 @@ def fetch_lfw_pairs(subset='train', data_home=None, funneled=True, resize=0.5,
     The original images are 250 x 250 pixels, but the default slice and resize
     arguments reduce them to 62 x 74.
 
+    Read more in the :ref:`User Guide <labeled_faces_in_the_wild>`.
+
     Parameters
     ----------
     subset : optional, default: 'train'
@@ -484,6 +491,9 @@ def fetch_lfw_pairs(subset='train', data_home=None, funneled=True, resize=0.5,
                  DESCR="'%s' segment of the LFW pairs dataset" % subset)
 
 
+@deprecated("Function 'load_lfw_pairs' has been deprecated in 0.17 and will be "
+            "removed in 0.19."
+            "Use fetch_lfw_pairs(download_if_missing=False) instead.")
 def load_lfw_pairs(download_if_missing=False, **kwargs):
     """Alias for fetch_lfw_pairs(download_if_missing=False)
 
diff --git a/sklearn/datasets/mlcomp.py b/sklearn/datasets/mlcomp.py
index 315faa34f77f..545492834c18 100644
--- a/sklearn/datasets/mlcomp.py
+++ b/sklearn/datasets/mlcomp.py
@@ -36,6 +36,8 @@ def load_mlcomp(name_or_id, set_="raw", mlcomp_root=None, **kwargs):
 
     **kwargs : domain specific kwargs to be passed to the dataset loader.
 
+    Read more in the :ref:`User Guide <datasets>`.
+
     Returns
     -------
 
diff --git a/sklearn/datasets/olivetti_faces.py b/sklearn/datasets/olivetti_faces.py
index 408a36bd576e..826da090d704 100644
--- a/sklearn/datasets/olivetti_faces.py
+++ b/sklearn/datasets/olivetti_faces.py
@@ -54,6 +54,8 @@ def fetch_olivetti_faces(data_home=None, shuffle=False, random_state=0,
                          download_if_missing=True):
     """Loader for the Olivetti faces data-set from AT&T.
 
+    Read more in the :ref:`User Guide <olivetti_faces>`.
+
     Parameters
     ----------
     data_home : optional, default: None
@@ -83,7 +85,8 @@ def fetch_olivetti_faces(data_home=None, shuffle=False, random_state=0,
         Each row is a face image corresponding to one of the 40 subjects of the dataset.
 
     target : numpy array of shape (400, )
-        Labels associated to each face image. Those labels are ranging from 0-39 and correspond to the Subject IDs.
+        Labels associated to each face image. Those labels are ranging from
+        0-39 and correspond to the Subject IDs.
 
     DESCR : string
         Description of the modified Olivetti Faces Dataset.
diff --git a/sklearn/datasets/samples_generator.py b/sklearn/datasets/samples_generator.py
index dae2b5ae8236..3ed0bceb3870 100644
--- a/sklearn/datasets/samples_generator.py
+++ b/sklearn/datasets/samples_generator.py
@@ -52,6 +52,8 @@ def make_classification(n_samples=100, n_features=20, n_informative=2,
     features, "redundant" linear combinations of these, "repeated" duplicates
     of sampled features, and arbitrary noise for and remaining features.
 
+    Read more in the :ref:`User Guide <sample_generators>`.
+
     Parameters
     ----------
     n_samples : int, optional (default=100)
@@ -263,6 +265,8 @@ def make_multilabel_classification(n_samples=100, n_features=20, n_classes=5,
     n is never zero or more than `n_classes`, and that the document length
     is never zero. Likewise, we reject classes which have already been chosen.
 
+    Read more in the :ref:`User Guide <sample_generators>`.
+
     Parameters
     ----------
     n_samples : int, optional (default=100)
@@ -404,6 +408,8 @@ def make_hastie_10_2(n_samples=12000, random_state=None):
 
       y[i] = 1 if np.sum(X[i] ** 2) > 9.34 else -1
 
+    Read more in the :ref:`User Guide <sample_generators>`.
+
     Parameters
     ----------
     n_samples : int, optional (default=12000)
@@ -457,6 +463,8 @@ def make_regression(n_samples=100, n_features=100, n_informative=10,
     generated input and some gaussian centered noise with some adjustable
     scale.
 
+    Read more in the :ref:`User Guide <sample_generators>`.
+
     Parameters
     ----------
     n_samples : int, optional (default=100)
@@ -570,6 +578,8 @@ def make_circles(n_samples=100, shuffle=True, noise=None, random_state=None,
     A simple toy dataset to visualize clustering and classification
     algorithms.
 
+    Read more in the :ref:`User Guide <sample_generators>`.
+
     Parameters
     ----------
     n_samples : int, optional (default=100)
@@ -635,6 +645,8 @@ def make_moons(n_samples=100, shuffle=True, noise=None, random_state=None):
     noise : double or None (default=None)
         Standard deviation of Gaussian noise added to the data.
 
+    Read more in the :ref:`User Guide <sample_generators>`.
+
     Returns
     -------
     X : array of shape [n_samples, 2]
@@ -672,6 +684,8 @@ def make_blobs(n_samples=100, n_features=2, centers=3, cluster_std=1.0,
                center_box=(-10.0, 10.0), shuffle=True, random_state=None):
     """Generate isotropic Gaussian blobs for clustering.
 
+    Read more in the :ref:`User Guide <sample_generators>`.
+
     Parameters
     ----------
     n_samples : int, optional (default=100)
@@ -773,6 +787,8 @@ def make_friedman1(n_samples=100, n_features=10, noise=0.0, random_state=None):
 
     The number of features has to be >= 5.
 
+    Read more in the :ref:`User Guide <sample_generators>`.
+
     Parameters
     ----------
     n_samples : int, optional (default=100)
@@ -836,6 +852,8 @@ def make_friedman2(n_samples=100, noise=0.0, random_state=None):
         y(X) = (X[:, 0] ** 2 + (X[:, 1] * X[:, 2] \
  - 1 / (X[:, 1] * X[:, 3])) ** 2) ** 0.5 + noise * N(0, 1).
 
+    Read more in the :ref:`User Guide <sample_generators>`.
+
     Parameters
     ----------
     n_samples : int, optional (default=100)
@@ -900,6 +918,8 @@ def make_friedman3(n_samples=100, noise=0.0, random_state=None):
         y(X) = arctan((X[:, 1] * X[:, 2] - 1 / (X[:, 1] * X[:, 3])) \
 / X[:, 0]) + noise * N(0, 1).
 
+    Read more in the :ref:`User Guide <sample_generators>`.
+
     Parameters
     ----------
     n_samples : int, optional (default=100)
@@ -967,6 +987,8 @@ def make_low_rank_matrix(n_samples=100, n_features=100, effective_rank=10,
      - gray level pictures of faces
      - TF-IDF vectors of text documents crawled from the web
 
+    Read more in the :ref:`User Guide <sample_generators>`.
+
     Parameters
     ----------
     n_samples : int, optional (default=100)
@@ -1021,6 +1043,8 @@ def make_sparse_coded_signal(n_samples, n_components, n_features,
     X is (n_components, n_samples) and each column of X has exactly
     n_nonzero_coefs non-zero elements.
 
+    Read more in the :ref:`User Guide <sample_generators>`.
+
     Parameters
     ----------
     n_samples : int
@@ -1082,6 +1106,8 @@ def make_sparse_uncorrelated(n_samples=100, n_features=10, random_state=None):
     Only the first 4 features are informative. The remaining features are
     useless.
 
+    Read more in the :ref:`User Guide <sample_generators>`.
+
     Parameters
     ----------
     n_samples : int, optional (default=100)
@@ -1124,6 +1150,8 @@ def make_sparse_uncorrelated(n_samples=100, n_features=10, random_state=None):
 def make_spd_matrix(n_dim, random_state=None):
     """Generate a random symmetric, positive-definite matrix.
 
+    Read more in the :ref:`User Guide <sample_generators>`.
+
     Parameters
     ----------
     n_dim : int
@@ -1158,6 +1186,8 @@ def make_sparse_spd_matrix(dim=1, alpha=0.95, norm_diag=False,
                            random_state=None):
     """Generate a sparse symmetric definite positive matrix.
 
+    Read more in the :ref:`User Guide <sample_generators>`.
+
     Parameters
     ----------
     dim: integer, optional (default=1)
@@ -1228,6 +1258,8 @@ def make_sparse_spd_matrix(dim=1, alpha=0.95, norm_diag=False,
 def make_swiss_roll(n_samples=100, noise=0.0, random_state=None):
     """Generate a swiss roll dataset.
 
+    Read more in the :ref:`User Guide <sample_generators>`.
+
     Parameters
     ----------
     n_samples : int, optional (default=100)
@@ -1279,6 +1311,8 @@ def make_swiss_roll(n_samples=100, noise=0.0, random_state=None):
 def make_s_curve(n_samples=100, noise=0.0, random_state=None):
     """Generate an S curve dataset.
 
+    Read more in the :ref:`User Guide <sample_generators>`.
+
     Parameters
     ----------
     n_samples : int, optional (default=100)
@@ -1327,6 +1361,8 @@ def make_gaussian_quantiles(mean=None, cov=1., n_samples=100,
     concentric multi-dimensional spheres such that roughly equal numbers of
     samples are in each class (quantiles of the :math:`\chi^2` distribution).
 
+    Read more in the :ref:`User Guide <sample_generators>`.
+
     Parameters
     ----------
     mean : array of shape [n_features], optional (default=None)
@@ -1416,6 +1452,8 @@ def make_biclusters(shape, n_clusters, noise=0.0, minval=10,
     """Generate an array with constant block diagonal structure for
     biclustering.
 
+    Read more in the :ref:`User Guide <sample_generators>`.
+
     Parameters
     ----------
     shape : iterable (n_rows, n_cols)
@@ -1503,9 +1541,12 @@ def make_biclusters(shape, n_clusters, noise=0.0, minval=10,
 
 def make_checkerboard(shape, n_clusters, noise=0.0, minval=10,
                       maxval=100, shuffle=True, random_state=None):
+
     """Generate an array with block checkerboard structure for
     biclustering.
 
+    Read more in the :ref:`User Guide <sample_generators>`.
+
     Parameters
     ----------
     shape : iterable (n_rows, n_cols)
diff --git a/sklearn/datasets/species_distributions.py b/sklearn/datasets/species_distributions.py
index daf3e42a982e..f8872eed2e0b 100644
--- a/sklearn/datasets/species_distributions.py
+++ b/sklearn/datasets/species_distributions.py
@@ -134,6 +134,8 @@ def fetch_species_distributions(data_home=None,
                                 download_if_missing=True):
     """Loader for species distribution dataset from Phillips et. al. (2006)
 
+    Read more in the :ref:`User Guide <datasets>`.
+
     Parameters
     ----------
     data_home : optional, default: None
diff --git a/sklearn/datasets/tests/test_lfw.py b/sklearn/datasets/tests/test_lfw.py
index 5e4788d3bfba..639465a3136a 100644
--- a/sklearn/datasets/tests/test_lfw.py
+++ b/sklearn/datasets/tests/test_lfw.py
@@ -24,9 +24,12 @@
 
 from sklearn.datasets import load_lfw_pairs
 from sklearn.datasets import load_lfw_people
+from sklearn.datasets import fetch_lfw_pairs
+from sklearn.datasets import fetch_lfw_people
 
 from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_equal
+from sklearn.utils.testing import assert_warns_message
 from sklearn.utils.testing import SkipTest
 from sklearn.utils.testing import raises
 
@@ -112,12 +115,20 @@ def teardown_module():
 
 @raises(IOError)
 def test_load_empty_lfw_people():
-    load_lfw_people(data_home=SCIKIT_LEARN_EMPTY_DATA)
+    fetch_lfw_people(data_home=SCIKIT_LEARN_EMPTY_DATA, download_if_missing=False)
+
+
+def test_load_lfw_people_deprecation():
+    msg = ("Function 'load_lfw_people' has been deprecated in 0.17 and will be "
+           "removed in 0.19."
+           "Use fetch_lfw_people(download_if_missing=False) instead.")
+    assert_warns_message(DeprecationWarning, msg, load_lfw_people,
+                         data_home=SCIKIT_LEARN_DATA)
 
 
 def test_load_fake_lfw_people():
-    lfw_people = load_lfw_people(data_home=SCIKIT_LEARN_DATA,
-                                 min_faces_per_person=3)
+    lfw_people = fetch_lfw_people(data_home=SCIKIT_LEARN_DATA,
+                                  min_faces_per_person=3, download_if_missing=False)
 
     # The data is croped around the center as a rectangular bounding box
     # arounthe the face. Colors are converted to gray levels:
@@ -133,8 +144,8 @@ def test_load_fake_lfw_people():
 
     # It is possible to ask for the original data without any croping or color
     # conversion and not limit on the number of picture per person
-    lfw_people = load_lfw_people(data_home=SCIKIT_LEARN_DATA,
-                                 resize=None, slice_=None, color=True)
+    lfw_people = fetch_lfw_people(data_home=SCIKIT_LEARN_DATA,
+                                  resize=None, slice_=None, color=True, download_if_missing=False)
     assert_equal(lfw_people.images.shape, (17, 250, 250, 3))
 
     # the ids and class names are the same as previously
@@ -147,16 +158,24 @@ def test_load_fake_lfw_people():
 
 @raises(ValueError)
 def test_load_fake_lfw_people_too_restrictive():
-    load_lfw_people(data_home=SCIKIT_LEARN_DATA, min_faces_per_person=100)
+    fetch_lfw_people(data_home=SCIKIT_LEARN_DATA, min_faces_per_person=100, download_if_missing=False)
 
 
 @raises(IOError)
 def test_load_empty_lfw_pairs():
-    load_lfw_pairs(data_home=SCIKIT_LEARN_EMPTY_DATA)
+    fetch_lfw_pairs(data_home=SCIKIT_LEARN_EMPTY_DATA, download_if_missing=False)
+
+
+def test_load_lfw_pairs_deprecation():
+    msg = ("Function 'load_lfw_pairs' has been deprecated in 0.17 and will be "
+           "removed in 0.19."
+           "Use fetch_lfw_pairs(download_if_missing=False) instead.")
+    assert_warns_message(DeprecationWarning, msg, load_lfw_pairs,
+                         data_home=SCIKIT_LEARN_DATA)
 
 
 def test_load_fake_lfw_pairs():
-    lfw_pairs_train = load_lfw_pairs(data_home=SCIKIT_LEARN_DATA)
+    lfw_pairs_train = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA, download_if_missing=False)
 
     # The data is croped around the center as a rectangular bounding box
     # arounthe the face. Colors are converted to gray levels:
@@ -171,8 +190,8 @@ def test_load_fake_lfw_pairs():
 
     # It is possible to ask for the original data without any croping or color
     # conversion
-    lfw_pairs_train = load_lfw_pairs(data_home=SCIKIT_LEARN_DATA,
-                                     resize=None, slice_=None, color=True)
+    lfw_pairs_train = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA,
+                                      resize=None, slice_=None, color=True, download_if_missing=False)
     assert_equal(lfw_pairs_train.pairs.shape, (10, 2, 250, 250, 3))
 
     # the ids and class names are the same as previously
diff --git a/sklearn/datasets/twenty_newsgroups.py b/sklearn/datasets/twenty_newsgroups.py
index 8582d992aaca..39f7ed3f9088 100644
--- a/sklearn/datasets/twenty_newsgroups.py
+++ b/sklearn/datasets/twenty_newsgroups.py
@@ -156,6 +156,8 @@ def fetch_20newsgroups(data_home=None, subset='train', categories=None,
                        download_if_missing=True):
     """Load the filenames and data from the 20 newsgroups dataset.
 
+    Read more in the :ref:`User Guide <20newsgroups>`.
+
     Parameters
     ----------
     subset: 'train' or 'test', 'all', optional
@@ -287,6 +289,8 @@ def fetch_20newsgroups_vectorized(subset="train", remove=(), data_home=None):
     advanced usage (stopword filtering, n-gram extraction, etc.), combine
     fetch_20newsgroups with a custom `Vectorizer` or `CountVectorizer`.
 
+    Read more in the :ref:`User Guide <20newsgroups>`.
+
     Parameters
     ----------
 
diff --git a/sklearn/decomposition/dict_learning.py b/sklearn/decomposition/dict_learning.py
index 5f7b4e8de9d9..e0a22fce086b 100644
--- a/sklearn/decomposition/dict_learning.py
+++ b/sklearn/decomposition/dict_learning.py
@@ -150,6 +150,8 @@ def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars',
 
         X ~= code * dictionary
 
+    Read more in the :ref:`User Guide <SparseCoder>`.
+
     Parameters
     ----------
     X: array of shape (n_samples, n_features)
@@ -345,6 +347,8 @@ def dict_learning(X, n_components, alpha, max_iter=100, tol=1e-8,
 
     where V is the dictionary and U is the sparse code.
 
+    Read more in the :ref:`User Guide <DictionaryLearning>`.
+
     Parameters
     ----------
     X: array of shape (n_samples, n_features)
@@ -518,6 +522,8 @@ def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,
     accomplished by repeatedly iterating over mini-batches by slicing
     the input data.
 
+    Read more in the :ref:`User Guide <DictionaryLearning>`.
+
     Parameters
     ----------
     X: array of shape (n_samples, n_features)
@@ -787,6 +793,8 @@ class SparseCoder(BaseEstimator, SparseCodingMixin):
 
         X ~= code * dictionary
 
+    Read more in the :ref:`User Guide <SparseCoder>`.
+
     Parameters
     ----------
     dictionary : array, [n_components, n_features]
@@ -871,6 +879,8 @@ class DictionaryLearning(BaseEstimator, SparseCodingMixin):
                     (U,V)
                     with || V_k ||_2 = 1 for all  0 <= k < n_components
 
+    Read more in the :ref:`User Guide <DictionaryLearning>`.
+
     Parameters
     ----------
     n_components : int,
@@ -1029,6 +1039,8 @@ class MiniBatchDictionaryLearning(BaseEstimator, SparseCodingMixin):
                     (U,V)
                     with || V_k ||_2 = 1 for all  0 <= k < n_components
 
+    Read more in the :ref:`User Guide <DictionaryLearning>`.
+
     Parameters
     ----------
     n_components : int,
diff --git a/sklearn/decomposition/factor_analysis.py b/sklearn/decomposition/factor_analysis.py
index 40deb0d25f29..e5b96457455e 100644
--- a/sklearn/decomposition/factor_analysis.py
+++ b/sklearn/decomposition/factor_analysis.py
@@ -52,6 +52,8 @@ class FactorAnalysis(BaseEstimator, TransformerMixin):
     `loading` matrix, the transformation of the latent variables to the
     observed ones, using expectation-maximization (EM).
 
+    Read more in the :ref:`User Guide <FA>`.
+
     Parameters
     ----------
     n_components : int | None
@@ -316,7 +318,7 @@ def score_samples(self, X):
             Log-likelihood of each sample under the current model
         """
         check_is_fitted(self, 'components_')
-        
+
         Xr = X - self.mean_
         precision = self.get_precision()
         n_features = X.shape[1]
diff --git a/sklearn/decomposition/fastica_.py b/sklearn/decomposition/fastica_.py
index a8b084064ac9..15bdf47f2015 100644
--- a/sklearn/decomposition/fastica_.py
+++ b/sklearn/decomposition/fastica_.py
@@ -149,6 +149,8 @@ def fastica(X, n_components=None, algorithm="parallel", whiten=True,
             return_n_iter=False):
     """Perform Fast Independent Component Analysis.
 
+    Read more in the :ref:`User Guide <ICA>`.
+
     Parameters
     ----------
     X : array-like, shape (n_samples, n_features)
@@ -373,6 +375,8 @@ def g(x, fun_args):
 class FastICA(BaseEstimator, TransformerMixin):
     """FastICA: a fast algorithm for Independent Component Analysis.
 
+    Read more in the :ref:`User Guide <ICA>`.
+
     Parameters
     ----------
     n_components : int, optional
diff --git a/sklearn/decomposition/incremental_pca.py b/sklearn/decomposition/incremental_pca.py
index 9b5ccff00af2..16411b8bedb8 100644
--- a/sklearn/decomposition/incremental_pca.py
+++ b/sklearn/decomposition/incremental_pca.py
@@ -31,6 +31,8 @@ class IncrementalPCA(_BasePCA):
     computations to get the principal components, versus 1 large SVD of
     complexity ``O(n_samples * n_features ** 2)`` for PCA.
 
+    Read more in the :ref:`User Guide <IncrementalPCA>`.
+
     Parameters
     ----------
     n_components : int or None, (default=None)
diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py
index f447e47934e1..eddc3ab0c562 100644
--- a/sklearn/decomposition/kernel_pca.py
+++ b/sklearn/decomposition/kernel_pca.py
@@ -19,6 +19,8 @@ class KernelPCA(BaseEstimator, TransformerMixin):
     Non-linear dimensionality reduction through the use of kernels (see
     :ref:`metrics`).
 
+    Read more in the :ref:`User Guide <kernel_PCA>`.
+
     Parameters
     ----------
     n_components: int or None
diff --git a/sklearn/decomposition/nmf.py b/sklearn/decomposition/nmf.py
index 7afc0b3ab029..61c319a21fe9 100644
--- a/sklearn/decomposition/nmf.py
+++ b/sklearn/decomposition/nmf.py
@@ -95,7 +95,7 @@ def _initialize_nmf(X, n_components, variant=None, eps=1e-6,
 
     References
     ----------
-    C. Boutsidis, E. Gallopoulos: SVD based initialization: A head start for 
+    C. Boutsidis, E. Gallopoulos: SVD based initialization: A head start for
     nonnegative matrix factorization - Pattern Recognition, 2008
 
     http://tinyurl.com/nndsvd
@@ -257,6 +257,8 @@ def _nls_subproblem(V, W, H, tol, max_iter, sigma=0.01, beta=0.1):
 class ProjectedGradientNMF(BaseEstimator, TransformerMixin):
     """Non-Negative matrix factorization by Projected Gradient (NMF)
 
+    Read more in the :ref:`User Guide <NMF>`.
+
     Parameters
     ----------
     n_components : int or None
diff --git a/sklearn/decomposition/pca.py b/sklearn/decomposition/pca.py
index fd733c6ceaf7..f773d3d6e130 100644
--- a/sklearn/decomposition/pca.py
+++ b/sklearn/decomposition/pca.py
@@ -110,6 +110,8 @@ class PCA(BaseEstimator, TransformerMixin):
     The time complexity of this implementation is ``O(n ** 3)`` assuming
     n ~ n_samples ~ n_features.
 
+    Read more in the :ref:`User Guide <PCA>`.
+
     Parameters
     ----------
     n_components : int, None or string
@@ -415,7 +417,6 @@ def inverse_transform(self, X):
         else:
             return fast_dot(X, self.components_) + self.mean_
 
-
     def score_samples(self, X):
         """Return the log-likelihood of each sample
 
@@ -472,6 +473,8 @@ class RandomizedPCA(BaseEstimator, TransformerMixin):
     Decomposition of the data and keeping only the most significant
     singular vectors to project the data to a lower dimensional space.
 
+    Read more in the :ref:`User Guide <RandomizedPCA>`.
+
     Parameters
     ----------
     n_components : int, optional
diff --git a/sklearn/decomposition/sparse_pca.py b/sklearn/decomposition/sparse_pca.py
index 38f8e2e3209d..392704eda80f 100644
--- a/sklearn/decomposition/sparse_pca.py
+++ b/sklearn/decomposition/sparse_pca.py
@@ -18,6 +18,8 @@ class SparsePCA(BaseEstimator, TransformerMixin):
     the data.  The amount of sparseness is controllable by the coefficient
     of the L1 penalty, given by the parameter alpha.
 
+    Read more in the :ref:`User Guide <SparsePCA>`.
+
     Parameters
     ----------
     n_components : int,
@@ -172,6 +174,8 @@ class MiniBatchSparsePCA(SparsePCA):
     the data.  The amount of sparseness is controllable by the coefficient
     of the L1 penalty, given by the parameter alpha.
 
+    Read more in the :ref:`User Guide <SparsePCA>`.
+
     Parameters
     ----------
     n_components : int,
@@ -275,7 +279,6 @@ def fit(self, X, y=None):
             shuffle=self.shuffle,
             n_jobs=self.n_jobs, method=self.method,
             random_state=random_state,
-            return_n_iter=True
-            )
+            return_n_iter=True)
         self.components_ = Vt.T
         return self
diff --git a/sklearn/decomposition/tests/test_fastica.py b/sklearn/decomposition/tests/test_fastica.py
index 66963701d15b..e47489e51488 100644
--- a/sklearn/decomposition/tests/test_fastica.py
+++ b/sklearn/decomposition/tests/test_fastica.py
@@ -234,8 +234,3 @@ def test_inverse_transform():
             # reversibility test in non-reduction case
             if n_components == X.shape[1]:
                 assert_array_almost_equal(X, X2)
-
-
-if __name__ == '__main__':
-    import nose
-    nose.run(argv=['', __file__])
diff --git a/sklearn/decomposition/tests/test_kernel_pca.py b/sklearn/decomposition/tests/test_kernel_pca.py
index 9cd9adbc2bb1..f0475f2af7fa 100644
--- a/sklearn/decomposition/tests/test_kernel_pca.py
+++ b/sklearn/decomposition/tests/test_kernel_pca.py
@@ -207,8 +207,3 @@ def test_nested_circles():
     # The data is perfectly linearly separable in that space
     train_score = Perceptron().fit(X_kpca, y).score(X_kpca, y)
     assert_equal(train_score, 1.0)
-
-
-if __name__ == '__main__':
-    import nose
-    nose.run(argv=['', __file__])
diff --git a/sklearn/decomposition/tests/test_nmf.py b/sklearn/decomposition/tests/test_nmf.py
index 4793935bdce4..8b6dee85fc56 100644
--- a/sklearn/decomposition/tests/test_nmf.py
+++ b/sklearn/decomposition/tests/test_nmf.py
@@ -164,8 +164,3 @@ def test_sparse_transform():
     A_tr = model.transform(A)
     # This solver seems pretty inconsistent
     assert_array_almost_equal(A_fit_tr, A_tr, decimal=2)
-
-
-if __name__ == '__main__':
-    import nose
-    nose.run(argv=['', __file__])
diff --git a/sklearn/decomposition/tests/test_pca.py b/sklearn/decomposition/tests/test_pca.py
index 34fe795d2b19..d696f9b3f0bc 100644
--- a/sklearn/decomposition/tests/test_pca.py
+++ b/sklearn/decomposition/tests/test_pca.py
@@ -328,8 +328,3 @@ def test_pca_score3():
         ll[k] = pca.score(Xt)
 
     assert_true(ll.argmax() == 1)
-
-
-if __name__ == '__main__':
-    import nose
-    nose.run(argv=['', __file__])
diff --git a/sklearn/decomposition/truncated_svd.py b/sklearn/decomposition/truncated_svd.py
index 3b2033204e50..fd4909e2b6c8 100644
--- a/sklearn/decomposition/truncated_svd.py
+++ b/sklearn/decomposition/truncated_svd.py
@@ -38,6 +38,8 @@ class TruncatedSVD(BaseEstimator, TransformerMixin):
     a "naive" algorithm that uses ARPACK as an eigensolver on (X * X.T) or
     (X.T * X), whichever is more efficient.
 
+    Read more in the :ref:`User Guide <LSA>`.
+
     Parameters
     ----------
     n_components : int, default = 2
diff --git a/sklearn/dummy.py b/sklearn/dummy.py
index f87a6d0def98..06aa14d7db1b 100644
--- a/sklearn/dummy.py
+++ b/sklearn/dummy.py
@@ -24,6 +24,8 @@ class DummyClassifier(BaseEstimator, ClassifierMixin):
     This classifier is useful as a simple baseline to compare with other
     (real) classifiers. Do not use it for real problems.
 
+    Read more in the :ref:`User Guide <dummy_estimators>`.
+
     Parameters
     ----------
     strategy : str
@@ -323,6 +325,8 @@ class DummyRegressor(BaseEstimator, RegressorMixin):
     This regressor is useful as a simple baseline to compare with other
     (real) regressors. Do not use it for real problems.
 
+    Read more in the :ref:`User Guide <dummy_estimators>`.
+
     Parameters
     ----------
     strategy : str
diff --git a/sklearn/ensemble/bagging.py b/sklearn/ensemble/bagging.py
index 0037f5af69c0..bec6e040577c 100644
--- a/sklearn/ensemble/bagging.py
+++ b/sklearn/ensemble/bagging.py
@@ -369,6 +369,8 @@ class BaggingClassifier(BaseBagging, ClassifierMixin):
     on subsets of both samples and features, then the method is known as
     Random Patches [4]_.
 
+    Read more in the :ref:`User Guide <bagging>`.
+
     Parameters
     ----------
     base_estimator : object or None, optional (default=None)
@@ -732,6 +734,8 @@ class BaggingRegressor(BaseBagging, RegressorMixin):
     on subsets of both samples and features, then the method is known as
     Random Patches [4]_.
 
+    Read more in the :ref:`User Guide <bagging>`.
+
     Parameters
     ----------
     base_estimator : object or None, optional (default=None)
diff --git a/sklearn/ensemble/forest.py b/sklearn/ensemble/forest.py
index bd8052828ffc..d63ebf625394 100644
--- a/sklearn/ensemble/forest.py
+++ b/sklearn/ensemble/forest.py
@@ -41,7 +41,9 @@ class calls the ``fit`` method of each sub-estimator on random samples
 
 from __future__ import division
 
+import warnings
 from warnings import warn
+
 from abc import ABCMeta, abstractmethod
 
 import numpy as np
@@ -89,7 +91,11 @@ def _parallel_build_trees(tree, forest, X, y, sample_weight, tree_idx, n_trees,
         curr_sample_weight *= sample_counts
 
         if class_weight == 'subsample':
-            curr_sample_weight *= compute_sample_weight('auto', y, indices)
+            with warnings.catch_warnings():
+                warnings.simplefilter('ignore', DeprecationWarning)
+                curr_sample_weight *= compute_sample_weight('auto', y, indices)
+        elif class_weight == 'balanced_subsample':
+            curr_sample_weight *= compute_sample_weight('balanced', y, indices)
 
         tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)
 
@@ -414,30 +420,40 @@ def _validate_y_class_weight(self, y):
             self.n_classes_.append(classes_k.shape[0])
 
         if self.class_weight is not None:
-            valid_presets = ('auto', 'subsample')
+            valid_presets = ('auto', 'balanced', 'balanced_subsample', 'subsample', 'auto')
             if isinstance(self.class_weight, six.string_types):
                 if self.class_weight not in valid_presets:
                     raise ValueError('Valid presets for class_weight include '
-                                     '"auto" and "subsample". Given "%s".'
+                                     '"balanced" and "balanced_subsample". Given "%s".'
                                      % self.class_weight)
+                if self.class_weight == "subsample":
+                    warn("class_weight='subsample' is deprecated and will be removed in 0.18."
+                         " It was replaced by class_weight='balanced_subsample' "
+                         "using the balanced strategy.", DeprecationWarning)
                 if self.warm_start:
-                    warn('class_weight presets "auto" or "subsample" are '
+                    warn('class_weight presets "balanced" or "balanced_subsample" are '
                          'not recommended for warm_start if the fitted data '
                          'differs from the full dataset. In order to use '
-                         '"auto" weights, use compute_class_weight("auto", '
+                         '"balanced" weights, use compute_class_weight("balanced", '
                          'classes, y). In place of y you can use a large '
                          'enough sample of the full training set target to '
                          'properly estimate the class frequency '
                          'distributions. Pass the resulting weights as the '
                          'class_weight parameter.')
 
-            if self.class_weight != 'subsample' or not self.bootstrap:
+            if (self.class_weight not in ['subsample', 'balanced_subsample'] or
+                    not self.bootstrap):
                 if self.class_weight == 'subsample':
                     class_weight = 'auto'
+                elif self.class_weight == "balanced_subsample":
+                    class_weight = "balanced"
                 else:
                     class_weight = self.class_weight
-                expanded_class_weight = compute_sample_weight(class_weight,
-                                                              y_original)
+                with warnings.catch_warnings():
+                    if class_weight == "auto":
+                        warnings.simplefilter('ignore', DeprecationWarning)
+                    expanded_class_weight = compute_sample_weight(class_weight,
+                                                                  y_original)
 
         return y, expanded_class_weight
 
@@ -677,6 +693,8 @@ class RandomForestClassifier(ForestClassifier):
     classifiers on various sub-samples of the dataset and use averaging to
     improve the predictive accuracy and control over-fitting.
 
+    Read more in the :ref:`User Guide <forest>`.
+
     Parameters
     ----------
     n_estimators : integer, optional (default=10)
@@ -758,17 +776,18 @@ class RandomForestClassifier(ForestClassifier):
         and add more estimators to the ensemble, otherwise, just fit a whole
         new forest.
 
-    class_weight : dict, list of dicts, "auto", "subsample" or None, optional
+    class_weight : dict, list of dicts, "balanced", "balanced_subsample" or None, optional
 
         Weights associated with classes in the form ``{class_label: weight}``.
         If not given, all classes are supposed to have weight one. For
         multi-output problems, a list of dicts can be provided in the same
         order as the columns of y.
 
-        The "auto" mode uses the values of y to automatically adjust
-        weights inversely proportional to class frequencies in the input data.
+        The "balanced" mode uses the values of y to automatically adjust
+        weights inversely proportional to class frequencies in the input data
+        as ``n_samples / (n_classes * np.bincount(y))``
 
-        The "subsample" mode is the same as "auto" except that weights are
+        The "balanced_subsample" mode is the same as "balanced" except that weights are
         computed based on the bootstrap sample for every tree grown.
 
         For multi-output, the weights of each column of y will be multiplied.
@@ -863,6 +882,8 @@ class RandomForestRegressor(ForestRegressor):
     decision trees on various sub-samples of the dataset and use averaging
     to improve the predictive accuracy and control over-fitting.
 
+    Read more in the :ref:`User Guide <forest>`.
+
     Parameters
     ----------
     n_estimators : integer, optional (default=10)
@@ -1019,6 +1040,8 @@ class ExtraTreesClassifier(ForestClassifier):
     of the dataset and use averaging to improve the predictive accuracy
     and control over-fitting.
 
+    Read more in the :ref:`User Guide <forest>`.
+
     Parameters
     ----------
     n_estimators : integer, optional (default=10)
@@ -1100,17 +1123,18 @@ class ExtraTreesClassifier(ForestClassifier):
         and add more estimators to the ensemble, otherwise, just fit a whole
         new forest.
 
-    class_weight : dict, list of dicts, "auto", "subsample" or None, optional
+    class_weight : dict, list of dicts, "balanced", "balanced_subsample" or None, optional
 
         Weights associated with classes in the form ``{class_label: weight}``.
         If not given, all classes are supposed to have weight one. For
         multi-output problems, a list of dicts can be provided in the same
         order as the columns of y.
 
-        The "auto" mode uses the values of y to automatically adjust
-        weights inversely proportional to class frequencies in the input data.
+        The "balanced" mode uses the values of y to automatically adjust
+        weights inversely proportional to class frequencies in the input data
+        as ``n_samples / (n_classes * np.bincount(y))``
 
-        The "subsample" mode is the same as "auto" except that weights are
+        The "balanced_subsample" mode is the same as "balanced" except that weights are
         computed based on the bootstrap sample for every tree grown.
 
         For multi-output, the weights of each column of y will be multiplied.
@@ -1208,6 +1232,8 @@ class ExtraTreesRegressor(ForestRegressor):
     of the dataset and use averaging to improve the predictive accuracy
     and control over-fitting.
 
+    Read more in the :ref:`User Guide <forest>`.
+
     Parameters
     ----------
     n_estimators : integer, optional (default=10)
@@ -1372,6 +1398,8 @@ class RandomTreesEmbedding(BaseForest):
     ``n_out <= n_estimators * max_leaf_nodes``. If ``max_leaf_nodes == None``,
     the number of leaf nodes is at most ``n_estimators * 2 ** max_depth``.
 
+    Read more in the :ref:`User Guide <random_trees_embedding>`.
+
     Parameters
     ----------
     n_estimators : int
diff --git a/sklearn/ensemble/gradient_boosting.py b/sklearn/ensemble/gradient_boosting.py
index fd9e11b24d5e..4d741299b2cf 100644
--- a/sklearn/ensemble/gradient_boosting.py
+++ b/sklearn/ensemble/gradient_boosting.py
@@ -505,7 +505,7 @@ def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
 
     def _score_to_proba(self, score):
         proba = np.ones((score.shape[0], 2), dtype=np.float64)
-        proba[:, 1] = 1.0 / (1.0 + np.exp(-score.ravel()))
+        proba[:, 1] = expit(score.ravel())
         proba[:, 0] -= proba[:, 1]
         return proba
 
@@ -628,7 +628,7 @@ def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
 
     def _score_to_proba(self, score):
         proba = np.ones((score.shape[0], 2), dtype=np.float64)
-        proba[:, 1] = 1.0 / (1.0 + np.exp(-2.0 * score.ravel()))
+        proba[:, 1] = expit(2.0 * score.ravel())
         proba[:, 0] -= proba[:, 1]
         return proba
 
@@ -1195,6 +1195,8 @@ class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):
     binomial or multinomial deviance loss function. Binary classification
     is a special case where only a single regression tree is induced.
 
+    Read more in the :ref:`User Guide <gradient_boosting>`.
+
     Parameters
     ----------
     loss : {'deviance', 'exponential'}, optional (default='deviance')
@@ -1515,6 +1517,8 @@ class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):
     In each stage a regression tree is fit on the negative gradient of the
     given loss function.
 
+    Read more in the :ref:`User Guide <gradient_boosting>`.
+
     Parameters
     ----------
     loss : {'ls', 'lad', 'huber', 'quantile'}, optional (default='ls')
diff --git a/sklearn/ensemble/partial_dependence.py b/sklearn/ensemble/partial_dependence.py
index bf5b1e9e2562..3b1756ba73d0 100644
--- a/sklearn/ensemble/partial_dependence.py
+++ b/sklearn/ensemble/partial_dependence.py
@@ -76,6 +76,8 @@ def partial_dependence(gbrt, target_variables, grid=None, X=None,
     of the ``target_variables`` and the function represented
     by the ``gbrt``.
 
+    Read more in the :ref:`User Guide <partial_dependence>`.
+
     Parameters
     ----------
     gbrt : BaseGradientBoosting
@@ -173,6 +175,8 @@ def plot_partial_dependence(gbrt, X, features, feature_names=None,
     columns. Two-way partial dependence plots are plotted as contour
     plots.
 
+    Read more in the :ref:`User Guide <partial_dependence>`.
+
     Parameters
     ----------
     gbrt : BaseGradientBoosting
diff --git a/sklearn/ensemble/tests/test_bagging.py b/sklearn/ensemble/tests/test_bagging.py
index e567a8ab9339..a5f2a3d08619 100644
--- a/sklearn/ensemble/tests/test_bagging.py
+++ b/sklearn/ensemble/tests/test_bagging.py
@@ -662,8 +662,3 @@ def test_oob_score_removed_on_warm_start():
     clf.fit(X, y)
 
     assert_raises(AttributeError, getattr, clf, "oob_score_")
-
-
-if __name__ == "__main__":
-    import nose
-    nose.runmodule()
diff --git a/sklearn/ensemble/tests/test_forest.py b/sklearn/ensemble/tests/test_forest.py
index 33aa5cb3e405..00f49c7389c9 100644
--- a/sklearn/ensemble/tests/test_forest.py
+++ b/sklearn/ensemble/tests/test_forest.py
@@ -24,6 +24,7 @@
 from sklearn.utils.testing import assert_greater_equal
 from sklearn.utils.testing import assert_raises
 from sklearn.utils.testing import assert_warns
+from sklearn.utils.testing import assert_warns_message
 from sklearn.utils.testing import ignore_warnings
 
 from sklearn import datasets
@@ -329,7 +330,7 @@ def test_parallel():
         yield check_parallel, name, iris.data, iris.target
 
     for name in FOREST_REGRESSORS:
-        yield check_parallel, name,  boston.data, boston.target
+        yield check_parallel, name, boston.data, boston.target
 
 
 def check_pickle(name, X, y):
@@ -352,7 +353,7 @@ def test_pickle():
         yield check_pickle, name, iris.data[::2], iris.target[::2]
 
     for name in FOREST_REGRESSORS:
-        yield check_pickle, name,  boston.data[::2], boston.target[::2]
+        yield check_pickle, name, boston.data[::2], boston.target[::2]
 
 
 def check_multioutput(name):
@@ -749,10 +750,10 @@ def check_class_weights(name):
     # Check class_weights resemble sample_weights behavior.
     ForestClassifier = FOREST_CLASSIFIERS[name]
 
-    # Iris is balanced, so no effect expected for using 'auto' weights
+    # Iris is balanced, so no effect expected for using 'balanced' weights
     clf1 = ForestClassifier(random_state=0)
     clf1.fit(iris.data, iris.target)
-    clf2 = ForestClassifier(class_weight='auto', random_state=0)
+    clf2 = ForestClassifier(class_weight='balanced', random_state=0)
     clf2.fit(iris.data, iris.target)
     assert_almost_equal(clf1.feature_importances_, clf2.feature_importances_)
 
@@ -765,8 +766,8 @@ def check_class_weights(name):
                             random_state=0)
     clf3.fit(iris.data, iris_multi)
     assert_almost_equal(clf2.feature_importances_, clf3.feature_importances_)
-    # Check against multi-output "auto" which should also have no effect
-    clf4 = ForestClassifier(class_weight='auto', random_state=0)
+    # Check against multi-output "balanced" which should also have no effect
+    clf4 = ForestClassifier(class_weight='balanced', random_state=0)
     clf4.fit(iris.data, iris_multi)
     assert_almost_equal(clf3.feature_importances_, clf4.feature_importances_)
 
@@ -782,7 +783,7 @@ def check_class_weights(name):
 
     # Check that sample_weight and class_weight are multiplicative
     clf1 = ForestClassifier(random_state=0)
-    clf1.fit(iris.data, iris.target, sample_weight**2)
+    clf1.fit(iris.data, iris.target, sample_weight ** 2)
     clf2 = ForestClassifier(class_weight=class_weight, random_state=0)
     clf2.fit(iris.data, iris.target, sample_weight)
     assert_almost_equal(clf1.feature_importances_, clf2.feature_importances_)
@@ -793,22 +794,26 @@ def test_class_weights():
         yield check_class_weights, name
 
 
-def check_class_weight_auto_and_bootstrap_multi_output(name):
-    # Test class_weight works for multi-output
+def check_class_weight_balanced_and_bootstrap_multi_output(name):
+    # Test class_weight works for multi-output"""
     ForestClassifier = FOREST_CLASSIFIERS[name]
     _y = np.vstack((y, np.array(y) * 2)).T
-    clf = ForestClassifier(class_weight='auto', random_state=0)
+    clf = ForestClassifier(class_weight='balanced', random_state=0)
     clf.fit(X, _y)
     clf = ForestClassifier(class_weight=[{-1: 0.5, 1: 1.}, {-2: 1., 2: 1.}],
                            random_state=0)
     clf.fit(X, _y)
+    # smoke test for subsample and balanced subsample
+    clf = ForestClassifier(class_weight='balanced_subsample', random_state=0)
+    clf.fit(X, _y)
     clf = ForestClassifier(class_weight='subsample', random_state=0)
+    #assert_warns_message(DeprecationWarning, "balanced_subsample", clf.fit, X, _y)
     clf.fit(X, _y)
 
 
-def test_class_weight_auto_and_bootstrap_multi_output():
+def test_class_weight_balanced_and_bootstrap_multi_output():
     for name in FOREST_CLASSIFIERS:
-        yield check_class_weight_auto_and_bootstrap_multi_output, name
+        yield check_class_weight_balanced_and_bootstrap_multi_output, name
 
 
 def check_class_weight_errors(name):
@@ -973,8 +978,3 @@ def test_warm_start_oob():
         yield check_warm_start_oob, name
     for name in FOREST_REGRESSORS:
         yield check_warm_start_oob, name
-
-
-if __name__ == "__main__":
-    import nose
-    nose.runmodule()
diff --git a/sklearn/ensemble/tests/test_gradient_boosting.py b/sklearn/ensemble/tests/test_gradient_boosting.py
index 3f7f7f23f656..c1043e8da482 100644
--- a/sklearn/ensemble/tests/test_gradient_boosting.py
+++ b/sklearn/ensemble/tests/test_gradient_boosting.py
@@ -962,8 +962,8 @@ def test_probability_exponential():
     assert np.all(y_proba >= 0.0)
     assert np.all(y_proba <= 1.0)
     score = clf.decision_function(T).ravel()
-    assert_array_equal(y_proba[:, 1],
-                       1.0 / (1.0 + np.exp(-2 * score)))
+    assert_array_almost_equal(y_proba[:, 1],
+                              1.0 / (1.0 + np.exp(-2 * score)))
 
     # derive predictions from probabilities
     y_pred = clf.classes_.take(y_proba.argmax(axis=1), axis=0)
@@ -990,14 +990,14 @@ def test_non_uniform_weights_toy_min_weight_leaf():
          [1, 0],
          [1, 0],
          [0, 1],
-        ]
+         ]
     y = [0, 0, 1, 0]
     # ignore the first 2 training samples by setting their weight to 0
     sample_weight = [0, 0, 1, 1]
     gb = GradientBoostingRegressor(n_estimators=5, min_weight_fraction_leaf=0.1)
     gb.fit(X, y, sample_weight=sample_weight)
     assert_true(gb.predict([[1, 0]])[0] > 0.5)
-    assert_almost_equal(gb.estimators_[0,0].splitter.min_weight_leaf, 0.2)
+    assert_almost_equal(gb.estimators_[0, 0].splitter.min_weight_leaf, 0.2)
 
 
 def test_non_uniform_weights_toy_edge_case_clf():
@@ -1012,8 +1012,3 @@ def test_non_uniform_weights_toy_edge_case_clf():
         gb = GradientBoostingClassifier(n_estimators=5)
         gb.fit(X, y, sample_weight=sample_weight)
         assert_array_equal(gb.predict([[1, 0]]), [1])
-
-
-if __name__ == "__main__":
-    import nose
-    nose.runmodule()
diff --git a/sklearn/ensemble/tests/test_weight_boosting.py b/sklearn/ensemble/tests/test_weight_boosting.py
index 8a5a99dfa5d3..ff45d8d8b9ae 100755
--- a/sklearn/ensemble/tests/test_weight_boosting.py
+++ b/sklearn/ensemble/tests/test_weight_boosting.py
@@ -419,8 +419,3 @@ def fit(self, X, y, sample_weight=None):
 
         assert all([(t == csc_matrix or t == csr_matrix)
                    for t in types])
-
-
-if __name__ == "__main__":
-    import nose
-    nose.runmodule()
diff --git a/sklearn/ensemble/voting_classifier.py b/sklearn/ensemble/voting_classifier.py
index d423e1c7c854..c940db85b90c 100644
--- a/sklearn/ensemble/voting_classifier.py
+++ b/sklearn/ensemble/voting_classifier.py
@@ -24,6 +24,8 @@
 class VotingClassifier(BaseEstimator, ClassifierMixin, TransformerMixin):
     """Soft Voting/Majority Rule classifier for unfitted estimators.
 
+    Read more in the :ref:`User Guide <voting_classifier>`.
+
     Parameters
     ----------
     estimators : list of (string, estimator) tuples
@@ -143,8 +145,7 @@ def predict(self, X):
 
         else:  # 'hard' voting
             predictions = self._predict(X)
-            maj = np.apply_along_axis(
-                                      lambda x:
+            maj = np.apply_along_axis(lambda x:
                                       np.argmax(np.bincount(x,
                                                 weights=self.weights)),
                                       axis=1,
diff --git a/sklearn/ensemble/weight_boosting.py b/sklearn/ensemble/weight_boosting.py
index 3a4187f6fae5..3d0faa1f2544 100644
--- a/sklearn/ensemble/weight_boosting.py
+++ b/sklearn/ensemble/weight_boosting.py
@@ -39,9 +39,7 @@
 from ..tree._tree import DTYPE
 from ..utils import check_array, check_X_y, check_random_state
 from ..metrics import accuracy_score, r2_score
-from sklearn.utils.validation import (
-        has_fit_parameter,
-        check_is_fitted)
+from sklearn.utils.validation import has_fit_parameter, check_is_fitted
 
 __all__ = [
     'AdaBoostClassifier',
@@ -271,6 +269,7 @@ def _validate_X_predict(self, X):
 
         return X
 
+
 def _samme_proba(estimator, n_classes, X):
     """Calculate algorithm 4, step 2, equation c) of Zhu et al [1].
 
@@ -302,6 +301,8 @@ class AdaBoostClassifier(BaseWeightBoosting, ClassifierMixin):
 
     This class implements the algorithm known as AdaBoost-SAMME [2].
 
+    Read more in the :ref:`User Guide <adaboost>`.
+
     Parameters
     ----------
     base_estimator : object, optional (default=DecisionTreeClassifier)
@@ -750,7 +751,7 @@ def predict_proba(self, X):
             outputs is the same of that of the `classes_` attribute.
         """
         check_is_fitted(self, "n_classes_")
-        
+
         n_classes = self.n_classes_
         X = self._validate_X_predict(X)
 
@@ -856,6 +857,8 @@ class AdaBoostRegressor(BaseWeightBoosting, RegressorMixin):
 
     This class implements the algorithm known as AdaBoost.R2 [2].
 
+    Read more in the :ref:`User Guide <adaboost>`.
+
     Parameters
     ----------
     base_estimator : object, optional (default=DecisionTreeRegressor)
diff --git a/sklearn/feature_extraction/dict_vectorizer.py b/sklearn/feature_extraction/dict_vectorizer.py
index fa05fce6ecb7..b0335bb53ad8 100644
--- a/sklearn/feature_extraction/dict_vectorizer.py
+++ b/sklearn/feature_extraction/dict_vectorizer.py
@@ -40,6 +40,8 @@ class DictVectorizer(BaseEstimator, TransformerMixin):
     Features that do not occur in a sample (mapping) will have a zero value
     in the resulting array/matrix.
 
+    Read more in the :ref:`User Guide <dict_feature_extraction>`.
+
     Parameters
     ----------
     dtype : callable, optional
@@ -322,7 +324,7 @@ def restrict(self, support, indices=False):
             Boolean mask or list of indices (as returned by the get_support
             member of feature selectors).
         indices : boolean, optional
-            Whether support is a list of indices. 
+            Whether support is a list of indices.
 
         Returns
         -------
diff --git a/sklearn/feature_extraction/hashing.py b/sklearn/feature_extraction/hashing.py
index 87ea0e35ff72..c69835808c0c 100644
--- a/sklearn/feature_extraction/hashing.py
+++ b/sklearn/feature_extraction/hashing.py
@@ -31,6 +31,8 @@ class FeatureHasher(BaseEstimator, TransformerMixin):
     where memory is tight, e.g. when running prediction code on embedded
     devices.
 
+    Read more in the :ref:`User Guide <feature_hashing>`.
+
     Parameters
     ----------
     n_features : integer, optional
diff --git a/sklearn/feature_extraction/image.py b/sklearn/feature_extraction/image.py
index 7ca737449aa8..7b9c5c94e0cd 100644
--- a/sklearn/feature_extraction/image.py
+++ b/sklearn/feature_extraction/image.py
@@ -135,21 +135,23 @@ def img_to_graph(img, mask=None, return_as=sparse.coo_matrix, dtype=None):
 
     Edges are weighted with the gradient values.
 
+    Read more in the :ref:`User Guide <image_feature_extraction>`.
+
     Parameters
-    ===========
-    img: ndarray, 2D or 3D
+    ----------
+    img : ndarray, 2D or 3D
         2D or 3D image
     mask : ndarray of booleans, optional
         An optional mask of the image, to consider only part of the
         pixels.
-    return_as: np.ndarray or a sparse matrix class, optional
+    return_as : np.ndarray or a sparse matrix class, optional
         The class to use to build the returned adjacency matrix.
-    dtype: None or dtype, optional
+    dtype : None or dtype, optional
         The data of the returned sparse matrix. By default it is the
         dtype of img
 
     Notes
-    =====
+    -----
     For sklearn versions 0.14.1 and prior, return_as=np.ndarray was handled
     by returning a dense np.matrix instance.  Going forward, np.ndarray
     returns an np.ndarray, as expected.
@@ -169,23 +171,23 @@ def grid_to_graph(n_x, n_y, n_z=1, mask=None, return_as=sparse.coo_matrix,
     Edges exist if 2 voxels are connected.
 
     Parameters
-    ===========
-    n_x: int
+    ----------
+    n_x : int
         Dimension in x axis
-    n_y: int
+    n_y : int
         Dimension in y axis
-    n_z: int, optional, default 1
+    n_z : int, optional, default 1
         Dimension in z axis
     mask : ndarray of booleans, optional
         An optional mask of the image, to consider only part of the
         pixels.
-    return_as: np.ndarray or a sparse matrix class, optional
+    return_as : np.ndarray or a sparse matrix class, optional
         The class to use to build the returned adjacency matrix.
-    dtype: dtype, optional, default int
+    dtype : dtype, optional, default int
         The data of the returned sparse matrix. By default it is int
 
     Notes
-    =====
+    -----
     For sklearn versions 0.14.1 and prior, return_as=np.ndarray was handled
     by returning a dense np.matrix instance.  Going forward, np.ndarray
     returns an np.ndarray, as expected.
@@ -203,17 +205,19 @@ def grid_to_graph(n_x, n_y, n_z=1, mask=None, return_as=sparse.coo_matrix,
 def _compute_n_patches(i_h, i_w, p_h, p_w, max_patches=None):
     """Compute the number of patches that will be extracted in an image.
 
+    Read more in the :ref:`User Guide <image_feature_extraction>`.
+
     Parameters
-    ===========
-    i_h: int
+    ----------
+    i_h : int
         The image height
-    i_w: int
+    i_w : int
         The image with
-    p_h: int
+    p_h : int
         The height of a patch
-    p_w: int
+    p_w : int
         The width of a patch
-    max_patches: integer or float, optional default is None
+    max_patches : integer or float, optional default is None
         The maximum number of patches to extract. If max_patches is a float
         between 0 and 1, it is taken to be a proportion of the total number
         of patches.
@@ -244,24 +248,26 @@ def extract_patches(arr, patch_shape=8, extraction_step=1):
     performed on the first n dimensions will cause numpy to copy data, leading
     to a list of extracted patches.
 
+    Read more in the :ref:`User Guide <image_feature_extraction>`.
+
     Parameters
     ----------
-    arr: ndarray
+    arr : ndarray
         n-dimensional array of which patches are to be extracted
 
-    patch_shape: integer or tuple of length arr.ndim
+    patch_shape : integer or tuple of length arr.ndim
         Indicates the shape of the patches to be extracted. If an
         integer is given, the shape will be a hypercube of
         sidelength given by its value.
 
-    extraction_step: integer or tuple of length arr.ndim
+    extraction_step : integer or tuple of length arr.ndim
         Indicates step size at which extraction shall be performed.
         If integer is given, then the step is uniform in all dimensions.
 
 
     Returns
     -------
-    patches: strided ndarray
+    patches : strided ndarray
         2n-dimensional array indexing patches on first n dimensions and
         containing patches on the last n dimensions. These dimensions
         are fake, but this way no data is copied. A simple reshape invokes
@@ -296,28 +302,30 @@ def extract_patches_2d(image, patch_size, max_patches=None, random_state=None):
 
     The resulting patches are allocated in a dedicated array.
 
+    Read more in the :ref:`User Guide <image_feature_extraction>`.
+
     Parameters
     ----------
-    image: array, shape = (image_height, image_width) or
+    image : array, shape = (image_height, image_width) or
         (image_height, image_width, n_channels)
         The original image data. For color images, the last dimension specifies
         the channel: a RGB image would have `n_channels=3`.
 
-    patch_size: tuple of ints (patch_height, patch_width)
+    patch_size : tuple of ints (patch_height, patch_width)
         the dimensions of one patch
 
-    max_patches: integer or float, optional default is None
+    max_patches : integer or float, optional default is None
         The maximum number of patches to extract. If max_patches is a float
         between 0 and 1, it is taken to be a proportion of the total number
         of patches.
 
-    random_state: int or RandomState
+    random_state : int or RandomState
         Pseudo number generator state used for random sampling to use if
         `max_patches` is not None.
 
     Returns
     -------
-    patches: array, shape = (n_patches, patch_height, patch_width) or
+    patches : array, shape = (n_patches, patch_height, patch_width) or
          (n_patches, patch_height, patch_width, n_channels)
          The collection of patches extracted from the image, where `n_patches`
          is either `max_patches` or the total number of patches that can be
@@ -389,21 +397,23 @@ def reconstruct_from_patches_2d(patches, image_size):
     the patches from left to right, top to bottom, averaging the overlapping
     regions.
 
+    Read more in the :ref:`User Guide <image_feature_extraction>`.
+
     Parameters
     ----------
-    patches: array, shape = (n_patches, patch_height, patch_width) or
+    patches : array, shape = (n_patches, patch_height, patch_width) or
         (n_patches, patch_height, patch_width, n_channels)
         The complete set of patches. If the patches contain colour information,
         channels are indexed along the last dimension: RGB patches would
         have `n_channels=3`.
 
-    image_size: tuple of ints (image_height, image_width) or
+    image_size : tuple of ints (image_height, image_width) or
         (image_height, image_width, n_channels)
         the size of the image that will be reconstructed
 
     Returns
     -------
-    image: array, shape = image_size
+    image : array, shape = image_size
         the reconstructed image
 
     """
@@ -428,17 +438,19 @@ def reconstruct_from_patches_2d(patches, image_size):
 class PatchExtractor(BaseEstimator):
     """Extracts patches from a collection of images
 
+    Read more in the :ref:`User Guide <image_feature_extraction>`.
+
     Parameters
     ----------
-    patch_size: tuple of ints (patch_height, patch_width)
+    patch_size : tuple of ints (patch_height, patch_width)
         the dimensions of one patch
 
-    max_patches: integer or float, optional default is None
+    max_patches : integer or float, optional default is None
         The maximum number of patches per image to extract. If max_patches is a
         float in (0, 1), it is taken to mean a proportion of the total number
         of patches.
 
-    random_state: int or RandomState
+    random_state : int or RandomState
         Pseudo number generator state used for random sampling.
 
     """
diff --git a/sklearn/feature_extraction/tests/test_image.py b/sklearn/feature_extraction/tests/test_image.py
index fd5a4bc57c87..9732cd97245b 100644
--- a/sklearn/feature_extraction/tests/test_image.py
+++ b/sklearn/feature_extraction/tests/test_image.py
@@ -288,8 +288,3 @@ def test_width_patch():
     x = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
     assert_raises(ValueError, extract_patches_2d, x, (4, 1))
     assert_raises(ValueError, extract_patches_2d, x, (1, 4))
-
-
-if __name__ == '__main__':
-    import nose
-    nose.runmodule()
diff --git a/sklearn/feature_extraction/text.py b/sklearn/feature_extraction/text.py
index e3aafc4bcc3d..632a22493545 100644
--- a/sklearn/feature_extraction/text.py
+++ b/sklearn/feature_extraction/text.py
@@ -317,6 +317,8 @@ class HashingVectorizer(BaseEstimator, VectorizerMixin):
 
     The hash function employed is the signed 32-bit version of Murmurhash3.
 
+    Read more in the :ref:`User Guide <text_feature_extraction>`.
+
     Parameters
     ----------
 
@@ -503,6 +505,8 @@ class CountVectorizer(BaseEstimator, VectorizerMixin):
     that does some kind of feature selection then the number of features will
     be equal to the vocabulary size found by analyzing the data.
 
+    Read more in the :ref:`User Guide <text_feature_extraction>`.
+
     Parameters
     ----------
     input : string {'filename', 'file', 'content'}
@@ -921,6 +925,8 @@ class TfidfTransformer(BaseEstimator, TransformerMixin):
     Idf is "t" when use_idf is given, "n" (none) otherwise.
     Normalization is "c" (cosine) when norm='l2', "n" (none) when norm=None.
 
+    Read more in the :ref:`User Guide <text_feature_extraction>`.
+
     Parameters
     ----------
     norm : 'l1', 'l2' or None, optional
@@ -1039,6 +1045,8 @@ class TfidfVectorizer(CountVectorizer):
 
     Equivalent to CountVectorizer followed by TfidfTransformer.
 
+    Read more in the :ref:`User Guide <text_feature_extraction>`.
+
     Parameters
     ----------
     input : string {'filename', 'file', 'content'}
diff --git a/sklearn/feature_selection/rfe.py b/sklearn/feature_selection/rfe.py
index 6c230617078d..1119e1a22f25 100644
--- a/sklearn/feature_selection/rfe.py
+++ b/sklearn/feature_selection/rfe.py
@@ -14,7 +14,7 @@
 from ..base import MetaEstimatorMixin
 from ..base import clone
 from ..base import is_classifier
-from ..cross_validation import _check_cv as check_cv
+from ..cross_validation import check_cv
 from ..cross_validation import _safe_split, _score
 from ..metrics.scorer import check_scoring
 from .base import SelectorMixin
@@ -32,6 +32,8 @@ class RFE(BaseEstimator, MetaEstimatorMixin, SelectorMixin):
     That procedure is recursively repeated on the pruned set until the desired
     number of features to select is eventually reached.
 
+    Read more in the :ref:`User Guide <rfe>`.
+
     Parameters
     ----------
     estimator : object
@@ -110,6 +112,10 @@ def __init__(self, estimator, n_features_to_select=None, step=1,
         self.estimator_params = estimator_params
         self.verbose = verbose
 
+    @property
+    def _estimator_type(self):
+        return self.estimator._estimator_type
+
     def fit(self, X, y):
         """Fit the RFE model and then the underlying estimator on the selected
            features.
@@ -265,6 +271,8 @@ class RFECV(RFE, MetaEstimatorMixin):
     """Feature ranking with recursive feature elimination and cross-validated
     selection of the best number of features.
 
+    Read more in the :ref:`User Guide <rfe>`.
+
     Parameters
     ----------
     estimator : object
diff --git a/sklearn/feature_selection/tests/test_rfe.py b/sklearn/feature_selection/tests/test_rfe.py
index 65747b772812..b768e271973e 100644
--- a/sklearn/feature_selection/tests/test_rfe.py
+++ b/sklearn/feature_selection/tests/test_rfe.py
@@ -8,15 +8,16 @@
 from scipy import sparse
 
 from sklearn.feature_selection.rfe import RFE, RFECV
-from sklearn.datasets import load_iris, make_friedman1, make_regression
+from sklearn.datasets import load_iris, make_friedman1
 from sklearn.metrics import zero_one_loss
 from sklearn.svm import SVC, SVR
-from sklearn.linear_model import LinearRegression
 from sklearn.ensemble import RandomForestClassifier
+from sklearn.cross_validation import cross_val_score
 
 from sklearn.utils import check_random_state
 from sklearn.utils.testing import ignore_warnings
 from sklearn.utils.testing import assert_warns_message
+from sklearn.utils.testing import assert_greater
 
 from sklearn.metrics import make_scorer
 from sklearn.metrics import get_scorer
@@ -94,7 +95,6 @@ def test_rfe_features_importance():
     assert_array_equal(rfe.get_support(), rfe_svc.get_support())
 
 
-
 def test_rfe_deprecation_estimator_params():
     deprecation_message = ("The parameter 'estimator_params' is deprecated as "
                            "of version 0.16 and will be removed in 0.18. The "
@@ -240,6 +240,15 @@ def test_rfecv_mockclassifier():
     assert_equal(len(rfecv.ranking_), X.shape[1])
 
 
+def test_rfe_estimator_tags():
+    rfe = RFE(SVC(kernel='linear'))
+    assert_equal(rfe._estimator_type, "classifier")
+    # make sure that cross-validation is stratified
+    iris = load_iris()
+    score = cross_val_score(rfe, iris.data, iris.target)
+    assert_greater(score.min(), .7)
+
+
 def test_rfe_min_step():
     n_features = 10
     X, y = make_friedman1(n_samples=50, n_features=n_features, random_state=0)
@@ -289,7 +298,7 @@ def formula2(n_features, n_features_to_select, step):
         X = generator.normal(size=(100, n_features))
         y = generator.rand(100).round()
         rfe = RFE(estimator=SVC(kernel="linear"),
-              n_features_to_select=n_features_to_select, step=step)
+                  n_features_to_select=n_features_to_select, step=step)
         rfe.fit(X, y)
         # this number also equals to the maximum of ranking_
         assert_equal(np.max(rfe.ranking_),
@@ -317,6 +326,6 @@ def formula2(n_features, n_features_to_select, step):
         rfecv.fit(X, y)
 
         assert_equal(rfecv.grid_scores_.shape[0],
-                 formula1(n_features, n_features_to_select, step))
+                     formula1(n_features, n_features_to_select, step))
         assert_equal(rfecv.grid_scores_.shape[0],
-                 formula2(n_features, n_features_to_select, step))
+                     formula2(n_features, n_features_to_select, step))
diff --git a/sklearn/feature_selection/univariate_selection.py b/sklearn/feature_selection/univariate_selection.py
index 69a2198b695e..108ab660a604 100644
--- a/sklearn/feature_selection/univariate_selection.py
+++ b/sklearn/feature_selection/univariate_selection.py
@@ -46,6 +46,8 @@ def f_oneway(*args):
     the same population mean. The test is applied to samples from two or
     more groups, possibly with differing sizes.
 
+    Read more in the :ref:`User Guide <univariate_feature_selection>`.
+
     Parameters
     ----------
     sample1, sample2, ... : array_like, sparse matrices
@@ -119,6 +121,8 @@ def f_oneway(*args):
 def f_classif(X, y):
     """Compute the ANOVA F-value for the provided sample.
 
+    Read more in the :ref:`User Guide <univariate_feature_selection>`.
+
     Parameters
     ----------
     X : {array-like, sparse matrix} shape = [n_samples, n_features]
@@ -176,6 +180,8 @@ def chi2(X, y):
     most likely to be independent of class and therefore irrelevant for
     classification.
 
+    Read more in the :ref:`User Guide <univariate_feature_selection>`.
+
     Parameters
     ----------
     X : {array-like, sparse matrix}, shape = (n_samples, n_features_in)
@@ -233,6 +239,8 @@ def f_regression(X, y, center=True):
     2. The cross correlation between data and regressors is computed.
     3. It is converted to an F score then to a p-value.
 
+    Read more in the :ref:`User Guide <univariate_feature_selection>`.
+
     Parameters
     ----------
     X : {array-like, sparse matrix}  shape = (n_samples, n_features)
@@ -335,6 +343,8 @@ def _check_params(self, X, y):
 class SelectPercentile(_BaseFilter):
     """Select features according to a percentile of the highest scores.
 
+    Read more in the :ref:`User Guide <univariate_feature_selection>`.
+
     Parameters
     ----------
     score_func : callable
@@ -402,6 +412,8 @@ def _get_support_mask(self):
 class SelectKBest(_BaseFilter):
     """Select features according to the k highest scores.
 
+    Read more in the :ref:`User Guide <univariate_feature_selection>`.
+
     Parameters
     ----------
     score_func : callable
@@ -470,6 +482,8 @@ class SelectFpr(_BaseFilter):
     FPR test stands for False Positive Rate test. It controls the total
     amount of false detections.
 
+    Read more in the :ref:`User Guide <univariate_feature_selection>`.
+
     Parameters
     ----------
     score_func : callable
@@ -515,6 +529,8 @@ class SelectFdr(_BaseFilter):
     This uses the Benjamini-Hochberg procedure. ``alpha`` is an upper bound
     on the expected false discovery rate.
 
+    Read more in the :ref:`User Guide <univariate_feature_selection>`.
+
     Parameters
     ----------
     score_func : callable
@@ -568,6 +584,8 @@ def _get_support_mask(self):
 class SelectFwe(_BaseFilter):
     """Filter: Select the p-values corresponding to Family-wise error rate
 
+    Read more in the :ref:`User Guide <univariate_feature_selection>`.
+
     Parameters
     ----------
     score_func : callable
@@ -616,6 +634,8 @@ def _get_support_mask(self):
 class GenericUnivariateSelect(_BaseFilter):
     """Univariate feature selector with configurable strategy.
 
+    Read more in the :ref:`User Guide <univariate_feature_selection>`.
+
     Parameters
     ----------
     score_func : callable
@@ -648,11 +668,11 @@ class GenericUnivariateSelect(_BaseFilter):
     SelectFwe: Select features based on family-wise error rate.
     """
 
-    _selection_modes = {'percentile':   SelectPercentile,
-                        'k_best':       SelectKBest,
-                        'fpr':          SelectFpr,
-                        'fdr':          SelectFdr,
-                        'fwe':          SelectFwe}
+    _selection_modes = {'percentile': SelectPercentile,
+                        'k_best': SelectKBest,
+                        'fpr': SelectFpr,
+                        'fdr': SelectFdr,
+                        'fwe': SelectFwe}
 
     def __init__(self, score_func=f_classif, mode='percentile', param=1e-5):
         super(GenericUnivariateSelect, self).__init__(score_func)
diff --git a/sklearn/feature_selection/variance_threshold.py b/sklearn/feature_selection/variance_threshold.py
index 202a86d1a88b..23987a8ece56 100644
--- a/sklearn/feature_selection/variance_threshold.py
+++ b/sklearn/feature_selection/variance_threshold.py
@@ -15,6 +15,8 @@ class VarianceThreshold(BaseEstimator, SelectorMixin):
     This feature selection algorithm looks only at the features (X), not the
     desired outputs (y), and can thus be used for unsupervised learning.
 
+    Read more in the :ref:`User Guide <variance_threshold>`.
+
     Parameters
     ----------
     threshold : float, optional
diff --git a/sklearn/gaussian_process/gaussian_process.py b/sklearn/gaussian_process/gaussian_process.py
index d1507f98414f..542bf14f2819 100644
--- a/sklearn/gaussian_process/gaussian_process.py
+++ b/sklearn/gaussian_process/gaussian_process.py
@@ -59,6 +59,8 @@ def l1_cross_distances(X):
 class GaussianProcess(BaseEstimator, RegressorMixin):
     """The Gaussian Process model class.
 
+    Read more in the :ref:`User Guide <gaussian_process>`.
+
     Parameters
     ----------
     regr : string or callable, optional
diff --git a/sklearn/grid_search.py b/sklearn/grid_search.py
index 021dcf7938f6..3e13a134a624 100644
--- a/sklearn/grid_search.py
+++ b/sklearn/grid_search.py
@@ -20,8 +20,8 @@
 import numpy as np
 
 from .base import BaseEstimator, is_classifier, clone
-from .base import MetaEstimatorMixin
-from .cross_validation import _check_cv as check_cv
+from .base import MetaEstimatorMixin, ChangedBehaviorWarning
+from .cross_validation import check_cv
 from .cross_validation import _fit_and_score
 from .externals.joblib import Parallel, delayed
 from .externals import six
@@ -42,6 +42,8 @@ class ParameterGrid(object):
     Can be used to iterate over parameter value combinations with the
     Python built-in function iter.
 
+    Read more in the :ref:`User Guide <grid_search>`.
+
     Parameters
     ----------
     param_grid : dict of string to sequence, or sequence of such
@@ -126,6 +128,8 @@ class ParameterSampler(object):
     deterministic iteration whenever ``scipy.stats`` distributions are used to
     define the parameter search space.
 
+    Read more in the :ref:`User Guide <grid_search>`.
+
     Parameters
     ----------
     param_distributions : dict
@@ -304,10 +308,6 @@ def __repr__(self):
             self.parameters)
 
 
-class ChangedBehaviorWarning(UserWarning):
-    pass
-
-
 class BaseSearchCV(six.with_metaclass(ABCMeta, BaseEstimator,
                                       MetaEstimatorMixin)):
     """Base class for hyper parameter search with cross-validation."""
@@ -566,6 +566,8 @@ class GridSearchCV(BaseSearchCV):
     any classifier except that the parameters of the classifier
     used to predict is optimized by cross-validation.
 
+    Read more in the :ref:`User Guide <grid_search>`.
+
     Parameters
     ----------
     estimator : object type that implements the "fit" and "predict" methods
@@ -642,9 +644,10 @@ class GridSearchCV(BaseSearchCV):
     ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
     GridSearchCV(cv=None, error_score=...,
            estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,
-                         degree=..., gamma=..., kernel='rbf', max_iter=-1,
-                         probability=False, random_state=None, shrinking=True,
-                         tol=..., verbose=False),
+                         decision_function_shape=None, degree=..., gamma=...,
+                         kernel='rbf', max_iter=-1, probability=False,
+                         random_state=None, shrinking=True, tol=...,
+                         verbose=False),
            fit_params={}, iid=..., n_jobs=1,
            param_grid=..., pre_dispatch=..., refit=...,
            scoring=..., verbose=...)
@@ -751,6 +754,8 @@ class RandomizedSearchCV(BaseSearchCV):
     It is highly recommended to use continuous distributions for continuous
     parameters.
 
+    Read more in the :ref:`User Guide <randomized_parameter_search>`.
+
     Parameters
     ----------
     estimator : object type that implements the "fit" and "predict" methods
diff --git a/sklearn/isotonic.py b/sklearn/isotonic.py
index 0d59bd215043..e4bafa53a826 100644
--- a/sklearn/isotonic.py
+++ b/sklearn/isotonic.py
@@ -88,6 +88,8 @@ def isotonic_regression(y, sample_weight=None, y_min=None, y_max=None,
         - y_[i] are fitted
         - w[i] are optional strictly positive weights (default to 1.0)
 
+    Read more in the :ref:`User Guide <isotonic>`.
+
     Parameters
     ----------
     y : iterable of floating-point values
@@ -163,6 +165,8 @@ class IsotonicRegression(BaseEstimator, TransformerMixin, RegressorMixin):
           If ``X`` is non-decreasing then ``y_`` is non-decreasing.
         - ``w[i]`` are optional strictly positive weights (default to 1.0)
 
+    Read more in the :ref:`User Guide <isotonic>`.
+
     Parameters
     ----------
     y_min : optional, default: None
diff --git a/sklearn/kernel_approximation.py b/sklearn/kernel_approximation.py
index dc29e7c3f805..9be7b114c482 100644
--- a/sklearn/kernel_approximation.py
+++ b/sklearn/kernel_approximation.py
@@ -24,9 +24,11 @@
 class RBFSampler(BaseEstimator, TransformerMixin):
     """Approximates feature map of an RBF kernel by Monte Carlo approximation
     of its Fourier transform.
-    
+
     It implements a variant of Random Kitchen Sinks.[1]
 
+    Read more in the :ref:`User Guide <rbf_kernel_approx>`.
+
     Parameters
     ----------
     gamma : float
@@ -111,6 +113,8 @@ class SkewedChi2Sampler(BaseEstimator, TransformerMixin):
     """Approximates feature map of the "skewed chi-squared" kernel by Monte
     Carlo approximation of its Fourier transform.
 
+    Read more in the :ref:`User Guide <skewed_chi_kernel_approx>`.
+
     Parameters
     ----------
     skewedness : float
@@ -214,6 +218,8 @@ class AdditiveChi2Sampler(BaseEstimator, TransformerMixin):
     Optimal choices for the sampling interval for certain data ranges can be
     computed (see the reference). The default values should be reasonable.
 
+    Read more in the :ref:`User Guide <additive_chi_kernel_approx>`.
+
     Parameters
     ----------
     sample_steps : int, optional
@@ -358,6 +364,8 @@ class Nystroem(BaseEstimator, TransformerMixin):
     Constructs an approximate feature map for an arbitrary kernel
     using a subset of the data as basis.
 
+    Read more in the :ref:`User Guide <nystroem_kernel_approx>`.
+
     Parameters
     ----------
     kernel : string or callable, default="rbf"
diff --git a/sklearn/kernel_ridge.py b/sklearn/kernel_ridge.py
index 9dccbc684134..80be07e8a221 100644
--- a/sklearn/kernel_ridge.py
+++ b/sklearn/kernel_ridge.py
@@ -34,6 +34,8 @@ class KernelRidge(BaseEstimator, RegressorMixin):
     This estimator has built-in support for multi-variate regression
     (i.e., when y is a 2d-array of shape [n_samples, n_targets]).
 
+    Read more in the :ref:`User Guide <kernel_ridge>`.
+
     Parameters
     ----------
     alpha : {float, array-like}, shape = [n_targets]
@@ -143,7 +145,6 @@ def fit(self, X, y=None, sample_weight=None):
         # Convert data
         X, y = check_X_y(X, y, accept_sparse=("csr", "csc"), multi_output=True)
 
-        n_samples = X.shape[0]
         K = self._get_kernel(X)
         alpha = np.atleast_1d(self.alpha)
 
diff --git a/sklearn/lda.py b/sklearn/lda.py
index f95960b480d9..6a6da317015a 100644
--- a/sklearn/lda.py
+++ b/sklearn/lda.py
@@ -136,6 +136,8 @@ class LDA(BaseEstimator, LinearClassifierMixin, TransformerMixin):
     The fitted model can also be used to reduce the dimensionality of the input
     by projecting it to the most discriminative directions.
 
+    Read more in the :ref:`User Guide <lda_qda>`.
+
     Parameters
     ----------
     solver : string, optional
diff --git a/sklearn/learning_curve.py b/sklearn/learning_curve.py
index 7171cecc4d77..6b64bb2aa1fa 100644
--- a/sklearn/learning_curve.py
+++ b/sklearn/learning_curve.py
@@ -9,7 +9,7 @@
 import numpy as np
 
 from .base import is_classifier, clone
-from .cross_validation import _check_cv
+from .cross_validation import check_cv
 from .externals.joblib import Parallel, delayed
 from .cross_validation import _safe_split, _score, _fit_and_score
 from .metrics.scorer import check_scoring
@@ -34,6 +34,8 @@ def learning_curve(estimator, X, y, train_sizes=np.linspace(0.1, 1.0, 5),
     test set will be computed. Afterwards, the scores will be averaged over
     all k runs for each training subset size.
 
+    Read more in the :ref:`User Guide <learning_curves>`.
+
     Parameters
     ----------
     estimator : object type that implements the "fit" and "predict" methods
@@ -106,7 +108,7 @@ def learning_curve(estimator, X, y, train_sizes=np.linspace(0.1, 1.0, 5),
 
     X, y = indexable(X, y)
     # Make a list since we will be iterating multiple times over the folds
-    cv = list(_check_cv(cv, X, y, classifier=is_classifier(estimator)))
+    cv = list(check_cv(cv, X, y, classifier=is_classifier(estimator)))
     scorer = check_scoring(estimator, scoring=scoring)
 
     # HACK as long as boolean indices are allowed in cv generators
@@ -238,6 +240,8 @@ def validation_curve(estimator, X, y, param_name, param_range, cv=None,
     will also compute training scores and is merely a utility for plotting the
     results.
 
+    Read more in the :ref:`User Guide <validation_curve>`.
+
     Parameters
     ----------
     estimator : object type that implements the "fit" and "predict" methods
@@ -293,7 +297,7 @@ def validation_curve(estimator, X, y, param_name, param_range, cv=None,
     <example_model_selection_plot_validation_curve.py>`
     """
     X, y = indexable(X, y)
-    cv = _check_cv(cv, X, y, classifier=is_classifier(estimator))
+    cv = check_cv(cv, X, y, classifier=is_classifier(estimator))
     scorer = check_scoring(estimator, scoring=scoring)
 
     parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch,
diff --git a/sklearn/linear_model/bayes.py b/sklearn/linear_model/bayes.py
index 2f40648527ad..6e291e1846d2 100644
--- a/sklearn/linear_model/bayes.py
+++ b/sklearn/linear_model/bayes.py
@@ -25,6 +25,8 @@ class BayesianRidge(LinearModel, RegressorMixin):
     Fit a Bayesian ridge model and optimize the regularization parameters
     lambda (precision of the weights) and alpha (precision of the noise).
 
+    Read more in the :ref:`User Guide <bayesian_regression>`.
+
     Parameters
     ----------
     n_iter : int, optional
@@ -225,6 +227,8 @@ class ARDRegression(LinearModel, RegressorMixin):
     weights) and alpha (precision of the distribution of the noise).
     The estimation is done by an iterative procedures (Evidence Maximization)
 
+    Read more in the :ref:`User Guide <bayesian_regression>`.
+
     Parameters
     ----------
     n_iter : int, optional
diff --git a/sklearn/linear_model/coordinate_descent.py b/sklearn/linear_model/coordinate_descent.py
index 886e3e7da8fa..3adbfe9c6b09 100644
--- a/sklearn/linear_model/coordinate_descent.py
+++ b/sklearn/linear_model/coordinate_descent.py
@@ -17,7 +17,7 @@
 from .base import center_data, sparse_center_data
 from ..utils import check_array, check_X_y, deprecated
 from ..utils.validation import check_random_state
-from ..cross_validation import _check_cv as check_cv
+from ..cross_validation import check_cv
 from ..externals.joblib import Parallel, delayed
 from ..externals import six
 from ..externals.six.moves import xrange
@@ -132,6 +132,8 @@ def lasso_path(X, y, eps=1e-3, n_alphas=100, alphas=None,
 
     i.e. the sum of norm of each row.
 
+    Read more in the :ref:`User Guide <lasso>`.
+
     Parameters
     ----------
     X : {array-like, sparse matrix}, shape (n_samples, n_features)
@@ -274,6 +276,8 @@ def enet_path(X, y, l1_ratio=0.5, eps=1e-3, n_alphas=100, alphas=None,
 
     i.e. the sum of norm of each row.
 
+    Read more in the :ref:`User Guide <elastic_net>`.
+
     Parameters
     ----------
     X : {array-like}, shape (n_samples, n_features)
@@ -483,6 +487,8 @@ class ElasticNet(LinearModel, RegressorMixin):
     = 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable,
     unless you supply your own sequence of alpha.
 
+    Read more in the :ref:`User Guide <elastic_net>`.
+
     Parameters
     ----------
     alpha : float
@@ -737,6 +743,8 @@ class Lasso(ElasticNet):
     Technically the Lasso model is optimizing the same objective function as
     the Elastic Net with ``l1_ratio=1.0`` (no L2 penalty).
 
+    Read more in the :ref:`User Guide <lasso>`.
+
     Parameters
     ----------
     alpha : float, optional
@@ -1161,6 +1169,8 @@ class LassoCV(LinearModelCV, RegressorMixin):
 
         (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1
 
+    Read more in the :ref:`User Guide <lasso>`.
+
     Parameters
     ----------
     eps : float, optional
@@ -1286,6 +1296,8 @@ class ElasticNetCV(LinearModelCV, RegressorMixin):
 
     The best model is selected by cross-validation.
 
+    Read more in the :ref:`User Guide <elastic_net>`.
+
     Parameters
     ----------
     l1_ratio : float, optional
@@ -1464,6 +1476,8 @@ class MultiTaskElasticNet(Lasso):
 
     i.e. the sum of norm of each row.
 
+    Read more in the :ref:`User Guide <multi_task_lasso>`.
+
     Parameters
     ----------
     alpha : float, optional
@@ -1647,6 +1661,8 @@ class MultiTaskLasso(MultiTaskElasticNet):
 
     i.e. the sum of norm of earch row.
 
+    Read more in the :ref:`User Guide <multi_task_lasso>`.
+
     Parameters
     ----------
     alpha : float, optional
@@ -1755,6 +1771,8 @@ class MultiTaskElasticNetCV(LinearModelCV, RegressorMixin):
 
     i.e. the sum of norm of each row.
 
+    Read more in the :ref:`User Guide <multi_task_lasso>`.
+
     Parameters
     ----------
     eps : float, optional
@@ -1909,6 +1927,8 @@ class MultiTaskLassoCV(LinearModelCV, RegressorMixin):
 
     i.e. the sum of norm of each row.
 
+    Read more in the :ref:`User Guide <multi_task_lasso>`.
+
     Parameters
     ----------
     eps : float, optional
diff --git a/sklearn/linear_model/least_angle.py b/sklearn/linear_model/least_angle.py
index 15c0704f32fc..f4d51a5b9bef 100644
--- a/sklearn/linear_model/least_angle.py
+++ b/sklearn/linear_model/least_angle.py
@@ -22,7 +22,7 @@
 from .base import LinearModel
 from ..base import RegressorMixin
 from ..utils import arrayfuncs, as_float_array, check_X_y
-from ..cross_validation import _check_cv as check_cv
+from ..cross_validation import check_cv
 from ..utils import ConvergenceWarning
 from ..externals.joblib import Parallel, delayed
 from ..externals.six.moves import xrange
@@ -47,6 +47,8 @@ def lars_path(X, y, Xy=None, Gram=None, max_iter=500,
     in the case of method='lars', the objective function is only known in
     the form of an implicit equation (see discussion in [1])
 
+    Read more in the :ref:`User Guide <least_angle_regression>`.
+
     Parameters
     -----------
     X : array, shape: (n_samples, n_features)
@@ -465,6 +467,8 @@ def lars_path(X, y, Xy=None, Gram=None, max_iter=500,
 class Lars(LinearModel, RegressorMixin):
     """Least Angle Regression model a.k.a. LAR
 
+    Read more in the :ref:`User Guide <least_angle_regression>`.
+
     Parameters
     ----------
     n_nonzero_coefs : int, optional
@@ -671,6 +675,8 @@ class LassoLars(Lars):
 
     (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1
 
+    Read more in the :ref:`User Guide <least_angle_regression>`.
+
     Parameters
     ----------
     alpha : float
@@ -798,32 +804,43 @@ def _lars_path_residues(X_train, y_train, X_test, y_test, Gram=None,
     -----------
     X_train : array, shape (n_samples, n_features)
         The data to fit the LARS on
+
     y_train : array, shape (n_samples)
         The target variable to fit LARS on
+
     X_test : array, shape (n_samples, n_features)
         The data to compute the residues on
+
     y_test : array, shape (n_samples)
         The target variable to compute the residues on
+
     Gram : None, 'auto', array, shape: (n_features, n_features), optional
         Precomputed Gram matrix (X' * X), if ``'auto'``, the Gram
         matrix is precomputed from the given X, if there are more samples
         than features
+
     copy : boolean, optional
         Whether X_train, X_test, y_train and y_test should be copied;
         if False, they may be overwritten.
+
     method : 'lar' | 'lasso'
         Specifies the returned model. Select ``'lar'`` for Least Angle
         Regression, ``'lasso'`` for the Lasso.
+
     verbose : integer, optional
         Sets the amount of verbosity
+
     fit_intercept : boolean
         whether to calculate the intercept for this model. If set
         to false, no intercept will be used in calculations
         (e.g. data is expected to be already centered).
+
     normalize : boolean, optional, default False
         If True, the regressors X will be normalized before regression.
+
     max_iter : integer, optional
         Maximum number of iterations to perform.
+
     eps : float, optional
         The machine-precision regularization in the computation of the
         Cholesky diagonal factors. Increase this for very ill-conditioned
@@ -880,6 +897,8 @@ def _lars_path_residues(X_train, y_train, X_test, y_test, Gram=None,
 class LarsCV(Lars):
     """Cross-validated Least Angle Regression model
 
+    Read more in the :ref:`User Guide <least_angle_regression>`.
+
     Parameters
     ----------
     fit_intercept : boolean
@@ -1056,6 +1075,8 @@ class LassoLarsCV(LarsCV):
 
     (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1
 
+    Read more in the :ref:`User Guide <least_angle_regression>`.
+
     Parameters
     ----------
     fit_intercept : boolean
@@ -1157,6 +1178,8 @@ class LassoLarsIC(LassoLars):
     goodness of fit and the complexity of the model. A good model should
     explain well the data while being simple.
 
+    Read more in the :ref:`User Guide <least_angle_regression>`.
+
     Parameters
     ----------
     criterion : 'bic' | 'aic'
@@ -1262,7 +1285,7 @@ def fit(self, X, y, copy_X=True):
 
         y : array-like, shape (n_samples,)
             target values.
-    
+
         copy_X : boolean, optional, default True
             If ``True``, X will be copied; else, it may be overwritten.
 
diff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py
index 660073fee35c..02be4d4154dc 100644
--- a/sklearn/linear_model/logistic.py
+++ b/sklearn/linear_model/logistic.py
@@ -28,7 +28,7 @@
                                 check_X_y)
 from ..utils.fixes import expit
 from ..externals.joblib import Parallel, delayed
-from ..cross_validation import _check_cv
+from ..cross_validation import check_cv
 from ..externals import six
 from ..metrics import SCORERS
 
@@ -427,6 +427,8 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
     to speed up computations along the set of solutions, making it faster
     than sequentially calling LogisticRegression for the different parameters.
 
+    Read more in the :ref:`User Guide <logistic_regression>`.
+
     Parameters
     ----------
     X : array-like or sparse matrix, shape (n_samples, n_features)
@@ -473,11 +475,13 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
         is called repeatedly with the same data, as y is modified
         along the path.
 
-    class_weight : {dict, 'auto'}, optional
-        Over-/undersamples the samples of each class according to the given
-        weights. If not given, all classes are supposed to have weight one.
-        The 'auto' mode selects weights inversely proportional to class
-        frequencies in the training set.
+    class_weight : dict or 'balanced', optional
+        Weights associated with classes in the form ``{class_label: weight}``.
+        If not given, all classes are supposed to have weight one.
+
+        The "balanced" mode uses the values of y to automatically adjust
+        weights inversely proportional to class frequencies in the input data
+        as ``n_samples / (n_classes * np.bincount(y))``
 
     dual : bool
         Dual or primal formulation. Dual formulation is only implemented for
@@ -599,8 +603,7 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
             if coef.size not in (n_features, w0.size):
                 raise ValueError(
                     'Initialization coef is of shape %d, expected shape '
-                    '%d or %d' % (coef.size, n_features, w0.size)
-                    )
+                    '%d or %d' % (coef.size, n_features, w0.size))
             w0[:coef.size] = coef
         else:
             # For binary problems coef.shape[0] should be 1, otherwise it
@@ -615,9 +618,7 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
                     'Initialization coef is of shape (%d, %d), expected '
                     'shape (%d, %d) or (%d, %d)' % (
                         coef.shape[0], coef.shape[1], classes.size,
-                        n_features, classes.size, n_features + 1
-                        )
-                    )
+                        n_features, classes.size, n_features + 1))
             w0[:, :coef.shape[1]] = coef
 
     if multi_class == 'multinomial':
@@ -647,15 +648,13 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
                 w0, loss, info = optimize.fmin_l_bfgs_b(
                     func, w0, fprime=None,
                     args=(X, target, 1. / C, sample_weight),
-                    iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter
-                    )
+                    iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
             except TypeError:
                 # old scipy doesn't have maxiter
                 w0, loss, info = optimize.fmin_l_bfgs_b(
                     func, w0, fprime=None,
                     args=(X, target, 1. / C, sample_weight),
-                    iprint=(verbose > 0) - 1, pgtol=tol
-                    )
+                    iprint=(verbose > 0) - 1, pgtol=tol)
             if info["warnflag"] == 1 and verbose > 0:
                 warnings.warn("lbfgs failed to converge. Increase the number "
                               "of iterations.")
@@ -666,8 +665,7 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
         elif solver == 'liblinear':
             coef_, intercept_, _, = _fit_liblinear(
                 X, y, C, fit_intercept, intercept_scaling, class_weight,
-                penalty, dual, verbose, max_iter, tol, random_state
-                )
+                penalty, dual, verbose, max_iter, tol, random_state)
             if fit_intercept:
                 w0 = np.concatenate([coef_.ravel(), intercept_])
             else:
@@ -734,11 +732,13 @@ def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,
     tol : float
         Tolerance for stopping criteria.
 
-    class_weight : {dict, 'auto'}, optional
-        Over-/undersamples the samples of each class according to the given
-        weights. If not given, all classes are supposed to have weight one.
-        The 'auto' mode selects weights inversely proportional to class
-        frequencies in the training set.
+    class_weight : dict or 'balanced', optional
+        Weights associated with classes in the form ``{class_label: weight}``.
+        If not given, all classes are supposed to have weight one.
+
+        The "balanced" mode uses the values of y to automatically adjust
+        weights inversely proportional to class frequencies in the input data
+        as ``n_samples / (n_classes * np.bincount(y))``
 
     verbose : int
         For the liblinear and lbfgs solvers set verbose to any positive
@@ -871,6 +871,8 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,
     formulation. The liblinear solver supports both L1 and L2 regularization,
     with a dual formulation only for the L2 penalty.
 
+    Read more in the :ref:`User Guide <logistic_regression>`.
+
     Parameters
     ----------
     penalty : str, 'l1' or 'l2'
@@ -903,11 +905,13 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,
         To lessen the effect of regularization on synthetic feature weight
         (and therefore on the intercept) intercept_scaling has to be increased.
 
-    class_weight : {dict, 'auto'}, optional
-        Over-/undersamples the samples of each class according to the given
-        weights. If not given, all classes are supposed to have weight one.
-        The 'auto' mode selects weights inversely proportional to class
-        frequencies in the training set.
+    class_weight : dict or 'balanced', optional
+        Weights associated with classes in the form ``{class_label: weight}``.
+        If not given, all classes are supposed to have weight one.
+
+        The "balanced" mode uses the values of y to automatically adjust
+        weights inversely proportional to class frequencies in the input data
+        as ``n_samples / (n_classes * np.bincount(y))``
 
     max_iter : int
         Useful only for the newton-cg and lbfgs solvers. Maximum number of
@@ -1036,8 +1040,7 @@ def fit(self, X, y):
             self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(
                 X, y, self.C, self.fit_intercept, self.intercept_scaling,
                 self.class_weight, self.penalty, self.dual, self.verbose,
-                self.max_iter, self.tol, self.random_state
-                )
+                self.max_iter, self.tol, self.random_state)
             return self
 
         n_classes = len(self.classes_)
@@ -1137,6 +1140,8 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,
     using the best scores got by doing a one-vs-rest in parallel across all
     folds and classes. Hence this is not the true multinomial loss.
 
+    Read more in the :ref:`User Guide <logistic_regression>`.
+
     Parameters
     ----------
     Cs : list of floats | int
@@ -1150,11 +1155,13 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,
         Specifies if a constant (a.k.a. bias or intercept) should be
         added the decision function.
 
-    class_weight : {dict, 'auto'}, optional
-        Over-/undersamples the samples of each class according to the given
-        weights. If not given, all classes are supposed to have weight one.
-        The 'auto' mode selects weights inversely proportional to class
-        frequencies in the training set.
+    class_weight : dict or 'balanced', optional
+        Weights associated with classes in the form ``{class_label: weight}``.
+        If not given, all classes are supposed to have weight one.
+
+        The "balanced" mode uses the values of y to automatically adjust
+        weights inversely proportional to class frequencies in the input data
+        as ``n_samples / (n_classes * np.bincount(y))``
 
     cv : integer or cross-validation generator
         The default cross-validation generator used is Stratified K-Folds.
@@ -1185,11 +1192,13 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,
     max_iter : int, optional
         Maximum number of iterations of the optimization algorithm.
 
-    class_weight : {dict, 'auto'}, optional
-        Over-/undersamples the samples of each class according to the given
-        weights. If not given, all classes are supposed to have weight one.
-        The 'auto' mode selects weights inversely proportional to class
-        frequencies in the training set.
+    class_weight : dict or 'balanced', optional
+        Weights associated with classes in the form ``{class_label: weight}``.
+        If not given, all classes are supposed to have weight one.
+
+        The "balanced" mode uses the values of y to automatically adjust
+        weights inversely proportional to class frequencies in the input data
+        as ``n_samples / (n_classes * np.bincount(y))``
 
     n_jobs : int, optional
         Number of CPU cores used during the cross-validation loop. If given
@@ -1330,14 +1339,13 @@ def fit(self, X, y):
                 "A column-vector y was passed when a 1d array was"
                 " expected. Please change the shape of y to "
                 "(n_samples, ), for example using ravel().",
-                DataConversionWarning
-                )
+                DataConversionWarning)
             y = np.ravel(y)
 
         check_consistent_length(X, y)
 
         # init cross-validation generator
-        cv = _check_cv(self.cv, X, y, classifier=True)
+        cv = check_cv(self.cv, X, y, classifier=True)
         folds = list(cv)
 
         self._enc = LabelEncoder()
@@ -1363,9 +1371,9 @@ def fit(self, X, y):
             iter_labels = [None]
 
         if self.class_weight and not(isinstance(self.class_weight, dict) or
-                                     self.class_weight == 'auto'):
+                                     self.class_weight in ['balanced', 'auto']):
             raise ValueError("class_weight provided should be a "
-                             "dict or 'auto'")
+                             "dict or 'balanced'")
 
         path_func = delayed(_log_reg_scoring_path)
 
@@ -1446,10 +1454,8 @@ def fit(self, X, y):
                 # Take the best scores across every fold and the average of all
                 # coefficients corresponding to the best scores.
                 best_indices = np.argmax(scores, axis=1)
-                w = np.mean([
-                    coefs_paths[i][best_indices[i]]
-                    for i in range(len(folds))
-                    ], axis=0)
+                w = np.mean([coefs_paths[i][best_indices[i]]
+                             for i in range(len(folds))], axis=0)
                 self.C_.append(np.mean(self.Cs_[best_indices]))
 
             if self.multi_class == 'multinomial':
diff --git a/sklearn/linear_model/omp.py b/sklearn/linear_model/omp.py
index 2da1d434e36a..19435cdacc73 100644
--- a/sklearn/linear_model/omp.py
+++ b/sklearn/linear_model/omp.py
@@ -15,7 +15,7 @@
 from .base import LinearModel, _pre_fit
 from ..base import RegressorMixin
 from ..utils import as_float_array, check_array, check_X_y
-from ..cross_validation import _check_cv as check_cv
+from ..cross_validation import check_cv
 from ..externals.joblib import Parallel, delayed
 
 import scipy
@@ -281,6 +281,8 @@ def orthogonal_mp(X, y, n_nonzero_coefs=None, tol=None, precompute=False,
     When parametrized by error using the parameter `tol`:
     argmin ||\gamma||_0 subject to ||y - X\gamma||^2 <= tol
 
+    Read more in the :ref:`User Guide <omp>`.
+
     Parameters
     ----------
     X : array, shape (n_samples, n_features)
@@ -415,6 +417,8 @@ def orthogonal_mp_gram(Gram, Xy, n_nonzero_coefs=None, tol=None,
     Solves n_targets Orthogonal Matching Pursuit problems using only
     the Gram matrix X.T * X and the product X.T * y.
 
+    Read more in the :ref:`User Guide <omp>`.
+
     Parameters
     ----------
     Gram : array, shape (n_features, n_features)
@@ -562,6 +566,8 @@ class OrthogonalMatchingPursuit(LinearModel, RegressorMixin):
         very large. Note that if you already have such matrices, you can pass
         them directly to the fit method.
 
+    Read more in the :ref:`User Guide <omp>`.
+
     Attributes
     ----------
     coef_ : array, shape (n_features,) or (n_features, n_targets)
@@ -760,6 +766,8 @@ class OrthogonalMatchingPursuitCV(LinearModel, RegressorMixin):
     verbose : boolean or integer, optional
         Sets the verbosity amount
 
+    Read more in the :ref:`User Guide <omp>`.
+
     Attributes
     ----------
     intercept_ : float or array, shape (n_targets,)
diff --git a/sklearn/linear_model/passive_aggressive.py b/sklearn/linear_model/passive_aggressive.py
index d4ec3098de7e..fb96a1823b9d 100644
--- a/sklearn/linear_model/passive_aggressive.py
+++ b/sklearn/linear_model/passive_aggressive.py
@@ -9,6 +9,8 @@
 class PassiveAggressiveClassifier(BaseSGDClassifier):
     """Passive Aggressive Classifier
 
+    Read more in the :ref:`User Guide <passive_aggressive>`.
+
     Parameters
     ----------
 
@@ -144,6 +146,8 @@ def fit(self, X, y, coef_init=None, intercept_init=None):
 class PassiveAggressiveRegressor(BaseSGDRegressor):
     """Passive Aggressive Regressor
 
+    Read more in the :ref:`User Guide <passive_aggressive>`.
+
     Parameters
     ----------
 
diff --git a/sklearn/linear_model/perceptron.py b/sklearn/linear_model/perceptron.py
index 49946152fb9a..0eb2ac2d3af0 100644
--- a/sklearn/linear_model/perceptron.py
+++ b/sklearn/linear_model/perceptron.py
@@ -8,6 +8,8 @@
 class Perceptron(BaseSGDClassifier, _LearntSelectorMixin):
     """Perceptron
 
+    Read more in the :ref:`User Guide <perceptron>`.
+
     Parameters
     ----------
 
@@ -44,14 +46,15 @@ class Perceptron(BaseSGDClassifier, _LearntSelectorMixin):
     eta0 : double
         Constant by which the updates are multiplied. Defaults to 1.
 
-    class_weight : dict, {class_label: weight} or "auto" or None, optional
+    class_weight : dict, {class_label: weight} or "balanced" or None, optional
         Preset for the class_weight fit parameter.
 
         Weights associated with classes. If not given, all classes
         are supposed to have weight one.
 
-        The "auto" mode uses the values of y to automatically adjust
-        weights inversely proportional to class frequencies.
+        The "balanced" mode uses the values of y to automatically adjust
+        weights inversely proportional to class frequencies in the input data
+        as ``n_samples / (n_classes * np.bincount(y))``
 
     warm_start : bool, optional
         When set to True, reuse the solution of the previous call to fit as
diff --git a/sklearn/linear_model/randomized_l1.py b/sklearn/linear_model/randomized_l1.py
index 04a3a2ee87dd..1647b6fa16c4 100644
--- a/sklearn/linear_model/randomized_l1.py
+++ b/sklearn/linear_model/randomized_l1.py
@@ -190,6 +190,8 @@ class RandomizedLasso(BaseRandomizedLinearModel):
     a Lasso on each resampling. In short, the features selected more
     often are good features. It is also known as stability selection.
 
+    Read more in the :ref:`User Guide <randomized_l1>`.
+
     Parameters
     ----------
     alpha : float, 'aic', or 'bic', optional
@@ -372,6 +374,8 @@ class RandomizedLogisticRegression(BaseRandomizedLinearModel):
     a LogisticRegression on each resampling. In short, the features selected
     more often are good features. It is also known as stability selection.
 
+    Read more in the :ref:`User Guide <randomized_l1>`.
+
     Parameters
     ----------
     C : float, optional, default=1
@@ -537,6 +541,8 @@ def lasso_stability_path(X, y, scaling=0.5, random_state=None,
                          verbose=False):
     """Stabiliy path based on randomized Lasso estimates
 
+    Read more in the :ref:`User Guide <randomized_l1>`.
+
     Parameters
     ----------
     X : array-like, shape = [n_samples, n_features]
diff --git a/sklearn/linear_model/ransac.py b/sklearn/linear_model/ransac.py
index f7a32ba222c5..c6493e63f1ee 100644
--- a/sklearn/linear_model/ransac.py
+++ b/sklearn/linear_model/ransac.py
@@ -60,6 +60,8 @@ class RANSACRegressor(BaseEstimator, MetaEstimatorMixin, RegressorMixin):
     A detailed description of the algorithm can be found in the documentation
     of the ``linear_model`` sub-package.
 
+    Read more in the :ref:`User Guide <RansacRegression>`.
+
     Parameters
     ----------
     base_estimator : object, optional
diff --git a/sklearn/linear_model/ridge.py b/sklearn/linear_model/ridge.py
index 8a7d30bb23f4..77c3e10dbbfd 100644
--- a/sklearn/linear_model/ridge.py
+++ b/sklearn/linear_model/ridge.py
@@ -201,6 +201,8 @@ def ridge_regression(X, y, alpha, sample_weight=None, solver='auto',
                      max_iter=None, tol=1e-3, verbose=0):
     """Solve the ridge equation by the method of normal equations.
 
+    Read more in the :ref:`User Guide <ridge_regression>`.
+
     Parameters
     ----------
     X : {array-like, sparse matrix, LinearOperator},
@@ -396,6 +398,8 @@ class Ridge(_BaseRidge, RegressorMixin):
     This estimator has built-in support for multi-variate regression
     (i.e., when y is a 2d-array of shape [n_samples, n_targets]).
 
+    Read more in the :ref:`User Guide <ridge_regression>`.
+
     Parameters
     ----------
     alpha : {float, array-like}
@@ -499,6 +503,8 @@ def fit(self, X, y, sample_weight=None):
 class RidgeClassifier(LinearClassifierMixin, _BaseRidge):
     """Classifier using Ridge regression.
 
+    Read more in the :ref:`User Guide <ridge_regression>`.
+
     Parameters
     ----------
     alpha : float
@@ -507,10 +513,13 @@ class RidgeClassifier(LinearClassifierMixin, _BaseRidge):
         ``(2*C)^-1`` in other linear models such as LogisticRegression or
         LinearSVC.
 
-    class_weight : dict, optional
-        Weights associated with classes in the form
-        ``{class_label : weight}``. If not given, all classes are
-        supposed to have weight one.
+    class_weight : dict or 'balanced', optional
+        Weights associated with classes in the form ``{class_label: weight}``.
+        If not given, all classes are supposed to have weight one.
+
+        The "balanced" mode uses the values of y to automatically adjust
+        weights inversely proportional to class frequencies in the input data
+        as ``n_samples / (n_classes * np.bincount(y))``
 
     copy_X : boolean, optional, default True
         If True, X will be copied; else, it may be overwritten.
@@ -884,6 +893,8 @@ class RidgeCV(_BaseRidgeCV, RegressorMixin):
     By default, it performs Generalized Cross-Validation, which is a form of
     efficient Leave-One-Out cross-validation.
 
+    Read more in the :ref:`User Guide <ridge_regression>`.
+
     Parameters
     ----------
     alphas : numpy array of shape [n_alphas]
@@ -968,6 +979,8 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):
     efficient Leave-One-Out cross-validation. Currently, only the n_features >
     n_samples case is handled efficiently.
 
+    Read more in the :ref:`User Guide <ridge_regression>`.
+
     Parameters
     ----------
     alphas : numpy array of shape [n_alphas]
@@ -994,10 +1007,13 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):
         If None, Generalized Cross-Validation (efficient Leave-One-Out)
         will be used.
 
-    class_weight : dict, optional
-        Weights associated with classes in the form
-        ``{class_label : weight}``. If not given, all classes are
-        supposed to have weight one.
+    class_weight : dict or 'balanced', optional
+        Weights associated with classes in the form ``{class_label: weight}``.
+        If not given, all classes are supposed to have weight one.
+
+        The "balanced" mode uses the values of y to automatically adjust
+        weights inversely proportional to class frequencies in the input data
+        as ``n_samples / (n_classes * np.bincount(y))``
 
     Attributes
     ----------
diff --git a/sklearn/linear_model/stochastic_gradient.py b/sklearn/linear_model/stochastic_gradient.py
index f4a38431513e..a0429fa37470 100644
--- a/sklearn/linear_model/stochastic_gradient.py
+++ b/sklearn/linear_model/stochastic_gradient.py
@@ -512,15 +512,15 @@ def partial_fit(self, X, y, classes=None, sample_weight=None):
         -------
         self : returns an instance of self.
         """
-        if self.class_weight == 'auto':
-            raise ValueError("class_weight 'auto' is not supported for "
-                             "partial_fit. In order to use 'auto' weights, "
-                             "use compute_class_weight('auto', classes, y). "
+        if self.class_weight in ['balanced', 'auto']:
+            raise ValueError("class_weight '{0}' is not supported for "
+                             "partial_fit. In order to use 'balanced' weights, "
+                             "use compute_class_weight('{0}', classes, y). "
                              "In place of y you can us a large enough sample "
                              "of the full training set target to properly "
                              "estimate the class frequency distributions. "
                              "Pass the resulting weights as the class_weight "
-                             "parameter.")
+                             "parameter.".format(self.class_weight))
         return self._partial_fit(X, y, alpha=self.alpha, C=1.0, loss=self.loss,
                                  learning_rate=self.learning_rate, n_iter=1,
                                  classes=classes, sample_weight=sample_weight,
@@ -582,6 +582,8 @@ class SGDClassifier(BaseSGDClassifier, _LearntSelectorMixin):
     update is truncated to 0.0 to allow for learning sparse models and achieve
     online feature selection.
 
+    Read more in the :ref:`User Guide <sgd>`.
+
     Parameters
     ----------
     loss : str, 'hinge', 'log', 'modified_huber', 'squared_hinge',\
@@ -659,14 +661,15 @@ class SGDClassifier(BaseSGDClassifier, _LearntSelectorMixin):
     power_t : double
         The exponent for inverse scaling learning rate [default 0.5].
 
-    class_weight : dict, {class_label: weight} or "auto" or None, optional
+    class_weight : dict, {class_label: weight} or "balanced" or None, optional
         Preset for the class_weight fit parameter.
 
         Weights associated with classes. If not given, all classes
         are supposed to have weight one.
 
-        The "auto" mode uses the values of y to automatically adjust
-        weights inversely proportional to class frequencies.
+        The "balanced" mode uses the values of y to automatically adjust
+        weights inversely proportional to class frequencies in the input data
+        as ``n_samples / (n_classes * np.bincount(y))``
 
     warm_start : bool, optional
         When set to True, reuse the solution of the previous call to fit as
@@ -984,6 +987,20 @@ def decision_function(self, X):
         ----------
         X : {array-like, sparse matrix}, shape (n_samples, n_features)
 
+        Returns
+        -------
+        array, shape (n_samples,)
+           Predicted target values per element in X.
+        """
+        return self._decision_function(X)
+
+    def _decision_function(self, X):
+        """Predict using the linear model
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
+
         Returns
         -------
         array, shape (n_samples,)
@@ -1009,7 +1026,7 @@ def predict(self, X):
         array, shape (n_samples,)
            Predicted target values per element in X.
         """
-        return self.decision_function(X)
+        return self._decision_function(X)
 
     def _fit_regressor(self, X, y, alpha, C, loss, learning_rate,
                        sample_weight, n_iter):
@@ -1100,6 +1117,8 @@ class SGDRegressor(BaseSGDRegressor, _LearntSelectorMixin):
     This implementation works with data represented as dense numpy arrays of
     floating point values for the features.
 
+    Read more in the :ref:`User Guide <sgd>`.
+
     Parameters
     ----------
     loss : str, 'squared_loss', 'huber', 'epsilon_insensitive', \
diff --git a/sklearn/linear_model/tests/test_coordinate_descent.py b/sklearn/linear_model/tests/test_coordinate_descent.py
index 21dcd1262ab6..da8d40478a95 100644
--- a/sklearn/linear_model/tests/test_coordinate_descent.py
+++ b/sklearn/linear_model/tests/test_coordinate_descent.py
@@ -604,8 +604,3 @@ def test_sparse_dense_descent_paths():
         _, coefs, _ = path(X, y, fit_intercept=False)
         _, sparse_coefs, _ = path(csr, y, fit_intercept=False)
         assert_array_almost_equal(coefs, sparse_coefs)
-
-
-if __name__ == '__main__':
-    import nose
-    nose.runmodule()
diff --git a/sklearn/linear_model/tests/test_least_angle.py b/sklearn/linear_model/tests/test_least_angle.py
index 18c44af18d5a..da8fd306f97e 100644
--- a/sklearn/linear_model/tests/test_least_angle.py
+++ b/sklearn/linear_model/tests/test_least_angle.py
@@ -306,7 +306,6 @@ def test_lasso_lars_vs_lasso_cd_ill_conditioned():
     y = y.squeeze()
     lars_alphas, _, lars_coef = linear_model.lars_path(X, y, method='lasso')
 
-
     _, lasso_coef2, _ = linear_model.lasso_path(X, y,
                                                 alphas=lars_alphas,
                                                 tol=1e-6,
@@ -457,8 +456,3 @@ def test_lars_path_readonly_data():
             shutil.rmtree(temp_folder)
         except shutil.WindowsError:
             warnings.warn("Could not delete temporary folder %s" % temp_folder)
-
-
-if __name__ == '__main__':
-    import nose
-    nose.runmodule()
diff --git a/sklearn/linear_model/tests/test_logistic.py b/sklearn/linear_model/tests/test_logistic.py
index cf96eca1c760..aa3444eb3ed6 100644
--- a/sklearn/linear_model/tests/test_logistic.py
+++ b/sklearn/linear_model/tests/test_logistic.py
@@ -528,14 +528,14 @@ def test_logistic_regressioncv_class_weights():
     clf_lib.fit(X, y_)
     assert_array_equal(clf_lib.classes_, [0, 1])
 
-    # Test for class_weight=auto
+    # Test for class_weight=balanced
     X, y = make_classification(n_samples=20, n_features=20, n_informative=10,
                                random_state=0)
     clf_lbf = LogisticRegressionCV(solver='lbfgs', fit_intercept=False,
-                                   class_weight='auto')
+                                   class_weight='balanced')
     clf_lbf.fit(X, y)
     clf_lib = LogisticRegressionCV(solver='liblinear', fit_intercept=False,
-                                   class_weight='auto')
+                                   class_weight='balanced')
     clf_lib.fit(X, y)
     assert_array_almost_equal(clf_lib.coef_, clf_lbf.coef_, decimal=4)
 
diff --git a/sklearn/linear_model/tests/test_ransac.py b/sklearn/linear_model/tests/test_ransac.py
index 8f75363b715d..460521b7a29e 100644
--- a/sklearn/linear_model/tests/test_ransac.py
+++ b/sklearn/linear_model/tests/test_ransac.py
@@ -340,7 +340,3 @@ def test_ransac_dynamic_max_trials():
     ransac_estimator = RANSACRegressor(base_estimator, min_samples=2,
                                        stop_probability=1.1)
     assert_raises(ValueError, ransac_estimator.fit, X, y)
-
-
-if __name__ == "__main__":
-    np.testing.run_module_suite()
diff --git a/sklearn/linear_model/tests/test_ridge.py b/sklearn/linear_model/tests/test_ridge.py
index 0b1ac2487745..daa8f8bbcc72 100644
--- a/sklearn/linear_model/tests/test_ridge.py
+++ b/sklearn/linear_model/tests/test_ridge.py
@@ -469,18 +469,18 @@ def test_class_weights():
     # the prediction on this point should shift
     assert_array_equal(clf.predict([[0.2, -1.0]]), np.array([-1]))
 
-    # check if class_weight = 'auto' can handle negative labels.
-    clf = RidgeClassifier(class_weight='auto')
+    # check if class_weight = 'balanced' can handle negative labels.
+    clf = RidgeClassifier(class_weight='balanced')
     clf.fit(X, y)
     assert_array_equal(clf.predict([[0.2, -1.0]]), np.array([1]))
 
-    # class_weight = 'auto', and class_weight = None should return
+    # class_weight = 'balanced', and class_weight = None should return
     # same values when y has equal number of all labels
     X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0], [1.0, 1.0]])
     y = [1, 1, -1, -1]
     clf = RidgeClassifier(class_weight=None)
     clf.fit(X, y)
-    clfa = RidgeClassifier(class_weight='auto')
+    clfa = RidgeClassifier(class_weight='balanced')
     clfa.fit(X, y)
     assert_equal(len(clfa.classes_), 2)
     assert_array_almost_equal(clf.coef_, clfa.coef_)
@@ -558,12 +558,12 @@ def fit_ridge_not_ok_2():
             ridge.fit(X, y, sample_weights_not_OK_2)
 
         assert_raise_message(ValueError,
-                              "Sample weights must be 1D array or scalar",
-                              fit_ridge_not_ok)
+                             "Sample weights must be 1D array or scalar",
+                             fit_ridge_not_ok)
 
         assert_raise_message(ValueError,
-                              "Sample weights must be 1D array or scalar",
-                              fit_ridge_not_ok_2)
+                             "Sample weights must be 1D array or scalar",
+                             fit_ridge_not_ok_2)
 
 
 def test_sparse_design_with_sample_weights():
diff --git a/sklearn/linear_model/tests/test_sgd.py b/sklearn/linear_model/tests/test_sgd.py
index 0e7d9b4c9952..cdf254708e2c 100644
--- a/sklearn/linear_model/tests/test_sgd.py
+++ b/sklearn/linear_model/tests/test_sgd.py
@@ -257,6 +257,7 @@ def test_late_onset_averaging_reached(self):
 class DenseSGDClassifierTestCase(unittest.TestCase, CommonTest):
     """Test suite for the dense representation variant of SGD"""
     factory_class = SGDClassifier
+
     def test_sgd(self):
         # Check that SGD gives any results :-)
 
@@ -371,18 +372,18 @@ def test_sgd_at_least_two_labels(self):
         # Target must have at least two labels
         self.factory(alpha=0.01, n_iter=20).fit(X2, np.ones(9))
 
-    def test_partial_fit_weight_class_auto(self):
-        # partial_fit with class_weight='auto' not supported
+    def test_partial_fit_weight_class_balanced(self):
+        # partial_fit with class_weight='balanced' not supported"""
         assert_raises_regexp(ValueError,
-                             "class_weight 'auto' is not supported for "
-                             "partial_fit. In order to use 'auto' weights, "
-                             "use compute_class_weight\('auto', classes, y\). "
+                             "class_weight 'balanced' is not supported for "
+                             "partial_fit. In order to use 'balanced' weights, "
+                             "use compute_class_weight\('balanced', classes, y\). "
                              "In place of y you can us a large enough sample "
                              "of the full training set target to properly "
                              "estimate the class frequency distributions. "
                              "Pass the resulting weights as the class_weight "
                              "parameter.",
-                             self.factory(class_weight='auto').partial_fit,
+                             self.factory(class_weight='balanced').partial_fit,
                              X, Y, classes=np.unique(Y))
 
     def test_sgd_multiclass(self):
@@ -614,8 +615,8 @@ def test_weights_multiplied(self):
 
         assert_almost_equal(clf1.coef_, clf2.coef_)
 
-    def test_auto_weight(self):
-        # Test class weights for imbalanced data
+    def test_balanced_weight(self):
+        # Test class weights for imbalanced data"""
         # compute reference metrics on iris dataset that is quite balanced by
         # default
         X, y = iris.data, iris.target
@@ -630,15 +631,16 @@ def test_auto_weight(self):
         assert_almost_equal(metrics.f1_score(y, clf.predict(X), average='weighted'), 0.96,
                             decimal=1)
 
-        # make the same prediction using automated class_weight
-        clf_auto = self.factory(alpha=0.0001, n_iter=1000,
-                                class_weight="auto", shuffle=False).fit(X, y)
-        assert_almost_equal(metrics.f1_score(y, clf_auto.predict(X), average='weighted'), 0.96,
+        # make the same prediction using balanced class_weight
+        clf_balanced = self.factory(alpha=0.0001, n_iter=1000,
+                                    class_weight="balanced",
+                                    shuffle=False).fit(X, y)
+        assert_almost_equal(metrics.f1_score(y, clf_balanced.predict(X), average='weighted'), 0.96,
                             decimal=1)
 
         # Make sure that in the balanced case it does not change anything
-        # to use "auto"
-        assert_array_almost_equal(clf.coef_, clf_auto.coef_, 6)
+        # to use "balanced"
+        assert_array_almost_equal(clf.coef_, clf_balanced.coef_, 6)
 
         # build an very very imbalanced dataset out of iris data
         X_0 = X[y == 0, :]
@@ -653,14 +655,14 @@ def test_auto_weight(self):
         y_pred = clf.predict(X)
         assert_less(metrics.f1_score(y, y_pred, average='weighted'), 0.96)
 
-        # fit a model with auto class_weight enabled
-        clf = self.factory(n_iter=1000, class_weight="auto", shuffle=False)
+        # fit a model with balanced class_weight enabled
+        clf = self.factory(n_iter=1000, class_weight="balanced", shuffle=False)
         clf.fit(X_imbalanced, y_imbalanced)
         y_pred = clf.predict(X)
         assert_greater(metrics.f1_score(y, y_pred, average='weighted'), 0.96)
 
         # fit another using a fit parameter override
-        clf = self.factory(n_iter=1000, class_weight="auto", shuffle=False)
+        clf = self.factory(n_iter=1000, class_weight="balanced", shuffle=False)
         clf.fit(X_imbalanced, y_imbalanced)
         y_pred = clf.predict(X)
         assert_greater(metrics.f1_score(y, y_pred, average='weighted'), 0.96)
diff --git a/sklearn/linear_model/theil_sen.py b/sklearn/linear_model/theil_sen.py
index 0705162e12fd..52d5aae7bb81 100644
--- a/sklearn/linear_model/theil_sen.py
+++ b/sklearn/linear_model/theil_sen.py
@@ -207,6 +207,8 @@ class TheilSenRegressor(LinearModel, RegressorMixin):
     reached, the subsets are chosen randomly. In a final step, the spatial
     median (or L1 median) is calculated of all least square solutions.
 
+    Read more in the :ref:`User Guide <theil_sen_regression>`.
+
     Parameters
     ----------
     fit_intercept : boolean, optional, default True
diff --git a/sklearn/manifold/isomap.py b/sklearn/manifold/isomap.py
index b3cbdc61e92e..136b8177ff22 100644
--- a/sklearn/manifold/isomap.py
+++ b/sklearn/manifold/isomap.py
@@ -17,6 +17,8 @@ class Isomap(BaseEstimator, TransformerMixin):
 
     Non-linear dimensionality reduction through Isometric Mapping
 
+    Read more in the :ref:`User Guide <isomap>`.
+
     Parameters
     ----------
     n_neighbors : integer
diff --git a/sklearn/manifold/locally_linear.py b/sklearn/manifold/locally_linear.py
index 179f4759a8df..fe9c778ce667 100644
--- a/sklearn/manifold/locally_linear.py
+++ b/sklearn/manifold/locally_linear.py
@@ -180,6 +180,8 @@ def locally_linear_embedding(
         random_state=None):
     """Perform a Locally Linear Embedding analysis on the data.
 
+    Read more in the :ref:`User Guide <locally_linear_embedding>`.
+
     Parameters
     ----------
     X : {array-like, sparse matrix, BallTree, KDTree, NearestNeighbors}
@@ -497,6 +499,8 @@ def locally_linear_embedding(
 class LocallyLinearEmbedding(BaseEstimator, TransformerMixin):
     """Locally Linear Embedding
 
+    Read more in the :ref:`User Guide <locally_linear_embedding>`.
+
     Parameters
     ----------
     n_neighbors : integer
diff --git a/sklearn/manifold/mds.py b/sklearn/manifold/mds.py
index 77c764616be1..0ab8bd68ea9a 100644
--- a/sklearn/manifold/mds.py
+++ b/sklearn/manifold/mds.py
@@ -274,6 +274,8 @@ def smacof(similarities, metric=True, n_components=2, init=None, n_init=8,
 class MDS(BaseEstimator):
     """Multidimensional scaling
 
+    Read more in the :ref:`User Guide <multidimensional_scaling>`.
+
     Parameters
     ----------
     metric : boolean, optional, default: True
diff --git a/sklearn/manifold/spectral_embedding_.py b/sklearn/manifold/spectral_embedding_.py
index c7aafbda0591..9edcaa97a168 100644
--- a/sklearn/manifold/spectral_embedding_.py
+++ b/sklearn/manifold/spectral_embedding_.py
@@ -137,6 +137,8 @@ def spectral_embedding(adjacency, n_components=8, eigen_solver=None,
     However care must taken to always make the affinity matrix symmetric
     so that the eigenvector decomposition works as expected.
 
+    Read more in the :ref:`User Guide <spectral_embedding>`.
+
     Parameters
     ----------
     adjacency : array-like or sparse matrix, shape: (n_samples, n_samples)
@@ -316,6 +318,8 @@ class SpectralEmbedding(BaseEstimator):
     The resulting transformation is given by the value of the
     eigenvectors for each data point.
 
+    Read more in the :ref:`User Guide <spectral_embedding>`.
+
     Parameters
     -----------
     n_components : integer, default: 2
diff --git a/sklearn/manifold/t_sne.py b/sklearn/manifold/t_sne.py
index 9165a286a564..80b6003f2520 100644
--- a/sklearn/manifold/t_sne.py
+++ b/sklearn/manifold/t_sne.py
@@ -304,6 +304,8 @@ class TSNE(BaseEstimator):
     noise and speed up the computation of pairwise distances between
     samples. For more tips see Laurens van der Maaten's FAQ [2].
 
+    Read more in the :ref:`User Guide <t_sne>`.
+
     Parameters
     ----------
     n_components : int, optional (default: 2)
diff --git a/sklearn/manifold/tests/test_isomap.py b/sklearn/manifold/tests/test_isomap.py
index 08b2eada00d2..37c08e6f2182 100644
--- a/sklearn/manifold/tests/test_isomap.py
+++ b/sklearn/manifold/tests/test_isomap.py
@@ -111,8 +111,3 @@ def test_pipeline():
          ('clf', neighbors.KNeighborsClassifier())])
     clf.fit(X, y)
     assert_less(.9, clf.score(X, y))
-
-
-if __name__ == '__main__':
-    import nose
-    nose.runmodule()
diff --git a/sklearn/manifold/tests/test_locally_linear.py b/sklearn/manifold/tests/test_locally_linear.py
index deea74b87858..04220a465dfe 100644
--- a/sklearn/manifold/tests/test_locally_linear.py
+++ b/sklearn/manifold/tests/test_locally_linear.py
@@ -121,8 +121,3 @@ def test_singular_matrix():
     f = ignore_warnings
     assert_raises(ValueError, f(manifold.locally_linear_embedding),
                   M, 2, 1, method='standard', eigen_solver='arpack')
-
-
-if __name__ == '__main__':
-    import nose
-    nose.runmodule()
diff --git a/sklearn/metrics/classification.py b/sklearn/metrics/classification.py
index 0ffc7eec34d4..08acb8524ad6 100644
--- a/sklearn/metrics/classification.py
+++ b/sklearn/metrics/classification.py
@@ -125,6 +125,8 @@ def accuracy_score(y_true, y_pred, normalize=True, sample_weight=None):
     the set of labels predicted for a sample must *exactly* match the
     corresponding set of labels in y_true.
 
+    Read more in the :ref:`User Guide <accuracy_score>`.
+
     Parameters
     ----------
     y_true : 1d array-like, or label indicator array / sparse matrix
@@ -193,6 +195,8 @@ def confusion_matrix(y_true, y_pred, labels=None):
     is equal to the number of observations known to be in group :math:`i` but
     predicted to be in group :math:`j`.
 
+    Read more in the :ref:`User Guide <confusion_matrix>`.
+
     Parameters
     ----------
     y_true : array, shape = [n_samples]
@@ -271,6 +275,8 @@ def jaccard_similarity_score(y_true, y_pred, normalize=True,
     sets, is used to compare set of predicted labels for a sample to the
     corresponding set of labels in ``y_true``.
 
+    Read more in the :ref:`User Guide <jaccard_similarity_score>`.
+
     Parameters
     ----------
     y_true : 1d array-like, or label indicator array / sparse matrix
@@ -365,6 +371,8 @@ def matthews_corrcoef(y_true, y_pred):
     Only in the binary case does this relate to information about true and
     false positives and negatives. See references below.
 
+    Read more in the :ref:`User Guide <matthews_corrcoef>`.
+
     Parameters
     ----------
     y_true : array, shape = [n_samples]
@@ -423,6 +431,8 @@ def zero_one_loss(y_true, y_pred, normalize=True, sample_weight=None):
     (float), else it returns the number of misclassifications (int). The best
     performance is 0.
 
+    Read more in the :ref:`User Guide <zero_one_loss>`.
+
     Parameters
     ----------
     y_true : 1d array-like, or label indicator array / sparse matrix
@@ -497,6 +507,8 @@ def f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary',
     In the multi-class and multi-label case, this is the weighted average of
     the F1 score of each class.
 
+    Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.
+
     Parameters
     ----------
     y_true : 1d array-like, or label indicator array / sparse matrix
@@ -505,8 +517,14 @@ def f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary',
     y_pred : 1d array-like, or label indicator array / sparse matrix
         Estimated targets as returned by a classifier.
 
-    labels : array
-        Integer array of labels.
+    labels : list, optional
+        The set of labels to include when ``average != 'binary'``, and their
+        order if ``average is None``. Labels present in the data can be
+        excluded, for example to calculate a multiclass average ignoring a
+        majority negative class, while labels not present in the data will
+        result in 0 components in a macro average. For multilabel targets,
+        labels are column indices. By default, all labels in ``y_true`` and
+        ``y_pred`` are used in sorted order.
 
     pos_label : str or int, 1 by default
         The class to report if ``average='binary'``. Until version 0.18 it is
@@ -589,6 +607,8 @@ def fbeta_score(y_true, y_pred, beta, labels=None, pos_label=1,
     favors recall (``beta -> 0`` considers only precision, ``beta -> inf``
     only recall).
 
+    Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.
+
     Parameters
     ----------
     y_true : 1d array-like, or label indicator array / sparse matrix
@@ -600,8 +620,14 @@ def fbeta_score(y_true, y_pred, beta, labels=None, pos_label=1,
     beta: float
         Weight of precision in harmonic mean.
 
-    labels : array
-        Integer array of labels.
+    labels : list, optional
+        The set of labels to include when ``average != 'binary'``, and their
+        order if ``average is None``. Labels present in the data can be
+        excluded, for example to calculate a multiclass average ignoring a
+        majority negative class, while labels not present in the data will
+        result in 0 components in a macro average. For multilabel targets,
+        labels are column indices. By default, all labels in ``y_true`` and
+        ``y_pred`` are used in sorted order.
 
     pos_label : str or int, 1 by default
         The class to report if ``average='binary'``. Until version 0.18 it is
@@ -757,6 +783,8 @@ def precision_recall_fscore_support(y_true, y_pred, beta=1.0, labels=None,
     returns the average precision, recall and F-measure if ``average``
     is one of ``'micro'``, ``'macro'``, ``'weighted'`` or ``'samples'``.
 
+    Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.
+
     Parameters
     ----------
     y_true : 1d array-like, or label indicator array / sparse matrix
@@ -768,8 +796,14 @@ def precision_recall_fscore_support(y_true, y_pred, beta=1.0, labels=None,
     beta : float, 1.0 by default
         The strength of recall versus precision in the F-score.
 
-    labels : array
-        Integer array of labels.
+    labels : list, optional
+        The set of labels to include when ``average != 'binary'``, and their
+        order if ``average is None``. Labels present in the data can be
+        excluded, for example to calculate a multiclass average ignoring a
+        majority negative class, while labels not present in the data will
+        result in 0 components in a macro average. For multilabel targets,
+        labels are column indices. By default, all labels in ``y_true`` and
+        ``y_pred`` are used in sorted order.
 
     pos_label : str or int, 1 by default
         The class to report if ``average='binary'``. Until version 0.18 it is
@@ -863,6 +897,7 @@ def precision_recall_fscore_support(y_true, y_pred, beta=1.0, labels=None,
         raise ValueError("beta should be >0 in the F-beta score")
 
     y_type, y_true, y_pred = _check_targets(y_true, y_pred)
+    present_labels = unique_labels(y_true, y_pred)
 
     if average == 'binary' and (y_type != 'binary' or pos_label is None):
         warnings.warn('The default `weighted` averaging is deprecated, '
@@ -875,17 +910,49 @@ def precision_recall_fscore_support(y_true, y_pred, beta=1.0, labels=None,
                       % str(average_options), DeprecationWarning, stacklevel=2)
         average = 'weighted'
 
-    label_order = labels  # save this for later
+    if y_type == 'binary' and pos_label is not None and average is not None:
+        if average != 'binary':
+            warnings.warn('From version 0.18, binary input will not be '
+                          'handled specially when using averaged '
+                          'precision/recall/F-score. '
+                          'Please use average=\'binary\' to report only the '
+                          'positive class performance.', DeprecationWarning)
+        if labels is None or len(labels) <= 2:
+            if pos_label not in present_labels:
+                if len(present_labels) < 2:
+                    # Only negative labels
+                    return (0., 0., 0., 0)
+                else:
+                    raise ValueError("pos_label=%r is not a valid label: %r" %
+                                     (pos_label, present_labels))
+            labels = [pos_label]
     if labels is None:
-        labels = unique_labels(y_true, y_pred)
+        labels = present_labels
+        n_labels = None
     else:
-        labels = np.sort(labels)
+        n_labels = len(labels)
+        labels = np.hstack([labels, np.setdiff1d(present_labels, labels,
+                                                 assume_unique=True)])
 
     ### Calculate tp_sum, pred_sum, true_sum ###
 
     if y_type.startswith('multilabel'):
         sum_axis = 1 if average == 'samples' else 0
 
+        # All labels are index integers for multilabel.
+        # Select labels:
+        if not np.all(labels == present_labels):
+            if np.max(labels) > np.max(present_labels):
+                raise ValueError('All labels must be in [0, n labels). '
+                                 'Got %d > %d' %
+                                 (np.max(labels), np.max(present_labels)))
+            if np.min(labels) < 0:
+                raise ValueError('All labels must be in [0, n labels). '
+                                 'Got %d < 0' % np.min(labels))
+
+            y_true = y_true[:, labels[:n_labels]]
+            y_pred = y_pred[:, labels[:n_labels]]
+
         # calculate weighted counts
         true_and_pred = y_true.multiply(y_pred)
         tp_sum = count_nonzero(true_and_pred, axis=sum_axis,
@@ -900,11 +967,11 @@ def precision_recall_fscore_support(y_true, y_pred, beta=1.0, labels=None,
                          "not meaningful outside multilabel "
                          "classification. See the accuracy_score instead.")
     else:
-        lb = LabelEncoder()
-        lb.fit(labels)
-        y_true = lb.transform(y_true)
-        y_pred = lb.transform(y_pred)
-        labels = lb.classes_
+        le = LabelEncoder()
+        le.fit(labels)
+        y_true = le.transform(y_true)
+        y_pred = le.transform(y_pred)
+        sorted_labels = le.classes_
 
         # labels are now from 0 to len(labels) - 1 -> use bincount
         tp = y_true == y_pred
@@ -927,28 +994,13 @@ def precision_recall_fscore_support(y_true, y_pred, beta=1.0, labels=None,
             true_sum = bincount(y_true, weights=sample_weight,
                                 minlength=len(labels))
 
-    ### Select labels to keep ###
+        # Retain only selected labels
+        indices = np.searchsorted(sorted_labels, labels[:n_labels])
+        tp_sum = tp_sum[indices]
+        true_sum = true_sum[indices]
+        pred_sum = pred_sum[indices]
 
-    if y_type == 'binary' and average is not None and pos_label is not None:
-        if average != 'binary':
-            warnings.warn('From version 0.18, binary input will not be '
-                          'handled specially when using averaged '
-                          'precision/recall/F-score. '
-                          'Please use average=\'binary\' to report only the '
-                          'positive class performance.', DeprecationWarning)
-        if pos_label not in labels:
-            if len(labels) == 1:
-                # Only negative labels
-                return (0., 0., 0., 0)
-            else:
-                raise ValueError("pos_label=%r is not a valid label: %r" %
-                                 (pos_label, labels))
-        pos_label_idx = labels == pos_label
-        tp_sum = tp_sum[pos_label_idx]
-        pred_sum = pred_sum[pos_label_idx]
-        true_sum = true_sum[pos_label_idx]
-
-    elif average == 'micro':
+    if average == 'micro':
         tp_sum = np.array([tp_sum.sum()])
         pred_sum = np.array([pred_sum.sum()])
         true_sum = np.array([true_sum.sum()])
@@ -988,12 +1040,6 @@ def precision_recall_fscore_support(y_true, y_pred, beta=1.0, labels=None,
         recall = np.average(recall, weights=weights)
         f_score = np.average(f_score, weights=weights)
         true_sum = None  # return no support
-    elif label_order is not None:
-        indices = np.searchsorted(labels, label_order)
-        precision = precision[indices]
-        recall = recall[indices]
-        f_score = f_score[indices]
-        true_sum = true_sum[indices]
 
     return precision, recall, f_score, true_sum
 
@@ -1009,6 +1055,8 @@ def precision_score(y_true, y_pred, labels=None, pos_label=1,
 
     The best value is 1 and the worst value is 0.
 
+    Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.
+
     Parameters
     ----------
     y_true : 1d array-like, or label indicator array / sparse matrix
@@ -1017,8 +1065,14 @@ def precision_score(y_true, y_pred, labels=None, pos_label=1,
     y_pred : 1d array-like, or label indicator array / sparse matrix
         Estimated targets as returned by a classifier.
 
-    labels : array
-        Integer array of labels.
+    labels : list, optional
+        The set of labels to include when ``average != 'binary'``, and their
+        order if ``average is None``. Labels present in the data can be
+        excluded, for example to calculate a multiclass average ignoring a
+        majority negative class, while labels not present in the data will
+        result in 0 components in a macro average. For multilabel targets,
+        labels are column indices. By default, all labels in ``y_true`` and
+        ``y_pred`` are used in sorted order.
 
     pos_label : str or int, 1 by default
         The class to report if ``average='binary'``. Until version 0.18 it is
@@ -1100,6 +1154,8 @@ def recall_score(y_true, y_pred, labels=None, pos_label=1, average='binary',
 
     The best value is 1 and the worst value is 0.
 
+    Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.
+
     Parameters
     ----------
     y_true : 1d array-like, or label indicator array / sparse matrix
@@ -1108,8 +1164,14 @@ def recall_score(y_true, y_pred, labels=None, pos_label=1, average='binary',
     y_pred : 1d array-like, or label indicator array / sparse matrix
         Estimated targets as returned by a classifier.
 
-    labels : array
-        Integer array of labels.
+    labels : list, optional
+        The set of labels to include when ``average != 'binary'``, and their
+        order if ``average is None``. Labels present in the data can be
+        excluded, for example to calculate a multiclass average ignoring a
+        majority negative class, while labels not present in the data will
+        result in 0 components in a macro average. For multilabel targets,
+        labels are column indices. By default, all labels in ``y_true`` and
+        ``y_pred`` are used in sorted order.
 
     pos_label : str or int, 1 by default
         The class to report if ``average='binary'``. Until version 0.18 it is
@@ -1184,6 +1246,8 @@ def classification_report(y_true, y_pred, labels=None, target_names=None,
                           sample_weight=None, digits=2):
     """Build a text report showing the main classification metrics
 
+    Read more in the :ref:`User Guide <classification_report>`.
+
     Parameters
     ----------
     y_true : 1d array-like, or label indicator array / sparse matrix
@@ -1281,6 +1345,8 @@ def hamming_loss(y_true, y_pred, classes=None):
 
     The Hamming loss is the fraction of labels that are incorrectly predicted.
 
+    Read more in the :ref:`User Guide <hamming_loss>`.
+
     Parameters
     ----------
     y_true : 1d array-like, or label indicator array / sparse matrix
@@ -1367,6 +1433,8 @@ def log_loss(y_true, y_pred, eps=1e-15, normalize=True, sample_weight=None):
 
         -log P(yt|yp) = -(yt log(yp) + (1 - yt) log(1 - yp))
 
+    Read more in the :ref:`User Guide <log_loss>`.
+
     Parameters
     ----------
     y_true : array-like or label indicator matrix
@@ -1455,6 +1523,8 @@ def hinge_loss(y_true, pred_decision, labels=None, sample_weight=None):
     to Crammer-Singer's method. As in the binary case, the cumulated hinge loss
     is an upper bound of the number of mistakes made by the classifier.
 
+    Read more in the :ref:`User Guide <hinge_loss>`.
+
     Parameters
     ----------
     y_true : array, shape = [n_samples]
@@ -1605,6 +1675,7 @@ def brier_score_loss(y_true, y_prob, sample_weight=None, pos_label=None):
     "distant" from one another). Which label is considered to be the positive
     label is controlled via the parameter pos_label, which defaults to 1.
 
+    Read more in the :ref:`User Guide <calibration>`.
 
     Parameters
     ----------
diff --git a/sklearn/metrics/cluster/bicluster.py b/sklearn/metrics/cluster/bicluster.py
index 0ec2af9e6b2f..6a911277b468 100644
--- a/sklearn/metrics/cluster/bicluster.py
+++ b/sklearn/metrics/cluster/bicluster.py
@@ -54,6 +54,8 @@ def consensus_score(a, b, similarity="jaccard"):
     The final score is the sum of similarities divided by the size of
     the larger set.
 
+    Read more in the :ref:`User Guide <biclustering>`.
+
     Parameters
     ----------
     a : (rows, columns)
diff --git a/sklearn/metrics/cluster/supervised.py b/sklearn/metrics/cluster/supervised.py
index b7cc4123afc9..2d7d27c5a4c1 100644
--- a/sklearn/metrics/cluster/supervised.py
+++ b/sklearn/metrics/cluster/supervised.py
@@ -109,6 +109,8 @@ def adjusted_rand_score(labels_true, labels_pred):
 
         adjusted_rand_score(a, b) == adjusted_rand_score(b, a)
 
+    Read more in the :ref:`User Guide <adjusted_rand_score>`.
+
     Parameters
     ----------
     labels_true : int array, shape = [n_samples]
@@ -119,7 +121,7 @@ def adjusted_rand_score(labels_true, labels_pred):
 
     Returns
     -------
-    ari: float
+    ari : float
        Similarity score between -1.0 and 1.0. Random labelings have an ARI
        close to 0.0. 1.0 stands for perfect match.
 
@@ -214,6 +216,8 @@ def homogeneity_completeness_v_measure(labels_true, labels_pred):
     ``label_pred`` will give the same score. This does not hold for
     homogeneity and completeness.
 
+    Read more in the :ref:`User Guide <homogeneity_completeness>`.
+
     Parameters
     ----------
     labels_true : int array, shape = [n_samples]
@@ -275,6 +279,8 @@ def homogeneity_score(labels_true, labels_pred):
     will return the :func:`completeness_score` which will be different in
     general.
 
+    Read more in the :ref:`User Guide <homogeneity_completeness>`.
+
     Parameters
     ----------
     labels_true : int array, shape = [n_samples]
@@ -347,6 +353,8 @@ def completeness_score(labels_true, labels_pred):
     will return the :func:`homogeneity_score` which will be different in
     general.
 
+    Read more in the :ref:`User Guide <homogeneity_completeness>`.
+
     Parameters
     ----------
     labels_true : int array, shape = [n_samples]
@@ -419,6 +427,8 @@ def v_measure_score(labels_true, labels_pred):
     measure the agreement of two independent label assignments strategies
     on the same dataset when the real ground truth is not known.
 
+    Read more in the :ref:`User Guide <homogeneity_completeness>`.
+
     Parameters
     ----------
     labels_true : int array, shape = [n_samples]
@@ -519,6 +529,8 @@ def mutual_info_score(labels_true, labels_pred, contingency=None):
     measure the agreement of two independent label assignments strategies
     on the same dataset when the real ground truth is not known.
 
+    Read more in the :ref:`User Guide <mutual_info_score>`.
+
     Parameters
     ----------
     labels_true : int array, shape = [n_samples]
@@ -586,6 +598,8 @@ def adjusted_mutual_info_score(labels_true, labels_pred):
     Be mindful that this function is an order of magnitude slower than other
     metrics, such as the Adjusted Rand Index.
 
+    Read more in the :ref:`User Guide <mutual_info_score>`.
+
     Parameters
     ----------
     labels_true : int array, shape = [n_samples]
@@ -677,6 +691,8 @@ def normalized_mutual_info_score(labels_true, labels_pred):
     measure the agreement of two independent label assignments strategies
     on the same dataset when the real ground truth is not known.
 
+    Read more in the :ref:`User Guide <mutual_info_score>`.
+
     Parameters
     ----------
     labels_true : int array, shape = [n_samples]
diff --git a/sklearn/metrics/cluster/unsupervised.py b/sklearn/metrics/cluster/unsupervised.py
index d4249c9ea13f..c956dc3d2196 100644
--- a/sklearn/metrics/cluster/unsupervised.py
+++ b/sklearn/metrics/cluster/unsupervised.py
@@ -29,6 +29,8 @@ def silhouette_score(X, labels, metric='euclidean', sample_size=None,
     overlapping clusters. Negative values generally indicate that a sample has
     been assigned to the wrong cluster, as a different cluster is more similar.
 
+    Read more in the :ref:`User Guide <silhouette_coefficient>`.
+
     Parameters
     ----------
     X : array [n_samples_a, n_samples_a] if metric == "precomputed", or, \
@@ -113,6 +115,8 @@ def silhouette_samples(X, labels, metric='euclidean', **kwds):
     The best value is 1 and the worst value is -1. Values near 0 indicate
     overlapping clusters.
 
+    Read more in the :ref:`User Guide <silhouette_coefficient>`.
+
     Parameters
     ----------
     X : array [n_samples_a, n_samples_a] if metric == "precomputed", or, \
diff --git a/sklearn/metrics/pairwise.py b/sklearn/metrics/pairwise.py
index e726fe152226..aee203daaf81 100644
--- a/sklearn/metrics/pairwise.py
+++ b/sklearn/metrics/pairwise.py
@@ -152,6 +152,8 @@ def euclidean_distances(X, Y=None, Y_norm_squared=None, squared=False):
     the distance matrix returned by this function may not be exactly
     symmetric as required by, e.g., ``scipy.spatial.distance`` functions.
 
+    Read more in the :ref:`User Guide <metrics>`.
+
     Parameters
     ----------
     X : {array-like, sparse matrix}, shape (n_samples_1, n_features)
@@ -361,7 +363,7 @@ def pairwise_distances_argmin(X, Y, axis=1, metric="euclidean",
     This function works with dense 2D arrays only.
 
     Parameters
-    ==========
+    ----------
     X : array-like
         Arrays containing points. Respective shapes (n_samples1, n_features)
         and (n_samples2, n_features)
@@ -410,12 +412,12 @@ def pairwise_distances_argmin(X, Y, axis=1, metric="euclidean",
         Axis along which the argmin and distances are to be computed.
 
     Returns
-    =======
+    -------
     argmin : numpy.ndarray
         Y[argmin[i], :] is the row in Y that is closest to X[i, :].
 
     See also
-    ========
+    --------
     sklearn.metrics.pairwise_distances
     sklearn.metrics.pairwise_distances_argmin_min
     """
@@ -433,6 +435,8 @@ def manhattan_distances(X, Y=None, sum_over_features=True,
     With sum_over_features equal to False it returns the componentwise
     distances.
 
+    Read more in the :ref:`User Guide <metrics>`.
+
     Parameters
     ----------
     X : array_like
@@ -507,6 +511,8 @@ def cosine_distances(X, Y=None):
 
     Cosine distance is defined as 1.0 minus the cosine similarity.
 
+    Read more in the :ref:`User Guide <metrics>`.
+
     Parameters
     ----------
     X : array_like, sparse matrix
@@ -537,6 +543,8 @@ def paired_euclidean_distances(X, Y):
     """
     Computes the paired euclidean distances between X and Y
 
+    Read more in the :ref:`User Guide <metrics>`.
+
     Parameters
     ----------
     X : array-like, shape (n_samples, n_features)
@@ -554,6 +562,8 @@ def paired_euclidean_distances(X, Y):
 def paired_manhattan_distances(X, Y):
     """Compute the L1 distances between the vectors in X and Y.
 
+    Read more in the :ref:`User Guide <metrics>`.
+
     Parameters
     ----------
     X : array-like, shape (n_samples, n_features)
@@ -577,6 +587,8 @@ def paired_cosine_distances(X, Y):
     """
     Computes the paired cosine distances between X and Y
 
+    Read more in the :ref:`User Guide <metrics>`.
+
     Parameters
     ----------
     X : array-like, shape (n_samples, n_features)
@@ -611,6 +623,8 @@ def paired_distances(X, Y, metric="euclidean", **kwds):
 
     Computes the distances between (X[0], Y[0]), (X[1], Y[1]), etc...
 
+    Read more in the :ref:`User Guide <metrics>`.
+
     Parameters
     ----------
     X : ndarray (n_samples, n_features)
@@ -665,6 +679,8 @@ def linear_kernel(X, Y=None):
     """
     Compute the linear kernel between X and Y.
 
+    Read more in the :ref:`User Guide <linear_kernel>`.
+
     Parameters
     ----------
     X : array of shape (n_samples_1, n_features)
@@ -685,6 +701,8 @@ def polynomial_kernel(X, Y=None, degree=3, gamma=None, coef0=1):
 
         K(X, Y) = (gamma <X, Y> + coef0)^degree
 
+    Read more in the :ref:`User Guide <polynomial_kernel>`.
+
     Parameters
     ----------
     X : ndarray of shape (n_samples_1, n_features)
@@ -716,6 +734,8 @@ def sigmoid_kernel(X, Y=None, gamma=None, coef0=1):
 
         K(X, Y) = tanh(gamma <X, Y> + coef0)
 
+    Read more in the :ref:`User Guide <sigmoid_kernel>`.
+
     Parameters
     ----------
     X : ndarray of shape (n_samples_1, n_features)
@@ -747,6 +767,8 @@ def rbf_kernel(X, Y=None, gamma=None):
 
     for each pair of rows x in X and y in Y.
 
+    Read more in the :ref:`User Guide <rbf_kernel>`.
+
     Parameters
     ----------
     X : array of shape (n_samples_X, n_features)
@@ -779,6 +801,8 @@ def cosine_similarity(X, Y=None):
 
     On L2-normalized data, this function is equivalent to linear_kernel.
 
+    Read more in the :ref:`User Guide <cosine_similarity>`.
+
     Parameters
     ----------
     X : array_like, sparse matrix
@@ -820,6 +844,8 @@ def additive_chi2_kernel(X, Y=None):
 
     It can be interpreted as a weighted difference per entry.
 
+    Read more in the :ref:`User Guide <chi2_kernel>`.
+
     Notes
     -----
     As the negative of a distance, this kernel is only conditionally positive
@@ -879,6 +905,8 @@ def chi2_kernel(X, Y=None, gamma=1.):
 
     It can be interpreted as a weighted difference per entry.
 
+    Read more in the :ref:`User Guide <chi2_kernel>`.
+
     Parameters
     ----------
     X : array-like of shape (n_samples_X, n_features)
@@ -944,6 +972,8 @@ def distance_metrics():
     'manhattan'      metrics.pairwise.manhattan_distances
     ============     ====================================
 
+    Read more in the :ref:`User Guide <metrics>`.
+
     """
     return PAIRWISE_DISTANCE_FUNCTIONS
 
@@ -1043,6 +1073,8 @@ def pairwise_distances(X, Y=None, metric="euclidean", n_jobs=1, **kwds):
     scikit-learn, see the __doc__ of the sklearn.pairwise.distance_metrics
     function.
 
+    Read more in the :ref:`User Guide <metrics>`.
+
     Parameters
     ----------
     X : array [n_samples_a, n_samples_a] if metric == "precomputed", or, \
@@ -1146,6 +1178,8 @@ def kernel_metrics():
       'sigmoid'         sklearn.pairwise.sigmoid_kernel
       'cosine'          sklearn.pairwise.cosine_similarity
       ===============   ========================================
+
+    Read more in the :ref:`User Guide <metrics>`.
     """
     return PAIRWISE_KERNEL_FUNCTIONS
 
@@ -1181,6 +1215,8 @@ def pairwise_kernels(X, Y=None, metric="linear", filter_params=False,
     Valid values for metric are::
         ['rbf', 'sigmoid', 'polynomial', 'poly', 'linear', 'cosine']
 
+    Read more in the :ref:`User Guide <metrics>`.
+
     Parameters
     ----------
     X : array [n_samples_a, n_samples_a] if metric == "precomputed", or, \
diff --git a/sklearn/metrics/ranking.py b/sklearn/metrics/ranking.py
index e0c7003d1e1a..f87854d69594 100644
--- a/sklearn/metrics/ranking.py
+++ b/sklearn/metrics/ranking.py
@@ -112,6 +112,8 @@ def average_precision_score(y_true, y_score, average="macro",
     Note: this implementation is restricted to the binary classification task
     or multilabel classification task.
 
+    Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.
+
     Parameters
     ----------
     y_true : array, shape = [n_samples] or [n_samples, n_classes]
@@ -181,6 +183,8 @@ def roc_auc_score(y_true, y_score, average="macro", sample_weight=None):
     Note: this implementation is restricted to the binary classification task
     or multilabel classification task in label indicator format.
 
+    Read more in the :ref:`User Guide <roc_metrics>`.
+
     Parameters
     ----------
     y_true : array, shape = [n_samples] or [n_samples, n_classes]
@@ -349,6 +353,8 @@ def precision_recall_curve(y_true, probas_pred, pos_label=None,
     have a corresponding threshold.  This ensures that the graph starts on the
     x axis.
 
+    Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.
+
     Parameters
     ----------
     y_true : array, shape = [n_samples]
@@ -412,6 +418,8 @@ def roc_curve(y_true, y_score, pos_label=None, sample_weight=None):
 
     Note: this implementation is restricted to the binary classification task.
 
+    Read more in the :ref:`User Guide <roc_metrics>`.
+
     Parameters
     ----------
 
@@ -516,6 +524,8 @@ def label_ranking_average_precision_score(y_true, y_score):
     The obtained score is always strictly greater than 0 and
     the best value is 1.
 
+    Read more in the :ref:`User Guide <label_ranking_average_precision>`.
+
     Parameters
     ----------
     y_true : array or sparse matrix, shape = [n_samples, n_labels]
@@ -586,6 +596,8 @@ def coverage_error(y_true, y_score, sample_weight=None):
     Ties in ``y_scores`` are broken by giving maximal rank that would have
     been assigned to all tied values.
 
+    Read more in the :ref:`User Guide <coverage_error>`.
+
     Parameters
     ----------
     y_true : array, shape = [n_samples, n_labels]
@@ -639,6 +651,8 @@ def label_ranking_loss(y_true, y_score, sample_weight=None):
     relevant and irrelevant labels. The best performance is achieved with
     a ranking loss of zero.
 
+    Read more in the :ref:`User Guide <label_ranking_loss>`.
+
     Parameters
     ----------
     y_true : array or sparse matrix, shape = [n_samples, n_labels]
diff --git a/sklearn/metrics/regression.py b/sklearn/metrics/regression.py
index 68220a125f47..db0189741641 100644
--- a/sklearn/metrics/regression.py
+++ b/sklearn/metrics/regression.py
@@ -106,6 +106,8 @@ def mean_absolute_error(y_true, y_pred,
                         multioutput='uniform_average'):
     """Mean absolute error regression loss
 
+    Read more in the :ref:`User Guide <mean_absolute_error>`.
+
     Parameters
     ----------
     y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)
@@ -174,6 +176,8 @@ def mean_squared_error(y_true, y_pred,
                        multioutput='uniform_average'):
     """Mean squared error regression loss
 
+    Read more in the :ref:`User Guide <mean_squared_error>`.
+
     Parameters
     ----------
     y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)
@@ -237,6 +241,8 @@ def mean_squared_error(y_true, y_pred,
 def median_absolute_error(y_true, y_pred):
     """Median absolute error regression loss
 
+    Read more in the :ref:`User Guide <median_absolute_error>`.
+
     Parameters
     ----------
     y_true : array-like of shape = (n_samples)
@@ -273,6 +279,8 @@ def explained_variance_score(y_true, y_pred,
 
     Best possible score is 1.0, lower values are worse.
 
+    Read more in the :ref:`User Guide <explained_variance_score>`.
+
     Parameters
     ----------
     y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)
@@ -362,6 +370,8 @@ def r2_score(y_true, y_pred,
 
     Best possible score is 1.0, lower values are worse.
 
+    Read more in the :ref:`User Guide <r2_score>`.
+
     Parameters
     ----------
     y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)
diff --git a/sklearn/metrics/scorer.py b/sklearn/metrics/scorer.py
index 5dd91786f0eb..6628fb40d4e7 100644
--- a/sklearn/metrics/scorer.py
+++ b/sklearn/metrics/scorer.py
@@ -255,6 +255,8 @@ def make_scorer(score_func, greater_is_better=True, needs_proba=False,
     ``mean_squared_error``, ``adjusted_rand_index`` or ``average_precision``
     and returns a callable that scores an estimator's output.
 
+    Read more in the :ref:`User Guide <scoring>`.
+
     Parameters
     ----------
     score_func : callable,
diff --git a/sklearn/metrics/tests/test_classification.py b/sklearn/metrics/tests/test_classification.py
index 1f0365b6a680..3ac8e40ae860 100644
--- a/sklearn/metrics/tests/test_classification.py
+++ b/sklearn/metrics/tests/test_classification.py
@@ -11,6 +11,7 @@
 
 from sklearn.datasets import make_multilabel_classification
 from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer
+from sklearn.preprocessing import label_binarize
 from sklearn.utils.fixes import np_version
 from sklearn.utils.validation import check_random_state
 
@@ -173,6 +174,73 @@ def test_precision_recall_f_binary_single_class():
     assert_equal(0., f1_score([-1, -1], [-1, -1]))
 
 
+@ignore_warnings
+def test_precision_recall_f_extra_labels():
+    """Test handling of explicit additional (not in input) labels to PRF
+    """
+    y_true = [1, 3, 3, 2]
+    y_pred = [1, 1, 3, 2]
+    y_true_bin = label_binarize(y_true, classes=np.arange(5))
+    y_pred_bin = label_binarize(y_pred, classes=np.arange(5))
+    data = [(y_true, y_pred),
+            (y_true_bin, y_pred_bin)]
+
+    for i, (y_true, y_pred) in enumerate(data):
+        # No average: zeros in array
+        actual = recall_score(y_true, y_pred, labels=[0, 1, 2, 3, 4],
+                              average=None)
+        assert_array_almost_equal([0., 1., 1., .5, 0.], actual)
+
+        # Macro average is changed
+        actual = recall_score(y_true, y_pred, labels=[0, 1, 2, 3, 4],
+                              average='macro')
+        assert_array_almost_equal(np.mean([0., 1., 1., .5, 0.]), actual)
+
+        # No effect otheriwse
+        for average in ['micro', 'weighted', 'samples']:
+            if average == 'samples' and i == 0:
+                continue
+            assert_almost_equal(recall_score(y_true, y_pred,
+                                             labels=[0, 1, 2, 3, 4],
+                                             average=average),
+                                recall_score(y_true, y_pred, labels=None,
+                                             average=average))
+
+    # Error when introducing invalid label in multilabel case
+    # (although it would only affect performance if average='macro'/None)
+    for average in [None, 'macro', 'micro', 'samples']:
+        assert_raises(ValueError, recall_score, y_true_bin, y_pred_bin,
+                      labels=np.arange(6), average=average)
+        assert_raises(ValueError, recall_score, y_true_bin, y_pred_bin,
+                      labels=np.arange(-1, 4), average=average)
+
+
+@ignore_warnings
+def test_precision_recall_f_ignored_labels():
+    """Test a subset of labels may be requested for PRF"""
+    y_true = [1, 1, 2, 3]
+    y_pred = [1, 3, 3, 3]
+    y_true_bin = label_binarize(y_true, classes=np.arange(5))
+    y_pred_bin = label_binarize(y_pred, classes=np.arange(5))
+    data = [(y_true, y_pred),
+            (y_true_bin, y_pred_bin)]
+
+    for i, (y_true, y_pred) in enumerate(data):
+        recall_13 = partial(recall_score, y_true, y_pred, labels=[1, 3])
+        recall_all = partial(recall_score, y_true, y_pred, labels=None)
+
+        assert_array_almost_equal([.5, 1.], recall_13(average=None))
+        assert_almost_equal((.5 + 1.) / 2, recall_13(average='macro'))
+        assert_almost_equal((.5 * 2 + 1. * 1) / 3,
+                            recall_13(average='weighted'))
+        assert_almost_equal(2. / 3, recall_13(average='micro'))
+
+        # ensure the above were meaningful tests:
+        for average in ['macro', 'weighted', 'micro']:
+            assert_not_equal(recall_13(average=average),
+                             recall_all(average=average))
+
+
 def test_average_precision_score_score_non_binary_class():
     # Test that average_precision_score function returns an error when trying
     # to compute average_precision_score for multiclass task.
@@ -315,7 +383,7 @@ def test_precision_refcall_f1_score_multilabel_unordered_labels():
     y_pred = np.array([[0, 0, 1, 1]])
     for average in ['samples', 'micro', 'macro', 'weighted', None]:
         p, r, f, s = precision_recall_fscore_support(
-            y_true, y_pred, labels=[4, 1, 2, 3], warn_for=[], average=average)
+            y_true, y_pred, labels=[3, 0, 1, 2], warn_for=[], average=average)
         assert_array_equal(p, 0)
         assert_array_equal(r, 0)
         assert_array_equal(f, 0)
diff --git a/sklearn/metrics/tests/test_common.py b/sklearn/metrics/tests/test_common.py
index 9e5640f6562e..578f92866a81 100644
--- a/sklearn/metrics/tests/test_common.py
+++ b/sklearn/metrics/tests/test_common.py
@@ -1085,9 +1085,9 @@ def test_no_averaging_labels():
     # in multi-class and multi-label cases
     y_true_multilabel = np.array([[1, 1, 0, 0], [1, 1, 0, 0]])
     y_pred_multilabel = np.array([[0, 0, 1, 1], [0, 1, 1, 0]])
-    y_true_multiclass = np.array([1, 2, 3])
-    y_pred_multiclass = np.array([1, 3, 4])
-    labels = np.array([4, 1, 2, 3])
+    y_true_multiclass = np.array([0, 1, 2])
+    y_pred_multiclass = np.array([0, 2, 3])
+    labels = np.array([3, 0, 1, 2])
     _, inverse_labels = np.unique(labels, return_inverse=True)
 
     for name in METRICS_WITH_AVERAGING:
diff --git a/sklearn/mixture/dpgmm.py b/sklearn/mixture/dpgmm.py
index 766a0b9bf152..74465cff3288 100644
--- a/sklearn/mixture/dpgmm.py
+++ b/sklearn/mixture/dpgmm.py
@@ -125,6 +125,8 @@ class DPGMM(GMM):
     Initialization is with normally-distributed means and identity
     covariance, for proper convergence.
 
+    Read more in the :ref:`User Guide <dpgmm>`.
+
     Parameters
     ----------
     n_components: int, default 1
@@ -621,6 +623,8 @@ class VBGMM(DPGMM):
     Initialization is with normally-distributed means and identity
     covariance, for proper convergence.
 
+    Read more in the :ref:`User Guide <vbgmm>`.
+
     Parameters
     ----------
     n_components: int, default 1
diff --git a/sklearn/mixture/gmm.py b/sklearn/mixture/gmm.py
index 9300ced09a2a..bb2a3fed630e 100644
--- a/sklearn/mixture/gmm.py
+++ b/sklearn/mixture/gmm.py
@@ -33,9 +33,11 @@ def log_multivariate_normal_density(X, means, covars, covariance_type='diag'):
     X : array_like, shape (n_samples, n_features)
         List of n_features-dimensional data points.  Each row corresponds to a
         single data point.
+
     means : array_like, shape (n_components, n_features)
         List of n_features-dimensional mean vectors for n_components Gaussians.
         Each row corresponds to a single mean vector.
+
     covars : array_like
         List of n_components covariance parameters for each Gaussian. The shape
         depends on `covariance_type`:
@@ -43,6 +45,7 @@ def log_multivariate_normal_density(X, means, covars, covariance_type='diag'):
             (n_features, n_features)    if 'tied',
             (n_components, n_features)    if 'diag',
             (n_components, n_features, n_features) if 'full'
+
     covariance_type : string
         Type of the covariance parameters.  Must be one of
         'spherical', 'tied', 'diag', 'full'.  Defaults to 'diag'.
@@ -119,6 +122,7 @@ class GMM(BaseEstimator):
     Initializes parameters such that every mixture component has zero
     mean and identity covariance.
 
+    Read more in the :ref:`User Guide <gmm>`.
 
     Parameters
     ----------
@@ -182,8 +186,6 @@ class GMM(BaseEstimator):
     converged_ : bool
         True when convergence was reached in fit(), False otherwise.
 
-
-
     See Also
     --------
 
@@ -268,13 +270,15 @@ def __init__(self, n_components=1, covariance_type='diag',
 
     def _get_covars(self):
         """Covariance parameters for each mixture component.
-        The shape depends on `cvtype`::
 
-            (`n_states`, 'n_features')                if 'spherical',
-            (`n_features`, `n_features`)              if 'tied',
-            (`n_states`, `n_features`)                if 'diag',
-            (`n_states`, `n_features`, `n_features`)  if 'full'
-            """
+        The shape depends on ``cvtype``::
+
+            (n_states, n_features)                if 'spherical',
+            (n_features, n_features)              if 'tied',
+            (n_states, n_features)                if 'diag',
+            (n_states, n_features, n_features)    if 'full'
+
+        """
         if self.covariance_type == 'full':
             return self.covars_
         elif self.covariance_type == 'diag':
@@ -323,8 +327,8 @@ def score_samples(self, X):
             raise ValueError('The shape of X  is not compatible with self')
 
         lpr = (log_multivariate_normal_density(X, self.means_, self.covars_,
-                                               self.covariance_type)
-               + np.log(self.weights_))
+                                               self.covariance_type) +
+               np.log(self.weights_))
         logprob = logsumexp(lpr, axis=1)
         responsibilities = np.exp(lpr - logprob[:, np.newaxis])
         return logprob, responsibilities
@@ -420,8 +424,8 @@ def sample(self, n_samples=1, random_state=None):
         return X
 
     def fit_predict(self, X, y=None):
-        """
-        Fit and then predict labels for data.
+        """Fit and then predict labels for data.
+
         Warning: due to the final maximization step in the EM algorithm,
         with low iterations the prediction may not be 100% accurate
 
@@ -471,7 +475,7 @@ def _fit(self, X, y=None, do_prediction=False):
 
         for init in range(self.n_init):
             if self.verbose > 0:
-                print('Initialization '+str(init+1))
+                print('Initialization ' + str(init + 1))
                 start_init_time = time()
 
             if 'm' in self.init_params or not hasattr(self, 'means_'):
@@ -508,7 +512,7 @@ def _fit(self, X, y=None, do_prediction=False):
 
             for i in range(self.n_iter):
                 if self.verbose > 0:
-                    print('\tEM iteration '+str(i+1))
+                    print('\tEM iteration ' + str(i + 1))
                     start_iter_time = time()
                 prev_log_likelihood = current_log_likelihood
                 # Expectation step
@@ -521,7 +525,7 @@ def _fit(self, X, y=None, do_prediction=False):
                 if prev_log_likelihood is not None:
                     change = abs(current_log_likelihood - prev_log_likelihood)
                     if self.verbose > 1:
-                        print('\t\tChange: '+str(change))
+                        print('\t\tChange: ' + str(change))
                     if change < tol:
                         self.converged_ = True
                         if self.verbose > 0:
@@ -532,8 +536,8 @@ def _fit(self, X, y=None, do_prediction=False):
                 self._do_mstep(X, responsibilities, self.params,
                                self.min_covar)
                 if self.verbose > 1:
-                    print('\t\tEM iteration '+str(i+1)+' took {0:.5f}s'.format(
-                        time()-start_iter_time))
+                    print('\t\tEM iteration ' + str(i + 1) + ' took {0:.5f}s'.format(
+                        time() - start_iter_time))
 
             # if the results are better, keep it
             if self.n_iter:
@@ -546,8 +550,8 @@ def _fit(self, X, y=None, do_prediction=False):
                         print('\tBetter parameters were found.')
 
             if self.verbose > 1:
-                print('\tInitialization '+str(init+1)+' took {0:.5f}s'.format(
-                    time()-start_init_time))
+                print('\tInitialization ' + str(init + 1) + ' took {0:.5f}s'.format(
+                    time() - start_init_time))
 
         # check the existence of an init param that was not subject to
         # likelihood computation issue.
@@ -653,7 +657,7 @@ def aic(self, X):
 
 
 #########################################################################
-## some helper routines
+# some helper routines
 #########################################################################
 
 
@@ -684,8 +688,7 @@ def _log_multivariate_normal_density_tied(X, means, covars):
 
 
 def _log_multivariate_normal_density_full(X, means, covars, min_covar=1.e-7):
-    """Log probability for full covariance matrices.
-    """
+    """Log probability for full covariance matrices."""
     n_samples, n_dim = X.shape
     nmix = len(means)
     log_prob = np.empty((n_samples, nmix))
@@ -751,8 +754,7 @@ def _validate_covars(covars, covariance_type, n_components):
 
 def distribute_covar_matrix_to_match_covariance_type(
         tied_cv, covariance_type, n_components):
-    """Create all the covariance matrices from a given template
-    """
+    """Create all the covariance matrices from a given template"""
     if covariance_type == 'spherical':
         cv = np.tile(tied_cv.mean() * np.ones(tied_cv.shape[1]),
                      (n_components, 1))
diff --git a/sklearn/mixture/tests/test_dpgmm.py b/sklearn/mixture/tests/test_dpgmm.py
index 0cfc24a5bb23..acdb8f7b56d8 100644
--- a/sklearn/mixture/tests/test_dpgmm.py
+++ b/sklearn/mixture/tests/test_dpgmm.py
@@ -1,8 +1,6 @@
 import unittest
 import sys
 
-import nose
-
 import numpy as np
 
 from sklearn.mixture import DPGMM, VBGMM
@@ -157,7 +155,3 @@ class TestVBGMMWithTiedCovars(unittest.TestCase, VBGMMTester):
 class TestVBGMMWithFullCovars(unittest.TestCase, VBGMMTester):
     covariance_type = 'full'
     setUp = GMMTester._setUp
-
-
-if __name__ == '__main__':
-    nose.runmodule()
diff --git a/sklearn/mixture/tests/test_gmm.py b/sklearn/mixture/tests/test_gmm.py
index cf09597e3f1a..dd3a4380ffca 100644
--- a/sklearn/mixture/tests/test_gmm.py
+++ b/sklearn/mixture/tests/test_gmm.py
@@ -471,8 +471,3 @@ def test_verbose_second_level():
         g.fit(X)
     finally:
         sys.stdout = old_stdout
-
-
-if __name__ == '__main__':
-    import nose
-    nose.runmodule()
diff --git a/sklearn/multiclass.py b/sklearn/multiclass.py
index 336a38f7e7de..969660f0adf9 100644
--- a/sklearn/multiclass.py
+++ b/sklearn/multiclass.py
@@ -221,6 +221,8 @@ class OneVsRestClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):
     In the multilabel learning literature, OvR is also known as the binary
     relevance method.
 
+    Read more in the :ref:`User Guide <ovr_classification>`.
+
     Parameters
     ----------
     estimator : estimator object
@@ -462,6 +464,8 @@ class OneVsOneClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):
     a small subset of the data whereas, with one-vs-the-rest, the complete
     dataset is used `n_classes` times.
 
+    Read more in the :ref:`User Guide <ovo_classification>`.
+
     Parameters
     ----------
     estimator : estimator object
@@ -552,36 +556,58 @@ def decision_function(self, X):
         """
         check_is_fitted(self, 'estimators_')
 
-        n_samples = X.shape[0]
-        n_classes = self.classes_.shape[0]
-        votes = np.zeros((n_samples, n_classes))
-        sum_of_confidences = np.zeros((n_samples, n_classes))
-
-        k = 0
-        for i in range(n_classes):
-            for j in range(i + 1, n_classes):
-                pred = self.estimators_[k].predict(X)
-                confidence_levels_ij = _predict_binary(self.estimators_[k], X)
-                sum_of_confidences[:, i] -= confidence_levels_ij
-                sum_of_confidences[:, j] += confidence_levels_ij
-                votes[pred == 0, i] += 1
-                votes[pred == 1, j] += 1
-                k += 1
-
-        max_confidences = sum_of_confidences.max()
-        min_confidences = sum_of_confidences.min()
-
-        if max_confidences == min_confidences:
-            return votes
-
-        # Scale the sum_of_confidences to (-0.5, 0.5) and add it with votes.
-        # The motivation is to use confidence levels as a way to break ties in
-        # the votes without switching any decision made based on a difference
-        # of 1 vote.
-        eps = np.finfo(sum_of_confidences.dtype).eps
-        max_abs_confidence = max(abs(max_confidences), abs(min_confidences))
-        scale = (0.5 - eps) / max_abs_confidence
-        return votes + sum_of_confidences * scale
+        predictions = np.vstack([est.predict(X) for est in self.estimators_]).T
+        confidences = np.vstack([_predict_binary(est, X) for est in self.estimators_]).T
+        return _ovr_decision_function(predictions, confidences,
+                                      len(self.classes_))
+
+
+def _ovr_decision_function(predictions, confidences, n_classes):
+    """Compute a continuous, tie-breaking ovr decision function.
+
+    It is important to include a continuous value, not only votes,
+    to make computing AUC or calibration meaningful.
+
+    Parameters
+    ----------
+    predictions : array-like, shape (n_samples, n_classifiers)
+        Predicted classes for each binary classifier.
+
+    confidences : array-like, shape (n_samples, n_classifiers)
+        Decision functions or predicted probabilities for positive class
+        for each binary classifier.
+
+    n_classes : int
+        Number of classes. n_classifiers must be
+        ``n_classes * (n_classes - 1 ) / 2``
+    """
+    n_samples = predictions.shape[0]
+    votes = np.zeros((n_samples, n_classes))
+    sum_of_confidences = np.zeros((n_samples, n_classes))
+
+    k = 0
+    for i in range(n_classes):
+        for j in range(i + 1, n_classes):
+            sum_of_confidences[:, i] -= confidences[:, k]
+            sum_of_confidences[:, j] += confidences[:, k]
+            votes[predictions[:, k] == 0, i] += 1
+            votes[predictions[:, k] == 1, j] += 1
+            k += 1
+
+    max_confidences = sum_of_confidences.max()
+    min_confidences = sum_of_confidences.min()
+
+    if max_confidences == min_confidences:
+        return votes
+
+    # Scale the sum_of_confidences to (-0.5, 0.5) and add it with votes.
+    # The motivation is to use confidence levels as a way to break ties in
+    # the votes without switching any decision made based on a difference
+    # of 1 vote.
+    eps = np.finfo(sum_of_confidences.dtype).eps
+    max_abs_confidence = max(abs(max_confidences), abs(min_confidences))
+    scale = (0.5 - eps) / max_abs_confidence
+    return votes + sum_of_confidences * scale
 
 
 @deprecated("fit_ecoc is deprecated and will be removed in 0.18."
@@ -643,6 +669,8 @@ class OutputCodeClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):
     for compressing the model (0 < code_size < 1) or for making the model more
     robust to errors (code_size > 1). See the documentation for more details.
 
+    Read more in the :ref:`User Guide <ecoc>`.
+
     Parameters
     ----------
     estimator : estimator object
diff --git a/sklearn/naive_bayes.py b/sklearn/naive_bayes.py
index 7161a2a99f44..f80524028258 100644
--- a/sklearn/naive_bayes.py
+++ b/sklearn/naive_bayes.py
@@ -113,6 +113,8 @@ class GaussianNB(BaseNB):
 
         http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf
 
+    Read more in the :ref:`User Guide <gaussian_naive_bayes>`.
+
     Attributes
     ----------
     class_prior_ : array, shape (n_classes,)
@@ -564,6 +566,8 @@ class MultinomialNB(BaseDiscreteNB):
     multinomial distribution normally requires integer feature counts. However,
     in practice, fractional counts such as tf-idf may also work.
 
+    Read more in the :ref:`User Guide <multinomial_naive_bayes>`.
+
     Parameters
     ----------
     alpha : float, optional (default=1.0)
@@ -665,6 +669,8 @@ class BernoulliNB(BaseDiscreteNB):
     difference is that while MultinomialNB works with occurrence counts,
     BernoulliNB is designed for binary/boolean features.
 
+    Read more in the :ref:`User Guide <bernoulli_naive_bayes>`.
+
     Parameters
     ----------
     alpha : float, optional (default=1.0)
diff --git a/sklearn/neighbors/approximate.py b/sklearn/neighbors/approximate.py
index 9d586a1dcca6..81d9474acce2 100644
--- a/sklearn/neighbors/approximate.py
+++ b/sklearn/neighbors/approximate.py
@@ -122,6 +122,8 @@ class LSHForest(BaseEstimator, KNeighborsMixin, RadiusNeighborsMixin):
     points. Its value does not depend on the norm of the vector points but
     only on their relative angles.
 
+    Read more in the :ref:`User Guide <approximate_nearest_neighbors>`.
+
     Parameters
     ----------
 
diff --git a/sklearn/neighbors/classification.py b/sklearn/neighbors/classification.py
index 43f67f06d5ee..4dbbe66f4ee5 100644
--- a/sklearn/neighbors/classification.py
+++ b/sklearn/neighbors/classification.py
@@ -24,6 +24,8 @@ class KNeighborsClassifier(NeighborsBase, KNeighborsMixin,
                            SupervisedIntegerMixin, ClassifierMixin):
     """Classifier implementing the k-nearest neighbors vote.
 
+    Read more in the :ref:`User Guide <classification>`.
+
     Parameters
     ----------
     n_neighbors : int, optional (default = 5)
@@ -220,6 +222,8 @@ class RadiusNeighborsClassifier(NeighborsBase, RadiusNeighborsMixin,
                                 SupervisedIntegerMixin, ClassifierMixin):
     """Classifier implementing a vote among neighbors within a given radius
 
+    Read more in the :ref:`User Guide <classification>`.
+
     Parameters
     ----------
     radius : float, optional (default = 1.0)
diff --git a/sklearn/neighbors/graph.py b/sklearn/neighbors/graph.py
index 3798871a43c5..3ce46b27c699 100644
--- a/sklearn/neighbors/graph.py
+++ b/sklearn/neighbors/graph.py
@@ -9,6 +9,7 @@
 from .base import KNeighborsMixin, RadiusNeighborsMixin
 from .unsupervised import NearestNeighbors
 
+
 def _check_params(X, metric, p, metric_params):
     """Check the validity of the input parameters"""
     params = zip(['metric', 'p', 'metric_params'],
@@ -50,6 +51,8 @@ def kneighbors_graph(X, n_neighbors, mode='connectivity', metric='minkowski',
                      p=2, metric_params=None, include_self=None):
     """Computes the (weighted) graph of k-Neighbors for points in X
 
+    Read more in the :ref:`User Guide <unsupervised_neighbors>`.
+
     Parameters
     ----------
     X : array-like or BallTree, shape = [n_samples, n_features]
@@ -105,9 +108,8 @@ def kneighbors_graph(X, n_neighbors, mode='connectivity', metric='minkowski',
     radius_neighbors_graph
     """
     if not isinstance(X, KNeighborsMixin):
-        X = NearestNeighbors(
-            n_neighbors, metric=metric, p=p, metric_params=metric_params
-            ).fit(X)
+        X = NearestNeighbors(n_neighbors, metric=metric, p=p,
+                             metric_params=metric_params).fit(X)
     else:
         _check_params(X, metric, p, metric_params)
 
@@ -122,6 +124,8 @@ def radius_neighbors_graph(X, radius, mode='connectivity', metric='minkowski',
     Neighborhoods are restricted the points at a distance lower than
     radius.
 
+    Read more in the :ref:`User Guide <unsupervised_neighbors>`.
+
     Parameters
     ----------
     X : array-like or BallTree, shape = [n_samples, n_features]
@@ -177,10 +181,8 @@ def radius_neighbors_graph(X, radius, mode='connectivity', metric='minkowski',
     kneighbors_graph
     """
     if not isinstance(X, RadiusNeighborsMixin):
-        X = NearestNeighbors(
-            radius=radius, metric=metric, p=p,
-            metric_params=metric_params
-            ).fit(X)
+        X = NearestNeighbors(radius=radius, metric=metric, p=p,
+                             metric_params=metric_params).fit(X)
     else:
         _check_params(X, metric, p, metric_params)
 
diff --git a/sklearn/neighbors/kde.py b/sklearn/neighbors/kde.py
index 8d940264f437..dfb349a8dc42 100644
--- a/sklearn/neighbors/kde.py
+++ b/sklearn/neighbors/kde.py
@@ -24,6 +24,8 @@
 class KernelDensity(BaseEstimator):
     """Kernel Density Estimation
 
+    Read more in the :ref:`User Guide <kernel_density>`.
+
     Parameters
     ----------
     bandwidth : float
diff --git a/sklearn/neighbors/nearest_centroid.py b/sklearn/neighbors/nearest_centroid.py
index b10c05e6fcd8..4aca5af34245 100644
--- a/sklearn/neighbors/nearest_centroid.py
+++ b/sklearn/neighbors/nearest_centroid.py
@@ -13,7 +13,6 @@
 from scipy import sparse as sp
 
 from ..base import BaseEstimator, ClassifierMixin
-from ..externals.six.moves import xrange
 from ..metrics.pairwise import pairwise_distances
 from ..preprocessing import LabelEncoder
 from ..utils.validation import check_array, check_X_y, check_is_fitted
@@ -26,6 +25,8 @@ class NearestCentroid(BaseEstimator, ClassifierMixin):
     Each class is represented by its centroid, with test samples classified to
     the class with the nearest centroid.
 
+    Read more in the :ref:`User Guide <nearest_centroid_classifier>`.
+
     Parameters
     ----------
     metric: string, or callable
diff --git a/sklearn/neighbors/regression.py b/sklearn/neighbors/regression.py
index a6285c77d9fe..9e28bcbcd9c9 100644
--- a/sklearn/neighbors/regression.py
+++ b/sklearn/neighbors/regression.py
@@ -24,6 +24,8 @@ class KNeighborsRegressor(NeighborsBase, KNeighborsMixin,
     The target is predicted by local interpolation of the targets
     associated of the nearest neighbors in the training set.
 
+    Read more in the :ref:`User Guide <regression>`.
+
     Parameters
     ----------
     n_neighbors : int, optional (default = 5)
@@ -164,6 +166,8 @@ class RadiusNeighborsRegressor(NeighborsBase, RadiusNeighborsMixin,
     The target is predicted by local interpolation of the targets
     associated of the nearest neighbors in the training set.
 
+    Read more in the :ref:`User Guide <regression>`.
+
     Parameters
     ----------
     radius : float, optional (default = 1.0)
diff --git a/sklearn/neighbors/tests/test_ball_tree.py b/sklearn/neighbors/tests/test_ball_tree.py
index 55007331ef78..76cc8999c7c4 100644
--- a/sklearn/neighbors/tests/test_ball_tree.py
+++ b/sklearn/neighbors/tests/test_ball_tree.py
@@ -309,8 +309,3 @@ def test_query_haversine():
 
     assert_array_almost_equal(dist1, dist2)
     assert_array_almost_equal(ind1, ind2)
-
-
-if __name__ == '__main__':
-    import nose
-    nose.runmodule()
diff --git a/sklearn/neighbors/tests/test_dist_metrics.py b/sklearn/neighbors/tests/test_dist_metrics.py
index 96c4f8d6fb24..c838735b2170 100644
--- a/sklearn/neighbors/tests/test_dist_metrics.py
+++ b/sklearn/neighbors/tests/test_dist_metrics.py
@@ -145,8 +145,3 @@ def test_pyfunc_metric():
 
     assert_array_almost_equal(D1, D2)
     assert_array_almost_equal(D1_pkl, D2_pkl)
-
-
-if __name__ == '__main__':
-    import nose
-    nose.runmodule()
diff --git a/sklearn/neighbors/tests/test_kd_tree.py b/sklearn/neighbors/tests/test_kd_tree.py
index 8bccc4608799..4c2433ff538d 100644
--- a/sklearn/neighbors/tests/test_kd_tree.py
+++ b/sklearn/neighbors/tests/test_kd_tree.py
@@ -235,8 +235,3 @@ def test_simultaneous_sort(n_rows=10, n_pts=201):
 
     assert_array_almost_equal(dist, dist2)
     assert_array_almost_equal(ind, ind2)
-
-
-if __name__ == '__main__':
-    import nose
-    nose.runmodule()
diff --git a/sklearn/neighbors/tests/test_kde.py b/sklearn/neighbors/tests/test_kde.py
index c1c63288d647..69bac3d7442b 100644
--- a/sklearn/neighbors/tests/test_kde.py
+++ b/sklearn/neighbors/tests/test_kde.py
@@ -139,8 +139,3 @@ def test_kde_pipeline_gridsearch():
     search = GridSearchCV(pipe1, param_grid=params, cv=5)
     search.fit(X)
     assert_equal(search.best_params_['kerneldensity__bandwidth'], .1)
-
-
-if __name__ == '__main__':
-    import nose
-    nose.runmodule()
diff --git a/sklearn/neighbors/tests/test_nearest_centroid.py b/sklearn/neighbors/tests/test_nearest_centroid.py
index 8ecb226edef1..7f1c8ee355f5 100644
--- a/sklearn/neighbors/tests/test_nearest_centroid.py
+++ b/sklearn/neighbors/tests/test_nearest_centroid.py
@@ -134,8 +134,3 @@ def test_manhattan_metric():
     clf.fit(X_csr, y)
     assert_array_equal(clf.centroids_, dense_centroid)
     assert_array_equal(dense_centroid, [[-1, -1], [1, 1]])
-
-
-if __name__ == "__main__":
-    import nose
-    nose.runmodule()
diff --git a/sklearn/neighbors/tests/test_neighbors.py b/sklearn/neighbors/tests/test_neighbors.py
index 494f3fe11f14..cb289f1eda05 100644
--- a/sklearn/neighbors/tests/test_neighbors.py
+++ b/sklearn/neighbors/tests/test_neighbors.py
@@ -1082,8 +1082,3 @@ def test_include_self_neighbors_graph():
         X, 5.0, include_self=False).A
     assert_array_equal(rng, [[1., 1.], [1., 1.]])
     assert_array_equal(rng_not_self, [[0., 1.], [1., 0.]])
-
-
-if __name__ == '__main__':
-    import nose
-    nose.runmodule()
diff --git a/sklearn/neighbors/unsupervised.py b/sklearn/neighbors/unsupervised.py
index a599ce2f3532..099e882ee236 100644
--- a/sklearn/neighbors/unsupervised.py
+++ b/sklearn/neighbors/unsupervised.py
@@ -10,6 +10,8 @@ class NearestNeighbors(NeighborsBase, KNeighborsMixin,
                        RadiusNeighborsMixin, UnsupervisedMixin):
     """Unsupervised learner for implementing neighbor searches.
 
+    Read more in the :ref:`User Guide <unsupervised_neighbors>`.
+
     Parameters
     ----------
     n_neighbors : int, optional (default = 5)
@@ -42,7 +44,7 @@ class NearestNeighbors(NeighborsBase, KNeighborsMixin,
         sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is
         equivalent to using manhattan_distance (l1), and euclidean_distance
         (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.
- 
+
     metric : string or callable, default 'minkowski'
         metric to use for distance computation. Any metric from scikit-learn
         or scipy.spatial.distance can be used.
diff --git a/sklearn/neural_network/rbm.py b/sklearn/neural_network/rbm.py
index b646dae0d9d9..a236a98ae1e8 100644
--- a/sklearn/neural_network/rbm.py
+++ b/sklearn/neural_network/rbm.py
@@ -36,6 +36,8 @@ class BernoulliRBM(BaseEstimator, TransformerMixin):
     The time complexity of this implementation is ``O(d ** 2)`` assuming
     d ~ n_features ~ n_components.
 
+    Read more in the :ref:`User Guide <rbm>`.
+
     Parameters
     ----------
     n_components : int, optional
diff --git a/sklearn/pipeline.py b/sklearn/pipeline.py
index a29d7613eba2..85fedd926e94 100644
--- a/sklearn/pipeline.py
+++ b/sklearn/pipeline.py
@@ -37,6 +37,8 @@ class Pipeline(BaseEstimator):
     For this, it enables setting parameters of the various steps using their
     names and the parameter name separated by a '__', as in the example below.
 
+    Read more in the :ref:`User Guide <pipeline>`.
+
     Parameters
     ----------
     steps : list
@@ -419,6 +421,8 @@ class FeatureUnion(BaseEstimator, TransformerMixin):
     input data, then concatenates the results. This is useful to combine
     several feature extraction mechanisms into a single transformer.
 
+    Read more in the :ref:`User Guide <feature_union>`.
+
     Parameters
     ----------
     transformer_list: list of (string, transformer) tuples
diff --git a/sklearn/preprocessing/data.py b/sklearn/preprocessing/data.py
index a06351eb8d95..aadecc598ec4 100644
--- a/sklearn/preprocessing/data.py
+++ b/sklearn/preprocessing/data.py
@@ -20,7 +20,7 @@
 from ..utils.sparsefuncs_fast import (inplace_csr_row_normalize_l1,
                                       inplace_csr_row_normalize_l2)
 from ..utils.sparsefuncs import (inplace_column_scale, mean_variance_axis,
-                                 min_max_axis)
+                                 min_max_axis, inplace_row_scale)
 from ..utils.validation import check_is_fitted, FLOAT_DTYPES
 
 
@@ -74,6 +74,8 @@ def scale(X, axis=0, with_mean=True, with_std=True, copy=True):
 
     Center to the mean and component wise scale to unit variance.
 
+    Read more in the :ref:`User Guide <preprocessing_scaler>`.
+
     Parameters
     ----------
     X : array-like or CSR matrix.
@@ -194,6 +196,8 @@ class MinMaxScaler(BaseEstimator, TransformerMixin):
     This standardization is often used as an alternative to zero mean,
     unit variance scaling.
 
+    Read more in the :ref:`User Guide <preprocessing_scaler>`.
+
     Parameters
     ----------
     feature_range: tuple (min, max), default=(0, 1)
@@ -296,6 +300,8 @@ class StandardScaler(BaseEstimator, TransformerMixin):
     that others, it might dominate the objective function and make the
     estimator unable to learn from other features correctly as expected.
 
+    Read more in the :ref:`User Guide <preprocessing_scaler>`.
+
     Parameters
     ----------
     with_mean : boolean, True by default
@@ -449,6 +455,8 @@ class RobustScaler(BaseEstimator, TransformerMixin):
     sample mean / variance in a negative way. In such cases, the median and
     the interquartile range often give better results.
 
+    Read more in the :ref:`User Guide <preprocessing_scaler>`.
+
     Parameters
     ----------
     with_centering : boolean, True by default
@@ -606,6 +614,8 @@ def robust_scale(X, axis=0, with_centering=True, with_scaling=True, copy=True):
     Center to the median and component wise scale
     according to the interquartile range.
 
+    Read more in the :ref:`User Guide <preprocessing_scaler>`.
+
     Parameters
     ----------
     X : array-like.
@@ -789,6 +799,8 @@ def transform(self, X, y=None):
 def normalize(X, norm='l2', axis=1, copy=True):
     """Scale input vectors individually to unit norm (vector length).
 
+    Read more in the :ref:`User Guide <preprocessing_normalization>`.
+
     Parameters
     ----------
     X : array or scipy.sparse matrix with shape [n_samples, n_features]
@@ -873,6 +885,8 @@ class Normalizer(BaseEstimator, TransformerMixin):
     of the vectors and is the base similarity metric for the Vector
     Space Model commonly used by the Information Retrieval community.
 
+    Read more in the :ref:`User Guide <preprocessing_normalization>`.
+
     Parameters
     ----------
     norm : 'l1', 'l2', or 'max', optional ('l2' by default)
@@ -924,6 +938,8 @@ def transform(self, X, y=None, copy=None):
 def binarize(X, threshold=0.0, copy=True):
     """Boolean thresholding of array-like or scipy.sparse matrix
 
+    Read more in the :ref:`User Guide <preprocessing_binarization>`.
+
     Parameters
     ----------
     X : array or scipy.sparse matrix with shape [n_samples, n_features]
@@ -979,6 +995,8 @@ class Binarizer(BaseEstimator, TransformerMixin):
     consider boolean random variables (e.g. modelled using the Bernoulli
     distribution in a Bayesian setting).
 
+    Read more in the :ref:`User Guide <preprocessing_binarization>`.
+
     Parameters
     ----------
     threshold : float, optional (0.0 by default)
@@ -1033,6 +1051,8 @@ class KernelCenterer(BaseEstimator, TransformerMixin):
     normalize to have zero mean) the data without explicitly computing phi(x).
     It is equivalent to centering phi(x) with
     sklearn.preprocessing.StandardScaler(with_std=False).
+
+    Read more in the :ref:`User Guide <kernel_centering>`.
     """
 
     def fit(self, K, y=None):
@@ -1207,6 +1227,8 @@ class OneHotEncoder(BaseEstimator, TransformerMixin):
     This encoding is needed for feeding categorical data to many scikit-learn
     estimators, notably linear models and SVMs with the standard kernels.
 
+    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.
+
     Parameters
     ----------
     n_values : 'auto', int or array of ints
diff --git a/sklearn/preprocessing/imputation.py b/sklearn/preprocessing/imputation.py
index e3c4a426d820..0ef23c471bd6 100644
--- a/sklearn/preprocessing/imputation.py
+++ b/sklearn/preprocessing/imputation.py
@@ -64,6 +64,8 @@ def _most_frequent(array, extra_value, n_repeat):
 class Imputer(BaseEstimator, TransformerMixin):
     """Imputation transformer for completing missing values.
 
+    Read more in the :ref:`User Guide <imputation>`.
+
     Parameters
     ----------
     missing_values : integer or "NaN", optional (default="NaN")
diff --git a/sklearn/preprocessing/label.py b/sklearn/preprocessing/label.py
index a5722ea47968..17ad8231e1dd 100644
--- a/sklearn/preprocessing/label.py
+++ b/sklearn/preprocessing/label.py
@@ -56,6 +56,8 @@ def _check_numpy_unicode_bug(labels):
 class LabelEncoder(BaseEstimator, TransformerMixin):
     """Encode labels with value between 0 and n_classes-1.
 
+    Read more in the :ref:`User Guide <preprocessing_targets>`.
+
     Attributes
     ----------
     classes_ : array of shape (n_class,)
@@ -186,6 +188,8 @@ class LabelBinarizer(BaseEstimator, TransformerMixin):
     model gave the greatest confidence. LabelBinarizer makes this easy
     with the inverse_transform method.
 
+    Read more in the :ref:`User Guide <preprocessing_targets>`.
+
     Parameters
     ----------
 
diff --git a/sklearn/preprocessing/tests/test_function_transformer.py b/sklearn/preprocessing/tests/test_function_transformer.py
index c0ae70225979..e02e7580ce5e 100644
--- a/sklearn/preprocessing/tests/test_function_transformer.py
+++ b/sklearn/preprocessing/tests/test_function_transformer.py
@@ -43,7 +43,7 @@ def test_delegate_to_func():
     )
 
     # reset the argument stores.
-    args_store.clear()
+    args_store[:] = []  # python2 compatible inplace list clear.
     kwargs_store.clear()
     y = object()
 
diff --git a/sklearn/qda.py b/sklearn/qda.py
index 6b05f0b0d14f..951846172572 100644
--- a/sklearn/qda.py
+++ b/sklearn/qda.py
@@ -29,6 +29,8 @@ class QDA(BaseEstimator, ClassifierMixin):
 
     The model fits a Gaussian density to each class.
 
+    Read more in the :ref:`User Guide <lda_qda>`.
+
     Parameters
     ----------
     priors : array, optional, shape = [n_classes]
@@ -97,7 +99,7 @@ def fit(self, X, y, store_covariances=False, tol=1.0e-4):
         store_covariances : boolean
             If True the covariance matrices are computed and stored in the
             `self.covariances_` attribute.
-        
+
         tol : float, optional, default 1.0e-4
             Threshold used for rank estimation.
         """
diff --git a/sklearn/random_projection.py b/sklearn/random_projection.py
index 30cb5b78e397..0dccfd97d74c 100644
--- a/sklearn/random_projection.py
+++ b/sklearn/random_projection.py
@@ -75,6 +75,8 @@ def johnson_lindenstrauss_min_dim(n_samples, eps=0.1):
     the larger the dataset, the higher is the minimal dimensionality of
     an eps-embedding.
 
+    Read more in the :ref:`User Guide <johnson_lindenstrauss>`.
+
     Parameters
     ----------
     n_samples : int or numpy array of int greater than 0,
@@ -158,6 +160,8 @@ def gaussian_random_matrix(n_components, n_features, random_state=None):
 
         N(0, 1.0 / n_components).
 
+    Read more in the :ref:`User Guide <gaussian_random_matrix>`.
+
     Parameters
     ----------
     n_components : int,
@@ -203,6 +207,8 @@ def sparse_random_matrix(n_components, n_features, density='auto',
       -  0                              with probability 1 - 1 / s
       - +sqrt(s) / sqrt(n_components)   with probability 1 / 2s
 
+    Read more in the :ref:`User Guide <sparse_random_matrix>`.
+
     Parameters
     ----------
     n_components : int,
@@ -420,6 +426,8 @@ class GaussianRandomProjection(BaseRandomProjection):
 
     The components of the random matrix are drawn from N(0, 1 / n_components).
 
+    Read more in the :ref:`User Guide <gaussian_random_matrix>`.
+
     Parameters
     ----------
     n_components : int or 'auto', optional (default = 'auto')
@@ -504,6 +512,8 @@ class SparseRandomProjection(BaseRandomProjection):
       -  0                              with probability 1 - 1 / s
       - +sqrt(s) / sqrt(n_components)   with probability 1 / 2s
 
+    Read more in the :ref:`User Guide <sparse_random_matrix>`.
+
     Parameters
     ----------
     n_components : int or 'auto', optional (default = 'auto')
diff --git a/sklearn/semi_supervised/label_propagation.py b/sklearn/semi_supervised/label_propagation.py
index 31429a2a7c2b..855f6c085795 100644
--- a/sklearn/semi_supervised/label_propagation.py
+++ b/sklearn/semi_supervised/label_propagation.py
@@ -98,7 +98,7 @@ class BaseLabelPropagation(six.with_metaclass(ABCMeta, BaseEstimator,
 
     n_neighbors : integer > 0
         Parameter for knn kernel
- 
+
     """
 
     def __init__(self, kernel='rbf', gamma=20, n_neighbors=7,
@@ -268,19 +268,26 @@ def fit(self, X, y):
 class LabelPropagation(BaseLabelPropagation):
     """Label Propagation classifier
 
+    Read more in the :ref:`User Guide <label_propagation>`.
+
     Parameters
     ----------
     kernel : {'knn', 'rbf'}
         String identifier for kernel function to use.
         Only 'rbf' and 'knn' kernels are currently supported..
+
     gamma : float
         Parameter for rbf kernel
+
     n_neighbors : integer > 0
         Parameter for knn kernel
+
     alpha : float
         Clamping factor
+
     max_iter : float
         Change maximum number of iterations allowed
+
     tol : float
         Convergence tolerance: threshold to consider the system at steady
         state
@@ -350,19 +357,26 @@ class LabelSpreading(BaseLabelPropagation):
     but uses affinity matrix based on the normalized graph Laplacian
     and soft clamping across the labels.
 
+    Read more in the :ref:`User Guide <label_propagation>`.
+
     Parameters
     ----------
     kernel : {'knn', 'rbf'}
         String identifier for kernel function to use.
         Only 'rbf' and 'knn' kernels are currently supported.
+
     gamma : float
       parameter for rbf kernel
+
     n_neighbors : integer > 0
       parameter for knn kernel
+
     alpha : float
       clamping factor
+
     max_iter : float
       maximum number of iterations allowed
+
     tol : float
       Convergence tolerance: threshold to consider the system at steady
       state
diff --git a/sklearn/svm/base.py b/sklearn/svm/base.py
index e43fcb17a984..08585d120de0 100644
--- a/sklearn/svm/base.py
+++ b/sklearn/svm/base.py
@@ -7,8 +7,9 @@
 
 from . import libsvm, liblinear
 from . import libsvm_sparse
-from ..base import BaseEstimator, ClassifierMixin
+from ..base import BaseEstimator, ClassifierMixin, ChangedBehaviorWarning
 from ..preprocessing import LabelEncoder
+from ..multiclass import _ovr_decision_function
 from ..utils import check_array, check_random_state, column_or_1d
 from ..utils import ConvergenceWarning, compute_class_weight, deprecated
 from ..utils.extmath import safe_sparse_dot
@@ -73,6 +74,15 @@ def __init__(self, impl, kernel, degree, gamma, coef0,
             raise ValueError("impl should be one of %s, %s was given" % (
                 LIBSVM_IMPL, impl))
 
+        # FIXME Remove gamma=0.0 support in 0.18
+        if gamma == 0:
+            msg = ("gamma=%s has been deprecated in favor of "
+                   "gamma='%s' as of 0.17. Backward compatibility"
+                   " for gamma=%s will be removed in %s")
+            invalid_gamma = 0.0
+            warnings.warn(msg % (invalid_gamma, "auto",
+                invalid_gamma, "0.18"), DeprecationWarning)
+
         self._impl = impl
         self.kernel = kernel
         self.degree = degree
@@ -160,9 +170,13 @@ def fit(self, X, y, sample_weight=None):
                              "boolean masks (use `indices=True` in CV)."
                              % (sample_weight.shape, X.shape))
 
-        if (self.kernel in ['poly', 'rbf']) and (self.gamma == 0):
+        # FIXME remove (self.gamma == 0) in 0.18
+        if (self.kernel in ['poly', 'rbf']) and ((self.gamma == 0)
+                or (self.gamma == 'auto')):
             # if custom gamma is not provided ...
             self._gamma = 1.0 / X.shape[1]
+        elif self.gamma == 'auto':
+            self._gamma = 0.0
         else:
             self._gamma = self.gamma
 
@@ -481,8 +495,19 @@ def _get_coef(self):
         return safe_sparse_dot(self._dual_coef_, self.support_vectors_)
 
 
-class BaseSVC(BaseLibSVM, ClassifierMixin):
+class BaseSVC(six.with_metaclass(ABCMeta, BaseLibSVM, ClassifierMixin)):
     """ABC for LibSVM-based classifiers."""
+    @abstractmethod
+    def __init__(self, impl, kernel, degree, gamma, coef0, tol, C, nu,
+                 shrinking, probability, cache_size, class_weight, verbose,
+                 max_iter, decision_function_shape, random_state):
+        self.decision_function_shape = decision_function_shape
+        super(BaseSVC, self).__init__(
+            impl=impl, kernel=kernel, degree=degree, gamma=gamma, coef0=coef0,
+            tol=tol, C=C, nu=nu, epsilon=0., shrinking=shrinking,
+            probability=probability, cache_size=cache_size,
+            class_weight=class_weight, verbose=verbose, max_iter=max_iter,
+            random_state=random_state)
 
     def _validate_targets(self, y):
         y_ = column_or_1d(y, warn=True)
@@ -506,11 +531,21 @@ def decision_function(self, X):
 
         Returns
         -------
-        X : array-like, shape (n_samples, n_class * (n_class-1) / 2)
+        X : array-like, shape (n_samples, n_classes * (n_classes-1) / 2)
             Returns the decision function of the sample for each class
             in the model.
+            If decision_function_shape='ovr', the shape is (n_samples,
+            n_classes)
         """
-        return self._decision_function(X)
+        dec = self._decision_function(X)
+        if self.decision_function_shape is None and len(self.classes_) > 2:
+            warnings.warn("The decision_function_shape default value will "
+                          "change from 'ovo' to 'ovr' in 0.18. This will change "
+                          "the shape of the decision function returned by "
+                          "SVC.", ChangedBehaviorWarning)
+        if self.decision_function_shape == 'ovr':
+            return _ovr_decision_function(dec < 0, dec, len(self.classes_))
+        return dec
 
     def predict(self, X):
         """Perform classification on samples in X.
@@ -760,10 +795,15 @@ def _fit_liblinear(X, y, C, fit_intercept, intercept_scaling, class_weight,
         In order to avoid this, one should increase the intercept_scaling.
         such that the feature vector becomes [x, intercept_scaling].
 
-    class_weight : {dict, 'auto'}, optional
-        Weight assigned to each class. If class_weight provided is 'auto',
-        then the weights provided are inverses of the frequency in the
-        target vector.
+    class_weight : {dict, 'balanced'}, optional
+        Weights associated with classes in the form ``{class_label: weight}``.
+        If not given, all classes are supposed to have weight one. For
+        multi-output problems, a list of dicts can be provided in the same
+        order as the columns of y.
+
+        The "balanced" mode uses the values of y to automatically adjust
+        weights inversely proportional to class frequencies in the input data
+        as ``n_samples / (n_classes * np.bincount(y))``
 
     penalty : str, {'l1', 'l2'}
         The norm of the penalty used in regularization.
diff --git a/sklearn/svm/classes.py b/sklearn/svm/classes.py
index cd610a870710..036adb540b44 100644
--- a/sklearn/svm/classes.py
+++ b/sklearn/svm/classes.py
@@ -21,6 +21,8 @@ class LinearSVC(BaseEstimator, LinearClassifierMixin,
     This class supports both dense and sparse input and the multiclass support
     is handled according to a one-vs-the-rest scheme.
 
+    Read more in the :ref:`User Guide <svm_classification>`.
+
     Parameters
     ----------
     C : float, optional (default=1.0)
@@ -49,8 +51,8 @@ class LinearSVC(BaseEstimator, LinearClassifierMixin,
         `ovr` trains n_classes one-vs-rest classifiers, while `crammer_singer`
         optimizes a joint objective over all classes.
         While `crammer_singer` is interesting from a theoretical perspective
-        as it is consistent, it is seldom used in practice as it rarely leads to
-        better accuracy and is more expensive to compute.
+        as it is consistent, it is seldom used in practice as it rarely leads
+        to better accuracy and is more expensive to compute.
         If `crammer_singer` is chosen, the options loss, penalty and dual will
         be ignored.
 
@@ -70,12 +72,13 @@ class LinearSVC(BaseEstimator, LinearClassifierMixin,
         To lessen the effect of regularization on synthetic feature weight
         (and therefore on the intercept) intercept_scaling has to be increased.
 
-    class_weight : {dict, 'auto'}, optional
+    class_weight : {dict, 'balanced'}, optional
         Set the parameter C of class i to class_weight[i]*C for
         SVC. If not given, all classes are supposed to have
-        weight one. The 'auto' mode uses the values of y to
-        automatically adjust weights inversely proportional to
-        class frequencies.
+        weight one.
+        The "balanced" mode uses the values of y to automatically adjust
+        weights inversely proportional to class frequencies in the input data
+        as ``n_samples / (n_classes * np.bincount(y))``
 
     verbose : int, (default=0)
         Enable verbose output. Note that this setting takes advantage of a
@@ -225,6 +228,8 @@ class LinearSVR(LinearModel, RegressorMixin):
 
     This class supports both dense and sparse input.
 
+    Read more in the :ref:`User Guide <svm_regression>`.
+
     Parameters
     ----------
     C : float, optional (default=1.0)
@@ -388,7 +393,7 @@ class SVC(BaseSVC):
     other, see the corresponding section in the narrative documentation:
     :ref:`svm_kernels`.
 
-    .. The narrative documentation is available at http://scikit-learn.org/
+    Read more in the :ref:`User Guide <svm_classification>`.
 
     Parameters
     ----------
@@ -406,9 +411,9 @@ class SVC(BaseSVC):
         Degree of the polynomial kernel function ('poly').
         Ignored by all other kernels.
 
-    gamma : float, optional (default=0.0)
+    gamma : float, optional (default='auto')
         Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.
-        If gamma is 0.0 then 1/n_features will be used instead.
+        If gamma is 'auto' then 1/n_features will be used instead.
 
     coef0 : float, optional (default=0.0)
         Independent term in kernel function.
@@ -427,12 +432,13 @@ class SVC(BaseSVC):
     cache_size : float, optional
         Specify the size of the kernel cache (in MB).
 
-    class_weight : {dict, 'auto'}, optional
+    class_weight : {dict, 'balanced'}, optional
         Set the parameter C of class i to class_weight[i]*C for
         SVC. If not given, all classes are supposed to have
-        weight one. The 'auto' mode uses the values of y to
-        automatically adjust weights inversely proportional to
-        class frequencies.
+        weight one.
+        The "balanced" mode uses the values of y to automatically adjust
+        weights inversely proportional to class frequencies in the input data
+        as ``n_samples / (n_classes * np.bincount(y))``
 
     verbose : bool, default: False
         Enable verbose output. Note that this setting takes advantage of a
@@ -442,6 +448,15 @@ class frequencies.
     max_iter : int, optional (default=-1)
         Hard limit on iterations within solver, or -1 for no limit.
 
+    decision_function_shape : 'ovo', 'ovr' or None, default=None
+        Whether to return a one-vs-rest ('ovr') ecision function of shape
+        (n_samples, n_classes) as all other classifiers, or the original
+        one-vs-one ('ovo') decision function of libsvm which has shape
+        (n_samples, n_classes * (n_classes - 1) / 2).
+        The default of None will currently behave as 'ovo' for backward
+        compatibility and raise a deprecation warning, but will change 'ovr'
+        in 0.18.
+
     random_state : int seed, RandomState instance, or None (default)
         The seed of the pseudo random number generator to use when
         shuffling the data for probability estimation.
@@ -482,9 +497,10 @@ class frequencies.
     >>> from sklearn.svm import SVC
     >>> clf = SVC()
     >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE
-    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
-        gamma=0.0, kernel='rbf', max_iter=-1, probability=False,
-        random_state=None, shrinking=True, tol=0.001, verbose=False)
+    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
+        decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
+        max_iter=-1, probability=False, random_state=None, shrinking=True,
+        tol=0.001, verbose=False)
     >>> print(clf.predict([[-0.8, -1]]))
     [1]
 
@@ -500,15 +516,19 @@ class frequencies.
 
     """
 
-    def __init__(self, C=1.0, kernel='rbf', degree=3, gamma=0.0,
+    def __init__(self, C=1.0, kernel='rbf', degree=3, gamma='auto',
                  coef0=0.0, shrinking=True, probability=False,
                  tol=1e-3, cache_size=200, class_weight=None,
-                 verbose=False, max_iter=-1, random_state=None):
+                 verbose=False, max_iter=-1, decision_function_shape=None,
+                 random_state=None):
 
         super(SVC, self).__init__(
-            'c_svc', kernel, degree, gamma, coef0, tol, C, 0., 0., shrinking,
-            probability, cache_size, class_weight, verbose, max_iter,
-            random_state)
+            impl='c_svc', kernel=kernel, degree=degree, gamma=gamma,
+            coef0=coef0, tol=tol, C=C, nu=0., shrinking=shrinking,
+            probability=probability, cache_size=cache_size,
+            class_weight=class_weight, verbose=verbose, max_iter=max_iter,
+            decision_function_shape=decision_function_shape,
+            random_state=random_state)
 
 
 class NuSVC(BaseSVC):
@@ -519,6 +539,8 @@ class NuSVC(BaseSVC):
 
     The implementation is based on libsvm.
 
+    Read more in the :ref:`User Guide <svm_classification>`.
+
     Parameters
     ----------
     nu : float, optional (default=0.5)
@@ -537,9 +559,9 @@ class NuSVC(BaseSVC):
         Degree of the polynomial kernel function ('poly').
         Ignored by all other kernels.
 
-    gamma : float, optional (default=0.0)
+    gamma : float, optional (default='auto')
         Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.
-        If gamma is 0.0 then 1/n_features will be used instead.
+        If gamma is 'auto' then 1/n_features will be used instead.
 
     coef0 : float, optional (default=0.0)
         Independent term in kernel function.
@@ -558,6 +580,13 @@ class NuSVC(BaseSVC):
     cache_size : float, optional
         Specify the size of the kernel cache (in MB).
 
+    class_weight : {dict, 'auto'}, optional
+        Set the parameter C of class i to class_weight[i]*C for
+        SVC. If not given, all classes are supposed to have
+        weight one. The 'auto' mode uses the values of y to
+        automatically adjust weights inversely proportional to
+        class frequencies.
+
     verbose : bool, default: False
         Enable verbose output. Note that this setting takes advantage of a
         per-process runtime setting in libsvm that, if enabled, may not work
@@ -566,6 +595,15 @@ class NuSVC(BaseSVC):
     max_iter : int, optional (default=-1)
         Hard limit on iterations within solver, or -1 for no limit.
 
+    decision_function_shape : 'ovo', 'ovr' or None, default=None
+        Whether to return a one-vs-rest ('ovr') ecision function of shape
+        (n_samples, n_classes) as all other classifiers, or the original
+        one-vs-one ('ovo') decision function of libsvm which has shape
+        (n_samples, n_classes * (n_classes - 1) / 2).
+        The default of None will currently behave as 'ovo' for backward
+        compatibility and raise a deprecation warning, but will change 'ovr'
+        in 0.18.
+
     random_state : int seed, RandomState instance, or None (default)
         The seed of the pseudo random number generator to use when
         shuffling the data for probability estimation.
@@ -579,7 +617,7 @@ class NuSVC(BaseSVC):
         Support vectors.
 
     n_support_ : array-like, dtype=int32, shape = [n_class]
-        Number of support vector for each class.
+        Number of support vectors for each class.
 
     dual_coef_ : array, shape = [n_class-1, n_SV]
         Coefficients of the support vector in the decision function. \
@@ -606,7 +644,8 @@ class NuSVC(BaseSVC):
     >>> from sklearn.svm import NuSVC
     >>> clf = NuSVC()
     >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE
-    NuSVC(cache_size=200, coef0=0.0, degree=3, gamma=0.0, kernel='rbf',
+    NuSVC(cache_size=200, class_weight=None, coef0=0.0,
+          decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
           max_iter=-1, nu=0.5, probability=False, random_state=None,
           shrinking=True, tol=0.001, verbose=False)
     >>> print(clf.predict([[-0.8, -1]]))
@@ -622,14 +661,18 @@ class NuSVC(BaseSVC):
         liblinear.
     """
 
-    def __init__(self, nu=0.5, kernel='rbf', degree=3, gamma=0.0,
+    def __init__(self, nu=0.5, kernel='rbf', degree=3, gamma='auto',
                  coef0=0.0, shrinking=True, probability=False,
-                 tol=1e-3, cache_size=200, verbose=False, max_iter=-1,
-                 random_state=None):
+                 tol=1e-3, cache_size=200, class_weight=None, verbose=False,
+                 max_iter=-1, decision_function_shape=None, random_state=None):
 
         super(NuSVC, self).__init__(
-            'nu_svc', kernel, degree, gamma, coef0, tol, 0., nu, 0., shrinking,
-            probability, cache_size, None, verbose, max_iter, random_state)
+            impl='nu_svc', kernel=kernel, degree=degree, gamma=gamma,
+            coef0=coef0, tol=tol, C=0., nu=nu, shrinking=shrinking,
+            probability=probability, cache_size=cache_size,
+            class_weight=class_weight, verbose=verbose, max_iter=max_iter,
+            decision_function_shape=decision_function_shape,
+            random_state=random_state)
 
 
 class SVR(BaseLibSVM, RegressorMixin):
@@ -639,6 +682,8 @@ class SVR(BaseLibSVM, RegressorMixin):
 
     The implementation is based on libsvm.
 
+    Read more in the :ref:`User Guide <svm_regression>`.
+
     Parameters
     ----------
     C : float, optional (default=1.0)
@@ -661,9 +706,9 @@ class SVR(BaseLibSVM, RegressorMixin):
         Degree of the polynomial kernel function ('poly').
         Ignored by all other kernels.
 
-    gamma : float, optional (default=0.0)
+    gamma : float, optional (default='auto')
         Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.
-        If gamma is 0.0 then 1/n_features will be used instead.
+        If gamma is 'auto' then 1/n_features will be used instead.
 
     coef0 : float, optional (default=0.0)
         Independent term in kernel function.
@@ -717,7 +762,7 @@ class SVR(BaseLibSVM, RegressorMixin):
     >>> X = np.random.randn(n_samples, n_features)
     >>> clf = SVR(C=1.0, epsilon=0.2)
     >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE
-    SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.2, gamma=0.0,
+    SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.2, gamma='auto',
         kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)
 
     See also
@@ -730,9 +775,9 @@ class SVR(BaseLibSVM, RegressorMixin):
         Scalable Linear Support Vector Machine for regression
         implemented using liblinear.
     """
-    def __init__(self, kernel='rbf', degree=3, gamma=0.0, coef0=0.0, tol=1e-3,
-                 C=1.0, epsilon=0.1, shrinking=True, cache_size=200,
-                 verbose=False, max_iter=-1):
+    def __init__(self, kernel='rbf', degree=3, gamma='auto', coef0=0.0,
+                 tol=1e-3, C=1.0, epsilon=0.1, shrinking=True,
+                 cache_size=200, verbose=False, max_iter=-1):
 
         super(SVR, self).__init__(
             'epsilon_svr', kernel=kernel, degree=degree, gamma=gamma,
@@ -750,6 +795,8 @@ class NuSVR(BaseLibSVM, RegressorMixin):
 
     The implementation is based on libsvm.
 
+    Read more in the :ref:`User Guide <svm_regression>`.
+
     Parameters
     ----------
     C : float, optional (default=1.0)
@@ -771,9 +818,9 @@ class NuSVR(BaseLibSVM, RegressorMixin):
         Degree of the polynomial kernel function ('poly').
         Ignored by all other kernels.
 
-    gamma : float, optional (default=0.0)
+    gamma : float, optional (default='auto')
         Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.
-        If gamma is 0.0 then 1/n_features will be used instead.
+        If gamma is 'auto' then 1/n_features will be used instead.
 
     coef0 : float, optional (default=0.0)
         Independent term in kernel function.
@@ -827,8 +874,9 @@ class NuSVR(BaseLibSVM, RegressorMixin):
     >>> X = np.random.randn(n_samples, n_features)
     >>> clf = NuSVR(C=1.0, nu=0.1)
     >>> clf.fit(X, y)  #doctest: +NORMALIZE_WHITESPACE
-    NuSVR(C=1.0, cache_size=200, coef0=0.0, degree=3, gamma=0.0, kernel='rbf',
-          max_iter=-1, nu=0.1, shrinking=True, tol=0.001, verbose=False)
+    NuSVR(C=1.0, cache_size=200, coef0=0.0, degree=3, gamma='auto',
+          kernel='rbf', max_iter=-1, nu=0.1, shrinking=True, tol=0.001,
+          verbose=False)
 
     See also
     --------
@@ -841,7 +889,7 @@ class NuSVR(BaseLibSVM, RegressorMixin):
     """
 
     def __init__(self, nu=0.5, C=1.0, kernel='rbf', degree=3,
-                 gamma=0.0, coef0=0.0, shrinking=True, tol=1e-3,
+                 gamma='auto', coef0=0.0, shrinking=True, tol=1e-3,
                  cache_size=200, verbose=False, max_iter=-1):
 
         super(NuSVR, self).__init__(
@@ -858,6 +906,8 @@ class OneClassSVM(BaseLibSVM):
 
     The implementation is based on libsvm.
 
+    Read more in the :ref:`User Guide <svm_outlier_detection>`.
+
     Parameters
     ----------
     kernel : string, optional (default='rbf')
@@ -877,9 +927,9 @@ class OneClassSVM(BaseLibSVM):
         Degree of the polynomial kernel function ('poly').
         Ignored by all other kernels.
 
-    gamma : float, optional (default=0.0)
+    gamma : float, optional (default='auto')
         Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.
-        If gamma is 0.0 then 1/n_features will be used instead.
+        If gamma is 'auto' then 1/n_features will be used instead.
 
     coef0 : float, optional (default=0.0)
         Independent term in kernel function.
@@ -928,9 +978,9 @@ class OneClassSVM(BaseLibSVM):
         Constants in decision function.
 
     """
-    def __init__(self, kernel='rbf', degree=3, gamma=0.0, coef0=0.0, tol=1e-3,
-                 nu=0.5, shrinking=True, cache_size=200, verbose=False,
-                 max_iter=-1, random_state=None):
+    def __init__(self, kernel='rbf', degree=3, gamma='auto', coef0=0.0,
+                 tol=1e-3, nu=0.5, shrinking=True, cache_size=200,
+                 verbose=False, max_iter=-1, random_state=None):
 
         super(OneClassSVM, self).__init__(
             'one_class', kernel, degree, gamma, coef0, tol, 0., nu, 0.,
@@ -964,3 +1014,18 @@ def fit(self, X, y=None, sample_weight=None, **params):
         super(OneClassSVM, self).fit(X, [], sample_weight=sample_weight,
                                      **params)
         return self
+
+    def decision_function(self, X):
+        """Distance of the samples X to the separating hyperplane.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+
+        Returns
+        -------
+        X : array-like, shape (n_samples,)
+            Returns the decision function of the samples.
+        """
+        dec = self._decision_function(X)
+        return dec
diff --git a/sklearn/svm/tests/test_sparse.py b/sklearn/svm/tests/test_sparse.py
index 052fbf799e86..f73ac2dae3a7 100644
--- a/sklearn/svm/tests/test_sparse.py
+++ b/sklearn/svm/tests/test_sparse.py
@@ -139,13 +139,11 @@ def test_svc_iris():
 
 
 def test_sparse_decision_function():
-    """
-    Test decision_function
+    #Test decision_function
 
-    Sanity check, test that decision_function implemented in python
-    returns the same as the one in libsvm
+    #Sanity check, test that decision_function implemented in python
+    #returns the same as the one in libsvm
 
-    """
     # multi class:
     clf = svm.SVC(kernel='linear', C=0.1).fit(iris.data, iris.target)
 
diff --git a/sklearn/svm/tests/test_svm.py b/sklearn/svm/tests/test_svm.py
index 08cb2d5c9ca8..50cf976fee77 100644
--- a/sklearn/svm/tests/test_svm.py
+++ b/sklearn/svm/tests/test_svm.py
@@ -12,8 +12,10 @@
 from scipy import sparse
 from nose.tools import assert_raises, assert_true, assert_equal, assert_false
 
+from sklearn.base import ChangedBehaviorWarning
 from sklearn import svm, linear_model, datasets, metrics, base
-from sklearn.datasets.samples_generator import make_classification
+from sklearn.cross_validation import train_test_split
+from sklearn.datasets import make_classification, make_blobs
 from sklearn.metrics import f1_score
 from sklearn.metrics.pairwise import rbf_kernel
 from sklearn.utils import check_random_state
@@ -288,7 +290,8 @@ def test_decision_function():
     # Sanity check, test that decision_function implemented in python
     # returns the same as the one in libsvm
     # multi class:
-    clf = svm.SVC(kernel='linear', C=0.1).fit(iris.data, iris.target)
+    clf = svm.SVC(kernel='linear', C=0.1,
+                  decision_function_shape='ovo').fit(iris.data, iris.target)
 
     dec = np.dot(iris.data, clf.coef_.T) + clf.intercept_
 
@@ -306,14 +309,48 @@ def test_decision_function():
     assert_array_almost_equal(clf.decision_function(X), expected, 2)
 
     # kernel binary:
-    clf = svm.SVC(kernel='rbf', gamma=1)
+    clf = svm.SVC(kernel='rbf', gamma=1, decision_function_shape='ovo')
     clf.fit(X, Y)
-    
+
     rbfs = rbf_kernel(X, clf.support_vectors_, gamma=clf.gamma)
     dec = np.dot(rbfs, clf.dual_coef_.T) + clf.intercept_
     assert_array_almost_equal(dec.ravel(), clf.decision_function(X))
 
 
+def test_decision_function_shape():
+    # check that decision_function_shape='ovr' gives
+    # correct shape and is consistent with predict
+
+    clf = svm.SVC(kernel='linear', C=0.1,
+                  decision_function_shape='ovr').fit(iris.data, iris.target)
+    dec = clf.decision_function(iris.data)
+    assert_equal(dec.shape, (len(iris.data), 3))
+    assert_array_equal(clf.predict(iris.data), np.argmax(dec, axis=1))
+
+    # with five classes:
+    X, y = make_blobs(n_samples=80, centers=5, random_state=0)
+    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
+
+    clf = svm.SVC(kernel='linear', C=0.1,
+                  decision_function_shape='ovr').fit(X_train, y_train)
+    dec = clf.decision_function(X_test)
+    assert_equal(dec.shape, (len(X_test), 5))
+    assert_array_equal(clf.predict(X_test), np.argmax(dec, axis=1))
+
+    # check shape of ovo_decition_function=True
+    clf = svm.SVC(kernel='linear', C=0.1,
+                  decision_function_shape='ovo').fit(X_train, y_train)
+    dec = clf.decision_function(X_train)
+    assert_equal(dec.shape, (len(X_train), 10))
+
+    # check deprecation warning
+    clf.decision_function_shape = None
+    msg = "change the shape of the decision function"
+    dec = assert_warns_message(ChangedBehaviorWarning, msg,
+                               clf.decision_function, X_train)
+    assert_equal(dec.shape, (len(X_train), 10))
+
+
 def test_svr_decision_function():
     # Test SVR's decision_function
     # Sanity check, test that decision_function implemented in python
@@ -330,7 +367,7 @@ def test_svr_decision_function():
 
     # rbf kernel
     reg = svm.SVR(kernel='rbf', gamma=1).fit(X, y)
-    
+
     rbfs = rbf_kernel(X, reg.support_vectors_, gamma=reg.gamma)
     dec = np.dot(rbfs, reg.dual_coef_.T) + reg.intercept_
     assert_array_almost_equal(dec.ravel(), reg.decision_function(X).ravel())
@@ -381,21 +418,21 @@ def test_auto_weight():
     # We take as dataset the two-dimensional projection of iris so
     # that it is not separable and remove half of predictors from
     # class 1.
-    # We add one to the targets as a non-regression test: class_weight="auto"
+    # We add one to the targets as a non-regression test: class_weight="balanced"
     # used to work only when the labels where a range [0..K).
     from sklearn.utils import compute_class_weight
     X, y = iris.data[:, :2], iris.target + 1
     unbalanced = np.delete(np.arange(y.size), np.where(y > 2)[0][::2])
 
     classes = np.unique(y[unbalanced])
-    class_weights = compute_class_weight('auto', classes, y[unbalanced])
+    class_weights = compute_class_weight('balanced', classes, y[unbalanced])
     assert_true(np.argmax(class_weights) == 2)
 
     for clf in (svm.SVC(kernel='linear'), svm.LinearSVC(random_state=0),
                 LogisticRegression()):
-        # check that score is better when class='auto' is set.
+        # check that score is better when class='balanced' is set.
         y_pred = clf.fit(X[unbalanced], y[unbalanced]).predict(X)
-        clf.set_params(class_weight='auto')
+        clf.set_params(class_weight='balanced')
         y_pred_balanced = clf.fit(X[unbalanced], y[unbalanced],).predict(X)
         assert_true(metrics.f1_score(y, y_pred, average='weighted')
                     <= metrics.f1_score(y, y_pred_balanced,
@@ -691,19 +728,6 @@ def test_immutable_coef_property():
                       clf.coef_.__setitem__, (0, 0), 0)
 
 
-def test_inheritance():
-    # check that SVC classes can do inheritance
-    class ChildSVC(svm.SVC):
-        def __init__(self, foo=0):
-            self.foo = foo
-            svm.SVC.__init__(self)
-
-    clf = ChildSVC()
-    clf.fit(iris.data, iris.target)
-    clf.predict(iris.data[-1])
-    clf.decision_function(iris.data[-1])
-
-
 def test_linearsvc_verbose():
     # stdout: redirect
     import os
@@ -722,12 +746,14 @@ def test_svc_clone_with_callable_kernel():
     # create SVM with callable linear kernel, check that results are the same
     # as with built-in linear kernel
     svm_callable = svm.SVC(kernel=lambda x, y: np.dot(x, y.T),
-                           probability=True, random_state=0)
+                           probability=True, random_state=0,
+                           decision_function_shape='ovr')
     # clone for checking clonability with lambda functions..
     svm_cloned = base.clone(svm_callable)
     svm_cloned.fit(iris.data, iris.target)
 
-    svm_builtin = svm.SVC(kernel='linear', probability=True, random_state=0)
+    svm_builtin = svm.SVC(kernel='linear', probability=True, random_state=0,
+                          decision_function_shape='ovr')
     svm_builtin.fit(iris.data, iris.target)
 
     assert_array_almost_equal(svm_cloned.dual_coef_,
@@ -813,8 +839,3 @@ def test_lsvc_intercept_scaling_zero():
     lsvc = svm.LinearSVC(fit_intercept=False)
     lsvc.fit(X, Y)
     assert_equal(lsvc.intercept_, 0.)
-
-
-if __name__ == '__main__':
-    import nose
-    nose.runmodule()
diff --git a/sklearn/tests/test_common.py b/sklearn/tests/test_common.py
index fe482d022d4a..65be1f6892d6 100644
--- a/sklearn/tests/test_common.py
+++ b/sklearn/tests/test_common.py
@@ -27,7 +27,7 @@
     _yield_all_checks,
     CROSS_DECOMPOSITION,
     check_parameters_default_constructible,
-    check_class_weight_auto_linear_classifier,
+    check_class_weight_balanced_linear_classifier,
     check_transformer_n_iter,
     check_non_transformer_estimators_n_iter,
     check_get_params_invariance)
@@ -94,7 +94,7 @@ def test_configure():
         os.chdir(cwd)
 
 
-def test_class_weight_auto_linear_classifiers():
+def test_class_weight_balanced_linear_classifiers():
     classifiers = all_estimators(type_filter='classifier')
 
     clean_warning_registry()
@@ -112,7 +112,7 @@ def test_class_weight_auto_linear_classifiers():
             # the coef. Therefore it is expected to not behave exactly as the
             # other linear model.
             continue
-        yield check_class_weight_auto_linear_classifier, name, Classifier
+        yield check_class_weight_balanced_linear_classifier, name, Classifier
 
 
 @ignore_warnings
diff --git a/sklearn/tests/test_cross_validation.py b/sklearn/tests/test_cross_validation.py
index c1b8a1c99a75..59796b10eae2 100644
--- a/sklearn/tests/test_cross_validation.py
+++ b/sklearn/tests/test_cross_validation.py
@@ -950,15 +950,15 @@ def test_permutation_test_score_allow_nans():
 
 def test_check_cv_return_types():
     X = np.ones((9, 2))
-    cv = cval._check_cv(3, X, classifier=False)
+    cv = cval.check_cv(3, X, classifier=False)
     assert_true(isinstance(cv, cval.KFold))
 
     y_binary = np.array([0, 1, 0, 1, 0, 0, 1, 1, 1])
-    cv = cval._check_cv(3, X, y_binary, classifier=True)
+    cv = cval.check_cv(3, X, y_binary, classifier=True)
     assert_true(isinstance(cv, cval.StratifiedKFold))
 
     y_multiclass = np.array([0, 1, 0, 1, 2, 1, 2, 0, 2])
-    cv = cval._check_cv(3, X, y_multiclass, classifier=True)
+    cv = cval.check_cv(3, X, y_multiclass, classifier=True)
     assert_true(isinstance(cv, cval.StratifiedKFold))
 
     X = np.ones((5, 2))
@@ -966,15 +966,15 @@ def test_check_cv_return_types():
 
     with warnings.catch_warnings(record=True):
         # deprecated sequence of sequence format
-        cv = cval._check_cv(3, X, y_seq_of_seqs, classifier=True)
+        cv = cval.check_cv(3, X, y_seq_of_seqs, classifier=True)
     assert_true(isinstance(cv, cval.KFold))
 
     y_indicator_matrix = LabelBinarizer().fit_transform(y_seq_of_seqs)
-    cv = cval._check_cv(3, X, y_indicator_matrix, classifier=True)
+    cv = cval.check_cv(3, X, y_indicator_matrix, classifier=True)
     assert_true(isinstance(cv, cval.KFold))
 
     y_multioutput = np.array([[1, 2], [0, 3], [0, 0], [3, 1], [2, 0]])
-    cv = cval._check_cv(3, X, y_multioutput, classifier=True)
+    cv = cval.check_cv(3, X, y_multioutput, classifier=True)
     assert_true(isinstance(cv, cval.KFold))
 
 
diff --git a/sklearn/tests/test_dummy.py b/sklearn/tests/test_dummy.py
index cee01788d9f7..b3555da25b01 100644
--- a/sklearn/tests/test_dummy.py
+++ b/sklearn/tests/test_dummy.py
@@ -597,8 +597,3 @@ def test_dummy_regressor_sample_weight(n_samples=10):
     est = DummyRegressor(strategy="quantile", quantile=.95).fit(X, y,
                                                                 sample_weight)
     assert_equal(est.constant_, _weighted_percentile(y, sample_weight, 95.))
-
-
-if __name__ == '__main__':
-    import nose
-    nose.runmodule()
diff --git a/sklearn/tests/test_isotonic.py b/sklearn/tests/test_isotonic.py
index f6b48020e0e5..e77f188d1c71 100644
--- a/sklearn/tests/test_isotonic.py
+++ b/sklearn/tests/test_isotonic.py
@@ -346,8 +346,3 @@ def test_isotonic_zero_weight_loop():
 
     # This will hang in failure case.
     regression.fit(x, y, sample_weight=w)
-
-
-if __name__ == "__main__":
-    import nose
-    nose.run(argv=['', __file__])
diff --git a/sklearn/tests/test_multiclass.py b/sklearn/tests/test_multiclass.py
index b10b55925847..39b1547ea628 100644
--- a/sklearn/tests/test_multiclass.py
+++ b/sklearn/tests/test_multiclass.py
@@ -616,8 +616,3 @@ def test_deprecated():
             assert_almost_equal(predict_func(estimators_, classes_or_lb,
                                              codebook, X_test),
                                 meta_est.predict(X_test))
-
-
-if __name__ == "__main__":
-    import nose
-    nose.runmodule()
diff --git a/sklearn/tree/export.py b/sklearn/tree/export.py
index 7637ef1f7e1b..2d38498e9af3 100644
--- a/sklearn/tree/export.py
+++ b/sklearn/tree/export.py
@@ -37,7 +37,7 @@ def _color_brew(n):
     c = s * v
     m = v - c
 
-    for h in np.arange(25, 385, 360./n).astype(int):
+    for h in np.arange(25, 385, 360. / n).astype(int):
         # Calculate some intermediate values
         h_bar = h / 60.
         x = c * (1 - abs((h_bar % 2) - 1))
@@ -76,6 +76,8 @@ def export_graphviz(decision_tree, out_file="tree.dot", max_depth=None,
     The sample counts that are shown are weighted with any sample_weights that
     might be present.
 
+    Read more in the :ref:`User Guide <tree>`.
+
     Parameters
     ----------
     decision_tree : decision tree classifier
@@ -304,11 +306,11 @@ def recurse(tree, node_id, criterion, parent=None, depth=0):
                     if tree.n_outputs != 1:
                         # Find max and min impurities for multi-output
                         colors['bounds'] = (np.min(-tree.impurity),
-                                             np.max(-tree.impurity))
+                                            np.max(-tree.impurity))
                     elif tree.n_classes[0] == 1:
                         # Find max and min values in leaf nodes for regression
                         colors['bounds'] = (np.min(tree.value),
-                                             np.max(tree.value))
+                                            np.max(tree.value))
                 if tree.n_outputs == 1:
                     node_val = (tree.value[node_id][0, :] /
                                 tree.weighted_n_node_samples[node_id])
diff --git a/sklearn/tree/tests/test_export.py b/sklearn/tree/tests/test_export.py
index 3b4731ec3cb7..3b39eb324f20 100644
--- a/sklearn/tree/tests/test_export.py
+++ b/sklearn/tree/tests/test_export.py
@@ -223,8 +223,3 @@ def test_graphviz_errors():
     # Check class_names error
     out = StringIO()
     assert_raises(IndexError, export_graphviz, clf, out, class_names=[])
-
-
-if __name__ == "__main__":
-    import nose
-    nose.runmodule()
diff --git a/sklearn/tree/tests/test_tree.py b/sklearn/tree/tests/test_tree.py
index 72d9f16b3fe6..9b6a014eba58 100644
--- a/sklearn/tree/tests/test_tree.py
+++ b/sklearn/tree/tests/test_tree.py
@@ -12,7 +12,6 @@
 from scipy.sparse import coo_matrix
 
 from sklearn.random_projection import sparse_random_matrix
-from sklearn.utils.random import sample_without_replacement
 
 from sklearn.metrics import accuracy_score
 from sklearn.metrics import mean_squared_error
@@ -859,10 +858,10 @@ def check_class_weights(name):
     """Check class_weights resemble sample_weights behavior."""
     TreeClassifier = CLF_TREES[name]
 
-    # Iris is balanced, so no effect expected for using 'auto' weights
+    # Iris is balanced, so no effect expected for using 'balanced' weights
     clf1 = TreeClassifier(random_state=0)
     clf1.fit(iris.data, iris.target)
-    clf2 = TreeClassifier(class_weight='auto', random_state=0)
+    clf2 = TreeClassifier(class_weight='balanced', random_state=0)
     clf2.fit(iris.data, iris.target)
     assert_almost_equal(clf1.feature_importances_, clf2.feature_importances_)
 
@@ -876,7 +875,7 @@ def check_class_weights(name):
     clf3.fit(iris.data, iris_multi)
     assert_almost_equal(clf2.feature_importances_, clf3.feature_importances_)
     # Check against multi-output "auto" which should also have no effect
-    clf4 = TreeClassifier(class_weight='auto', random_state=0)
+    clf4 = TreeClassifier(class_weight='balanced', random_state=0)
     clf4.fit(iris.data, iris_multi)
     assert_almost_equal(clf3.feature_importances_, clf4.feature_importances_)
 
@@ -892,7 +891,7 @@ def check_class_weights(name):
 
     # Check that sample_weight and class_weight are multiplicative
     clf1 = TreeClassifier(random_state=0)
-    clf1.fit(iris.data, iris.target, sample_weight**2)
+    clf1.fit(iris.data, iris.target, sample_weight ** 2)
     clf2 = TreeClassifier(class_weight=class_weight, random_state=0)
     clf2.fit(iris.data, iris.target, sample_weight)
     assert_almost_equal(clf1.feature_importances_, clf2.feature_importances_)
diff --git a/sklearn/tree/tree.py b/sklearn/tree/tree.py
index c4fbe7af3f4e..76ae550696e2 100644
--- a/sklearn/tree/tree.py
+++ b/sklearn/tree/tree.py
@@ -26,7 +26,7 @@
 from ..externals import six
 from ..feature_selection.from_model import _LearntSelectorMixin
 from ..utils import check_array, check_random_state, compute_sample_weight
-from ..utils.validation import NotFittedError, check_is_fitted
+from ..utils.validation import NotFittedError
 
 
 from ._tree import Criterion
@@ -433,6 +433,8 @@ def feature_importances_(self):
 class DecisionTreeClassifier(BaseDecisionTree, ClassifierMixin):
     """A decision tree classifier.
 
+    Read more in the :ref:`User Guide <tree>`.
+
     Parameters
     ----------
     criterion : string, optional (default="gini")
@@ -481,14 +483,16 @@ class DecisionTreeClassifier(BaseDecisionTree, ClassifierMixin):
         If None then unlimited number of leaf nodes.
         If not None then ``max_depth`` will be ignored.
 
-    class_weight : dict, list of dicts, "auto" or None, optional (default=None)
+    class_weight : dict, list of dicts, "balanced" or None, optional
+                   (default=None)
         Weights associated with classes in the form ``{class_label: weight}``.
         If not given, all classes are supposed to have weight one. For
         multi-output problems, a list of dicts can be provided in the same
         order as the columns of y.
 
-        The "auto" mode uses the values of y to automatically adjust
-        weights inversely proportional to class frequencies in the input data.
+        The "balanced" mode uses the values of y to automatically adjust
+        weights inversely proportional to class frequencies in the input data
+        as ``n_samples / (n_classes * np.bincount(y))``
 
         For multi-output, the weights of each column of y will be multiplied.
 
@@ -663,6 +667,8 @@ def predict_log_proba(self, X):
 class DecisionTreeRegressor(BaseDecisionTree, RegressorMixin):
     """A decision tree regressor.
 
+    Read more in the :ref:`User Guide <tree>`.
+
     Parameters
     ----------
     criterion : string, optional (default="mse")
@@ -803,6 +809,8 @@ class ExtraTreeClassifier(DecisionTreeClassifier):
 
     Warning: Extra-trees should only be used within ensemble methods.
 
+    Read more in the :ref:`User Guide <tree>`.
+
     See also
     --------
     ExtraTreeRegressor, ExtraTreesClassifier, ExtraTreesRegressor
@@ -849,6 +857,8 @@ class ExtraTreeRegressor(DecisionTreeRegressor):
 
     Warning: Extra-trees should only be used within ensemble methods.
 
+    Read more in the :ref:`User Guide <tree>`.
+
     See also
     --------
     ExtraTreeClassifier, ExtraTreesClassifier, ExtraTreesRegressor
diff --git a/sklearn/utils/class_weight.py b/sklearn/utils/class_weight.py
index 88b07eb1dec0..901a6feea0e8 100644
--- a/sklearn/utils/class_weight.py
+++ b/sklearn/utils/class_weight.py
@@ -2,6 +2,7 @@
 #          Manoj Kumar
 # License: BSD 3 clause
 
+import warnings
 import numpy as np
 from ..externals import six
 from ..utils.fixes import in1d
@@ -14,9 +15,9 @@ def compute_class_weight(class_weight, classes, y):
 
     Parameters
     ----------
-    class_weight : dict, 'auto' or None
-        If 'auto', class weights will be given inverse proportional
-        to the frequency of the class in the data.
+    class_weight : dict, 'balanced' or None
+        If 'balanced', class weights will be given by
+        ``n_samples / (n_classes * np.bincount(y))``.
         If a dictionary is given, keys are classes and values
         are corresponding class weights.
         If None is given, the class weights will be uniform.
@@ -32,6 +33,11 @@ def compute_class_weight(class_weight, classes, y):
     -------
     class_weight_vect : ndarray, shape (n_classes,)
         Array with class_weight_vect[i] the weight for i-th class
+
+    References
+    ----------
+    The "balanced" heuristic is inspired by
+    Logistic Regression in Rare Events Data, King, Zen, 2001.
     """
     # Import error caused by circular imports.
     from ..preprocessing import LabelEncoder
@@ -39,7 +45,7 @@ def compute_class_weight(class_weight, classes, y):
     if class_weight is None or len(class_weight) == 0:
         # uniform class weights
         weight = np.ones(classes.shape[0], dtype=np.float64, order='C')
-    elif class_weight == 'auto':
+    elif class_weight in ['auto', 'balanced']:
         # Find the weight of each class as present in y.
         le = LabelEncoder()
         y_ind = le.fit_transform(y)
@@ -47,8 +53,16 @@ def compute_class_weight(class_weight, classes, y):
             raise ValueError("classes should have valid labels that are in y")
 
         # inversely proportional to the number of samples in the class
-        recip_freq = 1. / bincount(y_ind)
-        weight = recip_freq[le.transform(classes)] / np.mean(recip_freq)
+        if class_weight == 'auto':
+            recip_freq = 1. / bincount(y_ind)
+            weight = recip_freq[le.transform(classes)] / np.mean(recip_freq)
+            warnings.warn("The class_weight='auto' heuristic is deprecated in"
+                          " favor of a new heuristic class_weight='balanced'."
+                          " 'auto' will be removed in 0.18", DeprecationWarning)
+        else:
+            recip_freq = len(y) / (len(le.classes_) *
+                                   bincount(y_ind).astype(np.float64))
+            weight = recip_freq[le.transform(classes)]
     else:
         # user-defined dictionary
         weight = np.ones(classes.shape[0], dtype=np.float64, order='C')
@@ -70,14 +84,15 @@ def compute_sample_weight(class_weight, y, indices=None):
 
     Parameters
     ----------
-    class_weight : dict, list of dicts, "auto", or None, optional
+    class_weight : dict, list of dicts, "balanced", or None, optional
         Weights associated with classes in the form ``{class_label: weight}``.
         If not given, all classes are supposed to have weight one. For
         multi-output problems, a list of dicts can be provided in the same
         order as the columns of y.
 
-        The "auto" mode uses the values of y to automatically adjust
-        weights inversely proportional to class frequencies in the input data.
+        The "balanced" mode uses the values of y to automatically adjust
+        weights inversely proportional to class frequencies in the input data:
+        ``n_samples / (n_classes * np.bincount(y))``.
 
         For multi-output, the weights of each column of y will be multiplied.
 
@@ -103,13 +118,13 @@ def compute_sample_weight(class_weight, y, indices=None):
     n_outputs = y.shape[1]
 
     if isinstance(class_weight, six.string_types):
-        if class_weight != 'auto':
+        if class_weight not in ['balanced', 'auto']:
             raise ValueError('The only valid preset for class_weight is '
-                             '"auto". Given "%s".' % class_weight)
+                             '"balanced". Given "%s".' % class_weight)
     elif (indices is not None and
           not isinstance(class_weight, six.string_types)):
         raise ValueError('The only valid class_weight for subsampling is '
-                         '"auto". Given "%s".' % class_weight)
+                         '"balanced". Given "%s".' % class_weight)
     elif n_outputs > 1:
         if (not hasattr(class_weight, "__iter__") or
                 isinstance(class_weight, dict)):
@@ -126,7 +141,7 @@ def compute_sample_weight(class_weight, y, indices=None):
         classes_full = np.unique(y_full)
         classes_missing = None
 
-        if class_weight == 'auto' or n_outputs == 1:
+        if class_weight in ['balanced', 'auto'] or n_outputs == 1:
             class_weight_k = class_weight
         else:
             class_weight_k = class_weight[k]
diff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py
index 470b776f9cae..8ea5f32804c9 100644
--- a/sklearn/utils/estimator_checks.py
+++ b/sklearn/utils/estimator_checks.py
@@ -878,6 +878,7 @@ def check_classifiers_classes(name, Classifier):
         if name == 'BernoulliNB':
             classifier.set_params(binarize=X.mean())
         set_fast_parameters(classifier)
+        set_random_state(classifier)
         # fit
         classifier.fit(X, y_)
 
@@ -1066,8 +1067,8 @@ def check_class_weight_classifiers(name, Classifier):
         assert_greater(np.mean(y_pred == 0), 0.89)
 
 
-def check_class_weight_auto_classifiers(name, Classifier, X_train, y_train,
-                                        X_test, y_test, weights):
+def check_class_weight_balanced_classifiers(name, Classifier, X_train, y_train,
+                                            X_test, y_test, weights):
     with warnings.catch_warnings(record=True):
         classifier = Classifier()
     if hasattr(classifier, "n_iter"):
@@ -1077,18 +1078,18 @@ def check_class_weight_auto_classifiers(name, Classifier, X_train, y_train,
     classifier.fit(X_train, y_train)
     y_pred = classifier.predict(X_test)
 
-    classifier.set_params(class_weight='auto')
+    classifier.set_params(class_weight='balanced')
     classifier.fit(X_train, y_train)
-    y_pred_auto = classifier.predict(X_test)
-    assert_greater(f1_score(y_test, y_pred_auto, average='weighted'),
+    y_pred_balanced = classifier.predict(X_test)
+    assert_greater(f1_score(y_test, y_pred_balanced, average='weighted'),
                    f1_score(y_test, y_pred, average='weighted'))
 
 
-def check_class_weight_auto_linear_classifier(name, Classifier):
+def check_class_weight_balanced_linear_classifier(name, Classifier):
     """Test class weights with non-contiguous class labels."""
     X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],
                   [1.0, 1.0], [1.0, 0.0]])
-    y = [1, 1, 1, -1, -1]
+    y = np.array([1, 1, 1, -1, -1])
 
     with warnings.catch_warnings(record=True):
         classifier = Classifier()
@@ -1099,19 +1100,19 @@ def check_class_weight_auto_linear_classifier(name, Classifier):
     set_random_state(classifier)
 
     # Let the model compute the class frequencies
-    classifier.set_params(class_weight='auto')
-    coef_auto = classifier.fit(X, y).coef_.copy()
+    classifier.set_params(class_weight='balanced')
+    coef_balanced = classifier.fit(X, y).coef_.copy()
 
     # Count each label occurrence to reweight manually
-    mean_weight = (1. / 3 + 1. / 2) / 2
-    class_weight = {
-        1: 1. / 3 / mean_weight,
-        -1: 1. / 2 / mean_weight,
-    }
+    n_samples = len(y)
+    n_classes = float(len(np.unique(y)))
+
+    class_weight = {1: n_samples / (np.sum(y == 1) * n_classes),
+                    -1: n_samples / (np.sum(y == -1) * n_classes)}
     classifier.set_params(class_weight=class_weight)
     coef_manual = classifier.fit(X, y).coef_.copy()
 
-    assert_array_almost_equal(coef_auto, coef_manual)
+    assert_array_almost_equal(coef_balanced, coef_manual)
 
 
 def check_estimators_overwrite_params(name, Estimator):
diff --git a/sklearn/utils/tests/test_class_weight.py b/sklearn/utils/tests/test_class_weight.py
index daf3cfbefb83..2d42695cef2c 100644
--- a/sklearn/utils/tests/test_class_weight.py
+++ b/sklearn/utils/tests/test_class_weight.py
@@ -1,5 +1,8 @@
 import numpy as np
 
+from sklearn.linear_model import LogisticRegression
+from sklearn.datasets import make_blobs
+
 from sklearn.utils.class_weight import compute_class_weight
 from sklearn.utils.class_weight import compute_sample_weight
 
@@ -8,22 +11,59 @@
 from sklearn.utils.testing import assert_raises
 from sklearn.utils.testing import assert_true
 from sklearn.utils.testing import assert_equal
+from sklearn.utils.testing import assert_warns
 
 
 def test_compute_class_weight():
     # Test (and demo) compute_class_weight.
     y = np.asarray([2, 2, 2, 3, 3, 4])
     classes = np.unique(y)
-    cw = compute_class_weight("auto", classes, y)
+    cw = assert_warns(DeprecationWarning,
+                      compute_class_weight, "auto", classes, y)
     assert_almost_equal(cw.sum(), classes.shape)
     assert_true(cw[0] < cw[1] < cw[2])
 
+    cw = compute_class_weight("balanced", classes, y)
+    # total effect of samples is preserved
+    class_counts = np.bincount(y)[2:]
+    assert_almost_equal(np.dot(cw, class_counts), y.shape[0])
+    assert_true(cw[0] < cw[1] < cw[2])
+
 
 def test_compute_class_weight_not_present():
     # Raise error when y does not contain all class labels
     classes = np.arange(4)
     y = np.asarray([0, 0, 0, 1, 1, 2])
     assert_raises(ValueError, compute_class_weight, "auto", classes, y)
+    assert_raises(ValueError, compute_class_weight, "balanced", classes, y)
+
+
+def test_compute_class_weight_invariance():
+    # Test that results with class_weight="balanced" is invariant wrt
+    # class imbalance if the number of samples is identical.
+    # The test uses a balanced two class dataset with 100 datapoints.
+    # It creates three versions, one where class 1 is duplicated
+    # resulting in 150 points of class 1 and 50 of class 0,
+    # one where there are 50 points in class 1 and 150 in class 0,
+    # and one where there are 100 points of each class (this one is balanced
+    # again).
+    # With balancing class weights, all three should give the same model.
+    X, y = make_blobs(centers=2, random_state=0)
+    # create dataset where class 1 is duplicated twice
+    X_1 = np.vstack([X] + [X[y == 1]] * 2)
+    y_1 = np.hstack([y] + [y[y == 1]] * 2)
+    # create dataset where class 0 is duplicated twice
+    X_0 = np.vstack([X] + [X[y == 0]] * 2)
+    y_0 = np.hstack([y] + [y[y == 0]] * 2)
+    # cuplicate everything
+    X_ = np.vstack([X] * 2)
+    y_ = np.hstack([y] * 2)
+    # results should be identical
+    logreg1 = LogisticRegression(class_weight="balanced").fit(X_1, y_1)
+    logreg0 = LogisticRegression(class_weight="balanced").fit(X_0, y_0)
+    logreg = LogisticRegression(class_weight="balanced").fit(X_, y_)
+    assert_array_almost_equal(logreg1.coef_, logreg0.coef_)
+    assert_array_almost_equal(logreg.coef_, logreg0.coef_)
 
 
 def test_compute_class_weight_auto_negative():
@@ -31,34 +71,55 @@ def test_compute_class_weight_auto_negative():
     # Test with balanced class labels.
     classes = np.array([-2, -1, 0])
     y = np.asarray([-1, -1, 0, 0, -2, -2])
-    cw = compute_class_weight("auto", classes, y)
+    cw = assert_warns(DeprecationWarning, compute_class_weight, "auto",
+                      classes, y)
     assert_almost_equal(cw.sum(), classes.shape)
     assert_equal(len(cw), len(classes))
     assert_array_almost_equal(cw, np.array([1., 1., 1.]))
 
+    cw = compute_class_weight("balanced", classes, y)
+    assert_equal(len(cw), len(classes))
+    assert_array_almost_equal(cw, np.array([1., 1., 1.]))
+
     # Test with unbalanced class labels.
     y = np.asarray([-1, 0, 0, -2, -2, -2])
-    cw = compute_class_weight("auto", classes, y)
+    cw = assert_warns(DeprecationWarning, compute_class_weight, "auto",
+                      classes, y)
     assert_almost_equal(cw.sum(), classes.shape)
     assert_equal(len(cw), len(classes))
     assert_array_almost_equal(cw, np.array([0.545, 1.636, 0.818]), decimal=3)
 
+    cw = compute_class_weight("balanced", classes, y)
+    assert_equal(len(cw), len(classes))
+    class_counts = np.bincount(y + 2)
+    assert_almost_equal(np.dot(cw, class_counts), y.shape[0])
+    assert_array_almost_equal(cw, [2. / 3, 2., 1.])
+
 
 def test_compute_class_weight_auto_unordered():
     # Test compute_class_weight when classes are unordered
     classes = np.array([1, 0, 3])
     y = np.asarray([1, 0, 0, 3, 3, 3])
-    cw = compute_class_weight("auto", classes, y)
+    cw = assert_warns(DeprecationWarning, compute_class_weight, "auto",
+                      classes, y)
     assert_almost_equal(cw.sum(), classes.shape)
     assert_equal(len(cw), len(classes))
     assert_array_almost_equal(cw, np.array([1.636, 0.818, 0.545]), decimal=3)
 
+    cw = compute_class_weight("balanced", classes, y)
+    class_counts = np.bincount(y)[classes]
+    assert_almost_equal(np.dot(cw, class_counts), y.shape[0])
+    assert_array_almost_equal(cw, [2., 1., 2. / 3])
+
 
 def test_compute_sample_weight():
     # Test (and demo) compute_sample_weight.
     # Test with balanced classes
     y = np.asarray([1, 1, 1, 2, 2, 2])
-    sample_weight = compute_sample_weight("auto", y)
+    sample_weight = assert_warns(DeprecationWarning,
+                                 compute_sample_weight, "auto", y)
+    assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1.])
+    sample_weight = compute_sample_weight("balanced", y)
     assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1.])
 
     # Test with user-defined weights
@@ -67,14 +128,21 @@ def test_compute_sample_weight():
 
     # Test with column vector of balanced classes
     y = np.asarray([[1], [1], [1], [2], [2], [2]])
-    sample_weight = compute_sample_weight("auto", y)
+    sample_weight = assert_warns(DeprecationWarning,
+                                 compute_sample_weight, "auto", y)
+    assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1.])
+    sample_weight = compute_sample_weight("balanced", y)
     assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1.])
 
     # Test with unbalanced classes
     y = np.asarray([1, 1, 1, 2, 2, 2, 3])
-    sample_weight = compute_sample_weight("auto", y)
-    expected = np.asarray([.6, .6, .6, .6, .6, .6, 1.8])
-    assert_array_almost_equal(sample_weight, expected)
+    sample_weight = assert_warns(DeprecationWarning,
+                                 compute_sample_weight, "auto", y)
+    expected_auto = np.asarray([.6, .6, .6, .6, .6, .6, 1.8])
+    assert_array_almost_equal(sample_weight, expected_auto)
+    sample_weight = compute_sample_weight("balanced", y)
+    expected_balanced = np.array([0.7777, 0.7777, 0.7777, 0.7777, 0.7777, 0.7777, 2.3333])
+    assert_array_almost_equal(sample_weight, expected_balanced, decimal=4)
 
     # Test with `None` weights
     sample_weight = compute_sample_weight(None, y)
@@ -82,7 +150,10 @@ def test_compute_sample_weight():
 
     # Test with multi-output of balanced classes
     y = np.asarray([[1, 0], [1, 0], [1, 0], [2, 1], [2, 1], [2, 1]])
-    sample_weight = compute_sample_weight("auto", y)
+    sample_weight = assert_warns(DeprecationWarning,
+                                 compute_sample_weight, "auto", y)
+    assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1.])
+    sample_weight = compute_sample_weight("balanced", y)
     assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1.])
 
     # Test with multi-output with user-defined weights
@@ -92,46 +163,72 @@ def test_compute_sample_weight():
 
     # Test with multi-output of unbalanced classes
     y = np.asarray([[1, 0], [1, 0], [1, 0], [2, 1], [2, 1], [2, 1], [3, -1]])
-    sample_weight = compute_sample_weight("auto", y)
-    assert_array_almost_equal(sample_weight, expected ** 2)
+    sample_weight = assert_warns(DeprecationWarning,
+                                 compute_sample_weight, "auto", y)
+    assert_array_almost_equal(sample_weight, expected_auto ** 2)
+    sample_weight = compute_sample_weight("balanced", y)
+    assert_array_almost_equal(sample_weight, expected_balanced ** 2, decimal=3)
 
 
 def test_compute_sample_weight_with_subsample():
     # Test compute_sample_weight with subsamples specified.
     # Test with balanced classes and all samples present
     y = np.asarray([1, 1, 1, 2, 2, 2])
-    sample_weight = compute_sample_weight("auto", y, range(6))
+    sample_weight = assert_warns(DeprecationWarning,
+                                 compute_sample_weight, "auto", y)
+    assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1.])
+    sample_weight = compute_sample_weight("balanced", y, range(6))
     assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1.])
 
     # Test with column vector of balanced classes and all samples present
     y = np.asarray([[1], [1], [1], [2], [2], [2]])
-    sample_weight = compute_sample_weight("auto", y, range(6))
+    sample_weight = assert_warns(DeprecationWarning,
+                                 compute_sample_weight, "auto", y)
+    assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1.])
+    sample_weight = compute_sample_weight("balanced", y, range(6))
     assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1.])
 
     # Test with a subsample
     y = np.asarray([1, 1, 1, 2, 2, 2])
-    sample_weight = compute_sample_weight("auto", y, range(4))
+    sample_weight = assert_warns(DeprecationWarning,
+                                 compute_sample_weight, "auto", y, range(4))
     assert_array_almost_equal(sample_weight, [.5, .5, .5, 1.5, 1.5, 1.5])
+    sample_weight = compute_sample_weight("balanced", y, range(4))
+    assert_array_almost_equal(sample_weight, [2. / 3, 2. / 3,
+                                              2. / 3, 2., 2., 2.])
 
     # Test with a bootstrap subsample
     y = np.asarray([1, 1, 1, 2, 2, 2])
-    sample_weight = compute_sample_weight("auto", y, [0, 1, 1, 2, 2, 3])
-    expected = np.asarray([1/3., 1/3., 1/3., 5/3., 5/3., 5/3.])
-    assert_array_almost_equal(sample_weight, expected)
+    sample_weight = assert_warns(DeprecationWarning, compute_sample_weight,
+                                 "auto", y, [0, 1, 1, 2, 2, 3])
+    expected_auto = np.asarray([1 / 3., 1 / 3., 1 / 3., 5 / 3., 5 / 3., 5 / 3.])
+    assert_array_almost_equal(sample_weight, expected_auto)
+    sample_weight = compute_sample_weight("balanced", y, [0, 1, 1, 2, 2, 3])
+    expected_balanced = np.asarray([0.6, 0.6, 0.6, 3., 3., 3.])
+    assert_array_almost_equal(sample_weight, expected_balanced)
 
     # Test with a bootstrap subsample for multi-output
     y = np.asarray([[1, 0], [1, 0], [1, 0], [2, 1], [2, 1], [2, 1]])
-    sample_weight = compute_sample_weight("auto", y, [0, 1, 1, 2, 2, 3])
-    assert_array_almost_equal(sample_weight, expected ** 2)
+    sample_weight = assert_warns(DeprecationWarning, compute_sample_weight,
+                                 "auto", y, [0, 1, 1, 2, 2, 3])
+    assert_array_almost_equal(sample_weight, expected_auto ** 2)
+    sample_weight = compute_sample_weight("balanced", y, [0, 1, 1, 2, 2, 3])
+    assert_array_almost_equal(sample_weight, expected_balanced ** 2)
 
     # Test with a missing class
     y = np.asarray([1, 1, 1, 2, 2, 2, 3])
-    sample_weight = compute_sample_weight("auto", y, range(6))
+    sample_weight = assert_warns(DeprecationWarning, compute_sample_weight,
+                                 "auto", y, range(6))
+    assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1., 0.])
+    sample_weight = compute_sample_weight("balanced", y, range(6))
     assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1., 0.])
 
     # Test with a missing class for multi-output
     y = np.asarray([[1, 0], [1, 0], [1, 0], [2, 1], [2, 1], [2, 1], [2, 2]])
-    sample_weight = compute_sample_weight("auto", y, range(6))
+    sample_weight = assert_warns(DeprecationWarning, compute_sample_weight,
+                                 "auto", y, range(6))
+    assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1., 0.])
+    sample_weight = compute_sample_weight("balanced", y, range(6))
     assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1., 0.])
 
 
diff --git a/sklearn/utils/tests/test_extmath.py b/sklearn/utils/tests/test_extmath.py
index 7c84548b4dce..b3f91b29568f 100644
--- a/sklearn/utils/tests/test_extmath.py
+++ b/sklearn/utils/tests/test_extmath.py
@@ -465,8 +465,3 @@ def test_vector_sign_flip():
     assert_array_equal(max_abs_rows, max_rows)
     signs = np.sign(data[range(data.shape[0]), max_abs_rows])
     assert_array_equal(data, data_flipped * signs[:, np.newaxis])
-
-
-if __name__ == '__main__':
-    import nose
-    nose.runmodule()
diff --git a/sklearn/utils/tests/test_multiclass.py b/sklearn/utils/tests/test_multiclass.py
index efd0d9ab7e92..989b98a878c8 100644
--- a/sklearn/utils/tests/test_multiclass.py
+++ b/sklearn/utils/tests/test_multiclass.py
@@ -392,8 +392,3 @@ def test_class_distribution():
         assert_array_almost_equal(classes_sp[k], classes_expected[k])
         assert_array_almost_equal(n_classes_sp[k], n_classes_expected[k])
         assert_array_almost_equal(class_prior_sp[k], class_prior_expected[k])
-
-
-if __name__ == "__main__":
-    import nose
-    nose.runmodule()
diff --git a/sklearn/utils/tests/test_random.py b/sklearn/utils/tests/test_random.py
index 7538117d5390..0f1ae0aaa524 100644
--- a/sklearn/utils/tests/test_random.py
+++ b/sklearn/utils/tests/test_random.py
@@ -180,8 +180,3 @@ def test_random_choice_csc_errors():
     class_probabilites = [np.array([0.5, 0.6]), np.array([0.6, 0.1, 0.3])]
     assert_raises(ValueError, random_choice_csc, 4, classes,
                   class_probabilites, 1)
-
-
-if __name__ == '__main__':
-    import nose
-    nose.runmodule()
diff --git a/sklearn/utils/tests/test_shortest_path.py b/sklearn/utils/tests/test_shortest_path.py
index 71f2d711dd3f..058fa2586401 100644
--- a/sklearn/utils/tests/test_shortest_path.py
+++ b/sklearn/utils/tests/test_shortest_path.py
@@ -93,8 +93,3 @@ def test_dijkstra_bug_fix():
     dist_FW = graph_shortest_path(X, directed=False, method='FW')
     dist_D = graph_shortest_path(X, directed=False, method='D')
     assert_array_almost_equal(dist_D, dist_FW)
-
-
-if __name__ == '__main__':
-    import nose
-    nose.runmodule()
