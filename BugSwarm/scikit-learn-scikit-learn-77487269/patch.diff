diff --git a/doc/whats_new.rst b/doc/whats_new.rst
index ff975e8577..3cf978da77 100755
--- a/doc/whats_new.rst
+++ b/doc/whats_new.rst
@@ -128,7 +128,7 @@ Bug fixes
     - Fixed bug in :class:`linear_model.LogisticRegressionCV` where `penalty` was ignored
       in the final fit. By `Manoj Kumar`_.
 
-    - Fixed bug in :class:`ensemble.forest.ForestClassifier` while computing 
+    - Fixed bug in :class:`ensemble.forest.ForestClassifier` while computing
       oob_score and X is a sparse.csc_matrix. By `Ankur Ankan`_.
 
     - All regressors now consistently handle and warn when given ``y`` that is of
@@ -137,9 +137,12 @@ Bug fixes
     - Fix in :class:`cluster.KMeans` cluster reassignment for sparse input by
       `Lars Buitinck`_.
 
-    - Fixed a bug in :class:`lda.LDA` that could cause asymmetric covariance 
+    - Fixed a bug in :class:`lda.LDA` that could cause asymmetric covariance
       matrices when using shrinkage. By `Martin Billinger`_.
 
+    - Fixed :func:`cross_validation.cross_val_predict` for estimators with
+      sparse predictions. By Buddha Prakash.
+
 API changes summary
 -------------------
 
diff --git a/examples/cross_decomposition/plot_compare_cross_decomposition.py b/examples/cross_decomposition/plot_compare_cross_decomposition.py
index f582be9e00..d563a18aa1 100755
--- a/examples/cross_decomposition/plot_compare_cross_decomposition.py
+++ b/examples/cross_decomposition/plot_compare_cross_decomposition.py
@@ -121,9 +121,9 @@
 pls2.fit(X, Y)
 print("True B (such that: Y = XB + Err)")
 print(B)
-# compare pls2.coefs with B
+# compare pls2.coef_ with B
 print("Estimated B")
-print(np.round(pls2.coefs, 1))
+print(np.round(pls2.coef_, 1))
 pls2.predict(X)
 
 ###############################################################################
@@ -137,7 +137,7 @@
 pls1.fit(X, y)
 # note that the number of compements exceeds 1 (the dimension of y)
 print("Estimated betas")
-print(np.round(pls1.coefs, 1))
+print(np.round(pls1.coef_, 1))
 
 ###############################################################################
 # CCA (PLS mode B with symmetric deflation)
diff --git a/sklearn/cluster/dbscan_.py b/sklearn/cluster/dbscan_.py
index 1d8f7b52dd..1a63817daf 100755
--- a/sklearn/cluster/dbscan_.py
+++ b/sklearn/cluster/dbscan_.py
@@ -94,6 +94,11 @@ def dbscan(X, eps=0.5, min_samples=5, metric='minkowski',
     the memory complexity to O(n.d) where d is the average number of neighbors,
     while original DBSCAN had memory complexity O(n).
 
+    Sparse neighborhoods can be precomputed using
+    :func:`NearestNeighbors.radius_neighbors_graph
+    <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>`
+    with ``mode='distance'``.
+
     References
     ----------
     Ester, M., H. P. Kriegel, J. Sander, and X. Xu, "A Density-Based
@@ -121,22 +126,29 @@ def dbscan(X, eps=0.5, min_samples=5, metric='minkowski',
         D = pairwise_distances(X, metric=metric)
         neighborhoods = np.empty(X.shape[0], dtype=object)
         if sparse.issparse(D):
-            D = D <= eps
-            neighborhoods[:] = np.split(D.indices.astype(np.intp),
-                                        D.indptr[1:-1])
+            D_mask = D.data <= eps
+            masked_indices = D.indices.astype(np.intp)[D_mask]
+            masked_indptr = np.cumsum(D_mask)[D.indptr[1:] - 1]
+            # insert the diagonal
+            masked_indices = np.insert(masked_indices, masked_indptr,
+                                       np.arange(D.shape[0]))
+            masked_indptr = masked_indptr[:-1] + np.arange(1, D.shape[0])
+            # split into rows
+            neighborhoods[:] = np.split(masked_indices, masked_indptr)
         else:
-            neighborhoods[:] = [np.where(x <= eps)[0] for x in D]
+            neighborhoods[:] = [np.where(x <= eps)[0] for i, x in enumerate(D)]
     else:
         neighbors_model = NearestNeighbors(radius=eps, algorithm=algorithm,
                                            leaf_size=leaf_size,
                                            metric=metric, p=p)
         neighbors_model.fit(X)
         # This has worst case O(n^2) memory complexity
-        neighborhoods = neighbors_model.radius_neighbors(X, eps,
+        neighborhoods = neighbors_model.radius_neighbors(X,
                                                          return_distance=False)
 
     if sample_weight is None:
-        n_neighbors = np.array([len(neighbors) for neighbors in neighborhoods])
+        n_neighbors = np.array([len(neighbors)
+                                for neighbors in neighborhoods])
     else:
         n_neighbors = np.array([np.sum(sample_weight[neighbors])
                                 for neighbors in neighborhoods])
@@ -208,6 +220,11 @@ class DBSCAN(BaseEstimator, ClusterMixin):
     the memory complexity to O(n.d) where d is the average number of neighbors,
     while original DBSCAN had memory complexity O(n).
 
+    Sparse neighborhoods can be precomputed using
+    :func:`NearestNeighbors.radius_neighbors_graph
+    <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>`
+    with ``mode='distance'``.
+
     References
     ----------
     Ester, M., H. P. Kriegel, J. Sander, and X. Xu, "A Density-Based
diff --git a/sklearn/cluster/tests/test_dbscan.py b/sklearn/cluster/tests/test_dbscan.py
index 0e1f2afc08..23b08d8be6 100755
--- a/sklearn/cluster/tests/test_dbscan.py
+++ b/sklearn/cluster/tests/test_dbscan.py
@@ -14,6 +14,7 @@
 from sklearn.utils.testing import assert_raises
 from sklearn.utils.testing import assert_in
 from sklearn.utils.testing import assert_not_in
+from sklearn.neighbors import NearestNeighbors
 from sklearn.cluster.dbscan_ import DBSCAN
 from sklearn.cluster.dbscan_ import dbscan
 from sklearn.cluster.tests.common import generate_clustered_data
@@ -80,11 +81,16 @@ def test_dbscan_sparse():
 
 def test_dbscan_sparse_precomputed():
     D = pairwise_distances(X)
-    core_sparse, labels_sparse = dbscan(sparse.lil_matrix(D),
+    nn = NearestNeighbors(radius=.9).fit(X)
+    D_sparse = nn.radius_neighbors_graph(mode='distance')
+    # Ensure it is sparse not merely on diagonals:
+    assert D_sparse.nnz < D.shape[0] * (D.shape[0] - 1)
+    core_sparse, labels_sparse = dbscan(D_sparse,
                                         eps=.8,
                                         min_samples=10,
                                         metric='precomputed')
-    core_dense, labels_dense = dbscan(X, eps=.8, min_samples=10)
+    core_dense, labels_dense = dbscan(D, eps=.8, min_samples=10,
+                                      metric='precomputed')
     assert_array_equal(core_dense, core_sparse)
     assert_array_equal(labels_dense, labels_sparse)
 
diff --git a/sklearn/cross_validation.py b/sklearn/cross_validation.py
index fa7c7f210b..5b7f2d5ad7 100755
--- a/sklearn/cross_validation.py
+++ b/sklearn/cross_validation.py
@@ -1042,13 +1042,20 @@ def cross_val_predict(estimator, X, y=None, cv=None, n_jobs=1,
                                                       train, test, verbose,
                                                       fit_params)
                             for train, test in cv)
-    p = np.concatenate([p for p, _ in preds_blocks])
+
+    preds = [p for p, _ in preds_blocks]
     locs = np.concatenate([loc for _, loc in preds_blocks])
     if not _check_is_partition(locs, _num_samples(X)):
         raise ValueError('cross_val_predict only works for partitions')
-    preds = p.copy()
-    preds[locs] = p
-    return preds
+    inv_locs = np.empty(len(locs), dtype=int)
+    inv_locs[locs] = np.arange(len(locs))
+
+    # Check for sparse predictions
+    if sp.issparse(preds[0]):
+        preds = sp.vstack(preds, format=preds[0].format)
+    else:
+        preds = np.concatenate(preds)
+    return preds[inv_locs]
 
 
 def _fit_and_predict(estimator, X, y, train, test, verbose, fit_params):
diff --git a/sklearn/tests/test_cross_validation.py b/sklearn/tests/test_cross_validation.py
index b33e2b4c27..f8d7165c3d 100755
--- a/sklearn/tests/test_cross_validation.py
+++ b/sklearn/tests/test_cross_validation.py
@@ -4,6 +4,7 @@
 
 import numpy as np
 from scipy.sparse import coo_matrix
+from scipy.sparse import csr_matrix
 from scipy import stats
 
 from sklearn.utils.testing import assert_true
@@ -25,14 +26,15 @@
 from sklearn.datasets import load_boston
 from sklearn.datasets import load_digits
 from sklearn.datasets import load_iris
+from sklearn.datasets import make_multilabel_classification
 from sklearn.metrics import explained_variance_score
 from sklearn.metrics import make_scorer
 from sklearn.metrics import precision_score
-
 from sklearn.externals import six
 from sklearn.externals.six.moves import zip
 
 from sklearn.linear_model import Ridge
+from sklearn.multiclass import OneVsRestClassifier
 from sklearn.neighbors import KNeighborsClassifier
 from sklearn.svm import SVC
 from sklearn.cluster import KMeans
@@ -1094,3 +1096,18 @@ def test_check_is_partition():
 
     p[0] = 23
     assert_false(cval._check_is_partition(p, 100))
+
+
+def test_cross_val_predict_sparse_prediction():
+    # check that cross_val_predict gives same result for sparse and dense input
+    X, y = make_multilabel_classification(n_classes=2, n_labels=1,
+                                          allow_unlabeled=False,
+                                          return_indicator=True,
+                                          random_state=1)
+    X_sparse = csr_matrix(X)
+    y_sparse = csr_matrix(y)
+    classif = OneVsRestClassifier(SVC(kernel='linear'))
+    preds = cval.cross_val_predict(classif, X, y, cv=10)
+    preds_sparse = cval.cross_val_predict(classif, X_sparse, y_sparse, cv=10)
+    preds_sparse = preds_sparse.toarray()
+    assert_array_almost_equal(preds_sparse, preds)
