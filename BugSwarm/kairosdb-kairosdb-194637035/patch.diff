diff --git a/build.groovy b/build.groovy
index 33f26f444d..b72b6fef33 100755
--- a/build.groovy
+++ b/build.groovy
@@ -168,6 +168,7 @@ ivyTestResolve = ivy.getResolveRule("test")
 testCompileRule.addDepend(ivyTestResolve)
 testCompileRule.getDefinition().set("unchecked")
 testCompileRule.getDefinition().set("deprecation")
+//testCompileRule.getDefinition().set("verbose")
 
 new SimpleRule("compile-test").addDepend(testCompileRule)
 
@@ -465,20 +466,25 @@ integrationClassPath = new Classpath(jp.getLibraryJars())
 //.addPaths(new RegExFileSet("lib/ivy/integration", ".*\\.jar").getFullFilePaths())
 		.addPath("src/integration-test/resources")
 
+ivyIntegrationRule = ivy.getResolveRule("integration")
+
 integrationBuildRule = new JavaCRule("build/integration")
 		.addSourceDir("src/integration-test/java")
 		.addClasspath(integrationClassPath)
-		.addDepend(ivy.getResolveRule("integration"))
+		.addDepend(ivyIntegrationRule)
 
 new SimpleRule("integration")
 		.setMakeAction("doIntegration")
 		.addDepend(integrationBuildRule)
+		.addDepend(ivyIntegrationRule)
 
 def doIntegration(Rule rule)
 {
+	integrationClassPath.addPaths(ivyIntegrationRule.getClasspath())
+	integrationClassPath.addPath("build/integration")
 	host = saw.getProperty("host", "127.0.0.1")
 	port = saw.getProperty("port", "8080")
-	saw.exec("java  -Dhost=${host} -Dport=${port} -cp ${integrationBuildRule.classpath} org.testng.TestNG src/integration-test/testng.xml")
+	saw.exec("java  -Dhost=${host} -Dport=${port} -cp ${integrationClassPath} org.testng.TestNG src/integration-test/testng.xml")
 }
 
 //------------------------------------------------------------------------------
diff --git a/definitions.xml b/definitions.xml
index e089b34fe4..b42aaa1c0f 100755
--- a/definitions.xml
+++ b/definitions.xml
@@ -2,6 +2,41 @@
 
 <definitions xmlns="http://www.cpmake.org">
 
+	<definition name="sun_javac" command="javac">
+
+		<mode>debug</mode>
+		<mode>release</mode>
+
+		<property name="compiler">Sun Java</property>
+
+		<option name="deprecation">-deprecation</option>
+		<option name="unchecked">-Xlint:unchecked</option>
+		<option name="fallthrough">-Xlint:fallthrough</option>
+		<option name="path" >-Xlint:path</option>
+		<option name="serial" >-Xlint:serial</option>
+		<option name="finally">-Xlint:finally</option>
+		<option name="lintall">-Xlint:all</option>
+		<option name="verbose">-verbose</option>
+
+		<option name="source" pattern="(.+)">-source $1</option>
+		<option name="target" pattern="(.+)">-target $1</option>
+
+		<option name="classpath"
+		        pattern="(.+)">-classpath $1</option>
+
+		<option name="class_dir" pattern="(.+)">-d $1</option>
+
+		<option name="sourcepath" pattern="(.+)">-sourcepath $1</option>
+
+		<option name="encoding" pattern="(.+)">-encoding $1</option>
+
+		<option mode="debug">-g</option>
+		<option mode="release">-g:none</option>
+
+		<option name="sourcefile" pattern="(.+)">$1</option>
+
+	</definition>
+
 	<definition name="genormous" command="java">
 		<!-- <command name="genorm">java</command>
 		<command name="genquery">java</command> -->
diff --git a/ivy.xml b/ivy.xml
index 4299e41c6f..efd334afa1 100755
--- a/ivy.xml
+++ b/ivy.xml
@@ -15,7 +15,7 @@
 
 		<dependency org="org.json" name="org.json" rev="chargebee-1.0" />
 		<dependency org="com.google.code.gson" name="gson" rev="2.2.4" />
-		<dependency org="junit" name="junit" rev="4.11" conf="test->default" />
+		<dependency org="junit" name="junit" rev="4.12" conf="test->default" />
 
 		<dependency org="org.hamcrest" name="hamcrest-library" rev="1.3"
 		            conf="test->default"/>
@@ -47,7 +47,11 @@
 		<dependency org="ch.qos.logback" name="logback-classic" rev="1.1.7" />
 
 		<!--Genormous dependencies-->
-		<dependency org="org.agileclick.genorm" name="genormous" rev="1.6.0.jdbc4"/>
+		<dependency org="org.agileclick.genorm" name="genormous" rev="1.6.4.jdbc41">
+			<exclude org="javax.servlet" name="servlet-api"/>
+			<exclude org="org.agileclick.slickxml" name="slickxml"/>
+			<exclude org="org.agileclick.ultramc" name="ultramc"/>
+		</dependency>
 		<dependency org="jaxen" name="jaxen" rev="1.1.6" transitive="false">
 			<artifact name="jaxen" type="jar"/>
 		</dependency>
@@ -56,14 +60,14 @@
 		<dependency org="com.datastax.cassandra" name="cassandra-driver-core" rev="3.1.2" />
 		<dependency org="net.jpountz.lz4" name="lz4" rev="1.3.0"/>
 
-		<dependency org="org.hectorclient" name="hector-core" rev="1.1-4">
+		<!--<dependency org="org.hectorclient" name="hector-core" rev="1.1-4">
 			<artifact name="hector-core" type="bundle" ext="jar"/>
 			<exclude org="javax.servlet" module="servlet-api"/>
-		</dependency>
+		</dependency>-->
 
-		<dependency org="com.netflix.astyanax" name="astyanax-core" rev="3.9.0" />
+		<!--<dependency org="com.netflix.astyanax" name="astyanax-core" rev="3.9.0" />
 		<dependency org="com.netflix.astyanax" name="astyanax-thrift" rev="3.9.0" />
-		<dependency org="com.netflix.astyanax" name="astyanax-cassandra" rev="3.9.0" />
+		<dependency org="com.netflix.astyanax" name="astyanax-cassandra" rev="3.9.0" />-->
 
 		<!-- Jetty server stuff -->
 		<dependency org="org.eclipse.jetty" name="jetty-server"
diff --git a/src/integration-test/resources/tests/test_aggregators/query.json b/src/integration-test/resources/tests/test_aggregators/query.json
index 05656d15a4..ebd33e1260 100755
--- a/src/integration-test/resources/tests/test_aggregators/query.json
+++ b/src/integration-test/resources/tests/test_aggregators/query.json
@@ -81,6 +81,16 @@
 				}
 			]
 		},
+		{
+			"name": "<metric_name>",
+			"order": "desc",
+			"aggregators":[
+				{
+					"name":"rate",
+					"unit": "milliseconds"
+				}
+			]
+		},
 		{
 			"name": "<metric_name>",
 			"aggregators":[
diff --git a/src/integration-test/resources/tests/test_aggregators/response.json b/src/integration-test/resources/tests/test_aggregators/response.json
index d2e9f2489d..ae8373aeb5 100755
--- a/src/integration-test/resources/tests/test_aggregators/response.json
+++ b/src/integration-test/resources/tests/test_aggregators/response.json
@@ -100,8 +100,29 @@
 					[1375202962776, 0.5],
 					[1375202962778, 0.5],
 					[1375202962780, 0.5],
-					[1375202962782, 0.5],
-					[1375202962783, 0]
+					[1375202962782, 0.5]
+				]
+			}
+		]
+	},
+	{
+		// Rate descending
+		"results": [
+			{
+				"name": "<metric_name>",
+				"tags": {
+					"host": ["test"]
+				},
+				"values": [
+					[1375202962780, 0.5],
+					[1375202962778, 0.5],
+					[1375202962776, 0.5],
+					[1375202962774, 0.5],
+					[1375202962772, 0.5],
+					[1375202962770, 0.5],
+					[1375202962768, 0.5],
+					[1375202962766, 0.5],
+					[1375202962764, 0.5]
 				]
 			}
 		]
diff --git a/src/main/conf/tables.xml b/src/main/conf/tables.xml
index d506e0090e..5a6de7fb38 100755
--- a/src/main/conf/tables.xml
+++ b/src/main/conf/tables.xml
@@ -1,4 +1,4 @@
-<!DOCTYPE tables SYSTEM "http://genormous.googlecode.com/svn/trunk/genormous/doc/tables_1_4.dtd">
+<!DOCTYPE tables SYSTEM "https://raw.githubusercontent.com/brianhks/genormous/master/doc/tables_1_4.dtd">
 
 <tables>
 	<configuration>
@@ -31,6 +31,44 @@
 		<col name="type" type="string"/>
 	</table>
 
+	<table name="service_index">
+		<property key="hsqldb_tableType" value="CACHED"/>
+		<col name="service" type="string" primary_key="true"/>
+		<col name="service_key" type="string" primary_key="true"/>
+		<col name="key" type="string" primary_key="true"/>
+		<col name="value" type="string"/>
+
+		<table_query name="keys" result_type="multi">
+			<input>
+				<param name="service" type="string" test="foo"/>
+				<param name="service_key" type="string" test="foo"/>
+			</input>
+			<sql>
+				from service_index this
+				where
+				this."service" = ?
+				and this."service_key" = ?
+				order by this."key" asc
+			</sql>
+		</table_query>
+
+		<table_query name="keys_like" result_type="multi">
+			<input>
+				<param name="service" type="string" test="foo"/>
+				<param name="service_key" type="string" test="foo"/>
+				<param name="key_prefix" type="string" test="key%"/>
+			</input>
+			<sql>
+				from service_index this
+				where
+				this."service" = ?
+				AND this."service_key" = ?
+				AND this."key" LIKE ?
+				ORDER BY this."key" asc
+			</sql>
+		</table_query>
+	</table>
+
 	<table name="tag">
 		<property key="hsqldb_tableType" value="CACHED"/>
 		<col name="name" type="string" primary_key="true"/>
diff --git a/src/main/java/org/kairosdb/core/CoreModule.java b/src/main/java/org/kairosdb/core/CoreModule.java
index 553fc4b93d..9664f4d757 100755
--- a/src/main/java/org/kairosdb/core/CoreModule.java
+++ b/src/main/java/org/kairosdb/core/CoreModule.java
@@ -40,14 +40,14 @@
 import org.kairosdb.core.queue.QueueProcessor;
 import org.kairosdb.core.scheduler.KairosDBScheduler;
 import org.kairosdb.core.scheduler.KairosDBSchedulerImpl;
-import org.kairosdb.util.CongestionExecutorService;
+import org.kairosdb.util.AdaptiveExecutorService;
 import org.kairosdb.util.MemoryMonitor;
+import org.kairosdb.util.SimpleStatsReporter;
 import org.kairosdb.util.Util;
 import se.ugli.bigqueue.BigArray;
 
 import javax.inject.Named;
 import javax.inject.Singleton;
-import java.util.List;
 import java.util.MissingResourceException;
 import java.util.Properties;
 import java.util.concurrent.Executor;
@@ -124,6 +124,7 @@ public void afterInjection(I i)
 		bind(KairosDBSchedulerImpl.class).in(Singleton.class);
 		bind(MemoryMonitor.class).in(Singleton.class);
 		bind(DataPointEventSerializer.class).in(Singleton.class);
+		bind(SimpleStatsReporter.class);
 
 		bind(SumAggregator.class);
 		bind(MinAggregator.class);
@@ -184,7 +185,7 @@ public void afterInjection(I i)
 
 		bind(KairosDataPointFactory.class).to(GuiceKairosDataPointFactory.class).in(Singleton.class);
 
-		bind(CongestionExecutorService.class);
+		bind(AdaptiveExecutorService.class);
 
 		String hostIp = m_props.getProperty("kairosdb.host_ip");
 		bindConstant().annotatedWith(Names.named("HOST_IP")).to(hostIp != null ? hostIp: InetAddresses.toAddrString(Util.findPublicIp()));
diff --git a/src/main/java/org/kairosdb/core/datastore/CachedSearchResult.java b/src/main/java/org/kairosdb/core/datastore/CachedSearchResult.java
index db8d66b2ae..85a8c45953 100755
--- a/src/main/java/org/kairosdb/core/datastore/CachedSearchResult.java
+++ b/src/main/java/org/kairosdb/core/datastore/CachedSearchResult.java
@@ -29,7 +29,7 @@
 import java.util.*;
 import java.util.concurrent.atomic.AtomicInteger;
 
-public class CachedSearchResult implements QueryCallback
+public class CachedSearchResult implements SearchResult
 {
 	public static final Logger logger = LoggerFactory.getLogger(CachedSearchResult.class);
 
@@ -250,6 +250,7 @@ protected void decrementClose()
 	 */
 	public void startDataPointSet(String type, Map<String, String> tags) throws IOException
 	{
+		//todo: need a lock around this, cql returns results overlapping.
 		if (m_randomAccessFile == null)
 			openCacheFile();
 
@@ -270,6 +271,7 @@ public void addDataPoint(DataPoint datapoint) throws IOException
 		m_currentFilePositionMarker.incrementDataPointCount();
 	}
 
+	@Override
 	public List<DataPointRow> getRows()
 	{
 		List<DataPointRow> ret = new ArrayList<DataPointRow>();
diff --git a/src/main/java/org/kairosdb/core/datastore/Datastore.java b/src/main/java/org/kairosdb/core/datastore/Datastore.java
index 4d51ffa530..386342c725 100755
--- a/src/main/java/org/kairosdb/core/datastore/Datastore.java
+++ b/src/main/java/org/kairosdb/core/datastore/Datastore.java
@@ -27,4 +27,13 @@
 	public void deleteDataPoints(DatastoreMetricQuery deleteQuery) throws DatastoreException;
 
 	public TagSet queryMetricTags(DatastoreMetricQuery query) throws DatastoreException;
+
+	void setValue(String service, String serviceKey, String key, String value) throws DatastoreException;
+
+	String getValue(String service, String serviceKey, String key) throws DatastoreException;
+
+	Iterable<String> listKeys(String service, String serviceKey) throws DatastoreException;
+
+	Iterable<String> listKeys(String service, String serviceKey, String keyStartsWith) throws DatastoreException;
+
 }
diff --git a/src/main/java/org/kairosdb/core/datastore/KairosDatastore.java b/src/main/java/org/kairosdb/core/datastore/KairosDatastore.java
index 42f215e96f..b6be033d9c 100755
--- a/src/main/java/org/kairosdb/core/datastore/KairosDatastore.java
+++ b/src/main/java/org/kairosdb/core/datastore/KairosDatastore.java
@@ -451,7 +451,7 @@ public int getSampleSize()
 		{
 			long queryStartTime = System.currentTimeMillis();
 			
-			CachedSearchResult cachedResults = null;
+			SearchResult searchResult = null;
 
 			List<DataPointRow> returnedRows = null;
 
@@ -459,28 +459,33 @@ public int getSampleSize()
 			{
 				String tempFile = m_cacheDir + m_cacheFilename;
 
+				/*searchResult = new MemorySearchResult(m_metric.getName());
+				m_datastore.queryDatabase(m_metric, searchResult);
+				returnedRows = searchResult.getRows();*/
+
 				if (m_metric.getCacheTime() > 0)
 				{
-					cachedResults = CachedSearchResult.openCachedSearchResult(m_metric.getName(),
+					searchResult = CachedSearchResult.openCachedSearchResult(m_metric.getName(),
 							tempFile, m_metric.getCacheTime(), m_dataPointFactory, m_keepCacheFiles);
-					if (cachedResults != null)
+					if (searchResult != null)
 					{
-						returnedRows = cachedResults.getRows();
+						returnedRows = searchResult.getRows();
 						logger.debug("Cache HIT!");
 					}
 				}
 
-				if (cachedResults == null)
+				if (searchResult == null)
 				{
 					logger.debug("Cache MISS!");
-					cachedResults = CachedSearchResult.createCachedSearchResult(m_metric.getName(),
+					searchResult = CachedSearchResult.createCachedSearchResult(m_metric.getName(),
 							tempFile, m_dataPointFactory, m_keepCacheFiles);
-					m_datastore.queryDatabase(m_metric, cachedResults);
-					returnedRows = cachedResults.getRows();
+					m_datastore.queryDatabase(m_metric, searchResult);
+					returnedRows = searchResult.getRows();
 				}
 			}
 			catch (Exception e)
 			{
+				logger.error("Query Error", e);
 				throw new DatastoreException(e);
 			}
 
diff --git a/src/main/java/org/kairosdb/core/datastore/MemorySearchResult.java b/src/main/java/org/kairosdb/core/datastore/MemorySearchResult.java
new file mode 100755
index 0000000000..e3f4408991
--- /dev/null
+++ b/src/main/java/org/kairosdb/core/datastore/MemorySearchResult.java
@@ -0,0 +1,127 @@
+package org.kairosdb.core.datastore;
+
+import org.kairosdb.core.DataPoint;
+import org.kairosdb.util.SimpleStats;
+
+import java.io.IOException;
+import java.util.*;
+
+/**
+ Created by bhawkins on 1/28/17.
+ */
+public class MemorySearchResult implements SearchResult
+{
+	private final String m_metricName;
+	private final List<DataPointRow> m_dataPointRows;
+	private MemoryDataPointRow m_currentRow;
+
+	public MemorySearchResult(String metricName)
+	{
+		m_metricName = metricName;
+		m_dataPointRows = new ArrayList<>();
+	}
+
+	@Override
+	public List<DataPointRow> getRows()
+	{
+		return m_dataPointRows;
+	}
+
+	@Override
+	public void addDataPoint(DataPoint datapoint) throws IOException
+	{
+		m_currentRow.addDataPoint(datapoint);
+	}
+
+	@Override
+	public void startDataPointSet(String dataType, Map<String, String> tags) throws IOException
+	{
+		m_currentRow = new MemoryDataPointRow(dataType, tags);
+		m_dataPointRows.add(m_currentRow);
+	}
+
+	@Override
+	public void endDataPoints() throws IOException
+	{
+		if (m_currentRow != null)
+			m_currentRow.endRow();
+	}
+
+	private class MemoryDataPointRow implements DataPointRow
+	{
+		private final String m_dataType;
+		private final Map<String, String> m_tags;
+		private final List<DataPoint> m_dataPoints;
+		private Iterator<DataPoint> m_dataPointIterator;
+
+		private MemoryDataPointRow(String dataType, Map<String, String> tags)
+		{
+			m_dataType = dataType;
+			m_tags = tags;
+			m_dataPoints = new ArrayList<>();
+		}
+
+		@Override
+		public String getName()
+		{
+			return m_metricName;
+		}
+
+		@Override
+		public String getDatastoreType()
+		{
+			return m_dataType;
+		}
+
+		@Override
+		public Set<String> getTagNames()
+		{
+			return m_tags.keySet();
+		}
+
+		@Override
+		public String getTagValue(String tag)
+		{
+			return m_tags.get(tag);
+		}
+
+		@Override
+		public void close()
+		{
+		}
+
+		public void endRow()
+		{
+			m_dataPointIterator = m_dataPoints.iterator();
+		}
+
+		@Override
+		public int getDataPointCount()
+		{
+			return m_dataPoints.size();
+		}
+
+		public void addDataPoint(DataPoint dp)
+		{
+			m_dataPoints.add(dp);
+		}
+
+		@Override
+		public boolean hasNext()
+		{
+			return m_dataPointIterator.hasNext();
+		}
+
+		@Override
+		public DataPoint next()
+		{
+			return m_dataPointIterator.next();
+		}
+
+		@Override
+		public void remove()
+		{
+
+		}
+	}
+}
diff --git a/src/main/java/org/kairosdb/core/datastore/SearchResult.java b/src/main/java/org/kairosdb/core/datastore/SearchResult.java
new file mode 100755
index 0000000000..caafdd7d0a
--- /dev/null
+++ b/src/main/java/org/kairosdb/core/datastore/SearchResult.java
@@ -0,0 +1,11 @@
+package org.kairosdb.core.datastore;
+
+import java.util.List;
+
+/**
+ Created by bhawkins on 1/28/17.
+ */
+public interface SearchResult extends QueryCallback
+{
+	List<DataPointRow> getRows();
+}
diff --git a/src/main/java/org/kairosdb/core/http/rest/MetricsResource.java b/src/main/java/org/kairosdb/core/http/rest/MetricsResource.java
index 91a0d311f6..6d7d4e2112 100755
--- a/src/main/java/org/kairosdb/core/http/rest/MetricsResource.java
+++ b/src/main/java/org/kairosdb/core/http/rest/MetricsResource.java
@@ -24,7 +24,6 @@
 import com.google.gson.stream.MalformedJsonException;
 import com.google.inject.Inject;
 import com.google.inject.name.Named;
-import org.apache.commons.math3.stat.descriptive.DescriptiveStatistics;
 import org.kairosdb.core.DataPointSet;
 import org.kairosdb.core.KairosDataPointFactory;
 import org.kairosdb.core.datapoints.*;
@@ -40,6 +39,8 @@
 import org.kairosdb.core.reporting.KairosMetricReporter;
 import org.kairosdb.core.reporting.ThreadReporter;
 import org.kairosdb.util.MemoryMonitorException;
+import org.kairosdb.util.SimpleStats;
+import org.kairosdb.util.SimpleStatsReporter;
 import org.kairosdb.util.StatsMap;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -121,6 +122,9 @@
 	@Named("HOSTNAME")
 	private String hostName = "localhost";
 
+	@Inject
+	private SimpleStatsReporter m_simpleStatsReporter = new SimpleStatsReporter();
+
 	@Inject
 	public MetricsResource(KairosDatastore datastore, QueryParser queryParser,
 			KairosDataPointFactory dataPointFactory, EventBus eventBus)
@@ -711,38 +715,14 @@ private Response executeNameQuery(NameType type)
 			ret.add(dpsTime);
 		}
 
-		Map<String, DescriptiveStatistics> statsMap = m_statsMap.getStatsMap();
+		Map<String, SimpleStats> statsMap = m_statsMap.getStatsMap();
 
-		for (Map.Entry<String, DescriptiveStatistics> entry : statsMap.entrySet())
+		for (Map.Entry<String, SimpleStats> entry : statsMap.entrySet())
 		{
 			String metric = entry.getKey();
-			DescriptiveStatistics stats = entry.getValue();
-
-			DataPointSet min = new DataPointSet(new StringBuilder(metric).append(".").append("min").toString());
-			min.addTag("host", hostName);
-			min.addDataPoint(m_doubleDataPointFactory.createDataPoint(now, stats.getMin()));
-			ret.add(min);
-
-			DataPointSet max = new DataPointSet(new StringBuilder(metric).append(".").append("max").toString());
-			max.addTag("host", hostName);
-			max.addDataPoint(m_doubleDataPointFactory.createDataPoint(now, stats.getMax()));
-			ret.add(max);
-
-			DataPointSet avg = new DataPointSet(new StringBuilder(metric).append(".").append("avg").toString());
-			avg.addTag("host", hostName);
-			avg.addDataPoint(m_doubleDataPointFactory.createDataPoint(now, stats.getMean()));
-			ret.add(avg);
-
-			DataPointSet statCnt = new DataPointSet(new StringBuilder(metric).append(".").append("count").toString());
-			statCnt.addTag("host", hostName);
-			statCnt.addDataPoint(m_doubleDataPointFactory.createDataPoint(now, stats.getN()));
-			ret.add(statCnt);
-
-			DataPointSet sum = new DataPointSet(new StringBuilder(metric).append(".").append("sum").toString());
-			sum.addTag("host", hostName);
-			sum.addDataPoint(m_doubleDataPointFactory.createDataPoint(now, stats.getSum()));
-			ret.add(sum);
+			SimpleStats.Data stats = entry.getValue().getAndClear();
 
+			m_simpleStatsReporter.reportStats(stats, now, metric, ret);
 		}
 
 		return ret;
diff --git a/src/main/java/org/kairosdb/core/queue/FileQueueProcessor.java b/src/main/java/org/kairosdb/core/queue/FileQueueProcessor.java
index 9afef8bffe..066175d7ff 100755
--- a/src/main/java/org/kairosdb/core/queue/FileQueueProcessor.java
+++ b/src/main/java/org/kairosdb/core/queue/FileQueueProcessor.java
@@ -171,12 +171,9 @@ public void put(DataPointEvent dataPointEvent)
 						}
 					}
 
-					if (event != null)
-					{
-						returnIndex = m_nextIndex;
-						m_nextIndex = incrementIndex(m_nextIndex);
-						ret.add(event.m_dataPointEvent);
-					}
+					returnIndex = m_nextIndex;
+					m_nextIndex = incrementIndex(m_nextIndex);
+					ret.add(event.m_dataPointEvent);
 				}
 				else
 					break; //exhausted queue
@@ -184,7 +181,7 @@ public void put(DataPointEvent dataPointEvent)
 			}
 		}
 
-		System.out.println(ret.size());
+		//System.out.println(ret.size());
 		m_readFromQueueCount.getAndAdd(ret.size());
 
 		m_lastCallback.increment();
@@ -199,7 +196,7 @@ protected EventCompletionCallBack getCompletionCallBack()
 
 		if (m_stopwatch.elapsed(TimeUnit.SECONDS) > m_secondsTillCheckpoint)
 		{
-			System.out.println("Checkpoint");
+			//System.out.println("Checkpoint");
 			callbackToReturn.finalize();
 			m_lastCallback = new CompletionCallBack();
 			callbackToReturn.setChildCallBack(m_lastCallback);
@@ -215,13 +212,12 @@ protected EventCompletionCallBack getCompletionCallBack()
 	@Override
 	public List<DataPointSet> getMetrics(long now)
 	{
-		System.out.println("Called");
 		//todo make member variable
 		ImmutableSortedMap<String, String> tag = ImmutableSortedMap.of("host", m_hostName);
 		long arraySize = m_bigArray.getHeadIndex() - m_nextIndex;
 		long readFromFile = m_readFromFileCount.getAndSet(0);
 		long readFromQueue = m_readFromQueueCount.getAndSet(0);
-		System.out.println(readFromQueue);
+		//System.out.println(readFromQueue);
 
 		synchronized (m_lock)
 		{
@@ -314,7 +310,6 @@ public void complete()
 		{
 			if (m_counter.decrementAndGet() == 0 && m_finalized)
 			{
-				System.out.println("Setting index");
 				m_childCallBack.complete();
 				//Checkpoint big queue
 				m_bigArray.removeBeforeIndex(m_completionIndex);
diff --git a/src/main/java/org/kairosdb/core/queue/ProcessorHandler.java b/src/main/java/org/kairosdb/core/queue/ProcessorHandler.java
index 05556df695..12386358c3 100755
--- a/src/main/java/org/kairosdb/core/queue/ProcessorHandler.java
+++ b/src/main/java/org/kairosdb/core/queue/ProcessorHandler.java
@@ -9,5 +9,6 @@
  */
 public interface ProcessorHandler
 {
-	void handleEvents(List<DataPointEvent> events, EventCompletionCallBack eventCompletionCallBack);
+	void handleEvents(List<DataPointEvent> events, EventCompletionCallBack eventCompletionCallBack,
+			boolean fullBatch);
 }
diff --git a/src/main/java/org/kairosdb/core/queue/QueueProcessor.java b/src/main/java/org/kairosdb/core/queue/QueueProcessor.java
index 9fad8e03e4..de4053c1ff 100755
--- a/src/main/java/org/kairosdb/core/queue/QueueProcessor.java
+++ b/src/main/java/org/kairosdb/core/queue/QueueProcessor.java
@@ -73,7 +73,7 @@ public void shutdown()
 		}
 
 		/**
-		 Used for testing the queue processor to reset the running state
+		 Used for testing the queue processor to clear the running state
 		 @param running
 		 */
 		public void setRunning(boolean running)
@@ -115,8 +115,12 @@ public void run()
 					//getCompletionCallBack must be called after get()
 					EventCompletionCallBack callbackToPass = getCompletionCallBack();
 
-					//System.out.println(results.size());
-					m_processorHandler.handleEvents(results, callbackToPass);
+					boolean fullBatch = false;
+
+					if (results.size() == m_batchSize)
+						fullBatch = true;
+
+					m_processorHandler.handleEvents(results, callbackToPass, fullBatch);
 				}
 				catch (Exception e)
 				{
diff --git a/src/main/java/org/kairosdb/core/reporting/DataPointsMonitor.java b/src/main/java/org/kairosdb/core/reporting/DataPointsMonitor.java
index b17fa9c24b..78b2cbe359 100755
--- a/src/main/java/org/kairosdb/core/reporting/DataPointsMonitor.java
+++ b/src/main/java/org/kairosdb/core/reporting/DataPointsMonitor.java
@@ -94,8 +94,8 @@ public void dataPoint(DataPointEvent event)
 	{
 		String metricName = event.getMetricName();
 
-		if (metricName.startsWith("kairosdb"))
-			return; //Skip our own metrics.
+		//if (metricName.startsWith("kairosdb"))
+		//	return; //Skip our own metrics.
 
 		addCounter(metricName, 1);
 	}
diff --git a/src/main/java/org/kairosdb/datastore/cassandra/AstyanaxClient.java b/src/main/java/org/kairosdb/datastore/cassandra/AstyanaxClient.java
deleted file mode 100755
index c59f9e5d60..0000000000
--- a/src/main/java/org/kairosdb/datastore/cassandra/AstyanaxClient.java
+++ /dev/null
@@ -1,134 +0,0 @@
-package org.kairosdb.datastore.cassandra;
-
-import com.datastax.driver.core.PreparedStatement;
-import com.datastax.driver.core.Session;
-import com.google.common.eventbus.EventBus;
-import com.google.inject.name.Named;
-import com.netflix.astyanax.AstyanaxContext;
-import com.netflix.astyanax.Keyspace;
-import com.netflix.astyanax.MutationBatch;
-import com.netflix.astyanax.connectionpool.NodeDiscoveryType;
-import com.netflix.astyanax.connectionpool.exceptions.ConnectionException;
-import com.netflix.astyanax.connectionpool.impl.ConnectionPoolConfigurationImpl;
-import com.netflix.astyanax.connectionpool.impl.CountingConnectionPoolMonitor;
-import com.netflix.astyanax.impl.AstyanaxConfigurationImpl;
-import com.netflix.astyanax.model.ColumnFamily;
-import com.netflix.astyanax.serializers.BytesArraySerializer;
-import com.netflix.astyanax.serializers.IntegerSerializer;
-import com.netflix.astyanax.serializers.StringSerializer;
-import com.netflix.astyanax.thrift.ThriftFamilyFactory;
-import org.kairosdb.core.DataPoint;
-import org.kairosdb.core.queue.EventCompletionCallBack;
-import org.kairosdb.events.DataPointEvent;
-import org.kairosdb.util.KDataOutput;
-
-import javax.inject.Inject;
-import java.io.IOException;
-import java.util.List;
-
-import static org.kairosdb.datastore.cassandra.CassandraClientImpl.HOST_LIST_PROPERTY;
-import static org.kairosdb.datastore.cassandra.CassandraDatastore.*;
-
-/**
- Created by bhawkins on 12/12/16.
- */
-public class AstyanaxClient
-{
-	private ColumnFamily<DataPointsRowKey, Integer> CF_DATA_POINTS =
-			new ColumnFamily<>(CF_DATA_POINTS_NAME,
-					new AstyanaxDataPointsRowKeySerializer(), IntegerSerializer.get(), BytesArraySerializer.get());
-
-	private ColumnFamily<String, DataPointsRowKey> CF_ROW_KEY_INDEX =
-			new ColumnFamily<>(CF_ROW_KEY_INDEX_NAME,
-					StringSerializer.get(), new AstyanaxDataPointsRowKeySerializer());
-
-	private ColumnFamily<String, String> CF_STRING_INDEX =
-			new ColumnFamily<>(CF_STRING_INDEX_NAME,
-					StringSerializer.get(), StringSerializer.get());
-
-	private Keyspace m_keyspace;
-
-	@Inject
-	public AstyanaxClient(CassandraConfiguration cassandraConfiguration,
-			@Named(HOST_LIST_PROPERTY)String hostList)
-	{
-		AstyanaxContext<Keyspace> context = new AstyanaxContext.Builder()
-				.forCluster("ClusterName")
-				.forKeyspace(cassandraConfiguration.getKeyspaceName())
-				.withAstyanaxConfiguration(new AstyanaxConfigurationImpl()
-						.setDiscoveryType(NodeDiscoveryType.RING_DESCRIBE)
-				)
-				.withConnectionPoolConfiguration(new ConnectionPoolConfigurationImpl("MyConnectionPool")
-						.setPort(9160)
-						.setMaxConnsPerHost(10)
-						.setSeeds(hostList)
-				)
-				.withConnectionPoolMonitor(new CountingConnectionPoolMonitor())
-				.buildKeyspace(ThriftFamilyFactory.getInstance());
-
-		context.start();
-		m_keyspace = context.getClient();
-	}
-
-
-	public BatchHandler getBatchHandler(List<DataPointEvent> events, EventCompletionCallBack callBack,
-			int defaultTtl, DataCache<DataPointsRowKey>
-			rowKeyCache, DataCache<String> metricNameCache, EventBus eventBus,
-			Session session, PreparedStatement psInsertData,
-			PreparedStatement psInsertRowKey, PreparedStatement psInsertString)
-	{
-		return new AstyanaxBatchHandler(events, callBack, defaultTtl,
-				rowKeyCache, metricNameCache, eventBus, session, psInsertData,
-				psInsertRowKey, psInsertString);
-	}
-
-	private class AstyanaxBatchHandler extends CQLBatchHandler
-	{
-		MutationBatch m_batch;
-
-		public AstyanaxBatchHandler(List<DataPointEvent> events, EventCompletionCallBack callBack,
-				int defaultTtl, DataCache<DataPointsRowKey>
-				rowKeyCache, DataCache<String> metricNameCache, EventBus eventBus,
-				Session session, PreparedStatement psInsertData,
-				PreparedStatement psInsertRowKey, PreparedStatement psInsertString)
-		{
-			super(events, callBack, defaultTtl, rowKeyCache, metricNameCache, eventBus,
-					session, psInsertData, psInsertRowKey, psInsertString);
-
-			m_batch = m_keyspace.prepareMutationBatch();
-		}
-
-
-		@Override
-		public void addMetricName(String metricName)
-		{
-			m_batch.withRow(CF_STRING_INDEX, ROW_KEY_METRIC_NAMES)
-					.putColumn(metricName, (String)null);
-		}
-
-
-		@Override
-		public void addDataPoint(DataPointsRowKey rowKey, int columnTime, DataPoint dataPoint, int ttl) throws IOException
-		{
-			KDataOutput kDataOutput = new KDataOutput();
-			dataPoint.writeValueToBuffer(kDataOutput);
-
-			m_batch.withRow(CF_DATA_POINTS, rowKey)
-					.putColumn(columnTime, kDataOutput.getBytes(), ttl);
-		}
-
-		@Override
-		public void submitBatch()
-		{
-			try
-			{
-				super.submitBatch();
-				m_batch.execute();
-			}
-			catch (ConnectionException e)
-			{
-				e.printStackTrace();
-			}
-		}
-	}
-}
diff --git a/src/main/java/org/kairosdb/datastore/cassandra/AstyanaxDataPointsRowKeySerializer.java b/src/main/java/org/kairosdb/datastore/cassandra/AstyanaxDataPointsRowKeySerializer.java
deleted file mode 100755
index ad9e41a0df..0000000000
--- a/src/main/java/org/kairosdb/datastore/cassandra/AstyanaxDataPointsRowKeySerializer.java
+++ /dev/null
@@ -1,30 +0,0 @@
-package org.kairosdb.datastore.cassandra;
-
-import com.netflix.astyanax.serializers.AbstractSerializer;
-
-import java.nio.ByteBuffer;
-
-/**
- Created by bhawkins on 12/12/16.
- */
-public class AstyanaxDataPointsRowKeySerializer extends AbstractSerializer<DataPointsRowKey>
-{
-	private DataPointsRowKeySerializer m_serializer;
-
-	public AstyanaxDataPointsRowKeySerializer()
-	{
-		m_serializer = new DataPointsRowKeySerializer();
-	}
-
-	@Override
-	public ByteBuffer toByteBuffer(DataPointsRowKey obj)
-	{
-		return m_serializer.toByteBuffer(obj);
-	}
-
-	@Override
-	public DataPointsRowKey fromByteBuffer(ByteBuffer byteBuffer)
-	{
-		return m_serializer.fromByteBuffer(byteBuffer);
-	}
-}
diff --git a/src/main/java/org/kairosdb/datastore/cassandra/BatchHandler.java b/src/main/java/org/kairosdb/datastore/cassandra/BatchHandler.java
index 6451b82025..7b639614e8 100755
--- a/src/main/java/org/kairosdb/datastore/cassandra/BatchHandler.java
+++ b/src/main/java/org/kairosdb/datastore/cassandra/BatchHandler.java
@@ -30,10 +30,11 @@
 	private final DataCache<DataPointsRowKey> m_rowKeyCache;
 	private final DataCache<String> m_metricNameCache;
 	private final EventBus m_eventBus;
+	private final boolean m_fullBatch;
 
 	public BatchHandler(List<DataPointEvent> events, EventCompletionCallBack callBack,
 			int defaultTtl, DataCache<DataPointsRowKey> rowKeyCache,
-			DataCache<String> metricNameCache, EventBus eventBus)
+			DataCache<String> metricNameCache, EventBus eventBus, boolean fullBatch)
 	{
 		m_events = events;
 		m_callBack = callBack;
@@ -41,6 +42,7 @@ public BatchHandler(List<DataPointEvent> events, EventCompletionCallBack callBac
 		m_rowKeyCache = rowKeyCache;
 		m_metricNameCache = metricNameCache;
 		m_eventBus = eventBus;
+		m_fullBatch = fullBatch;
 	}
 
 	protected abstract void addRowKey(String metricName, DataPointsRowKey rowKey,
@@ -53,6 +55,11 @@ protected abstract void addDataPoint(DataPointsRowKey rowKey, int columnTime,
 
 	protected abstract void submitBatch();
 
+	public boolean isFullBatch()
+	{
+		return m_fullBatch;
+	}
+
 	@Override
 	public Long call() throws Exception
 	{
diff --git a/src/main/java/org/kairosdb/datastore/cassandra/BatchStats.java b/src/main/java/org/kairosdb/datastore/cassandra/BatchStats.java
new file mode 100755
index 0000000000..f376e777e0
--- /dev/null
+++ b/src/main/java/org/kairosdb/datastore/cassandra/BatchStats.java
@@ -0,0 +1,47 @@
+package org.kairosdb.datastore.cassandra;
+
+import org.kairosdb.util.SimpleStats;
+
+/**
+ Created by bhawkins on 1/26/17.
+ */
+public class BatchStats
+{
+	private final SimpleStats m_dataPointStats = new SimpleStats();
+	private final SimpleStats m_rowKeyStats = new SimpleStats();
+	private final SimpleStats m_nameStats = new SimpleStats();
+
+	public BatchStats()
+	{
+	}
+
+	public void addNameBatch(long count)
+	{
+		m_nameStats.addValue(count);
+	}
+
+	public void addRowKeyBatch(long count)
+	{
+		m_rowKeyStats.addValue(count);
+	}
+
+	public void addDatapointsBatch(long count)
+	{
+		m_dataPointStats.addValue(count);
+	}
+
+	public SimpleStats.Data getDataPointStats()
+	{
+		return m_dataPointStats.getAndClear();
+	}
+
+	public SimpleStats.Data getRowKeyStats()
+	{
+		return m_rowKeyStats.getAndClear();
+	}
+
+	public SimpleStats.Data getNameStats()
+	{
+		return m_nameStats.getAndClear();
+	}
+}
diff --git a/src/main/java/org/kairosdb/datastore/cassandra/CQLBatchHandler.java b/src/main/java/org/kairosdb/datastore/cassandra/CQLBatchHandler.java
index 53c7014f68..aeec65e0aa 100755
--- a/src/main/java/org/kairosdb/datastore/cassandra/CQLBatchHandler.java
+++ b/src/main/java/org/kairosdb/datastore/cassandra/CQLBatchHandler.java
@@ -1,9 +1,6 @@
 package org.kairosdb.datastore.cassandra;
 
-import com.datastax.driver.core.BatchStatement;
-import com.datastax.driver.core.BoundStatement;
-import com.datastax.driver.core.PreparedStatement;
-import com.datastax.driver.core.Session;
+import com.datastax.driver.core.*;
 import com.google.common.eventbus.EventBus;
 import org.kairosdb.core.DataPoint;
 import org.kairosdb.core.queue.EventCompletionCallBack;
@@ -13,6 +10,7 @@
 import java.io.IOException;
 import java.nio.ByteBuffer;
 import java.nio.charset.Charset;
+import java.util.Date;
 import java.util.List;
 
 import static org.kairosdb.datastore.cassandra.CassandraDatastore.*;
@@ -26,46 +24,68 @@
 	private static final Charset UTF_8 = Charset.forName("UTF-8");
 
 	private final Session m_session;
-	private final PreparedStatement m_psInsertData;
-	private final PreparedStatement m_psInsertRowKey;
-	private final PreparedStatement m_psInsertString;
+	private final CassandraDatastore.PreparedStatements m_preparedStatements;
+	private final BatchStats m_batchStats;
+	private final ConsistencyLevel m_consistencyLevel;
+	private final long m_now;
 
 	private BatchStatement metricNamesBatch = new BatchStatement(BatchStatement.Type.UNLOGGED);
 	private BatchStatement dataPointBatch = new BatchStatement(BatchStatement.Type.UNLOGGED);
 	private BatchStatement rowKeyBatch = new BatchStatement(BatchStatement.Type.UNLOGGED);
 
 	public CQLBatchHandler(List<DataPointEvent> events, EventCompletionCallBack callBack,
-			int defaultTtl, DataCache<DataPointsRowKey>
+			int defaultTtl, ConsistencyLevel consistencyLevel, DataCache<DataPointsRowKey>
 			rowKeyCache, DataCache<String> metricNameCache, EventBus eventBus,
-			Session session, PreparedStatement psInsertData,
-			PreparedStatement psInsertRowKey, PreparedStatement psInsertString)
+			Session session, CassandraDatastore.PreparedStatements preparedStatements,
+			boolean fullBatch, BatchStats batchStats)
 	{
-		super(events, callBack, defaultTtl, rowKeyCache, metricNameCache, eventBus);
-
+		super(events, callBack, defaultTtl, rowKeyCache, metricNameCache, eventBus, fullBatch);
 
+		m_consistencyLevel = consistencyLevel;
 		m_session = session;
-		m_psInsertData = psInsertData;
-		m_psInsertRowKey = psInsertRowKey;
-		m_psInsertString = psInsertString;
+		m_preparedStatements = preparedStatements;
+		m_batchStats = batchStats;
+		m_now = System.currentTimeMillis();
 	}
 
 	@Override
 	public void addRowKey(String metricName, DataPointsRowKey rowKey, int rowKeyTtl)
 	{
-		BoundStatement bs = new BoundStatement(m_psInsertRowKey);
-		bs.setBytesUnsafe(0, ByteBuffer.wrap(metricName.getBytes(UTF_8)));
-		bs.setBytesUnsafe(1, DATA_POINTS_ROW_KEY_SERIALIZER.toByteBuffer(rowKey));
-		bs.setInt(2, rowKeyTtl);
+		ByteBuffer bb = ByteBuffer.allocate(8);
+		bb.putLong(0, rowKey.getTimestamp());
+
+		BoundStatement bs = m_preparedStatements.psRowKeyTimeInsert.bind()
+				.setString(0, metricName)
+				.setTimestamp(1, new Date(rowKey.getTimestamp()))
+				//.setBytesUnsafe(1, bb) //Setting timestamp in a more optimal way
+				.setInt(2, rowKeyTtl)
+				.setLong(3, m_now);
+
+		bs.setConsistencyLevel(m_consistencyLevel);
+
+		rowKeyBatch.add(bs);
+
+		bs = m_preparedStatements.psRowKeyInsert.bind()
+				.setString(0, metricName)
+				.setTimestamp(1, new Date(rowKey.getTimestamp()))
+				//.setBytesUnsafe(1, bb)  //Setting timestamp in a more optimal way
+				.setString(2, rowKey.getDataType())
+				.setMap(3, rowKey.getTags())
+				.setInt(4, rowKeyTtl);
+				//.setLong(5, m_now);
+
+		bs.setConsistencyLevel(m_consistencyLevel);
+
 		rowKeyBatch.add(bs);
 	}
 
 	@Override
 	public void addMetricName(String metricName)
 	{
-		BoundStatement bs = new BoundStatement(m_psInsertString);
+		BoundStatement bs = new BoundStatement(m_preparedStatements.psStringIndexInsert);
 		bs.setBytesUnsafe(0, ByteBuffer.wrap(ROW_KEY_METRIC_NAMES.getBytes(UTF_8)));
 		bs.setBytesUnsafe(1, ByteBuffer.wrap(metricName.getBytes(UTF_8)));
-
+		bs.setConsistencyLevel(m_consistencyLevel);
 		metricNamesBatch.add(bs);
 	}
 
@@ -75,7 +95,7 @@ public void addDataPoint(DataPointsRowKey rowKey, int columnTime, DataPoint data
 		KDataOutput kDataOutput = new KDataOutput();
 		dataPoint.writeValueToBuffer(kDataOutput);
 
-		BoundStatement boundStatement = new BoundStatement(m_psInsertData);
+		BoundStatement boundStatement = new BoundStatement(m_preparedStatements.psDataPointsInsert);
 		boundStatement.setBytesUnsafe(0, DATA_POINTS_ROW_KEY_SERIALIZER.toByteBuffer(rowKey));
 		ByteBuffer b = ByteBuffer.allocate(4);
 		b.putInt(columnTime);
@@ -83,6 +103,8 @@ public void addDataPoint(DataPointsRowKey rowKey, int columnTime, DataPoint data
 		boundStatement.setBytesUnsafe(1, b);
 		boundStatement.setBytesUnsafe(2, ByteBuffer.wrap(kDataOutput.getBytes()));
 		boundStatement.setInt(3, ttl);
+		boundStatement.setLong(4, m_now);
+		boundStatement.setConsistencyLevel(m_consistencyLevel);
 
 		dataPointBatch.add(boundStatement);
 	}
@@ -91,12 +113,22 @@ public void addDataPoint(DataPointsRowKey rowKey, int columnTime, DataPoint data
 	public void submitBatch()
 	{
 		if (metricNamesBatch.size() != 0)
+		{
 			m_session.executeAsync(metricNamesBatch);
+			m_batchStats.addNameBatch(metricNamesBatch.size());
+		}
 
 		if (rowKeyBatch.size() != 0)
-			m_session.executeAsync(rowKeyBatch);
+		{
+			rowKeyBatch.enableTracing();
+			ResultSet resultSet = m_session.execute(rowKeyBatch);
+			m_batchStats.addRowKeyBatch(rowKeyBatch.size());
+		}
 
 		if (dataPointBatch.size() != 0)
+		{
 			m_session.execute(dataPointBatch);
+			m_batchStats.addDatapointsBatch(dataPointBatch.size());
+		}
 	}
 }
diff --git a/src/main/java/org/kairosdb/datastore/cassandra/CassandraClientImpl.java b/src/main/java/org/kairosdb/datastore/cassandra/CassandraClientImpl.java
index d2762748b8..d9eeb219ed 100755
--- a/src/main/java/org/kairosdb/datastore/cassandra/CassandraClientImpl.java
+++ b/src/main/java/org/kairosdb/datastore/cassandra/CassandraClientImpl.java
@@ -1,18 +1,26 @@
 package org.kairosdb.datastore.cassandra;
 
+import com.codahale.metrics.*;
 import com.datastax.driver.core.*;
 import com.datastax.driver.core.ConsistencyLevel;
 import com.datastax.driver.core.policies.DCAwareRoundRobinPolicy;
 import com.datastax.driver.core.policies.TokenAwarePolicy;
 import com.google.inject.Inject;
 import com.google.inject.name.Named;
+import org.kairosdb.core.DataPointSet;
+import org.kairosdb.core.datapoints.DoubleDataPointFactory;
+import org.kairosdb.core.datapoints.DoubleDataPointFactoryImpl;
+import org.kairosdb.core.datapoints.LongDataPointFactory;
+import org.kairosdb.core.datapoints.LongDataPointFactoryImpl;
+import org.kairosdb.core.reporting.KairosMetricReporter;
 
+import java.util.ArrayList;
 import java.util.List;
 
 /**
  Created by bhawkins on 3/4/15.
  */
-public class CassandraClientImpl implements CassandraClient
+public class CassandraClientImpl implements CassandraClient, KairosMetricReporter
 {
 	public static final String KEYSPACE_PROPERTY = "kairosdb.datastore.cassandra.keyspace";
 	public static final String HOST_LIST_PROPERTY = "kairosdb.datastore.cassandra.cql_host_list";
@@ -21,15 +29,36 @@
 	private final Cluster m_cluster;
 	private String m_keyspace;
 
+	@Inject
+	@Named("HOSTNAME")
+	private String m_hostName = "localhost";
+
+	@Inject
+	private LongDataPointFactory m_longDataPointFactory = new LongDataPointFactoryImpl();
+
+	@Inject
+	private DoubleDataPointFactory m_doubleDataPointFactory = new DoubleDataPointFactoryImpl();
+
 	@Inject
 	public CassandraClientImpl(@Named(KEYSPACE_PROPERTY)String keyspace,
 			@Named(HOST_LIST_PROPERTY)String hostList)
 	{
 		final Cluster.Builder builder = new Cluster.Builder()
-				.withPoolingOptions(new PoolingOptions().setConnectionsPerHost(HostDistance.LOCAL, 3, 100))
+				.withPoolingOptions(new PoolingOptions().setConnectionsPerHost(HostDistance.LOCAL, 3, 100)
+					.setMaxRequestsPerConnection(HostDistance.LOCAL, 1024))
 				.withLoadBalancingPolicy(new TokenAwarePolicy(DCAwareRoundRobinPolicy.builder().build()))
 				.withQueryOptions(new QueryOptions().setConsistencyLevel(ConsistencyLevel.QUORUM))
-				.withCompression(ProtocolOptions.Compression.LZ4);
+				.withCompression(ProtocolOptions.Compression.LZ4)
+				.withoutJMXReporting()
+				.withTimestampGenerator(new TimestampGenerator()
+				{
+					@Override
+					public long next()
+					{
+						return System.currentTimeMillis();
+					}
+				});
+
 
 		for (String node : hostList.split(","))
 		{
@@ -66,4 +95,71 @@ public void close()
 	}
 
 
+	private DataPointSet newDataPointSet(String metricPrefix, String metricSuffix,
+			long now, long value)
+	{
+		DataPointSet dps = new DataPointSet(new StringBuilder(metricPrefix).append(".").append(metricSuffix).toString());
+		dps.addTag("host", m_hostName);
+		dps.addDataPoint(m_longDataPointFactory.createDataPoint(now, value));
+
+		return dps;
+	}
+
+	private DataPointSet newDataPointSet(String metricPrefix, String metricSuffix,
+			long now, double value)
+	{
+		DataPointSet dps = new DataPointSet(new StringBuilder(metricPrefix).append(".").append(metricSuffix).toString());
+		dps.addTag("host", m_hostName);
+		dps.addDataPoint(m_doubleDataPointFactory.createDataPoint(now, value));
+
+		return dps;
+	}
+
+	@Override
+	public List<DataPointSet> getMetrics(long now)
+	{
+		String prefix = "kairosdb.datastore.cassandra.client";
+		List<DataPointSet> ret = new ArrayList<>();
+		Metrics metrics = m_cluster.getMetrics();
+
+		ret.add(newDataPointSet(prefix, "blocking_executor_queue_depth", now,
+				metrics.getBlockingExecutorQueueDepth().getValue()));
+
+		ret.add(newDataPointSet(prefix, "connected_to_hosts", now,
+				metrics.getConnectedToHosts().getValue()));
+
+		ret.add(newDataPointSet(prefix, "executor_queue_depth", now,
+				metrics.getExecutorQueueDepth().getValue()));
+
+		ret.add(newDataPointSet(prefix, "known_hosts", now,
+				metrics.getKnownHosts().getValue()));
+
+		ret.add(newDataPointSet(prefix, "open_connections", now,
+				metrics.getOpenConnections().getValue()));
+
+		ret.add(newDataPointSet(prefix, "reconnection_scheduler_queue_size", now,
+				metrics.getReconnectionSchedulerQueueSize().getValue()));
+
+		ret.add(newDataPointSet(prefix, "task_scheduler_queue_size", now,
+				metrics.getTaskSchedulerQueueSize().getValue()));
+
+		ret.add(newDataPointSet(prefix, "trashed_connections", now,
+				metrics.getTrashedConnections().getValue()));
+
+		Snapshot snapshot = metrics.getRequestsTimer().getSnapshot();
+		prefix = prefix + ".requests_timer";
+		ret.add(newDataPointSet(prefix, "max", now,
+				snapshot.getMax()));
+
+		ret.add(newDataPointSet(prefix, "min", now,
+				snapshot.getMin()));
+
+		ret.add(newDataPointSet(prefix, "avg", now,
+				snapshot.getMean()));
+
+		ret.add(newDataPointSet(prefix, "count", now,
+				snapshot.size()));
+
+		return ret;
+	}
 }
diff --git a/src/main/java/org/kairosdb/datastore/cassandra/CassandraConfiguration.java b/src/main/java/org/kairosdb/datastore/cassandra/CassandraConfiguration.java
index 33ded0c71e..6deb81aded 100755
--- a/src/main/java/org/kairosdb/datastore/cassandra/CassandraConfiguration.java
+++ b/src/main/java/org/kairosdb/datastore/cassandra/CassandraConfiguration.java
@@ -1,5 +1,6 @@
 package org.kairosdb.datastore.cassandra;
 
+import com.datastax.driver.core.ConsistencyLevel;
 import com.google.inject.Inject;
 import com.google.inject.name.Named;
 
@@ -18,15 +19,7 @@
 	public static final String STRING_CACHE_SIZE_PROPERTY = "kairosdb.datastore.cassandra.string_cache_size";
 
 	public static final String KEYSPACE_PROPERTY = "kairosdb.datastore.cassandra.keyspace";
-	public static final String REPLICATION_FACTOR_PROPERTY = "kairosdb.datastore.cassandra.replication_factor";
-	public static final String WRITE_DELAY_PROPERTY = "kairosdb.datastore.cassandra.write_delay";
-
-	public static final String WRITE_BUFFER_SIZE = "kairosdb.datastore.cassandra.write_buffer_max_size";
-	public static final String SINGLE_ROW_READ_SIZE_PROPERTY = "kairosdb.datastore.cassandra.single_row_read_size";
-	public static final String MULTI_ROW_READ_SIZE_PROPERTY = "kairosdb.datastore.cassandra.multi_row_read_size";
-	public static final String MULTI_ROW_SIZE_PROPERTY = "kairosdb.datastore.cassandra.multi_row_size";
-
-	public static final String USE_THRIFT = "kairosdb.datastore.cassandra.use_thrift";
+	public static final String SIMULTANIOUS_QUERIES = "kairosdb.datastore.cassandra.simultaneous_cql_queries";
 
 
 	@Inject
@@ -54,56 +47,20 @@
 	private Map<String, String> m_cassandraAuthentication;
 
 	@Inject
-	@Named(REPLICATION_FACTOR_PROPERTY)
-	private int m_replicationFactor;
-
-	@Inject
-	@Named(SINGLE_ROW_READ_SIZE_PROPERTY)
-	private int m_singleRowReadSize;
-
-	@Inject
-	@Named(MULTI_ROW_SIZE_PROPERTY)
-	private int m_multiRowSize;
-
-	@Inject
-	@Named(MULTI_ROW_READ_SIZE_PROPERTY)
-	private int m_multiRowReadSize;
-
-	@Inject
-	@Named(WRITE_DELAY_PROPERTY)
-	private int m_writeDelay;
-
-	@Inject
-	@Named(WRITE_BUFFER_SIZE)
-	private int m_maxWriteSize;
+	@Named(SIMULTANIOUS_QUERIES)
+	private int m_simultaneousQueries = 100;
 
 	@Inject
 	@Named(KEYSPACE_PROPERTY)
 	private String m_keyspaceName;
 
-	@Inject
-	@Named(USE_THRIFT)
-	private boolean m_useThrift = false;
-
 
 	public CassandraConfiguration()
 	{
 	}
 
-	public CassandraConfiguration(int replicationFactor,
-			int singleRowReadSize,
-			int multiRowSize,
-			int multiRowReadSize,
-			int writeDelay,
-			int maxWriteSize,
-			String keyspaceName)
+	public CassandraConfiguration(String keyspaceName)
 	{
-		m_replicationFactor = replicationFactor;
-		m_singleRowReadSize = singleRowReadSize;
-		m_multiRowSize = multiRowSize;
-		m_multiRowReadSize = multiRowReadSize;
-		m_writeDelay = writeDelay;
-		m_maxWriteSize = maxWriteSize;
 		m_keyspaceName = keyspaceName;
 	}
 
@@ -137,43 +94,13 @@ public int getStringCacheSize()
 		return m_cassandraAuthentication;
 	}
 
-	public int getReplicationFactor()
-	{
-		return m_replicationFactor;
-	}
-
-	public int getSingleRowReadSize()
-	{
-		return m_singleRowReadSize;
-	}
-
-	public int getMultiRowSize()
-	{
-		return m_multiRowSize;
-	}
-
-	public int getMultiRowReadSize()
-	{
-		return m_multiRowReadSize;
-	}
-
-	public int getWriteDelay()
-	{
-		return m_writeDelay;
-	}
-
-	public int getMaxWriteSize()
-	{
-		return m_maxWriteSize;
-	}
-
 	public String getKeyspaceName()
 	{
 		return m_keyspaceName;
 	}
 
-	public boolean isUseThrift()
+	public int getSimultaneousQueries()
 	{
-		return m_useThrift;
+		return m_simultaneousQueries;
 	}
 }
diff --git a/src/main/java/org/kairosdb/datastore/cassandra/CassandraDatastore.java b/src/main/java/org/kairosdb/datastore/cassandra/CassandraDatastore.java
index 6fb1250d09..175301709a 100755
--- a/src/main/java/org/kairosdb/datastore/cassandra/CassandraDatastore.java
+++ b/src/main/java/org/kairosdb/datastore/cassandra/CassandraDatastore.java
@@ -24,18 +24,8 @@
 import com.google.common.util.concurrent.ListenableFuture;
 import com.google.inject.Inject;
 import com.google.inject.name.Named;
-//import com.netflix.astyanax.serializers.StringSerializer;
-import me.prettyprint.cassandra.model.ConfigurableConsistencyLevel;
-import me.prettyprint.cassandra.serializers.StringSerializer;
-import me.prettyprint.cassandra.service.CassandraHostConfigurator;
-import me.prettyprint.cassandra.service.ColumnSliceIterator;
-import me.prettyprint.hector.api.Cluster;
-import me.prettyprint.hector.api.Keyspace;
-import me.prettyprint.hector.api.ddl.KeyspaceDefinition;
-import me.prettyprint.hector.api.exceptions.HectorException;
-import me.prettyprint.hector.api.factory.HFactory;
-import me.prettyprint.hector.api.query.SliceQuery;
 import org.kairosdb.core.DataPoint;
+import org.kairosdb.core.DataPointSet;
 import org.kairosdb.core.KairosDataPointFactory;
 import org.kairosdb.core.datapoints.*;
 import org.kairosdb.core.datastore.*;
@@ -43,11 +33,13 @@
 import org.kairosdb.core.queue.EventCompletionCallBack;
 import org.kairosdb.core.queue.ProcessorHandler;
 import org.kairosdb.core.queue.QueueProcessor;
+import org.kairosdb.core.reporting.KairosMetricReporter;
 import org.kairosdb.core.reporting.ThreadReporter;
 import org.kairosdb.events.DataPointEvent;
-import org.kairosdb.util.CongestionExecutorService;
+import org.kairosdb.util.AdaptiveExecutorService;
 import org.kairosdb.util.KDataInput;
 import org.kairosdb.util.MemoryMonitor;
+import org.kairosdb.util.SimpleStatsReporter;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -60,7 +52,7 @@
 
 import static com.google.common.base.Preconditions.checkNotNull;
 
-public class CassandraDatastore implements Datastore, ProcessorHandler
+public class CassandraDatastore implements Datastore, ProcessorHandler, KairosMetricReporter
 {
 	public static final Logger logger = LoggerFactory.getLogger(CassandraDatastore.class);
 
@@ -112,26 +104,54 @@
 			"  PRIMARY KEY ((key), column1)\n" +
 			") WITH COMPACT STORAGE";
 
+	public static final String SERVICE_INDEX = "" +
+			"CREATE TABLE IF NOT EXISTS service_index (" +
+			" service text," +
+			" service_key text," +
+			" key text," +
+			" value text," +
+			" PRIMARY KEY ((service, service_key), key)" +
+			")";
 
-	//todo desc order for data queries
+	//All inserts and deletes add millisecond timestamp consistency with old code and TWCS instead of nanos
 	public static final String DATA_POINTS_INSERT = "INSERT INTO data_points " +
-			"(key, column1, value) VALUES (?, ?, ?) USING TTL ?";
+			"(key, column1, value) VALUES (?, ?, ?) USING TTL ? AND TIMESTAMP ?";
+
+	public static final String ROW_KEY_TIME_INSERT = "INSERT INTO row_key_time_index " +
+			"(metric, row_time) VALUES (?, ?) USING TTL ? AND TIMESTAMP ?";
 
-	public static final String ROW_KEY_INDEX_INSERT = "INSERT INTO row_key_index " +
-			"(key, column1, value) VALUES (?, ?, 0x00) USING TTL ?";
+	public static final String ROW_KEY_INSERT = "INSERT INTO row_keys " +
+			"(metric, row_time, data_type, tags) VALUES (?, ?, ?, ?) USING TTL ?"; // AND TIMESTAMP ?";
 
 	public static final String STRING_INDEX_INSERT = "INSERT INTO string_index " +
 			"(key, column1, value) VALUES (?, ?, 0x00)";
 
 	public static final String DATA_POINTS_QUERY = "SELECT column1, value FROM data_points WHERE key = ? AND " +
-			"column1 >= ? AND column1 < ?";
+			"column1 >= ? AND column1 < ? ORDER BY column1";
+
+	public static final String DATA_POINTS_QUERY_ASC = DATA_POINTS_QUERY+" ASC";
+	public static final String DATA_POINTS_QUERY_DESC = DATA_POINTS_QUERY+" DESC";
+
+	public static final String DATA_POINTS_DELETE = "DELETE FROM data_points " +
+			"WHERE key = ? AND column1 = ?";
+
+	public static final String DATA_POINTS_DELETE_ROW = "DELETE FROM data_points " +
+			"WHERE key = ?";
 
-	public static final String STRING_QUERY = "SELECT column1 FROM string_index " +
+	public static final String STRING_INDEX_QUERY = "SELECT column1 FROM string_index " +
 			"WHERE key = ?";
 
+	//This is the old row key index query
 	public static final String ROW_KEY_INDEX_QUERY = "SELECT column1 FROM row_key_index " +
 			"WHERE key = ? AND column1 >= ? AND column1 < ?";
 
+	public static final String ROW_KEY_INDEX_DELETE = "DELETE FROM row_key_index " +
+			"WHERE KEY = ? AND column1 = ?";
+
+	public static final String ROW_KEY_INDEX_DELETE_ROW = "DELETE FROM row_key_index " +
+			"WHERE KEY = ?";
+
+	//New Row key queries
 	public static final String ROW_KEY_TIME_QUERY = "SELECT row_time " +
 			"FROM row_key_time_index WHERE metric = ? AND " +
 			"row_time >= ? AND row_time <= ?";
@@ -163,121 +183,109 @@
 	public static final String ROW_KEY_TAG_VALUES = "tag_values";
 	private static final Charset UTF_8 = Charset.forName("UTF-8");
 
-	private final Cluster m_cluster;
-	private final Keyspace m_keyspace;
+	//private final Cluster m_cluster;
 	private final EventBus m_eventBus;
 
 
 	//new properties
 	private final CassandraClient m_cassandraClient;
-	private final AstyanaxClient m_astyanaxClient;
+	//private final AstyanaxClient m_astyanaxClient;
+
 
+	private final PreparedStatements m_preparedStatements;
 	private Session m_session;
-	private final PreparedStatement m_psInsertData;
-	private final PreparedStatement m_psInsertRowKey;
-	private final PreparedStatement m_psInsertString;
-	private final PreparedStatement m_psQueryData;
-	private final PreparedStatement m_psStringQuery;
-	private final PreparedStatement m_psRowKeyIndexQuery;
-	private final PreparedStatement m_psRowKeyQuery;
-	private final PreparedStatement m_psRowKeyTimeQuery;
-
-	private BatchStatement m_batchStatement;
-	private final Object m_batchLock = new Object();
-
-	private final boolean m_useThrift;
-	//End new props
-
-	private String m_keyspaceName;
-	private int m_singleRowReadSize;
-	private int m_multiRowSize;
-	private int m_multiRowReadSize;
+
+
+	public class PreparedStatements
+	{
+		public final PreparedStatement psDataPointsInsert;
+		//public final PreparedStatement m_psInsertRowKey;
+		public final PreparedStatement psStringIndexInsert;
+		public final PreparedStatement psDataPointsQueryAsc;
+		public final PreparedStatement psStringIndexQuery;
+		public final PreparedStatement psRowKeyIndexQuery;
+		public final PreparedStatement psRowKeyQuery;
+		public final PreparedStatement psRowKeyTimeQuery;
+		public final PreparedStatement psDataPointsDeleteRow;
+		public final PreparedStatement psDataPointsDelete;
+		public final PreparedStatement psRowKeyIndexDelete;
+		public final PreparedStatement psRowKeyIndexDeleteRow;
+		public final PreparedStatement psDataPointsQueryDesc;
+		public final PreparedStatement psRowKeyTimeInsert;
+		public final PreparedStatement psRowKeyInsert;
+
+		public PreparedStatements()
+		{
+			psDataPointsInsert  = m_session.prepare(DATA_POINTS_INSERT);
+			//m_psInsertRowKey      = m_session.prepare(ROW_KEY_INDEX_INSERT);
+			psRowKeyTimeInsert = m_session.prepare(ROW_KEY_TIME_INSERT);
+			psRowKeyInsert = m_session.prepare(ROW_KEY_INSERT);
+			psStringIndexInsert = m_session.prepare(STRING_INDEX_INSERT);
+			psDataPointsQueryAsc = m_session.prepare(DATA_POINTS_QUERY_ASC);
+			psDataPointsQueryDesc = m_session.prepare(DATA_POINTS_QUERY_DESC);
+			psStringIndexQuery = m_session.prepare(STRING_INDEX_QUERY);
+			psRowKeyIndexQuery  = m_session.prepare(ROW_KEY_INDEX_QUERY);
+			psRowKeyQuery       = m_session.prepare(ROW_KEY_QUERY);
+			psRowKeyTimeQuery   = m_session.prepare(ROW_KEY_TIME_QUERY);
+			psDataPointsDelete = m_session.prepare(DATA_POINTS_DELETE);
+			psDataPointsDeleteRow = m_session.prepare(DATA_POINTS_DELETE_ROW);
+			psRowKeyIndexDelete = m_session.prepare(ROW_KEY_INDEX_DELETE);
+			psRowKeyIndexDeleteRow = m_session.prepare(ROW_KEY_INDEX_DELETE_ROW);
+		}
+	}
+
+
+
+	private final BatchStats m_batchStats = new BatchStats();
 
 	private DataCache<DataPointsRowKey> m_rowKeyCache = new DataCache<DataPointsRowKey>(1024);
 	private DataCache<String> m_metricNameCache = new DataCache<String>(1024);
 
 	private final KairosDataPointFactory m_kairosDataPointFactory;
 	private final QueueProcessor m_queueProcessor;
-	private final CongestionExecutorService m_congestionExecutor;
+	private final AdaptiveExecutorService m_congestionExecutor;
 
 	private CassandraConfiguration m_cassandraConfiguration;
 
 	@Inject
-	private LongDataPointFactory m_longDataPointFactory = new LongDataPointFactoryImpl();
+	private SimpleStatsReporter m_simpleStatsReporter = new SimpleStatsReporter();
 
 
 	@Inject
 	public CassandraDatastore(@Named("HOSTNAME") final String hostname,
 			CassandraClient cassandraClient,
-			AstyanaxClient astyanaxClient,
 			CassandraConfiguration cassandraConfiguration,
-			HectorConfiguration configuration,
 			KairosDataPointFactory kairosDataPointFactory,
 			QueueProcessor queueProcessor,
 			EventBus eventBus,
-			CongestionExecutorService congestionExecutor) throws DatastoreException
+			AdaptiveExecutorService congestionExecutor) throws DatastoreException
 	{
 		m_cassandraClient = cassandraClient;
-		m_astyanaxClient = astyanaxClient;
+		//m_astyanaxClient = astyanaxClient;
 		m_kairosDataPointFactory = kairosDataPointFactory;
 		m_queueProcessor = queueProcessor;
 		m_congestionExecutor = congestionExecutor;
 		m_eventBus = eventBus;
 
-		m_useThrift = cassandraConfiguration.isUseThrift();
-
 		setupSchema();
 
 		m_session = m_cassandraClient.getKeyspaceSession();
 		//Prepare queries
-
-		m_psInsertData = m_session.prepare(DATA_POINTS_INSERT);
-		m_psInsertRowKey = m_session.prepare(ROW_KEY_INDEX_INSERT);
-		m_psInsertString = m_session.prepare(STRING_INDEX_INSERT);
-		m_psQueryData = m_session.prepare(DATA_POINTS_QUERY);
-		m_psStringQuery = m_session.prepare(STRING_QUERY);
-		m_psRowKeyIndexQuery = m_session.prepare(ROW_KEY_INDEX_QUERY);
-		m_psRowKeyQuery = m_session.prepare(ROW_KEY_QUERY);
-		m_psRowKeyTimeQuery = m_session.prepare(ROW_KEY_TIME_QUERY);
+		m_preparedStatements = new PreparedStatements();
 
 		m_cassandraConfiguration = cassandraConfiguration;
-		m_singleRowReadSize = m_cassandraConfiguration.getSingleRowReadSize();
-		m_multiRowSize = m_cassandraConfiguration.getMultiRowSize();
-		m_multiRowReadSize = m_cassandraConfiguration.getMultiRowReadSize();
-		m_keyspaceName = m_cassandraConfiguration.getKeyspaceName();
 
 		m_rowKeyCache = new DataCache<DataPointsRowKey>(m_cassandraConfiguration.getRowKeyCacheSize());
 		m_metricNameCache = new DataCache<String>(m_cassandraConfiguration.getStringCacheSize());
 
-		try
-		{
-			CassandraHostConfigurator hostConfig = configuration.getConfiguration();
-			int threadCount = hostConfig.buildCassandraHosts().length + 3;
-
-			m_cluster = HFactory.getOrCreateCluster("kairosdb-cluster",
-					hostConfig, m_cassandraConfiguration.getCassandraAuthentication());
-
-			KeyspaceDefinition keyspaceDef = m_cluster.describeKeyspace(m_keyspaceName);
-
-
-			//set global consistency level
-			ConfigurableConsistencyLevel confConsLevel = new ConfigurableConsistencyLevel();
-			confConsLevel.setDefaultReadConsistencyLevel(m_cassandraConfiguration.getDataReadLevel().getHectorLevel());
-			confConsLevel.setDefaultWriteConsistencyLevel(m_cassandraConfiguration.getDataWriteLevel().getHectorLevel());
-
-			//create keyspace instance with specified consistency
-			m_keyspace = HFactory.createKeyspace(m_keyspaceName, m_cluster, confConsLevel);
-		}
-		catch (HectorException e)
-		{
-			throw new DatastoreException(e);
-		}
-
-
 		//This needs to be done last as it tells the processor we are ready for data
 		m_queueProcessor.setProcessorHandler(this);
 	}
 
+	private static ByteBuffer serializeString(String str)
+	{
+		return ByteBuffer.wrap(str.getBytes(UTF_8));
+	}
 
 	private void setupSchema()
 	{
@@ -325,26 +333,20 @@ public void close() throws InterruptedException
 	@Subscribe
 	public void putDataPoint(DataPointEvent dataPointEvent) throws DatastoreException
 	{
-		/*if (dataPointEvent.getMetricName().startsWith("blast"))
-			return;*/
 		m_queueProcessor.put(dataPointEvent);
 	}
 
 	@Override
-	public void handleEvents(List<DataPointEvent> events, EventCompletionCallBack eventCompletionCallBack)
+	public void handleEvents(List<DataPointEvent> events, EventCompletionCallBack eventCompletionCallBack,
+			boolean fullBatch)
 	{
 		BatchHandler batchHandler;
 
-		if (m_useThrift)
-			batchHandler = m_astyanaxClient.getBatchHandler(events, eventCompletionCallBack,
-					m_cassandraConfiguration.getDatapointTtl(),
-					m_rowKeyCache, m_metricNameCache, m_eventBus, m_session,
-					m_psInsertData, m_psInsertRowKey, m_psInsertString);
-		else
-			batchHandler = new CQLBatchHandler(events, eventCompletionCallBack,
-					m_cassandraConfiguration.getDatapointTtl(),
-					m_rowKeyCache, m_metricNameCache, m_eventBus, m_session,
-					m_psInsertData, m_psInsertRowKey, m_psInsertString);
+		batchHandler = new CQLBatchHandler(events, eventCompletionCallBack,
+				m_cassandraConfiguration.getDatapointTtl(),
+				m_cassandraConfiguration.getDataWriteLevel(),
+				m_rowKeyCache, m_metricNameCache, m_eventBus, m_session,
+				m_preparedStatements, fullBatch, m_batchStats);
 
 		m_congestionExecutor.submit(batchHandler);
 	}
@@ -352,8 +354,9 @@ public void handleEvents(List<DataPointEvent> events, EventCompletionCallBack ev
 
 	private Iterable<String> queryStringIndex(final String key)
 	{
-		BoundStatement boundStatement = new BoundStatement(m_psStringQuery);
-		boundStatement.setBytesUnsafe(0, StringSerializer.get().toByteBuffer(key));
+		BoundStatement boundStatement = new BoundStatement(m_preparedStatements.psStringIndexQuery);
+		boundStatement.setBytesUnsafe(0, serializeString(key));
+		boundStatement.setConsistencyLevel(m_cassandraConfiguration.getDataReadLevel());
 
 		ResultSet resultSet = m_session.execute(boundStatement);
 
@@ -362,7 +365,7 @@ public void handleEvents(List<DataPointEvent> events, EventCompletionCallBack ev
 		while (!resultSet.isExhausted())
 		{
 			Row row = resultSet.one();
-			ret.add(StringSerializer.get().fromByteBuffer(row.getBytes(0)));
+			ret.add(UTF_8.decode(row.getBytes(0)).toString());
 		}
 
 		return ret;
@@ -406,13 +409,54 @@ public TagSet queryMetricTags(DatastoreMetricQuery query) throws DatastoreExcept
 		return (tagSet);
 	}
 
+	@Override
+	public void setValue(String service, String serviceKey, String key, String value) throws DatastoreException
+	{
+
+	}
+
+	@Override
+	public String getValue(String service, String serviceKey, String key) throws DatastoreException
+	{
+		return null;
+	}
+
+	@Override
+	public Iterable<String> listKeys(String service, String serviceKey) throws DatastoreException
+	{
+		return null;
+	}
+
+	@Override
+	public Iterable<String> listKeys(String service, String serviceKey, String keyStartsWith) throws DatastoreException
+	{
+		return null;
+	}
+
 	@Override
 	public void queryDatabase(DatastoreMetricQuery query, QueryCallback queryCallback) throws DatastoreException
 	{
-		//queryWithRowKeys(query, queryCallback, getKeysForQueryIterator(query));
 		cqlQueryWithRowKeys(query, queryCallback, getKeysForQueryIterator(query));
 	}
 
+	@Override
+	public List<DataPointSet> getMetrics(long now)
+	{
+		List<DataPointSet> ret = new ArrayList<>();
+
+		m_simpleStatsReporter.reportStats(m_batchStats.getNameStats(), now,
+				"kairosdb.datastore.cassandra.write_batch",
+				"table", "string_index", ret);
+		m_simpleStatsReporter.reportStats(m_batchStats.getDataPointStats(), now,
+				"kairosdb.datastore.cassandra.write_batch",
+				"table", "data_points", ret);
+		m_simpleStatsReporter.reportStats(m_batchStats.getRowKeyStats(), now,
+				"kairosdb.datastore.cassandra.write_batch",
+				"table", "row_keys", ret);
+
+		return ret;
+	}
+
 	private class QueryListener implements FutureCallback<ResultSet>
 	{
 		private final DataPointsRowKey m_rowKey;
@@ -498,15 +542,15 @@ private void cqlQueryWithRowKeys(DatastoreMetricQuery query,
 		int rowCount = 0;
 		long queryStartTime = query.getStartTime();
 		long queryEndTime = query.getEndTime();
+
 		ExecutorService resultsExecutor = Executors.newSingleThreadExecutor();
 		//Controls the number of queries sent out at the same time.
-		Semaphore querySemaphor = new Semaphore(100); //todo: add config for this
+		Semaphore querySemaphor = new Semaphore(m_cassandraConfiguration.getSimultaneousQueries());
 
 		while (rowKeys.hasNext())
 		{
 			rowCount ++;
 			DataPointsRowKey rowKey = rowKeys.next();
-			//System.out.println("Query for "+rowKey.toString());
 			long tierRowTime = rowKey.getTimestamp();
 			int startTime;
 			int endTime;
@@ -528,10 +572,18 @@ private void cqlQueryWithRowKeys(DatastoreMetricQuery query,
 			endBuffer.putInt(endTime);
 			endBuffer.rewind();
 
-			BoundStatement boundStatement = new BoundStatement(m_psQueryData);
+			BoundStatement boundStatement;
+			if (query.getOrder() == Order.ASC)
+				boundStatement = new BoundStatement(m_preparedStatements.psDataPointsQueryAsc);
+			else
+				boundStatement = new BoundStatement(m_preparedStatements.psDataPointsQueryDesc);
+
 			boundStatement.setBytesUnsafe(0, DATA_POINTS_ROW_KEY_SERIALIZER.toByteBuffer(rowKey));
 			boundStatement.setBytesUnsafe(1, startBuffer);
 			boundStatement.setBytesUnsafe(2, endBuffer);
+			//boundStatement.setInt(3, Integer.MAX_VALUE);
+
+			boundStatement.setConsistencyLevel(m_cassandraConfiguration.getDataReadLevel());
 
 			try
 			{
@@ -551,7 +603,7 @@ private void cqlQueryWithRowKeys(DatastoreMetricQuery query,
 
 		try
 		{
-			querySemaphor.acquire(100); //todo use same as above
+			querySemaphor.acquire(m_cassandraConfiguration.getSimultaneousQueries());
 			queryCallback.endDataPoints();
 			resultsExecutor.shutdown();
 		}
@@ -565,86 +617,12 @@ private void cqlQueryWithRowKeys(DatastoreMetricQuery query,
 		}
 	}
 
-	private void queryWithRowKeys(DatastoreMetricQuery query,
-			QueryCallback queryCallback, Iterator<DataPointsRowKey> rowKeys)
-	{
-		long startTime = System.currentTimeMillis();
-		long currentTimeTier = 0L;
-		String currentType = null;
-		int rowCount = 0;
-
-		List<QueryRunner> runners = new ArrayList<QueryRunner>();
-		List<DataPointsRowKey> queryKeys = new ArrayList<DataPointsRowKey>();
-
-		MemoryMonitor mm = new MemoryMonitor(20);
-		while (rowKeys.hasNext())
-		{
-			rowCount++;
-			DataPointsRowKey rowKey = rowKeys.next();
-			if (currentTimeTier == 0L)
-				currentTimeTier = rowKey.getTimestamp();
-
-			if (currentType == null)
-				currentType = rowKey.getDataType();
-
-			if ((rowKey.getTimestamp() == currentTimeTier) && (queryKeys.size() < m_multiRowSize) &&
-					(currentType.equals(rowKey.getDataType())))
-			{
-				queryKeys.add(rowKey);
-			}
-			else
-			{
-				runners.add(new QueryRunner(m_keyspace, CF_DATA_POINTS_NAME, m_kairosDataPointFactory,
-						queryKeys,
-						query.getStartTime(), query.getEndTime(), queryCallback, m_singleRowReadSize,
-						m_multiRowReadSize, query.getLimit(), query.getOrder()));
-
-				queryKeys = new ArrayList<DataPointsRowKey>();
-				queryKeys.add(rowKey);
-				currentTimeTier = rowKey.getTimestamp();
-				currentType = rowKey.getDataType();
-			}
-
-			mm.checkMemoryAndThrowException();
-		}
-
-		ThreadReporter.addDataPoint(ROW_KEY_COUNT, rowCount);
-
-		//There may be stragglers that are not ran
-		if (!queryKeys.isEmpty())
-		{
-			runners.add(new QueryRunner(m_keyspace, CF_DATA_POINTS_NAME, m_kairosDataPointFactory,
-					queryKeys,
-					query.getStartTime(), query.getEndTime(), queryCallback, m_singleRowReadSize,
-					m_multiRowReadSize, query.getLimit(), query.getOrder()));
-		}
-
-		ThreadReporter.addDataPoint(KEY_QUERY_TIME, System.currentTimeMillis() - startTime);
-
-		//Changing the check rate
-		mm.setCheckRate(1);
-		try
-		{
-			//TODO: Run this with multiple threads
-			for (QueryRunner runner : runners)
-			{
-				runner.runQuery();
-
-				mm.checkMemoryAndThrowException();
-			}
-
-			queryCallback.endDataPoints();
-		}
-		catch (IOException e)
-		{
-			e.printStackTrace();
-		}
-	}
 
 	@Override
 	public void deleteDataPoints(DatastoreMetricQuery deleteQuery) throws DatastoreException
 	{
 		checkNotNull(deleteQuery);
+		boolean clearCache = false;
 
 		long now = System.currentTimeMillis();
 
@@ -662,9 +640,17 @@ public void deleteDataPoints(DatastoreMetricQuery deleteQuery) throws DatastoreE
 			if (deleteQuery.getStartTime() <= rowKeyTimestamp && (deleteQuery.getEndTime() >= rowKeyTimestamp + ROW_WIDTH - 1))
 			{
 				//todo fix me
-				//m_dataPointWriteBuffer.deleteRow(rowKey, now);  // delete the whole row
-				//m_rowKeyWriteBuffer.deleteColumn(rowKey.getMetricName(), rowKey, now); // Delete the index
-				m_rowKeyCache.clear();
+				BoundStatement statement = new BoundStatement(m_preparedStatements.psDataPointsDeleteRow);
+				statement.setBytesUnsafe(0, DATA_POINTS_ROW_KEY_SERIALIZER.toByteBuffer(rowKey));
+				statement.setConsistencyLevel(m_cassandraConfiguration.getDataReadLevel());
+				m_session.executeAsync(statement);
+
+				statement = new BoundStatement(m_preparedStatements.psRowKeyIndexDelete);
+				statement.setBytesUnsafe(0, serializeString(rowKey.getMetricName()));
+				statement.setBytesUnsafe(1, DATA_POINTS_ROW_KEY_SERIALIZER.toByteBuffer(rowKey));
+				statement.setConsistencyLevel(m_cassandraConfiguration.getDataReadLevel());
+				m_session.executeAsync(statement);
+				clearCache = true;
 			}
 			else
 			{
@@ -672,17 +658,26 @@ public void deleteDataPoints(DatastoreMetricQuery deleteQuery) throws DatastoreE
 			}
 		}
 
-		queryWithRowKeys(deleteQuery, new DeletingCallback(deleteQuery.getName()), partialRows.iterator());
+
+		cqlQueryWithRowKeys(deleteQuery, new DeletingCallback(deleteQuery.getName()), partialRows.iterator());
 
 		// If index is gone, delete metric name from Strings column family
 		if (deleteAll)
 		{
-			//m_rowKeyWriteBuffer.deleteRow(deleteQuery.getName(), now);
+			BoundStatement statement = new BoundStatement(m_preparedStatements.psRowKeyIndexDeleteRow);
+			statement.setBytesUnsafe(0, serializeString(deleteQuery.getName()));
+			statement.setConsistencyLevel(m_cassandraConfiguration.getDataReadLevel());
+			m_session.executeAsync(statement);
+
 			//todo fix me
 			//m_stringIndexWriteBuffer.deleteColumn(ROW_KEY_METRIC_NAMES, deleteQuery.getName(), now);
-			m_rowKeyCache.clear();
+			clearCache = true;
 			m_metricNameCache.clear();
 		}
+
+
+		if (clearCache)
+			m_rowKeyCache.clear();
 	}
 
 	private SortedMap<String, String> getTags(DataPointRow row)
@@ -801,26 +796,29 @@ public CQLFilteredRowKeyIterator(String metricName, long startTime, long endTime
 			//Legacy key index - index is all in one row
 			if ((startTime < 0) && (endTime >= 0))
 			{
-				BoundStatement negStatement = new BoundStatement(m_psRowKeyIndexQuery);
-				negStatement.setBytesUnsafe(0, StringSerializer.get().toByteBuffer(metricName));
+				BoundStatement negStatement = new BoundStatement(m_preparedStatements.psRowKeyIndexQuery);
+				negStatement.setBytesUnsafe(0, serializeString(metricName));
 				setStartEndKeys(negStatement, metricName, startTime, -1L);
+				negStatement.setConsistencyLevel(m_cassandraConfiguration.getDataReadLevel());
 
 				ResultSetFuture future = m_session.executeAsync(negStatement);
 				futures.add(future);
 
 
-				BoundStatement posStatement = new BoundStatement(m_psRowKeyIndexQuery);
-				posStatement.setBytesUnsafe(0, StringSerializer.get().toByteBuffer(metricName));
+				BoundStatement posStatement = new BoundStatement(m_preparedStatements.psRowKeyIndexQuery);
+				posStatement.setBytesUnsafe(0, serializeString(metricName));
 				setStartEndKeys(posStatement, metricName, 0L, endTime);
+				posStatement.setConsistencyLevel(m_cassandraConfiguration.getDataReadLevel());
 
 				future = m_session.executeAsync(posStatement);
 				futures.add(future);
 			}
 			else
 			{
-				BoundStatement statement = new BoundStatement(m_psRowKeyIndexQuery);
-				statement.setBytesUnsafe(0, StringSerializer.get().toByteBuffer(metricName));
+				BoundStatement statement = new BoundStatement(m_preparedStatements.psRowKeyIndexQuery);
+				statement.setBytesUnsafe(0, serializeString(metricName));
 				setStartEndKeys(statement, metricName, startTime, endTime);
+				statement.setConsistencyLevel(m_cassandraConfiguration.getDataReadLevel());
 
 				ResultSetFuture future = m_session.executeAsync(statement);
 				futures.add(future);
@@ -830,9 +828,10 @@ public CQLFilteredRowKeyIterator(String metricName, long startTime, long endTime
 			List<Long> queryKeyList = createQueryKeyList(metricName, startTime, endTime);
 			for (Long keyTime : queryKeyList)
 			{
-				BoundStatement statement = new BoundStatement(m_psRowKeyQuery);
+				BoundStatement statement = new BoundStatement(m_preparedStatements.psRowKeyQuery);
 				statement.setString(0, metricName);
-				statement.setTime(1, keyTime);
+				statement.setTimestamp(1, new Date(keyTime));
+				statement.setConsistencyLevel(m_cassandraConfiguration.getDataReadLevel());
 
 				ResultSetFuture future = m_session.executeAsync(statement);
 				futures.add(future);
@@ -860,7 +859,7 @@ private DataPointsRowKey nextKeyFromIterator(ResultSet iterator)
 		{
 			DataPointsRowKey next = null;
 			boolean newIndex = false;
-			if (iterator.getColumnDefinitions().contains("metric"))
+			if (iterator.getColumnDefinitions().contains("row_time"))
 				newIndex = true;
 
 outer:
@@ -870,7 +869,7 @@ private DataPointsRowKey nextKeyFromIterator(ResultSet iterator)
 				Row record = iterator.one();
 
 				if (newIndex)
-					rowKey = new DataPointsRowKey(m_metricName, record.getTime(0),
+					rowKey = new DataPointsRowKey(m_metricName, record.getTimestamp(0).getTime(),
 							record.getString(1), new TreeMap<String, String>(record.getMap(2, String.class, String.class)));
 				else
 					rowKey = DATA_POINTS_ROW_KEY_SERIALIZER.fromByteBuffer(record.getBytes(0));
@@ -895,16 +894,17 @@ private DataPointsRowKey nextKeyFromIterator(ResultSet iterator)
 		{
 			List<Long> ret = new ArrayList<>();
 
-			BoundStatement statement = new BoundStatement(m_psRowKeyTimeQuery);
+			BoundStatement statement = new BoundStatement(m_preparedStatements.psRowKeyTimeQuery);
 			statement.setString(0, metricName);
 			statement.setTimestamp(1, new Date(calculateRowTime(startTime)));
 			statement.setTimestamp(2, new Date(endTime));
+			statement.setConsistencyLevel(m_cassandraConfiguration.getDataReadLevel());
 
 			ResultSet rows = m_session.execute(statement);
 
 			while (!rows.isExhausted())
 			{
-				ret.add(rows.one().getTime(0));
+				ret.add(rows.one().getTimestamp(0).getTime());
 			}
 
 			return ret;
@@ -929,7 +929,7 @@ private void setStartEndKeys(
 		public boolean hasNext()
 		{
 			m_nextKey = null;
-			while (m_currentResultSet != null && !m_currentResultSet.isExhausted())
+			while (m_currentResultSet != null && (!m_currentResultSet.isExhausted() || m_resultSets.hasNext()))
 			{
 				m_nextKey = nextKeyFromIterator(m_currentResultSet);
 
@@ -955,122 +955,11 @@ public void remove()
 		}
 	}
 
-	private class FilteredRowKeyIterator implements Iterator<DataPointsRowKey>
-	{
-		private ColumnSliceIterator<String, DataPointsRowKey, String> m_sliceIterator;
-
-		/**
-		 Used when a query spans positive and negative time values, we have to
-		 query the positive separate from the negative times as negative times
-		 are sorted after the positive ones.
-		 */
-		private ColumnSliceIterator<String, DataPointsRowKey, String> m_continueSliceIterator;
-		private DataPointsRowKey m_nextKey;
-		private SetMultimap<String, String> m_filterTags;
-
-		public FilteredRowKeyIterator(String metricName, long startTime, long endTime,
-				SetMultimap<String, String> filterTags)
-		{
-			m_filterTags = filterTags;
-			SliceQuery<String, DataPointsRowKey, String> sliceQuery =
-					HFactory.createSliceQuery(m_keyspace, StringSerializer.get(),
-							new DataPointsRowKeySerializer(true), StringSerializer.get());
-
-			sliceQuery.setColumnFamily(CF_ROW_KEY_INDEX_NAME)
-					.setKey(metricName);
-
-			if ((startTime < 0) && (endTime >= 0))
-			{
-				m_sliceIterator = createSliceIterator(sliceQuery, metricName,
-						startTime, -1L);
-
-				SliceQuery<String, DataPointsRowKey, String> sliceQuery2 =
-						HFactory.createSliceQuery(m_keyspace, StringSerializer.get(),
-								new DataPointsRowKeySerializer(true), StringSerializer.get());
-
-				sliceQuery2.setColumnFamily(CF_ROW_KEY_INDEX_NAME)
-						.setKey(metricName);
-
-				m_continueSliceIterator = createSliceIterator(sliceQuery2, metricName,
-						0, endTime);
-			}
-			else
-			{
-				m_sliceIterator = createSliceIterator(sliceQuery, metricName,
-						startTime, endTime);
-			}
-
-		}
-
-		private ColumnSliceIterator<String, DataPointsRowKey, String> createSliceIterator(
-				SliceQuery<String, DataPointsRowKey, String> sliceQuery,
-				String metricName, long startTime, long endTime)
-		{
-			DataPointsRowKey startKey = new DataPointsRowKey(metricName,
-					calculateRowTime(startTime), "");
-
-			DataPointsRowKey endKey = new DataPointsRowKey(metricName,
-					calculateRowTime(endTime), "");
-			endKey.setEndSearchKey(true);
-
-			ColumnSliceIterator<String, DataPointsRowKey, String> iterator = new ColumnSliceIterator<String, DataPointsRowKey, String>(sliceQuery,
-					startKey, endKey, false, m_singleRowReadSize);
-
-			return (iterator);
-		}
-
-		private DataPointsRowKey nextKeyFromIterator(ColumnSliceIterator<String, DataPointsRowKey, String> iterator)
-		{
-			DataPointsRowKey next = null;
-
-outer:
-			while (iterator.hasNext())
-			{
-				DataPointsRowKey rowKey = iterator.next().getName();
-
-				Map<String, String> keyTags = rowKey.getTags();
-				for (String tag : m_filterTags.keySet())
-				{
-					String value = keyTags.get(tag);
-					if (value == null || !m_filterTags.get(tag).contains(value))
-						continue outer; //Don't want this key
-				}
-
-				next = rowKey;
-				break;
-			}
-
-			return (next);
-		}
-
-		@Override
-		public boolean hasNext()
-		{
-			m_nextKey = nextKeyFromIterator(m_sliceIterator);
-
-			if ((m_nextKey == null) && (m_continueSliceIterator != null))
-				m_nextKey = nextKeyFromIterator(m_continueSliceIterator);
-
-			return (m_nextKey != null);
-		}
-
-		@Override
-		public DataPointsRowKey next()
-		{
-			return m_nextKey;
-		}
-
-		@Override
-		public void remove()
-		{
-		}
-	}
 
 	private class DeletingCallback implements QueryCallback
 	{
 		private SortedMap<String, String> m_currentTags;
 		private DataPointsRowKey m_currentRow;
-		private long m_now = System.currentTimeMillis();
 		private final String m_metric;
 		private String m_currentType;
 
@@ -1101,8 +990,15 @@ public void addDataPoint(DataPoint datapoint) throws IOException
 			else
 				columnName = getColumnName(rowTime, time);
 
-			//todo fix me
-			//m_dataPointWriteBuffer.deleteColumn(m_currentRow, columnName, m_now);
+			//Todo: may want to send these off in batches
+			BoundStatement statement = new BoundStatement(m_preparedStatements.psDataPointsDelete);
+			statement.setBytesUnsafe(0, DATA_POINTS_ROW_KEY_SERIALIZER.toByteBuffer(m_currentRow));
+			ByteBuffer b = ByteBuffer.allocate(4);
+			b.putInt(columnName);
+			b.rewind();
+			statement.setBytesUnsafe(1, b);
+			statement.setConsistencyLevel(m_cassandraConfiguration.getDataWriteLevel());
+			m_session.executeAsync(statement);
 		}
 
 		@Override
@@ -1110,7 +1006,7 @@ public void startDataPointSet(String dataType, Map<String, String> tags) throws
 		{
 			m_currentType = dataType;
 			m_currentTags = new TreeMap<String, String>(tags);
-			//This causes the row key to get reset with the first data point
+			//This causes the row key to get clear with the first data point
 			m_currentRow = null;
 		}
 
diff --git a/src/main/java/org/kairosdb/datastore/cassandra/CassandraModule.java b/src/main/java/org/kairosdb/datastore/cassandra/CassandraModule.java
index b4306933a2..54b6afda30 100755
--- a/src/main/java/org/kairosdb/datastore/cassandra/CassandraModule.java
+++ b/src/main/java/org/kairosdb/datastore/cassandra/CassandraModule.java
@@ -65,10 +65,9 @@ protected void configure()
 		bind(Datastore.class).to(CassandraDatastore.class).in(Scopes.SINGLETON);
 		bind(CassandraDatastore.class).in(Scopes.SINGLETON);
 		bind(CleanRowKeyCache.class).in(Scopes.SINGLETON);
-		bind(HectorConfiguration.class).in(Scopes.SINGLETON);
 		bind(CassandraConfiguration.class).in(Scopes.SINGLETON);
 		bind(CassandraClient.class).to(CassandraClientImpl.class);
-		bind(AstyanaxClient.class).in(Scopes.SINGLETON);
+		bind(CassandraClientImpl.class).in(Scopes.SINGLETON);
 
 		bind(new TypeLiteral<Map<String, String>>(){}).annotatedWith(Names.named(CASSANDRA_AUTH_MAP))
 				.toInstance(m_authMap);
diff --git a/src/main/java/org/kairosdb/datastore/cassandra/ConsistencyLevel.java b/src/main/java/org/kairosdb/datastore/cassandra/ConsistencyLevel.java
deleted file mode 100755
index 3a1aa92141..0000000000
--- a/src/main/java/org/kairosdb/datastore/cassandra/ConsistencyLevel.java
+++ /dev/null
@@ -1,50 +0,0 @@
-/*
- * Copyright 2016 KairosDB Authors
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.kairosdb.datastore.cassandra;
-
-import me.prettyprint.hector.api.HConsistencyLevel;
-
-/**
- Created with IntelliJ IDEA.
- User: bhawkins
- Date: 11/18/13
- Time: 8:42 AM
- To change this template use File | Settings | File Templates.
- */
-public enum ConsistencyLevel
-{
-	ANY (HConsistencyLevel.ANY),
-	ONE (HConsistencyLevel.ONE),
-	TWO (HConsistencyLevel.TWO),
-	THREE (HConsistencyLevel.THREE),
-	LOCAL_QUORUM (HConsistencyLevel.LOCAL_QUORUM),
-	EACH_QUORUM (HConsistencyLevel.EACH_QUORUM),
-	QUORUM (HConsistencyLevel.QUORUM),
-	ALL (HConsistencyLevel.ALL);
-
-	private final HConsistencyLevel m_level;
-
-	ConsistencyLevel(HConsistencyLevel level)
-	{
-		m_level = level;
-	}
-
-	public HConsistencyLevel getHectorLevel()
-	{
-		return (m_level);
-	}
-}
diff --git a/src/main/java/org/kairosdb/datastore/cassandra/DataPointsRowKeySerializer.java b/src/main/java/org/kairosdb/datastore/cassandra/DataPointsRowKeySerializer.java
index a891228611..f314b89236 100755
--- a/src/main/java/org/kairosdb/datastore/cassandra/DataPointsRowKeySerializer.java
+++ b/src/main/java/org/kairosdb/datastore/cassandra/DataPointsRowKeySerializer.java
@@ -15,7 +15,6 @@
  */
 package org.kairosdb.datastore.cassandra;
 
-import me.prettyprint.cassandra.serializers.AbstractSerializer;
 import org.kairosdb.core.datapoints.LegacyDataPointFactory;
 import org.kairosdb.util.StringPool;
 
@@ -23,7 +22,7 @@
 import java.nio.charset.Charset;
 import java.util.SortedMap;
 
-public class DataPointsRowKeySerializer extends AbstractSerializer<DataPointsRowKey>
+public class DataPointsRowKeySerializer
 {
 	public static final Charset UTF8 = Charset.forName("UTF-8");
 
@@ -53,7 +52,6 @@ private String getString(String str)
 			return (str);
 	}
 
-	@Override
 	public ByteBuffer toByteBuffer(DataPointsRowKey dataPointsRowKey)
 	{
 		ByteBuffer buffer = dataPointsRowKey.getSerializedBuffer();
@@ -210,7 +208,6 @@ private void extractTags(DataPointsRowKey rowKey, String tagString)
 	}
 
 
-	@Override
 	public DataPointsRowKey fromByteBuffer(ByteBuffer byteBuffer)
 	{
 		int start = byteBuffer.position();
diff --git a/src/main/java/org/kairosdb/datastore/cassandra/HectorConfiguration.java b/src/main/java/org/kairosdb/datastore/cassandra/HectorConfiguration.java
deleted file mode 100755
index 2c246aba83..0000000000
--- a/src/main/java/org/kairosdb/datastore/cassandra/HectorConfiguration.java
+++ /dev/null
@@ -1,185 +0,0 @@
-//
-//  HectorConfiguration.java
-//
-// Copyright 2016, KairosDB Authors
-//        
-package org.kairosdb.datastore.cassandra;
-
-import com.google.inject.Inject;
-import com.google.inject.name.Named;
-import me.prettyprint.cassandra.connection.DynamicLoadBalancingPolicy;
-import me.prettyprint.cassandra.connection.LeastActiveBalancingPolicy;
-import me.prettyprint.cassandra.connection.RoundRobinBalancingPolicy;
-import me.prettyprint.cassandra.service.CassandraHostConfigurator;
-
-import java.util.List;
-
-public class HectorConfiguration
-{
-	private static final String HOST_LIST_PROPERTY = "kairosdb.datastore.cassandra.host_list";
-
-	private static final String MAX_ACTIVE = "kairosdb.datastore.cassandra.hector.maxActive";
-	private static final String MAX_WAIT_TIME_WHEN_EXHAUSTED = "kairosdb.datastore.cassandra.hector.maxWaitTimeWhenExhausted";
-	private static final String USE_SOCKET_KEEP_ALIVE = "kairosdb.datastore.cassandra.hector.useSocketKeepalive";
-	private static final String CASSANDRA_THRIFT_SOCKET_TIMEOUT = "kairosdb.datastore.cassandra.hector.cassandraThriftSocketTimeout";
-	private static final String RETRY_DOWNED_HOSTS = "kairosdb.datastore.cassandra.hector.retryDownedHosts";
-	private static final String RETRY_DOWNED_HOSTS_DELAY_IN_SECONDS = "kairosdb.datastore.cassandra.hector.retryDownedHostsDelayInSeconds";
-	private static final String RETRY_DOWNED_HOSTS_QUEUE_SIZE = "kairosdb.datastore.cassandra.hector.retryDownedHostsQueueSize";
-	private static final String AUTO_DISCOVER_HOSTS = "kairosdb.datastore.cassandra.hector.autoDiscoverHosts";
-	private static final String AUTO_DISCOVER_DELAY_IN_SECONDS = "kairosdb.datastore.cassandra.hector.autoDiscoveryDelayInSeconds";
-	private static final String AUTO_DISCOVERY_DATA_CENTERS = "kairosdb.datastore.cassandra.hector.autoDiscoveryDataCenters";
-	private static final String RUN_AUTO_DISCOVERY_AT_STARTUP = "kairosdb.datastore.cassandra.hector.runAutoDiscoveryAtStartup";
-	private static final String USE_HOST_TIME_OUT_TRACKER = "kairosdb.datastore.cassandra.hector.useHostTimeoutTracker";
-	private static final String MAX_FRAME_SIZE = "kairosdb.datastore.cassandra.hector.maxFrameSize";
-	private static final String LOAD_BALANCING_POLICY = "kairosdb.datastore.cassandra.hector.loadBalancingPolicy";
-	private static final String HOST_TIME_OUT_COUNTER = "kairosdb.datastore.cassandra.hector.hostTimeoutCounter";
-	private static final String HOST_TIME_OUT_WINDOW = "kairosdb.datastore.cassandra.hector.hostTimeoutWindow";
-	private static final String HOST_TIME_OUT_SUSPENSION_DURATION_IN_SECONDS = "kairosdb.datastore.cassandra.hector.hostTimeoutSuspensionDurationInSeconds";
-	private static final String HOST_TIME_OUT_UNSUSPEND_CHECK_DELAY = "kairosdb.datastore.cassandra.hector.hostTimeoutUnsuspendCheckDelay";
-	private static final String MAX_CONNECT_TIME_MILLIS = "kairosdb.datastore.cassandra.hector.maxConnectTimeMillis";
-	private static final String MAX_LAST_SUCCESS_TIME_MILLIS = "kairosdb.datastore.cassandra.hector.maxLastSuccessTimeMillis";
-
-	private CassandraHostConfigurator hostConfig;
-
-
-	@Inject
-	public HectorConfiguration(@Named(HOST_LIST_PROPERTY) String cassandraHostList)
-	{
-		hostConfig = new CassandraHostConfigurator(cassandraHostList);
-	}
-
-	public CassandraHostConfigurator getConfiguration()
-	{
-		return hostConfig;
-	}
-
-	@Inject(optional = true)
-	public void setMaxActive(@Named(MAX_ACTIVE) int maxActive)
-	{
-		hostConfig.setMaxActive(maxActive);
-	}
-
-	@Inject(optional = true)
-	public void setMaxWaitTimeWhenExhausted(@Named(MAX_WAIT_TIME_WHEN_EXHAUSTED) long maxWaitTimeWhenExhausted)
-	{
-		hostConfig.setMaxWaitTimeWhenExhausted(maxWaitTimeWhenExhausted);
-	}
-
-	@Inject(optional = true)
-	public void setUseSocketKeepalive(@Named(USE_SOCKET_KEEP_ALIVE) boolean useSocketKeepalive)
-	{
-		hostConfig.setUseSocketKeepalive(useSocketKeepalive);
-	}
-
-	@Inject(optional = true)
-	public void setCassandraThriftSocketTimeout(@Named(CASSANDRA_THRIFT_SOCKET_TIMEOUT) int cassandraThriftSocketTimeout)
-	{
-		hostConfig.setCassandraThriftSocketTimeout(cassandraThriftSocketTimeout);
-	}
-
-	@Inject(optional = true)
-	public void setRetryDownedHosts(@Named(RETRY_DOWNED_HOSTS) boolean retryDownedHosts)
-	{
-		hostConfig.setRetryDownedHosts(retryDownedHosts);
-	}
-
-	@Inject(optional = true)
-	public void setRetryDownedHostsDelayInSeconds(@Named(RETRY_DOWNED_HOSTS_DELAY_IN_SECONDS)
-	                                              int retryDownedHostsDelayInSeconds)
-	{
-		hostConfig.setRetryDownedHostsDelayInSeconds(retryDownedHostsDelayInSeconds);
-	}
-
-	@Inject(optional = true)
-	public void setRetryDownedHostsQueueSize(@Named(RETRY_DOWNED_HOSTS_QUEUE_SIZE) int retryDownedHostsQueueSize)
-	{
-		hostConfig.setRetryDownedHostsQueueSize(retryDownedHostsQueueSize);
-	}
-
-	@Inject(optional = true)
-	public void setAutoDiscoverHosts(@Named(AUTO_DISCOVER_HOSTS) boolean autoDiscoverHosts)
-	{
-		hostConfig.setAutoDiscoverHosts(autoDiscoverHosts);
-	}
-
-	@Inject(optional = true)
-	public void setAutoDiscoveryDelayInSeconds(@Named(AUTO_DISCOVER_DELAY_IN_SECONDS) int autoDiscoveryDelayInSeconds)
-	{
-		hostConfig.setAutoDiscoveryDelayInSeconds(autoDiscoveryDelayInSeconds);
-	}
-
-	@Inject(optional = true)
-	public void setAutoDiscoveryDataCenters(@Named(AUTO_DISCOVERY_DATA_CENTERS) List<String> autoDiscoveryDataCenters)
-	{
-		hostConfig.setAutoDiscoveryDataCenter(autoDiscoveryDataCenters);
-	}
-
-	@Inject(optional = true)
-	public void setRunAutoDiscoveryAtStartup(@Named(RUN_AUTO_DISCOVERY_AT_STARTUP) boolean runAutoDiscoveryAtStartup)
-	{
-		hostConfig.setRunAutoDiscoveryAtStartup(runAutoDiscoveryAtStartup);
-	}
-
-	@Inject(optional = true)
-	public void setUseHostTimeoutTracker(@Named(USE_HOST_TIME_OUT_TRACKER) boolean useHostTimeoutTracker)
-	{
-		hostConfig.setUseHostTimeoutTracker(useHostTimeoutTracker);
-	}
-
-	@Inject(optional = true)
-	public void setMaxFrameSize(@Named(MAX_FRAME_SIZE) int maxFrameSize)
-	{
-		hostConfig.setMaxFrameSize(maxFrameSize);
-	}
-
-	@Inject(optional = true)
-	public void setLoadBalancingPolicy(@Named(LOAD_BALANCING_POLICY) String loadBalancingPolicy)
-	{
-		if (loadBalancingPolicy.equals("dynamic"))
-			hostConfig.setLoadBalancingPolicy(new DynamicLoadBalancingPolicy());
-		else if (loadBalancingPolicy.equals("leastActive"))
-			hostConfig.setLoadBalancingPolicy(new LeastActiveBalancingPolicy());
-		else if (loadBalancingPolicy.equals("roundRobin"))
-			hostConfig.setLoadBalancingPolicy(new RoundRobinBalancingPolicy());
-		else
-			throw new IllegalArgumentException("Invalid load balancing policy. Must be dynamic, leastActive, or roundRobin");
-	}
-
-	@Inject(optional = true)
-	public void setHostTimeOutCounter(@Named(HOST_TIME_OUT_COUNTER) int hostTimeoutCounter)
-	{
-		hostConfig.setHostTimeoutCounter(hostTimeoutCounter);
-	}
-
-	@Inject(optional = true)
-	public void setHostTimeoutWindow(@Named(HOST_TIME_OUT_WINDOW) int hostTimeoutWindow)
-	{
-		hostConfig.setHostTimeoutWindow(hostTimeoutWindow);
-	}
-
-	@Inject(optional = true)
-	public void setHostTimeOutSuspensionDurationInSeconds(@Named(HOST_TIME_OUT_SUSPENSION_DURATION_IN_SECONDS)
-	                                                      int hostTimeoutSuspensionDurationInSeconds)
-	{
-		hostConfig.setHostTimeoutSuspensionDurationInSeconds(hostTimeoutSuspensionDurationInSeconds);
-	}
-
-	@Inject(optional = true)
-	public void setHostTimeOutUnsuspendCheckDelay(@Named(HOST_TIME_OUT_UNSUSPEND_CHECK_DELAY)
-	                                              int hostTimeoutUnsuspendCheckDelay)
-	{
-		hostConfig.setHostTimeoutUnsuspendCheckDelay(hostTimeoutUnsuspendCheckDelay);
-	}
-
-	@Inject(optional = true)
-	public void setMaxConnectTimeMillis(@Named(MAX_CONNECT_TIME_MILLIS) long maxConnectTimeMillis)
-	{
-		hostConfig.setMaxConnectTimeMillis(maxConnectTimeMillis);
-	}
-
-	@Inject(optional = true)
-	public void setMaxLastSuccessTimeMillis(@Named(MAX_LAST_SUCCESS_TIME_MILLIS) long maxLastSuccessTimeMillis)
-	{
-		hostConfig.setMaxLastSuccessTimeMillis(maxLastSuccessTimeMillis);
-	}
-}
\ No newline at end of file
diff --git a/src/main/java/org/kairosdb/datastore/cassandra/QueryRunner.java b/src/main/java/org/kairosdb/datastore/cassandra/QueryRunner.java
deleted file mode 100755
index 776c65b776..0000000000
--- a/src/main/java/org/kairosdb/datastore/cassandra/QueryRunner.java
+++ /dev/null
@@ -1,204 +0,0 @@
-/*
- * Copyright 2016 KairosDB Authors
- *
- *    Licensed under the Apache License, Version 2.0 (the "License");
- *    you may not use this file except in compliance with the License.
- *    You may obtain a copy of the License at
- *
- *        http://www.apache.org/licenses/LICENSE-2.0
- *
- *    Unless required by applicable law or agreed to in writing, software
- *    distributed under the License is distributed on an "AS IS" BASIS,
- *    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- *    See the License for the specific language governing permissions and
- *    limitations under the License.
- */
-package org.kairosdb.datastore.cassandra;
-
-import me.prettyprint.cassandra.serializers.ByteBufferSerializer;
-import me.prettyprint.cassandra.serializers.BytesArraySerializer;
-import me.prettyprint.cassandra.serializers.IntegerSerializer;
-import me.prettyprint.hector.api.Keyspace;
-import me.prettyprint.hector.api.beans.HColumn;
-import me.prettyprint.hector.api.beans.Row;
-import me.prettyprint.hector.api.beans.Rows;
-import me.prettyprint.hector.api.factory.HFactory;
-import me.prettyprint.hector.api.query.MultigetSliceQuery;
-import me.prettyprint.hector.api.query.SliceQuery;
-import org.kairosdb.core.KairosDataPointFactory;
-import org.kairosdb.core.datapoints.*;
-import org.kairosdb.core.datastore.CachedSearchResult;
-import org.kairosdb.core.datastore.Order;
-import org.kairosdb.core.datastore.QueryCallback;
-import org.kairosdb.util.KDataInput;
-
-import java.io.IOException;
-import java.nio.ByteBuffer;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Map;
-
-import static org.kairosdb.datastore.cassandra.CassandraDatastore.*;
-
-public class QueryRunner
-{
-	public static final DataPointsRowKeySerializer ROW_KEY_SERIALIZER = new DataPointsRowKeySerializer();
-
-	private Keyspace m_keyspace;
-	private String m_columnFamily;
-	private List<DataPointsRowKey> m_rowKeys;
-	private int m_startTime; //relative row time
-	private int m_endTime; //relative row time
-	private QueryCallback m_queryCallback;
-	private int m_singleRowReadSize;
-	private int m_multiRowReadSize;
-	private boolean m_limit = false;
-	private boolean m_descending = false;
-	private LongDataPointFactory m_longDataPointFactory = new LongDataPointFactoryImpl();
-	private DoubleDataPointFactory m_doubleDataPointFactory = new DoubleDataPointFactoryImpl();
-
-	private final KairosDataPointFactory m_kairosDataPointFactory;
-
-	public QueryRunner(Keyspace keyspace, String columnFamily,
-			KairosDataPointFactory kairosDataPointFactory,
-			List<DataPointsRowKey> rowKeys, long startTime, long endTime,
-			QueryCallback csResult,
-			int singleRowReadSize, int multiRowReadSize, int limit, Order order)
-	{
-		m_keyspace = keyspace;
-		m_columnFamily = columnFamily;
-		m_rowKeys = rowKeys;
-		m_kairosDataPointFactory = kairosDataPointFactory;
-		long m_tierRowTime = rowKeys.get(0).getTimestamp();
-		if (startTime < m_tierRowTime)
-			m_startTime = 0;
-		else
-			m_startTime = getColumnName(m_tierRowTime, startTime);
-
-		if (endTime > (m_tierRowTime + ROW_WIDTH))
-			m_endTime = getColumnName(m_tierRowTime, m_tierRowTime + ROW_WIDTH) +1;
-		else
-			m_endTime = getColumnName(m_tierRowTime, endTime) +1; //add 1 so we get 0x1 for last bit
-
-		m_queryCallback = csResult;
-		m_singleRowReadSize = singleRowReadSize;
-		m_multiRowReadSize = multiRowReadSize;
-
-		if (limit != 0)
-		{
-			m_limit = true;
-			m_singleRowReadSize = limit;
-			m_multiRowReadSize = limit;
-		}
-
-		if (order == Order.DESC)
-			m_descending = true;
-	}
-
-	public void runQuery() throws IOException
-	{
-		MultigetSliceQuery<DataPointsRowKey, Integer, byte[]> msliceQuery =
-				HFactory.createMultigetSliceQuery(m_keyspace,
-						ROW_KEY_SERIALIZER,
-						IntegerSerializer.get(), BytesArraySerializer.get());
-
-		msliceQuery.setColumnFamily(m_columnFamily);
-		msliceQuery.setKeys(m_rowKeys);
-		if (m_descending)
-			msliceQuery.setRange(m_endTime, m_startTime, true, m_multiRowReadSize);
-		else
-			msliceQuery.setRange(m_startTime, m_endTime, false, m_multiRowReadSize);
-
-		Rows<DataPointsRowKey, Integer, byte[]> rows =
-				msliceQuery.execute().get();
-
-		List<Row<DataPointsRowKey, Integer, byte[]>> unfinishedRows =
-				new ArrayList<Row<DataPointsRowKey, Integer, byte[]>>();
-
-		for (Row<DataPointsRowKey, Integer, byte[]> row : rows)
-		{
-			List<HColumn<Integer, byte[]>> columns = row.getColumnSlice().getColumns();
-			if (!m_limit && columns.size() == m_multiRowReadSize)
-				unfinishedRows.add(row);
-
-			writeColumns(row.getKey(), columns);
-		}
-
-
-		//Iterate through the unfinished rows and get the rest of the data.
-		//todo: use multiple threads to retrieve this data
-		for (Row<DataPointsRowKey, Integer, byte[]> unfinishedRow : unfinishedRows)
-		{
-			DataPointsRowKey key = unfinishedRow.getKey();
-
-			SliceQuery<DataPointsRowKey, Integer, byte[]> sliceQuery =
-					HFactory.createSliceQuery(m_keyspace, ROW_KEY_SERIALIZER,
-					IntegerSerializer.get(), BytesArraySerializer.get());
-
-			sliceQuery.setColumnFamily(m_columnFamily);
-			sliceQuery.setKey(key);
-
-			List<HColumn<Integer, byte[]>> columns = unfinishedRow.getColumnSlice().getColumns();
-
-			do
-			{
-				Integer lastTime = columns.get(columns.size() -1).getName();
-
-				if (m_descending)
-					sliceQuery.setRange(lastTime-1, m_startTime, true, m_singleRowReadSize);
-				else
-					sliceQuery.setRange(lastTime+1, m_endTime, false, m_singleRowReadSize);
-
-				columns = sliceQuery.execute().get().getColumns();
-				writeColumns(key, columns);
-			} while (columns.size() == m_singleRowReadSize);
-		}
-	}
-
-
-	private void writeColumns(DataPointsRowKey rowKey, List<HColumn<Integer, byte[]>> columns)
-			throws IOException
-	{
-		if (columns.size() != 0)
-		{
-			Map<String, String> tags = rowKey.getTags();
-			String type = rowKey.getDataType();
-
-			m_queryCallback.startDataPointSet(type, tags);
-
-			DataPointFactory dataPointFactory = null;
-			dataPointFactory = m_kairosDataPointFactory.getFactoryForDataStoreType(type);
-
-			for (HColumn<Integer, byte[]> column : columns)
-			{
-				int columnTime = column.getName();
-
-				byte[] value = column.getValue();
-				long timestamp = getColumnTimestamp(rowKey.getTimestamp(), columnTime);
-
-				//If type is legacy type it will point to the same object, no need for equals
-				if (type == LegacyDataPointFactory.DATASTORE_TYPE)
-				{
-					if (isLongValue(columnTime))
-					{
-						m_queryCallback.addDataPoint(
-								new LegacyLongDataPoint(timestamp,
-										ValueSerializer.getLongFromByteBuffer(ByteBuffer.wrap(value))));
-					}
-					else
-					{
-						m_queryCallback.addDataPoint(
-								new LegacyDoubleDataPoint(timestamp,
-										ValueSerializer.getDoubleFromByteBuffer(ByteBuffer.wrap(value))));
-					}
-				}
-				else
-				{
-					m_queryCallback.addDataPoint(
-							dataPointFactory.getDataPoint(timestamp, KDataInput.createInput(value)));
-				}
-			}
-		}
-	}
-
-}
diff --git a/src/main/java/org/kairosdb/datastore/cassandra/WriteBuffer.java b/src/main/java/org/kairosdb/datastore/cassandra/WriteBuffer.java
deleted file mode 100755
index 65bbb3b407..0000000000
--- a/src/main/java/org/kairosdb/datastore/cassandra/WriteBuffer.java
+++ /dev/null
@@ -1,340 +0,0 @@
-/*
- * Copyright 2016 KairosDB Authors
- *
- *    Licensed under the Apache License, Version 2.0 (the "License");
- *    you may not use this file except in compliance with the License.
- *    You may obtain a copy of the License at
- *
- *        http://www.apache.org/licenses/LICENSE-2.0
- *
- *    Unless required by applicable law or agreed to in writing, software
- *    distributed under the License is distributed on an "AS IS" BASIS,
- *    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- *    See the License for the specific language governing permissions and
- *    limitations under the License.
- */
-package org.kairosdb.datastore.cassandra;
-
-import com.google.common.util.concurrent.ThreadFactoryBuilder;
-import me.prettyprint.cassandra.model.HColumnImpl;
-import me.prettyprint.cassandra.model.MutatorImpl;
-import me.prettyprint.hector.api.Keyspace;
-import me.prettyprint.hector.api.Serializer;
-import me.prettyprint.hector.api.mutation.Mutator;
-import org.kairosdb.util.Triple;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.util.ArrayList;
-import java.util.List;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.Executors;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.locks.ReentrantLock;
-
-
-public class WriteBuffer<RowKeyType, ColumnKeyType, ValueType>  implements Runnable
-{
-	public static final Logger logger = LoggerFactory.getLogger(WriteBuffer.class);
-
-	private Keyspace m_keyspace;
-	private String m_cfName;
-	private List<Triple<RowKeyType, ColumnKeyType, ValueType>> m_buffer;
-	private Mutator<RowKeyType> m_mutator;
-	private volatile int m_bufferCount = 0;
-	private ReentrantLock m_mutatorLock;
-
-	private Thread m_writeThread;
-	private boolean m_exit = false;
-	private int m_writeDelay;
-	private Serializer<RowKeyType> m_rowKeySerializer;
-	private Serializer<ColumnKeyType> m_columnKeySerializer;
-	private Serializer<ValueType> m_valueSerializer;
-	private WriteBufferStats m_writeStats;
-	private int m_maxBufferSize;
-	private int m_initialMaxBufferSize;
-	private ExecutorService m_executorService;
-
-	public WriteBuffer(Keyspace keyspace, String cfName,
-			int writeDelay, int maxWriteSize, Serializer<RowKeyType> keySerializer,
-			Serializer<ColumnKeyType> columnKeySerializer,
-			Serializer<ValueType> valueSerializer,
-			WriteBufferStats stats,
-			ReentrantLock mutatorLock,
-			int threadCount)
-	{
-		m_executorService = Executors.newFixedThreadPool(threadCount,
-				new ThreadFactoryBuilder().setNameFormat("WriteBuffer-"+cfName+"-%d").build());
-
-		m_keyspace = keyspace;
-		m_cfName = cfName;
-		m_writeDelay = writeDelay;
-		m_initialMaxBufferSize = m_maxBufferSize = maxWriteSize;
-		m_rowKeySerializer = keySerializer;
-		m_columnKeySerializer = columnKeySerializer;
-		m_valueSerializer = valueSerializer;
-		m_writeStats = stats;
-		m_mutatorLock = mutatorLock;
-
-		m_buffer = new ArrayList<>();
-		m_mutator = new MutatorImpl<>(keyspace, keySerializer);
-		m_writeThread = new Thread(this, "WriteBuffer Scheduler for "+cfName);
-		m_writeThread.start();
-	}
-	
-	/**
-	 * Add a datapoint without a TTL. 
-	 * This datapoint will never be automatically deleted
-	 */
-	public void addData(
-		RowKeyType rowKey, 
-		ColumnKeyType columnKey, 
-		ValueType value, 
-		long timestamp)
-	{
-		addData(rowKey, columnKey, value, timestamp, 0);
-	}
-
-	/**
-	 * Add a datapoint with a TTL.
-	 * This datapoint will be removed after ttl seconds
-	 */
-	public void addData(
-			RowKeyType rowKey,
-			ColumnKeyType columnKey,
-			ValueType value,
-			long timestamp,
-			int ttl)
-	{
-		m_mutatorLock.lock();
-		try
-		{
-			waitOnBufferFull();
-			m_bufferCount ++;
-
-			if (columnKey.toString().length() > 0)
-			{
-				m_buffer.add(new Triple<>(rowKey, columnKey, value, timestamp, ttl));
-			}
-			else
-			{
-				logger.info("Discarded " + m_cfName + " row with empty column name. This should never happen.");
-			}
-		}
-		finally
-		{
-			m_mutatorLock.unlock();
-		}
-	}
-
-	public void deleteRow(RowKeyType rowKey, long timestamp)
-	{
-		m_mutatorLock.lock();
-		try
-		{
-			waitOnBufferFull();
-
-			m_bufferCount ++;
-			m_mutator.addDeletion(rowKey, m_cfName, timestamp);
-		}
-		finally
-		{
-			m_mutatorLock.unlock();
-		}
-	}
-
-	public void deleteColumn(RowKeyType rowKey, ColumnKeyType columnKey, long timestamp)
-	{
-		m_mutatorLock.lock();
-		try
-		{
-			waitOnBufferFull();
-
-			m_bufferCount ++;
-			m_mutator.addDeletion(rowKey, m_cfName, columnKey, m_columnKeySerializer, timestamp);
-		}
-		finally
-		{
-			m_mutatorLock.unlock();
-		}
-	}
-
-	private void waitOnBufferFull()
-	{
-		if ((m_bufferCount > m_maxBufferSize) && (m_mutatorLock.getHoldCount() == 1))
-		{
-			submitJob();
-		}
-	}
-
-	public void close() throws InterruptedException
-	{
-		m_exit = true;
-		m_writeThread.interrupt();
-		m_writeThread.join();
-		m_executorService.shutdown();
-		m_executorService.awaitTermination(1, TimeUnit.MINUTES);
-	}
-
-	/**
-	 This will slowly increase the max buffer size up to the initial size.
-	 The design is that this method is called periodically to correct 3/4
-	 throttling that occurs down below.
-	 */
-	public void increaseMaxBufferSize()
-	{
-		if (m_maxBufferSize < m_initialMaxBufferSize)
-		{
-			m_maxBufferSize += 1000;
-			logger.info("Increasing write buffer " + m_cfName + " size to "+m_maxBufferSize);
-		}
-	}
-
-	private void submitJob()
-	{
-		Mutator<RowKeyType> pendingMutations;
-		List<Triple<RowKeyType, ColumnKeyType, ValueType>> buffer;
-
-		m_writeStats.saveWriteSize(m_bufferCount);
-
-		pendingMutations = m_mutator;
-		buffer = m_buffer;
-		m_mutator = new MutatorImpl<>(m_keyspace, m_rowKeySerializer);
-		m_buffer = new ArrayList<>();
-		m_bufferCount = 0;
-
-		WriteDataJob writeDataJob = new WriteDataJob(pendingMutations, buffer);
-		//submit job
-		m_executorService.submit(writeDataJob);
-		writeDataJob.waitTillStarted();
-	}
-
-
-	@Override
-	public void run()
-	{
-		while (!m_exit)
-		{
-			try
-			{
-				Thread.sleep(m_writeDelay);
-			}
-			catch (InterruptedException ignored) {}
-
-			if (m_bufferCount != 0)
-			{
-				m_mutatorLock.lock();
-				try
-				{
-					submitJob();
-				}
-				finally
-				{
-					m_mutatorLock.unlock();
-				}
-			}
-		}
-	}
-
-	private class WriteDataJob implements Runnable
-	{
-		private final Object m_jobLock = new Object();
-		private boolean m_started = false;
-		private Mutator<RowKeyType> m_pendingMutations;
-		private final List<Triple<RowKeyType, ColumnKeyType, ValueType>> m_buffer;
-
-		public WriteDataJob(Mutator<RowKeyType> pendingMutations, List<Triple<RowKeyType, ColumnKeyType, ValueType>> buffer)
-		{
-			m_pendingMutations = pendingMutations;
-			m_buffer = buffer;
-		}
-
-		public void waitTillStarted()
-		{
-			synchronized (m_jobLock)
-			{
-				while (!m_started)
-				{
-					try
-					{
-						m_jobLock.wait();
-					}
-					catch (InterruptedException e)
-					{
-						e.printStackTrace();
-					}
-				}
-			}
-		}
-
-		@Override
-		public void run()
-		{
-			synchronized (m_jobLock)
-			{
-				m_started = true;
-				m_jobLock.notifyAll();
-			}
-
-			try
-			{
-				if (m_pendingMutations != null)
-				{
-					for (Triple<RowKeyType, ColumnKeyType, ValueType> data : m_buffer)
-					{
-						HColumnImpl<ColumnKeyType, ValueType> col =
-								new HColumnImpl<>(data.getSecond(), data.getThird(), data.getTime(), m_columnKeySerializer, m_valueSerializer);
-
-						//if a TTL is set apply it to the column. This will
-						//cause it to be removed after this number of seconds
-						if (data.getTtl() != 0)
-						{
-							col.setTtl(data.getTtl());
-						}
-
-						m_pendingMutations.addInsertion(
-								data.getFirst(),
-								m_cfName,
-								col
-						);
-					}
-					m_pendingMutations.execute();
-				}
-
-				m_pendingMutations = null;
-			}
-			catch (Exception e)
-			{
-				logger.error("Error sending data to Cassandra (" + m_cfName + ")", e);
-
-				m_maxBufferSize = m_maxBufferSize * 3 / 4;
-
-				logger.error("Reducing write buffer size to " + m_maxBufferSize +
-						".  You need to increase your cassandra capacity or change the kairosdb.datastore.cassandra.write_buffer_max_size property.");
-			}
-
-
-			//If the batch failed we will retry it without changing the buffer size.
-			while (m_pendingMutations != null)
-			{
-				try
-				{
-					Thread.sleep(100);
-				}
-				catch (InterruptedException ignored)
-				{
-				}
-
-				try
-				{
-					m_pendingMutations.execute();
-					m_pendingMutations = null;
-				}
-				catch (Exception e)
-				{
-					logger.error("Error resending data", e);
-				}
-			}
-		}
-	}
-}
diff --git a/src/main/java/org/kairosdb/datastore/h2/H2Datastore.java b/src/main/java/org/kairosdb/datastore/h2/H2Datastore.java
index d91ee2a983..b7a8822cb3 100755
--- a/src/main/java/org/kairosdb/datastore/h2/H2Datastore.java
+++ b/src/main/java/org/kairosdb/datastore/h2/H2Datastore.java
@@ -17,6 +17,7 @@
 package org.kairosdb.datastore.h2;
 
 import com.google.common.collect.ImmutableSortedMap;
+import com.google.common.eventbus.EventBus;
 import com.google.common.eventbus.Subscribe;
 import com.google.inject.Inject;
 import com.google.inject.name.Named;
@@ -26,9 +27,11 @@
 import org.kairosdb.core.KairosDataPointFactory;
 import org.kairosdb.core.datastore.*;
 import org.kairosdb.core.exception.DatastoreException;
+import org.kairosdb.datastore.cassandra.DataPointsRowKey;
 import org.kairosdb.datastore.h2.orm.*;
 import org.kairosdb.datastore.h2.orm.DataPoint;
 import org.kairosdb.events.DataPointEvent;
+import org.kairosdb.events.RowKeyEvent;
 import org.kairosdb.util.KDataInput;
 import org.kairosdb.util.KDataOutput;
 import org.slf4j.Logger;
@@ -50,13 +53,16 @@
 	public static final String DATABASE_PATH_PROPERTY = "kairosdb.datastore.h2.database_path";
 
 	private Connection m_holdConnection;  //Connection that holds the database open
-	private KairosDataPointFactory m_dataPointFactory;
+	private final KairosDataPointFactory m_dataPointFactory;
+	private final EventBus m_eventBus;
 
 	@Inject
 	public H2Datastore(@Named(DATABASE_PATH_PROPERTY) String dbPath, 
-			KairosDataPointFactory dataPointFactory) throws DatastoreException
+			KairosDataPointFactory dataPointFactory,
+			EventBus eventBus) throws DatastoreException
 	{
 		m_dataPointFactory = dataPointFactory;
+		m_eventBus = eventBus;
 		boolean createDB = false;
 
 		File dataDir = new File(dbPath);
@@ -164,6 +170,10 @@ public synchronized void putDataPoint(DataPointEvent event) throws DatastoreExce
 				}
 
 				GenOrmDataSource.flush();
+				DataPointsRowKey dataPointsRowKey = new DataPointsRowKey(metricName,
+						0, dataPoint.getDataStoreDataType(), tags);
+				m_eventBus.post(new RowKeyEvent(metricName, dataPointsRowKey, 0));
+
 			}
 
 			KDataOutput dataOutput = new KDataOutput();
@@ -420,6 +430,96 @@ public TagSet queryMetricTags(DatastoreMetricQuery query) throws DatastoreExcept
 		return tagSet;
 	}
 
+	@Override
+	public void setValue(String service, String serviceKey, String key, String value) throws DatastoreException
+	{
+		GenOrmDataSource.attachAndBegin();
+		try
+		{
+			ServiceIndex serviceIndex = ServiceIndex.factory.create(service, serviceKey, key);
+			if (value != null)
+				serviceIndex.setValue(value);
+
+			GenOrmDataSource.commit();
+		}
+		finally
+		{
+			GenOrmDataSource.close();
+		}
+	}
+
+	@Override
+	public String getValue(String service, String serviceKey, String key) throws DatastoreException
+	{
+		ServiceIndex serviceIndex = ServiceIndex.factory.find(service, serviceKey, key);
+		if (serviceIndex != null)
+			return serviceIndex.getValue();
+		else
+			return null;
+	}
+
+	@Override
+	public Iterable<String> listKeys(String service, String serviceKey) throws DatastoreException
+	{
+		final ServiceIndex_base.ResultSet keys = ServiceIndex.factory.getKeys(service, serviceKey);
+
+		return new Iterable<String>()
+		{
+			@Override
+			public Iterator<String> iterator()
+			{
+				return new Iterator<String>()
+				{
+					@Override
+					public boolean hasNext()
+					{
+						return keys.next();
+					}
+
+					@Override
+					public String next()
+					{
+						return keys.getRecord().getKey();
+					}
+
+					@Override
+					public void remove() { }
+				};
+			}
+		};
+	}
+
+	@Override
+	public Iterable<String> listKeys(String service, String serviceKey, String keyStartsWith) throws DatastoreException
+	{
+		final ServiceIndex_base.ResultSet keys = ServiceIndex.factory.getKeysLike(service, serviceKey, keyStartsWith+"%");
+
+		return new Iterable<String>()
+		{
+			@Override
+			public Iterator<String> iterator()
+			{
+				return new Iterator<String>()
+				{
+					@Override
+					public boolean hasNext()
+					{
+						return keys.next();
+					}
+
+					@Override
+					public String next()
+					{
+						return keys.getRecord().getKey();
+					}
+
+					@Override
+					public void remove() { }
+				};
+			}
+		};
+	}
+
 	private String createMetricKey(String metricName, SortedMap<String, String> tags,
 			String type)
 	{
diff --git a/src/main/java/org/kairosdb/datastore/h2/orm/CountDataPointsForMetricQuery.java b/src/main/java/org/kairosdb/datastore/h2/orm/CountDataPointsForMetricQuery.java
index d7ac8d9580..bacca414fd 100755
--- a/src/main/java/org/kairosdb/datastore/h2/orm/CountDataPointsForMetricQuery.java
+++ b/src/main/java/org/kairosdb/datastore/h2/orm/CountDataPointsForMetricQuery.java
@@ -13,6 +13,8 @@
 import java.sql.Timestamp;
 import org.xml.sax.ContentHandler;
 import org.xml.sax.Attributes;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import org.agileclick.genorm.runtime.*;
 
 
@@ -117,7 +119,7 @@ public ResultSet runQuery(String metricId, java.sql.Timestamp startTime, java.sq
 			genorm_statement.setTimestamp(3, endTime);
 
 			long genorm_queryTimeStart = 0L;
-			if (s_logger.isInfo())
+			if (s_logger.isInfoEnabled())
 				{
 				genorm_queryTimeStart = System.currentTimeMillis();
 				}
@@ -127,7 +129,7 @@ public ResultSet runQuery(String metricId, java.sql.Timestamp startTime, java.sq
 			if (genorm_queryTimeStart != 0L)
 				{
 				long genorm_quryTime = System.currentTimeMillis() - genorm_queryTimeStart;
-				s_logger.info(genorm_quryTime);
+				s_logger.info(String.valueOf(genorm_quryTime));
 				}
 			
 			ResultSet genorm_ret = new SQLResultSet(genorm_resultSet, genorm_statement, genorm_query);
@@ -160,7 +162,7 @@ public ResultSet runQuery()
 			genorm_statement.setTimestamp(3, m_endTime);
 
 			long genorm_queryTimeStart = 0L;
-			if (s_logger.isInfo())
+			if (s_logger.isInfoEnabled())
 				{
 				genorm_queryTimeStart = System.currentTimeMillis();
 				}
@@ -170,7 +172,7 @@ public ResultSet runQuery()
 			if (genorm_queryTimeStart != 0L)
 				{
 				long genorm_quryTime = System.currentTimeMillis() - genorm_queryTimeStart;
-				s_logger.info(genorm_quryTime);
+				s_logger.info(String.valueOf(genorm_quryTime));
 				}
 			
 			ResultSet genorm_ret = new SQLResultSet(genorm_resultSet, genorm_statement, genorm_query);
diff --git a/src/main/java/org/kairosdb/datastore/h2/orm/DataPoint_base.java b/src/main/java/org/kairosdb/datastore/h2/orm/DataPoint_base.java
index e07aaafa7d..39052f46c1 100755
--- a/src/main/java/org/kairosdb/datastore/h2/orm/DataPoint_base.java
+++ b/src/main/java/org/kairosdb/datastore/h2/orm/DataPoint_base.java
@@ -1,10 +1,9 @@
 package org.kairosdb.datastore.h2.orm;
 
+import java.util.*;
 import org.agileclick.genorm.runtime.*;
-
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 /**
 	This class has been automatically generated by GenORMous.  This file
@@ -51,12 +50,10 @@
 		public ResultSet getForMetricId(String metricId, java.sql.Timestamp startTime, java.sql.Timestamp endTime, String order);/**
 		*/
 		public ResultSet getForMetricIdWithLimit(String metricId, java.sql.Timestamp startTime, java.sql.Timestamp endTime, int limit, String order);/**
-		 Check for at least a single data point for a given metric id
+			Check for at least a single data point for a given metric id
+		*/
+		public DataPoint getWithMetricId(String metricId);/**
 		*/
-		public DataPoint getWithMetricId(String metricId);
-
-			/**
-			 */
 		public ResultSet getByMetric(String metricId);
 		}
 	
@@ -362,7 +359,7 @@ public ResultSet getForMetricId(String metricId, java.sql.Timestamp startTime, j
 					}
 				catch (java.sql.SQLException sqle2) { }
 					
-				if (s_logger.isDebug())
+				if (s_logger.isDebugEnabled())
 					sqle.printStackTrace();
 				throw new GenOrmException(sqle);
 				}
@@ -400,7 +397,7 @@ public ResultSet getForMetricIdWithLimit(String metricId, java.sql.Timestamp sta
 					}
 				catch (java.sql.SQLException sqle2) { }
 					
-				if (s_logger.isDebug())
+				if (s_logger.isDebugEnabled())
 					sqle.printStackTrace();
 				throw new GenOrmException(sqle);
 				}
@@ -408,46 +405,43 @@ public ResultSet getForMetricIdWithLimit(String metricId, java.sql.Timestamp sta
 			
 		//---------------------------------------------------------------------------
 		/**
-		 Check for at least a single data point for a given metric id
+			Check for at least a single data point for a given metric id
 		*/
 		public DataPoint getWithMetricId(String metricId)
-		{
-			String query = SELECT + "from data_point this\n				where\n				this.\"metric_id\" = ?\n				limit 1";
-
+			{
+			String query = SELECT+"from data_point this\n				where\n				this.\"metric_id\" = ?\n				limit 1";
+			
 			java.sql.PreparedStatement genorm_statement = null;
-
+			
 			try
-			{
+				{
 				genorm_statement = GenOrmDataSource.prepareStatement(query);
 				genorm_statement.setString(1, metricId);
-
+				
 				s_logger.debug(genorm_statement.toString());
-
+				
 				ResultSet rs = new SQLResultSet(genorm_statement.executeQuery(), query, genorm_statement);
-
+				
 				return (rs.getOnlyRecord());
-			}
+				}
 			catch (java.sql.SQLException sqle)
-			{
-				try
 				{
+				try
+					{
 					if (genorm_statement != null)
 						genorm_statement.close();
-				}
-				catch (java.sql.SQLException sqle2)
-				{
-				}
-
-				if (s_logger.isDebug())
+					}
+				catch (java.sql.SQLException sqle2) { }
+					
+				if (s_logger.isDebugEnabled())
 					sqle.printStackTrace();
 				throw new GenOrmException(sqle);
+				}
 			}
-		}
-
-			//---------------------------------------------------------------------------
-
-			/**
-			 */
+			
+		//---------------------------------------------------------------------------
+		/**
+		*/
 		public ResultSet getByMetric(String metricId)
 			{
 			String query = SELECT+"FROM data_point this WHERE this.\"metric_id\" = ?";
@@ -474,7 +468,7 @@ public ResultSet getByMetric(String metricId)
 					}
 				catch (java.sql.SQLException sqle2) { }
 					
-				if (s_logger.isDebug())
+				if (s_logger.isDebugEnabled())
 					sqle.printStackTrace();
 				throw new GenOrmException(sqle);
 				}
@@ -495,8 +489,8 @@ public void testQueryMethods()
 			System.out.println("DataPoint.getForMetricIdWithLimit");
 			rs = getForMetricIdWithLimit("foo", new java.sql.Timestamp(0L), new java.sql.Timestamp(0L), 10, "asc");
 			rs.close();
-				System.out.println("DataPoint.getWithMetricId");
-				getWithMetricId("foo");
+			System.out.println("DataPoint.getWithMetricId");
+			getWithMetricId("foo");
 
 
 			}
@@ -824,7 +818,7 @@ protected void initialize(java.sql.ResultSet rs)
 		{
 		try
 			{
-			if (s_logger.isDebug())
+			if (s_logger.isDebugEnabled())
 				{
 				java.sql.ResultSetMetaData meta = rs.getMetaData();
 				for (int I = 1; I <= meta.getColumnCount(); I++)
@@ -853,13 +847,13 @@ protected void initialize(java.sql.ResultSet rs)
 		
 
 		m_metricId = new GenOrmString(METRIC_ID_FIELD_META);
-		addField(m_metricId);
+		addField(COL_METRIC_ID, m_metricId);
 
 		m_timestamp = new GenOrmTimestamp(TIMESTAMP_FIELD_META);
-		addField(m_timestamp);
+		addField(COL_TIMESTAMP, m_timestamp);
 
 		m_value = new GenOrmBinary(VALUE_FIELD_META);
-		addField(m_value);
+		addField(COL_VALUE, m_value);
 
 		GenOrmRecordKey foreignKey;
 		foreignKey = new GenOrmRecordKey("metric");
diff --git a/src/main/java/org/kairosdb/datastore/h2/orm/DeleteMetricsQuery.java b/src/main/java/org/kairosdb/datastore/h2/orm/DeleteMetricsQuery.java
index 9522bcaf61..b652bc90de 100755
--- a/src/main/java/org/kairosdb/datastore/h2/orm/DeleteMetricsQuery.java
+++ b/src/main/java/org/kairosdb/datastore/h2/orm/DeleteMetricsQuery.java
@@ -13,6 +13,8 @@
 import java.sql.Timestamp;
 import org.xml.sax.ContentHandler;
 import org.xml.sax.Attributes;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import org.agileclick.genorm.runtime.*;
 
 
diff --git a/src/main/java/org/kairosdb/datastore/h2/orm/GenOrmUnitTest.java b/src/main/java/org/kairosdb/datastore/h2/orm/GenOrmUnitTest.java
index 89a79ef5b5..008ddeead2 100755
--- a/src/main/java/org/kairosdb/datastore/h2/orm/GenOrmUnitTest.java
+++ b/src/main/java/org/kairosdb/datastore/h2/orm/GenOrmUnitTest.java
@@ -5,6 +5,7 @@
 	public static void performUnitTests()
 		{
 		Metric.factory.testQueryMethods();
+		ServiceIndex.factory.testQueryMethods();
 		Tag.factory.testQueryMethods();
 		DataPoint.factory.testQueryMethods();
 		MetricTag.factory.testQueryMethods();
diff --git a/src/main/java/org/kairosdb/datastore/h2/orm/InsertDataPointQuery.java b/src/main/java/org/kairosdb/datastore/h2/orm/InsertDataPointQuery.java
index 4cb46ec820..7240f9513a 100755
--- a/src/main/java/org/kairosdb/datastore/h2/orm/InsertDataPointQuery.java
+++ b/src/main/java/org/kairosdb/datastore/h2/orm/InsertDataPointQuery.java
@@ -13,6 +13,8 @@
 import java.sql.Timestamp;
 import org.xml.sax.ContentHandler;
 import org.xml.sax.Attributes;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import org.agileclick.genorm.runtime.*;
 
 
diff --git a/src/main/java/org/kairosdb/datastore/h2/orm/MetricIdsQuery.java b/src/main/java/org/kairosdb/datastore/h2/orm/MetricIdsQuery.java
index 04b2683000..4538791071 100755
--- a/src/main/java/org/kairosdb/datastore/h2/orm/MetricIdsQuery.java
+++ b/src/main/java/org/kairosdb/datastore/h2/orm/MetricIdsQuery.java
@@ -13,6 +13,8 @@
 import java.sql.Timestamp;
 import org.xml.sax.ContentHandler;
 import org.xml.sax.Attributes;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import org.agileclick.genorm.runtime.*;
 
 
@@ -110,7 +112,7 @@ public ResultSet runQuery(String metricName)
 			genorm_statement.setString(1, metricName);
 
 			long genorm_queryTimeStart = 0L;
-			if (s_logger.isInfo())
+			if (s_logger.isInfoEnabled())
 				{
 				genorm_queryTimeStart = System.currentTimeMillis();
 				}
@@ -120,7 +122,7 @@ public ResultSet runQuery(String metricName)
 			if (genorm_queryTimeStart != 0L)
 				{
 				long genorm_quryTime = System.currentTimeMillis() - genorm_queryTimeStart;
-				s_logger.info(genorm_quryTime);
+				s_logger.info(String.valueOf(genorm_quryTime));
 				}
 			
 			ResultSet genorm_ret = new SQLResultSet(genorm_resultSet, genorm_statement, genorm_query);
@@ -151,7 +153,7 @@ public ResultSet runQuery()
 			genorm_statement.setString(1, m_metricName);
 
 			long genorm_queryTimeStart = 0L;
-			if (s_logger.isInfo())
+			if (s_logger.isInfoEnabled())
 				{
 				genorm_queryTimeStart = System.currentTimeMillis();
 				}
@@ -161,7 +163,7 @@ public ResultSet runQuery()
 			if (genorm_queryTimeStart != 0L)
 				{
 				long genorm_quryTime = System.currentTimeMillis() - genorm_queryTimeStart;
-				s_logger.info(genorm_quryTime);
+				s_logger.info(String.valueOf(genorm_quryTime));
 				}
 			
 			ResultSet genorm_ret = new SQLResultSet(genorm_resultSet, genorm_statement, genorm_query);
diff --git a/src/main/java/org/kairosdb/datastore/h2/orm/MetricIdsWithTagsQuery.java b/src/main/java/org/kairosdb/datastore/h2/orm/MetricIdsWithTagsQuery.java
index 52b670b173..dad1ab696b 100755
--- a/src/main/java/org/kairosdb/datastore/h2/orm/MetricIdsWithTagsQuery.java
+++ b/src/main/java/org/kairosdb/datastore/h2/orm/MetricIdsWithTagsQuery.java
@@ -13,6 +13,8 @@
 import java.sql.Timestamp;
 import org.xml.sax.ContentHandler;
 import org.xml.sax.Attributes;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import org.agileclick.genorm.runtime.*;
 
 
@@ -120,7 +122,7 @@ public ResultSet runQuery(String metricName, int tagCount, String tags)
 			genorm_statement.setInt(2, tagCount);
 
 			long genorm_queryTimeStart = 0L;
-			if (s_logger.isInfo())
+			if (s_logger.isInfoEnabled())
 				{
 				genorm_queryTimeStart = System.currentTimeMillis();
 				}
@@ -130,7 +132,7 @@ public ResultSet runQuery(String metricName, int tagCount, String tags)
 			if (genorm_queryTimeStart != 0L)
 				{
 				long genorm_quryTime = System.currentTimeMillis() - genorm_queryTimeStart;
-				s_logger.info(genorm_quryTime);
+				s_logger.info(String.valueOf(genorm_quryTime));
 				}
 			
 			ResultSet genorm_ret = new SQLResultSet(genorm_resultSet, genorm_statement, genorm_query);
@@ -165,7 +167,7 @@ public ResultSet runQuery()
 			genorm_statement.setInt(2, m_tagCount);
 
 			long genorm_queryTimeStart = 0L;
-			if (s_logger.isInfo())
+			if (s_logger.isInfoEnabled())
 				{
 				genorm_queryTimeStart = System.currentTimeMillis();
 				}
@@ -175,7 +177,7 @@ public ResultSet runQuery()
 			if (genorm_queryTimeStart != 0L)
 				{
 				long genorm_quryTime = System.currentTimeMillis() - genorm_queryTimeStart;
-				s_logger.info(genorm_quryTime);
+				s_logger.info(String.valueOf(genorm_quryTime));
 				}
 			
 			ResultSet genorm_ret = new SQLResultSet(genorm_resultSet, genorm_statement, genorm_query);
diff --git a/src/main/java/org/kairosdb/datastore/h2/orm/MetricNamesQuery.java b/src/main/java/org/kairosdb/datastore/h2/orm/MetricNamesQuery.java
index 09f8410f1f..6cb51d9c8a 100755
--- a/src/main/java/org/kairosdb/datastore/h2/orm/MetricNamesQuery.java
+++ b/src/main/java/org/kairosdb/datastore/h2/orm/MetricNamesQuery.java
@@ -13,6 +13,8 @@
 import java.sql.Timestamp;
 import org.xml.sax.ContentHandler;
 import org.xml.sax.Attributes;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import org.agileclick.genorm.runtime.*;
 
 
@@ -99,7 +101,7 @@ public ResultSet runQuery()
 			
 			genorm_statement = org.kairosdb.datastore.h2.orm.GenOrmDataSource.prepareStatement(genorm_query);
 			long genorm_queryTimeStart = 0L;
-			if (s_logger.isInfo())
+			if (s_logger.isInfoEnabled())
 				{
 				genorm_queryTimeStart = System.currentTimeMillis();
 				}
@@ -109,7 +111,7 @@ public ResultSet runQuery()
 			if (genorm_queryTimeStart != 0L)
 				{
 				long genorm_quryTime = System.currentTimeMillis() - genorm_queryTimeStart;
-				s_logger.info(genorm_quryTime);
+				s_logger.info(String.valueOf(genorm_quryTime));
 				}
 			
 			ResultSet genorm_ret = new SQLResultSet(genorm_resultSet, genorm_statement, genorm_query);
diff --git a/src/main/java/org/kairosdb/datastore/h2/orm/MetricTag_base.java b/src/main/java/org/kairosdb/datastore/h2/orm/MetricTag_base.java
index e679f75f66..339b35cc92 100755
--- a/src/main/java/org/kairosdb/datastore/h2/orm/MetricTag_base.java
+++ b/src/main/java/org/kairosdb/datastore/h2/orm/MetricTag_base.java
@@ -1,9 +1,9 @@
 package org.kairosdb.datastore.h2.orm;
 
+import java.util.*;
 import org.agileclick.genorm.runtime.*;
-
-import java.util.ArrayList;
-import java.util.List;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 /**
 	This class has been automatically generated by GenORMous.  This file
@@ -55,7 +55,7 @@
 	public static class MetricTagFactoryImpl //Inherit interfaces
 			implements MetricTagFactory 
 		{
-			public static final String CREATE_SQL = "CREATE CACHED TABLE metric_tag (\n	\"metric_id\" VARCHAR  NOT NULL,\n	\"tag_name\" VARCHAR  NOT NULL,\n	\"tag_value\" VARCHAR  NOT NULL,\n	PRIMARY KEY (\"metric_id\", \"tag_name\", \"tag_value\"),\n	CONSTRAINT metric_tag_metric_id_fkey FOREIGN KEY (\"metric_id\")\n		REFERENCES metric (\"id\") ON DELETE CASCADE,\n	CONSTRAINT metric_tag_tag_name_fkey FOREIGN KEY (\"tag_name\", \"tag_value\")\n		REFERENCES tag (\"name\", \"value\") \n	)";
+		public static final String CREATE_SQL = "CREATE CACHED TABLE metric_tag (\n	\"metric_id\" VARCHAR  NOT NULL,\n	\"tag_name\" VARCHAR  NOT NULL,\n	\"tag_value\" VARCHAR  NOT NULL,\n	PRIMARY KEY (\"metric_id\", \"tag_name\", \"tag_value\"),\n	CONSTRAINT metric_tag_metric_id_fkey FOREIGN KEY (\"metric_id\")\n		REFERENCES metric (\"id\") ON DELETE CASCADE,\n	CONSTRAINT metric_tag_tag_name_fkey FOREIGN KEY (\"tag_name\", \"tag_value\")\n		REFERENCES tag (\"name\", \"value\") \n	)";
 
 		private ArrayList<GenOrmFieldMeta> m_fieldMeta;
 		private ArrayList<GenOrmConstraint> m_foreignKeyConstraints;
@@ -68,7 +68,7 @@ protected MetricTagFactoryImpl()
 			m_fieldMeta.add(TAG_VALUE_FIELD_META);
 
 			m_foreignKeyConstraints = new ArrayList<GenOrmConstraint>();
-				m_foreignKeyConstraints.add(new GenOrmConstraint("metric", "metric_tag_metric_id_fkey", "CONSTRAINT metric_tag_metric_id_fkey FOREIGN KEY (\"metric_id\")\n	REFERENCES metric (\"id\") ON DELETE CASCADE"));
+			m_foreignKeyConstraints.add(new GenOrmConstraint("metric", "metric_tag_metric_id_fkey", "CONSTRAINT metric_tag_metric_id_fkey FOREIGN KEY (\"metric_id\")\n	REFERENCES metric (\"id\") ON DELETE CASCADE"));
 			m_foreignKeyConstraints.add(new GenOrmConstraint("tag", "metric_tag_tag_name_fkey", "CONSTRAINT metric_tag_tag_name_fkey FOREIGN KEY (\"tag_name\", \"tag_value\")\n	REFERENCES tag (\"name\", \"value\")"));
 
 			}
@@ -354,7 +354,7 @@ public ResultSet getByTag(String tagName, String tagValue)
 					}
 				catch (java.sql.SQLException sqle2) { }
 					
-				if (s_logger.isDebug())
+				if (s_logger.isDebugEnabled())
 					sqle.printStackTrace();
 				throw new GenOrmException(sqle);
 				}
@@ -389,7 +389,7 @@ public ResultSet getByMetric(String metricId)
 					}
 				catch (java.sql.SQLException sqle2) { }
 					
-				if (s_logger.isDebug())
+				if (s_logger.isDebugEnabled())
 					sqle.printStackTrace();
 				throw new GenOrmException(sqle);
 				}
@@ -743,7 +743,7 @@ protected void initialize(java.sql.ResultSet rs)
 		{
 		try
 			{
-			if (s_logger.isDebug())
+			if (s_logger.isDebugEnabled())
 				{
 				java.sql.ResultSetMetaData meta = rs.getMetaData();
 				for (int I = 1; I <= meta.getColumnCount(); I++)
@@ -772,13 +772,13 @@ protected void initialize(java.sql.ResultSet rs)
 		
 
 		m_metricId = new GenOrmString(METRIC_ID_FIELD_META);
-		addField(m_metricId);
+		addField(COL_METRIC_ID, m_metricId);
 
 		m_tagName = new GenOrmString(TAG_NAME_FIELD_META);
-		addField(m_tagName);
+		addField(COL_TAG_NAME, m_tagName);
 
 		m_tagValue = new GenOrmString(TAG_VALUE_FIELD_META);
-		addField(m_tagValue);
+		addField(COL_TAG_VALUE, m_tagValue);
 
 		GenOrmRecordKey foreignKey;
 		foreignKey = new GenOrmRecordKey("metric");
diff --git a/src/main/java/org/kairosdb/datastore/h2/orm/Metric_base.java b/src/main/java/org/kairosdb/datastore/h2/orm/Metric_base.java
index 1b1ad15e03..d2b35c3c53 100755
--- a/src/main/java/org/kairosdb/datastore/h2/orm/Metric_base.java
+++ b/src/main/java/org/kairosdb/datastore/h2/orm/Metric_base.java
@@ -2,6 +2,8 @@
 
 import java.util.*;
 import org.agileclick.genorm.runtime.*;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 /**
 	This class has been automatically generated by GenORMous.  This file
@@ -648,7 +650,7 @@ protected void initialize(java.sql.ResultSet rs)
 		{
 		try
 			{
-			if (s_logger.isDebug())
+			if (s_logger.isDebugEnabled())
 				{
 				java.sql.ResultSetMetaData meta = rs.getMetaData();
 				for (int I = 1; I <= meta.getColumnCount(); I++)
@@ -677,13 +679,13 @@ protected void initialize(java.sql.ResultSet rs)
 		
 
 		m_id = new GenOrmString(ID_FIELD_META);
-		addField(m_id);
+		addField(COL_ID, m_id);
 
 		m_name = new GenOrmString(NAME_FIELD_META);
-		addField(m_name);
+		addField(COL_NAME, m_name);
 
 		m_type = new GenOrmString(TYPE_FIELD_META);
-		addField(m_type);
+		addField(COL_TYPE, m_type);
 
 		GenOrmRecordKey foreignKey;
 		}
diff --git a/src/main/java/org/kairosdb/datastore/h2/orm/ServiceIndex.java b/src/main/java/org/kairosdb/datastore/h2/orm/ServiceIndex.java
new file mode 100755
index 0000000000..b35d718ce8
--- /dev/null
+++ b/src/main/java/org/kairosdb/datastore/h2/orm/ServiceIndex.java
@@ -0,0 +1,11 @@
+package org.kairosdb.datastore.h2.orm;
+
+/**
+	This class has been automatically generated by GenORMous.  This file is for
+	adding custom code to.  This file will not be regenerated once it exists.
+	
+*/
+public class ServiceIndex extends ServiceIndex_base
+	{
+	
+	}
\ No newline at end of file
diff --git a/src/main/java/org/kairosdb/datastore/h2/orm/ServiceIndex_base.java b/src/main/java/org/kairosdb/datastore/h2/orm/ServiceIndex_base.java
new file mode 100755
index 0000000000..cb6ee540ea
--- /dev/null
+++ b/src/main/java/org/kairosdb/datastore/h2/orm/ServiceIndex_base.java
@@ -0,0 +1,840 @@
+package org.kairosdb.datastore.h2.orm;
+
+import java.util.*;
+import org.agileclick.genorm.runtime.*;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+	This class has been automatically generated by GenORMous.  This file
+	should not be modified.
+	
+*/
+public class ServiceIndex_base extends GenOrmRecord
+	{
+	protected static final Logger s_logger = LoggerFactory.getLogger(ServiceIndex.class.getName());
+
+	public static final String COL_SERVICE = "service";
+	public static final String COL_SERVICE_KEY = "service_key";
+	public static final String COL_KEY = "key";
+	public static final String COL_VALUE = "value";
+
+	//Change this value to true to turn on warning messages
+	private static final boolean WARNINGS = false;
+	private static final String SELECT = "SELECT this.\"service\", this.\"service_key\", this.\"key\", this.\"value\" ";
+	private static final String FROM = "FROM service_index this ";
+	private static final String WHERE = "WHERE ";
+	private static final String KEY_WHERE = "WHERE \"service\" = ? AND \"service_key\" = ? AND \"key\" = ?";
+	
+	public static final String TABLE_NAME = "service_index";
+	public static final int NUMBER_OF_COLUMNS = 4;
+	
+	
+	private static final String s_fieldEscapeString = "\""; 
+	
+	public static final GenOrmFieldMeta SERVICE_FIELD_META = new GenOrmFieldMeta("service", "string", 0, true, false);
+	public static final GenOrmFieldMeta SERVICE_KEY_FIELD_META = new GenOrmFieldMeta("service_key", "string", 1, true, false);
+	public static final GenOrmFieldMeta KEY_FIELD_META = new GenOrmFieldMeta("key", "string", 2, true, false);
+	public static final GenOrmFieldMeta VALUE_FIELD_META = new GenOrmFieldMeta("value", "string", 3, false, false);
+
+	
+		
+	//===========================================================================
+	public static ServiceIndexFactoryImpl factory = new ServiceIndexFactoryImpl();
+	
+	public static interface ServiceIndexFactory extends GenOrmRecordFactory
+		{
+		public boolean delete(String service, String serviceKey, String key);
+		public ServiceIndex find(String service, String serviceKey, String key);
+		public ServiceIndex findOrCreate(String service, String serviceKey, String key);
+		/**
+		*/
+		public ResultSet getKeys(String service, String serviceKey);/**
+		*/
+		public ResultSet getKeysLike(String service, String serviceKey, String keyPrefix);
+		}
+	
+	public static class ServiceIndexFactoryImpl //Inherit interfaces
+			implements ServiceIndexFactory 
+		{
+		public static final String CREATE_SQL = "CREATE CACHED TABLE service_index (\n	\"service\" VARCHAR  NOT NULL,\n	\"service_key\" VARCHAR  NOT NULL,\n	\"key\" VARCHAR  NOT NULL,\n	\"value\" VARCHAR  NULL,\n	PRIMARY KEY (\"service\", \"service_key\", \"key\")\n	)";
+
+		private ArrayList<GenOrmFieldMeta> m_fieldMeta;
+		private ArrayList<GenOrmConstraint> m_foreignKeyConstraints;
+		
+		protected ServiceIndexFactoryImpl()
+			{
+			m_fieldMeta = new ArrayList<GenOrmFieldMeta>();
+			m_fieldMeta.add(SERVICE_FIELD_META);
+			m_fieldMeta.add(SERVICE_KEY_FIELD_META);
+			m_fieldMeta.add(KEY_FIELD_META);
+			m_fieldMeta.add(VALUE_FIELD_META);
+
+			m_foreignKeyConstraints = new ArrayList<GenOrmConstraint>();
+			}
+			
+		protected ServiceIndex newServiceIndex(java.sql.ResultSet rs)
+			{
+			ServiceIndex rec = new ServiceIndex();
+			((ServiceIndex_base)rec).initialize(rs);
+			return ((ServiceIndex)GenOrmDataSource.getGenOrmConnection().getUniqueRecord(rec));
+			}
+	
+		//---------------------------------------------------------------------------
+		/**
+			Returns a list of the feild meta for the class that this is a factory of
+		*/
+		public List<GenOrmFieldMeta> getFields()
+			{
+			return (m_fieldMeta);
+			}
+
+		//---------------------------------------------------------------------------
+		/**
+			Returns a list of foreign key constraints
+		*/
+		public List<GenOrmConstraint> getForeignKeyConstraints()
+			{
+			return (m_foreignKeyConstraints);
+			}
+			
+		//---------------------------------------------------------------------------
+		/**
+			Returns the SQL create statement for this table
+		*/
+		public String getCreateStatement()
+			{
+			return (CREATE_SQL);
+			}
+			
+		//---------------------------------------------------------------------------
+		/**
+			Creates a new entry with the specified primary keys.
+		*/
+		public ServiceIndex create(String service, String serviceKey, String key)
+			{
+			ServiceIndex rec = new ServiceIndex();
+			rec.m_isNewRecord = true;
+			
+			((ServiceIndex_base)rec).setService(service);
+			((ServiceIndex_base)rec).setServiceKey(serviceKey);
+			((ServiceIndex_base)rec).setKey(key);
+
+			
+			return ((ServiceIndex)GenOrmDataSource.getGenOrmConnection().getUniqueRecord(rec));
+			}
+		//---------------------------------------------------------------------------
+		/**
+			Creates a new entry that is empty
+		*/
+		public ServiceIndex createRecord()
+			{
+			ServiceIndex rec = new ServiceIndex();
+			rec.m_isNewRecord = true;
+			
+			return ((ServiceIndex)GenOrmDataSource.getGenOrmConnection().getUniqueRecord(rec));
+			}
+			
+		//---------------------------------------------------------------------------
+		/**
+		If the table has a primary key that has a key generator this method will 
+		return a new table entry with a generated primary key.
+		@return ServiceIndex with generated primary key
+		*/
+		public ServiceIndex createWithGeneratedKey()
+			{
+			throw new UnsupportedOperationException("ServiceIndex does not support a generated primary key");
+			}
+			
+		//---------------------------------------------------------------------------
+		/**
+		A generic api for finding a record.
+		@param keys This must match the primary key for this record.  If the 
+		record has multiple primary keys this parameter must be of type Object[] 
+		where each element is the corresponding key.
+		@return ServiceIndex or null if no record is found
+		*/
+		public ServiceIndex findRecord(Object keys)
+			{
+			Object[] kArr = (Object[])keys;
+			return (find((String)kArr[0], (String)kArr[1], (String)kArr[2]));
+			}
+			
+		//---------------------------------------------------------------------------
+		/**
+			Deletes the record with the specified primary keys.
+			The point of this api is to prevent a hit on the db to see if the record
+			is there.  This call will add a record to the next transaction that is 
+			marked for delete. 
+			
+			@return Returns true if the record was previous created and existed
+			either in the transaction cache or the db.
+		*/
+		public boolean delete(String service, String serviceKey, String key)
+			{
+			boolean ret = false;
+			ServiceIndex rec = new ServiceIndex();
+			
+			((ServiceIndex_base)rec).initialize(service, serviceKey, key);
+			GenOrmConnection con = GenOrmDataSource.getGenOrmConnection();
+			ServiceIndex cachedRec = (ServiceIndex)con.getCachedRecord(rec.getRecordKey());
+			
+			if (cachedRec != null)
+				{
+				ret = true;
+				cachedRec.delete();
+				}
+			else
+				{
+				rec = (ServiceIndex)con.getUniqueRecord(rec);  //This adds the record to the cache
+				rec.delete();
+				ret = rec.flush();
+				rec.setIgnored(true); //So the system does not try to delete it again at commmit
+				}
+				
+			return (ret);
+			}
+			
+		//---------------------------------------------------------------------------
+		/**
+		Find the record with the specified primary keys
+		@return ServiceIndex or null if no record is found
+		*/
+		public ServiceIndex find(String service, String serviceKey, String key)
+			{
+			ServiceIndex rec = new ServiceIndex();
+			
+			//Create temp object and look in cache for it
+			((ServiceIndex_base)rec).initialize(service, serviceKey, key);
+			rec = (ServiceIndex)GenOrmDataSource.getGenOrmConnection().getCachedRecord(rec.getRecordKey());
+			
+			java.sql.PreparedStatement genorm_statement = null;
+			java.sql.ResultSet genorm_rs = null;
+			
+			if (rec == null)
+				{
+				try
+					{
+					//No cached object so look in db
+					genorm_statement = GenOrmDataSource.prepareStatement(SELECT+FROM+KEY_WHERE);
+					genorm_statement.setString(1, service);
+					genorm_statement.setString(2, serviceKey);
+					genorm_statement.setString(3, key);
+
+					s_logger.debug(genorm_statement.toString());
+						
+					genorm_rs = genorm_statement.executeQuery();
+					if (genorm_rs.next())
+						rec = newServiceIndex(genorm_rs);
+					}
+				catch (java.sql.SQLException sqle)
+					{
+					throw new GenOrmException(sqle);
+					}
+				finally
+					{
+					try
+						{
+						if (genorm_rs != null)
+							genorm_rs.close();
+							
+						if (genorm_statement != null)
+							genorm_statement.close();
+						}
+					catch (java.sql.SQLException sqle2)
+						{
+						throw new GenOrmException(sqle2);
+						}
+					}
+				}
+				
+			return (rec);
+			}
+		
+		//---------------------------------------------------------------------------
+		/**
+		This is the same as find except if the record returned is null a new one 
+		is created with the specified primary keys
+		@return A new or existing record.  
+		*/
+		public ServiceIndex findOrCreate(String service, String serviceKey, String key)
+			{
+			ServiceIndex rec = find(service, serviceKey, key);
+			if (rec == null)
+				rec = create(service, serviceKey, key);
+				
+			return (rec);
+			}
+			
+		//---------------------------------------------------------------------------
+		/**
+			Convenience method for selecting records.  Ideally this should not be use, 
+			instead a custom query for this table should be used.
+			@param where sql where statement.
+		*/
+		public ResultSet select(String where)
+			{
+			return (select(where, null));
+			}
+			
+		//---------------------------------------------------------------------------
+		/**
+			Convenience method for selecting records.  Ideally this should not be use, 
+			instead a custom query for this table should be used.
+			@param where sql where statement.
+			@param orderBy sql order by statement
+		*/
+		public ResultSet select(String where, String orderBy)
+			{
+			ResultSet rs = null;
+			java.sql.Statement stmnt = null;
+			
+			try
+				{
+				stmnt = GenOrmDataSource.createStatement();
+				StringBuilder sb = new StringBuilder();
+				sb.append(SELECT);
+				sb.append(FROM);
+				if (where != null)
+					{
+					sb.append(WHERE);
+					sb.append(where);
+					}
+					
+				if (orderBy != null)
+					{
+					sb.append(" ");
+					sb.append(orderBy);
+					}
+				
+				String query = sb.toString();
+				rs = new SQLResultSet(stmnt.executeQuery(query), query, stmnt);
+				}
+			catch (java.sql.SQLException sqle)
+				{
+				try
+					{
+					if (stmnt != null)
+						stmnt.close();
+					}
+				catch (java.sql.SQLException sqle2) { }
+					
+				throw new GenOrmException(sqle);
+				}
+				
+			return (rs);
+			}
+			
+		//---------------------------------------------------------------------------
+		/**
+		*/
+		public ResultSet getKeys(String service, String serviceKey)
+			{
+			String query = SELECT+"from service_index this\n				where\n				this.\"service\" = ?\n				and this.\"service_key\" = ?\n				order by this.\"key\" asc";
+			
+			java.sql.PreparedStatement genorm_statement = null;
+			
+			try
+				{
+				genorm_statement = GenOrmDataSource.prepareStatement(query);
+				genorm_statement.setString(1, service);genorm_statement.setString(2, serviceKey);
+				
+				s_logger.debug(genorm_statement.toString());
+				
+				ResultSet rs = new SQLResultSet(genorm_statement.executeQuery(), query, genorm_statement);
+				
+				return (rs);
+				}
+			catch (java.sql.SQLException sqle)
+				{
+				try
+					{
+					if (genorm_statement != null)
+						genorm_statement.close();
+					}
+				catch (java.sql.SQLException sqle2) { }
+					
+				if (s_logger.isDebugEnabled())
+					sqle.printStackTrace();
+				throw new GenOrmException(sqle);
+				}
+			}
+			
+		//---------------------------------------------------------------------------
+		/**
+		*/
+		public ResultSet getKeysLike(String service, String serviceKey, String keyPrefix)
+			{
+			String query = SELECT+"from service_index this\n				where\n				this.\"service\" = ?\n				AND this.\"service_key\" = ?\n				AND this.\"key\" LIKE ?\n				ORDER BY this.\"key\" asc";
+			
+			java.sql.PreparedStatement genorm_statement = null;
+			
+			try
+				{
+				genorm_statement = GenOrmDataSource.prepareStatement(query);
+				genorm_statement.setString(1, service);genorm_statement.setString(2, serviceKey);genorm_statement.setString(3, keyPrefix);
+				
+				s_logger.debug(genorm_statement.toString());
+				
+				ResultSet rs = new SQLResultSet(genorm_statement.executeQuery(), query, genorm_statement);
+				
+				return (rs);
+				}
+			catch (java.sql.SQLException sqle)
+				{
+				try
+					{
+					if (genorm_statement != null)
+						genorm_statement.close();
+					}
+				catch (java.sql.SQLException sqle2) { }
+					
+				if (s_logger.isDebugEnabled())
+					sqle.printStackTrace();
+				throw new GenOrmException(sqle);
+				}
+			}
+			
+
+		
+		//---------------------------------------------------------------------------
+		/**
+			Calls all query methods with test parameters.
+		*/
+		public void testQueryMethods()
+			{
+			ResultSet rs;
+			System.out.println("ServiceIndex.getKeys");
+			rs = getKeys("foo", "foo");
+			rs.close();
+			System.out.println("ServiceIndex.getKeysLike");
+			rs = getKeysLike("foo", "foo", "key%");
+			rs.close();
+
+			}
+		}
+		
+	//===========================================================================
+	public static interface ResultSet extends GenOrmResultSet
+		{
+		public ArrayList<ServiceIndex> getArrayList(int maxRows);
+		public ArrayList<ServiceIndex> getArrayList();
+		public ServiceIndex getRecord();
+		public ServiceIndex getOnlyRecord();
+		}
+		
+	//===========================================================================
+	private static class SQLResultSet 
+			implements ResultSet
+		{
+		private java.sql.ResultSet m_resultSet;
+		private java.sql.Statement m_statement;
+		private String m_query;
+		private boolean m_onFirstResult;
+		
+		//------------------------------------------------------------------------
+		protected SQLResultSet(java.sql.ResultSet resultSet, String query, java.sql.Statement statement)
+			{
+			m_resultSet = resultSet;
+			m_statement = statement;
+			m_query = query;
+			m_onFirstResult = false;
+			}
+		
+		//------------------------------------------------------------------------
+		/**
+			Closes any underlying java.sql.Result set and java.sql.Statement 
+			that was used to create this results set.
+		*/
+		public void close()
+			{
+			try
+				{
+				m_resultSet.close();
+				m_statement.close();
+				}
+			catch (java.sql.SQLException sqle)
+				{
+				throw new GenOrmException(sqle);
+				}
+			}
+			
+		//------------------------------------------------------------------------
+		/**
+			Returns the reults as an ArrayList of Record objects.
+			The Result set is closed within this call
+			@param maxRows if the result set contains more than this param
+				then an exception is thrown
+		*/
+		public ArrayList<ServiceIndex> getArrayList(int maxRows)
+			{
+			ArrayList<ServiceIndex> results = new ArrayList<ServiceIndex>();
+			int count = 0;
+			
+			try
+				{
+				if (m_onFirstResult)
+					{
+					count ++;
+					results.add(factory.newServiceIndex(m_resultSet));
+					}
+					
+				while (m_resultSet.next() && (count < maxRows))
+					{
+					count ++;
+					results.add(factory.newServiceIndex(m_resultSet));
+					}
+					
+				if (m_resultSet.next())
+					throw new GenOrmException("Bound of "+maxRows+" is too small for query ["+m_query+"]");
+				}
+			catch (java.sql.SQLException sqle)
+				{
+				sqle.printStackTrace();
+				throw new GenOrmException(sqle);
+				}
+				
+			close();
+			return (results);
+			}
+		
+		//------------------------------------------------------------------------
+		/**
+			Returns the reults as an ArrayList of Record objects.
+			The Result set is closed within this call
+		*/
+		public ArrayList<ServiceIndex> getArrayList()
+			{
+			ArrayList<ServiceIndex> results = new ArrayList<ServiceIndex>();
+			
+			try
+				{
+				if (m_onFirstResult)
+					results.add(factory.newServiceIndex(m_resultSet));
+					
+				while (m_resultSet.next())
+					results.add(factory.newServiceIndex(m_resultSet));
+				}
+			catch (java.sql.SQLException sqle)
+				{
+				sqle.printStackTrace();
+				throw new GenOrmException(sqle);
+				}
+				
+			close();
+			return (results);
+			}
+			
+		//------------------------------------------------------------------------
+		/**
+			Returns the underlying java.sql.ResultSet object
+		*/
+		public java.sql.ResultSet getResultSet()
+			{
+			return (m_resultSet);
+			}
+			
+		//------------------------------------------------------------------------
+		/**
+			Returns the current record in the result set
+		*/
+		public ServiceIndex getRecord()
+			{
+			return (factory.newServiceIndex(m_resultSet));
+			}
+			
+		//------------------------------------------------------------------------
+		/**
+			This call expects only one record in the result set.  If multiple records
+			are found an excpetion is thrown.
+			The ResultSet object is automatically closed by this call.
+		*/
+		public ServiceIndex getOnlyRecord()
+			{
+			ServiceIndex ret = null;
+			
+			try
+				{
+				if (m_resultSet.next())
+					ret = factory.newServiceIndex(m_resultSet);
+					
+				if (m_resultSet.next())
+					throw new GenOrmException("Multiple rows returned in call from ServiceIndex.getOnlyRecord");
+				}
+			catch (java.sql.SQLException sqle)
+				{
+				throw new GenOrmException(sqle);
+				}
+				
+			close();
+			return (ret);
+			}
+			
+		//------------------------------------------------------------------------
+		/**
+			Returns true if there is another record in the result set.
+		*/
+		public boolean next()
+			{
+			boolean ret = false;
+			m_onFirstResult = true;
+			try
+				{
+				ret = m_resultSet.next();
+				}
+			catch (java.sql.SQLException sqle)
+				{
+				throw new GenOrmException(sqle);
+				}
+			
+			return (ret);
+			}
+		}
+		
+	//===========================================================================
+		
+	private GenOrmString m_service;
+	private GenOrmString m_serviceKey;
+	private GenOrmString m_key;
+	private GenOrmString m_value;
+
+	
+	private List<GenOrmRecordKey> m_foreignKeys;
+	
+	public List<GenOrmRecordKey> getForeignKeys() { return (m_foreignKeys); }
+
+
+	//---------------------------------------------------------------------------
+	/**
+	*/
+	public String getService() { return (m_service.getValue()); }
+	public ServiceIndex setService(String data)
+		{
+		boolean changed = m_service.setValue(data);
+		
+		//Add the now dirty record to the transaction only if it is not previously dirty
+		if (changed)
+			{
+			if (m_dirtyFlags.isEmpty())
+				GenOrmDataSource.getGenOrmConnection().addToTransaction(this);
+				
+			m_dirtyFlags.set(SERVICE_FIELD_META.getDirtyFlag());
+			
+			if (m_isNewRecord) //Force set the prev value
+				m_service.setPrevValue(data);
+			}
+			
+		return ((ServiceIndex)this);
+		}
+		
+
+	//---------------------------------------------------------------------------
+	/**
+	*/
+	public String getServiceKey() { return (m_serviceKey.getValue()); }
+	public ServiceIndex setServiceKey(String data)
+		{
+		boolean changed = m_serviceKey.setValue(data);
+		
+		//Add the now dirty record to the transaction only if it is not previously dirty
+		if (changed)
+			{
+			if (m_dirtyFlags.isEmpty())
+				GenOrmDataSource.getGenOrmConnection().addToTransaction(this);
+				
+			m_dirtyFlags.set(SERVICE_KEY_FIELD_META.getDirtyFlag());
+			
+			if (m_isNewRecord) //Force set the prev value
+				m_serviceKey.setPrevValue(data);
+			}
+			
+		return ((ServiceIndex)this);
+		}
+		
+
+	//---------------------------------------------------------------------------
+	/**
+	*/
+	public String getKey() { return (m_key.getValue()); }
+	public ServiceIndex setKey(String data)
+		{
+		boolean changed = m_key.setValue(data);
+		
+		//Add the now dirty record to the transaction only if it is not previously dirty
+		if (changed)
+			{
+			if (m_dirtyFlags.isEmpty())
+				GenOrmDataSource.getGenOrmConnection().addToTransaction(this);
+				
+			m_dirtyFlags.set(KEY_FIELD_META.getDirtyFlag());
+			
+			if (m_isNewRecord) //Force set the prev value
+				m_key.setPrevValue(data);
+			}
+			
+		return ((ServiceIndex)this);
+		}
+		
+
+	//---------------------------------------------------------------------------
+	/**
+	*/
+	public String getValue() { return (m_value.getValue()); }
+	public ServiceIndex setValue(String data)
+		{
+		boolean changed = m_value.setValue(data);
+		
+		//Add the now dirty record to the transaction only if it is not previously dirty
+		if (changed)
+			{
+			if (m_dirtyFlags.isEmpty())
+				GenOrmDataSource.getGenOrmConnection().addToTransaction(this);
+				
+			m_dirtyFlags.set(VALUE_FIELD_META.getDirtyFlag());
+			
+			if (m_isNewRecord) //Force set the prev value
+				m_value.setPrevValue(data);
+			}
+			
+		return ((ServiceIndex)this);
+		}
+		
+	public boolean isValueNull()
+		{
+		return (m_value.isNull());
+		}
+		
+	public ServiceIndex setValueNull()
+		{
+		boolean changed = m_value.setNull();
+		
+		if (changed)
+			{
+			if (m_dirtyFlags.isEmpty())
+				GenOrmDataSource.getGenOrmConnection().addToTransaction(this);
+				
+			m_dirtyFlags.set(VALUE_FIELD_META.getDirtyFlag());
+			}
+		
+		return ((ServiceIndex)this);
+		}
+	
+	
+	
+	
+	//---------------------------------------------------------------------------
+	protected void initialize(String service, String serviceKey, String key)
+		{
+		m_service.setValue(service);
+		m_service.setPrevValue(service);
+		m_serviceKey.setValue(serviceKey);
+		m_serviceKey.setPrevValue(serviceKey);
+		m_key.setValue(key);
+		m_key.setPrevValue(key);
+
+		}
+		
+	//---------------------------------------------------------------------------
+	protected void initialize(java.sql.ResultSet rs)
+		{
+		try
+			{
+			if (s_logger.isDebugEnabled())
+				{
+				java.sql.ResultSetMetaData meta = rs.getMetaData();
+				for (int I = 1; I <= meta.getColumnCount(); I++)
+					{
+					s_logger.debug("Reading - "+meta.getColumnName(I) +" : "+rs.getString(I));
+					}
+				}
+			m_service.setValue(rs, 1);
+			m_serviceKey.setValue(rs, 2);
+			m_key.setValue(rs, 3);
+			m_value.setValue(rs, 4);
+
+			}
+		catch (java.sql.SQLException sqle)
+			{
+			throw new GenOrmException(sqle);
+			}
+		}
+	
+	//---------------------------------------------------------------------------
+	/*package*/ ServiceIndex_base()
+		{
+		super(TABLE_NAME);
+		m_logger = s_logger;
+		m_foreignKeys = new ArrayList<GenOrmRecordKey>();
+		m_dirtyFlags = new java.util.BitSet(NUMBER_OF_COLUMNS);
+		
+
+		m_service = new GenOrmString(SERVICE_FIELD_META);
+		addField(COL_SERVICE, m_service);
+
+		m_serviceKey = new GenOrmString(SERVICE_KEY_FIELD_META);
+		addField(COL_SERVICE_KEY, m_serviceKey);
+
+		m_key = new GenOrmString(KEY_FIELD_META);
+		addField(COL_KEY, m_key);
+
+		m_value = new GenOrmString(VALUE_FIELD_META);
+		addField(COL_VALUE, m_value);
+
+		GenOrmRecordKey foreignKey;
+		}
+	
+	//---------------------------------------------------------------------------
+	@Override
+	public GenOrmConnection getGenOrmConnection()
+		{
+		return (GenOrmDataSource.getGenOrmConnection());
+		}
+		
+	//---------------------------------------------------------------------------
+	@Override
+	public String getFieldEscapeString()
+		{
+		return (s_fieldEscapeString);
+		}
+		
+	//---------------------------------------------------------------------------
+	@Override
+	public void setMTS()
+		{
+		}
+		
+	//---------------------------------------------------------------------------
+	@Override
+	public void setCTS()
+		{
+		}
+		
+	//---------------------------------------------------------------------------
+	public String toString()
+		{
+		StringBuilder sb = new StringBuilder();
+		
+		sb.append("service=\"");
+		sb.append(m_service.getValue());
+		sb.append("\" ");
+		sb.append("service_key=\"");
+		sb.append(m_serviceKey.getValue());
+		sb.append("\" ");
+		sb.append("key=\"");
+		sb.append(m_key.getValue());
+		sb.append("\" ");
+		sb.append("value=\"");
+		sb.append(m_value.getValue());
+		sb.append("\" ");
+
+		
+		return (sb.toString().trim());
+		}
+		
+	//===========================================================================
+
+	
+	
+	}
+	
+	
\ No newline at end of file
diff --git a/src/main/java/org/kairosdb/datastore/h2/orm/TagNamesQuery.java b/src/main/java/org/kairosdb/datastore/h2/orm/TagNamesQuery.java
index eff7a20ad8..25efdd9916 100755
--- a/src/main/java/org/kairosdb/datastore/h2/orm/TagNamesQuery.java
+++ b/src/main/java/org/kairosdb/datastore/h2/orm/TagNamesQuery.java
@@ -13,6 +13,8 @@
 import java.sql.Timestamp;
 import org.xml.sax.ContentHandler;
 import org.xml.sax.Attributes;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import org.agileclick.genorm.runtime.*;
 
 
@@ -99,7 +101,7 @@ public ResultSet runQuery()
 			
 			genorm_statement = org.kairosdb.datastore.h2.orm.GenOrmDataSource.prepareStatement(genorm_query);
 			long genorm_queryTimeStart = 0L;
-			if (s_logger.isInfo())
+			if (s_logger.isInfoEnabled())
 				{
 				genorm_queryTimeStart = System.currentTimeMillis();
 				}
@@ -109,7 +111,7 @@ public ResultSet runQuery()
 			if (genorm_queryTimeStart != 0L)
 				{
 				long genorm_quryTime = System.currentTimeMillis() - genorm_queryTimeStart;
-				s_logger.info(genorm_quryTime);
+				s_logger.info(String.valueOf(genorm_quryTime));
 				}
 			
 			ResultSet genorm_ret = new SQLResultSet(genorm_resultSet, genorm_statement, genorm_query);
diff --git a/src/main/java/org/kairosdb/datastore/h2/orm/TagValuesQuery.java b/src/main/java/org/kairosdb/datastore/h2/orm/TagValuesQuery.java
index e54e0c9fac..c6df948488 100755
--- a/src/main/java/org/kairosdb/datastore/h2/orm/TagValuesQuery.java
+++ b/src/main/java/org/kairosdb/datastore/h2/orm/TagValuesQuery.java
@@ -13,6 +13,8 @@
 import java.sql.Timestamp;
 import org.xml.sax.ContentHandler;
 import org.xml.sax.Attributes;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import org.agileclick.genorm.runtime.*;
 
 
@@ -99,7 +101,7 @@ public ResultSet runQuery()
 			
 			genorm_statement = org.kairosdb.datastore.h2.orm.GenOrmDataSource.prepareStatement(genorm_query);
 			long genorm_queryTimeStart = 0L;
-			if (s_logger.isInfo())
+			if (s_logger.isInfoEnabled())
 				{
 				genorm_queryTimeStart = System.currentTimeMillis();
 				}
@@ -109,7 +111,7 @@ public ResultSet runQuery()
 			if (genorm_queryTimeStart != 0L)
 				{
 				long genorm_quryTime = System.currentTimeMillis() - genorm_queryTimeStart;
-				s_logger.info(genorm_quryTime);
+				s_logger.info(String.valueOf(genorm_quryTime));
 				}
 			
 			ResultSet genorm_ret = new SQLResultSet(genorm_resultSet, genorm_statement, genorm_query);
diff --git a/src/main/java/org/kairosdb/datastore/h2/orm/Tag_base.java b/src/main/java/org/kairosdb/datastore/h2/orm/Tag_base.java
index 05041d1905..370f300c0c 100755
--- a/src/main/java/org/kairosdb/datastore/h2/orm/Tag_base.java
+++ b/src/main/java/org/kairosdb/datastore/h2/orm/Tag_base.java
@@ -2,6 +2,8 @@
 
 import java.util.*;
 import org.agileclick.genorm.runtime.*;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 /**
 	This class has been automatically generated by GenORMous.  This file
@@ -575,7 +577,7 @@ protected void initialize(java.sql.ResultSet rs)
 		{
 		try
 			{
-			if (s_logger.isDebug())
+			if (s_logger.isDebugEnabled())
 				{
 				java.sql.ResultSetMetaData meta = rs.getMetaData();
 				for (int I = 1; I <= meta.getColumnCount(); I++)
@@ -603,10 +605,10 @@ protected void initialize(java.sql.ResultSet rs)
 		
 
 		m_name = new GenOrmString(NAME_FIELD_META);
-		addField(m_name);
+		addField(COL_NAME, m_name);
 
 		m_value = new GenOrmString(VALUE_FIELD_META);
-		addField(m_value);
+		addField(COL_VALUE, m_value);
 
 		GenOrmRecordKey foreignKey;
 		}
diff --git a/src/main/java/org/kairosdb/datastore/h2/orm/create.sql b/src/main/java/org/kairosdb/datastore/h2/orm/create.sql
new file mode 100755
index 0000000000..79091ed223
--- /dev/null
+++ b/src/main/java/org/kairosdb/datastore/h2/orm/create.sql
@@ -0,0 +1,41 @@
+CREATE CACHED TABLE metric (
+	"id" VARCHAR  NOT NULL,
+	"name" VARCHAR  NULL,
+	"type" VARCHAR  NULL,
+	PRIMARY KEY ("id")
+	);
+
+CREATE CACHED TABLE service_index (
+	"service" VARCHAR  NOT NULL,
+	"service_key" VARCHAR  NOT NULL,
+	"key" VARCHAR  NOT NULL,
+	"value" VARCHAR  NULL,
+	PRIMARY KEY ("service", "service_key", "key")
+	);
+
+CREATE CACHED TABLE tag (
+	"name" VARCHAR  NOT NULL,
+	"value" VARCHAR  NOT NULL,
+	PRIMARY KEY ("name", "value")
+	);
+
+CREATE CACHED TABLE data_point (
+	"metric_id" VARCHAR  NOT NULL,
+	"timestamp" TIMESTAMP  NOT NULL,
+	"value" BINARY  NULL,
+	PRIMARY KEY ("metric_id", "timestamp"),
+	CONSTRAINT data_point_metric_id_fkey FOREIGN KEY ("metric_id")
+		REFERENCES metric ("id") 
+	);
+
+CREATE CACHED TABLE metric_tag (
+	"metric_id" VARCHAR  NOT NULL,
+	"tag_name" VARCHAR  NOT NULL,
+	"tag_value" VARCHAR  NOT NULL,
+	PRIMARY KEY ("metric_id", "tag_name", "tag_value"),
+	CONSTRAINT metric_tag_metric_id_fkey FOREIGN KEY ("metric_id")
+		REFERENCES metric ("id") ON DELETE CASCADE,
+	CONSTRAINT metric_tag_tag_name_fkey FOREIGN KEY ("tag_name", "tag_value")
+		REFERENCES tag ("name", "value") 
+	);
+
diff --git a/src/main/java/org/kairosdb/datastore/remote/RemoteDatastore.java b/src/main/java/org/kairosdb/datastore/remote/RemoteDatastore.java
index 8c4353b55d..b00438d499 100755
--- a/src/main/java/org/kairosdb/datastore/remote/RemoteDatastore.java
+++ b/src/main/java/org/kairosdb/datastore/remote/RemoteDatastore.java
@@ -154,7 +154,7 @@ private void flushMap()
 				{
 					for (DataPointKey dataPointKey : flushMap.keySet())
 					{
-						//We have to reset the writer every time or it gets confused
+						//We have to clear the writer every time or it gets confused
 						//because we are only writing partial json each time.
 						JSONWriter writer = new JSONWriter(m_dataWriter);
 
@@ -456,6 +456,30 @@ public void deleteDataPoints(DatastoreMetricQuery deleteQuery) throws DatastoreE
 	@Override
 	public TagSet queryMetricTags(DatastoreMetricQuery query) throws DatastoreException
 	{
-		return null;  //To change body of implemented methods use File | Settings | File Templates.
+		throw new DatastoreException("Method not implemented.");
+	}
+
+	@Override
+	public void setValue(String service, String serviceKey, String key, String value) throws DatastoreException
+	{
+		throw new DatastoreException("Method not implemented.");
+	}
+
+	@Override
+	public String getValue(String service, String serviceKey, String key) throws DatastoreException
+	{
+		throw new DatastoreException("Method not implemented.");
+	}
+
+	@Override
+	public Iterable<String> listKeys(String service, String serviceKey) throws DatastoreException
+	{
+		throw new DatastoreException("Method not implemented.");
+	}
+
+	@Override
+	public Iterable<String> listKeys(String service, String serviceKey, String keyStartsWith) throws DatastoreException
+	{
+		throw new DatastoreException("Method not implemented.");
 	}
 }
diff --git a/src/main/java/org/kairosdb/util/CongestionExecutorService.java b/src/main/java/org/kairosdb/util/AdaptiveExecutorService.java
similarity index 78%
rename from src/main/java/org/kairosdb/util/CongestionExecutorService.java
rename to src/main/java/org/kairosdb/util/AdaptiveExecutorService.java
index 5e3c9d486f..e94313d707 100755
--- a/src/main/java/org/kairosdb/util/CongestionExecutorService.java
+++ b/src/main/java/org/kairosdb/util/AdaptiveExecutorService.java
@@ -5,6 +5,7 @@
 import com.google.common.eventbus.EventBus;
 import org.kairosdb.core.datapoints.DoubleDataPointFactory;
 import org.kairosdb.core.datapoints.DoubleDataPointFactoryImpl;
+import org.kairosdb.datastore.cassandra.BatchHandler;
 import org.kairosdb.events.DataPointEvent;
 
 import javax.inject.Inject;
@@ -14,20 +15,20 @@
 /**
  Created by bhawkins on 10/27/16.
  */
-public class CongestionExecutorService extends AbstractExecutorService
+public class AdaptiveExecutorService
 {
 	private final EventBus m_eventBus;
 	private final ExecutorService m_internalExecutor;
 	private final ThreadGroup m_threadGroup;
 	private final CongestionSemaphore m_semaphore;
 	private final CongestionTimer m_congestionTimer;
-	private int m_permitCount = 10;
+	private int m_permitCount = 5;
 
 	@Inject
 	private DoubleDataPointFactory m_dataPointFactory = new DoubleDataPointFactoryImpl();
 
 	@Inject
-	public CongestionExecutorService(EventBus eventBus)
+	public AdaptiveExecutorService(EventBus eventBus)
 	{
 		m_eventBus = eventBus;
 		m_congestionTimer = new CongestionTimer(m_permitCount);
@@ -48,51 +49,30 @@ public Thread newThread(Runnable r)
 
 	private void increasePermitCount()
 	{
-		/*m_permitCount ++;
+		m_permitCount ++;
 		m_congestionTimer.setTaskPerBatch(m_permitCount);
-		m_semaphore.release();*/
+		m_semaphore.release();
 	}
 
-	@Override
 	public void shutdown()
 	{
 
 	}
 
-	@Override
-	public List<Runnable> shutdownNow()
-	{
-		return null;
-	}
-
-	@Override
-	public boolean isShutdown()
-	{
-		return false;
-	}
-
-	@Override
-	public boolean isTerminated()
-	{
-		return false;
-	}
-
-	@Override
-	public boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException
-	{
-		return false;
-	}
 
 	private Stopwatch m_timer = Stopwatch.createStarted();
 
-	@Override
-	public void execute(Runnable command)
+	public void submit(BatchHandler batchHandler)
 	{
-		if (m_timer.elapsed(TimeUnit.SECONDS) >= 15)
+		if (m_timer.elapsed(TimeUnit.SECONDS) >= 5)
 		{
+			/*if ((m_semaphore.availablePermits() == 0) && (batchHandler.isFullBatch()))
+			{
+				increasePermitCount();
+			}*/
+
 			m_timer.reset();
 			m_timer.start();
-			increasePermitCount();
 		}
 
 		try
@@ -100,7 +80,7 @@ public void execute(Runnable command)
 			//System.out.println("Execute called");
 			m_semaphore.acquire();
 			//System.out.println("Submitting");
-			m_internalExecutor.submit(command);
+			m_internalExecutor.submit(newTaskFor(batchHandler));
 			//System.out.println("Done submitting");
 		}
 		catch (InterruptedException e)
@@ -109,7 +89,6 @@ public void execute(Runnable command)
 		}
 	}
 
-	@Override
 	protected <T> RunnableFuture<T> newTaskFor(Callable<T> callable)
 	{
 		//System.out.println("Returning new future");
@@ -137,7 +116,7 @@ public void run()
 			m_stopwatch.stop();
 
 			//Todo do something with elapsed time
-			CongestionTimer.TimerStat timerStat = m_congestionTimer.reportTaskTime(m_stopwatch.elapsed(TimeUnit.MILLISECONDS));
+			SimpleStats.Data timerStat = m_congestionTimer.reportTaskTime(m_stopwatch.elapsed(TimeUnit.MILLISECONDS));
 
 			m_semaphore.release();
 
@@ -158,10 +137,6 @@ public void run()
 						m_dataPointFactory.createDataPoint(now, timerStat.avg));
 				m_eventBus.post(dpe);
 
-				dpe = new DataPointEvent("kairosdb.congestion.stats.median", tags,
-						m_dataPointFactory.createDataPoint(now, timerStat.median));
-				m_eventBus.post(dpe);
-
 				dpe = new DataPointEvent("kairosdb.congestion.stats.permit_count", tags,
 						m_dataPointFactory.createDataPoint(now, m_permitCount));
 				m_eventBus.post(dpe);
diff --git a/src/main/java/org/kairosdb/util/CongestionTimer.java b/src/main/java/org/kairosdb/util/CongestionTimer.java
index a8cec733b2..8fc53d2b8f 100755
--- a/src/main/java/org/kairosdb/util/CongestionTimer.java
+++ b/src/main/java/org/kairosdb/util/CongestionTimer.java
@@ -1,7 +1,5 @@
 package org.kairosdb.util;
 
-import com.google.common.collect.TreeMultiset;
-import org.apache.commons.math3.stat.descriptive.DescriptiveStatistics;
 
 /**
  Created by bhawkins on 10/27/16.
@@ -9,13 +7,13 @@
 public class CongestionTimer
 {
 	private volatile int m_taskPerBatch;
-	private final DescriptiveStatistics m_stats;
+	private final SimpleStats m_stats;
 	private final Object m_statsLock;
 
 	public CongestionTimer(int taskPerBatch)
 	{
 		m_taskPerBatch = taskPerBatch;
-		m_stats = new DescriptiveStatistics();
+		m_stats = new SimpleStats();
 		m_statsLock = new Object();
 	}
 
@@ -24,41 +22,20 @@ public void setTaskPerBatch(int taskPerBatch)
 		m_taskPerBatch = taskPerBatch;
 	}
 
-	public TimerStat reportTaskTime(long time)
+	public SimpleStats.Data reportTaskTime(long time)
 	{
 		synchronized(m_statsLock)
 		{
 			m_stats.addValue(time);
 
-			if (m_stats.getN() == m_taskPerBatch)
+			if (m_stats.getCount() == m_taskPerBatch)
 			{
-				TimerStat ts = new TimerStat(m_stats.getMin(), m_stats.getMax(),
-						m_stats.getMean(), m_stats.getPercentile(50));
+				SimpleStats.Data data = m_stats.getAndClear();
 
-				m_stats.clear();
-
-
-				return ts;
+				return data;
 			}
 		}
 
 		return null;
 	}
-
-	public static class TimerStat
-	{
-		public final double min;
-		public final double max;
-		public final double avg;
-		public final double median;
-
-
-		public TimerStat(double min, double max, double avg, double median)
-		{
-			this.min = min;
-			this.max = max;
-			this.avg = avg;
-			this.median = median;
-		}
-	}
 }
diff --git a/src/main/java/org/kairosdb/util/SimpleStats.java b/src/main/java/org/kairosdb/util/SimpleStats.java
new file mode 100755
index 0000000000..cef1c2c65e
--- /dev/null
+++ b/src/main/java/org/kairosdb/util/SimpleStats.java
@@ -0,0 +1,79 @@
+package org.kairosdb.util;
+
+/**
+ Created by bhawkins on 1/26/17.
+ */
+public class SimpleStats
+{
+	private long m_min;
+	private long m_max;
+	private long m_sum;
+	private long m_count;
+	private final Object m_dataLock = new Object();
+
+	public SimpleStats()
+	{
+		clear();
+	}
+
+	public void addValue(long value)
+	{
+		synchronized (m_dataLock)
+		{
+			m_min = Math.min(m_min, value);
+			m_max = Math.max(m_max, value);
+			m_sum += value;
+			m_count++;
+		}
+	}
+
+	private void clear()
+	{
+		m_min = Long.MAX_VALUE;
+		m_max = Long.MIN_VALUE;
+		m_sum = 0;
+		m_count = 0;
+	}
+
+	/**
+	 Not thread safe
+	 @return
+	 */
+	public long getCount()
+	{
+		return m_count;
+	}
+
+	public Data getAndClear()
+	{
+		synchronized (m_dataLock)
+		{
+			Data ret;
+			if (m_count != 0)
+				ret = new Data(m_min, m_max, m_sum, m_count, ((double)m_sum)/((double)m_count));
+			else
+				ret = new Data(0, 0, 0, 0, 0.0);
+
+			clear();
+			return ret;
+		}
+	}
+
+	public static class Data
+	{
+		public final long min;
+		public final long max;
+		public final long sum;
+		public final long count;
+		public final double avg;
+
+		public Data(long min, long max, long sum, long count, double avg)
+		{
+			this.min = min;
+			this.max = max;
+			this.sum = sum;
+			this.count = count;
+			this.avg = avg;
+		}
+	}
+}
diff --git a/src/main/java/org/kairosdb/util/SimpleStatsReporter.java b/src/main/java/org/kairosdb/util/SimpleStatsReporter.java
new file mode 100755
index 0000000000..d81da67a62
--- /dev/null
+++ b/src/main/java/org/kairosdb/util/SimpleStatsReporter.java
@@ -0,0 +1,102 @@
+package org.kairosdb.util;
+
+import com.google.inject.name.Named;
+import org.kairosdb.core.DataPointSet;
+import org.kairosdb.core.datapoints.DoubleDataPointFactory;
+import org.kairosdb.core.datapoints.DoubleDataPointFactoryImpl;
+import org.kairosdb.core.datapoints.LongDataPointFactory;
+import org.kairosdb.core.datapoints.LongDataPointFactoryImpl;
+
+import javax.inject.Inject;
+import java.util.List;
+
+/**
+ Created by bhawkins on 1/26/17.
+ */
+public class SimpleStatsReporter
+{
+	private final String m_hostName;
+	private final LongDataPointFactory m_longDataPointFactory;
+	private final DoubleDataPointFactory m_doubleDataPointFactory;
+
+	@Inject
+	public SimpleStatsReporter(@Named("HOSTNAME")String hostName,
+			LongDataPointFactory longDataPointFactory, DoubleDataPointFactory doubleDataPointFactory)
+	{
+		m_hostName = hostName;
+		m_longDataPointFactory = longDataPointFactory;
+		m_doubleDataPointFactory = doubleDataPointFactory;
+	}
+
+	public SimpleStatsReporter()
+	{
+		m_hostName = "localhost";
+		m_longDataPointFactory = new LongDataPointFactoryImpl();
+		m_doubleDataPointFactory = new DoubleDataPointFactoryImpl();
+	}
+
+	private DataPointSet newDataPointSet(String metricPrefix, String metricSuffix)
+	{
+		DataPointSet dps = new DataPointSet(new StringBuilder(metricPrefix).append(".").append(metricSuffix).toString());
+		dps.addTag("host", m_hostName);
+
+		return dps;
+	}
+
+	private DataPointSet newDataPointSet(String metricPrefix, String metricSuffix, long now, long value)
+	{
+		DataPointSet dps = newDataPointSet(metricPrefix, metricSuffix);
+		dps.addDataPoint(m_longDataPointFactory.createDataPoint(now, value));
+
+		return dps;
+	}
+
+	private DataPointSet newDataPointSet(String metricPrefix, String metricSuffix, long now, double value)
+	{
+		DataPointSet dps = newDataPointSet(metricPrefix, metricSuffix);
+		dps.addDataPoint(m_doubleDataPointFactory.createDataPoint(now, value));
+
+		return dps;
+	}
+
+
+	public void reportStats(SimpleStats.Data stats, long now, String metricPrefix,
+			List<DataPointSet> dataPointSets)
+	{
+		dataPointSets.add(newDataPointSet(metricPrefix, "min", now, stats.min));
+
+		dataPointSets.add(newDataPointSet(metricPrefix, "max", now, stats.max));
+
+		dataPointSets.add(newDataPointSet(metricPrefix, "avg", now, stats.avg));
+
+		dataPointSets.add(newDataPointSet(metricPrefix, "count", now, stats.count));
+
+		dataPointSets.add(newDataPointSet(metricPrefix, "sum", now, stats.sum));
+	}
+
+	public void reportStats(SimpleStats.Data stats, long now, String metricPrefix,
+			String tagName, String tagValue, List<DataPointSet> dataPointSets)
+	{
+		DataPointSet dps;
+
+		dps = newDataPointSet(metricPrefix, "min", now, stats.min);
+		dps.addTag(tagName, tagValue);
+		dataPointSets.add(dps);
+
+		dps = newDataPointSet(metricPrefix, "max", now, stats.max);
+		dps.addTag(tagName, tagValue);
+		dataPointSets.add(dps);
+
+		dps = newDataPointSet(metricPrefix, "avg", now, stats.avg);
+		dps.addTag(tagName, tagValue);
+		dataPointSets.add(dps);
+
+		dps = newDataPointSet(metricPrefix, "count", now, stats.count);
+		dps.addTag(tagName, tagValue);
+		dataPointSets.add(dps);
+
+		dps = newDataPointSet(metricPrefix, "sum", now, stats.sum);
+		dps.addTag(tagName, tagValue);
+		dataPointSets.add(dps);
+	}
+}
diff --git a/src/main/java/org/kairosdb/util/StatsMap.java b/src/main/java/org/kairosdb/util/StatsMap.java
new file mode 100755
index 0000000000..45241da994
--- /dev/null
+++ b/src/main/java/org/kairosdb/util/StatsMap.java
@@ -0,0 +1,47 @@
+package org.kairosdb.util;
+
+
+import java.util.HashMap;
+import java.util.Map;
+
+/**
+ Contains a map of SimpleStats
+ Created by bhawkins on 1/20/17.
+ */
+public class StatsMap
+{
+	private Map<String, SimpleStats> m_statsMap;
+	private Object m_mapLock = new Object();
+
+	public StatsMap()
+	{
+		m_statsMap = new HashMap<>();
+	}
+
+	public void addMetric(String name, long value)
+	{
+		synchronized (m_mapLock)
+		{
+			SimpleStats stats = m_statsMap.get(name);
+			if (stats == null)
+			{
+				stats = new SimpleStats();
+				m_statsMap.put(name, stats);
+			}
+
+			stats.addValue(value);
+		}
+	}
+
+	public Map<String, SimpleStats> getStatsMap()
+	{
+		Map<String, SimpleStats> ret;
+		synchronized (m_mapLock)
+		{
+			ret = m_statsMap;
+			m_statsMap = new HashMap<>();
+		}
+
+		return ret;
+	}
+}
diff --git a/src/main/resources/logback.xml b/src/main/resources/logback.xml
index 8e1ebe17a4..5a190daed5 100755
--- a/src/main/resources/logback.xml
+++ b/src/main/resources/logback.xml
@@ -36,7 +36,7 @@
 		</encoder>
 	</appender>
 
-	<logger name="com.mchange.v2.c3p0" level="WARN"/>
+	<logger name="org.kairosdb.datastore.h2.orm" level="WARN"/>
 
 	<!--<logger name="org.hbase.async.RegionClient" level="DEBUG"/>-->
 
diff --git a/src/test/java/org/kairosdb/core/datastore/KairosDatastoreTest.java b/src/test/java/org/kairosdb/core/datastore/KairosDatastoreTest.java
index 90a3c9b869..6584c5af8a 100755
--- a/src/test/java/org/kairosdb/core/datastore/KairosDatastoreTest.java
+++ b/src/test/java/org/kairosdb/core/datastore/KairosDatastoreTest.java
@@ -401,5 +401,29 @@ public TagSet queryMetricTags(DatastoreMetricQuery query) throws DatastoreExcept
 		{
 			return null;  //To change body of implemented methods use File | Settings | File Templates.
 		}
+
+		@Override
+		public void setValue(String service, String serviceKey, String key, String value) throws DatastoreException
+		{
+
+		}
+
+		@Override
+		public String getValue(String service, String serviceKey, String key) throws DatastoreException
+		{
+			return null;
+		}
+
+		@Override
+		public Iterable<String> listKeys(String service, String serviceKey) throws DatastoreException
+		{
+			return null;
+		}
+
+		@Override
+		public Iterable<String> listKeys(String service, String serviceKey, String keyStartsWith) throws DatastoreException
+		{
+			return null;
+		}
 	}
 }
\ No newline at end of file
diff --git a/src/test/java/org/kairosdb/core/http/rest/MetricsResourceTest.java b/src/test/java/org/kairosdb/core/http/rest/MetricsResourceTest.java
index 6a4ba834c7..c90c2cddb4 100755
--- a/src/test/java/org/kairosdb/core/http/rest/MetricsResourceTest.java
+++ b/src/test/java/org/kairosdb/core/http/rest/MetricsResourceTest.java
@@ -44,6 +44,7 @@
 import org.kairosdb.testing.Client;
 import org.kairosdb.testing.JsonResponse;
 import org.kairosdb.util.LoggingUtils;
+import org.kairosdb.util.SimpleStatsReporter;
 import org.slf4j.bridge.SLF4JBridgeHandler;
 
 import java.io.IOException;
@@ -113,6 +114,7 @@ public void afterInjection(I i)
 				bindConstant().annotatedWith(Names.named("kairosdb.query_cache.keep_cache_files")).to(false);
 				bind(KairosDataPointFactory.class).to(GuiceKairosDataPointFactory.class);
 				bind(QueryPluginFactory.class).to(TestQueryPluginFactory.class);
+				bind(SimpleStatsReporter.class);
 
 				Properties props = new Properties();
 				InputStream is = getClass().getClassLoader().getResourceAsStream("kairosdb.properties");
@@ -405,6 +407,30 @@ public TagSet queryMetricTags(DatastoreMetricQuery query) throws DatastoreExcept
 		{
 			return null;
 		}
+
+		@Override
+		public void setValue(String service, String serviceKey, String key, String value) throws DatastoreException
+		{
+
+		}
+
+		@Override
+		public String getValue(String service, String serviceKey, String key) throws DatastoreException
+		{
+			return null;
+		}
+
+		@Override
+		public Iterable<String> listKeys(String service, String serviceKey) throws DatastoreException
+		{
+			return null;
+		}
+
+		@Override
+		public Iterable<String> listKeys(String service, String serviceKey, String keyStartsWith) throws DatastoreException
+		{
+			return null;
+		}
 	}
 
 }
\ No newline at end of file
diff --git a/src/test/java/org/kairosdb/core/http/rest/json/DataPointsParserTest.java b/src/test/java/org/kairosdb/core/http/rest/json/DataPointsParserTest.java
index 28e8df2a85..b07edc6313 100755
--- a/src/test/java/org/kairosdb/core/http/rest/json/DataPointsParserTest.java
+++ b/src/test/java/org/kairosdb/core/http/rest/json/DataPointsParserTest.java
@@ -790,5 +790,29 @@ public TagSet queryMetricTags(DatastoreMetricQuery query) throws DatastoreExcept
 		{
 			return null;
 		}
+
+		@Override
+		public void setValue(String service, String serviceKey, String key, String value) throws DatastoreException
+		{
+
+		}
+
+		@Override
+		public String getValue(String service, String serviceKey, String key) throws DatastoreException
+		{
+			return null;
+		}
+
+		@Override
+		public Iterable<String> listKeys(String service, String serviceKey) throws DatastoreException
+		{
+			return null;
+		}
+
+		@Override
+		public Iterable<String> listKeys(String service, String serviceKey, String keyStartsWith) throws DatastoreException
+		{
+			return null;
+		}
 	}
 }
\ No newline at end of file
diff --git a/src/test/java/org/kairosdb/core/queue/QueueProcessorTest.java b/src/test/java/org/kairosdb/core/queue/QueueProcessorTest.java
index aeee5a1aea..1446b4d87c 100755
--- a/src/test/java/org/kairosdb/core/queue/QueueProcessorTest.java
+++ b/src/test/java/org/kairosdb/core/queue/QueueProcessorTest.java
@@ -80,7 +80,6 @@ private DataPointEvent createDataPointEvent()
 	@Test
 	public void test_eventIsPulledFromMemoryQueue()
 	{
-		EventBus eventBus = mock(EventBus.class);
 		BigArray bigArray = mock(BigArray.class);
 
 		when(bigArray.append(Matchers.<byte[]>any())).thenReturn(0L);
@@ -90,7 +89,7 @@ public void test_eventIsPulledFromMemoryQueue()
 		DataPointEventSerializer serializer = new DataPointEventSerializer(new TestDataPointFactory());
 		ProcessorHandler processorHandler = mock(ProcessorHandler.class);
 
-		QueueProcessor queueProcessor = new QueueProcessor(serializer,
+		QueueProcessor queueProcessor = new FileQueueProcessor(serializer,
 				bigArray, new TestExecutor(), 2, 10, 500);
 
 		queueProcessor.setProcessorHandler(processorHandler);
@@ -103,14 +102,13 @@ public void test_eventIsPulledFromMemoryQueue()
 		m_deliveryThread.run();
 
 		verify(bigArray, times(1)).append(eq(serializer.serializeEvent(event)));
-		verify(processorHandler, times(1)).handleEvents(eq(Arrays.asList(event)), Matchers.<EventCompletionCallBack>any());
+		verify(processorHandler, times(1)).handleEvents(eq(Arrays.asList(event)), Matchers.<EventCompletionCallBack>any(), eq(false));
 		verify(bigArray, times(0)).get(anyLong());
 	}
 
 	@Test
 	public void test_eventIsPulledFromMemoryQueueThenBigArray()
 	{
-		EventBus eventBus = mock(EventBus.class);
 		BigArray bigArray = mock(BigArray.class);
 
 		when(bigArray.append(Matchers.<byte[]>any())).thenReturn(0L);
@@ -119,7 +117,7 @@ public void test_eventIsPulledFromMemoryQueueThenBigArray()
 		DataPointEventSerializer serializer = new DataPointEventSerializer(new TestDataPointFactory());
 		ProcessorHandler processorHandler = mock(ProcessorHandler.class);
 
-		QueueProcessor queueProcessor = new QueueProcessor(serializer,
+		QueueProcessor queueProcessor = new FileQueueProcessor(serializer,
 				bigArray, new TestExecutor(), 3, 1, 500);
 
 		queueProcessor.setProcessorHandler(processorHandler);
@@ -127,22 +125,24 @@ public void test_eventIsPulledFromMemoryQueueThenBigArray()
 		DataPointEvent event = createDataPointEvent();
 
 		queueProcessor.put(event);
+		when(bigArray.append(Matchers.<byte[]>any())).thenReturn(1L);
 		queueProcessor.put(event);
 
+		when(bigArray.get(0L)).thenReturn(serializer.serializeEvent(event));
 		when(bigArray.get(1L)).thenReturn(serializer.serializeEvent(event));
 
 		m_deliveryThread.setRunOnce(true);
 		m_deliveryThread.run();
 
 		verify(bigArray, times(2)).append(eq(serializer.serializeEvent(event)));
-		verify(processorHandler, times(1)).handleEvents(eq(Arrays.asList(event, event)), Matchers.<EventCompletionCallBack>any());
+		verify(processorHandler, times(1)).handleEvents(eq(Arrays.asList(event, event)), Matchers.<EventCompletionCallBack>any(), eq(false));
 		verify(bigArray, times(1)).get(anyLong());
 	}
 
 	@Test
 	public void test_checkPointIsCalled()
 	{
-		EventBus eventBus = mock(EventBus.class);
+		final EventBus eventBus = mock(EventBus.class);
 		BigArray bigArray = mock(BigArray.class);
 
 		when(bigArray.append(Matchers.<byte[]>any())).thenReturn(0L);
@@ -152,20 +152,22 @@ public void test_checkPointIsCalled()
 		ProcessorHandler processorHandler = new ProcessorHandler()
 		{
 			@Override
-			public void handleEvents(List<DataPointEvent> events, EventCompletionCallBack eventCompletionCallBack)
+			public void handleEvents(List<DataPointEvent> events, EventCompletionCallBack eventCompletionCallBack, boolean fullBatch)
 			{
+				System.out.println("Handling events "+events.size());
 				eventCompletionCallBack.complete();
 			}
 		};
 
-		QueueProcessor queueProcessor = new QueueProcessor(serializer,
-				bigArray, new TestExecutor(), 3, 1, -1);
+		QueueProcessor queueProcessor = new FileQueueProcessor(serializer,
+				bigArray, new TestExecutor(), 3, 2, -1);
 
 		queueProcessor.setProcessorHandler(processorHandler);
 
 		DataPointEvent event = createDataPointEvent();
 
 		queueProcessor.put(event);
+		when(bigArray.append(Matchers.<byte[]>any())).thenReturn(1L);
 		queueProcessor.put(event);
 
 		when(bigArray.get(1L)).thenReturn(serializer.serializeEvent(event));
@@ -174,7 +176,7 @@ public void handleEvents(List<DataPointEvent> events, EventCompletionCallBack ev
 		m_deliveryThread.run();
 
 		verify(bigArray, times(2)).append(eq(serializer.serializeEvent(event)));
-		verify(bigArray, times(1)).get(anyLong());
+		//verify(bigArray, times(1)).get(anyLong()); //Item taken from memory
 		verify(bigArray, times(1)).removeBeforeIndex(eq(1l));
 	}
 }
diff --git a/src/test/java/org/kairosdb/core/telnet/PutCommandTest.java b/src/test/java/org/kairosdb/core/telnet/PutCommandTest.java
index c73131bc88..f5d6507f69 100755
--- a/src/test/java/org/kairosdb/core/telnet/PutCommandTest.java
+++ b/src/test/java/org/kairosdb/core/telnet/PutCommandTest.java
@@ -388,5 +388,29 @@ public TagSet queryMetricTags(DatastoreMetricQuery query) throws DatastoreExcept
 		{
 			return null;
 		}
+
+		@Override
+		public void setValue(String service, String serviceKey, String key, String value) throws DatastoreException
+		{
+
+		}
+
+		@Override
+		public String getValue(String service, String serviceKey, String key) throws DatastoreException
+		{
+			return null;
+		}
+
+		@Override
+		public Iterable<String> listKeys(String service, String serviceKey) throws DatastoreException
+		{
+			return null;
+		}
+
+		@Override
+		public Iterable<String> listKeys(String service, String serviceKey, String keyStartsWith) throws DatastoreException
+		{
+			return null;
+		}
 	}
 }
\ No newline at end of file
diff --git a/src/test/java/org/kairosdb/datastore/cassandra/CassandraDatastoreTest.java b/src/test/java/org/kairosdb/datastore/cassandra/CassandraDatastoreTest.java
index 5c333bb3a7..7273ab605e 100755
--- a/src/test/java/org/kairosdb/datastore/cassandra/CassandraDatastoreTest.java
+++ b/src/test/java/org/kairosdb/datastore/cassandra/CassandraDatastoreTest.java
@@ -221,7 +221,7 @@ private static void deleteMetric(String metricName) throws IOException, Datastor
 	}
 
 	@Test
-	public void test_getKeysForQuery()
+	public void test_getKeysForQuery() throws DatastoreException
 	{
 		DatastoreMetricQuery query = new DatastoreMetricQueryImpl(ROW_KEY_TEST_METRIC,
 				HashMultimap.<String, String>create(), s_dataPointTime, s_dataPointTime);
@@ -232,7 +232,7 @@ public void test_getKeysForQuery()
 	}
 
 	@Test
-	public void test_getKeysForQuery_withFilter()
+	public void test_getKeysForQuery_withFilter() throws DatastoreException
 	{
 		SetMultimap<String, String> tagFilter = HashMultimap.create();
 		tagFilter.put("client", "bar");
diff --git a/src/test/java/org/kairosdb/datastore/cassandra/HectorConfigurationTest.java b/src/test/java/org/kairosdb/datastore/cassandra/HectorConfigurationTest.java
deleted file mode 100755
index c38e3f3663..0000000000
--- a/src/test/java/org/kairosdb/datastore/cassandra/HectorConfigurationTest.java
+++ /dev/null
@@ -1,148 +0,0 @@
-/*
- * Copyright 2016 KairosDB Authors
- *
- *    Licensed under the Apache License, Version 2.0 (the "License");
- *    you may not use this file except in compliance with the License.
- *    You may obtain a copy of the License at
- *
- *        http://www.apache.org/licenses/LICENSE-2.0
- *
- *    Unless required by applicable law or agreed to in writing, software
- *    distributed under the License is distributed on an "AS IS" BASIS,
- *    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- *    See the License for the specific language governing permissions and
- *    limitations under the License.
- */
-package org.kairosdb.datastore.cassandra;
-
-import me.prettyprint.cassandra.connection.DynamicLoadBalancingPolicy;
-import me.prettyprint.cassandra.connection.LoadBalancingPolicy;
-import me.prettyprint.cassandra.connection.RoundRobinBalancingPolicy;
-import me.prettyprint.cassandra.service.CassandraHostConfigurator;
-import org.junit.Test;
-
-import java.lang.reflect.Field;
-import java.util.Arrays;
-import java.util.List;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.junit.Assert.assertThat;
-
-public class HectorConfigurationTest
-{
-	@Test
-	public void test_defaults() throws NoSuchFieldException, IllegalAccessException
-	{
-		HectorConfiguration hectorConfiguration = new HectorConfiguration("host");
-		CassandraHostConfigurator config = hectorConfiguration.getConfiguration();
-
-		assertThat(getIntFieldValue(config, "maxActive"), equalTo(50));
-		assertThat(getLongFieldValue(config, "maxWaitTimeWhenExhausted"), equalTo(-1L));
-		assertThat(config.getUseSocketKeepalive(), equalTo(false));
-		assertThat(getIntFieldValue(config, "cassandraThriftSocketTimeout"), equalTo(0));
-		assertThat(getBooleanFieldValue(config, "retryDownedHosts"), equalTo(true));
-		assertThat(getIntFieldValue(config, "retryDownedHostsDelayInSeconds"), equalTo(10));
-		assertThat(getIntFieldValue(config, "retryDownedHostsQueueSize"), equalTo(-1));
-		assertThat(getBooleanFieldValue(config, "autoDiscoverHosts"), equalTo(false));
-		assertThat(getIntFieldValue(config, "autoDiscoveryDelayInSeconds"), equalTo(30));
-		assertThat(getListFieldValue(config, "autoDiscoveryDataCenters"), equalTo(null));
-		assertThat(getBooleanFieldValue(config, "runAutoDiscoveryAtStartup"), equalTo(false));
-		assertThat(getBooleanFieldValue(config, "useHostTimeoutTracker"), equalTo(false));
-		assertThat(getIntFieldValue(config, "maxFrameSize"), equalTo(2147483647));
-		assertThat(getPolicyFieldValue(config, "loadBalancingPolicy").getClass().getName(), equalTo(RoundRobinBalancingPolicy.class.getName()));
-		assertThat(getIntFieldValue(config, "hostTimeoutCounter"), equalTo(10));
-		assertThat(getIntFieldValue(config, "hostTimeoutWindow"), equalTo(500));
-		assertThat(getIntFieldValue(config, "hostTimeoutSuspensionDurationInSeconds"), equalTo(10));
-		assertThat(getIntFieldValue(config, "hostTimeoutUnsuspendCheckDelay"), equalTo(10));
-		assertThat(getLongFieldValue(config, "maxConnectTimeMillis"), equalTo(-1L));
-		assertThat(getLongFieldValue(config, "maxLastSuccessTimeMillis"), equalTo(-1L));
-	}
-
-	@Test
-	public void test_setValues() throws NoSuchFieldException, IllegalAccessException
-	{
-		HectorConfiguration hectorConfiguration = new HectorConfiguration("host");
-		hectorConfiguration.setMaxActive(100);
-		hectorConfiguration.setMaxWaitTimeWhenExhausted(101);
-		hectorConfiguration.setUseSocketKeepalive(true);
-		hectorConfiguration.setCassandraThriftSocketTimeout(102);
-		hectorConfiguration.setRetryDownedHosts(false);
-		hectorConfiguration.setRetryDownedHostsDelayInSeconds(103);
-		hectorConfiguration.setRetryDownedHostsQueueSize(104);
-		hectorConfiguration.setAutoDiscoverHosts(true);
-		hectorConfiguration.setAutoDiscoveryDelayInSeconds(105);
-		hectorConfiguration.setAutoDiscoveryDataCenters(Arrays.asList("foo", "bar"));
-		hectorConfiguration.setRunAutoDiscoveryAtStartup(true);
-		hectorConfiguration.setUseHostTimeoutTracker(true);
-		hectorConfiguration.setMaxFrameSize(106);
-		hectorConfiguration.setLoadBalancingPolicy("dynamic");
-		hectorConfiguration.setHostTimeOutCounter(107);
-		hectorConfiguration.setHostTimeoutWindow(108);
-		hectorConfiguration.setHostTimeOutSuspensionDurationInSeconds(109);
-		hectorConfiguration.setHostTimeOutUnsuspendCheckDelay(110);
-		hectorConfiguration.setMaxConnectTimeMillis(111L);
-		hectorConfiguration.setMaxLastSuccessTimeMillis(112L);
-
-		CassandraHostConfigurator config = hectorConfiguration.getConfiguration();
-
-		assertThat(getIntFieldValue(config, "maxActive"), equalTo(100));
-		assertThat(getLongFieldValue(config, "maxWaitTimeWhenExhausted"), equalTo(101L));
-		assertThat(config.getUseSocketKeepalive(), equalTo(true));
-		assertThat(getIntFieldValue(config, "cassandraThriftSocketTimeout"), equalTo(102));
-		assertThat(getBooleanFieldValue(config, "retryDownedHosts"), equalTo(false));
-		assertThat(getIntFieldValue(config, "retryDownedHostsDelayInSeconds"), equalTo(103));
-		assertThat(getIntFieldValue(config, "retryDownedHostsQueueSize"), equalTo(104));
-		assertThat(getBooleanFieldValue(config, "autoDiscoverHosts"), equalTo(true));
-		assertThat(getIntFieldValue(config, "autoDiscoveryDelayInSeconds"), equalTo(105));
-		assertThat(getListFieldValue(config, "autoDiscoveryDataCenters"), equalTo(Arrays.asList("foo", "bar")));
-		assertThat(getBooleanFieldValue(config, "runAutoDiscoveryAtStartup"), equalTo(true));
-		assertThat(getBooleanFieldValue(config, "useHostTimeoutTracker"), equalTo(true));
-		assertThat(getIntFieldValue(config, "maxFrameSize"), equalTo(106));
-		assertThat(getPolicyFieldValue(config, "loadBalancingPolicy").getClass().getName(), equalTo(DynamicLoadBalancingPolicy.class.getName()));
-		assertThat(getIntFieldValue(config, "hostTimeoutCounter"), equalTo(107));
-		assertThat(getIntFieldValue(config, "hostTimeoutWindow"), equalTo(108));
-		assertThat(getIntFieldValue(config, "hostTimeoutSuspensionDurationInSeconds"), equalTo(109));
-		assertThat(getIntFieldValue(config, "hostTimeoutUnsuspendCheckDelay"), equalTo(110));
-		assertThat(getLongFieldValue(config, "maxConnectTimeMillis"), equalTo(111L));
-		assertThat(getLongFieldValue(config, "maxLastSuccessTimeMillis"), equalTo(112L));
-	}
-
-
-	private static int getIntFieldValue(Object config, String fieldName) throws NoSuchFieldException, IllegalAccessException
-	{
-		Field field = config.getClass().getDeclaredField(fieldName);
-		field.setAccessible(true);
-		return field.getInt(config);
-	}
-
-	private static long getLongFieldValue(Object config, String fieldName) throws NoSuchFieldException, IllegalAccessException
-	{
-		Field field = config.getClass().getDeclaredField(fieldName);
-		field.setAccessible(true);
-		return field.getLong(config);
-	}
-
-	private static boolean getBooleanFieldValue(Object config, String fieldName) throws NoSuchFieldException, IllegalAccessException
-	{
-		Field field = config.getClass().getDeclaredField(fieldName);
-		field.setAccessible(true);
-		return field.getBoolean(config);
-	}
-
-	@SuppressWarnings("unchecked")
-	private static List<String> getListFieldValue(Object config, String fieldName) throws NoSuchFieldException, IllegalAccessException
-	{
-		Field field = config.getClass().getDeclaredField(fieldName);
-		field.setAccessible(true);
-		return (List<String>) field.get(config);
-	}
-
-	private static LoadBalancingPolicy getPolicyFieldValue(Object config, String fieldName) throws NoSuchFieldException, IllegalAccessException
-	{
-		Field field = config.getClass().getDeclaredField(fieldName);
-		field.setAccessible(true);
-		return (LoadBalancingPolicy) field.get(config);
-	}
-
-}
-
diff --git a/src/test/java/org/kairosdb/datastore/h2/H2DatastoreTest.java b/src/test/java/org/kairosdb/datastore/h2/H2DatastoreTest.java
index fd89e6f98b..4513396cbf 100755
--- a/src/test/java/org/kairosdb/datastore/h2/H2DatastoreTest.java
+++ b/src/test/java/org/kairosdb/datastore/h2/H2DatastoreTest.java
@@ -64,7 +64,7 @@ private static void deltree(File directory)
 	public static void setupDatabase() throws DatastoreException
 	{
 		KairosDataPointFactory dataPointFactory = new TestDataPointFactory();
-		H2Datastore h2Datastore = new H2Datastore(DB_PATH, dataPointFactory);
+		H2Datastore h2Datastore = new H2Datastore(DB_PATH, dataPointFactory, s_eventBus);
 
 		s_datastore = new KairosDatastore(h2Datastore,
 				new QueryQueuingManager(1, "hostname"),
diff --git a/src/test/java/org/kairosdb/rollup/RollUpJobTest.java b/src/test/java/org/kairosdb/rollup/RollUpJobTest.java
index 5a9e6a7db8..f3471546cc 100755
--- a/src/test/java/org/kairosdb/rollup/RollUpJobTest.java
+++ b/src/test/java/org/kairosdb/rollup/RollUpJobTest.java
@@ -360,6 +360,30 @@ public TagSet queryMetricTags(DatastoreMetricQuery query) throws
 		{
 			throw new UnsupportedOperationException();
 		}
+
+		@Override
+		public void setValue(String service, String serviceKey, String key, String value) throws DatastoreException
+		{
+
+		}
+
+		@Override
+		public String getValue(String service, String serviceKey, String key) throws DatastoreException
+		{
+			return null;
+		}
+
+		@Override
+		public Iterable<String> listKeys(String service, String serviceKey) throws DatastoreException
+		{
+			return null;
+		}
+
+		@Override
+		public Iterable<String> listKeys(String service, String serviceKey, String keyStartsWith) throws DatastoreException
+		{
+			return null;
+		}
 	}
 
 
diff --git a/webroot/css/cupertino/jquery-ui-1.10.0.custom.css b/webroot/css/cupertino/jquery-ui-1.10.0.custom.css
index 8ff13cc321..8456e518c2 100755
--- a/webroot/css/cupertino/jquery-ui-1.10.0.custom.css
+++ b/webroot/css/cupertino/jquery-ui-1.10.0.custom.css
@@ -290,7 +290,7 @@ input.ui-button {
 }
 
 /* workarounds */
-/* reset extra padding in Firefox, see h5bp.com/l */
+/* clear extra padding in Firefox, see h5bp.com/l */
 input.ui-button::-moz-focus-inner,
 button.ui-button::-moz-focus-inner {
 	border: 0;
diff --git a/webroot/grafana/vendor/bootstrap/less/forms.less b/webroot/grafana/vendor/bootstrap/less/forms.less
index ffa68212a9..9e96d5fa18 100755
--- a/webroot/grafana/vendor/bootstrap/less/forms.less
+++ b/webroot/grafana/vendor/bootstrap/less/forms.less
@@ -347,7 +347,7 @@ textarea[readonly] {
   background-color: @inputDisabledBackground;
 }
 
-// Explicitly reset the colors here
+// Explicitly clear the colors here
 input[type="radio"][disabled],
 input[type="checkbox"][disabled],
 input[type="radio"][readonly],
diff --git a/webroot/grafana/vendor/bootstrap/less/navbar.less b/webroot/grafana/vendor/bootstrap/less/navbar.less
index fe724f4961..c83884ce1e 100755
--- a/webroot/grafana/vendor/bootstrap/less/navbar.less
+++ b/webroot/grafana/vendor/bootstrap/less/navbar.less
@@ -31,7 +31,7 @@
 }
 
 // Set width to auto for default container
-// We then reset it for fixed navbars in the #gridSystem mixin
+// We then clear it for fixed navbars in the #gridSystem mixin
 .navbar .container {
   width: auto;
 }
@@ -190,7 +190,7 @@
 }
 
 // Reset container width
-// Required here as we reset the width earlier on and the grid mixins don't override early enough
+// Required here as we clear the width earlier on and the grid mixins don't override early enough
 .navbar-static-top .container,
 .navbar-fixed-top .container,
 .navbar-fixed-bottom .container {
diff --git a/webroot/grafana/vendor/bootstrap/less/popovers.less b/webroot/grafana/vendor/bootstrap/less/popovers.less
index 68d9552442..bbd5068ca1 100755
--- a/webroot/grafana/vendor/bootstrap/less/popovers.less
+++ b/webroot/grafana/vendor/bootstrap/less/popovers.less
@@ -39,7 +39,7 @@
 }
 
 .popover-title {
-  margin: 0; // reset heading margin
+  margin: 0; // clear heading margin
   padding: 8px 14px;
   font-size: 14px;
   font-weight: normal;
diff --git a/webroot/grafana/vendor/bootstrap/less/responsive-768px-979px.less b/webroot/grafana/vendor/bootstrap/less/responsive-768px-979px.less
index 047a34bab4..b716740908 100755
--- a/webroot/grafana/vendor/bootstrap/less/responsive-768px-979px.less
+++ b/webroot/grafana/vendor/bootstrap/less/responsive-768px-979px.less
@@ -13,6 +13,6 @@
   // Input grid
   #grid > .input(@gridColumnWidth768, @gridGutterWidth768);
 
-  // No need to reset .thumbnails here since it's the same @gridGutterWidth
+  // No need to clear .thumbnails here since it's the same @gridGutterWidth
 
 }
diff --git a/webroot/grafana/vendor/bootstrap/less/scaffolding.less b/webroot/grafana/vendor/bootstrap/less/scaffolding.less
index c836828e60..192b42eb1a 100755
--- a/webroot/grafana/vendor/bootstrap/less/scaffolding.less
+++ b/webroot/grafana/vendor/bootstrap/less/scaffolding.less
@@ -2,7 +2,7 @@
 // Scaffolding
 // --------------------------------------------------
 
-// Body reset
+// Body clear
 // -------------------------
 
 body {
