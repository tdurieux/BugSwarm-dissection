diff --git a/doc/developers/advanced_installation.rst b/doc/developers/advanced_installation.rst
index 29e8e54d275d..84a4e82adbd6 100644
--- a/doc/developers/advanced_installation.rst
+++ b/doc/developers/advanced_installation.rst
@@ -139,11 +139,11 @@ the ``--install-option="--prefix="`` flag is only required if python has a
 from source package
 ~~~~~~~~~~~~~~~~~~~
 
-download the source package from 
-`pypi <https://pypi.python.org/pypi/scikit-learn>`_,
-, unpack the sources and cd into the source directory.
+download the source package from
+`pypi <https://pypi.python.org/pypi/scikit-learn>`_, unpack the sources and
+cd into the source directory.
 
-this packages uses distutils, which is the default way of installing
+This packages uses distutils, which is the default way of installing
 python modules. the install command is::
 
     python setup.py install
@@ -183,7 +183,7 @@ or from a :ref:`python distribution <install_by_distribution>` instead.
 
 .. _install_by_distribution:
 
-third party distributions of scikit-learn
+Third party distributions of scikit-learn
 =========================================
 
 some third-party distributions are now providing versions of
diff --git a/doc/modules/classes.rst b/doc/modules/classes.rst
index 8020918e19e4..d5a32a52e791 100644
--- a/doc/modules/classes.rst
+++ b/doc/modules/classes.rst
@@ -803,6 +803,7 @@ details.
    metrics.average_precision_score
    metrics.brier_score_loss
    metrics.classification_report
+   metrics.cohen_kappa_score
    metrics.confusion_matrix
    metrics.f1_score
    metrics.fbeta_score
@@ -928,7 +929,7 @@ See the :ref:`metrics` section of the user guide for further details.
    metrics.pairwise.paired_manhattan_distances
    metrics.pairwise.paired_cosine_distances
    metrics.pairwise.paired_distances
-   
+
 
 .. _mixture_ref:
 
diff --git a/doc/modules/grid_search.rst b/doc/modules/grid_search.rst
index de66f8bc0290..cdade7dd2f31 100644
--- a/doc/modules/grid_search.rst
+++ b/doc/modules/grid_search.rst
@@ -101,7 +101,7 @@ For each parameter, either a distribution over possible values or a list of
 discrete choices (which will be sampled uniformly) can be specified::
 
   [{'C': scipy.stats.expon(scale=100), 'gamma': scipy.stats.expon(scale=.1),
-    'kernel': ['rbf'], 'class_weight':['auto', None]}]
+    'kernel': ['rbf'], 'class_weight':['balanced', None]}]
 
 This example uses the ``scipy.stats`` module, which contains many useful
 distributions for sampling parameters, such as ``expon``, ``gamma``,
diff --git a/doc/modules/linear_model.rst b/doc/modules/linear_model.rst
index 26805b8764f7..8344233f6130 100644
--- a/doc/modules/linear_model.rst
+++ b/doc/modules/linear_model.rst
@@ -742,7 +742,7 @@ In a nutshell, one may choose the solver with the following rules:
 Case                               Solver
 =================================  =============================
 Small dataset or L1 penalty        "liblinear"
-Multinomial loss or large dataset  "lbfgs", "sag" or newton-cg"
+Multinomial loss or large dataset  "lbfgs", "sag" or "newton-cg"
 Very Large dataset                 "sag"
 =================================  =============================
 For large dataset, you may also consider using :class:`SGDClassifier` with 'log' loss.
diff --git a/doc/modules/tree.rst b/doc/modules/tree.rst
index 118d22de7d29..cd355c8e4f5b 100644
--- a/doc/modules/tree.rst
+++ b/doc/modules/tree.rst
@@ -129,7 +129,6 @@ Once trained, we can export the tree in `Graphviz
 exporter. Below is an example export of a tree trained on the entire
 iris dataset::
 
-    >>> from sklearn.externals.six import StringIO
     >>> with open("iris.dot", 'w') as f:
     ...     f = tree.export_graphviz(clf, out_file=f)
 
diff --git a/sklearn/calibration.py b/sklearn/calibration.py
index f9b17628d5a0..22edc9db8224 100644
--- a/sklearn/calibration.py
+++ b/sklearn/calibration.py
@@ -33,9 +33,9 @@ class CalibratedClassifierCV(BaseEstimator, ClassifierMixin):
     cross-validation generator and the test set is used for calibration.
     The probabilities for each of the folds are then averaged
     for prediction. In case that cv="prefit" is passed to __init__,
-    it is it is assumed that base_estimator has been
-    fitted already and all data is used for calibration. Note that
-    data for fitting the classifier and for calibrating it must be disjoint.
+    it is assumed that base_estimator has been fitted already and all
+    data is used for calibration. Note that data for fitting the
+    classifier and for calibrating it must be disjoint.
 
     Read more in the :ref:`User Guide <calibration>`.
 
@@ -126,7 +126,7 @@ def fit(self, X, y, sample_weight=None):
         lb = LabelBinarizer().fit(y)
         self.classes_ = lb.classes_
 
-        # Check that we each cross-validation fold can have at least one
+        # Check that each cross-validation fold can have at least one
         # example per class
         n_folds = self.cv if isinstance(self.cv, int) \
             else self.cv.n_folds if hasattr(self.cv, "n_folds") else None
diff --git a/sklearn/cluster/_dbscan_inner.pyx b/sklearn/cluster/_dbscan_inner.pyx
index 2822da49110d..6bab988e920b 100644
--- a/sklearn/cluster/_dbscan_inner.pyx
+++ b/sklearn/cluster/_dbscan_inner.pyx
@@ -4,6 +4,7 @@
 
 cimport cython
 from libcpp.vector cimport vector
+from libcpp.set cimport set as cset
 cimport numpy as np
 import numpy as np
 
@@ -22,6 +23,7 @@ def dbscan_inner(np.ndarray[np.uint8_t, ndim=1, mode='c'] is_core,
     cdef np.npy_intp i, label_num = 0, v
     cdef np.ndarray[np.npy_intp, ndim=1] neighb
     cdef vector[np.npy_intp] stack
+    cdef cset[np.npy_intp] seen
 
     for i in range(labels.shape[0]):
         if labels[i] != -1 or not is_core[i]:
@@ -38,7 +40,8 @@ def dbscan_inner(np.ndarray[np.uint8_t, ndim=1, mode='c'] is_core,
                     neighb = neighborhoods[i]
                     for i in range(neighb.shape[0]):
                         v = neighb[i]
-                        if labels[v] == -1:
+                        if labels[v] == -1 and seen.count(v) == 0:
+                            seen.insert(v)
                             push(stack, v)
 
             if stack.size() == 0:
@@ -46,4 +49,6 @@ def dbscan_inner(np.ndarray[np.uint8_t, ndim=1, mode='c'] is_core,
             i = stack.back()
             stack.pop_back()
 
+        seen.clear()
+
         label_num += 1
diff --git a/sklearn/cluster/dbscan_.py b/sklearn/cluster/dbscan_.py
index 871b5679fe22..7976c8f5e6d3 100644
--- a/sklearn/cluster/dbscan_.py
+++ b/sklearn/cluster/dbscan_.py
@@ -196,6 +196,10 @@ class DBSCAN(BaseEstimator, ClusterMixin):
         to store the tree. The optimal value depends
         on the nature of the problem.
 
+    p : float, optional
+        The power of the Minkowski metric to be used to calculate distance
+        between points.
+
     n_jobs : int, optional (default = 1)
         The number of parallel jobs to run.
         If ``-1``, then the number of jobs is set to the number of CPU cores.
diff --git a/sklearn/cross_validation.py b/sklearn/cross_validation.py
index 29c900201710..67969f0bcec7 100644
--- a/sklearn/cross_validation.py
+++ b/sklearn/cross_validation.py
@@ -1548,7 +1548,7 @@ def _fit_and_score(estimator, X, y, scorer, train, test, verbose,
     """
     if verbose > 1:
         if parameters is None:
-            msg = "no parameters to be set"
+            msg = ''
         else:
             msg = '%s' % (', '.join('%s=%s' % (k, v)
                           for k, v in parameters.items()))
@@ -1648,6 +1648,13 @@ def _score(estimator, X_test, y_test, scorer):
         score = scorer(estimator, X_test)
     else:
         score = scorer(estimator, X_test, y_test)
+    if hasattr(score, 'item'):
+        try:
+            # e.g. unwrap memmapped scalars
+            score = score.item()
+        except ValueError:
+            # non-scalar?
+            pass
     if not isinstance(score, numbers.Number):
         raise ValueError("scoring must return a number, got %s (%s) instead."
                          % (str(score), type(score)))
diff --git a/sklearn/decomposition/nmf.py b/sklearn/decomposition/nmf.py
index 476252a85ff3..aa4fae9973e9 100644
--- a/sklearn/decomposition/nmf.py
+++ b/sklearn/decomposition/nmf.py
@@ -19,7 +19,6 @@
 import numpy as np
 import scipy.sparse as sp
 
-from ..externals import six
 from ..base import BaseEstimator, TransformerMixin
 from ..utils import check_random_state, check_array
 from ..utils.extmath import randomized_svd, safe_sparse_dot, squared_norm
@@ -747,11 +746,11 @@ def non_negative_factorization(X, W=None, H=None, n_components=None,
     if n_components is None:
         n_components = n_features
 
-    if not isinstance(n_components, six.integer_types) or n_components <= 0:
-        raise ValueError("Number of components must be positive;"
+    if not isinstance(n_components, numbers.Integral) or n_components <= 0:
+        raise ValueError("Number of components must be a positive integer;"
                          " got (n_components=%r)" % n_components)
-    if not isinstance(max_iter, numbers.Number) or max_iter < 0:
-        raise ValueError("Maximum number of iteration must be positive;"
+    if not isinstance(max_iter, numbers.Integral) or max_iter < 0:
+        raise ValueError("Maximum number of iterations must be a positive integer;"
                          " got (max_iter=%r)" % max_iter)
     if not isinstance(tol, numbers.Number) or tol < 0:
         raise ValueError("Tolerance for stopping criteria must be "
diff --git a/sklearn/decomposition/tests/test_nmf.py b/sklearn/decomposition/tests/test_nmf.py
index ce83db627c83..43ca1423b92c 100644
--- a/sklearn/decomposition/tests/test_nmf.py
+++ b/sklearn/decomposition/tests/test_nmf.py
@@ -7,7 +7,7 @@
 
 from sklearn.utils.testing import assert_true
 from sklearn.utils.testing import assert_false
-from sklearn.utils.testing import assert_raise_message
+from sklearn.utils.testing import assert_raise_message, assert_no_warnings
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_almost_equal
 from sklearn.utils.testing import assert_greater
@@ -133,7 +133,6 @@ def test_nmf_transform_custom_init():
     t = m.transform(A)
 
 
-
 @ignore_warnings
 def test_nmf_inverse_transform():
     # Test that NMF.inverse_transform returns close values
@@ -235,7 +234,10 @@ def test_non_negative_factorization_checking():
     A = np.ones((2, 2))
     # Test parameters checking is public function
     nnmf = non_negative_factorization
-    msg = "Number of components must be positive; got (n_components='2')"
+    assert_no_warnings(nnmf, A, A, A, np.int64(1))
+    msg = "Number of components must be a positive integer; got (n_components=1.5)"
+    assert_raise_message(ValueError, msg, nnmf, A, A, A, 1.5)
+    msg = "Number of components must be a positive integer; got (n_components='2')"
     assert_raise_message(ValueError, msg, nnmf, A, A, A, '2')
     msg = "Negative values in data passed to NMF (input H)"
     assert_raise_message(ValueError, msg, nnmf, A, A, -A, 2, 'custom')
diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py
index 3e92a83f166c..118b3a9acae7 100644
--- a/sklearn/ensemble/iforest.py
+++ b/sklearn/ensemble/iforest.py
@@ -10,6 +10,7 @@
 
 from scipy.sparse import issparse
 
+import numbers
 from ..externals import six
 from ..tree import ExtraTreeRegressor
 from ..utils import check_random_state, check_array
@@ -167,7 +168,7 @@ def fit(self, X, y=None, sample_weight=None):
                                  'Valid choices are: "auto", int or'
                                  'float' % self.max_samples)
 
-        elif isinstance(self.max_samples, six.integer_types):
+        elif isinstance(self.max_samples, numbers.Integral):
             if self.max_samples > n_samples:
                 warn("max_samples (%s) is greater than the "
                      "total number of samples (%s). max_samples "
@@ -277,7 +278,7 @@ def _average_path_length(n_samples_leaf):
     average_path_length : array, same shape as n_samples_leaf
 
     """
-    if isinstance(n_samples_leaf, six.integer_types):
+    if isinstance(n_samples_leaf, numbers.Integral):
         if n_samples_leaf <= 1:
             return 1.
         else:
diff --git a/sklearn/ensemble/tests/test_iforest.py b/sklearn/ensemble/tests/test_iforest.py
index 1d52208e09cd..3c486543f035 100644
--- a/sklearn/ensemble/tests/test_iforest.py
+++ b/sklearn/ensemble/tests/test_iforest.py
@@ -104,8 +104,9 @@ def test_iforest_error():
                          "max_samples will be set to n_samples for estimation",
                          IsolationForest(max_samples=1000).fit, X)
     assert_no_warnings(IsolationForest(max_samples='auto').fit, X)
-    assert_raises(ValueError,
-                  IsolationForest(max_samples='foobar').fit, X)
+    assert_no_warnings(IsolationForest(max_samples=np.int64(2)).fit, X)
+    assert_raises(ValueError, IsolationForest(max_samples='foobar').fit, X)
+    assert_raises(ValueError, IsolationForest(max_samples=1.5).fit, X)
 
 
 def test_recalculate_max_depth():
diff --git a/sklearn/feature_selection/univariate_selection.py b/sklearn/feature_selection/univariate_selection.py
index 1292252fdc8b..d6ecf1397423 100644
--- a/sklearn/feature_selection/univariate_selection.py
+++ b/sklearn/feature_selection/univariate_selection.py
@@ -355,6 +355,8 @@ class SelectPercentile(_BaseFilter):
     score_func : callable
         Function taking two arrays X and y, and returning a pair of arrays
         (scores, pvalues) or a single array with scores.
+        Default is f_classif (see below "See also"). The default function only
+        works with classification tasks.
 
     percentile : int, optional, default=10
         Percent of features to keep.
@@ -426,6 +428,8 @@ class SelectKBest(_BaseFilter):
     score_func : callable
         Function taking two arrays X and y, and returning a pair of arrays
         (scores, pvalues) or a single array with scores.
+        Default is f_classif (see below "See also"). The default function only
+        works with classification tasks.
 
     k : int or "all", optional, default=10
         Number of top features to select.
@@ -498,6 +502,8 @@ class SelectFpr(_BaseFilter):
     score_func : callable
         Function taking two arrays X and y, and returning a pair of arrays
         (scores, pvalues).
+        Default is f_classif (see below "See also"). The default function only
+        works with classification tasks.
 
     alpha : float, optional
         The highest p-value for features to be kept.
@@ -547,6 +553,8 @@ class SelectFdr(_BaseFilter):
     score_func : callable
         Function taking two arrays X and y, and returning a pair of arrays
         (scores, pvalues).
+        Default is f_classif (see below "See also"). The default function only
+        works with classification tasks.
 
     alpha : float, optional
         The highest uncorrected p-value for features to keep.
@@ -604,6 +612,8 @@ class SelectFwe(_BaseFilter):
     score_func : callable
         Function taking two arrays X and y, and returning a pair of arrays
         (scores, pvalues).
+        Default is f_classif (see below "See also"). The default function only
+        works with classification tasks.
 
     alpha : float, optional
         The highest uncorrected p-value for features to keep.
diff --git a/sklearn/linear_model/least_angle.py b/sklearn/linear_model/least_angle.py
index 9fce600950c9..129c763efcab 100644
--- a/sklearn/linear_model/least_angle.py
+++ b/sklearn/linear_model/least_angle.py
@@ -63,7 +63,7 @@ def lars_path(X, y, Xy=None, Gram=None, max_iter=500,
         When using this option together with method 'lasso' the model
         coefficients will not converge to the ordinary-least-squares solution
         for small values of alpha (neither will they when using method 'lar'
-        ..). Only coeffiencts up to the smallest alpha value (``alphas_[alphas_ >
+        ..). Only coefficients up to the smallest alpha value (``alphas_[alphas_ >
         0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso
         algorithm are typically in congruence with the solution of the
         coordinate descent lasso_path function.
diff --git a/sklearn/linear_model/ridge.py b/sklearn/linear_model/ridge.py
index cfaa7ee3fbd7..b3d14bd2e16b 100644
--- a/sklearn/linear_model/ridge.py
+++ b/sklearn/linear_model/ridge.py
@@ -210,8 +210,13 @@ def ridge_regression(X, y, alpha, sample_weight=None, solver='auto',
 
     alpha : {float, array-like},
         shape = [n_targets] if array-like
-        The l_2 penalty to be used. If an array is passed, penalties are
-        assumed to be specific to targets
+        Regularization strength; must be a positive float. Regularization
+        improves the conditioning of the problem and reduces the variance of
+        the estimates. Larger values specify stronger regularization.
+        Alpha corresponds to ``C^-1`` in other linear models such as 
+        LogisticRegression or LinearSVC. If an array is passed, penalties are
+        assumed to be specific to the targets. Hence they must correspond in
+        number.
 
     max_iter : int, optional
         Maximum number of iterations for conjugate gradient solver.
@@ -500,11 +505,13 @@ class Ridge(_BaseRidge, RegressorMixin):
     Parameters
     ----------
     alpha : {float, array-like}, shape (n_targets)
-        Small positive values of alpha improve the conditioning of the problem
-        and reduce the variance of the estimates.  Alpha corresponds to
-        ``C^-1`` in other linear models such as LogisticRegression or
-        LinearSVC. If an array is passed, penalties are assumed to be specific
-        to the targets. Hence they must correspond in number.
+        Regularization strength; must be a positive float. Regularization
+        improves the conditioning of the problem and reduces the variance of
+        the estimates. Larger values specify stronger regularization.
+        Alpha corresponds to ``C^-1`` in other linear models such as 
+        LogisticRegression or LinearSVC. If an array is passed, penalties are
+        assumed to be specific to the targets. Hence they must correspond in
+        number.
 
     copy_X : boolean, optional, default True
         If True, X will be copied; else, it may be overwritten.
@@ -643,10 +650,11 @@ class RidgeClassifier(LinearClassifierMixin, _BaseRidge):
     Parameters
     ----------
     alpha : float
-        Small positive values of alpha improve the conditioning of the problem
-        and reduce the variance of the estimates.  Alpha corresponds to
-        ``C^-1`` in other linear models such as LogisticRegression or
-        LinearSVC.
+        Regularization strength; must be a positive float. Regularization
+        improves the conditioning of the problem and reduces the variance of
+        the estimates. Larger values specify stronger regularization.
+        Alpha corresponds to ``C^-1`` in other linear models such as 
+        LogisticRegression or LinearSVC.
 
     class_weight : dict or 'balanced', optional
         Weights associated with classes in the form ``{class_label: weight}``.
@@ -1087,10 +1095,11 @@ class RidgeCV(_BaseRidgeCV, RegressorMixin):
     ----------
     alphas : numpy array of shape [n_alphas]
         Array of alpha values to try.
-        Small positive values of alpha improve the conditioning of the
-        problem and reduce the variance of the estimates.
-        Alpha corresponds to ``C^-1`` in other linear models such as
-        LogisticRegression or LinearSVC.
+        Regularization strength; must be a positive float. Regularization
+        improves the conditioning of the problem and reduces the variance of
+        the estimates. Larger values specify stronger regularization.
+        Alpha corresponds to ``C^-1`` in other linear models such as 
+        LogisticRegression or LinearSVC. 
 
     fit_intercept : boolean
         Whether to calculate the intercept for this model. If set
@@ -1188,10 +1197,11 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):
     ----------
     alphas : numpy array of shape [n_alphas]
         Array of alpha values to try.
-        Small positive values of alpha improve the conditioning of the
-        problem and reduce the variance of the estimates.
-        Alpha corresponds to ``C^-1`` in other linear models such as
-        LogisticRegression or LinearSVC.
+        Regularization strength; must be a positive float. Regularization
+        improves the conditioning of the problem and reduces the variance of
+        the estimates. Larger values specify stronger regularization.
+        Alpha corresponds to ``C^-1`` in other linear models such as 
+        LogisticRegression or LinearSVC. 
 
     fit_intercept : boolean
         Whether to calculate the intercept for this model. If set
diff --git a/sklearn/metrics/classification.py b/sklearn/metrics/classification.py
index 05905b808a7a..af04a518cbe2 100644
--- a/sklearn/metrics/classification.py
+++ b/sklearn/metrics/classification.py
@@ -271,7 +271,7 @@ def confusion_matrix(y_true, y_pred, labels=None, sample_weight=None):
 def cohen_kappa_score(y1, y2, labels=None, weights=None):
     """Cohen's kappa: a statistic that measures inter-annotator agreement.
 
-    This function computes Cohen's kappa [1], a score that expresses the level
+    This function computes Cohen's kappa [1]_, a score that expresses the level
     of agreement between two annotators on a classification problem. It is
     defined as
 
@@ -282,7 +282,9 @@ def cohen_kappa_score(y1, y2, labels=None, weights=None):
     assigned to any sample (the observed agreement ratio), and :math:`p_e` is
     the expected agreement when both annotators assign labels randomly.
     :math:`p_e` is estimated using a per-annotator empirical prior over the
-    class labels [2].
+    class labels [2]_.
+
+    Read more in the :ref:`User Guide <cohen_kappa>`.
 
     Parameters
     ----------
@@ -313,8 +315,11 @@ class labels [2].
     .. [1] J. Cohen (1960). "A coefficient of agreement for nominal scales".
            Educational and Psychological Measurement 20(1):37-46.
            doi:10.1177/001316446002000104.
-    .. [2] R. Artstein and M. Poesio (2008). "Inter-coder agreement for
-           computational linguistics". Computational Linguistic 34(4):555-596.
+    .. [2] `R. Artstein and M. Poesio (2008). "Inter-coder agreement for
+           computational linguistics". Computational Linguistics 34(4):555-596.
+           <http://www.mitpressjournals.org/doi/abs/10.1162/coli.07-034-R2#.V0J1MJMrIWo>`_
+    .. [3] `Wikipedia entry for the Cohen's kappa.
+            <https://en.wikipedia.org/wiki/Cohen%27s_kappa>`_
     """
     confusion = confusion_matrix(y1, y2, labels=labels)
     n_classes = confusion.shape[0]
@@ -1831,7 +1836,8 @@ def brier_score_loss(y_true, y_prob, sample_weight=None, pos_label=None):
 
     References
     ----------
-    https://en.wikipedia.org/wiki/Brier_score
+    .. [1] `Wikipedia entry for the Brier score.
+            <https://en.wikipedia.org/wiki/Brier_score>`_
     """
     y_true = column_or_1d(y_true)
     y_prob = column_or_1d(y_prob)
diff --git a/sklearn/mixture/__init__.py b/sklearn/mixture/__init__.py
index 8269ec4a31d9..3622518352ca 100644
--- a/sklearn/mixture/__init__.py
+++ b/sklearn/mixture/__init__.py
@@ -8,6 +8,7 @@
 from .dpgmm import DPGMM, VBGMM
 
 from .gaussian_mixture import GaussianMixture
+from .bayesian_mixture import BayesianGaussianMixture
 
 
 __all__ = ['DPGMM',
@@ -17,4 +18,5 @@
            'distribute_covar_matrix_to_match_covariance_type',
            'log_multivariate_normal_density',
            'sample_gaussian',
-           'GaussianMixture']
+           'GaussianMixture',
+           'BayesianGaussianMixture']
diff --git a/sklearn/mixture/bayesian_mixture.py b/sklearn/mixture/bayesian_mixture.py
new file mode 100644
index 000000000000..9db5b62fa20a
--- /dev/null
+++ b/sklearn/mixture/bayesian_mixture.py
@@ -0,0 +1,906 @@
+"""Bayesian Gaussian Mixture Model."""
+
+# Author: Wei Xue <xuewei4d@gmail.com>
+#         Thierry Guillemot <thierry.guillemot.work@gmail.com>
+
+import numpy as np
+from scipy import linalg
+from scipy.special import digamma, gammaln
+
+from .base import BaseMixture, _check_shape
+from .gaussian_mixture import _check_precision_matrix
+from .gaussian_mixture import _check_precision_positivity
+from .gaussian_mixture import _compute_precision_cholesky
+from .gaussian_mixture import _estimate_gaussian_parameters
+from ..utils import check_array
+from ..utils.validation import check_is_fitted
+
+
+def log_dirichlet_norm(alpha):
+    """Estimate the log of the Dirichlet distribution normalization term.
+
+    Parameters
+    ----------
+    alpha : array-like, shape (n_samples,)
+        The parameters values of the Dirichlet distribution.
+
+    Returns
+    -------
+    log_dirichlet_norm : float
+        The log normalization of the Dirichlet distribution.
+    """
+    return gammaln(np.sum(alpha)) - np.sum(gammaln(alpha))
+
+
+def log_wishart_norm(nu, precision_chol, n_features):
+    """Estimate the log of the Wishart distribution normalization term.
+
+    Parameters
+    ----------
+    nu : float
+        The parameters values of the Whishart distribution.
+
+    precision_chol : array-like, shape (n_features, n_features)
+        The Cholesky decomposition of the precision matrix.
+
+    n_features : int
+        The number of features.
+
+    Return
+    ------
+    log_wishart_norm : float
+        The log normalization of the Wishart distribution.
+    """
+    return (-nu * np.sum(np.log(np.diag(precision_chol))) -
+            nu * n_features * .5 * np.log(2.) -
+            n_features * (n_features - 1.) * .25 * np.log(np.pi) -
+            np.sum(gammaln(.5 * (nu + 1. - np.arange(1, n_features + 1.)))))
+
+
+def estimate_wishart_entropy(nu, precision_chol, log_lambda, n_features):
+    """Estimate the entropy of the Wishart distribution.
+
+    Parameters
+    ----------
+    nu : float
+        The parameters values of the Whishart distribution.
+
+    precision_chol : float
+        The Cholesky decomposition of the precision matrix.
+
+    log_lambda : array-like, shape (n_components,)
+        The log values of the Wishart lambda.
+
+    n_features : int
+        The number of features.
+
+    Return
+    ------
+    wishart_entropy : float
+        The entropy of the Wishart distribution.
+    """
+    return (- log_wishart_norm(nu, precision_chol, n_features) -
+            .5 * (nu - n_features - 1.) * log_lambda + .5 * nu * n_features)
+
+
+def gamma_entropy_spherical(a, b):
+    """Estimate the entropy of the Gamma distribution with 'diag' precision.
+
+    Parameters
+    ----------
+    a : array-like, shape (n_components,)
+
+    b : array-like, shape (n_components,)
+
+    Returns
+    -------
+    spherical_gamma_entropy : array-like, shape (n_components,)
+    """
+    return gammaln(a) - (a - 1.) * digamma(a) + np.log(b) + a
+
+
+def gamma_entropy_diag(a, b):
+    """The entropy of the Gamma distribution with 'diag' precision.
+
+    Parameters
+    ----------
+    a : array-like, shape (n_components,)
+
+    b : array-like, shape (n_components, n_features)
+
+    Returns
+    -------
+    diag_gamma_entropy : array-like, shape (n_components,)
+    """
+    return ((gammaln(a) - (a - 1.) * digamma(a) + a) * len(b) +
+            np.sum(np.log(b)))
+
+
+class BayesianGaussianMixture(BaseMixture):
+    """Variational estimation of a Gaussian mixture.
+
+    Variational inference for a Bayesian Gaussian mixture model probability
+    distribution. This class allows for easy and efficient inference
+    of an approximate posterior distribution over the parameters of a
+    Gaussian mixture model. The number of components can be inferred from the
+    data.
+
+    Read more in the :ref:`User Guide <bgmm>`.
+
+    Parameters
+    ----------
+    n_components: int, default to 1.
+        The number of mixture components.
+
+    covariance_type : {'full', 'tied', 'diag', 'spherical'},
+        defaults to 'full'.
+        String describing the type of covariance parameters to use.
+        Must be one of::
+        'full' (each component has its own general covariance matrix).
+        'tied' (all components share the same general covariance matrix),
+        'diag' (each component has its own diagonal covariance matrix),
+        'spherical' (each component has its own single variance),
+
+    tol : float, defaults to 1e-6.
+        The convergence threshold. EM iterations will stop when the
+        log_likelihood average gain is below this threshold.
+
+    reg_covar : float, defaults to 0.
+        Non-negative regularization added to the diagonal of covariance.
+        Allows to assure that the covariance matrices are all positive.
+
+    max_iter : int, default to 100.
+        The number of EM iterations to perform.
+
+    n_init : int, default to 1.
+        The number of initializations to perform. The best results is kept.
+
+    init_params : {'kmeans', 'random'}, defaults to 'kmeans'.
+        The method used to initialize the weights, the means and the
+        covariances.
+        Must be one of::
+        'kmeans' : responsibilities are initialized using kmeans.
+        'random' : responsibilities are initialized randomly.
+
+    alpha_init : float, optional.
+        The user-provided alpha prior parameter of the Dirichlet distribution.
+        If is None, the alpha prior is set to 1. / n_components.
+
+    beta_init : float, optional.
+        The user-provided beta prior parameter of the Gaussian
+        distribution. If it is None, beta prior is set to 1.
+
+    mean_init : array-like, shape (`n_features`,), optional
+        The user-provided mean prior of the Gaussian distribution.
+        If it is None, the mean prior is set to the mean of X.
+
+    nu_init : float, optional.
+        The user-provided nu prior parameter of the precision distribution.
+        If it is None, the nu prior is set to `n_features`.
+
+    covariance_init : float or array-like, optional
+        The user-provided covariance prior of the precision distribution.
+        If it is None, the covariance prior is initialized using the covariance
+        of X. The shape depends on `covariance_type`::
+            (`n_features`, `n_features`) if 'full',
+            (`n_features`, `n_features`) if 'tied',
+            (`n_features`)               if 'diag',
+            float                        if 'spherical'
+
+    random_state: RandomState or an int seed, defaults to None.
+        A random number generator instance.
+
+    warm_start : bool, default to False.
+        If 'warm_start' is True, the solution of the last fitting is used as
+        initialization for the next call of fit(). This can speed up
+        convergence when fit is called several time on similar problems.
+
+    verbose : int, default to 0.
+        Enable verbose output. If 1 then it prints the current
+        initialization and each iteration step. If greater than 1 then
+        it prints also the log probability and the time needed
+        for each step.
+
+    Attributes
+    ----------
+    weights_ : array-like, shape (`n_components`,)
+        The weights of each mixture components.
+
+    beta_ : array-like, shape (`n_components`, )
+        The beta parameters of the Gaussian distributions of the means.
+
+    means_ : array-like, shape (`n_components`, `n_features`)
+        The mean of each mixture component.
+
+    nu_ : array-like, shape (`n_components`,)
+        The nu parameters of the precision distribution.
+
+    covariances_ : array-like
+        The covariance of each mixture component.
+        The shape depends on `covariance_type`::
+            (n_components,)                        if 'spherical',
+            (n_features, n_features)               if 'tied',
+            (n_components, n_features)             if 'diag',
+            (n_components, n_features, n_features) if 'full'
+
+    precisions_ : array-like
+        The precision matrices for each component in the mixture. A precision
+        matrix is the inverse of a covariance matrix. A covariance matrix is
+        symmetric positive definite so the mixture of Gaussian can be
+        equivalently parameterized by the precision matrices. Storing the
+        precision matrices instead of the covariance matrices makes it more
+        efficient to compute the log-likelihood of new samples at test time.
+        The shape depends on `covariance_type`::
+            (n_components,)                        if 'spherical',
+            (n_features, n_features)               if 'tied',
+            (n_components, n_features)             if 'diag',
+            (n_components, n_features, n_features) if 'full'
+
+    precisions_cholesky_ : array-like
+        The cholesky decomposition of the precision matrices of each mixture
+        component. A precision matrix is the inverse of a covariance matrix.
+        A covariance matrix is symmetric positive definite so the mixture of
+        Gaussian can be equivalently parameterized by the precision matrices.
+        Storing the precision matrices instead of the covariance matrices makes
+        it more efficient to compute the log-likelihood of new samples at test
+        time. The shape depends on `covariance_type`::
+            (n_components,)                        if 'spherical',
+            (n_features, n_features)               if 'tied',
+            (n_components, n_features)             if 'diag',
+            (n_components, n_features, n_features) if 'full'
+
+    converged_ : bool
+        True when convergence was reached in fit(), False otherwise.
+
+    n_iter : int
+        Number of step used by the best fit of EM to reach the convergence.
+
+    See Also
+    --------
+    GaussianMixture : Finite Gaussian mixture fit with EM.
+    """
+
+    def __init__(self, n_components=1, covariance_type='full', tol=1e-6,
+                 reg_covar=0, max_iter=100, n_init=1, init_params='kmeans',
+                 alpha_init=None, beta_init=None, mean_init=None,
+                 nu_init=None, covariance_init=None, random_state=None,
+                 warm_start=False, verbose=0, verbose_interval=10):
+        super(BayesianGaussianMixture, self).__init__(
+            n_components=n_components, tol=tol, reg_covar=reg_covar,
+            max_iter=max_iter, n_init=n_init, init_params=init_params,
+            random_state=random_state, warm_start=warm_start,
+            verbose=verbose, verbose_interval=verbose_interval)
+
+        self.covariance_type = covariance_type
+        self.alpha_init = alpha_init
+        self.beta_init = beta_init
+        self.mean_init = mean_init
+        self.nu_init = nu_init
+        self.covariance_init = covariance_init
+
+    def _check_parameters(self, X):
+        """Check the Gaussian mixture parameters are well defined."""
+        if self.covariance_type not in ['spherical', 'tied', 'diag', 'full']:
+            raise ValueError("Invalid value for 'covariance_type': %s "
+                             "'covariance_type' should be in "
+                             "['spherical', 'tied', 'diag', 'full']"
+                             % self.covariance_type)
+
+    def _initialize(self, X, resp):
+        """Initialization of the mixture parameters.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+
+        resp : array-like, shape (n_samples, n_components)
+        """
+        nk, xk, sk = _estimate_gaussian_parameters(X, resp, self.reg_covar,
+                                                   self.covariance_type)
+
+        self._initialize_weights_distribution(nk)
+        self._initialize_means_distribution(X, nk, xk)
+        self._initialize_precisions_distribution(X, nk, xk, sk)
+        self._estimate_distribution_norms()
+
+    def _initialize_weights_distribution(self, nk):
+        """Initialize the parameter of the Dirichlet distribution.
+
+        Parameters
+        ----------
+        nk : array-like, shape (n_components,)
+        """
+        if self.alpha_init is None:
+            self._alpha_prior = 1. / self.n_components
+        elif self.alpha_init > 0.:
+            self._alpha_prior = self.alpha_init
+        else:
+            raise ValueError("The parameter 'alpha_init' should be "
+                             "greater than 0., but got %.3f."
+                             % self.alpha_init)
+
+        self._estimate_weights(nk)
+
+    def _estimate_weights(self, nk):
+        """Estimate the parameters of the Dirichlet distribution.
+
+        Parameters
+        ----------
+        nk : array-like, shape (n_components,)
+        """
+        self.alpha_ = self._alpha_prior + nk
+        self.alpha_ /= np.sum(self.alpha_)
+
+        # XXX Check if we can normalize here directly
+
+    def _initialize_means_distribution(self, X, nk, xk):
+        """Initialize the parameters of the Gaussian distribution.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+
+        nk : array-like, shape (n_components,)
+
+        xk : array-like, shape (n_components, n_features)
+        """
+        _, n_features = X.shape
+
+        if self.beta_init is None:
+            self._beta_prior = 1.
+        elif self.beta_init <= 0.:
+            raise ValueError("The parameter 'beta_init' should be "
+                             "greater than 0., but got %.3f."
+                             % self.beta_init)
+        else:
+            self._beta_prior = self.beta_init
+
+        if self.mean_init is None:
+            self._mean_prior = X.mean(axis=0)
+        else:
+            self._mean_prior = check_array(self.mean_init,
+                                           dtype=[np.float64, np.float32],
+                                           ensure_2d=False)
+            _check_shape(self._mean_prior, (n_features, ), 'means')
+        self._estimate_means(nk, xk)
+
+    def _estimate_means(self, nk, xk):
+        """Estimate the parameters of the Gaussian distribution.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+
+        nk : array-like, shape (n_components,)
+
+        xk : array-like, shape (n_components, n_features)
+        """
+        self.beta_ = self._beta_prior + nk
+        self.means_ = (self._beta_prior * self._mean_prior +
+                       nk[:, np.newaxis] * xk) / self.beta_[:, np.newaxis]
+
+    def _initialize_precisions_distribution(self, X, nk, xk, sk):
+        """Initialize the prior parameters of the precision distribution.
+
+        The precision distribution is the Wishart distribution for 'full' or
+        'tied' covariance models, or the Gamma distribution for 'diagonal' and
+        'spherical' covariance models.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+
+        nk : array-like, shape (n_components,)
+
+        xk : array-like, shape (n_components, n_features)
+        """
+        _, n_features = X.shape
+
+        if self.nu_init is None:
+            self._nu_prior = n_features
+        elif self.nu_init > n_features - 1.:
+            self._nu_prior = self.nu_init
+        else:
+            raise ValueError("The parameter 'nu_init' "
+                             "should be greater than %d, but got %.3f."
+                             % (n_features - 1, self.nu_init))
+
+        self._initialize_covariance_prior(X)
+
+        self._estimate_precisions(nk, xk, sk)
+
+    def _initialize_covariance_prior(self, X):
+        """Initialize `_covariance_prior` depending of `covariance_type`.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+
+        nk : array-like, shape (n_components,)
+
+        xk : array-like, shape (n_components, n_features)
+
+        sk : array-like
+            The shape depends of `covariance_type`:
+            'full' : (n_components, n_features, n_features)
+            'tied' : (n_features, n_features)
+            'diag' : (n_components, n_features)
+            'spherical' : (n_components,)
+        """
+        _, n_features = X.shape
+
+        if self.covariance_init is None:
+            self._covariance_prior = {
+                'full': np.eye(X.shape[1]),
+                'tied': np.eye(X.shape[1]),
+                'diag': .5 * np.diag(np.atleast_2d(np.cov(X.T, bias=1))),
+                'spherical': .5 * np.var(X, axis=0).mean()
+            }[self.covariance_type]
+
+        elif self.covariance_type in ['full', 'tied']:
+            self._covariance_prior = check_array(
+                self.covariance_init, dtype=[np.float64, np.float32],
+                ensure_2d=False)
+            _check_shape(self._covariance_prior, (n_features, n_features),
+                         '%s covariance_init' % self.covariance_type)
+            _check_precision_matrix(self._covariance_prior,
+                                    self.covariance_type)
+        elif self.covariance_type is 'diag':
+            self._covariance_prior = check_array(
+                self.covariance_init, dtype=[np.float64, np.float32],
+                ensure_2d=False)
+            _check_shape(self._covariance_prior, (n_features,),
+                         '%s covariance_init' % self.covariance_type)
+            _check_precision_positivity(self._covariance_prior,
+                                        self.covariance_type)
+        # spherical case
+        elif self.covariance_init > 0.:
+            self._covariance_prior = self.covariance_init
+        else:
+            raise ValueError("The parameter 'spherical covariance_init' "
+                             "should be greater than 0., but got %.3f."
+                             % self.covariance_init)
+
+    def _estimate_precisions(self, nk, xk, Sk):
+        """Estimate the precisions parameters of the precision distribution.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+
+        nk : array-like, shape (n_components,)
+
+        xk : array-like, shape (n_components, n_features)
+
+        sk : array-like
+            The shape depends of `covariance_type`:
+            'full' : (n_components, n_features, n_features)
+            'tied' : (n_features, n_features)
+            'diag' : (n_components, n_features)
+            'spherical' : (n_components,)
+        """
+        {"full": self._estimate_wishart_full,
+         "tied": self._estimate_wishart_tied,
+         "diag": self._estimate_gamma_diag,
+         "spherical": self._estimate_gamma_spherical
+         }[self.covariance_type](nk, xk, Sk)
+
+        self.precisions_cholesky_ = _compute_precision_cholesky(
+            self.covariances_, self.covariance_type)
+
+        if self.covariance_type is 'full':
+            self.precisions_ = np.array([
+                np.dot(prec_chol, prec_chol)
+                for prec_chol in self.precisions_cholesky_])
+        elif self.covariance_type is 'tied':
+            self.precisions_ = np.dot(self.precisions_cholesky_,
+                                      self.precisions_cholesky_.T)
+        else:
+            self.precisions_ = self.precisions_cholesky_ ** 2
+
+    def _estimate_wishart_full(self, nk, xk, Sk):
+        """Estimate the Wishart distribution parameters.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+
+        nk : array-like, shape (n_components,)
+
+        xk : array-like, shape (n_components, n_features)
+
+        sk : array-like, shape (n_components, n_features, n_features)
+        """
+        _, n_features = xk.shape
+
+        self.nu_ = self._nu_prior + nk
+
+        self.covariances_ = np.empty((self.n_components, n_features,
+                                      n_features))
+        for k in range(self.n_components):
+            diff = xk[k] - self._mean_prior
+            self.covariances_[k] = (self._covariance_prior + nk[k] * Sk[k] +
+                                    nk[k] * self._beta_prior / self.beta_[k] *
+                                    np.outer(diff, diff))
+
+        # XXX Check if we cannot directly normalized with nu
+        # self.covariances_ /= self.nu_[:, np.newaxis, np.newaxis]
+
+    def _estimate_wishart_tied(self, nk, xk, Sk):
+        """Estimate the Wishart distribution parameters.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+
+        nk : array-like, shape (n_components,)
+
+        xk : array-like, shape (n_components, n_features)
+
+        sk : array-like, shape (n_features, n_features)
+        """
+        _, n_features = xk.shape
+
+        self.nu_ = self._nu_prior + nk.sum() / self.n_components
+
+        diff = xk - self._mean_prior
+        self.covariances_ = (self._covariance_prior +
+                             Sk * nk.sum() / self.n_components +
+                             self._beta_prior / self.n_components *
+                             (np.dot((nk / self.beta_) * diff.T, diff)))
+        # XXX Check if we cannot directly normalized with nu
+
+    def _estimate_gamma_diag(self, nk, xk, Sk):
+        """Estimate the Gamma distribution parameters.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+
+        nk : array-like, shape (n_components,)
+
+        xk : array-like, shape (n_components, n_features)
+
+        sk : array-like, shape (n_components, n_features)
+        """
+        _, n_features = xk.shape
+
+        self.nu_ = self._nu_prior + .5 * nk
+
+        diff = xk - self._mean_prior
+        self.covariances_ = (
+            self._covariance_prior +
+            .5 * (nk[:, np.newaxis] * Sk +
+                  (nk * self._beta_prior / self.beta_)[:, np.newaxis] *
+                  np.square(diff)))
+        # XXX Check if we cannot directly normalized with nu
+
+    def _estimate_gamma_spherical(self, nk, xk, Sk):
+        """Estimate the Gamma distribution parameters.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+
+        nk : array-like, shape (n_components,)
+
+        xk : array-like, shape (n_components, n_features)
+
+        sk : array-like, shape (n_components,)
+        """
+        n_features = xk.shape[1]
+
+        self.nu_ = self._nu_prior + .5 * nk
+
+        diff = xk - self._mean_prior
+        self.covariances_ = (self._covariance_prior + .5 / n_features *
+                             (nk * Sk + (nk * self._beta_prior / self.beta_) *
+                              np.mean(np.square(diff), 1)))
+        # XXX Check if we cannot directly normalized with nu
+
+    def _estimate_distribution_norms(self):
+        """Estimate the distributions norm used to define the lowerbounds."""
+        # XXX check this is ok
+        n_features, = self._mean_prior.shape
+
+        self._log_dirichlet_norm_prior = log_dirichlet_norm(
+            self._alpha_prior * np.ones(self.n_components))
+
+        self._log_gaussian_norm_prior = (
+            .5 * n_features * np.log(self._beta_prior / (2. * np.pi)))
+
+        if self.covariance_type in ['full', 'tied']:
+            # Computation of the cholesky decomposition of the precision matrix
+            try:
+                covariance_chol = linalg.cholesky(self._covariance_prior,
+                                                  lower=True)
+            except linalg.LinAlgError:
+                raise ValueError("Invalid value for 'covariance_init'. The "
+                                 "'covariance_init' should be a full rank.")
+
+            precision_prior_chol = linalg.solve_triangular(
+                covariance_chol, np.eye(n_features), lower=True).T
+            self._log_wishart_norm_prior = (
+                log_wishart_norm(self._nu_prior, precision_prior_chol,
+                                 n_features))
+
+        elif self.covariance_type == 'diag':
+            # lambda_inv_W_prior has n_feature Gamma distribution
+            self._log_gamma_norm_prior = (
+                self._nu_prior * np.sum(np.log(self._covariance_prior)) -
+                len(self._covariance_prior) * gammaln(self._nu_prior))
+        elif self.covariance_type == 'spherical':
+            # lambda_inv_W_prior has only 1 Gamma distribution
+            self._log_gamma_norm_prior = (
+                self._nu_prior * np.log(self._covariance_prior) -
+                gammaln(self._nu_prior))
+
+    def _check_is_fitted(self):
+        check_is_fitted(self, ['alpha_', 'beta_', 'means_', 'nu_',
+                               'covariances_', 'precisions_',
+                               'precisions_cholesky_'])
+
+    def _m_step(self, X, resp):
+        nk, xk, sk = _estimate_gaussian_parameters(X, resp, self.reg_covar,
+                                                   self.covariance_type)
+
+        self._estimate_weights(nk)
+        self._estimate_means(nk, xk)
+        self._estimate_precisions(nk, xk, sk)
+
+    def _e_step(self, X):
+        _, log_prob, log_resp = self._estimate_log_prob_resp(X)
+        resp = np.exp(log_resp)
+        self._lower_bound = self._estimate_lower_bound(log_prob, resp,
+                                                       log_resp)
+        return self._lower_bound, resp
+
+    def _estimate_log_weights(self):
+        # save the value for computing the lower bound
+        self._log_pi = digamma(self.alpha_) - digamma(np.sum(self.alpha_))
+        return self._log_pi
+
+    def _estimate_log_prob(self, X):
+        return {"full": self._estimate_log_prob_full,
+                "tied": self._estimate_log_prob_tied,
+                "diag": self._estimate_log_prob_diag,
+                "spherical": self._estimate_log_prob_spherical
+                }[self.covariance_type](X)
+
+    def _estimate_log_prob_full(self, X):
+        # second item in Equation 3.10
+        n_samples, n_features = X.shape
+
+        log_prob = np.empty((n_samples, self.n_components))
+        self._log_lambda = np.empty((self.n_components, ))
+
+        for k in range(self.n_components):
+            log_det_precisions = -2. * np.sum(np.log(np.diag(
+                self.precisions_cholesky_[k])))
+            # Equation 3.43
+            self._log_lambda[k] = (
+                np.sum(digamma(.5 * (self.nu_[k] -
+                                     np.arange(0, n_features)))) +
+                n_features * np.log(2.) - log_det_precisions)
+
+            y = np.dot(X - self.means_[k], self.precisions_cholesky_[k])
+            mahala_dist = np.sum(np.square(y), axis=1)
+
+            log_prob[:, k] = -.5 * (self._log_lambda[k] +
+                                    n_features / self.beta_[k] +
+                                    self.nu_[k] * mahala_dist)
+        log_prob -= .5 * n_features * np.log(2. * np.pi)
+
+        return log_prob
+
+    def _estimate_log_prob_tied(self, X):
+        n_samples, n_features = X.shape
+
+        log_prob = np.empty((n_samples, self.n_components))
+
+        log_det_precisions = -2. * np.sum(np.log(np.diag(
+            self.precisions_cholesky_)))
+
+        self._log_lambda = (
+            np.sum(digamma(.5 * (self.nu_ + np.arange(0, n_features)))) +
+            n_features * np.log(2) - log_det_precisions)
+
+        for k in range(self.n_components):
+            y = np.dot(X - self.means_[k], self.precisions_cholesky_)
+            mahala_dist = np.sum(np.square(y), axis=1)
+
+            log_prob[:, k] = -.5 * (- self._log_lambda +
+                                    n_features / self.beta_[k] +
+                                    self.nu_ * mahala_dist)
+        log_prob -= .5 * n_features * np.log(2. * np.pi)
+
+        return log_prob
+
+    def _estimate_log_prob_diag(self, X):
+        _, n_features = X.shape
+        self._log_lambda = (n_features * digamma(self.nu_) +
+                            np.sum(np.log(self.precisions_), axis=1))
+
+        log_gauss = self.nu_ * (
+            np.sum((self.means_ * self.precisions_cholesky_) ** 2, axis=1) -
+            2. * np.dot(X, (self.means_ * self.precisions_).T) +
+            np.dot(X ** 2, self.precisions_.T))
+
+        return -.5 * (n_features * np.log(2. * np.pi) - self._log_lambda +
+                      n_features / self.beta_ + log_gauss)
+
+    def _estimate_log_prob_spherical(self, X):
+        _, n_features = X.shape
+
+        self._log_lambda = n_features * (digamma(self.nu_) +
+                                         np.log(self.precisions_))
+
+        log_gauss = self.nu_ * self.precisions_ * (
+            np.sum((self.means_ ** 2), 1) - 2. * np.dot(X, self.means_.T) +
+            np.sum(X ** 2, 1)[:, np.newaxis])
+
+        return -.5 * (n_features * np.log(2. * np.pi) - self._log_lambda +
+                      n_features / self.beta_ + log_gauss)
+
+    def _estimate_lower_bound(self, log_prob, resp, log_resp):
+        """Estimate the lower bound of the model to check the convergence."""
+        # Equation 7.5, 7.6
+        log_p_XZ = np.sum(log_prob * resp)
+        # Equation 7.7
+        log_p_weight = ((self._alpha_prior - 1.) * np.sum(self._log_pi) +
+                        self._log_dirichlet_norm_prior)
+        log_p_lambda = self._estimate_p_lambda()
+
+        # Equation 7.10
+        log_q_z = np.sum(resp * log_resp)
+        # Equation 7.11
+        log_q_weight = (np.sum((self.alpha_ - 1.) * self._log_pi) +
+                        log_dirichlet_norm(self.alpha_))
+        log_q_lambda = self._estimate_q_lambda()
+
+        return (log_p_XZ + log_p_weight + log_p_lambda -
+                log_q_z - log_q_weight - log_q_lambda)
+
+    def _estimate_p_lambda(self):
+        return {'full': self._estimate_p_lambda_full,
+                'tied': self._estimate_p_lambda_tied,
+                'diag': self._estimate_p_lambda_diag,
+                'spherical': self._estimate_p_lambda_spherical
+                }[self.covariance_type]()
+
+    def _estimate_p_lambda_full(self):
+        n_features, = self._mean_prior.shape
+        # Equation 7.9
+        temp1 = np.empty(self.n_components)
+        for k in range(self.n_components):
+            y = np.dot(self.means_[k] - self._mean_prior,
+                       self.precisions_cholesky_[k])
+            temp1[k] = np.sum(np.square(y))
+
+        temp1 = (self.n_components * self._log_gaussian_norm_prior +
+                 .5 * np.sum(self._log_lambda -
+                             self._beta_prior * self.nu_ * temp1 -
+                             n_features * self._beta_prior / self.beta_))
+
+        temp2 = (self.n_components * self._log_wishart_norm_prior +
+                 .5 * (self._nu_prior - n_features - 1.) *
+                 np.sum(self._log_lambda))
+
+        trace_W0invW = np.empty(self.n_components)
+        for k, precision in enumerate(self.precisions_):
+            trace_W0invW[k] = np.sum((self._covariance_prior * precision))
+        temp3 = -.5 * np.sum(self.nu_ * trace_W0invW)
+
+        return temp1 + temp2 + temp3
+
+    def _estimate_p_lambda_tied(self):
+        n_features, = self._mean_prior.shape
+
+        temp1 = np.empty(self.n_components)
+        for k in range(self.n_components):
+            y = np.dot(self.means_[k] - self._mean_prior,
+                       self.precisions_cholesky_)
+            temp1[k] = np.sum(np.square(y))
+
+        temp1 = (self.n_components * self._log_gaussian_norm_prior +
+                 .5 * (self.n_components * self._log_lambda -
+                       np.sum(self._beta_prior * self.nu_ * temp1 +
+                              n_features * self._beta_prior / self.beta_)))
+
+        temp2 = (self.n_components * self._log_wishart_norm_prior +
+                 .5 * (self._nu_prior - n_features - 1.) *
+                 self.n_components * self._log_lambda)
+
+        trace_W0inv_W = np.sum(self._covariance_prior * self.precisions_)
+        temp3 = -.5 * self.n_components * self.nu_ * trace_W0inv_W
+
+        return temp1 + temp2 + temp3
+
+    def _estimate_p_lambda_diag(self):
+        n_features, = self._mean_prior.shape
+        sum_y = np.sum(np.square(self.means_ - self._mean_prior) *
+                       self.precisions_, axis=1)
+        temp1 = (self.n_components * self._log_gaussian_norm_prior +
+                 .5 * np.sum(self._log_lambda - self._beta_prior *
+                             (n_features / self.beta_ + self.nu_ * sum_y)))
+
+        temp2 = (self.n_components * self._log_gamma_norm_prior +
+                 (self._nu_prior - 1.) * np.sum(self._log_lambda))
+
+        temp3 = -np.sum(self.nu_ * np.sum(self._covariance_prior *
+                                          self.precisions_, axis=1))
+        return temp1 + temp2 + temp3
+
+    def _estimate_p_lambda_spherical(self):
+        n_features, = self._mean_prior.shape
+        sum_y = self.precisions_ * np.sum(np.square(self.means_ -
+                                                    self._mean_prior), axis=1)
+
+        temp1 = (self.n_components * self._log_gaussian_norm_prior +
+                 .5 * np.sum(self._log_lambda - self._beta_prior *
+                             (n_features / self.beta_ + self.nu_ * sum_y)))
+
+        temp2 = (self.n_components * self._log_gamma_norm_prior +
+                 (self._nu_prior - 1.) * np.sum(self._log_lambda))
+
+        temp3 = np.sum(- self.nu_ * self._covariance_prior * self.precisions_)
+
+        return temp1 + temp2 + temp3
+
+    def _estimate_q_lambda(self):
+        return {'full': self._estimate_q_lambda_full,
+                'tied': self._estimate_q_lambda_tied,
+                'diag': self._estimate_q_lambda_diag,
+                'spherical': self._estimate_q_lambda_spherical
+                }[self.covariance_type]()
+
+    def _estimate_q_lambda_full(self):
+        n_features, = self._mean_prior.shape
+        wishart_entropy = np.empty(self.n_components)
+        for k in range(self.n_components):
+            wishart_entropy[k] = estimate_wishart_entropy(
+                self.nu_[k], self.precisions_cholesky_[k],
+                self._log_lambda[k], n_features)
+        return np.sum(.5 * self._log_lambda +
+                      .5 * n_features * np.log(self.beta_ / (2. * np.pi)) -
+                      .5 * n_features - wishart_entropy)
+
+    def _estimate_q_lambda_tied(self):
+        n_features, = self._mean_prior.shape
+        wishart_entropy = estimate_wishart_entropy(
+            self.nu_, self.precisions_cholesky_, self._log_lambda, n_features)
+        return (.5 * self.n_components * self._log_lambda +
+                .5 * n_features * np.sum(np.log(self.beta_ / (2. * np.pi))) -
+                .5 * n_features * self.n_components -
+                self.n_components * wishart_entropy)
+
+    def _estimate_q_lambda_diag(self):
+        n_features, = self._mean_prior.shape
+        return np.sum(
+            .5 * self._log_lambda +
+            .5 * n_features * (np.log(self.beta_ / (2. * np.pi)) - 1.) -
+            gamma_entropy_diag(self.nu_, self.precisions_))
+
+    def _estimate_q_lambda_spherical(self):
+        n_features = self._mean_prior.shape[0]
+        return np.sum(
+            .5 * self._log_lambda +
+            .5 * n_features * (np.log(self.beta_ / (2. * np.pi)) - 1.) -
+            n_features * gamma_entropy_spherical(self.nu_, self.precisions_))
+
+    def _get_parameters(self):
+        return (self.alpha_, self.beta_, self.means_, self.nu_,
+                self.covariances_, self.precisions_, self.precisions_cholesky_)
+
+    def _set_parameters(self, params):
+        (self.alpha_, self.beta_, self.means_, self.nu_, self.covariances_,
+         self.precisions_, self.precisions_cholesky_) = params
+
+        # Attributes computation
+        self. weights_ = self.alpha_ / np.sum(self.alpha_)
+        if self.covariance_type is 'full':
+            self.covariances_ /= self.nu_[:, np.newaxis, np.newaxis]
+        elif self.covariance_type is 'diag':
+            self.covariances_ /= self.nu_[:, np.newaxis]
+        else:
+            self.covariances_ /= self.nu_
diff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py
index 87e215ee2c9f..e6d3d2737747 100644
--- a/sklearn/mixture/gaussian_mixture.py
+++ b/sklearn/mixture/gaussian_mixture.py
@@ -137,14 +137,9 @@ def _check_precisions(precisions, covariance_type, n_components, n_features):
 
 ###############################################################################
 # Gaussian mixture parameters estimators (used by the M-Step)
-ESTIMATE_PRECISION_ERROR_MESSAGE = ("The algorithm has diverged because of "
-                                    "too few samples per components. Try to "
-                                    "decrease the number of components, "
-                                    "or increase reg_covar.")
 
-
-def _estimate_gaussian_precisions_cholesky_full(resp, X, nk, means, reg_covar):
-    """Estimate the full precision matrices.
+def _estimate_gaussian_covariances_full(resp, X, nk, means, reg_covar):
+    """Estimate the full covariance matrices.
 
     Parameters
     ----------
@@ -160,27 +155,20 @@ def _estimate_gaussian_precisions_cholesky_full(resp, X, nk, means, reg_covar):
 
     Returns
     -------
-    precisions_chol : array, shape (n_components, n_features, n_features)
-        The cholesky decomposition of the precision matrix.
+    covariances : array, shape (n_components, n_features, n_features)
+        The covariance matrix of the current components.
     """
     n_components, n_features = means.shape
-    precisions_chol = np.empty((n_components, n_features, n_features))
+    covariances = np.empty((n_components, n_features, n_features))
     for k in range(n_components):
         diff = X - means[k]
-        covariance = np.dot(resp[:, k] * diff.T, diff) / nk[k]
-        covariance.flat[::n_features + 1] += reg_covar
-        try:
-            cov_chol = linalg.cholesky(covariance, lower=True)
-        except linalg.LinAlgError:
-            raise ValueError(ESTIMATE_PRECISION_ERROR_MESSAGE)
-        precisions_chol[k] = linalg.solve_triangular(cov_chol,
-                                                     np.eye(n_features),
-                                                     lower=True).T
-    return precisions_chol
+        covariances[k] = np.dot(resp[:, k] * diff.T, diff) / nk[k]
+        covariances[k].flat[::n_features + 1] += reg_covar
+    return covariances
 
 
-def _estimate_gaussian_precisions_cholesky_tied(resp, X, nk, means, reg_covar):
-    """Estimate the tied precision matrix.
+def _estimate_gaussian_covariances_tied(resp, X, nk, means, reg_covar):
+    """Estimate the tied covariance matrix.
 
     Parameters
     ----------
@@ -196,26 +184,20 @@ def _estimate_gaussian_precisions_cholesky_tied(resp, X, nk, means, reg_covar):
 
     Returns
     -------
-    precisions_chol : array, shape (n_features, n_features)
-        The cholesky decomposition of the precision matrix.
+    covariance : array, shape (n_features, n_features)
+        The tied covariance matrix of the components.
     """
-    n_samples, n_features = X.shape
+    n_samples, _ = X.shape
     avg_X2 = np.dot(X.T, X)
     avg_means2 = np.dot(nk * means.T, means)
-    covariances = avg_X2 - avg_means2
-    covariances /= n_samples
-    covariances.flat[::len(covariances) + 1] += reg_covar
-    try:
-        cov_chol = linalg.cholesky(covariances, lower=True)
-    except linalg.LinAlgError:
-        raise ValueError(ESTIMATE_PRECISION_ERROR_MESSAGE)
-    precisions_chol = linalg.solve_triangular(cov_chol, np.eye(n_features),
-                                              lower=True).T
-    return precisions_chol
+    covariance = avg_X2 - avg_means2
+    covariance /= n_samples
+    covariance.flat[::len(covariance) + 1] += reg_covar
+    return covariance
 
 
-def _estimate_gaussian_precisions_cholesky_diag(resp, X, nk, means, reg_covar):
-    """Estimate the diagonal precision matrices.
+def _estimate_gaussian_covariances_diag(resp, X, nk, means, reg_covar):
+    """Estimate the diagonal covariance vectors.
 
     Parameters
     ----------
@@ -231,21 +213,17 @@ def _estimate_gaussian_precisions_cholesky_diag(resp, X, nk, means, reg_covar):
 
     Returns
     -------
-    precisions_chol : array, shape (n_components, n_features)
-        The cholesky decomposition of the precision matrix.
+    covariances : array, shape (n_components, n_features)
+        The covariance vector of the current components.
     """
     avg_X2 = np.dot(resp.T, X * X) / nk[:, np.newaxis]
     avg_means2 = means ** 2
     avg_X_means = means * np.dot(resp.T, X) / nk[:, np.newaxis]
-    covariances = avg_X2 - 2 * avg_X_means + avg_means2 + reg_covar
-    if np.any(np.less_equal(covariances, 0.0)):
-        raise ValueError(ESTIMATE_PRECISION_ERROR_MESSAGE)
-    return 1. / np.sqrt(covariances)
+    return avg_X2 - 2 * avg_X_means + avg_means2 + reg_covar
 
 
-def _estimate_gaussian_precisions_cholesky_spherical(resp, X, nk, means,
-                                                     reg_covar):
-    """Estimate the spherical precision matrices.
+def _estimate_gaussian_covariances_spherical(resp, X, nk, means, reg_covar):
+    """Estimate the spherical variance values.
 
     Parameters
     ----------
@@ -261,16 +239,11 @@ def _estimate_gaussian_precisions_cholesky_spherical(resp, X, nk, means,
 
     Returns
     -------
-    precisions_chol : array, shape (n_components,)
-        The cholesky decomposition of the precision matrix.
+    variances : array, shape (n_components,)
+        The variance values of each components.
     """
-    avg_X2 = np.dot(resp.T, X * X) / nk[:, np.newaxis]
-    avg_means2 = means ** 2
-    avg_X_means = means * np.dot(resp.T, X) / nk[:, np.newaxis]
-    covariances = (avg_X2 - 2 * avg_X_means + avg_means2 + reg_covar).mean(1)
-    if np.any(np.less_equal(covariances, 0.0)):
-        raise ValueError(ESTIMATE_PRECISION_ERROR_MESSAGE)
-    return 1. / np.sqrt(covariances)
+    return _estimate_gaussian_covariances_diag(resp, X, nk,
+                                               means, reg_covar).mean(1)
 
 
 def _estimate_gaussian_parameters(X, resp, reg_covar, covariance_type):
@@ -292,29 +265,77 @@ def _estimate_gaussian_parameters(X, resp, reg_covar, covariance_type):
 
     Returns
     -------
-    nk : array, shape (n_components,)
+    nk : array-like, shape (n_components,)
         The numbers of data samples in the current components.
 
-    means : array, shape (n_components, n_features)
+    means : array-like, shape (n_components, n_features)
         The centers of the current components.
 
-    precisions_cholesky : array
-        The cholesky decomposition of sample precisions of the current
-        components. The shape depends of the covariance_type.
+    covariances : array-like
+        The covariance matrix of the current components.
+        The shape depends of the covariance_type.
     """
     nk = resp.sum(axis=0) + 10 * np.finfo(resp.dtype).eps
     means = np.dot(resp.T, X) / nk[:, np.newaxis]
-    precs_chol = {"full": _estimate_gaussian_precisions_cholesky_full,
-                  "tied": _estimate_gaussian_precisions_cholesky_tied,
-                  "diag": _estimate_gaussian_precisions_cholesky_diag,
-                  "spherical": _estimate_gaussian_precisions_cholesky_spherical
-                  }[covariance_type](resp, X, nk, means, reg_covar)
-    return nk, means, precs_chol
+    covariances = {"full": _estimate_gaussian_covariances_full,
+                   "tied": _estimate_gaussian_covariances_tied,
+                   "diag": _estimate_gaussian_covariances_diag,
+                   "spherical": _estimate_gaussian_covariances_spherical
+                   }[covariance_type](resp, X, nk, means, reg_covar)
+    return nk, means, covariances
+
+
+def _compute_precision_cholesky(covariances, covariance_type):
+    """Compute the Cholesky decomposition of the precisions.
+
+    Parameters
+    ----------
+    covariances : array-like
+        The covariance matrix of the current components.
+        The shape depends of the covariance_type.
+
+    covariance_type : {'full', 'tied', 'diag', 'spherical'}
+        The type of precision matrices.
+
+    Returns
+    -------
+    precisions_cholesky : array-like
+        The cholesky decomposition of sample precisions of the current
+        components. The shape depends of the covariance_type.
+    """
+    estimate_precision_error_message = (
+        "The algorithm has diverged because of too few samples per "
+        "components. Try to decrease the number of components, "
+        "or increase reg_covar.")
+
+    if covariance_type in 'full':
+        n_components, n_features, _ = covariances.shape
+        precisions_chol = np.empty((n_components, n_features, n_features))
+        for k, covariance in enumerate(covariances):
+            try:
+                cov_chol = linalg.cholesky(covariance, lower=True)
+            except linalg.LinAlgError:
+                raise ValueError(estimate_precision_error_message)
+            precisions_chol[k] = linalg.solve_triangular(cov_chol,
+                                                         np.eye(n_features),
+                                                         lower=True).T
+    elif covariance_type is 'tied':
+        _, n_features = covariances.shape
+        try:
+            cov_chol = linalg.cholesky(covariances, lower=True)
+        except linalg.LinAlgError:
+            raise ValueError(estimate_precision_error_message)
+        precisions_chol = linalg.solve_triangular(cov_chol, np.eye(n_features),
+                                                  lower=True).T
+    else:
+        if np.any(np.less_equal(covariances, 0.0)):
+            raise ValueError(estimate_precision_error_message)
+        precisions_chol = 1. / np.sqrt(covariances)
+    return precisions_chol
 
 
 ###############################################################################
 # Gaussian mixture probability estimators
-
 def _estimate_log_gaussian_prob_full(X, means, precisions_chol):
     """Estimate the log Gaussian probability for 'full' precision.
 
@@ -497,13 +518,13 @@ class GaussianMixture(BaseMixture):
 
     Attributes
     ----------
-    weights_ : array, shape (n_components,)
+    weights_ : array-like, shape (n_components,)
         The weights of each mixture components.
 
-    means_ : array, shape (n_components, n_features)
+    means_ : array-like, shape (n_components, n_features)
         The mean of each mixture component.
 
-    covariances_ : array
+    covariances_ : array-like
         The covariance of each mixture component.
         The shape depends on `covariance_type`::
             (n_components,)                        if 'spherical',
@@ -511,7 +532,7 @@ class GaussianMixture(BaseMixture):
             (n_components, n_features)             if 'diag',
             (n_components, n_features, n_features) if 'full'
 
-    precisions_ : array
+    precisions_ : array-like
         The precision matrices for each component in the mixture. A precision
         matrix is the inverse of a covariance matrix. A covariance matrix is
         symmetric positive definite so the mixture of Gaussian can be
@@ -524,7 +545,7 @@ class GaussianMixture(BaseMixture):
             (n_components, n_features)             if 'diag',
             (n_components, n_features, n_features) if 'full'
 
-    precisions_cholesky_ : array
+    precisions_cholesky_ : array-like
         The cholesky decomposition of the precision matrices of each mixture
         component. A precision matrix is the inverse of a covariance matrix.
         A covariance matrix is symmetric positive definite so the mixture of
@@ -542,6 +563,11 @@ class GaussianMixture(BaseMixture):
 
     n_iter_ : int
         Number of step used by the best fit of EM to reach the convergence.
+
+    See Also
+    --------
+    BayesianGaussianMixture : Finite gaussian mixture model fit with a
+        variational algorithm.
     """
 
     def __init__(self, n_components=1, covariance_type='full', tol=1e-3,
@@ -594,7 +620,7 @@ def _initialize(self, X, resp):
         """
         n_samples, _ = X.shape
 
-        weights, means, precisions_cholesky = _estimate_gaussian_parameters(
+        weights, means, covariances = _estimate_gaussian_parameters(
             X, resp, self.reg_covar, self.covariance_type)
         weights /= n_samples
 
@@ -603,7 +629,9 @@ def _initialize(self, X, resp):
         self.means_ = means if self.means_init is None else self.means_init
 
         if self.precisions_init is None:
-            self.precisions_cholesky_ = precisions_cholesky
+            self.covariances_ = covariances
+            self.precisions_cholesky_ = _compute_precision_cholesky(
+                covariances, self.covariance_type)
         elif self.covariance_type is 'full':
             self.precisions_cholesky_ = np.array(
                 [linalg.cholesky(prec_init, lower=True)
@@ -619,10 +647,13 @@ def _e_step(self, X):
         return np.mean(log_prob_norm), np.exp(log_resp)
 
     def _m_step(self, X, resp):
-        self.weights_, self.means_, self.precisions_cholesky_ = (
+        n_samples, _ = X.shape
+        self.weights_, self.means_, self.covariances_ = (
             _estimate_gaussian_parameters(X, resp, self.reg_covar,
                                           self.covariance_type))
-        self.weights_ /= X.shape[0]
+        self.weights_ /= n_samples
+        self.precisions_cholesky_ = _compute_precision_cholesky(
+            self.covariances_, self.covariance_type)
 
     def _estimate_log_prob(self, X):
         return {"full": _estimate_log_gaussian_prob_full,
@@ -649,22 +680,14 @@ def _set_parameters(self, params):
 
         if self.covariance_type is 'full':
             self.precisions_ = np.empty(self.precisions_cholesky_.shape)
-            self.covariances_ = np.empty(self.precisions_cholesky_.shape)
             for k, prec_chol in enumerate(self.precisions_cholesky_):
                 self.precisions_[k] = np.dot(prec_chol, prec_chol.T)
-                cov_chol = linalg.solve_triangular(prec_chol,
-                                                   np.eye(n_features))
-                self.covariances_[k] = np.dot(cov_chol.T, cov_chol)
 
         elif self.covariance_type is 'tied':
             self.precisions_ = np.dot(self.precisions_cholesky_,
                                       self.precisions_cholesky_.T)
-            cov_chol = linalg.solve_triangular(self.precisions_cholesky_,
-                                               np.eye(n_features))
-            self.covariances_ = np.dot(cov_chol.T, cov_chol)
         else:
             self.precisions_ = self.precisions_cholesky_ ** 2
-            self.covariances_ = 1. / self.precisions_
 
     def _n_parameters(self):
         """Return the number of free parameters in the model."""
diff --git a/sklearn/mixture/tests/test_bayesian_mixture.py b/sklearn/mixture/tests/test_bayesian_mixture.py
new file mode 100644
index 000000000000..39df6569b45e
--- /dev/null
+++ b/sklearn/mixture/tests/test_bayesian_mixture.py
@@ -0,0 +1,306 @@
+import numpy as np
+from scipy import linalg
+from scipy.special import gammaln, digamma
+
+from sklearn.datasets.samples_generator import make_spd_matrix
+from sklearn.utils.testing import assert_array_almost_equal
+from sklearn.utils.testing import assert_raise_message
+from sklearn.utils.testing import assert_almost_equal
+
+from sklearn.mixture.bayesian_mixture import log_dirichlet_norm
+from sklearn.mixture.bayesian_mixture import log_wishart_norm
+from sklearn.mixture.bayesian_mixture import estimate_wishart_entropy
+from sklearn.mixture.bayesian_mixture import gamma_entropy_spherical
+from sklearn.mixture.bayesian_mixture import gamma_entropy_diag
+
+from sklearn.mixture import BayesianGaussianMixture
+
+
+def test_log_dirichlet_norm():
+    rng = np.random.RandomState(0)
+
+    alpha = rng.rand(2)
+    expected_norm = gammaln(np.sum(alpha)) - np.sum(gammaln(alpha))
+    predected_norm = log_dirichlet_norm(alpha)
+
+    assert_almost_equal(expected_norm, predected_norm)
+
+
+def test_log_wishart_norm():
+    rng = np.random.RandomState(0)
+
+    n_features = 2
+    nu = np.abs(rng.rand()) + 1.
+    inv_W = linalg.inv(make_spd_matrix(n_features, rng))
+    inv_W_chol = linalg.cholesky(inv_W, lower=True)
+
+    expected_norm = (-nu * np.sum(np.log(np.diag(inv_W_chol))) -
+                     .5 * n_features * nu * np.log(2.) -
+                     .25 * n_features * (n_features - 1) * np.log(np.pi) -
+                     np.sum(gammaln(.5 * (nu + 1. -
+                                          np.arange(1, n_features + 1.)))))
+    predected_norm = log_wishart_norm(nu, inv_W_chol, n_features)
+
+    assert_almost_equal(expected_norm, predected_norm)
+
+
+def test_estimate_wishart_entropy():
+    rng = np.random.RandomState(0)
+
+    n_features = 2
+    nu = np.abs(rng.rand()) + 1.
+    inv_W = linalg.inv(make_spd_matrix(n_features, rng))
+    inv_W_chol = linalg.cholesky(inv_W, lower=True)
+    log_lambda = rng.rand()
+
+    expected_entropy = (.5 * nu * n_features -
+                        .5 * (nu - n_features - 1.) * log_lambda -
+                        log_wishart_norm(nu, inv_W_chol, n_features))
+    predected_entropy = estimate_wishart_entropy(nu, inv_W_chol, log_lambda,
+                                                 n_features)
+
+    assert_almost_equal(expected_entropy, predected_entropy)
+
+
+def test_gamma_entropy_spherical():
+    rng = np.random.RandomState(0)
+
+    n_components = 5
+    a = rng.rand(n_components)
+    b = rng.rand(n_components)
+
+    expected_entropy = gammaln(a) - (a - 1.) * digamma(a) + np.log(b) + a
+    predected_entropy = gamma_entropy_spherical(a, b)
+
+    assert_almost_equal(expected_entropy, predected_entropy)
+
+
+def test_gamma_entropy_diag():
+    rng = np.random.RandomState(0)
+
+    n_components, n_features = 5, 2
+    a = rng.rand(n_components)
+    b = rng.rand(n_components, n_features)
+
+    expected_entropy = ((gammaln(a) - (a - 1.) * digamma(a) + a) * len(b) +
+                        np.sum(np.log(b)))
+    predected_entropy = gamma_entropy_diag(a, b)
+
+    assert_almost_equal(expected_entropy, predected_entropy)
+
+
+def test_bayesian_mixture_covariance_type():
+    rng = np.random.RandomState(0)
+    n_samples, n_features = 10, 2
+    X = rng.rand(n_samples, n_features)
+
+    covariance_type = 'bad_covariance_type'
+    bgmm = BayesianGaussianMixture(covariance_type=covariance_type)
+    assert_raise_message(ValueError,
+                         "Invalid value for 'covariance_type': %s "
+                         "'covariance_type' should be in "
+                         "['spherical', 'tied', 'diag', 'full']"
+                         % covariance_type,
+                         bgmm.fit, X)
+
+
+def test_bayesian_mixture_weights_prior_initialisation():
+    rng = np.random.RandomState(0)
+    n_samples, n_components, n_features = 10, 5, 2
+    X = rng.rand(n_samples, n_features)
+
+    # Check raise message for a bad value of alpha_init
+    bad_alpha_init = 0.
+    bgmm = BayesianGaussianMixture(alpha_init=bad_alpha_init)
+    assert_raise_message(ValueError,
+                         "The parameter 'alpha_init' should be "
+                         "greater than 0., but got %.3f."
+                         % bad_alpha_init,
+                         bgmm.fit, X)
+
+    # Check correct init for a given value of alpha_init
+    alpha_init = rng.rand()
+    bgmm = BayesianGaussianMixture(alpha_init=alpha_init).fit(X)
+    assert_almost_equal(alpha_init, bgmm._alpha_prior)
+
+    # Check correct init for the default value of alpha_init
+    bgmm = BayesianGaussianMixture(n_components=n_components).fit(X)
+    assert_almost_equal(1. / n_components, bgmm._alpha_prior)
+
+
+def test_bayesian_mixture_means_prior_initialisation():
+    rng = np.random.RandomState(0)
+    n_samples, n_components, n_features = 10, 3, 2
+    X = rng.rand(n_samples, n_features)
+
+    # Check raise message for a bad value of beta_init
+    bad_beta_init = 0.
+    bgmm = BayesianGaussianMixture(beta_init=bad_beta_init)
+    assert_raise_message(ValueError,
+                         "The parameter 'beta_init' should be "
+                         "greater than 0., but got %.3f."
+                         % bad_beta_init,
+                         bgmm.fit, X)
+
+    # Check correct init for a given value of beta_init
+    beta_init = rng.rand()
+    bgmm = BayesianGaussianMixture(beta_init=beta_init).fit(X)
+    assert_almost_equal(beta_init, bgmm._beta_prior)
+
+    # Check correct init for the default value of beta_init
+    bgmm = BayesianGaussianMixture().fit(X)
+    assert_almost_equal(1., bgmm._beta_prior)
+
+    # Check raise message for a bad shape of mean_init
+    mean_init = rng.rand(n_features + 1)
+    bgmm = BayesianGaussianMixture(n_components=n_components,
+                                   mean_init=mean_init)
+    assert_raise_message(ValueError,
+                         "The parameter 'means' should have the shape of ",
+                         bgmm.fit, X)
+
+    # Check correct init for a given value of mean_init
+    mean_init = rng.rand(n_features)
+    bgmm = BayesianGaussianMixture(n_components=n_components,
+                                   mean_init=mean_init).fit(X)
+    assert_almost_equal(mean_init, bgmm._mean_prior)
+
+    # Check correct init for the default value of bemean_initta
+    bgmm = BayesianGaussianMixture(n_components=n_components).fit(X)
+    assert_almost_equal(X.mean(axis=0), bgmm._mean_prior)
+
+
+def test_bayesian_mixture_precisions_prior_initialisation():
+    rng = np.random.RandomState(0)
+    n_samples, n_features = 10, 2
+    X = rng.rand(n_samples, n_features)
+
+    # Check raise message for a bad value of nu_init
+    bad_nu_init = n_features - 1.
+    bgmm = BayesianGaussianMixture(nu_init=bad_nu_init)
+    assert_raise_message(ValueError,
+                         "The parameter 'nu_init' should be "
+                         "greater than %d, but got %.3f."
+                         % (n_features - 1, bad_nu_init),
+                         bgmm.fit, X)
+
+    # Check correct init for a given value of nu_init
+    nu_init = rng.rand() + n_features - 1.
+    bgmm = BayesianGaussianMixture(nu_init=nu_init).fit(X)
+    assert_almost_equal(nu_init, bgmm._nu_prior)
+
+    # Check correct init for the default value of nu_init
+    nu_init_default = n_features
+    bgmm = BayesianGaussianMixture(nu_init=nu_init_default).fit(X)
+    assert_almost_equal(nu_init_default, bgmm._nu_prior)
+
+    # Check correct init for a given value of covariance_init
+    covariance_init = {
+        'full': np.cov(X.T, bias=1),
+        'tied': np.cov(X.T, bias=1),
+        'diag': np.diag(np.atleast_2d(np.cov(X.T, bias=1))),
+        'spherical': rng.rand()}
+
+    bgmm = BayesianGaussianMixture()
+    for cov_type in ['full', 'tied', 'diag', 'spherical']:
+        print(cov_type)
+        bgmm.covariance_type = cov_type
+        bgmm.covariance_init = covariance_init[cov_type]
+        bgmm.fit(X)
+        assert_almost_equal(covariance_init[cov_type],
+                            bgmm._covariance_prior)
+
+    # Check raise message for a bad spherical value of covariance_init
+    bad_covariance_init = -1.
+    bgmm = BayesianGaussianMixture(covariance_type='spherical',
+                                   covariance_init=bad_covariance_init)
+    assert_raise_message(ValueError,
+                         "The parameter 'spherical covariance_init' "
+                         "should be greater than 0., but got %.3f."
+                         % bad_covariance_init,
+                         bgmm.fit, X)
+
+    # Check correct init for the default value of covariance_init
+    covariance_init_default = {
+        'full': np.eye(X.shape[1]),
+        'tied': np.eye(X.shape[1]),
+        'diag': .5 * np.diag(np.atleast_2d(np.cov(X.T, bias=1))),
+        'spherical': .5 * np.diag(np.atleast_2d(np.cov(X.T, bias=1))).mean()}
+
+    bgmm = BayesianGaussianMixture()
+    for cov_type in ['full', 'tied', 'diag', 'spherical']:
+        bgmm.covariance_type = cov_type
+        bgmm.fit(X)
+        assert_almost_equal(covariance_init_default[cov_type],
+                            bgmm._covariance_prior)
+
+
+def test_bayesian_mixture_check_is_fitted():
+    rng = np.random.RandomState(0)
+    n_samples, n_features = 10, 2
+
+    # Check raise message
+    bgmm = BayesianGaussianMixture()
+    X = rng.rand(n_samples, n_features)
+    assert_raise_message(ValueError,
+                         'This BayesianGaussianMixture instance is not '
+                         'fitted yet.', bgmm.score, X)
+
+
+def test_bayesian_mixture_weights():
+    rng = np.random.RandomState(0)
+    n_samples, n_features = 10, 2
+
+    X = rng.rand(n_samples, n_features)
+    bgmm = BayesianGaussianMixture().fit(X)
+
+    # Check the weights values
+    expected_weights = bgmm.alpha_ / np.sum(bgmm.alpha_)
+    predected_weights = bgmm.weights_
+
+    assert_almost_equal(expected_weights, predected_weights)
+
+    # Check the weights sum = 1
+    assert_almost_equal(np.sum(bgmm.weights_), 1.0)
+
+
+# def test_bayesian_mixture_means():
+#     rng = np.random.RandomState(0)
+#     n_samples, n_features = 10, 2
+
+#     X = rng.rand(n_samples, n_features)
+#     bgmm = BayesianGaussianMixture().fit(X)
+
+#     # Check the means values
+#     assert_almost_equal(bgmm.means_, bgmm.m_)
+
+
+# def test_bayessian_mixture_covariances():
+#     rng = np.random.RandomState(0)
+#     n_samples, n_features = 10, 2
+
+#     X = rng.rand(n_samples, n_features)
+#     bgmm = BayesianGaussianMixture().fit(X)
+
+#     for covariance_type in ['full', 'tied', 'diag', 'spherical']:
+#         bgmm.covariance_type = covariance_type
+#         bgmm.fit(X)
+
+#         if covariance_type is 'full':
+#             pred_covar = bgmm.precisions_ / bgmm.nu_[:, np.newaxis, np.newaxis]
+#         elif covariance_type is 'diag':
+#             pred_covar = bgmm.precisions_ / bgmm.nu_[:, np.newaxis]
+#         else:
+#             pred_covar = bgmm.precisions_ / bgmm.nu_
+
+#         assert_array_almost_equal(pred_covar, bgmm.covariances_)
+
+
+def generate_data(n_samples, means, covars, random_state=0):
+    rng = np.random.RandomState(random_state)
+    n_components = len(n_samples)
+    X = np.vstack([rng.multivariate_normal(means[j], covars[j], n_samples[j])
+                  for j in range(n_components)])
+    y = np.concatenate([j * np.ones(n_samples[j])
+                       for j in range(n_components)])
+    return X, y
diff --git a/sklearn/mixture/tests/test_gaussian_mixture.py b/sklearn/mixture/tests/test_gaussian_mixture.py
index 8e3e5516d7d2..9e4070fd7531 100644
--- a/sklearn/mixture/tests/test_gaussian_mixture.py
+++ b/sklearn/mixture/tests/test_gaussian_mixture.py
@@ -11,10 +11,11 @@
 from sklearn.metrics.cluster import adjusted_rand_score
 from sklearn.mixture.gaussian_mixture import GaussianMixture
 from sklearn.mixture.gaussian_mixture import (
-    _estimate_gaussian_precisions_cholesky_full,
-    _estimate_gaussian_precisions_cholesky_tied,
-    _estimate_gaussian_precisions_cholesky_diag,
-    _estimate_gaussian_precisions_cholesky_spherical)
+    _estimate_gaussian_covariances_full,
+    _estimate_gaussian_covariances_tied,
+    _estimate_gaussian_covariances_diag,
+    _estimate_gaussian_covariances_spherical,
+    _compute_precision_cholesky)
 from sklearn.exceptions import ConvergenceWarning, NotFittedError
 from sklearn.utils.extmath import fast_logdet
 from sklearn.utils.testing import assert_allclose
@@ -327,25 +328,33 @@ def test_suffstat_sk_full():
     X_resp = np.sqrt(resp) * X
     nk = np.array([n_samples])
     xk = np.zeros((1, n_features))
-    precs_pred = _estimate_gaussian_precisions_cholesky_full(resp, X,
-                                                             nk, xk, 0)
-    covars_pred = linalg.inv(np.dot(precs_pred[0], precs_pred[0].T))
+    covars_pred = _estimate_gaussian_covariances_full(resp, X, nk, xk, 0)
     ecov = EmpiricalCovariance(assume_centered=True)
     ecov.fit(X_resp)
-    assert_almost_equal(ecov.error_norm(covars_pred, norm='frobenius'), 0)
-    assert_almost_equal(ecov.error_norm(covars_pred, norm='spectral'), 0)
+    assert_almost_equal(ecov.error_norm(covars_pred[0], norm='frobenius'), 0)
+    assert_almost_equal(ecov.error_norm(covars_pred[0], norm='spectral'), 0)
+
+    # check the precision computation
+    precs_chol_pred = _compute_precision_cholesky(covars_pred, 'full')
+    precs_pred = np.array([np.dot(prec, prec.T) for prec in precs_chol_pred])
+    precs_est = np.array([linalg.inv(cov) for cov in covars_pred])
+    assert_array_almost_equal(precs_est, precs_pred)
 
     # special case 2, assuming resp are all ones
     resp = np.ones((n_samples, 1))
     nk = np.array([n_samples])
     xk = X.mean(axis=0).reshape((1, -1))
-    precs_pred = _estimate_gaussian_precisions_cholesky_full(resp, X,
-                                                             nk, xk, 0)
-    covars_pred = linalg.inv(np.dot(precs_pred[0], precs_pred[0].T))
+    covars_pred = _estimate_gaussian_covariances_full(resp, X, nk, xk, 0)
     ecov = EmpiricalCovariance(assume_centered=False)
     ecov.fit(X)
-    assert_almost_equal(ecov.error_norm(covars_pred, norm='frobenius'), 0)
-    assert_almost_equal(ecov.error_norm(covars_pred, norm='spectral'), 0)
+    assert_almost_equal(ecov.error_norm(covars_pred[0], norm='frobenius'), 0)
+    assert_almost_equal(ecov.error_norm(covars_pred[0], norm='spectral'), 0)
+
+    # check the precision computation
+    precs_chol_pred = _compute_precision_cholesky(covars_pred, 'full')
+    precs_pred = np.array([np.dot(prec, prec.T) for prec in precs_chol_pred])
+    precs_est = np.array([linalg.inv(cov) for cov in covars_pred])
+    assert_array_almost_equal(precs_est, precs_pred)
 
 
 def test_suffstat_sk_tied():
@@ -359,22 +368,22 @@ def test_suffstat_sk_tied():
     nk = resp.sum(axis=0)
     xk = np.dot(resp.T, X) / nk[:, np.newaxis]
 
-    precs_pred_full = _estimate_gaussian_precisions_cholesky_full(resp, X,
-                                                                  nk, xk, 0)
-    covars_pred_full = [linalg.inv(np.dot(precision_chol, precision_chol.T))
-                        for precision_chol in precs_pred_full]
+    covars_pred_full = _estimate_gaussian_covariances_full(resp, X, nk, xk, 0)
     covars_pred_full = np.sum(nk[:, np.newaxis, np.newaxis] * covars_pred_full,
                               0) / n_samples
 
-    precs_pred_tied = _estimate_gaussian_precisions_cholesky_tied(resp, X,
-                                                                  nk, xk, 0)
-    covars_pred_tied = linalg.inv(np.dot(precs_pred_tied, precs_pred_tied.T))
+    covars_pred_tied = _estimate_gaussian_covariances_tied(resp, X, nk, xk, 0)
 
     ecov = EmpiricalCovariance()
     ecov.covariance_ = covars_pred_full
     assert_almost_equal(ecov.error_norm(covars_pred_tied, norm='frobenius'), 0)
     assert_almost_equal(ecov.error_norm(covars_pred_tied, norm='spectral'), 0)
 
+    # check the precision computation
+    precs_chol_pred = _compute_precision_cholesky(covars_pred_tied, 'tied')
+    precs_pred = np.dot(precs_chol_pred, precs_chol_pred.T)
+    precs_est = linalg.inv(covars_pred_tied)
+    assert_array_almost_equal(precs_est, precs_pred)
 
 def test_suffstat_sk_diag():
     # test against 'full' case
@@ -386,22 +395,20 @@ def test_suffstat_sk_diag():
     X = rng.rand(n_samples, n_features)
     nk = resp.sum(axis=0)
     xk = np.dot(resp.T, X) / nk[:, np.newaxis]
-    precs_pred_full = _estimate_gaussian_precisions_cholesky_full(resp, X,
-                                                                  nk, xk, 0)
-    covars_pred_full = [linalg.inv(np.dot(precision_chol, precision_chol.T))
-                        for precision_chol in precs_pred_full]
-
-    precs_pred_diag = _estimate_gaussian_precisions_cholesky_diag(resp, X,
-                                                                  nk, xk, 0)
-    covars_pred_diag = np.array([np.diag(1. / d) ** 2
-                                 for d in precs_pred_diag])
+    covars_pred_full = _estimate_gaussian_covariances_full(resp, X, nk, xk, 0)
+    covars_pred_diag = _estimate_gaussian_covariances_diag(resp, X, nk, xk, 0)
 
     ecov = EmpiricalCovariance()
     for (cov_full, cov_diag) in zip(covars_pred_full, covars_pred_diag):
         ecov.covariance_ = np.diag(np.diag(cov_full))
+        cov_diag = np.diag(cov_diag)
         assert_almost_equal(ecov.error_norm(cov_diag, norm='frobenius'), 0)
         assert_almost_equal(ecov.error_norm(cov_diag, norm='spectral'), 0)
 
+    # check the precision computation
+    precs_chol_pred = _compute_precision_cholesky(covars_pred_diag, 'diag')
+    assert_almost_equal(covars_pred_diag, 1. / precs_chol_pred ** 2)
+
 
 def test_gaussian_suffstat_sk_spherical():
     # computing spherical covariance equals to the variance of one-dimension
@@ -414,12 +421,16 @@ def test_gaussian_suffstat_sk_spherical():
     resp = np.ones((n_samples, 1))
     nk = np.array([n_samples])
     xk = X.mean()
-    precs_pred_spherical = _estimate_gaussian_precisions_cholesky_spherical(
-        resp, X, nk, xk, 0)
-    covars_pred_spherical = (np.dot(X.flatten().T, X.flatten()) /
-                             (n_features * n_samples))
-    assert_almost_equal(1. / precs_pred_spherical ** 2, covars_pred_spherical)
-
+    covars_pred_spherical = _estimate_gaussian_covariances_spherical(resp, X,
+                                                                     nk, xk, 0)
+    covars_pred_spherical2 = (np.dot(X.flatten().T, X.flatten()) /
+                              (n_features * n_samples))
+    assert_almost_equal(covars_pred_spherical, covars_pred_spherical2)
+
+    # check the precision computation
+    precs_chol_pred = _compute_precision_cholesky(covars_pred_spherical,
+                                                  'spherical')
+    assert_almost_equal(covars_pred_spherical, 1. / precs_chol_pred ** 2)
 
 def _naive_lmvnpdf_diag(X, means, covars):
     resp = np.empty((len(X), len(means)))
diff --git a/sklearn/model_selection/_validation.py b/sklearn/model_selection/_validation.py
index 051fc0948ca3..2a591bfc0979 100644
--- a/sklearn/model_selection/_validation.py
+++ b/sklearn/model_selection/_validation.py
@@ -235,7 +235,7 @@ def _fit_and_score(estimator, X, y, scorer, train, test, verbose,
     """
     if verbose > 1:
         if parameters is None:
-            msg = "no parameters to be set"
+            msg = ''
         else:
             msg = '%s' % (', '.join('%s=%s' % (k, v)
                           for k, v in parameters.items()))
@@ -301,6 +301,13 @@ def _score(estimator, X_test, y_test, scorer):
         score = scorer(estimator, X_test)
     else:
         score = scorer(estimator, X_test, y_test)
+    if hasattr(score, 'item'):
+        try:
+            # e.g. unwrap memmapped scalars
+            score = score.item()
+        except ValueError:
+            # non-scalar?
+            pass
     if not isinstance(score, numbers.Number):
         raise ValueError("scoring must return a number, got %s (%s) instead."
                          % (str(score), type(score)))
diff --git a/sklearn/model_selection/tests/test_validation.py b/sklearn/model_selection/tests/test_validation.py
index 66b307c31d83..2e694fd45e59 100644
--- a/sklearn/model_selection/tests/test_validation.py
+++ b/sklearn/model_selection/tests/test_validation.py
@@ -3,6 +3,9 @@
 
 import sys
 import warnings
+import tempfile
+import os
+from time import sleep
 
 import numpy as np
 from scipy.sparse import coo_matrix, csr_matrix
@@ -60,6 +63,12 @@
 from sklearn.model_selection.tests.test_split import MockClassifier
 
 
+try:
+    WindowsError
+except NameError:
+    WindowsError = None
+
+
 class MockImprovingEstimator(BaseEstimator):
     """Dummy classifier to test the learning curve"""
     def __init__(self, n_max_train_sizes):
@@ -769,3 +778,30 @@ def test_cross_val_predict_with_method():
         predictions = cross_val_predict(est, X, y, method=method,
                                         cv=kfold)
         assert_array_almost_equal(expected_predictions, predictions)
+
+
+def test_score_memmap():
+    # Ensure a scalar score of memmap type is accepted
+    iris = load_iris()
+    X, y = iris.data, iris.target
+    clf = MockClassifier()
+    tf = tempfile.NamedTemporaryFile(mode='wb', delete=False)
+    tf.write(b'Hello world!!!!!')
+    tf.close()
+    scores = np.memmap(tf.name, dtype=np.float64)
+    score = np.memmap(tf.name, shape=(), mode='r', dtype=np.float64)
+    try:
+        cross_val_score(clf, X, y, scoring=lambda est, X, y: score)
+        # non-scalar should still fail
+        assert_raises(ValueError, cross_val_score, clf, X, y,
+                      scoring=lambda est, X, y: scores)
+    finally:
+        # Best effort to release the mmap file handles before deleting the
+        # backing file under Windows
+        scores, score = None, None
+        for _ in range(3):
+            try:
+                os.unlink(tf.name)
+                break
+            except WindowsError:
+                sleep(1.)
diff --git a/sklearn/svm/classes.py b/sklearn/svm/classes.py
index 13f96b18041c..cb3f542f6916 100644
--- a/sklearn/svm/classes.py
+++ b/sklearn/svm/classes.py
@@ -588,12 +588,12 @@ class NuSVC(BaseSVC):
     cache_size : float, optional
         Specify the size of the kernel cache (in MB).
 
-    class_weight : {dict, 'auto'}, optional
+    class_weight : {dict, 'balanced'}, optional
         Set the parameter C of class i to class_weight[i]*C for
         SVC. If not given, all classes are supposed to have
-        weight one. The 'auto' mode uses the values of y to
-        automatically adjust weights inversely proportional to
-        class frequencies.
+        weight one. The "balanced" mode uses the values of y to automatically adjust
+        weights inversely proportional to class frequencies as
+        ``n_samples / (n_classes * np.bincount(y))``
 
     verbose : bool, default: False
         Enable verbose output. Note that this setting takes advantage of a
diff --git a/sklearn/svm/src/libsvm/svm.cpp b/sklearn/svm/src/libsvm/svm.cpp
index f6d6882e6364..29a4dfd8a71f 100644
--- a/sklearn/svm/src/libsvm/svm.cpp
+++ b/sklearn/svm/src/libsvm/svm.cpp
@@ -1009,7 +1009,7 @@ int Solver::select_working_set(int &out_i, int &out_j)
 		}
 	}
 
-	if(Gmax+Gmax2 < eps)
+	if(Gmax+Gmax2 < eps || Gmin_idx == -1)
 		return 1;
 
 	out_i = Gmax_idx;
@@ -1261,7 +1261,7 @@ int Solver_NU::select_working_set(int &out_i, int &out_j)
 		}
 	}
 
-	if(max(Gmaxp+Gmaxp2,Gmaxn+Gmaxn2) < eps)
+	if(max(Gmaxp+Gmaxp2,Gmaxn+Gmaxn2) < eps || Gmin_idx == -1)
 		return 1;
 
 	if (y[Gmin_idx] == +1)
diff --git a/sklearn/utils/class_weight.py b/sklearn/utils/class_weight.py
index b2ba15807c2f..5b778423f192 100644
--- a/sklearn/utils/class_weight.py
+++ b/sklearn/utils/class_weight.py
@@ -71,7 +71,7 @@ def compute_class_weight(class_weight, classes, y):
         # user-defined dictionary
         weight = np.ones(classes.shape[0], dtype=np.float64, order='C')
         if not isinstance(class_weight, dict):
-            raise ValueError("class_weight must be dict, 'auto', or None,"
+            raise ValueError("class_weight must be dict, 'balanced', or None,"
                              " got: %r" % class_weight)
         for c in class_weight:
             i = np.searchsorted(classes, c)
diff --git a/sklearn/utils/sparsefuncs_fast.pyx b/sklearn/utils/sparsefuncs_fast.pyx
index 554f2836d24a..dc444169df3c 100644
--- a/sklearn/utils/sparsefuncs_fast.pyx
+++ b/sklearn/utils/sparsefuncs_fast.pyx
@@ -23,24 +23,29 @@ ctypedef np.float64_t DOUBLE
 
 def csr_row_norms(X):
     """L2 norm of each row in CSR matrix X."""
+    if X.dtype != np.float32:
+        X = X.astype(np.float64)
+    return _csr_row_norms(X.data, X.shape, X.indices, X.indptr)
+
+
+def _csr_row_norms(np.ndarray[floating, ndim=1, mode="c"] X_data,
+                   shape,
+                   np.ndarray[int, ndim=1, mode="c"] X_indices,
+                   np.ndarray[int, ndim=1, mode="c"] X_indptr):
     cdef:
-        unsigned int n_samples = X.shape[0]
-        unsigned int n_features = X.shape[1]
+        unsigned int n_samples = shape[0]
+        unsigned int n_features = shape[1]
         np.ndarray[DOUBLE, ndim=1, mode="c"] norms
-        np.ndarray[DOUBLE, ndim=1, mode="c"] data
-        np.ndarray[int, ndim=1, mode="c"] indices = X.indices
-        np.ndarray[int, ndim=1, mode="c"] indptr = X.indptr
 
         np.npy_intp i, j
         double sum_
 
     norms = np.zeros(n_samples, dtype=np.float64)
-    data = np.asarray(X.data, dtype=np.float64)     # might copy!
 
     for i in range(n_samples):
         sum_ = 0.0
-        for j in range(indptr[i], indptr[i + 1]):
-            sum_ += data[j] * data[j]
+        for j in range(X_indptr[i], X_indptr[i + 1]):
+            sum_ += X_data[j] * X_data[j]
         norms[i] = sum_
 
     return norms
diff --git a/sklearn/utils/tests/test_extmath.py b/sklearn/utils/tests/test_extmath.py
index ea3ee6077005..5847d0566a9e 100644
--- a/sklearn/utils/tests/test_extmath.py
+++ b/sklearn/utils/tests/test_extmath.py
@@ -148,14 +148,23 @@ def test_norm_squared_norm():
 
 def test_row_norms():
     X = np.random.RandomState(42).randn(100, 100)
-    sq_norm = (X ** 2).sum(axis=1)
-
-    assert_array_almost_equal(sq_norm, row_norms(X, squared=True), 5)
-    assert_array_almost_equal(np.sqrt(sq_norm), row_norms(X))
-
-    Xcsr = sparse.csr_matrix(X, dtype=np.float32)
-    assert_array_almost_equal(sq_norm, row_norms(Xcsr, squared=True), 5)
-    assert_array_almost_equal(np.sqrt(sq_norm), row_norms(Xcsr))
+    for dtype in (np.float32, np.float64):
+        if dtype is np.float32:
+            precision = 4
+        else:
+            precision = 5
+
+        X = X.astype(dtype)
+        sq_norm = (X ** 2).sum(axis=1)
+
+        assert_array_almost_equal(sq_norm, row_norms(X, squared=True),
+                                  precision)
+        assert_array_almost_equal(np.sqrt(sq_norm), row_norms(X), precision)
+
+        Xcsr = sparse.csr_matrix(X, dtype=dtype)
+        assert_array_almost_equal(sq_norm, row_norms(Xcsr, squared=True),
+                                  precision)
+        assert_array_almost_equal(np.sqrt(sq_norm), row_norms(Xcsr), precision)
 
 
 def test_randomized_svd_low_rank_with_noise():
