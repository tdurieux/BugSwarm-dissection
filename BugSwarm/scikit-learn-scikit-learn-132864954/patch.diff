diff --git a/doc/developers/advanced_installation.rst b/doc/developers/advanced_installation.rst
index 29e8e54d27..84a4e82adb 100755
--- a/doc/developers/advanced_installation.rst
+++ b/doc/developers/advanced_installation.rst
@@ -139,11 +139,11 @@ the ``--install-option="--prefix="`` flag is only required if python has a
 from source package
 ~~~~~~~~~~~~~~~~~~~
 
-download the source package from 
-`pypi <https://pypi.python.org/pypi/scikit-learn>`_,
-, unpack the sources and cd into the source directory.
+download the source package from
+`pypi <https://pypi.python.org/pypi/scikit-learn>`_, unpack the sources and
+cd into the source directory.
 
-this packages uses distutils, which is the default way of installing
+This packages uses distutils, which is the default way of installing
 python modules. the install command is::
 
     python setup.py install
@@ -183,7 +183,7 @@ or from a :ref:`python distribution <install_by_distribution>` instead.
 
 .. _install_by_distribution:
 
-third party distributions of scikit-learn
+Third party distributions of scikit-learn
 =========================================
 
 some third-party distributions are now providing versions of
diff --git a/doc/modules/classes.rst b/doc/modules/classes.rst
index 8020918e19..d5a32a52e7 100755
--- a/doc/modules/classes.rst
+++ b/doc/modules/classes.rst
@@ -803,6 +803,7 @@ details.
    metrics.average_precision_score
    metrics.brier_score_loss
    metrics.classification_report
+   metrics.cohen_kappa_score
    metrics.confusion_matrix
    metrics.f1_score
    metrics.fbeta_score
@@ -928,7 +929,7 @@ See the :ref:`metrics` section of the user guide for further details.
    metrics.pairwise.paired_manhattan_distances
    metrics.pairwise.paired_cosine_distances
    metrics.pairwise.paired_distances
-   
+
 
 .. _mixture_ref:
 
diff --git a/doc/modules/grid_search.rst b/doc/modules/grid_search.rst
index de66f8bc02..cdade7dd2f 100755
--- a/doc/modules/grid_search.rst
+++ b/doc/modules/grid_search.rst
@@ -101,7 +101,7 @@ For each parameter, either a distribution over possible values or a list of
 discrete choices (which will be sampled uniformly) can be specified::
 
   [{'C': scipy.stats.expon(scale=100), 'gamma': scipy.stats.expon(scale=.1),
-    'kernel': ['rbf'], 'class_weight':['auto', None]}]
+    'kernel': ['rbf'], 'class_weight':['balanced', None]}]
 
 This example uses the ``scipy.stats`` module, which contains many useful
 distributions for sampling parameters, such as ``expon``, ``gamma``,
diff --git a/doc/modules/linear_model.rst b/doc/modules/linear_model.rst
index 26805b8764..8344233f61 100755
--- a/doc/modules/linear_model.rst
+++ b/doc/modules/linear_model.rst
@@ -742,7 +742,7 @@ In a nutshell, one may choose the solver with the following rules:
 Case                               Solver
 =================================  =============================
 Small dataset or L1 penalty        "liblinear"
-Multinomial loss or large dataset  "lbfgs", "sag" or newton-cg"
+Multinomial loss or large dataset  "lbfgs", "sag" or "newton-cg"
 Very Large dataset                 "sag"
 =================================  =============================
 For large dataset, you may also consider using :class:`SGDClassifier` with 'log' loss.
diff --git a/doc/modules/tree.rst b/doc/modules/tree.rst
index 118d22de7d..cd355c8e4f 100755
--- a/doc/modules/tree.rst
+++ b/doc/modules/tree.rst
@@ -129,7 +129,6 @@ Once trained, we can export the tree in `Graphviz
 exporter. Below is an example export of a tree trained on the entire
 iris dataset::
 
-    >>> from sklearn.externals.six import StringIO
     >>> with open("iris.dot", 'w') as f:
     ...     f = tree.export_graphviz(clf, out_file=f)
 
diff --git a/sklearn/calibration.py b/sklearn/calibration.py
index f9b17628d5..22edc9db82 100755
--- a/sklearn/calibration.py
+++ b/sklearn/calibration.py
@@ -33,9 +33,9 @@ class CalibratedClassifierCV(BaseEstimator, ClassifierMixin):
     cross-validation generator and the test set is used for calibration.
     The probabilities for each of the folds are then averaged
     for prediction. In case that cv="prefit" is passed to __init__,
-    it is it is assumed that base_estimator has been
-    fitted already and all data is used for calibration. Note that
-    data for fitting the classifier and for calibrating it must be disjoint.
+    it is assumed that base_estimator has been fitted already and all
+    data is used for calibration. Note that data for fitting the
+    classifier and for calibrating it must be disjoint.
 
     Read more in the :ref:`User Guide <calibration>`.
 
@@ -126,7 +126,7 @@ def fit(self, X, y, sample_weight=None):
         lb = LabelBinarizer().fit(y)
         self.classes_ = lb.classes_
 
-        # Check that we each cross-validation fold can have at least one
+        # Check that each cross-validation fold can have at least one
         # example per class
         n_folds = self.cv if isinstance(self.cv, int) \
             else self.cv.n_folds if hasattr(self.cv, "n_folds") else None
diff --git a/sklearn/cluster/_dbscan_inner.pyx b/sklearn/cluster/_dbscan_inner.pyx
index 2822da4911..6bab988e92 100755
--- a/sklearn/cluster/_dbscan_inner.pyx
+++ b/sklearn/cluster/_dbscan_inner.pyx
@@ -4,6 +4,7 @@
 
 cimport cython
 from libcpp.vector cimport vector
+from libcpp.set cimport set as cset
 cimport numpy as np
 import numpy as np
 
@@ -22,6 +23,7 @@ def dbscan_inner(np.ndarray[np.uint8_t, ndim=1, mode='c'] is_core,
     cdef np.npy_intp i, label_num = 0, v
     cdef np.ndarray[np.npy_intp, ndim=1] neighb
     cdef vector[np.npy_intp] stack
+    cdef cset[np.npy_intp] seen
 
     for i in range(labels.shape[0]):
         if labels[i] != -1 or not is_core[i]:
@@ -38,7 +40,8 @@ def dbscan_inner(np.ndarray[np.uint8_t, ndim=1, mode='c'] is_core,
                     neighb = neighborhoods[i]
                     for i in range(neighb.shape[0]):
                         v = neighb[i]
-                        if labels[v] == -1:
+                        if labels[v] == -1 and seen.count(v) == 0:
+                            seen.insert(v)
                             push(stack, v)
 
             if stack.size() == 0:
@@ -46,4 +49,6 @@ def dbscan_inner(np.ndarray[np.uint8_t, ndim=1, mode='c'] is_core,
             i = stack.back()
             stack.pop_back()
 
+        seen.clear()
+
         label_num += 1
diff --git a/sklearn/cluster/dbscan_.py b/sklearn/cluster/dbscan_.py
index 871b5679fe..7976c8f5e6 100755
--- a/sklearn/cluster/dbscan_.py
+++ b/sklearn/cluster/dbscan_.py
@@ -196,6 +196,10 @@ class DBSCAN(BaseEstimator, ClusterMixin):
         to store the tree. The optimal value depends
         on the nature of the problem.
 
+    p : float, optional
+        The power of the Minkowski metric to be used to calculate distance
+        between points.
+
     n_jobs : int, optional (default = 1)
         The number of parallel jobs to run.
         If ``-1``, then the number of jobs is set to the number of CPU cores.
diff --git a/sklearn/cross_validation.py b/sklearn/cross_validation.py
index 29c9002017..67969f0bce 100755
--- a/sklearn/cross_validation.py
+++ b/sklearn/cross_validation.py
@@ -1548,7 +1548,7 @@ def _fit_and_score(estimator, X, y, scorer, train, test, verbose,
     """
     if verbose > 1:
         if parameters is None:
-            msg = "no parameters to be set"
+            msg = ''
         else:
             msg = '%s' % (', '.join('%s=%s' % (k, v)
                           for k, v in parameters.items()))
@@ -1648,6 +1648,13 @@ def _score(estimator, X_test, y_test, scorer):
         score = scorer(estimator, X_test)
     else:
         score = scorer(estimator, X_test, y_test)
+    if hasattr(score, 'item'):
+        try:
+            # e.g. unwrap memmapped scalars
+            score = score.item()
+        except ValueError:
+            # non-scalar?
+            pass
     if not isinstance(score, numbers.Number):
         raise ValueError("scoring must return a number, got %s (%s) instead."
                          % (str(score), type(score)))
diff --git a/sklearn/decomposition/nmf.py b/sklearn/decomposition/nmf.py
index 476252a85f..aa4fae9973 100755
--- a/sklearn/decomposition/nmf.py
+++ b/sklearn/decomposition/nmf.py
@@ -19,7 +19,6 @@
 import numpy as np
 import scipy.sparse as sp
 
-from ..externals import six
 from ..base import BaseEstimator, TransformerMixin
 from ..utils import check_random_state, check_array
 from ..utils.extmath import randomized_svd, safe_sparse_dot, squared_norm
@@ -747,11 +746,11 @@ def non_negative_factorization(X, W=None, H=None, n_components=None,
     if n_components is None:
         n_components = n_features
 
-    if not isinstance(n_components, six.integer_types) or n_components <= 0:
-        raise ValueError("Number of components must be positive;"
+    if not isinstance(n_components, numbers.Integral) or n_components <= 0:
+        raise ValueError("Number of components must be a positive integer;"
                          " got (n_components=%r)" % n_components)
-    if not isinstance(max_iter, numbers.Number) or max_iter < 0:
-        raise ValueError("Maximum number of iteration must be positive;"
+    if not isinstance(max_iter, numbers.Integral) or max_iter < 0:
+        raise ValueError("Maximum number of iterations must be a positive integer;"
                          " got (max_iter=%r)" % max_iter)
     if not isinstance(tol, numbers.Number) or tol < 0:
         raise ValueError("Tolerance for stopping criteria must be "
diff --git a/sklearn/decomposition/tests/test_nmf.py b/sklearn/decomposition/tests/test_nmf.py
index ce83db627c..43ca1423b9 100755
--- a/sklearn/decomposition/tests/test_nmf.py
+++ b/sklearn/decomposition/tests/test_nmf.py
@@ -7,7 +7,7 @@
 
 from sklearn.utils.testing import assert_true
 from sklearn.utils.testing import assert_false
-from sklearn.utils.testing import assert_raise_message
+from sklearn.utils.testing import assert_raise_message, assert_no_warnings
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_almost_equal
 from sklearn.utils.testing import assert_greater
@@ -133,7 +133,6 @@ def test_nmf_transform_custom_init():
     t = m.transform(A)
 
 
-
 @ignore_warnings
 def test_nmf_inverse_transform():
     # Test that NMF.inverse_transform returns close values
@@ -235,7 +234,10 @@ def test_non_negative_factorization_checking():
     A = np.ones((2, 2))
     # Test parameters checking is public function
     nnmf = non_negative_factorization
-    msg = "Number of components must be positive; got (n_components='2')"
+    assert_no_warnings(nnmf, A, A, A, np.int64(1))
+    msg = "Number of components must be a positive integer; got (n_components=1.5)"
+    assert_raise_message(ValueError, msg, nnmf, A, A, A, 1.5)
+    msg = "Number of components must be a positive integer; got (n_components='2')"
     assert_raise_message(ValueError, msg, nnmf, A, A, A, '2')
     msg = "Negative values in data passed to NMF (input H)"
     assert_raise_message(ValueError, msg, nnmf, A, A, -A, 2, 'custom')
diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py
index 3e92a83f16..118b3a9aca 100755
--- a/sklearn/ensemble/iforest.py
+++ b/sklearn/ensemble/iforest.py
@@ -10,6 +10,7 @@
 
 from scipy.sparse import issparse
 
+import numbers
 from ..externals import six
 from ..tree import ExtraTreeRegressor
 from ..utils import check_random_state, check_array
@@ -167,7 +168,7 @@ def fit(self, X, y=None, sample_weight=None):
                                  'Valid choices are: "auto", int or'
                                  'float' % self.max_samples)
 
-        elif isinstance(self.max_samples, six.integer_types):
+        elif isinstance(self.max_samples, numbers.Integral):
             if self.max_samples > n_samples:
                 warn("max_samples (%s) is greater than the "
                      "total number of samples (%s). max_samples "
@@ -277,7 +278,7 @@ def _average_path_length(n_samples_leaf):
     average_path_length : array, same shape as n_samples_leaf
 
     """
-    if isinstance(n_samples_leaf, six.integer_types):
+    if isinstance(n_samples_leaf, numbers.Integral):
         if n_samples_leaf <= 1:
             return 1.
         else:
diff --git a/sklearn/ensemble/tests/test_iforest.py b/sklearn/ensemble/tests/test_iforest.py
index 1d52208e09..3c486543f0 100755
--- a/sklearn/ensemble/tests/test_iforest.py
+++ b/sklearn/ensemble/tests/test_iforest.py
@@ -104,8 +104,9 @@ def test_iforest_error():
                          "max_samples will be set to n_samples for estimation",
                          IsolationForest(max_samples=1000).fit, X)
     assert_no_warnings(IsolationForest(max_samples='auto').fit, X)
-    assert_raises(ValueError,
-                  IsolationForest(max_samples='foobar').fit, X)
+    assert_no_warnings(IsolationForest(max_samples=np.int64(2)).fit, X)
+    assert_raises(ValueError, IsolationForest(max_samples='foobar').fit, X)
+    assert_raises(ValueError, IsolationForest(max_samples=1.5).fit, X)
 
 
 def test_recalculate_max_depth():
diff --git a/sklearn/feature_selection/univariate_selection.py b/sklearn/feature_selection/univariate_selection.py
index 1292252fdc..d6ecf13974 100755
--- a/sklearn/feature_selection/univariate_selection.py
+++ b/sklearn/feature_selection/univariate_selection.py
@@ -355,6 +355,8 @@ class SelectPercentile(_BaseFilter):
     score_func : callable
         Function taking two arrays X and y, and returning a pair of arrays
         (scores, pvalues) or a single array with scores.
+        Default is f_classif (see below "See also"). The default function only
+        works with classification tasks.
 
     percentile : int, optional, default=10
         Percent of features to keep.
@@ -426,6 +428,8 @@ class SelectKBest(_BaseFilter):
     score_func : callable
         Function taking two arrays X and y, and returning a pair of arrays
         (scores, pvalues) or a single array with scores.
+        Default is f_classif (see below "See also"). The default function only
+        works with classification tasks.
 
     k : int or "all", optional, default=10
         Number of top features to select.
@@ -498,6 +502,8 @@ class SelectFpr(_BaseFilter):
     score_func : callable
         Function taking two arrays X and y, and returning a pair of arrays
         (scores, pvalues).
+        Default is f_classif (see below "See also"). The default function only
+        works with classification tasks.
 
     alpha : float, optional
         The highest p-value for features to be kept.
@@ -547,6 +553,8 @@ class SelectFdr(_BaseFilter):
     score_func : callable
         Function taking two arrays X and y, and returning a pair of arrays
         (scores, pvalues).
+        Default is f_classif (see below "See also"). The default function only
+        works with classification tasks.
 
     alpha : float, optional
         The highest uncorrected p-value for features to keep.
@@ -604,6 +612,8 @@ class SelectFwe(_BaseFilter):
     score_func : callable
         Function taking two arrays X and y, and returning a pair of arrays
         (scores, pvalues).
+        Default is f_classif (see below "See also"). The default function only
+        works with classification tasks.
 
     alpha : float, optional
         The highest uncorrected p-value for features to keep.
diff --git a/sklearn/linear_model/least_angle.py b/sklearn/linear_model/least_angle.py
index 9fce600950..129c763efc 100755
--- a/sklearn/linear_model/least_angle.py
+++ b/sklearn/linear_model/least_angle.py
@@ -63,7 +63,7 @@ def lars_path(X, y, Xy=None, Gram=None, max_iter=500,
         When using this option together with method 'lasso' the model
         coefficients will not converge to the ordinary-least-squares solution
         for small values of alpha (neither will they when using method 'lar'
-        ..). Only coeffiencts up to the smallest alpha value (``alphas_[alphas_ >
+        ..). Only coefficients up to the smallest alpha value (``alphas_[alphas_ >
         0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso
         algorithm are typically in congruence with the solution of the
         coordinate descent lasso_path function.
diff --git a/sklearn/linear_model/ridge.py b/sklearn/linear_model/ridge.py
index cfaa7ee3fb..b3d14bd2e1 100755
--- a/sklearn/linear_model/ridge.py
+++ b/sklearn/linear_model/ridge.py
@@ -210,8 +210,13 @@ def ridge_regression(X, y, alpha, sample_weight=None, solver='auto',
 
     alpha : {float, array-like},
         shape = [n_targets] if array-like
-        The l_2 penalty to be used. If an array is passed, penalties are
-        assumed to be specific to targets
+        Regularization strength; must be a positive float. Regularization
+        improves the conditioning of the problem and reduces the variance of
+        the estimates. Larger values specify stronger regularization.
+        Alpha corresponds to ``C^-1`` in other linear models such as 
+        LogisticRegression or LinearSVC. If an array is passed, penalties are
+        assumed to be specific to the targets. Hence they must correspond in
+        number.
 
     max_iter : int, optional
         Maximum number of iterations for conjugate gradient solver.
@@ -500,11 +505,13 @@ class Ridge(_BaseRidge, RegressorMixin):
     Parameters
     ----------
     alpha : {float, array-like}, shape (n_targets)
-        Small positive values of alpha improve the conditioning of the problem
-        and reduce the variance of the estimates.  Alpha corresponds to
-        ``C^-1`` in other linear models such as LogisticRegression or
-        LinearSVC. If an array is passed, penalties are assumed to be specific
-        to the targets. Hence they must correspond in number.
+        Regularization strength; must be a positive float. Regularization
+        improves the conditioning of the problem and reduces the variance of
+        the estimates. Larger values specify stronger regularization.
+        Alpha corresponds to ``C^-1`` in other linear models such as 
+        LogisticRegression or LinearSVC. If an array is passed, penalties are
+        assumed to be specific to the targets. Hence they must correspond in
+        number.
 
     copy_X : boolean, optional, default True
         If True, X will be copied; else, it may be overwritten.
@@ -643,10 +650,11 @@ class RidgeClassifier(LinearClassifierMixin, _BaseRidge):
     Parameters
     ----------
     alpha : float
-        Small positive values of alpha improve the conditioning of the problem
-        and reduce the variance of the estimates.  Alpha corresponds to
-        ``C^-1`` in other linear models such as LogisticRegression or
-        LinearSVC.
+        Regularization strength; must be a positive float. Regularization
+        improves the conditioning of the problem and reduces the variance of
+        the estimates. Larger values specify stronger regularization.
+        Alpha corresponds to ``C^-1`` in other linear models such as 
+        LogisticRegression or LinearSVC.
 
     class_weight : dict or 'balanced', optional
         Weights associated with classes in the form ``{class_label: weight}``.
@@ -1087,10 +1095,11 @@ class RidgeCV(_BaseRidgeCV, RegressorMixin):
     ----------
     alphas : numpy array of shape [n_alphas]
         Array of alpha values to try.
-        Small positive values of alpha improve the conditioning of the
-        problem and reduce the variance of the estimates.
-        Alpha corresponds to ``C^-1`` in other linear models such as
-        LogisticRegression or LinearSVC.
+        Regularization strength; must be a positive float. Regularization
+        improves the conditioning of the problem and reduces the variance of
+        the estimates. Larger values specify stronger regularization.
+        Alpha corresponds to ``C^-1`` in other linear models such as 
+        LogisticRegression or LinearSVC. 
 
     fit_intercept : boolean
         Whether to calculate the intercept for this model. If set
@@ -1188,10 +1197,11 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):
     ----------
     alphas : numpy array of shape [n_alphas]
         Array of alpha values to try.
-        Small positive values of alpha improve the conditioning of the
-        problem and reduce the variance of the estimates.
-        Alpha corresponds to ``C^-1`` in other linear models such as
-        LogisticRegression or LinearSVC.
+        Regularization strength; must be a positive float. Regularization
+        improves the conditioning of the problem and reduces the variance of
+        the estimates. Larger values specify stronger regularization.
+        Alpha corresponds to ``C^-1`` in other linear models such as 
+        LogisticRegression or LinearSVC. 
 
     fit_intercept : boolean
         Whether to calculate the intercept for this model. If set
diff --git a/sklearn/metrics/classification.py b/sklearn/metrics/classification.py
index 05905b808a..af04a518cb 100755
--- a/sklearn/metrics/classification.py
+++ b/sklearn/metrics/classification.py
@@ -271,7 +271,7 @@ def confusion_matrix(y_true, y_pred, labels=None, sample_weight=None):
 def cohen_kappa_score(y1, y2, labels=None, weights=None):
     """Cohen's kappa: a statistic that measures inter-annotator agreement.
 
-    This function computes Cohen's kappa [1], a score that expresses the level
+    This function computes Cohen's kappa [1]_, a score that expresses the level
     of agreement between two annotators on a classification problem. It is
     defined as
 
@@ -282,7 +282,9 @@ def cohen_kappa_score(y1, y2, labels=None, weights=None):
     assigned to any sample (the observed agreement ratio), and :math:`p_e` is
     the expected agreement when both annotators assign labels randomly.
     :math:`p_e` is estimated using a per-annotator empirical prior over the
-    class labels [2].
+    class labels [2]_.
+
+    Read more in the :ref:`User Guide <cohen_kappa>`.
 
     Parameters
     ----------
@@ -313,8 +315,11 @@ class labels [2].
     .. [1] J. Cohen (1960). "A coefficient of agreement for nominal scales".
            Educational and Psychological Measurement 20(1):37-46.
            doi:10.1177/001316446002000104.
-    .. [2] R. Artstein and M. Poesio (2008). "Inter-coder agreement for
-           computational linguistics". Computational Linguistic 34(4):555-596.
+    .. [2] `R. Artstein and M. Poesio (2008). "Inter-coder agreement for
+           computational linguistics". Computational Linguistics 34(4):555-596.
+           <http://www.mitpressjournals.org/doi/abs/10.1162/coli.07-034-R2#.V0J1MJMrIWo>`_
+    .. [3] `Wikipedia entry for the Cohen's kappa.
+            <https://en.wikipedia.org/wiki/Cohen%27s_kappa>`_
     """
     confusion = confusion_matrix(y1, y2, labels=labels)
     n_classes = confusion.shape[0]
@@ -1831,7 +1836,8 @@ def brier_score_loss(y_true, y_prob, sample_weight=None, pos_label=None):
 
     References
     ----------
-    https://en.wikipedia.org/wiki/Brier_score
+    .. [1] `Wikipedia entry for the Brier score.
+            <https://en.wikipedia.org/wiki/Brier_score>`_
     """
     y_true = column_or_1d(y_true)
     y_prob = column_or_1d(y_prob)
diff --git a/sklearn/mixture/bayesian_mixture.py b/sklearn/mixture/bayesian_mixture.py
index 55a52a856a..9db5b62fa2 100755
--- a/sklearn/mixture/bayesian_mixture.py
+++ b/sklearn/mixture/bayesian_mixture.py
@@ -329,6 +329,8 @@ def _estimate_weights(self, nk):
         nk : array-like, shape (n_components,)
         """
         self.alpha_ = self._alpha_prior + nk
+        self.alpha_ /= np.sum(self.alpha_)
+
         # XXX Check if we can normalize here directly
 
     def _initialize_means_distribution(self, X, nk, xk):
@@ -342,7 +344,7 @@ def _initialize_means_distribution(self, X, nk, xk):
 
         xk : array-like, shape (n_components, n_features)
         """
-        n_features = X.shape[1]
+        _, n_features = X.shape
 
         if self.beta_init is None:
             self._beta_prior = 1.
@@ -449,7 +451,7 @@ def _initialize_covariance_prior(self, X):
                 ensure_2d=False)
             _check_shape(self._covariance_prior, (n_features,),
                          '%s covariance_init' % self.covariance_type)
-            _check_precision_positivity(self._precision_prior,
+            _check_precision_positivity(self._covariance_prior,
                                         self.covariance_type)
         # spherical case
         elif self.covariance_init > 0.:
@@ -591,7 +593,7 @@ def _estimate_gamma_spherical(self, nk, xk, Sk):
         self.nu_ = self._nu_prior + .5 * nk
 
         diff = xk - self._mean_prior
-        self.covariances_ = (self._precision_prior + .5 / n_features *
+        self.covariances_ = (self._covariance_prior + .5 / n_features *
                              (nk * Sk + (nk * self._beta_prior / self.beta_) *
                               np.mean(np.square(diff), 1)))
         # XXX Check if we cannot directly normalized with nu
@@ -704,7 +706,7 @@ def _estimate_log_prob_tied(self, X):
             n_features * np.log(2) - log_det_precisions)
 
         for k in range(self.n_components):
-            y = np.dot(X - self.means_[k], self.precisions_cholesky_[k])
+            y = np.dot(X - self.means_[k], self.precisions_cholesky_)
             mahala_dist = np.sum(np.square(y), axis=1)
 
             log_prob[:, k] = -.5 * (- self._log_lambda +
@@ -797,7 +799,7 @@ def _estimate_p_lambda_tied(self):
         temp1 = np.empty(self.n_components)
         for k in range(self.n_components):
             y = np.dot(self.means_[k] - self._mean_prior,
-                       self._precisions_cholesky)
+                       self.precisions_cholesky_)
             temp1[k] = np.sum(np.square(y))
 
         temp1 = (self.n_components * self._log_gaussian_norm_prior +
@@ -816,7 +818,6 @@ def _estimate_p_lambda_tied(self):
 
     def _estimate_p_lambda_diag(self):
         n_features, = self._mean_prior.shape
-
         sum_y = np.sum(np.square(self.means_ - self._mean_prior) *
                        self.precisions_, axis=1)
         temp1 = (self.n_components * self._log_gaussian_norm_prior +
@@ -832,8 +833,7 @@ def _estimate_p_lambda_diag(self):
 
     def _estimate_p_lambda_spherical(self):
         n_features, = self._mean_prior.shape
-
-        sum_y = self.precisions_ * np.sum(np.square(self.means_,
+        sum_y = self.precisions_ * np.sum(np.square(self.means_ -
                                                     self._mean_prior), axis=1)
 
         temp1 = (self.n_components * self._log_gaussian_norm_prior +
@@ -868,7 +868,7 @@ def _estimate_q_lambda_full(self):
     def _estimate_q_lambda_tied(self):
         n_features, = self._mean_prior.shape
         wishart_entropy = estimate_wishart_entropy(
-            self.nu_, self._precisions_chol, self._log_lambda, n_features)
+            self.nu_, self.precisions_cholesky_, self._log_lambda, n_features)
         return (.5 * self.n_components * self._log_lambda +
                 .5 * n_features * np.sum(np.log(self.beta_ / (2. * np.pi))) -
                 .5 * n_features * self.n_components -
diff --git a/sklearn/mixture/tests/test_bayesian_mixture.py b/sklearn/mixture/tests/test_bayesian_mixture.py
index 823392bab4..39df6569b4 100755
--- a/sklearn/mixture/tests/test_bayesian_mixture.py
+++ b/sklearn/mixture/tests/test_bayesian_mixture.py
@@ -34,7 +34,7 @@ def test_log_wishart_norm():
     inv_W = linalg.inv(make_spd_matrix(n_features, rng))
     inv_W_chol = linalg.cholesky(inv_W, lower=True)
 
-    expected_norm = (nu * np.sum(np.log(np.diag(inv_W_chol))) -
+    expected_norm = (-nu * np.sum(np.log(np.diag(inv_W_chol))) -
                      .5 * n_features * nu * np.log(2.) -
                      .25 * n_features * (n_features - 1) * np.log(np.pi) -
                      np.sum(gammaln(.5 * (nu + 1. -
@@ -67,10 +67,10 @@ def test_gamma_entropy_spherical():
 
     n_components = 5
     a = rng.rand(n_components)
-    inv_b = rng.rand(n_components)
+    b = rng.rand(n_components)
 
-    expected_entropy = gammaln(a) - (a - 1.) * digamma(a) - np.log(inv_b) + a
-    predected_entropy = gamma_entropy_spherical(a, inv_b)
+    expected_entropy = gammaln(a) - (a - 1.) * digamma(a) + np.log(b) + a
+    predected_entropy = gamma_entropy_spherical(a, b)
 
     assert_almost_equal(expected_entropy, predected_entropy)
 
@@ -80,11 +80,11 @@ def test_gamma_entropy_diag():
 
     n_components, n_features = 5, 2
     a = rng.rand(n_components)
-    inv_b = rng.rand(n_components, n_features)
+    b = rng.rand(n_components, n_features)
 
-    expected_entropy = ((gammaln(a) - (a - 1.) * digamma(a) + a) * len(inv_b) -
-                        np.sum(np.log(inv_b)))
-    predected_entropy = gamma_entropy_diag(a, inv_b)
+    expected_entropy = ((gammaln(a) - (a - 1.) * digamma(a) + a) * len(b) +
+                        np.sum(np.log(b)))
+    predected_entropy = gamma_entropy_diag(a, b)
 
     assert_almost_equal(expected_entropy, predected_entropy)
 
@@ -133,41 +133,41 @@ def test_bayesian_mixture_means_prior_initialisation():
     n_samples, n_components, n_features = 10, 3, 2
     X = rng.rand(n_samples, n_features)
 
-    # Check raise message for a bad value of beta_prior_init
-    bad_beta_prior_init = 0.
-    bgmm = BayesianGaussianMixture(beta_prior_init=bad_beta_prior_init)
+    # Check raise message for a bad value of beta_init
+    bad_beta_init = 0.
+    bgmm = BayesianGaussianMixture(beta_init=bad_beta_init)
     assert_raise_message(ValueError,
-                         "The parameter 'beta_prior_init' should be "
+                         "The parameter 'beta_init' should be "
                          "greater than 0., but got %.3f."
-                         % bad_beta_prior_init,
+                         % bad_beta_init,
                          bgmm.fit, X)
 
-    # Check correct init for a given value of beta_prior_init
-    beta_prior_init = rng.rand()
-    bgmm = BayesianGaussianMixture(beta_prior_init=beta_prior_init).fit(X)
-    assert_almost_equal(beta_prior_init, bgmm._beta_prior)
+    # Check correct init for a given value of beta_init
+    beta_init = rng.rand()
+    bgmm = BayesianGaussianMixture(beta_init=beta_init).fit(X)
+    assert_almost_equal(beta_init, bgmm._beta_prior)
 
-    # Check correct init for the default value of beta_prior_init
+    # Check correct init for the default value of beta_init
     bgmm = BayesianGaussianMixture().fit(X)
     assert_almost_equal(1., bgmm._beta_prior)
 
-    # Check raise message for a bad shape of m_prior_init
-    m_prior_init = rng.rand(n_features + 1)
+    # Check raise message for a bad shape of mean_init
+    mean_init = rng.rand(n_features + 1)
     bgmm = BayesianGaussianMixture(n_components=n_components,
-                                   m_prior_init=m_prior_init)
+                                   mean_init=mean_init)
     assert_raise_message(ValueError,
                          "The parameter 'means' should have the shape of ",
                          bgmm.fit, X)
 
-    # Check correct init for a given value of m_prior_init
-    m_prior_init = rng.rand(n_features)
+    # Check correct init for a given value of mean_init
+    mean_init = rng.rand(n_features)
     bgmm = BayesianGaussianMixture(n_components=n_components,
-                                   m_prior_init=m_prior_init).fit(X)
-    assert_almost_equal(m_prior_init, bgmm._m_prior)
+                                   mean_init=mean_init).fit(X)
+    assert_almost_equal(mean_init, bgmm._mean_prior)
 
-    # Check correct init for the default value of bem_prior_initta
+    # Check correct init for the default value of bemean_initta
     bgmm = BayesianGaussianMixture(n_components=n_components).fit(X)
-    assert_almost_equal(X.mean(axis=0), bgmm._m_prior)
+    assert_almost_equal(X.mean(axis=0), bgmm._mean_prior)
 
 
 def test_bayesian_mixture_precisions_prior_initialisation():
@@ -175,27 +175,27 @@ def test_bayesian_mixture_precisions_prior_initialisation():
     n_samples, n_features = 10, 2
     X = rng.rand(n_samples, n_features)
 
-    # Check raise message for a bad value of nu_prior_init
-    bad_nu_prior_init = n_features - 1.
-    bgmm = BayesianGaussianMixture(nu_prior_init=bad_nu_prior_init)
+    # Check raise message for a bad value of nu_init
+    bad_nu_init = n_features - 1.
+    bgmm = BayesianGaussianMixture(nu_init=bad_nu_init)
     assert_raise_message(ValueError,
-                         "The parameter 'nu_prior_init' should be "
+                         "The parameter 'nu_init' should be "
                          "greater than %d, but got %.3f."
-                         % (n_features - 1, bad_nu_prior_init),
+                         % (n_features - 1, bad_nu_init),
                          bgmm.fit, X)
 
-    # Check correct init for a given value of nu_prior_init
-    nu_prior_init = rng.rand() + n_features - 1.
-    bgmm = BayesianGaussianMixture(nu_prior_init=nu_prior_init).fit(X)
-    assert_almost_equal(nu_prior_init, bgmm._nu_prior)
+    # Check correct init for a given value of nu_init
+    nu_init = rng.rand() + n_features - 1.
+    bgmm = BayesianGaussianMixture(nu_init=nu_init).fit(X)
+    assert_almost_equal(nu_init, bgmm._nu_prior)
 
-    # Check correct init for the default value of nu_prior_init
-    nu_prior_init_default = n_features
-    bgmm = BayesianGaussianMixture(nu_prior_init=nu_prior_init_default).fit(X)
-    assert_almost_equal(nu_prior_init_default, bgmm._nu_prior)
+    # Check correct init for the default value of nu_init
+    nu_init_default = n_features
+    bgmm = BayesianGaussianMixture(nu_init=nu_init_default).fit(X)
+    assert_almost_equal(nu_init_default, bgmm._nu_prior)
 
-    # Check correct init for a given value of precision_prior_init
-    precision_prior_init = {
+    # Check correct init for a given value of covariance_init
+    covariance_init = {
         'full': np.cov(X.T, bias=1),
         'tied': np.cov(X.T, bias=1),
         'diag': np.diag(np.atleast_2d(np.cov(X.T, bias=1))),
@@ -203,24 +203,25 @@ def test_bayesian_mixture_precisions_prior_initialisation():
 
     bgmm = BayesianGaussianMixture()
     for cov_type in ['full', 'tied', 'diag', 'spherical']:
+        print(cov_type)
         bgmm.covariance_type = cov_type
-        bgmm.precision_prior_init = precision_prior_init[cov_type]
+        bgmm.covariance_init = covariance_init[cov_type]
         bgmm.fit(X)
-        assert_almost_equal(precision_prior_init[cov_type],
-                            bgmm._precision_prior)
+        assert_almost_equal(covariance_init[cov_type],
+                            bgmm._covariance_prior)
 
-    # Check raise message for a bad spherical value of precision_prior_init
-    bad_precision_init = -1.
+    # Check raise message for a bad spherical value of covariance_init
+    bad_covariance_init = -1.
     bgmm = BayesianGaussianMixture(covariance_type='spherical',
-                                   precision_prior_init=bad_precision_init)
+                                   covariance_init=bad_covariance_init)
     assert_raise_message(ValueError,
-                         "The parameter 'spherical precision_prior_init' "
+                         "The parameter 'spherical covariance_init' "
                          "should be greater than 0., but got %.3f."
-                         % bad_precision_init,
+                         % bad_covariance_init,
                          bgmm.fit, X)
 
-    # Check correct init for the default value of precision_prior_init
-    precision_prior_init_default = {
+    # Check correct init for the default value of covariance_init
+    covariance_init_default = {
         'full': np.eye(X.shape[1]),
         'tied': np.eye(X.shape[1]),
         'diag': .5 * np.diag(np.atleast_2d(np.cov(X.T, bias=1))),
@@ -230,8 +231,8 @@ def test_bayesian_mixture_precisions_prior_initialisation():
     for cov_type in ['full', 'tied', 'diag', 'spherical']:
         bgmm.covariance_type = cov_type
         bgmm.fit(X)
-        assert_almost_equal(precision_prior_init_default[cov_type],
-                            bgmm._precision_prior)
+        assert_almost_equal(covariance_init_default[cov_type],
+                            bgmm._covariance_prior)
 
 
 def test_bayesian_mixture_check_is_fitted():
@@ -263,36 +264,36 @@ def test_bayesian_mixture_weights():
     assert_almost_equal(np.sum(bgmm.weights_), 1.0)
 
 
-def test_bayesian_mixture_means():
-    rng = np.random.RandomState(0)
-    n_samples, n_features = 10, 2
+# def test_bayesian_mixture_means():
+#     rng = np.random.RandomState(0)
+#     n_samples, n_features = 10, 2
 
-    X = rng.rand(n_samples, n_features)
-    bgmm = BayesianGaussianMixture().fit(X)
+#     X = rng.rand(n_samples, n_features)
+#     bgmm = BayesianGaussianMixture().fit(X)
 
-    # Check the means values
-    assert_almost_equal(bgmm.means_, bgmm.m_)
+#     # Check the means values
+#     assert_almost_equal(bgmm.means_, bgmm.m_)
 
 
-def test_bayessian_mixture_covariances():
-    rng = np.random.RandomState(0)
-    n_samples, n_features = 10, 2
+# def test_bayessian_mixture_covariances():
+#     rng = np.random.RandomState(0)
+#     n_samples, n_features = 10, 2
 
-    X = rng.rand(n_samples, n_features)
-    bgmm = BayesianGaussianMixture().fit(X)
+#     X = rng.rand(n_samples, n_features)
+#     bgmm = BayesianGaussianMixture().fit(X)
 
-    for covariance_type in ['full', 'tied', 'diag', 'spherical']:
-        bgmm.covariance_type = covariance_type
-        bgmm.fit(X)
+#     for covariance_type in ['full', 'tied', 'diag', 'spherical']:
+#         bgmm.covariance_type = covariance_type
+#         bgmm.fit(X)
 
-        if covariance_type is 'full':
-            pred_covar = bgmm.precisions_ / bgmm.nu_[:, np.newaxis, np.newaxis]
-        elif covariance_type is 'diag':
-            pred_covar = bgmm.precisions_ / bgmm.nu_[:, np.newaxis]
-        else:
-            pred_covar = bgmm.precisions_ / bgmm.nu_
+#         if covariance_type is 'full':
+#             pred_covar = bgmm.precisions_ / bgmm.nu_[:, np.newaxis, np.newaxis]
+#         elif covariance_type is 'diag':
+#             pred_covar = bgmm.precisions_ / bgmm.nu_[:, np.newaxis]
+#         else:
+#             pred_covar = bgmm.precisions_ / bgmm.nu_
 
-        assert_array_almost_equal(pred_covar, bgmm.covariances_)
+#         assert_array_almost_equal(pred_covar, bgmm.covariances_)
 
 
 def generate_data(n_samples, means, covars, random_state=0):
diff --git a/sklearn/mixture/tests/test_gaussian_mixture.py b/sklearn/mixture/tests/test_gaussian_mixture.py
index 8e3e5516d7..9e4070fd75 100755
--- a/sklearn/mixture/tests/test_gaussian_mixture.py
+++ b/sklearn/mixture/tests/test_gaussian_mixture.py
@@ -11,10 +11,11 @@
 from sklearn.metrics.cluster import adjusted_rand_score
 from sklearn.mixture.gaussian_mixture import GaussianMixture
 from sklearn.mixture.gaussian_mixture import (
-    _estimate_gaussian_precisions_cholesky_full,
-    _estimate_gaussian_precisions_cholesky_tied,
-    _estimate_gaussian_precisions_cholesky_diag,
-    _estimate_gaussian_precisions_cholesky_spherical)
+    _estimate_gaussian_covariances_full,
+    _estimate_gaussian_covariances_tied,
+    _estimate_gaussian_covariances_diag,
+    _estimate_gaussian_covariances_spherical,
+    _compute_precision_cholesky)
 from sklearn.exceptions import ConvergenceWarning, NotFittedError
 from sklearn.utils.extmath import fast_logdet
 from sklearn.utils.testing import assert_allclose
@@ -327,25 +328,33 @@ def test_suffstat_sk_full():
     X_resp = np.sqrt(resp) * X
     nk = np.array([n_samples])
     xk = np.zeros((1, n_features))
-    precs_pred = _estimate_gaussian_precisions_cholesky_full(resp, X,
-                                                             nk, xk, 0)
-    covars_pred = linalg.inv(np.dot(precs_pred[0], precs_pred[0].T))
+    covars_pred = _estimate_gaussian_covariances_full(resp, X, nk, xk, 0)
     ecov = EmpiricalCovariance(assume_centered=True)
     ecov.fit(X_resp)
-    assert_almost_equal(ecov.error_norm(covars_pred, norm='frobenius'), 0)
-    assert_almost_equal(ecov.error_norm(covars_pred, norm='spectral'), 0)
+    assert_almost_equal(ecov.error_norm(covars_pred[0], norm='frobenius'), 0)
+    assert_almost_equal(ecov.error_norm(covars_pred[0], norm='spectral'), 0)
+
+    # check the precision computation
+    precs_chol_pred = _compute_precision_cholesky(covars_pred, 'full')
+    precs_pred = np.array([np.dot(prec, prec.T) for prec in precs_chol_pred])
+    precs_est = np.array([linalg.inv(cov) for cov in covars_pred])
+    assert_array_almost_equal(precs_est, precs_pred)
 
     # special case 2, assuming resp are all ones
     resp = np.ones((n_samples, 1))
     nk = np.array([n_samples])
     xk = X.mean(axis=0).reshape((1, -1))
-    precs_pred = _estimate_gaussian_precisions_cholesky_full(resp, X,
-                                                             nk, xk, 0)
-    covars_pred = linalg.inv(np.dot(precs_pred[0], precs_pred[0].T))
+    covars_pred = _estimate_gaussian_covariances_full(resp, X, nk, xk, 0)
     ecov = EmpiricalCovariance(assume_centered=False)
     ecov.fit(X)
-    assert_almost_equal(ecov.error_norm(covars_pred, norm='frobenius'), 0)
-    assert_almost_equal(ecov.error_norm(covars_pred, norm='spectral'), 0)
+    assert_almost_equal(ecov.error_norm(covars_pred[0], norm='frobenius'), 0)
+    assert_almost_equal(ecov.error_norm(covars_pred[0], norm='spectral'), 0)
+
+    # check the precision computation
+    precs_chol_pred = _compute_precision_cholesky(covars_pred, 'full')
+    precs_pred = np.array([np.dot(prec, prec.T) for prec in precs_chol_pred])
+    precs_est = np.array([linalg.inv(cov) for cov in covars_pred])
+    assert_array_almost_equal(precs_est, precs_pred)
 
 
 def test_suffstat_sk_tied():
@@ -359,22 +368,22 @@ def test_suffstat_sk_tied():
     nk = resp.sum(axis=0)
     xk = np.dot(resp.T, X) / nk[:, np.newaxis]
 
-    precs_pred_full = _estimate_gaussian_precisions_cholesky_full(resp, X,
-                                                                  nk, xk, 0)
-    covars_pred_full = [linalg.inv(np.dot(precision_chol, precision_chol.T))
-                        for precision_chol in precs_pred_full]
+    covars_pred_full = _estimate_gaussian_covariances_full(resp, X, nk, xk, 0)
     covars_pred_full = np.sum(nk[:, np.newaxis, np.newaxis] * covars_pred_full,
                               0) / n_samples
 
-    precs_pred_tied = _estimate_gaussian_precisions_cholesky_tied(resp, X,
-                                                                  nk, xk, 0)
-    covars_pred_tied = linalg.inv(np.dot(precs_pred_tied, precs_pred_tied.T))
+    covars_pred_tied = _estimate_gaussian_covariances_tied(resp, X, nk, xk, 0)
 
     ecov = EmpiricalCovariance()
     ecov.covariance_ = covars_pred_full
     assert_almost_equal(ecov.error_norm(covars_pred_tied, norm='frobenius'), 0)
     assert_almost_equal(ecov.error_norm(covars_pred_tied, norm='spectral'), 0)
 
+    # check the precision computation
+    precs_chol_pred = _compute_precision_cholesky(covars_pred_tied, 'tied')
+    precs_pred = np.dot(precs_chol_pred, precs_chol_pred.T)
+    precs_est = linalg.inv(covars_pred_tied)
+    assert_array_almost_equal(precs_est, precs_pred)
 
 def test_suffstat_sk_diag():
     # test against 'full' case
@@ -386,22 +395,20 @@ def test_suffstat_sk_diag():
     X = rng.rand(n_samples, n_features)
     nk = resp.sum(axis=0)
     xk = np.dot(resp.T, X) / nk[:, np.newaxis]
-    precs_pred_full = _estimate_gaussian_precisions_cholesky_full(resp, X,
-                                                                  nk, xk, 0)
-    covars_pred_full = [linalg.inv(np.dot(precision_chol, precision_chol.T))
-                        for precision_chol in precs_pred_full]
-
-    precs_pred_diag = _estimate_gaussian_precisions_cholesky_diag(resp, X,
-                                                                  nk, xk, 0)
-    covars_pred_diag = np.array([np.diag(1. / d) ** 2
-                                 for d in precs_pred_diag])
+    covars_pred_full = _estimate_gaussian_covariances_full(resp, X, nk, xk, 0)
+    covars_pred_diag = _estimate_gaussian_covariances_diag(resp, X, nk, xk, 0)
 
     ecov = EmpiricalCovariance()
     for (cov_full, cov_diag) in zip(covars_pred_full, covars_pred_diag):
         ecov.covariance_ = np.diag(np.diag(cov_full))
+        cov_diag = np.diag(cov_diag)
         assert_almost_equal(ecov.error_norm(cov_diag, norm='frobenius'), 0)
         assert_almost_equal(ecov.error_norm(cov_diag, norm='spectral'), 0)
 
+    # check the precision computation
+    precs_chol_pred = _compute_precision_cholesky(covars_pred_diag, 'diag')
+    assert_almost_equal(covars_pred_diag, 1. / precs_chol_pred ** 2)
+
 
 def test_gaussian_suffstat_sk_spherical():
     # computing spherical covariance equals to the variance of one-dimension
@@ -414,12 +421,16 @@ def test_gaussian_suffstat_sk_spherical():
     resp = np.ones((n_samples, 1))
     nk = np.array([n_samples])
     xk = X.mean()
-    precs_pred_spherical = _estimate_gaussian_precisions_cholesky_spherical(
-        resp, X, nk, xk, 0)
-    covars_pred_spherical = (np.dot(X.flatten().T, X.flatten()) /
-                             (n_features * n_samples))
-    assert_almost_equal(1. / precs_pred_spherical ** 2, covars_pred_spherical)
-
+    covars_pred_spherical = _estimate_gaussian_covariances_spherical(resp, X,
+                                                                     nk, xk, 0)
+    covars_pred_spherical2 = (np.dot(X.flatten().T, X.flatten()) /
+                              (n_features * n_samples))
+    assert_almost_equal(covars_pred_spherical, covars_pred_spherical2)
+
+    # check the precision computation
+    precs_chol_pred = _compute_precision_cholesky(covars_pred_spherical,
+                                                  'spherical')
+    assert_almost_equal(covars_pred_spherical, 1. / precs_chol_pred ** 2)
 
 def _naive_lmvnpdf_diag(X, means, covars):
     resp = np.empty((len(X), len(means)))
diff --git a/sklearn/model_selection/_validation.py b/sklearn/model_selection/_validation.py
index 051fc0948c..2a591bfc09 100755
--- a/sklearn/model_selection/_validation.py
+++ b/sklearn/model_selection/_validation.py
@@ -235,7 +235,7 @@ def _fit_and_score(estimator, X, y, scorer, train, test, verbose,
     """
     if verbose > 1:
         if parameters is None:
-            msg = "no parameters to be set"
+            msg = ''
         else:
             msg = '%s' % (', '.join('%s=%s' % (k, v)
                           for k, v in parameters.items()))
@@ -301,6 +301,13 @@ def _score(estimator, X_test, y_test, scorer):
         score = scorer(estimator, X_test)
     else:
         score = scorer(estimator, X_test, y_test)
+    if hasattr(score, 'item'):
+        try:
+            # e.g. unwrap memmapped scalars
+            score = score.item()
+        except ValueError:
+            # non-scalar?
+            pass
     if not isinstance(score, numbers.Number):
         raise ValueError("scoring must return a number, got %s (%s) instead."
                          % (str(score), type(score)))
diff --git a/sklearn/model_selection/tests/test_validation.py b/sklearn/model_selection/tests/test_validation.py
index 66b307c31d..2e694fd45e 100755
--- a/sklearn/model_selection/tests/test_validation.py
+++ b/sklearn/model_selection/tests/test_validation.py
@@ -3,6 +3,9 @@
 
 import sys
 import warnings
+import tempfile
+import os
+from time import sleep
 
 import numpy as np
 from scipy.sparse import coo_matrix, csr_matrix
@@ -60,6 +63,12 @@
 from sklearn.model_selection.tests.test_split import MockClassifier
 
 
+try:
+    WindowsError
+except NameError:
+    WindowsError = None
+
+
 class MockImprovingEstimator(BaseEstimator):
     """Dummy classifier to test the learning curve"""
     def __init__(self, n_max_train_sizes):
@@ -769,3 +778,30 @@ def test_cross_val_predict_with_method():
         predictions = cross_val_predict(est, X, y, method=method,
                                         cv=kfold)
         assert_array_almost_equal(expected_predictions, predictions)
+
+
+def test_score_memmap():
+    # Ensure a scalar score of memmap type is accepted
+    iris = load_iris()
+    X, y = iris.data, iris.target
+    clf = MockClassifier()
+    tf = tempfile.NamedTemporaryFile(mode='wb', delete=False)
+    tf.write(b'Hello world!!!!!')
+    tf.close()
+    scores = np.memmap(tf.name, dtype=np.float64)
+    score = np.memmap(tf.name, shape=(), mode='r', dtype=np.float64)
+    try:
+        cross_val_score(clf, X, y, scoring=lambda est, X, y: score)
+        # non-scalar should still fail
+        assert_raises(ValueError, cross_val_score, clf, X, y,
+                      scoring=lambda est, X, y: scores)
+    finally:
+        # Best effort to release the mmap file handles before deleting the
+        # backing file under Windows
+        scores, score = None, None
+        for _ in range(3):
+            try:
+                os.unlink(tf.name)
+                break
+            except WindowsError:
+                sleep(1.)
diff --git a/sklearn/svm/classes.py b/sklearn/svm/classes.py
index 13f96b1804..cb3f542f69 100755
--- a/sklearn/svm/classes.py
+++ b/sklearn/svm/classes.py
@@ -588,12 +588,12 @@ class NuSVC(BaseSVC):
     cache_size : float, optional
         Specify the size of the kernel cache (in MB).
 
-    class_weight : {dict, 'auto'}, optional
+    class_weight : {dict, 'balanced'}, optional
         Set the parameter C of class i to class_weight[i]*C for
         SVC. If not given, all classes are supposed to have
-        weight one. The 'auto' mode uses the values of y to
-        automatically adjust weights inversely proportional to
-        class frequencies.
+        weight one. The "balanced" mode uses the values of y to automatically adjust
+        weights inversely proportional to class frequencies as
+        ``n_samples / (n_classes * np.bincount(y))``
 
     verbose : bool, default: False
         Enable verbose output. Note that this setting takes advantage of a
diff --git a/sklearn/utils/class_weight.py b/sklearn/utils/class_weight.py
index b2ba15807c..5b778423f1 100755
--- a/sklearn/utils/class_weight.py
+++ b/sklearn/utils/class_weight.py
@@ -71,7 +71,7 @@ def compute_class_weight(class_weight, classes, y):
         # user-defined dictionary
         weight = np.ones(classes.shape[0], dtype=np.float64, order='C')
         if not isinstance(class_weight, dict):
-            raise ValueError("class_weight must be dict, 'auto', or None,"
+            raise ValueError("class_weight must be dict, 'balanced', or None,"
                              " got: %r" % class_weight)
         for c in class_weight:
             i = np.searchsorted(classes, c)
diff --git a/sklearn/utils/sparsefuncs_fast.pyx b/sklearn/utils/sparsefuncs_fast.pyx
index 554f2836d2..dc444169df 100755
--- a/sklearn/utils/sparsefuncs_fast.pyx
+++ b/sklearn/utils/sparsefuncs_fast.pyx
@@ -23,24 +23,29 @@ ctypedef np.float64_t DOUBLE
 
 def csr_row_norms(X):
     """L2 norm of each row in CSR matrix X."""
+    if X.dtype != np.float32:
+        X = X.astype(np.float64)
+    return _csr_row_norms(X.data, X.shape, X.indices, X.indptr)
+
+
+def _csr_row_norms(np.ndarray[floating, ndim=1, mode="c"] X_data,
+                   shape,
+                   np.ndarray[int, ndim=1, mode="c"] X_indices,
+                   np.ndarray[int, ndim=1, mode="c"] X_indptr):
     cdef:
-        unsigned int n_samples = X.shape[0]
-        unsigned int n_features = X.shape[1]
+        unsigned int n_samples = shape[0]
+        unsigned int n_features = shape[1]
         np.ndarray[DOUBLE, ndim=1, mode="c"] norms
-        np.ndarray[DOUBLE, ndim=1, mode="c"] data
-        np.ndarray[int, ndim=1, mode="c"] indices = X.indices
-        np.ndarray[int, ndim=1, mode="c"] indptr = X.indptr
 
         np.npy_intp i, j
         double sum_
 
     norms = np.zeros(n_samples, dtype=np.float64)
-    data = np.asarray(X.data, dtype=np.float64)     # might copy!
 
     for i in range(n_samples):
         sum_ = 0.0
-        for j in range(indptr[i], indptr[i + 1]):
-            sum_ += data[j] * data[j]
+        for j in range(X_indptr[i], X_indptr[i + 1]):
+            sum_ += X_data[j] * X_data[j]
         norms[i] = sum_
 
     return norms
diff --git a/sklearn/utils/tests/test_extmath.py b/sklearn/utils/tests/test_extmath.py
index ea3ee60770..5847d0566a 100755
--- a/sklearn/utils/tests/test_extmath.py
+++ b/sklearn/utils/tests/test_extmath.py
@@ -148,14 +148,23 @@ def test_norm_squared_norm():
 
 def test_row_norms():
     X = np.random.RandomState(42).randn(100, 100)
-    sq_norm = (X ** 2).sum(axis=1)
-
-    assert_array_almost_equal(sq_norm, row_norms(X, squared=True), 5)
-    assert_array_almost_equal(np.sqrt(sq_norm), row_norms(X))
-
-    Xcsr = sparse.csr_matrix(X, dtype=np.float32)
-    assert_array_almost_equal(sq_norm, row_norms(Xcsr, squared=True), 5)
-    assert_array_almost_equal(np.sqrt(sq_norm), row_norms(Xcsr))
+    for dtype in (np.float32, np.float64):
+        if dtype is np.float32:
+            precision = 4
+        else:
+            precision = 5
+
+        X = X.astype(dtype)
+        sq_norm = (X ** 2).sum(axis=1)
+
+        assert_array_almost_equal(sq_norm, row_norms(X, squared=True),
+                                  precision)
+        assert_array_almost_equal(np.sqrt(sq_norm), row_norms(X), precision)
+
+        Xcsr = sparse.csr_matrix(X, dtype=dtype)
+        assert_array_almost_equal(sq_norm, row_norms(Xcsr, squared=True),
+                                  precision)
+        assert_array_almost_equal(np.sqrt(sq_norm), row_norms(Xcsr), precision)
 
 
 def test_randomized_svd_low_rank_with_noise():
