diff --git a/.circleci/config.yml b/.circleci/config.yml
index 7ad3e5427b27..3a1bc848942d 100644
--- a/.circleci/config.yml
+++ b/.circleci/config.yml
@@ -65,6 +65,7 @@ jobs:
           path: ~/log.txt
           destination: log.txt
 
+
   deploy:
     docker:
       - image: circleci/python:3.6.1
@@ -91,4 +92,3 @@ workflows:
       - deploy:
           requires:
             - python3
-            - python2
diff --git a/.travis.yml b/.travis.yml
index 92363f82ad72..7196296a386d 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -38,13 +38,15 @@ matrix:
            NUMPY_VERSION="1.10.4" SCIPY_VERSION="0.16.1" CYTHON_VERSION="0.25.2"
            PILLOW_VERSION="4.0.0" COVERAGE=true
       if: type != cron
-    # This environment tests the newest supported Anaconda release (5.0.0)
-    # It also runs tests requiring Pandas and PyAMG
-    - env: DISTRIB="conda" PYTHON_VERSION="3.6.2" INSTALL_MKL="true"
-           NUMPY_VERSION="1.14.2" SCIPY_VERSION="1.0.0" PANDAS_VERSION="0.20.3"
-           CYTHON_VERSION="0.26.1" PYAMG_VERSION="3.3.2" PILLOW_VERSION="4.3.0"
-           COVERAGE=true
+    # This environment tests the latest available dependencies.
+    # It runs tests requiring pandas and PyAMG.
+    # It also runs with the site joblib instead of the vendored copy of joblib.
+    - env: DISTRIB="conda" PYTHON_VERSION="*" INSTALL_MKL="true"
+           NUMPY_VERSION="*" SCIPY_VERSION="*" PANDAS_VERSION="*"
+           CYTHON_VERSION="*" PYAMG_VERSION="*" PILLOW_VERSION="*"
+           JOBLIB_VERSION="*" COVERAGE=true
            CHECK_PYTEST_SOFT_DEPENDENCY="true" TEST_DOCSTRINGS="true"
+           SKLEARN_SITE_JOBLIB=1
       if: type != cron
     # flake8 linting on diff wrt common ancestor with upstream/master
     - env: RUN_FLAKE8="true" SKIP_TESTS="true"
diff --git a/AUTHORS.rst b/AUTHORS.rst
index 628851aaed51..48427fc0a2b3 100644
--- a/AUTHORS.rst
+++ b/AUTHORS.rst
@@ -47,6 +47,7 @@ The following people have been core contributors to scikit-learn's development a
   * `Kyle Kastner <http://kastnerkyle.github.io>`_
   * `Manoj Kumar <https://manojbits.wordpress.com>`_
   * Robert Layton
+  * `Guillaume Lemaitre <https://github.com/glemaitre>`_
   * `Wei Li <http://kuantkid.github.io/>`_
   * Paolo Losi
   * `Gilles Louppe <http://glouppe.github.io/>`_
@@ -59,11 +60,14 @@ The following people have been core contributors to scikit-learn's development a
   * `Alexandre Passos <http://atpassos.posterous.com>`_
   * `Fabian Pedregosa <http://fa.bianp.net/blog/>`_
   * `Peter Prettenhofer <https://sites.google.com/site/peterprettenhofer/>`_
+  * `Hanmin Qin <https://github.com/qinhanmin2014>`_
   * Bertrand Thirion
+  * `Joris Van den Bossche <https://github.com/jorisvandenbossche>`_
   * `Jake VanderPlas <http://staff.washington.edu/jakevdp/>`_
   * Nelle Varoquaux
   * `Gael Varoquaux <http://gael-varoquaux.info/>`_
   * Ron Weiss
+  * `Roman Yurchak <https://github.com/rth>`_
 
 Please do not email the authors directly to ask for assistance or report issues.
 Instead, please see `What's the best way to ask questions about scikit-learn
diff --git a/MANIFEST.in b/MANIFEST.in
index ed0ca0e87274..db605f55f748 100644
--- a/MANIFEST.in
+++ b/MANIFEST.in
@@ -2,7 +2,7 @@ include *.rst
 recursive-include doc *
 recursive-include examples *
 recursive-include sklearn *.c *.h *.pyx *.pxd *.pxi
-recursive-include sklearn/datasets *.csv *.csv.gz *.rst *.jpg *.txt
+recursive-include sklearn/datasets *.csv *.csv.gz *.rst *.jpg *.txt *.arff.gz *.json.gz
 include COPYING
 include AUTHORS.rst
 include README.rst
diff --git a/README.rst b/README.rst
index 4df228acd4c4..eb1957686aca 100644
--- a/README.rst
+++ b/README.rst
@@ -53,6 +53,9 @@ scikit-learn requires:
 - NumPy (>= 1.8.2)
 - SciPy (>= 0.13.3)
 
+**Scikit-learn 0.20 is the last version to support Python2.7.**
+Scikit-learn 0.21 and later will require Python 3.5 or newer.
+
 For running the examples Matplotlib >= 1.3.1 is required. A few examples
 require scikit-image >= 0.9.3 and a few examples require pandas >= 0.13.1.
 
diff --git a/appveyor.yml b/appveyor.yml
index d982cca850c5..5eb4d08a8737 100644
--- a/appveyor.yml
+++ b/appveyor.yml
@@ -17,22 +17,14 @@ environment:
     SKLEARN_SKIP_NETWORK_TESTS: 1
 
   matrix:
-    - PYTHON: "C:\\Python27"
-      PYTHON_VERSION: "2.7.8"
-      PYTHON_ARCH: "32"
-
-    - PYTHON: "C:\\Python27-x64"
-      PYTHON_VERSION: "2.7.8"
+    - PYTHON: "C:\\Python37-x64"
+      PYTHON_VERSION: "3.7.0"
       PYTHON_ARCH: "64"
 
-    - PYTHON: "C:\\Python36"
-      PYTHON_VERSION: "3.6.1"
+    - PYTHON: "C:\\Python27"
+      PYTHON_VERSION: "2.7.8"
       PYTHON_ARCH: "32"
 
-    - PYTHON: "C:\\Python36-x64"
-      PYTHON_VERSION: "3.6.1"
-      PYTHON_ARCH: "64"
-
 
 # Because we only have a single worker, we don't want to waste precious
 # appveyor CI time and make other PRs wait for repeated failures in a failing
@@ -49,7 +41,7 @@ install:
   # directly to master instead of just PR builds.
   # credits: JuliaLang developers.
   - ps: if ($env:APPVEYOR_PULL_REQUEST_NUMBER -and $env:APPVEYOR_BUILD_NUMBER -ne ((Invoke-RestMethod `
-        https://ci.appveyor.com/api/projects/$env:APPVEYOR_ACCOUNT_NAME/$env:APPVEYOR_PROJECT_SLUG/history?recordsNumber=50).builds | `
+        https://ci.appveyor.com/api/projects/$env:APPVEYOR_ACCOUNT_NAME/$env:APPVEYOR_PROJECT_SLUG/history?recordsNumber=500).builds | `
         Where-Object pullRequestId -eq $env:APPVEYOR_PULL_REQUEST_NUMBER)[0].buildNumber) { `
         throw "There are newer queued builds for this pull request, failing early." }
 
diff --git a/benchmarks/bench_covertype.py b/benchmarks/bench_covertype.py
index d5ee0c04eba6..c7b23f82d2d1 100644
--- a/benchmarks/bench_covertype.py
+++ b/benchmarks/bench_covertype.py
@@ -59,7 +59,7 @@
 from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
 from sklearn.ensemble import GradientBoostingClassifier
 from sklearn.metrics import zero_one_loss
-from sklearn.externals.joblib import Memory
+from sklearn.utils import Memory
 from sklearn.utils import check_array
 
 # Memoize the data extraction and memory map the resulting
diff --git a/benchmarks/bench_isolation_forest.py b/benchmarks/bench_isolation_forest.py
index 547b4f3ed2dd..585ead9a3be8 100644
--- a/benchmarks/bench_isolation_forest.py
+++ b/benchmarks/bench_isolation_forest.py
@@ -119,7 +119,8 @@ def print_outlier_ratio(y):
     y_test = y[n_samples_train:]
 
     print('--- Fitting the IsolationForest estimator...')
-    model = IsolationForest(n_jobs=-1, random_state=random_state)
+    model = IsolationForest(behaviour='new', n_jobs=-1,
+                            random_state=random_state)
     tstart = time()
     model.fit(X_train)
     fit_time = time() - tstart
diff --git a/benchmarks/bench_mnist.py b/benchmarks/bench_mnist.py
index f84eed5f9479..0182cfad7a71 100644
--- a/benchmarks/bench_mnist.py
+++ b/benchmarks/bench_mnist.py
@@ -41,7 +41,7 @@
 from sklearn.ensemble import ExtraTreesClassifier
 from sklearn.ensemble import RandomForestClassifier
 from sklearn.dummy import DummyClassifier
-from sklearn.externals.joblib import Memory
+from sklearn.utils import Memory
 from sklearn.kernel_approximation import Nystroem
 from sklearn.kernel_approximation import RBFSampler
 from sklearn.metrics import zero_one_loss
diff --git a/benchmarks/bench_plot_nmf.py b/benchmarks/bench_plot_nmf.py
index c48977a49a72..87885f091da8 100644
--- a/benchmarks/bench_plot_nmf.py
+++ b/benchmarks/bench_plot_nmf.py
@@ -22,7 +22,7 @@
 from sklearn.decomposition.nmf import _initialize_nmf
 from sklearn.decomposition.nmf import _beta_divergence
 from sklearn.decomposition.nmf import INTEGER_TYPES, _check_init
-from sklearn.externals.joblib import Memory
+from sklearn.utils import Memory
 from sklearn.exceptions import ConvergenceWarning
 from sklearn.utils.extmath import safe_sparse_dot, squared_norm
 from sklearn.utils import check_array
diff --git a/benchmarks/bench_rcv1_logreg_convergence.py b/benchmarks/bench_rcv1_logreg_convergence.py
index 417cae5aac1d..a4116a68f6c3 100644
--- a/benchmarks/bench_rcv1_logreg_convergence.py
+++ b/benchmarks/bench_rcv1_logreg_convergence.py
@@ -8,7 +8,7 @@
 import gc
 import time
 
-from sklearn.externals.joblib import Memory
+from sklearn.utils import Memory
 from sklearn.linear_model import (LogisticRegression, SGDClassifier)
 from sklearn.datasets import fetch_rcv1
 from sklearn.linear_model.sag import get_auto_step_size
diff --git a/benchmarks/bench_saga.py b/benchmarks/bench_saga.py
index 10aca379123a..9e79c536c5b2 100644
--- a/benchmarks/bench_saga.py
+++ b/benchmarks/bench_saga.py
@@ -12,7 +12,7 @@
 
 from sklearn.datasets import fetch_rcv1, load_iris, load_digits, \
     fetch_20newsgroups_vectorized
-from sklearn.externals.joblib import delayed, Parallel, Memory
+from sklearn.utils import delayed, Parallel, Memory
 from sklearn.linear_model import LogisticRegression
 from sklearn.metrics import log_loss
 from sklearn.model_selection import train_test_split
diff --git a/benchmarks/bench_tsne_mnist.py b/benchmarks/bench_tsne_mnist.py
index 26dde6aac312..36630eeb15d2 100644
--- a/benchmarks/bench_tsne_mnist.py
+++ b/benchmarks/bench_tsne_mnist.py
@@ -15,7 +15,7 @@
 import json
 import argparse
 
-from sklearn.externals.joblib import Memory
+from sklearn.utils import Memory
 from sklearn.datasets import fetch_mldata
 from sklearn.manifold import TSNE
 from sklearn.neighbors import NearestNeighbors
diff --git a/build_tools/appveyor/requirements.txt b/build_tools/appveyor/requirements.txt
index 35c772b52d32..6cd6f13928b1 100644
--- a/build_tools/appveyor/requirements.txt
+++ b/build_tools/appveyor/requirements.txt
@@ -1,15 +1,7 @@
-# Fetch numpy and scipy wheels from the sklearn rackspace wheelhouse.
-# Those wheels were collected from https://www.lfd.uci.edu/~gohlke/pythonlibs/
-# This is a temporary solution. As soon as numpy and scipy provide official
-# wheel for windows we ca delete this --find-links line.
---find-links http://28daf2247a33ed269873-7b1aad3fab3cc330e1fd9d109892382a.r6.cf2.rackcdn.com/
-
-# fix the versions of numpy to force the use of numpy and scipy to use the whl
-# of the rackspace folder instead of trying to install from more recent
-# source tarball published on PyPI
-numpy==1.13.0
-scipy==0.19.0
-cython
+numpy
+scipy
+# Pin Cython to avoid bug with 0.28.x on Python 3.7 
+cython==0.27.3
 pytest
 wheel
 wheelhouse_uploader
diff --git a/build_tools/circle/build_doc.sh b/build_tools/circle/build_doc.sh
index 86231f8de40c..e290b8ca93db 100755
--- a/build_tools/circle/build_doc.sh
+++ b/build_tools/circle/build_doc.sh
@@ -92,6 +92,8 @@ else
     make_args=html
 fi
 
+make_args="SPHINXOPTS=-T $make_args"  # show full traceback on exception
+
 # Installing required system packages to support the rendering of math
 # notation in the HTML documentation
 sudo -E apt-get -yq update
diff --git a/build_tools/circle/build_test_pypy.sh b/build_tools/circle/build_test_pypy.sh
new file mode 100755
index 000000000000..18fa361821d1
--- /dev/null
+++ b/build_tools/circle/build_test_pypy.sh
@@ -0,0 +1,30 @@
+#!/usr/bin/env bash
+set -x
+set -e
+
+apt-get -yq update
+apt-get -yq install libatlas-dev libatlas-base-dev liblapack-dev gfortran ccache
+
+pip install virtualenv
+
+if command -v pypy3; then
+    virtualenv -p $(command -v pypy3) pypy-env
+elif command -v pypy; then
+    virtualenv -p $(command -v pypy) pypy-env
+fi
+
+source pypy-env/bin/activate
+
+python --version
+which python
+
+pip install --extra-index https://antocuni.github.io/pypy-wheels/ubuntu numpy==1.14.4 Cython pytest
+pip install "scipy>=1.1.0" sphinx numpydoc docutils
+
+ccache -M 512M
+export CCACHE_COMPRESS=1
+export PATH=/usr/lib/ccache:$PATH
+
+pip install -e .
+
+make test
diff --git a/build_tools/travis/install.sh b/build_tools/travis/install.sh
index 2b03f4a98039..d41e746a1ab2 100755
--- a/build_tools/travis/install.sh
+++ b/build_tools/travis/install.sh
@@ -24,7 +24,8 @@ export CXX=/usr/lib/ccache/g++
 # ~60M is used by .ccache when compiling from scratch at the time of writing
 ccache --max-size 100M --show-stats
 
-if [[ "$DISTRIB" == "conda" ]]; then
+make_conda() {
+	TO_INSTALL="$@"
     # Deactivate the travis-provided virtual environment and setup a
     # conda-based environment instead
     deactivate
@@ -37,6 +38,11 @@ if [[ "$DISTRIB" == "conda" ]]; then
     export PATH=$MINICONDA_PATH/bin:$PATH
     conda update --yes conda
 
+    conda create -n testenv --yes $TO_INSTALL
+    source activate testenv
+}
+
+if [[ "$DISTRIB" == "conda" ]]; then
     TO_INSTALL="python=$PYTHON_VERSION pip pytest pytest-cov \
                 numpy=$NUMPY_VERSION scipy=$SCIPY_VERSION \
                 cython=$CYTHON_VERSION"
@@ -59,8 +65,10 @@ if [[ "$DISTRIB" == "conda" ]]; then
         TO_INSTALL="$TO_INSTALL pillow=$PILLOW_VERSION"
     fi
 
-    conda create -n testenv --yes $TO_INSTALL
-    source activate testenv
+    if [[ -n "$JOBLIB_VERSION" ]]; then
+        TO_INSTALL="$TO_INSTALL joblib=$JOBLIB_VERSION"
+    fi
+	  make_conda $TO_INSTALL
 
     # for python 3.4, conda does not have recent pytest packages
     if [[ "$PYTHON_VERSION" == "3.4" ]]; then
@@ -79,11 +87,7 @@ elif [[ "$DISTRIB" == "ubuntu" ]]; then
     pip install pytest pytest-cov cython==$CYTHON_VERSION
 
 elif [[ "$DISTRIB" == "scipy-dev" ]]; then
-    # Set up our own virtualenv environment to avoid travis' numpy.
-    # This venv points to the python interpreter of the travis build
-    # matrix.
-    virtualenv --python=python ~/testvenv
-    source ~/testvenv/bin/activate
+    make_conda python=3.7
     pip install --upgrade pip setuptools
 
     echo "Installing numpy and scipy master wheels"
diff --git a/conftest.py b/conftest.py
index c2b9ae203887..621097bfc47a 100644
--- a/conftest.py
+++ b/conftest.py
@@ -5,6 +5,7 @@
 # doc/modules/clustering.rst and use sklearn from the local folder rather than
 # the one from site-packages.
 
+import platform
 from distutils.version import LooseVersion
 
 import pytest
@@ -12,6 +13,15 @@
 
 
 def pytest_collection_modifyitems(config, items):
+
+    # FeatureHasher is not compatible with PyPy
+    if platform.python_implementation() == 'PyPy':
+        skip_marker = pytest.mark.skip(
+            reason='FeatureHasher is not compatible with PyPy')
+        for item in items:
+            if item.name == 'sklearn.feature_extraction.hashing.FeatureHasher':
+                item.add_marker(skip_marker)
+
     # numpy changed the str/repr formatting of numpy arrays in 1.14. We want to
     # run doctests only for numpy >= 1.14.
     skip_doctests = True
diff --git a/doc/Makefile b/doc/Makefile
index 557eeaa188d2..fcb547d14e2b 100644
--- a/doc/Makefile
+++ b/doc/Makefile
@@ -98,7 +98,7 @@ doctest:
 	      "results in $(BUILDDIR)/doctest/output.txt."
 
 download-data:
-	python -c "from sklearn.datasets.lfw import check_fetch_lfw; check_fetch_lfw()"
+	python -c "from sklearn.datasets.lfw import _check_fetch_lfw; _check_fetch_lfw()"
 
 # Optimize PNG files. Needs OptiPNG. Change the -P argument to the number of
 # cores you have available, so -P 64 if you have a real computer ;)
diff --git a/doc/about.rst b/doc/about.rst
index ca7bf4ad8d6c..90295b96fb6f 100644
--- a/doc/about.rst
+++ b/doc/about.rst
@@ -125,6 +125,15 @@ Andreas Müller also received a grant to improve scikit-learn from the `Alfred P
    :align: center
    :target: http://www.sydney.edu.au/
 
+`The Labex DigiCosme <https://digicosme.lri.fr>`_ funded Nicolas Goix (2015-2016),
+Tom Dupré la Tour (2015-2016 and 2017-2018), Mathurin Massias (2018-2019) to work part time
+on scikit-learn during their PhDs. It also funded a scikit-learn coding sprint in 2015.
+
+.. image:: themes/scikit-learn/static/img/digicosme.png
+   :width: 200pt
+   :align: center
+   :target: https://digicosme.lri.fr
+
 The following students were sponsored by `Google <https://developers.google.com/open-source/>`_
 to work on scikit-learn through the
 `Google Summer of Code <https://en.wikipedia.org/wiki/Google_Summer_of_Code>`_
diff --git a/doc/conf.py b/doc/conf.py
index fac3b9fc043f..e0dc4c6f4abf 100644
--- a/doc/conf.py
+++ b/doc/conf.py
@@ -237,6 +237,7 @@
     'scipy': ('https://docs.scipy.org/doc/scipy/reference', None),
     'matplotlib': ('https://matplotlib.org/', None),
     'pandas': ('https://pandas.pydata.org/pandas-docs/stable/', None),
+    'joblib': ('https://joblib.readthedocs.io/en/latest/', None),
 }
 
 sphinx_gallery_conf = {
diff --git a/doc/conftest.py b/doc/conftest.py
index 463df3f38221..7e229781cd32 100644
--- a/doc/conftest.py
+++ b/doc/conftest.py
@@ -1,8 +1,11 @@
+import os
 from os.path import exists
 from os.path import join
+import warnings
 
 import numpy as np
 
+from sklearn.utils import IS_PYPY
 from sklearn.utils.testing import SkipTest
 from sklearn.utils.testing import check_skip_network
 from sklearn.datasets import get_data_home
@@ -55,6 +58,8 @@ def setup_twenty_newsgroups():
 
 
 def setup_working_with_text_data():
+    if IS_PYPY and os.environ.get('CI', None):
+        raise SkipTest('Skipping too slow test with PyPy on CI')
     check_skip_network()
     cache_path = _pkl_filepath(get_data_home(), CACHE_NAME)
     if not exists(cache_path):
@@ -75,6 +80,12 @@ def setup_impute():
         raise SkipTest("Skipping impute.rst, pandas not installed")
 
 
+def setup_unsupervised_learning():
+    # ignore deprecation warnings from scipy.misc.face
+    warnings.filterwarnings('ignore', 'The binary mode of fromstring',
+                            DeprecationWarning)
+
+
 def pytest_runtest_setup(item):
     fname = item.fspath.strpath
     is_index = fname.endswith('datasets/index.rst')
@@ -91,8 +102,12 @@ def pytest_runtest_setup(item):
         setup_working_with_text_data()
     elif fname.endswith('modules/compose.rst') or is_index:
         setup_compose()
+    elif IS_PYPY and fname.endswith('modules/feature_extraction.rst'):
+        raise SkipTest('FeatureHasher is not compatible with PyPy')
     elif fname.endswith('modules/impute.rst'):
         setup_impute()
+    elif fname.endswith('statistical_inference/unsupervised_learning.rst'):
+        setup_unsupervised_learning()
 
 
 def pytest_runtest_teardown(item):
diff --git a/doc/datasets/index.rst b/doc/datasets/index.rst
index d8eabfaabd27..947e55f0c4c3 100644
--- a/doc/datasets/index.rst
+++ b/doc/datasets/index.rst
@@ -44,7 +44,7 @@ some contain ``feature_names`` and ``target_names``. See the dataset
 descriptions below for details.  
 
 **The dataset generation functions.** They can be used to generate controlled 
-synthetic datasets, described in the :ref:`generated_datasets` section.
+synthetic datasets, described in the :ref:`sample_generators` section.
 
 These functions return a tuple ``(X, y)`` consisting of a ``n_samples`` *
 ``n_features`` numpy array ``X`` and an array of length ``n_samples``
@@ -130,6 +130,7 @@ They can be loaded using the following functions:
    fetch_covtype
    fetch_rcv1
    fetch_kddcup99
+   fetch_california_housing
 
 .. toctree::
     :maxdepth: 2
@@ -141,20 +142,23 @@ They can be loaded using the following functions:
     covtype
     rcv1
     kddcup99
+    california_housing
 
-.. include:: ./olivetti_faces.rst
+.. include:: ../../sklearn/datasets/descr/olivetti_faces.rst
 
-.. include:: ./twenty_newsgroups.rst
+.. include:: ../../sklearn/datasets/descr/twenty_newsgroups.rst
 
-.. include:: ./labeled_faces.rst
+.. include:: ../../sklearn/datasets/descr/lfw.rst
 
-.. include:: ./covtype.rst
+.. include:: ../../sklearn/datasets/descr/covtype.rst
 
-.. include:: ./rcv1.rst
+.. include:: ../../sklearn/datasets/descr/rcv1.rst
 
-.. include:: ./kddcup99.rst
+.. include:: ../../sklearn/datasets/descr/kddcup99.rst
 
-.. _generated_datasets:
+.. include:: ../../sklearn/datasets/descr/california_housing.rst
+
+.. _sample_generators:
 
 Generated datasets
 ==================
@@ -347,89 +351,6 @@ features::
 
  _`Faster API-compatible implementation`: https://github.com/mblondel/svmlight-loader
 
-..
-    For doctests:
-
-    >>> import numpy as np
-    >>> import os
-    >>> import tempfile
-    >>> # Create a temporary folder for the data fetcher
-    >>> custom_data_home = tempfile.mkdtemp()
-    >>> os.makedirs(os.path.join(custom_data_home, 'mldata'))
-
-
-.. _mldata:
-
-Downloading datasets from the mldata.org repository
----------------------------------------------------
-
-`mldata.org <http://mldata.org>`_ is a public repository for machine learning
-data, supported by the `PASCAL network <http://www.pascal-network.org>`_ .
-
-The ``sklearn.datasets`` package is able to directly download data
-sets from the repository using the function
-:func:`sklearn.datasets.fetch_mldata`.
-
-For example, to download the MNIST digit recognition database::
-
-  >>> from sklearn.datasets import fetch_mldata
-  >>> mnist = fetch_mldata('MNIST original', data_home=custom_data_home)
-
-The MNIST database contains a total of 70000 examples of handwritten digits
-of size 28x28 pixels, labeled from 0 to 9::
-
-  >>> mnist.data.shape
-  (70000, 784)
-  >>> mnist.target.shape
-  (70000,)
-  >>> np.unique(mnist.target)
-  array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])
-
-After the first download, the dataset is cached locally in the path
-specified by the ``data_home`` keyword argument, which defaults to
-``~/scikit_learn_data/``::
-
-  >>> os.listdir(os.path.join(custom_data_home, 'mldata'))
-  ['mnist-original.mat']
-
-Data sets in `mldata.org <http://mldata.org>`_ do not adhere to a strict
-naming or formatting convention. :func:`sklearn.datasets.fetch_mldata` is
-able to make sense of the most common cases, but allows to tailor the
-defaults to individual datasets:
-
-* The data arrays in `mldata.org <http://mldata.org>`_ are most often
-  shaped as ``(n_features, n_samples)``. This is the opposite of the
-  ``scikit-learn`` convention, so :func:`sklearn.datasets.fetch_mldata`
-  transposes the matrix by default. The ``transpose_data`` keyword controls
-  this behavior::
-
-    >>> iris = fetch_mldata('iris', data_home=custom_data_home)
-    >>> iris.data.shape
-    (150, 4)
-    >>> iris = fetch_mldata('iris', transpose_data=False,
-    ...                     data_home=custom_data_home)
-    >>> iris.data.shape
-    (4, 150)
-
-* For datasets with multiple columns, :func:`sklearn.datasets.fetch_mldata`
-  tries to identify the target and data columns and rename them to ``target``
-  and ``data``. This is done by looking for arrays named ``label`` and
-  ``data`` in the dataset, and failing that by choosing the first array to be
-  ``target`` and the second to be ``data``. This behavior can be changed with
-  the ``target_name`` and ``data_name`` keywords, setting them to a specific
-  name or index number (the name and order of the columns in the datasets
-  can be found at its `mldata.org <http://mldata.org>`_ under the tab "Data"::
-
-    >>> iris2 = fetch_mldata('datasets-UCI iris', target_name=1, data_name=0,
-    ...                      data_home=custom_data_home)
-    >>> iris3 = fetch_mldata('datasets-UCI iris', target_name='class',
-    ...                      data_name='double0', data_home=custom_data_home)
-
-
-..
-    >>> import shutil
-    >>> shutil.rmtree(custom_data_home)
-
 .. _external_datasets:
 
 Loading from external datasets
diff --git a/doc/datasets/kddcup99.rst b/doc/datasets/kddcup99.rst
deleted file mode 100644
index e770a9a2d60e..000000000000
--- a/doc/datasets/kddcup99.rst
+++ /dev/null
@@ -1,35 +0,0 @@
-.. _kddcup99:
-
-Kddcup 99 dataset
------------------
-
-The KDD Cup '99 dataset was created by processing the tcpdump portions
-of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset,
-created by MIT Lincoln Lab. The artificial data (described on the `dataset's
-homepage <http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html>`__) was
-generated using a closed network and hand-injected attacks to produce a
-large number of different types of attack with normal activity in the
-background. As the initial goal was to produce a large training set for
-supervised learning algorithms, there is a large proportion (80.1%) of
-abnormal data which is unrealistic in real world, and inappropriate for
-unsupervised anomaly detection which aims at detecting 'abnormal' data, ie
-1) qualitatively different from normal data
-2) in large minority among the observations.
-We thus transform the KDD Data set into two different data sets: SA and SF.
-
--SA is obtained by simply selecting all the normal data, and a small
-proportion of abnormal data to gives an anomaly proportion of 1%.
-
--SF is obtained as in [2]
-by simply picking up the data whose attribute logged_in is positive, thus
-focusing on the intrusion attack, which gives a proportion of 0.3% of
-attack.
-
--http and smtp are two subsets of SF corresponding with third feature
-equal to 'http' (resp. to 'smtp')
-
-:func:`sklearn.datasets.fetch_kddcup99` will load the kddcup99 dataset;
-it returns a dictionary-like object
-with the feature matrix in the ``data`` member
-and the target values in ``target``.
-The dataset will be downloaded from the web if necessary.
diff --git a/doc/datasets/openml.rst b/doc/datasets/openml.rst
new file mode 100644
index 000000000000..52dd45391952
--- /dev/null
+++ b/doc/datasets/openml.rst
@@ -0,0 +1,148 @@
+..
+    For doctests:
+
+    >>> import numpy as np
+    >>> import os
+
+
+.. _openml:
+
+Downloading datasets from the openml.org repository
+===================================================
+
+`openml.org <https://openml.org>`_ is a public repository for machine learning
+data and experiments, that allows everybody to upload open datasets.
+
+The ``sklearn.datasets`` package is able to download datasets
+from the repository using the function
+:func:`sklearn.datasets.fetch_openml`.
+
+For example, to download a dataset of gene expressions in mice brains::
+
+  >>> from sklearn.datasets import fetch_openml
+  >>> mice = fetch_openml(name='miceprotein', version=4)
+
+To fully specify a dataset, you need to provide a name and a version, though
+the version is optional, see :ref:`openml_versions` below.
+The dataset contains a total of 1080 examples belonging to 8 different
+classes::
+
+  >>> mice.data.shape
+  (1080, 77)
+  >>> mice.target.shape
+  (1080,)
+  >>> np.unique(mice.target) # doctest: +NORMALIZE_WHITESPACE
+  array(['c-CS-m', 'c-CS-s', 'c-SC-m', 'c-SC-s', 't-CS-m', 't-CS-s', 't-SC-m', 't-SC-s'], dtype=object)
+
+You can get more information on the dataset by looking at the ``DESCR``
+and ``details`` attributes::
+
+  >>> print(mice.DESCR) # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS +SKIP
+  **Author**: Clara Higuera, Katheleen J. Gardiner, Krzysztof J. Cios
+  **Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression) - 2015
+  **Please cite**: Higuera C, Gardiner KJ, Cios KJ (2015) Self-Organizing
+  Feature Maps Identify Proteins Critical to Learning in a Mouse Model of Down
+  Syndrome. PLoS ONE 10(6): e0129126...
+
+  >>> mice.details # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS +SKIP
+  {'id': '40966', 'name': 'MiceProtein', 'version': '4', 'format': 'ARFF',
+  'upload_date': '2017-11-08T16:00:15', 'licence': 'Public',
+  'url': 'https://www.openml.org/data/v1/download/17928620/MiceProtein.arff',
+  'file_id': '17928620', 'default_target_attribute': 'class',
+  'row_id_attribute': 'MouseID',
+  'ignore_attribute': ['Genotype', 'Treatment', 'Behavior'],
+  'tag': ['OpenML-CC18', 'study_135', 'study_98', 'study_99'],
+  'visibility': 'public', 'status': 'active',
+  'md5_checksum': '3c479a6885bfa0438971388283a1ce32'}
+
+
+The ``DESCR`` contains a free-text description of the data, while ``details``
+contains a dictionary of meta-data stored by openml, like the dataset id.
+For more details, see the `OpenML documentation
+<https://docs.openml.org/#data>`_ The ``data_id`` of the mice protein dataset
+is 40966, and you can use this (or the name) to get more information on the
+dataset on the openml website::
+
+  >>> mice.url
+  'https://www.openml.org/d/40966'
+
+The ``data_id`` also uniquely identifies a dataset from OpenML::
+
+  >>> mice = fetch_openml(data_id=40966)
+  >>> mice.details # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS +SKIP
+  {'id': '4550', 'name': 'MiceProtein', 'version': '1', 'format': 'ARFF',
+  'creator': ...,
+  'upload_date': '2016-02-17T14:32:49', 'licence': 'Public', 'url':
+  'https://www.openml.org/data/v1/download/1804243/MiceProtein.ARFF', 'file_id':
+  '1804243', 'default_target_attribute': 'class', 'citation': 'Higuera C,
+  Gardiner KJ, Cios KJ (2015) Self-Organizing Feature Maps Identify Proteins
+  Critical to Learning in a Mouse Model of Down Syndrome. PLoS ONE 10(6):
+  e0129126. [Web Link] journal.pone.0129126', 'tag': ['OpenML100', 'study_14',
+  'study_34'], 'visibility': 'public', 'status': 'active', 'md5_checksum':
+  '3c479a6885bfa0438971388283a1ce32'}
+
+.. _openml_versions:
+
+Dataset Versions
+----------------
+
+A dataset is uniquely specified by its ``data_id``, but not necessarily by its
+name. Several different "versions" of a dataset with the same name can exist
+which can contain entirely different datasets.
+If a particular version of a dataset has been found to contain significant
+issues, it might be deactivated. Using a name to specify a dataset will yield
+the earliest version of a dataset that is still active. That means that
+``fetch_openml(name="miceprotein")`` can yield different results at different
+times if earlier versions become inactive.
+You can see that the dataset with ``data_id`` 40966 that we fetched above is
+the version 1 of the "miceprotein" dataset::
+
+  >>> mice.details['version']  #doctest: +SKIP
+  '1'
+
+In fact, this dataset only has one version. The iris dataset on the other hand
+has multiple versions::
+
+  >>> iris = fetch_openml(name="iris")
+  >>> iris.details['version']  #doctest: +SKIP
+  '1'
+  >>> iris.details['id']  #doctest: +SKIP
+  '61'
+
+  >>> iris_61 = fetch_openml(data_id=61)
+  >>> iris_61.details['version']
+  '1'
+  >>> iris_61.details['id']
+  '61'
+
+  >>> iris_969 = fetch_openml(data_id=969)
+  >>> iris_969.details['version']
+  '3'
+  >>> iris_969.details['id']
+  '969'
+
+Specifying the dataset by the name "iris" yields the lowest version, version 1,
+with the ``data_id`` 61. To make sure you always get this exact dataset, it is
+safest to specify it by the dataset ``data_id``. The other dataset, with
+``data_id`` 969, is version 3 (version 2 has become inactive), and contains a
+binarized version of the data::
+
+  >>> np.unique(iris_969.target)
+  array(['N', 'P'], dtype=object)
+
+You can also specify both the name and the version, which also uniquely
+identifies the dataset::
+
+  >>> iris_version_3 = fetch_openml(name="iris", version=3)
+  >>> iris_version_3.details['version']
+  '3'
+  >>> iris_version_3.details['id']
+  '969'
+
+
+.. topic:: References:
+
+ * Vanschoren, van Rijn, Bischl and Torgo
+   `"OpenML: networked science in machine learning"
+   <https://arxiv.org/pdf/1407.7722.pdf>`_,
+   ACM SIGKDD Explorations Newsletter, 15(2), 49-60, 2014.
diff --git a/doc/datasets/rcv1.rst b/doc/datasets/rcv1.rst
deleted file mode 100644
index afbe797cc0c0..000000000000
--- a/doc/datasets/rcv1.rst
+++ /dev/null
@@ -1,52 +0,0 @@
-
-.. _rcv1:
-
-RCV1 dataset
-------------
-
-Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories made available by Reuters, Ltd. for research purposes. The dataset is extensively described in [1]_.
-
-:func:`sklearn.datasets.fetch_rcv1` will load the following version: RCV1-v2, vectors, full sets, topics multilabels::
-
-    >>> from sklearn.datasets import fetch_rcv1
-    >>> rcv1 = fetch_rcv1()
-
-It returns a dictionary-like object, with the following attributes:
-
-``data``:
-The feature matrix is a scipy CSR sparse matrix, with 804414 samples and
-47236 features. Non-zero values contains cosine-normalized, log TF-IDF vectors.
-A nearly chronological split is proposed in [1]_: The first 23149 samples are the training set. The last 781265 samples are the testing set. This follows the official LYRL2004 chronological split.
-The array has 0.16% of non zero values::
-
-    >>> rcv1.data.shape
-    (804414, 47236)
-
-``target``:
-The target values are stored in a scipy CSR sparse matrix, with 804414 samples and 103 categories. Each sample has a value of 1 in its categories, and 0 in others. The array has 3.15% of non zero values::
-
-    >>> rcv1.target.shape
-    (804414, 103)
-
-``sample_id``:
-Each sample can be identified by its ID, ranging (with gaps) from 2286 to 810596::
-
-    >>> rcv1.sample_id[:3]
-    array([2286, 2287, 2288], dtype=uint32)
-
-``target_names``:
-The target values are the topics of each sample. Each sample belongs to at least one topic, and to up to 17 topics.
-There are 103 topics, each represented by a string. Their corpus frequencies span five orders of magnitude, from 5 occurrences for 'GMIL', to 381327 for 'CCAT'::
-
-    >>> rcv1.target_names[:3].tolist()  # doctest: +SKIP
-    ['E11', 'ECAT', 'M11']
-
-The dataset will be downloaded from the `rcv1 homepage`_ if necessary.
-The compressed size is about 656 MB.
-
-.. _rcv1 homepage: http://jmlr.csail.mit.edu/papers/volume5/lewis04a/
-
-
-.. topic:: References
-
-    .. [1] Lewis, D. D., Yang, Y., Rose, T. G., & Li, F. (2004). RCV1: A new benchmark collection for text categorization research. The Journal of Machine Learning Research, 5, 361-397.
diff --git a/doc/developers/advanced_installation.rst b/doc/developers/advanced_installation.rst
index 19b6c2e45557..720c11ed98f4 100644
--- a/doc/developers/advanced_installation.rst
+++ b/doc/developers/advanced_installation.rst
@@ -38,6 +38,12 @@ Scikit-learn requires:
 - NumPy (>= 1.8.2),
 - SciPy (>= 0.13.3).
 
+.. note::
+
+   For installing on PyPy, PyPy3-v5.10+, Numpy 1.14.0+, and scipy 1.1.0+
+   are required. For PyPy, only installation instructions with pip apply.
+
+
 Building Scikit-learn also requires
 
 - Cython >=0.23 
diff --git a/doc/developers/contributing.rst b/doc/developers/contributing.rst
index 8012b6eecb2b..31f5e5ef840a 100644
--- a/doc/developers/contributing.rst
+++ b/doc/developers/contributing.rst
@@ -41,7 +41,7 @@ ticket to the
 also welcome to post feature requests or pull requests.
 
 
-=======
+==================
 Ways to contribute
 ==================
 
@@ -79,6 +79,7 @@ link to it from your website, or simply star to say "I use it":
    * `joblib <https://github.com/joblib/joblib/issues>`__
    * `sphinx-gallery <https://github.com/sphinx-gallery/sphinx-gallery/issues>`__
    * `numpydoc <https://github.com/numpy/numpydoc/issues>`__
+   * `liac-arff <https://github.com/renatopp/liac-arff>`__
 
    and larger projects:
 
@@ -140,6 +141,14 @@ feedback:
   your **Python, scikit-learn, numpy, and scipy versions**. This information
   can be found by running the following code snippet::
 
+    >>> import sklearn
+    >>> sklearn.show_versions()  # doctest: +SKIP
+
+  .. note::
+
+    This utility function is only available in scikit-learn v0.20+.
+    For previous versions, one has to explicitly run::
+
      import platform; print(platform.platform())
      import sys; print("Python", sys.version)
      import numpy; print("NumPy", numpy.__version__)
@@ -352,7 +361,7 @@ and Cython optimizations.
 
    * Travis is used for testing on Linux platforms
    * Appveyor is used for testing on Windows platforms
-   * CircleCI is used to build the docs for viewing
+   * CircleCI is used to build the docs for viewing and for testing with PyPy on Linux
 
    Please note that if one of the following markers appear in the latest commit
    message, the following actions are taken.
@@ -698,7 +707,7 @@ Here's a simple example of code using some of the above guidelines::
 
         Parameters
         ----------
-        X : array-like, shape = (n_samples, n_features)
+        X : array-like, shape (n_samples, n_features)
             array representing the data
         random_state : RandomState or an int seed (0 by default)
             A random number generator instance to define the state of the
@@ -706,7 +715,7 @@ Here's a simple example of code using some of the above guidelines::
 
         Returns
         -------
-        x : numpy array, shape = (n_features,)
+        x : numpy array, shape (n_features,)
             A random point selected from X
         """
         X = check_array(X)
@@ -790,17 +799,35 @@ In the following example, k is deprecated and renamed to n_clusters::
 
     import warnings
 
-    def example_function(n_clusters=8, k=None):
-        if k is not None:
+    def example_function(n_clusters=8, k='not_used'):
+        if k != 'not_used':
             warnings.warn("'k' was renamed to n_clusters in version 0.13 and "
                           "will be removed in 0.15.", DeprecationWarning)
             n_clusters = k
 
+When the change is in a class, we validate and raise warning in ``fit``::
+
+  import warnings
+
+  class ExampleEstimator(BaseEstimator):
+      def __init__(self, n_clusters=8, k='not_used'):
+          self.n_clusters = n_clusters
+          self.k = k
+
+      def fit(self, X, y):
+          if k != 'not_used':
+              warnings.warn("'k' was renamed to n_clusters in version 0.13 and "
+                            "will be removed in 0.15.", DeprecationWarning)
+              self._n_clusters = k
+          else:
+              self._n_clusters = self.n_clusters
+
 As in these examples, the warning message should always give both the
 version in which the deprecation happened and the version in which the
 old behavior will be removed. If the deprecation happened in version
 0.x-dev, the message should say deprecation occurred in version 0.x and
-the removal will be in 0.(x+2). For example, if the deprecation happened
+the removal will be in 0.(x+2), so that users will have enough time to
+adapt their code to the new behaviour. For example, if the deprecation happened
 in version 0.18-dev, the message should say it happened in version 0.18
 and the old behavior will be removed in version 0.20.
 
@@ -812,6 +839,51 @@ same information as the deprecation warning as explained above. Use the
      ``k`` was renamed to ``n_clusters`` in version 0.13 and will be removed
      in 0.15.
 
+What's more, a deprecation requires a test which ensures that the warning is
+raised in relevant cases but not in other cases. The warning should be caught
+in all other tests (using e.g., ``@pytest.mark.filterwarnings``),
+and there should be no warning in the examples.
+
+
+Change the default value of a parameter
+---------------------------------------
+
+If the default value of a parameter needs to be changed, please replace the
+default value with a specific value (e.g., ``warn``) and raise
+``FutureWarning`` when users are using the default value. In the following
+example, we change the default value of ``n_clusters`` from 5 to 10
+(current version is 0.20)::
+
+    import warnings
+
+    def example_function(n_clusters='warn'):
+        if n_clusters == 'warn':
+            warnings.warn("The default value of n_clusters will change from "
+                          "5 to 10 in 0.22.", FutureWarning)
+            n_clusters = 5
+
+When the change is in a class, we validate and raise warning in ``fit``::
+
+  import warnings
+
+  class ExampleEstimator:
+      def __init__(self, n_clusters='warn'):
+          self.n_clusters = n_clusters
+
+      def fit(self, X, y):
+          if self.n_clusters == 'warn':
+            warnings.warn("The default value of n_clusters will change from "
+                          "5 to 10 in 0.22.", FutureWarning)
+            self._n_clusters = 5
+
+Similar to deprecations, the warning message should always give both the
+version in which the change happened and the version in which the old behavior
+will be removed. The docstring needs to be updated accordingly. We need a test
+which ensures that the warning is raised in relevant cases but not in other
+cases. The warning should be caught in all other tests
+(using e.g., ``@pytest.mark.filterwarnings``), and there should be no warning
+in the examples.
+
 
 .. currentmodule:: sklearn
 
@@ -916,41 +988,41 @@ multiple interfaces):
 
     The base object, implements a ``fit`` method to learn from data, either::
 
-      estimator = obj.fit(data, targets)
+      estimator = estimator.fit(data, targets)
 
     or::
 
-      estimator = obj.fit(data)
+      estimator = estimator.fit(data)
 
 :Predictor:
 
     For supervised learning, or some unsupervised problems, implements::
 
-      prediction = obj.predict(data)
+      prediction = predictor.predict(data)
 
     Classification algorithms usually also offer a way to quantify certainty
     of a prediction, either using ``decision_function`` or ``predict_proba``::
 
-      probability = obj.predict_proba(data)
+      probability = predictor.predict_proba(data)
 
 :Transformer:
 
     For filtering or modifying the data, in a supervised or unsupervised
     way, implements::
 
-      new_data = obj.transform(data)
+      new_data = transformer.transform(data)
 
     When fitting and transforming can be performed much more efficiently
     together than separately, implements::
 
-      new_data = obj.fit_transform(data)
+      new_data = transformer.fit_transform(data)
 
 :Model:
 
     A model that can give a `goodness of fit <https://en.wikipedia.org/wiki/Goodness_of_fit>`_
     measure or a likelihood of unseen data, implements (higher is better)::
 
-      score = obj.score(data)
+      score = model.score(data)
 
 Estimators
 ----------
@@ -1037,11 +1109,9 @@ the predict method.
 ============= ======================================================
 Parameters
 ============= ======================================================
-X             array-like, with shape = [N, D], where N is the number
-              of samples and D is the number of features.
+X             array-like, shape (n_samples, n_features)
 
-y             array, with shape = [N], where N is the number of
-              samples.
+y             array, shape (n_samples,)
 
 kwargs        optional data-dependent parameters.
 ============= ======================================================
diff --git a/doc/developers/tips.rst b/doc/developers/tips.rst
index 1388edd44196..9369b650fbc5 100644
--- a/doc/developers/tips.rst
+++ b/doc/developers/tips.rst
@@ -103,6 +103,11 @@ replies <https://github.com/settings/replies/>`_ for reviewing:
 ..
     Note that putting this content on a single line in a literal is the easiest way to make it copyable and wrapped on screen.
 
+Issue: Usage questions
+    ::
+
+        You're asking a usage question. The issue tracker is mainly for bugs and new features. For usage questions, it is recommended to try [Stack Overflow](https://stackoverflow.com/questions/tagged/scikit-learn) or [the Mailing List](https://mail.python.org/mailman/listinfo/scikit-learn).
+
 Issue: You're welcome to update the docs
     ::
 
@@ -116,15 +121,11 @@ Issue: Self-contained example for bug
 Issue: Software versions
     ::
 
-        To help diagnose your issue, could you please paste the output of:
+        To help diagnose your issue, please paste the output of:
         ```py
-        import platform; print(platform.platform())
-        import sys; print("Python", sys.version)
-        import numpy; print("NumPy", numpy.__version__)
-        import scipy; print("SciPy", scipy.__version__)
-        import sklearn; print("Scikit-Learn", sklearn.__version__)
+        import sklearn; sklearn.show_versions()
         ```
-        ? Thanks.
+        Thanks.
 
 Issue: Code blocks
     ::
diff --git a/doc/developers/utilities.rst b/doc/developers/utilities.rst
index b72b7c8e5c5d..e8e8a9723e07 100644
--- a/doc/developers/utilities.rst
+++ b/doc/developers/utilities.rst
@@ -45,7 +45,7 @@ should be used when applicable.
 
 - :func:`validation.check_memory` checks that input is ``joblib.Memory``-like,
   which means that it can be converted into a
-  ``sklearn.externals.joblib.Memory`` instance (typically a str denoting
+  ``sklearn.utils.Memory`` instance (typically a str denoting
   the ``cachedir``) or has the same interface.
 
 If your code relies on a random number generator, it should never use
diff --git a/doc/faq.rst b/doc/faq.rst
index 8d5a6f4f4dde..989b5b66437c 100644
--- a/doc/faq.rst
+++ b/doc/faq.rst
@@ -179,12 +179,10 @@ careful choice of algorithms.
 Do you support PyPy?
 --------------------
 
-In case you didn't know, `PyPy <http://pypy.org/>`_ is the new, fast,
-just-in-time compiling Python implementation. We don't support it.
-When the `NumPy support <http://buildbot.pypy.org/numpy-status/latest.html>`_
-in PyPy is complete or near-complete, and SciPy is ported over as well,
-we can start thinking of a port.
-We use too much of NumPy to work with a partial implementation.
+In case you didn't know, `PyPy <http://pypy.org/>`_ is an alternative
+Python implementation with a built-in just-in-time compiler. Experimental
+support for PyPy3-v5.10+ has been added, which requires Numpy 1.14.0+,
+and scipy 1.1.0+.
 
 How do I deal with string data (or trees, graphs...)?
 -----------------------------------------------------
@@ -362,10 +360,11 @@ of a single numeric dtype. These do not explicitly represent categorical
 variables at present. Thus, unlike R's data.frames or pandas.DataFrame, we
 require explicit conversion of categorical features to numeric values, as
 discussed in :ref:`preprocessing_categorical_features`.
-See also :ref:`sphx_glr_auto_examples_compose_column_transformer_mixed_types.py` for an
+See also :ref:`sphx_glr_auto_examples_compose_plot_column_transformer_mixed_types.py` for an
 example of working with heterogeneous (e.g. categorical and numeric) data.
 
 Why does Scikit-learn not directly work with, for example, pandas.DataFrame?
+----------------------------------------------------------------------------
 
 The homogeneous NumPy and SciPy data objects currently expected are most
 efficient to process for most operations. Extensive work would also be needed
diff --git a/doc/glossary.rst b/doc/glossary.rst
index adb47934f19a..4d93696c3ab3 100644
--- a/doc/glossary.rst
+++ b/doc/glossary.rst
@@ -202,7 +202,7 @@ General Concepts
         We use deprecation to slowly violate our :term:`backwards
         compatibility` assurances, usually to to:
 
-        * change the the default value of a parameter; or
+        * change the default value of a parameter; or
         * remove a parameter, attribute, method, class, etc.
 
         We will ordinarily issue a warning when a deprecated element is used,
@@ -289,6 +289,13 @@ General Concepts
           support for some feature, we use :term:`estimator tags` instead of
           duck typing.
 
+    early stopping
+        This consists in stopping an iterative optimization method before the
+        convergence of the training loss, to avoid over-fitting. This is
+        generally done by monitoring the generalization score on a validation
+        set. When available, it is activated through the parameter
+        ``early_stopping`` or by setting a positive :term:`n_iter_no_change`.
+
     estimator instance
         We sometimes use this terminology to distinguish an :term:`estimator`
         class from a constructed instance. For example, in the following,
@@ -1376,6 +1383,8 @@ functions or non-estimator constructors.
         equal weight by giving each sample a weight inversely related
         to its class's prevalence in the training data:
         ``n_samples / (n_classes * np.bincount(y))``.
+        **Note** however that this rebalancing does not take the weight of
+        samples in each class into account.
 
         For multioutput classification, a list of dicts is used to specify
         weights for each output. For example, for four-class multilabel
@@ -1407,7 +1416,8 @@ functions or non-estimator constructors.
         - An iterable yielding train/test splits.
 
         With some exceptions (especially where not using cross validation at
-        all is an option), the default is 3-fold.
+        all is an option), the default is 3-fold and will change to 5-fold
+        in version 0.22.
 
         ``cv`` values are validated and interpreted with :func:`utils.check_cv`.
 
@@ -1454,6 +1464,12 @@ functions or non-estimator constructors.
         input into. See :term:`components_` for the special case of affine
         projection.
 
+    ``n_iter_no_change``
+        Number of iterations with no improvement to wait before stopping the
+        iterative procedure. This is also known as a *patience* parameter. It
+        is typically used with :term:`early stopping` to avoid stopping too
+        early.
+
     ``n_jobs``
         This is used to specify how many concurrent processes/threads should be
         used for parallelized routines.  Scikit-learn uses one processor for
@@ -1463,8 +1479,15 @@ functions or non-estimator constructors.
 
         ``n_jobs`` is an int, specifying the maximum number of concurrently
         running jobs.  If set to -1, all CPUs are used. If 1 is given, no
-        parallel computing code is used at all.  For n_jobs below -1, (n_cpus +
-        1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used.
+        joblib level parallelism is used at all, which is useful for
+        debugging. Even with ``n_jobs = 1``, parallelism may occur due to
+        numerical processing libraries (see :ref:`FAQ <faq_mkl_threading>`).
+        For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for
+        ``n_jobs = -2``, all CPUs but one are used.
+
+        ``n_jobs=None`` means *unset*; it will generally be interpreted as
+        ``n_jobs=1``, unless the current :class:`joblib.Parallel` backend
+        context specifies otherwise.
 
         The use of ``n_jobs``-based parallelism in estimators varies:
 
@@ -1472,7 +1495,7 @@ functions or non-estimator constructors.
           sometimes parallelism happens in prediction (e.g. in random forests).
         * Some parallelism uses a multi-threading backend by default, some
           a multi-processing backend.  It is possible to override the default
-          backend by using :func:`sklearn.externals.joblib.parallel.parallel_backend`.
+          backend by using :func:`sklearn.utils.parallel_backend`.
         * Whether parallel processing is helpful at improving runtime depends
           on many factors, and it's usually a good idea to experiment rather
           than assuming that increasing the number of jobs is always a good
diff --git a/doc/index.rst b/doc/index.rst
index cfcaedc38017..0de085b1d721 100644
--- a/doc/index.rst
+++ b/doc/index.rst
@@ -207,17 +207,17 @@
                     <li><em>On-going development:</em>
                     <a href="/dev/whats_new.html"><em>What's new</em> (Changelog)</a>
                     </li>
-                    <li><em>October 2017.</em> scikit-learn 0.19.1 is available for download (<a href="whats_new.html#version-0-19">Changelog</a>).
+                    <li><strong>Scikit-learn 0.21 will drop support for Python 2.7 and Python 3.4.</strong>
                     </li>
-                    <li><em>July 2017.</em> scikit-learn 0.19.0 is available for download (<a href="whats_new/v0.19.html#version-0-19">Changelog</a>).
+                    <li><em>July 2018.</em> scikit-learn 0.20 is available for download (<a href="whats_new.html#version-0-20">Changelog</a>).
                     </li>
-                    <li><em>June 2017.</em> scikit-learn 0.18.2 is available for download (<a href="whats_new/v0.18.html#version-0-18-2">Changelog</a>).
+                    <li><em>July 2018.</em> scikit-learn 0.19.2 is available for download (<a href="whats_new.html#version-0-19">Changelog</a>).
                     </li>
-                    <li><em>September 2016.</em> scikit-learn 0.18.0 is available for download (<a href="whats_new/v0.18.html#version-0-18">Changelog</a>).
+                    <li><em>October 2017.</em> scikit-learn 0.19.1 is available for download (<a href="whats_new.html#version-0-19">Changelog</a>).
                     </li>
-                    <li><em>November 2015.</em> scikit-learn 0.17.0 is available for download (<a href="whats_new/v0.17.html">Changelog</a>).
+                    <li><em>July 2017.</em> scikit-learn 0.19.0 is available for download (<a href="whats_new/v0.19.html#version-0-19">Changelog</a>).
                     </li>
-                    <li><em>March 2015.</em> scikit-learn 0.16.0 is available for download (<a href="whats_new/v0.16.html">Changelog</a>).
+                    <li><em>June 2017.</em> scikit-learn 0.18.2 is available for download (<a href="whats_new/v0.18.html#version-0-18-2">Changelog</a>).
                     </li>
                     </ul>
                 </div>
diff --git a/doc/install.rst b/doc/install.rst
index 20a409a6872d..7dbb2287c406 100644
--- a/doc/install.rst
+++ b/doc/install.rst
@@ -21,6 +21,12 @@ Scikit-learn requires:
 - NumPy (>= 1.8.2),
 - SciPy (>= 0.13.3).
 
+
+.. warning::
+
+    Scikit-learn 0.20 is the last version to support Python 2.7 and Python 3.4.
+    Scikit-learn 0.21 will require Python 3.5 or newer.
+
 If you already have a working installation of numpy and scipy,
 the easiest way to install scikit-learn is using ``pip`` ::
 
@@ -46,6 +52,12 @@ it as ``scikit-learn[alldeps]``. The most common use case for this is in a
 application or a Docker image. This option is not intended for manual
 installation from the command line.
 
+.. note::
+
+   For installing on PyPy, PyPy3-v5.10+, Numpy 1.14.0+, and scipy 1.1.0+
+   are required.
+
+
 For installation instructions for more distributions see
 :ref:`other distributions <install_by_distribution>`.
 For compiling the development version from source, or building the package
diff --git a/doc/modules/classes.rst b/doc/modules/classes.rst
index df46579f7fd8..57ccfb5cff70 100644
--- a/doc/modules/classes.rst
+++ b/doc/modules/classes.rst
@@ -48,6 +48,7 @@ Functions
    config_context
    get_config
    set_config
+   show_versions
 
 .. _calibration_ref:
 
@@ -98,6 +99,7 @@ Classes
    cluster.AgglomerativeClustering
    cluster.Birch
    cluster.DBSCAN
+   cluster.OPTICS
    cluster.FeatureAgglomeration
    cluster.KMeans
    cluster.MiniBatchKMeans
@@ -112,6 +114,7 @@ Functions
 
    cluster.affinity_propagation
    cluster.dbscan
+   cluster.optics
    cluster.estimate_bandwidth
    cluster.k_means
    cluster.mean_shift
@@ -254,8 +257,8 @@ Loaders
    datasets.fetch_kddcup99
    datasets.fetch_lfw_pairs
    datasets.fetch_lfw_people
-   datasets.fetch_mldata
    datasets.fetch_olivetti_faces
+   datasets.fetch_openml
    datasets.fetch_rcv1
    datasets.fetch_species_distributions
    datasets.get_data_home
@@ -653,7 +656,7 @@ Kernels:
    :template: class.rst
 
    impute.SimpleImputer
-   impute.ChainedImputer
+   impute.MissingIndicator
 
 .. _kernel_approximation_ref:
 
@@ -1240,6 +1243,7 @@ Model validation
 
    preprocessing.Binarizer
    preprocessing.FunctionTransformer
+   preprocessing.KBinsDiscretizer
    preprocessing.KernelCenterer
    preprocessing.LabelBinarizer
    preprocessing.LabelEncoder
@@ -1471,6 +1475,22 @@ Low-level methods
    utils.testing.assert_raise_message
    utils.testing.all_estimators
 
+Utilities from joblib:
+
+.. autosummary::
+   :toctree: generated/
+   :template: class.rst
+
+   utils.Memory
+   utils.Parallel
+
+.. autosummary::
+   :toctree: generated/
+   :template: function.rst
+
+   utils.cpu_count
+   utils.delayed
+   utils.parallel_backend
 
 Recently deprecated
 ===================
@@ -1492,6 +1512,7 @@ To be removed in 0.22
    :template: deprecated_function.rst
 
    covariance.graph_lasso
+   datasets.fetch_mldata
 
 
 To be removed in 0.21
diff --git a/doc/modules/clustering.rst b/doc/modules/clustering.rst
index 21c342d3ff1a..968a66e67fdc 100644
--- a/doc/modules/clustering.rst
+++ b/doc/modules/clustering.rst
@@ -91,6 +91,12 @@ Overview of clustering methods
      - Non-flat geometry, uneven cluster sizes
      - Distances between nearest points
 
+   * - :ref:`OPTICS <optics>`
+     - minimum cluster membership
+     - Very large ``n_samples``, large ``n_clusters``
+     - Non-flat geometry, uneven cluster sizes, variable cluster density 
+     - Distances between points
+   
    * - :ref:`Gaussian mixtures <mixture>`
      - many
      - Not scalable
@@ -796,6 +802,11 @@ by black points below.
     be used (e.g. with sparse matrices). This matrix will consume n^2 floats.
     A couple of mechanisms for getting around this are:
 
+    - Use :ref:`OPTICS <optics>` clustering in conjunction with the
+      `extract_dbscan` method. OPTICS clustering also calculates the full
+      pairwise matrix, but only keeps one row in memory at a time (memory
+      complexity n).
+
     - A sparse radius neighborhood graph (where missing entries are presumed to
       be out of eps) can be precomputed in a memory-efficient way and dbscan
       can be run over this with ``metric='precomputed'``.  See
@@ -814,6 +825,106 @@ by black points below.
    In Proceedings of the 2nd International Conference on Knowledge Discovery
    and Data Mining, Portland, OR, AAAI Press, pp. 226–231. 1996
 
+.. _optics:
+
+OPTICS
+======
+
+The :class:`OPTICS` algorithm shares many similarities with the
+:class:`DBSCAN` algorithm, and can be considered a generalization of
+DBSCAN that relaxes the ``eps`` requirement from a single value to a value
+range. The key difference between DBSCAN and OPTICS is that the OPTICS
+algorithm builds a *reachability* graph, which assigns each sample both a
+``reachability_`` distance, and a spot within the cluster ``ordering_``
+attribute; these two attributes are assigned when the model is fitted, and are
+used to determine cluster membership. If OPTICS is run with the default value
+of *inf* set for ``max_bound``, then DBSCAN style cluster extraction can be
+performed in linear time for any given ``eps`` value using the
+``extract_dbscan`` method. Setting ``max_bound`` to a lower value will result
+in shorter run times, and can be thought of as the maximum cluster object size
+(in diameter) that OPTICS will be able to extract.
+
+.. |optics_results| image:: ../auto_examples/cluster/images/sphx_glr_plot_optics_001.png
+        :target: ../auto_examples/cluster/plot_optics.html
+        :scale: 50
+
+.. centered:: |optics_results|
+
+The *reachability* distances generated by OPTICS allow for variable density
+extraction of clusters within a single data set. As shown in the above plot,
+combining *reachability* distances and data set ``ordering_`` produces a
+*reachability plot*, where point density is represented on the Y-axis, and
+points are ordered such that nearby points are adjacent. 'Cutting' the
+reachability plot at a single value produces DBSCAN like results; all points
+above the 'cut' are classified as noise, and each time that there is a break
+when reading from left to right signifies a new cluster. The default cluster
+extraction with OPTICS looks at changes in slope within the graph to guess at
+natural clusters. There are also other possibilities for analysis on the graph
+itself, such as generating hierarchical representations of the data through
+reachability-plot dendrograms. The plot above has been color-coded so that
+cluster colors in planar space match the linear segment clusters of the
+reachability plot-- note that the blue and red clusters are adjacent in the
+reachability plot, and can be hierarchically represented as children of a
+larger parent cluster.
+
+.. topic:: Examples:
+
+     * :ref:`sphx_glr_auto_examples_cluster_plot_optics.py`
+
+
+.. topic:: Comparison with DBSCAN
+    
+    The results from OPTICS ``extract_dbscan`` method and DBSCAN are not quite
+    identical. Specifically, while *core_samples* returned from both OPTICS
+    and DBSCAN are guaranteed to be identical, labeling of periphery and noise
+    points is not. This is in part because the first sample processed by
+    OPTICS will always have a reachability distance that is set to ``inf``,
+    and will thus generally be marked as noise rather than periphery. This
+    affects adjacent points when they are considered as candidates for being
+    marked as either periphery or noise. While this effect is quite local to
+    the starting point of the dataset and is unlikely to be noticed on even
+    moderately large datasets, it is worth also noting that non-core boundry
+    points may switch cluster labels on the rare occasion that they are
+    equidistant to a competeing cluster due to how the graph is read from left
+    to right when assigning labels. 
+
+    Note that for any single value of ``eps``, DBSCAN will tend to have a
+    shorter run time than OPTICS; however, for repeated runs at varying ``eps``
+    values, a single run of OPTICS may require less cumulative runtime than
+    DBSCAN. It is also important to note that OPTICS output can be unstable at
+    ``eps`` values very close to the initial ``max_bound`` value. OPTICS seems
+    to produce near identical results to DBSCAN provided that ``eps`` passed to
+    ``extract_dbscan`` is a half order of magnitude less than the inital
+    ``max_bound`` that was used to fit; using a value close to ``max_bound``
+    will throw a warning, and using a value larger will result in an exception. 
+
+.. topic:: Computational Complexity
+
+    Spatial indexing trees are used to avoid calculating the full distance
+    matrix, and allow for efficient memory usage on large sets of samples.
+    Different distance metrics can be supplied via the ``metric`` keyword.
+    
+    For large datasets, similar (but not identical) results can be obtained via
+    `HDBSCAN <https://hdbscan.readthedocs.io>`_. The HDBSCAN implementation is
+    multithreaded, and has better algorithmic runtime complexity than OPTICS--
+    at the cost of worse memory scaling. For extremely large datasets that
+    exhaust system memory using HDBSCAN, OPTICS will maintain *n* (as opposed
+    to *n^2* memory scaling); however, tuning of the ``max_bound`` parameter
+    will likely need to be used to give a solution in a reasonable amount of
+    wall time.
+
+.. topic:: References:
+
+ *  "OPTICS: ordering points to identify the clustering structure."
+    Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel, and Jörg Sander.
+    In ACM Sigmod Record, vol. 28, no. 2, pp. 49-60. ACM, 1999.
+
+ *  "Automatic extraction of clusters from hierarchical clustering
+    representations."
+    Sander, Jörg, Xuejie Qin, Zhiyong Lu, Nan Niu, and Alex Kovarsky.
+    In Advances in knowledge discovery and data mining,
+    pp. 75-87. Springer Berlin Heidelberg, 2003. 
+
 .. _birch:
 
 Birch
@@ -1048,50 +1159,50 @@ Given the knowledge of the ground truth class assignments ``labels_true`` and
 our clustering algorithm assignments of the same samples ``labels_pred``, the
 **Mutual Information** is a function that measures the **agreement** of the two
 assignments, ignoring permutations.  Two different normalized versions of this
-measure are available, **Normalized Mutual Information(NMI)** and **Adjusted
-Mutual Information(AMI)**. NMI is often used in the literature while AMI was
+measure are available, **Normalized Mutual Information (NMI)** and **Adjusted
+Mutual Information (AMI)**. NMI is often used in the literature, while AMI was
 proposed more recently and is **normalized against chance**::
 
   >>> from sklearn import metrics
   >>> labels_true = [0, 0, 0, 1, 1, 1]
   >>> labels_pred = [0, 0, 1, 1, 2, 2]
 
-  >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +ELLIPSIS
+  >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP
   0.22504...
 
 One can permute 0 and 1 in the predicted labels, rename 2 to 3 and get
 the same score::
 
   >>> labels_pred = [1, 1, 0, 0, 3, 3]
-  >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +ELLIPSIS
+  >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP
   0.22504...
 
 All, :func:`mutual_info_score`, :func:`adjusted_mutual_info_score` and
 :func:`normalized_mutual_info_score` are symmetric: swapping the argument does
 not change the score. Thus they can be used as a **consensus measure**::
 
-  >>> metrics.adjusted_mutual_info_score(labels_pred, labels_true)  # doctest: +ELLIPSIS
+  >>> metrics.adjusted_mutual_info_score(labels_pred, labels_true)  # doctest: +SKIP
   0.22504...
 
 Perfect labeling is scored 1.0::
 
   >>> labels_pred = labels_true[:]
-  >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)
+  >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP
   1.0
 
-  >>> metrics.normalized_mutual_info_score(labels_true, labels_pred)
+  >>> metrics.normalized_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP
   1.0
 
 This is not true for ``mutual_info_score``, which is therefore harder to judge::
 
-  >>> metrics.mutual_info_score(labels_true, labels_pred)  # doctest: +ELLIPSIS
+  >>> metrics.mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP
   0.69...
 
 Bad (e.g. independent labelings) have non-positive scores::
 
   >>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]
   >>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]
-  >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +ELLIPSIS
+  >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP
   -0.10526...
 
 
@@ -1102,17 +1213,11 @@ Advantages
   for any value of ``n_clusters`` and ``n_samples`` (which is not the
   case for raw Mutual Information or the V-measure for instance).
 
-- **Bounded range [0, 1]**:  Values close to zero indicate two label
+- **Upper bound  of 1**:  Values close to zero indicate two label
   assignments that are largely independent, while values close to one
-  indicate significant agreement. Further, values of exactly 0 indicate
-  **purely** independent label assignments and a AMI of exactly 1 indicates
+  indicate significant agreement. Further, an AMI of exactly 1 indicates
   that the two label assignments are equal (with or without permutation).
 
-- **No assumption is made on the cluster structure**: can be used
-  to compare clustering algorithms such as k-means which assumes isotropic
-  blob shapes with results of spectral clustering algorithms which can
-  find cluster with "folded" shapes.
-
 
 Drawbacks
 ~~~~~~~~~
@@ -1164,7 +1269,7 @@ It also can be expressed in set cardinality formulation:
 
 The normalized mutual information is defined as
 
-.. math:: \text{NMI}(U, V) = \frac{\text{MI}(U, V)}{\sqrt{H(U)H(V)}}
+.. math:: \text{NMI}(U, V) = \frac{\text{MI}(U, V)}{\text{mean}(H(U), H(V))}
 
 This value of the mutual information and also the normalized variant is not
 adjusted for chance and will tend to increase as the number of different labels
@@ -1172,12 +1277,12 @@ adjusted for chance and will tend to increase as the number of different labels
 between the label assignments.
 
 The expected value for the mutual information can be calculated using the
-following equation, from Vinh, Epps, and Bailey, (2009). In this equation,
+following equation [VEB2009]_. In this equation,
 :math:`a_i = |U_i|` (the number of elements in :math:`U_i`) and
 :math:`b_j = |V_j|` (the number of elements in :math:`V_j`).
 
 
-.. math:: E[\text{MI}(U,V)]=\sum_{i=1}^|U| \sum_{j=1}^|V| \sum_{n_{ij}=(a_i+b_j-N)^+
+.. math:: E[\text{MI}(U,V)]=\sum_{i=1}^{|U|} \sum_{j=1}^{|V|} \sum_{n_{ij}=(a_i+b_j-N)^+
    }^{\min(a_i, b_j)} \frac{n_{ij}}{N}\log \left( \frac{ N.n_{ij}}{a_i b_j}\right)
    \frac{a_i!b_j!(N-a_i)!(N-b_j)!}{N!n_{ij}!(a_i-n_{ij})!(b_j-n_{ij})!
    (N-a_i-b_j+n_{ij})!}
@@ -1185,7 +1290,19 @@ following equation, from Vinh, Epps, and Bailey, (2009). In this equation,
 Using the expected value, the adjusted mutual information can then be
 calculated using a similar form to that of the adjusted Rand index:
 
-.. math:: \text{AMI} = \frac{\text{MI} - E[\text{MI}]}{\max(H(U), H(V)) - E[\text{MI}]}
+.. math:: \text{AMI} = \frac{\text{MI} - E[\text{MI}]}{\text{mean}(H(U), H(V)) - E[\text{MI}]}
+
+For normalized mutual information and adjusted mutual information, the normalizing
+value is typically some *generalized* mean of the entropies of each clustering.
+Various generalized means exist, and no firm rules exist for preferring one over the
+others.  The decision is largely a field-by-field basis; for instance, in community
+detection, the arithmetic mean is most common. Each
+normalizing method provides "qualitatively similar behaviours" [YAT2016]_. In our
+implementation, this is controlled by the ``average_method`` parameter.
+
+Vinh et al. (2010) named variants of NMI and AMI by their averaging method [VEB2010]_. Their
+'sqrt' and 'sum' averages are the geometric and arithmetic means; we use these
+more broadly common names.
 
 .. topic:: References
 
@@ -1194,22 +1311,29 @@ calculated using a similar form to that of the adjusted Rand index:
    Machine Learning Research 3: 583–617.
    `doi:10.1162/153244303321897735 <http://strehl.com/download/strehl-jmlr02.pdf>`_.
 
- * Vinh, Epps, and Bailey, (2009). "Information theoretic measures
+ * [VEB2009] Vinh, Epps, and Bailey, (2009). "Information theoretic measures
    for clusterings comparison". Proceedings of the 26th Annual International
    Conference on Machine Learning - ICML '09.
    `doi:10.1145/1553374.1553511 <https://dl.acm.org/citation.cfm?doid=1553374.1553511>`_.
    ISBN 9781605585161.
 
- * Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for
+ * [VEB2010] Vinh, Epps, and Bailey, (2010). "Information Theoretic Measures for
    Clusterings Comparison: Variants, Properties, Normalization and
-   Correction for Chance, JMLR
-   http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf
+   Correction for Chance". JMLR
+   <http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>
 
  * `Wikipedia entry for the (normalized) Mutual Information
    <https://en.wikipedia.org/wiki/Mutual_Information>`_
 
  * `Wikipedia entry for the Adjusted Mutual Information
    <https://en.wikipedia.org/wiki/Adjusted_Mutual_Information>`_
+   
+ * [YAT2016] Yang, Algesheimer, and Tessone, (2016). "A comparative analysis of
+   community
+   detection algorithms on artificial networks". Scientific Reports 6: 30750.
+   `doi:10.1038/srep30750 <https://www.nature.com/articles/srep30750>`_.
+   
+   
 
 .. _homogeneity_completeness:
 
@@ -1249,7 +1373,7 @@ Their harmonic mean called **V-measure** is computed by
   0.51...
 
 The V-measure is actually equivalent to the mutual information (NMI)
-discussed above normalized by the sum of the label entropies [B2011]_.
+discussed above, with the aggregation function being the arithmetic mean [B2011]_.
 
 Homogeneity, completeness and V-measure can be computed at once using
 :func:`homogeneity_completeness_v_measure` as follows::
@@ -1424,7 +1548,7 @@ Advantages
   for any value of ``n_clusters`` and ``n_samples`` (which is not the
   case for raw Mutual Information or the V-measure for instance).
 
-- **Bounded range [0, 1]**:  Values close to zero indicate two label
+- **Upper-bounded at 1**:  Values close to zero indicate two label
   assignments that are largely independent, while values close to one
   indicate significant agreement. Further, values of exactly 0 indicate
   **purely** independent label assignments and a AMI of exactly 1 indicates
diff --git a/doc/modules/compose.rst b/doc/modules/compose.rst
index ebb879182b23..5a291bfaebf1 100644
--- a/doc/modules/compose.rst
+++ b/doc/modules/compose.rst
@@ -342,7 +342,7 @@ and ``value`` is an estimator object::
     >>> estimators = [('linear_pca', PCA()), ('kernel_pca', KernelPCA())]
     >>> combined = FeatureUnion(estimators)
     >>> combined # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS
-    FeatureUnion(n_jobs=1,
+    FeatureUnion(n_jobs=None,
                  transformer_list=[('linear_pca', PCA(copy=True,...)),
                                    ('kernel_pca', KernelPCA(alpha=1.0,...))],
                  transformer_weights=None)
@@ -357,7 +357,7 @@ and ignored by setting to ``None``::
 
     >>> combined.set_params(kernel_pca=None)
     ... # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS
-    FeatureUnion(n_jobs=1,
+    FeatureUnion(n_jobs=None,
                  transformer_list=[('linear_pca', PCA(copy=True,...)),
                                    ('kernel_pca', None)],
                  transformer_weights=None)
@@ -413,7 +413,7 @@ variable, but apply a :class:`feature_extraction.text.CountVectorizer
 <sklearn.feature_extraction.text.CountVectorizer>` to the ``'title'`` column.
 As we might use multiple feature extraction methods on the same column, we give
 each transformer a unique name, say ``'city_category'`` and ``'title_bow'``.
-We can ignore the remaining rating columns by setting ``remainder='drop'``::
+By default, the remaining rating columns are ignored (``remainder='drop'``)::
 
   >>> from sklearn.compose import ColumnTransformer
   >>> from sklearn.feature_extraction.text import CountVectorizer
@@ -423,7 +423,8 @@ We can ignore the remaining rating columns by setting ``remainder='drop'``::
   ...      remainder='drop')
 
   >>> column_trans.fit(X) # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
-  ColumnTransformer(n_jobs=1, remainder='drop', transformer_weights=None,
+  ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,
+      transformer_weights=None,
       transformers=...)
 
   >>> column_trans.get_feature_names()
@@ -461,7 +462,7 @@ transformation::
   ...      ('title_bow', CountVectorizer(), 'title')],
   ...      remainder='passthrough')
 
-  >>> column_trans.fit_transform(X).toarray()
+  >>> column_trans.fit_transform(X)
   ... # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
   array([[1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 5, 4],
          [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 3, 5],
@@ -478,7 +479,7 @@ the transformation::
   ...      ('title_bow', CountVectorizer(), 'title')],
   ...      remainder=MinMaxScaler())
 
-  >>> column_trans.fit_transform(X)[:, -2:].toarray()
+  >>> column_trans.fit_transform(X)[:, -2:]
   ... # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
   array([[1. , 0.5],
          [0. , 1. ],
@@ -495,10 +496,11 @@ above example would be::
   ...     ('city', CountVectorizer(analyzer=lambda x: [x])),
   ...     ('title', CountVectorizer()))
   >>> column_trans # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
-  ColumnTransformer(n_jobs=1, remainder='passthrough', transformer_weights=None,
+  ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,
+           transformer_weights=None,
            transformers=[('countvectorizer-1', ...)
 
 .. topic:: Examples:
 
- * :ref:`sphx_glr_auto_examples_compose_column_transformer.py`
- * :ref:`sphx_glr_auto_examples_compose_column_transformer_mixed_types.py`
+ * :ref:`sphx_glr_auto_examples_compose_plot_column_transformer.py`
+ * :ref:`sphx_glr_auto_examples_compose_plot_column_transformer_mixed_types.py`
diff --git a/doc/modules/computing.rst b/doc/modules/computing.rst
index 347e1333630a..6c28bd338596 100644
--- a/doc/modules/computing.rst
+++ b/doc/modules/computing.rst
@@ -292,7 +292,7 @@ families of supervised models.
 
 For :mod:`sklearn.linear_model` (e.g. Lasso, ElasticNet,
 SGDClassifier/Regressor, Ridge & RidgeClassifier,
-PassiveAgressiveClassifier/Regressor, LinearSVC, LogisticRegression...) the
+PassiveAggressiveClassifier/Regressor, LinearSVC, LogisticRegression...) the
 decision function that is applied at prediction time is the same (a dot product)
 , so latency should be equivalent.
 
diff --git a/doc/modules/cross_validation.rst b/doc/modules/cross_validation.rst
index c6d301f8edb6..2d05e4b81c69 100644
--- a/doc/modules/cross_validation.rst
+++ b/doc/modules/cross_validation.rst
@@ -138,11 +138,9 @@ validation iterator instead, for instance::
 
   >>> from sklearn.model_selection import ShuffleSplit
   >>> n_samples = iris.data.shape[0]
-  >>> cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=0)
-  >>> cross_val_score(clf, iris.data, iris.target, cv=cv)
-  ...                                                     # doctest: +ELLIPSIS
-  array([0.97..., 0.97..., 1.        ])
-
+  >>> cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)
+  >>> cross_val_score(clf, iris.data, iris.target, cv=cv)  # doctest: +ELLIPSIS
+  array([0.977..., 0.977..., 1.  ..., 0.955..., 1.        ])
 
 .. topic:: Data transformation with held out data
 
@@ -168,7 +166,7 @@ validation iterator instead, for instance::
       >>> clf = make_pipeline(preprocessing.StandardScaler(), svm.SVC(C=1))
       >>> cross_val_score(clf, iris.data, iris.target, cv=cv)
       ...                                                 # doctest: +ELLIPSIS
-      array([0.97..., 0.93..., 0.95...])
+      array([0.977..., 0.933..., 0.955..., 0.933..., 0.977...])
 
     See :ref:`combining_estimators`.
 
@@ -230,7 +228,7 @@ Or as a dict mapping scorer name to a predefined or custom scoring function::
 Here is an example of ``cross_validate`` using a single metric::
 
     >>> scores = cross_validate(clf, iris.data, iris.target,
-    ...                         scoring='precision_macro',
+    ...                         scoring='precision_macro', cv=5,
     ...                         return_estimator=True)
     >>> sorted(scores.keys())
     ['estimator', 'fit_time', 'score_time', 'test_score', 'train_score']
@@ -325,6 +323,14 @@ Example of 2-fold cross-validation on a dataset with 4 samples::
   [2 3] [0 1]
   [0 1] [2 3]
 
+Here is a visualization of the cross-validation behavior. Note that
+:class:`KFold` is not affected by classes or groups.
+
+.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_004.png
+   :target: ../auto_examples/model_selection/plot_cv_indices.html
+   :align: center
+   :scale: 75%
+
 Each fold is constituted by two arrays: the first one is related to the
 *training set*, and the second one to the *test set*.
 Thus, one can create the training/test sets using numpy indexing::
@@ -462,15 +468,24 @@ generator.
 Here is a usage example::
 
   >>> from sklearn.model_selection import ShuffleSplit
-  >>> X = np.arange(5)
-  >>> ss = ShuffleSplit(n_splits=3, test_size=0.25,
+  >>> X = np.arange(10)
+  >>> ss = ShuffleSplit(n_splits=5, test_size=0.25,
   ...     random_state=0)
   >>> for train_index, test_index in ss.split(X):
   ...     print("%s %s" % (train_index, test_index))
-  ...
-  [1 3 4] [2 0]
-  [1 4 3] [0 2]
-  [4 0 2] [1 3]
+  [9 1 6 7 3 0 5] [2 8 4]
+  [2 9 8 0 6 7 4] [3 5 1]
+  [4 5 1 0 6 9 7] [2 3 8]
+  [2 7 5 8 0 3 4] [6 1 9]
+  [4 1 0 6 8 9 3] [5 2 7]
+
+Here is a visualization of the cross-validation behavior. Note that
+:class:`ShuffleSplit` is not affected by classes or groups.
+
+.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_006.png
+   :target: ../auto_examples/model_selection/plot_cv_indices.html
+   :align: center
+   :scale: 75%
 
 :class:`ShuffleSplit` is thus a good alternative to :class:`KFold` cross
 validation that allows a finer control on the number of iterations and
@@ -507,6 +522,13 @@ two slightly unbalanced classes::
   [0 1 3 4 5 8 9] [2 6 7]
   [0 1 2 4 5 6 7] [3 8 9]
 
+Here is a visualization of the cross-validation behavior.
+
+.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_007.png
+   :target: ../auto_examples/model_selection/plot_cv_indices.html
+   :align: center
+   :scale: 75%
+
 :class:`RepeatedStratifiedKFold` can be used to repeat Stratified K-Fold n times
 with different randomization in each repetition.
 
@@ -518,6 +540,13 @@ Stratified Shuffle Split
 stratified splits, *i.e* which creates splits by preserving the same
 percentage for each target class as in the complete set.
 
+Here is a visualization of the cross-validation behavior.
+
+.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_009.png
+   :target: ../auto_examples/model_selection/plot_cv_indices.html
+   :align: center
+   :scale: 75%
+
 .. _group_cv:
 
 Cross-validation iterators for grouped data.
@@ -570,6 +599,12 @@ Each subject is in a different testing fold, and the same subject is never in
 both testing and training. Notice that the folds do not have exactly the same
 size due to the imbalance in the data.
 
+Here is a visualization of the cross-validation behavior.
+
+.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_005.png
+   :target: ../auto_examples/model_selection/plot_cv_indices.html
+   :align: center
+   :scale: 75%
 
 Leave One Group Out
 ^^^^^^^^^^^^^^^^^^^
@@ -646,6 +681,13 @@ Here is a usage example::
   [2 3 4 5] [0 1 6 7]
   [4 5 6 7] [0 1 2 3]
 
+Here is a visualization of the cross-validation behavior.
+
+.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_008.png
+   :target: ../auto_examples/model_selection/plot_cv_indices.html
+   :align: center
+   :scale: 75%
+
 This class is useful when the behavior of :class:`LeavePGroupsOut` is
 desired, but the number of groups is large enough that generating all
 possible partitions with :math:`P` groups withheld would be prohibitively
@@ -710,6 +752,12 @@ Example of 3-split time series cross-validation on a dataset with 6 samples::
   [0 1 2 3] [4]
   [0 1 2 3 4] [5]
 
+Here is a visualization of the cross-validation behavior.
+
+.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_010.png
+   :target: ../auto_examples/model_selection/plot_cv_indices.html
+   :align: center
+   :scale: 75%
 
 A note on shuffling
 ===================
diff --git a/doc/modules/decomposition.rst b/doc/modules/decomposition.rst
index e011f2143053..608e7b7d0d90 100644
--- a/doc/modules/decomposition.rst
+++ b/doc/modules/decomposition.rst
@@ -417,10 +417,10 @@ Generic dictionary learning
 
 Dictionary learning (:class:`DictionaryLearning`) is a matrix factorization
 problem that amounts to finding a (usually overcomplete) dictionary that will
-perform good at sparsely encoding the fitted data.
+perform well at sparsely encoding the fitted data.
 
 Representing data as sparse combinations of atoms from an overcomplete
-dictionary is suggested to be the way the mammal primary visual cortex works.
+dictionary is suggested to be the way the mammalian primary visual cortex works.
 Consequently, dictionary learning applied on image patches has been shown to
 give good results in image processing tasks such as image completion,
 inpainting and denoising, as well as for supervised recognition tasks.
@@ -604,7 +604,7 @@ about these components (e.g. whether they are orthogonal):
 
 .. centered:: |pca_img3| |fa_img3|
 
-The main advantage for Factor Analysis (over :class:`PCA` is that
+The main advantage for Factor Analysis over :class:`PCA` is that
 it can model the variance in every direction of the input space independently
 (heteroscedastic noise):
 
diff --git a/doc/modules/ensemble.rst b/doc/modules/ensemble.rst
index 83da35eb46ca..57a50d325497 100644
--- a/doc/modules/ensemble.rst
+++ b/doc/modules/ensemble.rst
@@ -167,19 +167,19 @@ in bias::
 
     >>> clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,
     ...     random_state=0)
-    >>> scores = cross_val_score(clf, X, y)
-    >>> scores.mean()                             # doctest: +ELLIPSIS
-    0.97...
+    >>> scores = cross_val_score(clf, X, y, cv=5)
+    >>> scores.mean()                               # doctest: +ELLIPSIS
+    0.98...
 
     >>> clf = RandomForestClassifier(n_estimators=10, max_depth=None,
     ...     min_samples_split=2, random_state=0)
-    >>> scores = cross_val_score(clf, X, y)
-    >>> scores.mean()                             # doctest: +ELLIPSIS
+    >>> scores = cross_val_score(clf, X, y, cv=5)
+    >>> scores.mean()                               # doctest: +ELLIPSIS
     0.999...
 
     >>> clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,
     ...     min_samples_split=2, random_state=0)
-    >>> scores = cross_val_score(clf, X, y)
+    >>> scores = cross_val_score(clf, X, y, cv=5)
     >>> scores.mean() > 0.999
     True
 
@@ -257,14 +257,19 @@ Feature importance evaluation
 The relative rank (i.e. depth) of a feature used as a decision node in a
 tree can be used to assess the relative importance of that feature with
 respect to the predictability of the target variable. Features used at
-the top of the tree contribute to the final prediction decision of a 
-larger fraction of the input samples. The **expected fraction of the 
+the top of the tree contribute to the final prediction decision of a
+larger fraction of the input samples. The **expected fraction of the
 samples** they contribute to can thus be used as an estimate of the
-**relative importance of the features**.
+**relative importance of the features**. In scikit-learn, the fraction of
+samples a feature contributes to is combined with the decrease in impurity
+from splitting them to create a normalized estimate of the predictive power
+of that feature.
 
-By **averaging** those expected activity rates over several randomized
+By **averaging** the estimates of predictive ability over several randomized
 trees one can **reduce the variance** of such an estimate and use it
-for feature selection.
+for feature selection. This is known as the mean decrease in impurity, or MDI.
+Refer to [L2014]_ for more information on MDI and feature importance
+evaluation with Random Forests.
 
 The following example shows a color-coded representation of the relative
 importances of each individual pixel for a face recognition task using
@@ -288,6 +293,12 @@ to the prediction function.
 
 .. _random_trees_embedding:
 
+.. topic:: References
+
+ .. [L2014] G. Louppe,
+         "Understanding Random Forests: From Theory to Practice",
+         PhD Thesis, U. of Liege, 2014.
+
 Totally Random Trees Embedding
 ------------------------------
 
@@ -373,7 +384,7 @@ learners::
 
     >>> iris = load_iris()
     >>> clf = AdaBoostClassifier(n_estimators=100)
-    >>> scores = cross_val_score(clf, iris.data, iris.target)
+    >>> scores = cross_val_score(clf, iris.data, iris.target, cv=5)
     >>> scores.mean()                             # doctest: +ELLIPSIS
     0.9...
 
@@ -965,7 +976,7 @@ The following example shows how to fit the majority rule classifier::
    >>> X, y = iris.data[:, 1:3], iris.target
 
    >>> clf1 = LogisticRegression(random_state=1)
-   >>> clf2 = RandomForestClassifier(random_state=1)
+   >>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)
    >>> clf3 = GaussianNB()
 
    >>> eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')
@@ -974,7 +985,7 @@ The following example shows how to fit the majority rule classifier::
    ...     scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')
    ...     print("Accuracy: %0.2f (+/- %0.2f) [%s]" % (scores.mean(), scores.std(), label))
    Accuracy: 0.90 (+/- 0.05) [Logistic Regression]
-   Accuracy: 0.93 (+/- 0.05) [Random Forest]
+   Accuracy: 0.94 (+/- 0.04) [Random Forest]
    Accuracy: 0.91 (+/- 0.04) [naive Bayes]
    Accuracy: 0.95 (+/- 0.05) [Ensemble]
 
diff --git a/doc/modules/feature_extraction.rst b/doc/modules/feature_extraction.rst
index 611c7ecb60ee..97001f37fb04 100644
--- a/doc/modules/feature_extraction.rst
+++ b/doc/modules/feature_extraction.rst
@@ -380,6 +380,37 @@ last document::
   >>> X_2[:, feature_index]     # doctest: +ELLIPSIS
   array([0, 0, 0, 1]...)
 
+.. _stop_words:
+
+Using stop words
+................
+
+Stop words are words like "and", "the", "him", which are presumed to be
+uninformative in representing the content of a text, and which may be
+removed to avoid them being construed as signal for prediction.  Sometimes,
+however, similar words are useful for prediction, such as in classifying
+writing style or personality.
+
+There are several known issues in our provided 'english' stop word list. See
+[NQY18]_.
+
+Please take care in choosing a stop word list.
+Popular stop word lists may include words that are highly informative to
+some tasks, such as *computer*.
+
+You should also make sure that the stop word list has had the same
+preprocessing and tokenization applied as the one used in the vectorizer.
+The word *we've* is split into *we* and *ve* by CountVectorizer's default
+tokenizer, so if *we've* is in ``stop_words``, but *ve* is not, *ve* will
+be retained from *we've* in transformed text.  Our vectorizers will try to
+identify and warn about some kinds of inconsistencies.
+
+.. topic:: References
+
+    .. [NQY18] J. Nothman, H. Qin and R. Yurchak (2018).
+               `"Stop Word Lists in Free Open-source Software Packages"
+               <http://aclweb.org/anthology/W18-2502>`__.
+               In *Proc. Workshop for NLP Open Source Software*.
 
 .. _tfidf:
 
@@ -478,7 +509,7 @@ v{_2}^2 + \dots + v{_n}^2}}`
 For example, we can compute the tf-idf of the first term in the first
 document in the `counts` array as follows:
 
-:math:`n_{d, {\text{term1}}} = 6`
+:math:`n_{d} = 6`
 
 :math:`\text{df}(d, t)_{\text{term1}} = 6`
 
diff --git a/doc/modules/feature_selection.rst b/doc/modules/feature_selection.rst
index ae630af183cd..7c6df892e5be 100644
--- a/doc/modules/feature_selection.rst
+++ b/doc/modules/feature_selection.rst
@@ -245,7 +245,7 @@ meta-transformer)::
   >>> X, y = iris.data, iris.target
   >>> X.shape
   (150, 4)
-  >>> clf = ExtraTreesClassifier()
+  >>> clf = ExtraTreesClassifier(n_estimators=50)
   >>> clf = clf.fit(X, y)
   >>> clf.feature_importances_  # doctest: +SKIP
   array([ 0.04...,  0.05...,  0.4...,  0.4...])
diff --git a/doc/modules/impute.rst b/doc/modules/impute.rst
index 0f9089c98178..0fd119857177 100644
--- a/doc/modules/impute.rst
+++ b/doc/modules/impute.rst
@@ -16,22 +16,6 @@ values. However, this comes at the price of losing data which may be valuable
 i.e., to infer them from the known part of the data. See the :ref:`glossary`
 entry on imputation.
 
-
-Univariate vs. Multivariate Imputation
-======================================
-
-One type of imputation algorithm is univariate, which imputes values in the i-th
-feature dimension using only non-missing values in that feature dimension
-(e.g. :class:`impute.SimpleImputer`). By contrast, multivariate imputation
-algorithms use the entire set of available feature dimensions to estimate the
-missing values (e.g. :class:`impute.ChainedImputer`).
-
-
-.. _single_imputer:
-
-Univariate feature imputation
-=============================
-
 The :class:`SimpleImputer` class provides basic strategies for imputing missing
 values. Missing values can be imputed with a provided constant value, or using
 the statistics (mean, median or most frequent) of each column in which the
@@ -56,19 +40,19 @@ that contain the missing values::
 The :class:`SimpleImputer` class also supports sparse matrices::
 
     >>> import scipy.sparse as sp
-    >>> X = sp.csc_matrix([[1, 2], [0, 3], [7, 6]])
-    >>> imp = SimpleImputer(missing_values=0, strategy='mean')
+    >>> X = sp.csc_matrix([[1, 2], [0, -1], [8, 4]])
+    >>> imp = SimpleImputer(missing_values=-1, strategy='mean')
     >>> imp.fit(X)                  # doctest: +NORMALIZE_WHITESPACE
-    SimpleImputer(copy=True, fill_value=None, missing_values=0, strategy='mean', verbose=0)
-    >>> X_test = sp.csc_matrix([[0, 2], [6, 0], [7, 6]])
-    >>> print(imp.transform(X_test))      # doctest: +NORMALIZE_WHITESPACE  +ELLIPSIS
-    [[4.          2.        ]
-     [6.          3.666...]
-     [7.          6.        ]]
+    SimpleImputer(copy=True, fill_value=None, missing_values=-1, strategy='mean', verbose=0)
+    >>> X_test = sp.csc_matrix([[-1, 2], [6, -1], [7, 6]])
+    >>> print(imp.transform(X_test).toarray())      # doctest: +NORMALIZE_WHITESPACE
+    [[3. 2.]
+     [6. 3.]
+     [7. 6.]]
 
-Note that, here, missing values are encoded by 0 and are thus implicitly stored
-in the matrix. This format is thus suitable when there are many more missing
-values than observed values.
+Note that this format is not meant to be used to implicitly store missing values
+in the matrix because it would densify it at transform time. Missing values encoded
+by 0 must be used with dense input.
 
 The :class:`SimpleImputer` class also supports categorical data represented as
 string values or pandas categoricals when using the ``'most_frequent'`` or
@@ -87,58 +71,52 @@ string values or pandas categoricals when using the ``'most_frequent'`` or
      ['a' 'y']
      ['b' 'y']]
 
-.. _chained_imputer:
-
 
-Multivariate feature imputation
-===============================
-
-A more sophisticated approach is to use the :class:`ChainedImputer` class, which
-implements the imputation technique from MICE (Multivariate Imputation by
-Chained Equations). MICE models each feature with missing values as a function of
-other features, and uses that estimate for imputation. It does so in a round-robin
-fashion: at each step, a feature column is designated as output `y` and the other
-feature columns are treated as inputs `X`. A regressor is fit on `(X, y)` for known `y`.
-Then, the regressor is used to predict the unknown values of `y`. This is repeated
-for each feature in a chained fashion, and then is done for a number of imputation
-rounds. Here is an example snippet::
-
-    >>> import numpy as np
-    >>> from sklearn.impute import ChainedImputer
-    >>> imp = ChainedImputer(n_imputations=10, random_state=0)
-    >>> imp.fit([[1, 2], [np.nan, 3], [7, np.nan]])
-    ChainedImputer(imputation_order='ascending', initial_strategy='mean',
-            max_value=None, min_value=None, missing_values=nan, n_burn_in=10,
-            n_imputations=10, n_nearest_features=None, predictor=None,
-            random_state=0, verbose=False)
-    >>> X_test = [[np.nan, 2], [6, np.nan], [np.nan, 6]]
-    >>> print(np.round(imp.transform(X_test)))
-    [[ 1.  2.]
-     [ 6.  4.]
-     [13.  6.]]
-
-Both :class:`SimpleImputer` and :class:`ChainedImputer` can be used in a Pipeline
-as a way to build a composite estimator that supports imputation.
-See :ref:`sphx_glr_auto_examples_plot_missing_values.py`.
-
-
-.. _multiple_imputation:
-
-Multiple vs. Single Imputation
-==============================
-
-In the statistics community, it is common practice to perform multiple imputations,
-generating, for example, 10 separate imputations for a single feature matrix.
-Each of these 10 imputations is then put through the subsequent analysis pipeline
-(e.g. feature engineering, clustering, regression, classification). The 10 final
-analysis results (e.g. held-out validation error) allow the data scientist to
-obtain understanding of the uncertainty inherent in the missing values. The above
-practice is called multiple imputation. As implemented, the :class:`ChainedImputer`
-class generates a single (averaged) imputation for each missing value because this
-is the most common use case for machine learning applications. However, it can also be used
-for multiple imputations by applying it repeatedly to the same dataset with different
-random seeds with the ``n_imputations`` parameter set to 1.
-
-Note that a call to the ``transform`` method of :class:`ChainedImputer` is not
-allowed to change the number of samples. Therefore multiple imputations cannot be
-achieved by a single call to ``transform``.
+:class:`SimpleImputer` can be used in a Pipeline as a way to build a composite
+estimator that supports imputation. See :ref:`sphx_glr_auto_examples_plot_missing_values.py`.
+
+.. _missing_indicator:
+
+Marking imputed values
+======================
+
+The :class:`MissingIndicator` transformer is useful to transform a dataset into
+corresponding binary matrix indicating the presence of missing values in the
+dataset. This transformation is useful in conjunction with imputation. When
+using imputation, preserving the information about which values had been
+missing can be informative.
+
+``NaN`` is usually used as the placeholder for missing values. However, it
+enforces the data type to be float. The parameter ``missing_values`` allows to
+specify other placeholder such as integer. In the following example, we will
+use ``-1`` as missing values::
+
+  >>> from sklearn.impute import MissingIndicator
+  >>> X = np.array([[-1, -1, 1, 3],
+  ...               [4, -1, 0, -1],
+  ...               [8, -1, 1, 0]])
+  >>> indicator = MissingIndicator(missing_values=-1)
+  >>> mask_missing_values_only = indicator.fit_transform(X)
+  >>> mask_missing_values_only
+  array([[ True,  True, False],
+         [False,  True,  True],
+         [False,  True, False]])
+
+The ``features`` parameter is used to choose the features for which the mask is
+constructed. By default, it is ``'missing-only'`` which returns the imputer
+mask of the features containing missing values at ``fit`` time::
+
+  >>> indicator.features_
+  array([0, 1, 3])
+
+The ``features`` parameter can be set to ``'all'`` to returned all features
+whether or not they contain missing values::
+    
+  >>> indicator = MissingIndicator(missing_values=-1, features="all")
+  >>> mask_all = indicator.fit_transform(X)
+  >>> mask_all
+  array([[ True,  True, False, False],
+         [False,  True, False,  True],
+         [False,  True, False, False]])
+  >>> indicator.features_
+  array([0, 1, 2, 3])
diff --git a/doc/modules/kernel_approximation.rst b/doc/modules/kernel_approximation.rst
index fe920db11660..65a18bca9f11 100644
--- a/doc/modules/kernel_approximation.rst
+++ b/doc/modules/kernel_approximation.rst
@@ -59,13 +59,14 @@ a linear algorithm, for example a linear SVM::
     >>> y = [0, 0, 1, 1]
     >>> rbf_feature = RBFSampler(gamma=1, random_state=1)
     >>> X_features = rbf_feature.fit_transform(X)
-    >>> clf = SGDClassifier(max_iter=5)   # doctest: +NORMALIZE_WHITESPACE
-    >>> clf.fit(X_features, y)
-    SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,
-           eta0=0.0, fit_intercept=True, l1_ratio=0.15,
-           learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,
-           n_jobs=1, penalty='l2', power_t=0.5, random_state=None,
-           shuffle=True, tol=None, verbose=0, warm_start=False)
+    >>> clf = SGDClassifier(max_iter=5)
+    >>> clf.fit(X_features, y)   # doctest: +NORMALIZE_WHITESPACE
+    SGDClassifier(alpha=0.0001, average=False, class_weight=None,
+           early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,
+           l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,
+           n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',
+           power_t=0.5, random_state=None, shuffle=True, tol=None,
+           validation_fraction=0.1, verbose=0, warm_start=False)
     >>> clf.score(X_features, y)
     1.0
 
diff --git a/doc/modules/label_propagation.rst b/doc/modules/label_propagation.rst
index 1aba742723f0..5737368b868a 100644
--- a/doc/modules/label_propagation.rst
+++ b/doc/modules/label_propagation.rst
@@ -86,6 +86,7 @@ which can drastically reduce running times.
 
   * :ref:`sphx_glr_auto_examples_semi_supervised_plot_label_propagation_versus_svm_iris.py`
   * :ref:`sphx_glr_auto_examples_semi_supervised_plot_label_propagation_structure.py`
+  * :ref:`sphx_glr_auto_examples_semi_supervised_plot_label_propagation_digits.py`
   * :ref:`sphx_glr_auto_examples_semi_supervised_plot_label_propagation_digits_active_learning.py`
 
 .. topic:: References
diff --git a/doc/modules/learning_curve.rst b/doc/modules/learning_curve.rst
index 6ae5ac4a9b53..8656ee0c90a0 100644
--- a/doc/modules/learning_curve.rst
+++ b/doc/modules/learning_curve.rst
@@ -81,15 +81,16 @@ The function :func:`validation_curve` can help in this case::
   >>> X, y = X[indices], y[indices]
 
   >>> train_scores, valid_scores = validation_curve(Ridge(), X, y, "alpha",
-  ...                                               np.logspace(-7, 3, 3))
-  >>> train_scores           # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE
-  array([[0.94..., 0.92..., 0.92...],
-         [0.94..., 0.92..., 0.92...],
-         [0.47..., 0.45..., 0.42...]])
+  ...                                               np.logspace(-7, 3, 3),
+  ...                                               cv=5)
+  >>> train_scores            # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE
+  array([[0.93..., 0.94..., 0.92..., 0.91..., 0.92...],
+         [0.93..., 0.94..., 0.92..., 0.91..., 0.92...],
+         [0.51..., 0.52..., 0.49..., 0.47..., 0.49...]])
   >>> valid_scores           # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE
-  array([[0.90..., 0.92..., 0.94...],
-         [0.90..., 0.92..., 0.94...],
-         [0.44..., 0.39..., 0.45...]])
+  array([[0.90..., 0.84..., 0.94..., 0.96..., 0.93...],
+         [0.90..., 0.84..., 0.94..., 0.96..., 0.93...],
+         [0.46..., 0.25..., 0.50..., 0.49..., 0.52...]])
 
 If the training score and the validation score are both low, the estimator will
 be underfitting. If the training score is high and the validation score is low,
diff --git a/doc/modules/linear_model.rst b/doc/modules/linear_model.rst
index bb4a9e4e57f3..968cc663f943 100644
--- a/doc/modules/linear_model.rst
+++ b/doc/modules/linear_model.rst
@@ -45,7 +45,9 @@ and will store the coefficients :math:`w` of the linear model in its
     >>> from sklearn import linear_model
     >>> reg = linear_model.LinearRegression()
     >>> reg.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
-    LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
+    ...                                       # doctest: +NORMALIZE_WHITESPACE
+    LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,
+                     normalize=False)
     >>> reg.coef_
     array([0.5, 0.5])
 
@@ -138,9 +140,9 @@ as GridSearchCV except that it defaults to Generalized Cross-Validation
 (GCV), an efficient form of leave-one-out cross-validation::
 
     >>> from sklearn import linear_model
-    >>> reg = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])
+    >>> reg = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0], cv=3)
     >>> reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])       # doctest: +SKIP
-    RidgeCV(alphas=[0.1, 1.0, 10.0], cv=None, fit_intercept=True, scoring=None,
+    RidgeCV(alphas=[0.1, 1.0, 10.0], cv=3, fit_intercept=True, scoring=None,
         normalize=False)
     >>> reg.alpha_                                      # doctest: +SKIP
     0.1
diff --git a/doc/modules/model_evaluation.rst b/doc/modules/model_evaluation.rst
index eeb058e1440c..89bc3bb4a84e 100644
--- a/doc/modules/model_evaluation.rst
+++ b/doc/modules/model_evaluation.rst
@@ -98,13 +98,14 @@ Usage examples:
     >>> from sklearn.model_selection import cross_val_score
     >>> iris = datasets.load_iris()
     >>> X, y = iris.data, iris.target
-    >>> clf = svm.SVC(gamma='scale', probability=True, random_state=0)
-    >>> cross_val_score(clf, X, y, scoring='neg_log_loss') # doctest: +ELLIPSIS
-    array([-0.10..., -0.16..., -0.07...])
+    >>> clf = svm.SVC(gamma='scale', random_state=0)
+    >>> cross_val_score(clf, X, y, scoring='recall_macro',
+    ...                 cv=5)  # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE
+    array([0.96..., 1.  ..., 0.96..., 0.96..., 1.        ])
     >>> model = svm.SVC()
-    >>> cross_val_score(model, X, y, scoring='wrong_choice')
+    >>> cross_val_score(model, X, y, cv=5, scoring='wrong_choice')
     Traceback (most recent call last):
-    ValueError: 'wrong_choice' is not a valid scoring value. Valid options are ['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'balanced_accuracy', 'brier_score_loss', 'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'mutual_info_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'v_measure_score']
+    ValueError: 'wrong_choice' is not a valid scoring value. Use sorted(sklearn.metrics.SCORERS.keys()) to get valid options.
 
 .. note::
 
@@ -150,7 +151,8 @@ the :func:`fbeta_score` function::
     >>> ftwo_scorer = make_scorer(fbeta_score, beta=2)
     >>> from sklearn.model_selection import GridSearchCV
     >>> from sklearn.svm import LinearSVC
-    >>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]}, scoring=ftwo_scorer)
+    >>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},
+    ...                     scoring=ftwo_scorer, cv=5)
 
 The second use case is to build a completely custom scorer object
 from a simple python function using :func:`make_scorer`, which can
@@ -176,7 +178,7 @@ Here is an example of building custom scorers, and of using the
     >>> import numpy as np
     >>> def my_custom_loss_func(y_true, y_pred):
     ...     diff = np.abs(y_true - y_pred).max()
-    ...     return np.log(1 + diff)
+    ...     return np.log1p(diff)
     ...
     >>> # score will negate the return value of my_custom_loss_func,
     >>> # which will be np.log(2), 0.693, given the values for X
@@ -250,13 +252,14 @@ permitted and will require a wrapper to return a single metric::
     >>> def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 1]
     >>> scoring = {'tp' : make_scorer(tp), 'tn' : make_scorer(tn),
     ...            'fp' : make_scorer(fp), 'fn' : make_scorer(fn)}
-    >>> cv_results = cross_validate(svm.fit(X, y), X, y, scoring=scoring)
+    >>> cv_results = cross_validate(svm.fit(X, y), X, y,
+    ...                             scoring=scoring, cv=5)
     >>> # Getting the test set true positive scores
-    >>> print(cv_results['test_tp'])          # doctest: +NORMALIZE_WHITESPACE
-    [16 14  9]
+    >>> print(cv_results['test_tp'])  # doctest: +NORMALIZE_WHITESPACE
+    [10  9  8  7  8]
     >>> # Getting the test set false negative scores
-    >>> print(cv_results['test_fn'])          # doctest: +NORMALIZE_WHITESPACE
-    [1 3 7]
+    >>> print(cv_results['test_fn'])  # doctest: +NORMALIZE_WHITESPACE
+    [0 1 2 3 2]
 
 .. _classification_metrics:
 
@@ -417,66 +420,68 @@ In the multilabel case with binary label indicators: ::
 Balanced accuracy score
 -----------------------
 
-The :func:`balanced_accuracy_score` function computes the
-`balanced accuracy <https://en.wikipedia.org/wiki/Accuracy_and_precision>`_, which
-avoids inflated performance estimates on imbalanced datasets. It is defined as the
-arithmetic mean of `sensitivity <https://en.wikipedia.org/wiki/Sensitivity_and_specificity>`_
-(true positive rate) and `specificity <https://en.wikipedia.org/wiki/Sensitivity_and_specificity>`_
-(true negative rate), or the average of `recall scores <https://en.wikipedia.org/wiki/Precision_and_recall>`_
-obtained on either class.
+The :func:`balanced_accuracy_score` function computes the `balanced accuracy
+<https://en.wikipedia.org/wiki/Accuracy_and_precision>`_, which avoids inflated
+performance estimates on imbalanced datasets. It is the macro-average of recall
+scores per class or, equivalently, raw accuracy where each sample is weighted
+according to the inverse prevalence of its true class.
+Thus for balanced datasets, the score is equal to accuracy.
 
-If the classifier performs equally well on either class, this term reduces to the
-conventional accuracy (i.e., the number of correct predictions divided by the total
-number of predictions). In contrast, if the conventional accuracy is above chance only
-because the classifier takes advantage of an imbalanced test set, then the balanced
-accuracy, as appropriate, will drop to 50%.
+In the binary case, balanced accuracy is equal to the arithmetic mean of
+`sensitivity <https://en.wikipedia.org/wiki/Sensitivity_and_specificity>`_
+(true positive rate) and `specificity
+<https://en.wikipedia.org/wiki/Sensitivity_and_specificity>`_ (true negative
+rate), or the area under the ROC curve with binary predictions rather than
+scores.
 
-If :math:`\hat{y}_i\in\{0,1\}` is the predicted value of
-the :math:`i`-th sample and :math:`y_i\in\{0,1\}` is the corresponding true value,
-then the balanced accuracy is defined as
+If the classifier performs equally well on either class, this term reduces to
+the conventional accuracy (i.e., the number of correct predictions divided by
+the total number of predictions).
+
+In contrast, if the conventional accuracy is above chance only because the
+classifier takes advantage of an imbalanced test set, then the balanced
+accuracy, as appropriate, will drop to :math:`\frac{1}{\text{n\_classes}}`.
+
+The score ranges from 0 to 1, or when ``adjusted=True`` is used, it rescaled to
+the range :math:`\frac{1}{1 - \text{n\_classes}}` to 1, inclusive, with
+performance at random scoring 0.
+
+If :math:`y_i` is the true value of the :math:`i`-th sample, and :math:`w_i`
+is the corresponding sample weight, then we adjust the sample weight to:
 
 .. math::
 
-   \texttt{balanced-accuracy}(y, \hat{y}) = \frac{1}{2} \left(\frac{\sum_i 1(\hat{y}_i = 1 \land y_i = 1)}{\sum_i 1(y_i = 1)} + \frac{\sum_i 1(\hat{y}_i = 0 \land y_i = 0)}{\sum_i 1(y_i = 0)}\right)
+   \hat{w}_i = \frac{w_i}{\sum_j{1(y_j = y_i) w_j}}
 
 where :math:`1(x)` is the `indicator function <https://en.wikipedia.org/wiki/Indicator_function>`_.
+Given predicted :math:`\hat{y}_i` for sample :math:`i`, balanced accuracy is
+defined as:
 
-Under this definition, the balanced accuracy coincides with :func:`roc_auc_score`
-given binary ``y_true`` and ``y_pred``:
+.. math::
 
-  >>> import numpy as np
-  >>> from sklearn.metrics import balanced_accuracy_score, roc_auc_score
-  >>> y_true = [0, 1, 0, 0, 1, 0]
-  >>> y_pred = [0, 1, 0, 0, 0, 1]
-  >>> balanced_accuracy_score(y_true, y_pred)
-  0.625
-  >>> roc_auc_score(y_true, y_pred)
-  0.625
+   \texttt{balanced-accuracy}(y, \hat{y}, w) = \frac{1}{\sum{\hat{w}_i}} \sum_i 1(\hat{y}_i = y_i) \hat{w}_i
 
-(but in general, :func:`roc_auc_score` takes as its second argument non-binary scores).
+With ``adjusted=True``, balanced accuracy reports the relative increase from
+:math:`\texttt{balanced-accuracy}(y, \mathbf{0}, w) =
+\frac{1}{\text{n\_classes}}`.  In the binary case, this is also known as
+`*Youden's J statistic* <https://en.wikipedia.org/wiki/Youden%27s_J_statistic>`_,
+or *informedness*.
 
 .. note::
 
-    Currently this score function is only defined for binary classification problems, you
-    may need to wrap it by yourself if you want to use it for multilabel problems.
-
-    There is no clear consensus on the definition of a balanced accuracy for the
-    multiclass setting. Here are some definitions that can be found in the literature:
+    The multiclass definition here seems the most reasonable extension of the
+    metric used in binary classification, though there is no certain consensus
+    in the literature:
 
-    * Macro-average recall as described in [Mosley2013]_, [Kelleher2015]_ and [Guyon2015]_:
-      the recall for each class is computed independently and the average is taken over all classes.
-      In [Guyon2015]_, the macro-average recall is then adjusted to ensure that random predictions
-      have a score of :math:`0` while perfect predictions have a score of :math:`1`.
-      One can compute the macro-average recall using ``recall_score(average="macro")`` in :func:`recall_score`.
+    * Our definition: [Mosley2013]_, [Kelleher2015]_ and [Guyon2015]_, where
+      [Guyon2015]_ adopt the adjusted version to ensure that random predictions
+      have a score of :math:`0` and perfect predictions have a score of :math:`1`..
     * Class balanced accuracy as described in [Mosley2013]_: the minimum between the precision
       and the recall for each class is computed. Those values are then averaged over the total
       number of classes to get the balanced accuracy.
-    * Balanced Accuracy as described in [Urbanowicz2015]_: the average of sensitivity and selectivity
+    * Balanced Accuracy as described in [Urbanowicz2015]_: the average of sensitivity and specificity
       is computed for each class and then averaged over total number of classes.
 
-    Note that none of these different definitions are currently implemented within
-    the :func:`balanced_accuracy_score` function.
-
 .. topic:: References:
 
   .. [Guyon2015] I. Guyon, K. Bennett, G. Cawley, H.J. Escalante, S. Escalera, T.K. Ho, N. Macià,
@@ -586,13 +591,15 @@ and inferred labels::
    >>> y_pred = [0, 0, 2, 1, 0]
    >>> target_names = ['class 0', 'class 1', 'class 2']
    >>> print(classification_report(y_true, y_pred, target_names=target_names))
-                precision    recall  f1-score   support
+                 precision    recall  f1-score   support
    <BLANKLINE>
-       class 0       0.67      1.00      0.80         2
-       class 1       0.00      0.00      0.00         1
-       class 2       1.00      0.50      0.67         2
+        class 0       0.67      1.00      0.80         2
+        class 1       0.00      0.00      0.00         1
+        class 2       1.00      0.50      0.67         2
    <BLANKLINE>
-   avg / total       0.67      0.60      0.59         5
+      micro avg       0.60      0.60      0.60         5
+      macro avg       0.56      0.50      0.49         5
+   weighted avg       0.67      0.60      0.59         5
    <BLANKLINE>
 
 .. topic:: Example:
diff --git a/doc/modules/naive_bayes.rst b/doc/modules/naive_bayes.rst
index f3abe5720540..229ce6654d7c 100644
--- a/doc/modules/naive_bayes.rst
+++ b/doc/modules/naive_bayes.rst
@@ -85,7 +85,7 @@ classification. The likelihood of the features is assumed to be Gaussian:
 
 .. math::
 
-   P(x_i \mid y) &= \frac{1}{\sqrt{2\pi\sigma^2_y}} \exp\left(-\frac{(x_i - \mu_y)^2}{2\sigma^2_y}\right)
+   P(x_i \mid y) = \frac{1}{\sqrt{2\pi\sigma^2_y}} \exp\left(-\frac{(x_i - \mu_y)^2}{2\sigma^2_y}\right)
 
 The parameters :math:`\sigma_y` and :math:`\mu_y`
 are estimated using maximum likelihood.
@@ -125,7 +125,7 @@ version of maximum likelihood, i.e. relative frequency counting:
 where :math:`N_{yi} = \sum_{x \in T} x_i` is
 the number of times feature :math:`i` appears in a sample of class :math:`y`
 in the training set :math:`T`,
-and :math:`N_{y} = \sum_{i=1}^{|T|} N_{yi}` is the total count of
+and :math:`N_{y} = \sum_{i=1}^{n} N_{yi}` is the total count of
 all features for class :math:`y`.
 
 The smoothing priors :math:`\alpha \ge 0` accounts for
diff --git a/doc/modules/outlier_detection.rst b/doc/modules/outlier_detection.rst
index f35f823c5c5a..9dbe013bef5d 100644
--- a/doc/modules/outlier_detection.rst
+++ b/doc/modules/outlier_detection.rst
@@ -12,13 +12,27 @@ belongs to the same distribution as existing observations (it is an
 Often, this ability is used to clean real data sets. Two important
 distinction must be made:
 
-:novelty detection:
-  The training data is not polluted by outliers, and we are interested in
-  detecting anomalies in new observations.
-
 :outlier detection:
-  The training data contains outliers, and we need to fit the central
-  mode of the training data, ignoring the deviant observations.
+  The training data contains outliers which are defined as observations that
+  are far from the others. Outlier detection estimators thus try to fit the
+  regions where the training data is the most concentrated, ignoring the
+  deviant observations.
+
+:novelty detection:
+  The training data is not polluted by outliers and we are interested in
+  detecting whether a **new** observation is an outlier. In this context an
+  outlier is also called a novelty.
+
+Outlier detection and novelty detection are both used for anomaly
+detection, where one is interested in detecting abnormal or unusual
+observations. Outlier detection is then also known as unsupervised anomaly
+detection and novelty detection as semi-supervised anomaly detection. In the
+context of outlier detection, the outliers/anomalies cannot form a
+dense cluster as available estimators assume that the outliers/anomalies are
+located in low density regions. On the contrary, in the context of novelty
+detection, novelties/anomalies can form a dense cluster as long as they are in
+a low density region of the training data, considered as normal in this
+context.
 
 The scikit-learn project provides a set of machine learning tools that
 can be used both for novelty or outliers detection. This strategy is
@@ -44,19 +58,55 @@ inliers::
     estimator.decision_function(X_test)
 
 Note that :class:`neighbors.LocalOutlierFactor` does not support
-``predict`` and ``decision_function`` methods, as this algorithm is
-purely transductive and is thus not designed to deal with new data.
+``predict``, ``decision_function`` and ``score_samples`` methods by default
+but only a ``fit_predict`` method, as this estimator was originally meant to
+be applied for outlier detection. The scores of abnormality of the training
+samples are accessible through the ``negative_outlier_factor_`` attribute.
+
+If you really want to use :class:`neighbors.LocalOutlierFactor` for novelty
+detection, i.e. predict labels or compute the score of abnormality of new
+unseen data, you can instantiate the estimator with the ``novelty`` parameter
+set to ``True`` before fitting the estimator. In this case, ``fit_predict`` is
+not available.
+
+.. warning:: **Novelty detection with Local Outlier Factor**
+
+  When ``novelty`` is set to ``True`` be aware that you must only use
+  ``predict``, ``decision_function`` and ``score_samples`` on new unseen data
+  and not on the training samples as this would lead to wrong results.
+  The scores of abnormality of the training samples are always accessible
+  through the ``negative_outlier_factor_`` attribute.
+
 
 Overview of outlier detection methods
 =====================================
 
+A comparison of the outlier detection algorithms in scikit-learn. Local
+Outlier Factor (LOF) does not show a decision boundary in black as it
+has no predict method to be applied on new data when it is used for outlier
+detection.
+
 .. figure:: ../auto_examples/images/sphx_glr_plot_anomaly_comparison_001.png
    :target: ../auto_examples/plot_anomaly_comparison.html
    :align: center
    :scale: 50
 
-   A comparison of the outlier detection algorithms in scikit-learn
+:class:`ensemble.IsolationForest` and :class:`neighbors.LocalOutlierFactor`
+perform reasonably well on the data sets considered here.
+The :class:`svm.OneClassSVM` is known to be sensitive to outliers and thus
+does not perform very well for outlier detection. Finally,
+:class:`covariance.EllipticEnvelope` assumes the data is Gaussian and learns
+an ellipse. For more details on the different estimators refer to the example
+:ref:`sphx_glr_auto_examples_plot_anomaly_comparison.py` and the sections
+hereunder.
+
+.. topic:: Examples:
 
+  * See :ref:`sphx_glr_auto_examples_plot_anomaly_comparison.py`
+    for a comparison of the :class:`svm.OneClassSVM`, the
+    :class:`ensemble.IsolationForest`, the
+    :class:`neighbors.LocalOutlierFactor` and
+    :class:`covariance.EllipticEnvelope`.
 
 Novelty Detection
 =================
@@ -99,6 +149,7 @@ but regular, observation outside the frontier.
    * See :ref:`sphx_glr_auto_examples_svm_plot_oneclass.py` for visualizing the
      frontier learned around some data by a
      :class:`svm.OneClassSVM` object.
+   * :ref:`sphx_glr_auto_examples_applications_plot_species_distribution_modeling.py`
 
 .. figure:: ../auto_examples/svm/images/sphx_glr_plot_oneclass_001.png
    :target: ../auto_examples/svm/plot_oneclass.html
@@ -188,7 +239,7 @@ This strategy is illustrated below.
    * See :ref:`sphx_glr_auto_examples_ensemble_plot_isolation_forest.py` for
      an illustration of the use of IsolationForest.
 
-   * See :ref:`sphx_glr_auto_examples_covariance_plot_outlier_detection.py` for a
+   * See :ref:`sphx_glr_auto_examples_plot_anomaly_comparison.py` for a
      comparison of :class:`ensemble.IsolationForest` with
      :class:`neighbors.LocalOutlierFactor`,
      :class:`svm.OneClassSVM` (tuned to perform like an outlier detection
@@ -236,19 +287,28 @@ where abnormal samples have different underlying densities.
 The question is not, how isolated the sample is, but how isolated it is
 with respect to the surrounding neighborhood.
 
+When applying LOF for outlier detection, there are no ``predict``,
+``decision_function`` and ``score_samples`` methods but only a ``fit_predict``
+method. The scores of abnormality of the training samples are accessible
+through the ``negative_outlier_factor_`` attribute.
+Note that ``predict``, ``decision_function`` and ``score_samples`` can be used
+on new unseen data when LOF is applied for novelty detection, i.e. when the
+``novelty`` parameter is set to ``True``. See :ref:`novelty_with_lof`.
+
+
 This strategy is illustrated below.
 
-.. figure:: ../auto_examples/neighbors/images/sphx_glr_plot_lof_001.png
-   :target: ../auto_examples/neighbors/plot_lof.html
+.. figure:: ../auto_examples/neighbors/images/sphx_glr_plot_lof_outlier_detection_001.png
+   :target: ../auto_examples/neighbors/sphx_glr_plot_lof_outlier_detection.html
    :align: center
    :scale: 75%
 
 .. topic:: Examples:
 
-   * See :ref:`sphx_glr_auto_examples_neighbors_plot_lof.py` for
-     an illustration of the use of :class:`neighbors.LocalOutlierFactor`.
+   * See :ref:`sphx_glr_auto_examples_neighbors_plot_lof_outlier_detection.py`
+     for an illustration of the use of :class:`neighbors.LocalOutlierFactor`.
 
-   * See :ref:`sphx_glr_auto_examples_covariance_plot_outlier_detection.py` for a
+   * See :ref:`sphx_glr_auto_examples_plot_anomaly_comparison.py` for a
      comparison with other anomaly detection methods.
 
 .. topic:: References:
@@ -258,72 +318,45 @@ This strategy is illustrated below.
       <http://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf>`_
       Proc. ACM SIGMOD
 
-One-class SVM versus Elliptic Envelope versus Isolation Forest versus LOF
--------------------------------------------------------------------------
-
-Strictly-speaking, the One-class SVM is not an outlier-detection method,
-but a novelty-detection method: its training set should not be
-contaminated by outliers as it may fit them. That said, outlier detection
-in high-dimension, or without any assumptions on the distribution of the
-inlying data is very challenging, and a One-class SVM gives useful
-results in these situations.
-
-The examples below illustrate how the performance of the
-:class:`covariance.EllipticEnvelope` degrades as the data is less and
-less unimodal. The :class:`svm.OneClassSVM` works better on data with
-multiple modes and :class:`ensemble.IsolationForest` and
-:class:`neighbors.LocalOutlierFactor` perform well in every cases.
-
-.. |outlier1| image:: ../auto_examples/covariance/images/sphx_glr_plot_outlier_detection_001.png
-   :target: ../auto_examples/covariance/plot_outlier_detection.html
-   :scale: 50%
-
-.. |outlier2| image:: ../auto_examples/covariance/images/sphx_glr_plot_outlier_detection_002.png
-   :target: ../auto_examples/covariance/plot_outlier_detection.html
-   :scale: 50%
-
-.. |outlier3| image:: ../auto_examples/covariance/images/sphx_glr_plot_outlier_detection_003.png
-   :target: ../auto_examples/covariance/plot_outlier_detection.html
-   :scale: 50%
-
-.. list-table:: **Comparing One-class SVM, Isolation Forest, LOF, and Elliptic Envelope**
-   :widths: 40 60
-
-   *
-      - For a inlier mode well-centered and elliptic, the
-        :class:`svm.OneClassSVM` is not able to benefit from the
-        rotational symmetry of the inlier population. In addition, it
-        fits a bit the outliers present in the training set. On the
-        opposite, the decision rule based on fitting an
-        :class:`covariance.EllipticEnvelope` learns an ellipse, which
-        fits well the inlier distribution. The :class:`ensemble.IsolationForest`
-        and :class:`neighbors.LocalOutlierFactor` perform as well.
-      - |outlier1| 
-
-   *
-      - As the inlier distribution becomes bimodal, the
-        :class:`covariance.EllipticEnvelope` does not fit well the
-        inliers. However, we can see that :class:`ensemble.IsolationForest`,
-        :class:`svm.OneClassSVM` and :class:`neighbors.LocalOutlierFactor`
-        have difficulties to detect the two modes,
-        and that the :class:`svm.OneClassSVM`
-        tends to overfit: because it has no model of inliers, it
-        interprets a region where, by chance some outliers are
-        clustered, as inliers.
-      - |outlier2|
-
-   *
-      - If the inlier distribution is strongly non Gaussian, the
-        :class:`svm.OneClassSVM` is able to recover a reasonable
-        approximation as well as :class:`ensemble.IsolationForest`
-        and :class:`neighbors.LocalOutlierFactor`,
-        whereas the :class:`covariance.EllipticEnvelope` completely fails.
-      - |outlier3|
+.. _novelty_with_lof:
 
-.. topic:: Examples:
+Novelty detection with Local Outlier Factor
+===========================================
+
+To use :class:`neighbors.LocalOutlierFactor` for novelty detection, i.e.
+predict labels or compute the score of abnormality of new unseen data, you
+need to instantiate the estimator with the ``novelty`` parameter
+set to ``True`` before fitting the estimator::
+
+  lof = LocalOutlierFactor(novelty=True)
+  lof.fit(X_train)
+
+Note that ``fit_predict`` is not available in this case.
+
+.. warning:: **Novelty detection with Local Outlier Factor`**
+
+  When ``novelty`` is set to ``True`` be aware that you must only use
+  ``predict``, ``decision_function`` and ``score_samples`` on new unseen data
+  and not on the training samples as this would lead to wrong results.
+  The scores of abnormality of the training samples are always accessible
+  through the ``negative_outlier_factor_`` attribute.
+
+The behavior of LOF is summarized in the following table.
+
+====================  ================================  =====================
+Method                Outlier detection                 Novelty detection
+====================  ================================  =====================
+`fit_predict`         OK                                Not available
+`predict`             Not available                     Use only on test data
+`decision_function`   Not available                     Use only on test data
+`score_samples`       Use `negative_outlier_factor_`    Use only on test data
+====================  ================================  =====================
+
+
+This strategy is illustrated below.
+
+  .. figure:: ../auto_examples/neighbors/images/sphx_glr_plot_lof_novelty_detection_001.png
+     :target: ../auto_examples/neighbors/sphx_glr_plot_lof_novelty_detection.html
+     :align: center
+     :scale: 75%
 
-   * See :ref:`sphx_glr_auto_examples_covariance_plot_outlier_detection.py` for a
-     comparison of the :class:`svm.OneClassSVM` (tuned to perform like
-     an outlier detection method), the :class:`ensemble.IsolationForest`,
-     the :class:`neighbors.LocalOutlierFactor`
-     and a covariance-based outlier detection :class:`covariance.EllipticEnvelope`.
diff --git a/doc/modules/preprocessing.rst b/doc/modules/preprocessing.rst
index 0474a8a66501..dd1f798ccb3a 100644
--- a/doc/modules/preprocessing.rst
+++ b/doc/modules/preprocessing.rst
@@ -309,20 +309,34 @@ Power transforms are a family of parametric, monotonic transformations that aim
 to map data from any distribution to as close to a Gaussian distribution as
 possible in order to stabilize variance and minimize skewness.
 
-:class:`PowerTransformer` currently provides one such power transformation,
-the Box-Cox transform. The Box-Cox transform is given by:
+:class:`PowerTransformer` currently provides two such power transformations,
+the Yeo-Johnson transform and the Box-Cox transform.
+
+The Yeo-Johnson transform is given by:
 
 .. math::
-    y_i^{(\lambda)} =
+    x_i^{(\lambda)} =
     \begin{cases}
-    \dfrac{y_i^\lambda - 1}{\lambda} & \text{if } \lambda \neq 0, \\[8pt]
-    \ln{(y_i)} & \text{if } \lambda = 0,
+     [(x_i + 1)^\lambda - 1] / \lambda & \text{if } \lambda \neq 0, x_i \geq 0, \\[8pt]
+    \ln{(x_i) + 1} & \text{if } \lambda = 0, x_i \geq 0 \\[8pt]
+    -[(-x_i + 1)^{2 - \lambda} - 1] / (2 - \lambda) & \text{if } \lambda \neq 2, x_i < 0, \\[8pt]
+     - \ln (- x_i + 1) & \text{if } \lambda = 2, x_i < 0
     \end{cases}
 
-Box-Cox can only be applied to strictly positive data. The transformation is
-parameterized by :math:`\lambda`, which is determined through maximum likelihood
-estimation. Here is an example of using Box-Cox to map samples drawn from a
-lognormal distribution to a normal distribution::
+while the Box-Cox transform is given by:
+
+.. math::
+    x_i^{(\lambda)} =
+    \begin{cases}
+    \dfrac{x_i^\lambda - 1}{\lambda} & \text{if } \lambda \neq 0, \\[8pt]
+    \ln{(x_i)} & \text{if } \lambda = 0,
+    \end{cases}
+
+
+Box-Cox can only be applied to strictly positive data. In both methods, the
+transformation is parameterized by :math:`\lambda`, which is determined through
+maximum likelihood estimation. Here is an example of using Box-Cox to map
+samples drawn from a lognormal distribution to a normal distribution::
 
   >>> pt = preprocessing.PowerTransformer(method='box-cox', standardize=False)
   >>> X_lognormal = np.random.RandomState(616).lognormal(size=(3, 3))
@@ -339,13 +353,14 @@ While the above example sets the `standardize` option to `False`,
 :class:`PowerTransformer` will apply zero-mean, unit-variance normalization
 to the transformed output by default.
 
-Below are examples of Box-Cox applied to various probability distributions.
-Note that when applied to certain distributions, Box-Cox achieves very
-Gaussian-like results, but with others, it is ineffective. This highlights
-the importance of visualizing the data before and after transformation.
+Below are examples of Box-Cox and Yeo-Johnson applied to various probability
+distributions.  Note that when applied to certain distributions, the power
+transforms achieve very Gaussian-like results, but with others, they are
+ineffective. This highlights the importance of visualizing the data before and
+after transformation.
 
-.. figure:: ../auto_examples/preprocessing/images/sphx_glr_plot_power_transformer_001.png
-   :target: ../auto_examples/preprocessing/plot_power_transformer.html
+.. figure:: ../auto_examples/preprocessing/images/sphx_glr_plot_map_data_to_normal_001.png
+   :target: ../auto_examples/preprocessing/plot_map_data_to_normal.html
    :align: center
    :scale: 100
 
@@ -432,67 +447,6 @@ The normalizer instance can then be used on sample vectors as any transformer::
   efficient Cython routines. To avoid unnecessary memory copies, it is
   recommended to choose the CSR representation upstream.
 
-.. _preprocessing_binarization:
-
-Binarization
-============
-
-Feature binarization
---------------------
-
-**Feature binarization** is the process of **thresholding numerical
-features to get boolean values**. This can be useful for downstream
-probabilistic estimators that make assumption that the input data
-is distributed according to a multi-variate `Bernoulli distribution
-<https://en.wikipedia.org/wiki/Bernoulli_distribution>`_. For instance,
-this is the case for the :class:`sklearn.neural_network.BernoulliRBM`.
-
-It is also common among the text processing community to use binary
-feature values (probably to simplify the probabilistic reasoning) even
-if normalized counts (a.k.a. term frequencies) or TF-IDF valued features
-often perform slightly better in practice.
-
-As for the :class:`Normalizer`, the utility class
-:class:`Binarizer` is meant to be used in the early stages of
-:class:`sklearn.pipeline.Pipeline`. The ``fit`` method does nothing
-as each sample is treated independently of others::
-
-  >>> X = [[ 1., -1.,  2.],
-  ...      [ 2.,  0.,  0.],
-  ...      [ 0.,  1., -1.]]
-
-  >>> binarizer = preprocessing.Binarizer().fit(X)  # fit does nothing
-  >>> binarizer
-  Binarizer(copy=True, threshold=0.0)
-
-  >>> binarizer.transform(X)
-  array([[1., 0., 1.],
-         [1., 0., 0.],
-         [0., 1., 0.]])
-
-It is possible to adjust the threshold of the binarizer::
-
-  >>> binarizer = preprocessing.Binarizer(threshold=1.1)
-  >>> binarizer.transform(X)
-  array([[0., 0., 1.],
-         [1., 0., 0.],
-         [0., 0., 0.]])
-
-As for the :class:`StandardScaler` and :class:`Normalizer` classes, the
-preprocessing module provides a companion function :func:`binarize`
-to be used when the transformer API is not necessary.
-
-.. topic:: Sparse input
-
-  :func:`binarize` and :class:`Binarizer` accept **both dense array-like
-  and sparse matrices from scipy.sparse as input**.
-
-  For sparse input the data is **converted to the Compressed Sparse Rows
-  representation** (see ``scipy.sparse.csr_matrix``).
-  To avoid unnecessary memory copies, it is recommended to choose the CSR
-  representation upstream.
-
-
 .. _preprocessing_categorical_features:
 
 Encoding categorical features
@@ -589,6 +543,124 @@ columns for this feature will be all zeros
 See :ref:`dict_feature_extraction` for categorical features that are represented
 as a dict, not as scalars.
 
+.. _preprocessing_discretization:
+
+Discretization
+==============
+
+`Discretization <https://en.wikipedia.org/wiki/Discretization_of_continuous_features>`_
+(otherwise known as quantization or binning) provides a way to partition continuous
+features into discrete values. Certain datasets with continuous features
+may benefit from discretization, because discretization can transform the dataset
+of continuous attributes to one with only nominal attributes.
+
+K-bins discretization
+---------------------
+
+:class:`KBinsDiscretizer` discretizers features into ``k`` equal width bins::
+
+  >>> X = np.array([[ -3., 5., 15 ],
+  ...               [  0., 6., 14 ],
+  ...               [  6., 3., 11 ]])
+  >>> est = preprocessing.KBinsDiscretizer(n_bins=[3, 2, 2], encode='ordinal').fit(X)
+
+By default the output is one-hot encoded into a sparse matrix
+(See :ref:`preprocessing_categorical_features`)
+and this can be configured with the ``encode`` parameter.
+For each feature, the bin edges are computed during ``fit`` and together with
+the number of bins, they will define the intervals. Therefore, for the current
+example, these intervals are defined as:
+
+ - feature 1: :math:`{[-\infty, -1), [-1, 2), [2, \infty)}`
+ - feature 2: :math:`{[-\infty, 5), [5, \infty)}`
+ - feature 3: :math:`{[-\infty, 14), [14, \infty)}`
+
+ Based on these bin intervals, ``X`` is transformed as follows::
+
+  >>> est.transform(X)                      # doctest: +SKIP
+  array([[ 0., 1., 1.],
+         [ 1., 1., 1.],
+         [ 2., 0., 0.]])
+
+The resulting dataset contains ordinal attributes which can be further used
+in a :class:`sklearn.pipeline.Pipeline`.
+
+Discretization is similar to constructing histograms for continuous data.
+However, histograms focus on counting features which fall into particular
+bins, whereas discretization focuses on assigning feature values to these bins.
+
+:class:`KBinsDiscretizer` implements different binning strategies, which can be
+selected with the ``strategy`` parameter. The 'uniform' strategy uses
+constant-width bins. The 'quantile' strategy uses the quantiles values to have
+equally populated bins in each feature. The 'kmeans' strategy defines bins based
+on a k-means clustering procedure performed on each feature independently.
+
+.. topic:: Examples:
+
+  * :ref:`sphx_glr_auto_examples_preprocessing_plot_discretization.py`
+  * :ref:`sphx_glr_auto_examples_preprocessing_plot_discretization_classification.py`
+  * :ref:`sphx_glr_auto_examples_preprocessing_plot_discretization_strategies.py`
+
+.. _preprocessing_binarization:
+
+Feature binarization
+--------------------
+
+**Feature binarization** is the process of **thresholding numerical
+features to get boolean values**. This can be useful for downstream
+probabilistic estimators that make assumption that the input data
+is distributed according to a multi-variate `Bernoulli distribution
+<https://en.wikipedia.org/wiki/Bernoulli_distribution>`_. For instance,
+this is the case for the :class:`sklearn.neural_network.BernoulliRBM`.
+
+It is also common among the text processing community to use binary
+feature values (probably to simplify the probabilistic reasoning) even
+if normalized counts (a.k.a. term frequencies) or TF-IDF valued features
+often perform slightly better in practice.
+
+As for the :class:`Normalizer`, the utility class
+:class:`Binarizer` is meant to be used in the early stages of
+:class:`sklearn.pipeline.Pipeline`. The ``fit`` method does nothing
+as each sample is treated independently of others::
+
+  >>> X = [[ 1., -1.,  2.],
+  ...      [ 2.,  0.,  0.],
+  ...      [ 0.,  1., -1.]]
+
+  >>> binarizer = preprocessing.Binarizer().fit(X)  # fit does nothing
+  >>> binarizer
+  Binarizer(copy=True, threshold=0.0)
+
+  >>> binarizer.transform(X)
+  array([[1., 0., 1.],
+         [1., 0., 0.],
+         [0., 1., 0.]])
+
+It is possible to adjust the threshold of the binarizer::
+
+  >>> binarizer = preprocessing.Binarizer(threshold=1.1)
+  >>> binarizer.transform(X)
+  array([[0., 0., 1.],
+         [1., 0., 0.],
+         [0., 0., 0.]])
+
+As for the :class:`StandardScaler` and :class:`Normalizer` classes, the
+preprocessing module provides a companion function :func:`binarize`
+to be used when the transformer API is not necessary.
+
+Note that the :class:`Binarizer` is similar to the :class:`KBinsDiscretizer`
+when ``k = 2``, and when the bin edge is at the value ``threshold``.
+
+.. topic:: Sparse input
+
+  :func:`binarize` and :class:`Binarizer` accept **both dense array-like
+  and sparse matrices from scipy.sparse as input**.
+
+  For sparse input the data is **converted to the Compressed Sparse Rows
+  representation** (see ``scipy.sparse.csr_matrix``).
+  To avoid unnecessary memory copies, it is recommended to choose the CSR
+  representation upstream.
+
 .. _imputation:
 
 Imputation of missing values
@@ -649,7 +721,7 @@ a transformer that applies a log transformation in a pipeline, do::
 
     >>> import numpy as np
     >>> from sklearn.preprocessing import FunctionTransformer
-    >>> transformer = FunctionTransformer(np.log1p)
+    >>> transformer = FunctionTransformer(np.log1p, validate=True)
     >>> X = np.array([[0, 1], [2, 3]])
     >>> transformer.transform(X)
     array([[0.        , 0.69314718],
diff --git a/doc/modules/random_projection.rst b/doc/modules/random_projection.rst
index 5585be6f6b21..d3e2c8023089 100644
--- a/doc/modules/random_projection.rst
+++ b/doc/modules/random_projection.rst
@@ -152,11 +152,11 @@ projection transformer::
 
  * D. Achlioptas. 2003.
    `Database-friendly random projections: Johnson-Lindenstrauss  with binary
-   coins <www.cs.ucsc.edu/~optas/papers/jl.pdf>`_.
+   coins <http://www.cs.ucsc.edu/~optas/papers/jl.pdf>`_.
    Journal of Computer and System Sciences 66 (2003) 671–687
 
  * Ping Li, Trevor J. Hastie, and Kenneth W. Church. 2006.
-   `Very sparse random projections. <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.62.585&rep=rep1&type=pdf>`_
+   `Very sparse random projections. <https://web.stanford.edu/~hastie/Papers/Ping/KDD06_rp.pdf>`_
    In Proceedings of the 12th ACM SIGKDD international conference on
    Knowledge discovery and data mining (KDD '06). ACM, New York, NY, USA,
    287-296.
diff --git a/doc/modules/sgd.rst b/doc/modules/sgd.rst
index 64eea91a9fa9..5792badf508b 100644
--- a/doc/modules/sgd.rst
+++ b/doc/modules/sgd.rst
@@ -60,12 +60,13 @@ for the training samples::
     >>> X = [[0., 0.], [1., 1.]]
     >>> y = [0, 1]
     >>> clf = SGDClassifier(loss="hinge", penalty="l2", max_iter=5)
-    >>> clf.fit(X, y)
-    SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,
-           eta0=0.0, fit_intercept=True, l1_ratio=0.15,
-           learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,
-           n_jobs=1, penalty='l2', power_t=0.5, random_state=None,
-           shuffle=True, tol=None, verbose=0, warm_start=False)
+    >>> clf.fit(X, y)   # doctest: +NORMALIZE_WHITESPACE
+    SGDClassifier(alpha=0.0001, average=False, class_weight=None,
+               early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,
+               l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,
+               n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',
+               power_t=0.5, random_state=None, shuffle=True, tol=None,
+               validation_fraction=0.1, verbose=0, warm_start=False)
 
 
 After being fitted, the model can then be used to predict new values::
@@ -232,6 +233,27 @@ non-zero attributes per sample.
 Recent theoretical results, however, show that the runtime to get some
 desired optimization accuracy does not increase as the training set size increases.
 
+Stopping criterion
+==================
+
+The classes :class:`SGDClassifier` and :class:`SGDRegressor` provide two
+criteria to stop the algorithm when a given level of convergence is reached:
+
+  * With ``early_stopping=True``, the input data is split into a training set
+    and a validation set. The model is then fitted on the training set, and the
+    stopping criterion is based on the prediction score computed on the
+    validation set. The size of the validation set can be changed with the
+    parameter ``validation_fraction``.
+  * With ``early_stopping=False``, the model is fitted on the entire input data
+    and the stopping criterion is based on the objective function computed on
+    the input data.
+
+In both cases, the criterion is evaluated once by epoch, and the algorithm stops
+when the criterion does not improve ``n_iter_no_change`` times in a row. The
+improvement is evaluated with a tolerance ``tol``, and the algorithm stops in
+any case after a maximum number of iteration ``max_iter``.
+
+
 Tips on Practical Use
 =====================
 
@@ -257,7 +279,7 @@ Tips on Practical Use
 
   * Empirically, we found that SGD converges after observing
     approx. 10^6 training samples. Thus, a reasonable first guess
-    for the number of iterations is ``n_iter = np.ceil(10**6 / n)``,
+    for the number of iterations is ``max_iter = np.ceil(10**6 / n)``,
     where ``n`` is the size of the training set.
 
   * If you apply SGD to features extracted using PCA we found that
@@ -373,6 +395,11 @@ user via ``eta0`` and ``power_t``, resp.
 For a constant learning rate use ``learning_rate='constant'`` and use ``eta0``
 to specify the learning rate.
 
+For an adaptively decreasing learning rate, use ``learning_rate='adaptive'``
+and use ``eta0`` to specify the starting learning rate. When the stopping
+criterion is reached, the learning rate is divided by 5, and the algorithm
+does not stop. The algorithm stops when the learning rate goes below 1e-6.
+
 The model parameters can be accessed through the members ``coef_`` and
 ``intercept_``:
 
diff --git a/doc/modules/svm.rst b/doc/modules/svm.rst
index aac074cc2d99..bd065c14f744 100644
--- a/doc/modules/svm.rst
+++ b/doc/modules/svm.rst
@@ -336,27 +336,10 @@ floating point values instead of integer values::
 Density estimation, novelty detection
 =======================================
 
-One-class SVM is used for novelty detection, that is, given a set of
-samples, it will detect the soft boundary of that set so as to
-classify new points as belonging to that set or not. The class that
-implements this is called :class:`OneClassSVM`.
-
-In this case, as it is a type of unsupervised learning, the fit method
-will only take as input an array X, as there are no class labels.
-
-See, section :ref:`outlier_detection` for more details on this usage.
-
-.. figure:: ../auto_examples/svm/images/sphx_glr_plot_oneclass_001.png
-   :target: ../auto_examples/svm/plot_oneclass.html
-   :align: center
-   :scale: 75
-
-
-.. topic:: Examples:
-
- * :ref:`sphx_glr_auto_examples_svm_plot_oneclass.py`
- * :ref:`sphx_glr_auto_examples_applications_plot_species_distribution_modeling.py`
+The class :class:`OneClassSVM` implements a One-Class SVM which is used in
+outlier detection. 
 
+See :ref:`outlier_detection` for the description and usage of OneClassSVM.
 
 Complexity
 ==========
diff --git a/doc/themes/scikit-learn/static/css/bootstrap.css b/doc/themes/scikit-learn/static/css/bootstrap.css
index 2ae4c14088bd..2cdd4ac5f938 100644
--- a/doc/themes/scikit-learn/static/css/bootstrap.css
+++ b/doc/themes/scikit-learn/static/css/bootstrap.css
@@ -909,6 +909,11 @@ a.badge:focus {
 .badge-warning[href] {
   background-color: #c67605;
 }
+.label-danger,
+.badge-danger {
+  /* XXX: backported from later bootstrap */
+  background-color: #d9534f;
+}
 .label-success,
 .badge-success {
   background-color: #468847;
diff --git a/doc/themes/scikit-learn/static/css/bootstrap.min.css b/doc/themes/scikit-learn/static/css/bootstrap.min.css
index 2f4146227e04..0243215b6655 100644
--- a/doc/themes/scikit-learn/static/css/bootstrap.min.css
+++ b/doc/themes/scikit-learn/static/css/bootstrap.min.css
@@ -164,6 +164,7 @@ a.label:hover,a.label:focus,a.badge:hover,a.badge:focus{color:#ffffff;text-decor
 .label-important[href],.badge-important[href]{background-color:#953b39;}
 .label-warning,.badge-warning{background-color:#f89406;}
 .label-warning[href],.badge-warning[href]{background-color:#c67605;}
+.label-danger,.badge-danger {/* XXX: backported from later bootstrap */background-color: #d9534f;}
 .label-success,.badge-success{background-color:#468847;}
 .label-success[href],.badge-success[href]{background-color:#356635;}
 .label-info,.badge-info{background-color:#3a87ad;}
diff --git a/doc/themes/scikit-learn/static/img/digicosme.png b/doc/themes/scikit-learn/static/img/digicosme.png
new file mode 100644
index 000000000000..2190fc5a5177
Binary files /dev/null and b/doc/themes/scikit-learn/static/img/digicosme.png differ
diff --git a/doc/tutorial/basic/tutorial.rst b/doc/tutorial/basic/tutorial.rst
index 781495df9931..18189ee385bf 100644
--- a/doc/tutorial/basic/tutorial.rst
+++ b/doc/tutorial/basic/tutorial.rst
@@ -318,8 +318,8 @@ Refitting and updating parameters
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 Hyper-parameters of an estimator can be updated after it has been constructed
-via the :func:`sklearn.pipeline.Pipeline.set_params` method. Calling ``fit()``
-more than once will overwrite what was learned by any previous ``fit()``::
+via the :term:`set_params()<set_params>` method. Calling ``fit()`` more than
+once will overwrite what was learned by any previous ``fit()``::
 
   >>> import numpy as np
   >>> from sklearn.svm import SVC
@@ -346,9 +346,10 @@ more than once will overwrite what was learned by any previous ``fit()``::
   >>> clf.predict(X_test)
   array([1, 0, 1, 1, 0])
 
-Here, the default kernel ``rbf`` is first changed to ``linear`` after the
-estimator has been constructed via ``SVC()``, and changed back to ``rbf`` to
-refit the estimator and to make a second prediction.
+Here, the default kernel ``rbf`` is first changed to ``linear`` via
+:func:`SVC.set_params()<sklearn.svm.SVC.set_params>` after the estimator has
+been constructed, and changed back to ``rbf`` to refit the estimator and to
+make a second prediction.
 
 Multiclass vs. multilabel fitting
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
diff --git a/doc/tutorial/statistical_inference/model_selection.rst b/doc/tutorial/statistical_inference/model_selection.rst
index 3feba26c6a77..50e3a06b9a4d 100644
--- a/doc/tutorial/statistical_inference/model_selection.rst
+++ b/doc/tutorial/statistical_inference/model_selection.rst
@@ -60,19 +60,21 @@ of the chosen cross-validation strategy.
 This example shows an example usage of the ``split`` method.
 
     >>> from sklearn.model_selection import KFold, cross_val_score
-    >>> X = ["a", "a", "b", "c", "c", "c"]
-    >>> k_fold = KFold(n_splits=3)
+    >>> X = ["a", "a", "a", "b", "b", "c", "c", "c", "c", "c"]
+    >>> k_fold = KFold(n_splits=5)
     >>> for train_indices, test_indices in k_fold.split(X):
     ...      print('Train: %s | test: %s' % (train_indices, test_indices))
-    Train: [2 3 4 5] | test: [0 1]
-    Train: [0 1 4 5] | test: [2 3]
-    Train: [0 1 2 3] | test: [4 5]
+    Train: [2 3 4 5 6 7 8 9] | test: [0 1]
+    Train: [0 1 4 5 6 7 8 9] | test: [2 3]
+    Train: [0 1 2 3 6 7 8 9] | test: [4 5]
+    Train: [0 1 2 3 4 5 8 9] | test: [6 7]
+    Train: [0 1 2 3 4 5 6 7] | test: [8 9]
 
 The cross-validation can then be performed easily::
 
     >>> [svc.fit(X_digits[train], y_digits[train]).score(X_digits[test], y_digits[test])
     ...          for train, test in k_fold.split(X_digits)]  # doctest: +ELLIPSIS
-    [0.934..., 0.956..., 0.939...]
+    [0.963..., 0.922..., 0.963..., 0.963..., 0.930...]
 
 The cross-validation score can be directly calculated using the
 :func:`cross_val_score` helper. Given an estimator, the cross-validation object
@@ -86,7 +88,7 @@ Refer the :ref:`metrics module <metrics>` to learn more on the available scoring
 methods.
 
     >>> cross_val_score(svc, X_digits, y_digits, cv=k_fold, n_jobs=-1)
-    array([0.93489149, 0.95659432, 0.93989983])
+    array([0.96388889, 0.92222222, 0.9637883 , 0.9637883 , 0.93036212])
 
 `n_jobs=-1` means that the computation will be dispatched on all the CPUs
 of the computer.
@@ -96,7 +98,7 @@ scoring method.
 
     >>> cross_val_score(svc, X_digits, y_digits, cv=k_fold,
     ...                 scoring='precision_macro')
-    array([0.93969761, 0.95911415, 0.94041254])
+    array([0.96578289, 0.92708922, 0.96681476, 0.96362897, 0.93192644])
 
    **Cross-validation generators**
 
@@ -215,28 +217,28 @@ estimator during the construction and exposes an estimator API::
     >>> Cs = np.logspace(-6, -1, 10)
     >>> clf = GridSearchCV(estimator=svc, param_grid=dict(C=Cs),
     ...                    n_jobs=-1)
-    >>> clf.fit(X_digits[:1000], y_digits[:1000])        # doctest: +ELLIPSIS
+    >>> clf.fit(X_digits[:1000], y_digits[:1000])        # doctest: +SKIP
     GridSearchCV(cv=None,...
-    >>> clf.best_score_                                  # doctest: +ELLIPSIS
+    >>> clf.best_score_                                  # doctest: +SKIP
     0.925...
-    >>> clf.best_estimator_.C                            # doctest: +ELLIPSIS
+    >>> clf.best_estimator_.C                            # doctest: +SKIP
     0.0077...
 
     >>> # Prediction performance on test set is not as good as on train set
-    >>> clf.score(X_digits[1000:], y_digits[1000:])      # doctest: +ELLIPSIS
+    >>> clf.score(X_digits[1000:], y_digits[1000:])      # doctest: +SKIP
     0.943...
 
 
 By default, the :class:`GridSearchCV` uses a 3-fold cross-validation. However,
 if it detects that a classifier is passed, rather than a regressor, it uses
-a stratified 3-fold.
+a stratified 3-fold. The default will change to a 5-fold cross-validation in
+version 0.22.
 
 .. topic:: Nested cross-validation
 
     ::
 
-        >>> cross_val_score(clf, X_digits, y_digits)
-        ...                                               # doctest: +ELLIPSIS
+        >>> cross_val_score(clf, X_digits, y_digits) # doctest: +SKIP
         array([0.938..., 0.963..., 0.944...])
 
     Two cross-validation loops are performed in parallel: one by the
@@ -261,15 +263,15 @@ scikit-learn exposes :ref:`cross_validation` estimators that set their
 parameter automatically by cross-validation::
 
     >>> from sklearn import linear_model, datasets
-    >>> lasso = linear_model.LassoCV()
+    >>> lasso = linear_model.LassoCV(cv=3)
     >>> diabetes = datasets.load_diabetes()
     >>> X_diabetes = diabetes.data
     >>> y_diabetes = diabetes.target
     >>> lasso.fit(X_diabetes, y_diabetes)
-    LassoCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,
-        max_iter=1000, n_alphas=100, n_jobs=1, normalize=False, positive=False,
-        precompute='auto', random_state=None, selection='cyclic', tol=0.0001,
-        verbose=False)
+    LassoCV(alphas=None, copy_X=True, cv=3, eps=0.001, fit_intercept=True,
+        max_iter=1000, n_alphas=100, n_jobs=None, normalize=False,
+        positive=False, precompute='auto', random_state=None,
+        selection='cyclic', tol=0.0001, verbose=False)
     >>> # The estimator chose automatically its lambda:
     >>> lasso.alpha_ # doctest: +ELLIPSIS
     0.01229...
diff --git a/doc/tutorial/statistical_inference/supervised_learning.rst b/doc/tutorial/statistical_inference/supervised_learning.rst
index 49e69d9ec80d..e60751b7f688 100644
--- a/doc/tutorial/statistical_inference/supervised_learning.rst
+++ b/doc/tutorial/statistical_inference/supervised_learning.rst
@@ -95,7 +95,7 @@ Scikit-learn documentation for more information about this type of classifier.)
     >>> knn = KNeighborsClassifier()
     >>> knn.fit(iris_X_train, iris_y_train) # doctest: +NORMALIZE_WHITESPACE
     KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
-               metric_params=None, n_jobs=1, n_neighbors=5, p=2,
+               metric_params=None, n_jobs=None, n_neighbors=5, p=2,
                weights='uniform')
     >>> knn.predict(iris_X_test)
     array([1, 2, 1, 0, 0, 0, 2, 1, 2, 0])
@@ -176,13 +176,16 @@ Linear models: :math:`y = X\beta + \epsilon`
     >>> from sklearn import linear_model
     >>> regr = linear_model.LinearRegression()
     >>> regr.fit(diabetes_X_train, diabetes_y_train)
-    LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
+    ...                                       # doctest: +NORMALIZE_WHITESPACE
+    LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,
+                     normalize=False)
     >>> print(regr.coef_)
     [   0.30349955 -237.63931533  510.53060544  327.73698041 -814.13170937
       492.81458798  102.84845219  184.60648906  743.51961675   76.09517222]
 
     >>> # The mean square error
-    >>> np.mean((regr.predict(diabetes_X_test)-diabetes_y_test)**2)# doctest: +ELLIPSIS
+    >>> np.mean((regr.predict(diabetes_X_test)-diabetes_y_test)**2)
+    ...                                                   # doctest: +ELLIPSIS
     2004.56760268...
 
     >>> # Explained variance score: 1 is perfect prediction
@@ -257,8 +260,11 @@ diabetes dataset rather than our synthetic data::
     >>> from __future__ import print_function
     >>> print([regr.set_params(alpha=alpha
     ...             ).fit(diabetes_X_train, diabetes_y_train,
-    ...             ).score(diabetes_X_test, diabetes_y_test) for alpha in alphas]) # doctest: +ELLIPSIS
-    [0.5851110683883..., 0.5852073015444..., 0.5854677540698..., 0.5855512036503..., 0.5830717085554..., 0.57058999437...]
+    ...             ).score(diabetes_X_test, diabetes_y_test)
+    ...        for alpha in alphas])
+    ...                            # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE
+    [0.5851110683883..., 0.5852073015444..., 0.5854677540698...,
+     0.5855512036503..., 0.5830717085554..., 0.57058999437...]
 
 
 .. note::
@@ -372,7 +378,7 @@ function or **logistic** function:
     >>> logistic.fit(iris_X_train, iris_y_train)
     LogisticRegression(C=100000.0, class_weight=None, dual=False,
               fit_intercept=True, intercept_scaling=1, max_iter=100,
-              multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
+              multi_class='ovr', n_jobs=None, penalty='l2', random_state=None,
               solver='liblinear', tol=0.0001, verbose=0, warm_start=False)
 
 This is known as :class:`LogisticRegression`.
diff --git a/doc/tutorial/text_analytics/working_with_text_data.rst b/doc/tutorial/text_analytics/working_with_text_data.rst
index 24b0b5b3e371..589d83006f11 100644
--- a/doc/tutorial/text_analytics/working_with_text_data.rst
+++ b/doc/tutorial/text_analytics/working_with_text_data.rst
@@ -379,7 +379,9 @@ utilities for more detailed performance analysis of the results::
                  sci.med       0.94      0.90      0.92       396
   soc.religion.christian       0.90      0.95      0.93       398
   <BLANKLINE>
-             avg / total       0.92      0.91      0.91      1502
+               micro avg       0.91      0.91      0.91      1502
+               macro avg       0.92      0.91      0.91      1502
+            weighted avg       0.92      0.91      0.91      1502
   <BLANKLINE>
 
   >>> metrics.confusion_matrix(twenty_test.target, predicted)
@@ -439,7 +441,7 @@ parameter combinations in parallel with the ``n_jobs`` parameter. If we give
 this parameter a value of ``-1``, grid search will detect how many cores
 are installed and use them all::
 
-  >>> gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)
+  >>> gs_clf = GridSearchCV(text_clf, parameters, cv=5, iid=False, n_jobs=-1)
 
 The grid search instance behaves like a normal ``scikit-learn``
 model. Let's perform the search on a smaller subset of the training data
@@ -463,7 +465,7 @@ mean score and the parameters setting corresponding to that score::
   ...
   clf__alpha: 0.001
   tfidf__use_idf: True
-  vect__ngram_range: (1, 1)
+  vect__ngram_range: (1, 2)
 
 A more detailed summary of the search is available at ``gs_clf.cv_results_``.
 
diff --git a/doc/whats_new/_contributors.rst b/doc/whats_new/_contributors.rst
index 58edca3a46b3..aeb8b0638d72 100644
--- a/doc/whats_new/_contributors.rst
+++ b/doc/whats_new/_contributors.rst
@@ -4,6 +4,20 @@
     for core contributors, and occasionally for contributors who do not want
     their github page to be their URL target. Historically it was used to
     hyperlink all contributors' names, and ``:user:`` should now be preferred.
+    It also defines other ReST substitutions.
+
+.. role:: raw-html(raw)
+   :format: html
+
+.. role:: raw-latex(raw)
+   :format: latex
+
+.. |MajorFeature| replace:: :raw-html:`<span class="label label-success">Major Feature</span>` :raw-latex:`{\small\sc [Major Feature]}`
+.. |Feature| replace:: :raw-html:`<span class="label label-success">Feature</span>` :raw-latex:`{\small\sc [Feature]}`
+.. |Efficiency| replace:: :raw-html:`<span class="label label-info">Efficiency</span>` :raw-latex:`{\small\sc [Efficiency]}`
+.. |Enhancement| replace:: :raw-html:`<span class="label label-info">Enhancement</span>` :raw-latex:`{\small\sc [Enhancement]}`
+.. |Fix| replace:: :raw-html:`<span class="label label-danger">Fix</span>` :raw-latex:`{\small\sc [Fix]}`
+.. |API| replace:: :raw-html:`<span class="label label-warning">API Change</span>` :raw-latex:`{\small\sc [API Change]}`
 
 
 .. _Olivier Grisel: https://twitter.com/ogrisel
@@ -153,3 +167,5 @@
 .. _Joris Van den Bossche: https://github.com/jorisvandenbossche
 
 .. _Roman Yurchak: https://github.com/rth
+
+.. _Hanmin Qin: https://github.com/qinhanmin2014
diff --git a/doc/whats_new/v0.16.rst b/doc/whats_new/v0.16.rst
index 33d8cc47e939..931c7e0fbb92 100644
--- a/doc/whats_new/v0.16.rst
+++ b/doc/whats_new/v0.16.rst
@@ -499,8 +499,8 @@ API changes summary
 
 - The ``shuffle`` option of :class:`.linear_model.SGDClassifier`,
   :class:`linear_model.SGDRegressor`, :class:`linear_model.Perceptron`,
-  :class:`linear_model.PassiveAgressiveClassifier` and
-  :class:`linear_model.PassiveAgressiveRegressor` now defaults to ``True``.
+  :class:`linear_model.PassiveAggressiveClassifier` and
+  :class:`linear_model.PassiveAggressiveRegressor` now defaults to ``True``.
 
 - :class:`cluster.DBSCAN` now uses a deterministic initialization. The
   `random_state` parameter is deprecated. By :user:`Erich Schubert <kno10>`.
diff --git a/doc/whats_new/v0.17.rst b/doc/whats_new/v0.17.rst
index 35e895e5d418..08aba83ebbee 100644
--- a/doc/whats_new/v0.17.rst
+++ b/doc/whats_new/v0.17.rst
@@ -152,7 +152,7 @@ Enhancements
   By `Hanna Wallach`_ and `Andreas Müller`_.
 
 - Add ``class_weight`` parameter to automatically weight samples by class
-  frequency for :class:`linear_model.PassiveAgressiveClassifier`. By
+  frequency for :class:`linear_model.PassiveAggressiveClassifier`. By
   `Trevor Stephens`_.
 
 - Added backlinks from the API reference pages to the user guide. By
diff --git a/doc/whats_new/v0.19.rst b/doc/whats_new/v0.19.rst
index c28199716528..a689f40aee4f 100644
--- a/doc/whats_new/v0.19.rst
+++ b/doc/whats_new/v0.19.rst
@@ -4,6 +4,23 @@
 
 .. _changes_0_19:
 
+Version 0.19.2
+==============
+
+**July, 2018**
+
+This release is exclusively in order to support Python 3.7.
+
+Related changes
+---------------
+
+- ``n_iter_`` may vary from previous releases in
+  :class:`linear_model.LogisticRegression` with ``solver='lbfgs'`` and
+  :class:`linear_model.HuberRegressor`.  For Scipy <= 1.0.0, the optimizer could
+  perform more than the requested maximum number of iterations. Now both
+  estimators will report at most ``max_iter`` iterations even if more were
+  performed. :issue:`10723` by `Joel Nothman`_.
+
 Version 0.19.1
 ==============
 
diff --git a/doc/whats_new/v0.20.rst b/doc/whats_new/v0.20.rst
index 237349d1a4d8..3a9e68801afa 100644
--- a/doc/whats_new/v0.20.rst
+++ b/doc/whats_new/v0.20.rst
@@ -9,7 +9,12 @@ Version 0.20 (under development)
 
 This release packs in a mountain of bug fixes, features and enhancements for
 the Scikit-learn library, and improvements to the documentation and examples.
-Thanks to our many contributors!
+Thanks to our contributors!
+
+.. warning::
+
+    Version 0.20 is the last version of scikit-learn to support Python 2.7 and Python 3.4.
+    Scikit-learn 0.21 will require Python 3.5 or higher.
 
 Highlights
 ----------
@@ -17,9 +22,8 @@ Highlights
 We have tried to improve our support for common data-science use-cases
 including missing values, categorical variables, heterogeneous data, and
 features/targets with unusual distributions.
-
 Missing values in features, represented by NaNs, are now accepted in
-column-wise preprocessing such as scalers.  Each feature is fitted disregarding
+column-wise preprocessing such as scalers. Each feature is fitted disregarding
 NaNs, and data containing NaNs can be transformed. The new :mod:`impute`
 module provides estimators for learning despite missing data.
 
@@ -31,8 +35,8 @@ String or pandas Categorical columns can now be encoded with
 
 :class:`~compose.TransformedTargetRegressor` helps when the regression target
 needs to be transformed to be modeled. :class:`~preprocessing.PowerTransformer`
-joins :class:`~preprocessing.QuantileTransformer` as a non-linear
-transformation.
+and :class:`~preprocessing.KBinsDiscretizer` join
+:class:`~preprocessing.QuantileTransformer` as non-linear transformations.
 
 Beyond this, we have added :term:`sample_weight` support to several estimators
 (including :class:`~cluster.KMeans`, :class:`~linear_model.BayesianRidge` and
@@ -41,13 +45,14 @@ Beyond this, we have added :term:`sample_weight` support to several estimators
 :class:`~ensemble.GradientBoostingRegressor` and
 :class:`~linear_model.SGDRegressor`).
 
-.. FIXME: remove SGDRegressor if #9043 is not merged for release
-
 This release is also the first to be accompanied by a :ref:`glossary` developed
 by `Joel Nothman`_. The glossary is a reference resource to help users and
 contributors become familiar with the terminology and conventions used in
 Scikit-learn.
 
+Sorry if your contribution didn't make it into the highlights. There's a lot
+here...
+
 Changed models
 --------------
 
@@ -57,15 +62,22 @@ occurs due to changes in the modelling logic (bug fixes or enhancements), or in
 random sampling procedures.
 
 - :class:`decomposition.IncrementalPCA` in Python 2 (bug fix)
+- :class:`decomposition.SparsePCA` (bug fix)
+- :class:`ensemble.GradientBoostingClassifier` (bug fix affecting feature importances)
 - :class:`isotonic.IsotonicRegression` (bug fix)
 - :class:`linear_model.ARDRegression` (bug fix)
+- :class:`linear_model.LogisticRegressionCV` (bug fix)
 - :class:`linear_model.OrthogonalMatchingPursuit` (bug fix)
+- :class:`linear_model.PassiveAggressiveClassifier` (bug fix)
+- :class:`linear_model.PassiveAggressiveRegressor` (bug fix)
+- :class:`linear_model.Perceptron` (bug fix)
+- :class:`linear_model.SGDClassifier` (bug fix)
+- :class:`linear_model.SGDRegressor` (bug fix)
 - :class:`metrics.roc_auc_score` (bug fix)
 - :class:`metrics.roc_curve` (bug fix)
-- :class:`neural_network.MLPRegressor` (bug fix)
-- :class:`neural_network.MLPClassifier` (bug fix)
 - :class:`neural_network.BaseMultilayerPerceptron` (bug fix)
-- :class:`ensemble.gradient_boosting.GradientBoostingClassifier` (bug fix affecting feature importances)
+- :class:`neural_network.MLPClassifier` (bug fix)
+- :class:`neural_network.MLPRegressor` (bug fix)
 - The v0.19.0 release notes failed to mention a backwards incompatibility with
   :class:`model_selection.StratifiedKFold` when ``shuffle=True`` due to
   :issue:`7823`.
@@ -75,619 +87,852 @@ Details are listed in the changelog below.
 (While we are trying to better inform users by providing this information, we
 cannot assure that this list is complete.)
 
-**Other backward incompatible change** The vendored version of the joblib
-module is now found at `sklearn.externals._joblib` (:issue:`11166`). The
-main API of joblib is still exposed in `sklearn.externals.joblib`, but
-code doing imports of subpackages of `sklearn.externals.joblib` will
-break.
-
 Changelog
 ---------
 
 Support for Python 3.3 has been officially dropped.
 
-New features
-............
 
-Classifiers and regressors
+:mod:`sklearn.cluster`
+......................
 
-- :class:`ensemble.GradientBoostingClassifier` and
-  :class:`ensemble.GradientBoostingRegressor` now support early stopping
-  via ``n_iter_no_change``, ``validation_fraction`` and ``tol``. :issue:`7071`
-  by `Raghav RV`_
+- |MajorFeature| A new clustering algorithm: :class:`cluster.OPTICS`: an
+  algoritm related to :class:`cluster.DBSCAN`, that has hyperparameters easier
+  to set and tat scales better, by :user:`Shane <espg>`.
 
-- :class:`dummy.DummyRegressor` now has a ``return_std`` option in its
-  ``predict`` method. The returned standard deviations will be zeros.
+- |MajorFeature| :class:`cluster.AgglomerativeClustering` now supports Single
+  Linkage clustering via ``linkage='single'``. :issue:`9372` by :user:`Leland
+  McInnes <lmcinnes>` and :user:`Steve Astels <sastels>`.
 
-- Added :class:`multioutput.RegressorChain` for multi-target
-  regression. :issue:`9257` by :user:`Kumar Ashutosh <thechargedneutron>`.
+- |Feature| :class:`cluster.KMeans` and :class:`cluster.MiniBatchKMeans` now support
+  sample weights via new parameter ``sample_weight`` in ``fit`` function.
+  :issue:`10933` by :user:`Johannes Hansen <jnhansen>`.
 
-- Added :class:`naive_bayes.ComplementNB`, which implements the Complement
-  Naive Bayes classifier described in Rennie et al. (2003).
-  :issue:`8190` by :user:`Michael A. Alcorn <airalcorn2>`.
+- |Efficiency| :class:`cluster.KMeans`, :class:`cluster.MiniBatchKMeans` and
+  :func:`cluster.k_means` passed with ``algorithm='full'`` now enforces
+  row-major ordering, improving runtime.
+  :issue:`10471` by :user:`Gaurav Dhingra <gxyd>`.
 
-- :class:`ensemble.BaggingRegressor` and :class:`ensemble.BaggingClassifier` can now
-  be fit with missing/non-finite values in X and/or multi-output Y to support
-  wrapping pipelines that perform their own imputation.
-  :issue:`9707` by :user:`Jimmy Wan <jimmywan>`.
+- |Efficiency| :class:`cluster.DBSCAN` now is parallelized according to ``n_jobs``
+  regardless of ``algorithm``.
+  :issue:`8003` by :user:`Joël Billaud <recamshak>`.
 
-Preprocessing
+- |Enhancement| :class:`cluster.KMeans` now gives a warning, if the number of
+  distinct clusters found is smaller than ``n_clusters``. This may occur when
+  the number of distinct points in the data set is actually smaller than the
+  number of cluster one is looking for.
+  :issue:`10059` by :user:`Christian Braune <christianbraune79>`.
 
-- Expanded :class:`preprocessing.OneHotEncoder` to allow to encode
-  categorical string features as a numeric array using a one-hot (or dummy)
-  encoding scheme, and added :class:`preprocessing.OrdinalEncoder` to
-  convert to ordinal integers.  Those two classes now handle
-  encoding of all feature types (also handles string-valued features) and
-  derives the categories based on the unique values in the features instead of
-  the maximum value in the features. :issue:`9151` and :issue:`10521` by
-  :user:`Vighnesh Birodkar <vighneshbirodkar>` and `Joris Van den Bossche`_.
+- |Fix| Fixed a bug where the ``fit`` method of
+  :class:`cluster.AffinityPropagation` stored cluster
+  centers as 3d array instead of 2d array in case of non-convergence. For the
+  same class, fixed undefined and arbitrary behavior in case of training data
+  where all samples had equal similarity.
+  :issue:`9612`. By :user:`Jonatan Samoocha <jsamoocha>`.
 
-- Added :class:`compose.ColumnTransformer`, which allows to apply
-  different transformers to different columns of arrays or pandas
-  DataFrames. :issue:`9012` by `Andreas Müller`_ and `Joris Van den Bossche`_,
-  and :issue:`11315` by :user:`Thomas Fan <thomasjpfan>`_.
+- |Fix| Fixed a bug in :func:`cluster.spectral_clustering` where the normalization of
+  the spectrum was using a division instead of a multiplication. :issue:`8129`
+  by :user:`Jan Margeta <jmargeta>`, :user:`Guillaume Lemaitre <glemaitre>`,
+  and :user:`Devansh D. <devanshdalal>`.
 
-- Added :class:`preprocessing.PowerTransformer`, which implements the Box-Cox
-  power transformation, allowing users to map data from any distribution to a
-  Gaussian distribution. This is useful as a variance-stabilizing transformation
-  in situations where normality and homoscedasticity are desirable.
-  :issue:`10210` by :user:`Eric Chang <ericchang00>` and
-  :user:`Maniteja Nandana <maniteja123>`.
+- |Fix| Fixed a bug in :func:`cluster.k_means_elkan` where the returned
+  `iteration` was 1 less than the correct value. Also added the missing
+  `n_iter_` attribute in the docstring of :class:`cluster.KMeans`.
+  :issue:`11353` by :user:`Jeremie du Boisberranger <jeremiedbb>`.
 
-- Added the :class:`compose.TransformedTargetRegressor` which transforms
-  the target y before fitting a regression model. The predictions are mapped
-  back to the original space via an inverse transform. :issue:`9041` by
-  `Andreas Müller`_ and :user:`Guillaume Lemaitre <glemaitre>`.
+- |API| Deprecate ``pooling_func`` unused parameter in
+  :class:`cluster.AgglomerativeClustering`.
+  :issue:`9875` by :user:`Kumar Ashutosh <thechargedneutron>`.
 
-- Added :class:`impute.ChainedImputer`, which is a strategy for imputing missing
-  values by modeling each feature with missing values as a function of
-  other features in a round-robin fashion. :issue:`8478` by
-  :user:`Sergey Feldman <sergeyf>`.
 
-Model evaluation
+:mod:`sklearn.compose`
+......................
 
-- Added the :func:`metrics.davies_bouldin_score` metric for unsupervised
-  evaluation of clustering models. :issue:`10827` by :user:`Luis Osa <logc>`.
+- New module.
 
-- Added the :func:`metrics.balanced_accuracy_score` metric and a corresponding
-  ``'balanced_accuracy'`` scorer for binary classification.
-  :issue:`8066` by :user:`xyguo` and :user:`Aman Dalmia <dalmia>`.
+- |MajorFeature| Added :class:`compose.ColumnTransformer`, which allows to
+  apply different transformers to different columns of arrays or pandas
+  DataFrames. :issue:`9012` by `Andreas Müller`_ and `Joris Van den Bossche`_,
+  and :issue:`11315` by :user:`Thomas Fan <thomasjpfan>`.
 
-Decomposition, manifold learning and clustering
+- |MajorFeature| Added the :class:`compose.TransformedTargetRegressor` which
+  transforms the target y before fitting a regression model. The predictions
+  are mapped back to the original space via an inverse transform. :issue:`9041`
+  by `Andreas Müller`_ and :user:`Guillaume Lemaitre <glemaitre>`.
 
-- :class:`cluster.AgglomerativeClustering` now supports Single Linkage
-  clustering via ``linkage='single'``. :issue:`9372` by
-  :user:`Leland McInnes <lmcinnes>` and :user:`Steve Astels <sastels>`.
 
-- :class:`cluster.KMeans` and :class:`cluster.MiniBatchKMeans` now support
-  sample weights via new parameter ``sample_weight`` in ``fit`` function.
-  :issue:`10933` by :user:`Johannes Hansen <jnhansen>`.
+:mod:`sklearn.covariance`
+.........................
 
-- :mod:`dict_learning` functions and models now support positivity constraints.
-  This applies to the dictionary and sparse code.
-  :issue:`6374` by :user:`John Kirkham <jakirkham>`.
+- |Efficiency| Runtime improvements to :class:`covariance.GraphicalLasso`.
+  :issue:`9858` by :user:`Steven Brown <stevendbrown>`.
 
-Metrics
+- |API| The :func:`covariance.graph_lasso`,
+  :class:`covariance.GraphLasso` and :class:`covariance.GraphLassoCV` have been
+  renamed to :func:`covariance.graphical_lasso`,
+  :class:`covariance.GraphicalLasso` and :class:`covariance.GraphicalLassoCV`
+  respectively and will be removed in version 0.22.
+  :issue:`9993` by :user:`Artiem Krinitsyn <artiemq>`
 
-- Partial AUC is available via ``max_fpr`` parameter in
-  :func:`metrics.roc_auc_score`. :issue:`3273` by
-  :user:`Alexander Niederbühl <Alexander-N>`.
 
-- Added ``output_dict`` parameter in :func:`metrics.classification_report`
-  to return classification statistics as dictionary.
-  :issue:`11160` by :user:`Dan Barkhorn <danielbarkhorn>`.
+:mod:`sklearn.datasets`
+.......................
 
-Misc
+- |MajorFeature| Added :func:`datasets.fetch_openml` to fetch datasets from
+  `OpenML <http://openml.org>`. OpenML is a free, open data sharing platform
+  and will be used instead of mldata as it provides better service availability.
+  :issue:`9908` by `Andreas Müller`_ and :user:`Jan N. van Rijn <janvanrijn>`.
 
-- A new configuration parameter, ``working_memory`` was added to control memory
-  consumption limits in chunked operations, such as the new
-  :func:`metrics.pairwise_distances_chunked`.  See :ref:`working_memory`.
-  :issue:`10280` by `Joel Nothman`_ and :user:`Aman Dalmia <dalmia>`.
+- |Feature| In :func:`datasets.make_blobs`, one can now pass a list to the
+  `n_samples` parameter to indicate the number of samples to generate per
+  cluster. :issue:`8617` by :user:`Maskani Filali Mohamed <maskani-moh>` and
+  :user:`Konstantinos Katrioplas <kkatrio>`.
 
-- An environment variable to use the site joblib instead of the vendored
-  one was added (:ref:`environment_variable`).
-  :issue:`11166`by `Gael Varoquaux`_
+- |Feature| Add ``filename`` attribute to :mod:`datasets` that have a CSV file.
+  :issue:`9101` by :user:`alex-33 <alex-33>`
+  and :user:`Maskani Filali Mohamed <maskani-moh>`.
 
-Enhancements
-............
+- |Feature| ``return_X_y`` parameter has been added to several dataset loaders.
+  :issue:`10774` by :user:`Chris Catalfo <ccatalfo>`.
 
-Classifiers and regressors
+- |Fix| Fixed a bug in :func:`datasets.load_boston` which had a wrong data
+  point. :issue:`10795` by :user:`Takeshi Yoshizawa <tarcusx>`.
 
-- In :class:`gaussian_process.GaussianProcessRegressor`, method ``predict``
-  is faster when using ``return_std=True`` in particular more when called
-  several times in a row. :issue:`9234` by :user:`andrewww <andrewww>`
-  and :user:`Minghui Liu <minghui-liu>`.
+- |Fix| Fixed a bug in :func:`datasets.load_iris` which had two wrong data points.
+  :issue:`11082` by :user:`Sadhana Srinivasan <rotuna>`
+  and :user:`Hanmin Qin <qinhanmin2014>`.
 
-- Add `named_estimators_` parameter in
-  :class:`ensemble.VotingClassifier` to access fitted
-  estimators. :issue:`9157` by :user:`Herilalaina Rakotoarison <herilalaina>`.
+- |Fix| Fixed a bug in :func:`datasets.fetch_kddcup99`, where data were not
+  properly shuffled. :issue:`9731` by `Nicolas Goix`_.
 
-- Add `var_smoothing` parameter in
-  :class:`naive_bayes.GaussianNB` to give a precise control over
-  variances calculation. :issue:`9681` by :user:`Dmitry Mottl <Mottl>`.
+- |Fix| Fixed a bug in :func:`datasets.make_circles`, where no odd number of
+  data points could be generated. :issue:`10045` by :user:`Christian Braune
+  <christianbraune79>`.
 
-- Add `n_iter_no_change` parameter in
-  :class:`neural_network.BaseMultilayerPerceptron`,
-  :class:`neural_network.MLPRegressor`, and
-  :class:`neural_network.MLPClassifier` to give control over
-  maximum number of epochs to not meet ``tol`` improvement.
-  :issue:`9456` by :user:`Nicholas Nadeau <nnadeau>`.
+- |API| Deprecated :func:`sklearn.datasets.fetch_mldata` to be removed in
+  version 0.22. mldata.org is no longer operational. Until removal it will
+  remain possible to load cached datasets. :issue:`11466` by `Joel Nothman`_.
 
-- A parameter ``check_inverse`` was added to
-  :class:`preprocessing.FunctionTransformer` to ensure that ``func`` and
-  ``inverse_func`` are the inverse of each other.
-  :issue:`9399` by :user:`Guillaume Lemaitre <glemaitre>`.
+:mod:`sklearn.decomposition`
+............................
 
-- Add `sample_weight` parameter to the fit method of
-  :class:`linear_model.BayesianRidge` for weighted linear regression.
-  :issue:`10111` by :user:`Peter St. John <pstjohn>`.
+- |Feature| :func:`decomposition.dict_learning` functions and models now
+  support positivity constraints. This applies to the dictionary and sparse
+  code. :issue:`6374` by :user:`John Kirkham <jakirkham>`.
 
-- :class:`dummy.DummyClassifier` and :class:`dummy.DummyRegressor` now
-  only require X to be an object with finite length or shape.
-  :issue:`9832` by :user:`Vrishank Bhardwaj <vrishank97>`.
+- |Feature| |Fix| :class:`decomposition.SparsePCA` now exposes
+  ``normalize_components``. When set to True, the train and test data are
+  centered with the train mean repsectively during the fit phase and the
+  transform phase. This fixes the behavior of SparsePCA. When set to False,
+  which is the default, the previous abnormal behaviour still holds. The False
+  value is for backward compatibility and should not be used. :issue:`11585`
+  by :user:`Ivan Panico <FollowKenny>`.
 
-- Add `sample_weight` parameter to the fit method of
-  :class:`neighbors.KernelDensity` to enables weighting in kernel density
-  estimation.
-  :issue:`4394` by :user:`Samuel O. Ronsin <samronsin>`.
+- |Efficiency| Efficiency improvements in :func:`decomposition.dict_learning`.
+  :issue:`11420` and others by :user:`John Kirkham <jakirkham>`.
 
-- :class:`neighbors.RadiusNeighborsRegressor` and
-  :class:`neighbors.RadiusNeighborsClassifier` are now
-  parallelized according to ``n_jobs`` regardless of ``algorithm``.
-  :issue:`8003` by :user:`Joël Billaud <recamshak>`.
+- |Fix| Fix for uninformative error in :class:`decomposition.IncrementalPCA`:
+  now an error is raised if the number of components is larger than the
+  chosen batch size. The ``n_components=None`` case was adapted accordingly.
+  :issue:`6452`. By :user:`Wally Gauze <wallygauze>`.
 
-- Memory usage improvement for :func:`_class_means` and :func:`_class_cov`
-  in :class:`discriminant_analysis`.
-  :issue:`10898` by :user:`Nanxin Chen <bobchennan>`.`
+- |Fix| Fixed a bug where the ``partial_fit`` method of
+  :class:`decomposition.IncrementalPCA` used integer division instead of float
+  division on Python 2.
+  :issue:`9492` by :user:`James Bourbeau <jrbourbeau>`.
+
+- |Fix| In :class:`decomposition.PCA` selecting a n_components parameter greater
+  than the number of samples now raises an error. Similarly, the
+  ``n_components=None`` case now selects the minimum of n_samples and
+  n_features.
+  :issue:`8484` by :user:`Wally Gauze <wallygauze>`.
+
+- |Fix| Fixed a bug in :class:`decomposition.PCA` where users will get
+  unexpected error with large datasets when ``n_components='mle'`` on Python 3
+  versions.
+  :issue:`9886` by :user:`Hanmin Qin <qinhanmin2014>`.
 
-- :func:`manifold.t_sne.trustworthiness` accepts metrics other than
-  Euclidean. :issue:`9775` by :user:`William de Vazelhes <wdevazelhes>`.
+- |Fix| Fixed an underflow in calculating KL-divergence for
+  :class:`decomposition.NMF` :issue:`10142` by `Tom Dupre la Tour`_.
 
-- :mod:`Nearest neighbors <neighbors>` query methods are now more memory
-  efficient when ``algorithm='brute'``. :issue:`11136` by `Joel Nothman`_
-  and :user:`Aman Dalmia <dalmia>`.
+- |Fix| Fixed a bug in :class:`decomposition.SparseCoder` when running OMP
+  sparse coding in parallel using readonly memory mapped datastructures.
+  :issue:`5956` by :user:`Vighnesh Birodkar <vighneshbirodkar>` and
+  :user:`Olivier Grisel <ogrisel>`.
 
-Cluster
 
-- :class:`cluster.KMeans`, :class:`cluster.MiniBatchKMeans` and
-  :func:`cluster.k_means` passed with ``algorithm='full'`` now enforces
-  row-major ordering, improving runtime.
-  :issue:`10471` by :user:`Gaurav Dhingra <gxyd>`.
+:mod:`sklearn.discriminant_analysis`
+....................................
 
-- :class:`cluster.DBSCAN` now is parallelized according to ``n_jobs``
-  regardless of ``algorithm``.
-  :issue:`8003` by :user:`Joël Billaud <recamshak>`.
+- |Efficiency| Memory usage improvement for :func:`_class_means` and
+  :func:`_class_cov` in :mod:`discriminant_analysis`. :issue:`10898` by
+  :user:`Nanxin Chen <bobchennan>`.`
 
-Datasets
 
-- In :func:`datasets.make_blobs`, one can now pass a list to the `n_samples`
-  parameter to indicate the number of samples to generate per cluster.
-  :issue:`8617` by :user:`Maskani Filali Mohamed <maskani-moh>`
-  and :user:`Konstantinos Katrioplas <kkatrio>`.
+:mod:`sklearn.dummy`
+....................
 
-Preprocessing
+- |Feature| :class:`dummy.DummyRegressor` now has a ``return_std`` option in its
+  ``predict`` method. The returned standard deviations will be zeros.
 
-- :class:`preprocessing.PolynomialFeatures` now supports sparse input.
-  :issue:`10452` by :user:`Aman Dalmia <dalmia>` and `Joel Nothman`_.
+- |Feature| :class:`dummy.DummyClassifier` and :class:`dummy.DummyRegressor` now
+  only require X to be an object with finite length or shape. :issue:`9832` by
+  :user:`Vrishank Bhardwaj <vrishank97>`.
 
-- Enable the call to :meth:`get_feature_names` in unfitted
-  :class:`feature_extraction.text.CountVectorizer` initialized with a
-  vocabulary. :issue:`10908` by :user:`Mohamed Maskani <maskani-moh>`.
 
-- The ``transform`` method of :class:`sklearn.preprocessing.MultiLabelBinarizer`
-  now ignores any unknown classes. A warning is raised stating the unknown classes
-  classes found which are ignored.
-  :issue:`10913` by :user:`Rodrigo Agundez <rragundez>`.
+:mod:`sklearn.ensemble`
+.......................
 
-- :class:`preprocessing.QuantileTransformer` handles and ignores NaN values.
-  :issue:`10404` by :user:`Guillaume Lemaitre <glemaitre>`.
+- |Feature| :class:`ensemble.BaggingRegressor` and
+  :class:`ensemble.BaggingClassifier` can now be fit with missing/non-finite
+  values in X and/or multi-output Y to support wrapping pipelines that perform
+  their own imputation. :issue:`9707` by :user:`Jimmy Wan <jimmywan>`.
 
-- Updated :class:`preprocessing.MinMaxScaler` and
-  :func:`preprocessing.minmax_scale` to pass through NaN values.
-  :issue:`10404` and :issue:`11243` by :user:`Lucija Gregov <LucijaGregov>` and
-  :user:`Guillaume Lemaitre <glemaitre>`.
+- |Feature| :class:`ensemble.GradientBoostingClassifier` and
+  :class:`ensemble.GradientBoostingRegressor` now support early stopping
+  via ``n_iter_no_change``, ``validation_fraction`` and ``tol``. :issue:`7071`
+  by `Raghav RV`_
 
-- :class:`preprocessing.StandardScaler` and :func:`preprocessing.scale`
-  ignore and pass-through NaN values.
-  :issue:`11206` by :user:`Guillaume Lemaitre <glemaitre>`.
+- |Feature| Add `named_estimators_` parameter in
+  :class:`ensemble.VotingClassifier` to access fitted estimators.
+  :issue:`9157` by :user:`Herilalaina Rakotoarison <herilalaina>`.
 
-- :class:`preprocessing.MaxAbsScaler` and :func:`preprocessing.maxabs_scale`
-  handles and ignores NaN values.
-  :issue:`11011` by `Lucija Gregov <LucihaGregov>` and
-  :user:`Guillaume Lemaitre <glemaitre>`
+- |Fix| Fixed a bug when fitting :class:`ensemble.GradientBoostingClassifier` or
+  :class:`ensemble.GradientBoostingRegressor` with ``warm_start=True`` which
+  previously raised a segmentation fault due to a non-conversion of CSC matrix
+  into CSR format expected by ``decision_function``. Similarly, Fortran-ordered
+  arrays are converted to C-ordered arrays in the dense case. :issue:`9991` by
+  :user:`Guillaume Lemaitre <glemaitre>`.
 
-- :class:`preprocessing.PowerTransformer` and
-  :func:`preprocessing.power_transform` ignore and pass-through NaN values.
-  :issue:`11306` by :user:`Guillaume Lemaitre <glemaitre>`.
+- |Fix| Fixed a bug in :class:`ensemble.GradientBoostingRegressor`
+  and :class:`ensemble.GradientBoostingClassifier` to have
+  feature importances summed and then normalized, rather than normalizing on a
+  per-tree basis. The previous behavior over-weighted the Gini importance of
+  features that appear in later stages. This issue only affected feature
+  importances. :issue:`11176` by :user:`Gil Forsyth <gforsyth>`.
 
-Model evaluation and meta-estimators
+- |API| The default value of the ``n_estimators`` parameter of
+  :class:`ensemble.RandomForestClassifier`, :class:`ensemble.RandomForestRegressor`,
+  :class:`ensemble.ExtraTreesClassifier`, :class:`ensemble.ExtraTreesRegressor`,
+  and :class:`ensemble.RandomTreesEmbedding` will change from 10 in version 0.20
+  to 100 in 0.22. A FutureWarning is raised when the default value is used.
+  :issue:`11542` by :user:`Anna Ayzenshtat <annaayzenshtat>`.
 
-- A scorer based on :func:`metrics.brier_score_loss` is also available.
-  :issue:`9521` by :user:`Hanmin Qin <qinhanmin2014>`.
+- |API| Classes derived from :class:`ensemble.BaseBagging`. The attribute
+  ``estimators_samples_`` will return a list of arrays containing the indices
+  selected for each bootstrap instead of a list of arrays containing the mask
+  of the samples selected for each bootstrap. Indices allows to repeat samples
+  while mask does not allow this functionality.
+  :issue:`9524` by :user:`Guillaume Lemaitre <glemaitre>`.
 
-- The default of ``iid`` parameter of :class:`model_selection.GridSearchCV`
-  and :class:`model_selection.RandomizedSearchCV` will change from ``True`` to
-  ``False`` in version 0.22 to correspond to the standard definition of
-  cross-validation, and the parameter will be removed in version 0.24
-  altogether. This parameter is of greatest practical significance where the
-  sizes of different test sets in cross-validation were very unequal, i.e. in
-  group-based CV strategies. :issue:`9085` by :user:`Laurent Direr <ldirer>`
-  and `Andreas Müller`_.
+- |Fix| :class:`ensemble.BaseBagging` where one could not deterministically
+  reproduce ``fit`` result using the object attributes when ``random_state``
+  is set. :issue:`9723` by :user:`Guillaume Lemaitre <glemaitre>`.
 
-- The ``predict`` method of :class:`pipeline.Pipeline` now passes keyword
-  arguments on to the pipeline's last estimator, enabling the use of parameters
-  such as ``return_std`` in a pipeline with caution.
-  :issue:`9304` by :user:`Breno Freitas <brenolf>`.
 
-- Add `return_estimator` parameter in :func:`model_selection.cross_validate` to
-  return estimators fitted on each split.
-  :issue:`9686` by :user:`Aurélien Bellet <bellet>`.
+:mod:`sklearn.feature_extraction`
+.................................
 
-- New ``refit_time_`` attribute will be stored in
-  :class:`model_selection.GridSearchCV` and
-  :class:`model_selection.RandomizedSearchCV` if ``refit`` is set to ``True``.
-  This will allow measuring the complete time it takes to perform
-  hyperparameter optimization and refitting the best model on the whole
-  dataset. :issue:`11310` by :user:`Matthias Feurer <mfeurer>`.
+- |Feature| Enable the call to :term:`get_feature_names` in unfitted
+  :class:`feature_extraction.text.CountVectorizer` initialized with a
+  vocabulary. :issue:`10908` by :user:`Mohamed Maskani <maskani-moh>`.
 
-Decomposition and manifold learning
+- |Enhancement| ``idf_`` can now be set on a
+  :class:`feature_extraction.text.TfidfTransformer`.
+  :issue:`10899` by :user:`Sergey Melderis <serega>`.
 
-- Speed improvements for both 'exact' and 'barnes_hut' methods in
-  :class:`manifold.TSNE`. :issue:`10593` and :issue:`10610` by
-  `Tom Dupre la Tour`_.
+- |Fix| Fixed a bug in :func:`feature_extraction.image.extract_patches_2d` which
+  would throw an exception if ``max_patches`` was greater than or equal to the
+  number of all possible patches rather than simply returning the number of
+  possible patches. :issue:`10101` by :user:`Varun Agrawal <varunagrawal>`
 
-- Support sparse input in :meth:`manifold.Isomap.fit`. :issue:`8554` by
-  :user:`Leland McInnes <lmcinnes>`.
+- |Fix| Fixed a bug in :class:`feature_extraction.text.CountVectorizer`,
+  :class:`feature_extraction.text.TfidfVectorizer`,
+  :class:`feature_extraction.text.HashingVectorizer` to support 64 bit sparse
+  array indexing necessary to process large datasets with more than 2·10⁹ tokens
+  (words or n-grams). :issue:`9147` by :user:`Claes-Fredrik Mannby <mannby>`
+  and `Roman Yurchak`_.
 
-Metrics
+- |Fix| Fixed bug in :class:`feature_extraction.text.TfidfVectorizer` which
+  was ignoring the parameter ``dtype``. In addition,
+  :class:`feature_extraction.text.TfidfTransformer` will preserve ``dtype``
+  for floating and raise a warning if ``dtype`` requested is integer.
+  :issue:`10441` by :user:`Mayur Kulkarni <maykulkarni>` and
+  :user:`Guillaume Lemaitre <glemaitre>`.
 
-- :func:`metrics.roc_auc_score` now supports binary ``y_true`` other than
-  ``{0, 1}`` or ``{-1, 1}``.
-  :issue:`9828` by :user:`Hanmin Qin <qinhanmin2014>`.
 
-- :func:`metrics.label_ranking_average_precision_score` now supports vector
-  ``sample_weight``.
-  :issue:`10845` by :user:`Jose Perez-Parras Toledano <jopepato>`.
+:mod:`sklearn.feature_selection`
+................................
 
-- Add ``dense_output`` parameter to :func:`metrics.pairwise.linear_kernel`.
-  When False and both inputs are sparse, will return a sparse matrix.
-  :issue:`10999` by :user:`Taylor G Smith <tgsmith61591>`.
+- |Feature| Added select K best features functionality to
+  :class:`feature_selection.SelectFromModel`.
+  :issue:`6689` by :user:`Nihar Sheth <nsheth12>` and
+  :user:`Quazi Rahman <qmaruf>`.
 
-- :func:`metrics.cluster.silhouette_score` and
-  :func:`metrics.cluster.silhouette_samples` are more memory efficient and run
-  faster. This avoids some reported freezes and MemoryErrors.
-  :issue:`11135` by `Joel Nothman`_.
+- |Feature| Added ``min_features_to_select`` parameter to
+  :class:`feature_selection.RFECV` to bound evaluated features counts.
+  :issue:`11293` by :user:`Brent Yi <brentyi>`.
 
-Linear, kernelized and related models
+- |Feature| :class:`feature_selection.RFECV`'s fit method now supports
+  :term:`groups`.  :issue:`9656` by :user:`Adam Greenhall <adamgreenhall>`.
 
-- Deprecate ``random_state`` parameter in :class:`svm.OneClassSVM` as the
-  underlying implementation is not random.
-  :issue:`9497` by :user:`Albert Thomas <albertcthomas>`.
+- |Fix| Fixed computation of ``n_features_to_compute`` for edge case with tied
+  CV scores in :class:`feature_selection.RFECV`.
+  :issue:`9222` by :user:`Nick Hoh <nickypie>`.
 
-Decomposition, manifold learning and clustering
+:mod:`sklearn.gaussian_process`
+...............................
 
-- Deprecate ``precomputed`` parameter in function
-  :func:`manifold.t_sne.trustworthiness`. Instead, the new parameter
-  ``metric`` should be used with any compatible metric including
-  'precomputed', in which case the input matrix ``X`` should be a matrix of
-  pairwise distances or squared distances. :issue:`9775` by
-  :user:`William de Vazelhes <wdevazelhes>`.
+- |Efficiency| In :class:`gaussian_process.GaussianProcessRegressor`, method
+  ``predict`` is faster when using ``return_std=True`` in particular more when
+  called several times in a row. :issue:`9234` by :user:`andrewww <andrewww>`
+  and :user:`Minghui Liu <minghui-liu>`.
 
-Utils
 
-- Avoid copying the data in :func:`utils.check_array` when the input data is a
-  memmap (and ``copy=False``). :issue:`10663` by :user:`Arthur Mensch
-  <arthurmensch>` and :user:`Loïc Estève <lesteve>`.
+:mod:`sklearn.impute`
+.....................
 
-Miscellaneous
+- New module, adopting ``preprocessing.Imputer`` as
+  :class:`impute.SimpleImputer` with minor changes (see under preprocessing
+  below).
 
-- Add ``filename`` attribute to datasets that have a CSV file.
-  :issue:`9101` by :user:`alex-33 <alex-33>`
-  and :user:`Maskani Filali Mohamed <maskani-moh>`.
+- |MajorFeature| Added :class:`impute.MissingIndicator` which generates a
+  binary indicator for missing values. :issue:`8075` by :user:`Maniteja Nandana
+  <maniteja123>` and :user:`Guillaume Lemaitre <glemaitre>`.
+
+- |Feature| The :class:`impute.SimpleImputer` has a new strategy,
+  ``'constant'``, to complete missing values with a fixed one, given by the
+  ``fill_value`` parameter. This strategy supports numeric and non-numeric
+  data, and so does the ``'most_frequent'`` strategy now. :issue:`11211` by
+  :user:`Jeremie du Boisberranger <jeremiedbb>`.
 
-Bug fixes
-.........
 
-Classifiers and regressors
+:mod:`sklearn.isotonic`
+.......................
 
-- Fixed a bug in :class:`isotonic.IsotonicRegression` which incorrectly
+- |Fix| Fixed a bug in :class:`isotonic.IsotonicRegression` which incorrectly
   combined weights when fitting a model to data involving points with
   identical X values.
-  :issue:`9432` by :user:`Dallas Card <dallascard>`
+  :issue:`9484` by :user:`Dallas Card <dallascard>`
 
-- Fixed a bug in :class:`neural_network.BaseMultilayerPerceptron`,
-  :class:`neural_network.MLPRegressor`, and
-  :class:`neural_network.MLPClassifier` with new ``n_iter_no_change``
-  parameter now at 10 from previously hardcoded 2.
-  :issue:`9456` by :user:`Nicholas Nadeau <nnadeau>`.
 
-- Fixed a bug in :class:`neural_network.MLPRegressor` where fitting
-  quit unexpectedly early due to local minima or fluctuations.
-  :issue:`9456` by :user:`Nicholas Nadeau <nnadeau>`
+:mod:`sklearn.linear_model`
+...........................
 
-- Fixed a bug in :class:`naive_bayes.GaussianNB` which incorrectly raised
-  error for prior list which summed to 1.
-  :issue:`10005` by :user:`Gaurav Dhingra <gxyd>`.
+- |Feature| :class:`linear_model.SGDClassifier`,
+  :class:`linear_model.SGDRegressor`,
+  :class:`linear_model.PassiveAggressiveClassifier`,
+  :class:`linear_model.PassiveAggressiveRegressor` and
+  :class:`linear_model.Perceptron` now expose ``early_stopping``,
+  ``validation_fraction`` and ``n_iter_no_change`` parameters, to stop
+  optimization monitoring the score on a validation set. A new learning rate
+  ``"adaptive"`` strategy divides the learning rate by 5 each time
+  ``n_iter_no_change`` consecutive epochs fail to improve the model.
+  :issue:`9043` by `Tom Dupre la Tour`_.
+
+- |Feature| Add `sample_weight` parameter to the fit method of
+  :class:`linear_model.BayesianRidge` for weighted linear regression.
+  :issue:`10112` by :user:`Peter St. John <pstjohn>`.
 
-- Fixed a bug in :class:`linear_model.LogisticRegression` where when using the
-  parameter ``multi_class='multinomial'``, the ``predict_proba`` method was
+- |Fix| Fixed a bug in :func:`logistic.logistic_regression_path` to ensure
+  that the returned coefficients are correct when ``multiclass='multinomial'``.
+  Previously, some of the coefficients would override each other, leading to
+  incorrect results in :class:`linear_model.LogisticRegressionCV`.
+  :issue:`11724` by :user:`Nicolas Hug <NicolasHug>`.
+
+- |Fix| Fixed a bug in :class:`linear_model.LogisticRegression` where when using
+  the parameter ``multi_class='multinomial'``, the ``predict_proba`` method was
   returning incorrect probabilities in the case of binary outcomes.
   :issue:`9939` by :user:`Roger Westover <rwolst>`.
 
-- Fixed a bug in :class:`linear_model.LogisticRegressionCV` where the
+- |Fix| Fixed a bug in :class:`linear_model.LogisticRegressionCV` where the
   ``score`` method always computes accuracy, not the metric given by
   the ``scoring`` parameter.
   :issue:`10998` by :user:`Thomas Fan <thomasjpfan>`.
 
-- Fixed a bug in :class:`linear_model.OrthogonalMatchingPursuit` that was
+- |Fix| Fixed a bug in :class:`linear_model.LogisticRegressionCV` where the
+  'ovr' strategy was always used to compute cross-validation scores in the
+  multiclass setting, even if 'multinomial' was set.
+  :issue:`8720` by :user:`William de Vazelhes <wdevazelhes>`.
+
+- |Fix| Fixed a bug in :class:`linear_model.OrthogonalMatchingPursuit` that was
   broken when setting ``normalize=False``.
   :issue:`10071` by `Alexandre Gramfort`_.
 
-- Fixed a bug in :class:`linear_model.ARDRegression` which caused incorrectly
-  updated estimates for the standard deviation and the coefficients.
-  :issue:`10153` by :user:`Jörg Döpfert <jdoepfert>`.
-
-- Fixed a bug when fitting :class:`ensemble.GradientBoostingClassifier` or
-  :class:`ensemble.GradientBoostingRegressor` with ``warm_start=True`` which
-  previously raised a segmentation fault due to a non-conversion of CSC matrix
-  into CSR format expected by ``decision_function``. Similarly, Fortran-ordered
-  arrays are converted to C-ordered arrays in the dense case. :issue:`9991` by
-  :user:`Guillaume Lemaitre <glemaitre>`.
+- |Fix| Fixed a bug in :class:`linear_model.ARDRegression` which caused
+  incorrectly updated estimates for the standard deviation and the
+  coefficients. :issue:`10153` by :user:`Jörg Döpfert <jdoepfert>`.
 
-- Fixed a bug in :class:`neighbors.NearestNeighbors` where fitting a
-  NearestNeighbors model fails when a) the distance metric used is a
-  callable and b) the input to the NearestNeighbors model is sparse.
-  :issue:`9579` by :user:`Thomas Kober <tttthomasssss>`.
+- |Fix| Fixed a bug in :class:`linear_model.ARDRegression` and
+  :class:`linear_model.BayesianRidge` which caused NaN predictions when fitted
+  with a constant target.
+  :issue:`10095` by :user:`Jörg Döpfert <jdoepfert>`.
 
-- Fixed a bug in :class:`linear_model.RidgeClassifierCV` where
+- |Fix| Fixed a bug in :class:`linear_model.RidgeClassifierCV` where
   the parameter ``store_cv_values`` was not implemented though
   it was documented in ``cv_values`` as a way to set up the storage
   of cross-validation values for different alphas. :issue:`10297` by
   :user:`Mabel Villalba-Jiménez <mabelvj>`.
 
-- Fixed a bug in :class:`naive_bayes.MultinomialNB` which did not accept vector
-  valued pseudocounts (alpha).
-  :issue:`10346` by :user:`Tobias Madsen <TobiasMadsen>`
-
-- Fixed a bug in :class:`svm.SVC` where when the argument ``kernel`` is
-  unicode in Python2, the ``predict_proba`` method was raising an
-  unexpected TypeError given dense inputs.
-  :issue:`10412` by :user:`Jiongyan Zhang <qmick>`.
-
-- Fixed a bug in :class:`tree.BaseDecisionTree` with `splitter="best"`
-  where split threshold could become infinite when values in X were
-  near infinite. :issue:`10536` by :user:`Jonathan Ohayon <Johayon>`.
-
-- Fixed a bug in :class:`linear_model.ElasticNet` which caused the input to be
-  overridden when using parameter ``copy_X=True`` and ``check_input=False``.
-  :issue:`10581` by :user:`Yacine Mazari <ymazari>`.
+- |Fix| Fixed a bug in :class:`linear_model.ElasticNet` which caused the input
+  to be overridden when using parameter ``copy_X=True`` and
+  ``check_input=False``. :issue:`10581` by :user:`Yacine Mazari <ymazari>`.
 
-- Fixed a bug in :class:`sklearn.linear_model.Lasso`
+- |Fix| Fixed a bug in :class:`sklearn.linear_model.Lasso`
   where the coefficient had wrong shape when ``fit_intercept=False``.
   :issue:`10687` by :user:`Martin Hahn <martin-hahn>`.
-  
-- Fixed a bug in :func:`sklearn.linear_model.LogisticRegression` where the 
+
+- |Fix| Fixed a bug in :func:`sklearn.linear_model.LogisticRegression` where the
   multi_class='multinomial' with binary output with warm_start = True
   :issue:`10836` by :user:`Aishwarya Srinivasan <aishgrt1>`.
 
-- Fixed a bug in :class:`linear_model.RidgeCV` where using integer ``alphas``
-  raised an error. :issue:`10393` by :user:`Mabel Villalba-Jiménez <mabelvj>`.
-
-- Fixed condition triggering gap computation in :class:`linear_model.Lasso`
-  and :class:`linear_model.ElasticNet` when working with sparse matrices.
-  :issue:`10992` by `Alexandre Gramfort`_.
-
-- Fixed a bug where liblinear and libsvm-based estimators would segfault if
-  passed a scipy.sparse matrix with 64-bit indices. They now raise a
+- |Fix| Fixed a bug in :class:`linear_model.RidgeCV` where using integer
+  ``alphas`` raised an error.
+  :issue:`10397` by :user:`Mabel Villalba-Jiménez <mabelvj>`.
+
+- |Fix| Fixed condition triggering gap computation in
+  :class:`linear_model.Lasso` and :class:`linear_model.ElasticNet` when working
+  with sparse matrices. :issue:`10992` by `Alexandre Gramfort`_.
+
+- |Fix| Fixed a bug in :class:`linear_model.SGDClassifier`,
+  :class:`linear_model.SGDRegressor`,
+  :class:`linear_model.PassiveAggressiveClassifier`,
+  :class:`linear_model.PassiveAggressiveRegressor` and
+  :class:`linear_model.Perceptron`, where the stopping criterion was stopping
+  the algorithm before convergence. A parameter `n_iter_no_change` was added
+  and set by default to 5. Previous behavior is equivalent to setting the
+  parameter to 1. :issue:`9043` by `Tom Dupre la Tour`_.
+
+- |Fix| Fixed a bug where liblinear and libsvm-based estimators would segfault
+  if passed a scipy.sparse matrix with 64-bit indices. They now raise a
   ValueError.
   :issue:`11327` by :user:`Karan Dhingra <kdhingra307>` and `Joel Nothman`_.
 
-- Fixed a bug in :class:`ensemble.gradient_boosting.GradientBoostingRegressor`
-  and :class:`ensemble.gradient_boosting.GradientBoostingClassifier` to have
-  feature importances summed and then normalized, rather than normalizing on a
-  per-tree basis. The previous behavior over-weighted the Gini importance of
-  features that appear in later stages. This issue only affected feature
-  importances. :issue:`11176` by :user:`Gil Forsyth <gforsyth>`.
+- |API| Deprecate ``positive=True`` option in :class:`linear_model.Lars` as
+  the underlying implementation is broken. Use :class:`linear_model.Lasso`
+  instead. :issue:`9837` by `Alexandre Gramfort`_.
 
-Decomposition, manifold learning and clustering
+- |API| ``n_iter_`` may vary from previous releases in
+  :class:`linear_model.LogisticRegression` with ``solver='lbfgs'`` and
+  :class:`linear_model.HuberRegressor`. For Scipy <= 1.0.0, the optimizer could
+  perform more than the requested maximum number of iterations. Now both
+  estimators will report at most ``max_iter`` iterations even if more were
+  performed. :issue:`10723` by `Joel Nothman`_.
 
-- Fix for uninformative error in :class:`decomposition.IncrementalPCA`:
-  now an error is raised if the number of components is larger than the
-  chosen batch size. The ``n_components=None`` case was adapted accordingly.
-  :issue:`6452`. By :user:`Wally Gauze <wallygauze>`.
 
-- Fixed a bug where the ``partial_fit`` method of
-  :class:`decomposition.IncrementalPCA` used integer division instead of float
-  division on Python 2 versions. :issue:`9492` by
-  :user:`James Bourbeau <jrbourbeau>`.
+:mod:`sklearn.manifold`
+.......................
 
-- Fixed a bug where the ``fit`` method of
-  :class:`cluster.AffinityPropagation` stored cluster
-  centers as 3d array instead of 2d array in case of non-convergence. For the
-  same class, fixed undefined and arbitrary behavior in case of training data
-  where all samples had equal similarity.
-  :issue:`9612`. By :user:`Jonatan Samoocha <jsamoocha>`.
+- |Efficiency| Speed improvements for both 'exact' and 'barnes_hut' methods in
+  :class:`manifold.TSNE`. :issue:`10593` and :issue:`10610` by
+  `Tom Dupre la Tour`_.
 
-- In :class:`decomposition.PCA` selecting a n_components parameter greater than
-  the number of samples now raises an error.
-  Similarly, the ``n_components=None`` case now selects the minimum of
-  n_samples and n_features. :issue:`8484`. By :user:`Wally Gauze <wallygauze>`.
+- |Feature| Support sparse input in :meth:`manifold.Isomap.fit`.
+  :issue:`8554` by :user:`Leland McInnes <lmcinnes>`.
 
-- Fixed a bug in :func:`datasets.fetch_kddcup99`, where data were not properly
-  shuffled. :issue:`9731` by `Nicolas Goix`_.
+- |Feature| :func:`manifold.t_sne.trustworthiness` accepts metrics other than
+  Euclidean. :issue:`9775` by :user:`William de Vazelhes <wdevazelhes>`.
 
-- Fixed a bug in :class:`decomposition.PCA` where users will get unexpected error
-  with large datasets when ``n_components='mle'`` on Python 3 versions.
-  :issue:`9886` by :user:`Hanmin Qin <qinhanmin2014>`.
+- |Fix| Fixed a bug in :func:`manifold.spectral_embedding` where the
+  normalization of the spectrum was using a division instead of a
+  multiplication. :issue:`8129` by :user:`Jan Margeta <jmargeta>`,
+  :user:`Guillaume Lemaitre <glemaitre>`, and :user:`Devansh D.
+  <devanshdalal>`.
 
-- Fixed a bug when setting parameters on meta-estimator, involving both a
-  wrapped estimator and its parameter. :issue:`9999` by :user:`Marcus Voss
-  <marcus-voss>` and `Joel Nothman`_.
+- |API| |Feature| Deprecate ``precomputed`` parameter in function
+  :func:`manifold.t_sne.trustworthiness`. Instead, the new parameter ``metric``
+  should be used with any compatible metric including 'precomputed', in which
+  case the input matrix ``X`` should be a matrix of pairwise distances or
+  squared distances. :issue:`9775` by :user:`William de Vazelhes
+  <wdevazelhes>`.
+
+- |API| Deprecate ``precomputed`` parameter in function
+  :func:`manifold.t_sne.trustworthiness`. Instead, the new parameter
+  ``metric`` should be used with any compatible metric including
+  'precomputed', in which case the input matrix ``X`` should be a matrix of
+  pairwise distances or squared distances. :issue:`9775` by
+  :user:`William de Vazelhes <wdevazelhes>`.
 
-- ``k_means`` now gives a warning, if the number of distinct clusters found
-  is smaller than ``n_clusters``. This may occur when the number of distinct
-  points in the data set is actually smaller than the number of cluster one is
-  looking for. :issue:`10059` by :user:`Christian Braune <christianbraune79>`.
 
-- Fixed a bug in :func:`datasets.make_circles`, where no odd number of data
-  points could be generated. :issue:`10037`
-  by :user:`Christian Braune <christianbraune79>`.
+:mod:`sklearn.metrics`
+......................
 
-- Fixed a bug in :func:`cluster.spectral_clustering` where the normalization of
-  the spectrum was using a division instead of a multiplication. :issue:`8129`
-  by :user:`Jan Margeta <jmargeta>`, :user:`Guillaume Lemaitre <glemaitre>`,
-  and :user:`Devansh D. <devanshdalal>`.
+- |MajorFeature| Added the :func:`metrics.davies_bouldin_score` metric for
+  evaluation of clustering models without a ground truth. :issue:`10827` by
+  :user:`Luis Osa <logc>`.
 
-- Fixed a bug in :class:`mixture.BaseMixture` where the reported `n_iter_` was
-  missing an iteration. It affected :class:`mixture.GaussianMixture` and
-  :class:`mixture.BayesianGaussianMixture`. :issue:`10740` by :user:`Erich
-  Schubert <kno10>` and :user:`Guillaume Lemaitre <glemaitre>`.
+- |MajorFeature| Added the :func:`metrics.balanced_accuracy_score` metric and
+  a corresponding ``'balanced_accuracy'`` scorer for binary and multiclass
+  classification. :issue:`8066` by :user:`xyguo` and :user:`Aman Dalmia
+  <dalmia>`, and :issue:`10587` by `Joel Nothman`_.
 
-- Fixed a bug in :class:`decomposition.SparseCoder` when running OMP sparse
-  coding in parallel using readonly memory mapped datastructures. :issue:`5956`
-  by :user:`Vighnesh Birodkar <vighneshbirodkar>` and
-  :user:`Olivier Grisel <ogrisel>`.
+- |Feature| Partial AUC is available via ``max_fpr`` parameter in
+  :func:`metrics.roc_auc_score`. :issue:`3840` by
+  :user:`Alexander Niederbühl <Alexander-N>`.
 
-- Fixed a bug in :func:`cluster.k_means_elkan` where the returned `iteration`
-  was 1 less than the correct value. Also added the missing `n_iter_` attribute
-  in the docstring of :class:`cluster.KMeans`. :issue:`11353` by
-  :user:`Jeremie du Boisberranger <jeremiedbb>`.
+- |Feature| A scorer based on :func:`metrics.brier_score_loss` is also
+  available. :issue:`9521` by :user:`Hanmin Qin <qinhanmin2014>`.
+
+- |Feature| Added control over the normalization in
+  :func:`metrics.normalized_mutual_info_score` and
+  :func:`metrics.adjusted_mutual_info_score` via the ``average_method``
+  parameter. In version 0.22, the default normalizer for each will become
+  the *arithmetic* mean of the entropies of each clustering. :issue:`11124` by
+  :user:`Arya McCarthy <aryamccarthy>`.
+
+- |Feature| Added ``output_dict`` parameter in :func:`metrics.classification_report`
+  to return classification statistics as dictionary.
+  :issue:`11160` by :user:`Dan Barkhorn <danielbarkhorn>`.
+
+- |Feature| :func:`metrics.classification_report` now reports all applicable averages on
+  the given data, including micro, macro and weighted average as well as samples
+  average for multilabel data. :issue:`11679` by :user:`Alexander Pacha <apacha>`. 
+
+- |Feature| :func:`metrics.average_precision_score` now supports binary
+  ``y_true`` other than ``{0, 1}`` or ``{-1, 1}`` through ``pos_label``
+  parameter. :issue:`9980` by :user:`Hanmin Qin <qinhanmin2014>`.
+
+- |Feature| :func:`metrics.label_ranking_average_precision_score` now supports
+  ``sample_weight``.
+  :issue:`10845` by :user:`Jose Perez-Parras Toledano <jopepato>`.
+
+- |Feature| Add ``dense_output`` parameter to :func:`metrics.pairwise.linear_kernel`.
+  When False and both inputs are sparse, will return a sparse matrix.
+  :issue:`10999` by :user:`Taylor G Smith <tgsmith61591>`.
 
-Metrics
+- |Efficiency| :func:`metrics.silhouette_score` and
+  :func:`metrics.silhouette_samples` are more memory efficient and run
+  faster. This avoids some reported freezes and MemoryErrors.
+  :issue:`11135` by `Joel Nothman`_.
 
-- Fixed a bug in :func:`metrics.precision_recall_fscore_support`
+- |Fix| Fixed a bug in :func:`metrics.precision_recall_fscore_support`
   when truncated `range(n_labels)` is passed as value for `labels`.
   :issue:`10377` by :user:`Gaurav Dhingra <gxyd>`.
 
-- Fixed a bug due to floating point error in :func:`metrics.roc_auc_score` with
-  non-integer sample weights. :issue:`9786` by :user:`Hanmin Qin <qinhanmin2014>`.
+- |Fix| Fixed a bug due to floating point error in
+  :func:`metrics.roc_auc_score` with non-integer sample weights. :issue:`9786`
+  by :user:`Hanmin Qin <qinhanmin2014>`.
 
-- Fixed a bug where :func:`metrics.roc_curve` sometimes starts on y-axis instead
-  of (0, 0), which is inconsistent with the document and other implementations.
-  Note that this will not influence the result from :func:`metrics.roc_auc_score`
-  :issue:`10093` by :user:`alexryndin <alexryndin>`
-  and :user:`Hanmin Qin <qinhanmin2014>`.
+- |Fix| Fixed a bug where :func:`metrics.roc_curve` sometimes starts on y-axis
+  instead of (0, 0), which is inconsistent with the document and other
+  implementations. Note that this will not influence the result from
+  :func:`metrics.roc_auc_score` :issue:`10093` by :user:`alexryndin
+  <alexryndin>` and :user:`Hanmin Qin <qinhanmin2014>`.
 
-- Fixed a bug to avoid integer overflow. Casted product to 64 bits integer in
+- |Fix| Fixed a bug to avoid integer overflow. Casted product to 64 bits integer in
   :func:`metrics.mutual_info_score`.
   :issue:`9772` by :user:`Kumar Ashutosh <thechargedneutron>`.
 
-- Fixed a bug in :func:`metrics.fowlkes_mallows_score` to avoid integer
+- |Fix| Fixed a bug where :func:`metrics.average_precision_score` will sometimes return
+  ``nan`` when ``sample_weight`` contains 0.
+  :issue:`9980` by :user:`Hanmin Qin <qinhanmin2014>`.
+
+- |Fix| Fixed a bug in :func:`metrics.fowlkes_mallows_score` to avoid integer
   overflow. Casted return value of `contingency_matrix` to `int64` and computed
   product of square roots rather than square root of product.
   :issue:`9515` by :user:`Alan Liddell <aliddell>` and
   :user:`Manh Dao <manhdao>`.
 
-Neighbors
+- |API| Deprecate ``reorder`` parameter in :func:`metrics.auc` as it's no
+  longer required for :func:`metrics.roc_auc_score`. Moreover using
+  ``reorder=True`` can hide bugs due to floating point error in the input.
+  :issue:`9851` by :user:`Hanmin Qin <qinhanmin2014>`.
 
-- Fixed a bug so ``predict`` in :class:`neighbors.RadiusNeighborsRegressor` can
-  handle empty neighbor set when using non uniform weights. Also raises a new
-  warning when no neighbors are found for samples.  :issue:`9655` by
-  :user:`Andreas Bjerre-Nielsen <abjer>`.
+- |API| In :func:`metrics.normalized_mutual_info_score` and
+  :func:`metrics.adjusted_mutual_info_score`, warn that
+  ``average_method`` will have a new default value. In version 0.22, the
+  default normalizer for each will become the *arithmetic* mean of the
+  entropies of each clustering. Currently,
+  :func:`metrics.normalized_mutual_info_score` uses the default of
+  ``average_method='geometric'``, and
+  :func:`metrics.adjusted_mutual_info_score` uses the default of
+  ``average_method='max'`` to match their behaviors in version 0.19.
+  :issue:`11124` by :user:`Arya McCarthy <aryamccarthy>`.
+
+- |API| The ``batch_size`` parameter to :func:`metrics.pairwise_distances_argmin_min`
+  and :func:`metrics.pairwise_distances_argmin` is deprecated to be removed in
+  v0.22. It no longer has any effect, as batch size is determined by global
+  ``working_memory`` config. See :ref:`working_memory`. :issue:`10280` by `Joel
+  Nothman`_ and :user:`Aman Dalmia <dalmia>`.
 
-Feature Extraction
 
-- Fixed a bug in :func:`feature_extraction.image.extract_patches_2d` which would
-  throw an exception if ``max_patches`` was greater than or equal to the number
-  of all possible patches rather than simply returning the number of possible
-  patches. :issue:`10100` by :user:`Varun Agrawal <varunagrawal>`
+:mod:`sklearn.mixture`
+......................
 
-- Fixed a bug in :class:`feature_extraction.text.CountVectorizer`,
-  :class:`feature_extraction.text.TfidfVectorizer`,
-  :class:`feature_extraction.text.HashingVectorizer` to support 64 bit sparse
-  array indexing necessary to process large datasets with more than 2·10⁹ tokens
-  (words or n-grams). :issue:`9147` by :user:`Claes-Fredrik Mannby <mannby>`
-  and `Roman Yurchak`_.
+- |Feature| Added function :term:`fit_predict` to :class:`mixture.GaussianMixture`
+  and :class:`mixture.GaussianMixture`, which is essentially equivalent to
+  calling :term:`fit` and :term:`predict`. :issue:`10336` by :user:`Shu Haoran
+  <haoranShu>` and :user:`Andrew Peng <Andrew-peng>`.
 
-- Fixed bug in :class:`feature_extraction.text.TFIDFVectorizer` which 
-  was ignoring the parameter ``dtype``. In addition,
-  :class:`feature_extraction.text.TFIDFTransformer` will preserve ``dtype``
-  for floating and raise a warning if ``dtype`` requested is integer.
-  :issue:`10441` by :user:`Mayur Kulkarni <maykulkarni>` and
-  :user:`Guillaume Lemaitre <glemaitre>`.
-  
-Utils
+- |Fix| Fixed a bug in :class:`mixture.BaseMixture` where the reported `n_iter_` was
+  missing an iteration. It affected :class:`mixture.GaussianMixture` and
+  :class:`mixture.BayesianGaussianMixture`. :issue:`10740` by :user:`Erich
+  Schubert <kno10>` and :user:`Guillaume Lemaitre <glemaitre>`.
 
-- :func:`utils.check_array` yield a ``FutureWarning`` indicating
-  that arrays of bytes/strings will be interpreted as decimal numbers
-  beginning in version 0.22. :issue:`10229` by :user:`Ryan Lee <rtlee9>`
+- |Fix| Fixed a bug in :class:`mixture.BaseMixture` and its subclasses
+  :class:`mixture.GaussianMixture` and :class:`mixture.BayesianGaussianMixture`
+  where the ``lower_bound_`` was not the max lower bound across all
+  initializations (when ``n_init > 1``), but just the lower bound of the last
+  initialization. :issue:`10869` by :user:`Aurélien Géron <ageron>`.
 
-Preprocessing
 
-- Fixed bugs in :class:`preprocessing.LabelEncoder` which would sometimes throw
-  errors when ``transform`` or ``inverse_transform`` was called with empty arrays.
-  :issue:`10458` by :user:`Mayur Kulkarni <maykulkarni>`.
+:mod:`sklearn.model_selection`
+..............................
 
-- Fix ValueError in :class:`preprocessing.LabelEncoder` when using
-  ``inverse_transform`` on unseen labels. :issue:`9816` by :user:`Charlie Newey
-  <newey01c>`.
+- |Feature| Add `return_estimator` parameter in
+  :func:`model_selection.cross_validate` to return estimators fitted on each
+  split. :issue:`9686` by :user:`Aurélien Bellet <bellet>`.
 
-- Fix bug in :class:`preprocessing.OneHotEncoder` which discarded the ``dtype``
-  when returning a sparse matrix output. :issue:`11042` by :user:`Daniel
-  Morales <DanielMorales9>`.
+- |Feature| New ``refit_time_`` attribute will be stored in
+  :class:`model_selection.GridSearchCV` and
+  :class:`model_selection.RandomizedSearchCV` if ``refit`` is set to ``True``.
+  This will allow measuring the complete time it takes to perform
+  hyperparameter optimization and refitting the best model on the whole
+  dataset. :issue:`11310` by :user:`Matthias Feurer <mfeurer>`.
 
-- Fix ``fit`` and ``partial_fit`` in :class:`preprocessing.StandardScaler` in
-  the rare case when `with_mean=False` and `with_std=False` which was crashing
-  by calling ``fit`` more than once and giving inconsistent results for
-  ``mean_`` whether the input was a sparse or a dense matrix. ``mean_`` will be
-  set to ``None`` with both sparse and dense inputs. ``n_samples_seen_`` will
-  be also reported for both input types.
-  :issue:`11235` by :user:`Guillaume Lemaitre <glemaitre>`.
+- |Feature| Expose `error_score` parameter in
+  :func:`model_selection.cross_validate`,
+  :func:`model_selection.cross_val_score`,
+  :func:`model_selection.learning_curve` and
+  :func:`model_selection.validation_curve` to control the behavior triggered
+  when an error occurs in :func:`model_selection._fit_and_score`.
+  :issue:`11576` by :user:`Samuel O. Ronsin <samronsin>`.
+
+- |Feature| `BaseSearchCV` now has an experimental, private interface to
+  support customized parameter search strategies, through its ``_run_search``
+  method. See the implementations in :class:`model_selection.GridSearchCV` and
+  :class:`model_selection.RandomizedSearchCV` and please provide feedback if
+  you use this. Note that we do not assure the stability of this API beyond
+  version 0.20. :issue:`9599` by `Joel Nothman`_
+
+- |Enhancement| Add improved error message in
+  :func:`model_selection.cross_val_score` when multiple metrics are passed in
+  ``scoring`` keyword. :issue:`11006` by :user:`Ming Li <minggli>`.
+
+- |API| The default number of cross-validation folds ``cv`` and the default
+  number of splits ``n_splits`` in the :class:`model_selection.KFold`-like
+  splitters will change from 3 to 5 in 0.22 as 3-fold has a lot of variance.
+  :issue:`11557` by :user:`Alexandre Boucaud <aboucaud>`.
+
+- |API| The default of ``iid`` parameter of :class:`model_selection.GridSearchCV`
+  and :class:`model_selection.RandomizedSearchCV` will change from ``True`` to
+  ``False`` in version 0.22 to correspond to the standard definition of
+  cross-validation, and the parameter will be removed in version 0.24
+  altogether. This parameter is of greatest practical significance where the
+  sizes of different test sets in cross-validation were very unequal, i.e. in
+  group-based CV strategies. :issue:`9085` by :user:`Laurent Direr <ldirer>`
+  and `Andreas Müller`_.
 
-Feature selection
+- |API| The default value of the ``error_score`` parameter in
+  :class:`model_selection.GridSearchCV` and
+  :class:`model_selection.RandomizedSearchCV` will change to ``np.NaN`` in
+  version 0.22. :issue:`10677` by :user:`Kirill Zhdanovich <Zhdanovich>`.
 
-- Fixed computation of ``n_features_to_compute`` for edge case with tied CV
-  scores in :class:`feature_selection.RFECV`. :issue:`9222` by `Nick Hoh
-  <nickypie>`.
+- |API| Changed ValueError exception raised in
+  :class:`model_selection.ParameterSampler` to a UserWarning for case where the
+  class is instantiated with a greater value of ``n_iter`` than the total space
+  of parameters in the parameter grid. ``n_iter`` now acts as an upper bound on
+  iterations. :issue:`10982` by :user:`Juliet Lawton <julietcl>`
 
-Model evaluation and meta-estimators
+- |API| Invalid input for :class:`model_selection.ParameterGrid` now
+  raises TypeError.
+  :issue:`10928` by :user:`Solutus Immensus <solutusimmensus>`
 
-- Add improved error message in :func:`model_selection.cross_val_score` when
-  multiple metrics are passed in ``scoring`` keyword.
-  :issue:`11006` by :user:`Ming Li <minggli>`.
 
-Datasets
+:mod:`sklearn.multioutput`
+..........................
 
-- Fixed a bug in :func:`datasets.load_boston` which had a wrong data point.
-  :issue:`10801` by :user:`Takeshi Yoshizawa <tarcusx>`.
+- |MajorFeature| Added :class:`multioutput.RegressorChain` for multi-target
+  regression. :issue:`9257` by :user:`Kumar Ashutosh <thechargedneutron>`.
 
-- Fixed a bug in :func:`datasets.load_iris` which had two wrong data points.
-  :issue:`11082` by :user:`Sadhana Srinivasan <rotuna>`
-  and :user:`Hanmin Qin <qinhanmin2014>`.
 
-API changes summary
--------------------
+:mod:`sklearn.naive_bayes`
+..........................
 
-Linear, kernelized and related models
+- |MajorFeature| Added :class:`naive_bayes.ComplementNB`, which implements the
+  Complement Naive Bayes classifier described in Rennie et al. (2003).
+  :issue:`8190` by :user:`Michael A. Alcorn <airalcorn2>`.
 
-- Deprecate ``random_state`` parameter in :class:`svm.OneClassSVM` as the
-  underlying implementation is not random.
-  :issue:`9497` by :user:`Albert Thomas <albertcthomas>`.
+- |Feature| Add `var_smoothing` parameter in :class:`naive_bayes.GaussianNB`
+  to give a precise control over variances calculation.
+  :issue:`9681` by :user:`Dmitry Mottl <Mottl>`.
 
-- Deprecate ``positive=True`` option in :class:`linear_model.Lars` as the
-  underlying implementation is broken. Use :class:`linear_model.Lasso` instead.
-  :issue:`9837` by `Alexandre Gramfort`_.
+- |Fix| Fixed a bug in :class:`naive_bayes.GaussianNB` which incorrectly
+  raised error for prior list which summed to 1.
+  :issue:`10005` by :user:`Gaurav Dhingra <gxyd>`.
 
-- ``n_iter_`` may vary from previous releases in
-  :class:`linear_model.LogisticRegression` with ``solver='lbfgs'`` and
-  :class:`linear_model.HuberRegressor`.  For Scipy <= 1.0.0, the optimizer could
-  perform more than the requested maximum number of iterations. Now both
-  estimators will report at most ``max_iter`` iterations even if more were
-  performed. :issue:`10723` by `Joel Nothman`_.
+- |Fix| Fixed a bug in :class:`naive_bayes.MultinomialNB` which did not accept
+  vector valued pseudocounts (alpha).
+  :issue:`10346` by :user:`Tobias Madsen <TobiasMadsen>`
 
-- The default value of ``gamma`` parameter of :class:`svm.SVC`,
-  :class:`~svm.NuSVC`, :class:`~svm.SVR`, :class:`~svm.NuSVR`,
-  :class:`~svm.OneClassSVM` will change from ``'auto'`` to ``'scale'`` in
-  version 0.22 to account better for unscaled features. :issue:`8361` by
-  :user:`Gaurav Dhingra <gxyd>` and :user:`Ting Neo <neokt>`.
 
-- Added convergence warning to :class:`svm.LinearSVC` and
-  :class:`linear_model.LogisticRegression` when ``verbose`` is set to 0.
-  :issue:`10881` by :user:`Alexandre Sevin <AlexandreSev>`.
+:mod:`sklearn.neighbors`
+........................
+
+- |Efficiency| :class:`neighbors.RadiusNeighborsRegressor` and
+  :class:`neighbors.RadiusNeighborsClassifier` are now
+  parallelized according to ``n_jobs`` regardless of ``algorithm``.
+  :issue:`10887` by :user:`Joël Billaud <recamshak>`.
+
+- |Efficiency| :mod:`Nearest neighbors <neighbors>` query methods are now more
+  memory efficient when ``algorithm='brute'``.
+  :issue:`11136` by `Joel Nothman`_ and :user:`Aman Dalmia <dalmia>`.
+
+- |Feature| Add `sample_weight` parameter to the fit method of
+  :class:`neighbors.KernelDensity` to enable weighting in kernel density
+  estimation.
+  :issue:`4394` by :user:`Samuel O. Ronsin <samronsin>`.
+
+- |Feature| Novelty detection with :class:`neighbors.LocalOutlierFactor`:
+  Add a ``novelty`` parameter to :class:`neighbors.LocalOutlierFactor`. When
+  ``novelty`` is set to True, :class:`neighbors.LocalOutlierFactor` can then
+  be used for novelty detection, i.e. predict on new unseen data. Available
+  prediction methods are ``predict``, ``decision_function`` and
+  ``score_samples``. By default, ``novelty`` is set to ``False``, and only
+  the ``fit_predict`` method is avaiable.
+  By :user:`Albert Thomas <albertcthomas>`.
+
+- |Fix| Fixed a bug in :class:`neighbors.NearestNeighbors` where fitting a
+  NearestNeighbors model fails when a) the distance metric used is a
+  callable and b) the input to the NearestNeighbors model is sparse.
+  :issue:`9579` by :user:`Thomas Kober <tttthomasssss>`.
+
+- |Fix| Fixed a bug so ``predict`` in
+  :class:`neighbors.RadiusNeighborsRegressor` can handle empty neighbor set
+  when using non uniform weights. Also raises a new warning when no neighbors
+  are found for samples. :issue:`9655` by :user:`Andreas Bjerre-Nielsen
+  <abjer>`.
+
+- |Fix| |Efficiency| Fixed a bug in ``KDTree`` construction that results in
+  faster construction and querying times.
+  :issue:`11556` by :user:`Jake VanderPlas <jakevdp>`
+
+- |Fix| Fixed a bug in `neighbors.KDTree` and `neighbors.BallTree` where
+  pickled tree objects would change their type to the super class `BinaryTree`.
+  :issue:`11774` by :user:`Nicolas Hug <NicolasHug>`.
+
 
-Preprocessing
+:mod:`sklearn.neural_network`
+.............................
 
-- Deprecate ``n_values`` and ``categorical_features`` parameters and
+- |Feature| Add `n_iter_no_change` parameter in
+  :class:`neural_network.BaseMultilayerPerceptron`,
+  :class:`neural_network.MLPRegressor`, and
+  :class:`neural_network.MLPClassifier` to give control over
+  maximum number of epochs to not meet ``tol`` improvement.
+  :issue:`9456` by :user:`Nicholas Nadeau <nnadeau>`.
+
+- |Fix| Fixed a bug in :class:`neural_network.BaseMultilayerPerceptron`,
+  :class:`neural_network.MLPRegressor`, and
+  :class:`neural_network.MLPClassifier` with new ``n_iter_no_change``
+  parameter now at 10 from previously hardcoded 2.
+  :issue:`9456` by :user:`Nicholas Nadeau <nnadeau>`.
+
+- |Fix| Fixed a bug in :class:`neural_network.MLPRegressor` where fitting
+  quit unexpectedly early due to local minima or fluctuations.
+  :issue:`9456` by :user:`Nicholas Nadeau <nnadeau>`
+
+
+:mod:`sklearn.pipeline`
+.......................
+
+- |Feature| The ``predict`` method of :class:`pipeline.Pipeline` now passes
+  keyword arguments on to the pipeline's last estimator, enabling the use of
+  parameters such as ``return_std`` in a pipeline with caution.
+  :issue:`9304` by :user:`Breno Freitas <brenolf>`.
+
+
+:mod:`sklearn.preprocessing`
+............................
+
+- |MajorFeature| Expanded :class:`preprocessing.OneHotEncoder` to allow to
+  encode categorical string features as a numeric array using a one-hot (or
+  dummy) encoding scheme, and added :class:`preprocessing.OrdinalEncoder` to
+  convert to ordinal integers. Those two classes now handle encoding of all
+  feature types (also handles string-valued features) and derives the
+  categories based on the unique values in the features instead of the maximum
+  value in the features. :issue:`9151` and :issue:`10521` by :user:`Vighnesh
+  Birodkar <vighneshbirodkar>` and `Joris Van den Bossche`_.
+
+- |MajorFeature| Added :class:`preprocessing.KBinsDiscretizer` for turning
+  continuous features into categorical or one-hot encoded
+  features. :issue:`7668`, :issue:`9647`, :issue:`10195`,
+  :issue:`10192`, :issue:`11272`, :issue:`11467` and :issue:`11505`.
+  by :user:`Henry Lin <hlin117>`, `Hanmin Qin`_,
+  `Tom Dupre la Tour`_ and :user:`Giovanni Giuseppe Costa <ggc87>`.
+
+- |MajorFeature| Added :class:`preprocessing.PowerTransformer`, which
+  implements the Yeo-Johnson and Box-Cox power transformations. Power
+  transformations try to find a set of feature-wise parametric transformations
+  to approximately map data to a Gaussian distribution centered at zero and
+  with unit variance. This is useful as a variance-stabilizing transformation
+  in situations where normality and homoscedasticity are desirable.
+  :issue:`10210` by :user:`Eric Chang <chang>` and :user:`Maniteja
+  Nandana <maniteja123>`, and :issue:`11520` by :user:`Nicolas Hug
+  <nicolashug>`.
+
+- |MajorFeature| NaN values are ignored and handled in the following
+  preprocessing methods:
+  :class:`preprocessing.MaxAbsScaler`,
+  :class:`preprocessing.MinMaxScaler`,
+  :class:`preprocessing.RobustScaler`,
+  :class:`preprocessing.StandardScaler`,
+  :class:`preprocessing.PowerTransformer`,
+  :class:`preprocessing.QuantileTransformer` classes and
+  :func:`preprocessing.maxabs_scale`,
+  :func:`preprocessing.minmax_scale`,
+  :func:`preprocessing.robust_scale`,
+  :func:`preprocessing.scale`,
+  :func:`preprocessing.power_transform`,
+  :func:`preprocessing.quantile_transform` functions respectively addressed in
+  issues :issue:`11011`, :issue:`11005`, :issue:`11308`, :issue:`11206`,
+  :issue:`11306`, and :issue:`10437`.
+  By :user:`Lucija Gregov <LucijaGregov>` and
+  :user:`Guillaume Lemaitre <glemaitre>`.
+
+- |Feature| :class:`preprocessing.PolynomialFeatures` now supports sparse
+  input. :issue:`10452` by :user:`Aman Dalmia <dalmia>` and `Joel Nothman`_.
+
+- |Feature| :class:`preprocessing.RobustScaler` and
+  :func:`preprocessing.robust_scale` can be fitted using sparse matrices.
+  :issue:`11308` by :user:`Guillaume Lemaitre <glemaitre>`.
+
+- |Feature| :class:`preprocessing.OneHotEncoder` now supports the
+  :term:`get_feature_names` method to obtain the transformed feature names.
+  :issue:`10181` by :user:`Nirvan Anjirbag <Nirvan101>` and
+  `Joris Van den Bossche`_.
+
+- |Feature| A parameter ``check_inverse`` was added to
+  :class:`preprocessing.FunctionTransformer` to ensure that ``func`` and
+  ``inverse_func`` are the inverse of each other.
+  :issue:`9399` by :user:`Guillaume Lemaitre <glemaitre>`.
+
+- |Feature| The ``transform`` method of :class:`sklearn.preprocessing.MultiLabelBinarizer`
+  now ignores any unknown classes. A warning is raised stating the unknown classes
+  classes found which are ignored.
+  :issue:`10913` by :user:`Rodrigo Agundez <rragundez>`.
+
+- |Fix| Fixed bugs in :class:`preprocessing.LabelEncoder` which would
+  sometimes throw errors when ``transform`` or ``inverse_transform`` was called
+  with empty arrays. :issue:`10458` by :user:`Mayur Kulkarni <maykulkarni>`.
+
+- |Fix| Fix ValueError in :class:`preprocessing.LabelEncoder` when using
+  ``inverse_transform`` on unseen labels. :issue:`9816` by :user:`Charlie Newey
+  <newey01c>`.
+
+- |Fix| Fix bug in :class:`preprocessing.OneHotEncoder` which discarded the
+  ``dtype`` when returning a sparse matrix output.
+  :issue:`11042` by :user:`Daniel Morales <DanielMorales9>`.
+
+- |Fix| Fix ``fit`` and ``partial_fit`` in
+  :class:`preprocessing.StandardScaler` in the rare case when `with_mean=False`
+  and `with_std=False` which was crashing by calling ``fit`` more than once and
+  giving inconsistent results for ``mean_`` whether the input was a sparse or a
+  dense matrix. ``mean_`` will be set to ``None`` with both sparse and dense
+  inputs. ``n_samples_seen_`` will be also reported for both input types.
+  :issue:`11235` by :user:`Guillaume Lemaitre <glemaitre>`.
+
+- |API| Deprecate ``n_values`` and ``categorical_features`` parameters and
   ``active_features_``, ``feature_indices_`` and ``n_values_`` attributes
   of :class:`preprocessing.OneHotEncoder`. The ``n_values`` parameter can be
   replaced with the new ``categories`` parameter, and the attributes with the
@@ -696,61 +941,92 @@ Preprocessing
   :class:`compose.ColumnTransformer`.
   :issue:`10521` by `Joris Van den Bossche`_.
 
-Decomposition, manifold learning and clustering
+- |API| Deprecate :class:`preprocessing.Imputer` and move
+  the corresponding module to :class:`impute.SimpleImputer`.
+  :issue:`9726` by :user:`Kumar Ashutosh
+  <thechargedneutron>`.
 
-- Deprecate ``precomputed`` parameter in function
-  :func:`manifold.t_sne.trustworthiness`. Instead, the new parameter
-  ``metric`` should be used with any compatible metric including
-  'precomputed', in which case the input matrix ``X`` should be a matrix of
-  pairwise distances or squared distances. :issue:`9775` by
-  :user:`William de Vazelhes <wdevazelhes>`.
+- |API| The ``axis`` parameter that was in
+  :class:`preprocessing.Imputer` is no longer present in
+  :class:`impute.SimpleImputer`. The behavior is equivalent
+  to ``axis=0`` (impute along columns). Row-wise
+  imputation can be performed with FunctionTransformer
+  (e.g., ``FunctionTransformer(lambda X:
+  SimpleImputer().fit_transform(X.T).T)``). :issue:`10829`
+  by :user:`Guillaume Lemaitre <glemaitre>` and
+  :user:`Gilberto Olimpio <gilbertoolimpio>`.
+
+- |API| The NaN marker for the missing values has been changed
+  between the :class:`preprocessing.Imputer` and the
+  :class:`impute.SimpleImputer`.
+  ``missing_values='NaN'`` should now be
+  ``missing_values=np.nan``. :issue:`11211` by
+  :user:`Jeremie du Boisberranger <jeremiedbb>`.
 
-Metrics
+- |API| In :class:`preprocessing.FunctionTransformer`, the default of
+  ``validate`` will be from ``True`` to ``False`` in 0.22.
+  :issue:`10655` by :user:`Guillaume Lemaitre <glemaitre>`.
 
-- Deprecate ``reorder`` parameter in :func:`metrics.auc` as it's no longer required
-  for :func:`metrics.roc_auc_score`. Moreover using ``reorder=True`` can hide bugs
-  due to floating point error in the input.
-  :issue:`9851` by :user:`Hanmin Qin <qinhanmin2014>`.
 
-- The ``batch_size`` parameter to :func:`metrics.pairwise_distances_argmin_min`
-  and :func:`metrics.pairwise_distances_argmin` is deprecated to be removed in
-  v0.22.  It no longer has any effect, as batch size is determined by global
-  ``working_memory`` config. See :ref:`working_memory`. :issue:`10280` by `Joel
-  Nothman`_ and :user:`Aman Dalmia <dalmia>`.
+:mod:`sklearn.svm`
+..................
+
+- |Fix| Fixed a bug in :class:`svm.SVC` where when the argument ``kernel`` is
+  unicode in Python2, the ``predict_proba`` method was raising an
+  unexpected TypeError given dense inputs.
+  :issue:`10412` by :user:`Jiongyan Zhang <qmick>`.
 
-Cluster
+- |API| Deprecate ``random_state`` parameter in :class:`svm.OneClassSVM` as
+  the underlying implementation is not random.
+  :issue:`9497` by :user:`Albert Thomas <albertcthomas>`.
 
-- Deprecate ``pooling_func`` unused parameter in
-  :class:`cluster.AgglomerativeClustering`. :issue:`9875` by :user:`Kumar Ashutosh
-  <thechargedneutron>`.
+- |API| The default value of ``gamma`` parameter of :class:`svm.SVC`,
+  :class:`~svm.NuSVC`, :class:`~svm.SVR`, :class:`~svm.NuSVR`,
+  :class:`~svm.OneClassSVM` will change from ``'auto'`` to ``'scale'`` in
+  version 0.22 to account better for unscaled features. :issue:`8361` by
+  :user:`Gaurav Dhingra <gxyd>` and :user:`Ting Neo <neokt>`.
 
-Imputer
 
-- Deprecate :class:`preprocessing.Imputer` and move the corresponding module to
-  :class:`impute.SimpleImputer`. :issue:`9726` by :user:`Kumar Ashutosh
-  <thechargedneutron>`.
+:mod:`sklearn.tree`
+...................
+
+- |Enhancement| Although private (and hence not assured API stability),
+  :class:`tree._criterion.ClassificationCriterion` and
+  :class:`tree._criterion.RegressionCriterion` may now be cimported and
+  extended. :issue:`10325` by :user:`Camil Staps <camilstaps>`.
+
+- |Fix| Fixed a bug in :class:`tree.BaseDecisionTree` with `splitter="best"`
+  where split threshold could become infinite when values in X were
+  near infinite. :issue:`10536` by :user:`Jonathan Ohayon <Johayon>`.
+
+- |Fix| Fixed a bug in :class:`tree.MAE` to ensure sample weights are being
+  used during the calculation of tree MAE impurity. Previous behaviour could
+  cause suboptimal splits to be chosen since the impurity calculation
+  considered all samples to be of equal weight importance.
+  :issue:`11464` by :user:`John Stott <JohnStott>`.
+
+
+:mod:`sklearn.utils`
+....................
+
+- |Feature| :func:`utils.check_array` and :func:`utils.check_X_y` now have
+  ``accept_large_sparse`` to control whether scipy.sparse matrices with 64-bit
+  indices should be rejected.
+  :issue:`11327` by :user:`Karan Dhingra <kdhingra307>` and `Joel Nothman`_.
 
-- The ``axis`` parameter that was in :class:`preprocessing.Imputer` is no
-  longer present in :class:`impute.SimpleImputer`. The behavior is equivalent
-  to ``axis=0`` (impute along columns). Row-wise imputation can be performed
-  with FunctionTransformer (e.g., ``FunctionTransformer(lambda X:
-  SimpleImputer().fit_transform(X.T).T)``). :issue:`10829` by :user:`Guillaume
-  Lemaitre <glemaitre>` and :user:`Gilberto Olimpio <gilbertoolimpio>`.
+- |Efficiency| |Fix| Avoid copying the data in :func:`utils.check_array` when
+  the input data is a memmap (and ``copy=False``). :issue:`10663` by
+  :user:`Arthur Mensch <arthurmensch>` and :user:`Loïc Estève <lesteve>`.
 
-- The :class:`impute.SimpleImputer` has a new strategy, ``'constant'``, to
-  complete missing values with a fixed one, given by the ``fill_value``
-  parameter. This strategy supports numeric and non-numeric data, and so does
-  the ``'most_frequent'`` strategy now. :issue:`11211` by :user:`Jeremie du
-  Boisberranger <jeremiedbb>`.
+- |API| :func:`utils.check_array` yield a ``FutureWarning`` indicating
+  that arrays of bytes/strings will be interpreted as decimal numbers
+  beginning in version 0.22. :issue:`10229` by :user:`Ryan Lee <rtlee9>`
 
-- The NaN marker for the missing values has been changed between the
-  :class:`preprocessing.Imputer` and the :class:`impute.SimpleImputer`.
-  ``missing_values='NaN'`` should now be ``missing_values=np.nan``.
-  :issue:`11211` by :user:`Jeremie du Boisberranger <jeremiedbb>`.
 
-Outlier Detection models
+Multiple modules
+................
 
-- More consistent outlier detection API:
+- |Feature| |API| More consistent outlier detection API:
   Add a ``score_samples`` method in :class:`svm.OneClassSVM`,
   :class:`ensemble.IsolationForest`, :class:`neighbors.LocalOutlierFactor`,
   :class:`covariance.EllipticEnvelope`. It allows to access raw score
@@ -766,17 +1042,22 @@ Outlier Detection models
   ``raw_values`` parameter is deprecated as the shifted Mahalanobis distance
   will be always returned in 0.22. :issue:`9015` by `Nicolas Goix`_.
 
-Covariance
-
-- The :func:`covariance.graph_lasso`, :class:`covariance.GraphLasso` and
-  :class:`covariance.GraphLassoCV` have been renamed to
-  :func:`covariance.graphical_lasso`, :class:`covariance.GraphicalLasso` and
-  :class:`covariance.GraphicalLassoCV` respectively and will be removed in version 0.22.
-  :issue:`9993` by :user:`Artiem Krinitsyn <artiemq>`
-
-Misc
+- |Feature| |API| A ``behaviour`` parameter has been introduced in :class:`ensemble.IsolationForest`
+  to ensure backward compatibility.
+  In the old behaviour, the ``decision_function`` is independent of the ``contamination``
+  parameter. A threshold attribute depending on the ``contamination`` parameter is thus
+  used.
+  In the new behaviour the ``decision_function`` is dependent on the ``contamination``
+  parameter, in such a way that 0 becomes its natural threshold to detect outliers.
+  Setting behaviour to "old" is deprecated and will not be possible in version 0.22.
+  Beside, the behaviour parameter will be removed in 0.24.
+  :issue:`11553` by `Nicolas Goix`_.
+
+- |API| Added convergence warning to :class:`svm.LinearSVC` and
+  :class:`linear_model.LogisticRegression` when ``verbose`` is set to 0.
+  :issue:`10881` by :user:`Alexandre Sevin <AlexandreSev>`.
 
-- Changed warning type from :class:`UserWarning` to
+- |API| Changed warning type from :class:`UserWarning` to
   :class:`exceptions.ConvergenceWarning` for failing convergence in
   :func:`linear_model.logistic_regression_path`,
   :class:`linear_model.RANSACRegressor`, :func:`linear_model.ridge_regression`,
@@ -784,39 +1065,91 @@ Misc
   :class:`gaussian_process.GaussianProcessClassifier`,
   :func:`decomposition.fastica`, :class:`cross_decomposition.PLSCanonical`,
   :class:`cluster.AffinityPropagation`, and :class:`cluster.Birch`.
-  :issue:`#10306` by :user:`Jonathan Siebert <jotasi>`.
+  :issue:`10306` by :user:`Jonathan Siebert <jotasi>`.
 
-- Changed ValueError exception raised in :class:`model_selection.ParameterSampler`
-  to a UserWarning for case where the class is instantiated with a greater value of
-  ``n_iter`` than the total space of parameters in the parameter grid. ``n_iter`` now
-  acts as an upper bound on iterations.
-  :issue:`#10982` by :user:`Juliet Lawton <julietcl>`
 
-- Invalid input for :class:`model_selection.ParameterGrid` now raises TypeError.
-  :issue:`10928` by :user:`Solutus Immensus <solutusimmensus>`
+Miscellaneous
+.............
 
-- :func:`utils.check_array` and :func:`utils.check_X_y` now have
-  ``accept_large_sparse`` to control whether scipy.sparse matrices with 64-bit
-  indices should be rejected.
-  :issue:`11327` by :user:`Karan Dhingra <kdhingra307>` and `Joel Nothman`_.
+- |MajorFeature| A new configuration parameter, ``working_memory`` was added
+  to control memory consumption limits in chunked operations, such as the new
+  :func:`metrics.pairwise_distances_chunked`. See :ref:`working_memory`.
+  :issue:`10280` by `Joel Nothman`_ and :user:`Aman Dalmia <dalmia>`.
+
+- |Feature| The version of :mod:`joblib` bundled with Scikit-learn is now 0.12.
+  This uses a new default multiprocessing implementation, named `loky
+  <https://github.com/tomMoral/loky>`_. While this may incur some memory and
+  communication overhead, it should provide greater cross-platform stability
+  than relying on Python standard library multiprocessing. :issue:`11741` by
+  the Joblib developers, especially :user:`Thomas Moreau <tomMoral>` and
+  `Olivier Grisel`_.
+
+- |Feature| An environment variable to use the site joblib instead of the
+  vendored one was added (:ref:`environment_variable`). The main API of joblib
+  is now exposed in :mod:`sklearn.utils`.
+  :issue:`11166` by `Gael Varoquaux`_.
+
+- |Feature| Add almost complete PyPy 3 support. Known unsupported
+  functionalities are :func:`datasets.load_svmlight_file`,
+  :class:`feature_extraction.FeatureHasher` and
+  :class:`feature_extraction.text.HashingVectorizer`. For running on PyPy,
+  PyPy3-v5.10+, Numpy 1.14.0+, and scipy 1.1.0+ are required.
+  :issue:`11010` by :user:`Ronan Lamy <rlamy>` and `Roman Yurchak`_.
+
+- |Feature| A utility method :func:`sklearn.show_versions()` was added to
+  print out information relevant for debugging. It includes the user system,
+  the Python executable, the version of the main libraries and BLAS binding
+  information. :issue:`11596` by :user:`Alexandre Boucaud <aboucaud>`
+
+- |Fix| Fixed a bug when setting parameters on meta-estimator, involving both
+  a wrapped estimator and its parameter. :issue:`9999` by :user:`Marcus Voss
+  <marcus-voss>` and `Joel Nothman`_.
+
+- |Fix| Fixed a bug where calling :func:`sklearn.base.clone` was not thread
+  safe and could result in a "pop from empty list" error. :issue:`9569`
+  by `Andreas Müller`_.
+
+- |API| The default value of ``n_jobs`` is changed from ``1`` to ``None`` in
+  all related functions and classes. ``n_jobs=None`` means ``unset``. It will
+  generally be interpreted as ``n_jobs=1``, unless the current
+  ``joblib.Parallel`` backend context specifies otherwise (See
+  :term:`Glossary <n_jobs>` for additional information). Note that this change
+  happens immediately (i.e., without a deprecation cycle).
+  :issue:`11741` by `Olivier Grisel`_.
 
 Changes to estimator checks
 ---------------------------
 
 These changes mostly affect library developers.
 
+- Checks for transformers now apply if the estimator implements
+  :term:`transform`, regardless of whether it inherits from
+  :class:`sklearn.base.TransformerMixin`. :issue:`10474` by `Joel Nothman`_.
+
+- Classifiers are now checked for consistency between :term:`decision_function`
+  and categorical predictions.
+  :issue:`10500` by :user:`Narine Kokhlikyan <NarineK>`.
+
 - Allow tests in :func:`utils.estimator_checks.check_estimator` to test functions
   that accept pairwise data.
   :issue:`9701` by :user:`Kyle Johnson <gkjohns>`
 
-- Allow :func:`~utils.estimator_checks.check_estimator` to check that there is no
+- Allow :func:`utils.estimator_checks.check_estimator` to check that there is no
   private settings apart from parameters during estimator initialization.
   :issue:`9378` by :user:`Herilalaina Rakotoarison <herilalaina>`
 
+- The set of checks in :func:`utils.estimator_checks.check_estimator` now includes a
+  ``check_set_params`` test which checks that ``set_params`` is equivalent to
+  passing parameters in ``__init__`` and warns if it encounters parameter
+  validation. :issue:`7738` by :user:`Alvin Chiang <absolutelyNoWarranty>`
+
+- Add invariance tests for clustering metrics. :issue:`8102` by :user:`Ankita
+  Sinha <anki08>` and :user:`Guillaume Lemaitre <glemaitre>`.
+
 - Add ``check_methods_subset_invariance`` to
   :func:`~utils.estimator_checks.check_estimator`, which checks that
-  estimator methods are invariant if applied to a data subset.  :issue:`10420`
-  by :user:`Jonathan Ohayon <Johayon>`
+  estimator methods are invariant if applied to a data subset.
+  :issue:`10428` by :user:`Jonathan Ohayon <Johayon>`
 
 - Add tests in :func:`utils.estimator_checks.check_estimator` to check that an
   estimator can handle read-only memmap input data. :issue:`10663` by
@@ -825,3 +1158,7 @@ These changes mostly affect library developers.
 - ``check_sample_weights_pandas_series`` now uses 8 rather than 6 samples
   to accommodate for the default number of clusters in :class:`cluster.KMeans`.
   :issue:`10933` by :user:`Johannes Hansen <jnhansen>`.
+
+- Estimators are now checked for whether ``sample_weight=None`` equates to
+  ``sample_weight=np.ones(...)``.
+  :issue:`11558` by :user:`Sergul Aydore <sergulaydore>`.
diff --git a/examples/applications/plot_face_recognition.py b/examples/applications/plot_face_recognition.py
index 13a38d13bc00..dce3df1d3ee9 100644
--- a/examples/applications/plot_face_recognition.py
+++ b/examples/applications/plot_face_recognition.py
@@ -108,7 +108,8 @@
 t0 = time()
 param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],
               'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }
-clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)
+clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'),
+                   param_grid, cv=5)
 clf = clf.fit(X_train_pca, y_train)
 print("done in %0.3fs" % (time() - t0))
 print("Best estimator found by grid search:")
diff --git a/examples/applications/plot_prediction_latency.py b/examples/applications/plot_prediction_latency.py
index 8d4d9c746593..6eac023b71f5 100644
--- a/examples/applications/plot_prediction_latency.py
+++ b/examples/applications/plot_prediction_latency.py
@@ -26,7 +26,6 @@
 
 from sklearn.preprocessing import StandardScaler
 from sklearn.model_selection import train_test_split
-from scipy.stats import scoreatpercentile
 from sklearn.datasets.samples_generator import make_regression
 from sklearn.ensemble.forest import RandomForestRegressor
 from sklearn.linear_model.ridge import Ridge
@@ -50,7 +49,7 @@ def atomic_benchmark_estimator(estimator, X_test, verbose=False):
         estimator.predict(instance)
         runtimes[i] = time.time() - start
     if verbose:
-        print("atomic_benchmark runtimes:", min(runtimes), scoreatpercentile(
+        print("atomic_benchmark runtimes:", min(runtimes), np.percentile(
             runtimes, 50), max(runtimes))
     return runtimes
 
@@ -65,7 +64,7 @@ def bulk_benchmark_estimator(estimator, X_test, n_bulk_repeats, verbose):
         runtimes[i] = time.time() - start
     runtimes = np.array(list(map(lambda x: x / float(n_instances), runtimes)))
     if verbose:
-        print("bulk_benchmark runtimes:", min(runtimes), scoreatpercentile(
+        print("bulk_benchmark runtimes:", min(runtimes), np.percentile(
             runtimes, 50), max(runtimes))
     return runtimes
 
@@ -102,7 +101,7 @@ def generate_dataset(n_train, n_test, n_features, noise=0.1, verbose=False):
 
     random_seed = 13
     X_train, X_test, y_train, y_test = train_test_split(
-        X, y, train_size=n_train, random_state=random_seed)
+        X, y, train_size=n_train, test_size=n_test, random_state=random_seed)
     X_train, y_train = shuffle(X_train, y_train, random_state=random_seed)
 
     X_scaler = StandardScaler()
@@ -207,8 +206,8 @@ def n_feature_influence(estimators, n_train, n_test, n_features, percentile):
             estimator.fit(X_train, y_train)
             gc.collect()
             runtimes = bulk_benchmark_estimator(estimator, X_test, 30, False)
-            percentiles[cls_name][n] = 1e6 * scoreatpercentile(runtimes,
-                                                               percentile)
+            percentiles[cls_name][n] = 1e6 * np.percentile(runtimes,
+                                                           percentile)
     return percentiles
 
 
@@ -285,7 +284,7 @@ def plot_benchmark_throughput(throughputs, configuration):
          'complexity_label': 'non-zero coefficients',
          'complexity_computer': lambda clf: np.count_nonzero(clf.coef_)},
         {'name': 'RandomForest',
-         'instance': RandomForestRegressor(),
+         'instance': RandomForestRegressor(n_estimators=100),
          'complexity_label': 'estimators',
          'complexity_computer': lambda clf: clf.n_estimators},
         {'name': 'SVR',
diff --git a/examples/applications/plot_species_distribution_modeling.py b/examples/applications/plot_species_distribution_modeling.py
index 754b9d7cb267..a16b5b7153ce 100644
--- a/examples/applications/plot_species_distribution_modeling.py
+++ b/examples/applications/plot_species_distribution_modeling.py
@@ -154,7 +154,7 @@ def plot_species_distribution(species=("bradypus_variegatus_0",
         else:
             print(" - plot coastlines from coverage")
             plt.contour(X, Y, land_reference,
-                        levels=[-9999], colors="k",
+                        levels=[-9998], colors="k",
                         linestyles="solid")
             plt.xticks([])
             plt.yticks([])
diff --git a/examples/applications/plot_stock_market.py b/examples/applications/plot_stock_market.py
index 379efb8e4dfe..e2edc25b7eb7 100644
--- a/examples/applications/plot_stock_market.py
+++ b/examples/applications/plot_stock_market.py
@@ -166,7 +166,7 @@
 
 # #############################################################################
 # Learn a graphical structure from the correlations
-edge_model = covariance.GraphicalLassoCV()
+edge_model = covariance.GraphicalLassoCV(cv=5)
 
 # standardize the time series: using correlations rather than covariance
 # is more efficient for structure recovery
diff --git a/examples/applications/wikipedia_principal_eigenvector.py b/examples/applications/wikipedia_principal_eigenvector.py
index 3ef921bb3d05..e4a0ce003603 100644
--- a/examples/applications/wikipedia_principal_eigenvector.py
+++ b/examples/applications/wikipedia_principal_eigenvector.py
@@ -45,7 +45,7 @@
 from scipy import sparse
 
 from sklearn.decomposition import randomized_svd
-from sklearn.externals.joblib import Memory
+from sklearn.utils import Memory
 from sklearn.externals.six.moves.urllib.request import urlopen
 from sklearn.externals.six import iteritems
 
@@ -204,9 +204,10 @@ def centrality_scores(X, alpha=0.85, max_iter=100, tol=1e-10):
     print("Normalizing the graph")
     for i in incoming_counts.nonzero()[0]:
         X.data[X.indptr[i]:X.indptr[i + 1]] *= 1.0 / incoming_counts[i]
-    dangle = np.asarray(np.where(X.sum(axis=1) == 0, 1.0 / n, 0)).ravel()
+    dangle = np.asarray(np.where(np.isclose(X.sum(axis=1), 0),
+                                 1.0 / n, 0)).ravel()
 
-    scores = np.ones(n, dtype=np.float32) / n  # initial guess
+    scores = np.full(n, 1. / n, dtype=np.float32)  # initial guess
     for i in range(max_iter):
         print("power iteration #%d" % i)
         prev_scores = scores
diff --git a/examples/classification/plot_classification_probability.py b/examples/classification/plot_classification_probability.py
index 3991a70cb4c0..4542362817d7 100644
--- a/examples/classification/plot_classification_probability.py
+++ b/examples/classification/plot_classification_probability.py
@@ -76,7 +76,7 @@ class dataset, and we classify it with a Support Vector classifier, L1
         plt.yticks(())
         idx = (y_pred == k)
         if idx.any():
-            plt.scatter(X[idx, 0], X[idx, 1], marker='o', c='k')
+            plt.scatter(X[idx, 0], X[idx, 1], marker='o', c='w', edgecolor='k')
 
 ax = plt.axes([0.15, 0.04, 0.7, 0.05])
 plt.title("Probability")
diff --git a/examples/classification/plot_lda_qda.py b/examples/classification/plot_lda_qda.py
index a7da8549748b..920785131f39 100644
--- a/examples/classification/plot_lda_qda.py
+++ b/examples/classification/plot_lda_qda.py
@@ -130,8 +130,8 @@ def plot_lda_cov(lda, splot):
 
 
 def plot_qda_cov(qda, splot):
-    plot_ellipse(splot, qda.means_[0], qda.covariances_[0], 'red')
-    plot_ellipse(splot, qda.means_[1], qda.covariances_[1], 'blue')
+    plot_ellipse(splot, qda.means_[0], qda.covariance_[0], 'red')
+    plot_ellipse(splot, qda.means_[1], qda.covariance_[1], 'blue')
 
 for i, (X, y) in enumerate([dataset_fixed_cov(), dataset_cov()]):
     # Linear Discriminant Analysis
@@ -142,7 +142,7 @@ def plot_qda_cov(qda, splot):
     plt.axis('tight')
 
     # Quadratic Discriminant Analysis
-    qda = QuadraticDiscriminantAnalysis(store_covariances=True)
+    qda = QuadraticDiscriminantAnalysis(store_covariance=True)
     y_pred = qda.fit(X, y).predict(X)
     splot = plot_data(qda, X, y, y_pred, fig_index=2 * i + 2)
     plot_qda_cov(qda, splot)
diff --git a/examples/cluster/plot_cluster_comparison.py b/examples/cluster/plot_cluster_comparison.py
index 39d8bca458cc..f0853e88d9ff 100644
--- a/examples/cluster/plot_cluster_comparison.py
+++ b/examples/cluster/plot_cluster_comparison.py
@@ -116,6 +116,8 @@
         n_clusters=params['n_clusters'], eigen_solver='arpack',
         affinity="nearest_neighbors")
     dbscan = cluster.DBSCAN(eps=params['eps'])
+    optics = cluster.OPTICS(min_samples=30, maxima_ratio=.8,
+                            rejection_ratio=.4)
     affinity_propagation = cluster.AffinityPropagation(
         damping=params['damping'], preference=params['preference'])
     average_linkage = cluster.AgglomerativeClustering(
@@ -133,6 +135,7 @@
         ('Ward', ward),
         ('AgglomerativeClustering', average_linkage),
         ('DBSCAN', dbscan),
+        ('OPTICS', optics),
         ('Birch', birch),
         ('GaussianMixture', gmm)
     )
diff --git a/examples/cluster/plot_color_quantization.py b/examples/cluster/plot_color_quantization.py
index 7ef4ad635365..ccc45eff7330 100644
--- a/examples/cluster/plot_color_quantization.py
+++ b/examples/cluster/plot_color_quantization.py
@@ -61,7 +61,7 @@
 print("done in %0.3fs." % (time() - t0))
 
 
-codebook_random = shuffle(image_array, random_state=0)[:n_colors + 1]
+codebook_random = shuffle(image_array, random_state=0)[:n_colors]
 print("Predicting color indices on the full image (random)")
 t0 = time()
 labels_random = pairwise_distances_argmin(codebook_random,
diff --git a/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py b/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py
index 0801899f7034..b5826105a5e7 100644
--- a/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py
+++ b/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py
@@ -30,7 +30,7 @@
 from sklearn.cluster import FeatureAgglomeration
 from sklearn.linear_model import BayesianRidge
 from sklearn.pipeline import Pipeline
-from sklearn.externals.joblib import Memory
+from sklearn.utils import Memory
 from sklearn.model_selection import GridSearchCV
 from sklearn.model_selection import KFold
 
diff --git a/examples/cluster/plot_kmeans_silhouette_analysis.py b/examples/cluster/plot_kmeans_silhouette_analysis.py
index be622eeda65a..a3797ae061cb 100644
--- a/examples/cluster/plot_kmeans_silhouette_analysis.py
+++ b/examples/cluster/plot_kmeans_silhouette_analysis.py
@@ -139,4 +139,4 @@
                   "with n_clusters = %d" % n_clusters),
                  fontsize=14, fontweight='bold')
 
-    plt.show()
+plt.show()
diff --git a/examples/cluster/plot_optics.py b/examples/cluster/plot_optics.py
new file mode 100755
index 000000000000..19fd683dddc3
--- /dev/null
+++ b/examples/cluster/plot_optics.py
@@ -0,0 +1,101 @@
+"""
+===================================
+Demo of OPTICS clustering algorithm
+===================================
+
+Finds core samples of high density and expands clusters from them.
+This example uses data that is generated so that the clusters have
+different densities.
+
+The clustering is first used in its automatic settings, which is the
+:class:`sklearn.cluster.OPTICS` algorithm, and then setting specific
+thresholds on the reachability, which corresponds to DBSCAN.
+
+We can see that the different clusters of OPTICS can be recovered with
+different choices of thresholds in DBSCAN.
+
+"""
+
+# Authors: Shane Grigsby <refuge@rocktalus.com>
+#          Amy X. Zhang <axz@mit.edu>
+# License: BSD 3 clause
+
+
+from sklearn.cluster import OPTICS
+import matplotlib.gridspec as gridspec
+
+
+import numpy as np
+
+import matplotlib.pyplot as plt
+
+# Generate sample data
+
+np.random.seed(0)
+n_points_per_cluster = 250
+
+C1 = [-5, -2] + .8 * np.random.randn(n_points_per_cluster, 2)
+C2 = [4, -1] + .1 * np.random.randn(n_points_per_cluster, 2)
+C3 = [1, -2] + .2 * np.random.randn(n_points_per_cluster, 2)
+C4 = [-2, 3] + .3 * np.random.randn(n_points_per_cluster, 2)
+C5 = [3, -2] + 1.6 * np.random.randn(n_points_per_cluster, 2)
+C6 = [5, 6] + 2 * np.random.randn(n_points_per_cluster, 2)
+X = np.vstack((C1, C2, C3, C4, C5, C6))
+
+clust = OPTICS(min_samples=9, rejection_ratio=0.5)
+
+# Run the fit
+clust.fit(X)
+
+_, labels_025 = clust.extract_dbscan(0.25)
+_, labels_075 = clust.extract_dbscan(0.75)
+
+space = np.arange(len(X))
+reachability = clust.reachability_[clust.ordering_]
+labels = clust.labels_[clust.ordering_]
+
+plt.figure(figsize=(10, 7))
+G = gridspec.GridSpec(2, 3)
+ax1 = plt.subplot(G[0, :])
+ax2 = plt.subplot(G[1, 0])
+ax3 = plt.subplot(G[1, 1])
+ax4 = plt.subplot(G[1, 2])
+
+# Reachability plot
+color = ['g.', 'r.', 'b.', 'y.', 'c.']
+for k, c in zip(range(0, 5), color):
+    Xk = space[labels == k]
+    Rk = reachability[labels == k]
+    ax1.plot(Xk, Rk, c, alpha=0.3)
+ax1.plot(space[labels == -1], reachability[labels == -1], 'k.', alpha=0.3)
+ax1.plot(space, np.full_like(space, 0.75), 'k-', alpha=0.5)
+ax1.plot(space, np.full_like(space, 0.25), 'k-.', alpha=0.5)
+ax1.set_ylabel('Reachability (epsilon distance)')
+ax1.set_title('Reachability Plot')
+
+# OPTICS
+color = ['g.', 'r.', 'b.', 'y.', 'c.']
+for k, c in zip(range(0, 5), color):
+    Xk = X[clust.labels_ == k]
+    ax2.plot(Xk[:, 0], Xk[:, 1], c, alpha=0.3)
+ax2.plot(X[clust.labels_ == -1, 0], X[clust.labels_ == -1, 1], 'k+', alpha=0.1)
+ax2.set_title('Automatic Clustering\nOPTICS')
+
+# DBSCAN at 0.25
+color = ['g', 'greenyellow', 'olive', 'r', 'b', 'c']
+for k, c in zip(range(0, 6), color):
+    Xk = X[labels_025 == k]
+    ax3.plot(Xk[:, 0], Xk[:, 1], c, alpha=0.3, marker='.')
+ax3.plot(X[labels_025 == -1, 0], X[labels_025 == -1, 1], 'k+', alpha=0.1)
+ax3.set_title('Clustering at 0.25 epsilon cut\nDBSCAN')
+
+# DBSCAN at 0.75
+color = ['g.', 'm.', 'y.', 'c.']
+for k, c in zip(range(0, 4), color):
+    Xk = X[labels_075 == k]
+    ax4.plot(Xk[:, 0], Xk[:, 1], c, alpha=0.3)
+ax4.plot(X[labels_075 == -1, 0], X[labels_075 == -1, 1], 'k+', alpha=0.1)
+ax4.set_title('Clustering at 0.75 epsilon cut\nDBSCAN')
+
+plt.tight_layout()
+plt.show()
diff --git a/examples/cluster/plot_segmentation_toy.py b/examples/cluster/plot_segmentation_toy.py
index aa66c811eda8..a6980c5f271e 100644
--- a/examples/cluster/plot_segmentation_toy.py
+++ b/examples/cluster/plot_segmentation_toy.py
@@ -74,7 +74,7 @@
 # Force the solver to be arpack, since amg is numerically
 # unstable on this example
 labels = spectral_clustering(graph, n_clusters=4, eigen_solver='arpack')
-label_im = -np.ones(mask.shape)
+label_im = np.full(mask.shape, -1.)
 label_im[mask] = labels
 
 plt.matshow(img)
@@ -92,7 +92,7 @@
 graph.data = np.exp(-graph.data / graph.data.std())
 
 labels = spectral_clustering(graph, n_clusters=2, eigen_solver='arpack')
-label_im = -np.ones(mask.shape)
+label_im = np.full(mask.shape, -1.)
 label_im[mask] = labels
 
 plt.matshow(img)
diff --git a/examples/compose/plot_column_transformer.py b/examples/compose/plot_column_transformer.py
index 0161af7fe7e4..010717583ec0 100644
--- a/examples/compose/plot_column_transformer.py
+++ b/examples/compose/plot_column_transformer.py
@@ -40,7 +40,7 @@
 from sklearn.metrics import classification_report
 from sklearn.pipeline import Pipeline
 from sklearn.compose import ColumnTransformer
-from sklearn.svm import SVC
+from sklearn.svm import LinearSVC
 
 
 class TextStats(BaseEstimator, TransformerMixin):
@@ -117,7 +117,7 @@ def transform(self, posts):
     )),
 
     # Use a SVC classifier on the combined features
-    ('svc', SVC(kernel='linear')),
+    ('svc', LinearSVC()),
 ])
 
 # limit the list of categories to make running this example faster.
diff --git a/examples/compose/plot_compare_reduction.py b/examples/compose/plot_compare_reduction.py
index 0dc69b479276..1eca8e40a072 100755
--- a/examples/compose/plot_compare_reduction.py
+++ b/examples/compose/plot_compare_reduction.py
@@ -63,7 +63,7 @@
 ]
 reducer_labels = ['PCA', 'NMF', 'KBest(chi2)']
 
-grid = GridSearchCV(pipe, cv=3, n_jobs=1, param_grid=param_grid)
+grid = GridSearchCV(pipe, cv=5, n_jobs=1, param_grid=param_grid)
 digits = load_digits()
 grid.fit(digits.data, digits.target)
 
@@ -104,7 +104,7 @@
 
 from tempfile import mkdtemp
 from shutil import rmtree
-from sklearn.externals.joblib import Memory
+from sklearn.utils import Memory
 
 # Create a temporary folder to store the transformers of the pipeline
 cachedir = mkdtemp()
@@ -114,7 +114,7 @@
                        memory=memory)
 
 # This time, a cached pipeline will be used within the grid search
-grid = GridSearchCV(cached_pipe, cv=3, n_jobs=1, param_grid=param_grid)
+grid = GridSearchCV(cached_pipe, cv=5, n_jobs=1, param_grid=param_grid)
 digits = load_digits()
 grid.fit(digits.data, digits.target)
 
diff --git a/examples/compose/plot_digits_pipe.py b/examples/compose/plot_digits_pipe.py
index b95d2847ada1..2352abba4584 100644
--- a/examples/compose/plot_digits_pipe.py
+++ b/examples/compose/plot_digits_pipe.py
@@ -54,7 +54,7 @@
 # Parameters of pipelines can be set using ‘__’ separated parameter names:
 estimator = GridSearchCV(pipe,
                          dict(pca__n_components=n_components,
-                              logistic__C=Cs))
+                              logistic__C=Cs), cv=5)
 estimator.fit(X_digits, y_digits)
 
 plt.axvline(estimator.best_estimator_.named_steps['pca'].n_components,
diff --git a/examples/compose/plot_feature_union.py b/examples/compose/plot_feature_union.py
index 4798617f40cb..56d1e320c4e3 100644
--- a/examples/compose/plot_feature_union.py
+++ b/examples/compose/plot_feature_union.py
@@ -55,6 +55,6 @@
                   features__univ_select__k=[1, 2],
                   svm__C=[0.1, 1, 10])
 
-grid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)
+grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, verbose=10)
 grid_search.fit(X, y)
 print(grid_search.best_estimator_)
diff --git a/examples/covariance/plot_covariance_estimation.py b/examples/covariance/plot_covariance_estimation.py
index d33b77d68a43..acbe567c534f 100644
--- a/examples/covariance/plot_covariance_estimation.py
+++ b/examples/covariance/plot_covariance_estimation.py
@@ -83,7 +83,7 @@
 
 # GridSearch for an optimal shrinkage coefficient
 tuned_parameters = [{'shrinkage': shrinkages}]
-cv = GridSearchCV(ShrunkCovariance(), tuned_parameters)
+cv = GridSearchCV(ShrunkCovariance(), tuned_parameters, cv=5)
 cv.fit(X_train)
 
 # Ledoit-Wolf optimal shrinkage coefficient estimate
diff --git a/examples/covariance/plot_mahalanobis_distances.py b/examples/covariance/plot_mahalanobis_distances.py
index 21f295ce5830..816ad2ec2cc5 100644
--- a/examples/covariance/plot_mahalanobis_distances.py
+++ b/examples/covariance/plot_mahalanobis_distances.py
@@ -119,9 +119,9 @@
 emp_mahal = emp_cov.mahalanobis(X - np.mean(X, 0)) ** (0.33)
 subfig2 = plt.subplot(2, 2, 3)
 subfig2.boxplot([emp_mahal[:-n_outliers], emp_mahal[-n_outliers:]], widths=.25)
-subfig2.plot(1.26 * np.ones(n_samples - n_outliers),
+subfig2.plot(np.full(n_samples - n_outliers, 1.26),
              emp_mahal[:-n_outliers], '+k', markeredgewidth=1)
-subfig2.plot(2.26 * np.ones(n_outliers),
+subfig2.plot(np.full(n_outliers, 2.26),
              emp_mahal[-n_outliers:], '+k', markeredgewidth=1)
 subfig2.axes.set_xticklabels(('inliers', 'outliers'), size=15)
 subfig2.set_ylabel(r"$\sqrt[3]{\rm{(Mahal. dist.)}}$", size=16)
@@ -132,9 +132,9 @@
 subfig3 = plt.subplot(2, 2, 4)
 subfig3.boxplot([robust_mahal[:-n_outliers], robust_mahal[-n_outliers:]],
                 widths=.25)
-subfig3.plot(1.26 * np.ones(n_samples - n_outliers),
+subfig3.plot(np.full(n_samples - n_outliers, 1.26),
              robust_mahal[:-n_outliers], '+k', markeredgewidth=1)
-subfig3.plot(2.26 * np.ones(n_outliers),
+subfig3.plot(np.full(n_outliers, 2.26),
              robust_mahal[-n_outliers:], '+k', markeredgewidth=1)
 subfig3.axes.set_xticklabels(('inliers', 'outliers'), size=15)
 subfig3.set_ylabel(r"$\sqrt[3]{\rm{(Mahal. dist.)}}$", size=16)
diff --git a/examples/covariance/plot_outlier_detection.py b/examples/covariance/plot_outlier_detection.py
deleted file mode 100644
index 4c6ea43418b8..000000000000
--- a/examples/covariance/plot_outlier_detection.py
+++ /dev/null
@@ -1,129 +0,0 @@
-"""
-==========================================
-Outlier detection with several methods.
-==========================================
-
-When the amount of contamination is known, this example illustrates three
-different ways of performing :ref:`outlier_detection`:
-
-- based on a robust estimator of covariance, which is assuming that the
-  data are Gaussian distributed and performs better than the One-Class SVM
-  in that case.
-
-- using the One-Class SVM and its ability to capture the shape of the
-  data set, hence performing better when the data is strongly
-  non-Gaussian, i.e. with two well-separated clusters;
-
-- using the Isolation Forest algorithm, which is based on random forests and
-  hence more adapted to large-dimensional settings, even if it performs
-  quite well in the examples below.
-
-- using the Local Outlier Factor to measure the local deviation of a given
-  data point with respect to its neighbors by comparing their local density.
-
-The ground truth about inliers and outliers is given by the points colors
-while the orange-filled area indicates which points are reported as inliers
-by each method.
-
-Here, we assume that we know the fraction of outliers in the datasets.
-Thus rather than using the 'predict' method of the objects, we set the
-threshold on the decision_function to separate out the corresponding
-fraction.
-"""
-
-import numpy as np
-from scipy import stats
-import matplotlib.pyplot as plt
-import matplotlib.font_manager
-
-from sklearn import svm
-from sklearn.covariance import EllipticEnvelope
-from sklearn.ensemble import IsolationForest
-from sklearn.neighbors import LocalOutlierFactor
-
-print(__doc__)
-
-SEED = 42
-GRID_PRECISION = 100
-
-rng = np.random.RandomState(SEED)
-
-# Example settings
-n_samples = 200
-outliers_fraction = 0.25
-clusters_separation = (0, 1, 2)
-
-# define two outlier detection tools to be compared
-classifiers = {
-    "One-Class SVM": svm.OneClassSVM(nu=0.95 * outliers_fraction + 0.05,
-                                     kernel="rbf", gamma=0.1),
-    "Robust covariance": EllipticEnvelope(contamination=outliers_fraction),
-    "Isolation Forest": IsolationForest(max_samples=n_samples,
-                                        contamination=outliers_fraction,
-                                        random_state=rng),
-    "Local Outlier Factor": LocalOutlierFactor(
-        n_neighbors=35,
-        contamination=outliers_fraction)}
-
-# Compare given classifiers under given settings
-xx, yy = np.meshgrid(np.linspace(-7, 7, GRID_PRECISION),
-                     np.linspace(-7, 7, GRID_PRECISION))
-n_outliers = int(outliers_fraction * n_samples)
-n_inliers = n_samples - n_outliers
-ground_truth = np.ones(n_samples, dtype=int)
-ground_truth[-n_outliers:] = -1
-
-# Fit the problem with varying cluster separation
-for _, offset in enumerate(clusters_separation):
-    np.random.seed(SEED)
-    # Data generation
-    X1 = 0.3 * np.random.randn(n_inliers // 2, 2) - offset
-    X2 = 0.3 * np.random.randn(n_inliers // 2, 2) + offset
-    X = np.concatenate([X1, X2], axis=0)
-    # Add outliers
-    X = np.concatenate([X, np.random.uniform(low=-6, high=6,
-                       size=(n_outliers, 2))], axis=0)
-
-    # Fit the model
-    plt.figure(figsize=(9, 7))
-    for i, (clf_name, clf) in enumerate(classifiers.items()):
-        # fit the data and tag outliers
-        if clf_name == "Local Outlier Factor":
-            y_pred = clf.fit_predict(X)
-            scores_pred = clf.negative_outlier_factor_
-        else:
-            clf.fit(X)
-            scores_pred = clf.decision_function(X)
-            y_pred = clf.predict(X)
-        n_errors = (y_pred != ground_truth).sum()
-        # plot the levels lines and the points
-        if clf_name == "Local Outlier Factor":
-            # decision_function is private for LOF
-            Z = clf._decision_function(np.c_[xx.ravel(), yy.ravel()])
-        else:
-            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
-        Z = Z.reshape(xx.shape)
-        subplot = plt.subplot(2, 2, i + 1)
-        subplot.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7),
-                         cmap=plt.cm.Blues_r)
-        a = subplot.contour(xx, yy, Z, levels=[0],
-                            linewidths=2, colors='red')
-        subplot.contourf(xx, yy, Z, levels=[0, Z.max()],
-                         colors='orange')
-        b = subplot.scatter(X[:-n_outliers, 0], X[:-n_outliers, 1], c='white',
-                            s=20, edgecolor='k')
-        c = subplot.scatter(X[-n_outliers:, 0], X[-n_outliers:, 1], c='black',
-                            s=20, edgecolor='k')
-        subplot.axis('tight')
-        subplot.legend(
-            [a.collections[0], b, c],
-            ['learned decision function', 'true inliers', 'true outliers'],
-            prop=matplotlib.font_manager.FontProperties(size=10),
-            loc='lower right')
-        subplot.set_xlabel("%d. %s (errors: %d)" % (i + 1, clf_name, n_errors))
-        subplot.set_xlim((-7, 7))
-        subplot.set_ylim((-7, 7))
-    plt.subplots_adjust(0.04, 0.1, 0.96, 0.94, 0.1, 0.26)
-    plt.suptitle("Outlier detection")
-
-plt.show()
diff --git a/examples/covariance/plot_sparse_cov.py b/examples/covariance/plot_sparse_cov.py
index 313d2544c7e5..a2009bb330d9 100644
--- a/examples/covariance/plot_sparse_cov.py
+++ b/examples/covariance/plot_sparse_cov.py
@@ -83,7 +83,7 @@
 # Estimate the covariance
 emp_cov = np.dot(X.T, X) / n_samples
 
-model = GraphicalLassoCV()
+model = GraphicalLassoCV(cv=5)
 model.fit(X)
 cov_ = model.covariance_
 prec_ = model.precision_
diff --git a/examples/decomposition/plot_faces_decomposition.py b/examples/decomposition/plot_faces_decomposition.py
index 9c2144a77942..81ed20b44b1f 100644
--- a/examples/decomposition/plot_faces_decomposition.py
+++ b/examples/decomposition/plot_faces_decomposition.py
@@ -81,7 +81,8 @@ def plot_gallery(title, images, n_col=n_col, n_row=n_row, cmap=plt.cm.gray):
     ('Sparse comp. - MiniBatchSparsePCA',
      decomposition.MiniBatchSparsePCA(n_components=n_components, alpha=0.8,
                                       n_iter=100, batch_size=3,
-                                      random_state=rng),
+                                      random_state=rng,
+                                      normalize_components=True),
      True),
 
     ('MiniBatchDictionaryLearning',
diff --git a/examples/decomposition/plot_pca_vs_fa_model_selection.py b/examples/decomposition/plot_pca_vs_fa_model_selection.py
index b858434d910e..9d395f70c3dd 100644
--- a/examples/decomposition/plot_pca_vs_fa_model_selection.py
+++ b/examples/decomposition/plot_pca_vs_fa_model_selection.py
@@ -69,20 +69,20 @@ def compute_scores(X):
     for n in n_components:
         pca.n_components = n
         fa.n_components = n
-        pca_scores.append(np.mean(cross_val_score(pca, X)))
-        fa_scores.append(np.mean(cross_val_score(fa, X)))
+        pca_scores.append(np.mean(cross_val_score(pca, X, cv=5)))
+        fa_scores.append(np.mean(cross_val_score(fa, X, cv=5)))
 
     return pca_scores, fa_scores
 
 
 def shrunk_cov_score(X):
     shrinkages = np.logspace(-2, 0, 30)
-    cv = GridSearchCV(ShrunkCovariance(), {'shrinkage': shrinkages})
-    return np.mean(cross_val_score(cv.fit(X).best_estimator_, X))
+    cv = GridSearchCV(ShrunkCovariance(), {'shrinkage': shrinkages}, cv=5)
+    return np.mean(cross_val_score(cv.fit(X).best_estimator_, X, cv=5))
 
 
 def lw_score(X):
-    return np.mean(cross_val_score(LedoitWolf(), X))
+    return np.mean(cross_val_score(LedoitWolf(), X, cv=5))
 
 
 for X, title in [(X_homo, 'Homoscedastic Noise'),
diff --git a/examples/decomposition/plot_sparse_coding.py b/examples/decomposition/plot_sparse_coding.py
index ebc3feade5b0..14cab3374373 100644
--- a/examples/decomposition/plot_sparse_coding.py
+++ b/examples/decomposition/plot_sparse_coding.py
@@ -16,6 +16,8 @@
 """
 print(__doc__)
 
+from distutils.version import LooseVersion
+
 import numpy as np
 import matplotlib.pyplot as plt
 
@@ -64,6 +66,8 @@ def ricker_matrix(width, resolution, n_components):
 estimators = [('OMP', 'omp', None, 15, 'navy'),
               ('Lasso', 'lasso_cd', 2, None, 'turquoise'), ]
 lw = 2
+# Avoid FutureWarning about default value change when numpy >= 1.14
+lstsq_rcond = None if LooseVersion(np.__version__) >= '1.14' else -1
 
 plt.figure(figsize=(13, 6))
 for subplot, (D, title) in enumerate(zip((D_fixed, D_multi),
@@ -88,7 +92,7 @@ def ricker_matrix(width, resolution, n_components):
                         transform_alpha=20)
     x = coder.transform(y.reshape(1, -1))
     _, idx = np.where(x != 0)
-    x[0, idx], _, _, _ = np.linalg.lstsq(D[idx, :].T, y)
+    x[0, idx], _, _, _ = np.linalg.lstsq(D[idx, :].T, y, rcond=lstsq_rcond)
     x = np.ravel(np.dot(x, D))
     squared_error = np.sum((y - x) ** 2)
     plt.plot(x, color='darkorange', lw=lw,
diff --git a/examples/ensemble/plot_ensemble_oob.py b/examples/ensemble/plot_ensemble_oob.py
index 19b01772d5c2..081025c8170d 100644
--- a/examples/ensemble/plot_ensemble_oob.py
+++ b/examples/ensemble/plot_ensemble_oob.py
@@ -45,15 +45,18 @@
 # error trajectory during training.
 ensemble_clfs = [
     ("RandomForestClassifier, max_features='sqrt'",
-        RandomForestClassifier(warm_start=True, oob_score=True,
+        RandomForestClassifier(n_estimators=100,
+                               warm_start=True, oob_score=True,
                                max_features="sqrt",
                                random_state=RANDOM_STATE)),
     ("RandomForestClassifier, max_features='log2'",
-        RandomForestClassifier(warm_start=True, max_features='log2',
+        RandomForestClassifier(n_estimators=100,
+                               warm_start=True, max_features='log2',
                                oob_score=True,
                                random_state=RANDOM_STATE)),
     ("RandomForestClassifier, max_features=None",
-        RandomForestClassifier(warm_start=True, max_features=None,
+        RandomForestClassifier(n_estimators=100,
+                               warm_start=True, max_features=None,
                                oob_score=True,
                                random_state=RANDOM_STATE))
 ]
diff --git a/examples/ensemble/plot_feature_transformation.py b/examples/ensemble/plot_feature_transformation.py
index bb1fd7ec6684..5dbc2754b3a3 100644
--- a/examples/ensemble/plot_feature_transformation.py
+++ b/examples/ensemble/plot_feature_transformation.py
@@ -62,7 +62,7 @@
 
 # Supervised transformation based on random forests
 rf = RandomForestClassifier(max_depth=3, n_estimators=n_estimator)
-rf_enc = OneHotEncoder()
+rf_enc = OneHotEncoder(categories='auto')
 rf_lm = LogisticRegression()
 rf.fit(X_train, y_train)
 rf_enc.fit(rf.apply(X_train))
@@ -72,7 +72,7 @@
 fpr_rf_lm, tpr_rf_lm, _ = roc_curve(y_test, y_pred_rf_lm)
 
 grd = GradientBoostingClassifier(n_estimators=n_estimator)
-grd_enc = OneHotEncoder()
+grd_enc = OneHotEncoder(categories='auto')
 grd_lm = LogisticRegression()
 grd.fit(X_train, y_train)
 grd_enc.fit(grd.apply(X_train)[:, :, 0])
diff --git a/examples/ensemble/plot_gradient_boosting_oob.py b/examples/ensemble/plot_gradient_boosting_oob.py
index dfae1ad9b8a9..ea38b326ce5c 100644
--- a/examples/ensemble/plot_gradient_boosting_oob.py
+++ b/examples/ensemble/plot_gradient_boosting_oob.py
@@ -74,7 +74,7 @@ def heldout_score(clf, X_test, y_test):
     return score
 
 
-def cv_estimate(n_splits=3):
+def cv_estimate(n_splits=None):
     cv = KFold(n_splits=n_splits)
     cv_clf = ensemble.GradientBoostingClassifier(**params)
     val_scores = np.zeros((n_estimators,), dtype=np.float64)
diff --git a/examples/ensemble/plot_isolation_forest.py b/examples/ensemble/plot_isolation_forest.py
index e6c8ce3bf8ef..1b79072dff64 100644
--- a/examples/ensemble/plot_isolation_forest.py
+++ b/examples/ensemble/plot_isolation_forest.py
@@ -40,7 +40,8 @@
 X_outliers = rng.uniform(low=-4, high=4, size=(20, 2))
 
 # fit the model
-clf = IsolationForest(max_samples=100, random_state=rng)
+clf = IsolationForest(behaviour='new', max_samples=100,
+                      random_state=rng, contamination='auto')
 clf.fit(X_train)
 y_pred_train = clf.predict(X_train)
 y_pred_test = clf.predict(X_test)
diff --git a/examples/ensemble/plot_random_forest_regression_multioutput.py b/examples/ensemble/plot_random_forest_regression_multioutput.py
index 44618357cda4..8b7803361a60 100644
--- a/examples/ensemble/plot_random_forest_regression_multioutput.py
+++ b/examples/ensemble/plot_random_forest_regression_multioutput.py
@@ -39,16 +39,17 @@
 y = np.array([np.pi * np.sin(X).ravel(), np.pi * np.cos(X).ravel()]).T
 y += (0.5 - rng.rand(*y.shape))
 
-X_train, X_test, y_train, y_test = train_test_split(X, y,
-                                                    train_size=400,
-                                                    random_state=4)
+X_train, X_test, y_train, y_test = train_test_split(
+    X, y, train_size=400, test_size=200, random_state=4)
 
 max_depth = 30
-regr_multirf = MultiOutputRegressor(RandomForestRegressor(max_depth=max_depth,
+regr_multirf = MultiOutputRegressor(RandomForestRegressor(n_estimators=100,
+                                                          max_depth=max_depth,
                                                           random_state=0))
 regr_multirf.fit(X_train, y_train)
 
-regr_rf = RandomForestRegressor(max_depth=max_depth, random_state=2)
+regr_rf = RandomForestRegressor(n_estimators=100, max_depth=max_depth,
+                                random_state=2)
 regr_rf.fit(X_train, y_train)
 
 # Predict on new data
diff --git a/examples/ensemble/plot_voting_decision_regions.py b/examples/ensemble/plot_voting_decision_regions.py
index b0a3fbe05745..86da618d48cd 100644
--- a/examples/ensemble/plot_voting_decision_regions.py
+++ b/examples/ensemble/plot_voting_decision_regions.py
@@ -39,7 +39,7 @@
 # Training classifiers
 clf1 = DecisionTreeClassifier(max_depth=4)
 clf2 = KNeighborsClassifier(n_neighbors=7)
-clf3 = SVC(kernel='rbf', probability=True)
+clf3 = SVC(gamma=.1, kernel='rbf', probability=True)
 eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2),
                                     ('svc', clf3)],
                         voting='soft', weights=[2, 1, 2])
diff --git a/examples/ensemble/plot_voting_probas.py b/examples/ensemble/plot_voting_probas.py
index 7bed271fbf9b..c729818620a6 100644
--- a/examples/ensemble/plot_voting_probas.py
+++ b/examples/ensemble/plot_voting_probas.py
@@ -30,7 +30,7 @@
 from sklearn.ensemble import VotingClassifier
 
 clf1 = LogisticRegression(random_state=123)
-clf2 = RandomForestClassifier(random_state=123)
+clf2 = RandomForestClassifier(n_estimators=100, random_state=123)
 clf3 = GaussianNB()
 X = np.array([[-1.0, -1.0], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])
 y = np.array([1, 1, 2, 2])
diff --git a/examples/exercises/plot_cv_diabetes.py b/examples/exercises/plot_cv_diabetes.py
index 76b0d81b8998..d68fd21bd70a 100644
--- a/examples/exercises/plot_cv_diabetes.py
+++ b/examples/exercises/plot_cv_diabetes.py
@@ -29,7 +29,7 @@
 alphas = np.logspace(-4, -0.5, 30)
 
 tuned_parameters = [{'alpha': alphas}]
-n_folds = 3
+n_folds = 5
 
 clf = GridSearchCV(lasso, tuned_parameters, cv=n_folds, refit=False)
 clf.fit(X, y)
@@ -60,7 +60,7 @@
 # performs cross-validation on the training data it receives).
 # We use external cross-validation to see how much the automatically obtained
 # alphas differ across different cross-validation folds.
-lasso_cv = LassoCV(alphas=alphas, random_state=0)
+lasso_cv = LassoCV(alphas=alphas, cv=5, random_state=0)
 k_fold = KFold(3)
 
 print("Answer to the bonus question:",
diff --git a/examples/exercises/plot_cv_digits.py b/examples/exercises/plot_cv_digits.py
index a68f92afbdad..f51bcc7e0256 100644
--- a/examples/exercises/plot_cv_digits.py
+++ b/examples/exercises/plot_cv_digits.py
@@ -26,7 +26,7 @@
 scores_std = list()
 for C in C_s:
     svc.C = C
-    this_scores = cross_val_score(svc, X, y, n_jobs=1)
+    this_scores = cross_val_score(svc, X, y, cv=5, n_jobs=1)
     scores.append(np.mean(this_scores))
     scores_std.append(np.std(this_scores))
 
diff --git a/examples/feature_selection/plot_select_from_model_boston.py b/examples/feature_selection/plot_select_from_model_boston.py
index 17ef6d6bd014..400a736942b6 100644
--- a/examples/feature_selection/plot_select_from_model_boston.py
+++ b/examples/feature_selection/plot_select_from_model_boston.py
@@ -23,7 +23,7 @@
 X, y = boston['data'], boston['target']
 
 # We use the base estimator LassoCV since the L1 norm promotes sparsity of features.
-clf = LassoCV()
+clf = LassoCV(cv=5)
 
 # Set a minimum threshold of 0.25
 sfm = SelectFromModel(clf, threshold=0.25)
diff --git a/examples/gaussian_process/plot_gpc_xor.py b/examples/gaussian_process/plot_gpc_xor.py
index 957d9bd7edc3..04f014e13e8a 100644
--- a/examples/gaussian_process/plot_gpc_xor.py
+++ b/examples/gaussian_process/plot_gpc_xor.py
@@ -42,8 +42,8 @@
     image = plt.imshow(Z, interpolation='nearest',
                        extent=(xx.min(), xx.max(), yy.min(), yy.max()),
                        aspect='auto', origin='lower', cmap=plt.cm.PuOr_r)
-    contours = plt.contour(xx, yy, Z, levels=[0], linewidths=2,
-                           linetypes='--')
+    contours = plt.contour(xx, yy, Z, levels=[0.5], linewidths=2,
+                           colors=['k'])
     plt.scatter(X[:, 0], X[:, 1], s=30, c=Y, cmap=plt.cm.Paired,
                 edgecolors=(0, 0, 0))
     plt.xticks(())
diff --git a/examples/gaussian_process/plot_gpr_co2.py b/examples/gaussian_process/plot_gpr_co2.py
index b0b271a3644a..8170de01898d 100644
--- a/examples/gaussian_process/plot_gpr_co2.py
+++ b/examples/gaussian_process/plot_gpr_co2.py
@@ -8,7 +8,7 @@
 hyperparameter optimization using gradient ascent on the
 log-marginal-likelihood. The data consists of the monthly average atmospheric
 CO2 concentrations (in parts per million by volume (ppmv)) collected at the
-Mauna Loa Observatory in Hawaii, between 1958 and 1997. The objective is to
+Mauna Loa Observatory in Hawaii, between 1958 and 2001. The objective is to
 model the CO2 concentration as a function of the time t.
 
 The kernel is composed of several terms that are responsible for explaining
@@ -57,12 +57,12 @@
 explained by the model. The figure shows also that the model makes very
 confident predictions until around 2015.
 """
-print(__doc__)
-
 # Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
 #
 # License: BSD 3 clause
 
+from __future__ import division, print_function
+
 import numpy as np
 
 from matplotlib import pyplot as plt
@@ -70,11 +70,46 @@
 from sklearn.gaussian_process import GaussianProcessRegressor
 from sklearn.gaussian_process.kernels \
     import RBF, WhiteKernel, RationalQuadratic, ExpSineSquared
-from sklearn.datasets import fetch_mldata
+try:
+    from urllib.request import urlopen
+except ImportError:
+    # Python 2
+    from urllib2 import urlopen
+
+print(__doc__)
+
 
-data = fetch_mldata('mauna-loa-atmospheric-co2').data
-X = data[:, [1]]
-y = data[:, 0]
+def load_mauna_loa_atmospheric_c02():
+    url = ('http://cdiac.ess-dive.lbl.gov/'
+           'ftp/trends/co2/sio-keel-flask/maunaloa_c.dat')
+    months = []
+    ppmv_sums = []
+    counts = []
+    for line in urlopen(url):
+        line = line.decode('utf8')
+        if not line.startswith('MLO'):
+            # ignore headers
+            continue
+        station, date, weight, flag, ppmv = line.split()
+        y = date[:2]
+        m = date[2:4]
+        month_float = (int(('20' if y < '20' else '19') + y) +
+                       (int(m) - 1) / 12)
+        if not months or month_float != months[-1]:
+            months.append(month_float)
+            ppmv_sums.append(float(ppmv))
+            counts.append(1)
+        else:
+            # aggregate monthly sum to produce average
+            ppmv_sums[-1] += float(ppmv)
+            counts[-1] += 1
+
+    months = np.asarray(months).reshape(-1, 1)
+    avg_ppmvs = np.asarray(ppmv_sums) / counts
+    return months, avg_ppmvs
+
+
+X, y = load_mauna_loa_atmospheric_c02()
 
 # Kernel with parameters given in GPML book
 k1 = 66.0**2 * RBF(length_scale=67.0)  # long term smooth rising trend
diff --git a/examples/linear_model/plot_ard.py b/examples/linear_model/plot_ard.py
index 38c334a217df..177bd8ce24ad 100644
--- a/examples/linear_model/plot_ard.py
+++ b/examples/linear_model/plot_ard.py
@@ -76,7 +76,7 @@
 plt.figure(figsize=(6, 5))
 plt.title("Histogram of the weights")
 plt.hist(clf.coef_, bins=n_features, color='navy', log=True)
-plt.scatter(clf.coef_[relevant_features], 5 * np.ones(len(relevant_features)),
+plt.scatter(clf.coef_[relevant_features], np.full(len(relevant_features), 5.),
             color='gold', marker='o', label="Relevant features")
 plt.ylabel("Features")
 plt.xlabel("Values of the weights")
diff --git a/examples/linear_model/plot_bayesian_ridge.py b/examples/linear_model/plot_bayesian_ridge.py
index 4359c421ea86..43925e72c591 100644
--- a/examples/linear_model/plot_bayesian_ridge.py
+++ b/examples/linear_model/plot_bayesian_ridge.py
@@ -74,7 +74,7 @@
 plt.title("Histogram of the weights")
 plt.hist(clf.coef_, bins=n_features, color='gold', log=True,
          edgecolor='black')
-plt.scatter(clf.coef_[relevant_features], 5 * np.ones(len(relevant_features)),
+plt.scatter(clf.coef_[relevant_features], np.full(len(relevant_features), 5.),
             color='navy', label="Relevant features")
 plt.ylabel("Features")
 plt.xlabel("Values of the weights")
diff --git a/examples/linear_model/plot_omp.py b/examples/linear_model/plot_omp.py
index f07b7d723340..8a3b52fc588f 100644
--- a/examples/linear_model/plot_omp.py
+++ b/examples/linear_model/plot_omp.py
@@ -67,7 +67,7 @@
 
 # plot the noisy reconstruction with number of non-zeros set by CV
 ##################################################################
-omp_cv = OrthogonalMatchingPursuitCV()
+omp_cv = OrthogonalMatchingPursuitCV(cv=5)
 omp_cv.fit(X, y_noisy)
 coef = omp_cv.coef_
 idx_r, = coef.nonzero()
diff --git a/examples/linear_model/plot_sgd_early_stopping.py b/examples/linear_model/plot_sgd_early_stopping.py
new file mode 100644
index 000000000000..4076fa5f6b28
--- /dev/null
+++ b/examples/linear_model/plot_sgd_early_stopping.py
@@ -0,0 +1,150 @@
+"""
+=============================================
+Early stopping of Stochastic Gradient Descent
+=============================================
+
+Stochastic Gradient Descent is an optimization technique which minimizes a loss
+function in a stochastic fashion, performing a gradient descent step sample by
+sample. In particular, it is a very efficient method to fit linear models.
+
+As a stochastic method, the loss function is not necessarily decreasing at each
+iteration, and convergence is only guaranteed in expectation. For this reason,
+monitoring the convergence on the loss function can be difficult.
+
+Another approach is to monitor convergence on a validation score. In this case,
+the input data is split into a training set and a validation set. The model is
+then fitted on the training set and the stopping criterion is based on the
+prediction score computed on the validation set. This enables us to find the
+least number of iterations which is sufficient to build a model that
+generalizes well to unseen data and reduces the chance of over-fitting the
+training data.
+
+This early stopping strategy is activated if ``early_stopping=True``; otherwise
+the stopping criterion only uses the training loss on the entire input data. To
+better control the early stopping strategy, we can specify a parameter
+``validation_fraction`` which set the fraction of the input dataset that we
+keep aside to compute the validation score. The optimization will continue
+until the validation score did not improve by at least ``tol`` during the last
+``n_iter_no_change`` iterations. The actual number of iterations is available
+at the attribute ``n_iter_``.
+
+This example illustrates how the early stopping can used in the
+:class:`sklearn.linear_model.SGDClassifier` model to achieve almost the same
+accuracy as compared to a model built without early stopping. This can
+significantly reduce training time. Note that scores differ between the
+stopping criteria even from early iterations because some of the training data
+is held out with the validation stopping criterion.
+"""
+# Authors: Tom Dupre la Tour
+#
+# License: BSD 3 clause
+from __future__ import print_function
+import time
+import sys
+
+import pandas as pd
+import numpy as np
+import matplotlib.pyplot as plt
+
+from sklearn import linear_model
+from sklearn.datasets import fetch_openml
+from sklearn.model_selection import train_test_split
+from sklearn.utils.testing import ignore_warnings
+from sklearn.exceptions import ConvergenceWarning
+from sklearn.utils import shuffle
+
+print(__doc__)
+
+
+def load_mnist(n_samples=None, class_0='0', class_1='8'):
+    """Load MNIST, select two classes, shuffle and return only n_samples."""
+    # Load data from http://openml.org/d/554
+    mnist = fetch_openml('mnist_784', version=1)
+
+    # take only two classes for binary classification
+    mask = np.logical_or(mnist.target == class_0, mnist.target == class_1)
+
+    X, y = shuffle(mnist.data[mask], mnist.target[mask], random_state=42)
+    if n_samples is not None:
+        X, y = X[:n_samples], y[:n_samples]
+    return X, y
+
+
+@ignore_warnings(category=ConvergenceWarning)
+def fit_and_score(estimator, max_iter, X_train, X_test, y_train, y_test):
+    """Fit the estimator on the train set and score it on both sets"""
+    estimator.set_params(max_iter=max_iter)
+    estimator.set_params(random_state=0)
+
+    start = time.time()
+    estimator.fit(X_train, y_train)
+
+    fit_time = time.time() - start
+    n_iter = estimator.n_iter_
+    train_score = estimator.score(X_train, y_train)
+    test_score = estimator.score(X_test, y_test)
+
+    return fit_time, n_iter, train_score, test_score
+
+
+# Define the estimators to compare
+estimator_dict = {
+    'No stopping criterion':
+    linear_model.SGDClassifier(tol=None, n_iter_no_change=3),
+    'Training loss':
+    linear_model.SGDClassifier(early_stopping=False, n_iter_no_change=3,
+                               tol=0.1),
+    'Validation score':
+    linear_model.SGDClassifier(early_stopping=True, n_iter_no_change=3,
+                               tol=0.0001, validation_fraction=0.2)
+}
+
+# Load the dataset
+X, y = load_mnist(n_samples=10000)
+X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5,
+                                                    random_state=0)
+
+results = []
+for estimator_name, estimator in estimator_dict.items():
+    print(estimator_name + ': ', end='')
+    for max_iter in range(1, 50):
+        print('.', end='')
+        sys.stdout.flush()
+
+        fit_time, n_iter, train_score, test_score = fit_and_score(
+            estimator, max_iter, X_train, X_test, y_train, y_test)
+
+        results.append((estimator_name, max_iter, fit_time, n_iter,
+                        train_score, test_score))
+    print('')
+
+# Transform the results in a pandas dataframe for easy plotting
+columns = [
+    'Stopping criterion', 'max_iter', 'Fit time (sec)', 'n_iter_',
+    'Train score', 'Test score'
+]
+results_df = pd.DataFrame(results, columns=columns)
+
+# Define what to plot (x_axis, y_axis)
+lines = 'Stopping criterion'
+plot_list = [
+    ('max_iter', 'Train score'),
+    ('max_iter', 'Test score'),
+    ('max_iter', 'n_iter_'),
+    ('max_iter', 'Fit time (sec)'),
+]
+
+nrows = 2
+ncols = int(np.ceil(len(plot_list) / 2.))
+fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(6 * ncols,
+                                                            4 * nrows))
+axes[0, 0].get_shared_y_axes().join(axes[0, 0], axes[0, 1])
+
+for ax, (x_axis, y_axis) in zip(axes.ravel(), plot_list):
+    for criterion, group_df in results_df.groupby(lines):
+        group_df.plot(x=x_axis, y=y_axis, label=criterion, ax=ax)
+    ax.set_title(y_axis)
+    ax.legend(title=lines)
+
+fig.tight_layout()
+plt.show()
diff --git a/examples/linear_model/plot_sparse_logistic_regression_mnist.py b/examples/linear_model/plot_sparse_logistic_regression_mnist.py
index 5610f471b5d0..523f5683a5a1 100644
--- a/examples/linear_model/plot_sparse_logistic_regression_mnist.py
+++ b/examples/linear_model/plot_sparse_logistic_regression_mnist.py
@@ -20,7 +20,7 @@
 import matplotlib.pyplot as plt
 import numpy as np
 
-from sklearn.datasets import fetch_mldata
+from sklearn.datasets import fetch_openml
 from sklearn.linear_model import LogisticRegression
 from sklearn.model_selection import train_test_split
 from sklearn.preprocessing import StandardScaler
@@ -35,9 +35,9 @@
 t0 = time.time()
 train_samples = 5000
 
-mnist = fetch_mldata('MNIST original')
-X = mnist.data.astype('float64')
-y = mnist.target
+# Load data from https://www.openml.org/d/554
+X, y = fetch_openml('mnist_784', version=1, return_X_y=True)
+
 random_state = check_random_state(0)
 permutation = random_state.permutation(X.shape[0])
 X = X[permutation]
diff --git a/examples/manifold/plot_mds.py b/examples/manifold/plot_mds.py
index 29d9c548e1a2..6398e2f7a624 100644
--- a/examples/manifold/plot_mds.py
+++ b/examples/manifold/plot_mds.py
@@ -82,7 +82,7 @@
                     zorder=0, cmap=plt.cm.Blues,
                     norm=plt.Normalize(0, values.max()))
 lc.set_array(similarities.flatten())
-lc.set_linewidths(0.5 * np.ones(len(segments)))
+lc.set_linewidths(np.full(len(segments), 0.5))
 ax.add_collection(lc)
 
 plt.show()
diff --git a/examples/mixture/plot_concentration_prior.py b/examples/mixture/plot_concentration_prior.py
index b7e121c7cb30..14930a6eafdb 100644
--- a/examples/mixture/plot_concentration_prior.py
+++ b/examples/mixture/plot_concentration_prior.py
@@ -116,7 +116,7 @@ def plot_results(ax1, ax2, estimator, X, y, title, plot_title=False):
 X = np.vstack([
     rng.multivariate_normal(means[j], covars[j], samples[j])
     for j in range(n_components)])
-y = np.concatenate([j * np.ones(samples[j], dtype=int)
+y = np.concatenate([np.full(samples[j], j, dtype=int)
                     for j in range(n_components)])
 
 # Plot results in two different figures
diff --git a/examples/model_selection/grid_search_text_feature_extraction.py b/examples/model_selection/grid_search_text_feature_extraction.py
index 88090613fcd7..c220a43ed858 100644
--- a/examples/model_selection/grid_search_text_feature_extraction.py
+++ b/examples/model_selection/grid_search_text_feature_extraction.py
@@ -22,7 +22,7 @@
   pipeline: ['vect', 'tfidf', 'clf']
   parameters:
   {'clf__alpha': (1.0000000000000001e-05, 9.9999999999999995e-07),
-   'clf__n_iter': (10, 50, 80),
+   'clf__max_iter': (10, 50, 80),
    'clf__penalty': ('l2', 'elasticnet'),
    'tfidf__use_idf': (True, False),
    'vect__max_n': (1, 2),
@@ -33,7 +33,7 @@
   Best score: 0.940
   Best parameters set:
       clf__alpha: 9.9999999999999995e-07
-      clf__n_iter: 50
+      clf__max_iter: 50
       clf__penalty: 'elasticnet'
       tfidf__use_idf: True
       vect__max_n: 2
@@ -97,14 +97,14 @@
 # increase processing time in a combinatorial way
 parameters = {
     'vect__max_df': (0.5, 0.75, 1.0),
-    #'vect__max_features': (None, 5000, 10000, 50000),
+    # 'vect__max_features': (None, 5000, 10000, 50000),
     'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams
-    #'tfidf__use_idf': (True, False),
-    #'tfidf__norm': ('l1', 'l2'),
+    # 'tfidf__use_idf': (True, False),
+    # 'tfidf__norm': ('l1', 'l2'),
     'clf__max_iter': (5,),
     'clf__alpha': (0.00001, 0.000001),
     'clf__penalty': ('l2', 'elasticnet'),
-    #'clf__n_iter': (10, 50, 80),
+    # 'clf__max_iter': (10, 50, 80),
 }
 
 if __name__ == "__main__":
@@ -113,7 +113,8 @@
 
     # find the best parameters for both the feature extraction and the
     # classifier
-    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)
+    grid_search = GridSearchCV(pipeline, parameters, cv=5,
+                               n_jobs=-1, verbose=1)
 
     print("Performing grid search...")
     print("pipeline:", [name for name, _ in pipeline.steps])
diff --git a/examples/model_selection/plot_cv_indices.py b/examples/model_selection/plot_cv_indices.py
new file mode 100644
index 000000000000..078c3f5e54d1
--- /dev/null
+++ b/examples/model_selection/plot_cv_indices.py
@@ -0,0 +1,149 @@
+"""
+Visualizing cross-validation behavior in scikit-learn
+=====================================================
+
+Choosing the right cross-validation object is a crucial part of fitting a
+model properly. There are many ways to split data into training and test
+sets in order to avoid model overfitting, to standardize the number of
+groups in test sets, etc.
+
+This example visualizes the behavior of several common scikit-learn objects
+for comparison.
+"""
+
+from sklearn.model_selection import (TimeSeriesSplit, KFold, ShuffleSplit,
+                                     StratifiedKFold, GroupShuffleSplit,
+                                     GroupKFold, StratifiedShuffleSplit)
+import numpy as np
+import matplotlib.pyplot as plt
+from matplotlib.patches import Patch
+np.random.seed(1338)
+cmap_data = plt.cm.Paired
+cmap_cv = plt.cm.coolwarm
+n_splits = 4
+
+###############################################################################
+# Visualize our data
+# ------------------
+#
+# First, we must understand the structure of our data. It has 100 randomly
+# generated input datapoints, 3 classes split unevenly across datapoints,
+# and 10 "groups" split evenly across datapoints.
+#
+# As we'll see, some cross-validation objects do specific things with
+# labeled data, others behave differently with grouped data, and others
+# do not use this information.
+#
+# To begin, we'll visualize our data.
+
+# Generate the class/group data
+n_points = 100
+X = np.random.randn(100, 10)
+
+percentiles_classes = [.1, .3, .6]
+y = np.hstack([[ii] * int(100 * perc)
+               for ii, perc in enumerate(percentiles_classes)])
+
+# Evenly spaced groups repeated once
+groups = np.hstack([[ii] * 10 for ii in range(10)])
+
+
+def visualize_groups(classes, groups, name):
+    # Visualize dataset groups
+    fig, ax = plt.subplots()
+    ax.scatter(range(len(groups)),  [.5] * len(groups), c=groups, marker='_',
+               lw=50, cmap=cmap_data)
+    ax.scatter(range(len(groups)),  [3.5] * len(groups), c=classes, marker='_',
+               lw=50, cmap=cmap_data)
+    ax.set(ylim=[-1, 5], yticks=[.5, 3.5],
+           yticklabels=['Data\ngroup', 'Data\nclass'], xlabel="Sample index")
+
+
+visualize_groups(y, groups, 'no groups')
+
+###############################################################################
+# Define a function to visualize cross-validation behavior
+# --------------------------------------------------------
+#
+# We'll define a function that lets us visualize the behavior of each
+# cross-validation object. We'll perform 4 splits of the data. On each
+# split, we'll visualize the indices chosen for the training set
+# (in blue) and the test set (in red).
+
+
+def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):
+    """Create a sample plot for indices of a cross-validation object."""
+
+    # Generate the training/testing visualizations for each CV split
+    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):
+        # Fill in indices with the training/test groups
+        indices = np.array([np.nan] * len(X))
+        indices[tt] = 1
+        indices[tr] = 0
+
+        # Visualize the results
+        ax.scatter(range(len(indices)), [ii + .5] * len(indices),
+                   c=indices, marker='_', lw=lw, cmap=cmap_cv,
+                   vmin=-.2, vmax=1.2)
+
+    # Plot the data classes and groups at the end
+    ax.scatter(range(len(X)), [ii + 1.5] * len(X),
+               c=y, marker='_', lw=lw, cmap=cmap_data)
+
+    ax.scatter(range(len(X)), [ii + 2.5] * len(X),
+               c=group, marker='_', lw=lw, cmap=cmap_data)
+
+    # Formatting
+    yticklabels = list(range(n_splits)) + ['class', 'group']
+    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,
+           xlabel='Sample index', ylabel="CV iteration",
+           ylim=[n_splits+2.2, -.2], xlim=[0, 100])
+    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)
+    return ax
+
+
+###############################################################################
+# Let's see how it looks for the `KFold` cross-validation object:
+
+fig, ax = plt.subplots()
+cv = KFold(n_splits)
+plot_cv_indices(cv, X, y, groups, ax, n_splits)
+
+###############################################################################
+# As you can see, by default the KFold cross-validation iterator does not
+# take either datapoint class or group into consideration. We can change this
+# by using the ``StratifiedKFold`` like so.
+
+fig, ax = plt.subplots()
+cv = StratifiedKFold(n_splits)
+plot_cv_indices(cv, X, y, groups, ax, n_splits)
+
+###############################################################################
+# In this case, the cross-validation retained the same ratio of classes across
+# each CV split. Next we'll visualize this behavior for a number of CV
+# iterators.
+#
+# Visualize cross-validation indices for many CV objects
+# ------------------------------------------------------
+#
+# Let's visually compare the cross validation behavior for many
+# scikit-learn cross-validation objects. Below we will loop through several
+# common cross-validation objects, visualizing the behavior of each.
+#
+# Note how some use the group/class information while others do not.
+
+cvs = [KFold, GroupKFold, ShuffleSplit, StratifiedKFold,
+       GroupShuffleSplit, StratifiedShuffleSplit, TimeSeriesSplit]
+
+
+for cv in cvs:
+    this_cv = cv(n_splits=n_splits)
+    fig, ax = plt.subplots(figsize=(6, 3))
+    plot_cv_indices(this_cv, X, y, groups, ax, n_splits)
+
+    ax.legend([Patch(color=cmap_cv(.8)), Patch(color=cmap_cv(.02))],
+              ['Testing set', 'Training set'], loc=(1.02, .8))
+    # Make the legend fit
+    plt.tight_layout()
+    fig.subplots_adjust(right=.7)
+plt.show()
diff --git a/examples/model_selection/plot_learning_curve.py b/examples/model_selection/plot_learning_curve.py
index 6e022ebe2718..4d86c323f53b 100644
--- a/examples/model_selection/plot_learning_curve.py
+++ b/examples/model_selection/plot_learning_curve.py
@@ -25,7 +25,7 @@
 
 
 def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,
-                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):
+                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):
     """
     Generate a simple plot of the test and training learning curve.
 
@@ -63,8 +63,11 @@ def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validators that can be used here.
 
-    n_jobs : integer, optional
-        Number of jobs to run in parallel (default 1).
+    n_jobs : int or None, optional (default=None)
+        Number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     train_sizes : array-like, shape (n_ticks,), dtype float or int
         Relative or absolute numbers of training examples that will be used to
diff --git a/examples/model_selection/plot_multi_metric_evaluation.py b/examples/model_selection/plot_multi_metric_evaluation.py
index ea7d60dc20da..e1b5cbb30312 100644
--- a/examples/model_selection/plot_multi_metric_evaluation.py
+++ b/examples/model_selection/plot_multi_metric_evaluation.py
@@ -43,11 +43,11 @@
 # Setting refit='AUC', refits an estimator on the whole dataset with the
 # parameter setting that has the best cross-validated AUC score.
 # That estimator is made available at ``gs.best_estimator_`` along with
-# parameters like ``gs.best_score_``, ``gs.best_parameters_`` and
+# parameters like ``gs.best_score_``, ``gs.best_params_`` and
 # ``gs.best_index_``
 gs = GridSearchCV(DecisionTreeClassifier(random_state=42),
                   param_grid={'min_samples_split': range(2, 403, 10)},
-                  scoring=scoring, cv=5, refit='AUC')
+                  scoring=scoring, cv=5, refit='AUC', return_train_score=True)
 gs.fit(X, y)
 results = gs.cv_results_
 
@@ -61,9 +61,8 @@
 
 plt.xlabel("min_samples_split")
 plt.ylabel("Score")
-plt.grid()
 
-ax = plt.axes()
+ax = plt.gca()
 ax.set_xlim(0, 402)
 ax.set_ylim(0.73, 1)
 
diff --git a/examples/model_selection/plot_randomized_search.py b/examples/model_selection/plot_randomized_search.py
index bac6495090e6..68606a570f15 100644
--- a/examples/model_selection/plot_randomized_search.py
+++ b/examples/model_selection/plot_randomized_search.py
@@ -62,7 +62,7 @@ def report(results, n_top=3):
 # run randomized search
 n_iter_search = 20
 random_search = RandomizedSearchCV(clf, param_distributions=param_dist,
-                                   n_iter=n_iter_search)
+                                   n_iter=n_iter_search, cv=5)
 
 start = time()
 random_search.fit(X, y)
@@ -79,7 +79,7 @@ def report(results, n_top=3):
               "criterion": ["gini", "entropy"]}
 
 # run grid search
-grid_search = GridSearchCV(clf, param_grid=param_grid)
+grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5)
 start = time()
 grid_search.fit(X, y)
 
diff --git a/examples/multioutput/plot_classifier_chain_yeast.py b/examples/multioutput/plot_classifier_chain_yeast.py
index 6a90e14dfc37..cb3a5085e316 100644
--- a/examples/multioutput/plot_classifier_chain_yeast.py
+++ b/examples/multioutput/plot_classifier_chain_yeast.py
@@ -32,24 +32,23 @@
 with randomly ordered chains).
 """
 
-print(__doc__)
-
 # Author: Adam Kleczewski
 # License: BSD 3 clause
 
 import numpy as np
 import matplotlib.pyplot as plt
+from sklearn.datasets import fetch_openml
 from sklearn.multioutput import ClassifierChain
 from sklearn.model_selection import train_test_split
 from sklearn.multiclass import OneVsRestClassifier
 from sklearn.metrics import jaccard_similarity_score
 from sklearn.linear_model import LogisticRegression
-from sklearn.datasets import fetch_mldata
 
-# Load a multi-label dataset
-yeast = fetch_mldata('yeast')
-X = yeast['data']
-Y = yeast['target'].transpose().toarray()
+print(__doc__)
+
+# Load a multi-label dataset from https://www.openml.org/d/40597
+X, Y = fetch_openml('yeast', version=4, return_X_y=True)
+Y = Y == 'TRUE'
 X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2,
                                                     random_state=0)
 
diff --git a/examples/neighbors/plot_digits_kde_sampling.py b/examples/neighbors/plot_digits_kde_sampling.py
index 8367d16b955f..ca44c96f1302 100644
--- a/examples/neighbors/plot_digits_kde_sampling.py
+++ b/examples/neighbors/plot_digits_kde_sampling.py
@@ -27,7 +27,7 @@
 
 # use grid search cross-validation to optimize the bandwidth
 params = {'bandwidth': np.logspace(-1, 1, 20)}
-grid = GridSearchCV(KernelDensity(), params)
+grid = GridSearchCV(KernelDensity(), params, cv=5)
 grid.fit(data)
 
 print("best bandwidth: {0}".format(grid.best_estimator_.bandwidth))
diff --git a/examples/neighbors/plot_kde_1d.py b/examples/neighbors/plot_kde_1d.py
index 77ce5232da4f..aac5e6ea3e7f 100644
--- a/examples/neighbors/plot_kde_1d.py
+++ b/examples/neighbors/plot_kde_1d.py
@@ -67,7 +67,7 @@
 ax[1, 1].text(-3.5, 0.31, "Gaussian Kernel Density")
 
 for axi in ax.ravel():
-    axi.plot(X[:, 0], np.zeros(X.shape[0]) - 0.01, '+k')
+    axi.plot(X[:, 0], np.full(X.shape[0], -0.01), '+k')
     axi.set_xlim(-4, 9)
     axi.set_ylim(-0.02, 0.34)
 
diff --git a/examples/neighbors/plot_lof.py b/examples/neighbors/plot_lof.py
deleted file mode 100644
index 5c631de34245..000000000000
--- a/examples/neighbors/plot_lof.py
+++ /dev/null
@@ -1,59 +0,0 @@
-"""
-=================================================
-Anomaly detection with Local Outlier Factor (LOF)
-=================================================
-
-This example presents the Local Outlier Factor (LOF) estimator. The LOF
-algorithm is an unsupervised outlier detection method which computes the local
-density deviation of a given data point with respect to its neighbors.
-It considers as outlier samples that have a substantially lower density than
-their neighbors.
-
-The number of neighbors considered, (parameter n_neighbors) is typically
-chosen 1) greater than the minimum number of objects a cluster has to contain,
-so that other objects can be local outliers relative to this cluster, and 2)
-smaller than the maximum number of close by objects that can potentially be
-local outliers.
-In practice, such informations are generally not available, and taking
-n_neighbors=20 appears to work well in general.
-"""
-print(__doc__)
-
-import numpy as np
-import matplotlib.pyplot as plt
-from sklearn.neighbors import LocalOutlierFactor
-
-np.random.seed(42)
-
-# Generate train data
-X_inliers = 0.3 * np.random.randn(100, 2)
-X_inliers = np.r_[X_inliers + 2, X_inliers - 2]
-
-# Generate some abnormal novel observations
-X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))
-X = np.r_[X_inliers, X_outliers]
-
-# fit the model
-clf = LocalOutlierFactor(n_neighbors=20)
-y_pred = clf.fit_predict(X)
-
-# plot the level sets of the decision function
-xx, yy = np.meshgrid(np.linspace(-5, 5, 50), np.linspace(-5, 5, 50))
-Z = clf._decision_function(np.c_[xx.ravel(), yy.ravel()])
-Z = Z.reshape(xx.shape)
-
-plt.title("Local Outlier Factor (LOF)")
-plt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)
-
-a = plt.scatter(X_inliers[:, 0], X_inliers[:, 1], c='white',
-                edgecolor='k', s=20)
-b = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='red',
-                edgecolor='k', s=20)
-plt.axis('tight')
-plt.xlim((-5, 5))
-plt.ylim((-5, 5))
-plt.legend([a, b],
-           ["normal observations",
-            "abnormal observations"],
-           loc="upper left")
-plt.show()
diff --git a/examples/neighbors/plot_lof_novelty_detection.py b/examples/neighbors/plot_lof_novelty_detection.py
new file mode 100644
index 000000000000..71c0736a256a
--- /dev/null
+++ b/examples/neighbors/plot_lof_novelty_detection.py
@@ -0,0 +1,83 @@
+"""
+=================================================
+Novelty detection with Local Outlier Factor (LOF)
+=================================================
+
+The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection
+method which computes the local density deviation of a given data point with
+respect to its neighbors. It considers as outliers the samples that have a
+substantially lower density than their neighbors. This example shows how to
+use LOF for novelty detection. Note that when LOF is used for novelty
+detection you MUST not use predict, decision_function and score_samples on the
+training set as this would lead to wrong results. You must only use these
+methods on new unseen data (which are not in the training set). See
+:ref:`User Guide <outlier_detection>`: for details on the difference between
+outlier detection and novelty detection and how to use LOF for outlier
+detection.
+
+The number of neighbors considered, (parameter n_neighbors) is typically
+set 1) greater than the minimum number of samples a cluster has to contain,
+so that other samples can be local outliers relative to this cluster, and 2)
+smaller than the maximum number of close by samples that can potentially be
+local outliers.
+In practice, such informations are generally not available, and taking
+n_neighbors=20 appears to work well in general.
+"""
+
+import numpy as np
+import matplotlib
+import matplotlib.pyplot as plt
+from sklearn.neighbors import LocalOutlierFactor
+
+print(__doc__)
+
+np.random.seed(42)
+
+xx, yy = np.meshgrid(np.linspace(-5, 5, 500), np.linspace(-5, 5, 500))
+# Generate normal (not abnormal) training observations
+X = 0.3 * np.random.randn(100, 2)
+X_train = np.r_[X + 2, X - 2]
+# Generate new normal (not abnormal) observations
+X = 0.3 * np.random.randn(20, 2)
+X_test = np.r_[X + 2, X - 2]
+# Generate some abnormal novel observations
+X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))
+
+# fit the model for novelty detection (novelty=True)
+clf = LocalOutlierFactor(n_neighbors=20, novelty=True, contamination=0.1)
+clf.fit(X_train)
+# DO NOT use predict, decision_function and score_samples on X_train as this
+# would give wrong results but only on new unseen data (not used in X_train),
+# e.g. X_test, X_outliers or the meshgrid
+y_pred_test = clf.predict(X_test)
+y_pred_outliers = clf.predict(X_outliers)
+n_error_test = y_pred_test[y_pred_test == -1].size
+n_error_outliers = y_pred_outliers[y_pred_outliers == 1].size
+
+# plot the learned frontier, the points, and the nearest vectors to the plane
+Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
+Z = Z.reshape(xx.shape)
+
+plt.title("Novelty Detection with LOF")
+plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu)
+a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')
+plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='palevioletred')
+
+s = 40
+b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white', s=s, edgecolors='k')
+b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='blueviolet', s=s,
+                 edgecolors='k')
+c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='gold', s=s,
+                edgecolors='k')
+plt.axis('tight')
+plt.xlim((-5, 5))
+plt.ylim((-5, 5))
+plt.legend([a.collections[0], b1, b2, c],
+           ["learned frontier", "training observations",
+            "new regular observations", "new abnormal observations"],
+           loc="upper left",
+           prop=matplotlib.font_manager.FontProperties(size=11))
+plt.xlabel(
+    "errors novel regular: %d/40 ; errors novel abnormal: %d/40"
+    % (n_error_test, n_error_outliers))
+plt.show()
diff --git a/examples/neighbors/plot_lof_outlier_detection.py b/examples/neighbors/plot_lof_outlier_detection.py
new file mode 100644
index 000000000000..6f0e5bb490b9
--- /dev/null
+++ b/examples/neighbors/plot_lof_outlier_detection.py
@@ -0,0 +1,68 @@
+"""
+=================================================
+Outlier detection with Local Outlier Factor (LOF)
+=================================================
+
+The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection
+method which computes the local density deviation of a given data point with
+respect to its neighbors. It considers as outliers the samples that have a
+substantially lower density than their neighbors. This example shows how to
+use LOF for outlier detection which is the default use case of this estimator
+in scikit-learn. Note that when LOF is used for outlier detection it has no
+predict, decision_function and score_samples methods. See
+:ref:`User Guide <outlier_detection>`: for details on the difference between
+outlier detection and novelty detection and how to use LOF for novelty
+detection.
+
+The number of neighbors considered (parameter n_neighbors) is typically
+set 1) greater than the minimum number of samples a cluster has to contain,
+so that other samples can be local outliers relative to this cluster, and 2)
+smaller than the maximum number of close by samples that can potentially be
+local outliers.
+In practice, such informations are generally not available, and taking
+n_neighbors=20 appears to work well in general.
+"""
+
+import numpy as np
+import matplotlib.pyplot as plt
+from sklearn.neighbors import LocalOutlierFactor
+
+print(__doc__)
+
+np.random.seed(42)
+
+# Generate train data
+X_inliers = 0.3 * np.random.randn(100, 2)
+X_inliers = np.r_[X_inliers + 2, X_inliers - 2]
+
+# Generate some outliers
+X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))
+X = np.r_[X_inliers, X_outliers]
+
+n_outliers = len(X_outliers)
+ground_truth = np.ones(len(X), dtype=int)
+ground_truth[-n_outliers:] = -1
+
+# fit the model for outlier detection (default)
+clf = LocalOutlierFactor(n_neighbors=20, contamination=0.1)
+# use fit_predict to compute the predicted labels of the training samples
+# (when LOF is used for outlier detection, the estimator has no predict,
+# decision_function and score_samples methods).
+y_pred = clf.fit_predict(X)
+n_errors = (y_pred != ground_truth).sum()
+X_scores = clf.negative_outlier_factor_
+
+plt.title("Local Outlier Factor (LOF)")
+plt.scatter(X[:, 0], X[:, 1], color='k', s=3., label='Data points')
+# plot circles with radius proportional to the outlier scores
+radius = (X_scores.max() - X_scores) / (X_scores.max() - X_scores.min())
+plt.scatter(X[:, 0], X[:, 1], s=1000 * radius, edgecolors='r',
+            facecolors='none', label='Outlier scores')
+plt.axis('tight')
+plt.xlim((-5, 5))
+plt.ylim((-5, 5))
+plt.xlabel("prediction errors: %d" % (n_errors))
+legend = plt.legend(loc='upper left')
+legend.legendHandles[0]._sizes = [10]
+legend.legendHandles[1]._sizes = [20]
+plt.show()
diff --git a/examples/neighbors/plot_nearest_centroid.py b/examples/neighbors/plot_nearest_centroid.py
index a9c6b712f9c2..78a0141e50fe 100644
--- a/examples/neighbors/plot_nearest_centroid.py
+++ b/examples/neighbors/plot_nearest_centroid.py
@@ -50,7 +50,7 @@
 
     # Plot also the training points
     plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,
-                edgecolor='b', s=20)
+                edgecolor='k', s=20)
     plt.title("3-Class classification (shrink_threshold=%r)"
               % shrinkage)
     plt.axis('tight')
diff --git a/examples/neighbors/plot_species_kde.py b/examples/neighbors/plot_species_kde.py
index a333c7b6776d..a79805bd8f1e 100644
--- a/examples/neighbors/plot_species_kde.py
+++ b/examples/neighbors/plot_species_kde.py
@@ -87,7 +87,7 @@
     kde.fit(Xtrain[ytrain == i])
 
     # evaluate only on the land: -9999 indicates ocean
-    Z = -9999 + np.zeros(land_mask.shape[0])
+    Z = np.full(land_mask.shape[0], -9999, dtype='int')
     Z[land_mask] = np.exp(kde.score_samples(xy))
     Z = Z.reshape(X.shape)
 
@@ -105,7 +105,7 @@
     else:
         print(" - plot coastlines from coverage")
         plt.contour(X, Y, land_reference,
-                    levels=[-9999], colors="k",
+                    levels=[-9998], colors="k",
                     linestyles="solid")
         plt.xticks([])
         plt.yticks([])
diff --git a/examples/neural_networks/plot_mnist_filters.py b/examples/neural_networks/plot_mnist_filters.py
index 6c3b8b2284ea..ab50d4e59a81 100644
--- a/examples/neural_networks/plot_mnist_filters.py
+++ b/examples/neural_networks/plot_mnist_filters.py
@@ -20,15 +20,16 @@
 for a very short time. Training longer would result in weights with a much
 smoother spatial appearance.
 """
-print(__doc__)
-
 import matplotlib.pyplot as plt
-from sklearn.datasets import fetch_mldata
+from sklearn.datasets import fetch_openml
 from sklearn.neural_network import MLPClassifier
 
-mnist = fetch_mldata("MNIST original")
+print(__doc__)
+
+# Load data from https://www.openml.org/d/554
+X, y = fetch_openml('mnist_784', version=1, return_X_y=True)
+
 # rescale the data, use the traditional train/test split
-X, y = mnist.data / 255., mnist.target
 X_train, X_test = X[:60000], X[60000:]
 y_train, y_test = y[:60000], y[60000:]
 
diff --git a/examples/plot_anomaly_comparison.py b/examples/plot_anomaly_comparison.py
index 2248d9a91cd7..f3dc0f1dddff 100644
--- a/examples/plot_anomaly_comparison.py
+++ b/examples/plot_anomaly_comparison.py
@@ -10,10 +10,36 @@
 For each dataset, 15% of samples are generated as random uniform noise. This
 proportion is the value given to the nu parameter of the OneClassSVM and the
 contamination parameter of the other outlier detection algorithms.
-Decision boundaries between inliers and outliers are displayed in black.
-
-Local Outlier Factor (LOF) does not show a decision boundary in black as it
-has no predict method to be applied on new data.
+Decision boundaries between inliers and outliers are displayed in black
+except for Local Outlier Factor (LOF) as it has no predict method to be applied
+on new data when it is used for outlier detection.
+
+The :class:`svm.OneClassSVM` is known to be sensitive to outliers and thus does
+not perform very well for outlier detection. This estimator is best suited for
+novelty detection when the training set is not contaminated by outliers.
+That said, outlier detection in high-dimension, or without any assumptions on
+the distribution of the inlying data is very challenging, and a One-class SVM
+might give useful results in these situations depending on the value of its
+hyperparameters.
+
+:class:`covariance.EllipticEnvelope` assumes the data is Gaussian and learns
+an ellipse. It thus degrades when the data is not unimodal. Notice however
+that this estimator is robust to outliers.
+
+:class:`ensemble.IsolationForest` and :class:`neighbors.LocalOutlierFactor`
+seem to perform reasonably well for multi-modal data sets. The advantage of
+:class:`neighbors.LocalOutlierFactor` over the other estimators is shown for
+the third data set, where the two modes have different densities. This
+advantage is explained by the local aspect of LOF, meaning that it only
+compares the score of abnormality of one sample with the scores of its
+neighbors.
+
+Finally, for the last data set, it is hard to say that one sample is more
+abnormal than another sample as they are uniformly distributed in a
+hypercube. Except for the :class:`svm.OneClassSVM` which overfits a little, all
+estimators present decent solutions for this situation. In such a case, it
+would be wise to look more closely at the scores of abnormality of the samples
+as a good estimator should assign similar scores to all the samples.
 
 While these examples give some intuition about the algorithms, this
 intuition might not apply to very high dimensional data.
@@ -54,7 +80,8 @@
     ("Robust covariance", EllipticEnvelope(contamination=outliers_fraction)),
     ("One-Class SVM", svm.OneClassSVM(nu=outliers_fraction, kernel="rbf",
                                       gamma=0.1)),
-    ("Isolation Forest", IsolationForest(contamination=outliers_fraction,
+    ("Isolation Forest", IsolationForest(behaviour='new',
+                                         contamination=outliers_fraction,
                                          random_state=42)),
     ("Local Outlier Factor", LocalOutlierFactor(
         n_neighbors=35, contamination=outliers_fraction))]
@@ -64,6 +91,8 @@
 datasets = [
     make_blobs(centers=[[0, 0], [0, 0]], cluster_std=0.5,
                **blobs_params)[0],
+    make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[0.5, 0.5],
+               **blobs_params)[0],
     make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[1.5, .3],
                **blobs_params)[0],
     4. * (make_moons(n_samples=n_samples, noise=.05, random_state=0)[0] -
diff --git a/examples/plot_isotonic_regression.py b/examples/plot_isotonic_regression.py
index fd076b5afad6..1a240913b982 100644
--- a/examples/plot_isotonic_regression.py
+++ b/examples/plot_isotonic_regression.py
@@ -28,7 +28,7 @@
 n = 100
 x = np.arange(n)
 rs = check_random_state(0)
-y = rs.randint(-50, 50, size=(n,)) + 50. * np.log(1 + np.arange(n))
+y = rs.randint(-50, 50, size=(n,)) + 50. * np.log1p(np.arange(n))
 
 # #############################################################################
 # Fit IsotonicRegression and LinearRegression models
@@ -46,7 +46,7 @@
 segments = [[[i, y[i]], [i, y_[i]]] for i in range(n)]
 lc = LineCollection(segments, zorder=0)
 lc.set_array(np.ones(len(y)))
-lc.set_linewidths(0.5 * np.ones(n))
+lc.set_linewidths(np.full(n, 0.5))
 
 fig = plt.figure()
 plt.plot(x, y, 'r.', markersize=12)
diff --git a/examples/plot_missing_values.py b/examples/plot_missing_values.py
index d238a16592ed..755943fb55bd 100644
--- a/examples/plot_missing_values.py
+++ b/examples/plot_missing_values.py
@@ -3,26 +3,29 @@
 Imputing missing values before building an estimator
 ====================================================
 
+This example shows that imputing the missing values can give better
+results than discarding the samples containing any missing value.
+Imputing does not always improve the predictions, so please check via
+cross-validation.  Sometimes dropping rows or using marker values is
+more effective.
+
 Missing values can be replaced by the mean, the median or the most frequent
-value using the basic ``SimpleImputer``.
+value using the basic :func:`sklearn.impute.SimpleImputer`.
 The median is a more robust estimator for data with high magnitude variables
 which could dominate results (otherwise known as a 'long tail').
 
-Another option is the ``ChainedImputer``. This uses round-robin linear
-regression, treating every variable as an output in turn. The version
-implemented assumes Gaussian (output) variables. If your features are obviously
-non-Normal, consider transforming them to look more Normal so as to improve
-performance.
+In addition of using an imputing method, we can also keep an indication of the
+missing information using :func:`sklearn.impute.MissingIndicator` which might
+carry some information.
 """
-
 import numpy as np
 import matplotlib.pyplot as plt
 
 from sklearn.datasets import load_diabetes
 from sklearn.datasets import load_boston
 from sklearn.ensemble import RandomForestRegressor
-from sklearn.pipeline import Pipeline
-from sklearn.impute import SimpleImputer, ChainedImputer
+from sklearn.pipeline import make_pipeline, make_union
+from sklearn.impute import SimpleImputer, MissingIndicator
 from sklearn.model_selection import cross_val_score
 
 rng = np.random.RandomState(0)
@@ -36,7 +39,7 @@ def get_results(dataset):
     # Estimate the score on the entire dataset, with no missing values
     estimator = RandomForestRegressor(random_state=0, n_estimators=100)
     full_scores = cross_val_score(estimator, X_full, y_full,
-                                  scoring='neg_mean_squared_error')
+                                  scoring='neg_mean_squared_error', cv=5)
 
     # Add missing values in 75% of the lines
     missing_rate = 0.75
@@ -54,31 +57,25 @@ def get_results(dataset):
     y_missing = y_full.copy()
     estimator = RandomForestRegressor(random_state=0, n_estimators=100)
     zero_impute_scores = cross_val_score(estimator, X_missing, y_missing,
-                                         scoring='neg_mean_squared_error')
+                                         scoring='neg_mean_squared_error',
+                                         cv=5)
 
     # Estimate the score after imputation (mean strategy) of the missing values
     X_missing = X_full.copy()
     X_missing[np.where(missing_samples)[0], missing_features] = 0
     y_missing = y_full.copy()
-    estimator = Pipeline([("imputer", SimpleImputer(missing_values=0,
-                                                    strategy="mean")),
-                          ("forest", RandomForestRegressor(random_state=0,
-                                                           n_estimators=100))])
+    estimator = make_pipeline(
+        make_union(SimpleImputer(missing_values=0, strategy="mean"),
+                   MissingIndicator(missing_values=0)),
+        RandomForestRegressor(random_state=0, n_estimators=100))
     mean_impute_scores = cross_val_score(estimator, X_missing, y_missing,
-                                         scoring='neg_mean_squared_error')
+                                         scoring='neg_mean_squared_error',
+                                         cv=5)
 
-    # Estimate the score after chained imputation of the missing values
-    estimator = Pipeline([("imputer", ChainedImputer(missing_values=0,
-                                                     random_state=0)),
-                          ("forest", RandomForestRegressor(random_state=0,
-                                                           n_estimators=100))])
-    chained_impute_scores = cross_val_score(estimator, X_missing, y_missing,
-                                            scoring='neg_mean_squared_error')
 
     return ((full_scores.mean(), full_scores.std()),
             (zero_impute_scores.mean(), zero_impute_scores.std()),
-            (mean_impute_scores.mean(), mean_impute_scores.std()),
-            (chained_impute_scores.mean(), chained_impute_scores.std()))
+            (mean_impute_scores.mean(), mean_impute_scores.std()))
 
 
 results_diabetes = np.array(get_results(load_diabetes()))
@@ -94,8 +91,7 @@ def get_results(dataset):
 
 x_labels = ['Full data',
             'Zero imputation',
-            'Mean Imputation',
-            'Chained Imputation']
+            'Mean Imputation']
 colors = ['r', 'g', 'b', 'orange']
 
 # plot diabetes results
diff --git a/examples/preprocessing/plot_all_scaling.py b/examples/preprocessing/plot_all_scaling.py
index 92cd635e2a06..07fd3662da44 100755
--- a/examples/preprocessing/plot_all_scaling.py
+++ b/examples/preprocessing/plot_all_scaling.py
@@ -87,6 +87,8 @@
         MaxAbsScaler().fit_transform(X)),
     ('Data after robust scaling',
         RobustScaler(quantile_range=(25, 75)).fit_transform(X)),
+    ('Data after power transformation (Yeo-Johnson)',
+     PowerTransformer(method='yeo-johnson').fit_transform(X)),
     ('Data after power transformation (Box-Cox)',
      PowerTransformer(method='box-cox').fit_transform(X)),
     ('Data after quantile transformation (gaussian pdf)',
@@ -294,21 +296,21 @@ def make_plot(item_idx):
 make_plot(4)
 
 ##############################################################################
-# PowerTransformer (Box-Cox)
-# --------------------------
+# PowerTransformer
+# ----------------
 #
-# ``PowerTransformer`` applies a power transformation to each
-# feature to make the data more Gaussian-like. Currently,
-# ``PowerTransformer`` implements the Box-Cox transform. The Box-Cox transform
-# finds the optimal scaling factor to stabilize variance and mimimize skewness
-# through maximum likelihood estimation. By default, ``PowerTransformer`` also
-# applies zero-mean, unit variance normalization to the transformed output.
-# Note that Box-Cox can only be applied to positive, non-zero data. Income and
-# number of households happen to be strictly positive, but if negative values
-# are present, a constant can be added to each feature to shift it into the
-# positive range - this is known as the two-parameter Box-Cox transform.
+# ``PowerTransformer`` applies a power transformation to each feature to make
+# the data more Gaussian-like. Currently, ``PowerTransformer`` implements the
+# Yeo-Johnson and Box-Cox transforms. The power transform finds the optimal
+# scaling factor to stabilize variance and mimimize skewness through maximum
+# likelihood estimation. By default, ``PowerTransformer`` also applies
+# zero-mean, unit variance normalization to the transformed output. Note that
+# Box-Cox can only be applied to strictly positive data. Income and number of
+# households happen to be strictly positive, but if negative values are present
+# the Yeo-Johnson transformed is to be preferred.
 
 make_plot(5)
+make_plot(6)
 
 ##############################################################################
 # QuantileTransformer (Gaussian output)
@@ -319,7 +321,7 @@ def make_plot(item_idx):
 # Note that this non-parametetric transformer introduces saturation artifacts
 # for extreme values.
 
-make_plot(6)
+make_plot(7)
 
 ###################################################################
 # QuantileTransformer (uniform output)
@@ -337,7 +339,7 @@ def make_plot(item_idx):
 # any outlier by setting them to the a priori defined range boundaries (0 and
 # 1).
 
-make_plot(7)
+make_plot(8)
 
 ##############################################################################
 # Normalizer
@@ -350,6 +352,6 @@ def make_plot(item_idx):
 # transformed data only lie in the positive quadrant. This would not be the
 # case if some original features had a mix of positive and negative values.
 
-make_plot(8)
+make_plot(9)
 
 plt.show()
diff --git a/examples/preprocessing/plot_discretization.py b/examples/preprocessing/plot_discretization.py
new file mode 100644
index 000000000000..9cfcb30e6fdd
--- /dev/null
+++ b/examples/preprocessing/plot_discretization.py
@@ -0,0 +1,86 @@
+# -*- coding: utf-8 -*-
+
+"""
+================================================================
+Using KBinsDiscretizer to discretize continuous features
+================================================================
+
+The example compares prediction result of linear regression (linear model)
+and decision tree (tree based model) with and without discretization of
+real-valued features.
+
+As is shown in the result before discretization, linear model is fast to
+build and relatively straightforward to interpret, but can only model
+linear relationships, while decision tree can build a much more complex model
+of the data. One way to make linear model more powerful on continuous data
+is to use discretization (also known as binning). In the example, we
+discretize the feature and one-hot encode the transformed data. Note that if
+the bins are not reasonably wide, there would appear to be a substantially
+increased risk of overfitting, so the discretizer parameters should usually
+be tuned under cross validation.
+
+After discretization, linear regression and decision tree make exactly the
+same prediction. As features are constant within each bin, any model must
+predict the same value for all points within a bin. Compared with the result
+before discretization, linear model become much more flexible while decision
+tree gets much less flexible. Note that binning features generally has no
+beneficial effect for tree-based models, as these models can learn to split
+up the data anywhere.
+
+"""
+
+# Author: Andreas Müller
+#         Hanmin Qin <qinhanmin2005@sina.com>
+# License: BSD 3 clause
+
+import numpy as np
+import matplotlib.pyplot as plt
+
+from sklearn.linear_model import LinearRegression
+from sklearn.preprocessing import KBinsDiscretizer
+from sklearn.tree import DecisionTreeRegressor
+
+print(__doc__)
+
+# construct the dataset
+rnd = np.random.RandomState(42)
+X = rnd.uniform(-3, 3, size=100)
+y = np.sin(X) + rnd.normal(size=len(X)) / 3
+X = X.reshape(-1, 1)
+
+# transform the dataset with KBinsDiscretizer
+enc = KBinsDiscretizer(n_bins=10, encode='onehot')
+X_binned = enc.fit_transform(X)
+
+# predict with original dataset
+fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(10, 4))
+line = np.linspace(-3, 3, 1000, endpoint=False).reshape(-1, 1)
+reg = LinearRegression().fit(X, y)
+ax1.plot(line, reg.predict(line), linewidth=2, color='green',
+         label="linear regression")
+reg = DecisionTreeRegressor(min_samples_split=3, random_state=0).fit(X, y)
+ax1.plot(line, reg.predict(line), linewidth=2, color='red',
+         label="decision tree")
+ax1.plot(X[:, 0], y, 'o', c='k')
+ax1.legend(loc="best")
+ax1.set_ylabel("Regression output")
+ax1.set_xlabel("Input feature")
+ax1.set_title("Result before discretization")
+
+# predict with transformed dataset
+line_binned = enc.transform(line)
+reg = LinearRegression().fit(X_binned, y)
+ax2.plot(line, reg.predict(line_binned), linewidth=2, color='green',
+         linestyle='-', label='linear regression')
+reg = DecisionTreeRegressor(min_samples_split=3,
+                            random_state=0).fit(X_binned, y)
+ax2.plot(line, reg.predict(line_binned), linewidth=2, color='red',
+         linestyle=':', label='decision tree')
+ax2.plot(X[:, 0], y, 'o', c='k')
+ax2.vlines(enc.bin_edges_[0], *plt.gca().get_ylim(), linewidth=1, alpha=.2)
+ax2.legend(loc="best")
+ax2.set_xlabel("Input feature")
+ax2.set_title("Result after discretization")
+
+plt.tight_layout()
+plt.show()
diff --git a/examples/preprocessing/plot_discretization_classification.py b/examples/preprocessing/plot_discretization_classification.py
new file mode 100644
index 000000000000..7e6141c32597
--- /dev/null
+++ b/examples/preprocessing/plot_discretization_classification.py
@@ -0,0 +1,192 @@
+#!/usr/bin/python
+# -*- coding: utf-8 -*-
+"""
+======================
+Feature discretization
+======================
+
+A demonstration of feature discretization on synthetic classification datasets.
+Feature discretization decomposes each feature into a set of bins, here equally
+distributed in width. The discrete values are then one-hot encoded, and given
+to a linear classifier. This preprocessing enables a non-linear behavior even
+though the classifier is linear.
+
+On this example, the first two rows represent linearly non-separable datasets
+(moons and concentric circles) while the third is approximately linearly
+separable. On the two linearly non-separable datasets, feature discretization
+largely increases the performance of linear classifiers. On the linearly
+separable dataset, feature discretization decreases the performance of linear
+classifiers. Two non-linear classifiers are also shown for comparison.
+
+This example should be taken with a grain of salt, as the intuition conveyed
+does not necessarily carry over to real datasets. Particularly in
+high-dimensional spaces, data can more easily be separated linearly. Moreover,
+using feature discretization and one-hot encoding increases the number of
+features, which easily lead to overfitting when the number of samples is small.
+
+The plots show training points in solid colors and testing points
+semi-transparent. The lower right shows the classification accuracy on the test
+set.
+"""
+# Code source: Tom Dupré la Tour
+# Adapted from plot_classifier_comparison by Gaël Varoquaux and Andreas Müller
+#
+# License: BSD 3 clause
+
+import numpy as np
+import matplotlib.pyplot as plt
+from matplotlib.colors import ListedColormap
+from sklearn.model_selection import train_test_split
+from sklearn.preprocessing import StandardScaler
+from sklearn.datasets import make_moons, make_circles, make_classification
+from sklearn.linear_model import LogisticRegression
+from sklearn.model_selection import GridSearchCV
+from sklearn.pipeline import make_pipeline
+from sklearn.preprocessing import KBinsDiscretizer
+from sklearn.svm import SVC, LinearSVC
+from sklearn.ensemble import GradientBoostingClassifier
+from sklearn.utils.testing import ignore_warnings
+from sklearn.exceptions import ConvergenceWarning
+
+print(__doc__)
+
+h = .02  # step size in the mesh
+
+
+def get_name(estimator):
+    name = estimator.__class__.__name__
+    if name == 'Pipeline':
+        name = [get_name(est[1]) for est in estimator.steps]
+        name = ' + '.join(name)
+    return name
+
+
+# list of (estimator, param_grid), where param_grid is used in GridSearchCV
+classifiers = [
+    (LogisticRegression(solver='lbfgs', random_state=0), {
+        'C': np.logspace(-2, 7, 10)
+    }),
+    (LinearSVC(random_state=0), {
+        'C': np.logspace(-2, 7, 10)
+    }),
+    (make_pipeline(
+        KBinsDiscretizer(encode='onehot'),
+        LogisticRegression(solver='lbfgs', random_state=0)), {
+            'kbinsdiscretizer__n_bins': np.arange(2, 10),
+            'logisticregression__C': np.logspace(-2, 7, 10),
+        }),
+    (make_pipeline(
+        KBinsDiscretizer(encode='onehot'), LinearSVC(random_state=0)), {
+            'kbinsdiscretizer__n_bins': np.arange(2, 10),
+            'linearsvc__C': np.logspace(-2, 7, 10),
+        }),
+    (GradientBoostingClassifier(n_estimators=50, random_state=0), {
+        'learning_rate': np.logspace(-4, 0, 10)
+    }),
+    (SVC(random_state=0, gamma='scale'), {
+        'C': np.logspace(-2, 7, 10)
+    }),
+]
+
+names = [get_name(e) for e, g in classifiers]
+
+n_samples = 100
+datasets = [
+    make_moons(n_samples=n_samples, noise=0.2, random_state=0),
+    make_circles(n_samples=n_samples, noise=0.2, factor=0.5, random_state=1),
+    make_classification(n_samples=n_samples, n_features=2, n_redundant=0,
+                        n_informative=2, random_state=2,
+                        n_clusters_per_class=1)
+]
+
+fig, axes = plt.subplots(nrows=len(datasets), ncols=len(classifiers) + 1,
+                         figsize=(21, 9))
+
+cm = plt.cm.PiYG
+cm_bright = ListedColormap(['#b30065', '#178000'])
+
+# iterate over datasets
+for ds_cnt, (X, y) in enumerate(datasets):
+    print('\ndataset %d\n---------' % ds_cnt)
+
+    # preprocess dataset, split into training and test part
+    X = StandardScaler().fit_transform(X)
+    X_train, X_test, y_train, y_test = train_test_split(
+        X, y, test_size=.5, random_state=42)
+
+    # create the grid for background colors
+    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
+    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
+    xx, yy = np.meshgrid(
+        np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
+
+    # plot the dataset first
+    ax = axes[ds_cnt, 0]
+    if ds_cnt == 0:
+        ax.set_title("Input data")
+    # plot the training points
+    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,
+               edgecolors='k')
+    # and testing points
+    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,
+               edgecolors='k')
+    ax.set_xlim(xx.min(), xx.max())
+    ax.set_ylim(yy.min(), yy.max())
+    ax.set_xticks(())
+    ax.set_yticks(())
+
+    # iterate over classifiers
+    for est_idx, (name, (estimator, param_grid)) in \
+            enumerate(zip(names, classifiers)):
+        ax = axes[ds_cnt, est_idx + 1]
+
+        clf = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=5,
+                           iid=False)
+        with ignore_warnings(category=ConvergenceWarning):
+            clf.fit(X_train, y_train)
+        score = clf.score(X_test, y_test)
+        print('%s: %.2f' % (name, score))
+
+        # plot the decision boundary. For that, we will assign a color to each
+        # point in the mesh [x_min, x_max]*[y_min, y_max].
+        if hasattr(clf, "decision_function"):
+            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
+        else:
+            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]
+
+        # put the result into a color plot
+        Z = Z.reshape(xx.shape)
+        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)
+
+        # plot the training points
+        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,
+                   edgecolors='k')
+        # and testing points
+        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,
+                   edgecolors='k', alpha=0.6)
+        ax.set_xlim(xx.min(), xx.max())
+        ax.set_ylim(yy.min(), yy.max())
+        ax.set_xticks(())
+        ax.set_yticks(())
+
+        if ds_cnt == 0:
+            ax.set_title(name.replace(' + ', '\n'))
+        ax.text(0.95, 0.06, ('%.2f' % score).lstrip('0'), size=15,
+                bbox=dict(boxstyle='round', alpha=0.8, facecolor='white'),
+                transform=ax.transAxes, horizontalalignment='right')
+
+
+plt.tight_layout()
+
+# Add suptitles above the figure
+plt.subplots_adjust(top=0.90)
+suptitles = [
+    'Linear classifiers',
+    'Feature discretization and linear classifiers',
+    'Non-linear classifiers',
+]
+for i, suptitle in zip([1, 3, 5], suptitles):
+    ax = axes[0, i]
+    ax.text(1.05, 1.25, suptitle, transform=ax.transAxes,
+            horizontalalignment='center', size='x-large')
+plt.show()
diff --git a/examples/preprocessing/plot_discretization_strategies.py b/examples/preprocessing/plot_discretization_strategies.py
new file mode 100644
index 000000000000..9ef211a83ccf
--- /dev/null
+++ b/examples/preprocessing/plot_discretization_strategies.py
@@ -0,0 +1,95 @@
+# -*- coding: utf-8 -*-
+"""
+==========================================================
+Demonstrating the different strategies of KBinsDiscretizer
+==========================================================
+
+This example presents the different strategies implemented in KBinsDiscretizer:
+
+- 'uniform': The discretization is uniform in each feature, which means that
+  the bin widths are constant in each dimension.
+- quantile': The discretization is done on the quantiled values, which means
+  that each bin has approximately the same number of samples.
+- 'kmeans': The discretization is based on the centroids of a KMeans clustering
+  procedure.
+
+The plot shows the regions where the discretized encoding is constant.
+"""
+
+# Author: Tom Dupré la Tour
+# License: BSD 3 clause
+
+import numpy as np
+import matplotlib.pyplot as plt
+
+from sklearn.preprocessing import KBinsDiscretizer
+from sklearn.datasets import make_blobs
+
+print(__doc__)
+
+strategies = ['uniform', 'quantile', 'kmeans']
+
+n_samples = 200
+centers_0 = np.array([[0, 0], [0, 5], [2, 4], [8, 8]])
+centers_1 = np.array([[0, 0], [3, 1]])
+
+# construct the datasets
+random_state = 42
+X_list = [
+    np.random.RandomState(random_state).uniform(-3, 3, size=(n_samples, 2)),
+    make_blobs(n_samples=[n_samples // 10, n_samples * 4 // 10,
+                          n_samples // 10, n_samples * 4 // 10],
+               cluster_std=0.5, centers=centers_0,
+               random_state=random_state)[0],
+    make_blobs(n_samples=[n_samples // 5, n_samples * 4 // 5],
+               cluster_std=0.5, centers=centers_1,
+               random_state=random_state)[0],
+]
+
+figure = plt.figure(figsize=(14, 9))
+i = 1
+for ds_cnt, X in enumerate(X_list):
+
+    ax = plt.subplot(len(X_list), len(strategies) + 1, i)
+    ax.scatter(X[:, 0], X[:, 1], edgecolors='k')
+    if ds_cnt == 0:
+        ax.set_title("Input data", size=14)
+
+    xx, yy = np.meshgrid(
+        np.linspace(X[:, 0].min(), X[:, 0].max(), 300),
+        np.linspace(X[:, 1].min(), X[:, 1].max(), 300))
+    grid = np.c_[xx.ravel(), yy.ravel()]
+
+    ax.set_xlim(xx.min(), xx.max())
+    ax.set_ylim(yy.min(), yy.max())
+    ax.set_xticks(())
+    ax.set_yticks(())
+
+    i += 1
+    # transform the dataset with KBinsDiscretizer
+    for strategy in strategies:
+        enc = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy=strategy)
+        enc.fit(X)
+        grid_encoded = enc.transform(grid)
+
+        ax = plt.subplot(len(X_list), len(strategies) + 1, i)
+
+        # horizontal stripes
+        horizontal = grid_encoded[:, 0].reshape(xx.shape)
+        ax.contourf(xx, yy, horizontal, alpha=.5)
+        # vertical stripes
+        vertical = grid_encoded[:, 1].reshape(xx.shape)
+        ax.contourf(xx, yy, vertical, alpha=.5)
+
+        ax.scatter(X[:, 0], X[:, 1], edgecolors='k')
+        ax.set_xlim(xx.min(), xx.max())
+        ax.set_ylim(yy.min(), yy.max())
+        ax.set_xticks(())
+        ax.set_yticks(())
+        if ds_cnt == 0:
+            ax.set_title("strategy='%s'" % (strategy, ), size=14)
+
+        i += 1
+
+plt.tight_layout()
+plt.show()
diff --git a/examples/preprocessing/plot_map_data_to_normal.py b/examples/preprocessing/plot_map_data_to_normal.py
new file mode 100644
index 000000000000..b8b7625f3c02
--- /dev/null
+++ b/examples/preprocessing/plot_map_data_to_normal.py
@@ -0,0 +1,137 @@
+"""
+=================================
+Map data to a normal distribution
+=================================
+
+This example demonstrates the use of the Box-Cox and Yeo-Johnson transforms
+through :class:`preprocessing.PowerTransformer` to map data from various
+distributions to a normal distribution.
+
+The power transform is useful as a transformation in modeling problems where
+homoscedasticity and normality are desired. Below are examples of Box-Cox and
+Yeo-Johnwon applied to six different probability distributions: Lognormal,
+Chi-squared, Weibull, Gaussian, Uniform, and Bimodal.
+
+Note that the transformations successfully map the data to a normal
+distribution when applied to certain datasets, but are ineffective with others.
+This highlights the importance of visualizing the data before and after
+transformation.
+
+Also note that even though Box-Cox seems to perform better than Yeo-Johnson for
+lognormal and chi-squared distributions, keep in mind that Box-Cox does not
+support inputs with negative values.
+
+For comparison, we also add the output from
+:class:`preprocessing.QuantileTransformer`. It can force any arbitrary
+distribution into a gaussian, provided that there are enough training samples
+(thousands). Because it is a non-parametric method, it is harder to interpret
+than the parametric ones (Box-Cox and Yeo-Johnson).
+
+On "small" datasets (less than a few hundred points), the quantile transformer
+is prone to overfitting. The use of the power transform is then recommended.
+"""
+
+# Author: Eric Chang <ericchang2017@u.northwestern.edu>
+#         Nicolas Hug <contact@nicolas-hug.com>
+# License: BSD 3 clause
+
+import numpy as np
+import matplotlib.pyplot as plt
+
+from sklearn.preprocessing import PowerTransformer
+from sklearn.preprocessing import QuantileTransformer
+from sklearn.model_selection import train_test_split
+
+print(__doc__)
+
+
+N_SAMPLES = 1000
+FONT_SIZE = 6
+BINS = 30
+
+
+rng = np.random.RandomState(304)
+bc = PowerTransformer(method='box-cox')
+yj = PowerTransformer(method='yeo-johnson')
+qt = QuantileTransformer(output_distribution='normal', random_state=rng)
+size = (N_SAMPLES, 1)
+
+
+# lognormal distribution
+X_lognormal = rng.lognormal(size=size)
+
+# chi-squared distribution
+df = 3
+X_chisq = rng.chisquare(df=df, size=size)
+
+# weibull distribution
+a = 50
+X_weibull = rng.weibull(a=a, size=size)
+
+# gaussian distribution
+loc = 100
+X_gaussian = rng.normal(loc=loc, size=size)
+
+# uniform distribution
+X_uniform = rng.uniform(low=0, high=1, size=size)
+
+# bimodal distribution
+loc_a, loc_b = 100, 105
+X_a, X_b = rng.normal(loc=loc_a, size=size), rng.normal(loc=loc_b, size=size)
+X_bimodal = np.concatenate([X_a, X_b], axis=0)
+
+
+# create plots
+distributions = [
+    ('Lognormal', X_lognormal),
+    ('Chi-squared', X_chisq),
+    ('Weibull', X_weibull),
+    ('Gaussian', X_gaussian),
+    ('Uniform', X_uniform),
+    ('Bimodal', X_bimodal)
+]
+
+colors = ['firebrick', 'darkorange', 'goldenrod',
+          'seagreen', 'royalblue', 'darkorchid']
+
+fig, axes = plt.subplots(nrows=8, ncols=3, figsize=plt.figaspect(2))
+axes = axes.flatten()
+axes_idxs = [(0, 3, 6, 9), (1, 4, 7, 10), (2, 5, 8, 11), (12, 15, 18, 21),
+             (13, 16, 19, 22), (14, 17, 20, 23)]
+axes_list = [(axes[i], axes[j], axes[k], axes[l])
+             for (i, j, k, l) in axes_idxs]
+
+
+for distribution, color, axes in zip(distributions, colors, axes_list):
+    name, X = distribution
+    X_train, X_test = train_test_split(X, test_size=.5)
+
+    # perform power transforms and quantile transform
+    X_trans_bc = bc.fit(X_train).transform(X_test)
+    lmbda_bc = round(bc.lambdas_[0], 2)
+    X_trans_yj = yj.fit(X_train).transform(X_test)
+    lmbda_yj = round(yj.lambdas_[0], 2)
+    X_trans_qt = qt.fit(X_train).transform(X_test)
+
+    ax_original, ax_bc, ax_yj, ax_qt = axes
+
+    ax_original.hist(X_train, color=color, bins=BINS)
+    ax_original.set_title(name, fontsize=FONT_SIZE)
+    ax_original.tick_params(axis='both', which='major', labelsize=FONT_SIZE)
+
+    for ax, X_trans, meth_name, lmbda in zip(
+            (ax_bc, ax_yj, ax_qt),
+            (X_trans_bc, X_trans_yj, X_trans_qt),
+            ('Box-Cox', 'Yeo-Johnson', 'Quantile transform'),
+            (lmbda_bc, lmbda_yj, None)):
+        ax.hist(X_trans, color=color, bins=BINS)
+        title = 'After {}'.format(meth_name)
+        if lmbda is not None:
+            title += '\n$\lambda$ = {}'.format(lmbda)
+        ax.set_title(title, fontsize=FONT_SIZE)
+        ax.tick_params(axis='both', which='major', labelsize=FONT_SIZE)
+        ax.set_xlim([-3.5, 3.5])
+
+
+plt.tight_layout()
+plt.show()
diff --git a/examples/preprocessing/plot_power_transformer.py b/examples/preprocessing/plot_power_transformer.py
deleted file mode 100644
index 52ce0d3121f7..000000000000
--- a/examples/preprocessing/plot_power_transformer.py
+++ /dev/null
@@ -1,109 +0,0 @@
-"""
-==========================================================
-Using PowerTransformer to apply the Box-Cox transformation
-==========================================================
-
-This example demonstrates the use of the Box-Cox transform through
-:class:`preprocessing.PowerTransformer` to map data from various distributions
-to a normal distribution.
-
-Box-Cox is useful as a transformation in modeling problems where
-homoscedasticity and normality are desired. Below are examples of Box-Cox
-applied to six different probability distributions: Lognormal, Chi-squared,
-Weibull, Gaussian, Uniform, and Bimodal.
-
-Note that the transformation successfully maps the data to a normal
-distribution when applied to certain datasets, but is ineffective with others.
-This highlights the importance of visualizing the data before and after
-transformation. Also note that while the standardize option is set to False for
-the plot examples, by default, :class:`preprocessing.PowerTransformer` also
-applies zero-mean, unit-variance standardization to the transformed outputs.
-"""
-
-# Author: Eric Chang <ericchang2017@u.northwestern.edu>
-# License: BSD 3 clause
-
-import numpy as np
-import matplotlib.pyplot as plt
-
-from sklearn.preprocessing import PowerTransformer, minmax_scale
-
-print(__doc__)
-
-
-N_SAMPLES = 3000
-FONT_SIZE = 6
-BINS = 100
-
-
-pt = PowerTransformer(method='box-cox', standardize=False)
-rng = np.random.RandomState(304)
-size = (N_SAMPLES, 1)
-
-
-# lognormal distribution
-X_lognormal = rng.lognormal(size=size)
-
-# chi-squared distribution
-df = 3
-X_chisq = rng.chisquare(df=df, size=size)
-
-# weibull distribution
-a = 50
-X_weibull = rng.weibull(a=a, size=size)
-
-# gaussian distribution
-loc = 100
-X_gaussian = rng.normal(loc=loc, size=size)
-
-# uniform distribution
-X_uniform = rng.uniform(low=0, high=1, size=size)
-
-# bimodal distribution
-loc_a, loc_b = 100, 105
-X_a, X_b = rng.normal(loc=loc_a, size=size), rng.normal(loc=loc_b, size=size)
-X_bimodal = np.concatenate([X_a, X_b], axis=0)
-
-
-# create plots
-distributions = [
-    ('Lognormal', X_lognormal),
-    ('Chi-squared', X_chisq),
-    ('Weibull', X_weibull),
-    ('Gaussian', X_gaussian),
-    ('Uniform', X_uniform),
-    ('Bimodal', X_bimodal)
-]
-
-colors = ['firebrick', 'darkorange', 'goldenrod',
-          'seagreen', 'royalblue', 'darkorchid']
-
-fig, axes = plt.subplots(nrows=4, ncols=3)
-axes = axes.flatten()
-axes_idxs = [(0, 3), (1, 4), (2, 5), (6, 9), (7, 10), (8, 11)]
-axes_list = [(axes[i], axes[j]) for i, j in axes_idxs]
-
-
-for distribution, color, axes in zip(distributions, colors, axes_list):
-    name, X = distribution
-    # scale all distributions to the range [0, 10]
-    X = minmax_scale(X, feature_range=(1e-10, 10))
-
-    # perform power transform
-    X_trans = pt.fit_transform(X)
-    lmbda = round(pt.lambdas_[0], 2)
-
-    ax_original, ax_trans = axes
-
-    ax_original.hist(X, color=color, bins=BINS)
-    ax_original.set_title(name, fontsize=FONT_SIZE)
-    ax_original.tick_params(axis='both', which='major', labelsize=FONT_SIZE)
-
-    ax_trans.hist(X_trans, color=color, bins=BINS)
-    ax_trans.set_title('{} after Box-Cox, $\lambda$ = {}'.format(name, lmbda),
-                       fontsize=FONT_SIZE)
-    ax_trans.tick_params(axis='both', which='major', labelsize=FONT_SIZE)
-
-
-plt.tight_layout()
-plt.show()
diff --git a/examples/semi_supervised/plot_label_propagation_structure.py b/examples/semi_supervised/plot_label_propagation_structure.py
index 6363653077d9..ad9270307a39 100644
--- a/examples/semi_supervised/plot_label_propagation_structure.py
+++ b/examples/semi_supervised/plot_label_propagation_structure.py
@@ -24,7 +24,7 @@
 n_samples = 200
 X, y = make_circles(n_samples=n_samples, shuffle=False)
 outer, inner = 0, 1
-labels = -np.ones(n_samples)
+labels = np.full(n_samples, -1.)
 labels[0] = outer
 labels[-1] = inner
 
diff --git a/examples/semi_supervised/plot_label_propagation_versus_svm_iris.py b/examples/semi_supervised/plot_label_propagation_versus_svm_iris.py
index 1746aa5b02e3..32235d412b38 100644
--- a/examples/semi_supervised/plot_label_propagation_versus_svm_iris.py
+++ b/examples/semi_supervised/plot_label_propagation_versus_svm_iris.py
@@ -42,7 +42,7 @@
 ls50 = (label_propagation.LabelSpreading().fit(X, y_50),
         y_50)
 ls100 = (label_propagation.LabelSpreading().fit(X, y), y)
-rbf_svc = (svm.SVC(kernel='rbf').fit(X, y), y)
+rbf_svc = (svm.SVC(kernel='rbf', gamma=.5).fit(X, y), y)
 
 # create a mesh to plot in
 x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
diff --git a/examples/svm/plot_svm_anova.py b/examples/svm/plot_svm_anova.py
index e223730eb82b..08f9fddf71db 100644
--- a/examples/svm/plot_svm_anova.py
+++ b/examples/svm/plot_svm_anova.py
@@ -10,17 +10,19 @@
 
 import numpy as np
 import matplotlib.pyplot as plt
-from sklearn import svm, datasets, feature_selection
+from sklearn.datasets import load_digits
+from sklearn.feature_selection import SelectPercentile, chi2
 from sklearn.model_selection import cross_val_score
 from sklearn.pipeline import Pipeline
+from sklearn.svm import SVC
+
 
 # #############################################################################
 # Import some data to play with
-digits = datasets.load_digits()
-y = digits.target
+X, y = load_digits(return_X_y=True)
 # Throw away data, to be in the curse of dimension settings
+X = X[:200]
 y = y[:200]
-X = digits.data[:200]
 n_samples = len(y)
 X = X.reshape((n_samples, -1))
 # add 200 non-informative features
@@ -30,9 +32,9 @@
 # Create a feature-selection transform and an instance of SVM that we
 # combine together to have an full-blown estimator
 
-transform = feature_selection.SelectPercentile(feature_selection.f_classif)
+transform = SelectPercentile(chi2)
 
-clf = Pipeline([('anova', transform), ('svc', svm.SVC(C=1.0))])
+clf = Pipeline([('anova', transform), ('svc', SVC(gamma="auto"))])
 
 # #############################################################################
 # Plot the cross-validation score as a function of percentile of features
@@ -43,7 +45,7 @@
 for percentile in percentiles:
     clf.set_params(anova__percentile=percentile)
     # Compute cross-validation score using 1 CPU
-    this_scores = cross_val_score(clf, X, y, n_jobs=1)
+    this_scores = cross_val_score(clf, X, y, cv=5, n_jobs=1)
     score_means.append(this_scores.mean())
     score_stds.append(this_scores.std())
 
diff --git a/examples/svm/plot_svm_scale_c.py b/examples/svm/plot_svm_scale_c.py
index 5459d45e2206..5502e6829ea4 100644
--- a/examples/svm/plot_svm_scale_c.py
+++ b/examples/svm/plot_svm_scale_c.py
@@ -119,9 +119,9 @@
 colors = ['navy', 'cyan', 'darkorange']
 lw = 2
 
-for fignum, (clf, cs, X, y) in enumerate(clf_sets):
+for clf, cs, X, y in clf_sets:
     # set up the plot for each regressor
-    plt.figure(fignum, figsize=(9, 10))
+    fig, axes = plt.subplots(nrows=2, sharey=True, figsize=(9, 10))
 
     for k, train_size in enumerate(np.linspace(0.3, 0.7, 3)[::-1]):
         param_grid = dict(C=cs)
@@ -129,6 +129,7 @@
         # reduce the variance
         grid = GridSearchCV(clf, refit=False, param_grid=param_grid,
                             cv=ShuffleSplit(train_size=train_size,
+                                            test_size=.3,
                                             n_splits=250, random_state=1))
         grid.fit(X, y)
         scores = grid.cv_results_['mean_test_score']
@@ -137,15 +138,14 @@
                   ((n_samples * train_size), '1/n_samples'),
                   ]
 
-        for subplotnum, (scaler, name) in enumerate(scales):
-            plt.subplot(2, 1, subplotnum + 1)
-            plt.xlabel('C')
-            plt.ylabel('CV Score')
+        for ax, (scaler, name) in zip(axes, scales):
+            ax.set_xlabel('C')
+            ax.set_ylabel('CV Score')
             grid_cs = cs * float(scaler)  # scale the C's
-            plt.semilogx(grid_cs, scores, label="fraction %.2f" %
-                         train_size, color=colors[k], lw=lw)
-            plt.title('scaling=%s, penalty=%s, loss=%s' %
-                      (name, clf.penalty, clf.loss))
+            ax.semilogx(grid_cs, scores, label="fraction %.2f" %
+                        train_size, color=colors[k], lw=lw)
+            ax.set_title('scaling=%s, penalty=%s, loss=%s' %
+                         (name, clf.penalty, clf.loss))
 
     plt.legend(loc="best")
 plt.show()
diff --git a/examples/svm/plot_weighted_samples.py b/examples/svm/plot_weighted_samples.py
index be625c1446f5..0549da7a3808 100644
--- a/examples/svm/plot_weighted_samples.py
+++ b/examples/svm/plot_weighted_samples.py
@@ -45,13 +45,13 @@ def plot_decision_function(classifier, sample_weight, axis, title):
 sample_weight_last_ten[15:] *= 5
 sample_weight_last_ten[9] *= 15
 
-# for reference, first fit without class weights
+# for reference, first fit without sample weights
 
 # fit the model
-clf_weights = svm.SVC()
+clf_weights = svm.SVC(gamma=1)
 clf_weights.fit(X, y, sample_weight=sample_weight_last_ten)
 
-clf_no_weights = svm.SVC()
+clf_no_weights = svm.SVC(gamma=1)
 clf_no_weights.fit(X, y)
 
 fig, axes = plt.subplots(1, 2, figsize=(14, 6))
diff --git a/examples/text/plot_document_classification_20newsgroups.py b/examples/text/plot_document_classification_20newsgroups.py
index 8b9d66f9e09d..388439381815 100644
--- a/examples/text/plot_document_classification_20newsgroups.py
+++ b/examples/text/plot_document_classification_20newsgroups.py
@@ -249,9 +249,9 @@ def benchmark(clf):
 
 results = []
 for clf, name in (
-        (RidgeClassifier(tol=1e-2, solver="lsqr"), "Ridge Classifier"),
-        (Perceptron(n_iter=50, tol=1e-3), "Perceptron"),
-        (PassiveAggressiveClassifier(n_iter=50, tol=1e-3),
+        (RidgeClassifier(tol=1e-2, solver="sag"), "Ridge Classifier"),
+        (Perceptron(max_iter=50, tol=1e-3), "Perceptron"),
+        (PassiveAggressiveClassifier(max_iter=50, tol=1e-3),
          "Passive-Aggressive"),
         (KNeighborsClassifier(n_neighbors=10), "kNN"),
         (RandomForestClassifier(n_estimators=100), "Random forest")):
@@ -267,16 +267,14 @@ def benchmark(clf):
                                        tol=1e-3)))
 
     # Train SGD model
-    results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,
-                                           penalty=penalty,
-                                           max_iter=5)))
+    results.append(benchmark(SGDClassifier(alpha=.0001, max_iter=50,
+                                           penalty=penalty)))
 
 # Train SGD with Elastic Net penalty
 print('=' * 80)
 print("Elastic-Net penalty")
-results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,
-                                       penalty="elasticnet",
-                                       max_iter=5)))
+results.append(benchmark(SGDClassifier(alpha=.0001, max_iter=50,
+                                       penalty="elasticnet")))
 
 # Train NearestCentroid without threshold
 print('=' * 80)
diff --git a/setup.cfg b/setup.cfg
index b02383bae3b5..09c5c9829ae2 100644
--- a/setup.cfg
+++ b/setup.cfg
@@ -8,25 +8,16 @@ addopts =
     --doctest-modules
     --disable-pytest-warnings
     -rs
+filterwarnings =
+    error::DeprecationWarning
+    error::FutureWarning
 
 [wheelhouse_uploader]
 artifact_indexes=
-    # OSX wheels built by travis (only for specific tags):
+    # Wheels built by travis (only for specific tags):
     # https://github.com/MacPython/scikit-learn-wheels
     http://wheels.scipy.org
-    # Windows wheels built by:
-    # https://ci.appveyor.com/project/sklearn-ci/scikit-learn/
-    http://windows-wheels.scikit-learn.org/
 
 [flake8]
 # Default flake8 3.5 ignored flags
 ignore=E121,E123,E126,E226,E24,E704,W503,W504
-
-# Uncomment the following under windows to build using:
-# http://sourceforge.net/projects/mingw/
-
-#[build_ext]
-#compiler=mingw32
-#
-#[build]
-#compiler=mingw32
diff --git a/setup.py b/setup.py
index 206cd645afec..e25c50a114a3 100755
--- a/setup.py
+++ b/setup.py
@@ -3,10 +3,10 @@
 # Copyright (C) 2007-2009 Cournapeau David <cournape@gmail.com>
 #               2010 Fabian Pedregosa <fabian.pedregosa@inria.fr>
 # License: 3-clause BSD
-descr = """A set of python modules for machine learning and data mining"""
 
 import sys
 import os
+import platform
 import shutil
 from distutils.command.clean import clean as Clean
 from pkg_resources import parse_version
@@ -41,8 +41,12 @@
 
 VERSION = sklearn.__version__
 
-SCIPY_MIN_VERSION = '0.13.3'
-NUMPY_MIN_VERSION = '1.8.2'
+if platform.python_implementation() == 'PyPy':
+    SCIPY_MIN_VERSION = '1.1.0'
+    NUMPY_MIN_VERSION = '1.14.0'
+else:
+    SCIPY_MIN_VERSION = '0.13.3'
+    NUMPY_MIN_VERSION = '1.8.2'
 
 
 # Optional setuptools features
@@ -185,6 +189,11 @@ def setup_package():
                                  'Programming Language :: Python :: 3.4',
                                  'Programming Language :: Python :: 3.5',
                                  'Programming Language :: Python :: 3.6',
+                                 'Programming Language :: Python :: 3.7',
+                                 ('Programming Language :: Python :: '
+                                  'Implementation :: CPython'),
+                                 ('Programming Language :: Python :: '
+                                  'Implementation :: PyPy')
                                  ],
                     cmdclass=cmdclass,
                     install_requires=[
diff --git a/sklearn/__init__.py b/sklearn/__init__.py
index 36fb3afdc587..1d7cd2ef9200 100644
--- a/sklearn/__init__.py
+++ b/sklearn/__init__.py
@@ -49,7 +49,7 @@
 
 try:
     # This variable is injected in the __builtins__ by the build
-    # process. It used to enable importing subpackages of sklearn when
+    # process. It is used to enable importing subpackages of sklearn when
     # the binaries are not built
     __SKLEARN_SETUP__
 except NameError:
@@ -62,6 +62,8 @@
 else:
     from . import __check_build
     from .base import clone
+    from .utils._show_versions import show_versions
+
     __check_build  # avoid flakes unused variable error
 
     __all__ = ['calibration', 'cluster', 'covariance', 'cross_decomposition',
@@ -74,7 +76,8 @@
                'preprocessing', 'random_projection', 'semi_supervised',
                'svm', 'tree', 'discriminant_analysis', 'impute', 'compose',
                # Non-modules:
-               'clone', 'get_config', 'set_config', 'config_context']
+               'clone', 'get_config', 'set_config', 'config_context',
+               'show_versions']
 
 
 def setup_module(module):
diff --git a/sklearn/calibration.py b/sklearn/calibration.py
index 4bc376cc506a..ba7dcd0cb54f 100644
--- a/sklearn/calibration.py
+++ b/sklearn/calibration.py
@@ -75,6 +75,10 @@ class CalibratedClassifierCV(BaseEstimator, ClassifierMixin):
         If "prefit" is passed, it is assumed that base_estimator has been
         fitted already and all data is used for calibration.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     Attributes
     ----------
     classes_ : array, shape (n_classes)
@@ -99,7 +103,7 @@ class CalibratedClassifierCV(BaseEstimator, ClassifierMixin):
     .. [4] Predicting Good Probabilities with Supervised Learning,
            A. Niculescu-Mizil & R. Caruana, ICML 2005
     """
-    def __init__(self, base_estimator=None, method='sigmoid', cv=3):
+    def __init__(self, base_estimator=None, method='sigmoid', cv='warn'):
         self.base_estimator = base_estimator
         self.method = method
         self.cv = cv
diff --git a/sklearn/cluster/__init__.py b/sklearn/cluster/__init__.py
index c9afcd98f23c..e35670aac45b 100644
--- a/sklearn/cluster/__init__.py
+++ b/sklearn/cluster/__init__.py
@@ -11,6 +11,7 @@
                            FeatureAgglomeration)
 from .k_means_ import k_means, KMeans, MiniBatchKMeans
 from .dbscan_ import dbscan, DBSCAN
+from .optics_ import OPTICS, optics
 from .bicluster import SpectralBiclustering, SpectralCoclustering
 from .birch import Birch
 
@@ -18,6 +19,7 @@
            'AgglomerativeClustering',
            'Birch',
            'DBSCAN',
+           'OPTICS',
            'KMeans',
            'FeatureAgglomeration',
            'MeanShift',
@@ -30,6 +32,7 @@
            'k_means',
            'linkage_tree',
            'mean_shift',
+           'optics',
            'spectral_clustering',
            'ward_tree',
            'SpectralBiclustering',
diff --git a/sklearn/cluster/_hierarchical.pyx b/sklearn/cluster/_hierarchical.pyx
index 5d42d8494495..0a87b8222981 100644
--- a/sklearn/cluster/_hierarchical.pyx
+++ b/sklearn/cluster/_hierarchical.pyx
@@ -343,7 +343,7 @@ cdef class UnionFind(object):
     cdef ITYPE_t[:] size
 
     def __init__(self, N):
-        self.parent = -1 * np.ones(2 * N - 1, dtype=ITYPE, order='C')
+        self.parent = np.full(2 * N - 1, -1., dtype=ITYPE, order='C')
         self.next_label = N
         self.size = np.hstack((np.ones(N, dtype=ITYPE),
                                np.zeros(N - 1, dtype=ITYPE)))
@@ -448,4 +448,4 @@ def single_linkage_label(L):
     if not is_sorted(L[:, 2]):
         raise ValueError("Input MST array must be sorted by weight")
 
-    return _single_linkage_label(L)
\ No newline at end of file
+    return _single_linkage_label(L)
diff --git a/sklearn/cluster/_k_means.pyx b/sklearn/cluster/_k_means.pyx
index e8800ee79238..66fd620a90cd 100644
--- a/sklearn/cluster/_k_means.pyx
+++ b/sklearn/cluster/_k_means.pyx
@@ -20,9 +20,6 @@ from sklearn.utils.sparsefuncs_fast import assign_rows_csr
 ctypedef np.float64_t DOUBLE
 ctypedef np.int32_t INT
 
-ctypedef floating (*DOT)(int N, floating *X, int incX, floating *Y,
-                         int incY)
-
 cdef extern from "cblas.h":
     double ddot "cblas_ddot"(int N, double *X, int incX, double *Y, int incY)
     float sdot "cblas_sdot"(int N, float *X, int incX, float *Y, int incY)
@@ -58,7 +55,6 @@ cpdef DOUBLE _assign_labels_array(np.ndarray[floating, ndim=2] X,
         DOUBLE inertia = 0.0
         DOUBLE min_dist
         DOUBLE dist
-        DOT dot
 
     if floating is float:
         center_squared_norms = np.zeros(n_clusters, dtype=np.float32)
@@ -130,7 +126,6 @@ cpdef DOUBLE _assign_labels_csr(X, np.ndarray[floating, ndim=1] sample_weight,
         DOUBLE inertia = 0.0
         DOUBLE min_dist
         DOUBLE dist
-        DOT dot
 
     if floating is float:
         center_squared_norms = np.zeros(n_clusters, dtype=np.float32)
diff --git a/sklearn/cluster/_optics_inner.pyx b/sklearn/cluster/_optics_inner.pyx
new file mode 100644
index 000000000000..24e861907854
--- /dev/null
+++ b/sklearn/cluster/_optics_inner.pyx
@@ -0,0 +1,31 @@
+cimport numpy as np
+import numpy as np
+cimport cython
+
+ctypedef np.float64_t DTYPE_t
+ctypedef np.int_t DTYPE
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+# Checks for smallest reachability distance
+# In case of tie, preserves order and returns first instance
+# as sorted by distance
+cpdef quick_scan(double[:] rdists, double[:] dists):
+    cdef Py_ssize_t n
+    cdef int idx
+    cdef int i
+    cdef double rdist
+    cdef double dist
+    rdist = np.inf
+    dist = np.inf
+    n = len(rdists)
+    for i from 0 <= i < n:
+        if rdists[i] < rdist:
+            rdist = rdists[i]
+            dist = dists[i]
+            idx = i
+        if rdists[i] == rdist:
+            if dists[i] < dist:
+                dist = dists[i]
+                idx = i
+    return idx
diff --git a/sklearn/cluster/affinity_propagation_.py b/sklearn/cluster/affinity_propagation_.py
index 307a4fde6fd5..2e646746eee7 100644
--- a/sklearn/cluster/affinity_propagation_.py
+++ b/sklearn/cluster/affinity_propagation_.py
@@ -290,6 +290,24 @@ class AffinityPropagation(BaseEstimator, ClusterMixin):
     n_iter_ : int
         Number of iterations taken to converge.
 
+    Examples
+    --------
+    >>> from sklearn.cluster import AffinityPropagation
+    >>> import numpy as np
+    >>> X = np.array([[1, 2], [1, 4], [1, 0],
+    ...               [4, 2], [4, 4], [4, 0]])
+    >>> clustering = AffinityPropagation().fit(X)
+    >>> clustering # doctest: +NORMALIZE_WHITESPACE
+    AffinityPropagation(affinity='euclidean', convergence_iter=15, copy=True,
+              damping=0.5, max_iter=200, preference=None, verbose=False)
+    >>> clustering.labels_
+    array([0, 0, 0, 1, 1, 1])
+    >>> clustering.predict([[0, 0], [4, 4]])
+    array([0, 1])
+    >>> clustering.cluster_centers_
+    array([[1, 2],
+           [4, 2]])
+
     Notes
     -----
     For an example, see :ref:`examples/cluster/plot_affinity_propagation.py
diff --git a/sklearn/cluster/bicluster.py b/sklearn/cluster/bicluster.py
index 81f2f411a7f4..8bbf7353129a 100644
--- a/sklearn/cluster/bicluster.py
+++ b/sklearn/cluster/bicluster.py
@@ -93,7 +93,7 @@ class BaseSpectral(six.with_metaclass(ABCMeta, BaseEstimator,
     @abstractmethod
     def __init__(self, n_clusters=3, svd_method="randomized",
                  n_svd_vecs=None, mini_batch=False, init="k-means++",
-                 n_init=10, n_jobs=1, random_state=None):
+                 n_init=10, n_jobs=None, random_state=None):
         self.n_clusters = n_clusters
         self.svd_method = svd_method
         self.n_svd_vecs = n_svd_vecs
@@ -228,15 +228,14 @@ class SpectralCoclustering(BaseSpectral):
         chosen and the algorithm runs once. Otherwise, the algorithm
         is run for each initialization and the best solution chosen.
 
-    n_jobs : int, optional, default: 1
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. This works by breaking
         down the pairwise matrix into n_jobs even slices and computing them in
         parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For n_jobs below -1,
-        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
-        are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None (default)
         Used for randomizing the singular value decomposition and the k-means
@@ -258,6 +257,22 @@ class SpectralCoclustering(BaseSpectral):
     column_labels_ : array-like, shape (n_cols,)
         The bicluster label of each column.
 
+    Examples
+    --------
+    >>> from sklearn.cluster import SpectralCoclustering
+    >>> import numpy as np
+    >>> X = np.array([[1, 1], [2, 1], [1, 0],
+    ...               [4, 7], [3, 5], [3, 6]])
+    >>> clustering = SpectralCoclustering(n_clusters=2, random_state=0).fit(X)
+    >>> clustering.row_labels_
+    array([0, 1, 1, 0, 0, 0], dtype=int32)
+    >>> clustering.column_labels_
+    array([0, 0], dtype=int32)
+    >>> clustering # doctest: +NORMALIZE_WHITESPACE
+    SpectralCoclustering(init='k-means++', mini_batch=False, n_clusters=2,
+               n_init=10, n_jobs=None, n_svd_vecs=None, random_state=0,
+               svd_method='randomized')
+
     References
     ----------
 
@@ -268,7 +283,7 @@ class SpectralCoclustering(BaseSpectral):
     """
     def __init__(self, n_clusters=3, svd_method='randomized',
                  n_svd_vecs=None, mini_batch=False, init='k-means++',
-                 n_init=10, n_jobs=1, random_state=None):
+                 n_init=10, n_jobs=None, random_state=None):
         super(SpectralCoclustering, self).__init__(n_clusters,
                                                    svd_method,
                                                    n_svd_vecs,
@@ -359,15 +374,14 @@ class SpectralBiclustering(BaseSpectral):
         chosen and the algorithm runs once. Otherwise, the algorithm
         is run for each initialization and the best solution chosen.
 
-    n_jobs : int, optional, default: 1
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. This works by breaking
         down the pairwise matrix into n_jobs even slices and computing them in
         parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For n_jobs below -1,
-        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
-        are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None (default)
         Used for randomizing the singular value decomposition and the k-means
@@ -389,6 +403,23 @@ class SpectralBiclustering(BaseSpectral):
     column_labels_ : array-like, shape (n_cols,)
         Column partition labels.
 
+    Examples
+    --------
+    >>> from sklearn.cluster import SpectralBiclustering
+    >>> import numpy as np
+    >>> X = np.array([[1, 1], [2, 1], [1, 0],
+    ...               [4, 7], [3, 5], [3, 6]])
+    >>> clustering = SpectralBiclustering(n_clusters=2, random_state=0).fit(X)
+    >>> clustering.row_labels_
+    array([1, 1, 1, 0, 0, 0], dtype=int32)
+    >>> clustering.column_labels_
+    array([0, 1], dtype=int32)
+    >>> clustering # doctest: +NORMALIZE_WHITESPACE
+    SpectralBiclustering(init='k-means++', method='bistochastic',
+               mini_batch=False, n_best=3, n_clusters=2, n_components=6,
+               n_init=10, n_jobs=None, n_svd_vecs=None, random_state=0,
+               svd_method='randomized')
+
     References
     ----------
 
@@ -400,7 +431,7 @@ class SpectralBiclustering(BaseSpectral):
     def __init__(self, n_clusters=3, method='bistochastic',
                  n_components=6, n_best=3, svd_method='randomized',
                  n_svd_vecs=None, mini_batch=False, init='k-means++',
-                 n_init=10, n_jobs=1, random_state=None):
+                 n_init=10, n_jobs=None, random_state=None):
         super(SpectralBiclustering, self).__init__(n_clusters,
                                                    svd_method,
                                                    n_svd_vecs,
diff --git a/sklearn/cluster/birch.py b/sklearn/cluster/birch.py
index f50e7356ae63..02925598573b 100644
--- a/sklearn/cluster/birch.py
+++ b/sklearn/cluster/birch.py
@@ -74,7 +74,7 @@ def _split_node(node, threshold, branching_factor):
 
     farthest_idx = np.unravel_index(
         dist.argmax(), (n_clusters, n_clusters))
-    node1_dist, node2_dist = dist[[farthest_idx]]
+    node1_dist, node2_dist = dist[(farthest_idx,)]
 
     node1_closer = node1_dist < node2_dist
     for idx, subcluster in enumerate(node.subclusters_):
@@ -394,7 +394,7 @@ class Birch(BaseEstimator, TransformerMixin, ClusterMixin):
     >>> X = [[0, 1], [0.3, 1], [-0.3, 1], [0, -1], [0.3, -1], [-0.3, -1]]
     >>> brc = Birch(branching_factor=50, n_clusters=None, threshold=0.5,
     ... compute_labels=True)
-    >>> brc.fit(X)
+    >>> brc.fit(X) # doctest: +NORMALIZE_WHITESPACE
     Birch(branching_factor=50, compute_labels=True, copy=True, n_clusters=None,
        threshold=0.5)
     >>> brc.predict(X)
diff --git a/sklearn/cluster/dbscan_.py b/sklearn/cluster/dbscan_.py
index ea4e02badbfc..f10890e10f2c 100644
--- a/sklearn/cluster/dbscan_.py
+++ b/sklearn/cluster/dbscan_.py
@@ -20,7 +20,8 @@
 
 
 def dbscan(X, eps=0.5, min_samples=5, metric='minkowski', metric_params=None,
-           algorithm='auto', leaf_size=30, p=2, sample_weight=None, n_jobs=1):
+           algorithm='auto', leaf_size=30, p=2, sample_weight=None,
+           n_jobs=None):
     """Perform DBSCAN clustering from vector array or distance matrix.
 
     Read more in the :ref:`User Guide <dbscan>`.
@@ -75,9 +76,11 @@ def dbscan(X, eps=0.5, min_samples=5, metric='minkowski', metric_params=None,
         weight may inhibit its eps-neighbor from being core.
         Note that weights are absolute, and default to 1.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Returns
     -------
@@ -87,6 +90,14 @@ def dbscan(X, eps=0.5, min_samples=5, metric='minkowski', metric_params=None,
     labels : array [n_samples]
         Cluster labels for each point.  Noisy samples are given the label -1.
 
+    See also
+    --------
+    DBSCAN
+        An estimator interface for this clustering algorithm.
+    optics
+        A similar clustering at multiple values of eps. Our implementation
+        is optimized for memory usage.
+
     Notes
     -----
     For an example, see :ref:`examples/cluster/plot_dbscan.py
@@ -107,6 +118,9 @@ def dbscan(X, eps=0.5, min_samples=5, metric='minkowski', metric_params=None,
     Another way to reduce memory and computation time is to remove
     (near-)duplicate points and use ``sample_weight`` instead.
 
+    :func:`cluster.optics` provides a similar clustering with lower memory
+    usage.
+
     References
     ----------
     Ester, M., H. P. Kriegel, J. Sander, and X. Xu, "A Density-Based
@@ -158,7 +172,7 @@ def dbscan(X, eps=0.5, min_samples=5, metric='minkowski', metric_params=None,
                                 for neighbors in neighborhoods])
 
     # Initially, all samples are noise.
-    labels = -np.ones(X.shape[0], dtype=np.intp)
+    labels = np.full(X.shape[0], -1, dtype=np.intp)
 
     # A list of all core samples found.
     core_samples = np.asarray(n_neighbors >= min_samples, dtype=np.uint8)
@@ -217,9 +231,11 @@ class DBSCAN(BaseEstimator, ClusterMixin):
         The power of the Minkowski metric to be used to calculate distance
         between points.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+       ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -233,6 +249,25 @@ class DBSCAN(BaseEstimator, ClusterMixin):
         Cluster labels for each point in the dataset given to fit().
         Noisy samples are given the label -1.
 
+    Examples
+    --------
+    >>> from sklearn.cluster import DBSCAN
+    >>> import numpy as np
+    >>> X = np.array([[1, 2], [2, 2], [2, 3],
+    ...               [8, 7], [8, 8], [25, 80]])
+    >>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)
+    >>> clustering.labels_
+    array([ 0,  0,  0,  1,  1, -1])
+    >>> clustering # doctest: +NORMALIZE_WHITESPACE
+    DBSCAN(algorithm='auto', eps=3, leaf_size=30, metric='euclidean',
+        metric_params=None, min_samples=2, n_jobs=None, p=None)
+
+    See also
+    --------
+    OPTICS
+        A similar clustering at multiple values of eps. Our implementation
+        is optimized for memory usage.
+
     Notes
     -----
     For an example, see :ref:`examples/cluster/plot_dbscan.py
@@ -253,6 +288,9 @@ class DBSCAN(BaseEstimator, ClusterMixin):
     Another way to reduce memory and computation time is to remove
     (near-)duplicate points and use ``sample_weight`` instead.
 
+    :class:`cluster.OPTICS` provides a similar clustering with lower memory
+    usage.
+
     References
     ----------
     Ester, M., H. P. Kriegel, J. Sander, and X. Xu, "A Density-Based
@@ -263,7 +301,7 @@ class DBSCAN(BaseEstimator, ClusterMixin):
 
     def __init__(self, eps=0.5, min_samples=5, metric='euclidean',
                  metric_params=None, algorithm='auto', leaf_size=30, p=None,
-                 n_jobs=1):
+                 n_jobs=None):
         self.eps = eps
         self.min_samples = min_samples
         self.metric = metric
diff --git a/sklearn/cluster/hierarchical.py b/sklearn/cluster/hierarchical.py
index c462f2f2cda2..1d6755fd7206 100644
--- a/sklearn/cluster/hierarchical.py
+++ b/sklearn/cluster/hierarchical.py
@@ -93,7 +93,7 @@ def _single_linkage_tree(connectivity, n_samples, n_nodes, n_clusters,
     connectivity = connectivity.astype('float64')
 
     # Ensure zero distances aren't ignored by setting them to "epsilon"
-    epsilon_value = np.nextafter(0, 1, dtype=connectivity.data.dtype)
+    epsilon_value = np.finfo(dtype=connectivity.data.dtype).eps
     connectivity.data[connectivity.data == 0] = epsilon_value
 
     # Use scipy.sparse.csgraph to generate a minimum spanning tree
@@ -733,6 +733,20 @@ class AgglomerativeClustering(BaseEstimator, ClusterMixin):
         at the i-th iteration, children[i][0] and children[i][1]
         are merged to form node `n_samples + i`
 
+    Examples
+    --------
+    >>> from sklearn.cluster import AgglomerativeClustering
+    >>> import numpy as np
+    >>> X = np.array([[1, 2], [1, 4], [1, 0],
+    ...               [4, 2], [4, 4], [4, 0]])
+    >>> clustering = AgglomerativeClustering().fit(X)
+    >>> clustering # doctest: +NORMALIZE_WHITESPACE
+    AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto',
+                connectivity=None, linkage='ward', memory=None, n_clusters=2,
+                pooling_func='deprecated')
+    >>> clustering.labels_
+    array([1, 1, 1, 0, 0, 0])
+
     """
 
     def __init__(self, n_clusters=2, affinity="euclidean",
@@ -762,7 +776,8 @@ def fit(self, X, y=None):
         -------
         self
         """
-        if self.pooling_func != 'deprecated':
+        if (self.pooling_func != 'deprecated' and
+                not isinstance(self, AgglomerationTransform)):
             warnings.warn('Agglomerative "pooling_func" parameter is not used.'
                           ' It has been deprecated in version 0.20 and will be'
                           'removed in 0.22', DeprecationWarning)
@@ -902,6 +917,22 @@ class FeatureAgglomeration(AgglomerativeClustering, AgglomerationTransform):
         node and has children `children_[i - n_features]`. Alternatively
         at the i-th iteration, children[i][0] and children[i][1]
         are merged to form node `n_features + i`
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn import datasets, cluster
+    >>> digits = datasets.load_digits()
+    >>> images = digits.images
+    >>> X = np.reshape(images, (len(images), -1))
+    >>> agglo = cluster.FeatureAgglomeration(n_clusters=32)
+    >>> agglo.fit(X) # doctest: +ELLIPSIS
+    FeatureAgglomeration(affinity='euclidean', compute_full_tree='auto',
+               connectivity=None, linkage='ward', memory=None, n_clusters=32,
+               pooling_func=...)
+    >>> X_reduced = agglo.transform(X)
+    >>> X_reduced.shape
+    (1797, 32)
     """
 
     def __init__(self, n_clusters=2, affinity="euclidean",
diff --git a/sklearn/cluster/k_means_.py b/sklearn/cluster/k_means_.py
index 42ca6402df06..c402bf6c8b61 100644
--- a/sklearn/cluster/k_means_.py
+++ b/sklearn/cluster/k_means_.py
@@ -25,12 +25,13 @@
 from ..utils.sparsefuncs import mean_variance_axis
 from ..utils.validation import _num_samples
 from ..utils import check_array
-from ..utils import check_random_state
 from ..utils import gen_batches
+from ..utils import check_random_state
 from ..utils.validation import check_is_fitted
 from ..utils.validation import FLOAT_DTYPES
-from ..externals.joblib import Parallel
-from ..externals.joblib import delayed
+from ..utils import Parallel
+from ..utils import delayed
+from ..utils import effective_n_jobs
 from ..externals.six import string_types
 from ..exceptions import ConvergenceWarning
 from . import _k_means
@@ -184,8 +185,8 @@ def _check_sample_weight(X, sample_weight):
 
 def k_means(X, n_clusters, sample_weight=None, init='k-means++',
             precompute_distances='auto', n_init=10, max_iter=300,
-            verbose=False, tol=1e-4, random_state=None, copy_x=True, n_jobs=1,
-            algorithm="auto", return_n_iter=False):
+            verbose=False, tol=1e-4, random_state=None, copy_x=True,
+            n_jobs=None, algorithm="auto", return_n_iter=False):
     """K-means clustering algorithm.
 
     Read more in the :ref:`User Guide <k_means>`.
@@ -260,14 +261,13 @@ def k_means(X, n_clusters, sample_weight=None, init='k-means++',
         the data mean, in this case it will also not ensure that data is
         C-contiguous which may cause a significant slowdown.
 
-    n_jobs : int
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. This works by computing
         each of the n_init runs in parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For n_jobs below -1,
-        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
-        are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     algorithm : "auto", "full" or "elkan", default="auto"
         K-means algorithm to use. The classical EM-style algorithm is "full".
@@ -368,7 +368,7 @@ def k_means(X, n_clusters, sample_weight=None, init='k-means++',
     else:
         raise ValueError("Algorithm must be 'auto', 'full' or 'elkan', got"
                          " %s" % str(algorithm))
-    if n_jobs == 1:
+    if effective_n_jobs(n_jobs):
         # For a single thread, less memory is needed if we just store one set
         # of the best results (as opposed to one set per run per thread).
         for it in range(n_init):
@@ -669,7 +669,7 @@ def _labels_inertia(X, sample_weight, x_squared_norms, centers,
     sample_weight = _check_sample_weight(X, sample_weight)
     # set the default value of centers to -1 to be able to detect any anomaly
     # easily
-    labels = -np.ones(n_samples, np.int32)
+    labels = np.full(n_samples, -1, np.int32)
     if distances is None:
         distances = np.zeros(shape=(0,), dtype=X.dtype)
     # distances will be changed in-place
@@ -833,14 +833,13 @@ class KMeans(BaseEstimator, ClusterMixin, TransformerMixin):
         the data mean, in this case it will also not ensure that data is
         C-contiguous which may cause a significant slowdown.
 
-    n_jobs : int
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. This works by computing
         each of the n_init runs in parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For n_jobs below -1,
-        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
-        are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     algorithm : "auto", "full" or "elkan", default="auto"
         K-means algorithm to use. The classical EM-style algorithm is "full".
@@ -889,7 +888,7 @@ class KMeans(BaseEstimator, ClusterMixin, TransformerMixin):
 
     Notes
     ------
-    The k-means problem is solved using Lloyd's algorithm.
+    The k-means problem is solved using either Lloyd's or Elkan's algorithm.
 
     The average complexity is given by O(k n T), were n is the number of
     samples and T is the number of iteration.
@@ -902,12 +901,18 @@ class KMeans(BaseEstimator, ClusterMixin, TransformerMixin):
     clustering algorithms available), but it falls in local minima. That's why
     it can be useful to restart it several times.
 
+    If the algorithm stops before fully converging (because of ``tol`` of
+    ``max_iter``), ``labels_`` and ``means_`` will not be consistent, i.e. the
+    ``means_`` will not be the means of the points in each cluster.
+    Also, the estimator will reassign ``labels_`` after the last iteration to
+    make ``labels_`` consistent with ``predict`` on the training set.
+
     """
 
     def __init__(self, n_clusters=8, init='k-means++', n_init=10,
                  max_iter=300, tol=1e-4, precompute_distances='auto',
                  verbose=0, random_state=None, copy_x=True,
-                 n_jobs=1, algorithm='auto'):
+                 n_jobs=None, algorithm='auto'):
 
         self.n_clusters = n_clusters
         self.init = init
@@ -1398,6 +1403,36 @@ class MiniBatchKMeans(KMeans):
         defined as the sum of square distances of samples to their nearest
         neighbor.
 
+    Examples
+    --------
+    >>> from sklearn.cluster import MiniBatchKMeans
+    >>> import numpy as np
+    >>> X = np.array([[1, 2], [1, 4], [1, 0],
+    ...               [4, 2], [4, 0], [4, 4],
+    ...               [4, 5], [0, 1], [2, 2],
+    ...               [3, 2], [5, 5], [1, -1]])
+    >>> # manually fit on batches
+    >>> kmeans = MiniBatchKMeans(n_clusters=2,
+    ...         random_state=0,
+    ...         batch_size=6)
+    >>> kmeans = kmeans.partial_fit(X[0:6,:])
+    >>> kmeans = kmeans.partial_fit(X[6:12,:])
+    >>> kmeans.cluster_centers_
+    array([[1, 1],
+           [3, 4]])
+    >>> kmeans.predict([[0, 0], [4, 4]])
+    array([0, 1], dtype=int32)
+    >>> # fit on the whole data
+    >>> kmeans = MiniBatchKMeans(n_clusters=2,
+    ...         random_state=0,
+    ...         batch_size=6,
+    ...         max_iter=10).fit(X)
+    >>> kmeans.cluster_centers_
+    array([[3.95918367, 2.40816327],
+           [1.12195122, 1.3902439 ]])
+    >>> kmeans.predict([[0, 0], [4, 4]])
+    array([1, 0], dtype=int32)
+
     See also
     --------
 
diff --git a/sklearn/cluster/mean_shift_.py b/sklearn/cluster/mean_shift_.py
index 332531b13078..487545ac039d 100644
--- a/sklearn/cluster/mean_shift_.py
+++ b/sklearn/cluster/mean_shift_.py
@@ -24,12 +24,12 @@
 from ..base import BaseEstimator, ClusterMixin
 from ..neighbors import NearestNeighbors
 from ..metrics.pairwise import pairwise_distances_argmin
-from ..externals.joblib import Parallel
-from ..externals.joblib import delayed
+from ..utils import Parallel
+from ..utils import delayed
 
 
 def estimate_bandwidth(X, quantile=0.3, n_samples=None, random_state=0,
-                       n_jobs=1):
+                       n_jobs=None):
     """Estimate the bandwidth to use with the mean-shift algorithm.
 
     That this function takes time at least quadratic in n_samples. For large
@@ -53,9 +53,11 @@ def estimate_bandwidth(X, quantile=0.3, n_samples=None, random_state=0,
         deterministic.
         See :term:`Glossary <random_state>`.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Returns
     -------
@@ -107,7 +109,7 @@ def _mean_shift_single_seed(my_mean, X, nbrs, max_iter):
 
 def mean_shift(X, bandwidth=None, seeds=None, bin_seeding=False,
                min_bin_freq=1, cluster_all=True, max_iter=300,
-               n_jobs=1):
+               n_jobs=None):
     """Perform mean shift clustering of data using a flat kernel.
 
     Read more in the :ref:`User Guide <mean_shift>`.
@@ -152,14 +154,13 @@ def mean_shift(X, bandwidth=None, seeds=None, bin_seeding=False,
         Maximum number of iterations, per seed point before the clustering
         operation terminates (for that seed point), if has not converged yet.
 
-    n_jobs : int
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. This works by computing
         each of the n_init runs in parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For n_jobs below -1,
-        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
-        are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
         .. versionadded:: 0.17
            Parallel Execution using *n_jobs*.
@@ -334,14 +335,13 @@ class MeanShift(BaseEstimator, ClusterMixin):
         not within any kernel. Orphans are assigned to the nearest kernel.
         If false, then orphans are given cluster label -1.
 
-    n_jobs : int
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. This works by computing
         each of the n_init runs in parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For n_jobs below -1,
-        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
-        are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -351,6 +351,21 @@ class MeanShift(BaseEstimator, ClusterMixin):
     labels_ :
         Labels of each point.
 
+    Examples
+    --------
+    >>> from sklearn.cluster import MeanShift
+    >>> import numpy as np
+    >>> X = np.array([[1, 1], [2, 1], [1, 0],
+    ...               [4, 7], [3, 5], [3, 6]])
+    >>> clustering = MeanShift(bandwidth=2).fit(X)
+    >>> clustering.labels_
+    array([0, 0, 0, 1, 1, 1])
+    >>> clustering.predict([[0, 0], [5, 5]])
+    array([0, 1])
+    >>> clustering # doctest: +NORMALIZE_WHITESPACE
+    MeanShift(bandwidth=2, bin_seeding=False, cluster_all=True, min_bin_freq=1,
+         n_jobs=None, seeds=None)
+
     Notes
     -----
 
@@ -377,7 +392,7 @@ class MeanShift(BaseEstimator, ClusterMixin):
 
     """
     def __init__(self, bandwidth=None, seeds=None, bin_seeding=False,
-                 min_bin_freq=1, cluster_all=True, n_jobs=1):
+                 min_bin_freq=1, cluster_all=True, n_jobs=None):
         self.bandwidth = bandwidth
         self.seeds = seeds
         self.bin_seeding = bin_seeding
diff --git a/sklearn/cluster/optics_.py b/sklearn/cluster/optics_.py
new file mode 100755
index 000000000000..e10a92a7590e
--- /dev/null
+++ b/sklearn/cluster/optics_.py
@@ -0,0 +1,770 @@
+# -*- coding: utf-8 -*-
+"""Ordering Points To Identify the Clustering Structure (OPTICS)
+
+These routines execute the OPTICS algorithm, and implement various
+cluster extraction methods of the ordered list.
+
+Authors: Shane Grigsby <refuge@rocktalus.com>
+         Amy X. Zhang <axz@mit.edu>
+License: BSD 3 clause
+"""
+
+from __future__ import division
+import warnings
+import numpy as np
+
+from ..utils import check_array
+from ..utils.validation import check_is_fitted
+from ..neighbors import NearestNeighbors
+from ..base import BaseEstimator, ClusterMixin
+from ..metrics import pairwise_distances
+from ._optics_inner import quick_scan
+
+
+def optics(X, min_samples=5, max_bound=np.inf, metric='euclidean',
+           p=2, metric_params=None, maxima_ratio=.75,
+           rejection_ratio=.7, similarity_threshold=0.4,
+           significant_min=.003, min_cluster_size_ratio=.005,
+           min_maxima_ratio=0.001, algorithm='ball_tree',
+           leaf_size=30, n_jobs=None):
+    """Perform OPTICS clustering from vector array
+
+    OPTICS: Ordering Points To Identify the Clustering Structure
+    Equivalent to DBSCAN, finds core sample of high density and expands
+    clusters from them. Unlike DBSCAN, keeps cluster hierarchy for a variable
+    neighborhood radius. Optimized for usage on large point datasets.
+
+    Read more in the :ref:`User Guide <optics>`.
+
+    Parameters
+    ----------
+    X : array, shape (n_samples, n_features)
+        The data.
+
+    min_samples : int
+        The number of samples in a neighborhood for a point to be considered
+        as a core point.
+
+    max_bound : float, optional
+        The maximum distance between two samples for them to be considered
+        as in the same neighborhood. This is also the largest object size
+        expected within the dataset. Default value of "np.inf" will identify
+        clusters across all scales; reducing `max_bound` will result in
+        shorter run times.
+
+    metric : string or callable, optional
+        The distance metric to use for neighborhood lookups. Default is
+        "minkowski". Other options include "euclidean", "manhattan",
+        "chebyshev", "haversine", "seuclidean", "hamming", "canberra",
+        and "braycurtis". The "wminkowski" and "mahalanobis" metrics are
+        also valid with an additional argument.
+
+    p : integer, optional (default=2)
+        Parameter for the Minkowski metric from
+        :class:`sklearn.metrics.pairwise_distances`. When p = 1, this is
+        equivalent to using manhattan_distance (l1), and euclidean_distance
+        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.
+
+    metric_params : dict, optional (default=None)
+        Additional keyword arguments for the metric function.
+
+    maxima_ratio : float, optional
+        The maximum ratio we allow of average height of clusters on the
+        right and left to the local maxima in question. The higher the
+        ratio, the more generous the algorithm is to preserving local
+        minima, and the more cuts the resulting tree will have.
+
+    rejection_ratio : float, optional
+        Adjusts the fitness of the clustering. When the maxima_ratio is
+        exceeded, determine which of the clusters to the left and right to
+        reject based on rejection_ratio. Higher values will result in points
+        being more readily classified as noise; conversely, lower values will
+        result in more points being clustered.
+
+    similarity_threshold : float, optional
+        Used to check if nodes can be moved up one level, that is, if the
+        new cluster created is too "similar" to its parent, given the
+        similarity threshold. Similarity can be determined by 1) the size
+        of the new cluster relative to the size of the parent node or
+        2) the average of the reachability values of the new cluster
+        relative to the average of the reachability values of the parent
+        node. A lower value for the similarity threshold means less levels
+        in the tree.
+
+    significant_min : float, optional
+        Sets a lower threshold on how small a significant maxima can be.
+
+    min_cluster_size_ratio : float, optional
+        Minimum percentage of dataset expected for cluster membership.
+
+    min_maxima_ratio : float, optional
+        Used to determine neighborhood size for minimum cluster membership.
+
+    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional
+        Algorithm used to compute the nearest neighbors:
+
+        - 'ball_tree' will use :class:`BallTree`
+        - 'kd_tree' will use :class:`KDTree`
+        - 'brute' will use a brute-force search.
+        - 'auto' will attempt to decide the most appropriate algorithm
+          based on the values passed to :meth:`fit` method.
+
+        Note: fitting on sparse input will override the setting of
+        this parameter, using brute force.
+
+    leaf_size : int, optional (default=30)
+        Leaf size passed to :class:`BallTree` or :class:`KDTree`. This can
+        affect the speed of the construction and query, as well as the memory
+        required to store the tree. The optimal value depends on the
+        nature of the problem.
+
+    n_jobs : int or None, optional (default=None)
+        The number of parallel jobs to run for neighbors search.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
+
+    Returns
+    -------
+    core_sample_indices_ : array, shape (n_core_samples,)
+        The indices of the core samples.
+
+    labels_ : array, shape (n_samples,)
+        The estimated labels.
+
+    See also
+    --------
+    OPTICS
+        An estimator interface for this clustering algorithm.
+    dbscan
+        A similar clustering for a specified neighborhood radius (eps).
+        Our implementation is optimized for runtime.
+
+    References
+    ----------
+    Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel, and Jörg Sander.
+    "OPTICS: ordering points to identify the clustering structure." ACM SIGMOD
+    Record 28, no. 2 (1999): 49-60.
+    """
+
+    clust = OPTICS(min_samples, max_bound, metric, p, metric_params,
+                   maxima_ratio, rejection_ratio,
+                   similarity_threshold, significant_min,
+                   min_cluster_size_ratio, min_maxima_ratio,
+                   algorithm, leaf_size, n_jobs)
+    clust.fit(X)
+    return clust.core_sample_indices_, clust.labels_
+
+
+class OPTICS(BaseEstimator, ClusterMixin):
+    """Estimate clustering structure from vector array
+
+    OPTICS: Ordering Points To Identify the Clustering Structure
+    Equivalent to DBSCAN, finds core sample of high density and expands
+    clusters from them. Unlike DBSCAN, keeps cluster hierarchy for a variable
+    neighborhood radius. Optimized for usage on large point datasets.
+
+    Read more in the :ref:`User Guide <optics>`.
+
+    Parameters
+    ----------
+    min_samples : int
+        The number of samples in a neighborhood for a point to be considered
+        as a core point.
+
+    max_bound : float, optional
+        The maximum distance between two samples for them to be considered
+        as in the same neighborhood. This is also the largest object size
+        expected within the dataset. Default value of "np.inf" will identify
+        clusters across all scales; reducing `max_bound` will result in
+        shorter run times.
+
+    metric : string or callable, optional
+        The distance metric to use for neighborhood lookups. Default is
+        "minkowski". Other options include "euclidean", "manhattan",
+        "chebyshev", "haversine", "seuclidean", "hamming", "canberra",
+        and "braycurtis". The "wminkowski" and "mahalanobis" metrics are
+        also valid with an additional argument.
+
+    p : integer, optional (default=2)
+        Parameter for the Minkowski metric from
+        :class:`sklearn.metrics.pairwise_distances`. When p = 1, this is
+        equivalent to using manhattan_distance (l1), and euclidean_distance
+        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.
+
+    metric_params : dict, optional (default=None)
+        Additional keyword arguments for the metric function.
+
+    maxima_ratio : float, optional
+        The maximum ratio we allow of average height of clusters on the
+        right and left to the local maxima in question. The higher the
+        ratio, the more generous the algorithm is to preserving local
+        minima, and the more cuts the resulting tree will have.
+
+    rejection_ratio : float, optional
+        Adjusts the fitness of the clustering. When the maxima_ratio is
+        exceeded, determine which of the clusters to the left and right to
+        reject based on rejection_ratio. Higher values will result in points
+        being more readily classified as noise; conversely, lower values will
+        result in more points being clustered.
+
+    similarity_threshold : float, optional
+        Used to check if nodes can be moved up one level, that is, if the
+        new cluster created is too "similar" to its parent, given the
+        similarity threshold. Similarity can be determined by 1) the size
+        of the new cluster relative to the size of the parent node or
+        2) the average of the reachability values of the new cluster
+        relative to the average of the reachability values of the parent
+        node. A lower value for the similarity threshold means less levels
+        in the tree.
+
+    significant_min : float, optional
+        Sets a lower threshold on how small a significant maxima can be.
+
+    min_cluster_size_ratio : float, optional
+        Minimum percentage of dataset expected for cluster membership.
+
+    min_maxima_ratio : float, optional
+        Used to determine neighborhood size for minimum cluster membership.
+
+    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional
+        Algorithm used to compute the nearest neighbors:
+
+        - 'ball_tree' will use :class:`BallTree`
+        - 'kd_tree' will use :class:`KDTree`
+        - 'brute' will use a brute-force search.
+        - 'auto' will attempt to decide the most appropriate algorithm
+          based on the values passed to :meth:`fit` method.
+
+        Note: fitting on sparse input will override the setting of
+        this parameter, using brute force.
+
+    leaf_size : int, optional (default=30)
+        Leaf size passed to :class:`BallTree` or :class:`KDTree`. This can
+        affect the speed of the construction and query, as well as the memory
+        required to store the tree. The optimal value depends on the
+        nature of the problem.
+
+    n_jobs : int or None, optional (default=None)
+        The number of parallel jobs to run for neighbors search.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
+
+    Attributes
+    ----------
+    core_sample_indices_ : array, shape (n_core_samples,)
+        Indices of core samples.
+
+    labels_ : array, shape (n_samples,)
+        Cluster labels for each point in the dataset given to fit().
+        Noisy samples are given the label -1.
+
+    reachability_ : array, shape (n_samples,)
+        Reachability distances per sample.
+
+    ordering_ : array, shape (n_samples,)
+        The cluster ordered list of sample indices
+
+    core_distances_ : array, shape (n_samples,)
+        Distance at which each sample becomes a core point.
+        Points which will never be core have a distance of inf.
+
+    See also
+    --------
+
+    DBSCAN
+        A similar clustering for a specified neighborhood radius (eps).
+        Our implementation is optimized for runtime.
+
+    References
+    ----------
+    Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel, and Jörg Sander.
+    "OPTICS: ordering points to identify the clustering structure." ACM SIGMOD
+    Record 28, no. 2 (1999): 49-60.
+    """
+
+    def __init__(self, min_samples=5, max_bound=np.inf, metric='euclidean',
+                 p=2, metric_params=None, maxima_ratio=.75,
+                 rejection_ratio=.7, similarity_threshold=0.4,
+                 significant_min=.003, min_cluster_size_ratio=.005,
+                 min_maxima_ratio=0.001, algorithm='ball_tree',
+                 leaf_size=30, n_jobs=None):
+
+        self.max_bound = max_bound
+        self.min_samples = min_samples
+        self.maxima_ratio = maxima_ratio
+        self.rejection_ratio = rejection_ratio
+        self.similarity_threshold = similarity_threshold
+        self.significant_min = significant_min
+        self.min_cluster_size_ratio = min_cluster_size_ratio
+        self.min_maxima_ratio = min_maxima_ratio
+        self.algorithm = algorithm
+        self.metric = metric
+        self.metric_params = metric_params
+        self.p = p
+        self.leaf_size = leaf_size
+        self.n_jobs = n_jobs
+
+    def fit(self, X, y=None):
+        """Perform OPTICS clustering
+
+        Extracts an ordered list of points and reachability distances, and
+        performs initial clustering using `max_bound` distance specified at
+        OPTICS object instantiation.
+
+        Parameters
+        ----------
+        X : array, shape (n_samples, n_features)
+            The data.
+
+        y : ignored
+
+        Returns
+        -------
+        self : instance of OPTICS
+            The instance.
+        """
+        X = check_array(X, dtype=np.float)
+
+        n_samples = len(X)
+        # Start all points as 'unprocessed' ##
+        self._processed = np.zeros((n_samples, 1), dtype=bool)
+        self.reachability_ = np.empty(n_samples)
+        self.reachability_.fill(np.inf)
+        self.core_distances_ = np.empty(n_samples)
+        self.core_distances_.fill(np.nan)
+        # Start all points as noise ##
+        self.labels_ = np.full(n_samples, -1, dtype=int)
+        self.ordering_ = []
+
+        # Check for valid n_samples relative to min_samples
+        if self.min_samples > n_samples:
+            raise ValueError("Number of training samples (n_samples=%d) must "
+                             "be greater than min_samples (min_samples=%d) "
+                             "used for clustering." %
+                             (n_samples, self.min_samples))
+
+        nbrs = NearestNeighbors(n_neighbors=self.min_samples,
+                                algorithm=self.algorithm,
+                                leaf_size=self.leaf_size, metric=self.metric,
+                                metric_params=self.metric_params, p=self.p,
+                                n_jobs=self.n_jobs)
+
+        nbrs.fit(X)
+        self.core_distances_[:] = nbrs.kneighbors(X,
+                                                  self.min_samples)[0][:, -1]
+
+        # Main OPTICS loop. Not parallelizable. The order that entries are
+        # written to the 'ordering_' list is important!
+        for point in range(n_samples):
+            if not self._processed[point]:
+                self._expand_cluster_order(point, X, nbrs)
+
+        indices_, self.labels_ = _extract_optics(self.ordering_,
+                                                 self.reachability_,
+                                                 self.maxima_ratio,
+                                                 self.rejection_ratio,
+                                                 self.similarity_threshold,
+                                                 self.significant_min,
+                                                 self.min_cluster_size_ratio,
+                                                 self.min_maxima_ratio)
+        self.core_sample_indices_ = indices_
+        self.n_clusters_ = np.max(self.labels_)
+        return self
+
+    # OPTICS helper functions; these should not be public #
+
+    def _expand_cluster_order(self, point, X, nbrs):
+        # As above, not parallelizable. Parallelizing would allow items in
+        # the 'unprocessed' list to switch to 'processed'
+        if self.core_distances_[point] <= self.max_bound:
+            while not self._processed[point]:
+                self._processed[point] = True
+                self.ordering_.append(point)
+                point = self._set_reach_dist(point, X, nbrs)
+        else:  # For very noisy points
+            self.ordering_.append(point)
+            self._processed[point] = True
+
+    def _set_reach_dist(self, point_index, X, nbrs):
+        P = np.array(X[point_index]).reshape(1, -1)
+        indices = nbrs.radius_neighbors(P, radius=self.max_bound,
+                                        return_distance=False)[0]
+
+        # Getting indices of neighbors that have not been processed
+        unproc = np.compress((~np.take(self._processed, indices)).ravel(),
+                             indices, axis=0)
+        # Keep n_jobs = 1 in the following lines...please
+        if len(unproc) > 0:
+            dists = pairwise_distances(P, np.take(X, unproc, axis=0),
+                                       self.metric, n_jobs=None).ravel()
+
+            rdists = np.maximum(dists, self.core_distances_[point_index])
+            new_reach = np.minimum(np.take(self.reachability_, unproc), rdists)
+            self.reachability_[unproc] = new_reach
+
+        # Checks to see if everything is already processed;
+        # if so, return control to main loop
+        if unproc.size > 0:
+            # Define return order based on reachability distance
+            return(unproc[quick_scan(np.take(self.reachability_, unproc),
+                                     dists)])
+        else:
+            return point_index
+
+    def extract_dbscan(self, eps):
+        """Performs DBSCAN extraction for an arbitrary epsilon.
+
+        Extraction runs in linear time. Note that if the `max_bound` OPTICS
+        parameter was set to < inf for extracting reachability and ordering
+        arrays, DBSCAN extractions will be unstable for `eps` values close to
+        `max_bound`. Setting `eps` < (`max_bound` / 5.0) will guarantee
+        extraction parity with DBSCAN.
+
+        Parameters
+        ----------
+        eps : float or int, required
+            DBSCAN `eps` parameter. Must be set to < `max_bound`. Equivalence
+            with DBSCAN algorithm is achieved if `eps` is < (`max_bound` / 5)
+
+        Returns
+        -------
+        core_sample_indices_ : array, shape (n_core_samples,)
+            The indices of the core samples.
+
+        labels_ : array, shape (n_samples,)
+            The estimated labels.
+        """
+        check_is_fitted(self, 'reachability_')
+
+        if eps > self.max_bound:
+            raise ValueError('Specify an epsilon smaller than %s. Got %s.'
+                             % (self.max_bound, eps))
+
+        if eps * 5.0 > (self.max_bound * 1.05):
+            warnings.warn(
+                "Warning, max_bound (%s) is close to eps (%s): "
+                "Output may be unstable." % (self.max_bound, eps),
+                RuntimeWarning, stacklevel=2)
+        # Stability warning is documented in _extract_dbscan method...
+
+        return _extract_dbscan(self.ordering_, self.core_distances_,
+                               self.reachability_, eps)
+
+
+def _extract_dbscan(ordering, core_distances, reachability, eps):
+    """Performs DBSCAN extraction for an arbitrary epsilon (`eps`).
+
+    Parameters
+    ----------
+    ordering : array, shape (n_samples,)
+        OPTICS ordered point indices (`ordering_`)
+    core_distances : array, shape (n_samples,)
+        Distances at which points become core (`core_distances_`)
+    reachability : array, shape (n_samples,)
+        Reachability distances calculated by OPTICS (`reachability_`)
+    eps : float or int
+        DBSCAN `eps` parameter
+
+    Returns
+    -------
+    core_sample_indices_ : array, shape (n_core_samples,)
+        The indices of the core samples.
+
+    labels_ : array, shape (n_samples,)
+        The estimated labels.
+    """
+
+    n_samples = len(core_distances)
+    is_core = np.zeros(n_samples, dtype=bool)
+    labels = np.zeros(n_samples, dtype=int)
+
+    far_reach = reachability > eps
+    near_core = core_distances <= eps
+    labels[ordering] = np.cumsum(far_reach[ordering] & near_core[ordering]) - 1
+    labels[far_reach & ~near_core] = -1
+    is_core[near_core] = True
+    return np.arange(n_samples)[is_core], labels
+
+
+def _extract_optics(ordering, reachability, maxima_ratio=.75,
+                    rejection_ratio=.7, similarity_threshold=0.4,
+                    significant_min=.003, min_cluster_size_ratio=.005,
+                    min_maxima_ratio=0.001):
+    """Performs automatic cluster extraction for variable density data.
+
+    Parameters
+    ----------
+    ordering : array, shape (n_samples,)
+        OPTICS ordered point indices (`ordering_`)
+
+    reachability : array, shape (n_samples,)
+        Reachability distances calculated by OPTICS (`reachability_`)
+
+    maxima_ratio : float, optional
+        The maximum ratio we allow of average height of clusters on the
+        right and left to the local maxima in question. The higher the
+        ratio, the more generous the algorithm is to preserving local
+        minima, and the more cuts the resulting tree will have.
+
+    rejection_ratio : float, optional
+        Adjusts the fitness of the clustering. When the maxima_ratio is
+        exceeded, determine which of the clusters to the left and right to
+        reject based on rejection_ratio. Higher values will result in points
+        being more readily classified as noise; conversely, lower values will
+        result in more points being clustered.
+
+    similarity_threshold : float, optional
+        Used to check if nodes can be moved up one level, that is, if the
+        new cluster created is too "similar" to its parent, given the
+        similarity threshold. Similarity can be determined by 1) the size
+        of the new cluster relative to the size of the parent node or
+        2) the average of the reachability values of the new cluster
+        relative to the average of the reachability values of the parent
+        node. A lower value for the similarity threshold means less levels
+        in the tree.
+
+    significant_min : float, optional
+        Sets a lower threshold on how small a significant maxima can be.
+
+    min_cluster_size_ratio : float, optional
+        Minimum percentage of dataset expected for cluster membership.
+
+    min_maxima_ratio : float, optional
+        Used to determine neighborhood size for minimum cluster membership.
+
+    Returns
+    -------
+    core_sample_indices_ : array, shape (n_core_samples,)
+        The indices of the core samples.
+
+    labels_ : array, shape (n_samples,)
+        The estimated labels.
+    """
+
+    # Extraction wrapper
+    reachability = reachability / np.max(reachability[1:])
+    reachability_plot = reachability[ordering].tolist()
+    root_node = _automatic_cluster(reachability_plot, ordering,
+                                   maxima_ratio, rejection_ratio,
+                                   similarity_threshold, significant_min,
+                                   min_cluster_size_ratio, min_maxima_ratio)
+    leaves = _get_leaves(root_node, [])
+    # Start cluster id's at 0
+    clustid = 0
+    n_samples = len(reachability)
+    is_core = np.zeros(n_samples, dtype=bool)
+    labels = np.full(n_samples, -1, dtype=int)
+    # Start all points as non-core noise
+    for leaf in leaves:
+        index = ordering[leaf.start:leaf.end]
+        labels[index] = clustid
+        is_core[index] = 1
+        clustid += 1
+    return np.arange(n_samples)[is_core], labels
+
+
+def _automatic_cluster(reachability_plot, ordering,
+                       maxima_ratio, rejection_ratio,
+                       similarity_threshold, significant_min,
+                       min_cluster_size_ratio, min_maxima_ratio):
+    """Converts reachability plot to cluster tree and returns root node.
+
+    Parameters
+    ----------
+
+    reachability_plot : list, required
+        Reachability distances ordered by OPTICS ordering index.
+
+    """
+
+    min_neighborhood_size = 2
+    min_cluster_size = int(min_cluster_size_ratio * len(ordering))
+    neighborhood_size = int(min_maxima_ratio * len(ordering))
+
+    # Should this check for < min_samples? Should this be public?
+    if min_cluster_size < 5:
+        min_cluster_size = 5
+
+    # Again, should this check < min_samples, should the parameter be public?
+    if neighborhood_size < min_neighborhood_size:
+        neighborhood_size = min_neighborhood_size
+
+    local_maxima_points = _find_local_maxima(reachability_plot,
+                                             neighborhood_size)
+    root_node = _TreeNode(ordering, 0, len(ordering), None)
+    _cluster_tree(root_node, None, local_maxima_points,
+                  reachability_plot, ordering, min_cluster_size,
+                  maxima_ratio, rejection_ratio,
+                  similarity_threshold, significant_min)
+
+    return root_node
+
+
+class _TreeNode(object):
+    # automatic cluster helper classes and functions
+    def __init__(self, points, start, end, parent_node):
+        self.points = points
+        self.start = start
+        self.end = end
+        self.parent_node = parent_node
+        self.children = []
+        self.split_point = -1
+
+    def assign_split_point(self, split_point):
+        self.split_point = split_point
+
+    def add_child(self, child):
+        self.children.append(child)
+
+
+def _is_local_maxima(index, reachability_plot, neighborhood_size):
+    right_idx = slice(index + 1, index + neighborhood_size + 1)
+    left_idx = slice(max(1, index - neighborhood_size - 1), index)
+    return (np.all(reachability_plot[index] >= reachability_plot[left_idx]) and
+            np.all(reachability_plot[index] >= reachability_plot[right_idx]))
+
+
+def _find_local_maxima(reachability_plot, neighborhood_size):
+    local_maxima_points = {}
+    # 1st and last points on Reachability Plot are not taken
+    # as local maxima points
+    for i in range(1, len(reachability_plot) - 1):
+        # if the point is a local maxima on the reachability plot with
+        # regard to neighborhood_size, insert it into priority queue and
+        # maxima list
+        if (reachability_plot[i] > reachability_plot[i - 1] and
+            reachability_plot[i] >= reachability_plot[i + 1] and
+            _is_local_maxima(i, np.array(reachability_plot),
+                             neighborhood_size) == 1):
+            local_maxima_points[i] = reachability_plot[i]
+
+    return sorted(local_maxima_points,
+                  key=local_maxima_points.__getitem__, reverse=True)
+
+
+def _cluster_tree(node, parent_node, local_maxima_points,
+                  reachability_plot, reachability_ordering,
+                  min_cluster_size, maxima_ratio, rejection_ratio,
+                  similarity_threshold, significant_min):
+    """Recursively builds cluster tree to hold hierarchical cluster structure
+
+    node is a node or the root of the tree in the first call
+    parent_node is parent node of N or None if node is root of the tree
+    local_maxima_points is list of local maxima points sorted in
+    descending order of reachability
+    """
+
+    if len(local_maxima_points) == 0:
+        return  # parent_node is a leaf
+
+    # take largest local maximum as possible separation between clusters
+    s = local_maxima_points[0]
+    node.assign_split_point(s)
+    local_maxima_points = local_maxima_points[1:]
+
+    # create two new nodes and add to list of nodes
+    node_1 = _TreeNode(reachability_ordering[node.start:s],
+                       node.start, s, node)
+    node_2 = _TreeNode(reachability_ordering[s + 1:node.end],
+                       s + 1, node.end, node)
+    local_max_1 = []
+    local_max_2 = []
+
+    for i in local_maxima_points:
+        if i < s:
+            local_max_1.append(i)
+        if i > s:
+            local_max_2.append(i)
+
+    node_list = []
+    node_list.append((node_1, local_max_1))
+    node_list.append((node_2, local_max_2))
+
+    if reachability_plot[s] < significant_min:
+        node.assign_split_point(-1)
+        # if split_point is not significant, ignore this split and continue
+        _cluster_tree(node, parent_node, local_maxima_points,
+                      reachability_plot, reachability_ordering,
+                      min_cluster_size, maxima_ratio, rejection_ratio,
+                      similarity_threshold, significant_min)
+        return
+
+    # only check a certain ratio of points in the child
+    # nodes formed to the left and right of the maxima
+    # ...should check_ratio be a user settable parameter?
+    check_ratio = .8
+    check_value_1 = int(np.round(check_ratio * len(node_1.points)))
+    check_value_2 = int(np.round(check_ratio * len(node_2.points)))
+    avg_reach1 = np.mean(reachability_plot[(node_1.end -
+                                            check_value_1):node_1.end])
+    avg_reach2 = np.mean(reachability_plot[node_2.start:(node_2.start
+                                                         + check_value_2)])
+
+    if ((avg_reach1 / reachability_plot[s]) > maxima_ratio or
+            (avg_reach2 / reachability_plot[s]) > maxima_ratio):
+
+        if (avg_reach1 / reachability_plot[s]) < rejection_ratio:
+            # reject node 2
+            node_list.remove((node_2, local_max_2))
+        if (avg_reach2 / reachability_plot[s]) < rejection_ratio:
+            # reject node 1
+            node_list.remove((node_1, local_max_1))
+        if ((avg_reach1 / reachability_plot[s]) >= rejection_ratio and
+                (avg_reach2 / reachability_plot[s]) >= rejection_ratio):
+            # since split_point is not significant,
+            # ignore this split and continue (reject both child nodes)
+            node.assign_split_point(-1)
+            _cluster_tree(node, parent_node, local_maxima_points,
+                          reachability_plot, reachability_ordering,
+                          min_cluster_size, maxima_ratio, rejection_ratio,
+                          similarity_threshold, significant_min)
+            return
+
+    # remove clusters that are too small
+    if (len(node_1.points) < min_cluster_size and
+            node_list.count((node_1, local_max_1)) > 0):
+        # cluster 1 is too small
+        node_list.remove((node_1, local_max_1))
+    if (len(node_2.points) < min_cluster_size and
+            node_list.count((node_2, local_max_2)) > 0):
+        # cluster 2 is too small
+        node_list.remove((node_2, local_max_2))
+    if not node_list:
+        # parent_node will be a leaf
+        node.assign_split_point(-1)
+        return
+
+    # Check if nodes can be moved up one level - the new cluster created
+    # is too "similar" to its parent, given the similarity threshold.
+    bypass_node = 0
+    if parent_node is not None:
+        if ((node.end - node.start) / (parent_node.end - parent_node.start) >
+                similarity_threshold):
+
+            parent_node.children.remove(node)
+            bypass_node = 1
+
+    for nl in node_list:
+        if bypass_node == 1:
+            parent_node.add_child(nl[0])
+            _cluster_tree(nl[0], parent_node, nl[1],
+                          reachability_plot, reachability_ordering,
+                          min_cluster_size, maxima_ratio, rejection_ratio,
+                          similarity_threshold, significant_min)
+        else:
+            node.add_child(nl[0])
+            _cluster_tree(nl[0], node, nl[1], reachability_plot,
+                          reachability_ordering, min_cluster_size,
+                          maxima_ratio, rejection_ratio,
+                          similarity_threshold, significant_min)
+
+
+def _get_leaves(node, arr):
+    if node is not None:
+        if node.split_point == -1:
+            arr.append(node)
+        for n in node.children:
+            _get_leaves(n, arr)
+    return arr
diff --git a/sklearn/cluster/setup.py b/sklearn/cluster/setup.py
index 99c4dcd6177b..5f3424eea8d5 100644
--- a/sklearn/cluster/setup.py
+++ b/sklearn/cluster/setup.py
@@ -23,6 +23,10 @@ def configuration(parent_package='', top_path=None):
                          sources=['_dbscan_inner.pyx'],
                          include_dirs=[numpy.get_include()],
                          language="c++")
+    config.add_extension('_optics_inner',
+                         sources=['_optics_inner.pyx'],
+                         include_dirs=[numpy.get_include()],
+                         libraries=libraries)
 
     config.add_extension('_hierarchical',
                          sources=['_hierarchical.pyx'],
diff --git a/sklearn/cluster/spectral.py b/sklearn/cluster/spectral.py
index 58ef1ba6c9d0..31a2046dbf0e 100644
--- a/sklearn/cluster/spectral.py
+++ b/sklearn/cluster/spectral.py
@@ -358,9 +358,11 @@ class SpectralClustering(BaseEstimator, ClusterMixin):
         Parameters (keyword arguments) and values for kernel passed as
         callable object. Ignored by other kernels.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -371,6 +373,23 @@ class SpectralClustering(BaseEstimator, ClusterMixin):
     labels_ :
         Labels of each point
 
+    Examples
+    --------
+    >>> from sklearn.cluster import SpectralClustering
+    >>> import numpy as np
+    >>> X = np.array([[1, 1], [2, 1], [1, 0],
+    ...               [4, 7], [3, 5], [3, 6]])
+    >>> clustering = SpectralClustering(n_clusters=2,
+    ...         assign_labels="discretize",
+    ...         random_state=0).fit(X)
+    >>> clustering.labels_
+    array([1, 1, 1, 0, 0, 0])
+    >>> clustering # doctest: +NORMALIZE_WHITESPACE
+    SpectralClustering(affinity='rbf', assign_labels='discretize', coef0=1,
+              degree=3, eigen_solver=None, eigen_tol=0.0, gamma=1.0,
+              kernel_params=None, n_clusters=2, n_init=10, n_jobs=None,
+              n_neighbors=10, random_state=0)
+
     Notes
     -----
     If you have an affinity matrix, such as a distance matrix,
@@ -409,7 +428,7 @@ class SpectralClustering(BaseEstimator, ClusterMixin):
     def __init__(self, n_clusters=8, eigen_solver=None, random_state=None,
                  n_init=10, gamma=1., affinity='rbf', n_neighbors=10,
                  eigen_tol=0.0, assign_labels='kmeans', degree=3, coef0=1,
-                 kernel_params=None, n_jobs=1):
+                 kernel_params=None, n_jobs=None):
         self.n_clusters = n_clusters
         self.eigen_solver = eigen_solver
         self.random_state = random_state
diff --git a/sklearn/cluster/tests/test_dbscan.py b/sklearn/cluster/tests/test_dbscan.py
index f2d6c5836db8..f25cc8d7310d 100644
--- a/sklearn/cluster/tests/test_dbscan.py
+++ b/sklearn/cluster/tests/test_dbscan.py
@@ -339,7 +339,7 @@ def test_dbscan_core_samples_toy(algorithm):
     core_samples, labels = dbscan(X, algorithm=algorithm, eps=1,
                                   min_samples=4)
     assert_array_equal(core_samples, [])
-    assert_array_equal(labels, -np.ones(n_samples))
+    assert_array_equal(labels, np.full(n_samples, -1.))
 
 
 def test_dbscan_precomputed_metric_with_degenerate_input_arrays():
diff --git a/sklearn/cluster/tests/test_feature_agglomeration.py b/sklearn/cluster/tests/test_feature_agglomeration.py
index 98d5dfc4b72c..5c992109ffab 100644
--- a/sklearn/cluster/tests/test_feature_agglomeration.py
+++ b/sklearn/cluster/tests/test_feature_agglomeration.py
@@ -4,7 +4,7 @@
 # Authors: Sergul Aydore 2017
 import numpy as np
 from sklearn.cluster import FeatureAgglomeration
-from sklearn.utils.testing import assert_true
+from sklearn.utils.testing import assert_true, assert_no_warnings
 from sklearn.utils.testing import assert_array_almost_equal
 
 
@@ -16,8 +16,8 @@ def test_feature_agglomeration():
                                       pooling_func=np.mean)
     agglo_median = FeatureAgglomeration(n_clusters=n_clusters,
                                         pooling_func=np.median)
-    agglo_mean.fit(X)
-    agglo_median.fit(X)
+    assert_no_warnings(agglo_mean.fit, X)
+    assert_no_warnings(agglo_median.fit, X)
     assert_true(np.size(np.unique(agglo_mean.labels_)) == n_clusters)
     assert_true(np.size(np.unique(agglo_median.labels_)) == n_clusters)
     assert_true(np.size(agglo_mean.labels_) == X.shape[1])
diff --git a/sklearn/cluster/tests/test_hierarchical.py b/sklearn/cluster/tests/test_hierarchical.py
index b3056b95d225..6f03f9aa3210 100644
--- a/sklearn/cluster/tests/test_hierarchical.py
+++ b/sklearn/cluster/tests/test_hierarchical.py
@@ -7,6 +7,7 @@
 # License: BSD 3 clause
 from tempfile import mkdtemp
 import shutil
+import pytest
 from functools import partial
 
 import numpy as np
@@ -142,6 +143,8 @@ def test_agglomerative_clustering_wrong_arg_memory():
     assert_raises(ValueError, clustering.fit, X)
 
 
+@pytest.mark.filterwarnings("ignore:the behavior of nmi will "
+                            "change in version 0.22")
 def test_agglomerative_clustering():
     # Check that we obtain the correct number of clusters with
     # agglomerative clustering.
@@ -250,6 +253,8 @@ def test_ward_agglomeration():
     assert_raises(ValueError, agglo.fit, X[:0])
 
 
+@pytest.mark.filterwarnings("ignore:the behavior of nmi will "
+                            "change in version 0.22")
 def test_single_linkage_clustering():
     # Check that we get the correct result in two emblematic cases
     moons, moon_labels = make_moons(noise=0.05, random_state=42)
@@ -311,6 +316,8 @@ def test_scikit_vs_scipy():
     assert_raises(ValueError, _hc_cut, n_leaves + 1, children, n_leaves)
 
 
+@pytest.mark.filterwarnings("ignore:the behavior of nmi will "
+                            "change in version 0.22")
 def test_identical_points():
     # Ensure identical points are handled correctly when using mst with
     # a sparse connectivity matrix
@@ -499,7 +506,7 @@ def test_int_float_dict():
         assert d[key] == value
 
     other_keys = np.arange(50).astype(np.intp)[::2]
-    other_values = 0.5 * np.ones(50)[::2]
+    other_values = np.full(50, 0.5)[::2]
     other = IntFloatDict(other_keys, other_values)
     # Complete smoke test
     max_merge(d, other, mask=np.ones(100, dtype=np.intp), n_a=1, n_b=1)
diff --git a/sklearn/cluster/tests/test_k_means.py b/sklearn/cluster/tests/test_k_means.py
index 485b120b54ac..7935e7134d24 100644
--- a/sklearn/cluster/tests/test_k_means.py
+++ b/sklearn/cluster/tests/test_k_means.py
@@ -68,7 +68,7 @@ def test_labels_assignment_and_inertia():
     # implementation
     rng = np.random.RandomState(42)
     noisy_centers = centers + rng.normal(size=centers.shape)
-    labels_gold = - np.ones(n_samples, dtype=np.int)
+    labels_gold = np.full(n_samples, -1, dtype=np.int)
     mindist = np.empty(n_samples)
     mindist.fill(np.infty)
     for center_id in range(n_clusters):
@@ -917,7 +917,8 @@ def _sort_centers(centers):
 def test_weighted_vs_repeated():
     # a sample weight of N should yield the same result as an N-fold
     # repetition of the sample
-    sample_weight = np.random.randint(1, 5, size=n_samples)
+    rng = np.random.RandomState(0)
+    sample_weight = rng.randint(1, 5, size=n_samples)
     X_repeat = np.repeat(X, sample_weight, axis=0)
     estimators = [KMeans(init="k-means++", n_clusters=n_clusters,
                          random_state=42),
diff --git a/sklearn/cluster/tests/test_optics.py b/sklearn/cluster/tests/test_optics.py
new file mode 100755
index 000000000000..597785083cfe
--- /dev/null
+++ b/sklearn/cluster/tests/test_optics.py
@@ -0,0 +1,408 @@
+# Authors: Shane Grigsby <refuge@rocktalus.com>
+#          Amy X. Zhang <axz@mit.edu>
+# License: BSD 3 clause
+
+import numpy as np
+import pytest
+
+from sklearn.datasets.samples_generator import make_blobs
+from sklearn.cluster.optics_ import OPTICS
+from sklearn.cluster.optics_ import _TreeNode, _cluster_tree
+from sklearn.cluster.optics_ import _find_local_maxima
+from sklearn.metrics.cluster import contingency_matrix
+from sklearn.cluster.dbscan_ import DBSCAN
+from sklearn.utils.testing import assert_equal, assert_warns
+from sklearn.utils.testing import assert_array_almost_equal
+from sklearn.utils.testing import assert_array_equal
+from sklearn.utils.testing import assert_raise_message
+
+from sklearn.cluster.tests.common import generate_clustered_data
+
+
+def test_correct_number_of_clusters():
+    # in 'auto' mode
+
+    n_clusters = 3
+    X = generate_clustered_data(n_clusters=n_clusters)
+    # Parameters chosen specifically for this task.
+    # Compute OPTICS
+    clust = OPTICS(max_bound=5.0 * 6.0, min_samples=4, metric='euclidean')
+    clust.fit(X)
+    # number of clusters, ignoring noise if present
+    n_clusters_1 = len(set(clust.labels_)) - int(-1 in clust.labels_)
+    assert_equal(n_clusters_1, n_clusters)
+
+
+def test_minimum_number_of_sample_check():
+    # test that we check a minimum number of samples
+    msg = ("Number of training samples (n_samples=1) must be greater than "
+           "min_samples (min_samples=10) used for clustering.")
+
+    # Compute OPTICS
+    X = [[1, 1]]
+    clust = OPTICS(max_bound=5.0 * 0.3, min_samples=10)
+
+    # Run the fit
+    assert_raise_message(ValueError, msg, clust.fit, X)
+
+
+def test_empty_extract():
+    # Test extract where fit() has not yet been run.
+    msg = ("This OPTICS instance is not fitted yet. Call 'fit' with "
+           "appropriate arguments before using this method.")
+    clust = OPTICS(max_bound=5.0 * 0.3, min_samples=10)
+    assert_raise_message(ValueError, msg, clust.extract_dbscan, 0.01)
+
+
+def test_bad_extract():
+    # Test an extraction of eps too close to original eps
+    msg = "Specify an epsilon smaller than 0.015. Got 0.3."
+    centers = [[1, 1], [-1, -1], [1, -1]]
+    X, labels_true = make_blobs(n_samples=750, centers=centers,
+                                cluster_std=0.4, random_state=0)
+
+    # Compute OPTICS
+    clust = OPTICS(max_bound=5.0 * 0.003, min_samples=10)
+    clust2 = clust.fit(X)
+    assert_raise_message(ValueError, msg, clust2.extract_dbscan, 0.3)
+
+
+def test_close_extract():
+    # Test extract where extraction eps is close to scaled epsPrime
+
+    centers = [[1, 1], [-1, -1], [1, -1]]
+    X, labels_true = make_blobs(n_samples=750, centers=centers,
+                                cluster_std=0.4, random_state=0)
+
+    # Compute OPTICS
+    clust = OPTICS(max_bound=1.0, min_samples=10)
+    clust3 = clust.fit(X)
+    # check warning when centers are passed
+    assert_warns(RuntimeWarning, clust3.extract_dbscan, .3)
+    # Cluster ordering starts at 0; max cluster label = 2 is 3 clusters
+    assert_equal(max(clust3.extract_dbscan(.3)[1]), 2)
+
+
+@pytest.mark.parametrize('eps', [0.1, .3, .5])
+@pytest.mark.parametrize('min_samples', [3, 10, 20])
+def test_dbscan_optics_parity(eps, min_samples):
+    # Test that OPTICS clustering labels are <= 5% difference of DBSCAN
+
+    centers = [[1, 1], [-1, -1], [1, -1]]
+    X, labels_true = make_blobs(n_samples=750, centers=centers,
+                                cluster_std=0.4, random_state=0)
+
+    # calculate optics with dbscan extract at 0.3 epsilon
+    op = OPTICS(min_samples=min_samples).fit(X)
+    core_optics, labels_optics = op.extract_dbscan(eps)
+
+    # calculate dbscan labels
+    db = DBSCAN(eps=eps, min_samples=min_samples).fit(X)
+
+    contingency = contingency_matrix(db.labels_, labels_optics)
+    agree = min(np.sum(np.max(contingency, axis=0)),
+                np.sum(np.max(contingency, axis=1)))
+    disagree = X.shape[0] - agree
+
+    # verify core_labels match
+    assert_array_equal(core_optics, db.core_sample_indices_)
+
+    non_core_count = len(labels_optics) - len(core_optics)
+    percent_mismatch = np.round((disagree - 1) / non_core_count, 2)
+
+    # verify label mismatch is <= 5% labels
+    assert percent_mismatch <= 0.05
+
+
+def test_auto_extract_hier():
+    # Tests auto extraction gets correct # of clusters with varying density
+
+    # Generate sample data
+    rng = np.random.RandomState(0)
+    n_points_per_cluster = 250
+
+    C1 = [-5, -2] + .8 * rng.randn(n_points_per_cluster, 2)
+    C2 = [4, -1] + .1 * rng.randn(n_points_per_cluster, 2)
+    C3 = [1, -2] + .2 * rng.randn(n_points_per_cluster, 2)
+    C4 = [-2, 3] + .3 * rng.randn(n_points_per_cluster, 2)
+    C5 = [3, -2] + 1.6 * rng.randn(n_points_per_cluster, 2)
+    C6 = [5, 6] + 2 * rng.randn(n_points_per_cluster, 2)
+    X = np.vstack((C1, C2, C3, C4, C5, C6))
+
+    # Compute OPTICS
+
+    clust = OPTICS(min_samples=9)
+
+    # Run the fit
+    clust.fit(X)
+
+    assert_equal(len(set(clust.labels_)), 6)
+
+
+@pytest.mark.parametrize("reach, n_child, members", [
+    (np.array([np.inf, 0.9, 0.9, 1.0, 0.89, 0.88, 10, .9, .9, .9, 10, 0.9,
+               0.9, 0.89, 0.88, 10, .9, .9, .9, .9]), 2, np.r_[0:6]),
+    (np.array([np.inf, 0.9, 0.9, 0.9, 0.89, 0.88, 10, .9, .9, .9, 10, 0.9,
+               0.9, 0.89, 0.88, 100, .9, .9, .9, .9]), 1, np.r_[0:15])])
+def test_cluster_sigmin_pruning(reach, n_child, members):
+    # Tests pruning left and right, insignificant splitpoints, empty nodelists
+    # Parameters chosen specifically for this task
+
+    # Case 1: Three pseudo clusters, 2 of which are too small
+    # Case 2: Two pseudo clusters, 1 of which are too small
+    # Normalize
+    reach = reach / np.max(reach[1:])
+
+    ordering = np.r_[0:20]
+    cluster_boundaries = _find_local_maxima(reach, 5)
+    root = _TreeNode(ordering, 0, 20, None)
+
+    # Build cluster tree inplace on root node
+    _cluster_tree(root, None, cluster_boundaries, reach, ordering,
+                  5, .75, .7, .4, .3)
+    assert_equal(root.split_point, cluster_boundaries[0])
+    assert_equal(n_child, len(root.children))
+    assert_array_equal(members, root.children[0].points)
+
+
+def test_reach_dists():
+    # Tests against known extraction array
+
+    rng = np.random.RandomState(0)
+    n_points_per_cluster = 250
+
+    C1 = [-5, -2] + .8 * rng.randn(n_points_per_cluster, 2)
+    C2 = [4, -1] + .1 * rng.randn(n_points_per_cluster, 2)
+    C3 = [1, -2] + .2 * rng.randn(n_points_per_cluster, 2)
+    C4 = [-2, 3] + .3 * rng.randn(n_points_per_cluster, 2)
+    C5 = [3, -2] + 1.6 * rng.randn(n_points_per_cluster, 2)
+    C6 = [5, 6] + 2 * rng.randn(n_points_per_cluster, 2)
+    X = np.vstack((C1, C2, C3, C4, C5, C6))
+
+    # Compute OPTICS
+
+    clust = OPTICS(min_samples=10, metric='minkowski')
+
+    # Run the fit
+    clust.fit(X)
+
+    # Expected values, matches 'RD' results from:
+    # http://chemometria.us.edu.pl/download/optics.py
+
+    v = [np.inf, 0.606005, 0.472013, 0.162951, 0.161000, 0.385547, 0.179715,
+         0.213507, 0.348468, 0.308146, 0.560519, 0.266072, 0.764384, 0.253164,
+         0.435716, 0.153696, 0.363924, 0.194267, 0.392313, 0.230589, 0.260023,
+         0.535348, 0.168173, 0.296736, 0.310583, 0.277204, 0.250654, 0.153696,
+         0.215533, 0.175710, 0.168173, 0.283134, 0.256372, 0.313931, 0.234164,
+         0.179715, 0.352957, 0.277052, 0.180986, 0.203819, 0.296022, 0.356691,
+         0.515438, 0.219208, 0.265821, 0.346630, 0.275305, 0.229332, 0.433715,
+         0.153696, 0.584960, 0.265821, 0.471049, 0.259154, 0.461707, 0.400021,
+         0.422748, 0.300699, 0.162951, 0.290504, 0.315199, 0.327130, 0.168864,
+         0.462826, 0.188862, 0.259784, 0.216788, 0.259784, 0.195673, 0.315199,
+         0.313931, 0.189128, 0.461707, 0.265821, 0.233594, 0.433715, 0.222260,
+         0.251734, 0.352957, 0.218134, 0.453792, 0.179715, 0.296736, 0.260023,
+         0.311162, 0.214549, 0.266072, 0.318744, 0.180986, 0.194267, 0.262882,
+         0.420186, 0.352957, 0.288388, 0.360962, 0.328054, 0.293849, 0.198271,
+         0.248772, 0.461707, 0.216788, 0.396450, 0.352957, 0.289448, 0.241311,
+         0.213742, 0.220516, 0.218134, 0.153696, 0.516090, 0.218134, 0.221507,
+         0.328647, 0.255933, 0.195766, 0.233594, 0.205270, 0.296736, 0.726008,
+         0.251991, 0.168173, 0.214027, 0.262882, 0.342089, 0.260023, 0.266072,
+         0.253164, 0.230345, 0.262882, 0.296022, 0.227047, 0.205974, 0.328647,
+         0.184315, 0.196304, 0.831185, 0.514116, 0.168173, 0.189784, 0.664306,
+         0.327130, 0.379139, 0.208932, 0.266140, 0.362751, 0.168173, 0.764384,
+         0.327130, 0.187107, 0.194267, 0.414196, 0.251734, 0.220516, 0.363924,
+         0.166886, 0.327130, 0.233594, 0.203819, 0.230589, 0.203819, 0.222972,
+         0.311526, 0.218134, 0.422748, 0.314870, 0.315199, 0.315199, 0.594179,
+         0.328647, 0.415638, 0.244046, 0.250654, 0.214027, 0.203819, 0.213507,
+         0.260023, 0.311442, 0.168173, 0.389432, 0.229343, 0.162951, 0.311162,
+         0.153696, 0.214027, 0.250654, 0.315199, 0.172484, 0.153696, 0.352957,
+         0.314870, 0.328647, 0.546505, 0.378118, 0.260023, 0.387830, 0.199714,
+         0.262882, 0.250654, 0.345254, 0.396450, 0.250654, 0.179715, 0.328647,
+         0.179715, 0.263104, 0.265821, 0.231714, 0.514116, 0.213507, 0.474255,
+         0.212568, 0.376760, 0.196304, 0.844945, 0.194267, 0.264914, 0.210320,
+         0.316374, 0.184315, 0.179715, 0.250654, 0.153696, 0.162951, 0.315199,
+         0.179965, 0.297876, 0.213507, 0.475420, 0.439372, 0.241311, 0.260927,
+         0.194267, 0.422748, 0.222260, 0.411940, 0.414733, 0.260923, 0.396450,
+         0.380672, 0.333277, 0.290504, 0.196014, 0.844945, 0.506989, 0.153696,
+         0.218134, 0.392313, 0.698970, 0.168173, 0.227047, 0.028856, 0.033243,
+         0.028506, 0.057003, 0.038335, 0.051183, 0.063923, 0.022363, 0.030677,
+         0.036155, 0.017748, 0.062887, 0.036041, 0.051183, 0.078198, 0.068936,
+         0.032418, 0.040634, 0.022188, 0.022112, 0.036858, 0.040199, 0.025549,
+         0.083975, 0.032209, 0.025525, 0.032952, 0.034727, 0.068887, 0.040634,
+         0.048985, 0.047450, 0.022422, 0.023767, 0.028092, 0.047450, 0.029202,
+         0.026105, 0.030542, 0.032250, 0.062887, 0.038335, 0.026753, 0.028092,
+         0.099391, 0.021430, 0.020496, 0.021430, 0.025043, 0.023868, 0.050069,
+         0.023868, 0.044140, 0.038032, 0.022112, 0.044140, 0.031528, 0.028092,
+         0.020065, 0.055926, 0.031508, 0.025549, 0.028062, 0.036155, 0.023694,
+         0.029423, 0.026105, 0.028497, 0.023868, 0.044808, 0.035783, 0.033090,
+         0.038779, 0.032146, 0.038421, 0.057328, 0.020065, 0.020065, 0.028858,
+         0.021337, 0.041226, 0.022507, 0.028506, 0.030257, 0.057912, 0.050876,
+         0.120109, 0.020065, 0.034727, 0.038596, 0.037008, 0.031609, 0.095640,
+         0.083728, 0.064906, 0.030677, 0.057003, 0.037008, 0.018705, 0.030677,
+         0.044140, 0.034727, 0.045226, 0.032146, 0.032418, 0.029332, 0.030104,
+         0.033243, 0.030104, 0.032209, 0.026405, 0.024092, 0.048441, 0.036379,
+         0.030745, 0.023454, 0.018705, 0.124248, 0.041114, 0.020700, 0.042633,
+         0.042455, 0.028497, 0.029202, 0.057859, 0.053157, 0.036155, 0.029534,
+         0.032209, 0.038032, 0.024617, 0.023071, 0.033090, 0.023694, 0.047277,
+         0.024617, 0.023868, 0.043916, 0.025549, 0.046198, 0.041086, 0.042003,
+         0.022507, 0.021430, 0.038779, 0.025043, 0.036379, 0.036326, 0.029421,
+         0.023454, 0.058683, 0.025549, 0.039904, 0.022507, 0.046198, 0.029332,
+         0.032209, 0.036155, 0.038421, 0.025043, 0.023694, 0.030104, 0.022363,
+         0.048544, 0.035180, 0.030677, 0.022112, 0.030677, 0.036678, 0.022507,
+         0.024092, 0.064231, 0.022507, 0.032209, 0.025043, 0.221152, 0.029840,
+         0.038779, 0.040634, 0.024617, 0.032418, 0.025525, 0.033298, 0.028092,
+         0.045754, 0.032209, 0.017748, 0.033090, 0.017748, 0.048931, 0.038689,
+         0.022112, 0.027129, 0.032952, 0.036858, 0.027704, 0.032146, 0.052191,
+         0.042633, 0.071638, 0.044140, 0.022507, 0.046647, 0.028270, 0.050525,
+         0.036772, 0.058995, 0.038335, 0.025185, 0.022507, 0.040293, 0.032418,
+         0.064308, 0.026023, 0.036155, 0.032418, 0.038032, 0.018705, 0.040293,
+         0.030104, 0.030845, 0.064906, 0.025525, 0.036155, 0.022507, 0.022363,
+         0.032418, 0.021430, 0.032209, 0.102770, 0.036960, 0.031062, 0.025043,
+         0.036155, 0.031609, 0.036379, 0.030845, 0.048985, 0.021848, 0.025549,
+         0.022507, 0.035783, 0.023698, 0.034422, 0.032418, 0.022507, 0.023868,
+         0.020065, 0.023694, 0.040634, 0.055633, 0.054549, 0.044662, 0.087660,
+         0.048066, 0.143571, 0.068669, 0.065049, 0.076927, 0.044359, 0.041577,
+         0.052364, 0.100317, 0.062146, 0.067578, 0.054549, 0.047239, 0.062809,
+         0.033917, 0.087660, 0.077113, 0.055633, 0.061854, 0.059756, 0.059537,
+         0.052364, 0.060347, 0.170251, 0.108492, 0.046370, 0.070684, 0.049589,
+         0.044662, 0.049013, 0.043303, 0.069573, 0.075044, 0.054354, 0.065072,
+         0.073135, 0.046126, 0.055569, 0.047239, 0.062146, 0.056093, 0.059986,
+         0.096182, 0.100317, 0.051649, 0.054354, 0.077420, 0.100317, 0.046370,
+         0.043303, 0.045845, 0.061422, 0.091580, 0.206234, 0.051405, 0.071684,
+         0.061574, 0.063666, 0.052692, 0.051649, 0.100124, 0.077909, 0.033917,
+         0.058680, 0.044359, 0.065498, 0.080214, 0.123231, 0.052957, 0.056582,
+         0.061540, 0.076794, 0.043303, 0.054884, 0.044359, 0.145249, 0.081741,
+         0.041577, 0.056093, 0.076799, 0.044359, 0.068483, 0.051649, 0.092275,
+         0.044359, 0.108492, 0.092275, 0.046126, 0.106422, 0.054354, 0.052957,
+         0.073329, 0.046126, 0.086402, 0.048194, 0.128569, 0.104042, 0.061854,
+         0.069573, 0.070035, 0.050346, 0.043303, 0.053576, 0.054549, 0.033917,
+         0.063666, 0.058680, 0.099130, 0.080198, 0.050118, 0.054549, 0.041577,
+         0.143571, 0.095965, 0.047643, 0.052364, 0.105168, 0.048685, 0.043303,
+         0.052814, 0.076927, 0.054549, 0.041577, 0.066657, 0.189930, 0.046370,
+         0.075044, 0.121331, 0.043303, 0.223897, 0.198621, 0.150328, 0.100317,
+         0.053576, 0.070708, 0.100898, 0.047239, 0.043613, 0.065049, 0.049146,
+         0.068669, 0.055569, 0.062124, 0.096408, 0.044662, 0.087660, 0.083012,
+         0.050118, 0.069573, 0.046126, 0.049146, 0.049146, 0.050808, 0.080198,
+         0.059986, 0.071974, 0.047239, 0.050808, 0.059986, 0.065850, 0.044863,
+         0.052814, 0.044359, 0.052364, 0.108492, 0.143571, 0.050926, 0.049146,
+         0.049146, 0.055569, 0.033917, 0.527659, 0.143547, 0.077113, 0.046126,
+         0.106422, 0.068669, 0.108492, 0.063666, 0.054549, 0.054884, 0.056907,
+         0.068669, 0.080198, 0.120887, 0.054549, 0.052692, 0.085801, 0.054884,
+         0.050808, 0.094595, 0.059545, 0.054354, 0.062124, 0.087660, 0.052814,
+         0.086715, 0.146253, 0.046370, 0.041577, 0.116083, 0.076927, 0.047239,
+         0.084375, 0.134652, 0.217969, 0.063559, 0.061540, 0.044662, 0.054354,
+         0.063666, 0.145466, 0.101700, 0.090491, 0.078536, 0.054884, 0.062124,
+         0.041577, 0.043303, 0.194473, 0.079780, 0.059704, 0.054780, 0.048194,
+         0.062146, 0.069573, 0.086898, 0.046675, 0.056258, 0.074141, 0.048066,
+         0.052957, 0.057982, 0.058966, 0.061048, 0.050885, 0.049146, 0.080993,
+         0.056093, 0.061854, 0.124025, 0.062146, 0.060906, 0.150328, 0.058680,
+         0.077420, 0.051800, 0.102359, 0.113301, 0.073096, 0.116715, 0.131476,
+         0.140601, 0.097667, 0.051800, 0.051800, 0.127964, 0.108870, 0.111926,
+         0.093532, 0.102390, 0.144266, 0.098271, 0.102541, 0.136497, 0.127964,
+         0.085569, 0.157863, 0.096739, 0.054008, 0.106219, 0.076838, 0.099076,
+         0.093532, 0.059861, 0.079975, 0.116715, 0.133625, 0.053641, 0.066110,
+         0.122302, 0.081313, 0.140601, 0.259889, 0.094437, 0.098271, 0.105776,
+         0.225742, 0.100097, 0.147592, 0.099076, 0.093128, 0.093532, 0.134946,
+         0.133625, 0.120869, 0.065932, 0.103395, 0.125172, 0.147842, 0.105278,
+         0.173584, 0.168241, 0.111524, 0.093532, 0.099076, 0.100426, 0.137132,
+         0.065356, 0.091108, 0.141202, 0.054008, 0.075298, 0.073717, 0.122817,
+         0.105278, 0.094437, 0.067080, 0.108530, 0.115467, 0.093532, 0.085569,
+         0.145180, 0.100426, 0.116715, 0.151726, 0.073096, 0.193781, 0.090614,
+         0.081162, 0.051800, 0.133625, 0.136497, 0.100670, 0.081313, 0.506893,
+         0.084567, 0.108530, 0.087353, 0.063184, 0.123639, 0.168333, 0.314422,
+         0.091108, 0.079975, 0.091108, 0.136497, 0.122302, 0.167297, 0.067080,
+         0.144266, 0.065932, 0.087667, 0.100426, 0.099460, 0.091108, 0.100637,
+         0.116715, 0.079975, 0.077977, 0.090340, 0.136723, 1.943026, 0.108870,
+         0.090340, 0.065932, 0.102245, 0.157863, 0.157863, 0.215574, 0.156830,
+         0.093532, 0.122302, 0.097667, 0.063000, 0.116715, 0.076838, 0.148372,
+         0.093532, 0.099076, 0.141202, 0.096505, 0.096739, 0.091108, 0.099076,
+         0.079975, 0.108870, 0.102390, 0.079975, 0.244121, 0.167071, 0.096739,
+         0.102390, 0.103395, 0.073096, 0.094887, 0.065932, 0.190667, 0.099460,
+         0.102390, 0.096739, 0.102390, 0.116715, 0.100637, 0.256554, 0.103395,
+         0.081313, 0.068962, 0.109645, 0.059364, 0.147842, 0.099460, 0.079262,
+         0.099460, 0.065932, 0.123687, 0.090614, 0.131352, 0.098271, 0.102541,
+         0.098983, 0.057224, 0.074797, 0.057224, 0.250559, 0.079975, 0.103395,
+         0.100426, 0.065932, 0.120661, 0.079262, 0.065932, 0.118665, 0.081162,
+         0.066283, 0.099076, 0.102359, 0.108530, 0.079975, 0.168333, 0.096739,
+         0.168333, 0.097008, 0.055288, 0.172411, 0.092801, 0.051800, 0.102541,
+         0.084567, 0.054008, 0.090991, 0.172411, 0.057224, 0.148396, 0.200965,
+         0.076838, 0.157863, 0.053535, 0.121919, 0.126609, 0.123890, 0.118081,
+         0.097008, 0.125311, 0.099460, 0.122302, 0.134946, 0.080975, 0.084567,
+         0.110093, 0.102245, 0.103395, 0.171601, 0.094887, 0.126240, 0.137742,
+         0.099954, 0.108530, 0.157863, 0.096739, 0.051800, 0.127964, 0.066110,
+         0.061021, 0.105147, 0.100426, 0.079975, 0.088187, 0.116421, 0.076838,
+         0.098271, 0.116715, 0.137656, 0.075298, 0.148396, 0.112166, 1.083905,
+         0.326598, 0.428987, 0.395963, 0.224541, 0.326598, 0.030677, 0.410454,
+         0.122771, 1.140305, 0.641074, 0.432159, 0.429335, 0.422908, 0.461926,
+         0.293083, 0.477078, 0.714856, 0.515861, 0.405418, 0.054354, 0.341177,
+         0.410008, 0.514245, 0.641074, 0.816459, 0.455115, 0.400707, 0.382240,
+         0.431832, 1.618970, 0.683953, 0.182992, 0.763699, 0.515861, 0.717145,
+         0.409629, 0.074134, 0.398273, 0.864974, 0.400707, 0.591403, 0.435354,
+         0.514245, 1.337152, 0.841077, 0.410008, 0.683953, 0.338649, 0.557595,
+         0.442092, 0.326598, 0.984189, 0.429608, 0.395963, 1.152055, 0.587222,
+         1.748492, 0.477078, 0.395459, 0.717145, 0.575811, 0.210115, 0.487785,
+         0.431832, 0.383852, 0.806708, 0.428987, 0.278405, 0.395963, 0.395459,
+         0.383852, 1.083905, 0.428510, 0.326598, 0.108492, 0.541644, 0.612110,
+         0.382240, 0.833511, 0.382240, 0.456628, 0.326598, 0.458880, 0.398273,
+         0.957748, 0.326598, 0.295049, 0.629646, 0.429765, 0.439942, 0.633617,
+         0.566297, 0.429335, 0.086507, 0.477078, 0.526753, 0.375240, 0.584436,
+         0.355776, 0.395963, 0.644924, 0.129793, 0.484880, 0.470001, 0.572306,
+         0.383852, 1.110081, 0.841077, 0.395963, 0.683953, 0.428745, 0.387752,
+         0.545299, 0.686537, 0.635219, 0.840499, 0.527659, 0.400707, 0.480982,
+         0.541644, 0.714856, 0.942673, 0.398273, 0.428987, 0.356781, 0.428510,
+         1.140961, 0.395963, 0.356781, 0.410454, 0.541644, 0.641074, 0.484778,
+         0.410008, 0.433108, 0.278405, 0.278405, 0.503141, 0.428745, 0.125103,
+         0.633617, 0.410454, 0.124025, 0.461926, 0.398273, 0.410008, 1.181303,
+         0.635219, 0.593537, 0.395963, 0.717145, 0.409629, 0.492595, 0.806708,
+         0.503820, 0.423834, 0.557595, 0.429335, 0.470749, 0.461926, 1.890036,
+         0.236343, 0.806708, 0.123561, 0.433744, 0.427348, 0.427348, 0.962234,
+         0.395963, 0.409629, 0.527659, 0.425727, 0.602549, 0.901331, 0.326598,
+         0.635949, 0.541644, 0.375240, 0.598969, 1.140961, 0.391998, 0.719443,
+         0.410008, 0.515861, 0.714856, 0.842273, 0.410454, 0.389377, 0.431078,
+         0.515861, 0.515861, 0.429335, 0.332495, 0.398273, 0.428987, 0.635219,
+         0.387752, 0.384289, 0.383852, 0.430504, 0.428510, 0.431832, 0.375240,
+         0.278405, 0.374102, 0.428745, 0.692878, 1.152055, 0.503820, 0.428745,
+         0.352868, 0.429335, 0.375240, 0.400707, 0.427348, 0.256183, 0.962234,
+         0.505376, 0.058995, 0.410454, 0.172880, 0.395963, 0.470749, 0.356781,
+         1.332700, 0.683953, 0.395963, 0.806708, 0.400707, 0.330982, 0.427731,
+         0.934845, 0.375240, 0.191534, 0.047239, 1.083905, 0.348794, 0.409708,
+         0.503820, 0.557595, 0.429335, 0.498780, 0.293083, 0.363069, 0.442092,
+         1.152055, 0.375240, 0.335677, 0.452443, 0.655156, 0.929928, 0.614869,
+         1.411031, 1.101132, 0.469030, 0.404976, 0.538209, 0.655828, 0.674748,
+         0.365182, 0.641612, 0.555434, 0.521651, 0.386679, 0.386679, 0.980304,
+         0.659111, 0.651366, 0.538209, 0.521651, 0.884780, 1.287829, 0.558322,
+         0.446161, 0.817970, 0.568499, 0.533507, 0.639746, 0.484404, 0.591751,
+         0.913016, 0.446161, 0.533907, 0.606885, 0.672320, 1.150642, 0.655828,
+         0.365182, 0.665088, 1.094242, 0.629401, 0.540676, 0.733026, 1.248265,
+         1.273499, 0.867854, 0.538656, 0.386679, 0.922273, 0.515686, 1.321022,
+         0.624444, 0.655828, 0.922273, 0.386679, 0.762191, 0.779432, 0.601851,
+         0.655156, 0.926213, 0.762191, 0.641612, 0.558322, 1.025370, 0.641067,
+         0.651366, 0.633434, 0.459580, 0.859221, 0.552291, 0.591751, 0.819965,
+         0.669977, 1.185083, 0.499338, 0.533907, 0.752871, 0.571388, 0.539772,
+         0.449182, 1.025370, 0.365182, 1.321022, 0.926213, 0.886360, 0.562272,
+         0.669977, 0.796046, 0.557598, 0.596776, 0.672336, 0.659111, 0.453719,
+         0.477716, 0.477716, 1.592069, 0.591751, 0.539772, 0.641612, 0.946254,
+         0.744165, 0.386679, 0.593825, 0.539772, 0.449182, 0.604273, 0.794951,
+         0.752871, 0.539772, 0.648732, 0.469030, 0.665088, 1.332700, 1.341388,
+         0.533507, 0.544212, 1.025992, 0.645967, 0.612945, 0.868492, 0.648732,
+         0.752300, 0.624444, 1.219748, 0.446161, 0.520818, 0.469044, 0.669977,
+         0.926213, 0.638752, 0.762191, 0.922273, 0.794951, 0.606885, 0.669977,
+         0.550113, 0.641067, 0.733026, 0.604273, 0.648732, 0.533507, 0.746506,
+         0.733026, 0.980683, 0.538209, 0.669977, 0.469030, 0.648732, 0.609190,
+         1.219748, 0.373113, 0.539772, 1.744047, 1.004716, 0.926213, 0.562272,
+         0.752871, 0.538656, 0.449182, 0.365182, 0.469030, 0.446161, 0.484404,
+         0.768592, 0.648732, 0.655156, 0.521651, 0.779432, 0.446161, 0.596776,
+         0.538209, 0.726740, 0.539772, 0.469030, 0.521651, 0.561950, 0.601851,
+         0.533907, 0.922273, 1.248265, 0.476800, 0.737990, 0.817970, 0.792127,
+         0.533907, 0.486038, 0.624444, 0.798241, 0.476800, 1.059373, 0.645967,
+         0.619940, 0.528726, 0.669977, 0.865406, 0.980683, 0.980683, 0.834671,
+         1.001353, 0.752871, 0.449182, 1.096520, 0.449182, 0.593825, 0.636558,
+         0.762191, 0.638591, 0.538209, 0.865406, 0.779432, 0.469044, 0.645967,
+         0.557598, 0.499338, 0.484404, 0.515686, 0.794951, 0.619456, 0.733026,
+         0.821769, 0.752300, 0.643302, 0.636558, 0.655156, 0.655156, 0.484404,
+         0.648732, 0.726023, 0.365182, 0.606885, 0.499338, 0.520818, 0.612945,
+         0.446161, 0.557598, 0.469044, 1.134650, 0.629401, 0.538656, 0.561950,
+         1.364861, 0.459580, 1.025370, 0.980304, 0.607592, 0.533907, 1.134650,
+         0.446161, 0.629962]
+
+    assert_array_almost_equal(clust.reachability_, np.array(v))
diff --git a/sklearn/compose/_column_transformer.py b/sklearn/compose/_column_transformer.py
index aeedd4e3eaf4..e09d2d09d7e4 100644
--- a/sklearn/compose/_column_transformer.py
+++ b/sklearn/compose/_column_transformer.py
@@ -6,13 +6,15 @@
 # Author: Andreas Mueller
 #         Joris Van den Bossche
 # License: BSD
+from __future__ import division
+
 from itertools import chain
 
 import numpy as np
 from scipy import sparse
 
 from ..base import clone, TransformerMixin
-from ..externals.joblib import Parallel, delayed
+from ..utils import Parallel, delayed
 from ..externals import six
 from ..pipeline import (
     _fit_one_transformer, _fit_transform_one, _transform_one, _name_estimators)
@@ -61,28 +63,41 @@ class ColumnTransformer(_BaseComposition, TransformerMixin):
             strings 'drop' and 'passthrough' are accepted as well, to
             indicate to drop the columns or to pass them through untransformed,
             respectively.
-        column(s) : string or int, array-like of string or int, slice or \
-boolean mask array
+        column(s) : string or int, array-like of string or int, slice, \
+boolean mask array or callable
             Indexes the data on its second axis. Integers are interpreted as
             positional columns, while strings can reference DataFrame columns
             by name.  A scalar string or int should be used where
             ``transformer`` expects X to be a 1d array-like (vector),
             otherwise a 2d array will be passed to the transformer.
-
-    remainder : {'passthrough', 'drop'} or estimator, default 'passthrough'
-        By default, all remaining columns that were not specified in
-        `transformers` will be automatically passed through (default of
-        ``'passthrough'``). This subset of columns is concatenated with the
-        output of the transformers.
-        By using ``remainder='drop'``, only the specified columns in
-        `transformers` are transformed and combined in the output, and the
-        non-specified columns are dropped.
+            A callable is passed the input data `X` and can return any of the
+            above.
+
+    remainder : {'drop', 'passthrough'} or estimator, default 'drop'
+        By default, only the specified columns in `transformers` are
+        transformed and combined in the output, and the non-specified
+        columns are dropped. (default of ``'drop'``).
+        By specifying ``remainder='passthrough'``, all remaining columns that
+        were not specified in `transformers` will be automatically passed
+        through. This subset of columns is concatenated with the output of
+        the transformers.
         By setting ``remainder`` to be an estimator, the remaining
         non-specified columns will use the ``remainder`` estimator. The
         estimator must support `fit` and `transform`.
 
-    n_jobs : int, optional
-        Number of jobs to run in parallel (default 1).
+    sparse_threshold : float, default = 0.3
+        If the transformed output consists of a mix of sparse and dense data,
+        it will be stacked as a sparse matrix if the density is lower than this
+        value. Use ``sparse_threshold=0`` to always return dense.
+        When the transformed output consists of all sparse or all dense data,
+        the stacked result will be sparse or dense, respectively, and this
+        keyword will be ignored.
+
+    n_jobs : int or None, optional (default=None)
+        Number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     transformer_weights : dict, optional
         Multiplicative weights for features per transformer. The output of the
@@ -106,6 +121,11 @@ class ColumnTransformer(_BaseComposition, TransformerMixin):
         Keys are transformer names and values are the fitted transformer
         objects.
 
+    sparse_output_ : boolean
+        Boolean flag indicating wether the output of ``transform`` is a
+        sparse matrix or a dense numpy array, which depends on the output
+        of the individual transformers and the `sparse_threshold` keyword.
+
     Notes
     -----
     The order of the columns in the transformed feature matrix follows the
@@ -139,10 +159,11 @@ class ColumnTransformer(_BaseComposition, TransformerMixin):
 
     """
 
-    def __init__(self, transformers, remainder='passthrough', n_jobs=1,
-                 transformer_weights=None):
+    def __init__(self, transformers, remainder='drop', sparse_threshold=0.3,
+                 n_jobs=None, transformer_weights=None):
         self.transformers = transformers
         self.remainder = remainder
+        self.sparse_threshold = sparse_threshold
         self.n_jobs = n_jobs
         self.transformer_weights = transformer_weights
 
@@ -372,12 +393,9 @@ def fit(self, X, y=None):
             This estimator
 
         """
-        self._validate_remainder(X)
-        self._validate_transformers()
-
-        transformers = self._fit_transform(X, y, _fit_one_transformer)
-        self._update_fitted_transformers(transformers)
-
+        # we use fit_transform to make sure to set sparse_output_ (for which we
+        # need the transformed data) to have consistent output type in predict
+        self.fit_transform(X, y=y)
         return self
 
     def fit_transform(self, X, y=None):
@@ -407,15 +425,28 @@ def fit_transform(self, X, y=None):
         result = self._fit_transform(X, y, _fit_transform_one)
 
         if not result:
+            self._update_fitted_transformers([])
             # All transformers are None
             return np.zeros((X.shape[0], 0))
 
         Xs, transformers = zip(*result)
 
+        # determine if concatenated output will be sparse or not
+        if all(sparse.issparse(X) for X in Xs):
+            self.sparse_output_ = True
+        elif any(sparse.issparse(X) for X in Xs):
+            nnz = sum(X.nnz if sparse.issparse(X) else X.size for X in Xs)
+            total = sum(X.shape[0] * X.shape[1] if sparse.issparse(X)
+                        else X.size for X in Xs)
+            density = nnz / total
+            self.sparse_output_ = density < self.sparse_threshold
+        else:
+            self.sparse_output_ = False
+
         self._update_fitted_transformers(transformers)
         self._validate_output(Xs)
 
-        return _hstack(list(Xs))
+        return self._hstack(list(Xs))
 
     def transform(self, X):
         """Transform X separately by each transformer, concatenate results.
@@ -443,7 +474,23 @@ def transform(self, X):
             # All transformers are None
             return np.zeros((X.shape[0], 0))
 
-        return _hstack(list(Xs))
+        return self._hstack(list(Xs))
+
+    def _hstack(self, Xs):
+        """Stacks Xs horizontally.
+
+        This allows subclasses to control the stacking behavior, while reusing
+        everything else from ColumnTransformer.
+
+        Parameters
+        ----------
+        Xs : List of numpy arrays, sparse arrays, or DataFrames
+        """
+        if self.sparse_output_:
+            return sparse.hstack(Xs).tocsr()
+        else:
+            Xs = [f.toarray() if sparse.issparse(f) else f for f in Xs]
+            return np.hstack(Xs)
 
 
 def _check_key_type(key, superclass):
@@ -477,19 +524,6 @@ def _check_key_type(key, superclass):
     return False
 
 
-def _hstack(X):
-    """
-    Stacks X horizontally.
-
-    Supports input types (X): list of
-        numpy arrays, sparse arrays and DataFrames
-    """
-    if any(sparse.issparse(f) for f in X):
-        return sparse.hstack(X).tocsr()
-    else:
-        return np.hstack(X)
-
-
 def _get_column(X, key):
     """
     Get feature column(s) from input data X.
@@ -499,6 +533,7 @@ def _get_column(X, key):
     Supported key types (key):
     - scalar: output is 1D
     - lists, slices, boolean masks: output is 2D
+    - callable that returns any of the above
 
     Supported key data types:
 
@@ -510,6 +545,9 @@ def _get_column(X, key):
           can use any hashable object as key).
 
     """
+    if callable(key):
+        key = key(X)
+
     # check whether we have string column names or integers
     if _check_key_type(key, int):
         column_names = False
@@ -551,6 +589,9 @@ def _get_column_indices(X, key):
     """
     n_columns = X.shape[1]
 
+    if callable(key):
+        key = key(X)
+
     if _check_key_type(key, int):
         if isinstance(key, int):
             return [key]
@@ -616,20 +657,23 @@ def make_column_transformer(*transformers, **kwargs):
     ----------
     *transformers : tuples of column selections and transformers
 
-    remainder : {'passthrough', 'drop'} or estimator, default 'passthrough'
-        By default, all remaining columns that were not specified in
-        `transformers` will be automatically passed through (default of
-        ``'passthrough'``). This subset of columns is concatenated with the
-        output of the transformers.
-        By using ``remainder='drop'``, only the specified columns in
-        `transformers` are transformed and combined in the output, and the
-        non-specified columns are dropped.
+    remainder : {'drop', 'passthrough'} or estimator, default 'drop'
+        By default, only the specified columns in `transformers` are
+        transformed and combined in the output, and the non-specified
+        columns are dropped. (default of ``'drop'``).
+        By specifying ``remainder='passthrough'``, all remaining columns that
+        were not specified in `transformers` will be automatically passed
+        through. This subset of columns is concatenated with the output of
+        the transformers.
         By setting ``remainder`` to be an estimator, the remaining
         non-specified columns will use the ``remainder`` estimator. The
         estimator must support `fit` and `transform`.
 
-    n_jobs : int, optional
-        Number of jobs to run in parallel (default 1).
+    n_jobs : int or None, optional (default=None)
+        Number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Returns
     -------
@@ -649,7 +693,7 @@ def make_column_transformer(*transformers, **kwargs):
     ...     (['numerical_column'], StandardScaler()),
     ...     (['categorical_column'], OneHotEncoder()))
     ...     # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
-    ColumnTransformer(n_jobs=1, remainder='passthrough',
+    ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,
              transformer_weights=None,
              transformers=[('standardscaler',
                             StandardScaler(...),
@@ -659,8 +703,8 @@ def make_column_transformer(*transformers, **kwargs):
                             ['categorical_column'])])
 
     """
-    n_jobs = kwargs.pop('n_jobs', 1)
-    remainder = kwargs.pop('remainder', 'passthrough')
+    n_jobs = kwargs.pop('n_jobs', None)
+    remainder = kwargs.pop('remainder', 'drop')
     if kwargs:
         raise TypeError('Unknown keyword arguments: "{}"'
                         .format(list(kwargs.keys())[0]))
diff --git a/sklearn/compose/tests/test_column_transformer.py b/sklearn/compose/tests/test_column_transformer.py
index f1f7f9a24474..f67806a52c54 100644
--- a/sklearn/compose/tests/test_column_transformer.py
+++ b/sklearn/compose/tests/test_column_transformer.py
@@ -19,7 +19,7 @@
 from sklearn.externals import six
 from sklearn.compose import ColumnTransformer, make_column_transformer
 from sklearn.exceptions import NotFittedError
-from sklearn.preprocessing import StandardScaler, Normalizer
+from sklearn.preprocessing import StandardScaler, Normalizer, OneHotEncoder
 from sklearn.feature_extraction import DictVectorizer
 
 
@@ -99,6 +99,12 @@ def test_column_transformer():
         assert_array_equal(ct.fit_transform(X_array), res)
         assert_array_equal(ct.fit(X_array).transform(X_array), res)
 
+        # callable that returns any of the allowed specifiers
+        ct = ColumnTransformer([('trans', Trans(), lambda x: selection)],
+                               remainder='drop')
+        assert_array_equal(ct.fit_transform(X_array), res)
+        assert_array_equal(ct.fit(X_array).transform(X_array), res)
+
     ct = ColumnTransformer([('trans1', Trans(), [0]),
                             ('trans2', Trans(), [1])])
     assert_array_equal(ct.fit_transform(X_array), X_res_both)
@@ -166,6 +172,12 @@ def test_column_transformer_dataframe():
         assert_array_equal(ct.fit_transform(X_df), res)
         assert_array_equal(ct.fit(X_df).transform(X_df), res)
 
+        # callable that returns any of the allowed specifiers
+        ct = ColumnTransformer([('trans', Trans(), lambda X: selection)],
+                               remainder='drop')
+        assert_array_equal(ct.fit_transform(X_df), res)
+        assert_array_equal(ct.fit(X_df).transform(X_df), res)
+
     ct = ColumnTransformer([('trans1', Trans(), ['first']),
                             ('trans2', Trans(), ['second'])])
     assert_array_equal(ct.fit_transform(X_df), X_res_both)
@@ -250,14 +262,16 @@ def test_column_transformer_sparse_array():
         for remainder, res in [('drop', X_res_first),
                                ('passthrough', X_res_both)]:
             ct = ColumnTransformer([('trans', Trans(), col)],
-                                   remainder=remainder)
+                                   remainder=remainder,
+                                   sparse_threshold=0.8)
             assert_true(sparse.issparse(ct.fit_transform(X_sparse)))
             assert_allclose_dense_sparse(ct.fit_transform(X_sparse), res)
             assert_allclose_dense_sparse(ct.fit(X_sparse).transform(X_sparse),
                                          res)
 
     for col in [[0, 1], slice(0, 2)]:
-        ct = ColumnTransformer([('trans', Trans(), col)])
+        ct = ColumnTransformer([('trans', Trans(), col)],
+                               sparse_threshold=0.8)
         assert_true(sparse.issparse(ct.fit_transform(X_sparse)))
         assert_allclose_dense_sparse(ct.fit_transform(X_sparse), X_res_both)
         assert_allclose_dense_sparse(ct.fit(X_sparse).transform(X_sparse),
@@ -267,7 +281,8 @@ def test_column_transformer_sparse_array():
 def test_column_transformer_sparse_stacking():
     X_array = np.array([[0, 1, 2], [2, 4, 6]]).T
     col_trans = ColumnTransformer([('trans1', Trans(), [0]),
-                                   ('trans2', SparseMatrixTrans(), 1)])
+                                   ('trans2', SparseMatrixTrans(), 1)],
+                                  sparse_threshold=0.8)
     col_trans.fit(X_array)
     X_trans = col_trans.transform(X_array)
     assert_true(sparse.issparse(X_trans))
@@ -276,6 +291,57 @@ def test_column_transformer_sparse_stacking():
     assert len(col_trans.transformers_) == 2
     assert col_trans.transformers_[-1][0] != 'remainder'
 
+    col_trans = ColumnTransformer([('trans1', Trans(), [0]),
+                                   ('trans2', SparseMatrixTrans(), 1)],
+                                  sparse_threshold=0.1)
+    col_trans.fit(X_array)
+    X_trans = col_trans.transform(X_array)
+    assert not sparse.issparse(X_trans)
+    assert X_trans.shape == (X_trans.shape[0], X_trans.shape[0] + 1)
+    assert_array_equal(X_trans[:, 1:], np.eye(X_trans.shape[0]))
+
+
+def test_column_transformer_sparse_threshold():
+    X_array = np.array([['a', 'b'], ['A', 'B']], dtype=object).T
+    # above data has sparsity of 4 / 8 = 0.5
+
+    # if all sparse, keep sparse (even if above threshold)
+    col_trans = ColumnTransformer([('trans1', OneHotEncoder(), [0]),
+                                   ('trans2', OneHotEncoder(), [1])],
+                                  sparse_threshold=0.2)
+    res = col_trans.fit_transform(X_array)
+    assert sparse.issparse(res)
+    assert col_trans.sparse_output_
+
+    # mixed -> sparsity of (4 + 2) / 8 = 0.75
+    for thres in [0.75001, 1]:
+        col_trans = ColumnTransformer(
+            [('trans1', OneHotEncoder(sparse=True), [0]),
+             ('trans2', OneHotEncoder(sparse=False), [1])],
+            sparse_threshold=thres)
+        res = col_trans.fit_transform(X_array)
+        assert sparse.issparse(res)
+        assert col_trans.sparse_output_
+
+    for thres in [0.75, 0]:
+        col_trans = ColumnTransformer(
+            [('trans1', OneHotEncoder(sparse=True), [0]),
+             ('trans2', OneHotEncoder(sparse=False), [1])],
+            sparse_threshold=thres)
+        res = col_trans.fit_transform(X_array)
+        assert not sparse.issparse(res)
+        assert not col_trans.sparse_output_
+
+    # if nothing is sparse -> no sparse
+    for thres in [0.33, 0, 1]:
+        col_trans = ColumnTransformer(
+            [('trans1', OneHotEncoder(sparse=False), [0]),
+             ('trans2', OneHotEncoder(sparse=False), [1])],
+            sparse_threshold=thres)
+        res = col_trans.fit_transform(X_array)
+        assert not sparse.issparse(res)
+        assert not col_trans.sparse_output_
+
 
 def test_column_transformer_error_msg_1D():
     X_array = np.array([[0., 1., 2.], [2., 4., 6.]]).T
@@ -299,9 +365,9 @@ def test_2D_transformer_output():
                             ('trans2', TransNo2D(), 1)])
     assert_raise_message(ValueError, "the 'trans2' transformer should be 2D",
                          ct.fit_transform, X_array)
-    ct.fit(X_array)
+    # because fit is also doing transform, this raises already on fit
     assert_raise_message(ValueError, "the 'trans2' transformer should be 2D",
-                         ct.transform, X_array)
+                         ct.fit, X_array)
 
 
 def test_2D_transformer_output_pandas():
@@ -314,9 +380,9 @@ def test_2D_transformer_output_pandas():
     ct = ColumnTransformer([('trans1', TransNo2D(), 'col1')])
     assert_raise_message(ValueError, "the 'trans1' transformer should be 2D",
                          ct.fit_transform, X_df)
-    ct.fit(X_df)
+    # because fit is also doing transform, this raises already on fit
     assert_raise_message(ValueError, "the 'trans1' transformer should be 2D",
-                         ct.transform, X_df)
+                         ct.fit, X_df)
 
 
 @pytest.mark.parametrize("remainder", ['drop', 'passthrough'])
@@ -392,8 +458,9 @@ def test_column_transformer_get_set_params():
     ct = ColumnTransformer([('trans1', StandardScaler(), [0]),
                             ('trans2', StandardScaler(), [1])])
 
-    exp = {'n_jobs': 1,
-           'remainder': 'passthrough',
+    exp = {'n_jobs': None,
+           'remainder': 'drop',
+           'sparse_threshold': 0.3,
            'trans1': ct.transformers[0][1],
            'trans1__copy': True,
            'trans1__with_mean': True,
@@ -411,8 +478,9 @@ def test_column_transformer_get_set_params():
     assert_false(ct.get_params()['trans1__with_mean'])
 
     ct.set_params(trans1='passthrough')
-    exp = {'n_jobs': 1,
-           'remainder': 'passthrough',
+    exp = {'n_jobs': None,
+           'remainder': 'drop',
+           'sparse_threshold': 0.3,
            'trans1': 'passthrough',
            'trans2': ct.transformers[1][1],
            'trans2__copy': True,
@@ -480,7 +548,8 @@ def test_column_transformer_get_feature_names():
         NotImplementedError, 'get_feature_names is not yet supported',
         ct.get_feature_names)
 
-    ct = ColumnTransformer([('trans', DictVectorizer(), 0)])
+    ct = ColumnTransformer([('trans', DictVectorizer(), 0)],
+                           remainder='passthrough')
     ct.fit(X)
     assert_raise_message(
         NotImplementedError, 'get_feature_names is not yet supported',
@@ -540,23 +609,22 @@ def test_column_transformer_remainder():
     X_res_second = np.array([2, 4, 6]).reshape(-1, 1)
     X_res_both = X_array
 
-    # default passthrough
-    ct = ColumnTransformer([('trans', Trans(), [0])])
-    assert_array_equal(ct.fit_transform(X_array), X_res_both)
-    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)
+    # default drop
+    ct = ColumnTransformer([('trans1', Trans(), [0])])
+    assert_array_equal(ct.fit_transform(X_array), X_res_first)
+    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_first)
     assert len(ct.transformers_) == 2
     assert ct.transformers_[-1][0] == 'remainder'
-    assert ct.transformers_[-1][1] == 'passthrough'
+    assert ct.transformers_[-1][1] == 'drop'
     assert_array_equal(ct.transformers_[-1][2], [1])
 
-    # specify to drop remaining columns
-    ct = ColumnTransformer([('trans1', Trans(), [0])],
-                           remainder='drop')
-    assert_array_equal(ct.fit_transform(X_array), X_res_first)
-    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_first)
+    # specify passthrough
+    ct = ColumnTransformer([('trans', Trans(), [0])], remainder='passthrough')
+    assert_array_equal(ct.fit_transform(X_array), X_res_both)
+    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)
     assert len(ct.transformers_) == 2
     assert ct.transformers_[-1][0] == 'remainder'
-    assert ct.transformers_[-1][1] == 'drop'
+    assert ct.transformers_[-1][1] == 'passthrough'
     assert_array_equal(ct.transformers_[-1][2], [1])
 
     # column order is not preserved (passed through added to end)
@@ -590,6 +658,10 @@ def test_column_transformer_remainder():
         "remainder keyword needs to be one of \'drop\', \'passthrough\', "
         "or estimator.", ct.fit_transform, X_array)
 
+    # check default for make_column_transformer
+    ct = make_column_transformer(([0], Trans()))
+    assert ct.remainder == 'drop'
+
 
 @pytest.mark.parametrize("key", [[0], np.array([0]), slice(0, 1),
                                  np.array([True, False])])
@@ -693,7 +765,8 @@ def test_column_transformer_sparse_remainder_transformer():
                         [8, 6, 4]]).T
 
     ct = ColumnTransformer([('trans1', Trans(), [0])],
-                           remainder=SparseMatrixTrans())
+                           remainder=SparseMatrixTrans(),
+                           sparse_threshold=0.8)
 
     X_trans = ct.fit_transform(X_array)
     assert sparse.issparse(X_trans)
@@ -715,7 +788,8 @@ def test_column_transformer_drop_all_sparse_remainder_transformer():
                         [2, 4, 6],
                         [8, 6, 4]]).T
     ct = ColumnTransformer([('trans1', 'drop', [0])],
-                           remainder=SparseMatrixTrans())
+                           remainder=SparseMatrixTrans(),
+                           sparse_threshold=0.8)
 
     X_trans = ct.fit_transform(X_array)
     assert sparse.issparse(X_trans)
@@ -733,11 +807,12 @@ def test_column_transformer_get_set_params_with_remainder():
     ct = ColumnTransformer([('trans1', StandardScaler(), [0])],
                            remainder=StandardScaler())
 
-    exp = {'n_jobs': 1,
+    exp = {'n_jobs': None,
            'remainder': ct.remainder,
            'remainder__copy': True,
            'remainder__with_mean': True,
            'remainder__with_std': True,
+           'sparse_threshold': 0.3,
            'trans1': ct.transformers[0][1],
            'trans1__copy': True,
            'trans1__with_mean': True,
@@ -751,11 +826,12 @@ def test_column_transformer_get_set_params_with_remainder():
     assert not ct.get_params()['remainder__with_std']
 
     ct.set_params(trans1='passthrough')
-    exp = {'n_jobs': 1,
+    exp = {'n_jobs': None,
            'remainder': ct.remainder,
            'remainder__copy': True,
            'remainder__with_mean': True,
            'remainder__with_std': False,
+           'sparse_threshold': 0.3,
            'trans1': 'passthrough',
            'transformers': ct.transformers,
            'transformer_weights': None}
@@ -777,3 +853,36 @@ def test_column_transformer_no_estimators():
     assert len(ct.transformers_) == 1
     assert ct.transformers_[-1][0] == 'remainder'
     assert ct.transformers_[-1][2] == [0, 1, 2]
+
+
+def test_column_transformer_no_estimators_set_params():
+    ct = ColumnTransformer([]).set_params(n_jobs=2)
+    assert ct.n_jobs == 2
+
+
+def test_column_transformer_callable_specifier():
+    # assert that function gets the full array / dataframe
+    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T
+    X_res_first = np.array([[0, 1, 2]]).T
+
+    def func(X):
+        assert_array_equal(X, X_array)
+        return [0]
+
+    ct = ColumnTransformer([('trans', Trans(), func)],
+                           remainder='drop')
+    assert_array_equal(ct.fit_transform(X_array), X_res_first)
+    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_first)
+
+    pd = pytest.importorskip('pandas')
+    X_df = pd.DataFrame(X_array, columns=['first', 'second'])
+
+    def func(X):
+        assert_array_equal(X.columns, X_df.columns)
+        assert_array_equal(X.values, X_df.values)
+        return ['first']
+
+    ct = ColumnTransformer([('trans', Trans(), func)],
+                           remainder='drop')
+    assert_array_equal(ct.fit_transform(X_df), X_res_first)
+    assert_array_equal(ct.fit(X_df).transform(X_df), X_res_first)
diff --git a/sklearn/covariance/elliptic_envelope.py b/sklearn/covariance/elliptic_envelope.py
index 0f9075abfb2e..a150c032ed43 100644
--- a/sklearn/covariance/elliptic_envelope.py
+++ b/sklearn/covariance/elliptic_envelope.py
@@ -106,8 +106,7 @@ def fit(self, X, y=None):
         y : (ignored)
         """
         super(EllipticEnvelope, self).fit(X)
-        self.offset_ = sp.stats.scoreatpercentile(
-            -self.dist_, 100. * self.contamination)
+        self.offset_ = np.percentile(-self.dist_, 100. * self.contamination)
         return self
 
     def decision_function(self, X, raw_values=None):
@@ -122,9 +121,9 @@ def decision_function(self, X, raw_values=None):
             decision function. Must be False (default) for compatibility
             with the others outlier detection tools.
 
-        .. deprecated:: 0.20
-            ``raw_values`` has been deprecated in 0.20 and will be removed
-            in 0.22.
+            .. deprecated:: 0.20
+                ``raw_values`` has been deprecated in 0.20 and will be removed
+                in 0.22.
 
         Returns
         -------
@@ -179,7 +178,7 @@ def predict(self, X):
             Returns -1 for anomalies/outliers and +1 for inliers.
         """
         X = check_array(X)
-        is_inlier = -np.ones(X.shape[0], dtype=int)
+        is_inlier = np.full(X.shape[0], -1, dtype=int)
         values = self.decision_function(X)
         is_inlier[values >= 0] = 1
 
diff --git a/sklearn/covariance/graph_lasso_.py b/sklearn/covariance/graph_lasso_.py
index 92fb0c9a6040..b10e3c7f3f82 100644
--- a/sklearn/covariance/graph_lasso_.py
+++ b/sklearn/covariance/graph_lasso_.py
@@ -19,11 +19,11 @@
 from ..exceptions import ConvergenceWarning
 from ..utils.validation import check_random_state, check_array
 from ..utils import deprecated
+from ..utils.fixes import _Sequence as Sequence
 from ..linear_model import lars_path
 from ..linear_model import cd_fast
 from ..model_selection import check_cv, cross_val_score
-from ..externals.joblib import Parallel, delayed
-import collections
+from ..utils import Parallel, delayed
 
 
 # Helper functions to compute the objective and dual objective functions
@@ -383,6 +383,9 @@ def graphical_lasso_path(X, alphas, cov_init=None, X_test=None, mode='cd',
     alphas : list of positive floats
         The list of regularization parameters, decreasing order.
 
+    cov_init : 2D array (n_features, n_features), optional
+        The initial guess for the covariance.
+
     X_test : 2D array, shape (n_test_samples, n_features), optional
         Optional test matrix to measure generalisation error.
 
@@ -494,6 +497,10 @@ class GraphicalLassoCV(GraphicalLasso):
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     tol : positive float, optional
         The tolerance to declare convergence: if the dual gap goes below
         this value, iterations are stopped.
@@ -513,8 +520,11 @@ class GraphicalLassoCV(GraphicalLasso):
         than number of samples. Elsewhere prefer cd which is more numerically
         stable.
 
-    n_jobs : int, optional
-        number of jobs to run in parallel (default 1).
+    n_jobs : int or None, optional (default=None)
+        number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : boolean, optional
         If verbose is True, the objective function and duality gap are
@@ -563,8 +573,8 @@ class GraphicalLassoCV(GraphicalLasso):
     be close to these missing values.
     """
 
-    def __init__(self, alphas=4, n_refinements=4, cv=None, tol=1e-4,
-                 enet_tol=1e-4, max_iter=100, mode='cd', n_jobs=1,
+    def __init__(self, alphas=4, n_refinements=4, cv='warn', tol=1e-4,
+                 enet_tol=1e-4, max_iter=100, mode='cd', n_jobs=None,
                  verbose=False, assume_centered=False):
         super(GraphicalLassoCV, self).__init__(
             mode=mode, tol=tol, verbose=verbose, enet_tol=enet_tol,
@@ -605,7 +615,7 @@ def fit(self, X, y=None):
         n_alphas = self.alphas
         inner_verbose = max(0, self.verbose - 1)
 
-        if isinstance(n_alphas, collections.Sequence):
+        if isinstance(n_alphas, Sequence):
             alphas = self.alphas
             n_refinements = 1
         else:
@@ -681,7 +691,7 @@ def fit(self, X, y=None):
                 alpha_1 = path[best_index - 1][0]
                 alpha_0 = path[best_index + 1][0]
 
-            if not isinstance(n_alphas, collections.Sequence):
+            if not isinstance(n_alphas, Sequence):
                 alphas = np.logspace(np.log10(alpha_1), np.log10(alpha_0),
                                      n_alphas + 2)
                 alphas = alphas[1:-1]
@@ -897,6 +907,10 @@ class GraphLassoCV(GraphicalLassoCV):
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     tol : positive float, optional
         The tolerance to declare convergence: if the dual gap goes below
         this value, iterations are stopped.
@@ -916,8 +930,11 @@ class GraphLassoCV(GraphicalLassoCV):
         than number of samples. Elsewhere prefer cd which is more numerically
         stable.
 
-    n_jobs : int, optional
-        number of jobs to run in parallel (default 1).
+    n_jobs : int or None, optional (default=None)
+        number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : boolean, optional
         If verbose is True, the objective function and duality gap are
diff --git a/sklearn/covariance/robust_covariance.py b/sklearn/covariance/robust_covariance.py
index a613906f36d1..bcd561319a05 100644
--- a/sklearn/covariance/robust_covariance.py
+++ b/sklearn/covariance/robust_covariance.py
@@ -7,6 +7,8 @@
 # Author: Virgile Fritsch <virgile.fritsch@inria.fr>
 #
 # License: BSD 3 clause
+from __future__ import division
+
 import warnings
 import numbers
 import numpy as np
@@ -55,16 +57,16 @@ def c_step(X, n_support, remaining_iterations=30, initial_estimates=None,
     verbose : boolean, optional
         Verbose mode.
 
+    cov_computation_method : callable, default empirical_covariance
+        The function which will be used to compute the covariance.
+        Must return shape (n_features, n_features)
+
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
         If RandomState instance, random_state is the random number generator;
         If None, the random number generator is the RandomState instance used
         by `np.random`.
 
-    cov_computation_method : callable, default empirical_covariance
-        The function which will be used to compute the covariance.
-        Must return shape (n_features, n_features)
-
     Returns
     -------
     location : array-like, shape (n_features,)
@@ -161,8 +163,12 @@ def _c_step(X, n_support, random_state, remaining_iterations=30,
         results = location, covariance, det, support, dist
     elif det > previous_det:
         # determinant has increased (should not happen)
-        warnings.warn("Warning! det > previous_det (%.15f > %.15f)"
-                      % (det, previous_det), RuntimeWarning)
+        warnings.warn("Determinant has increased; this should not happen: "
+                      "log(det) > log(previous_det) (%.15f > %.15f). "
+                      "You may want to try with a higher value of "
+                      "support_fraction (current value: %.3f)."
+                      % (det, previous_det, n_support / n_samples),
+                      RuntimeWarning)
         results = previous_location, previous_covariance, \
             previous_det, previous_support, previous_dist
 
@@ -200,9 +206,6 @@ def select_candidates(X, n_support, n_trials, select=1, n_iter=30,
     n_support : int, [(n + p + 1)/2] < n_support < n
         The number of samples the pure data set must contain.
 
-    select : int, int > 0
-        Number of best candidates results to return.
-
     n_trials : int, nb_trials > 0 or 2-tuple
         Number of different initial sets of observations from which to
         run the algorithm.
@@ -214,22 +217,25 @@ def select_candidates(X, n_support, n_trials, select=1, n_iter=30,
         - n_trials[1]: array-like, shape (n_trials, n_features, n_features)
           is the list of `n_trials` initial covariances estimates
 
+    select : int, int > 0
+        Number of best candidates results to return.
+
     n_iter : int, nb_iter > 0
         Maximum number of iterations for the c_step procedure.
         (2 is enough to be close to the final solution. "Never" exceeds 20).
 
-    random_state : int, RandomState instance or None, optional (default=None)
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
+    verbose : boolean, default False
+        Control the output verbosity.
 
     cov_computation_method : callable, default empirical_covariance
         The function which will be used to compute the covariance.
         Must return shape (n_features, n_features)
 
-    verbose : boolean, default False
-        Control the output verbosity.
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
 
     See Also
     ---------
diff --git a/sklearn/covariance/tests/test_graph_lasso.py b/sklearn/covariance/tests/test_graph_lasso.py
index 69dee9a883fe..8c0753636361 100644
--- a/sklearn/covariance/tests/test_graph_lasso.py
+++ b/sklearn/covariance/tests/test_graph_lasso.py
@@ -2,6 +2,8 @@
 """
 import sys
 
+import pytest
+
 import numpy as np
 from scipy import linalg
 
@@ -118,6 +120,7 @@ def test_graph_lasso_iris_singular():
 
 
 @ignore_warnings(category=DeprecationWarning)
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_graph_lasso_cv(random_state=1):
     # Sample data from a sparse multivariate normal
     dim = 5
@@ -141,6 +144,7 @@ def test_graph_lasso_cv(random_state=1):
 
 
 @ignore_warnings(category=DeprecationWarning)
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_deprecated_grid_scores(random_state=1):
     dim = 5
     n_samples = 6
diff --git a/sklearn/covariance/tests/test_graphical_lasso.py b/sklearn/covariance/tests/test_graphical_lasso.py
index d476cc52373d..f1d6aab6a9b2 100644
--- a/sklearn/covariance/tests/test_graphical_lasso.py
+++ b/sklearn/covariance/tests/test_graphical_lasso.py
@@ -4,6 +4,7 @@
 
 import numpy as np
 from scipy import linalg
+import pytest
 
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_array_less
@@ -15,6 +16,7 @@
 from sklearn.externals.six.moves import StringIO
 from sklearn.utils import check_random_state
 from sklearn import datasets
+from sklearn.utils.fixes import PY3_OR_LATER
 
 from numpy.testing import assert_equal
 
@@ -114,6 +116,7 @@ def test_graphical_lasso_iris_singular():
         assert_array_almost_equal(icov, icov_R, decimal=5)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_graphical_lasso_cv(random_state=1):
     # Sample data from a sparse multivariate normal
     dim = 5
@@ -136,6 +139,9 @@ def test_graphical_lasso_cv(random_state=1):
     GraphicalLassoCV(alphas=[0.8, 0.5], tol=1e-1, n_jobs=1).fit(X)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
+@pytest.mark.skipif(not PY3_OR_LATER,
+                    reason='On Python 2 DeprecationWarning is not issued for some unkown reason.')
 def test_deprecated_grid_scores(random_state=1):
     dim = 5
     n_samples = 6
@@ -151,6 +157,5 @@ def test_deprecated_grid_scores(random_state=1):
                     "0.19 and will be removed in 0.21. Use "
                     "``grid_scores_`` instead")
 
-    assert_warns_message(DeprecationWarning, depr_message,
-                         lambda: graphical_lasso.grid_scores)
-    assert_equal(graphical_lasso.grid_scores, graphical_lasso.grid_scores_)
+    with pytest.warns(DeprecationWarning, match=depr_message):
+        assert_equal(graphical_lasso.grid_scores, graphical_lasso.grid_scores_)
diff --git a/sklearn/covariance/tests/test_robust_covariance.py b/sklearn/covariance/tests/test_robust_covariance.py
index b00069ffe973..39caa4dd18df 100644
--- a/sklearn/covariance/tests/test_robust_covariance.py
+++ b/sklearn/covariance/tests/test_robust_covariance.py
@@ -10,6 +10,7 @@
 
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_raise_message
+from sklearn.utils.testing import assert_warns_message
 
 from sklearn import datasets
 from sklearn.covariance import empirical_covariance, MinCovDet
@@ -133,3 +134,35 @@ def test_mcd_support_covariance_is_zero():
            'increase support_fraction')
     for X in [X_1, X_2]:
         assert_raise_message(ValueError, msg, MinCovDet().fit, X)
+
+
+def test_mcd_increasing_det_warning():
+    # Check that a warning is raised if we observe increasing determinants
+    # during the c_step. In theory the sequence of determinants should be
+    # decreasing. Increasing determinants are likely due to ill-conditioned
+    # covariance matrices that result in poor precision matrices.
+
+    X = [[5.1, 3.5, 1.4, 0.2],
+         [4.9, 3.0, 1.4, 0.2],
+         [4.7, 3.2, 1.3, 0.2],
+         [4.6, 3.1, 1.5, 0.2],
+         [5.0, 3.6, 1.4, 0.2],
+         [4.6, 3.4, 1.4, 0.3],
+         [5.0, 3.4, 1.5, 0.2],
+         [4.4, 2.9, 1.4, 0.2],
+         [4.9, 3.1, 1.5, 0.1],
+         [5.4, 3.7, 1.5, 0.2],
+         [4.8, 3.4, 1.6, 0.2],
+         [4.8, 3.0, 1.4, 0.1],
+         [4.3, 3.0, 1.1, 0.1],
+         [5.1, 3.5, 1.4, 0.3],
+         [5.7, 3.8, 1.7, 0.3],
+         [5.4, 3.4, 1.7, 0.2],
+         [4.6, 3.6, 1.0, 0.2],
+         [5.0, 3.0, 1.6, 0.2],
+         [5.2, 3.5, 1.5, 0.2]]
+
+    mcd = MinCovDet(random_state=1)
+    assert_warns_message(RuntimeWarning,
+                         "Determinant has increased",
+                         mcd.fit, X)
diff --git a/sklearn/cross_decomposition/pls_.py b/sklearn/cross_decomposition/pls_.py
index 7566ee49d220..df7cb22b895f 100644
--- a/sklearn/cross_decomposition/pls_.py
+++ b/sklearn/cross_decomposition/pls_.py
@@ -775,6 +775,25 @@ class PLSSVD(BaseEstimator, TransformerMixin):
     y_scores_ : array, [n_samples, n_components]
         Y scores.
 
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.cross_decomposition import PLSSVD
+    >>> X = np.array([[0., 0., 1.],
+    ...     [1.,0.,0.],
+    ...     [2.,2.,2.],
+    ...     [2.,5.,4.]])
+    >>> Y = np.array([[0.1, -0.2],
+    ...     [0.9, 1.1],
+    ...     [6.2, 5.9],
+    ...     [11.9, 12.3]])
+    >>> plsca = PLSSVD(n_components=2)
+    >>> plsca.fit(X, Y)
+    PLSSVD(copy=True, n_components=2, scale=True)
+    >>> X_c, Y_c = plsca.transform(X, Y)
+    >>> X_c.shape, Y_c.shape
+    ((4, 2), (4, 2))
+
     See also
     --------
     PLSCanonical
diff --git a/sklearn/datasets/__init__.py b/sklearn/datasets/__init__.py
index c43c0c4758b1..c7d78e633493 100644
--- a/sklearn/datasets/__init__.py
+++ b/sklearn/datasets/__init__.py
@@ -23,6 +23,7 @@
 from .twenty_newsgroups import fetch_20newsgroups
 from .twenty_newsgroups import fetch_20newsgroups_vectorized
 from .mldata import fetch_mldata, mldata_filename
+from .openml import fetch_openml
 from .samples_generator import make_classification
 from .samples_generator import make_multilabel_classification
 from .samples_generator import make_hastie_10_2
@@ -65,6 +66,7 @@
            'fetch_covtype',
            'fetch_rcv1',
            'fetch_kddcup99',
+           'fetch_openml',
            'get_data_home',
            'load_boston',
            'load_diabetes',
diff --git a/sklearn/datasets/base.py b/sklearn/datasets/base.py
index f19ee5e58641..6f1ceef70aaa 100644
--- a/sklearn/datasets/base.py
+++ b/sklearn/datasets/base.py
@@ -211,6 +211,9 @@ def load_data(module_path, data_file_name):
 
     Parameters
     ----------
+    module_path : string
+        The module path.
+
     data_file_name : string
         Name of csv file to be loaded from
         module_path/data/data_file_name. For example 'wine_data.csv'.
@@ -261,7 +264,7 @@ def load_wine(return_X_y=False):
     Features            real, positive
     =================   ==============
 
-    Read more in the :ref:`User Guide <datasets>`.
+    Read more in the :ref:`User Guide <wine_dataset>`.
 
     Parameters
     ----------
@@ -336,7 +339,7 @@ def load_iris(return_X_y=False):
     Features            real, positive
     =================   ==============
 
-    Read more in the :ref:`User Guide <datasets>`.
+    Read more in the :ref:`User Guide <iris_dataset>`.
 
     Parameters
     ----------
@@ -411,6 +414,8 @@ def load_breast_cancer(return_X_y=False):
     Features            real, positive
     =================   ==============
 
+    Read more in the :ref:`User Guide <breast_cancer_dataset>`.
+
     Parameters
     ----------
     return_X_y : boolean, default=False
@@ -495,7 +500,7 @@ def load_digits(n_class=10, return_X_y=False):
     Features             integers 0-16
     =================   ==============
 
-    Read more in the :ref:`User Guide <datasets>`.
+    Read more in the :ref:`User Guide <digits_dataset>`.
 
     Parameters
     ----------
@@ -572,7 +577,7 @@ def load_diabetes(return_X_y=False):
     Targets             integer 25 - 346
     ==============      ==================
 
-    Read more in the :ref:`User Guide <datasets>`.
+    Read more in the :ref:`User Guide <diabetes_dataset>`.
 
     Parameters
     ----------
@@ -625,6 +630,8 @@ def load_linnerud(return_X_y=False):
     Targets           integer
     ==============    ============================
 
+    Read more in the :ref:`User Guide <linnerrud_dataset>`.
+
     Parameters
     ----------
     return_X_y : boolean, default=False.
@@ -687,6 +694,8 @@ def load_boston(return_X_y=False):
     Targets             real 5. - 50.
     ==============     ==============
 
+    Read more in the :ref:`User Guide <boston_dataset>`.
+
     Parameters
     ----------
     return_X_y : boolean, default=False.
@@ -757,6 +766,8 @@ def load_sample_images():
 
     Loads both, ``china`` and ``flower``.
 
+    Read more in the :ref:`User Guide <sample_images>`.
+
     Returns
     -------
     data : Bunch
@@ -798,6 +809,8 @@ def load_sample_images():
 def load_sample_image(image_name):
     """Load the numpy array of a single sample image
 
+    Read more in the :ref:`User Guide <sample_images>`.
+
     Parameters
     -----------
     image_name : {`china.jpg`, `flower.jpg`}
diff --git a/sklearn/datasets/california_housing.py b/sklearn/datasets/california_housing.py
index 8973ba59ad21..76cb27dadd7a 100644
--- a/sklearn/datasets/california_housing.py
+++ b/sklearn/datasets/california_housing.py
@@ -21,7 +21,7 @@
 # Authors: Peter Prettenhofer
 # License: BSD 3 clause
 
-from os.path import exists
+from os.path import dirname, exists, join
 from os import makedirs, remove
 import tarfile
 
@@ -43,18 +43,21 @@
     checksum=('aaa5c9a6afe2225cc2aed2723682ae40'
               '3280c4a3695a2ddda4ffb5d8215ea681'))
 
-# Grab the module-level docstring to use as a description of the
-# dataset
-MODULE_DOCS = __doc__
-
 logger = logging.getLogger(__name__)
 
 
 def fetch_california_housing(data_home=None, download_if_missing=True,
                              return_X_y=False):
-    """Loader for the California housing dataset from StatLib.
+    """Load the California housing dataset (regression).
+
+    ==============     ==============
+    Samples total               20640
+    Dimensionality                  8
+    Features                     real
+    Target             real 0.15 - 5.
+    ==============     ==============
 
-    Read more in the :ref:`User Guide <datasets>`.
+    Read more in the :ref:`User Guide <california_housing_dataset>`.
 
     Parameters
     ----------
@@ -144,10 +147,14 @@ def fetch_california_housing(data_home=None, download_if_missing=True,
     # target in units of 100,000
     target = target / 100000.0
 
+    module_path = dirname(__file__)
+    with open(join(module_path, 'descr', 'california_housing.rst')) as dfile:
+        descr = dfile.read()
+
     if return_X_y:
         return data, target
 
     return Bunch(data=data,
                  target=target,
                  feature_names=feature_names,
-                 DESCR=MODULE_DOCS)
+                 DESCR=descr)
diff --git a/sklearn/datasets/covtype.py b/sklearn/datasets/covtype.py
index c7b880b116ea..a08f61f02b0c 100644
--- a/sklearn/datasets/covtype.py
+++ b/sklearn/datasets/covtype.py
@@ -16,7 +16,7 @@
 
 from gzip import GzipFile
 import logging
-from os.path import exists, join
+from os.path import dirname, exists, join
 from os import remove
 
 import numpy as np
@@ -43,9 +43,18 @@
 
 def fetch_covtype(data_home=None, download_if_missing=True,
                   random_state=None, shuffle=False, return_X_y=False):
-    """Load the covertype dataset, downloading it if necessary.
+    """Load the covertype dataset (classification).
 
-    Read more in the :ref:`User Guide <datasets>`.
+    Download it if necessary.
+
+    =================   ============
+    Classes                        7
+    Samples total             581012
+    Dimensionality                54
+    Features                     int
+    =================   ============
+
+    Read more in the :ref:`User Guide <covtype_dataset>`.
 
     Parameters
     ----------
@@ -127,7 +136,11 @@ def fetch_covtype(data_home=None, download_if_missing=True,
         X = X[ind]
         y = y[ind]
 
+    module_path = dirname(__file__)
+    with open(join(module_path, 'descr', 'covtype.rst')) as rst_file:
+        fdescr = rst_file.read()
+
     if return_X_y:
         return X, y
 
-    return Bunch(data=X, target=y, DESCR=__doc__)
+    return Bunch(data=X, target=y, DESCR=fdescr)
diff --git a/sklearn/datasets/descr/california_housing.rst b/sklearn/datasets/descr/california_housing.rst
new file mode 100644
index 000000000000..9ab3b679b68f
--- /dev/null
+++ b/sklearn/datasets/descr/california_housing.rst
@@ -0,0 +1,40 @@
+.. _california_housing_dataset:
+
+California Housing dataset
+--------------------------
+
+**Data Set Characteristics:**
+
+    :Number of Instances: 20640
+
+    :Number of Attributes: 8 numeric, predictive attributes and the target
+
+    :Attribute Information:
+        - MedInc        median income in block
+        - HouseAge      median house age in block
+        - AveRooms      average number of rooms
+        - AveBedrms     average number of bedrooms
+        - Population    block population
+        - AveOccup      average house occupancy
+        - Latitude      house block latitude
+        - Longitude     house block longitude
+
+    :Missing Attribute Values: None
+
+This dataset was obtained from the StatLib repository.
+http://lib.stat.cmu.edu/datasets/
+
+The target variable is the median house value for California districts.
+
+This dataset was derived from the 1990 U.S. census, using one row per census
+block group. A block group is the smallest geographical unit for which the U.S.
+Census Bureau publishes sample data (a block group typically has a population
+of 600 to 3,000 people).
+
+It can be downloaded/loaded using the
+:func:`sklearn.datasets.fetch_california_housing` function.
+
+.. topic:: References
+
+    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,
+      Statistics and Probability Letters, 33 (1997) 291-297
diff --git a/doc/datasets/covtype.rst b/sklearn/datasets/descr/covtype.rst
similarity index 73%
rename from doc/datasets/covtype.rst
rename to sklearn/datasets/descr/covtype.rst
index 4b31eff69cf0..08447403ebba 100644
--- a/doc/datasets/covtype.rst
+++ b/sklearn/datasets/descr/covtype.rst
@@ -1,4 +1,4 @@
-.. _covtype:
+.. _covtype_dataset:
 
 Forest covertypes
 -----------------
@@ -12,6 +12,15 @@ Each sample has 54 features, described on the
 Some of the features are boolean indicators,
 while others are discrete or continuous measurements.
 
+**Data Set Characteristics:**
+
+    =================   ============
+    Classes                        7
+    Samples total             581012
+    Dimensionality                54
+    Features                     int
+    =================   ============
+
 :func:`sklearn.datasets.fetch_covtype` will load the covertype dataset;
 it returns a dictionary-like object
 with the feature matrix in the ``data`` member
diff --git a/sklearn/datasets/descr/kddcup99.rst b/sklearn/datasets/descr/kddcup99.rst
new file mode 100644
index 000000000000..6e942246ea2e
--- /dev/null
+++ b/sklearn/datasets/descr/kddcup99.rst
@@ -0,0 +1,95 @@
+.. _kddcup99_dataset:
+
+Kddcup 99 dataset
+-----------------
+
+The KDD Cup '99 dataset was created by processing the tcpdump portions
+of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset,
+created by MIT Lincoln Lab [1]. The artificial data (described on the `dataset's
+homepage <http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html>`_) was
+generated using a closed network and hand-injected attacks to produce a
+large number of different types of attack with normal activity in the
+background. As the initial goal was to produce a large training set for
+supervised learning algorithms, there is a large proportion (80.1%) of
+abnormal data which is unrealistic in real world, and inappropriate for
+unsupervised anomaly detection which aims at detecting 'abnormal' data, ie
+
+1) qualitatively different from normal data
+
+2) in large minority among the observations.
+
+We thus transform the KDD Data set into two different data sets: SA and SF.
+
+-SA is obtained by simply selecting all the normal data, and a small
+proportion of abnormal data to gives an anomaly proportion of 1%.
+
+-SF is obtained as in [2]
+by simply picking up the data whose attribute logged_in is positive, thus
+focusing on the intrusion attack, which gives a proportion of 0.3% of
+attack.
+
+-http and smtp are two subsets of SF corresponding with third feature
+equal to 'http' (resp. to 'smtp')
+
+General KDD structure :
+
+    ================      ==========================================
+    Samples total         4898431
+    Dimensionality        41
+    Features              discrete (int) or continuous (float)
+    Targets               str, 'normal.' or name of the anomaly type
+    ================      ==========================================
+
+    SA structure :
+
+    ================      ==========================================
+    Samples total         976158
+    Dimensionality        41
+    Features              discrete (int) or continuous (float)
+    Targets               str, 'normal.' or name of the anomaly type
+    ================      ==========================================
+
+    SF structure :
+
+    ================      ==========================================
+    Samples total         699691
+    Dimensionality        4
+    Features              discrete (int) or continuous (float)
+    Targets               str, 'normal.' or name of the anomaly type
+    ================      ==========================================
+
+    http structure :
+
+    ================      ==========================================
+    Samples total         619052
+    Dimensionality        3
+    Features              discrete (int) or continuous (float)
+    Targets               str, 'normal.' or name of the anomaly type
+    ================      ==========================================
+
+    smtp structure :
+
+    ================      ==========================================
+    Samples total         95373
+    Dimensionality        3
+    Features              discrete (int) or continuous (float)
+    Targets               str, 'normal.' or name of the anomaly type
+    ================      ==========================================
+
+:func:`sklearn.datasets.fetch_kddcup99` will load the kddcup99 dataset; it
+returns a dictionary-like object with the feature matrix in the ``data`` member
+and the target values in ``target``. The dataset will be downloaded from the
+web if necessary.
+
+.. topic: References
+
+    .. [1] Analysis and Results of the 1999 DARPA Off-Line Intrusion
+           Detection Evaluation Richard Lippmann, Joshua W. Haines,
+           David J. Fried, Jonathan Korba, Kumar Das
+
+    .. [2] K. Yamanishi, J.-I. Takeuchi, G. Williams, and P. Milne. Online
+           unsupervised outlier detection using finite mixtures with
+           discounting learning algorithms. In Proceedings of the sixth
+           ACM SIGKDD international conference on Knowledge discovery
+           and data mining, pages 320-324. ACM Press, 2000.
+
diff --git a/doc/datasets/labeled_faces.rst b/sklearn/datasets/descr/lfw.rst
similarity index 91%
rename from doc/datasets/labeled_faces.rst
rename to sklearn/datasets/descr/lfw.rst
index a7b592ae1a94..e7fc35c3caab 100644
--- a/doc/datasets/labeled_faces.rst
+++ b/sklearn/datasets/descr/lfw.rst
@@ -1,4 +1,4 @@
-.. _labeled_faces_in_the_wild:
+.. _labeled_faces_in_the_wild_dataset:
 
 The Labeled Faces in the Wild face recognition dataset
 ------------------------------------------------------
@@ -23,6 +23,14 @@ most popular model for Face Detection is called Viola-Jones and is
 implemented in the OpenCV library. The LFW faces were extracted by this
 face detector from various online websites.
 
+**Data Set Characteristics:**
+
+    =================   =======================
+    Classes                                5749
+    Samples total                         13233
+    Dimensionality                         5828
+    Features            real, between 0 and 255
+    =================   =======================
 
 Usage
 ~~~~~
diff --git a/doc/datasets/olivetti_faces.rst b/sklearn/datasets/descr/olivetti_faces.rst
similarity index 68%
rename from doc/datasets/olivetti_faces.rst
rename to sklearn/datasets/descr/olivetti_faces.rst
index 71be4f66a2fc..c6193d505653 100644
--- a/doc/datasets/olivetti_faces.rst
+++ b/sklearn/datasets/descr/olivetti_faces.rst
@@ -1,12 +1,10 @@
-
-.. _olivetti_faces:
+.. _olivetti_faces_dataset:
 
 The Olivetti faces dataset
 --------------------------
 
-
-`This dataset contains a set of face images`_ taken between April 1992 and April
-1994 at AT&T Laboratories Cambridge. The
+`This dataset contains a set of face images`_ taken between April 1992 and 
+April 1994 at AT&T Laboratories Cambridge. The
 :func:`sklearn.datasets.fetch_olivetti_faces` function is the data
 fetching / caching function that downloads the data
 archive from AT&T.
@@ -19,12 +17,21 @@ As described on the original website:
     subjects, the images were taken at different times, varying the lighting,
     facial expressions (open / closed eyes, smiling / not smiling) and facial
     details (glasses / no glasses). All the images were taken against a dark
-    homogeneous background with the subjects in an upright, frontal position (with
-    tolerance for some side movement).
+    homogeneous background with the subjects in an upright, frontal position 
+    (with tolerance for some side movement).
+
+**Data Set Characteristics:**
+
+    =================   =====================
+    Classes                                40
+    Samples total                         400
+    Dimensionality                       4096
+    Features            real, between 0 and 1
+    =================   =====================
 
-The image is quantized to 256 grey levels and stored as unsigned 8-bit integers;
-the loader will convert these to floating point values on the interval [0, 1],
-which are easier to work with for many algorithms.
+The image is quantized to 256 grey levels and stored as unsigned 8-bit 
+integers; the loader will convert these to floating point values on the 
+interval [0, 1], which are easier to work with for many algorithms.
 
 The "target" for this database is an integer from 0 to 39 indicating the
 identity of the person pictured; however, with only 10 examples per class, this
diff --git a/sklearn/datasets/descr/rcv1.rst b/sklearn/datasets/descr/rcv1.rst
new file mode 100644
index 000000000000..afaadbfb45af
--- /dev/null
+++ b/sklearn/datasets/descr/rcv1.rst
@@ -0,0 +1,72 @@
+.. _rcv1_dataset:
+
+RCV1 dataset
+------------
+
+Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually 
+categorized newswire stories made available by Reuters, Ltd. for research 
+purposes. The dataset is extensively described in [1]_.
+
+**Data Set Characteristics:**
+
+    ==============     =====================
+    Classes                              103
+    Samples total                     804414
+    Dimensionality                     47236
+    Features           real, between 0 and 1
+    ==============     =====================
+
+:func:`sklearn.datasets.fetch_rcv1` will load the following 
+version: RCV1-v2, vectors, full sets, topics multilabels::
+
+    >>> from sklearn.datasets import fetch_rcv1
+    >>> rcv1 = fetch_rcv1()
+
+It returns a dictionary-like object, with the following attributes:
+
+``data``:
+The feature matrix is a scipy CSR sparse matrix, with 804414 samples and
+47236 features. Non-zero values contains cosine-normalized, log TF-IDF vectors.
+A nearly chronological split is proposed in [1]_: The first 23149 samples are
+the training set. The last 781265 samples are the testing set. This follows 
+the official LYRL2004 chronological split. The array has 0.16% of non zero 
+values::
+
+    >>> rcv1.data.shape
+    (804414, 47236)
+
+``target``:
+The target values are stored in a scipy CSR sparse matrix, with 804414 samples 
+and 103 categories. Each sample has a value of 1 in its categories, and 0 in 
+others. The array has 3.15% of non zero values::
+
+    >>> rcv1.target.shape
+    (804414, 103)
+
+``sample_id``:
+Each sample can be identified by its ID, ranging (with gaps) from 2286 
+to 810596::
+
+    >>> rcv1.sample_id[:3]
+    array([2286, 2287, 2288], dtype=uint32)
+
+``target_names``:
+The target values are the topics of each sample. Each sample belongs to at 
+least one topic, and to up to 17 topics. There are 103 topics, each 
+represented by a string. Their corpus frequencies span five orders of 
+magnitude, from 5 occurrences for 'GMIL', to 381327 for 'CCAT'::
+
+    >>> rcv1.target_names[:3].tolist()  # doctest: +SKIP
+    ['E11', 'ECAT', 'M11']
+
+The dataset will be downloaded from the `rcv1 homepage`_ if necessary.
+The compressed size is about 656 MB.
+
+.. _rcv1 homepage: http://jmlr.csail.mit.edu/papers/volume5/lewis04a/
+
+
+.. topic:: References
+
+    .. [1] Lewis, D. D., Yang, Y., Rose, T. G., & Li, F. (2004). 
+           RCV1: A new benchmark collection for text categorization research. 
+           The Journal of Machine Learning Research, 5, 361-397.
diff --git a/doc/datasets/twenty_newsgroups.rst b/sklearn/datasets/descr/twenty_newsgroups.rst
similarity index 94%
rename from doc/datasets/twenty_newsgroups.rst
rename to sklearn/datasets/descr/twenty_newsgroups.rst
index 5aaca66c5d67..764c19037dd3 100644
--- a/doc/datasets/twenty_newsgroups.rst
+++ b/sklearn/datasets/descr/twenty_newsgroups.rst
@@ -1,4 +1,4 @@
-.. _20newsgroups:
+.. _20newsgroups_dataset:
 
 The 20 newsgroups text dataset
 ------------------------------
@@ -18,6 +18,15 @@ The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,
 returns ready-to-use features, i.e., it is not necessary to use a feature
 extractor.
 
+**Data Set Characteristics:**
+
+    =================   ==========
+    Classes                     20
+    Samples total            18846
+    Dimensionality               1
+    Features                  text
+    =================   ==========
+
 Usage
 ~~~~~
 
@@ -104,11 +113,11 @@ The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero
 components by sample in a more than 30000-dimensional space
 (less than .5% non-zero features)::
 
-  >>> vectors.nnz / float(vectors.shape[0])
+  >>> vectors.nnz / float(vectors.shape[0])       # doctest: +ELLIPSIS
   159.01327...
 
-:func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which returns
-ready-to-use tfidf features instead of file names.
+:func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which 
+returns ready-to-use token counts features instead of file names.
 
 .. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/
 .. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf
@@ -135,7 +144,7 @@ which is fast to train and achieves a decent F-score::
   MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)
 
   >>> pred = clf.predict(vectors_test)
-  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')
+  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  # doctest: +ELLIPSIS
   0.88213...
 
 (The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles
@@ -158,6 +167,7 @@ Let's take a look at what the most informative features are:
   sci.space: edu it that is in and space to of the
   talk.religion.misc: not it you in is that and to of the
 
+
 You can now see many things that these features have overfit to:
 
 - Almost every group is distinguished by whether headers such as
@@ -185,7 +195,7 @@ blocks, and quotation blocks respectively.
   ...                                      categories=categories)
   >>> vectors_test = vectorizer.transform(newsgroups_test.data)
   >>> pred = clf.predict(vectors_test)
-  >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')
+  >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')  # doctest: +ELLIPSIS
   0.77310...
 
 This classifier lost over a lot of its F-score, just because we removed
@@ -202,7 +212,7 @@ It loses even more if we also strip this metadata from the training data:
 
   >>> vectors_test = vectorizer.transform(newsgroups_test.data)
   >>> pred = clf.predict(vectors_test)
-  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')
+  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  # doctest: +ELLIPSIS
   0.76995...
 
 Some other classifiers cope better with this harder version of the task. Try
diff --git a/sklearn/datasets/descr/wine_data.rst b/sklearn/datasets/descr/wine_data.rst
index f43e6524130b..9d506b4ab70b 100644
--- a/sklearn/datasets/descr/wine_data.rst
+++ b/sklearn/datasets/descr/wine_data.rst
@@ -21,6 +21,7 @@ Wine recognition dataset
  		- Hue
  		- OD280/OD315 of diluted wines
  		- Proline
+
     - class:
             - class_0
             - class_1
@@ -91,4 +92,4 @@ School of Information and Computer Science.
   "THE CLASSIFICATION PERFORMANCE OF RDA" 
   Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of 
   Mathematics and Statistics, James Cook University of North Queensland. 
-  (Also submitted to Journal of Chemometrics).
\ No newline at end of file
+  (Also submitted to Journal of Chemometrics).
diff --git a/sklearn/datasets/kddcup99.py b/sklearn/datasets/kddcup99.py
index 77175a1710f6..c8ed0e30884a 100644
--- a/sklearn/datasets/kddcup99.py
+++ b/sklearn/datasets/kddcup99.py
@@ -13,7 +13,7 @@
 from gzip import GzipFile
 import logging
 import os
-from os.path import exists, join
+from os.path import dirname, exists, join
 
 import numpy as np
 
@@ -48,80 +48,18 @@
 def fetch_kddcup99(subset=None, data_home=None, shuffle=False,
                    random_state=None,
                    percent10=True, download_if_missing=True, return_X_y=False):
-    """Load and return the kddcup 99 dataset (classification).
+    """Load the kddcup99 dataset (classification).
 
-    The KDD Cup '99 dataset was created by processing the tcpdump portions
-    of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset,
-    created by MIT Lincoln Lab [1]. The artificial data was generated using
-    a closed network and hand-injected attacks to produce a large number of
-    different types of attack with normal activity in the background.
-    As the initial goal was to produce a large training set for supervised
-    learning algorithms, there is a large proportion (80.1%) of abnormal
-    data which is unrealistic in real world, and inappropriate for unsupervised
-    anomaly detection which aims at detecting 'abnormal' data, ie
+    Download it if necessary.
 
-    1) qualitatively different from normal data.
+    =================   ====================================
+    Classes                                               23
+    Samples total                                    4898431
+    Dimensionality                                        41
+    Features            discrete (int) or continuous (float)
+    =================   ====================================
 
-    2) in large minority among the observations.
-
-    We thus transform the KDD Data set into two different data sets: SA and SF.
-
-    - SA is obtained by simply selecting all the normal data, and a small
-      proportion of abnormal data to gives an anomaly proportion of 1%.
-
-    - SF is obtained as in [2]
-      by simply picking up the data whose attribute logged_in is positive, thus
-      focusing on the intrusion attack, which gives a proportion of 0.3% of
-      attack.
-
-    - http and smtp are two subsets of SF corresponding with third feature
-      equal to 'http' (resp. to 'smtp')
-
-
-    General KDD structure :
-
-    ================      ==========================================
-    Samples total         4898431
-    Dimensionality        41
-    Features              discrete (int) or continuous (float)
-    Targets               str, 'normal.' or name of the anomaly type
-    ================      ==========================================
-
-    SA structure :
-
-    ================      ==========================================
-    Samples total         976158
-    Dimensionality        41
-    Features              discrete (int) or continuous (float)
-    Targets               str, 'normal.' or name of the anomaly type
-    ================      ==========================================
-
-    SF structure :
-
-    ================      ==========================================
-    Samples total         699691
-    Dimensionality        4
-    Features              discrete (int) or continuous (float)
-    Targets               str, 'normal.' or name of the anomaly type
-    ================      ==========================================
-
-    http structure :
-
-    ================      ==========================================
-    Samples total         619052
-    Dimensionality        3
-    Features              discrete (int) or continuous (float)
-    Targets               str, 'normal.' or name of the anomaly type
-    ================      ==========================================
-
-    smtp structure :
-
-    ================      ==========================================
-    Samples total         95373
-    Dimensionality        3
-    Features              discrete (int) or continuous (float)
-    Targets               str, 'normal.' or name of the anomaly type
-    ================      ==========================================
+    Read more in the :ref:`User Guide <kddcup99_dataset>`.
 
     .. versionadded:: 0.18
 
@@ -162,25 +100,13 @@ def fetch_kddcup99(subset=None, data_home=None, shuffle=False,
     -------
     data : Bunch
         Dictionary-like object, the interesting attributes are:
-        'data', the data to learn and 'target', the regression target for each
-        sample.
+         - 'data', the data to learn.
+         - 'target', the regression target for each sample.
+         - 'DESCR', a description of the dataset.
 
     (data, target) : tuple if ``return_X_y`` is True
 
         .. versionadded:: 0.20
-
-    References
-    ----------
-    .. [1] Analysis and Results of the 1999 DARPA Off-Line Intrusion
-           Detection Evaluation Richard Lippmann, Joshua W. Haines,
-           David J. Fried, Jonathan Korba, Kumar Das
-
-    .. [2] K. Yamanishi, J.-I. Takeuchi, G. Williams, and P. Milne. Online
-           unsupervised outlier detection using finite mixtures with
-           discounting learning algorithms. In Proceedings of the sixth
-           ACM SIGKDD international conference on Knowledge discovery
-           and data mining, pages 320-324. ACM Press, 2000.
-
     """
     data_home = get_data_home(data_home=data_home)
     kddcup99 = _fetch_brute_kddcup99(data_home=data_home,
@@ -236,10 +162,14 @@ def fetch_kddcup99(subset=None, data_home=None, shuffle=False,
     if shuffle:
         data, target = shuffle_method(data, target, random_state=random_state)
 
+    module_path = dirname(__file__)
+    with open(join(module_path, 'descr', 'kddcup99.rst')) as rst_file:
+        fdescr = rst_file.read()
+
     if return_X_y:
         return data, target
 
-    return Bunch(data=data, target=target)
+    return Bunch(data=data, target=target, DESCR=fdescr)
 
 
 def _fetch_brute_kddcup99(data_home=None,
@@ -375,7 +305,7 @@ def _fetch_brute_kddcup99(data_home=None,
         X = joblib.load(samples_path)
         y = joblib.load(targets_path)
 
-    return Bunch(data=X, target=y, DESCR=__doc__)
+    return Bunch(data=X, target=y)
 
 
 def _mkdirp(d):
diff --git a/sklearn/datasets/lfw.py b/sklearn/datasets/lfw.py
index 16fb2c2ef744..cf09f366cafb 100644
--- a/sklearn/datasets/lfw.py
+++ b/sklearn/datasets/lfw.py
@@ -1,37 +1,26 @@
-"""Loader for the Labeled Faces in the Wild (LFW) dataset
+"""Labeled Faces in the Wild (LFW) dataset
 
 This dataset is a collection of JPEG pictures of famous people collected
 over the internet, all details are available on the official website:
 
     http://vis-www.cs.umass.edu/lfw/
-
-Each picture is centered on a single face. The typical task is called
-Face Verification: given a pair of two pictures, a binary classifier
-must predict whether the two images are from the same person.
-
-An alternative task, Face Recognition or Face Identification is:
-given the picture of the face of an unknown person, identify the name
-of the person by referring to a gallery of previously seen pictures of
-identified persons.
-
-Both Face Verification and Face Recognition are tasks that are typically
-performed on the output of a model trained to perform Face Detection. The
-most popular model for Face Detection is called Viola-Johns and is
-implemented in the OpenCV library. The LFW faces were extracted by this face
-detector from various online websites.
 """
 # Copyright (c) 2011 Olivier Grisel <olivier.grisel@ensta.org>
 # License: BSD 3 clause
 
 from os import listdir, makedirs, remove
-from os.path import join, exists, isdir
+from os.path import dirname, join, exists, isdir
 
 import logging
+from distutils.version import LooseVersion
+
 import numpy as np
 
 from .base import get_data_home, _fetch_remote, RemoteFileMetadata
+from ..utils import deprecated
 from ..utils import Bunch
-from ..externals.joblib import Memory
+from ..utils import Memory
+from ..utils._joblib import __version__ as joblib_version
 from ..externals.six import b
 
 logger = logging.getLogger(__name__)
@@ -77,20 +66,36 @@
 )
 
 
+@deprecated('This function was deprecated in version 0.20 and will be removed '
+            'in 0.22.')
 def scale_face(face):
-    """Scale back to 0-1 range in case of normalization for plotting"""
+    """Scale back to 0-1 range in case of normalization for plotting.
+
+    .. deprecated:: 0.20
+    This function was deprecated in version 0.20 and will be removed in 0.22.
+
+
+    Parameters
+    ----------
+    face : array_like
+        The array to scale
+
+    Returns
+    -------
+    array_like
+        The scaled array
+    """
     scaled = face - face.min()
     scaled /= scaled.max()
     return scaled
 
-
 #
 # Common private utilities for data fetching from the original LFW website
 # local disk caching, and image decoding.
 #
 
 
-def check_fetch_lfw(data_home=None, funneled=True, download_if_missing=True):
+def _check_fetch_lfw(data_home=None, funneled=True, download_if_missing=True):
     """Helper function to download any missing LFW data"""
 
     data_home = get_data_home(data_home=data_home)
@@ -239,23 +244,19 @@ def fetch_lfw_people(data_home=None, funneled=True, resize=0.5,
                      min_faces_per_person=0, color=False,
                      slice_=(slice(70, 195), slice(78, 172)),
                      download_if_missing=True, return_X_y=False):
-    """Loader for the Labeled Faces in the Wild (LFW) people dataset
-
-    This dataset is a collection of JPEG pictures of famous people
-    collected on the internet, all details are available on the
-    official website:
+    """Load the Labeled Faces in the Wild (LFW) people dataset \
+(classification).
 
-        http://vis-www.cs.umass.edu/lfw/
+    Download it if necessary.
 
-    Each picture is centered on a single face. Each pixel of each channel
-    (color in RGB) is encoded by a float in range 0.0 - 1.0.
+    =================   =======================
+    Classes                                5749
+    Samples total                         13233
+    Dimensionality                         5828
+    Features            real, between 0 and 255
+    =================   =======================
 
-    The task is called Face Recognition (or Identification): given the
-    picture of a face, find the name of the person given a training set
-    (gallery).
-
-    The original images are 250 x 250 pixels, but the default slice and resize
-    arguments reduce them to 62 x 47.
+    Read more in the :ref:`User Guide <labeled_faces_in_the_wild_dataset>`.
 
     Parameters
     ----------
@@ -320,14 +321,18 @@ def fetch_lfw_people(data_home=None, funneled=True, resize=0.5,
         .. versionadded:: 0.20
 
     """
-    lfw_home, data_folder_path = check_fetch_lfw(
+    lfw_home, data_folder_path = _check_fetch_lfw(
         data_home=data_home, funneled=funneled,
         download_if_missing=download_if_missing)
     logger.debug('Loading LFW people faces from %s', lfw_home)
 
     # wrap the loader in a memoizing function that will return memmaped data
     # arrays for optimal memory usage
-    m = Memory(cachedir=lfw_home, compress=6, verbose=0)
+    if LooseVersion(joblib_version) < LooseVersion('0.12'):
+        # Deal with change of API in joblib
+        m = Memory(cachedir=lfw_home, compress=6, verbose=0)
+    else:
+        m = Memory(location=lfw_home, compress=6, verbose=0)
     load_func = m.cache(_fetch_lfw_people)
 
     # load and memoize the pairs as np arrays
@@ -337,13 +342,17 @@ def fetch_lfw_people(data_home=None, funneled=True, resize=0.5,
 
     X = faces.reshape(len(faces), -1)
 
+    module_path = dirname(__file__)
+    with open(join(module_path, 'descr', 'lfw.rst')) as rst_file:
+        fdescr = rst_file.read()
+
     if return_X_y:
         return X, target
 
     # pack the results as a Bunch instance
     return Bunch(data=X, images=faces,
                  target=target, target_names=target_names,
-                 DESCR="LFW faces dataset")
+                 DESCR=fdescr)
 
 
 #
@@ -405,20 +414,16 @@ def _fetch_lfw_pairs(index_file_path, data_folder_path, slice_=None,
 def fetch_lfw_pairs(subset='train', data_home=None, funneled=True, resize=0.5,
                     color=False, slice_=(slice(70, 195), slice(78, 172)),
                     download_if_missing=True):
-    """Loader for the Labeled Faces in the Wild (LFW) pairs dataset
-
-    This dataset is a collection of JPEG pictures of famous people
-    collected on the internet, all details are available on the
-    official website:
+    """Load the Labeled Faces in the Wild (LFW) pairs dataset (classification).
 
-        http://vis-www.cs.umass.edu/lfw/
+    Download it if necessary.
 
-    Each picture is centered on a single face. Each pixel of each channel
-    (color in RGB) is encoded by a float in range 0.0 - 1.0.
-
-    The task is called Face Verification: given a pair of two pictures,
-    a binary classifier must predict whether the two images are from
-    the same person.
+    =================   =======================
+    Classes                                5749
+    Samples total                         13233
+    Dimensionality                         5828
+    Features            real, between 0 and 255
+    =================   =======================
 
     In the official `README.txt`_ this task is described as the
     "Restricted" task.  As I am not sure as to implement the
@@ -429,7 +434,7 @@ def fetch_lfw_pairs(subset='train', data_home=None, funneled=True, resize=0.5,
     The original images are 250 x 250 pixels, but the default slice and resize
     arguments reduce them to 62 x 47.
 
-    Read more in the :ref:`User Guide <labeled_faces_in_the_wild>`.
+    Read more in the :ref:`User Guide <labeled_faces_in_the_wild_dataset>`.
 
     Parameters
     ----------
@@ -487,14 +492,18 @@ def fetch_lfw_pairs(subset='train', data_home=None, funneled=True, resize=0.5,
         Description of the Labeled Faces in the Wild (LFW) dataset.
 
     """
-    lfw_home, data_folder_path = check_fetch_lfw(
+    lfw_home, data_folder_path = _check_fetch_lfw(
         data_home=data_home, funneled=funneled,
         download_if_missing=download_if_missing)
     logger.debug('Loading %s LFW pairs from %s', subset, lfw_home)
 
     # wrap the loader in a memoizing function that will return memmaped data
     # arrays for optimal memory usage
-    m = Memory(cachedir=lfw_home, compress=6, verbose=0)
+    if LooseVersion(joblib_version) < LooseVersion('0.12'):
+        # Deal with change of API in joblib
+        m = Memory(cachedir=lfw_home, compress=6, verbose=0)
+    else:
+        m = Memory(location=lfw_home, compress=6, verbose=0)
     load_func = m.cache(_fetch_lfw_pairs)
 
     # select the right metadata file according to the requested subset
@@ -513,7 +522,11 @@ def fetch_lfw_pairs(subset='train', data_home=None, funneled=True, resize=0.5,
         index_file_path, data_folder_path, resize=resize, color=color,
         slice_=slice_)
 
+    module_path = dirname(__file__)
+    with open(join(module_path, 'descr', 'lfw.rst')) as rst_file:
+        fdescr = rst_file.read()
+
     # pack the results as a Bunch instance
     return Bunch(data=pairs.reshape(len(pairs), -1), pairs=pairs,
                  target=target, target_names=target_names,
-                 DESCR="'%s' segment of the LFW pairs dataset" % subset)
+                 DESCR=fdescr)
diff --git a/sklearn/datasets/mldata.py b/sklearn/datasets/mldata.py
index 141620858463..5948d04a8be8 100644
--- a/sklearn/datasets/mldata.py
+++ b/sklearn/datasets/mldata.py
@@ -25,13 +25,19 @@
 
 from .base import get_data_home
 from ..utils import Bunch
+from ..utils import deprecated
 
 MLDATA_BASE_URL = "http://mldata.org/repository/data/download/matlab/%s"
 
 
+@deprecated('mldata_filename was deprecated in version 0.20 and will be '
+            'removed in version 0.22')
 def mldata_filename(dataname):
     """Convert a raw name for a data set in a mldata.org filename.
 
+    .. deprecated:: 0.20
+        Will be removed in version 0.22
+
     Parameters
     ----------
     dataname : str
@@ -46,10 +52,14 @@ def mldata_filename(dataname):
     return re.sub(r'[().]', '', dataname)
 
 
+@deprecated('fetch_mldata was deprecated in version 0.20 and will be removed '
+            'in version 0.22')
 def fetch_mldata(dataname, target_name='label', data_name='data',
                  transpose_data=True, data_home=None):
     """Fetch an mldata.org data set
 
+    mldata.org is no longer operational.
+
     If the file does not exist yet, it is downloaded from mldata.org .
 
     mldata.org does not have an enforced convention for storing data or
@@ -70,6 +80,9 @@ def fetch_mldata(dataname, target_name='label', data_name='data',
     mldata.org data sets may have multiple columns, which are stored in the
     Bunch object with their original name.
 
+    .. deprecated:: 0.20
+        Will be removed in version 0.22
+
     Parameters
     ----------
 
@@ -99,40 +112,6 @@ def fetch_mldata(dataname, target_name='label', data_name='data',
         'data', the data to learn, 'target', the classification labels,
         'DESCR', the full description of the dataset, and
         'COL_NAMES', the original names of the dataset columns.
-
-    Examples
-    --------
-    Load the 'iris' dataset from mldata.org:
-
-    >>> from sklearn.datasets.mldata import fetch_mldata
-    >>> import tempfile
-    >>> test_data_home = tempfile.mkdtemp()
-
-    >>> iris = fetch_mldata('iris', data_home=test_data_home)
-    >>> iris.target.shape
-    (150,)
-    >>> iris.data.shape
-    (150, 4)
-
-    Load the 'leukemia' dataset from mldata.org, which needs to be transposed
-    to respects the scikit-learn axes convention:
-
-    >>> leuk = fetch_mldata('leukemia', transpose_data=True,
-    ...                     data_home=test_data_home)
-    >>> leuk.data.shape
-    (72, 7129)
-
-    Load an alternative 'iris' dataset, which has different names for the
-    columns:
-
-    >>> iris2 = fetch_mldata('datasets-UCI iris', target_name=1,
-    ...                      data_name=0, data_home=test_data_home)
-    >>> iris3 = fetch_mldata('datasets-UCI iris',
-    ...                      target_name='class', data_name='double0',
-    ...                      data_home=test_data_home)
-
-    >>> import shutil
-    >>> shutil.rmtree(test_data_home)
     """
 
     # normalize dataset name
diff --git a/sklearn/datasets/olivetti_faces.py b/sklearn/datasets/olivetti_faces.py
index fd1bea512840..74915c6c6957 100644
--- a/sklearn/datasets/olivetti_faces.py
+++ b/sklearn/datasets/olivetti_faces.py
@@ -1,6 +1,6 @@
 """Modified Olivetti faces dataset.
 
-The original database was available from
+The original database was available from (now defunct)
 
     http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html
 
@@ -8,21 +8,12 @@
 web page of Sam Roweis:
 
     http://www.cs.nyu.edu/~roweis/
-
-There are ten different images of each of 40 distinct subjects. For some
-subjects, the images were taken at different times, varying the lighting,
-facial expressions (open / closed eyes, smiling / not smiling) and facial
-details (glasses / no glasses). All the images were taken against a dark
-homogeneous background with the subjects in an upright, frontal position (with
-tolerance for some side movement).
-
-The original dataset consisted of 92 x 112, while the Roweis version
-consists of 64x64 images.
 """
+
 # Copyright (c) 2011 David Warde-Farley <wardefar at iro dot umontreal dot ca>
 # License: BSD 3 clause
 
-from os.path import exists
+from os.path import dirname, exists, join
 from os import makedirs, remove
 
 import numpy as np
@@ -43,16 +34,21 @@
     checksum=('b612fb967f2dc77c9c62d3e1266e0c73'
               'd5fca46a4b8906c18e454d41af987794'))
 
-# Grab the module-level docstring to use as a description of the
-# dataset
-MODULE_DOCS = __doc__
-
 
 def fetch_olivetti_faces(data_home=None, shuffle=False, random_state=0,
                          download_if_missing=True):
-    """Loader for the Olivetti faces data-set from AT&T.
+    """Load the Olivetti faces data-set from AT&T (classification).
 
-    Read more in the :ref:`User Guide <olivetti_faces>`.
+    Download it if necessary.
+
+    =================   =====================
+    Classes                                40
+    Samples total                         400
+    Dimensionality                       4096
+    Features            real, between 0 and 1
+    =================   =====================
+
+    Read more in the :ref:`User Guide <olivetti_faces_dataset>`.
 
     Parameters
     ----------
@@ -91,20 +87,6 @@ def fetch_olivetti_faces(data_home=None, shuffle=False, random_state=0,
 
     DESCR : string
         Description of the modified Olivetti Faces Dataset.
-
-    Notes
-    ------
-
-    This dataset consists of 10 pictures each of 40 individuals. The original
-    database was available from (now defunct)
-
-        http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html
-
-    The version retrieved here comes in MATLAB format from the personal
-    web page of Sam Roweis:
-
-        http://www.cs.nyu.edu/~roweis/
-
     """
     data_home = get_data_home(data_home=data_home)
     if not exists(data_home):
@@ -140,7 +122,12 @@ def fetch_olivetti_faces(data_home=None, shuffle=False, random_state=0,
         order = random_state.permutation(len(faces))
         faces = faces[order]
         target = target[order]
+
+    module_path = dirname(__file__)
+    with open(join(module_path, 'descr', 'covtype.rst')) as rst_file:
+        fdescr = rst_file.read()
+
     return Bunch(data=faces.reshape(len(faces), -1),
                  images=faces,
                  target=target,
-                 DESCR=MODULE_DOCS)
+                 DESCR=fdescr)
diff --git a/sklearn/datasets/openml.py b/sklearn/datasets/openml.py
new file mode 100644
index 000000000000..a35fa5130799
--- /dev/null
+++ b/sklearn/datasets/openml.py
@@ -0,0 +1,575 @@
+import gzip
+import json
+import os
+import shutil
+from os.path import join
+from warnings import warn
+
+try:
+    # Python 3+
+    from urllib.request import urlopen
+except ImportError:
+    # Python 2
+    from urllib2 import urlopen
+
+
+import numpy as np
+import scipy.sparse
+
+from sklearn.externals import _arff
+from .base import get_data_home
+from ..externals.six import string_types, PY2
+from ..externals.six.moves.urllib.error import HTTPError
+from ..utils import Bunch
+
+__all__ = ['fetch_openml']
+
+_OPENML_PREFIX = "https://openml.org/"
+_SEARCH_NAME = "api/v1/json/data/list/data_name/{}/limit/2"
+_DATA_INFO = "api/v1/json/data/{}"
+_DATA_FEATURES = "api/v1/json/data/features/{}"
+_DATA_FILE = "data/v1/download/{}"
+
+
+def _open_openml_url(openml_path, data_home):
+    """
+    Returns a resource from OpenML.org. Caches it to data_home if required.
+
+    Parameters
+    ----------
+    openml_path : str
+        OpenML URL that will be accessed. This will be prefixes with
+        _OPENML_PREFIX
+
+    data_home : str
+        Directory to which the files will be cached. If None, no caching will
+        be applied.
+
+    Returns
+    -------
+    result : stream
+        A stream to the OpenML resource
+    """
+    if data_home is None:
+        return urlopen(_OPENML_PREFIX + openml_path)
+    local_path = os.path.join(data_home, 'openml.org', openml_path + ".gz")
+    if not os.path.exists(local_path):
+        try:
+            os.makedirs(os.path.dirname(local_path))
+        except OSError:
+            # potentially, the directory has been created already
+            pass
+
+        try:
+            with gzip.GzipFile(local_path, 'wb') as fdst:
+                fsrc = urlopen(_OPENML_PREFIX + openml_path)
+                shutil.copyfileobj(fsrc, fdst)
+                fsrc.close()
+        except Exception:
+            os.unlink(local_path)
+            raise
+    # XXX: unnecessary decompression on first access
+    return gzip.GzipFile(local_path, 'rb')
+
+
+def _get_json_content_from_openml_api(url, error_message, raise_if_error,
+                                      data_home):
+    """
+    Loads json data from the openml api
+
+    Parameters
+    ----------
+    url : str
+        The URL to load from. Should be an official OpenML endpoint
+
+    error_message : str or None
+        The error message to raise if an acceptable OpenML error is thrown
+        (acceptable error is, e.g., data id not found. Other errors, like 404's
+        will throw the native error message)
+
+    raise_if_error : bool
+        Whether to raise an error if OpenML returns an acceptable error (e.g.,
+        date not found). If this argument is set to False, a None is returned
+        in case of acceptable errors. Note that all other errors (e.g., 404)
+        will still be raised as normal.
+
+    data_home : str or None
+        Location to cache the response. None if no cache is required.
+
+    Returns
+    -------
+    json_data : json or None
+        the json result from the OpenML server if the call was successful;
+        None otherwise iff raise_if_error was set to False and the error was
+        ``acceptable``
+    """
+    data_found = True
+    try:
+        response = _open_openml_url(url, data_home)
+    except HTTPError as error:
+        # 412 is an OpenML specific error code, indicating a generic error
+        # (e.g., data not found)
+        if error.code == 412:
+            data_found = False
+        else:
+            raise error
+    if not data_found:
+        # not in except for nicer traceback
+        if raise_if_error:
+            raise ValueError(error_message)
+        else:
+            return None
+    json_data = json.loads(response.read().decode("utf-8"))
+    response.close()
+    return json_data
+
+
+def _split_sparse_columns(arff_data, include_columns):
+    """
+    obtains several columns from sparse arff representation. Additionally, the
+    column indices are re-labelled, given the columns that are not included.
+    (e.g., when including [1, 2, 3], the columns will be relabelled to
+    [0, 1, 2])
+
+    Parameters
+    ----------
+    arff_data : tuple
+        A tuple of three lists of equal size; first list indicating the value,
+        second the x coordinate and the third the y coordinate.
+
+    include_columns : list
+        A list of columns to include.
+
+    Returns
+    -------
+    arff_data_new : tuple
+        Subset of arff data with only the include columns indicated by the
+        include_columns argument.
+    """
+    arff_data_new = (list(), list(), list())
+    reindexed_columns = {column_idx: array_idx for array_idx, column_idx
+                         in enumerate(include_columns)}
+    for val, row_idx, col_idx in zip(arff_data[0], arff_data[1], arff_data[2]):
+        if col_idx in include_columns:
+            arff_data_new[0].append(val)
+            arff_data_new[1].append(row_idx)
+            arff_data_new[2].append(reindexed_columns[col_idx])
+    return arff_data_new
+
+
+def _sparse_data_to_array(arff_data, include_columns):
+    # turns the sparse data back into an array (can't use toarray() function,
+    # as this does only work on numeric data)
+    num_obs = max(arff_data[1]) + 1
+    y_shape = (num_obs, len(include_columns))
+    reindexed_columns = {column_idx: array_idx for array_idx, column_idx
+                         in enumerate(include_columns)}
+    # TODO: improve for efficiency
+    y = np.empty(y_shape, dtype=np.float64)
+    for val, row_idx, col_idx in zip(arff_data[0], arff_data[1], arff_data[2]):
+        if col_idx in include_columns:
+            y[row_idx, reindexed_columns[col_idx]] = val
+    return y
+
+
+def _convert_arff_data(arff_data, col_slice_x, col_slice_y):
+    """
+    converts the arff object into the appropriate matrix type (np.array or
+    scipy.sparse.csr_matrix) based on the 'data part' (i.e., in the
+    liac-arff dict, the object from the 'data' key)
+
+    Parameters
+    ----------
+    arff_data : list or dict
+        as obtained from liac-arff object
+
+    col_slice_x : list
+        The column indices that are sliced from the original array to return
+        as X data
+
+    col_slice_y : list
+        The column indices that are sliced from the original array to return
+        as y data
+
+    Returns
+    -------
+    X : np.array or scipy.sparse.csr_matrix
+    y : np.array
+    """
+    if isinstance(arff_data, list):
+        data = np.array(arff_data, dtype=np.float64)
+        X = np.array(data[:, col_slice_x], dtype=np.float64)
+        y = np.array(data[:, col_slice_y], dtype=np.float64)
+        return X, y
+    elif isinstance(arff_data, tuple):
+        arff_data_X = _split_sparse_columns(arff_data, col_slice_x)
+        num_obs = max(arff_data[1]) + 1
+        X_shape = (num_obs, len(col_slice_x))
+        X = scipy.sparse.coo_matrix(
+            (arff_data_X[0], (arff_data_X[1], arff_data_X[2])),
+            shape=X_shape, dtype=np.float64)
+        X = X.tocsr()
+        y = _sparse_data_to_array(arff_data, col_slice_y)
+        return X, y
+    else:
+        # This should never happen
+        raise ValueError('Unexpected Data Type obtained from arff.')
+
+
+def _get_data_info_by_name(name, version, data_home):
+    """
+    Utilizes the openml dataset listing api to find a dataset by
+    name/version
+    OpenML api function:
+    https://www.openml.org/api_docs#!/data/get_data_list_data_name_data_name
+
+    Parameters
+    ----------
+    name : str
+        name of the dataset
+
+    version : int or str
+        If version is an integer, the exact name/version will be obtained from
+        OpenML. If version is a string (value: "active") it will take the first
+        version from OpenML that is annotated as active. Any other string
+        values except "active" are treated as integer.
+
+    data_home : str or None
+        Location to cache the response. None if no cache is required.
+
+    Returns
+    -------
+    first_dataset : json
+        json representation of the first dataset object that adhired to the
+        search criteria
+
+    """
+    if version == "active":
+        # situation in which we return the oldest active version
+        url = _SEARCH_NAME.format(name) + "/status/active/"
+        error_msg = "No active dataset {} found.".format(name)
+        json_data = _get_json_content_from_openml_api(url, error_msg, True,
+                                                      data_home)
+        res = json_data['data']['dataset']
+        if len(res) > 1:
+            warn("Multiple active versions of the dataset matching the name"
+                 " {name} exist. Versions may be fundamentally different, "
+                 "returning version"
+                 " {version}.".format(name=name, version=res[0]['version']))
+        return res[0]
+
+    # an integer version has been provided
+    url = (_SEARCH_NAME + "/data_version/{}").format(name, version)
+    json_data = _get_json_content_from_openml_api(url, None, False,
+                                                  data_home)
+    if json_data is None:
+        # we can do this in 1 function call if OpenML does not require the
+        # specification of the dataset status (i.e., return datasets with a
+        # given name / version regardless of active, deactivated, etc. )
+        # TODO: feature request OpenML.
+        url += "/status/deactivated"
+        error_msg = "Dataset {} with version {} not found.".format(name,
+                                                                   version)
+        json_data = _get_json_content_from_openml_api(url, error_msg, True,
+                                                      data_home)
+
+    return json_data['data']['dataset'][0]
+
+
+def _get_data_description_by_id(data_id, data_home):
+    # OpenML API function: https://www.openml.org/api_docs#!/data/get_data_id
+    url = _DATA_INFO.format(data_id)
+    error_message = "Dataset with data_id {} not found.".format(data_id)
+    json_data = _get_json_content_from_openml_api(url, error_message, True,
+                                                  data_home)
+    return json_data['data_set_description']
+
+
+def _get_data_features(data_id, data_home):
+    # OpenML function:
+    # https://www.openml.org/api_docs#!/data/get_data_features_id
+    url = _DATA_FEATURES.format(data_id)
+    error_message = "Dataset with data_id {} not found.".format(data_id)
+    json_data = _get_json_content_from_openml_api(url, error_message, True,
+                                                  data_home)
+    return json_data['data_features']['feature']
+
+
+def _download_data_arff(file_id, sparse, data_home, encode_nominal=True):
+    # Accesses an ARFF file on the OpenML server. Documentation:
+    # https://www.openml.org/api_data_docs#!/data/get_download_id
+    # encode_nominal argument is to ensure unit testing, do not alter in
+    # production!
+    url = _DATA_FILE.format(file_id)
+    response = _open_openml_url(url, data_home)
+    if sparse is True:
+        return_type = _arff.COO
+    else:
+        return_type = _arff.DENSE
+
+    if PY2:
+        arff_file = _arff.load(response, encode_nominal=encode_nominal,
+                               return_type=return_type, )
+    else:
+        arff_file = _arff.loads(response.read().decode('utf-8'),
+                                encode_nominal=encode_nominal,
+                                return_type=return_type)
+    response.close()
+    return arff_file
+
+
+def _verify_target_data_type(features_dict, target_columns):
+    # verifies the data type of the y array in case there are multiple targets
+    # (throws an error if these targets do not comply with sklearn support)
+    if not isinstance(target_columns, list):
+        raise ValueError('target_column should be list, '
+                         'got: %s' % type(target_columns))
+    found_types = set()
+    for target_column in target_columns:
+        if target_column not in features_dict:
+            raise KeyError('Could not find target_column={}')
+        if features_dict[target_column]['data_type'] == "numeric":
+            found_types.add(np.float64)
+        else:
+            found_types.add(object)
+
+        # note: we compare to a string, not boolean
+        if features_dict[target_column]['is_ignore'] == 'true':
+            warn('target_column={} has flag is_ignore.'.format(
+                target_column))
+        if features_dict[target_column]['is_row_identifier'] == 'true':
+            warn('target_column={} has flag is_row_identifier.'.format(
+                target_column))
+    if len(found_types) > 1:
+        raise ValueError('Can only handle homogeneous multi-target datasets, '
+                         'i.e., all targets are either numeric or '
+                         'categorical.')
+
+
+def fetch_openml(name=None, version='active', data_id=None, data_home=None,
+                 target_column='default-target', cache=True, return_X_y=False):
+    """Fetch dataset from openml by name or dataset id.
+
+    Datasets are uniquely identified by either an integer ID or by a
+    combination of name and version (i.e. there might be multiple
+    versions of the 'iris' dataset). Please give either name or data_id
+    (not both). In case a name is given, a version can also be
+    provided.
+
+    .. note:: EXPERIMENTAL
+
+        The API is experimental in version 0.20 (particularly the return value
+        structure), and might have small backward-incompatible changes in
+        future releases.
+
+    Parameters
+    ----------
+    name : str or None
+        String identifier of the dataset. Note that OpenML can have multiple
+        datasets with the same name.
+
+    version : integer or 'active', default='active'
+        Version of the dataset. Can only be provided if also ``name`` is given.
+        If 'active' the oldest version that's still active is used. Since
+        there may be more than one active version of a dataset, and those
+        versions may fundamentally be different from one another, setting an
+        exact version is highly recommended.
+
+    data_id : int or None
+        OpenML ID of the dataset. The most specific way of retrieving a
+        dataset. If data_id is not given, name (and potential version) are
+        used to obtain a dataset.
+
+    data_home : string or None, default None
+        Specify another download and cache folder for the data sets. By default
+        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.
+
+    target_column : string, list or None, default 'default-target'
+        Specify the column name in the data to use as target. If
+        'default-target', the standard target column a stored on the server
+        is used. If ``None``, all columns are returned as data and the
+        target is ``None``. If list (of strings), all columns with these names
+        are returned as multi-target (Note: not all scikit-learn classifiers
+        can handle all types of multi-output combinations)
+
+    cache : boolean, default=True
+        Whether to cache downloaded datasets using joblib.
+
+    return_X_y : boolean, default=False.
+        If True, returns ``(data, target)`` instead of a Bunch object. See
+        below for more information about the `data` and `target` objects.
+
+    Returns
+    -------
+
+    data : Bunch
+        Dictionary-like object, with attributes:
+
+        data : np.array or scipy.sparse.csr_matrix of floats
+            The feature matrix. Categorical features are encoded as ordinals.
+        target : np.array
+            The regression target or classification labels, if applicable.
+            Dtype is float if numeric, and object if categorical.
+        DESCR : str
+            The full description of the dataset
+        feature_names : list
+            The names of the dataset columns
+        categories : dict
+            Maps each categorical feature name to a list of values, such
+            that the value encoded as i is ith in the list.
+        details : dict
+            More metadata from OpenML
+
+    (data, target) : tuple if ``return_X_y`` is True
+
+        .. note:: EXPERIMENTAL
+
+            This interface is **experimental** as at version 0.20 and
+            subsequent releases may change attributes without notice
+            (although there should only be minor changes to ``data``
+            and ``target``).
+
+        Missing values in the 'data' are represented as NaN's. Missing values
+        in 'target' are represented as NaN's (numerical target) or None
+        (categorical target)
+    """
+    data_home = get_data_home(data_home=data_home)
+    data_home = join(data_home, 'openml')
+    if cache is False:
+        # no caching will be applied
+        data_home = None
+
+    # check valid function arguments. data_id XOR (name, version) should be
+    # provided
+    if name is not None:
+        # OpenML is case-insensitive, but the caching mechanism is not
+        # convert all data names (str) to lower case
+        name = name.lower()
+        if data_id is not None:
+            raise ValueError(
+                "Dataset data_id={} and name={} passed, but you can only "
+                "specify a numeric data_id or a name, not "
+                "both.".format(data_id, name))
+        data_info = _get_data_info_by_name(name, version, data_home)
+        data_id = data_info['did']
+    elif data_id is not None:
+        # from the previous if statement, it is given that name is None
+        if version is not "active":
+            raise ValueError(
+                "Dataset data_id={} and version={} passed, but you can only "
+                "specify a numeric data_id or a version, not "
+                "both.".format(data_id, name))
+    else:
+        raise ValueError(
+            "Neither name nor data_id are provided. Please provide name or "
+            "data_id.")
+
+    data_description = _get_data_description_by_id(data_id, data_home)
+    if data_description['status'] != "active":
+        warn("Version {} of dataset {} is inactive, meaning that issues have "
+             "been found in the dataset. Try using a newer version from "
+             "this URL: {}".format(
+                data_description['version'],
+                data_description['name'],
+                data_description['url']))
+
+    # download data features, meta-info about column types
+    features_list = _get_data_features(data_id, data_home)
+
+    for feature in features_list:
+        if 'true' in (feature['is_ignore'], feature['is_row_identifier']):
+            continue
+        if feature['data_type'] == 'string':
+            raise ValueError('STRING attributes are not yet supported')
+
+    if target_column == "default-target":
+        # determines the default target based on the data feature results
+        # (which is currently more reliable than the data description;
+        # see issue: https://github.com/openml/OpenML/issues/768)
+        target_column = [feature['name'] for feature in features_list
+                         if feature['is_target'] == 'true']
+    elif isinstance(target_column, string_types):
+        # for code-simplicity, make target_column by default a list
+        target_column = [target_column]
+    elif target_column is None:
+        target_column = []
+    elif not isinstance(target_column, list):
+        raise TypeError("Did not recognize type of target_column"
+                        "Should be six.string_type, list or None. Got: "
+                        "{}".format(type(target_column)))
+    data_columns = [feature['name'] for feature in features_list
+                    if (feature['name'] not in target_column and
+                        feature['is_ignore'] != 'true' and
+                        feature['is_row_identifier'] != 'true')]
+
+    # prepare which columns and data types should be returned for the X and y
+    features_dict = {feature['name']: feature for feature in features_list}
+
+    # XXX: col_slice_y should be all nominal or all numeric
+    _verify_target_data_type(features_dict, target_column)
+
+    col_slice_y = [int(features_dict[col_name]['index'])
+                   for col_name in target_column]
+
+    col_slice_x = [int(features_dict[col_name]['index'])
+                   for col_name in data_columns]
+    for col_idx in col_slice_y:
+        feat = features_list[col_idx]
+        nr_missing = int(feat['number_of_missing_values'])
+        if nr_missing > 0:
+            raise ValueError('Target column {} has {} missing values. '
+                             'Missing values are not supported for target '
+                             'columns. '.format(feat['name'], nr_missing))
+
+    # determine arff encoding to return
+    return_sparse = False
+    if data_description['format'].lower() == 'sparse_arff':
+        return_sparse = True
+
+    # obtain the data
+    arff = _download_data_arff(data_description['file_id'], return_sparse,
+                               data_home)
+    arff_data = arff['data']
+    nominal_attributes = {k: v for k, v in arff['attributes']
+                          if isinstance(v, list)}
+    for feature in features_list:
+        if 'true' in (feature['is_row_identifier'],
+                      feature['is_ignore']) and (feature['name'] not in
+                                                 target_column):
+            del nominal_attributes[feature['name']]
+    X, y = _convert_arff_data(arff_data, col_slice_x, col_slice_y)
+
+    is_classification = {col_name in nominal_attributes
+                         for col_name in target_column}
+    if not is_classification:
+        # No target
+        pass
+    elif all(is_classification):
+        y = np.hstack([np.take(np.asarray(nominal_attributes.pop(col_name),
+                                          dtype='O'),
+                               y[:, i:i+1].astype(int))
+                       for i, col_name in enumerate(target_column)])
+    elif any(is_classification):
+        raise ValueError('Mix of nominal and non-nominal targets is not '
+                         'currently supported')
+
+    description = u"{}\n\nDownloaded from openml.org.".format(
+        data_description.pop('description'))
+
+    # reshape y back to 1-D array, if there is only 1 target column; back
+    # to None if there are not target columns
+    if y.shape[1] == 1:
+        y = y.reshape((-1,))
+    elif y.shape[1] == 0:
+        y = None
+
+    if return_X_y:
+        return X, y
+
+    bunch = Bunch(
+        data=X, target=y, feature_names=data_columns,
+        DESCR=description, details=data_description,
+        categories=nominal_attributes,
+        url="https://www.openml.org/d/{}".format(data_id))
+
+    return bunch
diff --git a/sklearn/datasets/rcv1.py b/sklearn/datasets/rcv1.py
index b0ef91972a72..7890d7e18a88 100644
--- a/sklearn/datasets/rcv1.py
+++ b/sklearn/datasets/rcv1.py
@@ -1,4 +1,8 @@
 """RCV1 dataset.
+
+The dataset page is available at
+
+    http://jmlr.csail.mit.edu/papers/volume5/lewis04a/
 """
 
 # Author: Tom Dupre la Tour
@@ -7,7 +11,7 @@
 import logging
 
 from os import remove
-from os.path import exists, join
+from os.path import dirname, exists, join
 from gzip import GzipFile
 
 import numpy as np
@@ -74,18 +78,20 @@
 
 def fetch_rcv1(data_home=None, subset='all', download_if_missing=True,
                random_state=None, shuffle=False, return_X_y=False):
-    """Load the RCV1 multilabel dataset, downloading it if necessary.
+    """Load the RCV1 multilabel dataset (classification).
+
+    Download it if necessary.
 
     Version: RCV1-v2, vectors, full sets, topics multilabels.
 
-    ==============     =====================
-    Classes                              103
-    Samples total                     804414
-    Dimensionality                     47236
-    Features           real, between 0 and 1
-    ==============     =====================
+    =================   =====================
+    Classes                               103
+    Samples total                      804414
+    Dimensionality                      47236
+    Features            real, between 0 and 1
+    =================   =====================
 
-    Read more in the :ref:`User Guide <datasets>`.
+    Read more in the :ref:`User Guide <rcv1_dataset>`.
 
     .. versionadded:: 0.17
 
@@ -143,13 +149,6 @@ def fetch_rcv1(data_home=None, subset='all', download_if_missing=True,
     (data, target) : tuple if ``return_X_y`` is True
 
         .. versionadded:: 0.20
-
-    References
-    ----------
-    Lewis, D. D., Yang, Y., Rose, T. G., & Li, F. (2004). RCV1: A new
-    benchmark collection for text categorization research. The Journal of
-    Machine Learning Research, 5, 361-397.
-
     """
     N_SAMPLES = 804414
     N_FEATURES = 47236
@@ -265,11 +264,15 @@ def fetch_rcv1(data_home=None, subset='all', download_if_missing=True,
     if shuffle:
         X, y, sample_id = shuffle_(X, y, sample_id, random_state=random_state)
 
+    module_path = dirname(__file__)
+    with open(join(module_path, 'descr', 'rcv1.rst')) as rst_file:
+        fdescr = rst_file.read()
+
     if return_X_y:
         return X, y
 
     return Bunch(data=X, target=y, sample_id=sample_id,
-                 target_names=categories, DESCR=__doc__)
+                 target_names=categories, DESCR=fdescr)
 
 
 def _inverse_permutation(p):
diff --git a/sklearn/datasets/samples_generator.py b/sklearn/datasets/samples_generator.py
index 5cd54b438f13..04415f799bc7 100644
--- a/sklearn/datasets/samples_generator.py
+++ b/sklearn/datasets/samples_generator.py
@@ -11,11 +11,11 @@
 import numpy as np
 from scipy import linalg
 import scipy.sparse as sp
-from collections import Iterable
 
 from ..preprocessing import MultiLabelBinarizer
 from ..utils import check_array, check_random_state
 from ..utils import shuffle as util_shuffle
+from ..utils.fixes import _Iterable as Iterable
 from ..utils.random import sample_without_replacement
 from ..externals import six
 map = six.moves.map
@@ -807,7 +807,7 @@ def make_blobs(n_samples=100, n_features=2, centers=None, cluster_std=1.0,
                          "and cluster_std = {}".format(centers, cluster_std))
 
     if isinstance(cluster_std, numbers.Real):
-        cluster_std = np.ones(len(centers)) * cluster_std
+        cluster_std = np.full(len(centers), cluster_std)
 
     X = []
     y = []
diff --git a/sklearn/datasets/setup.py b/sklearn/datasets/setup.py
index a1def76c1bfc..3a8936bedffe 100644
--- a/sklearn/datasets/setup.py
+++ b/sklearn/datasets/setup.py
@@ -1,6 +1,7 @@
 
 import numpy
 import os
+import platform
 
 
 def configuration(parent_package='', top_path=None):
@@ -10,9 +11,10 @@ def configuration(parent_package='', top_path=None):
     config.add_data_dir('descr')
     config.add_data_dir('images')
     config.add_data_dir(os.path.join('tests', 'data'))
-    config.add_extension('_svmlight_format',
-                         sources=['_svmlight_format.pyx'],
-                         include_dirs=[numpy.get_include()])
+    if platform.python_implementation() != 'PyPy':
+        config.add_extension('_svmlight_format',
+                             sources=['_svmlight_format.pyx'],
+                             include_dirs=[numpy.get_include()])
     config.add_subpackage('tests')
     return config
 
diff --git a/sklearn/datasets/species_distributions.py b/sklearn/datasets/species_distributions.py
index d18af1806a31..6d8acddccb39 100644
--- a/sklearn/datasets/species_distributions.py
+++ b/sklearn/datasets/species_distributions.py
@@ -170,14 +170,14 @@ def fetch_species_distributions(data_home=None,
         The latitude/longitude values for the grid are discussed below.
         Missing data is represented by the value -9999.
 
-    train : record array, shape = (1623,)
+    train : record array, shape = (1624,)
         The training points for the data.  Each point has three fields:
 
         - train['species'] is the species name
         - train['dd long'] is the longitude, in degrees
         - train['dd lat'] is the latitude, in degrees
 
-    test : record array, shape = (619,)
+    test : record array, shape = (620,)
         The test points for the data.  Same format as the training data.
 
     Nx, Ny : integers
diff --git a/sklearn/datasets/svmlight_format.py b/sklearn/datasets/svmlight_format.py
index bf14edabea49..42de5943b6d5 100644
--- a/sklearn/datasets/svmlight_format.py
+++ b/sklearn/datasets/svmlight_format.py
@@ -22,12 +22,21 @@
 import numpy as np
 import scipy.sparse as sp
 
-from ._svmlight_format import _load_svmlight_file
 from .. import __version__
 from ..externals import six
 from ..externals.six import u, b
 from ..externals.six.moves import range, zip
-from ..utils import check_array
+from ..utils import check_array, IS_PYPY
+
+if not IS_PYPY:
+    from ._svmlight_format import _load_svmlight_file
+else:
+    def _load_svmlight_file(*args, **kwargs):
+        raise NotImplementedError(
+                'load_svmlight_file is currently not '
+                'compatible with PyPy (see '
+                'https://github.com/scikit-learn/scikit-learn/issues/11543 '
+                'for the status updates).')
 
 
 def load_svmlight_file(f, n_features=None, dtype=np.float64,
@@ -132,7 +141,7 @@ def load_svmlight_file(f, n_features=None, dtype=np.float64,
     --------
     To use joblib.Memory to cache the svmlight file::
 
-        from sklearn.externals.joblib import Memory
+        from sklearn.utils import Memory
         from sklearn.datasets import load_svmlight_file
         mem = Memory("./mycache")
 
diff --git a/sklearn/datasets/tests/data/openml/2/api-v1-json-data-2.json.gz b/sklearn/datasets/tests/data/openml/2/api-v1-json-data-2.json.gz
new file mode 100644
index 000000000000..22dfb6ff61c1
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/2/api-v1-json-data-2.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/2/api-v1-json-data-features-2.json.gz b/sklearn/datasets/tests/data/openml/2/api-v1-json-data-features-2.json.gz
new file mode 100644
index 000000000000..cb3d2750095b
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/2/api-v1-json-data-features-2.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/2/api-v1-json-data-list-data_name-anneal-limit-2-data_version-1.json.gz b/sklearn/datasets/tests/data/openml/2/api-v1-json-data-list-data_name-anneal-limit-2-data_version-1.json.gz
new file mode 100644
index 000000000000..a95a8131dde4
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/2/api-v1-json-data-list-data_name-anneal-limit-2-data_version-1.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/2/api-v1-json-data-list-data_name-anneal-limit-2-status-active-.json.gz b/sklearn/datasets/tests/data/openml/2/api-v1-json-data-list-data_name-anneal-limit-2-status-active-.json.gz
new file mode 100644
index 000000000000..e85c1b5ff9d8
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/2/api-v1-json-data-list-data_name-anneal-limit-2-status-active-.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/2/data-v1-download-1666876.arff.gz b/sklearn/datasets/tests/data/openml/2/data-v1-download-1666876.arff.gz
new file mode 100644
index 000000000000..cdf3254add76
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/2/data-v1-download-1666876.arff.gz differ
diff --git a/sklearn/datasets/tests/data/openml/292/api-v1-json-data-292.json.gz b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-292.json.gz
new file mode 100644
index 000000000000..888140f92b36
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-292.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/292/api-v1-json-data-40981.json.gz b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-40981.json.gz
new file mode 100644
index 000000000000..888140f92b36
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-40981.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/292/api-v1-json-data-features-292.json.gz b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-features-292.json.gz
new file mode 100644
index 000000000000..29016cc36bab
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-features-292.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/292/api-v1-json-data-features-40981.json.gz b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-features-40981.json.gz
new file mode 100644
index 000000000000..29016cc36bab
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-features-40981.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/292/api-v1-json-data-list-data_name-australian-limit-2-data_version-1-status-deactivated.json.gz b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-list-data_name-australian-limit-2-data_version-1-status-deactivated.json.gz
new file mode 100644
index 000000000000..8cb61626e1bb
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-list-data_name-australian-limit-2-data_version-1-status-deactivated.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/292/api-v1-json-data-list-data_name-australian-limit-2-data_version-1.json.gz b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-list-data_name-australian-limit-2-data_version-1.json.gz
new file mode 100644
index 000000000000..0e2c4395f1c2
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-list-data_name-australian-limit-2-data_version-1.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/292/api-v1-json-data-list-data_name-australian-limit-2-status-active-.json.gz b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-list-data_name-australian-limit-2-status-active-.json.gz
new file mode 100644
index 000000000000..b91949b9e48b
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-list-data_name-australian-limit-2-status-active-.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/292/data-v1-download-49822.arff.gz b/sklearn/datasets/tests/data/openml/292/data-v1-download-49822.arff.gz
new file mode 100644
index 000000000000..6821829e1e43
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/292/data-v1-download-49822.arff.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-40589.json.gz b/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-40589.json.gz
new file mode 100644
index 000000000000..9c71553ce513
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-40589.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-features-40589.json.gz b/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-features-40589.json.gz
new file mode 100644
index 000000000000..155460906a7b
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-features-40589.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-list-data_name-emotions-limit-2-data_version-3.json.gz b/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-list-data_name-emotions-limit-2-data_version-3.json.gz
new file mode 100644
index 000000000000..01e6648a91ce
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-list-data_name-emotions-limit-2-data_version-3.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-list-data_name-emotions-limit-2-status-active-.json.gz b/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-list-data_name-emotions-limit-2-status-active-.json.gz
new file mode 100644
index 000000000000..0fc8d5ba1f7e
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-list-data_name-emotions-limit-2-status-active-.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40589/data-v1-download-4644182.arff.gz b/sklearn/datasets/tests/data/openml/40589/data-v1-download-4644182.arff.gz
new file mode 100644
index 000000000000..96ed11d96955
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40589/data-v1-download-4644182.arff.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-40675.json.gz b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-40675.json.gz
new file mode 100644
index 000000000000..42b876f0a472
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-40675.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-features-40675.json.gz b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-features-40675.json.gz
new file mode 100644
index 000000000000..2d5c6f8a302e
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-features-40675.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-list-data_name-glass2-limit-2-data_version-1-status-deactivated.json.gz b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-list-data_name-glass2-limit-2-data_version-1-status-deactivated.json.gz
new file mode 100644
index 000000000000..f038de419649
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-list-data_name-glass2-limit-2-data_version-1-status-deactivated.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-list-data_name-glass2-limit-2-data_version-1.json.gz b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-list-data_name-glass2-limit-2-data_version-1.json.gz
new file mode 100644
index 000000000000..df1665b1db71
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-list-data_name-glass2-limit-2-data_version-1.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-list-data_name-glass2-limit-2-status-active-.json.gz b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-list-data_name-glass2-limit-2-status-active-.json.gz
new file mode 100644
index 000000000000..ff46d678f645
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-list-data_name-glass2-limit-2-status-active-.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40675/data-v1-download-4965250.arff.gz b/sklearn/datasets/tests/data/openml/40675/data-v1-download-4965250.arff.gz
new file mode 100644
index 000000000000..c59c3b769e11
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40675/data-v1-download-4965250.arff.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40945/api-v1-json-data-40945.json.gz b/sklearn/datasets/tests/data/openml/40945/api-v1-json-data-40945.json.gz
new file mode 100644
index 000000000000..aaafa4a2def6
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40945/api-v1-json-data-40945.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40945/api-v1-json-data-features-40945.json.gz b/sklearn/datasets/tests/data/openml/40945/api-v1-json-data-features-40945.json.gz
new file mode 100644
index 000000000000..24cb46957f27
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40945/api-v1-json-data-features-40945.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-40966.json.gz b/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-40966.json.gz
new file mode 100644
index 000000000000..02b25d717f92
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-40966.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-features-40966.json.gz b/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-features-40966.json.gz
new file mode 100644
index 000000000000..a372f9a7be75
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-features-40966.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-list-data_name-miceprotein-limit-2-data_version-4.json.gz b/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-list-data_name-miceprotein-limit-2-data_version-4.json.gz
new file mode 100644
index 000000000000..0931e0b2dadd
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-list-data_name-miceprotein-limit-2-data_version-4.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-list-data_name-miceprotein-limit-2-status-active-.json.gz b/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-list-data_name-miceprotein-limit-2-status-active-.json.gz
new file mode 100644
index 000000000000..190571cb65d9
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-list-data_name-miceprotein-limit-2-status-active-.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40966/data-v1-download-17928620.arff.gz b/sklearn/datasets/tests/data/openml/40966/data-v1-download-17928620.arff.gz
new file mode 100644
index 000000000000..43ec977bf67a
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40966/data-v1-download-17928620.arff.gz differ
diff --git a/sklearn/datasets/tests/data/openml/561/api-v1-json-data-561.json.gz b/sklearn/datasets/tests/data/openml/561/api-v1-json-data-561.json.gz
new file mode 100644
index 000000000000..e4df6060ca0b
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/561/api-v1-json-data-561.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/561/api-v1-json-data-features-561.json.gz b/sklearn/datasets/tests/data/openml/561/api-v1-json-data-features-561.json.gz
new file mode 100644
index 000000000000..54a3ab6a7a97
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/561/api-v1-json-data-features-561.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/561/api-v1-json-data-list-data_name-cpu-limit-2-data_version-1.json.gz b/sklearn/datasets/tests/data/openml/561/api-v1-json-data-list-data_name-cpu-limit-2-data_version-1.json.gz
new file mode 100644
index 000000000000..872c5a82052e
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/561/api-v1-json-data-list-data_name-cpu-limit-2-data_version-1.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/561/api-v1-json-data-list-data_name-cpu-limit-2-status-active-.json.gz b/sklearn/datasets/tests/data/openml/561/api-v1-json-data-list-data_name-cpu-limit-2-status-active-.json.gz
new file mode 100644
index 000000000000..99a631470ef4
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/561/api-v1-json-data-list-data_name-cpu-limit-2-status-active-.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/561/data-v1-download-52739.arff.gz b/sklearn/datasets/tests/data/openml/561/data-v1-download-52739.arff.gz
new file mode 100644
index 000000000000..eeb088c224a0
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/561/data-v1-download-52739.arff.gz differ
diff --git a/sklearn/datasets/tests/data/openml/61/api-v1-json-data-61.json.gz b/sklearn/datasets/tests/data/openml/61/api-v1-json-data-61.json.gz
new file mode 100644
index 000000000000..83c3ececcfab
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/61/api-v1-json-data-61.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/61/api-v1-json-data-features-61.json.gz b/sklearn/datasets/tests/data/openml/61/api-v1-json-data-features-61.json.gz
new file mode 100644
index 000000000000..6df4cf0dadbf
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/61/api-v1-json-data-features-61.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/61/api-v1-json-data-list-data_name-iris-limit-2-data_version-1.json.gz b/sklearn/datasets/tests/data/openml/61/api-v1-json-data-list-data_name-iris-limit-2-data_version-1.json.gz
new file mode 100644
index 000000000000..71b0c876adc8
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/61/api-v1-json-data-list-data_name-iris-limit-2-data_version-1.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/61/api-v1-json-data-list-data_name-iris-limit-2-status-active-.json.gz b/sklearn/datasets/tests/data/openml/61/api-v1-json-data-list-data_name-iris-limit-2-status-active-.json.gz
new file mode 100644
index 000000000000..7ea17070fbb5
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/61/api-v1-json-data-list-data_name-iris-limit-2-status-active-.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/61/data-v1-download-61.arff.gz b/sklearn/datasets/tests/data/openml/61/data-v1-download-61.arff.gz
new file mode 100644
index 000000000000..b05dadf99fb0
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/61/data-v1-download-61.arff.gz differ
diff --git a/sklearn/datasets/tests/test_mldata.py b/sklearn/datasets/tests/test_mldata.py
index 65e10a87818f..be0d994e9b18 100644
--- a/sklearn/datasets/tests/test_mldata.py
+++ b/sklearn/datasets/tests/test_mldata.py
@@ -13,6 +13,7 @@
 from sklearn.utils.testing import assert_equal
 from sklearn.utils.testing import assert_raises
 from sklearn.utils.testing import assert_array_equal
+from sklearn.utils.testing import assert_warns
 
 import pytest
 
@@ -26,6 +27,7 @@ def tmpdata(tmpdir_factory):
     shutil.rmtree(str(tmpdir))
 
 
+@pytest.mark.filterwarnings('ignore::DeprecationWarning')
 def test_mldata_filename():
     cases = [('datasets-UCI iris', 'datasets-uci-iris'),
              ('news20.binary', 'news20binary'),
@@ -36,6 +38,7 @@ def test_mldata_filename():
         assert_equal(mldata_filename(name), desired)
 
 
+@pytest.mark.filterwarnings('ignore::DeprecationWarning')
 def test_download(tmpdata):
     """Test that fetch_mldata is able to download and cache a data set."""
     _urlopen_ref = datasets.mldata.urlopen
@@ -46,7 +49,8 @@ def test_download(tmpdata):
         },
     })
     try:
-        mock = fetch_mldata('mock', data_home=tmpdata)
+        mock = assert_warns(DeprecationWarning, fetch_mldata,
+                            'mock', data_home=tmpdata)
         for n in ["COL_NAMES", "DESCR", "target", "data"]:
             assert_in(n, mock)
 
@@ -54,11 +58,13 @@ def test_download(tmpdata):
         assert_equal(mock.data.shape, (150, 4))
 
         assert_raises(datasets.mldata.HTTPError,
+                      assert_warns, DeprecationWarning,
                       fetch_mldata, 'not_existing_name')
     finally:
         datasets.mldata.urlopen = _urlopen_ref
 
 
+@pytest.mark.filterwarnings('ignore::DeprecationWarning')
 def test_fetch_one_column(tmpdata):
     _urlopen_ref = datasets.mldata.urlopen
     try:
@@ -82,6 +88,7 @@ def test_fetch_one_column(tmpdata):
         datasets.mldata.urlopen = _urlopen_ref
 
 
+@pytest.mark.filterwarnings('ignore::DeprecationWarning')
 def test_fetch_multiple_column(tmpdata):
     _urlopen_ref = datasets.mldata.urlopen
     try:
diff --git a/sklearn/datasets/tests/test_openml.py b/sklearn/datasets/tests/test_openml.py
new file mode 100644
index 000000000000..c5be1b41607b
--- /dev/null
+++ b/sklearn/datasets/tests/test_openml.py
@@ -0,0 +1,529 @@
+"""Test the openml loader.
+"""
+import gzip
+import json
+import numpy as np
+import os
+import re
+import scipy.sparse
+import sklearn
+
+from sklearn.datasets import fetch_openml
+from sklearn.datasets.openml import (_open_openml_url,
+                                     _get_data_description_by_id,
+                                     _download_data_arff)
+from sklearn.utils.testing import (assert_warns_message,
+                                   assert_raise_message)
+from sklearn.externals.six import string_types
+from sklearn.externals.six.moves.urllib.error import HTTPError
+from sklearn.datasets.tests.test_common import check_return_X_y
+from functools import partial
+
+
+currdir = os.path.dirname(os.path.abspath(__file__))
+# if True, urlopen will be monkey patched to only use local files
+test_offline = True
+test_gzip = True
+
+
+def _test_features_list(data_id):
+    # XXX Test is intended to verify/ensure correct decoding behavior
+    # Not usable with sparse data or datasets that have columns marked as
+    # {row_identifier, ignore}
+    def decode_column(data_bunch, col_idx):
+        col_name = data_bunch.feature_names[col_idx]
+        if col_name in data_bunch.categories:
+            # XXX: This would be faster with np.take, although it does not
+            # handle missing values fast (also not with mode='wrap')
+            cat = data_bunch.categories[col_name]
+            result = [cat[idx] if 0 <= idx < len(cat) else None for idx in
+                      data_bunch.data[:, col_idx].astype(int)]
+            return np.array(result, dtype='O')
+        else:
+            # non-nominal attribute
+            return data_bunch.data[:, col_idx]
+
+    data_bunch = fetch_openml(data_id=data_id, cache=False, target_column=None)
+
+    # also obtain decoded arff
+    data_description = _get_data_description_by_id(data_id, None)
+    sparse = data_description['format'].lower() == 'sparse_arff'
+    if sparse is True:
+        raise ValueError('This test is not intended for sparse data, to keep '
+                         'code relatively simple')
+    data_arff = _download_data_arff(data_description['file_id'],
+                                    sparse, None, False)
+    data_downloaded = np.array(data_arff['data'], dtype='O')
+
+    for i in range(len(data_bunch.feature_names)):
+        # XXX: Test per column, as this makes it easier to avoid problems with
+        # missing values
+
+        np.testing.assert_array_equal(data_downloaded[:, i],
+                                      decode_column(data_bunch, i))
+
+
+def _fetch_dataset_from_openml(data_id, data_name, data_version,
+                               target_column,
+                               expected_observations, expected_features,
+                               expected_missing,
+                               expected_data_dtype, expected_target_dtype,
+                               expect_sparse, compare_default_target):
+    # fetches a dataset in three various ways from OpenML, using the
+    # fetch_openml function, and does various checks on the validity of the
+    # result. Note that this function can be mocked (by invoking
+    # _monkey_patch_webbased_functions before invoking this function)
+    data_by_name_id = fetch_openml(name=data_name, version=data_version,
+                                   cache=False)
+    assert int(data_by_name_id.details['id']) == data_id
+
+    fetch_openml(name=data_name, cache=False)
+    # without specifying the version, there is no guarantee that the data id
+    # will be the same
+
+    # fetch with dataset id
+    data_by_id = fetch_openml(data_id=data_id, cache=False,
+                              target_column=target_column)
+    assert data_by_id.details['name'] == data_name
+    assert data_by_id.data.shape == (expected_observations, expected_features)
+    if isinstance(target_column, str):
+        # single target, so target is vector
+        assert data_by_id.target.shape == (expected_observations, )
+    elif isinstance(target_column, list):
+        # multi target, so target is array
+        assert data_by_id.target.shape == (expected_observations,
+                                           len(target_column))
+    assert data_by_id.data.dtype == np.float64
+    assert data_by_id.target.dtype == expected_target_dtype
+    assert len(data_by_id.feature_names) == expected_features
+    for feature in data_by_id.feature_names:
+        assert isinstance(feature, string_types)
+
+    # TODO: pass in a list of expected nominal features
+    for feature, categories in data_by_id.categories.items():
+        feature_idx = data_by_id.feature_names.index(feature)
+        values = np.unique(data_by_id.data[:, feature_idx])
+        values = values[np.isfinite(values)]
+        assert set(values) <= set(range(len(categories)))
+
+    if compare_default_target:
+        # check whether the data by id and data by id target are equal
+        data_by_id_default = fetch_openml(data_id=data_id, cache=False)
+        if data_by_id.data.dtype == np.float64:
+            np.testing.assert_allclose(data_by_id.data,
+                                       data_by_id_default.data)
+        else:
+            assert np.array_equal(data_by_id.data, data_by_id_default.data)
+        if data_by_id.target.dtype == np.float64:
+            np.testing.assert_allclose(data_by_id.target,
+                                       data_by_id_default.target)
+        else:
+            assert np.array_equal(data_by_id.target, data_by_id_default.target)
+
+    if expect_sparse:
+        assert isinstance(data_by_id.data, scipy.sparse.csr_matrix)
+    else:
+        assert isinstance(data_by_id.data, np.ndarray)
+        # np.isnan doesn't work on CSR matrix
+        assert (np.count_nonzero(np.isnan(data_by_id.data)) ==
+                expected_missing)
+
+    # test return_X_y option
+    fetch_func = partial(fetch_openml, data_id=data_id, cache=False,
+                         target_column=target_column)
+    check_return_X_y(data_by_id, fetch_func)
+    return data_by_id
+
+
+def _monkey_patch_webbased_functions(context, data_id, gziped_files):
+    url_prefix_data_description = "https://openml.org/api/v1/json/data/"
+    url_prefix_data_features = "https://openml.org/api/v1/json/data/features/"
+    url_prefix_download_data = "https://openml.org/data/v1/"
+    url_prefix_data_list = "https://openml.org/api/v1/json/data/list/"
+
+    path_suffix = ''
+    read_fn = open
+    if gziped_files:
+        path_suffix = '.gz'
+        read_fn = gzip.open
+
+    def _file_name(url, suffix):
+        return (re.sub(r'\W', '-', url[len("https://openml.org/"):])
+                + suffix + path_suffix)
+
+    def _mock_urlopen_data_description(url):
+        assert url.startswith(url_prefix_data_description)
+
+        path = os.path.join(currdir, 'data', 'openml', str(data_id),
+                            _file_name(url, '.json'))
+        return read_fn(path, 'rb')
+
+    def _mock_urlopen_data_features(url):
+        assert url.startswith(url_prefix_data_features)
+
+        path = os.path.join(currdir, 'data', 'openml', str(data_id),
+                            _file_name(url, '.json'))
+        return read_fn(path, 'rb')
+
+    def _mock_urlopen_download_data(url):
+        assert (url.startswith(url_prefix_download_data))
+
+        path = os.path.join(currdir, 'data', 'openml', str(data_id),
+                            _file_name(url, '.arff'))
+        return read_fn(path, 'rb')
+
+    def _mock_urlopen_data_list(url):
+        assert url.startswith(url_prefix_data_list)
+
+        json_file_path = os.path.join(currdir, 'data', 'openml',
+                                      str(data_id), _file_name(url, '.json'))
+        # load the file itself, to simulate a http error
+        json_data = json.loads(read_fn(json_file_path, 'rb').
+                               read().decode('utf-8'))
+        if 'error' in json_data:
+            raise HTTPError(url=None, code=412,
+                            msg='Simulated mock error',
+                            hdrs=None, fp=None)
+        return read_fn(json_file_path, 'rb')
+
+    def _mock_urlopen(url):
+        if url.startswith(url_prefix_data_list):
+            return _mock_urlopen_data_list(url)
+        elif url.startswith(url_prefix_data_features):
+            return _mock_urlopen_data_features(url)
+        elif url.startswith(url_prefix_download_data):
+            return _mock_urlopen_download_data(url)
+        elif url.startswith(url_prefix_data_description):
+            return _mock_urlopen_data_description(url)
+        else:
+            raise ValueError('Unknown mocking URL pattern: %s' % url)
+
+    # XXX: Global variable
+    if test_offline:
+        context.setattr(sklearn.datasets.openml, 'urlopen', _mock_urlopen)
+
+
+def test_fetch_openml_iris(monkeypatch):
+    # classification dataset with numeric only columns
+    data_id = 61
+    data_name = 'iris'
+    data_version = 1
+    target_column = 'class'
+    expected_observations = 150
+    expected_features = 4
+    expected_missing = 0
+
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    assert_warns_message(
+        UserWarning,
+        "Multiple active versions of the dataset matching the name"
+        " iris exist. Versions may be fundamentally different, "
+        "returning version 1.",
+        _fetch_dataset_from_openml,
+        **{'data_id': data_id, 'data_name': data_name,
+           'data_version': data_version,
+           'target_column': target_column,
+           'expected_observations': expected_observations,
+           'expected_features': expected_features,
+           'expected_missing': expected_missing,
+           'expect_sparse': False,
+           'expected_data_dtype': np.float64,
+           'expected_target_dtype': object,
+           'compare_default_target': True}
+    )
+
+
+def test_decode_iris():
+    data_id = 61
+    _test_features_list(data_id)
+
+
+def test_fetch_openml_iris_multitarget(monkeypatch):
+    # classification dataset with numeric only columns
+    data_id = 61
+    data_name = 'iris'
+    data_version = 1
+    target_column = ['sepallength', 'sepalwidth']
+    expected_observations = 150
+    expected_features = 3
+    expected_missing = 0
+
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    _fetch_dataset_from_openml(data_id, data_name, data_version, target_column,
+                               expected_observations, expected_features,
+                               expected_missing,
+                               object, np.float64, expect_sparse=False,
+                               compare_default_target=False)
+
+
+def test_fetch_openml_anneal(monkeypatch):
+    # classification dataset with numeric and categorical columns
+    data_id = 2
+    data_name = 'anneal'
+    data_version = 1
+    target_column = 'class'
+    # Not all original instances included for space reasons
+    expected_observations = 11
+    expected_features = 38
+    expected_missing = 267
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    _fetch_dataset_from_openml(data_id, data_name, data_version, target_column,
+                               expected_observations, expected_features,
+                               expected_missing,
+                               object, object, expect_sparse=False,
+                               compare_default_target=True)
+
+
+def test_decode_anneal():
+    data_id = 2
+    _test_features_list(data_id)
+
+
+def test_fetch_openml_anneal_multitarget(monkeypatch):
+    # classification dataset with numeric and categorical columns
+    data_id = 2
+    data_name = 'anneal'
+    data_version = 1
+    target_column = ['class', 'product-type', 'shape']
+    # Not all original instances included for space reasons
+    expected_observations = 11
+    expected_features = 36
+    expected_missing = 267
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    _fetch_dataset_from_openml(data_id, data_name, data_version, target_column,
+                               expected_observations, expected_features,
+                               expected_missing,
+                               object, object, expect_sparse=False,
+                               compare_default_target=False)
+
+
+def test_fetch_openml_cpu(monkeypatch):
+    # regression dataset with numeric and categorical columns
+    data_id = 561
+    data_name = 'cpu'
+    data_version = 1
+    target_column = 'class'
+    expected_observations = 209
+    expected_features = 7
+    expected_missing = 0
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    _fetch_dataset_from_openml(data_id, data_name, data_version, target_column,
+                               expected_observations, expected_features,
+                               expected_missing,
+                               object, np.float64, expect_sparse=False,
+                               compare_default_target=True)
+
+
+def test_decode_cpu():
+    data_id = 561
+    _test_features_list(data_id)
+
+
+def test_fetch_openml_australian(monkeypatch):
+    # sparse dataset
+    # Australian is the only sparse dataset that is reasonably small
+    # as it is inactive, we need to catch the warning. Due to mocking
+    # framework, it is not deactivated in our tests
+    data_id = 292
+    data_name = 'Australian'
+    data_version = 1
+    target_column = 'Y'
+    # Not all original instances included for space reasons
+    expected_observations = 85
+    expected_features = 14
+    expected_missing = 0
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    assert_warns_message(
+        UserWarning,
+        "Version 1 of dataset Australian is inactive,",
+        _fetch_dataset_from_openml,
+        **{'data_id': data_id, 'data_name': data_name,
+           'data_version': data_version,
+           'target_column': target_column,
+           'expected_observations': expected_observations,
+           'expected_features': expected_features,
+           'expected_missing': expected_missing,
+           'expect_sparse': True,
+           'expected_data_dtype': np.float64,
+           'expected_target_dtype': object,
+           'compare_default_target': False}  # numpy specific check
+    )
+
+
+def test_fetch_openml_miceprotein(monkeypatch):
+    # JvR: very important check, as this dataset defined several row ids
+    # and ignore attributes. Note that data_features json has 82 attributes,
+    # and row id (1), ignore attributes (3) have been removed (and target is
+    # stored in data.target)
+    data_id = 40966
+    data_name = 'MiceProtein'
+    data_version = 4
+    target_column = 'class'
+    # Not all original instances included for space reasons
+    expected_observations = 7
+    expected_features = 77
+    expected_missing = 7
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    _fetch_dataset_from_openml(data_id, data_name, data_version, target_column,
+                               expected_observations, expected_features,
+                               expected_missing,
+                               np.float64, object, expect_sparse=False,
+                               compare_default_target=True)
+
+
+def test_fetch_openml_emotions(monkeypatch):
+    # classification dataset with multiple targets (natively)
+    data_id = 40589
+    data_name = 'emotions'
+    data_version = 3
+    target_column = ['amazed.suprised', 'happy.pleased', 'relaxing.calm',
+                     'quiet.still', 'sad.lonely', 'angry.aggresive']
+    expected_observations = 13
+    expected_features = 72
+    expected_missing = 0
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+
+    _fetch_dataset_from_openml(data_id, data_name, data_version, target_column,
+                               expected_observations, expected_features,
+                               expected_missing,
+                               np.float64, object, expect_sparse=False,
+                               compare_default_target=True)
+
+
+def test_decode_emotions():
+    data_id = 40589
+    _test_features_list(data_id)
+
+
+def test_open_openml_url_cache(monkeypatch):
+    data_id = 61
+
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    openml_path = sklearn.datasets.openml._DATA_FILE.format(data_id)
+    test_directory = os.path.join(os.path.expanduser('~'), 'scikit_learn_data')
+    # first fill the cache
+    response1 = _open_openml_url(openml_path, test_directory)
+    # assert file exists
+    location = os.path.join(test_directory, 'openml.org', openml_path + '.gz')
+    assert os.path.isfile(location)
+    # redownload, to utilize cache
+    response2 = _open_openml_url(openml_path, test_directory)
+    assert response1.read() == response2.read()
+
+
+def test_fetch_openml_notarget(monkeypatch):
+    data_id = 61
+    target_column = None
+    expected_observations = 150
+    expected_features = 5
+
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    data = fetch_openml(data_id=data_id, target_column=target_column,
+                        cache=False)
+    assert data.data.shape == (expected_observations, expected_features)
+    assert data.target is None
+
+
+def test_fetch_openml_inactive(monkeypatch):
+    # fetch inactive dataset by id
+    data_id = 40675
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    glas2 = assert_warns_message(
+        UserWarning, "Version 1 of dataset glass2 is inactive,", fetch_openml,
+        data_id=data_id, cache=False)
+    # fetch inactive dataset by name and version
+    assert glas2.data.shape == (163, 9)
+    glas2_by_version = assert_warns_message(
+        UserWarning, "Version 1 of dataset glass2 is inactive,", fetch_openml,
+        data_id=None, name="glass2", version=1, cache=False)
+    assert int(glas2_by_version.details['id']) == data_id
+
+
+def test_fetch_nonexiting(monkeypatch):
+    # there is no active version of glass2
+    data_id = 40675
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    # Note that we only want to search by name (not data id)
+    assert_raise_message(ValueError, "No active dataset glass2 found",
+                         fetch_openml, name='glass2', cache=False)
+
+
+def test_raises_illegal_multitarget(monkeypatch):
+    data_id = 61
+    targets = ['sepalwidth', 'class']
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    # Note that we only want to search by name (not data id)
+    assert_raise_message(ValueError,
+                         "Can only handle homogeneous multi-target datasets,",
+                         fetch_openml, data_id=data_id,
+                         target_column=targets, cache=False)
+
+
+def test_warn_ignore_attribute(monkeypatch):
+    data_id = 40966
+    expected_row_id_msg = "target_column={} has flag is_row_identifier."
+    expected_ignore_msg = "target_column={} has flag is_ignore."
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    # single column test
+    assert_warns_message(UserWarning, expected_row_id_msg.format('MouseID'),
+                         fetch_openml, data_id=data_id,
+                         target_column='MouseID',
+                         cache=False)
+    assert_warns_message(UserWarning, expected_ignore_msg.format('Genotype'),
+                         fetch_openml, data_id=data_id,
+                         target_column='Genotype',
+                         cache=False)
+    # multi column test
+    assert_warns_message(UserWarning, expected_row_id_msg.format('MouseID'),
+                         fetch_openml, data_id=data_id,
+                         target_column=['MouseID', 'class'],
+                         cache=False)
+    assert_warns_message(UserWarning, expected_ignore_msg.format('Genotype'),
+                         fetch_openml, data_id=data_id,
+                         target_column=['Genotype', 'class'],
+                         cache=False)
+
+
+def test_string_attribute(monkeypatch):
+    data_id = 40945
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    # single column test
+    assert_raise_message(ValueError,
+                         'STRING attributes are not yet supported',
+                         fetch_openml, data_id=data_id, cache=False)
+
+
+def test_illegal_column(monkeypatch):
+    data_id = 61
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    assert_raise_message(KeyError, "Could not find target_column=",
+                         fetch_openml, data_id=data_id,
+                         target_column='undefined', cache=False)
+
+    assert_raise_message(KeyError, "Could not find target_column=",
+                         fetch_openml, data_id=data_id,
+                         target_column=['undefined', 'class'],
+                         cache=False)
+
+
+def test_fetch_openml_raises_missing_values_target(monkeypatch):
+    data_id = 2
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    assert_raise_message(ValueError, "Target column ",
+                         fetch_openml, data_id=data_id, target_column='family')
+
+
+def test_fetch_openml_raises_illegal_argument():
+    assert_raise_message(ValueError, "Dataset data_id=",
+                         fetch_openml, data_id=-1, name="name")
+
+    assert_raise_message(ValueError, "Dataset data_id=",
+                         fetch_openml, data_id=-1, name=None,
+                         version="version")
+
+    assert_raise_message(ValueError, "Dataset data_id=",
+                         fetch_openml, data_id=-1, name="name",
+                         version="version")
+
+    assert_raise_message(ValueError, "Neither name nor data_id are provided. "
+                         "Please provide name or data_id.", fetch_openml)
diff --git a/sklearn/datasets/tests/test_svmlight_format.py b/sklearn/datasets/tests/test_svmlight_format.py
index 3eab1d7c37eb..ca1f7ddae8ec 100644
--- a/sklearn/datasets/tests/test_svmlight_format.py
+++ b/sklearn/datasets/tests/test_svmlight_format.py
@@ -18,6 +18,7 @@
 from sklearn.utils.testing import assert_raises
 from sklearn.utils.testing import assert_raises_regex
 from sklearn.utils.testing import assert_in
+from sklearn.utils.testing import fails_if_pypy
 from sklearn.utils.fixes import sp_version
 
 import sklearn
@@ -30,6 +31,8 @@
 invalidfile = os.path.join(currdir, "data", "svmlight_invalid.txt")
 invalidfile2 = os.path.join(currdir, "data", "svmlight_invalid_order.txt")
 
+pytestmark = fails_if_pypy
+
 
 def test_load_svmlight_file():
     X, y = load_svmlight_file(datafile)
@@ -119,7 +122,8 @@ def test_load_compressed():
     with NamedTemporaryFile(prefix="sklearn-test", suffix=".gz") as tmp:
         tmp.close()  # necessary under windows
         with open(datafile, "rb") as f:
-            shutil.copyfileobj(f, gzip.open(tmp.name, "wb"))
+            with gzip.open(tmp.name, "wb") as fh_out:
+                shutil.copyfileobj(f, fh_out)
         Xgz, ygz = load_svmlight_file(tmp.name)
         # because we "close" it manually and write to it,
         # we need to remove it manually.
@@ -130,7 +134,8 @@ def test_load_compressed():
     with NamedTemporaryFile(prefix="sklearn-test", suffix=".bz2") as tmp:
         tmp.close()  # necessary under windows
         with open(datafile, "rb") as f:
-            shutil.copyfileobj(f, BZ2File(tmp.name, "wb"))
+            with BZ2File(tmp.name, "wb") as fh_out:
+                shutil.copyfileobj(f, fh_out)
         Xbz, ybz = load_svmlight_file(tmp.name)
         # because we "close" it manually and write to it,
         # we need to remove it manually.
diff --git a/sklearn/datasets/twenty_newsgroups.py b/sklearn/datasets/twenty_newsgroups.py
index 6eed41f0de88..8df908a2e2fc 100644
--- a/sklearn/datasets/twenty_newsgroups.py
+++ b/sklearn/datasets/twenty_newsgroups.py
@@ -20,22 +20,12 @@
 dataset and which features a point in time split between the train and
 test sets. The compressed dataset size is around 14 Mb compressed. Once
 uncompressed the train set is 52 MB and the test set is 34 MB.
-
-The data is downloaded, extracted and cached in the '~/scikit_learn_data'
-folder.
-
-The `fetch_20newsgroups` function will not vectorize the data into numpy
-arrays but the dataset lists the filenames of the posts and their categories
-as target labels.
-
-The `fetch_20newsgroups_vectorized` function will in addition do a simple
-tf-idf vectorization step.
-
 """
 # Copyright (c) 2011 Olivier Grisel <olivier.grisel@ensta.org>
 # License: BSD 3 clause
 
 import os
+from os.path import dirname, join
 import logging
 import tarfile
 import pickle
@@ -52,6 +42,7 @@
 from .base import _fetch_remote
 from .base import RemoteFileMetadata
 from ..utils import check_random_state, Bunch
+from ..utils import deprecated
 from ..feature_extraction.text import CountVectorizer
 from ..preprocessing import normalize
 from ..externals import joblib
@@ -71,7 +62,14 @@
 TEST_FOLDER = "20news-bydate-test"
 
 
+@deprecated("Function 'download_20newsgroups' was renamed to "
+            "'_download_20newsgroups' in version 0.20 and will be removed in "
+            "release 0.22.")
 def download_20newsgroups(target_dir, cache_path):
+    return _download_20newsgroups(target_dir, cache_path)
+
+
+def _download_20newsgroups(target_dir, cache_path):
     """Download the 20 newsgroups data and stored it as a zipped pickle."""
     train_path = os.path.join(target_dir, TRAIN_FOLDER)
     test_path = os.path.join(target_dir, TEST_FOLDER)
@@ -101,6 +99,11 @@ def strip_newsgroup_header(text):
     """
     Given text in "news" format, strip the headers, by removing everything
     before the first blank line.
+
+    Parameters
+    ----------
+    text : string
+        The text from which to remove the signature block.
     """
     _before, _blankline, after = text.partition('\n\n')
     return after
@@ -115,6 +118,11 @@ def strip_newsgroup_quoting(text):
     Given text in "news" format, strip lines beginning with the quote
     characters > or |, plus lines that often introduce a quoted section
     (for example, because they contain the string 'writes:'.)
+
+    Parameters
+    ----------
+    text : string
+        The text from which to remove the signature block.
     """
     good_lines = [line for line in text.split('\n')
                   if not _QUOTE_RE.search(line)]
@@ -128,6 +136,11 @@ def strip_newsgroup_footer(text):
     As a rough heuristic, we assume that signatures are set apart by either
     a blank line or a line made of hyphens, and that it is the last such line
     in the file (disregarding blank lines at the end).
+
+    Parameters
+    ----------
+    text : string
+        The text from which to remove the signature block.
     """
     lines = text.strip().split('\n')
     for line_num in range(len(lines) - 1, -1, -1):
@@ -145,9 +158,19 @@ def fetch_20newsgroups(data_home=None, subset='train', categories=None,
                        shuffle=True, random_state=42,
                        remove=(),
                        download_if_missing=True):
-    """Load the filenames and data from the 20 newsgroups dataset.
+    """Load the filenames and data from the 20 newsgroups dataset \
+(classification).
+
+    Download it if necessary.
+
+    =================   ==========
+    Classes                     20
+    Samples total            18846
+    Dimensionality               1
+    Features                  text
+    =================   ==========
 
-    Read more in the :ref:`User Guide <20newsgroups>`.
+    Read more in the :ref:`User Guide <20newsgroups_dataset>`.
 
     Parameters
     ----------
@@ -190,6 +213,14 @@ def fetch_20newsgroups(data_home=None, subset='train', categories=None,
     download_if_missing : optional, True by default
         If False, raise an IOError if the data is not locally available
         instead of trying to download the data from the source site.
+
+    Returns
+    -------
+    bunch : Bunch object
+        bunch.data: list, length [n_samples]
+        bunch.target: array, shape [n_samples]
+        bunch.filenames: list, length [n_classes]
+        bunch.DESCR: a description of the dataset.
     """
 
     data_home = get_data_home(data_home=data_home)
@@ -213,8 +244,8 @@ def fetch_20newsgroups(data_home=None, subset='train', categories=None,
         if download_if_missing:
             logger.info("Downloading 20news dataset. "
                         "This may take a few minutes.")
-            cache = download_20newsgroups(target_dir=twenty_home,
-                                          cache_path=cache_path)
+            cache = _download_20newsgroups(target_dir=twenty_home,
+                                           cache_path=cache_path)
         else:
             raise IOError('20Newsgroups dataset not found')
 
@@ -237,7 +268,11 @@ def fetch_20newsgroups(data_home=None, subset='train', categories=None,
         raise ValueError(
             "subset can only be 'train', 'test' or 'all', got '%s'" % subset)
 
-    data.description = 'the 20 newsgroups by date dataset'
+    module_path = dirname(__file__)
+    with open(join(module_path, 'descr', 'twenty_newsgroups.rst')) as rst_file:
+        fdescr = rst_file.read()
+
+    data.DESCR = fdescr
 
     if 'headers' in remove:
         data.data = [strip_newsgroup_header(text) for text in data.data]
@@ -278,14 +313,29 @@ def fetch_20newsgroups(data_home=None, subset='train', categories=None,
 
 def fetch_20newsgroups_vectorized(subset="train", remove=(), data_home=None,
                                   download_if_missing=True, return_X_y=False):
-    """Load the 20 newsgroups dataset and transform it into tf-idf vectors.
+    """Load the 20 newsgroups dataset and vectorize it into token counts \
+(classification).
 
-    This is a convenience function; the tf-idf transformation is done using the
-    default settings for `sklearn.feature_extraction.text.Vectorizer`. For more
+    Download it if necessary.
+
+    This is a convenience function; the transformation is done using the
+    default settings for
+    :class:`sklearn.feature_extraction.text.CountVectorizer`. For more
     advanced usage (stopword filtering, n-gram extraction, etc.), combine
-    fetch_20newsgroups with a custom `Vectorizer` or `CountVectorizer`.
+    fetch_20newsgroups with a custom
+    :class:`sklearn.feature_extraction.text.CountVectorizer`,
+    :class:`sklearn.feature_extraction.text.HashingVectorizer`,
+    :class:`sklearn.feature_extraction.text.TfidfTransformer` or
+    :class:`sklearn.feature_extraction.text.TfidfVectorizer`.
+
+    =================   ==========
+    Classes                     20
+    Samples total            18846
+    Dimensionality          130107
+    Features                  real
+    =================   ==========
 
-    Read more in the :ref:`User Guide <20newsgroups>`.
+    Read more in the :ref:`User Guide <20newsgroups_dataset>`.
 
     Parameters
     ----------
@@ -323,6 +373,7 @@ def fetch_20newsgroups_vectorized(subset="train", remove=(), data_home=None,
         bunch.data: sparse matrix, shape [n_samples, n_features]
         bunch.target: array, shape [n_samples]
         bunch.target_names: list, length [n_classes]
+        bunch.DESCR: a description of the dataset.
 
     (data, target) : tuple if ``return_X_y`` is True
 
@@ -381,7 +432,14 @@ def fetch_20newsgroups_vectorized(subset="train", remove=(), data_home=None,
         raise ValueError("%r is not a valid subset: should be one of "
                          "['train', 'test', 'all']" % subset)
 
+    module_path = dirname(__file__)
+    with open(join(module_path, 'descr', 'twenty_newsgroups.rst')) as rst_file:
+        fdescr = rst_file.read()
+
     if return_X_y:
         return data, target
 
-    return Bunch(data=data, target=target, target_names=target_names)
+    return Bunch(data=data,
+                 target=target,
+                 target_names=target_names,
+                 DESCR=fdescr)
diff --git a/sklearn/decomposition/dict_learning.py b/sklearn/decomposition/dict_learning.py
index dd0adb0c2a2f..17054dd0a4a7 100644
--- a/sklearn/decomposition/dict_learning.py
+++ b/sklearn/decomposition/dict_learning.py
@@ -8,17 +8,16 @@
 import sys
 import itertools
 
-from math import sqrt, ceil
+from math import ceil
 
 import numpy as np
 from scipy import linalg
-from numpy.lib.stride_tricks import as_strided
 
 from ..base import BaseEstimator, TransformerMixin
-from ..externals.joblib import Parallel, delayed, cpu_count
+from ..utils import Parallel, delayed, effective_n_jobs
 from ..externals.six.moves import zip
 from ..utils import (check_array, check_random_state, gen_even_slices,
-                     gen_batches, _get_n_jobs)
+                     gen_batches)
 from ..utils.extmath import randomized_svd, row_norms
 from ..utils.validation import check_is_fitted
 from ..linear_model import Lasso, orthogonal_mp_gram, LassoLars, Lars
@@ -160,7 +159,7 @@ def _sparse_encode(X, dictionary, gram, cov=None, algorithm='lasso_lars',
         new_code = ((np.sign(cov) *
                     np.maximum(np.abs(cov) - regularization, 0)).T)
         if positive:
-            new_code[new_code < 0] = 0
+            np.clip(new_code, 0, None, out=new_code)
 
     elif algorithm == 'omp':
         # TODO: Should verbose argument be passed to this?
@@ -184,7 +183,7 @@ def _sparse_encode(X, dictionary, gram, cov=None, algorithm='lasso_lars',
 # XXX : could be moved to the linear_model module
 def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars',
                   n_nonzero_coefs=None, alpha=None, copy_cov=True, init=None,
-                  max_iter=1000, n_jobs=1, check_input=True, verbose=0,
+                  max_iter=1000, n_jobs=None, check_input=True, verbose=0,
                   positive=False):
     """Sparse coding
 
@@ -246,8 +245,11 @@ def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars',
     max_iter : int, 1000 by default
         Maximum number of iterations to perform if `algorithm='lasso_cd'`.
 
-    n_jobs : int, optional
+    n_jobs : int or None, optional (default=None)
         Number of parallel jobs to run.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     check_input : boolean, optional
         If False, the input arrays X and dictionary will not be checked.
@@ -299,7 +301,7 @@ def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars',
         if regularization is None:
             regularization = 1.
 
-    if n_jobs == 1 or algorithm == 'threshold':
+    if effective_n_jobs(n_jobs) or algorithm == 'threshold':
         code = _sparse_encode(X,
                               dictionary, gram, cov=cov,
                               algorithm=algorithm,
@@ -313,7 +315,7 @@ def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars',
 
     # Enter parallel code block
     code = np.empty((n_samples, n_components))
-    slices = list(gen_even_slices(n_samples, _get_n_jobs(n_jobs)))
+    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))
 
     code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(
         delayed(_sparse_encode)(
@@ -373,20 +375,24 @@ def _update_dict(dictionary, Y, code, verbose=False, return_r2=False,
     n_components = len(code)
     n_features = Y.shape[0]
     random_state = check_random_state(random_state)
-    # Residuals, computed 'in-place' for efficiency
-    R = -np.dot(dictionary, code)
-    R += Y
-    R = np.asfortranarray(R)
+    # Get BLAS functions
+    gemm, = linalg.get_blas_funcs(('gemm',), (dictionary, code, Y))
     ger, = linalg.get_blas_funcs(('ger',), (dictionary, code))
+    nrm2, = linalg.get_blas_funcs(('nrm2',), (dictionary,))
+    # Residuals, computed with BLAS for speed and efficiency
+    # R <- -1.0 * U * V^T + 1.0 * Y
+    # Outputs R as Fortran array for efficiency
+    R = gemm(-1.0, dictionary, code, 1.0, Y)
     for k in range(n_components):
         # R <- 1.0 * U_k * V_k^T + R
         R = ger(1.0, dictionary[:, k], code[k, :], a=R, overwrite_a=True)
-        dictionary[:, k] = np.dot(R, code[k, :].T)
+        dictionary[:, k] = np.dot(R, code[k, :])
         if positive:
-            dictionary[:, k][dictionary[:, k] < 0] = 0.0
+            np.clip(dictionary[:, k], 0, None, out=dictionary[:, k])
         # Scale k'th atom
-        atom_norm_square = np.dot(dictionary[:, k], dictionary[:, k])
-        if atom_norm_square < 1e-20:
+        # (U_k * U_k) ** 0.5
+        atom_norm = nrm2(dictionary[:, k])
+        if atom_norm < 1e-10:
             if verbose == 1:
                 sys.stdout.write("+")
                 sys.stdout.flush()
@@ -394,29 +400,25 @@ def _update_dict(dictionary, Y, code, verbose=False, return_r2=False,
                 print("Adding new random atom")
             dictionary[:, k] = random_state.randn(n_features)
             if positive:
-                dictionary[:, k][dictionary[:, k] < 0] = 0.0
+                np.clip(dictionary[:, k], 0, None, out=dictionary[:, k])
             # Setting corresponding coefs to 0
             code[k, :] = 0.0
-            dictionary[:, k] /= sqrt(np.dot(dictionary[:, k],
-                                            dictionary[:, k]))
+            # (U_k * U_k) ** 0.5
+            atom_norm = nrm2(dictionary[:, k])
+            dictionary[:, k] /= atom_norm
         else:
-            dictionary[:, k] /= sqrt(atom_norm_square)
+            dictionary[:, k] /= atom_norm
             # R <- -1.0 * U_k * V_k^T + R
             R = ger(-1.0, dictionary[:, k], code[k, :], a=R, overwrite_a=True)
     if return_r2:
         R **= 2
-        # R is fortran-ordered. For numpy version < 1.6, sum does not
-        # follow the quick striding first, and is thus inefficient on
-        # fortran ordered data. We take a flat view of the data with no
-        # striding
-        R = as_strided(R, shape=(R.size, ), strides=(R.dtype.itemsize,))
-        R = np.sum(R)
+        R = R.sum()
         return dictionary, R
     return dictionary
 
 
 def dict_learning(X, n_components, alpha, max_iter=100, tol=1e-8,
-                  method='lars', n_jobs=1, dict_init=None, code_init=None,
+                  method='lars', n_jobs=None, dict_init=None, code_init=None,
                   callback=None, verbose=False, random_state=None,
                   return_n_iter=False, positive_dict=False,
                   positive_code=False):
@@ -457,8 +459,11 @@ def dict_learning(X, n_components, alpha, max_iter=100, tol=1e-8,
         Lasso solution (linear_model.Lasso). Lars will be faster if
         the estimated components are sparse.
 
-    n_jobs : int,
-        Number of parallel jobs to run, or -1 to autodetect.
+    n_jobs : int or None, optional (default=None)
+        Number of parallel jobs to run.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     dict_init : array of shape (n_components, n_features),
         Initial value for the dictionary for warm restart scenarios.
@@ -524,9 +529,6 @@ def dict_learning(X, n_components, alpha, max_iter=100, tol=1e-8,
     alpha = float(alpha)
     random_state = check_random_state(random_state)
 
-    if n_jobs == -1:
-        n_jobs = cpu_count()
-
     # Init the code and the dictionary with SVD of Y
     if code_init is not None and dict_init is not None:
         code = np.array(code_init, order='F')
@@ -603,11 +605,11 @@ def dict_learning(X, n_components, alpha, max_iter=100, tol=1e-8,
 
 def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,
                          return_code=True, dict_init=None, callback=None,
-                         batch_size=3, verbose=False, shuffle=True, n_jobs=1,
-                         method='lars', iter_offset=0, random_state=None,
-                         return_inner_stats=False, inner_stats=None,
-                         return_n_iter=False, positive_dict=False,
-                         positive_code=False):
+                         batch_size=3, verbose=False, shuffle=True,
+                         n_jobs=None, method='lars', iter_offset=0,
+                         random_state=None, return_inner_stats=False,
+                         inner_stats=None, return_n_iter=False,
+                         positive_dict=False, positive_code=False):
     """Solves a dictionary learning matrix factorization problem online.
 
     Finds the best dictionary and the corresponding sparse code for
@@ -655,8 +657,11 @@ def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,
     shuffle : boolean,
         Whether to shuffle the data before splitting it in batches.
 
-    n_jobs : int,
-        Number of parallel jobs to run, or -1 to autodetect.
+    n_jobs : int or None, optional (default=None)
+        Number of parallel jobs to run.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     method : {'lars', 'cd'}
         lars: uses the least angle regression method to solve the lasso problem
@@ -735,9 +740,6 @@ def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,
     alpha = float(alpha)
     random_state = check_random_state(random_state)
 
-    if n_jobs == -1:
-        n_jobs = cpu_count()
-
     # Init V with SVD of X
     if dict_init is not None:
         dictionary = dict_init
@@ -795,6 +797,7 @@ def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,
 
         this_code = sparse_encode(this_X, dictionary.T, algorithm=method,
                                   alpha=alpha, n_jobs=n_jobs,
+                                  check_input=False,
                                   positive=positive_code).T
 
         # Update the auxiliary variables
@@ -854,7 +857,7 @@ def _set_sparse_coding_params(self, n_components,
                                   transform_algorithm='omp',
                                   transform_n_nonzero_coefs=None,
                                   transform_alpha=None, split_sign=False,
-                                  n_jobs=1, positive_code=False):
+                                  n_jobs=None, positive_code=False):
         self.n_components = n_components
         self.transform_algorithm = transform_algorithm
         self.transform_n_nonzero_coefs = transform_n_nonzero_coefs
@@ -952,8 +955,11 @@ class SparseCoder(BaseEstimator, SparseCodingMixin):
         its negative part and its positive part. This can improve the
         performance of downstream classifiers.
 
-    n_jobs : int,
-        number of parallel jobs to run
+    n_jobs : int or None, optional (default=None)
+        Number of parallel jobs to run.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     positive_code : bool
         Whether to enforce positivity when finding the code.
@@ -977,7 +983,7 @@ class SparseCoder(BaseEstimator, SparseCodingMixin):
 
     def __init__(self, dictionary, transform_algorithm='omp',
                  transform_n_nonzero_coefs=None, transform_alpha=None,
-                 split_sign=False, n_jobs=1, positive_code=False):
+                 split_sign=False, n_jobs=None, positive_code=False):
         self._set_sparse_coding_params(dictionary.shape[0],
                                        transform_algorithm,
                                        transform_n_nonzero_coefs,
@@ -1072,8 +1078,11 @@ class DictionaryLearning(BaseEstimator, SparseCodingMixin):
         the reconstruction error targeted. In this case, it overrides
         `n_nonzero_coefs`.
 
-    n_jobs : int,
-        number of parallel jobs to run
+    n_jobs : int or None, optional (default=None)
+        Number of parallel jobs to run.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     code_init : array of shape (n_samples, n_components),
         initial value for the code, for warm restart
@@ -1133,7 +1142,7 @@ class DictionaryLearning(BaseEstimator, SparseCodingMixin):
     def __init__(self, n_components=None, alpha=1, max_iter=1000, tol=1e-8,
                  fit_algorithm='lars', transform_algorithm='omp',
                  transform_n_nonzero_coefs=None, transform_alpha=None,
-                 n_jobs=1, code_init=None, dict_init=None, verbose=False,
+                 n_jobs=None, code_init=None, dict_init=None, verbose=False,
                  split_sign=False, random_state=None,
                  positive_code=False, positive_dict=False):
 
@@ -1223,8 +1232,11 @@ class MiniBatchDictionaryLearning(BaseEstimator, SparseCodingMixin):
         Lasso solution (linear_model.Lasso). Lars will be faster if
         the estimated components are sparse.
 
-    n_jobs : int,
-        number of parallel jobs to run
+    n_jobs : int or None, optional (default=None)
+        Number of parallel jobs to run.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     batch_size : int,
         number of samples in each mini-batch
@@ -1317,7 +1329,7 @@ class MiniBatchDictionaryLearning(BaseEstimator, SparseCodingMixin):
 
     """
     def __init__(self, n_components=None, alpha=1, n_iter=1000,
-                 fit_algorithm='lars', n_jobs=1, batch_size=3,
+                 fit_algorithm='lars', n_jobs=None, batch_size=3,
                  shuffle=True, dict_init=None, transform_algorithm='omp',
                  transform_n_nonzero_coefs=None, transform_alpha=None,
                  verbose=False, split_sign=False, random_state=None,
diff --git a/sklearn/decomposition/factor_analysis.py b/sklearn/decomposition/factor_analysis.py
index 481a5e2322e3..eea477937e14 100644
--- a/sklearn/decomposition/factor_analysis.py
+++ b/sklearn/decomposition/factor_analysis.py
@@ -108,6 +108,16 @@ class FactorAnalysis(BaseEstimator, TransformerMixin):
     n_iter_ : int
         Number of iterations run.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.decomposition import FactorAnalysis
+    >>> X, _ = load_digits(return_X_y=True)
+    >>> transformer = FactorAnalysis(n_components=7, random_state=0)
+    >>> X_transformed = transformer.fit_transform(X)
+    >>> X_transformed.shape
+    (1797, 7)
+
     References
     ----------
     .. David Barber, Bayesian Reasoning and Machine Learning,
diff --git a/sklearn/decomposition/fastica_.py b/sklearn/decomposition/fastica_.py
index f2484672abe6..f64d4787b3f7 100644
--- a/sklearn/decomposition/fastica_.py
+++ b/sklearn/decomposition/fastica_.py
@@ -443,6 +443,17 @@ def my_g(x):
         maximum number of iterations run across all components. Else
         they are just the number of iterations taken to converge.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.decomposition import FastICA
+    >>> X, _ = load_digits(return_X_y=True)
+    >>> transformer = FastICA(n_components=7,
+    ...         random_state=0)
+    >>> X_transformed = transformer.fit_transform(X)
+    >>> X_transformed.shape
+    (1797, 7)
+
     Notes
     -----
     Implementation based on
diff --git a/sklearn/decomposition/incremental_pca.py b/sklearn/decomposition/incremental_pca.py
index 72f1326c5843..05e6693051f5 100644
--- a/sklearn/decomposition/incremental_pca.py
+++ b/sklearn/decomposition/incremental_pca.py
@@ -100,6 +100,20 @@ class IncrementalPCA(_BasePCA):
         The number of samples processed by the estimator. Will be reset on
         new calls to fit, but increments across ``partial_fit`` calls.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.decomposition import IncrementalPCA
+    >>> X, _ = load_digits(return_X_y=True)
+    >>> transformer = IncrementalPCA(n_components=7, batch_size=200)
+    >>> # either partially fit on smaller batches of data
+    >>> transformer.partial_fit(X[:100, :])
+    IncrementalPCA(batch_size=200, copy=True, n_components=7, whiten=False)
+    >>> # or let the fit function itself divide the data into batches
+    >>> X_transformed = transformer.fit_transform(X)
+    >>> X_transformed.shape
+    (1797, 7)
+
     Notes
     -----
     Implements the incremental PCA model from:
diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py
index 3316ddb24d2d..133717e13f67 100644
--- a/sklearn/decomposition/kernel_pca.py
+++ b/sklearn/decomposition/kernel_pca.py
@@ -89,9 +89,11 @@ class KernelPCA(BaseEstimator, TransformerMixin):
 
         .. versionadded:: 0.18
 
-    n_jobs : int, default=1
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run.
-        If `-1`, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
         .. versionadded:: 0.18
 
@@ -118,6 +120,16 @@ class KernelPCA(BaseEstimator, TransformerMixin):
         The data used to fit the model. If `copy_X=False`, then `X_fit_` is
         a reference. This attribute is used for the calls to transform.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.decomposition import KernelPCA
+    >>> X, _ = load_digits(return_X_y=True)
+    >>> transformer = KernelPCA(n_components=7, kernel='linear')
+    >>> X_transformed = transformer.fit_transform(X)
+    >>> X_transformed.shape
+    (1797, 7)
+
     References
     ----------
     Kernel PCA was introduced in:
@@ -131,7 +143,7 @@ def __init__(self, n_components=None, kernel="linear",
                  gamma=None, degree=3, coef0=1, kernel_params=None,
                  alpha=1.0, fit_inverse_transform=False, eigen_solver='auto',
                  tol=0, max_iter=None, remove_zero_eig=False,
-                 random_state=None, copy_X=True, n_jobs=1):
+                 random_state=None, copy_X=True, n_jobs=None):
         if fit_inverse_transform and kernel == 'precomputed':
             raise ValueError(
                 "Cannot fit_inverse_transform with a precomputed kernel.")
diff --git a/sklearn/decomposition/nmf.py b/sklearn/decomposition/nmf.py
index 5eee44a7396b..990d31bf2ccc 100644
--- a/sklearn/decomposition/nmf.py
+++ b/sklearn/decomposition/nmf.py
@@ -34,12 +34,25 @@ def norm(x):
     """Dot product-based Euclidean norm implementation
 
     See: http://fseoane.net/blog/2011/computing-the-vector-norm/
+
+    Parameters
+    ----------
+    x : array-like
+        Vector for which to compute the norm
     """
     return sqrt(squared_norm(x))
 
 
 def trace_dot(X, Y):
-    """Trace of np.dot(X, Y.T)."""
+    """Trace of np.dot(X, Y.T).
+
+    Parameters
+    ----------
+    X : array-like
+        First matrix
+    Y : array-like
+        Second matrix
+    """
     return np.dot(X.ravel(), Y.ravel())
 
 
@@ -1005,7 +1018,7 @@ def non_negative_factorization(X, W=None, H=None, n_components=None,
         # 'mu' solver should not be initialized by zeros
         if solver == 'mu':
             avg = np.sqrt(X.mean() / n_components)
-            W = avg * np.ones((n_samples, n_components))
+            W = np.full((n_samples, n_components), avg)
         else:
             W = np.zeros((n_samples, n_components))
     else:
diff --git a/sklearn/decomposition/online_lda.py b/sklearn/decomposition/online_lda.py
index fa40e2ef6802..5b48ea1a26b3 100644
--- a/sklearn/decomposition/online_lda.py
+++ b/sklearn/decomposition/online_lda.py
@@ -18,10 +18,10 @@
 
 from ..base import BaseEstimator, TransformerMixin
 from ..utils import (check_random_state, check_array,
-                     gen_batches, gen_even_slices, _get_n_jobs)
+                     gen_batches, gen_even_slices)
 from ..utils.fixes import logsumexp
 from ..utils.validation import check_non_negative
-from ..externals.joblib import Parallel, delayed
+from ..utils import Parallel, delayed, effective_n_jobs
 from ..externals.six.moves import xrange
 from ..exceptions import NotFittedError
 
@@ -215,9 +215,11 @@ class LatentDirichletAllocation(BaseEstimator, TransformerMixin):
         Max number of iterations for updating document topic distribution in
         the E-step.
 
-    n_jobs : int, optional (default=1)
-        The number of jobs to use in the E-step. If -1, all CPUs are used. For
-        ``n_jobs`` below -1, (n_cpus + 1 + n_jobs) are used.
+    n_jobs : int or None, optional (default=None)
+        The number of jobs to use in the E-step.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : int, optional (default=0)
         Verbosity level.
@@ -250,6 +252,22 @@ class LatentDirichletAllocation(BaseEstimator, TransformerMixin):
     n_iter_ : int
         Number of passes over the dataset.
 
+    Examples
+    --------
+    >>> from sklearn.decomposition import LatentDirichletAllocation
+    >>> from sklearn.datasets import make_multilabel_classification
+    >>> # This produces a feature matrix of token counts, similar to what
+    >>> # CountVectorizer would produce on text.
+    >>> X, _ = make_multilabel_classification(random_state=0)
+    >>> lda = LatentDirichletAllocation(n_components=5,
+    ...     random_state=0)
+    >>> lda.fit(X) # doctest: +ELLIPSIS
+    LatentDirichletAllocation(...)
+    >>> # get topics for some given samples:
+    >>> lda.transform(X[-2:])
+    array([[0.00360392, 0.25499205, 0.0036211 , 0.64236448, 0.09541846],
+           [0.15297572, 0.00362644, 0.44412786, 0.39568399, 0.003586  ]])
+
     References
     ----------
     [1] "Online Learning for Latent Dirichlet Allocation", Matthew D. Hoffman,
@@ -268,7 +286,7 @@ def __init__(self, n_components=10, doc_topic_prior=None,
                  learning_decay=.7, learning_offset=10., max_iter=10,
                  batch_size=128, evaluate_every=-1, total_samples=1e6,
                  perp_tol=1e-1, mean_change_tol=1e-3, max_doc_update_iter=100,
-                 n_jobs=1, verbose=0, random_state=None, n_topics=None):
+                 n_jobs=None, verbose=0, random_state=None, n_topics=None):
         self.n_components = n_components
         self.doc_topic_prior = doc_topic_prior
         self.topic_word_prior = topic_word_prior
@@ -374,7 +392,7 @@ def _e_step(self, X, cal_sstats, random_init, parallel=None):
         random_state = self.random_state_ if random_init else None
 
         # TODO: make Parallel._effective_n_jobs public instead?
-        n_jobs = _get_n_jobs(self.n_jobs)
+        n_jobs = effective_n_jobs(self.n_jobs)
         if parallel is None:
             parallel = Parallel(n_jobs=n_jobs, verbose=max(0,
                                 self.verbose - 1))
@@ -497,7 +515,7 @@ def partial_fit(self, X, y=None):
                 "the model was trained with feature size %d." %
                 (n_features, self.components_.shape[1]))
 
-        n_jobs = _get_n_jobs(self.n_jobs)
+        n_jobs = effective_n_jobs(self.n_jobs)
         with Parallel(n_jobs=n_jobs, verbose=max(0,
                       self.verbose - 1)) as parallel:
             for idx_slice in gen_batches(n_samples, batch_size):
@@ -538,7 +556,7 @@ def fit(self, X, y=None):
         self._init_latent_vars(n_features)
         # change to perplexity later
         last_bound = None
-        n_jobs = _get_n_jobs(self.n_jobs)
+        n_jobs = effective_n_jobs(self.n_jobs)
         with Parallel(n_jobs=n_jobs, verbose=max(0,
                       self.verbose - 1)) as parallel:
             for i in xrange(max_iter):
diff --git a/sklearn/decomposition/pca.py b/sklearn/decomposition/pca.py
index a070f887d199..db183af45af0 100644
--- a/sklearn/decomposition/pca.py
+++ b/sklearn/decomposition/pca.py
@@ -129,7 +129,7 @@ class PCA(_BasePCA):
 
             n_components == min(n_samples, n_features)
 
-        If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka\'s
+        If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's
         MLE is used to guess the dimension. Use of ``n_components == 'mle'``
         will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.
 
diff --git a/sklearn/decomposition/sparse_pca.py b/sklearn/decomposition/sparse_pca.py
index b8be8fdc4b51..95c9ab8960e6 100644
--- a/sklearn/decomposition/sparse_pca.py
+++ b/sklearn/decomposition/sparse_pca.py
@@ -48,8 +48,11 @@ class SparsePCA(BaseEstimator, TransformerMixin):
         Lasso solution (linear_model.Lasso). Lars will be faster if
         the estimated components are sparse.
 
-    n_jobs : int,
+    n_jobs : int or None, optional (default=None)
         Number of parallel jobs to run.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     U_init : array of shape (n_samples, n_components),
         Initial values for the loadings for warm restart scenarios.
@@ -66,6 +69,21 @@ class SparsePCA(BaseEstimator, TransformerMixin):
         If None, the random number generator is the RandomState instance used
         by `np.random`.
 
+    normalize_components : boolean, optional (default=False)
+        - if False, use a version of Sparse PCA without components
+          normalization and without data centering. This is likely a bug and
+          even though it's the default for backward compatibility,
+          this should not be used.
+        - if True, use a version of Sparse PCA with components normalization
+          and data centering.
+
+        .. versionadded:: 0.20
+
+        .. deprecated:: 0.22
+           ``normalize_components`` was added and set to ``False`` for
+           backward compatibility. It would be set to ``True`` from 0.22
+           onwards.
+
     Attributes
     ----------
     components_ : array, [n_components, n_features]
@@ -77,6 +95,28 @@ class SparsePCA(BaseEstimator, TransformerMixin):
     n_iter_ : int
         Number of iterations run.
 
+    mean_ : array, shape (n_features,)
+        Per-feature empirical mean, estimated from the training set.
+        Equal to ``X.mean(axis=0)``.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.datasets import make_friedman1
+    >>> from sklearn.decomposition import SparsePCA
+    >>> X, _ = make_friedman1(n_samples=200, n_features=30, random_state=0)
+    >>> transformer = SparsePCA(n_components=5,
+    ...         normalize_components=True,
+    ...         random_state=0)
+    >>> transformer.fit(X) # doctest: +ELLIPSIS
+    SparsePCA(...)
+    >>> X_transformed = transformer.transform(X)
+    >>> X_transformed.shape
+    (200, 5)
+    >>> # most values in the components_ are zero (sparsity)
+    >>> np.mean(transformer.components_ == 0) # doctest: +ELLIPSIS
+    0.9666...
+
     See also
     --------
     PCA
@@ -84,8 +124,9 @@ class SparsePCA(BaseEstimator, TransformerMixin):
     DictionaryLearning
     """
     def __init__(self, n_components=None, alpha=1, ridge_alpha=0.01,
-                 max_iter=1000, tol=1e-8, method='lars', n_jobs=1, U_init=None,
-                 V_init=None, verbose=False, random_state=None):
+                 max_iter=1000, tol=1e-8, method='lars', n_jobs=None,
+                 U_init=None, V_init=None, verbose=False, random_state=None,
+                 normalize_components=False):
         self.n_components = n_components
         self.alpha = alpha
         self.ridge_alpha = ridge_alpha
@@ -97,6 +138,7 @@ def __init__(self, n_components=None, alpha=1, ridge_alpha=0.01,
         self.V_init = V_init
         self.verbose = verbose
         self.random_state = random_state
+        self.normalize_components = normalize_components
 
     def fit(self, X, y=None):
         """Fit the model from data in X.
@@ -116,6 +158,17 @@ def fit(self, X, y=None):
         """
         random_state = check_random_state(self.random_state)
         X = check_array(X)
+
+        if self.normalize_components:
+            self.mean_ = X.mean(axis=0)
+            X = X - self.mean_
+        else:
+            warnings.warn("normalize_components=False is a "
+                          "backward-compatible setting that implements a "
+                          "non-standard definition of sparse PCA. This "
+                          "compatibility mode will be removed in 0.22.",
+                          DeprecationWarning)
+
         if self.n_components is None:
             n_components = X.shape[1]
         else:
@@ -134,6 +187,13 @@ def fit(self, X, y=None):
                                                return_n_iter=True
                                                )
         self.components_ = Vt.T
+
+        if self.normalize_components:
+            components_norm = \
+                    np.linalg.norm(self.components_, axis=1)[:, np.newaxis]
+            components_norm[components_norm == 0] = 1
+            self.components_ /= components_norm
+
         self.error_ = E
         return self
 
@@ -178,11 +238,18 @@ def transform(self, X, ridge_alpha='deprecated'):
                 ridge_alpha = self.ridge_alpha
         else:
             ridge_alpha = self.ridge_alpha
+
+        if self.normalize_components:
+            X = X - self.mean_
+
         U = ridge_regression(self.components_.T, X.T, ridge_alpha,
                              solver='cholesky')
-        s = np.sqrt((U ** 2).sum(axis=0))
-        s[s == 0] = 1
-        U /= s
+
+        if not self.normalize_components:
+            s = np.sqrt((U ** 2).sum(axis=0))
+            s[s == 0] = 1
+            U /= s
+
         return U
 
 
@@ -223,8 +290,11 @@ class MiniBatchSparsePCA(SparsePCA):
     shuffle : boolean,
         whether to shuffle the data before splitting it in batches
 
-    n_jobs : int,
-        number of parallel jobs to run, or -1 to autodetect.
+    n_jobs : int or None, optional (default=None)
+        Number of parallel jobs to run.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     method : {'lars', 'cd'}
         lars: uses the least angle regression method to solve the lasso problem
@@ -239,6 +309,21 @@ class MiniBatchSparsePCA(SparsePCA):
         If None, the random number generator is the RandomState instance used
         by `np.random`.
 
+    normalize_components : boolean, optional (default=False)
+        - if False, use a version of Sparse PCA without components
+          normalization and without data centering. This is likely a bug and
+          even though it's the default for backward compatibility,
+          this should not be used.
+        - if True, use a version of Sparse PCA with components normalization
+          and data centering.
+
+        .. versionadded:: 0.20
+
+        .. deprecated:: 0.22
+           ``normalize_components`` was added and set to ``False`` for
+           backward compatibility. It would be set to ``True`` from 0.22
+           onwards.
+
     Attributes
     ----------
     components_ : array, [n_components, n_features]
@@ -247,6 +332,29 @@ class MiniBatchSparsePCA(SparsePCA):
     n_iter_ : int
         Number of iterations run.
 
+    mean_ : array, shape (n_features,)
+        Per-feature empirical mean, estimated from the training set.
+        Equal to ``X.mean(axis=0)``.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.datasets import make_friedman1
+    >>> from sklearn.decomposition import MiniBatchSparsePCA
+    >>> X, _ = make_friedman1(n_samples=200, n_features=30, random_state=0)
+    >>> transformer = MiniBatchSparsePCA(n_components=5,
+    ...         batch_size=50,
+    ...         normalize_components=True,
+    ...         random_state=0)
+    >>> transformer.fit(X) # doctest: +ELLIPSIS
+    MiniBatchSparsePCA(...)
+    >>> X_transformed = transformer.transform(X)
+    >>> X_transformed.shape
+    (200, 5)
+    >>> # most values in the components_ are zero (sparsity)
+    >>> np.mean(transformer.components_ == 0)
+    0.94
+
     See also
     --------
     PCA
@@ -255,11 +363,13 @@ class MiniBatchSparsePCA(SparsePCA):
     """
     def __init__(self, n_components=None, alpha=1, ridge_alpha=0.01,
                  n_iter=100, callback=None, batch_size=3, verbose=False,
-                 shuffle=True, n_jobs=1, method='lars', random_state=None):
+                 shuffle=True, n_jobs=None, method='lars', random_state=None,
+                 normalize_components=False):
         super(MiniBatchSparsePCA, self).__init__(
             n_components=n_components, alpha=alpha, verbose=verbose,
             ridge_alpha=ridge_alpha, n_jobs=n_jobs, method=method,
-            random_state=random_state)
+            random_state=random_state,
+            normalize_components=normalize_components)
         self.n_iter = n_iter
         self.callback = callback
         self.batch_size = batch_size
@@ -283,6 +393,17 @@ def fit(self, X, y=None):
         """
         random_state = check_random_state(self.random_state)
         X = check_array(X)
+
+        if self.normalize_components:
+            self.mean_ = X.mean(axis=0)
+            X = X - self.mean_
+        else:
+            warnings.warn("normalize_components=False is a "
+                          "backward-compatible setting that implements a "
+                          "non-standard definition of sparse PCA. This "
+                          "compatibility mode will be removed in 0.22.",
+                          DeprecationWarning)
+
         if self.n_components is None:
             n_components = X.shape[1]
         else:
@@ -298,4 +419,11 @@ def fit(self, X, y=None):
             random_state=random_state,
             return_n_iter=True)
         self.components_ = Vt.T
+
+        if self.normalize_components:
+            components_norm = \
+                    np.linalg.norm(self.components_, axis=1)[:, np.newaxis]
+            components_norm[components_norm == 0] = 1
+            self.components_ /= components_norm
+
         return self
diff --git a/sklearn/decomposition/tests/test_dict_learning.py b/sklearn/decomposition/tests/test_dict_learning.py
index 831af46e4613..b5852f470187 100644
--- a/sklearn/decomposition/tests/test_dict_learning.py
+++ b/sklearn/decomposition/tests/test_dict_learning.py
@@ -58,6 +58,8 @@ def test_dict_learning_overcomplete():
     assert_true(dico.components_.shape == (n_components, n_features))
 
 
+# positive lars deprecated 0.22
+@pytest.mark.filterwarnings('ignore::DeprecationWarning')
 @pytest.mark.parametrize("transform_algorithm", [
     "lasso_lars",
     "lasso_cd",
@@ -170,6 +172,8 @@ def test_dict_learning_online_shapes():
     assert_equal(np.dot(code, dictionary).shape, X.shape)
 
 
+# positive lars deprecated 0.22
+@pytest.mark.filterwarnings('ignore::DeprecationWarning')
 @pytest.mark.parametrize("transform_algorithm", [
     "lasso_lars",
     "lasso_cd",
@@ -306,6 +310,8 @@ def test_sparse_encode_shapes():
         assert_equal(code.shape, (n_samples, n_components))
 
 
+# positive lars deprecated 0.22
+@pytest.mark.filterwarnings('ignore::DeprecationWarning')
 @pytest.mark.parametrize("positive", [
     False,
     True,
diff --git a/sklearn/decomposition/tests/test_kernel_pca.py b/sklearn/decomposition/tests/test_kernel_pca.py
index 63281ce33dd1..b0f2c5aeae52 100644
--- a/sklearn/decomposition/tests/test_kernel_pca.py
+++ b/sklearn/decomposition/tests/test_kernel_pca.py
@@ -1,9 +1,10 @@
 import numpy as np
 import scipy.sparse as sp
+import pytest
 
 from sklearn.utils.testing import (assert_array_almost_equal, assert_less,
                                    assert_equal, assert_not_equal,
-                                   assert_raises)
+                                   assert_raises, ignore_warnings)
 
 from sklearn.decomposition import PCA, KernelPCA
 from sklearn.datasets import make_circles
@@ -172,6 +173,7 @@ def test_kernel_pca_invalid_kernel():
     assert_raises(ValueError, kpca.fit, X_fit)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
 def test_gridsearch_pipeline():
     # Test if we can do a grid-search to find parameters to separate
     # circles with a perceptron model.
@@ -186,6 +188,7 @@ def test_gridsearch_pipeline():
     assert_equal(grid_search.best_score_, 1)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
 def test_gridsearch_pipeline_precomputed():
     # Test if we can do a grid-search to find parameters to separate
     # circles with a perceptron model using a precomputed kernel.
diff --git a/sklearn/decomposition/tests/test_nmf.py b/sklearn/decomposition/tests/test_nmf.py
index 8ae157176647..87fb4ef8c30b 100644
--- a/sklearn/decomposition/tests/test_nmf.py
+++ b/sklearn/decomposition/tests/test_nmf.py
@@ -86,8 +86,8 @@ def test_initialize_variants():
 @ignore_warnings(category=UserWarning)
 def test_nmf_fit_nn_output():
     # Test that the decomposition does not contain negative values
-    A = np.c_[5 * np.ones(5) - np.arange(1, 6),
-              5 * np.ones(5) + np.arange(1, 6)]
+    A = np.c_[5. - np.arange(1, 6),
+              5. + np.arange(1, 6)]
     for solver in ('cd', 'mu'):
         for init in (None, 'nndsvd', 'nndsvda', 'nndsvdar', 'random'):
             model = NMF(n_components=2, solver=solver, init=init,
@@ -278,7 +278,7 @@ def test_beta_divergence():
     # initialization
     rng = np.random.mtrand.RandomState(42)
     X = rng.randn(n_samples, n_features)
-    X[X < 0] = 0.
+    np.clip(X, 0, None, out=X)
     X_csr = sp.csr_matrix(X)
     W, H = nmf._initialize_nmf(X, n_components, init='random', random_state=42)
 
@@ -298,7 +298,7 @@ def test_special_sparse_dot():
     n_components = 3
     rng = np.random.mtrand.RandomState(42)
     X = rng.randn(n_samples, n_features)
-    X[X < 0] = 0.
+    np.clip(X, 0, None, out=X)
     X_csr = sp.csr_matrix(X)
 
     W = np.abs(rng.randn(n_samples, n_components))
@@ -377,7 +377,7 @@ def test_nmf_negative_beta_loss():
 
     rng = np.random.mtrand.RandomState(42)
     X = rng.randn(n_samples, n_features)
-    X[X < 0] = 0
+    np.clip(X, 0, None, out=X)
     X_csr = sp.csr_matrix(X)
 
     def _assert_nmf_no_nan(X, beta_loss):
diff --git a/sklearn/decomposition/tests/test_online_lda.py b/sklearn/decomposition/tests/test_online_lda.py
index b8b636d5a6fd..f3354cba375c 100644
--- a/sklearn/decomposition/tests/test_online_lda.py
+++ b/sklearn/decomposition/tests/test_online_lda.py
@@ -30,7 +30,7 @@ def _build_sparse_mtx():
     # Create 3 topics and each topic has 3 distinct words.
     # (Each word only belongs to a single topic.)
     n_components = 3
-    block = n_components * np.ones((3, 3))
+    block = np.full((3, 3), n_components, dtype=np.int)
     blocks = [block] * n_components
     X = block_diag(*blocks)
     X = csr_matrix(X)
@@ -176,7 +176,7 @@ def test_invalid_params():
 
 def test_lda_negative_input():
     # test pass dense matrix with sparse negative input.
-    X = -np.ones((5, 10))
+    X = np.full((5, 10), -1.)
     lda = LatentDirichletAllocation()
     regex = r"^Negative values in data passed"
     assert_raises_regexp(ValueError, regex, lda.fit, X)
diff --git a/sklearn/decomposition/tests/test_sparse_pca.py b/sklearn/decomposition/tests/test_sparse_pca.py
index 6172ac568b0c..5365ccb8f0d3 100644
--- a/sklearn/decomposition/tests/test_sparse_pca.py
+++ b/sklearn/decomposition/tests/test_sparse_pca.py
@@ -2,19 +2,20 @@
 # License: BSD 3 clause
 
 import sys
+import pytest
 
 import numpy as np
 
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_equal
-from sklearn.utils.testing import assert_array_equal
+from sklearn.utils.testing import assert_allclose
 from sklearn.utils.testing import SkipTest
 from sklearn.utils.testing import assert_true
 from sklearn.utils.testing import assert_false
 from sklearn.utils.testing import assert_warns_message
 from sklearn.utils.testing import if_safe_multiprocessing_with_blas
 
-from sklearn.decomposition import SparsePCA, MiniBatchSparsePCA
+from sklearn.decomposition import SparsePCA, MiniBatchSparsePCA, PCA
 from sklearn.utils import check_random_state
 
 
@@ -43,31 +44,37 @@ def generate_toy_data(n_components, n_samples, image_size, random_state=None):
 # test different aspects of the code in the same test
 
 
-def test_correct_shapes():
+@pytest.mark.filterwarnings("ignore:normalize_components")
+@pytest.mark.parametrize("norm_comp", [False, True])
+def test_correct_shapes(norm_comp):
     rng = np.random.RandomState(0)
     X = rng.randn(12, 10)
-    spca = SparsePCA(n_components=8, random_state=rng)
+    spca = SparsePCA(n_components=8, random_state=rng,
+                     normalize_components=norm_comp)
     U = spca.fit_transform(X)
     assert_equal(spca.components_.shape, (8, 10))
     assert_equal(U.shape, (12, 8))
     # test overcomplete decomposition
-    spca = SparsePCA(n_components=13, random_state=rng)
+    spca = SparsePCA(n_components=13, random_state=rng,
+                     normalize_components=norm_comp)
     U = spca.fit_transform(X)
     assert_equal(spca.components_.shape, (13, 10))
     assert_equal(U.shape, (12, 13))
 
 
-def test_fit_transform():
+@pytest.mark.filterwarnings("ignore:normalize_components")
+@pytest.mark.parametrize("norm_comp", [False, True])
+def test_fit_transform(norm_comp):
     alpha = 1
     rng = np.random.RandomState(0)
     Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)  # wide array
     spca_lars = SparsePCA(n_components=3, method='lars', alpha=alpha,
-                          random_state=0)
+                          random_state=0, normalize_components=norm_comp)
     spca_lars.fit(Y)
 
     # Test that CD gives similar results
     spca_lasso = SparsePCA(n_components=3, method='cd', random_state=0,
-                           alpha=alpha)
+                           alpha=alpha, normalize_components=norm_comp)
     spca_lasso.fit(Y)
     assert_array_almost_equal(spca_lasso.components_, spca_lars.components_)
 
@@ -79,92 +86,155 @@ def test_fit_transform():
                          Y, ridge_alpha=None)
 
 
+@pytest.mark.filterwarnings("ignore:normalize_components")
+@pytest.mark.parametrize("norm_comp", [False, True])
 @if_safe_multiprocessing_with_blas
-def test_fit_transform_parallel():
+def test_fit_transform_parallel(norm_comp):
     alpha = 1
     rng = np.random.RandomState(0)
     Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)  # wide array
     spca_lars = SparsePCA(n_components=3, method='lars', alpha=alpha,
-                          random_state=0)
+                          random_state=0, normalize_components=norm_comp)
     spca_lars.fit(Y)
     U1 = spca_lars.transform(Y)
     # Test multiple CPUs
     spca = SparsePCA(n_components=3, n_jobs=2, method='lars', alpha=alpha,
-                     random_state=0).fit(Y)
+                     random_state=0, normalize_components=norm_comp).fit(Y)
     U2 = spca.transform(Y)
     assert_true(not np.all(spca_lars.components_ == 0))
     assert_array_almost_equal(U1, U2)
 
 
-def test_transform_nan():
+@pytest.mark.filterwarnings("ignore:normalize_components")
+@pytest.mark.parametrize("norm_comp", [False, True])
+def test_transform_nan(norm_comp):
     # Test that SparsePCA won't return NaN when there is 0 feature in all
     # samples.
     rng = np.random.RandomState(0)
     Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)  # wide array
     Y[:, 0] = 0
-    estimator = SparsePCA(n_components=8)
+    estimator = SparsePCA(n_components=8, normalize_components=norm_comp)
     assert_false(np.any(np.isnan(estimator.fit_transform(Y))))
 
 
-def test_fit_transform_tall():
+@pytest.mark.filterwarnings("ignore:normalize_components")
+@pytest.mark.parametrize("norm_comp", [False, True])
+def test_fit_transform_tall(norm_comp):
     rng = np.random.RandomState(0)
     Y, _, _ = generate_toy_data(3, 65, (8, 8), random_state=rng)  # tall array
     spca_lars = SparsePCA(n_components=3, method='lars',
-                          random_state=rng)
+                          random_state=rng, normalize_components=norm_comp)
     U1 = spca_lars.fit_transform(Y)
-    spca_lasso = SparsePCA(n_components=3, method='cd', random_state=rng)
+    spca_lasso = SparsePCA(n_components=3, method='cd',
+                           random_state=rng, normalize_components=norm_comp)
     U2 = spca_lasso.fit(Y).transform(Y)
     assert_array_almost_equal(U1, U2)
 
 
-def test_initialization():
+@pytest.mark.filterwarnings("ignore:normalize_components")
+@pytest.mark.parametrize("norm_comp", [False, True])
+def test_initialization(norm_comp):
     rng = np.random.RandomState(0)
     U_init = rng.randn(5, 3)
     V_init = rng.randn(3, 4)
     model = SparsePCA(n_components=3, U_init=U_init, V_init=V_init, max_iter=0,
-                      random_state=rng)
+                      random_state=rng, normalize_components=norm_comp)
     model.fit(rng.randn(5, 4))
-    assert_array_equal(model.components_, V_init)
+    if norm_comp:
+        assert_allclose(model.components_,
+                        V_init / np.linalg.norm(V_init, axis=1)[:, None])
+    else:
+        assert_allclose(model.components_, V_init)
 
 
-def test_mini_batch_correct_shapes():
+@pytest.mark.filterwarnings("ignore:normalize_components")
+@pytest.mark.parametrize("norm_comp", [False, True])
+def test_mini_batch_correct_shapes(norm_comp):
     rng = np.random.RandomState(0)
     X = rng.randn(12, 10)
-    pca = MiniBatchSparsePCA(n_components=8, random_state=rng)
+    pca = MiniBatchSparsePCA(n_components=8, random_state=rng,
+                             normalize_components=norm_comp)
     U = pca.fit_transform(X)
     assert_equal(pca.components_.shape, (8, 10))
     assert_equal(U.shape, (12, 8))
     # test overcomplete decomposition
-    pca = MiniBatchSparsePCA(n_components=13, random_state=rng)
+    pca = MiniBatchSparsePCA(n_components=13, random_state=rng,
+                             normalize_components=norm_comp)
     U = pca.fit_transform(X)
     assert_equal(pca.components_.shape, (13, 10))
     assert_equal(U.shape, (12, 13))
 
 
-def test_mini_batch_fit_transform():
+@pytest.mark.filterwarnings("ignore:normalize_components")
+@pytest.mark.parametrize("norm_comp", [False, True])
+def test_mini_batch_fit_transform(norm_comp):
     raise SkipTest("skipping mini_batch_fit_transform.")
     alpha = 1
     rng = np.random.RandomState(0)
     Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)  # wide array
     spca_lars = MiniBatchSparsePCA(n_components=3, random_state=0,
-                                   alpha=alpha).fit(Y)
+                                   alpha=alpha,
+                                   normalize_components=norm_comp).fit(Y)
     U1 = spca_lars.transform(Y)
     # Test multiple CPUs
     if sys.platform == 'win32':  # fake parallelism for win32
-        import sklearn.externals.joblib.parallel as joblib_par
+        import sklearn.utils._joblib.parallel as joblib_par
         _mp = joblib_par.multiprocessing
         joblib_par.multiprocessing = None
         try:
-            U2 = MiniBatchSparsePCA(n_components=3, n_jobs=2, alpha=alpha,
-                                    random_state=0).fit(Y).transform(Y)
+            spca = MiniBatchSparsePCA(n_components=3, n_jobs=2, alpha=alpha,
+                                      random_state=0,
+                                      normalize_components=norm_comp)
+            U2 = spca.fit(Y).transform(Y)
         finally:
             joblib_par.multiprocessing = _mp
     else:  # we can efficiently use parallelism
-        U2 = MiniBatchSparsePCA(n_components=3, n_jobs=2, alpha=alpha,
-                                random_state=0).fit(Y).transform(Y)
+        spca = MiniBatchSparsePCA(n_components=3, n_jobs=2, alpha=alpha,
+                                  random_state=0,
+                                  normalize_components=norm_comp)
+        U2 = spca.fit(Y).transform(Y)
     assert_true(not np.all(spca_lars.components_ == 0))
     assert_array_almost_equal(U1, U2)
     # Test that CD gives similar results
     spca_lasso = MiniBatchSparsePCA(n_components=3, method='cd', alpha=alpha,
-                                    random_state=0).fit(Y)
+                                    random_state=0,
+                                    normalize_components=norm_comp).fit(Y)
     assert_array_almost_equal(spca_lasso.components_, spca_lars.components_)
+
+
+def test_scaling_fit_transform():
+    alpha = 1
+    rng = np.random.RandomState(0)
+    Y, _, _ = generate_toy_data(3, 1000, (8, 8), random_state=rng)
+    spca_lars = SparsePCA(n_components=3, method='lars', alpha=alpha,
+                          random_state=rng, normalize_components=True)
+    results_train = spca_lars.fit_transform(Y)
+    results_test = spca_lars.transform(Y[:10])
+    assert_allclose(results_train[0], results_test[0])
+
+
+def test_pca_vs_spca():
+    rng = np.random.RandomState(0)
+    Y, _, _ = generate_toy_data(3, 1000, (8, 8), random_state=rng)
+    Z, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)
+    spca = SparsePCA(alpha=0, ridge_alpha=0, n_components=2,
+                     normalize_components=True)
+    pca = PCA(n_components=2)
+    pca.fit(Y)
+    spca.fit(Y)
+    results_test_pca = pca.transform(Z)
+    results_test_spca = spca.transform(Z)
+    assert_allclose(np.abs(spca.components_.dot(pca.components_.T)),
+                    np.eye(2), atol=1e-5)
+    results_test_pca *= np.sign(results_test_pca[0, :])
+    results_test_spca *= np.sign(results_test_spca[0, :])
+    assert_allclose(results_test_pca, results_test_spca)
+
+
+@pytest.mark.parametrize("spca", [SparsePCA, MiniBatchSparsePCA])
+def test_spca_deprecation_warning(spca):
+    rng = np.random.RandomState(0)
+    Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)
+    warn_message = "normalize_components"
+    assert_warns_message(DeprecationWarning, warn_message,
+                         spca(normalize_components=False).fit, Y)
diff --git a/sklearn/discriminant_analysis.py b/sklearn/discriminant_analysis.py
index edb17294fa1a..a635792c6f6c 100644
--- a/sklearn/discriminant_analysis.py
+++ b/sklearn/discriminant_analysis.py
@@ -437,7 +437,7 @@ def fit(self, X, y):
 
         if (self.priors_ < 0).any():
             raise ValueError("priors must be non-negative")
-        if self.priors_.sum() != 1:
+        if not np.isclose(self.priors_.sum(), 1.0):
             warnings.warn("The priors do not sum to 1. Renormalizing",
                           UserWarning)
             self.priors_ = self.priors_ / self.priors_.sum()
@@ -567,6 +567,9 @@ class QuadraticDiscriminantAnalysis(BaseEstimator, ClassifierMixin):
 
         .. versionadded:: 0.17
 
+    store_covariances : boolean
+        Deprecated, use `store_covariance`.
+
     Attributes
     ----------
     covariance_ : list of array-like, shape = [n_features, n_features]
@@ -619,9 +622,9 @@ def __init__(self, priors=None, reg_param=0., store_covariance=False,
         self.tol = tol
 
     @property
-    @deprecated("Attribute covariances_ was deprecated in version"
+    @deprecated("Attribute ``covariances_`` was deprecated in version"
                 " 0.19 and will be removed in 0.21. Use "
-                "covariance_ instead")
+                "``covariance_`` instead")
     def covariances_(self):
         return self.covariance_
 
diff --git a/sklearn/dummy.py b/sklearn/dummy.py
index f9a4762806f1..f2c866413183 100644
--- a/sklearn/dummy.py
+++ b/sklearn/dummy.py
@@ -469,7 +469,8 @@ def predict(self, X, return_std=False):
         check_is_fitted(self, "constant_")
         n_samples = _num_samples(X)
 
-        y = np.ones((n_samples, self.n_outputs_)) * self.constant_
+        y = np.full((n_samples, self.n_outputs_), self.constant_,
+                    dtype=np.array(self.constant_).dtype)
         y_std = np.zeros((n_samples, self.n_outputs_))
 
         if self.n_outputs_ == 1 and not self.output_2d_:
diff --git a/sklearn/ensemble/bagging.py b/sklearn/ensemble/bagging.py
index 30324d6e0c87..51dce324a001 100644
--- a/sklearn/ensemble/bagging.py
+++ b/sklearn/ensemble/bagging.py
@@ -13,7 +13,7 @@
 
 from .base import BaseEnsemble, _partition_estimators
 from ..base import ClassifierMixin, RegressorMixin
-from ..externals.joblib import Parallel, delayed
+from ..utils import Parallel, delayed
 from ..externals.six import with_metaclass
 from ..externals.six.moves import zip
 from ..metrics import r2_score, accuracy_score
@@ -110,7 +110,6 @@ def _parallel_build_estimators(n_estimators, ensemble, X, y, sample_weight,
 
             estimator.fit(X[:, features], y, sample_weight=curr_sample_weight)
 
-        # Draw samples, using a mask, and then fit
         else:
             estimator.fit((X[indices])[:, features], y[indices])
 
@@ -202,7 +201,7 @@ def __init__(self,
                  bootstrap_features=False,
                  oob_score=False,
                  warm_start=False,
-                 n_jobs=1,
+                 n_jobs=None,
                  random_state=None,
                  verbose=0):
         super(BaseBagging, self).__init__(
@@ -412,7 +411,7 @@ def _get_estimators_indices(self):
     def estimators_samples_(self):
         """The subset of drawn samples for each base estimator.
 
-        Returns a dynamically generated list of boolean masks identifying
+        Returns a dynamically generated list of indices identifying
         the samples used for fitting each member of the ensemble, i.e.,
         the in-bag samples.
 
@@ -420,12 +419,8 @@ def estimators_samples_(self):
         to reduce the object memory footprint by not storing the sampling
         data. Thus fetching the property may be slower than expected.
         """
-        sample_masks = []
-        for _, sample_indices in self._get_estimators_indices():
-            mask = indices_to_mask(sample_indices, self._n_samples)
-            sample_masks.append(mask)
-
-        return sample_masks
+        return [sample_indices
+                for _, sample_indices in self._get_estimators_indices()]
 
 
 class BaggingClassifier(BaseBagging, ClassifierMixin):
@@ -489,9 +484,11 @@ class BaggingClassifier(BaseBagging, ClassifierMixin):
         .. versionadded:: 0.17
            *warm_start* constructor parameter.
 
-    n_jobs : int, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of jobs to run in parallel for both `fit` and `predict`.
-        If -1, then the number of jobs is set to the number of cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
@@ -512,7 +509,7 @@ class BaggingClassifier(BaseBagging, ClassifierMixin):
 
     estimators_samples_ : list of arrays
         The subset of drawn samples (i.e., the in-bag samples) for each base
-        estimator. Each subset is defined by a boolean mask.
+        estimator. Each subset is defined by an array of the indices selected.
 
     estimators_features_ : list of arrays
         The subset of drawn features for each base estimator.
@@ -557,7 +554,7 @@ def __init__(self,
                  bootstrap_features=False,
                  oob_score=False,
                  warm_start=False,
-                 n_jobs=1,
+                 n_jobs=None,
                  random_state=None,
                  verbose=0):
 
@@ -590,7 +587,7 @@ def _set_oob_score(self, X, y):
                                                 self.estimators_samples_,
                                                 self.estimators_features_):
             # Create mask for OOB samples
-            mask = ~samples
+            mask = ~indices_to_mask(samples, n_samples)
 
             if hasattr(estimator, "predict_proba"):
                 predictions[mask, :] += estimator.predict_proba(
@@ -865,9 +862,11 @@ class BaggingRegressor(BaseBagging, RegressorMixin):
         and add more estimators to the ensemble, otherwise, just fit
         a whole new ensemble. See :term:`the Glossary <warm_start>`.
 
-    n_jobs : int, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of jobs to run in parallel for both `fit` and `predict`.
-        If -1, then the number of jobs is set to the number of cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
@@ -885,7 +884,7 @@ class BaggingRegressor(BaseBagging, RegressorMixin):
 
     estimators_samples_ : list of arrays
         The subset of drawn samples (i.e., the in-bag samples) for each base
-        estimator. Each subset is defined by a boolean mask.
+        estimator. Each subset is defined by an array of the indices selected.
 
     estimators_features_ : list of arrays
         The subset of drawn features for each base estimator.
@@ -925,7 +924,7 @@ def __init__(self,
                  bootstrap_features=False,
                  oob_score=False,
                  warm_start=False,
-                 n_jobs=1,
+                 n_jobs=None,
                  random_state=None,
                  verbose=0):
         super(BaggingRegressor, self).__init__(
@@ -996,7 +995,7 @@ def _set_oob_score(self, X, y):
                                                 self.estimators_samples_,
                                                 self.estimators_features_):
             # Create mask for OOB samples
-            mask = ~samples
+            mask = ~indices_to_mask(samples, n_samples)
 
             predictions[mask] += estimator.predict((X[mask, :])[:, features])
             n_predictions[mask] += 1
diff --git a/sklearn/ensemble/base.py b/sklearn/ensemble/base.py
index 2477cc1c21c7..321031892d1e 100644
--- a/sklearn/ensemble/base.py
+++ b/sklearn/ensemble/base.py
@@ -11,8 +11,9 @@
 from ..base import clone
 from ..base import BaseEstimator
 from ..base import MetaEstimatorMixin
-from ..utils import _get_n_jobs, check_random_state
+from ..utils import check_random_state
 from ..externals import six
+from ..externals.joblib import effective_n_jobs
 from abc import ABCMeta, abstractmethod
 
 MAX_RAND_SEED = np.iinfo(np.int32).max
@@ -150,11 +151,11 @@ def __iter__(self):
 def _partition_estimators(n_estimators, n_jobs):
     """Private function used to partition estimators between jobs."""
     # Compute the number of jobs
-    n_jobs = min(_get_n_jobs(n_jobs), n_estimators)
+    n_jobs = min(effective_n_jobs(n_jobs), n_estimators)
 
     # Partition estimators between jobs
-    n_estimators_per_job = (n_estimators // n_jobs) * np.ones(n_jobs,
-                                                              dtype=np.int)
+    n_estimators_per_job = np.full(n_jobs, n_estimators // n_jobs,
+                                   dtype=np.int)
     n_estimators_per_job[:n_estimators % n_jobs] += 1
     starts = np.cumsum(n_estimators_per_job)
 
diff --git a/sklearn/ensemble/forest.py b/sklearn/ensemble/forest.py
index b7a349d4b5a8..f9b1348adca3 100644
--- a/sklearn/ensemble/forest.py
+++ b/sklearn/ensemble/forest.py
@@ -52,7 +52,7 @@ class calls the ``fit`` method of each sub-estimator on random samples
 
 
 from ..base import ClassifierMixin, RegressorMixin
-from ..externals.joblib import Parallel, delayed
+from ..utils import Parallel, delayed
 from ..externals import six
 from ..metrics import r2_score
 from ..preprocessing import OneHotEncoder
@@ -135,11 +135,11 @@ class BaseForest(six.with_metaclass(ABCMeta, BaseEnsemble)):
     @abstractmethod
     def __init__(self,
                  base_estimator,
-                 n_estimators=10,
+                 n_estimators=100,
                  estimator_params=tuple(),
                  bootstrap=False,
                  oob_score=False,
-                 n_jobs=1,
+                 n_jobs=None,
                  random_state=None,
                  verbose=0,
                  warm_start=False,
@@ -175,7 +175,7 @@ def apply(self, X):
         """
         X = self._validate_X_predict(X)
         results = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
-                           backend="threading")(
+                           prefer="threads")(
             delayed(parallel_helper)(tree, 'apply', X, check_input=False)
             for tree in self.estimators_)
 
@@ -206,9 +206,9 @@ def decision_path(self, X):
         """
         X = self._validate_X_predict(X)
         indicators = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
-                              backend="threading")(
+                              prefer="threads")(
             delayed(parallel_helper)(tree, 'decision_path', X,
-                                      check_input=False)
+                                     check_input=False)
             for tree in self.estimators_)
 
         n_nodes = [0]
@@ -223,8 +223,8 @@ def fit(self, X, y, sample_weight=None):
         Parameters
         ----------
         X : array-like or sparse matrix of shape = [n_samples, n_features]
-            The training input samples. Internally, its dtype will be converted to
-            ``dtype=np.float32``. If a sparse matrix is provided, it will be
+            The training input samples. Internally, its dtype will be converted
+            to ``dtype=np.float32``. If a sparse matrix is provided, it will be
             converted into a sparse ``csc_matrix``.
 
         y : array-like, shape = [n_samples] or [n_samples, n_outputs]
@@ -242,6 +242,12 @@ def fit(self, X, y, sample_weight=None):
         -------
         self : object
         """
+
+        if self.n_estimators == 'warn':
+            warnings.warn("The default value of n_estimators will change from "
+                          "10 in version 0.20 to 100 in 0.22.", FutureWarning)
+            self.n_estimators = 10
+
         # Validate or convert input data
         X = check_array(X, accept_sparse="csc", dtype=DTYPE)
         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)
@@ -315,12 +321,14 @@ def fit(self, X, y, sample_weight=None):
                                             random_state=random_state)
                 trees.append(tree)
 
-            # Parallel loop: we use the threading backend as the Cython code
+            # Parallel loop: we prefer the threading backend as the Cython code
             # for fitting the trees is internally releasing the Python GIL
-            # making threading always more efficient than multiprocessing in
-            # that case.
+            # making threading more efficient than multiprocessing in
+            # that case. However, we respect any parallel_backend contexts set
+            # at a higher level, since correctness does not rely on using
+            # threads.
             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
-                             backend="threading")(
+                             prefer="threads")(
                 delayed(_parallel_build_trees)(
                     t, self, X, y, sample_weight, i, len(trees),
                     verbose=self.verbose, class_weight=self.class_weight)
@@ -367,18 +375,19 @@ def feature_importances_(self):
         check_is_fitted(self, 'estimators_')
 
         all_importances = Parallel(n_jobs=self.n_jobs,
-                                   backend="threading")(
+                                   prefer="threads")(
             delayed(getattr)(tree, 'feature_importances_')
             for tree in self.estimators_)
 
         return sum(all_importances) / len(self.estimators_)
 
 
-# This is a utility function for joblib's Parallel. It can't go locally in
-# ForestClassifier or ForestRegressor, because joblib complains that it cannot
-# pickle it when placed there.
+def _accumulate_prediction(predict, X, out, lock):
+    """This is a utility function for joblib's Parallel.
 
-def accumulate_prediction(predict, X, out, lock):
+    It can't go locally in ForestClassifier or ForestRegressor, because joblib
+    complains that it cannot pickle it when placed there.
+    """
     prediction = predict(X, check_input=False)
     with lock:
         if len(out) == 1:
@@ -399,16 +408,15 @@ class ForestClassifier(six.with_metaclass(ABCMeta, BaseForest,
     @abstractmethod
     def __init__(self,
                  base_estimator,
-                 n_estimators=10,
+                 n_estimators=100,
                  estimator_params=tuple(),
                  bootstrap=False,
                  oob_score=False,
-                 n_jobs=1,
+                 n_jobs=None,
                  random_state=None,
                  verbose=0,
                  warm_start=False,
                  class_weight=None):
-
         super(ForestClassifier, self).__init__(
             base_estimator,
             n_estimators=n_estimators,
@@ -583,8 +591,9 @@ class in a leaf.
         all_proba = [np.zeros((X.shape[0], j), dtype=np.float64)
                      for j in np.atleast_1d(self.n_classes_)]
         lock = threading.Lock()
-        Parallel(n_jobs=n_jobs, verbose=self.verbose, backend="threading")(
-            delayed(accumulate_prediction)(e.predict_proba, X, all_proba, lock)
+        Parallel(n_jobs=n_jobs, verbose=self.verbose, require="sharedmem")(
+            delayed(_accumulate_prediction)(e.predict_proba, X, all_proba,
+                                            lock)
             for e in self.estimators_)
 
         for proba in all_proba:
@@ -638,11 +647,11 @@ class ForestRegressor(six.with_metaclass(ABCMeta, BaseForest, RegressorMixin)):
     @abstractmethod
     def __init__(self,
                  base_estimator,
-                 n_estimators=10,
+                 n_estimators=100,
                  estimator_params=tuple(),
                  bootstrap=False,
                  oob_score=False,
-                 n_jobs=1,
+                 n_jobs=None,
                  random_state=None,
                  verbose=0,
                  warm_start=False):
@@ -690,8 +699,8 @@ def predict(self, X):
 
         # Parallel loop
         lock = threading.Lock()
-        Parallel(n_jobs=n_jobs, verbose=self.verbose, backend="threading")(
-            delayed(accumulate_prediction)(e.predict, X, [y_hat], lock)
+        Parallel(n_jobs=n_jobs, verbose=self.verbose, require="sharedmem")(
+            delayed(_accumulate_prediction)(e.predict, X, [y_hat], lock)
             for e in self.estimators_)
 
         y_hat /= len(self.estimators_)
@@ -758,27 +767,15 @@ class RandomForestClassifier(ForestClassifier):
     n_estimators : integer, optional (default=10)
         The number of trees in the forest.
 
+        .. versionchanged:: 0.20
+           The default value of ``n_estimators`` will change from 10 in
+           version 0.20 to 100 in version 0.22.
+
     criterion : string, optional (default="gini")
         The function to measure the quality of a split. Supported criteria are
         "gini" for the Gini impurity and "entropy" for the information gain.
         Note: this parameter is tree-specific.
 
-    max_features : int, float, string or None, optional (default="auto")
-        The number of features to consider when looking for the best split:
-
-        - If int, then consider `max_features` features at each split.
-        - If float, then `max_features` is a fraction and
-          `int(max_features * n_features)` features are considered at each
-          split.
-        - If "auto", then `max_features=sqrt(n_features)`.
-        - If "sqrt", then `max_features=sqrt(n_features)` (same as "auto").
-        - If "log2", then `max_features=log2(n_features)`.
-        - If None, then `max_features=n_features`.
-
-        Note: the search for a split does not stop until at least one
-        valid partition of the node samples is found, even if it requires to
-        effectively inspect more than ``max_features`` features.
-
     max_depth : integer or None, optional (default=None)
         The maximum depth of the tree. If None, then nodes are expanded until
         all leaves are pure or until all leaves contain less than
@@ -811,20 +808,27 @@ class RandomForestClassifier(ForestClassifier):
         the input samples) required to be at a leaf node. Samples have
         equal weight when sample_weight is not provided.
 
+    max_features : int, float, string or None, optional (default="auto")
+        The number of features to consider when looking for the best split:
+
+        - If int, then consider `max_features` features at each split.
+        - If float, then `max_features` is a fraction and
+          `int(max_features * n_features)` features are considered at each
+          split.
+        - If "auto", then `max_features=sqrt(n_features)`.
+        - If "sqrt", then `max_features=sqrt(n_features)` (same as "auto").
+        - If "log2", then `max_features=log2(n_features)`.
+        - If None, then `max_features=n_features`.
+
+        Note: the search for a split does not stop until at least one
+        valid partition of the node samples is found, even if it requires to
+        effectively inspect more than ``max_features`` features.
+
     max_leaf_nodes : int or None, optional (default=None)
         Grow trees with ``max_leaf_nodes`` in best-first fashion.
         Best nodes are defined as relative reduction in impurity.
         If None then unlimited number of leaf nodes.
 
-    min_impurity_split : float,
-        Threshold for early stopping in tree growth. A node will split
-        if its impurity is above the threshold, otherwise it is a leaf.
-
-        .. deprecated:: 0.19
-           ``min_impurity_split`` has been deprecated in favor of
-           ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.
-           Use ``min_impurity_decrease`` instead.
-
     min_impurity_decrease : float, optional (default=0.)
         A node will be split if this split induces a decrease of the impurity
         greater than or equal to this value.
@@ -843,6 +847,15 @@ class RandomForestClassifier(ForestClassifier):
 
         .. versionadded:: 0.19
 
+    min_impurity_split : float,
+        Threshold for early stopping in tree growth. A node will split
+        if its impurity is above the threshold, otherwise it is a leaf.
+
+        .. deprecated:: 0.19
+           ``min_impurity_split`` has been deprecated in favor of
+           ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.
+           Use ``min_impurity_decrease`` instead.
+
     bootstrap : boolean, optional (default=True)
         Whether bootstrap samples are used when building trees.
 
@@ -850,9 +863,11 @@ class RandomForestClassifier(ForestClassifier):
         Whether to use out-of-bag samples to estimate
         the generalization accuracy.
 
-    n_jobs : integer, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of jobs to run in parallel for both `fit` and `predict`.
-        If -1, then the number of jobs is set to the number of cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
@@ -933,16 +948,17 @@ class labels (multi-output problem).
     >>> X, y = make_classification(n_samples=1000, n_features=4,
     ...                            n_informative=2, n_redundant=0,
     ...                            random_state=0, shuffle=False)
-    >>> clf = RandomForestClassifier(max_depth=2, random_state=0)
+    >>> clf = RandomForestClassifier(n_estimators=100, max_depth=2,
+    ...                              random_state=0)
     >>> clf.fit(X, y)
     RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
                 max_depth=2, max_features='auto', max_leaf_nodes=None,
                 min_impurity_decrease=0.0, min_impurity_split=None,
                 min_samples_leaf=1, min_samples_split=2,
-                min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,
+                min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,
                 oob_score=False, random_state=0, verbose=0, warm_start=False)
     >>> print(clf.feature_importances_)
-    [0.17287856 0.80608704 0.01884792 0.00218648]
+    [0.14205973 0.76664038 0.0282433  0.06305659]
     >>> print(clf.predict([[0, 0, 0, 0]]))
     [1]
 
@@ -971,7 +987,7 @@ class labels (multi-output problem).
     DecisionTreeClassifier, ExtraTreesClassifier
     """
     def __init__(self,
-                 n_estimators=10,
+                 n_estimators='warn',
                  criterion="gini",
                  max_depth=None,
                  min_samples_split=2,
@@ -983,7 +999,7 @@ def __init__(self,
                  min_impurity_split=None,
                  bootstrap=True,
                  oob_score=False,
-                 n_jobs=1,
+                 n_jobs=None,
                  random_state=None,
                  verbose=0,
                  warm_start=False,
@@ -1032,6 +1048,10 @@ class RandomForestRegressor(ForestRegressor):
     n_estimators : integer, optional (default=10)
         The number of trees in the forest.
 
+        .. versionchanged:: 0.20
+           The default value of ``n_estimators`` will change from 10 in
+           version 0.20 to 100 in version 0.22.
+
     criterion : string, optional (default="mse")
         The function to measure the quality of a split. Supported criteria
         are "mse" for the mean squared error, which is equal to variance
@@ -1041,22 +1061,6 @@ class RandomForestRegressor(ForestRegressor):
         .. versionadded:: 0.18
            Mean Absolute Error (MAE) criterion.
 
-    max_features : int, float, string or None, optional (default="auto")
-        The number of features to consider when looking for the best split:
-
-        - If int, then consider `max_features` features at each split.
-        - If float, then `max_features` is a fraction and
-          `int(max_features * n_features)` features are considered at each
-          split.
-        - If "auto", then `max_features=n_features`.
-        - If "sqrt", then `max_features=sqrt(n_features)`.
-        - If "log2", then `max_features=log2(n_features)`.
-        - If None, then `max_features=n_features`.
-
-        Note: the search for a split does not stop until at least one
-        valid partition of the node samples is found, even if it requires to
-        effectively inspect more than ``max_features`` features.
-
     max_depth : integer or None, optional (default=None)
         The maximum depth of the tree. If None, then nodes are expanded until
         all leaves are pure or until all leaves contain less than
@@ -1089,20 +1093,27 @@ class RandomForestRegressor(ForestRegressor):
         the input samples) required to be at a leaf node. Samples have
         equal weight when sample_weight is not provided.
 
+    max_features : int, float, string or None, optional (default="auto")
+        The number of features to consider when looking for the best split:
+
+        - If int, then consider `max_features` features at each split.
+        - If float, then `max_features` is a fraction and
+          `int(max_features * n_features)` features are considered at each
+          split.
+        - If "auto", then `max_features=n_features`.
+        - If "sqrt", then `max_features=sqrt(n_features)`.
+        - If "log2", then `max_features=log2(n_features)`.
+        - If None, then `max_features=n_features`.
+
+        Note: the search for a split does not stop until at least one
+        valid partition of the node samples is found, even if it requires to
+        effectively inspect more than ``max_features`` features.
+
     max_leaf_nodes : int or None, optional (default=None)
         Grow trees with ``max_leaf_nodes`` in best-first fashion.
         Best nodes are defined as relative reduction in impurity.
         If None then unlimited number of leaf nodes.
 
-    min_impurity_split : float,
-        Threshold for early stopping in tree growth. A node will split
-        if its impurity is above the threshold, otherwise it is a leaf.
-
-        .. deprecated:: 0.19
-           ``min_impurity_split`` has been deprecated in favor of
-           ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.
-           Use ``min_impurity_decrease`` instead.
-
     min_impurity_decrease : float, optional (default=0.)
         A node will be split if this split induces a decrease of the impurity
         greater than or equal to this value.
@@ -1121,6 +1132,15 @@ class RandomForestRegressor(ForestRegressor):
 
         .. versionadded:: 0.19
 
+    min_impurity_split : float,
+        Threshold for early stopping in tree growth. A node will split
+        if its impurity is above the threshold, otherwise it is a leaf.
+
+        .. deprecated:: 0.19
+           ``min_impurity_split`` has been deprecated in favor of
+           ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.
+           Use ``min_impurity_decrease`` instead.
+
     bootstrap : boolean, optional (default=True)
         Whether bootstrap samples are used when building trees.
 
@@ -1128,9 +1148,11 @@ class RandomForestRegressor(ForestRegressor):
         whether to use out-of-bag samples to estimate
         the R^2 on unseen data.
 
-    n_jobs : integer, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of jobs to run in parallel for both `fit` and `predict`.
-        If -1, then the number of jobs is set to the number of cores.
+        `None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
@@ -1173,18 +1195,19 @@ class RandomForestRegressor(ForestRegressor):
     >>>
     >>> X, y = make_regression(n_features=4, n_informative=2,
     ...                        random_state=0, shuffle=False)
-    >>> regr = RandomForestRegressor(max_depth=2, random_state=0)
+    >>> regr = RandomForestRegressor(max_depth=2, random_state=0,
+    ...                              n_estimators=100)
     >>> regr.fit(X, y)
     RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,
                max_features='auto', max_leaf_nodes=None,
                min_impurity_decrease=0.0, min_impurity_split=None,
                min_samples_leaf=1, min_samples_split=2,
-               min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,
+               min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,
                oob_score=False, random_state=0, verbose=0, warm_start=False)
     >>> print(regr.feature_importances_)
-    [0.17339552 0.81594114 0.         0.01066333]
+    [0.18146984 0.81473937 0.00145312 0.00233767]
     >>> print(regr.predict([[0, 0, 0, 0]]))
-    [-2.50699856]
+    [-8.32987858]
 
     Notes
     -----
@@ -1201,17 +1224,24 @@ class RandomForestRegressor(ForestRegressor):
     search of the best split. To obtain a deterministic behaviour during
     fitting, ``random_state`` has to be fixed.
 
+    The default value ``max_features="auto"`` uses ``n_features`` 
+    rather than ``n_features / 3``. The latter was originally suggested in
+    [1], whereas the former was more recently justified empirically in [2].
+
     References
     ----------
 
     .. [1] L. Breiman, "Random Forests", Machine Learning, 45(1), 5-32, 2001.
 
+    .. [2] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized 
+           trees", Machine Learning, 63(1), 3-42, 2006.
+
     See also
     --------
     DecisionTreeRegressor, ExtraTreesRegressor
     """
     def __init__(self,
-                 n_estimators=10,
+                 n_estimators='warn',
                  criterion="mse",
                  max_depth=None,
                  min_samples_split=2,
@@ -1223,7 +1253,7 @@ def __init__(self,
                  min_impurity_split=None,
                  bootstrap=True,
                  oob_score=False,
-                 n_jobs=1,
+                 n_jobs=None,
                  random_state=None,
                  verbose=0,
                  warm_start=False):
@@ -1268,26 +1298,14 @@ class ExtraTreesClassifier(ForestClassifier):
     n_estimators : integer, optional (default=10)
         The number of trees in the forest.
 
+        .. versionchanged:: 0.20
+           The default value of ``n_estimators`` will change from 10 in
+           version 0.20 to 100 in version 0.22.
+
     criterion : string, optional (default="gini")
         The function to measure the quality of a split. Supported criteria are
         "gini" for the Gini impurity and "entropy" for the information gain.
 
-    max_features : int, float, string or None, optional (default="auto")
-        The number of features to consider when looking for the best split:
-
-        - If int, then consider `max_features` features at each split.
-        - If float, then `max_features` is a fraction and
-          `int(max_features * n_features)` features are considered at each
-          split.
-        - If "auto", then `max_features=sqrt(n_features)`.
-        - If "sqrt", then `max_features=sqrt(n_features)`.
-        - If "log2", then `max_features=log2(n_features)`.
-        - If None, then `max_features=n_features`.
-
-        Note: the search for a split does not stop until at least one
-        valid partition of the node samples is found, even if it requires to
-        effectively inspect more than ``max_features`` features.
-
     max_depth : integer or None, optional (default=None)
         The maximum depth of the tree. If None, then nodes are expanded until
         all leaves are pure or until all leaves contain less than
@@ -1320,20 +1338,27 @@ class ExtraTreesClassifier(ForestClassifier):
         the input samples) required to be at a leaf node. Samples have
         equal weight when sample_weight is not provided.
 
+    max_features : int, float, string or None, optional (default="auto")
+        The number of features to consider when looking for the best split:
+
+        - If int, then consider `max_features` features at each split.
+        - If float, then `max_features` is a fraction and
+          `int(max_features * n_features)` features are considered at each
+          split.
+        - If "auto", then `max_features=sqrt(n_features)`.
+        - If "sqrt", then `max_features=sqrt(n_features)`.
+        - If "log2", then `max_features=log2(n_features)`.
+        - If None, then `max_features=n_features`.
+
+        Note: the search for a split does not stop until at least one
+        valid partition of the node samples is found, even if it requires to
+        effectively inspect more than ``max_features`` features.
+
     max_leaf_nodes : int or None, optional (default=None)
         Grow trees with ``max_leaf_nodes`` in best-first fashion.
         Best nodes are defined as relative reduction in impurity.
         If None then unlimited number of leaf nodes.
 
-    min_impurity_split : float,
-        Threshold for early stopping in tree growth. A node will split
-        if its impurity is above the threshold, otherwise it is a leaf.
-
-        .. deprecated:: 0.19
-           ``min_impurity_split`` has been deprecated in favor of
-           ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.
-           Use ``min_impurity_decrease`` instead.
-
     min_impurity_decrease : float, optional (default=0.)
         A node will be split if this split induces a decrease of the impurity
         greater than or equal to this value.
@@ -1352,6 +1377,15 @@ class ExtraTreesClassifier(ForestClassifier):
 
         .. versionadded:: 0.19
 
+    min_impurity_split : float,
+        Threshold for early stopping in tree growth. A node will split
+        if its impurity is above the threshold, otherwise it is a leaf.
+
+        .. deprecated:: 0.19
+           ``min_impurity_split`` has been deprecated in favor of
+           ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.
+           Use ``min_impurity_decrease`` instead.
+
     bootstrap : boolean, optional (default=False)
         Whether bootstrap samples are used when building trees.
 
@@ -1359,9 +1393,11 @@ class ExtraTreesClassifier(ForestClassifier):
         Whether to use out-of-bag samples to estimate
         the generalization accuracy.
 
-    n_jobs : integer, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of jobs to run in parallel for both `fit` and `predict`.
-        If -1, then the number of jobs is set to the number of cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
@@ -1444,8 +1480,8 @@ class labels (multi-output problem).
     References
     ----------
 
-    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized trees",
-           Machine Learning, 63(1), 3-42, 2006.
+    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized 
+           trees", Machine Learning, 63(1), 3-42, 2006.
 
     See also
     --------
@@ -1454,7 +1490,7 @@ class labels (multi-output problem).
         splits.
     """
     def __init__(self,
-                 n_estimators=10,
+                 n_estimators='warn',
                  criterion="gini",
                  max_depth=None,
                  min_samples_split=2,
@@ -1466,7 +1502,7 @@ def __init__(self,
                  min_impurity_split=None,
                  bootstrap=False,
                  oob_score=False,
-                 n_jobs=1,
+                 n_jobs=None,
                  random_state=None,
                  verbose=0,
                  warm_start=False,
@@ -1513,6 +1549,10 @@ class ExtraTreesRegressor(ForestRegressor):
     n_estimators : integer, optional (default=10)
         The number of trees in the forest.
 
+        .. versionchanged:: 0.20
+           The default value of ``n_estimators`` will change from 10 in
+           version 0.20 to 100 in version 0.22.
+
     criterion : string, optional (default="mse")
         The function to measure the quality of a split. Supported criteria
         are "mse" for the mean squared error, which is equal to variance
@@ -1522,22 +1562,6 @@ class ExtraTreesRegressor(ForestRegressor):
         .. versionadded:: 0.18
            Mean Absolute Error (MAE) criterion.
 
-    max_features : int, float, string or None, optional (default="auto")
-        The number of features to consider when looking for the best split:
-
-        - If int, then consider `max_features` features at each split.
-        - If float, then `max_features` is a fraction and
-          `int(max_features * n_features)` features are considered at each
-          split.
-        - If "auto", then `max_features=n_features`.
-        - If "sqrt", then `max_features=sqrt(n_features)`.
-        - If "log2", then `max_features=log2(n_features)`.
-        - If None, then `max_features=n_features`.
-
-        Note: the search for a split does not stop until at least one
-        valid partition of the node samples is found, even if it requires to
-        effectively inspect more than ``max_features`` features.
-
     max_depth : integer or None, optional (default=None)
         The maximum depth of the tree. If None, then nodes are expanded until
         all leaves are pure or until all leaves contain less than
@@ -1570,20 +1594,27 @@ class ExtraTreesRegressor(ForestRegressor):
         the input samples) required to be at a leaf node. Samples have
         equal weight when sample_weight is not provided.
 
+    max_features : int, float, string or None, optional (default="auto")
+        The number of features to consider when looking for the best split:
+
+        - If int, then consider `max_features` features at each split.
+        - If float, then `max_features` is a fraction and
+          `int(max_features * n_features)` features are considered at each
+          split.
+        - If "auto", then `max_features=n_features`.
+        - If "sqrt", then `max_features=sqrt(n_features)`.
+        - If "log2", then `max_features=log2(n_features)`.
+        - If None, then `max_features=n_features`.
+
+        Note: the search for a split does not stop until at least one
+        valid partition of the node samples is found, even if it requires to
+        effectively inspect more than ``max_features`` features.
+
     max_leaf_nodes : int or None, optional (default=None)
         Grow trees with ``max_leaf_nodes`` in best-first fashion.
         Best nodes are defined as relative reduction in impurity.
         If None then unlimited number of leaf nodes.
 
-    min_impurity_split : float,
-        Threshold for early stopping in tree growth. A node will split
-        if its impurity is above the threshold, otherwise it is a leaf.
-
-        .. deprecated:: 0.19
-           ``min_impurity_split`` has been deprecated in favor of
-           ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.
-           Use ``min_impurity_decrease`` instead.
-
     min_impurity_decrease : float, optional (default=0.)
         A node will be split if this split induces a decrease of the impurity
         greater than or equal to this value.
@@ -1602,15 +1633,26 @@ class ExtraTreesRegressor(ForestRegressor):
 
         .. versionadded:: 0.19
 
+    min_impurity_split : float,
+        Threshold for early stopping in tree growth. A node will split
+        if its impurity is above the threshold, otherwise it is a leaf.
+
+        .. deprecated:: 0.19
+           ``min_impurity_split`` has been deprecated in favor of
+           ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.
+           Use ``min_impurity_decrease`` instead.
+
     bootstrap : boolean, optional (default=False)
         Whether bootstrap samples are used when building trees.
 
     oob_score : bool, optional (default=False)
         Whether to use out-of-bag samples to estimate the R^2 on unseen data.
 
-    n_jobs : integer, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of jobs to run in parallel for both `fit` and `predict`.
-        If -1, then the number of jobs is set to the number of cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
@@ -1666,7 +1708,7 @@ class ExtraTreesRegressor(ForestRegressor):
     RandomForestRegressor: Ensemble regressor using trees with optimal splits.
     """
     def __init__(self,
-                 n_estimators=10,
+                 n_estimators='warn',
                  criterion="mse",
                  max_depth=None,
                  min_samples_split=2,
@@ -1678,7 +1720,7 @@ def __init__(self,
                  min_impurity_split=None,
                  bootstrap=False,
                  oob_score=False,
-                 n_jobs=1,
+                 n_jobs=None,
                  random_state=None,
                  verbose=0,
                  warm_start=False):
@@ -1728,6 +1770,10 @@ class RandomTreesEmbedding(BaseForest):
     n_estimators : integer, optional (default=10)
         Number of trees in the forest.
 
+        .. versionchanged:: 0.20
+           The default value of ``n_estimators`` will change from 10 in
+           version 0.20 to 100 in version 0.22.
+
     max_depth : integer, optional (default=5)
         The maximum depth of each tree. If None, then nodes are expanded until
         all leaves are pure or until all leaves contain less than
@@ -1765,15 +1811,6 @@ class RandomTreesEmbedding(BaseForest):
         Best nodes are defined as relative reduction in impurity.
         If None then unlimited number of leaf nodes.
 
-    min_impurity_split : float,
-        Threshold for early stopping in tree growth. A node will split
-        if its impurity is above the threshold, otherwise it is a leaf.
-
-        .. deprecated:: 0.19
-           ``min_impurity_split`` has been deprecated in favor of
-           ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.
-           Use ``min_impurity_decrease`` instead.
-
     min_impurity_decrease : float, optional (default=0.)
         A node will be split if this split induces a decrease of the impurity
         greater than or equal to this value.
@@ -1792,16 +1829,24 @@ class RandomTreesEmbedding(BaseForest):
 
         .. versionadded:: 0.19
 
-    bootstrap : boolean, optional (default=True)
-        Whether bootstrap samples are used when building trees.
+    min_impurity_split : float,
+        Threshold for early stopping in tree growth. A node will split
+        if its impurity is above the threshold, otherwise it is a leaf.
+
+        .. deprecated:: 0.19
+           ``min_impurity_split`` has been deprecated in favor of
+           ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.
+           Use ``min_impurity_decrease`` instead.
 
     sparse_output : bool, optional (default=True)
         Whether or not to return a sparse CSR matrix, as default behavior,
         or to return a dense array compatible with dense pipeline operators.
 
-    n_jobs : integer, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of jobs to run in parallel for both `fit` and `predict`.
-        If -1, then the number of jobs is set to the number of cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
@@ -1833,7 +1878,7 @@ class RandomTreesEmbedding(BaseForest):
     """
 
     def __init__(self,
-                 n_estimators=10,
+                 n_estimators='warn',
                  max_depth=5,
                  min_samples_split=2,
                  min_samples_leaf=1,
@@ -1842,7 +1887,7 @@ def __init__(self,
                  min_impurity_decrease=0.,
                  min_impurity_split=None,
                  sparse_output=True,
-                 n_jobs=1,
+                 n_jobs=None,
                  random_state=None,
                  verbose=0,
                  warm_start=False):
diff --git a/sklearn/ensemble/gradient_boosting.py b/sklearn/ensemble/gradient_boosting.py
index 4edf4dd1fa68..2b6165d403b5 100644
--- a/sklearn/ensemble/gradient_boosting.py
+++ b/sklearn/ensemble/gradient_boosting.py
@@ -38,7 +38,6 @@
 import numbers
 import numpy as np
 
-from scipy import stats
 from scipy.sparse import csc_matrix
 from scipy.sparse import csr_matrix
 from scipy.sparse import issparse
@@ -64,20 +63,51 @@
 
 
 class QuantileEstimator(object):
-    """An estimator predicting the alpha-quantile of the training targets."""
+    """An estimator predicting the alpha-quantile of the training targets.
+
+    Parameters
+    ----------
+    alpha : float
+        The quantile
+    """
     def __init__(self, alpha=0.9):
         if not 0 < alpha < 1.0:
             raise ValueError("`alpha` must be in (0, 1.0) but was %r" % alpha)
         self.alpha = alpha
 
     def fit(self, X, y, sample_weight=None):
+        """Fit the estimator.
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
+            Training data
+
+        y : array, shape (n_samples, n_targets)
+            Target values. Will be cast to X's dtype if necessary
+
+        sample_weight : numpy array of shape (n_samples,)
+            Individual weights for each sample
+        """
         if sample_weight is None:
-            self.quantile = stats.scoreatpercentile(y, self.alpha * 100.0)
+            self.quantile = np.percentile(y, self.alpha * 100.0)
         else:
             self.quantile = _weighted_percentile(y, sample_weight,
                                                  self.alpha * 100.0)
 
     def predict(self, X):
+        """Predict labels
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
+            Samples.
+
+        Returns
+        -------
+        y : array, shape (n_samples,)
+            Returns predicted values.
+        """
         check_is_fitted(self, 'quantile')
 
         y = np.empty((X.shape[0], 1), dtype=np.float64)
@@ -88,12 +118,37 @@ def predict(self, X):
 class MeanEstimator(object):
     """An estimator predicting the mean of the training targets."""
     def fit(self, X, y, sample_weight=None):
+        """Fit the estimator.
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
+            Training data
+
+        y : array, shape (n_samples, n_targets)
+            Target values. Will be cast to X's dtype if necessary
+
+        sample_weight : numpy array of shape (n_samples,)
+            Individual weights for each sample
+        """
         if sample_weight is None:
             self.mean = np.mean(y)
         else:
             self.mean = np.average(y, weights=sample_weight)
 
     def predict(self, X):
+        """Predict labels
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
+            Samples.
+
+        Returns
+        -------
+        y : array, shape (n_samples,)
+            Returns predicted values.
+        """
         check_is_fitted(self, 'mean')
 
         y = np.empty((X.shape[0], 1), dtype=np.float64)
@@ -106,6 +161,19 @@ class LogOddsEstimator(object):
     scale = 1.0
 
     def fit(self, X, y, sample_weight=None):
+        """Fit the estimator.
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
+            Training data
+
+        y : array, shape (n_samples, n_targets)
+            Target values. Will be cast to X's dtype if necessary
+
+        sample_weight : numpy array of shape (n_samples,)
+            Individual weights for each sample
+        """
         # pre-cond: pos, neg are encoded as 1, 0
         if sample_weight is None:
             pos = np.sum(y)
@@ -119,6 +187,18 @@ def fit(self, X, y, sample_weight=None):
         self.prior = self.scale * np.log(pos / neg)
 
     def predict(self, X):
+        """Predict labels
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
+            Samples.
+
+        Returns
+        -------
+        y : array, shape (n_samples,)
+            Returns predicted values.
+        """
         check_is_fitted(self, 'prior')
 
         y = np.empty((X.shape[0], 1), dtype=np.float64)
@@ -136,12 +216,37 @@ class PriorProbabilityEstimator(object):
     class in the training data.
     """
     def fit(self, X, y, sample_weight=None):
+        """Fit the estimator.
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
+            Training data
+
+        y : array, shape (n_samples, n_targets)
+            Target values. Will be cast to X's dtype if necessary
+
+        sample_weight : array, shape (n_samples,)
+            Individual weights for each sample
+        """
         if sample_weight is None:
             sample_weight = np.ones_like(y, dtype=np.float64)
         class_counts = np.bincount(y, weights=sample_weight)
         self.priors = class_counts / class_counts.sum()
 
     def predict(self, X):
+        """Predict labels
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
+            Samples.
+
+        Returns
+        -------
+        y : array, shape (n_samples,)
+            Returns predicted values.
+        """
         check_is_fitted(self, 'priors')
 
         y = np.empty((X.shape[0], self.priors.shape[0]), dtype=np.float64)
@@ -153,6 +258,19 @@ class ZeroEstimator(object):
     """An estimator that simply predicts zero. """
 
     def fit(self, X, y, sample_weight=None):
+        """Fit the estimator.
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
+            Training data
+
+        y : numpy, shape (n_samples, n_targets)
+            Target values. Will be cast to X's dtype if necessary
+
+        sample_weight : array, shape (n_samples,)
+            Individual weights for each sample
+        """
         if np.issubdtype(y.dtype, np.signedinteger):
             # classification
             self.n_classes = np.unique(y).shape[0]
@@ -163,6 +281,18 @@ def fit(self, X, y, sample_weight=None):
             self.n_classes = 1
 
     def predict(self, X):
+        """Predict labels
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
+            Samples.
+
+        Returns
+        -------
+        y : array, shape (n_samples,)
+            Returns predicted values.
+        """
         check_is_fitted(self, 'n_classes')
 
         y = np.empty((X.shape[0], self.n_classes), dtype=np.float64)
@@ -173,6 +303,11 @@ def predict(self, X):
 class LossFunction(six.with_metaclass(ABCMeta, object)):
     """Abstract base class for various loss functions.
 
+    Parameters
+    ----------
+    n_classes : int
+        Number of classes
+
     Attributes
     ----------
     K : int
@@ -192,17 +327,30 @@ def init_estimator(self):
 
     @abstractmethod
     def __call__(self, y, pred, sample_weight=None):
-        """Compute the loss of prediction ``pred`` and ``y``. """
+        """Compute the loss.
+
+        Parameters
+        ----------
+        y : array, shape (n_samples,)
+            True labels
+
+        pred : array, shape (n_samples,)
+            Predicted labels
+
+        sample_weight : array-like, shape (n_samples,), optional
+            Sample weights.
+        """
 
     @abstractmethod
     def negative_gradient(self, y, y_pred, **kargs):
         """Compute the negative gradient.
 
         Parameters
-        ---------
-        y : np.ndarray, shape=(n,)
+        ----------
+        y : array, shape (n_samples,)
             The target labels.
-        y_pred : np.ndarray, shape=(n,):
+
+        y_pred : array, shape (n_samples,)
             The predictions.
         """
 
@@ -217,17 +365,17 @@ def update_terminal_regions(self, tree, X, y, residual, y_pred,
         ----------
         tree : tree.Tree
             The tree object.
-        X : ndarray, shape=(n, m)
+        X : array, shape (n, m)
             The data array.
-        y : ndarray, shape=(n,)
+        y : array, shape (n,)
             The target labels.
-        residual : ndarray, shape=(n,)
+        residual : array, shape (n,)
             The residuals (usually the negative gradient).
-        y_pred : ndarray, shape=(n,)
+        y_pred : array, shape (n,)
             The predictions.
-        sample_weight : ndarray, shape=(n,)
+        sample_weight : array, shape (n,)
             The weight of each sample.
-        sample_mask : ndarray, shape=(n,)
+        sample_mask : array, shape (n,)
             The sample mask to be used.
         learning_rate : float, default=0.1
             learning rate shrinks the contribution of each tree by
@@ -260,8 +408,13 @@ def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
 
 
 class RegressionLossFunction(six.with_metaclass(ABCMeta, LossFunction)):
-    """Base class for regression loss functions. """
+    """Base class for regression loss functions.
 
+    Parameters
+    ----------
+    n_classes : int
+        Number of classes
+    """
     def __init__(self, n_classes):
         if n_classes != 1:
             raise ValueError("``n_classes`` must be 1 for regression but "
@@ -271,11 +424,31 @@ def __init__(self, n_classes):
 
 class LeastSquaresError(RegressionLossFunction):
     """Loss function for least squares (LS) estimation.
-    Terminal regions need not to be updated for least squares. """
+    Terminal regions need not to be updated for least squares.
+
+    Parameters
+    ----------
+    n_classes : int
+        Number of classes
+    """
+
     def init_estimator(self):
         return MeanEstimator()
 
     def __call__(self, y, pred, sample_weight=None):
+        """Compute the least squares loss.
+
+        Parameters
+        ----------
+        y : array, shape (n_samples,)
+            True labels
+
+        pred : array, shape (n_samples,)
+            Predicted labels
+
+        sample_weight : array-like, shape (n_samples,), optional
+            Sample weights.
+        """
         if sample_weight is None:
             return np.mean((y - pred.ravel()) ** 2.0)
         else:
@@ -283,6 +456,16 @@ def __call__(self, y, pred, sample_weight=None):
                     np.sum(sample_weight * ((y - pred.ravel()) ** 2.0)))
 
     def negative_gradient(self, y, pred, **kargs):
+        """Compute the negative gradient.
+
+        Parameters
+        ----------
+        y : array, shape (n_samples,)
+            The target labels.
+
+        pred : array, shape (n_samples,)
+            The predictions.
+        """
         return y - pred.ravel()
 
     def update_terminal_regions(self, tree, X, y, residual, y_pred,
@@ -291,6 +474,28 @@ def update_terminal_regions(self, tree, X, y, residual, y_pred,
         """Least squares does not need to update terminal regions.
 
         But it has to update the predictions.
+
+        Parameters
+        ----------
+        tree : tree.Tree
+            The tree object.
+        X : array, shape (n, m)
+            The data array.
+        y : array, shape (n,)
+            The target labels.
+        residual : array, shape (n,)
+            The residuals (usually the negative gradient).
+        y_pred : array, shape (n,)
+            The predictions.
+        sample_weight : array, shape (n,)
+            The weight of each sample.
+        sample_mask : array, shape (n,)
+            The sample mask to be used.
+        learning_rate : float, default=0.1
+            learning rate shrinks the contribution of each tree by
+             ``learning_rate``.
+        k : int, default 0
+            The index of the estimator being updated.
         """
         # update predictions
         y_pred[:, k] += learning_rate * tree.predict(X).ravel()
@@ -301,11 +506,30 @@ def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
 
 
 class LeastAbsoluteError(RegressionLossFunction):
-    """Loss function for least absolute deviation (LAD) regression. """
+    """Loss function for least absolute deviation (LAD) regression.
+
+    Parameters
+    ----------
+    n_classes : int
+        Number of classes
+    """
     def init_estimator(self):
         return QuantileEstimator(alpha=0.5)
 
     def __call__(self, y, pred, sample_weight=None):
+        """Compute the least absolute error.
+
+        Parameters
+        ----------
+        y : array, shape (n_samples,)
+            True labels
+
+        pred : array, shape (n_samples,)
+            Predicted labels
+
+        sample_weight : array-like, shape (n_samples,), optional
+            Sample weights.
+        """
         if sample_weight is None:
             return np.abs(y - pred.ravel()).mean()
         else:
@@ -313,7 +537,18 @@ def __call__(self, y, pred, sample_weight=None):
                     np.sum(sample_weight * np.abs(y - pred.ravel())))
 
     def negative_gradient(self, y, pred, **kargs):
-        """1.0 if y - pred > 0.0 else -1.0"""
+        """Compute the negative gradient.
+
+        1.0 if y - pred > 0.0 else -1.0
+
+        Parameters
+        ----------
+        y : array, shape (n_samples,)
+            The target labels.
+
+        pred : array, shape (n_samples,)
+            The predictions.
+        """
         pred = pred.ravel()
         return 2.0 * (y - pred > 0.0) - 1.0
 
@@ -335,6 +570,14 @@ class HuberLossFunction(RegressionLossFunction):
     ----------
     J. Friedman, Greedy Function Approximation: A Gradient Boosting
     Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.
+
+    Parameters
+    ----------
+    n_classes : int
+        Number of classes
+
+    alpha : float
+        Percentile at which to extract score
     """
 
     def __init__(self, n_classes, alpha=0.9):
@@ -346,12 +589,25 @@ def init_estimator(self):
         return QuantileEstimator(alpha=0.5)
 
     def __call__(self, y, pred, sample_weight=None):
+        """Compute the Huber loss.
+
+        Parameters
+        ----------
+        y : array, shape (n_samples,)
+            True labels
+
+        pred : array, shape (n_samples,)
+            Predicted labels
+
+        sample_weight : array-like, shape (n_samples,), optional
+            Sample weights.
+        """
         pred = pred.ravel()
         diff = y - pred
         gamma = self.gamma
         if gamma is None:
             if sample_weight is None:
-                gamma = stats.scoreatpercentile(np.abs(diff), self.alpha * 100)
+                gamma = np.percentile(np.abs(diff), self.alpha * 100)
             else:
                 gamma = _weighted_percentile(np.abs(diff), sample_weight, self.alpha * 100)
 
@@ -368,10 +624,23 @@ def __call__(self, y, pred, sample_weight=None):
         return loss
 
     def negative_gradient(self, y, pred, sample_weight=None, **kargs):
+        """Compute the negative gradient.
+
+        Parameters
+        ----------
+        y : array, shape (n_samples,)
+            The target labels.
+
+        pred : array, shape (n_samples,)
+            The predictions.
+
+        sample_weight : array-like, shape (n_samples,), optional
+            Sample weights.
+        """
         pred = pred.ravel()
         diff = y - pred
         if sample_weight is None:
-            gamma = stats.scoreatpercentile(np.abs(diff), self.alpha * 100)
+            gamma = np.percentile(np.abs(diff), self.alpha * 100)
         else:
             gamma = _weighted_percentile(np.abs(diff), sample_weight, self.alpha * 100)
         gamma_mask = np.abs(diff) <= gamma
@@ -400,8 +669,15 @@ class QuantileLossFunction(RegressionLossFunction):
 
     Quantile regression allows to estimate the percentiles
     of the conditional distribution of the target.
-    """
 
+    Parameters
+    ----------
+    n_classes : int
+        Number of classes.
+
+    alpha : float, optional (default = 0.9)
+        The percentile
+    """
     def __init__(self, n_classes, alpha=0.9):
         super(QuantileLossFunction, self).__init__(n_classes)
         self.alpha = alpha
@@ -411,6 +687,19 @@ def init_estimator(self):
         return QuantileEstimator(self.alpha)
 
     def __call__(self, y, pred, sample_weight=None):
+        """Compute the Quantile loss.
+
+        Parameters
+        ----------
+        y : array, shape (n_samples,)
+            True labels
+
+        pred : array, shape (n_samples,)
+            Predicted labels
+
+        sample_weight : array-like, shape (n_samples,), optional
+            Sample weights.
+        """
         pred = pred.ravel()
         diff = y - pred
         alpha = self.alpha
@@ -426,6 +715,16 @@ def __call__(self, y, pred, sample_weight=None):
         return loss
 
     def negative_gradient(self, y, pred, **kargs):
+        """Compute the negative gradient.
+
+        Parameters
+        ----------
+        y : array, shape (n_samples,)
+            The target labels.
+
+        pred : array, shape (n_samples,)
+            The predictions.
+        """
         alpha = self.alpha
         pred = pred.ravel()
         mask = y > pred
@@ -465,6 +764,11 @@ class BinomialDeviance(ClassificationLossFunction):
 
     Binary classification is a special case; here, we only need to
     fit one tree instead of ``n_classes`` trees.
+
+    Parameters
+    ----------
+    n_classes : int
+        Number of classes.
     """
     def __init__(self, n_classes):
         if n_classes != 2:
@@ -477,7 +781,19 @@ def init_estimator(self):
         return LogOddsEstimator()
 
     def __call__(self, y, pred, sample_weight=None):
-        """Compute the deviance (= 2 * negative log-likelihood). """
+        """Compute the deviance (= 2 * negative log-likelihood).
+
+        Parameters
+        ----------
+        y : array, shape (n_samples,)
+            True labels
+
+        pred : array, shape (n_samples,)
+            Predicted labels
+
+        sample_weight : array-like, shape (n_samples,), optional
+            Sample weights.
+        """
         # logaddexp(0, v) == log(1.0 + exp(v))
         pred = pred.ravel()
         if sample_weight is None:
@@ -487,7 +803,16 @@ def __call__(self, y, pred, sample_weight=None):
                     np.sum(sample_weight * ((y * pred) - np.logaddexp(0.0, pred))))
 
     def negative_gradient(self, y, pred, **kargs):
-        """Compute the residual (= negative gradient). """
+        """Compute the residual (= negative gradient).
+
+        Parameters
+        ----------
+        y : array, shape (n_samples,)
+            True labels
+
+        pred : array, shape (n_samples,)
+            Predicted labels
+        """
         return y - expit(pred.ravel())
 
     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
@@ -530,6 +855,11 @@ class MultinomialDeviance(ClassificationLossFunction):
 
     For multi-class classification we need to fit ``n_classes`` trees at
     each stage.
+
+    Parameters
+    ----------
+    n_classes : int
+        Number of classes
     """
 
     is_multi_class = True
@@ -544,6 +874,19 @@ def init_estimator(self):
         return PriorProbabilityEstimator()
 
     def __call__(self, y, pred, sample_weight=None):
+        """Compute the Multinomial deviance.
+
+        Parameters
+        ----------
+        y : array, shape (n_samples,)
+            True labels
+
+        pred : array, shape (n_samples,)
+            Predicted labels
+
+        sample_weight : array-like, shape (n_samples,), optional
+            Sample weights.
+        """
         # create one-hot label encoding
         Y = np.zeros((y.shape[0], self.K), dtype=np.float64)
         for k in range(self.K):
@@ -557,7 +900,19 @@ def __call__(self, y, pred, sample_weight=None):
                           logsumexp(pred, axis=1))
 
     def negative_gradient(self, y, pred, k=0, **kwargs):
-        """Compute negative gradient for the ``k``-th class. """
+        """Compute negative gradient for the ``k``-th class.
+
+        Parameters
+        ----------
+        y : array, shape (n_samples,)
+            The target labels.
+
+        pred : array, shape (n_samples,)
+            The predictions.
+
+        k : int, optional (default=0)
+            The index of the class
+        """
         return y - np.nan_to_num(np.exp(pred[:, k] -
                                         logsumexp(pred, axis=1)))
 
@@ -598,6 +953,11 @@ class ExponentialLoss(ClassificationLossFunction):
     References
     ----------
     Greg Ridgeway, Generalized Boosted Models: A guide to the gbm package, 2007
+
+    Parameters
+    ----------
+    n_classes : int
+        Number of classes.
     """
     def __init__(self, n_classes):
         if n_classes != 2:
@@ -610,6 +970,19 @@ def init_estimator(self):
         return ScaledLogOddsEstimator()
 
     def __call__(self, y, pred, sample_weight=None):
+        """Compute the exponential loss
+
+        Parameters
+        ----------
+        y : array, shape (n_samples,)
+            True labels
+
+        pred : array, shape (n_samples,)
+            Predicted labels
+
+        sample_weight : array-like, shape (n_samples,), optional
+            Sample weights.
+        """
         pred = pred.ravel()
         if sample_weight is None:
             return np.mean(np.exp(-(2. * y - 1.) * pred))
@@ -618,6 +991,16 @@ def __call__(self, y, pred, sample_weight=None):
                     np.sum(sample_weight * np.exp(-(2 * y - 1) * pred)))
 
     def negative_gradient(self, y, pred, **kargs):
+        """Compute the residual (= negative gradient).
+
+        Parameters
+        ----------
+        y : array, shape (n_samples,)
+            True labels
+
+        pred : array, shape (n_samples,)
+            Predicted labels
+        """
         y_ = -(2. * y - 1.)
         return y_ * np.exp(y_ * pred.ravel())
 
@@ -664,15 +1047,28 @@ def _score_to_decision(self, score):
 class VerboseReporter(object):
     """Reports verbose output to stdout.
 
-    If ``verbose==1`` output is printed once in a while (when iteration mod
-    verbose_mod is zero).; if larger than 1 then output is printed for
-    each update.
+    Parameters
+    ----------
+    verbose : int
+        Verbosity level. If ``verbose==1`` output is printed once in a while
+        (when iteration mod verbose_mod is zero).; if larger than 1 then output
+        is printed for each update.
     """
 
     def __init__(self, verbose):
         self.verbose = verbose
 
     def init(self, est, begin_at_stage=0):
+        """Initialize reporter
+
+        Parameters
+        ----------
+        est : Estimator
+            The estimator
+
+        begin_at_stage : int
+            stage at which to begin reporting
+        """
         # header fields and line format str
         header_fields = ['Iter', 'Train Loss']
         verbose_fmt = ['{iter:>10d}', '{train_score:>16.4f}']
@@ -694,7 +1090,15 @@ def init(self, est, begin_at_stage=0):
         self.begin_at_stage = begin_at_stage
 
     def update(self, j, est):
-        """Update reporter with new iteration. """
+        """Update reporter with new iteration.
+
+        Parameters
+        ----------
+        j : int
+            The new iteration
+        est : Estimator
+            The estimator
+        """
         do_oob = est.subsample < 1
         # we need to take into account if we fit additional estimators.
         i = j - self.begin_at_stage  # iteration relative to the start iter
@@ -931,12 +1335,14 @@ def _resize_state(self):
             raise ValueError('resize with smaller n_estimators %d < %d' %
                              (total_n_estimators, self.estimators_[0]))
 
-        self.estimators_.resize((total_n_estimators, self.loss_.K))
-        self.train_score_.resize(total_n_estimators)
+        self.estimators_ = np.resize(self.estimators_,
+                                     (total_n_estimators, self.loss_.K))
+        self.train_score_ = np.resize(self.train_score_, total_n_estimators)
         if (self.subsample < 1 or hasattr(self, 'oob_improvement_')):
             # if do oob resize arrays or create new if not available
             if hasattr(self, 'oob_improvement_'):
-                self.oob_improvement_.resize(total_n_estimators)
+                self.oob_improvement_ = np.resize(self.oob_improvement_,
+                                                  total_n_estimators)
             else:
                 self.oob_improvement_ = np.zeros((total_n_estimators,),
                                                  dtype=np.float64)
@@ -959,16 +1365,16 @@ def fit(self, X, y, sample_weight=None, monitor=None):
 
         Parameters
         ----------
-        X : array-like, shape = [n_samples, n_features]
+        X : array-like, shape (n_samples, n_features)
             Training vectors, where n_samples is the number of samples
             and n_features is the number of features.
 
-        y : array-like, shape = [n_samples]
+        y : array-like, shape (n_samples,)
             Target values (strings or integers in classification, real numbers
             in regression)
             For classification, labels must correspond to classes.
 
-        sample_weight : array-like, shape = [n_samples] or None
+        sample_weight : array-like, shape (n_samples,) or None
             Sample weights. If None, then samples are equally weighted. Splits
             that would create child nodes with net zero or negative weight are
             ignored while searching for a split in each node. In the case of
@@ -1106,7 +1512,7 @@ def _fit_stages(self, X, y, y_pred, sample_weight, random_state,
         X_csr = csr_matrix(X) if issparse(X) else None
 
         if self.n_iter_no_change is not None:
-            loss_history = np.ones(self.n_iter_no_change) * np.inf
+            loss_history = np.full(self.n_iter_no_change, np.inf)
             # We create a generator to get the predictions for X_val after
             # the addition of each successive stage
             y_val_pred_iter = self._staged_decision_function(X_val)
@@ -1197,14 +1603,14 @@ def _staged_decision_function(self, X):
 
         Parameters
         ----------
-        X : array-like or sparse matrix, shape = [n_samples, n_features]
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
             The input samples. Internally, it will be converted to
             ``dtype=np.float32`` and if a sparse matrix is provided
             to a sparse ``csr_matrix``.
 
         Returns
         -------
-        score : generator of array, shape = [n_samples, k]
+        score : generator of array, shape (n_samples, k)
             The decision function of the input samples. The order of the
             classes corresponds to that in the attribute `classes_`.
             Regression and binary classification are special cases with
@@ -1223,7 +1629,7 @@ def feature_importances_(self):
 
         Returns
         -------
-        feature_importances_ : array, shape = [n_features]
+        feature_importances_ : array, shape (n_features,)
         """
         self._check_initialized()
 
@@ -1253,14 +1659,14 @@ def apply(self, X):
 
         Parameters
         ----------
-        X : array-like or sparse matrix, shape = [n_samples, n_features]
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
             The input samples. Internally, its dtype will be converted to
             ``dtype=np.float32``. If a sparse matrix is provided, it will
             be converted to a sparse ``csr_matrix``.
 
         Returns
         -------
-        X_leaves : array_like, shape = [n_samples, n_estimators, n_classes]
+        X_leaves : array-like, shape (n_samples, n_estimators, n_classes)
             For each datapoint x in X and for each tree in the ensemble,
             return the index of the leaf x ends up in each estimator.
             In the case of binary classification n_classes is 1.
@@ -1311,11 +1717,12 @@ class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):
         is fairly robust to over-fitting so a large number usually
         results in better performance.
 
-    max_depth : integer, optional (default=3)
-        maximum depth of the individual regression estimators. The maximum
-        depth limits the number of nodes in the tree. Tune this parameter
-        for best performance; the best value depends on the interaction
-        of the input variables.
+    subsample : float, optional (default=1.0)
+        The fraction of samples to be used for fitting the individual base
+        learners. If smaller than 1.0 this results in Stochastic Gradient
+        Boosting. `subsample` interacts with the parameter `n_estimators`.
+        Choosing `subsample < 1.0` leads to a reduction of variance
+        and an increase in bias.
 
     criterion : string, optional (default="friedman_mse")
         The function to measure the quality of a split. Supported criteria
@@ -1354,45 +1761,11 @@ class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):
         the input samples) required to be at a leaf node. Samples have
         equal weight when sample_weight is not provided.
 
-    subsample : float, optional (default=1.0)
-        The fraction of samples to be used for fitting the individual base
-        learners. If smaller than 1.0 this results in Stochastic Gradient
-        Boosting. `subsample` interacts with the parameter `n_estimators`.
-        Choosing `subsample < 1.0` leads to a reduction of variance
-        and an increase in bias.
-
-    max_features : int, float, string or None, optional (default=None)
-        The number of features to consider when looking for the best split:
-
-        - If int, then consider `max_features` features at each split.
-        - If float, then `max_features` is a fraction and
-          `int(max_features * n_features)` features are considered at each
-          split.
-        - If "auto", then `max_features=sqrt(n_features)`.
-        - If "sqrt", then `max_features=sqrt(n_features)`.
-        - If "log2", then `max_features=log2(n_features)`.
-        - If None, then `max_features=n_features`.
-
-        Choosing `max_features < n_features` leads to a reduction of variance
-        and an increase in bias.
-
-        Note: the search for a split does not stop until at least one
-        valid partition of the node samples is found, even if it requires to
-        effectively inspect more than ``max_features`` features.
-
-    max_leaf_nodes : int or None, optional (default=None)
-        Grow trees with ``max_leaf_nodes`` in best-first fashion.
-        Best nodes are defined as relative reduction in impurity.
-        If None then unlimited number of leaf nodes.
-
-    min_impurity_split : float,
-        Threshold for early stopping in tree growth. A node will split
-        if its impurity is above the threshold, otherwise it is a leaf.
-
-        .. deprecated:: 0.19
-           ``min_impurity_split`` has been deprecated in favor of
-           ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.
-           Use ``min_impurity_decrease`` instead.
+    max_depth : integer, optional (default=3)
+        maximum depth of the individual regression estimators. The maximum
+        depth limits the number of nodes in the tree. Tune this parameter
+        for best performance; the best value depends on the interaction
+        of the input variables.
 
     min_impurity_decrease : float, optional (default=0.)
         A node will be split if this split induces a decrease of the impurity
@@ -1412,27 +1785,60 @@ class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):
 
         .. versionadded:: 0.19
 
+    min_impurity_split : float,
+        Threshold for early stopping in tree growth. A node will split
+        if its impurity is above the threshold, otherwise it is a leaf.
+
+        .. deprecated:: 0.19
+           ``min_impurity_split`` has been deprecated in favor of
+           ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.
+           Use ``min_impurity_decrease`` instead.
+
     init : estimator, optional
         An estimator object that is used to compute the initial
         predictions. ``init`` has to provide ``fit`` and ``predict``.
         If None it uses ``loss.init_estimator``.
 
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
+
+    max_features : int, float, string or None, optional (default=None)
+        The number of features to consider when looking for the best split:
+
+        - If int, then consider `max_features` features at each split.
+        - If float, then `max_features` is a fraction and
+          `int(max_features * n_features)` features are considered at each
+          split.
+        - If "auto", then `max_features=sqrt(n_features)`.
+        - If "sqrt", then `max_features=sqrt(n_features)`.
+        - If "log2", then `max_features=log2(n_features)`.
+        - If None, then `max_features=n_features`.
+
+        Choosing `max_features < n_features` leads to a reduction of variance
+        and an increase in bias.
+
+        Note: the search for a split does not stop until at least one
+        valid partition of the node samples is found, even if it requires to
+        effectively inspect more than ``max_features`` features.
+
     verbose : int, default: 0
         Enable verbose output. If 1 then it prints progress and performance
         once in a while (the more trees the lower the frequency). If greater
         than 1 then it prints progress and performance for every tree.
 
+    max_leaf_nodes : int or None, optional (default=None)
+        Grow trees with ``max_leaf_nodes`` in best-first fashion.
+        Best nodes are defined as relative reduction in impurity.
+        If None then unlimited number of leaf nodes.
+
     warm_start : bool, default: False
         When set to ``True``, reuse the solution of the previous call to fit
         and add more estimators to the ensemble, otherwise, just erase the
         previous solution. See :term:`the Glossary <warm_start>`.
 
-    random_state : int, RandomState instance or None, optional (default=None)
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
-
     presort : bool or 'auto', optional (default='auto')
         Whether to presort the data to speed up the finding of best splits in
         fitting. Auto mode by default will use presorting on dense data and
@@ -1476,16 +1882,16 @@ class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):
 
         .. versionadded:: 0.20
 
-    feature_importances_ : array, shape = [n_features]
+    feature_importances_ : array, shape (n_features,)
         The feature importances (the higher, the more important the feature).
 
-    oob_improvement_ : array, shape = [n_estimators]
+    oob_improvement_ : array, shape (n_estimators,)
         The improvement in loss (= deviance) on the out-of-bag samples
         relative to the previous iteration.
         ``oob_improvement_[0]`` is the improvement in
         loss of the first stage over the ``init`` estimator.
 
-    train_score_ : array, shape = [n_estimators]
+    train_score_ : array, shape (n_estimators,)
         The i-th score ``train_score_[i]`` is the deviance (= loss) of the
         model at iteration ``i`` on the in-bag sample.
         If ``subsample == 1`` this is the deviance on the training data.
@@ -1497,7 +1903,8 @@ class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):
         The estimator that provides the initial predictions.
         Set via the ``init`` argument or ``loss.init_estimator``.
 
-    estimators_ : ndarray of DecisionTreeRegressor, shape = [n_estimators, ``loss_.K``]
+    estimators_ : ndarray of DecisionTreeRegressor,\
+shape (n_estimators, ``loss_.K``)
         The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary
         classification, otherwise n_classes.
 
@@ -1570,14 +1977,14 @@ def decision_function(self, X):
 
         Parameters
         ----------
-        X : array-like or sparse matrix, shape = [n_samples, n_features]
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
             The input samples. Internally, it will be converted to
             ``dtype=np.float32`` and if a sparse matrix is provided
             to a sparse ``csr_matrix``.
 
         Returns
         -------
-        score : array, shape = [n_samples, n_classes] or [n_samples]
+        score : array, shape (n_samples, n_classes) or (n_samples,)
             The decision function of the input samples. The order of the
             classes corresponds to that in the attribute `classes_`.
             Regression and binary classification produce an array of shape
@@ -1597,14 +2004,14 @@ def staged_decision_function(self, X):
 
         Parameters
         ----------
-        X : array-like or sparse matrix, shape = [n_samples, n_features]
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
             The input samples. Internally, it will be converted to
             ``dtype=np.float32`` and if a sparse matrix is provided
             to a sparse ``csr_matrix``.
 
         Returns
         -------
-        score : generator of array, shape = [n_samples, k]
+        score : generator of array, shape (n_samples, k)
             The decision function of the input samples. The order of the
             classes corresponds to that in the attribute `classes_`.
             Regression and binary classification are special cases with
@@ -1619,14 +2026,14 @@ def predict(self, X):
 
         Parameters
         ----------
-        X : array-like or sparse matrix, shape = [n_samples, n_features]
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
             The input samples. Internally, it will be converted to
             ``dtype=np.float32`` and if a sparse matrix is provided
             to a sparse ``csr_matrix``.
 
         Returns
         -------
-        y : array of shape = [n_samples]
+        y : array, shape (n_samples,)
             The predicted values.
         """
         score = self.decision_function(X)
@@ -1641,14 +2048,14 @@ def staged_predict(self, X):
 
         Parameters
         ----------
-        X : array-like or sparse matrix, shape = [n_samples, n_features]
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
             The input samples. Internally, it will be converted to
             ``dtype=np.float32`` and if a sparse matrix is provided
             to a sparse ``csr_matrix``.
 
         Returns
         -------
-        y : generator of array of shape = [n_samples]
+        y : generator of array of shape (n_samples,)
             The predicted value of the input samples.
         """
         for score in self._staged_decision_function(X):
@@ -1660,7 +2067,7 @@ def predict_proba(self, X):
 
         Parameters
         ----------
-        X : array-like or sparse matrix, shape = [n_samples, n_features]
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
             The input samples. Internally, it will be converted to
             ``dtype=np.float32`` and if a sparse matrix is provided
             to a sparse ``csr_matrix``.
@@ -1672,7 +2079,7 @@ def predict_proba(self, X):
 
         Returns
         -------
-        p : array of shape = [n_samples, n_classes]
+        p : array, shape (n_samples, n_classes)
             The class probabilities of the input samples. The order of the
             classes corresponds to that in the attribute `classes_`.
         """
@@ -1690,7 +2097,7 @@ def predict_log_proba(self, X):
 
         Parameters
         ----------
-        X : array-like or sparse matrix, shape = [n_samples, n_features]
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
             The input samples. Internally, it will be converted to
             ``dtype=np.float32`` and if a sparse matrix is provided
             to a sparse ``csr_matrix``.
@@ -1702,7 +2109,7 @@ def predict_log_proba(self, X):
 
         Returns
         -------
-        p : array of shape = [n_samples, n_classes]
+        p : array, shape (n_samples, n_classes)
             The class log-probabilities of the input samples. The order of the
             classes corresponds to that in the attribute `classes_`.
         """
@@ -1717,14 +2124,14 @@ def staged_predict_proba(self, X):
 
         Parameters
         ----------
-        X : array-like or sparse matrix, shape = [n_samples, n_features]
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
             The input samples. Internally, it will be converted to
             ``dtype=np.float32`` and if a sparse matrix is provided
             to a sparse ``csr_matrix``.
 
         Returns
         -------
-        y : generator of array of shape = [n_samples]
+        y : generator of array of shape (n_samples,)
             The predicted value of the input samples.
         """
         try:
@@ -1765,11 +2172,12 @@ class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):
         is fairly robust to over-fitting so a large number usually
         results in better performance.
 
-    max_depth : integer, optional (default=3)
-        maximum depth of the individual regression estimators. The maximum
-        depth limits the number of nodes in the tree. Tune this parameter
-        for best performance; the best value depends on the interaction
-        of the input variables.
+    subsample : float, optional (default=1.0)
+        The fraction of samples to be used for fitting the individual base
+        learners. If smaller than 1.0 this results in Stochastic Gradient
+        Boosting. `subsample` interacts with the parameter `n_estimators`.
+        Choosing `subsample < 1.0` leads to a reduction of variance
+        and an increase in bias.
 
     criterion : string, optional (default="friedman_mse")
         The function to measure the quality of a split. Supported criteria
@@ -1808,45 +2216,11 @@ class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):
         the input samples) required to be at a leaf node. Samples have
         equal weight when sample_weight is not provided.
 
-    subsample : float, optional (default=1.0)
-        The fraction of samples to be used for fitting the individual base
-        learners. If smaller than 1.0 this results in Stochastic Gradient
-        Boosting. `subsample` interacts with the parameter `n_estimators`.
-        Choosing `subsample < 1.0` leads to a reduction of variance
-        and an increase in bias.
-
-    max_features : int, float, string or None, optional (default=None)
-        The number of features to consider when looking for the best split:
-
-        - If int, then consider `max_features` features at each split.
-        - If float, then `max_features` is a fraction and
-          `int(max_features * n_features)` features are considered at each
-          split.
-        - If "auto", then `max_features=n_features`.
-        - If "sqrt", then `max_features=sqrt(n_features)`.
-        - If "log2", then `max_features=log2(n_features)`.
-        - If None, then `max_features=n_features`.
-
-        Choosing `max_features < n_features` leads to a reduction of variance
-        and an increase in bias.
-
-        Note: the search for a split does not stop until at least one
-        valid partition of the node samples is found, even if it requires to
-        effectively inspect more than ``max_features`` features.
-
-    max_leaf_nodes : int or None, optional (default=None)
-        Grow trees with ``max_leaf_nodes`` in best-first fashion.
-        Best nodes are defined as relative reduction in impurity.
-        If None then unlimited number of leaf nodes.
-
-    min_impurity_split : float,
-        Threshold for early stopping in tree growth. A node will split
-        if its impurity is above the threshold, otherwise it is a leaf.
-
-        .. deprecated:: 0.19
-           ``min_impurity_split`` has been deprecated in favor of
-           ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.
-           Use ``min_impurity_decrease`` instead.
+    max_depth : integer, optional (default=3)
+        maximum depth of the individual regression estimators. The maximum
+        depth limits the number of nodes in the tree. Tune this parameter
+        for best performance; the best value depends on the interaction
+        of the input variables.
 
     min_impurity_decrease : float, optional (default=0.)
         A node will be split if this split induces a decrease of the impurity
@@ -1866,31 +2240,64 @@ class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):
 
         .. versionadded:: 0.19
 
-    alpha : float (default=0.9)
-        The alpha-quantile of the huber loss function and the quantile
-        loss function. Only if ``loss='huber'`` or ``loss='quantile'``.
+    min_impurity_split : float,
+        Threshold for early stopping in tree growth. A node will split
+        if its impurity is above the threshold, otherwise it is a leaf.
+
+        .. deprecated:: 0.19
+           ``min_impurity_split`` has been deprecated in favor of
+           ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.
+           Use ``min_impurity_decrease`` instead.
 
     init : estimator, optional (default=None)
         An estimator object that is used to compute the initial
         predictions. ``init`` has to provide ``fit`` and ``predict``.
         If None it uses ``loss.init_estimator``.
 
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
+
+    max_features : int, float, string or None, optional (default=None)
+        The number of features to consider when looking for the best split:
+
+        - If int, then consider `max_features` features at each split.
+        - If float, then `max_features` is a fraction and
+          `int(max_features * n_features)` features are considered at each
+          split.
+        - If "auto", then `max_features=n_features`.
+        - If "sqrt", then `max_features=sqrt(n_features)`.
+        - If "log2", then `max_features=log2(n_features)`.
+        - If None, then `max_features=n_features`.
+
+        Choosing `max_features < n_features` leads to a reduction of variance
+        and an increase in bias.
+
+        Note: the search for a split does not stop until at least one
+        valid partition of the node samples is found, even if it requires to
+        effectively inspect more than ``max_features`` features.
+
+    alpha : float (default=0.9)
+        The alpha-quantile of the huber loss function and the quantile
+        loss function. Only if ``loss='huber'`` or ``loss='quantile'``.
+
     verbose : int, default: 0
         Enable verbose output. If 1 then it prints progress and performance
         once in a while (the more trees the lower the frequency). If greater
         than 1 then it prints progress and performance for every tree.
 
+    max_leaf_nodes : int or None, optional (default=None)
+        Grow trees with ``max_leaf_nodes`` in best-first fashion.
+        Best nodes are defined as relative reduction in impurity.
+        If None then unlimited number of leaf nodes.
+
     warm_start : bool, default: False
         When set to ``True``, reuse the solution of the previous call to fit
         and add more estimators to the ensemble, otherwise, just erase the
         previous solution. See :term:`the Glossary <warm_start>`.
 
-    random_state : int, RandomState instance or None, optional (default=None)
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
-
     presort : bool or 'auto', optional (default='auto')
         Whether to presort the data to speed up the finding of best splits in
         fitting. Auto mode by default will use presorting on dense data and
@@ -1928,16 +2335,16 @@ class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):
 
     Attributes
     ----------
-    feature_importances_ : array, shape = [n_features]
+    feature_importances_ : array, shape (n_features,)
         The feature importances (the higher, the more important the feature).
 
-    oob_improvement_ : array, shape = [n_estimators]
+    oob_improvement_ : array, shape (n_estimators,)
         The improvement in loss (= deviance) on the out-of-bag samples
         relative to the previous iteration.
         ``oob_improvement_[0]`` is the improvement in
         loss of the first stage over the ``init`` estimator.
 
-    train_score_ : array, shape = [n_estimators]
+    train_score_ : array, shape (n_estimators,)
         The i-th score ``train_score_[i]`` is the deviance (= loss) of the
         model at iteration ``i`` on the in-bag sample.
         If ``subsample == 1`` this is the deviance on the training data.
@@ -1949,7 +2356,7 @@ class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):
         The estimator that provides the initial predictions.
         Set via the ``init`` argument or ``loss.init_estimator``.
 
-    estimators_ : ndarray of DecisionTreeRegressor, shape = [n_estimators, 1]
+    estimators_ : array of DecisionTreeRegressor, shape (n_estimators, 1)
         The collection of fitted sub-estimators.
 
     Notes
@@ -2006,14 +2413,14 @@ def predict(self, X):
 
         Parameters
         ----------
-        X : array-like or sparse matrix, shape = [n_samples, n_features]
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
             The input samples. Internally, it will be converted to
             ``dtype=np.float32`` and if a sparse matrix is provided
             to a sparse ``csr_matrix``.
 
         Returns
         -------
-        y : array of shape = [n_samples]
+        y : array, shape (n_samples,)
             The predicted values.
         """
         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
@@ -2027,14 +2434,14 @@ def staged_predict(self, X):
 
         Parameters
         ----------
-        X : array-like or sparse matrix, shape = [n_samples, n_features]
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
             The input samples. Internally, it will be converted to
             ``dtype=np.float32`` and if a sparse matrix is provided
             to a sparse ``csr_matrix``.
 
         Returns
         -------
-        y : generator of array of shape = [n_samples]
+        y : generator of array of shape (n_samples,)
             The predicted value of the input samples.
         """
         for y in self._staged_decision_function(X):
@@ -2047,14 +2454,14 @@ def apply(self, X):
 
         Parameters
         ----------
-        X : array-like or sparse matrix, shape = [n_samples, n_features]
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
             The input samples. Internally, its dtype will be converted to
             ``dtype=np.float32``. If a sparse matrix is provided, it will
             be converted to a sparse ``csr_matrix``.
 
         Returns
         -------
-        X_leaves : array_like, shape = [n_samples, n_estimators]
+        X_leaves : array-like, shape (n_samples, n_estimators)
             For each datapoint x in X and for each tree in the ensemble,
             return the index of the leaf x ends up in each estimator.
         """
diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py
index a1eb7ccd286b..4ab267fc737a 100644
--- a/sklearn/ensemble/iforest.py
+++ b/sklearn/ensemble/iforest.py
@@ -5,7 +5,6 @@
 from __future__ import division
 
 import numpy as np
-import scipy as sp
 import warnings
 from warnings import warn
 from sklearn.utils.fixes import euler_gamma
@@ -70,6 +69,10 @@ class IsolationForest(BaseBagging, OutlierMixin):
         on the decision function. If 'auto', the decision function threshold is
         determined as in the original paper.
 
+        .. versionchanged:: 0.20
+           The default value of ``contamination`` will change from 0.1 in 0.20
+           to ``'auto'`` in 0.22.
+
     max_features : int or float, optional (default=1.0)
         The number of features to draw from X to train each base estimator.
 
@@ -81,9 +84,31 @@ class IsolationForest(BaseBagging, OutlierMixin):
         data sampled with replacement. If False, sampling without replacement
         is performed.
 
-    n_jobs : integer, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of jobs to run in parallel for both `fit` and `predict`.
-        If -1, then the number of jobs is set to the number of cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
+
+    behaviour : str, default='old'
+        Behaviour of the ``decision_function`` which can be either 'old' or
+        'new'. Passing ``behaviour='new'`` makes the ``decision_function``
+        change to match other anomaly detection algorithm API which will be
+        the default behaviour in the future. As explained in details in the
+        ``offset_`` attribute documentation, the ``decision_function`` becomes
+        dependent on the contamination parameter, in such a way that 0 becomes
+        its natural threshold to detect outliers.
+
+        .. versionadded:: 0.20
+           ``behaviour`` is added in 0.20 for back-compatibility purpose.
+
+        .. deprecated:: 0.20
+           ``behaviour='old'`` is deprecated in 0.20 and will not be possible
+           in 0.22.
+
+        .. deprecated:: 0.22
+           ``behaviour`` parameter will be deprecated in 0.22 and removed in
+           0.24.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
@@ -110,12 +135,16 @@ class IsolationForest(BaseBagging, OutlierMixin):
     offset_ : float
         Offset used to define the decision function from the raw scores.
         We have the relation: ``decision_function = score_samples - offset_``.
+        Assuming behaviour == 'new', ``offset_`` is defined as follows.
         When the contamination parameter is set to "auto", the offset is equal
         to -0.5 as the scores of inliers are close to 0 and the scores of
         outliers are close to -1. When a contamination parameter different
         than "auto" is provided, the offset is defined in such a way we obtain
         the expected number of outliers (samples with decision function < 0)
         in training.
+        Assuming the behaviour parameter is set to 'old', we always have
+        ``offset_ = -0.5``, making the decision function independent from the
+        contamination parameter.
 
     References
     ----------
@@ -133,7 +162,8 @@ def __init__(self,
                  contamination="legacy",
                  max_features=1.,
                  bootstrap=False,
-                 n_jobs=1,
+                 n_jobs=None,
+                 behaviour='old',
                  random_state=None,
                  verbose=0):
         super(IsolationForest, self).__init__(
@@ -151,11 +181,7 @@ def __init__(self,
             random_state=random_state,
             verbose=verbose)
 
-        if contamination == "legacy":
-            warnings.warn('default contamination parameter 0.1 will change '
-                          'in version 0.22 to "auto". This will change the '
-                          'predict method behavior.',
-                          DeprecationWarning)
+        self.behaviour = behaviour
         self.contamination = contamination
 
     def _set_oob_score(self, X, y):
@@ -178,6 +204,22 @@ def fit(self, X, y=None, sample_weight=None):
         -------
         self : object
         """
+        if self.contamination == "legacy":
+            warnings.warn('default contamination parameter 0.1 will change '
+                          'in version 0.22 to "auto". This will change the '
+                          'predict method behavior.',
+                          FutureWarning)
+            self._contamination = 0.1
+        else:
+            self._contamination = self.contamination
+
+        if self.behaviour == 'old':
+            warnings.warn('behaviour="old" is deprecated and will be removed '
+                          'in version 0.22. Please use behaviour="new", which '
+                          'makes the decision_function change to match '
+                          'other anomaly detection algorithm API.',
+                          FutureWarning)
+
         X = check_array(X, accept_sparse=['csc'])
         if issparse(X):
             # Pre-sort indices to avoid that each individual tree of the
@@ -219,19 +261,29 @@ def fit(self, X, y=None, sample_weight=None):
                                           max_depth=max_depth,
                                           sample_weight=sample_weight)
 
-        if self.contamination == "auto":
+        if self.behaviour == 'old':
+            # in this case, decision_function = 0.5 + self.score_samples(X):
+            if self._contamination == "auto":
+                raise ValueError("contamination parameter cannot be set to "
+                                 "'auto' when behaviour == 'old'.")
+
+            self.offset_ = -0.5
+            self._threshold_ = np.percentile(self.decision_function(X),
+                                             100. * self._contamination)
+
+            return self
+
+        # else, self.behaviour == 'new':
+        if self._contamination == "auto":
             # 0.5 plays a special role as described in the original paper.
             # we take the opposite as we consider the opposite of their score.
             self.offset_ = -0.5
-            # need to save (depreciated) threshold_ in this case:
-            self._threshold_ = sp.stats.scoreatpercentile(
-                self.score_samples(X), 100. * 0.1)
-        elif self.contamination == "legacy":  # to be rm in 0.22
-            self.offset_ = sp.stats.scoreatpercentile(
-                self.score_samples(X), 100. * 0.1)
-        else:
-            self.offset_ = sp.stats.scoreatpercentile(
-                self.score_samples(X), 100. * self.contamination)
+            return self
+
+        # else, define offset_ wrt contamination parameter, so that the
+        # threshold_ attribute is implicitly 0 and is not needed anymore:
+        self.offset_ = np.percentile(self.score_samples(X),
+                                     100. * self._contamination)
 
         return self
 
@@ -254,7 +306,8 @@ def predict(self, X):
         check_is_fitted(self, ["offset_"])
         X = check_array(X, accept_sparse='csr')
         is_inlier = np.ones(X.shape[0], dtype=int)
-        is_inlier[self.decision_function(X) < 0] = -1
+        threshold = self.threshold_ if self.behaviour == 'old' else 0
+        is_inlier[self.decision_function(X) < threshold] = -1
         return is_inlier
 
     def decision_function(self, X):
@@ -355,11 +408,12 @@ def score_samples(self, X):
 
     @property
     def threshold_(self):
+        if self.behaviour != 'old':
+            raise AttributeError("threshold_ attribute does not exist when "
+                                 "behaviour != 'old'")
         warnings.warn("threshold_ attribute is deprecated in 0.20 and will"
                       " be removed in 0.22.", DeprecationWarning)
-        if self.contamination == 'auto':
-            return self._threshold_
-        return self.offset_
+        return self._threshold_
 
 
 def _average_path_length(n_samples_leaf):
diff --git a/sklearn/ensemble/partial_dependence.py b/sklearn/ensemble/partial_dependence.py
index e8bfc2110bb9..f8d5ca7f240a 100644
--- a/sklearn/ensemble/partial_dependence.py
+++ b/sklearn/ensemble/partial_dependence.py
@@ -10,7 +10,7 @@
 from scipy.stats.mstats import mquantiles
 
 from ..utils.extmath import cartesian
-from ..externals.joblib import Parallel, delayed
+from ..utils import Parallel, delayed
 from ..externals import six
 from ..externals.six.moves import map, range, zip
 from ..utils import check_array
@@ -165,7 +165,7 @@ def partial_dependence(gbrt, target_variables, grid=None, X=None,
 
 def plot_partial_dependence(gbrt, X, features, feature_names=None,
                             label=None, n_cols=3, grid_resolution=100,
-                            percentiles=(0.05, 0.95), n_jobs=1,
+                            percentiles=(0.05, 0.95), n_jobs=None,
                             verbose=0, ax=None, line_kw=None,
                             contour_kw=None, **fig_kw):
     """Partial dependence plots for ``features``.
@@ -198,14 +198,15 @@ def plot_partial_dependence(gbrt, X, features, feature_names=None,
         Only if gbrt is a multi-class model. Must be in ``gbrt.classes_``.
     n_cols : int
         The number of columns in the grid plot (default: 3).
+    grid_resolution : int, default=100
+        The number of equally spaced points on the axes.
     percentiles : (low, high), default=(0.05, 0.95)
         The lower and upper percentile used to create the extreme values
         for the PDP axes.
-    grid_resolution : int, default=100
-        The number of equally spaced points on the axes.
-    n_jobs : int
-        The number of CPUs to use to compute the PDs. -1 means 'all CPUs'.
-        Defaults to 1.
+    n_jobs : int or None, optional (default=None)
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
     verbose : int
         Verbose output during PD computations. Defaults to 0.
     ax : Matplotlib axis object, default None
@@ -216,7 +217,7 @@ def plot_partial_dependence(gbrt, X, features, feature_names=None,
     contour_kw : dict
         Dict with keywords passed to the ``matplotlib.pyplot.plot`` call.
         For two-way partial dependence plots.
-    fig_kw : dict
+    **fig_kw : dict
         Dict with keywords passed to the figure() call.
         Note that all keywords not recognized above will be automatically
         included here.
diff --git a/sklearn/ensemble/tests/test_bagging.py b/sklearn/ensemble/tests/test_bagging.py
index 626b34f58e5a..505ec2f17b24 100644
--- a/sklearn/ensemble/tests/test_bagging.py
+++ b/sklearn/ensemble/tests/test_bagging.py
@@ -5,6 +5,7 @@
 # Author: Gilles Louppe
 # License: BSD 3 clause
 
+import pytest
 import numpy as np
 
 from sklearn.base import BaseEstimator
@@ -28,12 +29,13 @@
 from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
 from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
 from sklearn.svm import SVC, SVR
+from sklearn.random_projection import SparseRandomProjection
 from sklearn.pipeline import make_pipeline
 from sklearn.feature_selection import SelectKBest
 from sklearn.model_selection import train_test_split
 from sklearn.datasets import load_boston, load_iris, make_hastie_10_2
-from sklearn.utils import check_random_state
-from sklearn.preprocessing import Imputer
+from sklearn.utils import check_random_state, hash
+from sklearn.preprocessing import FunctionTransformer
 
 from scipy.sparse import csc_matrix, csr_matrix
 
@@ -221,6 +223,13 @@ def fit(self, X, y):
             assert_array_almost_equal(sparse_results, dense_results)
 
 
+class DummySizeEstimator(BaseEstimator):
+
+    def fit(self, X, y):
+        self.training_size_ = X.shape[0]
+        self.training_hash_ = hash(X)
+
+
 def test_bootstrap_samples():
     # Test that bootstrapping samples generate non-perfect base estimators.
     rng = check_random_state(0)
@@ -248,6 +257,17 @@ def test_bootstrap_samples():
     assert_greater(base_estimator.score(X_train, y_train),
                    ensemble.score(X_train, y_train))
 
+    # check that each sampling correspond to a complete bootstrap resample.
+    # the size of each bootstrap should be the same as the input data but
+    # the data should be different (checked using the hash of the data).
+    ensemble = BaggingRegressor(base_estimator=DummySizeEstimator(),
+                                bootstrap=True).fit(X_train, y_train)
+    training_hash = []
+    for estimator in ensemble.estimators_:
+        assert estimator.training_size_ == X_train.shape[0]
+        training_hash.append(estimator.training_hash_)
+    assert len(set(training_hash)) == len(training_hash)
+
 
 def test_bootstrap_features():
     # Test that bootstrapping features may generate duplicate features.
@@ -496,6 +516,8 @@ def test_parallel_regression():
     assert_array_almost_equal(y1, y3)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_gridsearch():
     # Check that bagging ensembles can be grid-searched.
     # Transform iris into a binary classification task
@@ -707,8 +729,8 @@ def test_estimators_samples():
 
     # Test for correct formatting
     assert_equal(len(estimators_samples), len(estimators))
-    assert_equal(len(estimators_samples[0]), len(X))
-    assert_equal(estimators_samples[0].dtype.kind, 'b')
+    assert_equal(len(estimators_samples[0]), len(X) // 2)
+    assert_equal(estimators_samples[0].dtype.kind, 'i')
 
     # Re-fit single estimator to test for consistent sampling
     estimator_index = 0
@@ -726,6 +748,34 @@ def test_estimators_samples():
     assert_array_almost_equal(orig_coefs, new_coefs)
 
 
+def test_estimators_samples_deterministic():
+    # This test is a regression test to check that with a random step
+    # (e.g. SparseRandomProjection) and a given random state, the results
+    # generated at fit time can be identically reproduced at a later time using
+    # data saved in object attributes. Check issue #9524 for full discussion.
+
+    iris = load_iris()
+    X, y = iris.data, iris.target
+
+    base_pipeline = make_pipeline(SparseRandomProjection(n_components=2),
+                                  LogisticRegression())
+    clf = BaggingClassifier(base_estimator=base_pipeline,
+                            max_samples=0.5,
+                            random_state=0)
+    clf.fit(X, y)
+    pipeline_estimator_coef = clf.estimators_[0].steps[-1][1].coef_.copy()
+
+    estimator = clf.estimators_[0]
+    estimator_sample = clf.estimators_samples_[0]
+    estimator_feature = clf.estimators_features_[0]
+
+    X_train = (X[estimator_sample])[:, estimator_feature]
+    y_train = y[estimator_sample]
+
+    estimator.fit(X_train, y_train)
+    assert_array_equal(estimator.steps[-1][1].coef_, pipeline_estimator_coef)
+
+
 def test_max_samples_consistency():
     # Make sure validated max_samples and original max_samples are identical
     # when valid integer max_samples supplied by user
@@ -755,6 +805,12 @@ def test_set_oob_score_label_encoding():
     assert_equal([x1, x2], [x3, x3])
 
 
+def replace(X):
+    X = X.copy().astype('float')
+    X[~np.isfinite(X)] = 0
+    return X
+
+
 def test_bagging_regressor_with_missing_inputs():
     # Check that BaggingRegressor can accept X with missing/infinite data
     X = np.array([
@@ -777,9 +833,7 @@ def test_bagging_regressor_with_missing_inputs():
     for y in y_values:
         regressor = DecisionTreeRegressor()
         pipeline = make_pipeline(
-            Imputer(),
-            Imputer(missing_values=np.inf),
-            Imputer(missing_values=np.NINF),
+            FunctionTransformer(replace, validate=False),
             regressor
         )
         pipeline.fit(X, y).predict(X)
@@ -807,9 +861,7 @@ def test_bagging_classifier_with_missing_inputs():
     y = np.array([3, 6, 6, 6, 6])
     classifier = DecisionTreeClassifier()
     pipeline = make_pipeline(
-        Imputer(),
-        Imputer(missing_values=np.inf),
-        Imputer(missing_values=np.NINF),
+        FunctionTransformer(replace, validate=False),
         classifier
     )
     pipeline.fit(X, y).predict(X)
diff --git a/sklearn/ensemble/tests/test_forest.py b/sklearn/ensemble/tests/test_forest.py
index 0054707ba5a0..d7586c286657 100644
--- a/sklearn/ensemble/tests/test_forest.py
+++ b/sklearn/ensemble/tests/test_forest.py
@@ -21,6 +21,10 @@
 
 import pytest
 
+from sklearn.utils import parallel_backend
+from sklearn.utils import register_parallel_backend
+from sklearn.externals.joblib.parallel import LokyBackend
+
 from sklearn.utils.testing import assert_almost_equal
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_array_equal
@@ -31,6 +35,7 @@
 from sklearn.utils.testing import assert_raises
 from sklearn.utils.testing import assert_warns
 from sklearn.utils.testing import assert_warns_message
+from sklearn.utils.testing import assert_no_warnings
 from sklearn.utils.testing import ignore_warnings
 
 from sklearn import datasets
@@ -186,6 +191,7 @@ def check_regressor_attributes(name):
     assert_false(hasattr(r, "n_classes_"))
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 @pytest.mark.parametrize('name', FOREST_REGRESSORS)
 def test_regressor_attributes(name):
     check_regressor_attributes(name)
@@ -432,17 +438,19 @@ def check_oob_score_raise_error(name):
                                                   bootstrap=False).fit, X, y)
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 @pytest.mark.parametrize('name', FOREST_ESTIMATORS)
 def test_oob_score_raise_error(name):
     check_oob_score_raise_error(name)
 
-
 def check_gridsearch(name):
     forest = FOREST_CLASSIFIERS[name]()
     clf = GridSearchCV(forest, {'n_estimators': (1, 2), 'max_depth': (1, 2)})
     clf.fit(iris.data, iris.target)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 @pytest.mark.parametrize('name', FOREST_CLASSIFIERS)
 def test_gridsearch(name):
     # Check that base trees can be grid-searched.
@@ -489,6 +497,7 @@ def check_pickle(name, X, y):
     assert_equal(score, score2)
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 @pytest.mark.parametrize('name', FOREST_CLASSIFIERS_REGRESSORS)
 def test_pickle(name):
     if name in FOREST_CLASSIFIERS:
@@ -526,6 +535,7 @@ def check_multioutput(name):
             assert_equal(log_proba[1].shape, (4, 4))
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 @pytest.mark.parametrize('name', FOREST_CLASSIFIERS_REGRESSORS)
 def test_multioutput(name):
     check_multioutput(name)
@@ -549,6 +559,7 @@ def check_classes_shape(name):
     assert_array_equal(clf.classes_, [[-1, 1], [-2, 2]])
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 @pytest.mark.parametrize('name', FOREST_CLASSIFIERS)
 def test_classes_shape(name):
     check_classes_shape(name)
@@ -738,6 +749,7 @@ def check_min_samples_split(name):
                    "Failed with {0}".format(name))
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 @pytest.mark.parametrize('name', FOREST_ESTIMATORS)
 def test_min_samples_split(name):
     check_min_samples_split(name)
@@ -775,6 +787,7 @@ def check_min_samples_leaf(name):
                    "Failed with {0}".format(name))
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 @pytest.mark.parametrize('name', FOREST_ESTIMATORS)
 def test_min_samples_leaf(name):
     check_min_samples_leaf(name)
@@ -842,6 +855,7 @@ def check_sparse_input(name, X, X_sparse, y):
                                   dense.fit_transform(X).toarray())
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 @pytest.mark.parametrize('name', FOREST_ESTIMATORS)
 @pytest.mark.parametrize('sparse_matrix',
                          (csr_matrix, csc_matrix, coo_matrix))
@@ -899,6 +913,7 @@ def check_memory_layout(name, dtype):
     assert_array_almost_equal(est.fit(X, y).predict(X), y)
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 @pytest.mark.parametrize('name', FOREST_CLASSIFIERS_REGRESSORS)
 @pytest.mark.parametrize('dtype', (np.float64, np.float32))
 def test_memory_layout(name, dtype):
@@ -977,6 +992,7 @@ def check_class_weights(name):
     clf.fit(iris.data, iris.target, sample_weight=sample_weight)
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 @pytest.mark.parametrize('name', FOREST_CLASSIFIERS)
 def test_class_weights(name):
     check_class_weights(name)
@@ -996,6 +1012,7 @@ def check_class_weight_balanced_and_bootstrap_multi_output(name):
     clf.fit(X, _y)
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 @pytest.mark.parametrize('name', FOREST_CLASSIFIERS)
 def test_class_weight_balanced_and_bootstrap_multi_output(name):
     check_class_weight_balanced_and_bootstrap_multi_output(name)
@@ -1026,6 +1043,7 @@ def check_class_weight_errors(name):
     assert_raises(ValueError, clf.fit, X, _y)
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 @pytest.mark.parametrize('name', FOREST_CLASSIFIERS)
 def test_class_weight_errors(name):
     check_class_weight_errors(name)
@@ -1163,6 +1181,7 @@ def test_warm_start_oob(name):
     check_warm_start_oob(name)
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_dtype_convert(n_classes=15):
     classifier = RandomForestClassifier(random_state=0, bootstrap=False)
 
@@ -1201,6 +1220,7 @@ def test_decision_path(name):
     check_decision_path(name)
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_min_impurity_split():
     # Test if min_impurity_split of base estimators is set
     # Regression test for #8006
@@ -1216,6 +1236,7 @@ def test_min_impurity_split():
             assert_equal(tree.min_impurity_split, 0.1)
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_min_impurity_decrease():
     X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)
     all_estimators = [RandomForestClassifier, RandomForestRegressor,
@@ -1228,3 +1249,49 @@ def test_min_impurity_decrease():
             # Simply check if the parameter is passed on correctly. Tree tests
             # will suffice for the actual working of this param
             assert_equal(tree.min_impurity_decrease, 0.1)
+
+
+@pytest.mark.parametrize('forest',
+                         [RandomForestClassifier, RandomForestRegressor,
+                          ExtraTreesClassifier, ExtraTreesRegressor,
+                          RandomTreesEmbedding])
+def test_nestimators_future_warning(forest):
+    # FIXME: to be removed 0.22
+
+    # When n_estimators default value is used
+    msg_future = ("The default value of n_estimators will change from "
+                  "10 in version 0.20 to 100 in 0.22.")
+    est = forest()
+    est = assert_warns_message(FutureWarning, msg_future, est.fit, X, y)
+
+    # When n_estimators is a valid value not equal to the default
+    est = forest(n_estimators=100)
+    est = assert_no_warnings(est.fit, X, y)
+
+
+class MyBackend(LokyBackend):
+    def __init__(self, *args, **kwargs):
+        self.count = 0
+        super(MyBackend, self).__init__(*args, **kwargs)
+
+    def start_call(self):
+        self.count += 1
+        return super(MyBackend, self).start_call()
+
+
+register_parallel_backend('testing', MyBackend)
+
+
+def test_backend_respected():
+    clf = RandomForestClassifier(n_estimators=10, n_jobs=2)
+
+    with parallel_backend("testing") as (ba, _):
+        clf.fit(X, y)
+
+    assert ba.count > 0
+
+    # predict_proba requires shared memory. Ensure that's honored.
+    with parallel_backend("testing") as (ba, _):
+        clf.predict_proba(X)
+
+    assert ba.count == 0
diff --git a/sklearn/ensemble/tests/test_iforest.py b/sklearn/ensemble/tests/test_iforest.py
index 3833227ecfc2..634f45a25cf4 100644
--- a/sklearn/ensemble/tests/test_iforest.py
+++ b/sklearn/ensemble/tests/test_iforest.py
@@ -15,6 +15,7 @@
 from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_raises
+from sklearn.utils.testing import assert_raises_regex
 from sklearn.utils.testing import assert_warns_message
 from sklearn.utils.testing import assert_equal
 from sklearn.utils.testing import assert_greater
@@ -47,6 +48,7 @@
 boston.target = boston.target[perm]
 
 
+@pytest.mark.filterwarnings('ignore:threshold_ attribute')
 def test_iforest():
     """Check Isolation Forest for various parameter settings."""
     X_train = np.array([[0, 1], [1, 2]])
@@ -62,6 +64,9 @@ def test_iforest():
                             **params).fit(X_train).predict(X_test)
 
 
+@pytest.mark.filterwarnings('ignore:default contamination')
+@pytest.mark.filterwarnings('ignore:threshold_ attribute')
+@pytest.mark.filterwarnings('ignore:behaviour="old"')
 def test_iforest_sparse():
     """Check IForest for various parameter settings on sparse input."""
     rng = check_random_state(0)
@@ -89,6 +94,9 @@ def test_iforest_sparse():
             assert_array_equal(sparse_results, dense_results)
 
 
+@pytest.mark.filterwarnings('ignore:default contamination')
+@pytest.mark.filterwarnings('ignore:threshold_ attribute')
+@pytest.mark.filterwarnings('ignore:behaviour="old"')
 def test_iforest_error():
     """Test that it gives proper exception on deficient input."""
     X = iris.data
@@ -126,7 +134,14 @@ def test_iforest_error():
     # test X_test n_features match X_train one:
     assert_raises(ValueError, IsolationForest().fit(X).predict, X[:, 1:])
 
+    # test threshold_ attribute error when behaviour is not old:
+    msg = "threshold_ attribute does not exist when behaviour != 'old'"
+    assert_raises_regex(AttributeError, msg, getattr,
+                        IsolationForest(behaviour='new'), 'threshold_')
 
+
+@pytest.mark.filterwarnings('ignore:default contamination')
+@pytest.mark.filterwarnings('ignore:behaviour="old"')
 def test_recalculate_max_depth():
     """Check max_depth recalculation when max_samples is reset to n_samples"""
     X = iris.data
@@ -135,6 +150,8 @@ def test_recalculate_max_depth():
         assert_equal(est.max_depth, int(np.ceil(np.log2(X.shape[0]))))
 
 
+@pytest.mark.filterwarnings('ignore:default contamination')
+@pytest.mark.filterwarnings('ignore:behaviour="old"')
 def test_max_samples_attribute():
     X = iris.data
     clf = IsolationForest().fit(X)
@@ -150,6 +167,9 @@ def test_max_samples_attribute():
     assert_equal(clf.max_samples_, 0.4*X.shape[0])
 
 
+@pytest.mark.filterwarnings('ignore:default contamination')
+@pytest.mark.filterwarnings('ignore:threshold_ attribute')
+@pytest.mark.filterwarnings('ignore:behaviour="old"')
 def test_iforest_parallel_regression():
     """Check parallel regression."""
     rng = check_random_state(0)
@@ -174,6 +194,8 @@ def test_iforest_parallel_regression():
     assert_array_almost_equal(y1, y3)
 
 
+@pytest.mark.filterwarnings('ignore:default contamination')
+@pytest.mark.filterwarnings('ignore:behaviour="old"')
 def test_iforest_performance():
     """Test Isolation Forest performs well"""
 
@@ -198,13 +220,15 @@ def test_iforest_performance():
     assert_greater(roc_auc_score(y_test, y_pred), 0.98)
 
 
+@pytest.mark.filterwarnings('ignore:threshold_ attribute')
 def test_iforest_works():
     # toy sample (the last two samples are outliers)
     X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], [6, 3], [-4, 7]]
 
     # Test IsolationForest
     for contamination in [0.25, "auto"]:
-        clf = IsolationForest(random_state=rng, contamination=contamination)
+        clf = IsolationForest(behaviour='new', random_state=rng,
+                              contamination=contamination)
         clf.fit(X)
         decision_func = - clf.decision_function(X)
         pred = clf.predict(X)
@@ -213,6 +237,8 @@ def test_iforest_works():
         assert_array_equal(pred, 6 * [1] + 2 * [-1])
 
 
+@pytest.mark.filterwarnings('ignore:default contamination')
+@pytest.mark.filterwarnings('ignore:behaviour="old"')
 def test_max_samples_consistency():
     # Make sure validated max_samples in iforest and BaseBagging are identical
     X = iris.data
@@ -220,6 +246,9 @@ def test_max_samples_consistency():
     assert_equal(clf.max_samples_, clf._max_samples)
 
 
+@pytest.mark.filterwarnings('ignore:default contamination')
+@pytest.mark.filterwarnings('ignore:threshold_ attribute')
+@pytest.mark.filterwarnings('ignore:behaviour="old"')
 def test_iforest_subsampled_features():
     # It tests non-regression for #5732 which failed at predict.
     rng = check_random_state(0)
@@ -244,6 +273,8 @@ def test_iforest_average_path_length():
                               [1., result_one, result_two], decimal=10)
 
 
+@pytest.mark.filterwarnings('ignore:default contamination')
+@pytest.mark.filterwarnings('ignore:behaviour="old"')
 def test_score_samples():
     X_train = [[1, 1], [1, 2], [2, 1]]
     clf1 = IsolationForest(contamination=0.1).fit(X_train)
@@ -256,14 +287,34 @@ def test_score_samples():
                        clf2.score_samples([[2., 2.]]))
 
 
+@pytest.mark.filterwarnings('ignore:default contamination')
+@pytest.mark.filterwarnings('ignore:behaviour="old"')
 def test_deprecation():
-    assert_warns_message(DeprecationWarning,
+    X = [[0.0], [1.0]]
+    clf = IsolationForest()
+
+    assert_warns_message(FutureWarning,
                          'default contamination parameter 0.1 will change '
                          'in version 0.22 to "auto"',
-                         IsolationForest, )
-    X = [[0.0], [1.0]]
+                         clf.fit, X)
+
+    assert_warns_message(FutureWarning,
+                         'behaviour="old" is deprecated and will be removed '
+                         'in version 0.22',
+                         clf.fit, X)
+
     clf = IsolationForest().fit(X)
     assert_warns_message(DeprecationWarning,
                          "threshold_ attribute is deprecated in 0.20 and will"
                          " be removed in 0.22.",
                          getattr, clf, "threshold_")
+
+
+@pytest.mark.filterwarnings('ignore:default contamination')
+@pytest.mark.filterwarnings('ignore:behaviour="old"')
+def test_behaviour_param():
+    X_train = [[1, 1], [1, 2], [2, 1]]
+    clf1 = IsolationForest(behaviour='old').fit(X_train)
+    clf2 = IsolationForest(behaviour='new', contamination='auto').fit(X_train)
+    assert_array_equal(clf1.decision_function([[2., 2.]]),
+                       clf2.decision_function([[2., 2.]]))
diff --git a/sklearn/ensemble/tests/test_partial_dependence.py b/sklearn/ensemble/tests/test_partial_dependence.py
index cec7efc46f03..5bdb563199eb 100644
--- a/sklearn/ensemble/tests/test_partial_dependence.py
+++ b/sklearn/ensemble/tests/test_partial_dependence.py
@@ -1,6 +1,7 @@
 """
 Testing for the partial dependence module.
 """
+import pytest
 
 import numpy as np
 from numpy.testing import assert_array_equal
@@ -103,6 +104,8 @@ def test_partial_dependecy_input():
     assert_raises(ValueError, partial_dependence, clf, [0], grid=grid)
 
 
+@pytest.mark.filterwarnings('ignore: Using or importing the ABCs from')
+# matplotlib Python3.7 warning
 @if_matplotlib
 def test_plot_partial_dependence():
     # Test partial dependence plot function.
@@ -135,6 +138,8 @@ def test_plot_partial_dependence():
     assert all(ax.has_data for ax in axs)
 
 
+@pytest.mark.filterwarnings('ignore: Using or importing the ABCs from')
+# matplotlib Python3.7 warning
 @if_matplotlib
 def test_plot_partial_dependence_input():
     # Test partial dependence plot function input checks.
@@ -170,6 +175,8 @@ def test_plot_partial_dependence_input():
                   clf, X, [{'foo': 'bar'}])
 
 
+@pytest.mark.filterwarnings('ignore: Using or importing the ABCs from')
+# matplotlib Python3.7 warning
 @if_matplotlib
 def test_plot_partial_dependence_multiclass():
     # Test partial dependence plot function on multi-class input.
diff --git a/sklearn/ensemble/tests/test_voting_classifier.py b/sklearn/ensemble/tests/test_voting_classifier.py
index d5a8e055f5d4..f5bfdbd101be 100644
--- a/sklearn/ensemble/tests/test_voting_classifier.py
+++ b/sklearn/ensemble/tests/test_voting_classifier.py
@@ -1,6 +1,8 @@
 """Testing for the VotingClassifier"""
 
+import pytest
 import numpy as np
+
 from sklearn.utils.testing import assert_almost_equal, assert_array_equal
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_equal, assert_true, assert_false
@@ -74,6 +76,7 @@ def test_notfitted():
     assert_raise_message(NotFittedError, msg, eclf.predict_proba, X)
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_majority_label_iris():
     """Check classification by majority label on dataset iris."""
     clf1 = LogisticRegression(random_state=123)
@@ -86,6 +89,7 @@ def test_majority_label_iris():
     assert_almost_equal(scores.mean(), 0.95, decimal=2)
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_tie_situation():
     """Check voting classifier selects smaller class label in tie situation."""
     clf1 = LogisticRegression(random_state=123)
@@ -97,6 +101,7 @@ def test_tie_situation():
     assert_equal(eclf.fit(X, y).predict(X)[73], 1)
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_weights_iris():
     """Check classification by average probabilities on dataset iris."""
     clf1 = LogisticRegression(random_state=123)
@@ -110,6 +115,7 @@ def test_weights_iris():
     assert_almost_equal(scores.mean(), 0.93, decimal=2)
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_predict_on_toy_problem():
     """Manually check predicted class labels for toy dataset."""
     clf1 = LogisticRegression(random_state=123)
@@ -142,6 +148,7 @@ def test_predict_on_toy_problem():
     assert_equal(all(eclf.fit(X, y).predict(X)), all([1, 1, 1, 2, 2, 2]))
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_predict_proba_on_toy_problem():
     """Calculate predicted probabilities on toy dataset."""
     clf1 = LogisticRegression(random_state=123)
@@ -209,6 +216,7 @@ def test_multilabel():
         return
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_gridsearch():
     """Check GridSearch support."""
     clf1 = LogisticRegression(random_state=1)
@@ -226,6 +234,7 @@ def test_gridsearch():
     grid.fit(iris.data, iris.target)
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_parallel_fit():
     """Check parallel backend of VotingClassifier on toy dataset."""
     clf1 = LogisticRegression(random_state=123)
@@ -247,6 +256,7 @@ def test_parallel_fit():
     assert_array_almost_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_sample_weight():
     """Tests sample_weight parameter of VotingClassifier"""
     clf1 = LogisticRegression(random_state=123)
@@ -290,6 +300,7 @@ def fit(self, X, y, *args, **sample_weight):
     eclf.fit(X, y, sample_weight=np.ones((len(y),)))
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_set_params():
     """set_params should be able to set estimators"""
     clf1 = LogisticRegression(random_state=123, C=1.0)
@@ -324,6 +335,7 @@ def test_set_params():
                  eclf1.get_params()["lr"].get_params()['C'])
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_set_estimator_none():
     """VotingClassifier set_params should be able to set estimators as None"""
     # Test predict
@@ -359,10 +371,12 @@ def test_set_estimator_none():
     X1 = np.array([[1], [2]])
     y1 = np.array([1, 2])
     eclf1 = VotingClassifier(estimators=[('rf', clf2), ('nb', clf3)],
-                             voting='soft', weights=[0, 0.5]).fit(X1, y1)
+                             voting='soft', weights=[0, 0.5],
+                             flatten_transform=False).fit(X1, y1)
 
     eclf2 = VotingClassifier(estimators=[('rf', clf2), ('nb', clf3)],
-                             voting='soft', weights=[1, 0.5])
+                             voting='soft', weights=[1, 0.5],
+                             flatten_transform=False)
     eclf2.set_params(rf=None).fit(X1, y1)
     assert_array_almost_equal(eclf1.transform(X1),
                               np.array([[[0.7, 0.3], [0.3, 0.7]],
@@ -376,6 +390,7 @@ def test_set_estimator_none():
     assert_array_equal(eclf2.transform(X1), np.array([[0], [1]]))
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_estimator_weights_format():
     # Test estimator weights inputs as list and array
     clf1 = LogisticRegression(random_state=123)
@@ -393,6 +408,7 @@ def test_estimator_weights_format():
     assert_array_almost_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_transform():
     """Check transform method of VotingClassifier on toy dataset."""
     clf1 = LogisticRegression(random_state=123)
diff --git a/sklearn/ensemble/tests/test_weight_boosting.py b/sklearn/ensemble/tests/test_weight_boosting.py
index 4a8a806ed6a6..e6a6c9d36f44 100755
--- a/sklearn/ensemble/tests/test_weight_boosting.py
+++ b/sklearn/ensemble/tests/test_weight_boosting.py
@@ -1,6 +1,8 @@
 """Testing for the boost module (sklearn.ensemble.boost)."""
 
+import pytest
 import numpy as np
+
 from sklearn.utils.testing import assert_array_equal, assert_array_less
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_equal, assert_true, assert_greater
@@ -194,6 +196,8 @@ def test_staged_predict():
     assert_array_almost_equal(score, staged_scores[-1])
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_gridsearch():
     # Check that base trees can be grid-searched.
     # AdaBoost classification
@@ -277,6 +281,7 @@ def test_error():
                   X, y_class, sample_weight=np.asarray([-1]))
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_base_estimator():
     # Test different base estimators.
     from sklearn.ensemble import RandomForestClassifier
diff --git a/sklearn/ensemble/voting_classifier.py b/sklearn/ensemble/voting_classifier.py
index 2b0d63d2140b..f624df1c664f 100644
--- a/sklearn/ensemble/voting_classifier.py
+++ b/sklearn/ensemble/voting_classifier.py
@@ -18,7 +18,7 @@
 from ..base import TransformerMixin
 from ..base import clone
 from ..preprocessing import LabelEncoder
-from ..externals.joblib import Parallel, delayed
+from ..utils import Parallel, delayed
 from ..utils.validation import has_fit_parameter, check_is_fitted
 from ..utils.metaestimators import _BaseComposition
 from ..utils import Bunch
@@ -59,9 +59,11 @@ class VotingClassifier(_BaseComposition, ClassifierMixin, TransformerMixin):
         predicted class labels (`hard` voting) or class probabilities
         before averaging (`soft` voting). Uses uniform weights if `None`.
 
-    n_jobs : int, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of jobs to run in parallel for ``fit``.
-        If -1, then the number of jobs is set to the number of cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     flatten_transform : bool, optional (default=None)
         Affects shape of transform output only when voting='soft'
@@ -91,7 +93,7 @@ class VotingClassifier(_BaseComposition, ClassifierMixin, TransformerMixin):
     >>> from sklearn.naive_bayes import GaussianNB
     >>> from sklearn.ensemble import RandomForestClassifier, VotingClassifier
     >>> clf1 = LogisticRegression(random_state=1)
-    >>> clf2 = RandomForestClassifier(random_state=1)
+    >>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)
     >>> clf3 = GaussianNB()
     >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
     >>> y = np.array([1, 1, 1, 2, 2, 2])
@@ -121,7 +123,7 @@ class VotingClassifier(_BaseComposition, ClassifierMixin, TransformerMixin):
     >>>
     """
 
-    def __init__(self, estimators, voting='hard', weights=None, n_jobs=1,
+    def __init__(self, estimators, voting='hard', weights=None, n_jobs=None,
                  flatten_transform=None):
         self.estimators = estimators
         self.voting = voting
@@ -319,7 +321,7 @@ def set_params(self, **params):
 
         Parameters
         ----------
-        params : keyword arguments
+        **params : keyword arguments
             Specific parameters using e.g. set_params(parameter_name=new_value)
             In addition, to setting the parameters of the ``VotingClassifier``,
             the individual classifiers of the ``VotingClassifier`` can also be
@@ -342,7 +344,7 @@ def get_params(self, deep=True):
 
         Parameters
         ----------
-        deep: bool
+        deep : bool
             Setting it to True gets the various classifiers and the parameters
             of the classifiers as well
         """
diff --git a/sklearn/ensemble/weight_boosting.py b/sklearn/ensemble/weight_boosting.py
index f13d16befb14..d01f8516d01b 100644
--- a/sklearn/ensemble/weight_boosting.py
+++ b/sklearn/ensemble/weight_boosting.py
@@ -285,7 +285,7 @@ def _samme_proba(estimator, n_classes, X):
     # Displace zero probabilities so the log is defined.
     # Also fix negative elements which may occur with
     # negative sample weights.
-    proba[proba < np.finfo(proba.dtype).eps] = np.finfo(proba.dtype).eps
+    np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)
     log_proba = np.log(proba)
 
     return (n_classes - 1) * (log_proba - (1. / n_classes)
@@ -517,7 +517,7 @@ def _boost_real(self, iboost, X, y, sample_weight, random_state):
         # Also fix negative elements which may occur with
         # negative sample weights.
         proba = y_predict_proba  # alias for readability
-        proba[proba < np.finfo(proba.dtype).eps] = np.finfo(proba.dtype).eps
+        np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)
 
         # Boost weight using multi-class AdaBoost SAMME.R alg
         estimator_weight = (-1. * self.learning_rate
diff --git a/sklearn/externals/_arff.py b/sklearn/externals/_arff.py
new file mode 100644
index 000000000000..7fb445ef9d5a
--- /dev/null
+++ b/sklearn/externals/_arff.py
@@ -0,0 +1,1059 @@
+# -*- coding: utf-8 -*-
+# =============================================================================
+# Federal University of Rio Grande do Sul (UFRGS)
+# Connectionist Artificial Intelligence Laboratory (LIAC)
+# Renato de Pontes Pereira - rppereira@inf.ufrgs.br
+# =============================================================================
+# Copyright (c) 2011 Renato de Pontes Pereira, renato.ppontes at gmail dot com
+#
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
+#
+# The above copyright notice and this permission notice shall be included in
+# all copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+# SOFTWARE.
+# =============================================================================
+
+'''
+The liac-arff module implements functions to read and write ARFF files in
+Python. It was created in the Connectionist Artificial Intelligence Laboratory
+(LIAC), which takes place at the Federal University of Rio Grande do Sul 
+(UFRGS), in Brazil.
+
+ARFF (Attribute-Relation File Format) is an file format specially created for
+describe datasets which are commonly used for machine learning experiments and
+softwares. This file format was created to be used in Weka, the best 
+representative software for machine learning automated experiments.
+
+An ARFF file can be divided into two sections: header and data. The Header 
+describes the metadata of the dataset, including a general description of the 
+dataset, its name and its attributes. The source below is an example of a 
+header section in a XOR dataset::
+
+    % 
+    % XOR Dataset
+    % 
+    % Created by Renato Pereira
+    %            rppereira@inf.ufrgs.br
+    %            http://inf.ufrgs.br/~rppereira
+    % 
+    % 
+    @RELATION XOR
+
+    @ATTRIBUTE input1 REAL
+    @ATTRIBUTE input2 REAL
+    @ATTRIBUTE y REAL
+
+The Data section of an ARFF file describes the observations of the dataset, in 
+the case of XOR dataset::
+
+    @DATA
+    0.0,0.0,0.0
+    0.0,1.0,1.0
+    1.0,0.0,1.0
+    1.0,1.0,0.0
+    % 
+    % 
+    % 
+
+Notice that several lines are starting with an ``%`` symbol, denoting a 
+comment, thus, lines with ``%`` at the beginning will be ignored, except by the
+description part at the beginning of the file. The declarations ``@RELATION``, 
+``@ATTRIBUTE``, and ``@DATA`` are all case insensitive and obligatory.
+
+For more information and details about the ARFF file description, consult
+http://www.cs.waikato.ac.nz/~ml/weka/arff.html
+
+
+ARFF Files in Python
+~~~~~~~~~~~~~~~~~~~~
+
+This module uses built-ins python objects to represent a deserialized ARFF 
+file. A dictionary is used as the container of the data and metadata of ARFF,
+and have the following keys:
+
+- **description**: (OPTIONAL) a string with the description of the dataset.
+- **relation**: (OBLIGATORY) a string with the name of the dataset.
+- **attributes**: (OBLIGATORY) a list of attributes with the following 
+  template::
+
+    (attribute_name, attribute_type)
+
+  the attribute_name is a string, and attribute_type must be an string
+  or a list of strings.
+- **data**: (OBLIGATORY) a list of data instances. Each data instance must be 
+  a list with values, depending on the attributes.
+
+The above keys must follow the case which were described, i.e., the keys are 
+case sensitive. The attribute type ``attribute_type`` must be one of these 
+strings (they are not case sensitive): ``NUMERIC``, ``INTEGER``, ``REAL`` or 
+``STRING``. For nominal attributes, the ``atribute_type`` must be a list of 
+strings.
+
+In this format, the XOR dataset presented above can be represented as a python 
+object as::
+
+    xor_dataset = {
+        'description': 'XOR Dataset',
+        'relation': 'XOR',
+        'attributes': [
+            ('input1', 'REAL'),
+            ('input2', 'REAL'),
+            ('y', 'REAL'),
+        ],
+        'data': [
+            [0.0, 0.0, 0.0],
+            [0.0, 1.0, 1.0],
+            [1.0, 0.0, 1.0],
+            [1.0, 1.0, 0.0]
+        ]
+    }
+
+
+Features
+~~~~~~~~
+
+This module provides several features, including:
+
+- Read and write ARFF files using python built-in structures, such dictionaries
+  and lists;
+- Supports `scipy.sparse.coo <http://docs.scipy
+  .org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html#scipy.sparse.coo_matrix>`_
+  and lists of dictionaries as used by SVMLight
+- Supports the following attribute types: NUMERIC, REAL, INTEGER, STRING, and
+  NOMINAL;
+- Has an interface similar to other built-in modules such as ``json``, or 
+  ``zipfile``;
+- Supports read and write the descriptions of files;
+- Supports missing values and names with spaces;
+- Supports unicode values and names;
+- Fully compatible with Python 2.7+, Python 3.3+, pypy and pypy3;
+- Under `MIT License <http://opensource.org/licenses/MIT>`_
+
+'''
+__author__ = 'Renato de Pontes Pereira, Matthias Feurer, Joel Nothman'
+__author_email__ = ('renato.ppontes@gmail.com, '
+                    'feurerm@informatik.uni-freiburg.de, '
+                    'joel.nothman@gmail.com')
+__version__ = '2.3.1'
+
+import re
+import sys
+import csv
+
+# CONSTANTS ===================================================================
+_SIMPLE_TYPES = ['NUMERIC', 'REAL', 'INTEGER', 'STRING']
+
+_TK_DESCRIPTION = '%'
+_TK_COMMENT     = '%'
+_TK_RELATION    = '@RELATION'
+_TK_ATTRIBUTE   = '@ATTRIBUTE'
+_TK_DATA        = '@DATA'
+
+_RE_RELATION     = re.compile(r'^([^\{\}%,\s]*|\".*\"|\'.*\')$', re.UNICODE)
+_RE_ATTRIBUTE    = re.compile(r'^(\".*\"|\'.*\'|[^\{\}%,\s]*)\s+(.+)$', re.UNICODE)
+_RE_TYPE_NOMINAL = re.compile(r'^\{\s*((\".*\"|\'.*\'|\S*)\s*,\s*)*(\".*\"|\'.*\'|\S*)\s*\}$', re.UNICODE)
+_RE_QUOTE_CHARS = re.compile(r'["\'\\ \t%,]')
+_RE_ESCAPE_CHARS = re.compile(r'(?=["\'\\%])')  # don't need to capture anything
+_RE_SPARSE_LINE = re.compile(r'^\{.*\}$')
+_RE_NONTRIVIAL_DATA = re.compile('["\'{}\\s]')
+
+
+def _build_re_values():
+    quoted_re = r'''
+                    "      # open quote followed by zero or more of:
+                    (?:
+                        (?<!\\)    # no additional backslash
+                        (?:\\\\)*  # maybe escaped backslashes
+                        \\"        # escaped quote
+                    |
+                        \\[^"]     # escaping a non-quote
+                    |
+                        [^"\\]     # non-quote char
+                    )*
+                    "      # close quote
+                    '''
+    # a value is surrounded by " or by ' or contains no quotables
+    value_re = r'''(?:
+        %s|          # a value may be surrounded by "
+        %s|          # or by '
+        [^,\s"'{}]+  # or may contain no characters requiring quoting
+        )''' % (quoted_re,
+                quoted_re.replace('"', "'"))
+
+    # This captures (value, error) groups. Because empty values are allowed,
+    # we cannot just look for empty values to handle syntax errors.
+    # We presume the line has had ',' prepended...
+    dense = re.compile(r'''(?x)
+        ,                # may follow ','
+        \s*
+        ((?=,)|$|%(value_re)s)  # empty or value
+        |
+        (\S.*)           # error
+        ''' % {'value_re': value_re})
+
+    # This captures (key, value) groups and will have an empty key/value
+    # in case of syntax errors.
+    # It does not ensure that the line starts with '{' or ends with '}'.
+    sparse = re.compile(r'''(?x)
+        (?:^\s*\{|,)   # may follow ',', or '{' at line start
+        \s*
+        (\d+)          # attribute key
+        \s+
+        (%(value_re)s) # value
+        |
+        (?!}\s*$)      # not an error if it's }$
+        (?!^\s*{\s*}\s*$)  # not an error if it's ^{}$
+        \S.*           # error
+        ''' % {'value_re': value_re})
+    return dense, sparse
+
+
+_RE_DENSE_VALUES, _RE_SPARSE_KEY_VALUES = _build_re_values()
+
+
+def _unquote(v):
+    if v[:1] in ('"', "'"):
+        return re.sub(r'\\(.)', r'\1', v[1:-1])
+    elif v in ('?', ''):
+        return None
+    else:
+        return v
+
+
+def _parse_values(s):
+    '''(INTERNAL) Split a line into a list of values'''
+    if not _RE_NONTRIVIAL_DATA.search(s):
+        # Fast path for trivial cases (unfortunately we have to handle missing
+        # values because of the empty string case :(.)
+        return [None if s in ('?', '') else s
+                for s in next(csv.reader([s]))]
+
+    # _RE_DENSE_VALUES tokenizes despite quoting, whitespace, etc.
+    values, errors = zip(*_RE_DENSE_VALUES.findall(',' + s))
+    if not any(errors):
+        return [_unquote(v) for v in values]
+    if _RE_SPARSE_LINE.match(s):
+        try:
+            return {int(k): _unquote(v)
+                    for k, v in _RE_SPARSE_KEY_VALUES.findall(s)}
+        except ValueError as exc:
+            # an ARFF syntax error in sparse data
+            for match in _RE_SPARSE_KEY_VALUES.finditer(s):
+                if not match.group(1):
+                    raise BadLayout('Error parsing %r' % match.group())
+            raise BadLayout('Unknown parsing error')
+    else:
+        # an ARFF syntax error
+        for match in _RE_DENSE_VALUES.finditer(s):
+            if match.group(2):
+                raise BadLayout('Error parsing %r' % match.group())
+        raise BadLayout('Unknown parsing error')
+
+
+DENSE = 0   # Constant value representing a dense matrix
+COO = 1     # Constant value representing a sparse matrix in coordinate format
+LOD = 2     # Constant value representing a sparse matrix in list of
+            # dictionaries format
+_SUPPORTED_DATA_STRUCTURES = [DENSE, COO, LOD]
+
+# =============================================================================
+
+# COMPATIBILITY WITH PYTHON 3 =================================================
+PY3 = sys.version_info[0] == 3
+if PY3:
+    unicode = str
+    basestring = str
+    xrange = range
+# COMPABILITY WITH PYTHON 2 ===================================================
+# =============================================================================
+PY2 = sys.version_info[0] == 2
+if PY2:
+    from itertools import izip as zip
+
+# EXCEPTIONS ==================================================================
+class ArffException(Exception):
+    message = None
+
+    def __init__(self):
+        self.line = -1
+
+    def __str__(self):
+        return self.message%self.line
+
+class BadRelationFormat(ArffException):
+    '''Error raised when the relation declaration is in an invalid format.'''
+    message = 'Bad @RELATION format, at line %d.'
+
+class BadAttributeFormat(ArffException):
+    '''Error raised when some attribute declaration is in an invalid format.'''
+    message = 'Bad @ATTRIBUTE format, at line %d.'
+
+class BadDataFormat(ArffException):
+    '''Error raised when some data instance is in an invalid format.'''
+    def __init__(self, value):
+        super(BadDataFormat, self).__init__()
+        self.message = (
+            'Bad @DATA instance format in line %d: ' +
+            ('%s' % value)
+        )
+
+class BadAttributeType(ArffException):
+    '''Error raised when some invalid type is provided into the attribute 
+    declaration.'''
+    message = 'Bad @ATTRIBUTE type, at line %d.'
+
+class BadAttributeName(ArffException):
+    '''Error raised when an attribute name is provided twice the attribute
+    declaration.'''
+
+    def __init__(self, value, value2):
+        super(BadAttributeName, self).__init__()
+        self.message = (
+            ('Bad @ATTRIBUTE name %s at line' % value) +
+            ' %d, this name is already in use in line' +
+            (' %d.' % value2)
+        )
+
+class BadNominalValue(ArffException):
+    '''Error raised when a value in used in some data instance but is not 
+    declared into it respective attribute declaration.'''
+
+    def __init__(self, value):
+        super(BadNominalValue, self).__init__()
+        self.message = (
+            ('Data value %s not found in nominal declaration, ' % value)
+            + 'at line %d.'
+        )
+
+class BadNominalFormatting(ArffException):
+    '''Error raised when a nominal value with space is not properly quoted.'''
+    def __init__(self, value):
+        super(BadNominalFormatting, self).__init__()
+        self.message = (
+            ('Nominal data value "%s" not properly quoted in line ' % value) +
+            '%d.'
+        )
+
+class BadNumericalValue(ArffException):
+    '''Error raised when and invalid numerical value is used in some data 
+    instance.'''
+    message = 'Invalid numerical value, at line %d.'
+
+class BadStringValue(ArffException):
+    '''Error raise when a string contains space but is not quoted.'''
+    message = 'Invalid string value at line %d.'
+
+class BadLayout(ArffException):
+    '''Error raised when the layout of the ARFF file has something wrong.'''
+    message = 'Invalid layout of the ARFF file, at line %d.'
+
+    def __init__(self, msg=''):
+        super(BadLayout, self).__init__()
+        if msg:
+            self.message = BadLayout.message + ' ' + msg.replace('%', '%%')
+
+class BadObject(ArffException):
+    '''Error raised when the object representing the ARFF file has something 
+    wrong.'''
+
+    def __str__(self):
+        return 'Invalid object.'
+
+class BadObject(ArffException):
+    '''Error raised when the object representing the ARFF file has something 
+    wrong.'''
+    def __init__(self, msg=''):
+        self.msg = msg
+
+    def __str__(self):
+        return '%s'%self.msg
+# =============================================================================
+
+# INTERNAL ====================================================================
+def encode_string(s):
+    if _RE_QUOTE_CHARS.search(s):
+        return u"'%s'" % _RE_ESCAPE_CHARS.sub(r'\\', s)
+    return s
+
+
+class EncodedNominalConversor(object):
+    def __init__(self, values):
+        self.values = {v: i for i, v in enumerate(values)}
+        self.values[0] = 0
+
+    def __call__(self, value):
+        try:
+            return self.values[value]
+        except KeyError:
+            raise BadNominalValue(value)
+
+
+class NominalConversor(object):
+    def __init__(self, values):
+        self.values = set(values)
+        self.zero_value = values[0]
+
+    def __call__(self, value):
+        if value not in self.values:
+            if value == 0:
+                # Sparse decode
+                # See issue #52: nominals should take their first value when
+                # unspecified in a sparse matrix. Naturally, this is consistent
+                # with EncodedNominalConversor.
+                return self.zero_value
+            raise BadNominalValue(value)
+        return unicode(value)
+
+
+class Data(object):
+    '''Internal helper class to allow for different matrix types without
+    making the code a huge collection of if statements.'''
+    def __init__(self):
+        self.data = []
+
+    def decode_data(self, s, conversors):
+        values = _parse_values(s)
+
+        if isinstance(values, dict):
+            if max(values) >= len(conversors):
+                raise BadDataFormat(s)
+            # XXX: int 0 is used for implicit values, not '0'
+            values = [values[i] if i in values else 0 for i in
+                      xrange(len(conversors))]
+        else:
+            if len(values) != len(conversors):
+                raise BadDataFormat(s)
+
+        self.data.append(self._decode_values(values, conversors))
+
+    @staticmethod
+    def _decode_values(values, conversors):
+        try:
+            values = [None if value is None else conversor(value)
+                      for conversor, value
+                      in zip(conversors, values)]
+        except ValueError as exc:
+            if 'float: ' in str(exc):
+                raise BadNumericalValue()
+        return values
+
+    def _tuplify_sparse_data(self, x):
+        if len(x) != 2:
+            raise BadDataFormat(x)
+        return (int(x[0].strip('"').strip("'")), x[1])
+
+    def encode_data(self, data, attributes):
+        '''(INTERNAL) Encodes a line of data.
+
+        Data instances follow the csv format, i.e, attribute values are
+        delimited by commas. After converted from csv.
+
+        :param data: a list of values.
+        :param attributes: a list of attributes. Used to check if data is valid.
+        :return: a string with the encoded data line.
+        '''
+        current_row = 0
+
+        for inst in data:
+            if len(inst) != len(attributes):
+                raise BadObject(
+                    'Instance %d has %d attributes, expected %d' %
+                     (current_row, len(inst), len(attributes))
+                )
+
+            new_data = []
+            for value in inst:
+                if value is None or value == u'' or value != value:
+                    s = '?'
+                else:
+                    s = encode_string(unicode(value))
+                new_data.append(s)
+
+            current_row += 1
+            yield u','.join(new_data)
+
+class COOData(Data):
+    def __init__(self):
+        self.data = ([], [], [])
+        self._current_num_data_points = 0
+
+    def decode_data(self, s, conversors):
+        values = _parse_values(s)
+
+        if not isinstance(values, dict):
+            raise BadLayout()
+        if not values:
+            self._current_num_data_points += 1
+            return
+        col, values = zip(*sorted(values.items()))
+        try:
+            values = [value if value is None else conversors[key](value)
+                      for key, value in zip(col, values)]
+        except ValueError as exc:
+            if 'float: ' in str(exc):
+                raise BadNumericalValue()
+            raise
+        except IndexError:
+            # conversor out of range
+            raise BadDataFormat(s)
+        self.data[0].extend(values)
+        self.data[1].extend([self._current_num_data_points] * len(values))
+        self.data[2].extend(col)
+
+        self._current_num_data_points += 1
+
+    def encode_data(self, data, attributes):
+        num_attributes = len(attributes)
+        new_data = []
+        current_row = 0
+
+        row = data.row
+        col = data.col
+        data = data.data
+
+        # Check if the rows are sorted
+        if not all(row[i] <= row[i + 1] for i in xrange(len(row) - 1)):
+            raise ValueError("liac-arff can only output COO matrices with "
+                             "sorted rows.")
+
+        for v, col, row in zip(data, col, row):
+            if row > current_row:
+                # Add empty rows if necessary
+                while current_row < row:
+                    yield " ".join([u"{", u','.join(new_data), u"}"])
+                    new_data = []
+                    current_row += 1
+
+            if col >= num_attributes:
+                raise BadObject(
+                    'Instance %d has at least %d attributes, expected %d' %
+                    (current_row, col + 1, num_attributes)
+                )
+
+            if v is None or v == u'' or v != v:
+                s = '?'
+            else:
+                s = encode_string(unicode(v))
+            new_data.append("%d %s" % (col, s))
+
+        yield " ".join([u"{", u','.join(new_data), u"}"])
+
+class LODData(Data):
+    def __init__(self):
+        self.data = []
+
+    def decode_data(self, s, conversors):
+        values = _parse_values(s)
+        n_conversors = len(conversors)
+
+        if not isinstance(values, dict):
+            raise BadLayout()
+        try:
+            self.data.append({key: None if value is None else conversors[key](value)
+                              for key, value in values.items()})
+        except ValueError as exc:
+            if 'float: ' in str(exc):
+                raise BadNumericalValue()
+            raise
+        except IndexError:
+            # conversor out of range
+            raise BadDataFormat(s)
+
+    def encode_data(self, data, attributes):
+        current_row = 0
+
+        num_attributes = len(attributes)
+        for row in data:
+            new_data = []
+
+            if len(row) > 0 and max(row) >= num_attributes:
+                raise BadObject(
+                    'Instance %d has %d attributes, expected %d' %
+                    (current_row, max(row) + 1, num_attributes)
+                )
+
+            for col in sorted(row):
+                v = row[col]
+                if v is None or v == u'' or v != v:
+                    s = '?'
+                else:
+                    s = encode_string(unicode(v))
+                new_data.append("%d %s" % (col, s))
+
+            current_row += 1
+            yield " ".join([u"{", u','.join(new_data), u"}"])
+
+def _get_data_object_for_decoding(matrix_type):
+    if matrix_type == DENSE:
+        return Data()
+    elif matrix_type == COO:
+        return COOData()
+    elif matrix_type == LOD:
+        return LODData()
+    else:
+        raise ValueError("Matrix type %s not supported." % str(matrix_type))
+
+def _get_data_object_for_encoding(matrix):
+    # Probably a scipy.sparse
+    if hasattr(matrix, 'format'):
+        if matrix.format == 'coo':
+            return COOData()
+        else:
+            raise ValueError('Cannot guess matrix format!')
+    elif isinstance(matrix[0], dict):
+        return LODData()
+    else:
+        return Data()
+
+# =============================================================================
+
+# ADVANCED INTERFACE ==========================================================
+class ArffDecoder(object):
+    '''An ARFF decoder.'''
+
+    def __init__(self):
+        '''Constructor.'''
+        self._conversors = []
+        self._current_line = 0
+
+    def _decode_comment(self, s):
+        '''(INTERNAL) Decodes a comment line.
+
+        Comments are single line strings starting, obligatorily, with the ``%``
+        character, and can have any symbol, including whitespaces or special
+        characters.
+
+        This method must receive a normalized string, i.e., a string without
+        padding, including the "\r\n" characters. 
+
+        :param s: a normalized string.
+        :return: a string with the decoded comment.
+        '''
+        res = re.sub('^\%( )?', '', s)
+        return res
+
+    def _decode_relation(self, s):
+        '''(INTERNAL) Decodes a relation line.
+
+        The relation declaration is a line with the format ``@RELATION 
+        <relation-name>``, where ``relation-name`` is a string. The string must
+        start with alphabetic character and must be quoted if the name includes
+        spaces, otherwise this method will raise a `BadRelationFormat` exception.
+
+        This method must receive a normalized string, i.e., a string without
+        padding, including the "\r\n" characters. 
+
+        :param s: a normalized string.
+        :return: a string with the decoded relation name.
+        '''
+        _, v = s.split(' ', 1)
+        v = v.strip()
+
+        if not _RE_RELATION.match(v):
+            raise BadRelationFormat()
+
+        res = unicode(v.strip('"\''))
+        return res
+
+    def _decode_attribute(self, s):
+        '''(INTERNAL) Decodes an attribute line.
+
+        The attribute is the most complex declaration in an arff file. All 
+        attributes must follow the template::
+
+             @attribute <attribute-name> <datatype>
+
+        where ``attribute-name`` is a string, quoted if the name contains any 
+        whitespace, and ``datatype`` can be:
+
+        - Numerical attributes as ``NUMERIC``, ``INTEGER`` or ``REAL``.
+        - Strings as ``STRING``.
+        - Dates (NOT IMPLEMENTED).
+        - Nominal attributes with format:
+
+            {<nominal-name1>, <nominal-name2>, <nominal-name3>, ...} 
+
+        The nominal names follow the rules for the attribute names, i.e., they
+        must be quoted if the name contains whitespaces.
+
+        This method must receive a normalized string, i.e., a string without
+        padding, including the "\r\n" characters. 
+
+        :param s: a normalized string.
+        :return: a tuple (ATTRIBUTE_NAME, TYPE_OR_VALUES).
+        '''
+        _, v = s.split(' ', 1)
+        v = v.strip()
+
+        # Verify the general structure of declaration
+        m = _RE_ATTRIBUTE.match(v)
+        if not m:
+            raise BadAttributeFormat()
+
+        # Extracts the raw name and type
+        name, type_ = m.groups()
+
+        # Extracts the final name
+        name = unicode(name.strip('"\''))
+
+        # Extracts the final type
+        if _RE_TYPE_NOMINAL.match(type_):
+            try:
+                type_ = _parse_values(type_.strip('{} '))
+            except Exception:
+                raise BadAttributeType()
+            if isinstance(type_, dict):
+                raise BadAttributeType()
+
+        else:
+            # If not nominal, verify the type name
+            type_ = unicode(type_).upper()
+            if type_ not in ['NUMERIC', 'REAL', 'INTEGER', 'STRING']:
+                raise BadAttributeType()
+
+        return (name, type_)
+
+    def _decode(self, s, encode_nominal=False, matrix_type=DENSE):
+        '''Do the job the ``encode``.'''
+
+        # Make sure this method is idempotent
+        self._current_line = 0
+
+        # If string, convert to a list of lines
+        if isinstance(s, basestring):
+            s = s.strip('\r\n ').replace('\r\n', '\n').split('\n')
+
+        # Create the return object
+        obj = {
+            u'description': u'',
+            u'relation': u'',
+            u'attributes': [],
+            u'data': []
+        }
+        attribute_names = {}
+
+        # Create the data helper object
+        data = _get_data_object_for_decoding(matrix_type)
+
+        # Read all lines
+        STATE = _TK_DESCRIPTION
+        for row in s:
+            self._current_line += 1
+            # Ignore empty lines
+            row = row.strip(' \r\n')
+            if not row: continue
+
+            u_row = row.upper()
+
+            # DESCRIPTION -----------------------------------------------------
+            if u_row.startswith(_TK_DESCRIPTION) and STATE == _TK_DESCRIPTION:
+                obj['description'] += self._decode_comment(row) + '\n'
+            # -----------------------------------------------------------------
+
+            # RELATION --------------------------------------------------------
+            elif u_row.startswith(_TK_RELATION):
+                if STATE != _TK_DESCRIPTION:
+                    raise BadLayout()
+
+                STATE = _TK_RELATION
+                obj['relation'] = self._decode_relation(row)
+            # -----------------------------------------------------------------
+
+            # ATTRIBUTE -------------------------------------------------------
+            elif u_row.startswith(_TK_ATTRIBUTE):
+                if STATE != _TK_RELATION and STATE != _TK_ATTRIBUTE:
+                    raise BadLayout()
+
+                STATE = _TK_ATTRIBUTE
+
+                attr = self._decode_attribute(row)
+                if attr[0] in attribute_names:
+                    raise BadAttributeName(attr[0], attribute_names[attr[0]])
+                else:
+                    attribute_names[attr[0]] = self._current_line
+                obj['attributes'].append(attr)
+
+                if isinstance(attr[1], (list, tuple)):
+                    if encode_nominal:
+                        conversor = EncodedNominalConversor(attr[1])
+                    else:
+                        conversor = NominalConversor(attr[1])
+                else:
+                    CONVERSOR_MAP = {'STRING': unicode,
+                                     'INTEGER': lambda x: int(float(x)),
+                                     'NUMERIC': float,
+                                     'REAL': float}
+                    conversor = CONVERSOR_MAP[attr[1]]
+
+                self._conversors.append(conversor)
+            # -----------------------------------------------------------------
+
+            # DATA ------------------------------------------------------------
+            elif u_row.startswith(_TK_DATA):
+                if STATE != _TK_ATTRIBUTE:
+                    raise BadLayout()
+
+                STATE = _TK_DATA
+            # -----------------------------------------------------------------
+
+            # COMMENT ---------------------------------------------------------
+            elif u_row.startswith(_TK_COMMENT):
+                pass
+            # -----------------------------------------------------------------
+
+            # DATA INSTANCES --------------------------------------------------
+            elif STATE == _TK_DATA:
+                data.decode_data(row, self._conversors)
+            # -----------------------------------------------------------------
+
+            # UNKNOWN INFORMATION ---------------------------------------------
+            else:
+                raise BadLayout()
+            # -----------------------------------------------------------------
+
+        # Alter the data object
+        obj['data'] = data.data
+        if obj['description'].endswith('\n'):
+            obj['description'] = obj['description'][:-1]
+
+        return obj
+
+    def decode(self, s, encode_nominal=False, return_type=DENSE):
+        '''Returns the Python representation of a given ARFF file.
+
+        When a file object is passed as an argument, this method reads lines
+        iteratively, avoiding to load unnecessary information to the memory.
+
+        :param s: a string or file object with the ARFF file.
+        :param encode_nominal: boolean, if True perform a label encoding
+            while reading the .arff file.
+        :param return_type: determines the data structure used to store the
+            dataset. Can be one of `arff.DENSE`, `arff.COO` and `arff.LOD`.
+            Consult the section on `working with sparse data`_
+        '''
+        try:
+            return self._decode(s, encode_nominal=encode_nominal,
+                                matrix_type=return_type)
+        except ArffException as e:
+            e.line = self._current_line
+            raise e
+
+
+class ArffEncoder(object):
+    '''An ARFF encoder.'''
+
+    def _encode_comment(self, s=''):
+        '''(INTERNAL) Encodes a comment line.
+
+        Comments are single line strings starting, obligatorily, with the ``%``
+        character, and can have any symbol, including whitespaces or special
+        characters.
+
+        If ``s`` is None, this method will simply return an empty comment.
+
+        :param s: (OPTIONAL) string.
+        :return: a string with the encoded comment line.
+        '''
+        if s:
+            return u'%s %s'%(_TK_COMMENT, s)
+        else:
+            return u'%s' % _TK_COMMENT
+
+    def _encode_relation(self, name):
+        '''(INTERNAL) Decodes a relation line.
+
+        The relation declaration is a line with the format ``@RELATION 
+        <relation-name>``, where ``relation-name`` is a string. 
+
+        :param name: a string.
+        :return: a string with the encoded relation declaration.
+        '''
+        for char in ' %{},':
+            if char in name:
+                name = '"%s"'%name
+                break
+
+        return u'%s %s'%(_TK_RELATION, name)
+
+    def _encode_attribute(self, name, type_):
+        '''(INTERNAL) Encodes an attribute line.
+
+        The attribute follow the template::
+
+             @attribute <attribute-name> <datatype>
+
+        where ``attribute-name`` is a string, and ``datatype`` can be:
+
+        - Numerical attributes as ``NUMERIC``, ``INTEGER`` or ``REAL``.
+        - Strings as ``STRING``.
+        - Dates (NOT IMPLEMENTED).
+        - Nominal attributes with format:
+
+            {<nominal-name1>, <nominal-name2>, <nominal-name3>, ...} 
+
+        This method must receive a the name of the attribute and its type, if
+        the attribute type is nominal, ``type`` must be a list of values.
+
+        :param name: a string.
+        :param type_: a string or a list of string.
+        :return: a string with the encoded attribute declaration.
+        '''
+        for char in ' %{},':
+            if char in name:
+                name = '"%s"'%name
+                break
+
+        if isinstance(type_, (tuple, list)):
+            type_tmp = []
+            for i in range(len(type_)):
+                type_tmp.append(u'%s' % encode_string(type_[i]))
+            type_ = u'{%s}'%(u', '.join(type_tmp))
+
+        return u'%s %s %s'%(_TK_ATTRIBUTE, name, type_)
+
+    def encode(self, obj):
+        '''Encodes a given object to an ARFF file.
+
+        :param obj: the object containing the ARFF information.
+        :return: the ARFF file as an unicode string.
+        '''
+        data = [row for row in self.iter_encode(obj)]
+
+        return u'\n'.join(data)
+
+    def iter_encode(self, obj):
+        '''The iterative version of `arff.ArffEncoder.encode`.
+
+        This encodes iteratively a given object and return, one-by-one, the 
+        lines of the ARFF file.
+
+        :param obj: the object containing the ARFF information.
+        :return: (yields) the ARFF file as unicode strings.
+        '''
+        # DESCRIPTION
+        if obj.get('description', None):
+            for row in obj['description'].split('\n'):
+                yield self._encode_comment(row)
+
+        # RELATION
+        if not obj.get('relation'):
+            raise BadObject('Relation name not found or with invalid value.')
+
+        yield self._encode_relation(obj['relation'])
+        yield u''
+
+        # ATTRIBUTES
+        if not obj.get('attributes'):
+            raise BadObject('Attributes not found.')
+
+        attribute_names = set()
+        for attr in obj['attributes']:
+            # Verify for bad object format
+            if not isinstance(attr, (tuple, list)) or \
+               len(attr) != 2 or \
+               not isinstance(attr[0], basestring):
+                raise BadObject('Invalid attribute declaration "%s"'%str(attr))
+
+            if isinstance(attr[1], basestring):
+                # Verify for invalid types
+                if attr[1] not in _SIMPLE_TYPES:
+                    raise BadObject('Invalid attribute type "%s"'%str(attr))
+
+            # Verify for bad object format
+            elif not isinstance(attr[1], (tuple, list)):
+                raise BadObject('Invalid attribute type "%s"'%str(attr))
+
+            # Verify attribute name is not used twice
+            if attr[0] in attribute_names:
+                raise BadObject('Trying to use attribute name "%s" for the '
+                                'second time.' % str(attr[0]))
+            else:
+                attribute_names.add(attr[0])
+
+            yield self._encode_attribute(attr[0], attr[1])
+        yield u''
+        attributes = obj['attributes']
+
+        # DATA
+        yield _TK_DATA
+        if 'data' in obj:
+            data = _get_data_object_for_encoding(obj.get('data'))
+            for line in data.encode_data(obj.get('data'), attributes):
+                yield line
+
+        yield u''
+
+# =============================================================================
+
+# BASIC INTERFACE =============================================================
+def load(fp, encode_nominal=False, return_type=DENSE):
+    '''Load a file-like object containing the ARFF document and convert it into
+    a Python object. 
+
+    :param fp: a file-like object.
+    :param encode_nominal: boolean, if True perform a label encoding
+        while reading the .arff file.
+    :param return_type: determines the data structure used to store the
+        dataset. Can be one of `arff.DENSE`, `arff.COO` and `arff.LOD`.
+        Consult the section on `working with sparse data`_
+    :return: a dictionary.
+     '''
+    decoder = ArffDecoder()
+    return decoder.decode(fp, encode_nominal=encode_nominal,
+                          return_type=return_type)
+
+def loads(s, encode_nominal=False, return_type=DENSE):
+    '''Convert a string instance containing the ARFF document into a Python
+    object.
+
+    :param s: a string object.
+    :param encode_nominal: boolean, if True perform a label encoding
+        while reading the .arff file.
+    :param return_type: determines the data structure used to store the
+        dataset. Can be one of `arff.DENSE`, `arff.COO` and `arff.LOD`.
+        Consult the section on `working with sparse data`_
+    :return: a dictionary.
+    '''
+    decoder = ArffDecoder()
+    return decoder.decode(s, encode_nominal=encode_nominal,
+                          return_type=return_type)
+
+def dump(obj, fp):
+    '''Serialize an object representing the ARFF document to a given file-like 
+    object.
+
+    :param obj: a dictionary.
+    :param fp: a file-like object.
+    '''
+    encoder = ArffEncoder()
+    generator = encoder.iter_encode(obj)
+
+    last_row = next(generator)
+    for row in generator:
+        fp.write(last_row + u'\n')
+        last_row = row
+    fp.write(last_row)
+
+    return fp
+
+def dumps(obj):
+    '''Serialize an object representing the ARFF document, returning a string.
+
+    :param obj: a dictionary.
+    :return: a string with the ARFF document.
+    '''
+    encoder = ArffEncoder()
+    return encoder.encode(obj)
+# =============================================================================
diff --git a/sklearn/externals/_joblib/pool.py b/sklearn/externals/_joblib/pool.py
deleted file mode 100644
index ef3838e7e559..000000000000
--- a/sklearn/externals/_joblib/pool.py
+++ /dev/null
@@ -1,616 +0,0 @@
-"""Custom implementation of multiprocessing.Pool with custom pickler.
-
-This module provides efficient ways of working with data stored in
-shared memory with numpy.memmap arrays without inducing any memory
-copy between the parent and child processes.
-
-This module should not be imported if multiprocessing is not
-available as it implements subclasses of multiprocessing Pool
-that uses a custom alternative to SimpleQueue.
-
-"""
-# Author: Olivier Grisel <olivier.grisel@ensta.org>
-# Copyright: 2012, Olivier Grisel
-# License: BSD 3 clause
-
-from mmap import mmap
-import errno
-import os
-import stat
-import sys
-import threading
-import atexit
-import tempfile
-import shutil
-import warnings
-from time import sleep
-
-try:
-    WindowsError
-except NameError:
-    WindowsError = type(None)
-
-from pickle import whichmodule
-try:
-    # Python 2 compat
-    from cPickle import loads
-    from cPickle import dumps
-except ImportError:
-    from pickle import loads
-    from pickle import dumps
-    import copyreg
-
-# Customizable pure Python pickler in Python 2
-# customizable C-optimized pickler under Python 3.3+
-from pickle import Pickler
-
-from pickle import HIGHEST_PROTOCOL
-from io import BytesIO
-
-from ._multiprocessing_helpers import mp, assert_spawning
-# We need the class definition to derive from it not the multiprocessing.Pool
-# factory function
-from multiprocessing.pool import Pool
-
-try:
-    import numpy as np
-    from numpy.lib.stride_tricks import as_strided
-except ImportError:
-    np = None
-
-from .numpy_pickle import load
-from .numpy_pickle import dump
-from .hashing import hash
-from .backports import make_memmap
-# Some system have a ramdisk mounted by default, we can use it instead of /tmp
-# as the default folder to dump big arrays to share with subprocesses
-SYSTEM_SHARED_MEM_FS = '/dev/shm'
-
-# Folder and file permissions to chmod temporary files generated by the
-# memmaping pool. Only the owner of the Python process can access the
-# temporary files and folder.
-FOLDER_PERMISSIONS = stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR
-FILE_PERMISSIONS = stat.S_IRUSR | stat.S_IWUSR
-
-###############################################################################
-# Support for efficient transient pickling of numpy data structures
-
-
-def _get_backing_memmap(a):
-    """Recursively look up the original np.memmap instance base if any."""
-    b = getattr(a, 'base', None)
-    if b is None:
-        # TODO: check scipy sparse datastructure if scipy is installed
-        # a nor its descendants do not have a memmap base
-        return None
-
-    elif isinstance(b, mmap):
-        # a is already a real memmap instance.
-        return a
-
-    else:
-        # Recursive exploration of the base ancestry
-        return _get_backing_memmap(b)
-
-
-def has_shareable_memory(a):
-    """Return True if a is backed by some mmap buffer directly or not."""
-    return _get_backing_memmap(a) is not None
-
-
-def _strided_from_memmap(filename, dtype, mode, offset, order, shape, strides,
-                         total_buffer_len):
-    """Reconstruct an array view on a memory mapped file."""
-    if mode == 'w+':
-        # Do not zero the original data when unpickling
-        mode = 'r+'
-
-    if strides is None:
-        # Simple, contiguous memmap
-        return make_memmap(filename, dtype=dtype, shape=shape, mode=mode,
-                           offset=offset, order=order)
-    else:
-        # For non-contiguous data, memmap the total enclosing buffer and then
-        # extract the non-contiguous view with the stride-tricks API
-        base = make_memmap(filename, dtype=dtype, shape=total_buffer_len,
-                           mode=mode, offset=offset, order=order)
-        return as_strided(base, shape=shape, strides=strides)
-
-
-def _reduce_memmap_backed(a, m):
-    """Pickling reduction for memmap backed arrays.
-
-    a is expected to be an instance of np.ndarray (or np.memmap)
-    m is expected to be an instance of np.memmap on the top of the ``base``
-    attribute ancestry of a. ``m.base`` should be the real python mmap object.
-    """
-    # offset that comes from the striding differences between a and m
-    a_start, a_end = np.byte_bounds(a)
-    m_start = np.byte_bounds(m)[0]
-    offset = a_start - m_start
-
-    # offset from the backing memmap
-    offset += m.offset
-
-    if m.flags['F_CONTIGUOUS']:
-        order = 'F'
-    else:
-        # The backing memmap buffer is necessarily contiguous hence C if not
-        # Fortran
-        order = 'C'
-
-    if a.flags['F_CONTIGUOUS'] or a.flags['C_CONTIGUOUS']:
-        # If the array is a contiguous view, no need to pass the strides
-        strides = None
-        total_buffer_len = None
-    else:
-        # Compute the total number of items to map from which the strided
-        # view will be extracted.
-        strides = a.strides
-        total_buffer_len = (a_end - a_start) // a.itemsize
-    return (_strided_from_memmap,
-            (m.filename, a.dtype, m.mode, offset, order, a.shape, strides,
-             total_buffer_len))
-
-
-def reduce_memmap(a):
-    """Pickle the descriptors of a memmap instance to reopen on same file."""
-    m = _get_backing_memmap(a)
-    if m is not None:
-        # m is a real mmap backed memmap instance, reduce a preserving striding
-        # information
-        return _reduce_memmap_backed(a, m)
-    else:
-        # This memmap instance is actually backed by a regular in-memory
-        # buffer: this can happen when using binary operators on numpy.memmap
-        # instances
-        return (loads, (dumps(np.asarray(a), protocol=HIGHEST_PROTOCOL),))
-
-
-class ArrayMemmapReducer(object):
-    """Reducer callable to dump large arrays to memmap files.
-
-    Parameters
-    ----------
-    max_nbytes: int
-        Threshold to trigger memmaping of large arrays to files created
-        a folder.
-    temp_folder: str
-        Path of a folder where files for backing memmaped arrays are created.
-    mmap_mode: 'r', 'r+' or 'c'
-        Mode for the created memmap datastructure. See the documentation of
-        numpy.memmap for more details. Note: 'w+' is coerced to 'r+'
-        automatically to avoid zeroing the data on unpickling.
-    verbose: int, optional, 0 by default
-        If verbose > 0, memmap creations are logged.
-        If verbose > 1, both memmap creations, reuse and array pickling are
-        logged.
-    prewarm: bool, optional, False by default.
-        Force a read on newly memmaped array to make sure that OS pre-cache it
-        memory. This can be useful to avoid concurrent disk access when the
-        same data array is passed to different worker processes.
-    """
-
-    def __init__(self, max_nbytes, temp_folder, mmap_mode, verbose=0,
-                 context_id=None, prewarm=True):
-        self._max_nbytes = max_nbytes
-        self._temp_folder = temp_folder
-        self._mmap_mode = mmap_mode
-        self.verbose = int(verbose)
-        self._prewarm = prewarm
-        if context_id is not None:
-            warnings.warn('context_id is deprecated and ignored in joblib'
-                          ' 0.9.4 and will be removed in 0.11',
-                          DeprecationWarning)
-
-    def __call__(self, a):
-        m = _get_backing_memmap(a)
-        if m is not None:
-            # a is already backed by a memmap file, let's reuse it directly
-            return _reduce_memmap_backed(a, m)
-
-        if (not a.dtype.hasobject
-                and self._max_nbytes is not None
-                and a.nbytes > self._max_nbytes):
-            # check that the folder exists (lazily create the pool temp folder
-            # if required)
-            try:
-                os.makedirs(self._temp_folder)
-                os.chmod(self._temp_folder, FOLDER_PERMISSIONS)
-            except OSError as e:
-                if e.errno != errno.EEXIST:
-                    raise e
-
-            # Find a unique, concurrent safe filename for writing the
-            # content of this array only once.
-            basename = "%d-%d-%s.pkl" % (
-                os.getpid(), id(threading.current_thread()), hash(a))
-            filename = os.path.join(self._temp_folder, basename)
-
-            # In case the same array with the same content is passed several
-            # times to the pool subprocess children, serialize it only once
-
-            # XXX: implement an explicit reference counting scheme to make it
-            # possible to delete temporary files as soon as the workers are
-            # done processing this data.
-            if not os.path.exists(filename):
-                if self.verbose > 0:
-                    print("Memmaping (shape=%r, dtype=%s) to new file %s" % (
-                        a.shape, a.dtype, filename))
-                for dumped_filename in dump(a, filename):
-                    os.chmod(dumped_filename, FILE_PERMISSIONS)
-
-                if self._prewarm:
-                    # Warm up the data to avoid concurrent disk access in
-                    # multiple children processes
-                    load(filename, mmap_mode=self._mmap_mode).max()
-            elif self.verbose > 1:
-                print("Memmaping (shape=%s, dtype=%s) to old file %s" % (
-                    a.shape, a.dtype, filename))
-
-            # The worker process will use joblib.load to memmap the data
-            return (load, (filename, self._mmap_mode))
-        else:
-            # do not convert a into memmap, let pickler do its usual copy with
-            # the default system pickler
-            if self.verbose > 1:
-                print("Pickling array (shape=%r, dtype=%s)." % (
-                    a.shape, a.dtype))
-            return (loads, (dumps(a, protocol=HIGHEST_PROTOCOL),))
-
-
-###############################################################################
-# Enable custom pickling in Pool queues
-
-class CustomizablePickler(Pickler):
-    """Pickler that accepts custom reducers.
-
-    HIGHEST_PROTOCOL is selected by default as this pickler is used
-    to pickle ephemeral datastructures for interprocess communication
-    hence no backward compatibility is required.
-
-    `reducers` is expected to be a dictionary with key/values
-    being `(type, callable)` pairs where `callable` is a function that
-    give an instance of `type` will return a tuple `(constructor,
-    tuple_of_objects)` to rebuild an instance out of the pickled
-    `tuple_of_objects` as would return a `__reduce__` method. See the
-    standard library documentation on pickling for more details.
-
-    """
-
-    # We override the pure Python pickler as its the only way to be able to
-    # customize the dispatch table without side effects in Python 2.7
-    # to 3.2. For Python 3.3+ leverage the new dispatch_table
-    # feature from http://bugs.python.org/issue14166 that makes it possible
-    # to use the C implementation of the Pickler which is faster.
-
-    def __init__(self, writer, reducers=None, protocol=HIGHEST_PROTOCOL):
-        Pickler.__init__(self, writer, protocol=protocol)
-        if reducers is None:
-            reducers = {}
-        if hasattr(Pickler, 'dispatch'):
-            # Make the dispatch registry an instance level attribute instead of
-            # a reference to the class dictionary under Python 2
-            self.dispatch = Pickler.dispatch.copy()
-        else:
-            # Under Python 3 initialize the dispatch table with a copy of the
-            # default registry
-            self.dispatch_table = copyreg.dispatch_table.copy()
-        for type, reduce_func in reducers.items():
-            self.register(type, reduce_func)
-
-    def register(self, type, reduce_func):
-        """Attach a reducer function to a given type in the dispatch table."""
-        if hasattr(Pickler, 'dispatch'):
-            # Python 2 pickler dispatching is not explicitly customizable.
-            # Let us use a closure to workaround this limitation.
-            def dispatcher(self, obj):
-                reduced = reduce_func(obj)
-                self.save_reduce(obj=obj, *reduced)
-            self.dispatch[type] = dispatcher
-        else:
-            self.dispatch_table[type] = reduce_func
-
-
-class CustomizablePicklingQueue(object):
-    """Locked Pipe implementation that uses a customizable pickler.
-
-    This class is an alternative to the multiprocessing implementation
-    of SimpleQueue in order to make it possible to pass custom
-    pickling reducers, for instance to avoid memory copy when passing
-    memory mapped datastructures.
-
-    `reducers` is expected to be a dict with key / values being
-    `(type, callable)` pairs where `callable` is a function that, given an
-    instance of `type`, will return a tuple `(constructor, tuple_of_objects)`
-    to rebuild an instance out of the pickled `tuple_of_objects` as would
-    return a `__reduce__` method.
-
-    See the standard library documentation on pickling for more details.
-    """
-
-    def __init__(self, context, reducers=None):
-        self._reducers = reducers
-        self._reader, self._writer = context.Pipe(duplex=False)
-        self._rlock = context.Lock()
-        if sys.platform == 'win32':
-            self._wlock = None
-        else:
-            self._wlock = context.Lock()
-        self._make_methods()
-
-    def __getstate__(self):
-        assert_spawning(self)
-        return (self._reader, self._writer, self._rlock, self._wlock,
-                self._reducers)
-
-    def __setstate__(self, state):
-        (self._reader, self._writer, self._rlock, self._wlock,
-         self._reducers) = state
-        self._make_methods()
-
-    def empty(self):
-        return not self._reader.poll()
-
-    def _make_methods(self):
-        self._recv = recv = self._reader.recv
-        racquire, rrelease = self._rlock.acquire, self._rlock.release
-
-        def get():
-            racquire()
-            try:
-                return recv()
-            finally:
-                rrelease()
-
-        self.get = get
-
-        if self._reducers:
-            def send(obj):
-                buffer = BytesIO()
-                CustomizablePickler(buffer, self._reducers).dump(obj)
-                self._writer.send_bytes(buffer.getvalue())
-            self._send = send
-        else:
-            self._send = send = self._writer.send
-        if self._wlock is None:
-            # writes to a message oriented win32 pipe are atomic
-            self.put = send
-        else:
-            wlock_acquire, wlock_release = (
-                self._wlock.acquire, self._wlock.release)
-
-            def put(obj):
-                wlock_acquire()
-                try:
-                    return send(obj)
-                finally:
-                    wlock_release()
-
-            self.put = put
-
-
-class PicklingPool(Pool):
-    """Pool implementation with customizable pickling reducers.
-
-    This is useful to control how data is shipped between processes
-    and makes it possible to use shared memory without useless
-    copies induces by the default pickling methods of the original
-    objects passed as arguments to dispatch.
-
-    `forward_reducers` and `backward_reducers` are expected to be
-    dictionaries with key/values being `(type, callable)` pairs where
-    `callable` is a function that, given an instance of `type`, will return a
-    tuple `(constructor, tuple_of_objects)` to rebuild an instance out of the
-    pickled `tuple_of_objects` as would return a `__reduce__` method.
-    See the standard library documentation about pickling for more details.
-
-    """
-
-    def __init__(self, processes=None, forward_reducers=None,
-                 backward_reducers=None, **kwargs):
-        if forward_reducers is None:
-            forward_reducers = dict()
-        if backward_reducers is None:
-            backward_reducers = dict()
-        self._forward_reducers = forward_reducers
-        self._backward_reducers = backward_reducers
-        poolargs = dict(processes=processes)
-        poolargs.update(kwargs)
-        super(PicklingPool, self).__init__(**poolargs)
-
-    def _setup_queues(self):
-        context = getattr(self, '_ctx', mp)
-        self._inqueue = CustomizablePicklingQueue(context,
-                                                  self._forward_reducers)
-        self._outqueue = CustomizablePicklingQueue(context,
-                                                   self._backward_reducers)
-        self._quick_put = self._inqueue._send
-        self._quick_get = self._outqueue._recv
-
-
-def delete_folder(folder_path):
-    """Utility function to cleanup a temporary folder if still existing."""
-    try:
-        if os.path.exists(folder_path):
-            shutil.rmtree(folder_path)
-    except WindowsError:
-        warnings.warn("Failed to clean temporary folder: %s" % folder_path)
-
-
-class MemmapingPool(PicklingPool):
-    """Process pool that shares large arrays to avoid memory copy.
-
-    This drop-in replacement for `multiprocessing.pool.Pool` makes
-    it possible to work efficiently with shared memory in a numpy
-    context.
-
-    Existing instances of numpy.memmap are preserved: the child
-    suprocesses will have access to the same shared memory in the
-    original mode except for the 'w+' mode that is automatically
-    transformed as 'r+' to avoid zeroing the original data upon
-    instantiation.
-
-    Furthermore large arrays from the parent process are automatically
-    dumped to a temporary folder on the filesystem such as child
-    processes to access their content via memmaping (file system
-    backed shared memory).
-
-    Note: it is important to call the terminate method to collect
-    the temporary folder used by the pool.
-
-    Parameters
-    ----------
-    processes: int, optional
-        Number of worker processes running concurrently in the pool.
-    initializer: callable, optional
-        Callable executed on worker process creation.
-    initargs: tuple, optional
-        Arguments passed to the initializer callable.
-    temp_folder: str, optional
-        Folder to be used by the pool for memmaping large arrays
-        for sharing memory with worker processes. If None, this will try in
-        order:
-        - a folder pointed by the JOBLIB_TEMP_FOLDER environment variable,
-        - /dev/shm if the folder exists and is writable: this is a RAMdisk
-          filesystem available by default on modern Linux distributions,
-        - the default system temporary folder that can be overridden
-          with TMP, TMPDIR or TEMP environment variables, typically /tmp
-          under Unix operating systems.
-    max_nbytes int or None, optional, 1e6 by default
-        Threshold on the size of arrays passed to the workers that
-        triggers automated memory mapping in temp_folder.
-        Use None to disable memmaping of large arrays.
-    mmap_mode: {'r+', 'r', 'w+', 'c'}
-        Memmapping mode for numpy arrays passed to workers.
-        See 'max_nbytes' parameter documentation for more details.
-    forward_reducers: dictionary, optional
-        Reducers used to pickle objects passed from master to worker
-        processes: see below.
-    backward_reducers: dictionary, optional
-        Reducers used to pickle return values from workers back to the
-        master process.
-    verbose: int, optional
-        Make it possible to monitor how the communication of numpy arrays
-        with the subprocess is handled (pickling or memmaping)
-    prewarm: bool or str, optional, "auto" by default.
-        If True, force a read on newly memmaped array to make sure that OS pre-
-        cache it in memory. This can be useful to avoid concurrent disk access
-        when the same data array is passed to different worker processes.
-        If "auto" (by default), prewarm is set to True, unless the Linux shared
-        memory partition /dev/shm is available and used as temp_folder.
-
-    `forward_reducers` and `backward_reducers` are expected to be
-    dictionaries with key/values being `(type, callable)` pairs where
-    `callable` is a function that give an instance of `type` will return
-    a tuple `(constructor, tuple_of_objects)` to rebuild an instance out
-    of the pickled `tuple_of_objects` as would return a `__reduce__`
-    method. See the standard library documentation on pickling for more
-    details.
-
-    """
-
-    def __init__(self, processes=None, temp_folder=None, max_nbytes=1e6,
-                 mmap_mode='r', forward_reducers=None, backward_reducers=None,
-                 verbose=0, context_id=None, prewarm=False, **kwargs):
-        if forward_reducers is None:
-            forward_reducers = dict()
-        if backward_reducers is None:
-            backward_reducers = dict()
-        if context_id is not None:
-            warnings.warn('context_id is deprecated and ignored in joblib'
-                          ' 0.9.4 and will be removed in 0.11',
-                          DeprecationWarning)
-
-        # Prepare a sub-folder name for the serialization of this particular
-        # pool instance (do not create in advance to spare FS write access if
-        # no array is to be dumped):
-        use_shared_mem = False
-        pool_folder_name = "joblib_memmaping_pool_%d_%d" % (
-            os.getpid(), id(self))
-        if temp_folder is None:
-            temp_folder = os.environ.get('JOBLIB_TEMP_FOLDER', None)
-        if temp_folder is None:
-            if os.path.exists(SYSTEM_SHARED_MEM_FS):
-                try:
-                    temp_folder = SYSTEM_SHARED_MEM_FS
-                    pool_folder = os.path.join(temp_folder, pool_folder_name)
-                    if not os.path.exists(pool_folder):
-                        os.makedirs(pool_folder)
-                    use_shared_mem = True
-                except IOError:
-                    # Missing rights in the /dev/shm partition,
-                    # fallback to regular temp folder.
-                    temp_folder = None
-        if temp_folder is None:
-            # Fallback to the default tmp folder, typically /tmp
-            temp_folder = tempfile.gettempdir()
-        temp_folder = os.path.abspath(os.path.expanduser(temp_folder))
-        pool_folder = os.path.join(temp_folder, pool_folder_name)
-        self._temp_folder = pool_folder
-
-        # Register the garbage collector at program exit in case caller forgets
-        # to call terminate explicitly: note we do not pass any reference to
-        # self to ensure that this callback won't prevent garbage collection of
-        # the pool instance and related file handler resources such as POSIX
-        # semaphores and pipes
-        pool_module_name = whichmodule(delete_folder, 'delete_folder')
-
-        def _cleanup():
-            # In some cases the Python runtime seems to set delete_folder to
-            # None just before exiting when accessing the delete_folder
-            # function from the closure namespace. So instead we reimport
-            # the delete_folder function explicitly.
-            # https://github.com/joblib/joblib/issues/328
-            # We cannot just use from 'joblib.pool import delete_folder'
-            # because joblib should only use relative imports to allow
-            # easy vendoring.
-            delete_folder = __import__(
-                pool_module_name, fromlist=['delete_folder']).delete_folder
-            delete_folder(pool_folder)
-
-        atexit.register(_cleanup)
-
-        if np is not None:
-            # Register smart numpy.ndarray reducers that detects memmap backed
-            # arrays and that is else able to dump to memmap large in-memory
-            # arrays over the max_nbytes threshold
-            if prewarm == "auto":
-                prewarm = not use_shared_mem
-            forward_reduce_ndarray = ArrayMemmapReducer(
-                max_nbytes, pool_folder, mmap_mode, verbose,
-                prewarm=prewarm)
-            forward_reducers[np.ndarray] = forward_reduce_ndarray
-            forward_reducers[np.memmap] = reduce_memmap
-
-            # Communication from child process to the parent process always
-            # pickles in-memory numpy.ndarray without dumping them as memmap
-            # to avoid confusing the caller and make it tricky to collect the
-            # temporary folder
-            backward_reduce_ndarray = ArrayMemmapReducer(
-                None, pool_folder, mmap_mode, verbose)
-            backward_reducers[np.ndarray] = backward_reduce_ndarray
-            backward_reducers[np.memmap] = reduce_memmap
-
-        poolargs = dict(
-            processes=processes,
-            forward_reducers=forward_reducers,
-            backward_reducers=backward_reducers)
-        poolargs.update(kwargs)
-        super(MemmapingPool, self).__init__(**poolargs)
-
-    def terminate(self):
-        n_retries = 10
-        for i in range(n_retries):
-            try:
-                super(MemmapingPool, self).terminate()
-                break
-            except OSError as e:
-                if isinstance(e, WindowsError):
-                    # Workaround  occasional "[Error 5] Access is denied" issue
-                    # when trying to terminate a process under windows.
-                    sleep(0.1)
-                    if i + 1 == n_retries:
-                        warnings.warn("Failed to terminate worker processes in"
-                                      " multiprocessing pool: %r" % e)
-        delete_folder(self._temp_folder)
diff --git a/sklearn/externals/copy_joblib.sh b/sklearn/externals/copy_joblib.sh
index 8db0da232c64..878413297759 100755
--- a/sklearn/externals/copy_joblib.sh
+++ b/sklearn/externals/copy_joblib.sh
@@ -12,14 +12,14 @@ else
 fi
 
 pip install $JOBLIB --target $INSTALL_FOLDER
-cp -r $INSTALL_FOLDER/joblib _joblib
+cp -r $INSTALL_FOLDER/joblib joblib
 rm -rf $INSTALL_FOLDER
 
 # Needed to rewrite the doctests
 # Note: BSD sed -i needs an argument unders OSX
 # so first renaming to .bak and then deleting backup files
-find _joblib -name "*.py" | xargs sed -i.bak "s/from joblib/from sklearn.externals.joblib/"
-find _joblib -name "*.bak" | xargs rm
+find joblib -name "*.py" | xargs sed -i.bak "s/from joblib/from sklearn.externals.joblib/"
+find joblib -name "*.bak" | xargs rm
 
 # Remove the tests folders to speed-up test time for scikit-learn.
 # joblib is already tested on its own CI infrastructure upstream.
diff --git a/sklearn/externals/joblib.py b/sklearn/externals/joblib.py
deleted file mode 100644
index 3bd6ae73b875..000000000000
--- a/sklearn/externals/joblib.py
+++ /dev/null
@@ -1,15 +0,0 @@
-# We need the absolute_import to avoid the local joblib to override the
-# site one
-from __future__ import absolute_import
-import os as _os
-
-# An environment variable to use the site joblib
-if _os.environ.get('SKLEARN_SITE_JOBLIB', False):
-    from joblib import *
-    from joblib import __version__
-    from joblib import logger
-else:
-    from ._joblib import *
-    from ._joblib import __version__
-    from ._joblib import logger
-
diff --git a/sklearn/externals/_joblib/__init__.py b/sklearn/externals/joblib/__init__.py
similarity index 70%
rename from sklearn/externals/_joblib/__init__.py
rename to sklearn/externals/joblib/__init__.py
index 3455b7d79b51..1b5938350ee3 100644
--- a/sklearn/externals/_joblib/__init__.py
+++ b/sklearn/externals/joblib/__init__.py
@@ -1,27 +1,25 @@
 """Joblib is a set of tools to provide **lightweight pipelining in
-Python**. In particular, joblib offers:
+Python**. In particular:
 
-1. transparent disk-caching of the output values and lazy re-evaluation
+1. transparent disk-caching of functions and lazy re-evaluation
    (memoize pattern)
 
 2. easy simple parallel computing
 
-3. logging and tracing of the execution
-
 Joblib is optimized to be **fast** and **robust** in particular on large
 data and has specific optimizations for `numpy` arrays. It is
 **BSD-licensed**.
 
 
-    ========================= ================================================
-    **User documentation:**        http://pythonhosted.org/joblib
+    ==================== ===============================================
+    **Documentation:**       http://pythonhosted.org/joblib
 
-    **Download packages:**         http://pypi.python.org/pypi/joblib#downloads
+    **Download:**            http://pypi.python.org/pypi/joblib#downloads
 
-    **Source code:**               http://github.com/joblib/joblib
+    **Source code:**         http://github.com/joblib/joblib
 
-    **Report issues:**             http://github.com/joblib/joblib/issues
-    ========================= ================================================
+    **Report issues:**       http://github.com/joblib/joblib/issues
+    ==================== ===============================================
 
 
 Vision
@@ -43,9 +41,8 @@
     good for resuming an application status or computational job, eg
     after a crash.
 
-Joblib strives to address these problems while **leaving your code and
-your flow control as unmodified as possible** (no framework, no new
-paradigms).
+Joblib addresses these problems while **leaving your code and your flow
+control as unmodified as possible** (no framework, no new paradigms).
 
 Main features
 ------------------
@@ -59,16 +56,17 @@
    computation to disk and rerun it only if necessary::
 
       >>> from sklearn.externals.joblib import Memory
-      >>> mem = Memory(cachedir='/tmp/joblib')
+      >>> cachedir = 'your_cache_dir_goes_here'
+      >>> mem = Memory(cachedir)
       >>> import numpy as np
       >>> a = np.vander(np.arange(3)).astype(np.float)
       >>> square = mem.cache(np.square)
       >>> b = square(a)                                   # doctest: +ELLIPSIS
       ________________________________________________________________________________
       [Memory] Calling square...
-      square(array([[ 0.,  0.,  1.],
-             [ 1.,  1.,  1.],
-             [ 4.,  2.,  1.]]))
+      square(array([[0., 0., 1.],
+             [1., 1., 1.],
+             [4., 2., 1.]]))
       ___________________________________________________________square - 0...s, 0.0min
 
       >>> c = square(a)
@@ -83,19 +81,12 @@
       [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]
 
 
-3) **Logging/tracing:** The different functionalities will
-   progressively acquire better logging mechanism to help track what
-   has been ran, and capture I/O easily. In addition, Joblib will
-   provide a few I/O primitives, to easily define logging and
-   display streams, and provide a way of compiling a report.
-   We want to be able to quickly inspect what has been run.
-
-4) **Fast compressed Persistence**: a replacement for pickle to work
+3) **Fast compressed Persistence**: a replacement for pickle to work
    efficiently on Python objects containing large data (
    *joblib.dump* & *joblib.load* ).
 
 ..
-    >>> import shutil ; shutil.rmtree('/tmp/joblib/')
+    >>> import shutil ; shutil.rmtree(cachedir)
 
 """
 
@@ -115,15 +106,16 @@
 # Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
 # 'X.Y.dev0' is the canonical version of 'X.Y.dev'
 #
-__version__ = '0.11'
+__version__ = '0.12.2'
 
 
-from .memory import Memory, MemorizedResult
+from .memory import Memory, MemorizedResult, register_store_backend
 from .logger import PrintTime
 from .logger import Logger
 from .hashing import hash
 from .numpy_pickle import dump
 from .numpy_pickle import load
+from .compressor import register_compressor
 from .parallel import Parallel
 from .parallel import delayed
 from .parallel import cpu_count
@@ -134,4 +126,5 @@
 
 __all__ = ['Memory', 'MemorizedResult', 'PrintTime', 'Logger', 'hash', 'dump',
            'load', 'Parallel', 'delayed', 'cpu_count', 'effective_n_jobs',
-           'register_parallel_backend', 'parallel_backend']
+           'register_parallel_backend', 'parallel_backend',
+           'register_store_backend', 'register_compressor']
diff --git a/sklearn/externals/_joblib/_compat.py b/sklearn/externals/joblib/_compat.py
similarity index 100%
rename from sklearn/externals/_joblib/_compat.py
rename to sklearn/externals/joblib/_compat.py
diff --git a/sklearn/externals/joblib/_dask.py b/sklearn/externals/joblib/_dask.py
new file mode 100644
index 000000000000..92b9627d8ede
--- /dev/null
+++ b/sklearn/externals/joblib/_dask.py
@@ -0,0 +1,259 @@
+from __future__ import print_function, division, absolute_import
+
+import contextlib
+
+from uuid import uuid4
+import weakref
+
+from .parallel import AutoBatchingMixin, ParallelBackendBase, BatchedCalls
+from .parallel import parallel_backend
+
+try:
+    import distributed
+except ImportError:
+    distributed = None
+
+if distributed is not None:
+    from distributed.client import Client, _wait
+    from distributed.utils import funcname, itemgetter
+    from distributed import get_client, secede, rejoin
+    from distributed.worker import thread_state
+    from distributed.sizeof import sizeof
+    from tornado import gen
+
+
+def is_weakrefable(obj):
+    try:
+        weakref.ref(obj)
+        return True
+    except TypeError:
+        return False
+
+
+class _WeakKeyDictionary:
+    """A variant of weakref.WeakKeyDictionary for unhashable objects.
+
+    This datastructure is used to store futures for broadcasted data objects
+    such as large numpy arrays or pandas dataframes that are not hashable and
+    therefore cannot be used as keys of traditional python dicts.
+
+    Futhermore using a dict with id(array) as key is not safe because the
+    Python is likely to reuse id of recently collected arrays.
+    """
+
+    def __init__(self):
+        self._data = {}
+
+    def __getitem__(self, obj):
+        ref, val = self._data[id(obj)]
+        if ref() is not obj:
+            # In case of a race condition with on_destroy.
+            raise KeyError(obj)
+        return val
+
+    def __setitem__(self, obj, value):
+        key = id(obj)
+        try:
+            ref, _ = self._data[key]
+            if ref() is not obj:
+                # In case of race condition with on_destroy.
+                raise KeyError(obj)
+        except KeyError:
+            # Insert the new entry in the mapping along with a weakref
+            # callback to automatically delete the entry from the mapping
+            # as soon as the object used as key is garbage collected.
+            def on_destroy(_):
+                del self._data[key]
+            ref = weakref.ref(obj, on_destroy)
+        self._data[key] = ref, value
+
+    def __len__(self):
+        return len(self._data)
+
+    def clear(self):
+        self._data.clear()
+
+
+def _funcname(x):
+    try:
+        if isinstance(x, BatchedCalls):
+            x = x.items[0][0]
+    except Exception:
+        pass
+    return funcname(x)
+
+
+class Batch(object):
+    def __init__(self, tasks):
+        self.tasks = tasks
+
+    def __call__(self, *data):
+        results = []
+        with parallel_backend('dask'):
+            for func, args, kwargs in self.tasks:
+                args = [a(data) if isinstance(a, itemgetter) else a
+                        for a in args]
+                kwargs = {k: v(data) if isinstance(v, itemgetter) else v
+                          for (k, v) in kwargs.items()}
+                results.append(func(*args, **kwargs))
+        return results
+
+    def __reduce__(self):
+        return Batch, (self.tasks,)
+
+
+class DaskDistributedBackend(ParallelBackendBase, AutoBatchingMixin):
+    MIN_IDEAL_BATCH_DURATION = 0.2
+    MAX_IDEAL_BATCH_DURATION = 1.0
+
+    def __init__(self, scheduler_host=None, scatter=None,
+                 client=None, loop=None, **submit_kwargs):
+        if client is None:
+            if scheduler_host:
+                client = Client(scheduler_host, loop=loop,
+                                set_as_default=False)
+            else:
+                try:
+                    client = get_client()
+                except ValueError:
+                    msg = ("To use Joblib with Dask first create a Dask Client"
+                           "\n\n"
+                           "    from dask.distributed import Client\n"
+                           "    client = Client()\n"
+                           "or\n"
+                           "    client = Client('scheduler-address:8786')")
+                    raise ValueError(msg)
+
+        self.client = client
+
+        if scatter is not None and not isinstance(scatter, (list, tuple)):
+            raise TypeError("scatter must be a list/tuple, got "
+                            "`%s`" % type(scatter).__name__)
+
+        if scatter is not None and len(scatter) > 0:
+            # Keep a reference to the scattered data to keep the ids the same
+            self._scatter = list(scatter)
+            scattered = self.client.scatter(scatter, broadcast=True)
+            self.data_futures = {id(x): f for x, f in zip(scatter, scattered)}
+        else:
+            self._scatter = []
+            self.data_futures = {}
+        self.task_futures = set()
+        self.submit_kwargs = submit_kwargs
+
+    def __reduce__(self):
+        return (DaskDistributedBackend, ())
+
+    def get_nested_backend(self):
+        return DaskDistributedBackend(client=self.client)
+
+    def configure(self, n_jobs=1, parallel=None, **backend_args):
+        return self.effective_n_jobs(n_jobs)
+
+    def start_call(self):
+        self.call_data_futures = _WeakKeyDictionary()
+
+    def stop_call(self):
+        # The explicit call to clear is required to break a cycling reference
+        # to the futures.
+        self.call_data_futures.clear()
+
+    def effective_n_jobs(self, n_jobs):
+        return sum(self.client.ncores().values())
+
+    def _to_func_args(self, func):
+        collected_futures = []
+        itemgetters = dict()
+
+        # Futures that are dynamically generated during a single call to
+        # Parallel.__call__.
+        call_data_futures = getattr(self, 'call_data_futures', None)
+
+        def maybe_to_futures(args):
+            for arg in args:
+                arg_id = id(arg)
+                if arg_id in itemgetters:
+                    yield itemgetters[arg_id]
+                    continue
+
+                f = self.data_futures.get(arg_id, None)
+                if f is None and call_data_futures is not None:
+                    try:
+                        f = call_data_futures[arg]
+                    except KeyError:
+                        if is_weakrefable(arg) and sizeof(arg) > 1e3:
+                            # Automatically scatter large objects to some of
+                            # the workers to avoid duplicated data transfers.
+                            # Rely on automated inter-worker data stealing if
+                            # more workers need to reuse this data
+                            # concurrently.
+                            [f] = self.client.scatter([arg])
+                            call_data_futures[arg] = f
+
+                if f is not None:
+                    getter = itemgetter(len(collected_futures))
+                    collected_futures.append(f)
+                    itemgetters[arg_id] = getter
+                    arg = getter
+                yield arg
+
+        tasks = []
+        for f, args, kwargs in func.items:
+            args = list(maybe_to_futures(args))
+            kwargs = dict(zip(kwargs.keys(),
+                              maybe_to_futures(kwargs.values())))
+            tasks.append((f, args, kwargs))
+
+        if not collected_futures:
+            return func, ()
+        return (Batch(tasks), collected_futures)
+
+    def apply_async(self, func, callback=None):
+        key = '%s-batch-%s' % (_funcname(func), uuid4().hex)
+        func, args = self._to_func_args(func)
+
+        future = self.client.submit(func, *args, key=key, **self.submit_kwargs)
+        self.task_futures.add(future)
+
+        @gen.coroutine
+        def callback_wrapper():
+            result = yield _wait([future])
+            self.task_futures.remove(future)
+            if callback is not None:
+                callback(result)  # gets called in separate thread
+
+        self.client.loop.add_callback(callback_wrapper)
+
+        ref = weakref.ref(future)  # avoid reference cycle
+
+        def get():
+            return ref().result()
+
+        future.get = get  # monkey patch to achieve AsyncResult API
+        return future
+
+    def abort_everything(self, ensure_ready=True):
+        """ Tell the client to cancel any task submitted via this instance
+
+        joblib.Parallel will never access those results
+        """
+        self.client.cancel(self.task_futures)
+        self.task_futures.clear()
+
+    @contextlib.contextmanager
+    def retrieval_context(self):
+        """Override ParallelBackendBase.retrieval_context to avoid deadlocks.
+
+        This removes thread from the worker's thread pool (using 'secede').
+        Seceding avoids deadlock in nested parallelism settings.
+        """
+        # See 'joblib.Parallel.__call__' and 'joblib.Parallel.retrieve' for how
+        # this is used.
+        if hasattr(thread_state, 'execution_state'):
+            # we are in a worker. Secede to avoid deadlock.
+            secede()
+
+        yield
+
+        if hasattr(thread_state, 'execution_state'):
+            rejoin()
diff --git a/sklearn/externals/joblib/_memmapping_reducer.py b/sklearn/externals/joblib/_memmapping_reducer.py
new file mode 100644
index 000000000000..5ba78195b22c
--- /dev/null
+++ b/sklearn/externals/joblib/_memmapping_reducer.py
@@ -0,0 +1,434 @@
+"""
+Reducer using memory mapping for numpy arrays
+"""
+# Author: Thomas Moreau <thomas.moreau.2010@gmail.com>
+# Copyright: 2017, Thomas Moreau
+# License: BSD 3 clause
+
+from mmap import mmap
+import errno
+import os
+import stat
+import threading
+import atexit
+import tempfile
+import warnings
+import weakref
+from uuid import uuid4
+
+try:
+    WindowsError
+except NameError:
+    WindowsError = type(None)
+
+from pickle import whichmodule
+try:
+    # Python 2 compat
+    from cPickle import loads
+    from cPickle import dumps
+except ImportError:
+    from pickle import loads
+    from pickle import dumps
+
+from pickle import HIGHEST_PROTOCOL, PicklingError
+
+try:
+    import numpy as np
+    from numpy.lib.stride_tricks import as_strided
+except ImportError:
+    np = None
+
+from .numpy_pickle import load
+from .numpy_pickle import dump
+from .backports import make_memmap
+from .disk import delete_folder
+
+# Some system have a ramdisk mounted by default, we can use it instead of /tmp
+# as the default folder to dump big arrays to share with subprocesses.
+SYSTEM_SHARED_MEM_FS = '/dev/shm'
+
+# Minimal number of bytes available on SYSTEM_SHARED_MEM_FS to consider using
+# it as the default folder to dump big arrays to share with subprocesses.
+SYSTEM_SHARED_MEM_FS_MIN_SIZE = int(2e9)
+
+# Folder and file permissions to chmod temporary files generated by the
+# memmapping pool. Only the owner of the Python process can access the
+# temporary files and folder.
+FOLDER_PERMISSIONS = stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR
+FILE_PERMISSIONS = stat.S_IRUSR | stat.S_IWUSR
+
+
+class _WeakArrayKeyMap:
+    """A variant of weakref.WeakKeyDictionary for unhashable numpy arrays.
+
+    This datastructure will be used with numpy arrays as obj keys, therefore we
+    do not use the __get__ / __set__ methods to avoid any conflict with the
+    numpy fancy indexing syntax.
+    """
+
+    def __init__(self):
+        self._data = {}
+
+    def get(self, obj):
+        ref, val = self._data[id(obj)]
+        if ref() is not obj:
+            # In case of race condition with on_destroy: could never be
+            # triggered by the joblib tests with CPython.
+            raise KeyError(obj)
+        return val
+
+    def set(self, obj, value):
+        key = id(obj)
+        try:
+            ref, _ = self._data[key]
+            if ref() is not obj:
+                # In case of race condition with on_destroy: could never be
+                # triggered by the joblib tests with CPython.
+                raise KeyError(obj)
+        except KeyError:
+            # Insert the new entry in the mapping along with a weakref
+            # callback to automatically delete the entry from the mapping
+            # as soon as the object used as key is garbage collected.
+            def on_destroy(_):
+                del self._data[key]
+            ref = weakref.ref(obj, on_destroy)
+        self._data[key] = ref, value
+
+    def __getstate__(self):
+        raise PicklingError("_WeakArrayKeyMap is not pickleable")
+
+
+###############################################################################
+# Support for efficient transient pickling of numpy data structures
+
+
+def _get_backing_memmap(a):
+    """Recursively look up the original np.memmap instance base if any."""
+    b = getattr(a, 'base', None)
+    if b is None:
+        # TODO: check scipy sparse datastructure if scipy is installed
+        # a nor its descendants do not have a memmap base
+        return None
+
+    elif isinstance(b, mmap):
+        # a is already a real memmap instance.
+        return a
+
+    else:
+        # Recursive exploration of the base ancestry
+        return _get_backing_memmap(b)
+
+
+def _get_temp_dir(pool_folder_name, temp_folder=None):
+    """Get the full path to a subfolder inside the temporary folder.
+
+    Parameters
+    ----------
+    pool_folder_name : str
+        Sub-folder name used for the serialization of a pool instance.
+
+    temp_folder: str, optional
+        Folder to be used by the pool for memmapping large arrays
+        for sharing memory with worker processes. If None, this will try in
+        order:
+
+        - a folder pointed by the JOBLIB_TEMP_FOLDER environment
+          variable,
+        - /dev/shm if the folder exists and is writable: this is a
+          RAMdisk filesystem available by default on modern Linux
+          distributions,
+        - the default system temporary folder that can be
+          overridden with TMP, TMPDIR or TEMP environment
+          variables, typically /tmp under Unix operating systems.
+
+    Returns
+    -------
+    pool_folder : str
+       full path to the temporary folder
+    use_shared_mem : bool
+       whether the temporary folder is written to the system shared memory
+       folder or some other temporary folder.
+    """
+    use_shared_mem = False
+    if temp_folder is None:
+        temp_folder = os.environ.get('JOBLIB_TEMP_FOLDER', None)
+    if temp_folder is None:
+        if os.path.exists(SYSTEM_SHARED_MEM_FS):
+            try:
+                shm_stats = os.statvfs(SYSTEM_SHARED_MEM_FS)
+                available_nbytes = shm_stats.f_bsize * shm_stats.f_bavail
+                if available_nbytes > SYSTEM_SHARED_MEM_FS_MIN_SIZE:
+                    # Try to see if we have write access to the shared mem
+                    # folder only if it is reasonably large (that is 2GB or
+                    # more).
+                    temp_folder = SYSTEM_SHARED_MEM_FS
+                    pool_folder = os.path.join(temp_folder, pool_folder_name)
+                    if not os.path.exists(pool_folder):
+                        os.makedirs(pool_folder)
+                    use_shared_mem = True
+            except (IOError, OSError):
+                # Missing rights in the /dev/shm partition, fallback to regular
+                # temp folder.
+                temp_folder = None
+    if temp_folder is None:
+        # Fallback to the default tmp folder, typically /tmp
+        temp_folder = tempfile.gettempdir()
+    temp_folder = os.path.abspath(os.path.expanduser(temp_folder))
+    pool_folder = os.path.join(temp_folder, pool_folder_name)
+    return pool_folder, use_shared_mem
+
+
+def has_shareable_memory(a):
+    """Return True if a is backed by some mmap buffer directly or not."""
+    return _get_backing_memmap(a) is not None
+
+
+def _strided_from_memmap(filename, dtype, mode, offset, order, shape, strides,
+                         total_buffer_len):
+    """Reconstruct an array view on a memory mapped file."""
+    if mode == 'w+':
+        # Do not zero the original data when unpickling
+        mode = 'r+'
+
+    if strides is None:
+        # Simple, contiguous memmap
+        return make_memmap(filename, dtype=dtype, shape=shape, mode=mode,
+                           offset=offset, order=order)
+    else:
+        # For non-contiguous data, memmap the total enclosing buffer and then
+        # extract the non-contiguous view with the stride-tricks API
+        base = make_memmap(filename, dtype=dtype, shape=total_buffer_len,
+                           mode=mode, offset=offset, order=order)
+        return as_strided(base, shape=shape, strides=strides)
+
+
+def _reduce_memmap_backed(a, m):
+    """Pickling reduction for memmap backed arrays.
+
+    a is expected to be an instance of np.ndarray (or np.memmap)
+    m is expected to be an instance of np.memmap on the top of the ``base``
+    attribute ancestry of a. ``m.base`` should be the real python mmap object.
+    """
+    # offset that comes from the striding differences between a and m
+    a_start, a_end = np.byte_bounds(a)
+    m_start = np.byte_bounds(m)[0]
+    offset = a_start - m_start
+
+    # offset from the backing memmap
+    offset += m.offset
+
+    if m.flags['F_CONTIGUOUS']:
+        order = 'F'
+    else:
+        # The backing memmap buffer is necessarily contiguous hence C if not
+        # Fortran
+        order = 'C'
+
+    if a.flags['F_CONTIGUOUS'] or a.flags['C_CONTIGUOUS']:
+        # If the array is a contiguous view, no need to pass the strides
+        strides = None
+        total_buffer_len = None
+    else:
+        # Compute the total number of items to map from which the strided
+        # view will be extracted.
+        strides = a.strides
+        total_buffer_len = (a_end - a_start) // a.itemsize
+    return (_strided_from_memmap,
+            (m.filename, a.dtype, m.mode, offset, order, a.shape, strides,
+             total_buffer_len))
+
+
+def reduce_memmap(a):
+    """Pickle the descriptors of a memmap instance to reopen on same file."""
+    m = _get_backing_memmap(a)
+    if m is not None:
+        # m is a real mmap backed memmap instance, reduce a preserving striding
+        # information
+        return _reduce_memmap_backed(a, m)
+    else:
+        # This memmap instance is actually backed by a regular in-memory
+        # buffer: this can happen when using binary operators on numpy.memmap
+        # instances
+        return (loads, (dumps(np.asarray(a), protocol=HIGHEST_PROTOCOL),))
+
+
+class ArrayMemmapReducer(object):
+    """Reducer callable to dump large arrays to memmap files.
+
+    Parameters
+    ----------
+    max_nbytes: int
+        Threshold to trigger memmapping of large arrays to files created
+        a folder.
+    temp_folder: str
+        Path of a folder where files for backing memmapped arrays are created.
+    mmap_mode: 'r', 'r+' or 'c'
+        Mode for the created memmap datastructure. See the documentation of
+        numpy.memmap for more details. Note: 'w+' is coerced to 'r+'
+        automatically to avoid zeroing the data on unpickling.
+    verbose: int, optional, 0 by default
+        If verbose > 0, memmap creations are logged.
+        If verbose > 1, both memmap creations, reuse and array pickling are
+        logged.
+    prewarm: bool, optional, False by default.
+        Force a read on newly memmapped array to make sure that OS pre-cache it
+        memory. This can be useful to avoid concurrent disk access when the
+        same data array is passed to different worker processes.
+    """
+
+    def __init__(self, max_nbytes, temp_folder, mmap_mode, verbose=0,
+                 prewarm=True):
+        self._max_nbytes = max_nbytes
+        self._temp_folder = temp_folder
+        self._mmap_mode = mmap_mode
+        self.verbose = int(verbose)
+        self._prewarm = prewarm
+        self._memmaped_arrays = _WeakArrayKeyMap()
+
+    def __reduce__(self):
+        # The ArrayMemmapReducer is passed to the children processes: it needs
+        # to be pickled but the _WeakArrayKeyMap need to be skipped as it's
+        # only guaranteed to be consistent with the parent process memory
+        # garbage collection.
+        args = (self._max_nbytes, self._temp_folder, self._mmap_mode)
+        kwargs = {
+            'verbose': self.verbose,
+            'prewarm': self._prewarm,
+        }
+        return ArrayMemmapReducer, args, kwargs
+
+    def __call__(self, a):
+        m = _get_backing_memmap(a)
+        if m is not None:
+            # a is already backed by a memmap file, let's reuse it directly
+            return _reduce_memmap_backed(a, m)
+
+        if (not a.dtype.hasobject and self._max_nbytes is not None and
+                a.nbytes > self._max_nbytes):
+            # check that the folder exists (lazily create the pool temp folder
+            # if required)
+            try:
+                os.makedirs(self._temp_folder)
+                os.chmod(self._temp_folder, FOLDER_PERMISSIONS)
+            except OSError as e:
+                if e.errno != errno.EEXIST:
+                    raise e
+
+            try:
+                basename = self._memmaped_arrays.get(a)
+            except KeyError:
+                # Generate a new unique random filename. The process and thread
+                # ids are only useful for debugging purpose and to make it
+                # easier to cleanup orphaned files in case of hard process
+                # kill (e.g. by "kill -9" or segfault).
+                basename = "{}-{}-{}.pkl".format(
+                    os.getpid(), id(threading.current_thread()), uuid4().hex)
+                self._memmaped_arrays.set(a, basename)
+            filename = os.path.join(self._temp_folder, basename)
+
+            # In case the same array with the same content is passed several
+            # times to the pool subprocess children, serialize it only once
+
+            # XXX: implement an explicit reference counting scheme to make it
+            # possible to delete temporary files as soon as the workers are
+            # done processing this data.
+            if not os.path.exists(filename):
+                if self.verbose > 0:
+                    print("Memmapping (shape={}, dtype={}) to new file {}"
+                          .format(a.shape, a.dtype, filename))
+                for dumped_filename in dump(a, filename):
+                    os.chmod(dumped_filename, FILE_PERMISSIONS)
+
+                if self._prewarm:
+                    # Warm up the data by accessing it. This operation ensures
+                    # that the disk access required to create the memmapping
+                    # file are performed in the reducing process and avoids
+                    # concurrent memmap creation in multiple children
+                    # processes.
+                    load(filename, mmap_mode=self._mmap_mode).max()
+            elif self.verbose > 1:
+                print("Memmapping (shape={}, dtype={}) to old file {}"
+                      .format(a.shape, a.dtype, filename))
+
+            # The worker process will use joblib.load to memmap the data
+            return (load, (filename, self._mmap_mode))
+        else:
+            # do not convert a into memmap, let pickler do its usual copy with
+            # the default system pickler
+            if self.verbose > 1:
+                print("Pickling array (shape={}, dtype={})."
+                      .format(a.shape, a.dtype))
+            return (loads, (dumps(a, protocol=HIGHEST_PROTOCOL),))
+
+
+def get_memmapping_reducers(
+        pool_id, forward_reducers=None, backward_reducers=None,
+        temp_folder=None, max_nbytes=1e6, mmap_mode='r', verbose=0,
+        prewarm=False, **kwargs):
+    """Construct a pair of memmapping reducer linked to a tmpdir.
+
+    This function manage the creation and the clean up of the temporary folders
+    underlying the memory maps and should be use to get the reducers necessary
+    to construct joblib pool or executor.
+    """
+    if forward_reducers is None:
+        forward_reducers = dict()
+    if backward_reducers is None:
+        backward_reducers = dict()
+
+    # Prepare a sub-folder name for the serialization of this particular
+    # pool instance (do not create in advance to spare FS write access if
+    # no array is to be dumped):
+    pool_folder_name = "joblib_memmapping_folder_{}_{}".format(
+        os.getpid(), pool_id)
+    pool_folder, use_shared_mem = _get_temp_dir(pool_folder_name,
+                                                temp_folder)
+
+    # Register the garbage collector at program exit in case caller forgets
+    # to call terminate explicitly: note we do not pass any reference to
+    # self to ensure that this callback won't prevent garbage collection of
+    # the pool instance and related file handler resources such as POSIX
+    # semaphores and pipes
+    pool_module_name = whichmodule(delete_folder, 'delete_folder')
+
+    def _cleanup():
+        # In some cases the Python runtime seems to set delete_folder to
+        # None just before exiting when accessing the delete_folder
+        # function from the closure namespace. So instead we reimport
+        # the delete_folder function explicitly.
+        # https://github.com/joblib/joblib/issues/328
+        # We cannot just use from 'joblib.pool import delete_folder'
+        # because joblib should only use relative imports to allow
+        # easy vendoring.
+        delete_folder = __import__(
+            pool_module_name, fromlist=['delete_folder']).delete_folder
+        try:
+            delete_folder(pool_folder)
+        except WindowsError:
+            warnings.warn("Failed to clean temporary folder: {}"
+                          .format(pool_folder))
+
+    atexit.register(_cleanup)
+
+    if np is not None:
+        # Register smart numpy.ndarray reducers that detects memmap backed
+        # arrays and that is also able to dump to memmap large in-memory
+        # arrays over the max_nbytes threshold
+        if prewarm == "auto":
+            prewarm = not use_shared_mem
+        forward_reduce_ndarray = ArrayMemmapReducer(
+            max_nbytes, pool_folder, mmap_mode, verbose,
+            prewarm=prewarm)
+        forward_reducers[np.ndarray] = forward_reduce_ndarray
+        forward_reducers[np.memmap] = reduce_memmap
+
+        # Communication from child process to the parent process always
+        # pickles in-memory numpy.ndarray without dumping them as memmap
+        # to avoid confusing the caller and make it tricky to collect the
+        # temporary folder
+        backward_reduce_ndarray = ArrayMemmapReducer(
+            None, pool_folder, mmap_mode, verbose)
+        backward_reducers[np.ndarray] = backward_reduce_ndarray
+        backward_reducers[np.memmap] = reduce_memmap
+
+    return forward_reducers, backward_reducers, pool_folder
diff --git a/sklearn/externals/_joblib/_memory_helpers.py b/sklearn/externals/joblib/_memory_helpers.py
similarity index 100%
rename from sklearn/externals/_joblib/_memory_helpers.py
rename to sklearn/externals/joblib/_memory_helpers.py
diff --git a/sklearn/externals/_joblib/_multiprocessing_helpers.py b/sklearn/externals/joblib/_multiprocessing_helpers.py
similarity index 100%
rename from sklearn/externals/_joblib/_multiprocessing_helpers.py
rename to sklearn/externals/joblib/_multiprocessing_helpers.py
diff --git a/sklearn/externals/_joblib/_parallel_backends.py b/sklearn/externals/joblib/_parallel_backends.py
similarity index 57%
rename from sklearn/externals/_joblib/_parallel_backends.py
rename to sklearn/externals/joblib/_parallel_backends.py
index 7035f66e3845..85312abec6aa 100644
--- a/sklearn/externals/_joblib/_parallel_backends.py
+++ b/sklearn/externals/joblib/_parallel_backends.py
@@ -7,21 +7,39 @@
 import sys
 import warnings
 import threading
+import functools
+import contextlib
 from abc import ABCMeta, abstractmethod
 
 from .format_stack import format_exc
 from .my_exceptions import WorkerInterrupt, TransportableException
 from ._multiprocessing_helpers import mp
-from ._compat import with_metaclass
+from ._compat import with_metaclass, PY27
 if mp is not None:
-    from .pool import MemmapingPool
+    from .disk import delete_folder
+    from .pool import MemmappingPool
     from multiprocessing.pool import ThreadPool
+    from .executor import get_memmapping_executor
+
+    # Compat between concurrent.futures and multiprocessing TimeoutError
+    from multiprocessing import TimeoutError
+    from .externals.loky._base import TimeoutError as LokyTimeoutError
+    from .externals.loky import process_executor, cpu_count
 
 
 class ParallelBackendBase(with_metaclass(ABCMeta)):
     """Helper abc which defines all methods a ParallelBackend must implement"""
 
     supports_timeout = False
+    nesting_level = 0
+
+    def __init__(self, nesting_level=0):
+        self.nesting_level = nesting_level
+
+    SUPPORTED_CLIB_VARS = [
+        'OMP_NUM_THREADS', 'OPENBLAS_NUM_THREADS', 'MKL_NUM_THREADS',
+        'VECLIB_MAXIMUM_THREADS', 'NUMEXPR_NUM_THREADS'
+    ]
 
     @abstractmethod
     def effective_n_jobs(self, n_jobs):
@@ -45,7 +63,8 @@ def effective_n_jobs(self, n_jobs):
     def apply_async(self, func, callback=None):
         """Schedule a func to be run"""
 
-    def configure(self, n_jobs=1, parallel=None, **backend_args):
+    def configure(self, n_jobs=1, parallel=None, prefer=None, require=None,
+                  **backend_args):
         """Reconfigure the backend and return the number of workers.
 
         This makes it possible to reuse an existing backend instance for
@@ -54,8 +73,14 @@ def configure(self, n_jobs=1, parallel=None, **backend_args):
         self.parallel = parallel
         return self.effective_n_jobs(n_jobs)
 
+    def start_call(self):
+        """Call-back method called at the beginning of a Parallel call"""
+
+    def stop_call(self):
+        """Call-back method called at the end of a Parallel call"""
+
     def terminate(self):
-        """Shutdown the process or thread pool"""
+        """Shutdown the workers and free the shared memory."""
 
     def compute_batch_size(self):
         """Determine the optimal batch size"""
@@ -86,12 +111,56 @@ def abort_everything(self, ensure_ready=True):
         Setting ensure_ready to False is an optimization that can be leveraged
         when aborting tasks via killing processes from a local process pool
         managed by the backend it-self: if we expect no new tasks, there is no
-        point in re-creating a new working pool.
+        point in re-creating new workers.
         """
-        # Does nothing by default: to be overridden in subclasses when canceling
-        # tasks is possible.
+        # Does nothing by default: to be overridden in subclasses when
+        # canceling tasks is possible.
         pass
 
+    def get_nested_backend(self):
+        """Backend instance to be used by nested Parallel calls.
+
+        By default a thread-based backend is used for the first level of
+        nesting. Beyond, switch to sequential backend to avoid spawning too
+        many threads on the host.
+        """
+        nesting_level = getattr(self, 'nesting_level', 0) + 1
+        if nesting_level > 1:
+            return SequentialBackend(nesting_level=nesting_level)
+        else:
+            return ThreadingBackend(nesting_level=nesting_level)
+
+
+    @contextlib.contextmanager
+    def retrieval_context(self):
+        """Context manager to manage an execution context.
+
+        Calls to Parallel.retrieve will be made inside this context.
+
+        By default, this does nothing. It may be useful for subclasses to
+        handle nested parallelism. In particular, it may be required to avoid
+        deadlocks if a backend manages a fixed number of workers, when those
+        workers may be asked to do nested Parallel calls. Without
+        'retrieval_context' this could lead to deadlock, as all the workers
+        managed by the backend may be "busy" waiting for the nested parallel
+        calls to finish, but the backend has no free workers to execute those
+        tasks.
+        """
+        yield
+
+    @classmethod
+    def limit_clib_threads(cls, n_threads=1):
+        """Initializer to limit the number of threads used by some C-libraries.
+
+        This function set the number of threads to `n_threads` for OpenMP, MKL,
+        Accelerated and OpenBLAS libraries, that can be used with scientific
+        computing tools like numpy.
+        """
+        for var in cls.SUPPORTED_CLIB_VARS:
+            var_value = os.environ.get(var, None)
+            if var_value is None:
+                os.environ[var] = str(n_threads)
+
 
 class SequentialBackend(ParallelBackendBase):
     """A ParallelBackend which will execute all batches sequentially.
@@ -100,6 +169,9 @@ class SequentialBackend(ParallelBackendBase):
     overhead. Used when n_jobs == 1.
     """
 
+    uses_threads = True
+    supports_sharedmem = True
+
     def effective_n_jobs(self, n_jobs):
         """Determine the number of jobs which are going to run in parallel"""
         if n_jobs == 0:
@@ -113,10 +185,16 @@ def apply_async(self, func, callback=None):
             callback(result)
         return result
 
+    def get_nested_backend(self):
+        nested_level = getattr(self, 'nesting_level', 0) + 1
+        return SequentialBackend(nesting_level=nested_level)
+
 
 class PoolManagerMixin(object):
     """A helper class for managing pool of workers."""
 
+    _pool = None
+
     def effective_n_jobs(self, n_jobs):
         """Determine the number of jobs which are going to run in parallel"""
         if n_jobs == 0:
@@ -126,7 +204,7 @@ def effective_n_jobs(self, n_jobs):
             # to sequential mode
             return 1
         elif n_jobs < 0:
-            n_jobs = max(mp.cpu_count() + 1 + n_jobs, 1)
+            n_jobs = max(cpu_count() + 1 + n_jobs, 1)
         return n_jobs
 
     def terminate(self):
@@ -136,9 +214,14 @@ def terminate(self):
             self._pool.terminate()  # terminate does a join()
             self._pool = None
 
+    def _get_pool(self):
+        """Used by apply_async to make it possible to implement lazy init"""
+        return self._pool
+
     def apply_async(self, func, callback=None):
         """Schedule a func to be run"""
-        return self._pool.apply_async(SafeFunction(func), callback=callback)
+        return self._get_pool().apply_async(
+            SafeFunction(func), callback=callback)
 
     def abort_everything(self, ensure_ready=True):
         """Shutdown the pool and restart a new one with the same parameters"""
@@ -161,9 +244,13 @@ class AutoBatchingMixin(object):
     # on a single worker while other workers have no work to process any more.
     MAX_IDEAL_BATCH_DURATION = 2
 
-    # Batching counters
-    _effective_batch_size = 1
-    _smoothed_batch_duration = 0.0
+    # Batching counters default values
+    _DEFAULT_EFFECTIVE_BATCH_SIZE = 1
+    _DEFAULT_SMOOTHED_BATCH_DURATION = 0.0
+
+    def __init__(self):
+        self._effective_batch_size = self._DEFAULT_EFFECTIVE_BATCH_SIZE
+        self._smoothed_batch_duration = self._DEFAULT_SMOOTHED_BATCH_DURATION
 
     def compute_batch_size(self):
         """Determine the optimal batch size"""
@@ -207,7 +294,8 @@ def compute_batch_size(self):
             # CallBack as long as the batch_size is constant. Therefore
             # we need to reset the estimate whenever we re-tune the batch
             # size.
-            self._smoothed_batch_duration = 0
+            self._smoothed_batch_duration = \
+                self._DEFAULT_SMOOTHED_BATCH_DURATION
 
         return batch_size
 
@@ -217,7 +305,7 @@ def batch_completed(self, batch_size, duration):
             # Update the smoothed streaming estimate of the duration of a batch
             # from dispatch to completion
             old_duration = self._smoothed_batch_duration
-            if old_duration == 0:
+            if old_duration == self._DEFAULT_SMOOTHED_BATCH_DURATION:
                 # First record of duration for this batch size after the last
                 # reset.
                 new_duration = duration
@@ -227,6 +315,14 @@ def batch_completed(self, batch_size, duration):
                 new_duration = 0.8 * old_duration + 0.2 * duration
             self._smoothed_batch_duration = new_duration
 
+    def reset_batch_stats(self):
+        """Reset batch statistics to default values.
+
+        This avoids interferences with future jobs.
+        """
+        self._effective_batch_size = self._DEFAULT_EFFECTIVE_BATCH_SIZE
+        self._smoothed_batch_duration = self._DEFAULT_SMOOTHED_BATCH_DURATION
+
 
 class ThreadingBackend(PoolManagerMixin, ParallelBackendBase):
     """A ParallelBackend which will use a thread pool to execute batches in.
@@ -234,11 +330,18 @@ class ThreadingBackend(PoolManagerMixin, ParallelBackendBase):
     This is a low-overhead backend but it suffers from the Python Global
     Interpreter Lock if the called function relies a lot on Python objects.
     Mostly useful when the execution bottleneck is a compiled extension that
-    explicitly releases the GIL (for instance a Cython loop wrapped in a
-    "with nogil" block or an expensive call to a library such as NumPy).
+    explicitly releases the GIL (for instance a Cython loop wrapped in a "with
+    nogil" block or an expensive call to a library such as NumPy).
+
+    The actual thread pool is lazily initialized: the actual thread pool
+    construction is delayed to the first call to apply_async.
+
+    ThreadingBackend is used as the default backend for nested calls.
     """
 
     supports_timeout = True
+    uses_threads = True
+    supports_sharedmem = True
 
     def configure(self, n_jobs=1, parallel=None, **backend_args):
         """Build a process or thread pool and return the number of workers"""
@@ -247,9 +350,19 @@ def configure(self, n_jobs=1, parallel=None, **backend_args):
             # Avoid unnecessary overhead and use sequential backend instead.
             raise FallbackToBackend(SequentialBackend())
         self.parallel = parallel
-        self._pool = ThreadPool(n_jobs)
+        self._n_jobs = n_jobs
         return n_jobs
 
+    def _get_pool(self):
+        """Lazily initialize the thread pool
+
+        The actual pool of worker threads is only initialized at the first
+        call to apply_async.
+        """
+        if self._pool is None:
+            self._pool = ThreadPool(self._n_jobs)
+        return self._pool
+
 
 class MultiprocessingBackend(PoolManagerMixin, AutoBatchingMixin,
                              ParallelBackendBase):
@@ -283,17 +396,28 @@ def effective_n_jobs(self, n_jobs):
                     stacklevel=3)
             return 1
 
+        if process_executor._CURRENT_DEPTH > 0:
+            # Mixing loky and multiprocessing in nested loop is not supported
+            if n_jobs != 1:
+                warnings.warn(
+                    'Multiprocessing-backed parallel loops cannot be nested,'
+                    ' below loky, setting n_jobs=1',
+                    stacklevel=3)
+            return 1
+
         if not isinstance(threading.current_thread(), threading._MainThread):
             # Prevent posix fork inside in non-main posix threads
-            warnings.warn(
-                'Multiprocessing-backed parallel loops cannot be nested'
-                ' below threads, setting n_jobs=1',
-                stacklevel=3)
+            if n_jobs != 1:
+                warnings.warn(
+                    'Multiprocessing-backed parallel loops cannot be nested'
+                    ' below threads, setting n_jobs=1',
+                    stacklevel=3)
             return 1
 
         return super(MultiprocessingBackend, self).effective_n_jobs(n_jobs)
 
-    def configure(self, n_jobs=1, parallel=None, **backend_args):
+    def configure(self, n_jobs=1, parallel=None, prefer=None, require=None,
+                  **memmappingpool_args):
         """Build a process or thread pool and return the number of workers"""
         n_jobs = self.effective_n_jobs(n_jobs)
         if n_jobs == 1:
@@ -314,7 +438,8 @@ def configure(self, n_jobs=1, parallel=None, **backend_args):
 
         # Make sure to free as much memory as possible before forking
         gc.collect()
-        self._pool = MemmapingPool(n_jobs, **backend_args)
+        self._pool = MemmappingPool(
+            n_jobs, initializer=self.limit_clib_threads, **memmappingpool_args)
         self.parallel = parallel
         return n_jobs
 
@@ -324,6 +449,92 @@ def terminate(self):
         if self.JOBLIB_SPAWNED_PROCESS in os.environ:
             del os.environ[self.JOBLIB_SPAWNED_PROCESS]
 
+        self.reset_batch_stats()
+
+
+class LokyBackend(AutoBatchingMixin, ParallelBackendBase):
+    """Managing pool of workers with loky instead of multiprocessing."""
+
+    supports_timeout = True
+
+    def configure(self, n_jobs=1, parallel=None, prefer=None, require=None,
+                  idle_worker_timeout=300, **memmappingexecutor_args):
+        """Build a process executor and return the number of workers"""
+        n_jobs = self.effective_n_jobs(n_jobs)
+        if n_jobs == 1:
+            raise FallbackToBackend(SequentialBackend())
+
+        self._workers = get_memmapping_executor(
+            n_jobs, timeout=idle_worker_timeout,
+            initializer=self.limit_clib_threads,
+            **memmappingexecutor_args)
+        self.parallel = parallel
+        return n_jobs
+
+    def effective_n_jobs(self, n_jobs):
+        """Determine the number of jobs which are going to run in parallel"""
+        if n_jobs == 0:
+            raise ValueError('n_jobs == 0 in Parallel has no meaning')
+        elif mp is None or n_jobs is None:
+            # multiprocessing is not available or disabled, fallback
+            # to sequential mode
+            return 1
+        elif mp.current_process().daemon:
+            # Daemonic processes cannot have children
+            if n_jobs != 1:
+                warnings.warn(
+                    'Loky-backed parallel loops cannot be called in a'
+                    ' multiprocessing, setting n_jobs=1',
+                    stacklevel=3)
+            return 1
+        elif not isinstance(threading.current_thread(), threading._MainThread):
+            # Prevent posix fork inside in non-main posix threads
+            if n_jobs != 1:
+                warnings.warn(
+                    'Loky-backed parallel loops cannot be nested below '
+                    'threads, setting n_jobs=1',
+                    stacklevel=3)
+            return 1
+        elif n_jobs < 0:
+            n_jobs = max(cpu_count() + 1 + n_jobs, 1)
+        return n_jobs
+
+    def apply_async(self, func, callback=None):
+        """Schedule a func to be run"""
+        future = self._workers.submit(SafeFunction(func))
+        future.get = functools.partial(self.wrap_future_result, future)
+        if callback is not None:
+            future.add_done_callback(callback)
+        return future
+
+    @staticmethod
+    def wrap_future_result(future, timeout=None):
+        """Wrapper for Future.result to implement the same behaviour as
+        AsyncResults.get from multiprocessing."""
+        try:
+            return future.result(timeout=timeout)
+        except LokyTimeoutError:
+            raise TimeoutError()
+
+    def terminate(self):
+        if self._workers is not None:
+            # Terminate does not shutdown the workers as we want to reuse them
+            # in latter calls but we free as much memory as we can by deleting
+            # the shared memory
+            delete_folder(self._workers._temp_folder)
+            self._workers = None
+
+        self.reset_batch_stats()
+
+    def abort_everything(self, ensure_ready=True):
+        """Shutdown the workers and restart a new one with the same parameters
+        """
+        self._workers.shutdown(kill_workers=True)
+        delete_folder(self._workers._temp_folder)
+        self._workers = None
+        if ensure_ready:
+            self.configure(n_jobs=self.parallel.n_jobs, parallel=self.parallel)
+
 
 class ImmediateResult(object):
     def __init__(self, batch):
@@ -353,10 +564,17 @@ def __call__(self, *args, **kwargs):
             # something different, as multiprocessing does not
             # interrupt processing for a KeyboardInterrupt
             raise WorkerInterrupt()
-        except:
-            e_type, e_value, e_tb = sys.exc_info()
-            text = format_exc(e_type, e_value, e_tb, context=10, tb_offset=1)
-            raise TransportableException(text, e_type)
+        except BaseException:
+            if PY27:
+                # Capture the traceback of the worker to make it part of
+                # the final exception message.
+                e_type, e_value, e_tb = sys.exc_info()
+                text = format_exc(e_type, e_value, e_tb, context=10,
+                                  tb_offset=1)
+                raise TransportableException(text, e_type)
+            else:
+                # Rely on Python 3 built-in Remote Traceback reporting
+                raise
 
 
 class FallbackToBackend(Exception):
diff --git a/sklearn/externals/joblib/_store_backends.py b/sklearn/externals/joblib/_store_backends.py
new file mode 100644
index 000000000000..027fb9f9f7fb
--- /dev/null
+++ b/sklearn/externals/joblib/_store_backends.py
@@ -0,0 +1,412 @@
+"""Storage providers backends for Memory caching."""
+
+import re
+import os
+import os.path
+import datetime
+import json
+import shutil
+import warnings
+import collections
+import operator
+import threading
+from abc import ABCMeta, abstractmethod
+
+from ._compat import with_metaclass, _basestring
+from .backports import concurrency_safe_rename
+from .disk import mkdirp, memstr_to_bytes, rm_subdirs
+from . import numpy_pickle
+
+CacheItemInfo = collections.namedtuple('CacheItemInfo',
+                                       'path size last_access')
+
+
+def concurrency_safe_write(object_to_write, filename, write_func):
+    """Writes an object into a unique file in a concurrency-safe way."""
+    thread_id = id(threading.current_thread())
+    temporary_filename = '{}.thread-{}-pid-{}'.format(
+        filename, thread_id, os.getpid())
+    write_func(object_to_write, temporary_filename)
+
+    return temporary_filename
+
+
+class StoreBackendBase(with_metaclass(ABCMeta)):
+    """Helper Abstract Base Class which defines all methods that
+       a StorageBackend must implement."""
+
+    @abstractmethod
+    def _open_item(self, f, mode):
+        """Opens an item on the store and return a file-like object.
+
+        This method is private and only used by the StoreBackendMixin object.
+
+        Parameters
+        ----------
+        f: a file-like object
+            The file-like object where an item is stored and retrieved
+        mode: string, optional
+            the mode in which the file-like object is opened allowed valued are
+            'rb', 'wb'
+
+        Returns
+        -------
+        a file-like object
+        """
+
+    @abstractmethod
+    def _item_exists(self, location):
+        """Checks if an item location exists in the store.
+
+        This method is private and only used by the StoreBackendMixin object.
+
+        Parameters
+        ----------
+        location: string
+            The location of an item. On a filesystem, this corresponds to the
+            absolute path, including the filename, of a file.
+
+        Returns
+        -------
+        True if the item exists, False otherwise
+        """
+
+    @abstractmethod
+    def _move_item(self, src, dst):
+        """Moves an item from src to dst in the store.
+
+        This method is private and only used by the StoreBackendMixin object.
+
+        Parameters
+        ----------
+        src: string
+            The source location of an item
+        dst: string
+            The destination location of an item
+        """
+
+    @abstractmethod
+    def create_location(self, location):
+        """Creates a location on the store.
+
+        Parameters
+        ----------
+        location: string
+            The location in the store. On a filesystem, this corresponds to a
+            directory.
+        """
+
+    @abstractmethod
+    def clear_location(self, location):
+        """Clears a location on the store.
+
+        Parameters
+        ----------
+        location: string
+            The location in the store. On a filesystem, this corresponds to a
+            directory or a filename absolute path
+        """
+
+    @abstractmethod
+    def get_items(self):
+        """Returns the whole list of items available in the store.
+
+        Returns
+        -------
+        The list of items identified by their ids (e.g filename in a
+        filesystem).
+        """
+
+    @abstractmethod
+    def configure(self, location, verbose=0, backend_options=dict()):
+        """Configures the store.
+
+        Parameters
+        ----------
+        location: string
+            The base location used by the store. On a filesystem, this
+            corresponds to a directory.
+        verbose: int
+            The level of verbosity of the store
+        backend_options: dict
+            Contains a dictionnary of named paremeters used to configure the
+            store backend.
+        """
+
+
+class StoreBackendMixin(object):
+    """Class providing all logic for managing the store in a generic way.
+
+    The StoreBackend subclass has to implement 3 methods: create_location,
+    clear_location and configure. The StoreBackend also has to provide
+    a private _open_item, _item_exists and _move_item methods. The _open_item
+    method has to have the same signature as the builtin open and return a
+    file-like object.
+    """
+
+    def load_item(self, path, verbose=1, msg=None):
+        """Load an item from the store given its path as a list of
+           strings."""
+        full_path = os.path.join(self.location, *path)
+
+        if verbose > 1:
+            if verbose < 10:
+                print('{0}...'.format(msg))
+            else:
+                print('{0} from {1}'.format(msg, full_path))
+
+        mmap_mode = (None if not hasattr(self, 'mmap_mode')
+                     else self.mmap_mode)
+
+        filename = os.path.join(full_path, 'output.pkl')
+        if not self._item_exists(filename):
+            raise KeyError("Non-existing item (may have been "
+                           "cleared).\nFile %s does not exist" % filename)
+
+        # file-like object cannot be used when mmap_mode is set
+        if mmap_mode is None:
+            with self._open_item(filename, "rb") as f:
+                item = numpy_pickle.load(f)
+        else:
+            item = numpy_pickle.load(filename, mmap_mode=mmap_mode)
+        return item
+
+    def dump_item(self, path, item, verbose=1):
+        """Dump an item in the store at the path given as a list of
+           strings."""
+        try:
+            item_path = os.path.join(self.location, *path)
+            if not self._item_exists(item_path):
+                self.create_location(item_path)
+            filename = os.path.join(item_path, 'output.pkl')
+            if verbose > 10:
+                print('Persisting in %s' % item_path)
+
+            def write_func(to_write, dest_filename):
+                with self._open_item(dest_filename, "wb") as f:
+                    numpy_pickle.dump(to_write, f,
+                                      compress=self.compress)
+
+            self._concurrency_safe_write(item, filename, write_func)
+        except:  # noqa: E722
+            " Race condition in the creation of the directory "
+
+    def clear_item(self, path):
+        """Clear the item at the path, given as a list of strings."""
+        item_path = os.path.join(self.location, *path)
+        if self._item_exists(item_path):
+            self.clear_location(item_path)
+
+    def contains_item(self, path):
+        """Check if there is an item at the path, given as a list of
+           strings"""
+        item_path = os.path.join(self.location, *path)
+        filename = os.path.join(item_path, 'output.pkl')
+
+        return self._item_exists(filename)
+
+    def get_item_info(self, path):
+        """Return information about item."""
+        return {'location': os.path.join(self.location,
+                                         *path)}
+
+    def get_metadata(self, path):
+        """Return actual metadata of an item."""
+        try:
+            item_path = os.path.join(self.location, *path)
+            filename = os.path.join(item_path, 'metadata.json')
+            with self._open_item(filename, 'rb') as f:
+                return json.loads(f.read().decode('utf-8'))
+        except:  # noqa: E722
+            return {}
+
+    def store_metadata(self, path, metadata):
+        """Store metadata of a computation."""
+        try:
+            item_path = os.path.join(self.location, *path)
+            self.create_location(item_path)
+            filename = os.path.join(item_path, 'metadata.json')
+
+            def write_func(to_write, dest_filename):
+                with self._open_item(dest_filename, "wb") as f:
+                    f.write(json.dumps(to_write).encode('utf-8'))
+
+            self._concurrency_safe_write(metadata, filename, write_func)
+        except:  # noqa: E722
+            pass
+
+    def contains_path(self, path):
+        """Check cached function is available in store."""
+        func_path = os.path.join(self.location, *path)
+        return self.object_exists(func_path)
+
+    def clear_path(self, path):
+        """Clear all items with a common path in the store."""
+        func_path = os.path.join(self.location, *path)
+        if self._item_exists(func_path):
+            self.clear_location(func_path)
+
+    def store_cached_func_code(self, path, func_code=None):
+        """Store the code of the cached function."""
+        func_path = os.path.join(self.location, *path)
+        if not self._item_exists(func_path):
+            self.create_location(func_path)
+
+        if func_code is not None:
+            filename = os.path.join(func_path, "func_code.py")
+            with self._open_item(filename, 'wb') as f:
+                f.write(func_code.encode('utf-8'))
+
+    def get_cached_func_code(self, path):
+        """Store the code of the cached function."""
+        path += ['func_code.py', ]
+        filename = os.path.join(self.location, *path)
+        try:
+            with self._open_item(filename, 'rb') as f:
+                return f.read().decode('utf-8')
+        except:  # noqa: E722
+            raise
+
+    def get_cached_func_info(self, path):
+        """Return information related to the cached function if it exists."""
+        return {'location': os.path.join(self.location, *path)}
+
+    def clear(self):
+        """Clear the whole store content."""
+        self.clear_location(self.location)
+
+    def reduce_store_size(self, bytes_limit):
+        """Reduce store size to keep it under the given bytes limit."""
+        items_to_delete = self._get_items_to_delete(bytes_limit)
+
+        for item in items_to_delete:
+            if self.verbose > 10:
+                print('Deleting item {0}'.format(item))
+            try:
+                self.clear_location(item.path)
+            except OSError:
+                # Even with ignore_errors=True can shutil.rmtree
+                # can raise OSErrror with [Errno 116] Stale file
+                # handle if another process has deleted the folder
+                # already.
+                pass
+
+    def _get_items_to_delete(self, bytes_limit):
+        """Get items to delete to keep the store under a size limit."""
+        if isinstance(bytes_limit, _basestring):
+            bytes_limit = memstr_to_bytes(bytes_limit)
+
+        items = self.get_items()
+        size = sum(item.size for item in items)
+
+        to_delete_size = size - bytes_limit
+        if to_delete_size < 0:
+            return []
+
+        # We want to delete first the cache items that were accessed a
+        # long time ago
+        items.sort(key=operator.attrgetter('last_access'))
+
+        items_to_delete = []
+        size_so_far = 0
+
+        for item in items:
+            if size_so_far > to_delete_size:
+                break
+
+            items_to_delete.append(item)
+            size_so_far += item.size
+
+        return items_to_delete
+
+    def _concurrency_safe_write(self, to_write, filename, write_func):
+        """Writes an object into a file in a concurrency-safe way."""
+        temporary_filename = concurrency_safe_write(to_write,
+                                                    filename, write_func)
+        self._move_item(temporary_filename, filename)
+
+    def __repr__(self):
+        """Printable representation of the store location."""
+        return self.location
+
+
+class FileSystemStoreBackend(StoreBackendBase, StoreBackendMixin):
+    """A StoreBackend used with local or network file systems."""
+
+    _open_item = staticmethod(open)
+    _item_exists = staticmethod(os.path.exists)
+    _move_item = staticmethod(concurrency_safe_rename)
+
+    def clear_location(self, location):
+        """Delete location on store."""
+        if (location == self.location):
+            rm_subdirs(location)
+        else:
+            shutil.rmtree(location, ignore_errors=True)
+
+    def create_location(self, location):
+        """Create object location on store"""
+        mkdirp(location)
+
+    def get_items(self):
+        """Returns the whole list of items available in the store."""
+        items = []
+
+        for dirpath, _, filenames in os.walk(self.location):
+            is_cache_hash_dir = re.match('[a-f0-9]{32}',
+                                         os.path.basename(dirpath))
+
+            if is_cache_hash_dir:
+                output_filename = os.path.join(dirpath, 'output.pkl')
+                try:
+                    last_access = os.path.getatime(output_filename)
+                except OSError:
+                    try:
+                        last_access = os.path.getatime(dirpath)
+                    except OSError:
+                        # The directory has already been deleted
+                        continue
+
+                last_access = datetime.datetime.fromtimestamp(last_access)
+                try:
+                    full_filenames = [os.path.join(dirpath, fn)
+                                      for fn in filenames]
+                    dirsize = sum(os.path.getsize(fn)
+                                  for fn in full_filenames)
+                except OSError:
+                    # Either output_filename or one of the files in
+                    # dirpath does not exist any more. We assume this
+                    # directory is being cleaned by another process already
+                    continue
+
+                items.append(CacheItemInfo(dirpath, dirsize,
+                                           last_access))
+
+        return items
+
+    def configure(self, location, verbose=1, backend_options={}):
+        """Configure the store backend.
+
+        For this backend, valid store options are 'compress' and 'mmap_mode'
+        """
+
+        # setup location directory
+        self.location = location
+        if not os.path.exists(self.location):
+            mkdirp(self.location)
+
+        # item can be stored compressed for faster I/O
+        self.compress = backend_options['compress']
+
+        # FileSystemStoreBackend can be used with mmap_mode options under
+        # certain conditions.
+        mmap_mode = None
+        if 'mmap_mode' in backend_options:
+            mmap_mode = backend_options['mmap_mode']
+            if self.compress and mmap_mode is not None:
+                warnings.warn('Compressed items cannot be memmapped in a '
+                              'filesystem store. Option will be ignored.',
+                              stacklevel=2)
+
+        self.mmap_mode = mmap_mode
+        self.verbose = verbose
diff --git a/sklearn/externals/_joblib/backports.py b/sklearn/externals/joblib/backports.py
similarity index 93%
rename from sklearn/externals/_joblib/backports.py
rename to sklearn/externals/joblib/backports.py
index 7dd3df16f165..be6c9c506e89 100644
--- a/sklearn/externals/_joblib/backports.py
+++ b/sklearn/externals/joblib/backports.py
@@ -33,7 +33,8 @@ def make_memmap(filename, dtype='uint8', mode='r+', offset=0,
 
 
 if os.name == 'nt':
-    error_access_denied = 5
+    # https://github.com/joblib/joblib/issues/540
+    access_denied_errors = (5, 13)
     try:
         from os import replace
     except ImportError:
@@ -65,7 +66,7 @@ def concurrency_safe_rename(src, dst):
                 replace(src, dst)
                 break
             except Exception as exc:
-                if getattr(exc, 'winerror', None) == error_access_denied:
+                if getattr(exc, 'winerror', None) in access_denied_errors:
                     time.sleep(sleep_time)
                     total_sleep_time += sleep_time
                     sleep_time *= 2
diff --git a/sklearn/externals/_joblib/numpy_pickle_utils.py b/sklearn/externals/joblib/compressor.py
similarity index 52%
rename from sklearn/externals/_joblib/numpy_pickle_utils.py
rename to sklearn/externals/joblib/compressor.py
index 27d759be77b2..7692fd9f2888 100644
--- a/sklearn/externals/_joblib/numpy_pickle_utils.py
+++ b/sklearn/externals/joblib/compressor.py
@@ -1,267 +1,251 @@
-"""Utilities for fast persistence of big data, with optional compression."""
+"""Classes and functions for managing compressors."""
 
-# Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>
-# Copyright (c) 2009 Gael Varoquaux
-# License: BSD Style, 3 clauses.
-
-import pickle
 import sys
 import io
 import zlib
-import gzip
-import warnings
-import contextlib
-from contextlib import closing
+from distutils.version import LooseVersion
 
-from ._compat import PY3_OR_LATER, PY27, _basestring
+from ._compat import _basestring, PY3_OR_LATER
 
 try:
     from threading import RLock
 except ImportError:
     from dummy_threading import RLock
 
-if PY3_OR_LATER:
-    Unpickler = pickle._Unpickler
-    Pickler = pickle._Pickler
-    xrange = range
-else:
-    Unpickler = pickle.Unpickler
-    Pickler = pickle.Pickler
-
 try:
-    import numpy as np
+    import bz2
 except ImportError:
-    np = None
+    bz2 = None
 
 try:
     import lzma
 except ImportError:
     lzma = None
 
-
 try:
-    # The python standard library can be built without bz2 so we make bz2
-    # usage optional.
-    # see https://github.com/scikit-learn/scikit-learn/issues/7526 for more
-    # details.
-    import bz2
+    import lz4
+    if PY3_OR_LATER:
+        from lz4.frame import LZ4FrameFile
 except ImportError:
-    bz2 = None
+    lz4 = None
+
+LZ4_NOT_INSTALLED_ERROR = ('LZ4 is not installed. Install it with pip: '
+                           'http://python-lz4.readthedocs.io/')
 
+# Registered compressors
+_COMPRESSORS = {}
 
-# Magic numbers of supported compression file formats.        '
+# Magic numbers of supported compression file formats.
 _ZFILE_PREFIX = b'ZF'  # used with pickle files created before 0.9.3.
 _ZLIB_PREFIX = b'\x78'
 _GZIP_PREFIX = b'\x1f\x8b'
 _BZ2_PREFIX = b'BZ'
 _XZ_PREFIX = b'\xfd\x37\x7a\x58\x5a'
 _LZMA_PREFIX = b'\x5d\x00'
+_LZ4_PREFIX = b'\x04\x22\x4D\x18'
 
-# Supported compressors
-_COMPRESSORS = ('zlib', 'bz2', 'lzma', 'xz', 'gzip')
-_COMPRESSOR_CLASSES = [gzip.GzipFile]
 
-if bz2 is not None:
-    _COMPRESSOR_CLASSES.append(bz2.BZ2File)
+def register_compressor(compressor_name, compressor,
+                        force=False):
+    """Register a new compressor.
 
-if lzma is not None:
-    _COMPRESSOR_CLASSES.append(lzma.LZMAFile)
+    Parameters
+    -----------
+    compressor_name: str.
+        The name of the compressor.
+    compressor: CompressorWrapper
+        An instance of a 'CompressorWrapper'.
+    """
+    global _COMPRESSORS
+    if not isinstance(compressor_name, _basestring):
+        raise ValueError("Compressor name should be a string, "
+                         "'{}' given.".format(compressor_name))
 
-# The max magic number length of supported compression file types.
-_MAX_PREFIX_LEN = max(len(prefix)
-                      for prefix in (_ZFILE_PREFIX, _GZIP_PREFIX, _BZ2_PREFIX,
-                                     _XZ_PREFIX, _LZMA_PREFIX))
+    if not isinstance(compressor, CompressorWrapper):
+        raise ValueError("Compressor should implement the CompressorWrapper "
+                         "interface, '{}' given.".format(compressor))
 
-# Buffer size used in io.BufferedReader and io.BufferedWriter
-_IO_BUFFER_SIZE = 1024 ** 2
+    if (compressor.fileobj_factory is not None and
+            (not hasattr(compressor.fileobj_factory, 'read') or
+             not hasattr(compressor.fileobj_factory, 'write') or
+             not hasattr(compressor.fileobj_factory, 'seek') or
+             not hasattr(compressor.fileobj_factory, 'tell'))):
+        raise ValueError("Compressor 'fileobj_factory' attribute should "
+                         "implement the file object interface, '{}' given."
+                         .format(compressor.fileobj_factory))
 
+    if compressor_name in _COMPRESSORS and not force:
+        raise ValueError("Compressor '{}' already registered."
+                         .format(compressor_name))
 
-def _is_raw_file(fileobj):
-    """Check if fileobj is a raw file object, e.g created with open."""
-    if PY3_OR_LATER:
-        fileobj = getattr(fileobj, 'raw', fileobj)
-        return isinstance(fileobj, io.FileIO)
-    else:
-        return isinstance(fileobj, file)  # noqa
+    _COMPRESSORS[compressor_name] = compressor
 
 
-###############################################################################
-# Cache file utilities
-def _detect_compressor(fileobj):
-    """Return the compressor matching fileobj.
+class CompressorWrapper():
+    """A wrapper around a compressor file object.
 
-    Parameters
+    Attributes
     ----------
-    fileobj: file object
-
-    Returns
-    -------
-    str in {'zlib', 'gzip', 'bz2', 'lzma', 'xz', 'compat', 'not-compressed'}
+    obj: a file-like object
+        The object must implement the buffer interface and will be used
+        internally to compress/decompress the data.
+    prefix: bytestring
+        A bytestring corresponding to the magic number that identifies the
+        file format associated to the compressor.
+    extention: str
+        The file extension used to automatically select this compressor during
+        a dump to a file.
     """
-    # Read the magic number in the first bytes of the file.
-    if hasattr(fileobj, 'peek'):
-        # Peek allows to read those bytes without moving the cursor in the
-        # file which.
-        first_bytes = fileobj.peek(_MAX_PREFIX_LEN)
-    else:
-        # Fallback to seek if the fileobject is not peekable.
-        first_bytes = fileobj.read(_MAX_PREFIX_LEN)
-        fileobj.seek(0)
-
-    if first_bytes.startswith(_ZLIB_PREFIX):
-        return "zlib"
-    elif first_bytes.startswith(_GZIP_PREFIX):
-        return "gzip"
-    elif first_bytes.startswith(_BZ2_PREFIX):
-        return "bz2"
-    elif first_bytes.startswith(_LZMA_PREFIX):
-        return "lzma"
-    elif first_bytes.startswith(_XZ_PREFIX):
-        return "xz"
-    elif first_bytes.startswith(_ZFILE_PREFIX):
-        return "compat"
-
-    return "not-compressed"
-
-
-def _buffered_read_file(fobj):
-    """Return a buffered version of a read file object."""
-    if PY27 and bz2 is not None and isinstance(fobj, bz2.BZ2File):
-        # Python 2.7 doesn't work with BZ2File through a buffer: "no
-        # attribute 'readable'" error.
-        return fobj
-    else:
-        return io.BufferedReader(fobj, buffer_size=_IO_BUFFER_SIZE)
-
-
-def _buffered_write_file(fobj):
-    """Return a buffered version of a write file object."""
-    if PY27 and bz2 is not None and isinstance(fobj, bz2.BZ2File):
-        # Python 2.7 doesn't work with BZ2File through a buffer: no attribute
-        # 'writable'.
-        # BZ2File doesn't implement the file object context manager in python 2
-        # so we wrap the fileobj using `closing`.
-        return closing(fobj)
-    else:
-        return io.BufferedWriter(fobj, buffer_size=_IO_BUFFER_SIZE)
-
-
-@contextlib.contextmanager
-def _read_fileobject(fileobj, filename, mmap_mode=None):
-    """Utility function opening the right fileobject from a filename.
-
-    The magic number is used to choose between the type of file object to open:
-    * regular file object (default)
-    * zlib file object
-    * gzip file object
-    * bz2 file object
-    * lzma file object (for xz and lzma compressor)
 
-    Parameters
-    ----------
-    fileobj: file object
-    compressor: str in {'zlib', 'gzip', 'bz2', 'lzma', 'xz', 'compat',
-                        'not-compressed'}
-    filename: str
-        filename path corresponding to the fileobj parameter.
-    mmap_mode: str
-        memory map mode that should be used to open the pickle file. This
-        parameter is useful to verify that the user is not trying to one with
-        compression. Default: None.
-
-    Returns
-    -------
-        a file like object
+    def __init__(self, obj, prefix=b'', extension=''):
+        self.fileobj_factory = obj
+        self.prefix = prefix
+        self.extension = extension
 
-    """
-    # Detect if the fileobj contains compressed data.
-    compressor = _detect_compressor(fileobj)
-
-    if compressor == 'compat':
-        # Compatibility with old pickle mode: simply return the input
-        # filename "as-is" and let the compatibility function be called by the
-        # caller.
-        warnings.warn("The file '%s' has been generated with a joblib "
-                      "version less than 0.10. "
-                      "Please regenerate this pickle file." % filename,
-                      DeprecationWarning, stacklevel=2)
-        yield filename
-    else:
-        # based on the compressor detected in the file, we open the
-        # correct decompressor file object, wrapped in a buffer.
-        if compressor == 'zlib':
-            fileobj = _buffered_read_file(BinaryZlibFile(fileobj, 'rb'))
-        elif compressor == 'gzip':
-            fileobj = _buffered_read_file(BinaryGzipFile(fileobj, 'rb'))
-        elif compressor == 'bz2' and bz2 is not None:
-            if PY3_OR_LATER:
-                fileobj = _buffered_read_file(bz2.BZ2File(fileobj, 'rb'))
-            else:
-                # In python 2, BZ2File doesn't support a fileobj opened in
-                # binary mode. In this case, we pass the filename.
-                fileobj = _buffered_read_file(bz2.BZ2File(fileobj.name, 'rb'))
-        elif (compressor == 'lzma' or compressor == 'xz'):
-            if PY3_OR_LATER and lzma is not None:
-                # We support lzma only in python 3 because in python 2 users
-                # may have installed the pyliblzma package, which also provides
-                # the lzma module, but that unfortunately doesn't fully support
-                # the buffer interface required by joblib.
-                # See https://github.com/joblib/joblib/issues/403 for details.
-                fileobj = _buffered_read_file(lzma.LZMAFile(fileobj, 'rb'))
-            else:
-                raise NotImplementedError("Lzma decompression is not "
-                                          "supported for this version of "
-                                          "python ({}.{})"
-                                          .format(sys.version_info[0],
-                                                  sys.version_info[1]))
-        # Checking if incompatible load parameters with the type of file:
-        # mmap_mode cannot be used with compressed file or in memory buffers
-        # such as io.BytesIO.
-        if mmap_mode is not None:
-            if isinstance(fileobj, io.BytesIO):
-                warnings.warn('In memory persistence is not compatible with '
-                              'mmap_mode "%(mmap_mode)s" flag passed. '
-                              'mmap_mode option will be ignored.'
-                              % locals(), stacklevel=2)
-            elif compressor != 'not-compressed':
-                warnings.warn('mmap_mode "%(mmap_mode)s" is not compatible '
-                              'with compressed file %(filename)s. '
-                              '"%(mmap_mode)s" flag will be ignored.'
-                              % locals(), stacklevel=2)
-            elif not _is_raw_file(fileobj):
-                warnings.warn('"%(fileobj)r" is not a raw file, mmap_mode '
-                              '"%(mmap_mode)s" flag will be ignored.'
-                              % locals(), stacklevel=2)
-
-        yield fileobj
-
-
-def _write_fileobject(filename, compress=("zlib", 3)):
-    """Return the right compressor file object in write mode."""
-    compressmethod = compress[0]
-    compresslevel = compress[1]
-    if compressmethod == "gzip":
-        return _buffered_write_file(BinaryGzipFile(filename, 'wb',
-                                    compresslevel=compresslevel))
-    elif compressmethod == "bz2" and bz2 is not None:
-        return _buffered_write_file(bz2.BZ2File(filename, 'wb',
-                                                compresslevel=compresslevel))
-    elif lzma is not None and compressmethod == "xz":
-        return _buffered_write_file(lzma.LZMAFile(filename, 'wb',
-                                                  check=lzma.CHECK_NONE,
-                                                  preset=compresslevel))
-    elif lzma is not None and compressmethod == "lzma":
-        return _buffered_write_file(lzma.LZMAFile(filename, 'wb',
-                                                  preset=compresslevel,
-                                                  format=lzma.FORMAT_ALONE))
-    else:
-        return _buffered_write_file(BinaryZlibFile(filename, 'wb',
-                                    compresslevel=compresslevel))
+    def compressor_file(self, fileobj, compresslevel=None):
+        """Returns an instance of a compressor file object."""
+        if compresslevel is None:
+            return self.fileobj_factory(fileobj, 'wb')
+        else:
+            return self.fileobj_factory(fileobj, 'wb',
+                                        compresslevel=compresslevel)
 
+    def decompressor_file(self, fileobj):
+        """Returns an instance of a decompressor file object."""
+        return self.fileobj_factory(fileobj, 'rb')
 
-###############################################################################
-#  Joblib zlib compression file object definition
 
+class BZ2CompressorWrapper(CompressorWrapper):
+
+    prefix = _BZ2_PREFIX
+    extension = '.bz2'
+
+    def __init__(self):
+        if bz2 is not None:
+            self.fileobj_factory = bz2.BZ2File
+        else:
+            self.fileobj_factory = None
+
+    def _check_versions(self):
+        if bz2 is None:
+            raise ValueError('bz2 module is not compiled on your python '
+                             'standard library.')
+
+    def compressor_file(self, fileobj, compresslevel=None):
+        """Returns an instance of a compressor file object."""
+        self._check_versions()
+        if compresslevel is None:
+            return self.fileobj_factory(fileobj, 'wb')
+        else:
+            return self.fileobj_factory(fileobj, 'wb',
+                                        compresslevel=compresslevel)
+
+    def decompressor_file(self, fileobj):
+        """Returns an instance of a decompressor file object."""
+        self._check_versions()
+        if PY3_OR_LATER:
+            fileobj = self.fileobj_factory(fileobj, 'rb')
+        else:
+            # In python 2, BZ2File doesn't support a fileobj opened in
+            # binary mode. In this case, we pass the filename.
+            fileobj = self.fileobj_factory(fileobj.name, 'rb')
+        return fileobj
+
+
+class LZMACompressorWrapper(CompressorWrapper):
+
+    prefix = _LZMA_PREFIX
+    extension = '.lzma'
+
+    def __init__(self):
+        if lzma is not None:
+            self.fileobj_factory = lzma.LZMAFile
+        else:
+            self.fileobj_factory = None
+
+    def compressor_file(self, fileobj, compresslevel=None):
+        """Returns an instance of a compressor file object."""
+        if compresslevel is None:
+            return self.fileobj_factory(fileobj, 'wb',
+                                        format=lzma.FORMAT_ALONE)
+        else:
+            return self.fileobj_factory(fileobj, 'wb',
+                                        format=lzma.FORMAT_ALONE,
+                                        preset=compresslevel)
+
+    def decompressor_file(self, fileobj):
+        """Returns an instance of a decompressor file object."""
+        if PY3_OR_LATER and lzma is not None:
+            # We support lzma only in python 3 because in python 2 users
+            # may have installed the pyliblzma package, which also provides
+            # the lzma module, but that unfortunately doesn't fully support
+            # the buffer interface required by joblib.
+            # See https://github.com/joblib/joblib/issues/403 for details.
+            return lzma.LZMAFile(fileobj, 'rb')
+        else:
+            raise NotImplementedError("Lzma decompression is not "
+                                      "supported for this version of "
+                                      "python ({}.{})"
+                                      .format(sys.version_info[0],
+                                              sys.version_info[1]))
+
+
+class XZCompressorWrapper(LZMACompressorWrapper):
+
+    prefix = _XZ_PREFIX
+    extension = '.xz'
+
+    def __init__(self):
+        if lzma is not None:
+            self.fileobj_factory = lzma.LZMAFile
+        else:
+            self.fileobj_factory = None
+
+    def compressor_file(self, fileobj, compresslevel=None):
+        """Returns an instance of a compressor file object."""
+        if compresslevel is None:
+            return self.fileobj_factory(fileobj, 'wb', check=lzma.CHECK_NONE)
+        else:
+            return self.fileobj_factory(fileobj, 'wb', check=lzma.CHECK_NONE,
+                                        preset=compresslevel)
+
+
+class LZ4CompressorWrapper(CompressorWrapper):
+
+    prefix = _LZ4_PREFIX
+    extension = '.lz4'
+
+    def __init__(self):
+        if PY3_OR_LATER and lz4 is not None:
+            self.fileobj_factory = LZ4FrameFile
+        else:
+            self.fileobj_factory = None
+
+    def _check_versions(self):
+        if not PY3_OR_LATER:
+            raise ValueError('lz4 compression is only available with '
+                             'python3+.')
+
+        if lz4 is None or LooseVersion(lz4.__version__) < LooseVersion('0.19'):
+            raise ValueError(LZ4_NOT_INSTALLED_ERROR)
+
+    def compressor_file(self, fileobj, compresslevel=None):
+        """Returns an instance of a compressor file object."""
+        self._check_versions()
+        if compresslevel is None:
+            return self.fileobj_factory(fileobj, 'wb')
+        else:
+            return self.fileobj_factory(fileobj, 'wb',
+                                        compression_level=compresslevel)
+
+    def decompressor_file(self, fileobj):
+        """Returns an instance of a decompressor file object."""
+        self._check_versions()
+        return self.fileobj_factory(fileobj, 'rb')
+
+
+###############################################################################
+#  base file compression/decompression object definition
 _MODE_CLOSED = 0
 _MODE_READ = 1
 _MODE_READ_EOF = 2
@@ -289,12 +273,12 @@ class BinaryZlibFile(io.BufferedIOBase):
 
     If mode is 'wb', compresslevel can be a number between 1
     and 9 specifying the level of compression: 1 produces the least
-    compression, and 9 (default) produces the most compression.
+    compression, and 9 produces the most compression. 3 is the default.
     """
 
     wbits = zlib.MAX_WBITS
 
-    def __init__(self, filename, mode="rb", compresslevel=9):
+    def __init__(self, filename, mode="rb", compresslevel=3):
         # This lock must be recursive, so that BufferedIOBase's
         # readline(), readlines() and writelines() don't deadlock.
         self._lock = RLock()
@@ -303,6 +287,7 @@ def __init__(self, filename, mode="rb", compresslevel=9):
         self._mode = _MODE_CLOSED
         self._pos = 0
         self._size = -1
+        self.compresslevel = compresslevel
 
         if not isinstance(compresslevel, int) or not (1 <= compresslevel <= 9):
             raise ValueError("'compresslevel' must be an integer "
@@ -310,27 +295,23 @@ def __init__(self, filename, mode="rb", compresslevel=9):
                              .format(compresslevel))
 
         if mode == "rb":
-            mode_code = _MODE_READ
+            self._mode = _MODE_READ
             self._decompressor = zlib.decompressobj(self.wbits)
             self._buffer = b""
             self._buffer_offset = 0
         elif mode == "wb":
-            mode_code = _MODE_WRITE
-            self._compressor = zlib.compressobj(compresslevel,
-                                                zlib.DEFLATED,
-                                                self.wbits,
-                                                zlib.DEF_MEM_LEVEL,
-                                                0)
+            self._mode = _MODE_WRITE
+            self._compressor = zlib.compressobj(self.compresslevel,
+                                                zlib.DEFLATED, self.wbits,
+                                                zlib.DEF_MEM_LEVEL, 0)
         else:
             raise ValueError("Invalid mode: %r" % (mode,))
 
         if isinstance(filename, _basestring):
             self._fp = io.open(filename, mode)
             self._closefp = True
-            self._mode = mode_code
         elif hasattr(filename, "read") or hasattr(filename, "write"):
             self._fp = filename
-            self._mode = mode_code
         else:
             raise TypeError("filename must be a str or bytes object, "
                             "or a file")
@@ -425,7 +406,6 @@ def _fill_buffer(self):
             try:
                 rawblock = (self._decompressor.unused_data or
                             self._fp.read(_BUFFER_SIZE))
-
                 if not rawblock:
                     raise EOFError
             except EOFError:
@@ -583,6 +563,13 @@ def tell(self):
             return self._pos
 
 
+class ZlibCompressorWrapper(CompressorWrapper):
+
+    def __init__(self):
+        CompressorWrapper.__init__(self, obj=BinaryZlibFile,
+                                   prefix=_ZLIB_PREFIX, extension='.z')
+
+
 class BinaryGzipFile(BinaryZlibFile):
     """A file object providing transparent gzip (de)compression.
 
@@ -594,55 +581,14 @@ class BinaryGzipFile(BinaryZlibFile):
 
     If mode is 'wb', compresslevel can be a number between 1
     and 9 specifying the level of compression: 1 produces the least
-    compression, and 9 (default) produces the most compression.
+    compression, and 9 produces the most compression. 3 is the default.
     """
 
     wbits = 31  # zlib compressor/decompressor wbits value for gzip format.
 
 
-# Utility functions/variables from numpy required for writing arrays.
-# We need at least the functions introduced in version 1.9 of numpy. Here,
-# we use the ones from numpy 1.10.2.
-BUFFER_SIZE = 2 ** 18  # size of buffer for reading npz files in bytes
-
-
-def _read_bytes(fp, size, error_template="ran out of data"):
-    """Read from file-like object until size bytes are read.
+class GzipCompressorWrapper(CompressorWrapper):
 
-    Raises ValueError if not EOF is encountered before size bytes are read.
-    Non-blocking objects only supported if they derive from io objects.
-
-    Required as e.g. ZipExtFile in python 2.6 can return less data than
-    requested.
-
-    This function was taken from numpy/lib/format.py in version 1.10.2.
-
-    Parameters
-    ----------
-    fp: file-like object
-    size: int
-    error_template: str
-
-    Returns
-    -------
-    a bytes object
-        The data read in bytes.
-
-    """
-    data = bytes()
-    while True:
-        # io files (default in python3) return None or raise on
-        # would-block, python2 file will truncate, probably nothing can be
-        # done about that.  note that regular files can't be non-blocking
-        try:
-            r = fp.read(size - len(data))
-            data += r
-            if len(r) == 0 or len(data) == size:
-                break
-        except io.BlockingIOError:
-            pass
-    if len(data) != size:
-        msg = "EOF: reading %s, expected %d bytes got %d"
-        raise ValueError(msg % (error_template, size, len(data)))
-    else:
-        return data
+    def __init__(self):
+        CompressorWrapper.__init__(self, obj=BinaryGzipFile,
+                                   prefix=_GZIP_PREFIX, extension='.gz')
diff --git a/sklearn/externals/_joblib/disk.py b/sklearn/externals/joblib/disk.py
similarity index 65%
rename from sklearn/externals/_joblib/disk.py
rename to sklearn/externals/joblib/disk.py
index 30ad100839e3..c90c3df3609c 100644
--- a/sklearn/externals/_joblib/disk.py
+++ b/sklearn/externals/joblib/disk.py
@@ -8,11 +8,18 @@
 # License: BSD Style, 3 clauses.
 
 
-import errno
 import os
-import shutil
 import sys
 import time
+import errno
+import shutil
+import warnings
+
+
+try:
+    WindowsError
+except NameError:
+    WindowsError = OSError
 
 
 def disk_used(path):
@@ -57,8 +64,11 @@ def mkdirp(d):
 
 
 # if a rmtree operation fails in rm_subdirs, wait for this much time (in secs),
-# then retry once. if it still fails, raise the exception
+# then retry up to RM_SUBDIRS_N_RETRY times. If it still fails, raise the
+# exception. this mecanism ensures that the sub-process gc have the time to
+# collect and close the memmaps before we fail.
 RM_SUBDIRS_RETRY_TIME = 0.1
+RM_SUBDIRS_N_RETRY = 5
 
 
 def rm_subdirs(path, onerror=None):
@@ -80,7 +90,7 @@ def rm_subdirs(path, onerror=None):
     names = []
     try:
         names = os.listdir(path)
-    except os.error as err:
+    except os.error:
         if onerror is not None:
             onerror(os.listdir, path, sys.exc_info())
         else:
@@ -88,19 +98,27 @@ def rm_subdirs(path, onerror=None):
 
     for name in names:
         fullname = os.path.join(path, name)
-        if os.path.isdir(fullname):
-            if onerror is not None:
-                shutil.rmtree(fullname, False, onerror)
-            else:
-                # allow the rmtree to fail once, wait and re-try.
-                # if the error is raised again, fail
-                err_count = 0
-                while True:
-                    try:
-                        shutil.rmtree(fullname, False, None)
-                        break
-                    except os.error:
-                        if err_count > 0:
-                            raise
-                        err_count += 1
-                        time.sleep(RM_SUBDIRS_RETRY_TIME)
+        delete_folder(fullname, onerror=onerror)
+
+
+def delete_folder(folder_path, onerror=None):
+    """Utility function to cleanup a temporary folder if it still exists."""
+    if os.path.isdir(folder_path):
+        if onerror is not None:
+            shutil.rmtree(folder_path, False, onerror)
+        else:
+            # allow the rmtree to fail once, wait and re-try.
+            # if the error is raised again, fail
+            err_count = 0
+            while True:
+                try:
+                    shutil.rmtree(folder_path, False, None)
+                    break
+                except (OSError, WindowsError):
+                    err_count += 1
+                    if err_count > RM_SUBDIRS_N_RETRY:
+                        warnings.warn(
+                            "Unable to delete folder {} after {} tentatives."
+                            .format(folder_path, RM_SUBDIRS_N_RETRY))
+                        raise
+                    time.sleep(RM_SUBDIRS_RETRY_TIME)
diff --git a/sklearn/externals/joblib/executor.py b/sklearn/externals/joblib/executor.py
new file mode 100644
index 000000000000..c63472d60894
--- /dev/null
+++ b/sklearn/externals/joblib/executor.py
@@ -0,0 +1,67 @@
+"""Utility function to construct a loky.ReusableExecutor with custom pickler.
+
+This module provides efficient ways of working with data stored in
+shared memory with numpy.memmap arrays without inducing any memory
+copy between the parent and child processes.
+"""
+# Author: Thomas Moreau <thomas.moreau.2010@gmail.com>
+# Copyright: 2017, Thomas Moreau
+# License: BSD 3 clause
+
+import random
+from .disk import delete_folder
+from ._memmapping_reducer import get_memmapping_reducers
+from .externals.loky.reusable_executor import get_reusable_executor
+
+
+_backend_args = None
+
+
+def get_memmapping_executor(n_jobs, timeout=300, initializer=None, initargs=(),
+                            **backend_args):
+    """Factory for ReusableExecutor with automatic memmapping for large numpy
+    arrays.
+    """
+    global _backend_args
+    reuse = _backend_args is None or _backend_args == backend_args
+    _backend_args = backend_args
+
+    id_executor = random.randint(0, int(1e10))
+    job_reducers, result_reducers, temp_folder = get_memmapping_reducers(
+        id_executor, **backend_args)
+    _executor = get_reusable_executor(n_jobs, job_reducers=job_reducers,
+                                      result_reducers=result_reducers,
+                                      reuse=reuse, timeout=timeout,
+                                      initializer=initializer,
+                                      initargs=initargs)
+    # If executor doesn't have a _temp_folder, it means it is a new executor
+    # and the reducers have been used. Else, the previous reducers are used
+    # and we should not change this attibute.
+    if not hasattr(_executor, "_temp_folder"):
+        _executor._temp_folder = temp_folder
+    else:
+        delete_folder(temp_folder)
+    return _executor
+
+
+class _TestingMemmappingExecutor():
+    """Wrapper around ReusableExecutor to ease memmapping testing with Pool
+    and Executor. This is only for testing purposes.
+    """
+    def __init__(self, n_jobs, **backend_args):
+        self._executor = get_memmapping_executor(n_jobs, **backend_args)
+        self._temp_folder = self._executor._temp_folder
+
+    def apply_async(self, func, args):
+        """Schedule a func to be run"""
+        future = self._executor.submit(func, *args)
+        future.get = future.result
+        return future
+
+    def terminate(self):
+        self._executor.shutdown()
+        delete_folder(self._temp_folder)
+
+    def map(self, f, *args):
+        res = self._executor.map(f, *args)
+        return list(res)
diff --git a/sklearn/externals/joblib/externals/__init__.py b/sklearn/externals/joblib/externals/__init__.py
new file mode 100644
index 000000000000..e69de29bb2d1
diff --git a/sklearn/externals/joblib/externals/cloudpickle/__init__.py b/sklearn/externals/joblib/externals/cloudpickle/__init__.py
new file mode 100644
index 000000000000..c8c8fa208a16
--- /dev/null
+++ b/sklearn/externals/joblib/externals/cloudpickle/__init__.py
@@ -0,0 +1,5 @@
+from __future__ import absolute_import
+
+from .cloudpickle import *
+
+__version__ = '0.5.2'
diff --git a/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py b/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py
new file mode 100644
index 000000000000..e5aab0591f57
--- /dev/null
+++ b/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py
@@ -0,0 +1,1098 @@
+"""
+This class is defined to override standard pickle functionality
+
+The goals of it follow:
+-Serialize lambdas and nested functions to compiled byte code
+-Deal with main module correctly
+-Deal with other non-serializable objects
+
+It does not include an unpickler, as standard python unpickling suffices.
+
+This module was extracted from the `cloud` package, developed by `PiCloud, Inc.
+<https://web.archive.org/web/20140626004012/http://www.picloud.com/>`_.
+
+Copyright (c) 2012, Regents of the University of California.
+Copyright (c) 2009 `PiCloud, Inc. <https://web.archive.org/web/20140626004012/http://www.picloud.com/>`_.
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions
+are met:
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in the
+      documentation and/or other materials provided with the distribution.
+    * Neither the name of the University of California, Berkeley nor the
+      names of its contributors may be used to endorse or promote
+      products derived from this software without specific prior written
+      permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+"""
+from __future__ import print_function
+
+import dis
+from functools import partial
+import imp
+import io
+import itertools
+import logging
+import opcode
+import operator
+import pickle
+import struct
+import sys
+import traceback
+import types
+import weakref
+
+
+# cloudpickle is meant for inter process communication: we expect all
+# communicating processes to run the same Python version hence we favor
+# communication speed over compatibility:
+DEFAULT_PROTOCOL = pickle.HIGHEST_PROTOCOL
+
+
+if sys.version < '3':
+    from pickle import Pickler
+    try:
+        from cStringIO import StringIO
+    except ImportError:
+        from StringIO import StringIO
+    PY3 = False
+else:
+    types.ClassType = type
+    from pickle import _Pickler as Pickler
+    from io import BytesIO as StringIO
+    PY3 = True
+
+
+def _make_cell_set_template_code():
+    """Get the Python compiler to emit LOAD_FAST(arg); STORE_DEREF
+
+    Notes
+    -----
+    In Python 3, we could use an easier function:
+
+    .. code-block:: python
+
+       def f():
+           cell = None
+
+           def _stub(value):
+               nonlocal cell
+               cell = value
+
+           return _stub
+
+        _cell_set_template_code = f()
+
+    This function is _only_ a LOAD_FAST(arg); STORE_DEREF, but that is
+    invalid syntax on Python 2. If we use this function we also don't need
+    to do the weird freevars/cellvars swap below
+    """
+    def inner(value):
+        lambda: cell  # make ``cell`` a closure so that we get a STORE_DEREF
+        cell = value
+
+    co = inner.__code__
+
+    # NOTE: we are marking the cell variable as a free variable intentionally
+    # so that we simulate an inner function instead of the outer function. This
+    # is what gives us the ``nonlocal`` behavior in a Python 2 compatible way.
+    if not PY3:
+        return types.CodeType(
+            co.co_argcount,
+            co.co_nlocals,
+            co.co_stacksize,
+            co.co_flags,
+            co.co_code,
+            co.co_consts,
+            co.co_names,
+            co.co_varnames,
+            co.co_filename,
+            co.co_name,
+            co.co_firstlineno,
+            co.co_lnotab,
+            co.co_cellvars,  # this is the trickery
+            (),
+        )
+    else:
+        return types.CodeType(
+            co.co_argcount,
+            co.co_kwonlyargcount,
+            co.co_nlocals,
+            co.co_stacksize,
+            co.co_flags,
+            co.co_code,
+            co.co_consts,
+            co.co_names,
+            co.co_varnames,
+            co.co_filename,
+            co.co_name,
+            co.co_firstlineno,
+            co.co_lnotab,
+            co.co_cellvars,  # this is the trickery
+            (),
+        )
+
+
+_cell_set_template_code = _make_cell_set_template_code()
+
+
+def cell_set(cell, value):
+    """Set the value of a closure cell.
+    """
+    return types.FunctionType(
+        _cell_set_template_code,
+        {},
+        '_cell_set_inner',
+        (),
+        (cell,),
+    )(value)
+
+
+#relevant opcodes
+STORE_GLOBAL = opcode.opmap['STORE_GLOBAL']
+DELETE_GLOBAL = opcode.opmap['DELETE_GLOBAL']
+LOAD_GLOBAL = opcode.opmap['LOAD_GLOBAL']
+GLOBAL_OPS = (STORE_GLOBAL, DELETE_GLOBAL, LOAD_GLOBAL)
+HAVE_ARGUMENT = dis.HAVE_ARGUMENT
+EXTENDED_ARG = dis.EXTENDED_ARG
+
+
+def islambda(func):
+    return getattr(func,'__name__') == '<lambda>'
+
+
+_BUILTIN_TYPE_NAMES = {}
+for k, v in types.__dict__.items():
+    if type(v) is type:
+        _BUILTIN_TYPE_NAMES[v] = k
+
+
+def _builtin_type(name):
+    return getattr(types, name)
+
+
+def _make__new__factory(type_):
+    def _factory():
+        return type_.__new__
+    return _factory
+
+
+# NOTE: These need to be module globals so that they're pickleable as globals.
+_get_dict_new = _make__new__factory(dict)
+_get_frozenset_new = _make__new__factory(frozenset)
+_get_list_new = _make__new__factory(list)
+_get_set_new = _make__new__factory(set)
+_get_tuple_new = _make__new__factory(tuple)
+_get_object_new = _make__new__factory(object)
+
+# Pre-defined set of builtin_function_or_method instances that can be
+# serialized.
+_BUILTIN_TYPE_CONSTRUCTORS = {
+    dict.__new__: _get_dict_new,
+    frozenset.__new__: _get_frozenset_new,
+    set.__new__: _get_set_new,
+    list.__new__: _get_list_new,
+    tuple.__new__: _get_tuple_new,
+    object.__new__: _get_object_new,
+}
+
+
+if sys.version_info < (3, 4):
+    def _walk_global_ops(code):
+        """
+        Yield (opcode, argument number) tuples for all
+        global-referencing instructions in *code*.
+        """
+        code = getattr(code, 'co_code', b'')
+        if not PY3:
+            code = map(ord, code)
+
+        n = len(code)
+        i = 0
+        extended_arg = 0
+        while i < n:
+            op = code[i]
+            i += 1
+            if op >= HAVE_ARGUMENT:
+                oparg = code[i] + code[i + 1] * 256 + extended_arg
+                extended_arg = 0
+                i += 2
+                if op == EXTENDED_ARG:
+                    extended_arg = oparg * 65536
+                if op in GLOBAL_OPS:
+                    yield op, oparg
+
+else:
+    def _walk_global_ops(code):
+        """
+        Yield (opcode, argument number) tuples for all
+        global-referencing instructions in *code*.
+        """
+        for instr in dis.get_instructions(code):
+            op = instr.opcode
+            if op in GLOBAL_OPS:
+                yield op, instr.arg
+
+
+class CloudPickler(Pickler):
+
+    dispatch = Pickler.dispatch.copy()
+
+    def __init__(self, file, protocol=None):
+        if protocol is None:
+            protocol = DEFAULT_PROTOCOL
+        Pickler.__init__(self, file, protocol=protocol)
+        # set of modules to unpickle
+        self.modules = set()
+        # map ids to dictionary. used to ensure that functions can share global env
+        self.globals_ref = {}
+
+    def dump(self, obj):
+        self.inject_addons()
+        try:
+            return Pickler.dump(self, obj)
+        except RuntimeError as e:
+            if 'recursion' in e.args[0]:
+                msg = """Could not pickle object as excessively deep recursion required."""
+                raise pickle.PicklingError(msg)
+
+    def save_memoryview(self, obj):
+        self.save(obj.tobytes())
+    dispatch[memoryview] = save_memoryview
+
+    if not PY3:
+        def save_buffer(self, obj):
+            self.save(str(obj))
+        dispatch[buffer] = save_buffer
+
+    def save_unsupported(self, obj):
+        raise pickle.PicklingError("Cannot pickle objects of type %s" % type(obj))
+    dispatch[types.GeneratorType] = save_unsupported
+
+    # itertools objects do not pickle!
+    for v in itertools.__dict__.values():
+        if type(v) is type:
+            dispatch[v] = save_unsupported
+
+    def save_module(self, obj):
+        """
+        Save a module as an import
+        """
+        mod_name = obj.__name__
+        # If module is successfully found then it is not a dynamically created module
+        if hasattr(obj, '__file__'):
+            is_dynamic = False
+        else:
+            try:
+                _find_module(mod_name)
+                is_dynamic = False
+            except ImportError:
+                is_dynamic = True
+
+        self.modules.add(obj)
+        if is_dynamic:
+            self.save_reduce(dynamic_subimport, (obj.__name__, vars(obj)), obj=obj)
+        else:
+            self.save_reduce(subimport, (obj.__name__,), obj=obj)
+    dispatch[types.ModuleType] = save_module
+
+    def save_codeobject(self, obj):
+        """
+        Save a code object
+        """
+        if PY3:
+            args = (
+                obj.co_argcount, obj.co_kwonlyargcount, obj.co_nlocals, obj.co_stacksize,
+                obj.co_flags, obj.co_code, obj.co_consts, obj.co_names, obj.co_varnames,
+                obj.co_filename, obj.co_name, obj.co_firstlineno, obj.co_lnotab, obj.co_freevars,
+                obj.co_cellvars
+            )
+        else:
+            args = (
+                obj.co_argcount, obj.co_nlocals, obj.co_stacksize, obj.co_flags, obj.co_code,
+                obj.co_consts, obj.co_names, obj.co_varnames, obj.co_filename, obj.co_name,
+                obj.co_firstlineno, obj.co_lnotab, obj.co_freevars, obj.co_cellvars
+            )
+        self.save_reduce(types.CodeType, args, obj=obj)
+    dispatch[types.CodeType] = save_codeobject
+
+    def save_function(self, obj, name=None):
+        """ Registered with the dispatch to handle all function types.
+
+        Determines what kind of function obj is (e.g. lambda, defined at
+        interactive prompt, etc) and handles the pickling appropriately.
+        """
+        if obj in _BUILTIN_TYPE_CONSTRUCTORS:
+            # We keep a special-cased cache of built-in type constructors at
+            # global scope, because these functions are structured very
+            # differently in different python versions and implementations (for
+            # example, they're instances of types.BuiltinFunctionType in
+            # CPython, but they're ordinary types.FunctionType instances in
+            # PyPy).
+            #
+            # If the function we've received is in that cache, we just
+            # serialize it as a lookup into the cache.
+            return self.save_reduce(_BUILTIN_TYPE_CONSTRUCTORS[obj], (), obj=obj)
+
+        write = self.write
+
+        if name is None:
+            name = obj.__name__
+        try:
+            # whichmodule() could fail, see
+            # https://bitbucket.org/gutworth/six/issues/63/importing-six-breaks-pickling
+            modname = pickle.whichmodule(obj, name)
+        except Exception:
+            modname = None
+        # print('which gives %s %s %s' % (modname, obj, name))
+        try:
+            themodule = sys.modules[modname]
+        except KeyError:
+            # eval'd items such as namedtuple give invalid items for their function __module__
+            modname = '__main__'
+
+        if modname == '__main__':
+            themodule = None
+
+        try:
+            lookedup_by_name = getattr(themodule, name, None)
+        except Exception:
+            lookedup_by_name = None
+
+        if themodule:
+            self.modules.add(themodule)
+            if lookedup_by_name is obj:
+                return self.save_global(obj, name)
+
+        # a builtin_function_or_method which comes in as an attribute of some
+        # object (e.g., itertools.chain.from_iterable) will end
+        # up with modname "__main__" and so end up here. But these functions
+        # have no __code__ attribute in CPython, so the handling for
+        # user-defined functions below will fail.
+        # So we pickle them here using save_reduce; have to do it differently
+        # for different python versions.
+        if not hasattr(obj, '__code__'):
+            if PY3:
+                rv = obj.__reduce_ex__(self.proto)
+            else:
+                if hasattr(obj, '__self__'):
+                    rv = (getattr, (obj.__self__, name))
+                else:
+                    raise pickle.PicklingError("Can't pickle %r" % obj)
+            return self.save_reduce(obj=obj, *rv)
+
+        # if func is lambda, def'ed at prompt, is in main, or is nested, then
+        # we'll pickle the actual function object rather than simply saving a
+        # reference (as is done in default pickler), via save_function_tuple.
+        if (islambda(obj)
+                or getattr(obj.__code__, 'co_filename', None) == '<stdin>'
+                or themodule is None):
+            self.save_function_tuple(obj)
+            return
+        else:
+            # func is nested
+            if lookedup_by_name is None or lookedup_by_name is not obj:
+                self.save_function_tuple(obj)
+                return
+
+        if obj.__dict__:
+            # essentially save_reduce, but workaround needed to avoid recursion
+            self.save(_restore_attr)
+            write(pickle.MARK + pickle.GLOBAL + modname + '\n' + name + '\n')
+            self.memoize(obj)
+            self.save(obj.__dict__)
+            write(pickle.TUPLE + pickle.REDUCE)
+        else:
+            write(pickle.GLOBAL + modname + '\n' + name + '\n')
+            self.memoize(obj)
+    dispatch[types.FunctionType] = save_function
+
+    def _save_subimports(self, code, top_level_dependencies):
+        """
+        Ensure de-pickler imports any package child-modules that
+        are needed by the function
+        """
+        # check if any known dependency is an imported package
+        for x in top_level_dependencies:
+            if isinstance(x, types.ModuleType) and hasattr(x, '__package__') and x.__package__:
+                # check if the package has any currently loaded sub-imports
+                prefix = x.__name__ + '.'
+                for name, module in sys.modules.items():
+                    # Older versions of pytest will add a "None" module to sys.modules.
+                    if name is not None and name.startswith(prefix):
+                        # check whether the function can address the sub-module
+                        tokens = set(name[len(prefix):].split('.'))
+                        if not tokens - set(code.co_names):
+                            # ensure unpickler executes this import
+                            self.save(module)
+                            # then discards the reference to it
+                            self.write(pickle.POP)
+
+    def save_dynamic_class(self, obj):
+        """
+        Save a class that can't be stored as module global.
+
+        This method is used to serialize classes that are defined inside
+        functions, or that otherwise can't be serialized as attribute lookups
+        from global modules.
+        """
+        clsdict = dict(obj.__dict__)  # copy dict proxy to a dict
+        clsdict.pop('__weakref__', None)
+
+        # On PyPy, __doc__ is a readonly attribute, so we need to include it in
+        # the initial skeleton class.  This is safe because we know that the
+        # doc can't participate in a cycle with the original class.
+        type_kwargs = {'__doc__': clsdict.pop('__doc__', None)}
+
+        # If type overrides __dict__ as a property, include it in the type kwargs.
+        # In Python 2, we can't set this attribute after construction.
+        __dict__ = clsdict.pop('__dict__', None)
+        if isinstance(__dict__, property):
+            type_kwargs['__dict__'] = __dict__
+
+        save = self.save
+        write = self.write
+
+        # We write pickle instructions explicitly here to handle the
+        # possibility that the type object participates in a cycle with its own
+        # __dict__. We first write an empty "skeleton" version of the class and
+        # memoize it before writing the class' __dict__ itself. We then write
+        # instructions to "rehydrate" the skeleton class by restoring the
+        # attributes from the __dict__.
+        #
+        # A type can appear in a cycle with its __dict__ if an instance of the
+        # type appears in the type's __dict__ (which happens for the stdlib
+        # Enum class), or if the type defines methods that close over the name
+        # of the type, (which is common for Python 2-style super() calls).
+
+        # Push the rehydration function.
+        save(_rehydrate_skeleton_class)
+
+        # Mark the start of the args tuple for the rehydration function.
+        write(pickle.MARK)
+
+        # Create and memoize an skeleton class with obj's name and bases.
+        tp = type(obj)
+        self.save_reduce(tp, (obj.__name__, obj.__bases__, type_kwargs), obj=obj)
+
+        # Now save the rest of obj's __dict__. Any references to obj
+        # encountered while saving will point to the skeleton class.
+        save(clsdict)
+
+        # Write a tuple of (skeleton_class, clsdict).
+        write(pickle.TUPLE)
+
+        # Call _rehydrate_skeleton_class(skeleton_class, clsdict)
+        write(pickle.REDUCE)
+
+    def save_function_tuple(self, func):
+        """  Pickles an actual func object.
+
+        A func comprises: code, globals, defaults, closure, and dict.  We
+        extract and save these, injecting reducing functions at certain points
+        to recreate the func object.  Keep in mind that some of these pieces
+        can contain a ref to the func itself.  Thus, a naive save on these
+        pieces could trigger an infinite loop of save's.  To get around that,
+        we first create a skeleton func object using just the code (this is
+        safe, since this won't contain a ref to the func), and memoize it as
+        soon as it's created.  The other stuff can then be filled in later.
+        """
+        if is_tornado_coroutine(func):
+            self.save_reduce(_rebuild_tornado_coroutine, (func.__wrapped__,),
+                             obj=func)
+            return
+
+        save = self.save
+        write = self.write
+
+        code, f_globals, defaults, closure_values, dct, base_globals = self.extract_func_data(func)
+
+        save(_fill_function)  # skeleton function updater
+        write(pickle.MARK)    # beginning of tuple that _fill_function expects
+
+        self._save_subimports(
+            code,
+            itertools.chain(f_globals.values(), closure_values or ()),
+        )
+
+        # create a skeleton function object and memoize it
+        save(_make_skel_func)
+        save((
+            code,
+            len(closure_values) if closure_values is not None else -1,
+            base_globals,
+        ))
+        write(pickle.REDUCE)
+        self.memoize(func)
+
+        # save the rest of the func data needed by _fill_function
+        state = {
+            'globals': f_globals,
+            'defaults': defaults,
+            'dict': dct,
+            'module': func.__module__,
+            'closure_values': closure_values,
+        }
+        if hasattr(func, '__qualname__'):
+            state['qualname'] = func.__qualname__
+        save(state)
+        write(pickle.TUPLE)
+        write(pickle.REDUCE)  # applies _fill_function on the tuple
+
+    _extract_code_globals_cache = (
+        weakref.WeakKeyDictionary()
+        if not hasattr(sys, "pypy_version_info")
+        else {})
+
+    @classmethod
+    def extract_code_globals(cls, co):
+        """
+        Find all globals names read or written to by codeblock co
+        """
+        out_names = cls._extract_code_globals_cache.get(co)
+        if out_names is None:
+            try:
+                names = co.co_names
+            except AttributeError:
+                # PyPy "builtin-code" object
+                out_names = set()
+            else:
+                out_names = set(names[oparg]
+                                for op, oparg in _walk_global_ops(co))
+
+                # see if nested function have any global refs
+                if co.co_consts:
+                    for const in co.co_consts:
+                        if type(const) is types.CodeType:
+                            out_names |= cls.extract_code_globals(const)
+
+            cls._extract_code_globals_cache[co] = out_names
+
+        return out_names
+
+    def extract_func_data(self, func):
+        """
+        Turn the function into a tuple of data necessary to recreate it:
+            code, globals, defaults, closure_values, dict
+        """
+        code = func.__code__
+
+        # extract all global ref's
+        func_global_refs = self.extract_code_globals(code)
+
+        # process all variables referenced by global environment
+        f_globals = {}
+        for var in func_global_refs:
+            if var in func.__globals__:
+                f_globals[var] = func.__globals__[var]
+
+        # defaults requires no processing
+        defaults = func.__defaults__
+
+        # process closure
+        closure = (
+            list(map(_get_cell_contents, func.__closure__))
+            if func.__closure__ is not None
+            else None
+        )
+
+        # save the dict
+        dct = func.__dict__
+
+        base_globals = self.globals_ref.get(id(func.__globals__), {})
+        self.globals_ref[id(func.__globals__)] = base_globals
+
+        return (code, f_globals, defaults, closure, dct, base_globals)
+
+    def save_builtin_function(self, obj):
+        if obj.__module__ == "__builtin__":
+            return self.save_global(obj)
+        return self.save_function(obj)
+    dispatch[types.BuiltinFunctionType] = save_builtin_function
+
+    def save_global(self, obj, name=None, pack=struct.pack):
+        """
+        Save a "global".
+
+        The name of this method is somewhat misleading: all types get
+        dispatched here.
+        """
+        if obj.__module__ == "__main__":
+            return self.save_dynamic_class(obj)
+
+        try:
+            return Pickler.save_global(self, obj, name=name)
+        except Exception:
+            if obj.__module__ == "__builtin__" or obj.__module__ == "builtins":
+                if obj in _BUILTIN_TYPE_NAMES:
+                    return self.save_reduce(
+                        _builtin_type, (_BUILTIN_TYPE_NAMES[obj],), obj=obj)
+
+            typ = type(obj)
+            if typ is not obj and isinstance(obj, (type, types.ClassType)):
+                return self.save_dynamic_class(obj)
+
+            raise
+
+    dispatch[type] = save_global
+    dispatch[types.ClassType] = save_global
+
+    def save_instancemethod(self, obj):
+        # Memoization rarely is ever useful due to python bounding
+        if obj.__self__ is None:
+            self.save_reduce(getattr, (obj.im_class, obj.__name__))
+        else:
+            if PY3:
+                self.save_reduce(types.MethodType, (obj.__func__, obj.__self__), obj=obj)
+            else:
+                self.save_reduce(types.MethodType, (obj.__func__, obj.__self__, obj.__self__.__class__),
+                         obj=obj)
+    dispatch[types.MethodType] = save_instancemethod
+
+    def save_inst(self, obj):
+        """Inner logic to save instance. Based off pickle.save_inst"""
+        cls = obj.__class__
+
+        # Try the dispatch table (pickle module doesn't do it)
+        f = self.dispatch.get(cls)
+        if f:
+            f(self, obj)  # Call unbound method with explicit self
+            return
+
+        memo = self.memo
+        write = self.write
+        save = self.save
+
+        if hasattr(obj, '__getinitargs__'):
+            args = obj.__getinitargs__()
+            len(args)  # XXX Assert it's a sequence
+            pickle._keep_alive(args, memo)
+        else:
+            args = ()
+
+        write(pickle.MARK)
+
+        if self.bin:
+            save(cls)
+            for arg in args:
+                save(arg)
+            write(pickle.OBJ)
+        else:
+            for arg in args:
+                save(arg)
+            write(pickle.INST + cls.__module__ + '\n' + cls.__name__ + '\n')
+
+        self.memoize(obj)
+
+        try:
+            getstate = obj.__getstate__
+        except AttributeError:
+            stuff = obj.__dict__
+        else:
+            stuff = getstate()
+            pickle._keep_alive(stuff, memo)
+        save(stuff)
+        write(pickle.BUILD)
+
+    if not PY3:
+        dispatch[types.InstanceType] = save_inst
+
+    def save_property(self, obj):
+        # properties not correctly saved in python
+        self.save_reduce(property, (obj.fget, obj.fset, obj.fdel, obj.__doc__), obj=obj)
+    dispatch[property] = save_property
+
+    def save_classmethod(self, obj):
+        orig_func = obj.__func__
+        self.save_reduce(type(obj), (orig_func,), obj=obj)
+    dispatch[classmethod] = save_classmethod
+    dispatch[staticmethod] = save_classmethod
+
+    def save_itemgetter(self, obj):
+        """itemgetter serializer (needed for namedtuple support)"""
+        class Dummy:
+            def __getitem__(self, item):
+                return item
+        items = obj(Dummy())
+        if not isinstance(items, tuple):
+            items = (items, )
+        return self.save_reduce(operator.itemgetter, items)
+
+    if type(operator.itemgetter) is type:
+        dispatch[operator.itemgetter] = save_itemgetter
+
+    def save_attrgetter(self, obj):
+        """attrgetter serializer"""
+        class Dummy(object):
+            def __init__(self, attrs, index=None):
+                self.attrs = attrs
+                self.index = index
+            def __getattribute__(self, item):
+                attrs = object.__getattribute__(self, "attrs")
+                index = object.__getattribute__(self, "index")
+                if index is None:
+                    index = len(attrs)
+                    attrs.append(item)
+                else:
+                    attrs[index] = ".".join([attrs[index], item])
+                return type(self)(attrs, index)
+        attrs = []
+        obj(Dummy(attrs))
+        return self.save_reduce(operator.attrgetter, tuple(attrs))
+
+    if type(operator.attrgetter) is type:
+        dispatch[operator.attrgetter] = save_attrgetter
+
+    def save_file(self, obj):
+        """Save a file"""
+        try:
+            import StringIO as pystringIO #we can't use cStringIO as it lacks the name attribute
+        except ImportError:
+            import io as pystringIO
+
+        if not hasattr(obj, 'name') or  not hasattr(obj, 'mode'):
+            raise pickle.PicklingError("Cannot pickle files that do not map to an actual file")
+        if obj is sys.stdout:
+            return self.save_reduce(getattr, (sys,'stdout'), obj=obj)
+        if obj is sys.stderr:
+            return self.save_reduce(getattr, (sys,'stderr'), obj=obj)
+        if obj is sys.stdin:
+            raise pickle.PicklingError("Cannot pickle standard input")
+        if obj.closed:
+            raise pickle.PicklingError("Cannot pickle closed files")
+        if hasattr(obj, 'isatty') and obj.isatty():
+            raise pickle.PicklingError("Cannot pickle files that map to tty objects")
+        if 'r' not in obj.mode and '+' not in obj.mode:
+            raise pickle.PicklingError("Cannot pickle files that are not opened for reading: %s" % obj.mode)
+
+        name = obj.name
+
+        retval = pystringIO.StringIO()
+
+        try:
+            # Read the whole file
+            curloc = obj.tell()
+            obj.seek(0)
+            contents = obj.read()
+            obj.seek(curloc)
+        except IOError:
+            raise pickle.PicklingError("Cannot pickle file %s as it cannot be read" % name)
+        retval.write(contents)
+        retval.seek(curloc)
+
+        retval.name = name
+        self.save(retval)
+        self.memoize(obj)
+
+    def save_ellipsis(self, obj):
+        self.save_reduce(_gen_ellipsis, ())
+
+    def save_not_implemented(self, obj):
+        self.save_reduce(_gen_not_implemented, ())
+
+    if PY3:
+        dispatch[io.TextIOWrapper] = save_file
+    else:
+        dispatch[file] = save_file
+
+    dispatch[type(Ellipsis)] = save_ellipsis
+    dispatch[type(NotImplemented)] = save_not_implemented
+
+    def save_weakset(self, obj):
+        self.save_reduce(weakref.WeakSet, (list(obj),))
+
+    dispatch[weakref.WeakSet] = save_weakset
+
+    def save_logger(self, obj):
+        self.save_reduce(logging.getLogger, (obj.name,), obj=obj)
+
+    dispatch[logging.Logger] = save_logger
+
+    """Special functions for Add-on libraries"""
+    def inject_addons(self):
+        """Plug in system. Register additional pickling functions if modules already loaded"""
+        pass
+
+
+# Tornado support
+
+def is_tornado_coroutine(func):
+    """
+    Return whether *func* is a Tornado coroutine function.
+    Running coroutines are not supported.
+    """
+    if 'tornado.gen' not in sys.modules:
+        return False
+    gen = sys.modules['tornado.gen']
+    if not hasattr(gen, "is_coroutine_function"):
+        # Tornado version is too old
+        return False
+    return gen.is_coroutine_function(func)
+
+
+def _rebuild_tornado_coroutine(func):
+    from tornado import gen
+    return gen.coroutine(func)
+
+
+# Shorthands for legacy support
+
+def dump(obj, file, protocol=None):
+    """Serialize obj as bytes streamed into file
+
+    protocol defaults to cloudpickle.DEFAULT_PROTOCOL which is an alias to
+    pickle.HIGHEST_PROTOCOL. This setting favors maximum communication speed
+    between processes running the same Python version.
+
+    Set protocol=pickle.DEFAULT_PROTOCOL instead if you need to ensure
+    compatibility with older versions of Python.
+    """
+    CloudPickler(file, protocol=protocol).dump(obj)
+
+
+def dumps(obj, protocol=None):
+    """Serialize obj as a string of bytes allocated in memory
+
+    protocol defaults to cloudpickle.DEFAULT_PROTOCOL which is an alias to
+    pickle.HIGHEST_PROTOCOL. This setting favors maximum communication speed
+    between processes running the same Python version.
+
+    Set protocol=pickle.DEFAULT_PROTOCOL instead if you need to ensure
+    compatibility with older versions of Python.
+    """
+    file = StringIO()
+    try:
+        cp = CloudPickler(file, protocol=protocol)
+        cp.dump(obj)
+        return file.getvalue()
+    finally:
+        file.close()
+
+
+# including pickles unloading functions in this namespace
+load = pickle.load
+loads = pickle.loads
+
+
+# hack for __import__ not working as desired
+def subimport(name):
+    __import__(name)
+    return sys.modules[name]
+
+
+def dynamic_subimport(name, vars):
+    mod = imp.new_module(name)
+    mod.__dict__.update(vars)
+    sys.modules[name] = mod
+    return mod
+
+
+# restores function attributes
+def _restore_attr(obj, attr):
+    for key, val in attr.items():
+        setattr(obj, key, val)
+    return obj
+
+
+def _get_module_builtins():
+    return pickle.__builtins__
+
+
+def print_exec(stream):
+    ei = sys.exc_info()
+    traceback.print_exception(ei[0], ei[1], ei[2], None, stream)
+
+
+def _modules_to_main(modList):
+    """Force every module in modList to be placed into main"""
+    if not modList:
+        return
+
+    main = sys.modules['__main__']
+    for modname in modList:
+        if type(modname) is str:
+            try:
+                mod = __import__(modname)
+            except Exception as e:
+                sys.stderr.write('warning: could not import %s\n.  '
+                                 'Your function may unexpectedly error due to this import failing;'
+                                 'A version mismatch is likely.  Specific error was:\n' % modname)
+                print_exec(sys.stderr)
+            else:
+                setattr(main, mod.__name__, mod)
+
+
+#object generators:
+def _genpartial(func, args, kwds):
+    if not args:
+        args = ()
+    if not kwds:
+        kwds = {}
+    return partial(func, *args, **kwds)
+
+def _gen_ellipsis():
+    return Ellipsis
+
+def _gen_not_implemented():
+    return NotImplemented
+
+
+def _get_cell_contents(cell):
+    try:
+        return cell.cell_contents
+    except ValueError:
+        # sentinel used by ``_fill_function`` which will leave the cell empty
+        return _empty_cell_value
+
+
+def instance(cls):
+    """Create a new instance of a class.
+
+    Parameters
+    ----------
+    cls : type
+        The class to create an instance of.
+
+    Returns
+    -------
+    instance : cls
+        A new instance of ``cls``.
+    """
+    return cls()
+
+
+@instance
+class _empty_cell_value(object):
+    """sentinel for empty closures
+    """
+    @classmethod
+    def __reduce__(cls):
+        return cls.__name__
+
+
+def _fill_function(*args):
+    """Fills in the rest of function data into the skeleton function object
+
+    The skeleton itself is create by _make_skel_func().
+    """
+    if len(args) == 2:
+        func = args[0]
+        state = args[1]
+    elif len(args) == 5:
+        # Backwards compat for cloudpickle v0.4.0, after which the `module`
+        # argument was introduced
+        func = args[0]
+        keys = ['globals', 'defaults', 'dict', 'closure_values']
+        state = dict(zip(keys, args[1:]))
+    elif len(args) == 6:
+        # Backwards compat for cloudpickle v0.4.1, after which the function
+        # state was passed as a dict to the _fill_function it-self.
+        func = args[0]
+        keys = ['globals', 'defaults', 'dict', 'module', 'closure_values']
+        state = dict(zip(keys, args[1:]))
+    else:
+        raise ValueError('Unexpected _fill_value arguments: %r' % (args,))
+
+    func.__globals__.update(state['globals'])
+    func.__defaults__ = state['defaults']
+    func.__dict__ = state['dict']
+    if 'module' in state:
+        func.__module__ = state['module']
+    if 'qualname' in state:
+        func.__qualname__ = state['qualname']
+
+    cells = func.__closure__
+    if cells is not None:
+        for cell, value in zip(cells, state['closure_values']):
+            if value is not _empty_cell_value:
+                cell_set(cell, value)
+
+    return func
+
+
+def _make_empty_cell():
+    if False:
+        # trick the compiler into creating an empty cell in our lambda
+        cell = None
+        raise AssertionError('this route should not be executed')
+
+    return (lambda: cell).__closure__[0]
+
+
+def _make_skel_func(code, cell_count, base_globals=None):
+    """ Creates a skeleton function object that contains just the provided
+        code and the correct number of cells in func_closure.  All other
+        func attributes (e.g. func_globals) are empty.
+    """
+    if base_globals is None:
+        base_globals = {}
+    base_globals['__builtins__'] = __builtins__
+
+    closure = (
+        tuple(_make_empty_cell() for _ in range(cell_count))
+        if cell_count >= 0 else
+        None
+    )
+    return types.FunctionType(code, base_globals, None, None, closure)
+
+
+def _rehydrate_skeleton_class(skeleton_class, class_dict):
+    """Put attributes from `class_dict` back on `skeleton_class`.
+
+    See CloudPickler.save_dynamic_class for more info.
+    """
+    for attrname, attr in class_dict.items():
+        setattr(skeleton_class, attrname, attr)
+    return skeleton_class
+
+
+def _find_module(mod_name):
+    """
+    Iterate over each part instead of calling imp.find_module directly.
+    This function is able to find submodules (e.g. sickit.tree)
+    """
+    path = None
+    for part in mod_name.split('.'):
+        if path is not None:
+            path = [path]
+        file, path, description = imp.find_module(part, path)
+        if file is not None:
+            file.close()
+    return path, description
+
+"""Constructors for 3rd party libraries
+Note: These can never be renamed due to client compatibility issues"""
+
+def _getobject(modname, attribute):
+    mod = __import__(modname, fromlist=[attribute])
+    return mod.__dict__[attribute]
+
+
+""" Use copy_reg to extend global pickle definitions """
+
+if sys.version_info < (3, 4):
+    method_descriptor = type(str.upper)
+
+    def _reduce_method_descriptor(obj):
+        return (getattr, (obj.__objclass__, obj.__name__))
+
+    try:
+        import copy_reg as copyreg
+    except ImportError:
+        import copyreg
+    copyreg.pickle(method_descriptor, _reduce_method_descriptor)
diff --git a/sklearn/externals/joblib/externals/loky/__init__.py b/sklearn/externals/joblib/externals/loky/__init__.py
new file mode 100644
index 000000000000..6480423cffbb
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/__init__.py
@@ -0,0 +1,12 @@
+r"""The :mod:`loky` module manages a pool of worker that can be re-used across time.
+It provides a robust and dynamic implementation os the
+:class:`ProcessPoolExecutor` and a function :func:`get_reusable_executor` which
+hide the pool management under the hood.
+"""
+from .reusable_executor import get_reusable_executor  # noqa: F401
+from .process_executor import ProcessPoolExecutor  # noqa: F401
+from .process_executor import BrokenProcessPool  # noqa: F401
+
+from .backend.context import cpu_count  # noqa: F401
+
+__version__ = '2.2.0'
diff --git a/sklearn/externals/joblib/externals/loky/_base.py b/sklearn/externals/joblib/externals/loky/_base.py
new file mode 100644
index 000000000000..ff4ac92cf402
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/_base.py
@@ -0,0 +1,610 @@
+###############################################################################
+# Backport concurrent.futures for python2.7/3.3
+#
+# author: Thomas Moreau and Olivier Grisel
+#
+# adapted from concurrent/futures/_base.py (17/02/2017)
+#  * Do not use yield from
+#  * Use old super syntax
+#
+# Copyright 2009 Brian Quinlan. All Rights Reserved.
+# Licensed to PSF under a Contributor Agreement.
+
+import sys
+import collections
+import logging
+import threading
+import time
+
+FIRST_COMPLETED = 'FIRST_COMPLETED'
+FIRST_EXCEPTION = 'FIRST_EXCEPTION'
+ALL_COMPLETED = 'ALL_COMPLETED'
+_AS_COMPLETED = '_AS_COMPLETED'
+
+# Possible future states (for internal use by the futures package).
+PENDING = 'PENDING'
+RUNNING = 'RUNNING'
+# The future was cancelled by the user...
+CANCELLED = 'CANCELLED'
+# ...and _Waiter.add_cancelled() was called by a worker.
+CANCELLED_AND_NOTIFIED = 'CANCELLED_AND_NOTIFIED'
+FINISHED = 'FINISHED'
+
+_FUTURE_STATES = [
+    PENDING,
+    RUNNING,
+    CANCELLED,
+    CANCELLED_AND_NOTIFIED,
+    FINISHED
+]
+
+_STATE_TO_DESCRIPTION_MAP = {
+    PENDING: "pending",
+    RUNNING: "running",
+    CANCELLED: "cancelled",
+    CANCELLED_AND_NOTIFIED: "cancelled",
+    FINISHED: "finished"
+}
+
+# Logger for internal use by the futures package.
+LOGGER = logging.getLogger("concurrent.futures")
+
+
+if sys.version_info[:2] < (3, 3):
+
+    class Error(Exception):
+        """Base class for all future-related exceptions."""
+        pass
+
+    class CancelledError(Error):
+        """The Future was cancelled."""
+        pass
+
+    class TimeoutError(Error):
+        """The operation exceeded the given deadline."""
+        pass
+else:
+    from concurrent.futures import CancelledError, TimeoutError
+
+
+class _Waiter(object):
+    """Provides the event that wait() and as_completed() block on."""
+    def __init__(self):
+        self.event = threading.Event()
+        self.finished_futures = []
+
+    def add_result(self, future):
+        self.finished_futures.append(future)
+
+    def add_exception(self, future):
+        self.finished_futures.append(future)
+
+    def add_cancelled(self, future):
+        self.finished_futures.append(future)
+
+
+class _AsCompletedWaiter(_Waiter):
+    """Used by as_completed()."""
+
+    def __init__(self):
+        super(_AsCompletedWaiter, self).__init__()
+        self.lock = threading.Lock()
+
+    def add_result(self, future):
+        with self.lock:
+            super(_AsCompletedWaiter, self).add_result(future)
+            self.event.set()
+
+    def add_exception(self, future):
+        with self.lock:
+            super(_AsCompletedWaiter, self).add_exception(future)
+            self.event.set()
+
+    def add_cancelled(self, future):
+        with self.lock:
+            super(_AsCompletedWaiter, self).add_cancelled(future)
+            self.event.set()
+
+
+class _FirstCompletedWaiter(_Waiter):
+    """Used by wait(return_when=FIRST_COMPLETED)."""
+
+    def add_result(self, future):
+        super(_FirstCompletedWaiter, self).add_result(future)
+        self.event.set()
+
+    def add_exception(self, future):
+        super(_FirstCompletedWaiter, self).add_exception(future)
+        self.event.set()
+
+    def add_cancelled(self, future):
+        super(_FirstCompletedWaiter, self).add_cancelled(future)
+        self.event.set()
+
+
+class _AllCompletedWaiter(_Waiter):
+    """Used by wait(return_when=FIRST_EXCEPTION and ALL_COMPLETED)."""
+
+    def __init__(self, num_pending_calls, stop_on_exception):
+        self.num_pending_calls = num_pending_calls
+        self.stop_on_exception = stop_on_exception
+        self.lock = threading.Lock()
+        super(_AllCompletedWaiter, self).__init__()
+
+    def _decrement_pending_calls(self):
+        with self.lock:
+            self.num_pending_calls -= 1
+            if not self.num_pending_calls:
+                self.event.set()
+
+    def add_result(self, future):
+        super(_AllCompletedWaiter, self).add_result(future)
+        self._decrement_pending_calls()
+
+    def add_exception(self, future):
+        super(_AllCompletedWaiter, self).add_exception(future)
+        if self.stop_on_exception:
+            self.event.set()
+        else:
+            self._decrement_pending_calls()
+
+    def add_cancelled(self, future):
+        super(_AllCompletedWaiter, self).add_cancelled(future)
+        self._decrement_pending_calls()
+
+
+class _AcquireFutures(object):
+    """A context manager that does an ordered acquire of Future conditions."""
+
+    def __init__(self, futures):
+        self.futures = sorted(futures, key=id)
+
+    def __enter__(self):
+        for future in self.futures:
+            future._condition.acquire()
+
+    def __exit__(self, *args):
+        for future in self.futures:
+            future._condition.release()
+
+
+def _create_and_install_waiters(fs, return_when):
+    if return_when == _AS_COMPLETED:
+        waiter = _AsCompletedWaiter()
+    elif return_when == FIRST_COMPLETED:
+        waiter = _FirstCompletedWaiter()
+    else:
+        pending_count = sum(
+                f._state not in [CANCELLED_AND_NOTIFIED, FINISHED] for f in fs)
+
+        if return_when == FIRST_EXCEPTION:
+            waiter = _AllCompletedWaiter(pending_count, stop_on_exception=True)
+        elif return_when == ALL_COMPLETED:
+            waiter = _AllCompletedWaiter(pending_count,
+                                         stop_on_exception=False)
+        else:
+            raise ValueError("Invalid return condition: %r" % return_when)
+
+    for f in fs:
+        f._waiters.append(waiter)
+
+    return waiter
+
+
+def as_completed(fs, timeout=None):
+    """An iterator over the given futures that yields each as it completes.
+
+    Args:
+        fs: The sequence of Futures (possibly created by different Executors)
+            to iterate over.
+        timeout: The maximum number of seconds to wait. If None, then there
+            is no limit on the wait time.
+
+    Returns:
+        An iterator that yields the given Futures as they complete (finished or
+        cancelled). If any given Futures are duplicated, they will be returned
+        once.
+
+    Raises:
+        TimeoutError: If the entire result iterator could not be generated
+            before the given timeout.
+    """
+    if timeout is not None:
+        end_time = timeout + time.time()
+
+    fs = set(fs)
+    with _AcquireFutures(fs):
+        finished = set(
+                f for f in fs
+                if f._state in [CANCELLED_AND_NOTIFIED, FINISHED])
+        pending = fs - finished
+        waiter = _create_and_install_waiters(fs, _AS_COMPLETED)
+
+    try:
+        for future in finished:
+            yield future
+
+        while pending:
+            if timeout is None:
+                wait_timeout = None
+            else:
+                wait_timeout = end_time - time.time()
+                if wait_timeout < 0:
+                    raise TimeoutError('%d (of %d) futures unfinished' % (
+                        len(pending), len(fs)))
+
+            waiter.event.wait(wait_timeout)
+
+            with waiter.lock:
+                finished = waiter.finished_futures
+                waiter.finished_futures = []
+                waiter.event.clear()
+
+            for future in finished:
+                yield future
+                pending.remove(future)
+
+    finally:
+        for f in fs:
+            with f._condition:
+                f._waiters.remove(waiter)
+
+
+DoneAndNotDoneFutures = collections.namedtuple(
+        'DoneAndNotDoneFutures', 'done not_done')
+
+
+def wait(fs, timeout=None, return_when=ALL_COMPLETED):
+    """Wait for the futures in the given sequence to complete.
+
+    Args:
+        fs: The sequence of Futures (possibly created by different Executors)
+            to wait upon.
+        timeout: The maximum number of seconds to wait. If None, then there
+            is no limit on the wait time.
+        return_when: Indicates when this function should return. The options
+            are:
+
+            FIRST_COMPLETED - Return when any future finishes or is
+                              cancelled.
+            FIRST_EXCEPTION - Return when any future finishes by raising an
+                              exception. If no future raises an exception
+                              then it is equivalent to ALL_COMPLETED.
+            ALL_COMPLETED -   Return when all futures finish or are cancelled.
+
+    Returns:
+        A named 2-tuple of sets. The first set, named 'done', contains the
+        futures that completed (is finished or cancelled) before the wait
+        completed. The second set, named 'not_done', contains uncompleted
+        futures.
+    """
+    with _AcquireFutures(fs):
+        done = set(f for f in fs
+                   if f._state in [CANCELLED_AND_NOTIFIED, FINISHED])
+        not_done = set(fs) - done
+
+        if (return_when == FIRST_COMPLETED) and done:
+            return DoneAndNotDoneFutures(done, not_done)
+        elif (return_when == FIRST_EXCEPTION) and done:
+            if any(f for f in done
+                   if not f.cancelled() and f.exception() is not None):
+                return DoneAndNotDoneFutures(done, not_done)
+
+        if len(done) == len(fs):
+            return DoneAndNotDoneFutures(done, not_done)
+
+        waiter = _create_and_install_waiters(fs, return_when)
+
+    waiter.event.wait(timeout)
+    for f in fs:
+        with f._condition:
+            f._waiters.remove(waiter)
+
+    done.update(waiter.finished_futures)
+    return DoneAndNotDoneFutures(done, set(fs) - done)
+
+
+class Future(object):
+    """Represents the result of an asynchronous computation."""
+
+    def __init__(self):
+        """Initializes the future. Should not be called by clients."""
+        self._condition = threading.Condition()
+        self._state = PENDING
+        self._result = None
+        self._exception = None
+        self._waiters = []
+        self._done_callbacks = []
+
+    def _invoke_callbacks(self):
+        for callback in self._done_callbacks:
+            try:
+                callback(self)
+            except BaseException:
+                LOGGER.exception('exception calling callback for %r', self)
+
+    def __repr__(self):
+        with self._condition:
+            if self._state == FINISHED:
+                if self._exception:
+                    return '<%s at %#x state=%s raised %s>' % (
+                        self.__class__.__name__,
+                        id(self),
+                        _STATE_TO_DESCRIPTION_MAP[self._state],
+                        self._exception.__class__.__name__)
+                else:
+                    return '<%s at %#x state=%s returned %s>' % (
+                        self.__class__.__name__,
+                        id(self),
+                        _STATE_TO_DESCRIPTION_MAP[self._state],
+                        self._result.__class__.__name__)
+            return '<%s at %#x state=%s>' % (
+                    self.__class__.__name__,
+                    id(self),
+                   _STATE_TO_DESCRIPTION_MAP[self._state])
+
+    def cancel(self):
+        """Cancel the future if possible.
+
+        Returns True if the future was cancelled, False otherwise. A future
+        cannot be cancelled if it is running or has already completed.
+        """
+        with self._condition:
+            if self._state in [RUNNING, FINISHED]:
+                return False
+
+            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
+                return True
+
+            self._state = CANCELLED
+            self._condition.notify_all()
+
+        self._invoke_callbacks()
+        return True
+
+    def cancelled(self):
+        """Return True if the future was cancelled."""
+        with self._condition:
+            return self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]
+
+    def running(self):
+        """Return True if the future is currently executing."""
+        with self._condition:
+            return self._state == RUNNING
+
+    def done(self):
+        """Return True of the future was cancelled or finished executing."""
+        with self._condition:
+            return self._state in [CANCELLED, CANCELLED_AND_NOTIFIED, FINISHED]
+
+    def __get_result(self):
+        if self._exception:
+            raise self._exception
+        else:
+            return self._result
+
+    def add_done_callback(self, fn):
+        """Attaches a callable that will be called when the future finishes.
+
+        Args:
+            fn: A callable that will be called with this future as its only
+                argument when the future completes or is cancelled. The
+                callable will always be called by a thread in the same process
+                in which it was added. If the future has already completed or
+                been cancelled then the callable will be called immediately.
+                These callables are called in the order that they were added.
+        """
+        with self._condition:
+            if self._state not in [CANCELLED, CANCELLED_AND_NOTIFIED,
+                                   FINISHED]:
+                self._done_callbacks.append(fn)
+                return
+        fn(self)
+
+    def result(self, timeout=None):
+        """Return the result of the call that the future represents.
+
+        Args:
+            timeout: The number of seconds to wait for the result if the future
+                isn't done. If None, then there is no limit on the wait time.
+
+        Returns:
+            The result of the call that the future represents.
+
+        Raises:
+            CancelledError: If the future was cancelled.
+            TimeoutError: If the future didn't finish executing before the
+                given timeout.
+            Exception: If the call raised then that exception will be raised.
+        """
+        with self._condition:
+            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
+                raise CancelledError()
+            elif self._state == FINISHED:
+                return self.__get_result()
+
+            self._condition.wait(timeout)
+
+            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
+                raise CancelledError()
+            elif self._state == FINISHED:
+                return self.__get_result()
+            else:
+                raise TimeoutError()
+
+    def exception(self, timeout=None):
+        """Return the exception raised by the call that the future represents.
+
+        Args:
+            timeout: The number of seconds to wait for the exception if the
+                future isn't done. If None, then there is no limit on the wait
+                time.
+
+        Returns:
+            The exception raised by the call that the future represents or None
+            if the call completed without raising.
+
+        Raises:
+            CancelledError: If the future was cancelled.
+            TimeoutError: If the future didn't finish executing before the
+                given timeout.
+        """
+
+        with self._condition:
+            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
+                raise CancelledError()
+            elif self._state == FINISHED:
+                return self._exception
+
+            self._condition.wait(timeout)
+
+            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
+                raise CancelledError()
+            elif self._state == FINISHED:
+                return self._exception
+            else:
+                raise TimeoutError()
+
+    # The following methods should only be used by Executors and in tests.
+    def set_running_or_notify_cancel(self):
+        """Mark the future as running or process any cancel notifications.
+
+        Should only be used by Executor implementations and unit tests.
+
+        If the future has been cancelled (cancel() was called and returned
+        True) then any threads waiting on the future completing (though calls
+        to as_completed() or wait()) are notified and False is returned.
+
+        If the future was not cancelled then it is put in the running state
+        (future calls to running() will return True) and True is returned.
+
+        This method should be called by Executor implementations before
+        executing the work associated with this future. If this method returns
+        False then the work should not be executed.
+
+        Returns:
+            False if the Future was cancelled, True otherwise.
+
+        Raises:
+            RuntimeError: if this method was already called or if set_result()
+                or set_exception() was called.
+        """
+        with self._condition:
+            if self._state == CANCELLED:
+                self._state = CANCELLED_AND_NOTIFIED
+                for waiter in self._waiters:
+                    waiter.add_cancelled(self)
+                # self._condition.notify_all() is not necessary because
+                # self.cancel() triggers a notification.
+                return False
+            elif self._state == PENDING:
+                self._state = RUNNING
+                return True
+            else:
+                LOGGER.critical('Future %s in unexpected state: %s',
+                                id(self),
+                                self._state)
+                raise RuntimeError('Future in unexpected state')
+
+    def set_result(self, result):
+        """Sets the return value of work associated with the future.
+
+        Should only be used by Executor implementations and unit tests.
+        """
+        with self._condition:
+            self._result = result
+            self._state = FINISHED
+            for waiter in self._waiters:
+                waiter.add_result(self)
+            self._condition.notify_all()
+        self._invoke_callbacks()
+
+    def set_exception(self, exception):
+        """Sets the result of the future as being the given exception.
+
+        Should only be used by Executor implementations and unit tests.
+        """
+        with self._condition:
+            self._exception = exception
+            self._state = FINISHED
+            for waiter in self._waiters:
+                waiter.add_exception(self)
+            self._condition.notify_all()
+        self._invoke_callbacks()
+
+
+class Executor(object):
+    """This is an abstract base class for concrete asynchronous executors."""
+
+    def submit(self, fn, *args, **kwargs):
+        """Submits a callable to be executed with the given arguments.
+
+        Schedules the callable to be executed as fn(*args, **kwargs) and
+        returns a Future instance representing the execution of the callable.
+
+        Returns:
+            A Future representing the given call.
+        """
+        raise NotImplementedError()
+
+    def map(self, fn, *iterables, **kwargs):
+        """Returns an iterator equivalent to map(fn, iter).
+
+        Args:
+            fn: A callable that will take as many arguments as there are
+                passed iterables.
+            timeout: The maximum number of seconds to wait. If None, then there
+                is no limit on the wait time.
+            chunksize: The size of the chunks the iterable will be broken into
+                before being passed to a child process. This argument is only
+                used by ProcessPoolExecutor; it is ignored by
+                ThreadPoolExecutor.
+
+        Returns:
+            An iterator equivalent to: map(func, *iterables) but the calls may
+            be evaluated out-of-order.
+
+        Raises:
+            TimeoutError: If the entire result iterator could not be generated
+                before the given timeout.
+            Exception: If fn(*args) raises for any values.
+        """
+        timeout = kwargs.get('timeout')
+        if timeout is not None:
+            end_time = timeout + time.time()
+
+        fs = [self.submit(fn, *args) for args in zip(*iterables)]
+
+        # Yield must be hidden in closure so that the futures are submitted
+        # before the first iterator value is required.
+        def result_iterator():
+            try:
+                for future in fs:
+                    if timeout is None:
+                        yield future.result()
+                    else:
+                        yield future.result(end_time - time.time())
+            finally:
+                for future in fs:
+                    future.cancel()
+        return result_iterator()
+
+    def shutdown(self, wait=True):
+        """Clean-up the resources associated with the Executor.
+
+        It is safe to call this method several times. Otherwise, no other
+        methods can be called after this one.
+
+        Args:
+            wait: If True then shutdown will not return until all running
+                futures have finished executing and the resources used by the
+                executor have been reclaimed.
+        """
+        pass
+
+    def __enter__(self):
+        return self
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        self.shutdown(wait=True)
+        return False
diff --git a/sklearn/externals/joblib/externals/loky/backend/__init__.py b/sklearn/externals/joblib/externals/loky/backend/__init__.py
new file mode 100644
index 000000000000..b5868d057a40
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/__init__.py
@@ -0,0 +1,18 @@
+import os
+import sys
+
+from .context import get_context
+
+LOKY_PICKLER = os.environ.get("LOKY_PICKLER")
+
+if sys.version_info > (3, 4):
+
+    def _make_name():
+        name = '/loky-%i-%s' % (os.getpid(), next(synchronize.SemLock._rand))
+        return name
+
+    # monkey patch the name creation for multiprocessing
+    from multiprocessing import synchronize
+    synchronize.SemLock._make_name = staticmethod(_make_name)
+
+__all__ = ["get_context"]
diff --git a/sklearn/externals/joblib/externals/loky/backend/_posix_reduction.py b/sklearn/externals/joblib/externals/loky/backend/_posix_reduction.py
new file mode 100644
index 000000000000..e0e394d3cdf2
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/_posix_reduction.py
@@ -0,0 +1,76 @@
+###############################################################################
+# Extra reducers for Unix based system and connections objects
+#
+# author: Thomas Moreau and Olivier Grisel
+#
+# adapted from multiprocessing/reduction.py (17/02/2017)
+#  * Add adapted reduction for LokyProcesses and socket/Connection
+#
+import os
+import sys
+import socket
+import _socket
+
+from .reduction import register
+from .context import get_spawning_popen
+
+if sys.version_info >= (3, 3):
+    from multiprocessing.connection import Connection
+else:
+    from _multiprocessing import Connection
+
+
+HAVE_SEND_HANDLE = (hasattr(socket, 'CMSG_LEN') and
+                    hasattr(socket, 'SCM_RIGHTS') and
+                    hasattr(socket.socket, 'sendmsg'))
+
+
+def _mk_inheritable(fd):
+    if sys.version_info[:2] > (3, 3):
+        os.set_inheritable(fd, True)
+    return fd
+
+
+def DupFd(fd):
+    '''Return a wrapper for an fd.'''
+    popen_obj = get_spawning_popen()
+    if popen_obj is not None:
+        return popen_obj.DupFd(popen_obj.duplicate_for_child(fd))
+    elif HAVE_SEND_HANDLE and sys.version_info[:2] > (3, 3):
+        from multiprocessing import resource_sharer
+        return resource_sharer.DupFd(fd)
+    else:
+        raise TypeError(
+            'Cannot pickle connection object. This object can only be '
+            'passed when spawning a new process'
+        )
+
+
+if sys.version_info[:2] != (3, 3):
+    def _reduce_socket(s):
+        df = DupFd(s.fileno())
+        return _rebuild_socket, (df, s.family, s.type, s.proto)
+
+    def _rebuild_socket(df, family, type, proto):
+        fd = df.detach()
+        return socket.fromfd(fd, family, type, proto)
+else:
+    from multiprocessing.reduction import reduce_socket as _reduce_socket
+
+
+register(socket.socket, _reduce_socket)
+register(_socket.socket, _reduce_socket)
+
+
+if sys.version_info[:2] != (3, 3):
+    def reduce_connection(conn):
+        df = DupFd(conn.fileno())
+        return rebuild_connection, (df, conn.readable, conn.writable)
+
+    def rebuild_connection(df, readable, writable):
+        fd = df.detach()
+        return Connection(fd, readable, writable)
+else:
+    from multiprocessing.reduction import reduce_connection
+
+register(Connection, reduce_connection)
diff --git a/sklearn/externals/joblib/externals/loky/backend/_posix_wait.py b/sklearn/externals/joblib/externals/loky/backend/_posix_wait.py
new file mode 100644
index 000000000000..d935882dca5d
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/_posix_wait.py
@@ -0,0 +1,105 @@
+###############################################################################
+# Compat for wait function on UNIX based system
+#
+# author: Thomas Moreau and Olivier Grisel
+#
+# adapted from multiprocessing/connection.py (17/02/2017)
+#  * Backport wait function to python2.7
+#
+
+import platform
+import select
+import socket
+import errno
+SYSTEM = platform.system()
+
+try:
+    import ctypes
+except ImportError:  # pragma: no cover
+    ctypes = None  # noqa
+
+if SYSTEM == 'Darwin' and ctypes is not None:
+    from ctypes.util import find_library
+    libSystem = ctypes.CDLL(find_library('libSystem.dylib'))
+    CoreServices = ctypes.CDLL(find_library('CoreServices'),
+                               use_errno=True)
+    mach_absolute_time = libSystem.mach_absolute_time
+    mach_absolute_time.restype = ctypes.c_uint64
+    absolute_to_nanoseconds = CoreServices.AbsoluteToNanoseconds
+    absolute_to_nanoseconds.restype = ctypes.c_uint64
+    absolute_to_nanoseconds.argtypes = [ctypes.c_uint64]
+
+    def monotonic():
+        return absolute_to_nanoseconds(mach_absolute_time()) * 1e-9
+
+elif SYSTEM == 'Linux' and ctypes is not None:
+    # from stackoverflow:
+    # questions/1205722/how-do-i-get-monotonic-time-durations-in-python
+    import ctypes
+    import os
+
+    CLOCK_MONOTONIC = 1  # see <linux/time.h>
+
+    class timespec(ctypes.Structure):
+        _fields_ = [
+            ('tv_sec', ctypes.c_long),
+            ('tv_nsec', ctypes.c_long),
+        ]
+
+    librt = ctypes.CDLL('librt.so.1', use_errno=True)
+    clock_gettime = librt.clock_gettime
+    clock_gettime.argtypes = [
+        ctypes.c_int, ctypes.POINTER(timespec),
+    ]
+
+    def monotonic():  # noqa
+        t = timespec()
+        if clock_gettime(CLOCK_MONOTONIC, ctypes.pointer(t)) != 0:
+            errno_ = ctypes.get_errno()
+            raise OSError(errno_, os.strerror(errno_))
+        return t.tv_sec + t.tv_nsec * 1e-9
+else:  # pragma: no cover
+    from time import time as monotonic
+
+
+if hasattr(select, 'poll'):
+    def _poll(fds, timeout):
+        if timeout is not None:
+            timeout = int(timeout * 1000)  # timeout is in milliseconds
+        fd_map = {}
+        pollster = select.poll()
+        for fd in fds:
+            pollster.register(fd, select.POLLIN)
+            if hasattr(fd, 'fileno'):
+                fd_map[fd.fileno()] = fd
+            else:
+                fd_map[fd] = fd
+        ls = []
+        for fd, event in pollster.poll(timeout):
+            if event & select.POLLNVAL:  # pragma: no cover
+                raise ValueError('invalid file descriptor %i' % fd)
+            ls.append(fd_map[fd])
+        return ls
+else:
+    def _poll(fds, timeout):
+        return select.select(fds, [], [], timeout)[0]
+
+
+def wait(object_list, timeout=None):
+    '''
+    Wait till an object in object_list is ready/readable.
+    Returns list of those objects which are ready/readable.
+    '''
+    if timeout is not None:
+        if timeout <= 0:
+            return _poll(object_list, 0)
+        else:
+            deadline = monotonic() + timeout
+    while True:
+        try:
+            return _poll(object_list, timeout)
+        except (OSError, IOError, socket.error) as e:  # pragma: no cover
+            if e.errno != errno.EINTR:
+                raise
+        if timeout is not None:
+            timeout = deadline - monotonic()
diff --git a/sklearn/externals/joblib/externals/loky/backend/_win_reduction.py b/sklearn/externals/joblib/externals/loky/backend/_win_reduction.py
new file mode 100644
index 000000000000..142e6e7c80dd
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/_win_reduction.py
@@ -0,0 +1,99 @@
+###############################################################################
+# Extra reducers for Windows system and connections objects
+#
+# author: Thomas Moreau and Olivier Grisel
+#
+# adapted from multiprocessing/reduction.py (17/02/2017)
+#  * Add adapted reduction for LokyProcesses and socket/PipeConnection
+#
+import os
+import sys
+import socket
+from .reduction import register
+
+
+if sys.platform == 'win32':
+    if sys.version_info[:2] < (3, 3):
+        from _multiprocessing import PipeConnection
+    else:
+        import _winapi
+        from multiprocessing.connection import PipeConnection
+
+
+if sys.version_info[:2] >= (3, 4) and sys.platform == 'win32':
+    class DupHandle(object):
+        def __init__(self, handle, access, pid=None):
+            # duplicate handle for process with given pid
+            if pid is None:
+                pid = os.getpid()
+            proc = _winapi.OpenProcess(_winapi.PROCESS_DUP_HANDLE, False, pid)
+            try:
+                self._handle = _winapi.DuplicateHandle(
+                    _winapi.GetCurrentProcess(),
+                    handle, proc, access, False, 0)
+            finally:
+                _winapi.CloseHandle(proc)
+            self._access = access
+            self._pid = pid
+
+        def detach(self):
+            # retrieve handle from process which currently owns it
+            if self._pid == os.getpid():
+                return self._handle
+            proc = _winapi.OpenProcess(_winapi.PROCESS_DUP_HANDLE, False,
+                                       self._pid)
+            try:
+                return _winapi.DuplicateHandle(
+                    proc, self._handle, _winapi.GetCurrentProcess(),
+                    self._access, False, _winapi.DUPLICATE_CLOSE_SOURCE)
+            finally:
+                _winapi.CloseHandle(proc)
+
+    def reduce_pipe_connection(conn):
+        access = ((_winapi.FILE_GENERIC_READ if conn.readable else 0) |
+                  (_winapi.FILE_GENERIC_WRITE if conn.writable else 0))
+        dh = DupHandle(conn.fileno(), access)
+        return rebuild_pipe_connection, (dh, conn.readable, conn.writable)
+
+    def rebuild_pipe_connection(dh, readable, writable):
+        from multiprocessing.connection import PipeConnection
+        handle = dh.detach()
+        return PipeConnection(handle, readable, writable)
+    register(PipeConnection, reduce_pipe_connection)
+
+elif sys.platform == 'win32':
+    # Older Python versions
+    from multiprocessing.reduction import reduce_pipe_connection
+    register(PipeConnection, reduce_pipe_connection)
+
+
+if sys.version_info[:2] < (3, 3) and sys.platform == 'win32':
+    from _multiprocessing import win32
+    from multiprocessing.reduction import reduce_handle, rebuild_handle
+    close = win32.CloseHandle
+
+    def fromfd(handle, family, type_, proto=0):
+        s = socket.socket(family, type_, proto, fileno=handle)
+        if s.__class__ is not socket.socket:
+            s = socket.socket(_sock=s)
+        return s
+
+    def reduce_socket(s):
+        if not hasattr(socket, "fromfd"):
+            raise TypeError("sockets cannot be pickled on this system.")
+        reduced_handle = reduce_handle(s.fileno())
+        return _rebuild_socket, (reduced_handle, s.family, s.type, s.proto)
+
+    def _rebuild_socket(reduced_handle, family, type_, proto):
+        handle = rebuild_handle(reduced_handle)
+        s = fromfd(handle, family, type_, proto)
+        close(handle)
+        return s
+
+    register(socket.socket, reduce_socket)
+elif sys.version_info[:2] < (3, 4):
+    from multiprocessing.reduction import reduce_socket
+    register(socket.socket, reduce_socket)
+else:
+    from multiprocessing.reduction import _reduce_socket
+    register(socket.socket, _reduce_socket)
diff --git a/sklearn/externals/joblib/externals/loky/backend/_win_wait.py b/sklearn/externals/joblib/externals/loky/backend/_win_wait.py
new file mode 100644
index 000000000000..73271316d05a
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/_win_wait.py
@@ -0,0 +1,58 @@
+###############################################################################
+# Compat for wait function on Windows system
+#
+# author: Thomas Moreau and Olivier Grisel
+#
+# adapted from multiprocessing/connection.py (17/02/2017)
+#  * Backport wait function to python2.7
+#
+
+import ctypes
+import sys
+from time import sleep
+
+
+if sys.platform == 'win32' and sys.version_info[:2] < (3, 3):
+    from _subprocess import WaitForSingleObject, WAIT_OBJECT_0
+
+    try:
+        from time import monotonic
+    except ImportError:
+        # Backward old for crappy old Python that did not have cross-platform
+        # monotonic clock by default.
+
+        # TODO: do we want to add support for cygwin at some point? See:
+        # https://github.com/atdt/monotonic/blob/master/monotonic.py
+        GetTickCount64 = ctypes.windll.kernel32.GetTickCount64
+        GetTickCount64.restype = ctypes.c_ulonglong
+
+        def monotonic():
+            """Monotonic clock, cannot go backward."""
+            return GetTickCount64() / 1000.0
+
+    def wait(handles, timeout=None):
+        """Backward compat for python2.7
+
+        This function wait for either:
+        * one connection is ready for read,
+        * one process handle has exited or got killed,
+        * timeout is reached. Note that this function has a precision of 2
+          msec.
+        """
+        if timeout is not None:
+            deadline = monotonic() + timeout
+
+        while True:
+            # We cannot use select as in windows it only support sockets
+            ready = []
+            for h in handles:
+                if type(h) in [int, long]:
+                    if WaitForSingleObject(h, 0) == WAIT_OBJECT_0:
+                        ready += [h]
+                elif h.poll(0):
+                    ready.append(h)
+            if len(ready) > 0:
+                return ready
+            sleep(.001)
+            if timeout is not None and deadline - monotonic() <= 0:
+                return []
diff --git a/sklearn/externals/joblib/externals/loky/backend/compat.py b/sklearn/externals/joblib/externals/loky/backend/compat.py
new file mode 100644
index 000000000000..6366b23d9f38
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/compat.py
@@ -0,0 +1,26 @@
+# flake8: noqa
+###############################################################################
+# Compat file to import the correct modules for each platform and python
+# version.
+#
+# author: Thomas Moreau and Olivier grisel
+#
+import sys
+
+if sys.version_info[:2] >= (3, 3):
+    import queue
+    from _pickle import PicklingError
+else:
+    import Queue as queue
+    from pickle import PicklingError
+
+if sys.version_info >= (3, 4):
+    from multiprocessing.process import BaseProcess
+else:
+    from multiprocessing.process import Process as BaseProcess
+
+# Platform specific compat
+if sys.platform == "win32":
+    from .compat_win32 import *
+else:
+    from .compat_posix import *
diff --git a/sklearn/externals/joblib/externals/loky/backend/compat_posix.py b/sklearn/externals/joblib/externals/loky/backend/compat_posix.py
new file mode 100644
index 000000000000..c8e4e4a43cec
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/compat_posix.py
@@ -0,0 +1,13 @@
+# flake8: noqa
+###############################################################################
+# Compat file to load the correct wait function
+#
+# author: Thomas Moreau and Olivier grisel
+#
+import sys
+
+# Compat wait
+if sys.version_info < (3, 3):
+    from ._posix_wait import wait
+else:
+    from multiprocessing.connection import wait
diff --git a/sklearn/externals/joblib/externals/loky/backend/compat_win32.py b/sklearn/externals/joblib/externals/loky/backend/compat_win32.py
new file mode 100644
index 000000000000..aa0a1fa919a6
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/compat_win32.py
@@ -0,0 +1,59 @@
+# flake8: noqa: F401
+import sys
+import numbers
+
+if sys.platform == "win32":
+    # Avoid import error by code introspection tools such as test runners
+    # trying to import this module while running on non-Windows systems.
+
+    # Compat Popen
+    if sys.version_info[:2] >= (3, 4):
+        from multiprocessing.popen_spawn_win32 import Popen
+    else:
+        from multiprocessing.forking import Popen
+
+    # wait compat
+    if sys.version_info[:2] < (3, 3):
+        from ._win_wait import wait
+    else:
+        from multiprocessing.connection import wait
+
+    # Compat _winapi
+    if sys.version_info[:2] >= (3, 4):
+        import _winapi
+    else:
+        import os
+        import msvcrt
+        if sys.version_info[:2] < (3, 3):
+            import _subprocess as win_api
+            from _multiprocessing import win32
+        else:
+            import _winapi as win_api
+
+        class _winapi:
+            CreateProcess = win_api.CreateProcess
+
+            @staticmethod
+            def CreatePipe(*args):
+                rfd, wfd = os.pipe()
+                _current_process = win_api.GetCurrentProcess()
+                rhandle = win_api.DuplicateHandle(
+                    _current_process, msvcrt.get_osfhandle(rfd),
+                    _current_process, 0, True,
+                    win_api.DUPLICATE_SAME_ACCESS)
+                if sys.version_info[:2] < (3, 3):
+                    rhandle = rhandle.Detach()
+                os.close(rfd)
+                return rhandle, wfd
+
+            @staticmethod
+            def CloseHandle(h):
+                if isinstance(h, numbers.Integral):
+                    # Cast long to int for 64-bit Python 2.7 under Windows
+                    h = int(h)
+                if sys.version_info[:2] < (3, 3):
+                    if not isinstance(h, int):
+                        h = h.Detach()
+                    win32.CloseHandle(h)
+                else:
+                    win_api.CloseHandle(h)
diff --git a/sklearn/externals/joblib/externals/loky/backend/context.py b/sklearn/externals/joblib/externals/loky/backend/context.py
new file mode 100644
index 000000000000..52df5589cb7b
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/context.py
@@ -0,0 +1,227 @@
+###############################################################################
+# Basic context management with LokyContext and  provides
+# compat for UNIX 2.7 and 3.3
+#
+# author: Thomas Moreau and Olivier Grisel
+#
+# adapted from multiprocessing/context.py
+#  * Create a context ensuring loky uses only objects that are compatible
+#  * Add LokyContext to the list of context of multiprocessing so loky can be
+#    used with multiprocessing.set_start_method
+#  * Add some compat function for python2.7 and 3.3.
+#
+from __future__ import division
+
+import os
+import sys
+import warnings
+import multiprocessing as mp
+
+
+from .process import LokyProcess
+
+if sys.version_info[:2] >= (3, 4):
+    from multiprocessing import get_context as get_mp_context
+    from multiprocessing.context import assert_spawning, set_spawning_popen
+    from multiprocessing.context import get_spawning_popen, BaseContext
+
+    def get_context(method="loky"):
+        if method == "fork":
+            warnings.warn("`fork` start method should not be used with `loky` "
+                          "as it does not respect POSIX. Try using `spawn` or "
+                          "`loky` instead.", UserWarning)
+        return get_mp_context(method)
+
+else:
+    METHODS = ['loky', 'loky_init_main']
+    if sys.platform != 'win32':
+        import threading
+        # Mecanism to check that the current thread is spawning a child process
+        _tls = threading.local()
+        popen_attr = 'spawning_popen'
+    else:
+        from multiprocessing.forking import Popen
+        _tls = Popen._tls
+        popen_attr = 'process_handle'
+
+    BaseContext = object
+
+    def get_spawning_popen():
+        return getattr(_tls, popen_attr, None)
+
+    def set_spawning_popen(popen):
+        setattr(_tls, popen_attr, popen)
+
+    def assert_spawning(obj):
+        if get_spawning_popen() is None:
+            raise RuntimeError(
+                '%s objects should only be shared between processes'
+                ' through inheritance' % type(obj).__name__
+            )
+
+    def get_context(method="loky"):
+        if method == "loky":
+            return LokyContext()
+        elif method == "loky_init_main":
+            return LokyInitMainContext()
+        else:
+            raise ValueError("Method {} is not implemented. The available "
+                             "methods are {}".format(method, METHODS))
+
+
+def cpu_count():
+    """Return the number of CPUs the current process can use.
+
+    The returned number of CPUs accounts for:
+     * the number of CPUs in the system, as given by
+       ``multiprocessing.cpu_count``
+     * the CPU affinity settings of the current process
+       (available with Python 3.4+ on some Unix systems)
+     * CFS scheduler CPU bandwidth limit
+       (available on Linux only)
+    and is given as the minimum of these three constraints.
+    It is also always larger or equal to 1.
+    """
+    import math
+
+    try:
+        cpu_count_mp = mp.cpu_count()
+    except NotImplementedError:
+        cpu_count_mp = 1
+
+    # Number of available CPUs given affinity settings
+    cpu_count_affinity = cpu_count_mp
+    if hasattr(os, 'sched_getaffinity'):
+        try:
+            cpu_count_affinity = len(os.sched_getaffinity(0))
+        except NotImplementedError:
+            pass
+
+    # CFS scheduler CPU bandwidth limit
+    # available in Linux since 2.6 kernel
+    cpu_count_cfs = cpu_count_mp
+    cfs_quota_fname = "/sys/fs/cgroup/cpu/cpu.cfs_quota_us"
+    cfs_period_fname = "/sys/fs/cgroup/cpu/cpu.cfs_period_us"
+    if os.path.exists(cfs_quota_fname) and os.path.exists(cfs_period_fname):
+        with open(cfs_quota_fname, 'r') as fh:
+            cfs_quota_us = int(fh.read())
+        with open(cfs_period_fname, 'r') as fh:
+            cfs_period_us = int(fh.read())
+
+        if cfs_quota_us > 0 and cfs_period_us > 0:
+            cpu_count_cfs = math.ceil(cfs_quota_us / cfs_period_us)
+            cpu_count_cfs = max(cpu_count_cfs, 1)
+
+    return min(cpu_count_mp, cpu_count_affinity, cpu_count_cfs)
+
+
+class LokyContext(BaseContext):
+    """Context relying on the LokyProcess."""
+    _name = 'loky'
+    Process = LokyProcess
+    cpu_count = staticmethod(cpu_count)
+
+    def Queue(self, maxsize=0, reducers=None):
+        '''Returns a queue object'''
+        from .queues import Queue
+        return Queue(maxsize, reducers=reducers,
+                     ctx=self.get_context())
+
+    def SimpleQueue(self, reducers=None):
+        '''Returns a queue object'''
+        from .queues import SimpleQueue
+        return SimpleQueue(reducers=reducers, ctx=self.get_context())
+
+    if sys.version_info[:2] < (3, 4):
+        """Compat for python2.7/3.3 for necessary methods in Context"""
+        def get_context(self):
+            return self
+
+        def get_start_method(self):
+            return "loky"
+
+        def Pipe(self, duplex=True):
+            '''Returns two connection object connected by a pipe'''
+            return mp.Pipe(duplex)
+
+        if sys.platform != "win32":
+            """Use the compat Manager for python2.7/3.3 on UNIX to avoid
+            relying on fork processes
+            """
+            def Manager(self):
+                """Returns a manager object"""
+                from .managers import LokyManager
+                m = LokyManager()
+                m.start()
+                return m
+        else:
+            """Compat for context on Windows and python2.7/3.3. Using regular
+            multiprocessing objects as it does not rely on fork.
+            """
+            from multiprocessing import synchronize
+            Semaphore = staticmethod(synchronize.Semaphore)
+            BoundedSemaphore = staticmethod(synchronize.BoundedSemaphore)
+            Lock = staticmethod(synchronize.Lock)
+            RLock = staticmethod(synchronize.RLock)
+            Condition = staticmethod(synchronize.Condition)
+            Event = staticmethod(synchronize.Event)
+            Manager = staticmethod(mp.Manager)
+
+    if sys.platform != "win32":
+        """For Unix platform, use our custom implementation of synchronize
+        relying on ctypes to interface with pthread semaphores.
+        """
+        def Semaphore(self, value=1):
+            """Returns a semaphore object"""
+            from . import synchronize
+            return synchronize.Semaphore(value=value)
+
+        def BoundedSemaphore(self, value):
+            """Returns a bounded semaphore object"""
+            from .synchronize import BoundedSemaphore
+            return BoundedSemaphore(value)
+
+        def Lock(self):
+            """Returns a lock object"""
+            from .synchronize import Lock
+            return Lock()
+
+        def RLock(self):
+            """Returns a recurrent lock object"""
+            from .synchronize import RLock
+            return RLock()
+
+        def Condition(self, lock=None):
+            """Returns a condition object"""
+            from .synchronize import Condition
+            return Condition(lock)
+
+        def Event(self):
+            """Returns an event object"""
+            from .synchronize import Event
+            return Event()
+
+
+class LokyInitMainContext(LokyContext):
+    """Extra context with LokyProcess, which does load the main module
+
+    This context is used for compatibility in the case ``cloudpickle`` is not
+    present on the running system. This permits to load functions defined in
+    the ``main`` module, using proper safeguards. The declaration of the
+    ``executor`` should be protected by ``if __name__ == "__main__":`` and the
+    functions and variable used from main should be out of this block.
+
+    This mimics the default behavior of multiprocessing under Windows and the
+    behavior of the ``spawn`` start method on a posix system for python3.4+.
+    For more details, see the end of the following section of python doc
+    https://docs.python.org/3/library/multiprocessing.html#multiprocessing-programming
+    """
+    def Process(self, *args, **kwargs):
+        kwargs.pop('init_main_module', True)
+        return LokyProcess(*args, init_main_module=True, **kwargs)
+
+
+if sys.version_info > (3, 4):
+    """Register loky context so it works with multiprocessing.get_context"""
+    mp.context._concrete_contexts['loky'] = LokyContext()
+    mp.context._concrete_contexts['loky_init_main'] = LokyInitMainContext()
diff --git a/sklearn/externals/joblib/externals/loky/backend/fork_exec.py b/sklearn/externals/joblib/externals/loky/backend/fork_exec.py
new file mode 100644
index 000000000000..eee2a1c80a23
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/fork_exec.py
@@ -0,0 +1,43 @@
+###############################################################################
+# Launch a subprocess using forkexec and make sure only the needed fd are
+# shared in the two process.
+#
+# author: Thomas Moreau and Olivier Grisel
+#
+import os
+import sys
+
+if sys.platform == "darwin" and sys.version_info < (3, 3):
+    FileNotFoundError = OSError
+
+
+def close_fds(keep_fds):  # pragma: no cover
+    """Close all the file descriptors except those in keep_fds."""
+
+    # Make sure to keep stdout and stderr open for logging purpose
+    keep_fds = set(keep_fds).union([1, 2])
+
+    # We try to retrieve all the open fds
+    try:
+        open_fds = set(int(fd) for fd in os.listdir('/proc/self/fd'))
+    except FileNotFoundError:
+        import resource
+        max_nfds = resource.getrlimit(resource.RLIMIT_NOFILE)[0]
+        open_fds = set(fd for fd in range(3, max_nfds))
+        open_fds.add(0)
+
+    for i in open_fds - keep_fds:
+        try:
+            os.close(i)
+        except OSError:
+            pass
+
+
+def fork_exec(cmd, keep_fds):
+
+    pid = os.fork()
+    if pid == 0:  # pragma: no cover
+        close_fds(keep_fds)
+        os.execv(sys.executable, cmd)
+    else:
+        return pid
diff --git a/sklearn/externals/joblib/externals/loky/backend/managers.py b/sklearn/externals/joblib/externals/loky/backend/managers.py
new file mode 100644
index 000000000000..081f8976e4e7
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/managers.py
@@ -0,0 +1,51 @@
+###############################################################################
+# compat for UNIX 2.7 and 3.3
+# Manager with LokyContext server.
+# This avoids having a Manager using fork and breaks the fd.
+#
+# author: Thomas Moreau and Olivier Grisel
+#
+# based on multiprocessing/managers.py (17/02/2017)
+#  * Overload the start method to use LokyContext and launch a loky subprocess
+#
+
+import multiprocessing as mp
+from multiprocessing.managers import SyncManager, State
+from .process import LokyProcess as Process
+
+
+class LokyManager(SyncManager):
+    def start(self, initializer=None, initargs=()):
+        '''Spawn a server process for this manager object'''
+        assert self._state.value == State.INITIAL
+
+        if (initializer is not None
+                and not hasattr(initializer, '__call__')):
+            raise TypeError('initializer must be a callable')
+
+        # pipe over which we will retrieve address of server
+        reader, writer = mp.Pipe(duplex=False)
+
+        # spawn process which runs a server
+        self._process = Process(
+            target=type(self)._run_server,
+            args=(self._registry, self._address, bytes(self._authkey),
+                  self._serializer, writer, initializer, initargs),
+        )
+        ident = ':'.join(str(i) for i in self._process._identity)
+        self._process.name = type(self).__name__ + '-' + ident
+        self._process.start()
+
+        # get address of server
+        writer.close()
+        self._address = reader.recv()
+        reader.close()
+
+        # register a finalizer
+        self._state.value = State.STARTED
+        self.shutdown = mp.util.Finalize(
+            self, type(self)._finalize_manager,
+            args=(self._process, self._address, self._authkey,
+                  self._state, self._Client),
+            exitpriority=0
+        )
diff --git a/sklearn/externals/joblib/externals/loky/backend/popen_loky_posix.py b/sklearn/externals/joblib/externals/loky/backend/popen_loky_posix.py
new file mode 100644
index 000000000000..729c7c71fe68
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/popen_loky_posix.py
@@ -0,0 +1,214 @@
+###############################################################################
+# Popen for LokyProcess.
+#
+# author: Thomas Moreau and Olivier Grisel
+#
+import os
+import sys
+import signal
+import pickle
+from io import BytesIO
+
+from . import reduction, spawn
+from .context import get_spawning_popen, set_spawning_popen
+from multiprocessing import util, process
+
+if sys.version_info[:2] < (3, 3):
+    ProcessLookupError = OSError
+
+if sys.platform != "win32":
+    from . import semaphore_tracker
+
+
+__all__ = []
+
+if sys.platform != "win32":
+    #
+    # Wrapper for an fd used while launching a process
+    #
+
+    class _DupFd(object):
+        def __init__(self, fd):
+            self.fd = reduction._mk_inheritable(fd)
+
+        def detach(self):
+            return self.fd
+
+    #
+    # Start child process using subprocess.Popen
+    #
+
+    __all__.append('Popen')
+
+    class Popen(object):
+        method = 'loky'
+        DupFd = _DupFd
+
+        def __init__(self, process_obj):
+            sys.stdout.flush()
+            sys.stderr.flush()
+            self.returncode = None
+            self._fds = []
+            self._launch(process_obj)
+
+        if sys.version_info < (3, 4):
+            @classmethod
+            def duplicate_for_child(cls, fd):
+                popen = get_spawning_popen()
+                popen._fds.append(fd)
+                return reduction._mk_inheritable(fd)
+
+        else:
+            def duplicate_for_child(self, fd):
+                self._fds.append(fd)
+                return reduction._mk_inheritable(fd)
+
+        def poll(self, flag=os.WNOHANG):
+            if self.returncode is None:
+                while True:
+                    try:
+                        pid, sts = os.waitpid(self.pid, flag)
+                    except OSError as e:
+                        # Child process not yet created. See #1731717
+                        # e.errno == errno.ECHILD == 10
+                        return None
+                    else:
+                        break
+                if pid == self.pid:
+                    if os.WIFSIGNALED(sts):
+                        self.returncode = -os.WTERMSIG(sts)
+                    else:
+                        assert os.WIFEXITED(sts)
+                        self.returncode = os.WEXITSTATUS(sts)
+            return self.returncode
+
+        def wait(self, timeout=None):
+            if sys.version_info < (3, 3):
+                import time
+                if timeout is None:
+                    return self.poll(0)
+                deadline = time.time() + timeout
+                delay = 0.0005
+                while 1:
+                    res = self.poll()
+                    if res is not None:
+                        break
+                    remaining = deadline - time.time()
+                    if remaining <= 0:
+                        break
+                    delay = min(delay * 2, remaining, 0.05)
+                    time.sleep(delay)
+                return res
+
+            if self.returncode is None:
+                if timeout is not None:
+                    from multiprocessing.connection import wait
+                    if not wait([self.sentinel], timeout):
+                        return None
+                # This shouldn't block if wait() returned successfully.
+                return self.poll(os.WNOHANG if timeout == 0.0 else 0)
+            return self.returncode
+
+        def terminate(self):
+            if self.returncode is None:
+                try:
+                    os.kill(self.pid, signal.SIGTERM)
+                except ProcessLookupError:
+                    pass
+                except OSError:
+                    if self.wait(timeout=0.1) is None:
+                        raise
+
+        def _launch(self, process_obj):
+
+            tracker_fd = semaphore_tracker._semaphore_tracker.getfd()
+
+            fp = BytesIO()
+            set_spawning_popen(self)
+            try:
+                prep_data = spawn.get_preparation_data(
+                    process_obj._name, process_obj.init_main_module)
+                reduction.dump(prep_data, fp)
+                reduction.dump(process_obj, fp)
+
+            finally:
+                set_spawning_popen(None)
+
+            try:
+                parent_r, child_w = os.pipe()
+                child_r, parent_w = os.pipe()
+                # for fd in self._fds:
+                #     _mk_inheritable(fd)
+
+                cmd_python = [sys.executable]
+                cmd_python += ['-m', self.__module__]
+                cmd_python += ['--process-name', str(process_obj.name)]
+                cmd_python += ['--pipe',
+                               str(reduction._mk_inheritable(child_r))]
+                reduction._mk_inheritable(child_w)
+                if tracker_fd is not None:
+                    cmd_python += ['--semaphore',
+                                   str(reduction._mk_inheritable(tracker_fd))]
+                self._fds.extend([child_r, child_w, tracker_fd])
+                util.debug("launch python with cmd:\n%s" % cmd_python)
+                from .fork_exec import fork_exec
+                pid = fork_exec(cmd_python, self._fds)
+                self.sentinel = parent_r
+
+                method = 'getbuffer'
+                if not hasattr(fp, method):
+                    method = 'getvalue'
+                with os.fdopen(parent_w, 'wb') as f:
+                    f.write(getattr(fp, method)())
+                self.pid = pid
+            finally:
+                if parent_r is not None:
+                    util.Finalize(self, os.close, (parent_r,))
+                for fd in (child_r, child_w):
+                    if fd is not None:
+                        os.close(fd)
+
+        @staticmethod
+        def thread_is_spawning():
+            return True
+
+
+if __name__ == '__main__':
+    import argparse
+    parser = argparse.ArgumentParser('Command line parser')
+    parser.add_argument('--pipe', type=int, required=True,
+                        help='File handle for the pipe')
+    parser.add_argument('--semaphore', type=int, required=True,
+                        help='File handle name for the semaphore tracker')
+    parser.add_argument('--process-name', type=str, default=None,
+                        help='Identifier for debugging purpose')
+
+    args = parser.parse_args()
+
+    info = dict()
+    semaphore_tracker._semaphore_tracker._fd = args.semaphore
+
+    exitcode = 1
+    try:
+        with os.fdopen(args.pipe, 'rb') as from_parent:
+            process.current_process()._inheriting = True
+            try:
+                prep_data = pickle.load(from_parent)
+                spawn.prepare(prep_data)
+                process_obj = pickle.load(from_parent)
+            finally:
+                del process.current_process()._inheriting
+
+        exitcode = process_obj._bootstrap()
+    except Exception as e:
+        print('\n\n' + '-' * 80)
+        print('{} failed with traceback: '.format(args.process_name))
+        print('-' * 80)
+        import traceback
+        print(traceback.format_exc())
+        print('\n' + '-' * 80)
+    finally:
+        if from_parent is not None:
+            from_parent.close()
+
+        sys.exit(exitcode)
diff --git a/sklearn/externals/joblib/externals/loky/backend/popen_loky_win32.py b/sklearn/externals/joblib/externals/loky/backend/popen_loky_win32.py
new file mode 100644
index 000000000000..1f3e909e789a
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/popen_loky_win32.py
@@ -0,0 +1,99 @@
+import os
+import sys
+
+from .context import get_spawning_popen, set_spawning_popen
+from . import spawn
+from . import reduction
+from multiprocessing import util
+
+if sys.platform == "win32":
+    # Avoid import error by code introspection tools such as test runners
+    # trying to import this module while running on non-Windows systems.
+    import msvcrt
+    from .compat_win32 import _winapi
+    from .compat_win32 import Popen as _Popen
+else:
+    _Popen = object
+
+if sys.version_info[:2] < (3, 3):
+    from os import fdopen as open
+
+__all__ = ['Popen']
+
+#
+#
+#
+
+TERMINATE = 0x10000
+WINEXE = (sys.platform == 'win32' and getattr(sys, 'frozen', False))
+WINSERVICE = sys.executable.lower().endswith("pythonservice.exe")
+
+
+#
+# We define a Popen class similar to the one from subprocess, but
+# whose constructor takes a process object as its argument.
+#
+
+class Popen(_Popen):
+    '''
+    Start a subprocess to run the code of a process object
+    '''
+    method = 'loky'
+
+    def __init__(self, process_obj):
+        prep_data = spawn.get_preparation_data(
+            process_obj._name, process_obj.init_main_module)
+
+        # read end of pipe will be "stolen" by the child process
+        # -- see spawn_main() in spawn.py.
+        rhandle, wfd = _winapi.CreatePipe(None, 0)
+        if sys.version_info[:2] > (3, 3):
+            wfd = msvcrt.open_osfhandle(wfd, 0)
+
+        cmd = spawn.get_command_line(parent_pid=os.getpid(),
+                                     pipe_handle=rhandle)
+        cmd = ' '.join('"%s"' % x for x in cmd)
+
+        try:
+            with open(wfd, 'wb') as to_child:
+                # start process
+                try:
+                    inherit = sys.version_info[:2] < (3, 4)
+                    hp, ht, pid, tid = _winapi.CreateProcess(
+                        spawn.get_executable(), cmd,
+                        None, None, inherit, 0,
+                        None, None, None)
+                    _winapi.CloseHandle(ht)
+                except:
+                    _winapi.CloseHandle(rhandle)
+                    raise
+
+                # set attributes of self
+                self.pid = pid
+                self.returncode = None
+                self._handle = hp
+                self.sentinel = int(hp)
+                util.Finalize(self, _winapi.CloseHandle, (self.sentinel,))
+
+                # send information to child
+                set_spawning_popen(self)
+                if sys.version_info[:2] < (3, 4):
+                    Popen._tls.process_handle = int(hp)
+                try:
+                    reduction.dump(prep_data, to_child)
+                    reduction.dump(process_obj, to_child)
+                finally:
+                    set_spawning_popen(None)
+                    if sys.version_info[:2] < (3, 4):
+                        del Popen._tls.process_handle
+        except IOError as exc:
+            # IOError 22 happens when the launched subprocess terminated before
+            # wfd.close is called. Thus we can safely ignore it.
+            if exc.errno != 22:
+                raise
+            util.debug("While starting {}, ignored a IOError 22"
+                       .format(process_obj._name))
+
+    def duplicate_for_child(self, handle):
+        assert self is get_spawning_popen()
+        return reduction.duplicate(handle, self.sentinel)
diff --git a/sklearn/externals/joblib/externals/loky/backend/process.py b/sklearn/externals/joblib/externals/loky/backend/process.py
new file mode 100644
index 000000000000..401a46fa4f15
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/process.py
@@ -0,0 +1,91 @@
+###############################################################################
+# LokyProcess implementation
+#
+# authors: Thomas Moreau and Olivier Grisel
+#
+# based on multiprocessing/process.py  (17/02/2017)
+# * Add some compatibility function for python2.7 and 3.3
+#
+import os
+import sys
+from .compat import BaseProcess
+
+
+class LokyProcess(BaseProcess):
+    _start_method = 'loky'
+
+    def __init__(self, group=None, target=None, name=None, args=(),
+                 kwargs={}, daemon=None, init_main_module=False):
+        if sys.version_info < (3, 3):
+            super(LokyProcess, self).__init__(
+                group=group, target=target, name=name, args=args,
+                kwargs=kwargs)
+            self.daemon = daemon
+        else:
+            super(LokyProcess, self).__init__(
+                group=group, target=target, name=name, args=args,
+                kwargs=kwargs, daemon=daemon)
+        self.authkey = self.authkey
+        self.init_main_module = init_main_module
+
+    @staticmethod
+    def _Popen(process_obj):
+        if sys.platform == "win32":
+            from .popen_loky_win32 import Popen
+        else:
+            from .popen_loky_posix import Popen
+        return Popen(process_obj)
+
+    if sys.version_info < (3, 3):
+        def start(self):
+            '''
+            Start child process
+            '''
+            from multiprocessing.process import _current_process, _cleanup
+            assert self._popen is None, 'cannot start a process twice'
+            assert self._parent_pid == os.getpid(), \
+                'can only start a process object created by current process'
+            _cleanup()
+            self._popen = self._Popen(self)
+            self._sentinel = self._popen.sentinel
+            _current_process._children.add(self)
+
+        @property
+        def sentinel(self):
+            '''
+            Return a file descriptor (Unix) or handle (Windows) suitable for
+            waiting for process termination.
+            '''
+            try:
+                return self._sentinel
+            except AttributeError:
+                raise ValueError("process not started")
+
+    if sys.version_info < (3, 4):
+        @property
+        def authkey(self):
+            return self._authkey
+
+        @authkey.setter
+        def authkey(self, authkey):
+            '''
+            Set authorization key of process
+            '''
+            self._authkey = AuthenticationKey(authkey)
+
+
+#
+# We subclass bytes to avoid accidental transmission of auth keys over network
+#
+
+class AuthenticationKey(bytes):
+    def __reduce__(self):
+        from .context import assert_spawning
+        try:
+            assert_spawning(self)
+        except RuntimeError:
+            raise TypeError(
+                'Pickling an AuthenticationKey object is '
+                'disallowed for security reasons'
+            )
+        return AuthenticationKey, (bytes(self),)
diff --git a/sklearn/externals/joblib/externals/loky/backend/queues.py b/sklearn/externals/joblib/externals/loky/backend/queues.py
new file mode 100644
index 000000000000..f6401516875f
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/queues.py
@@ -0,0 +1,239 @@
+###############################################################################
+# Queue and SimpleQueue implementation for loky
+#
+# authors: Thomas Moreau, Olivier Grisel
+#
+# based on multiprocessing/queues.py (16/02/2017)
+# * Add some compatibility function for python2.7 and 3.3 and makes sure
+#   it uses the right synchronization primitive.
+# * Add some custom reducers for the Queues/SimpleQueue to tweak the
+#   pickling process. (overload Queue._feed/SimpleQueue.put)
+#
+import os
+import sys
+import errno
+import weakref
+import threading
+
+from multiprocessing import util
+from multiprocessing import connection
+from multiprocessing.synchronize import SEM_VALUE_MAX
+from multiprocessing.queues import Full
+from multiprocessing.queues import _sentinel, Queue as mp_Queue
+from multiprocessing.queues import SimpleQueue as mp_SimpleQueue
+
+from .reduction import CustomizableLokyPickler
+from .context import assert_spawning, get_context
+
+
+__all__ = ['Queue', 'SimpleQueue', 'Full']
+
+
+class Queue(mp_Queue):
+
+    def __init__(self, maxsize=0, reducers=None, ctx=None):
+
+        if sys.version_info[:2] >= (3, 4):
+            super().__init__(maxsize=maxsize, ctx=ctx)
+        else:
+            if maxsize <= 0:
+                # Can raise ImportError (see issues #3770 and #23400)
+                maxsize = SEM_VALUE_MAX
+            if ctx is None:
+                ctx = get_context()
+            self._maxsize = maxsize
+            self._reader, self._writer = connection.Pipe(duplex=False)
+            self._rlock = ctx.Lock()
+            self._opid = os.getpid()
+            if sys.platform == 'win32':
+                self._wlock = None
+            else:
+                self._wlock = ctx.Lock()
+            self._sem = ctx.BoundedSemaphore(maxsize)
+
+            # For use by concurrent.futures
+            self._ignore_epipe = False
+
+            self._after_fork()
+
+            if sys.platform != 'win32':
+                util.register_after_fork(self, Queue._after_fork)
+
+        self._reducers = reducers
+
+    # Use custom queue set/get state to be able to reduce the custom reducers
+    def __getstate__(self):
+        assert_spawning(self)
+        return (self._ignore_epipe, self._maxsize, self._reader, self._writer,
+                self._reducers, self._rlock, self._wlock, self._sem,
+                self._opid)
+
+    def __setstate__(self, state):
+        (self._ignore_epipe, self._maxsize, self._reader, self._writer,
+         self._reducers, self._rlock, self._wlock, self._sem,
+         self._opid) = state
+        self._after_fork()
+
+    # Overload _start_thread to correctly call our custom _feed
+    def _start_thread(self):
+        util.debug('Queue._start_thread()')
+
+        # Start thread which transfers data from buffer to pipe
+        self._buffer.clear()
+        self._thread = threading.Thread(
+            target=Queue._feed,
+            args=(self._buffer, self._notempty, self._send_bytes,
+                  self._wlock, self._writer.close, self._reducers,
+                  self._ignore_epipe, self._on_queue_feeder_error, self._sem),
+            name='QueueFeederThread'
+        )
+        self._thread.daemon = True
+
+        util.debug('doing self._thread.start()')
+        self._thread.start()
+        util.debug('... done self._thread.start()')
+
+        # On process exit we will wait for data to be flushed to pipe.
+        #
+        # However, if this process created the queue then all
+        # processes which use the queue will be descendants of this
+        # process.  Therefore waiting for the queue to be flushed
+        # is pointless once all the child processes have been joined.
+        created_by_this_process = (self._opid == os.getpid())
+        if not self._joincancelled and not created_by_this_process:
+            self._jointhread = util.Finalize(
+                self._thread, Queue._finalize_join,
+                [weakref.ref(self._thread)],
+                exitpriority=-5
+            )
+
+        # Send sentinel to the thread queue object when garbage collected
+        self._close = util.Finalize(
+            self, Queue._finalize_close,
+            [self._buffer, self._notempty],
+            exitpriority=10
+        )
+
+    # Overload the _feed methods to use our custom pickling strategy.
+    @staticmethod
+    def _feed(buffer, notempty, send_bytes, writelock, close, reducers,
+              ignore_epipe, onerror, queue_sem):
+        util.debug('starting thread to feed data to pipe')
+        nacquire = notempty.acquire
+        nrelease = notempty.release
+        nwait = notempty.wait
+        bpopleft = buffer.popleft
+        sentinel = _sentinel
+        if sys.platform != 'win32':
+            wacquire = writelock.acquire
+            wrelease = writelock.release
+        else:
+            wacquire = None
+
+        while 1:
+            try:
+                nacquire()
+                try:
+                    if not buffer:
+                        nwait()
+                finally:
+                    nrelease()
+                try:
+                    while 1:
+                        obj = bpopleft()
+                        if obj is sentinel:
+                            util.debug('feeder thread got sentinel -- exiting')
+                            close()
+                            return
+
+                        # serialize the data before acquiring the lock
+                        obj = CustomizableLokyPickler.dumps(
+                            obj, reducers=reducers)
+                        if wacquire is None:
+                            send_bytes(obj)
+                        else:
+                            wacquire()
+                            try:
+                                send_bytes(obj)
+                            finally:
+                                wrelease()
+                except IndexError:
+                    pass
+            except BaseException as e:
+                if ignore_epipe and getattr(e, 'errno', 0) == errno.EPIPE:
+                    return
+                # Since this runs in a daemon thread the resources it uses
+                # may be become unusable while the process is cleaning up.
+                # We ignore errors which happen after the process has
+                # started to cleanup.
+                if util.is_exiting():
+                    util.info('error in queue thread: %s', e)
+                    return
+                else:
+                    queue_sem.release()
+                    onerror(e, obj)
+
+    def _on_queue_feeder_error(self, e, obj):
+        """
+        Private API hook called when feeding data in the background thread
+        raises an exception.  For overriding by concurrent.futures.
+        """
+        import traceback
+        traceback.print_exc()
+
+    if sys.version_info[:2] < (3, 4):
+        # Compat for python2.7/3.3 that use _send instead of _send_bytes
+        def _after_fork(self):
+            super(Queue, self)._after_fork()
+            self._send_bytes = self._writer.send_bytes
+
+
+class SimpleQueue(mp_SimpleQueue):
+
+    def __init__(self, reducers=None, ctx=None):
+        if sys.version_info[:2] >= (3, 4):
+            super().__init__(ctx=ctx)
+        else:
+            # Use the context to create the sync objects for python2.7/3.3
+            if ctx is None:
+                ctx = get_context()
+            self._reader, self._writer = connection.Pipe(duplex=False)
+            self._rlock = ctx.Lock()
+            self._poll = self._reader.poll
+            if sys.platform == 'win32':
+                self._wlock = None
+            else:
+                self._wlock = ctx.Lock()
+
+        # Add possiblity to use custom reducers
+        self._reducers = reducers
+
+    # Use custom queue set/get state to be able to reduce the custom reducers
+    def __getstate__(self):
+        assert_spawning(self)
+        return (self._reader, self._writer, self._reducers, self._rlock,
+                self._wlock)
+
+    def __setstate__(self, state):
+        (self._reader, self._writer, self._reducers, self._rlock,
+         self._wlock) = state
+
+    if sys.version_info[:2] < (3, 4):
+        # For python2.7/3.3, overload get to avoid creating deadlocks with
+        # unpickling errors.
+        def get(self):
+            with self._rlock:
+                res = self._reader.recv_bytes()
+            # unserialize the data after having released the lock
+            return CustomizableLokyPickler.loads(res)
+
+    # Overload put to use our customizable reducer
+    def put(self, obj):
+        # serialize the data before acquiring the lock
+        obj = CustomizableLokyPickler.dumps(obj, reducers=self._reducers)
+        if self._wlock is None:
+            # writes to a message oriented win32 pipe are atomic
+            self._writer.send_bytes(obj)
+        else:
+            with self._wlock:
+                self._writer.send_bytes(obj)
diff --git a/sklearn/externals/joblib/externals/loky/backend/reduction.py b/sklearn/externals/joblib/externals/loky/backend/reduction.py
new file mode 100644
index 000000000000..20eb581cbfce
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/reduction.py
@@ -0,0 +1,206 @@
+###############################################################################
+# Customizable Pickler with some basic reducers
+#
+# author: Thomas Moreau
+#
+# adapted from multiprocessing/reduction.py (17/02/2017)
+#  * Replace the ForkingPickler with a similar _LokyPickler,
+#  * Add CustomizableLokyPickler to allow customizing pickling process
+#    on the fly.
+#
+import io
+import sys
+import functools
+import warnings
+from multiprocessing import util
+try:
+    # Python 2 compat
+    from cPickle import loads
+except ImportError:
+    from pickle import loads
+    import copyreg
+
+if sys.platform == "win32":
+    if sys.version_info[:2] > (3, 3):
+        from multiprocessing.reduction import duplicate
+    else:
+        from multiprocessing.forking import duplicate
+
+from pickle import HIGHEST_PROTOCOL
+from . import LOKY_PICKLER
+
+Pickler = None
+try:
+    if LOKY_PICKLER is None or LOKY_PICKLER == "":
+        from pickle import Pickler
+    elif LOKY_PICKLER == "cloudpickle":
+        from cloudpickle import CloudPickler as Pickler
+    elif LOKY_PICKLER == "dill":
+        from dill import Pickler
+    elif LOKY_PICKLER != "pickle":
+        from importlib import import_module
+        mpickle = import_module(LOKY_PICKLER)
+        Pickler = mpickle.Pickler
+    util.debug("Using default backend {} for pickling."
+               .format(LOKY_PICKLER if LOKY_PICKLER is not None
+                       else "pickle"))
+except ImportError:
+    warnings.warn("Failed to import {} as asked in LOKY_PICKLER. Make sure"
+                  " it is correctly installed on your system. Falling back"
+                  " to default builtin pickle.".format(LOKY_PICKLER))
+except AttributeError:  # pragma: no cover
+    warnings.warn("Failed to find Pickler object in module {}. The module "
+                  "specified in LOKY_PICKLER should implement a Pickler "
+                  "object. Falling back to default builtin pickle."
+                  .format(LOKY_PICKLER))
+
+
+if Pickler is None:
+    from pickle import Pickler
+
+
+###############################################################################
+# Enable custom pickling in Loky.
+# To allow instance customization of the pickling process, we use 2 classes.
+# _LokyPickler gives module level customization and CustomizablePickler permits
+# to use instance base custom reducers.  Only CustomizablePickler should be
+# used.
+
+class _LokyPickler(Pickler):
+    """Pickler that uses custom reducers.
+
+    HIGHEST_PROTOCOL is selected by default as this pickler is used
+    to pickle ephemeral datastructures for interprocess communication
+    hence no backward compatibility is required.
+
+    """
+
+    # We override the pure Python pickler as its the only way to be able to
+    # customize the dispatch table without side effects in Python 2.6
+    # to 3.2. For Python 3.3+ leverage the new dispatch_table
+    # feature from http://bugs.python.org/issue14166 that makes it possible
+    # to use the C implementation of the Pickler which is faster.
+
+    if hasattr(Pickler, 'dispatch'):
+        # Make the dispatch registry an instance level attribute instead of
+        # a reference to the class dictionary under Python 2
+        dispatch = Pickler.dispatch.copy()
+    else:
+        # Under Python 3 initialize the dispatch table with a copy of the
+        # default registry
+        dispatch_table = copyreg.dispatch_table.copy()
+
+    @classmethod
+    def register(cls, type, reduce_func):
+        """Attach a reducer function to a given type in the dispatch table."""
+        if hasattr(Pickler, 'dispatch'):
+            # Python 2 pickler dispatching is not explicitly customizable.
+            # Let us use a closure to workaround this limitation.
+            def dispatcher(cls, obj):
+                reduced = reduce_func(obj)
+                cls.save_reduce(obj=obj, *reduced)
+            cls.dispatch[type] = dispatcher
+        else:
+            cls.dispatch_table[type] = reduce_func
+
+
+class CustomizableLokyPickler(Pickler):
+    def __init__(self, writer, reducers=None, protocol=HIGHEST_PROTOCOL):
+        Pickler.__init__(self, writer, protocol=protocol)
+        if reducers is None:
+            reducers = {}
+        if hasattr(Pickler, 'dispatch'):
+            # Make the dispatch registry an instance level attribute instead of
+            # a reference to the class dictionary under Python 2
+            self.dispatch = _LokyPickler.dispatch.copy()
+        else:
+            # Under Python 3 initialize the dispatch table with a copy of the
+            # default registry
+            self.dispatch_table = _LokyPickler.dispatch_table.copy()
+        for type, reduce_func in reducers.items():
+            self.register(type, reduce_func)
+
+    def register(self, type, reduce_func):
+        """Attach a reducer function to a given type in the dispatch table."""
+        if hasattr(Pickler, 'dispatch'):
+            # Python 2 pickler dispatching is not explicitly customizable.
+            # Let us use a closure to workaround this limitation.
+            def dispatcher(self, obj):
+                reduced = reduce_func(obj)
+                self.save_reduce(obj=obj, *reduced)
+            self.dispatch[type] = dispatcher
+        else:
+            self.dispatch_table[type] = reduce_func
+
+    @classmethod
+    def loads(self, buf):
+        if sys.version_info < (3, 3) and isinstance(buf, io.BytesIO):
+            buf = buf.getvalue()
+        return loads(buf)
+
+    @classmethod
+    def dumps(cls, obj, reducers=None, protocol=None):
+        buf = io.BytesIO()
+        p = cls(buf, reducers=reducers, protocol=protocol)
+        p.dump(obj)
+        if sys.version_info < (3, 3):
+            return buf.getvalue()
+        return buf.getbuffer()
+
+
+def dump(obj, file, reducers=None, protocol=None):
+    '''Replacement for pickle.dump() using LokyPickler.'''
+    CustomizableLokyPickler(file, reducers=reducers,
+                            protocol=protocol).dump(obj)
+
+
+###############################################################################
+# Registers extra pickling routines to improve picklization  for loky
+
+register = _LokyPickler.register
+
+
+# make methods picklable
+def _reduce_method(m):
+    if m.__self__ is None:
+        return getattr, (m.__class__, m.__func__.__name__)
+    else:
+        return getattr, (m.__self__, m.__func__.__name__)
+
+
+class _C:
+    def f(self):
+        pass
+
+    @classmethod
+    def h(cls):
+        pass
+
+
+register(type(_C().f), _reduce_method)
+register(type(_C.h), _reduce_method)
+
+
+def _reduce_method_descriptor(m):
+    return getattr, (m.__objclass__, m.__name__)
+
+
+register(type(list.append), _reduce_method_descriptor)
+register(type(int.__add__), _reduce_method_descriptor)
+
+
+# Make partial func pickable
+def _reduce_partial(p):
+    return _rebuild_partial, (p.func, p.args, p.keywords or {})
+
+
+def _rebuild_partial(func, args, keywords):
+    return functools.partial(func, *args, **keywords)
+
+
+register(functools.partial, _reduce_partial)
+
+if sys.platform != "win32":
+    from ._posix_reduction import _mk_inheritable  # noqa: F401
+else:
+    from . import _win_reduction  # noqa: F401
diff --git a/sklearn/externals/joblib/externals/loky/backend/semaphore_tracker.py b/sklearn/externals/joblib/externals/loky/backend/semaphore_tracker.py
new file mode 100644
index 000000000000..f49423713c3a
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/semaphore_tracker.py
@@ -0,0 +1,235 @@
+###############################################################################
+# Server process to keep track of unlinked semaphores and clean them.
+#
+# author: Thomas Moreau
+#
+# adapted from multiprocessing/semaphore_tracker.py  (17/02/2017)
+#  * include custom spawnv_passfds to start the process
+#  * use custom unlink from our own SemLock implementation
+#  * add some VERBOSE logging
+#
+
+#
+# On Unix we run a server process which keeps track of unlinked
+# semaphores. The server ignores SIGINT and SIGTERM and reads from a
+# pipe.  Every other process of the program has a copy of the writable
+# end of the pipe, so we get EOF when all other processes have exited.
+# Then the server process unlinks any remaining semaphore names.
+#
+# This is important because the system only supports a limited number
+# of named semaphores, and they will not be automatically removed till
+# the next reboot.  Without this semaphore tracker process, "killall
+# python" would probably leave unlinked semaphores.
+#
+
+import os
+import signal
+import sys
+import threading
+import warnings
+
+from . import spawn
+from multiprocessing import util
+
+try:
+    from _multiprocessing import sem_unlink
+except ImportError:
+    from .semlock import sem_unlink
+
+__all__ = ['ensure_running', 'register', 'unregister']
+
+VERBOSE = False
+
+
+class SemaphoreTracker(object):
+
+    def __init__(self):
+        self._lock = threading.Lock()
+        self._fd = None
+        self._pid = None
+
+    def getfd(self):
+        self.ensure_running()
+        return self._fd
+
+    def ensure_running(self):
+        '''Make sure that semaphore tracker process is running.
+
+        This can be run from any process.  Usually a child process will use
+        the semaphore created by its parent.'''
+        with self._lock:
+            if self._fd is not None:
+                # semaphore tracker was launched before, is it still running?
+                if self._check_alive():
+                    # => still alive
+                    return
+                # => dead, launch it again
+                os.close(self._fd)
+                self._fd = None
+                self._pid = None
+
+                warnings.warn('semaphore_tracker: process died unexpectedly, '
+                              'relaunching.  Some semaphores might leak.')
+
+            fds_to_pass = []
+            try:
+                fds_to_pass.append(sys.stderr.fileno())
+            except Exception:
+                pass
+
+            cmd = 'from {} import main; main(%d)'.format(main.__module__)
+            r, w = os.pipe()
+            try:
+                fds_to_pass.append(r)
+                # process will out live us, so no need to wait on pid
+                exe = spawn.get_executable()
+                args = [exe] + util._args_from_interpreter_flags()
+                # In python 3.3, there is a bug which put `-RRRRR..` instead of
+                # `-R` in args. Replace it to get the correct flags.
+                # See https://github.com/python/cpython/blob/3.3/Lib/subprocess.py#L488
+                if sys.version_info[:2] <= (3, 3):
+                    import re
+                    for i in range(1, len(args)):
+                        args[i] = re.sub("-R+", "-R", args[i])
+                args += ['-c', cmd % r]
+                util.debug("launching Semaphore tracker: {}".format(args))
+                pid = spawnv_passfds(exe, args, fds_to_pass)
+            except BaseException:
+                os.close(w)
+                raise
+            else:
+                self._fd = w
+                self._pid = pid
+            finally:
+                os.close(r)
+
+    def _check_alive(self):
+        '''Check for the existence of the semaphore tracker process.'''
+        try:
+            self._send('PROBE', '')
+        except BrokenPipeError:
+            return False
+        else:
+            return True
+
+    def register(self, name):
+        '''Register name of semaphore with semaphore tracker.'''
+        self.ensure_running()
+        self._send('REGISTER', name)
+
+    def unregister(self, name):
+        '''Unregister name of semaphore with semaphore tracker.'''
+        self.ensure_running()
+        self._send('UNREGISTER', name)
+
+    def _send(self, cmd, name):
+        msg = '{0}:{1}\n'.format(cmd, name).encode('ascii')
+        if len(name) > 512:
+            # posix guarantees that writes to a pipe of less than PIPE_BUF
+            # bytes are atomic, and that PIPE_BUF >= 512
+            raise ValueError('name too long')
+        nbytes = os.write(self._fd, msg)
+        assert nbytes == len(msg)
+
+
+_semaphore_tracker = SemaphoreTracker()
+ensure_running = _semaphore_tracker.ensure_running
+register = _semaphore_tracker.register
+unregister = _semaphore_tracker.unregister
+getfd = _semaphore_tracker.getfd
+
+
+def main(fd):
+    '''Run semaphore tracker.'''
+    # protect the process from ^C and "killall python" etc
+    signal.signal(signal.SIGINT, signal.SIG_IGN)
+    signal.signal(signal.SIGTERM, signal.SIG_IGN)
+
+    for f in (sys.stdin, sys.stdout):
+        try:
+            f.close()
+        except Exception:
+            pass
+
+    if VERBOSE:  # pragma: no cover
+        sys.stderr.write("Main semaphore tracker is running\n")
+        sys.stderr.flush()
+
+    cache = set()
+    try:
+        # keep track of registered/unregistered semaphores
+        with os.fdopen(fd, 'rb') as f:
+            for line in f:
+                try:
+                    cmd, name = line.strip().split(b':')
+                    if cmd == b'REGISTER':
+                        name = name.decode('ascii')
+                        cache.add(name)
+                        if VERBOSE:  # pragma: no cover
+                            sys.stderr.write("[SemaphoreTracker] register {}\n"
+                                             .format(name))
+                            sys.stderr.flush()
+                    elif cmd == b'UNREGISTER':
+                        name = name.decode('ascii')
+                        cache.remove(name)
+                        if VERBOSE:  # pragma: no cover
+                            sys.stderr.write("[SemaphoreTracker] unregister {}"
+                                             ": cache({})\n"
+                                             .format(name, len(cache)))
+                            sys.stderr.flush()
+                    elif cmd == b'PROBE':
+                        pass
+                    else:
+                        raise RuntimeError('unrecognized command %r' % cmd)
+                except BaseException:
+                    try:
+                        sys.excepthook(*sys.exc_info())
+                    except BaseException:
+                        pass
+    finally:
+        # all processes have terminated; cleanup any remaining semaphores
+        if cache:
+            try:
+                warnings.warn('semaphore_tracker: There appear to be %d '
+                              'leaked semaphores to clean up at shutdown' %
+                              len(cache))
+            except Exception:
+                pass
+        for name in cache:
+            # For some reason the process which created and registered this
+            # semaphore has failed to unregister it. Presumably it has died.
+            # We therefore unlink it.
+            try:
+                try:
+                    sem_unlink(name)
+                    if VERBOSE:  # pragma: no cover
+                        sys.stderr.write("[SemaphoreTracker] unlink {}\n"
+                                         .format(name))
+                        sys.stderr.flush()
+                except Exception as e:
+                    warnings.warn('semaphore_tracker: %r: %r' % (name, e))
+            finally:
+                pass
+
+    if VERBOSE:  # pragma: no cover
+        sys.stderr.write("semaphore tracker shut down\n")
+        sys.stderr.flush()
+
+
+#
+# Start a program with only specified fds kept open
+#
+
+def spawnv_passfds(path, args, passfds):
+    passfds = sorted(passfds)
+    errpipe_read, errpipe_write = os.pipe()
+    try:
+        from .reduction import _mk_inheritable
+        _pass = []
+        for fd in passfds:
+            _pass += [_mk_inheritable(fd)]
+        from .fork_exec import fork_exec
+        return fork_exec(args, _pass)
+    finally:
+        os.close(errpipe_read)
+        os.close(errpipe_write)
diff --git a/sklearn/externals/joblib/externals/loky/backend/semlock.py b/sklearn/externals/joblib/externals/loky/backend/semlock.py
new file mode 100644
index 000000000000..2d35f6a2715a
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/semlock.py
@@ -0,0 +1,274 @@
+###############################################################################
+# Ctypes implementation for posix semaphore.
+#
+# author: Thomas Moreau and Olivier Grisel
+#
+# adapted from cpython/Modules/_multiprocessing/semaphore.c (17/02/2017)
+#  * use ctypes to access pthread semaphores and provide a full python
+#    semaphore management.
+#  * For OSX, as no sem_getvalue is not implemented, Semaphore with value > 1
+#    are not guaranteed to work.
+#  * Only work with LokyProcess on posix
+#
+import os
+import sys
+import time
+import errno
+import ctypes
+import tempfile
+import threading
+from ctypes.util import find_library
+
+# As we need to use ctypes return types for semlock object, failure value
+# needs to be cast to proper python value. Unix failure convention is to
+# return 0, whereas OSX returns -1
+SEM_FAILURE = ctypes.c_void_p(0).value
+if sys.platform == 'darwin':
+    SEM_FAILURE = ctypes.c_void_p(-1).value
+
+# Semaphore types
+RECURSIVE_MUTEX = 0
+SEMAPHORE = 1
+
+# Semaphore constants
+SEM_OFLAG = ctypes.c_int(os.O_CREAT | os.O_EXCL)
+SEM_PERM = ctypes.c_int(384)
+
+
+class timespec(ctypes.Structure):
+    _fields_ = [("tv_sec", ctypes.c_long), ("tv_nsec", ctypes.c_long)]
+
+
+if sys.platform != 'win32':
+    pthread = ctypes.CDLL(find_library('pthread'), use_errno=True)
+    pthread.sem_open.restype = ctypes.c_void_p
+    pthread.sem_close.argtypes = [ctypes.c_void_p]
+    pthread.sem_wait.argtypes = [ctypes.c_void_p]
+    pthread.sem_trywait.argtypes = [ctypes.c_void_p]
+    pthread.sem_post.argtypes = [ctypes.c_void_p]
+    pthread.sem_getvalue.argtypes = [ctypes.c_void_p, ctypes.c_void_p]
+    pthread.sem_unlink.argtypes = [ctypes.c_char_p]
+    if sys.platform != "darwin":
+        pthread.sem_timedwait.argtypes = [ctypes.c_void_p,
+                                          ctypes.POINTER(timespec)]
+
+try:
+    from threading import get_ident
+except ImportError:
+    def get_ident():
+        return threading.current_thread().ident
+
+
+if sys.version_info[:2] < (3, 3):
+    class FileExistsError(OSError):
+        pass
+
+    class FileNotFoundError(OSError):
+        pass
+
+
+def sem_unlink(name):
+    if pthread.sem_unlink(name.encode('ascii')) < 0:
+        raiseFromErrno()
+
+
+def _sem_open(name, value=None):
+    """ Construct or retrieve a semaphore with the given name
+
+    If value is None, try to retrieve an existing named semaphore.
+    Else create a new semaphore with the given value
+    """
+    if value is None:
+        handle = pthread.sem_open(ctypes.c_char_p(name), 0)
+    else:
+        handle = pthread.sem_open(ctypes.c_char_p(name), SEM_OFLAG, SEM_PERM,
+                                  ctypes.c_int(value))
+
+    if handle == SEM_FAILURE:
+        e = ctypes.get_errno()
+        if e == errno.EEXIST:
+            raise FileExistsError("a semaphore named %s already exists" % name)
+        elif e == errno.ENOENT:
+            raise FileNotFoundError('cannot find semaphore named %s' % name)
+        elif e == errno.ENOSYS:
+            raise NotImplementedError('No semaphore implementation on this '
+                                      'system')
+        else:
+            raiseFromErrno()
+
+    return handle
+
+
+def _sem_timedwait(handle, timeout):
+    t_start = time.time()
+    if sys.platform != "darwin":
+        sec = int(timeout)
+        tv_sec = int(t_start)
+        nsec = int(1e9 * (timeout - sec) + .5)
+        tv_nsec = int(1e9 * (t_start - tv_sec) + .5)
+        deadline = timespec(sec+tv_sec, nsec+tv_nsec)
+        deadline.tv_sec += int(deadline.tv_nsec / 1000000000)
+        deadline.tv_nsec %= 1000000000
+        return pthread.sem_timedwait(handle, ctypes.pointer(deadline))
+
+    # PERFORMANCE WARNING
+    # No sem_timedwait on OSX so we implement our own method. This method can
+    # degrade performances has the wait can have a latency up to 20 msecs
+    deadline = t_start + timeout
+    delay = 0
+    now = time.time()
+    while True:
+        # Poll the sem file
+        res = pthread.sem_trywait(handle)
+        if res == 0:
+            return 0
+        else:
+            e = ctypes.get_errno()
+            if e != errno.EAGAIN:
+                raiseFromErrno()
+
+        # check for timeout
+        now = time.time()
+        if now > deadline:
+            ctypes.set_errno(errno.ETIMEDOUT)
+            return -1
+
+        # calculate how much time left and check the delay is not too long
+        # -- maximum is 20 msecs
+        difference = (deadline - now)
+        delay = min(delay, 20e-3, difference)
+
+        # Sleep and increase delay
+        time.sleep(delay)
+        delay += 1e-3
+
+
+class SemLock(object):
+    """ctypes wrapper to the unix semaphore"""
+
+    _rand = tempfile._RandomNameSequence()
+
+    def __init__(self, kind, value, maxvalue, name=None, unlink_now=False):
+        self.count = 0
+        self.ident = 0
+        self.kind = kind
+        self.maxvalue = maxvalue
+        self.name = name
+        self.handle = _sem_open(self.name.encode('ascii'), value)
+
+    def __del__(self):
+        try:
+            res = pthread.sem_close(self.handle)
+            assert res == 0, "Issue while closing semaphores"
+        except AttributeError:
+            pass
+
+    def _is_mine(self):
+        return self.count > 0 and get_ident() == self.ident
+
+    def acquire(self, block=True, timeout=None):
+        if self.kind == RECURSIVE_MUTEX and self._is_mine():
+            self.count += 1
+            return True
+
+        if block and timeout is None:
+            res = pthread.sem_wait(self.handle)
+        elif not block or timeout <= 0:
+            res = pthread.sem_trywait(self.handle)
+        else:
+            res = _sem_timedwait(self.handle, timeout)
+        if res < 0:
+            e = ctypes.get_errno()
+            if e == errno.EINTR:
+                return None
+            elif e in [errno.EAGAIN, errno.ETIMEDOUT]:
+                return False
+            raiseFromErrno()
+        self.count += 1
+        self.ident = get_ident()
+        return True
+
+    def release(self):
+        if self.kind == RECURSIVE_MUTEX:
+            assert self._is_mine(), (
+                "attempt to release recursive lock not owned by thread")
+            if self.count > 1:
+                self.count -= 1
+                return
+            assert self.count == 1
+        else:
+            if sys.platform == 'darwin':
+                # Handle broken get_value for mac ==> only Lock will work
+                # as sem_get_value do not work properly
+                if self.maxvalue == 1:
+                    if pthread.sem_trywait(self.handle) < 0:
+                        e = ctypes.get_errno()
+                        if e != errno.EAGAIN:
+                            raise OSError(e, errno.errorcode[e])
+                    else:
+                        if pthread.sem_post(self.handle) < 0:
+                            raiseFromErrno()
+                        else:
+                            raise ValueError(
+                                "semaphore or lock released too many times")
+                else:
+                    import warnings
+                    warnings.warn("semaphore are broken on OSX, release might "
+                                  "increase its maximal value", RuntimeWarning)
+            else:
+                value = self._get_value()
+                if value >= self.maxvalue:
+                    raise ValueError(
+                        "semaphore or lock released too many times")
+
+        if pthread.sem_post(self.handle) < 0:
+            raiseFromErrno()
+
+        self.count -= 1
+
+    def _get_value(self):
+        value = ctypes.pointer(ctypes.c_int(-1))
+        if pthread.sem_getvalue(self.handle, value) < 0:
+            raiseFromErrno()
+        return value.contents.value
+
+    def _count(self):
+        return self.count
+
+    def _is_zero(self):
+        if sys.platform == 'darwin':
+            # Handle broken get_value for mac ==> only Lock will work
+            # as sem_get_value do not work properly
+            if pthread.sem_trywait(self.handle) < 0:
+                e = ctypes.get_errno()
+                if e == errno.EAGAIN:
+                    return True
+                raise OSError(e, errno.errorcode[e])
+            else:
+                if pthread.sem_post(self.handle) < 0:
+                    raiseFromErrno()
+                return False
+        else:
+            value = ctypes.pointer(ctypes.c_int(-1))
+            if pthread.sem_getvalue(self.handle, value) < 0:
+                raiseFromErrno()
+            return value.contents.value == 0
+
+    def _after_fork(self):
+        self.count = 0
+
+    @staticmethod
+    def _rebuild(handle, kind, maxvalue, name):
+        self = SemLock.__new__(SemLock)
+        self.count = 0
+        self.ident = 0
+        self.kind = kind
+        self.maxvalue = maxvalue
+        self.name = name
+        self.handle = _sem_open(name.encode('ascii'))
+        return self
+
+
+def raiseFromErrno():
+    e = ctypes.get_errno()
+    raise OSError(e, errno.errorcode[e])
diff --git a/sklearn/externals/joblib/externals/loky/backend/spawn.py b/sklearn/externals/joblib/externals/loky/backend/spawn.py
new file mode 100644
index 000000000000..a7e57b884c1b
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/spawn.py
@@ -0,0 +1,240 @@
+###############################################################################
+# Prepares and processes the data to setup the new process environment
+#
+# author: Thomas Moreau and Olivier Grisel
+#
+# adapted from multiprocessing/spawn.py (17/02/2017)
+#  * Improve logging data
+#
+import os
+import sys
+import runpy
+import types
+import multiprocessing as mp
+from multiprocessing import process, util
+
+
+if sys.platform != 'win32':
+    WINEXE = False
+    WINSERVICE = False
+else:
+    WINEXE = (sys.platform == 'win32' and getattr(sys, 'frozen', False))
+    WINSERVICE = sys.executable.lower().endswith("pythonservice.exe")
+
+if WINSERVICE:
+    _python_exe = os.path.join(sys.exec_prefix, 'python.exe')
+else:
+    _python_exe = sys.executable
+
+if sys.version_info[:2] < (3, 4):
+    def get_command_line(pipe_handle, **kwds):
+        '''
+        Returns prefix of command line used for spawning a child process
+        '''
+        if getattr(sys, 'frozen', False):
+            return ([sys.executable, '--multiprocessing-fork', pipe_handle])
+        else:
+            prog = 'from multiprocessing.forking import main; main()'
+            opts = util._args_from_interpreter_flags()
+            return [_python_exe] + opts + [
+                '-c', prog, '--multiprocessing-fork', pipe_handle]
+else:
+    from multiprocessing.spawn import get_command_line
+
+
+def get_executable():
+    return _python_exe
+
+
+def _check_not_importing_main():
+    if getattr(process.current_process(), '_inheriting', False):
+        raise RuntimeError('''
+        An attempt has been made to start a new process before the
+        current process has finished its bootstrapping phase.
+
+        This probably means that you are not using fork to start your
+        child processes and you have forgotten to use the proper idiom
+        in the main module:
+
+            if __name__ == '__main__':
+                freeze_support()
+                ...
+
+        The "freeze_support()" line can be omitted if the program
+        is not going to be frozen to produce an executable.''')
+
+
+def get_preparation_data(name, init_main_module=True):
+    '''
+    Return info about parent needed by child to unpickle process object
+    '''
+    _check_not_importing_main()
+    d = dict(
+        log_to_stderr=util._log_to_stderr,
+        authkey=bytes(process.current_process().authkey),
+    )
+
+    if util._logger is not None:
+        d['log_level'] = util._logger.getEffectiveLevel()
+        if len(util._logger.handlers) > 0:
+            h = util._logger.handlers[0]
+            d['log_fmt'] = h.formatter._fmt
+
+    sys_path = [p for p in sys.path]
+    try:
+        i = sys_path.index('')
+    except ValueError:
+        pass
+    else:
+        sys_path[i] = process.ORIGINAL_DIR
+
+    d.update(
+        name=name,
+        sys_path=sys_path,
+        sys_argv=sys.argv,
+        orig_dir=process.ORIGINAL_DIR,
+        dir=os.getcwd()
+    )
+
+    if sys.platform != "win32":
+        # Pass the semaphore_tracker pid to avoid re-spawning it in every child
+        from . import semaphore_tracker
+        semaphore_tracker.ensure_running()
+        d['tracker_pid'] = semaphore_tracker._semaphore_tracker._pid
+
+    # Figure out whether to initialise main in the subprocess as a module
+    # or through direct execution (or to leave it alone entirely)
+    if init_main_module:
+        main_module = sys.modules['__main__']
+        try:
+            main_mod_name = getattr(main_module.__spec__, "name", None)
+        except:
+            main_mod_name = None
+        if main_mod_name is not None:
+            d['init_main_from_name'] = main_mod_name
+        elif sys.platform != 'win32' or (not WINEXE and not WINSERVICE):
+            main_path = getattr(main_module, '__file__', None)
+            if main_path is not None:
+                if (not os.path.isabs(main_path) and
+                        process.ORIGINAL_DIR is not None):
+                    main_path = os.path.join(process.ORIGINAL_DIR, main_path)
+                d['init_main_from_path'] = os.path.normpath(main_path)
+                # Compat for python2.7
+                d['main_path'] = d['init_main_from_path']
+
+    return d
+
+
+#
+# Prepare current process
+#
+old_main_modules = []
+
+
+def prepare(data):
+    '''
+    Try to get current process ready to unpickle process object
+    '''
+    if 'name' in data:
+        process.current_process().name = data['name']
+
+    if 'authkey' in data:
+        process.current_process().authkey = data['authkey']
+
+    if 'log_to_stderr' in data and data['log_to_stderr']:
+        util.log_to_stderr()
+
+    if 'log_level' in data:
+        util.get_logger().setLevel(data['log_level'])
+
+    if 'log_fmt' in data:
+        import logging
+        util.get_logger().handlers[0].setFormatter(
+            logging.Formatter(data['log_fmt'])
+        )
+
+    if 'sys_path' in data:
+        sys.path = data['sys_path']
+
+    if 'sys_argv' in data:
+        sys.argv = data['sys_argv']
+
+    if 'dir' in data:
+        os.chdir(data['dir'])
+
+    if 'orig_dir' in data:
+        process.ORIGINAL_DIR = data['orig_dir']
+
+    if hasattr(mp, 'set_start_method'):
+        mp.set_start_method('loky', force=True)
+
+    if 'tacker_pid' in data:
+        from . import semaphore_tracker
+        semaphore_tracker._semaphore_tracker._pid = data["tracker_pid"]
+
+    if 'init_main_from_name' in data:
+        _fixup_main_from_name(data['init_main_from_name'])
+    elif 'init_main_from_path' in data:
+        _fixup_main_from_path(data['init_main_from_path'])
+
+
+# Multiprocessing module helpers to fix up the main module in
+# spawned subprocesses
+def _fixup_main_from_name(mod_name):
+    # __main__.py files for packages, directories, zip archives, etc, run
+    # their "main only" code unconditionally, so we don't even try to
+    # populate anything in __main__, nor do we make any changes to
+    # __main__ attributes
+    current_main = sys.modules['__main__']
+    if mod_name == "__main__" or mod_name.endswith(".__main__"):
+        return
+
+    # If this process was forked, __main__ may already be populated
+    if getattr(current_main.__spec__, "name", None) == mod_name:
+        return
+
+    # Otherwise, __main__ may contain some non-main code where we need to
+    # support unpickling it properly. We rerun it as __mp_main__ and make
+    # the normal __main__ an alias to that
+    old_main_modules.append(current_main)
+    main_module = types.ModuleType("__mp_main__")
+    main_content = runpy.run_module(mod_name,
+                                    run_name="__mp_main__",
+                                    alter_sys=True)
+    main_module.__dict__.update(main_content)
+    sys.modules['__main__'] = sys.modules['__mp_main__'] = main_module
+
+
+def _fixup_main_from_path(main_path):
+    # If this process was forked, __main__ may already be populated
+    current_main = sys.modules['__main__']
+
+    # Unfortunately, the main ipython launch script historically had no
+    # "if __name__ == '__main__'" guard, so we work around that
+    # by treating it like a __main__.py file
+    # See https://github.com/ipython/ipython/issues/4698
+    main_name = os.path.splitext(os.path.basename(main_path))[0]
+    if main_name == 'ipython':
+        return
+
+    # Otherwise, if __file__ already has the setting we expect,
+    # there's nothing more to do
+    if getattr(current_main, '__file__', None) == main_path:
+        return
+
+    # If the parent process has sent a path through rather than a module
+    # name we assume it is an executable script that may contain
+    # non-main code that needs to be executed
+    old_main_modules.append(current_main)
+    main_module = types.ModuleType("__mp_main__")
+    main_content = runpy.run_path(main_path,
+                                  run_name="__mp_main__")
+    main_module.__dict__.update(main_content)
+    sys.modules['__main__'] = sys.modules['__mp_main__'] = main_module
+
+
+def import_main_path(main_path):
+    '''
+    Set sys.modules['__main__'] to module at main_path
+    '''
+    _fixup_main_from_path(main_path)
diff --git a/sklearn/externals/joblib/externals/loky/backend/synchronize.py b/sklearn/externals/joblib/externals/loky/backend/synchronize.py
new file mode 100644
index 000000000000..4773b9dc87c5
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/synchronize.py
@@ -0,0 +1,381 @@
+###############################################################################
+# Synchronization primitives based on our SemLock implementation
+#
+# author: Thomas Moreau and Olivier Grisel
+#
+# adapted from multiprocessing/synchronize.py (17/02/2017)
+#  * Remove ctx argument for compatibility reason
+#  * Implementation of Condition/Event are necessary for compatibility
+#    with python2.7/3.3, Barrier should be reimplemented to for those
+#    version (but it is not used in loky).
+#
+
+import os
+import sys
+import tempfile
+import threading
+import _multiprocessing
+from time import time as _time
+
+from .context import assert_spawning
+from . import semaphore_tracker
+from multiprocessing import process
+from multiprocessing import util
+
+__all__ = [
+    'Lock', 'RLock', 'Semaphore', 'BoundedSemaphore', 'Condition', 'Event'
+    ]
+# Try to import the mp.synchronize module cleanly, if it fails
+# raise ImportError for platforms lacking a working sem_open implementation.
+# See issue 3770
+try:
+    if sys.version_info < (3, 4):
+        from .semlock import SemLock as _SemLock
+        from .semlock import sem_unlink
+    else:
+        from _multiprocessing import SemLock as _SemLock
+        from _multiprocessing import sem_unlink
+except (ImportError):
+    raise ImportError("This platform lacks a functioning sem_open" +
+                      " implementation, therefore, the required" +
+                      " synchronization primitives needed will not" +
+                      " function, see issue 3770.")
+
+if sys.version_info[:2] < (3, 3):
+    FileExistsError = OSError
+
+#
+# Constants
+#
+
+RECURSIVE_MUTEX, SEMAPHORE = list(range(2))
+SEM_VALUE_MAX = _multiprocessing.SemLock.SEM_VALUE_MAX
+
+
+#
+# Base class for semaphores and mutexes; wraps `_multiprocessing.SemLock`
+#
+
+class SemLock(object):
+
+    _rand = tempfile._RandomNameSequence()
+
+    def __init__(self, kind, value, maxvalue):
+        # unlink_now is only used on win32 or when we are using fork.
+        unlink_now = False
+        for i in range(100):
+            try:
+                self._semlock = _SemLock(
+                    kind, value, maxvalue, SemLock._make_name(),
+                    unlink_now)
+            except FileExistsError:  # pragma: no cover
+                pass
+            else:
+                break
+        else:  # pragma: no cover
+            raise FileExistsError('cannot find name for semaphore')
+
+        util.debug('created semlock with handle %s and name "%s"'
+                   % (self._semlock.handle, self._semlock.name))
+
+        self._make_methods()
+
+        def _after_fork(obj):
+            obj._semlock._after_fork()
+
+        util.register_after_fork(self, _after_fork)
+
+        # When the object is garbage collected or the
+        # process shuts down we unlink the semaphore name
+        semaphore_tracker.register(self._semlock.name)
+        util.Finalize(self, SemLock._cleanup, (self._semlock.name,),
+                      exitpriority=0)
+
+    @staticmethod
+    def _cleanup(name):
+        sem_unlink(name)
+        semaphore_tracker.unregister(name)
+
+    def _make_methods(self):
+        self.acquire = self._semlock.acquire
+        self.release = self._semlock.release
+
+    def __enter__(self):
+        return self._semlock.acquire()
+
+    def __exit__(self, *args):
+        return self._semlock.release()
+
+    def __getstate__(self):
+        assert_spawning(self)
+        sl = self._semlock
+        h = sl.handle
+        return (h, sl.kind, sl.maxvalue, sl.name)
+
+    def __setstate__(self, state):
+        self._semlock = _SemLock._rebuild(*state)
+        util.debug('recreated blocker with handle %r and name "%s"'
+                   % (state[0], state[3]))
+        self._make_methods()
+
+    @staticmethod
+    def _make_name():
+        # OSX does not support long names for semaphores
+        return '/loky-%i-%s' % (os.getpid(), next(SemLock._rand))
+
+
+#
+# Semaphore
+#
+
+class Semaphore(SemLock):
+
+    def __init__(self, value=1):
+        SemLock.__init__(self, SEMAPHORE, value, SEM_VALUE_MAX)
+
+    def get_value(self):
+        if sys.platform == 'darwin':
+            raise NotImplementedError("OSX does not implement sem_getvalue")
+        return self._semlock._get_value()
+
+    def __repr__(self):
+        try:
+            value = self._semlock._get_value()
+        except Exception:
+            value = 'unknown'
+        return '<%s(value=%s)>' % (self.__class__.__name__, value)
+
+
+#
+# Bounded semaphore
+#
+
+class BoundedSemaphore(Semaphore):
+
+    def __init__(self, value=1):
+        SemLock.__init__(self, SEMAPHORE, value, value)
+
+    def __repr__(self):
+        try:
+            value = self._semlock._get_value()
+        except Exception:
+            value = 'unknown'
+        return '<%s(value=%s, maxvalue=%s)>' % \
+               (self.__class__.__name__, value, self._semlock.maxvalue)
+
+
+#
+# Non-recursive lock
+#
+
+class Lock(SemLock):
+
+    def __init__(self):
+        super(Lock, self).__init__(SEMAPHORE, 1, 1)
+
+    def __repr__(self):
+        try:
+            if self._semlock._is_mine():
+                name = process.current_process().name
+                if threading.current_thread().name != 'MainThread':
+                    name += '|' + threading.current_thread().name
+            elif self._semlock._get_value() == 1:
+                name = 'None'
+            elif self._semlock._count() > 0:
+                name = 'SomeOtherThread'
+            else:
+                name = 'SomeOtherProcess'
+        except Exception:
+            name = 'unknown'
+        return '<%s(owner=%s)>' % (self.__class__.__name__, name)
+
+
+#
+# Recursive lock
+#
+
+class RLock(SemLock):
+
+    def __init__(self):
+        super(RLock, self).__init__(RECURSIVE_MUTEX, 1, 1)
+
+    def __repr__(self):
+        try:
+            if self._semlock._is_mine():
+                name = process.current_process().name
+                if threading.current_thread().name != 'MainThread':
+                    name += '|' + threading.current_thread().name
+                count = self._semlock._count()
+            elif self._semlock._get_value() == 1:
+                name, count = 'None', 0
+            elif self._semlock._count() > 0:
+                name, count = 'SomeOtherThread', 'nonzero'
+            else:
+                name, count = 'SomeOtherProcess', 'nonzero'
+        except Exception:
+            name, count = 'unknown', 'unknown'
+        return '<%s(%s, %s)>' % (self.__class__.__name__, name, count)
+
+
+#
+# Condition variable
+#
+
+class Condition(object):
+
+    def __init__(self, lock=None):
+        self._lock = lock or RLock()
+        self._sleeping_count = Semaphore(0)
+        self._woken_count = Semaphore(0)
+        self._wait_semaphore = Semaphore(0)
+        self._make_methods()
+
+    def __getstate__(self):
+        assert_spawning(self)
+        return (self._lock, self._sleeping_count,
+                self._woken_count, self._wait_semaphore)
+
+    def __setstate__(self, state):
+        (self._lock, self._sleeping_count,
+         self._woken_count, self._wait_semaphore) = state
+        self._make_methods()
+
+    def __enter__(self):
+        return self._lock.__enter__()
+
+    def __exit__(self, *args):
+        return self._lock.__exit__(*args)
+
+    def _make_methods(self):
+        self.acquire = self._lock.acquire
+        self.release = self._lock.release
+
+    def __repr__(self):
+        try:
+            num_waiters = (self._sleeping_count._semlock._get_value() -
+                           self._woken_count._semlock._get_value())
+        except Exception:
+            num_waiters = 'unknown'
+        return '<%s(%s, %s)>' % (self.__class__.__name__,
+                                 self._lock, num_waiters)
+
+    def wait(self, timeout=None):
+        assert self._lock._semlock._is_mine(), \
+               'must acquire() condition before using wait()'
+
+        # indicate that this thread is going to sleep
+        self._sleeping_count.release()
+
+        # release lock
+        count = self._lock._semlock._count()
+        for i in range(count):
+            self._lock.release()
+
+        try:
+            # wait for notification or timeout
+            return self._wait_semaphore.acquire(True, timeout)
+        finally:
+            # indicate that this thread has woken
+            self._woken_count.release()
+
+            # reacquire lock
+            for i in range(count):
+                self._lock.acquire()
+
+    def notify(self):
+        assert self._lock._semlock._is_mine(), 'lock is not owned'
+        assert not self._wait_semaphore.acquire(False)
+
+        # to take account of timeouts since last notify() we subtract
+        # woken_count from sleeping_count and rezero woken_count
+        while self._woken_count.acquire(False):
+            res = self._sleeping_count.acquire(False)
+            assert res
+
+        if self._sleeping_count.acquire(False):  # try grabbing a sleeper
+            self._wait_semaphore.release()       # wake up one sleeper
+            self._woken_count.acquire()          # wait for the sleeper to wake
+
+            # rezero _wait_semaphore in case a timeout just happened
+            self._wait_semaphore.acquire(False)
+
+    def notify_all(self):
+        assert self._lock._semlock._is_mine(), 'lock is not owned'
+        assert not self._wait_semaphore.acquire(False)
+
+        # to take account of timeouts since last notify*() we subtract
+        # woken_count from sleeping_count and rezero woken_count
+        while self._woken_count.acquire(False):
+            res = self._sleeping_count.acquire(False)
+            assert res
+
+        sleepers = 0
+        while self._sleeping_count.acquire(False):
+            self._wait_semaphore.release()        # wake up one sleeper
+            sleepers += 1
+
+        if sleepers:
+            for i in range(sleepers):
+                self._woken_count.acquire()       # wait for a sleeper to wake
+
+            # rezero wait_semaphore in case some timeouts just happened
+            while self._wait_semaphore.acquire(False):
+                pass
+
+    def wait_for(self, predicate, timeout=None):
+        result = predicate()
+        if result:
+            return result
+        if timeout is not None:
+            endtime = _time() + timeout
+        else:
+            endtime = None
+            waittime = None
+        while not result:
+            if endtime is not None:
+                waittime = endtime - _time()
+                if waittime <= 0:
+                    break
+            self.wait(waittime)
+            result = predicate()
+        return result
+
+
+#
+# Event
+#
+
+class Event(object):
+
+    def __init__(self):
+        self._cond = Condition(Lock())
+        self._flag = Semaphore(0)
+
+    def is_set(self):
+        with self._cond:
+            if self._flag.acquire(False):
+                self._flag.release()
+                return True
+            return False
+
+    def set(self):
+        with self._cond:
+            self._flag.acquire(False)
+            self._flag.release()
+            self._cond.notify_all()
+
+    def clear(self):
+        with self._cond:
+            self._flag.acquire(False)
+
+    def wait(self, timeout=None):
+        with self._cond:
+            if self._flag.acquire(False):
+                self._flag.release()
+            else:
+                self._cond.wait(timeout)
+
+            if self._flag.acquire(False):
+                self._flag.release()
+                return True
+            return False
diff --git a/sklearn/externals/joblib/externals/loky/backend/utils.py b/sklearn/externals/joblib/externals/loky/backend/utils.py
new file mode 100644
index 000000000000..d54098a816cd
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/utils.py
@@ -0,0 +1,114 @@
+import os
+import sys
+import errno
+import signal
+import warnings
+import threading
+import subprocess
+try:
+    import psutil
+except ImportError:
+    psutil = None
+
+
+def _flag_current_thread_clean_exit():
+    """Put a ``_clean_exit`` flag on the current thread"""
+    thread = threading.current_thread()
+    thread._clean_exit = True
+
+
+def recursive_terminate(process, use_psutil=True):
+    if use_psutil and psutil is not None:
+        _recursive_terminate_with_psutil(process)
+    else:
+        _recursive_terminate_without_psutil(process)
+
+
+def _recursive_terminate_with_psutil(process, retries=5):
+    try:
+        children = psutil.Process(process.pid).children(recursive=True)
+    except psutil.NoSuchProcess:
+        return
+
+    for child in children:
+        try:
+            child.terminate()
+        except psutil.NoSuchProcess:
+            pass
+
+    gone, still_alive = psutil.wait_procs(children, timeout=5)
+    for child_process in still_alive:
+        child_process.kill()
+
+    process.terminate()
+    process.join()
+
+
+def _recursive_terminate_without_psutil(process):
+    """Terminate a process and its descendants.
+    """
+    try:
+        _recursive_terminate(process.pid)
+    except OSError as e:
+        warnings.warn("Failed to kill subprocesses on this platform. Please"
+                      "install psutil: https://github.com/giampaolo/psutil")
+        # In case we cannot introspect the children, we fall back to the
+        # classic Process.terminate.
+        process.terminate()
+    process.join()
+
+
+def _recursive_terminate(pid):
+    """Recursively kill the descendants of a process before killing it.
+    """
+
+    if sys.platform == "win32":
+        # On windows, the taskkill function with option `/T` terminate a given
+        # process pid and its children.
+        try:
+            subprocess.check_output(
+                ["taskkill", "/F", "/T", "/PID", str(pid)],
+                stderr=None)
+        except subprocess.CalledProcessError as e:
+            # In windows, taskkill return 1 for permission denied and 128, 255
+            # for no process found.
+            if e.returncode not in [1, 128, 255]:
+                raise
+            elif e.returncode == 1:
+                # Try to kill the process without its descendants if taskkill
+                # was denied permission. If this fails too, with an error
+                # different from process not found, let the top level function
+                # raise a warning and retry to kill the process.
+                try:
+                    os.kill(pid, signal.SIGTERM)
+                except OSError as e:
+                    if e.errno != errno.ESRCH:
+                        raise
+
+    else:
+        try:
+            children_pids = subprocess.check_output(
+                ["pgrep", "-P", str(pid)],
+                stderr=None
+            )
+        except subprocess.CalledProcessError as e:
+            # `ps` returns 1 when no child process has been found
+            if e.returncode == 1:
+                children_pids = b''
+            else:
+                raise
+
+        # Decode the result, split the cpid and remove the trailing line
+        children_pids = children_pids.decode().split('\n')[:-1]
+        for cpid in children_pids:
+            cpid = int(cpid)
+            _recursive_terminate(cpid)
+
+        try:
+            os.kill(pid, signal.SIGTERM)
+        except OSError as e:
+            # if OSError is raised with [Errno 3] no such process, the process
+            # is already terminated, else, raise the error and let the top
+            # level function raise a warning and retry to kill the process.
+            if e.errno != errno.ESRCH:
+                raise
diff --git a/sklearn/externals/joblib/externals/loky/process_executor.py b/sklearn/externals/joblib/externals/loky/process_executor.py
new file mode 100644
index 000000000000..6868f527069d
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/process_executor.py
@@ -0,0 +1,1084 @@
+###############################################################################
+# Re-implementation of the ProcessPoolExecutor more robust to faults
+#
+# author: Thomas Moreau and Olivier Grisel
+#
+# adapted from concurrent/futures/process_pool_executor.py (17/02/2017)
+#  * Backport for python2.7/3.3,
+#  * Add an extra management thread to detect queue_management_thread failures,
+#  * Improve the shutdown process to avoid deadlocks,
+#  * Add timeout for workers,
+#  * More robust pickling process.
+#
+# Copyright 2009 Brian Quinlan. All Rights Reserved.
+# Licensed to PSF under a Contributor Agreement.
+
+"""Implements ProcessPoolExecutor.
+
+The follow diagram and text describe the data-flow through the system:
+
+|======================= In-process =====================|== Out-of-process ==|
+
++----------+     +----------+       +--------+     +-----------+    +---------+
+|          |  => | Work Ids |       |        |     | Call Q    |    | Process |
+|          |     +----------+       |        |     +-----------+    |  Pool   |
+|          |     | ...      |       |        |     | ...       |    +---------+
+|          |     | 6        |    => |        |  => | 5, call() | => |         |
+|          |     | 7        |       |        |     | ...       |    |         |
+| Process  |     | ...      |       | Local  |     +-----------+    | Process |
+|  Pool    |     +----------+       | Worker |                      |  #1..n  |
+| Executor |                        | Thread |                      |         |
+|          |     +----------- +     |        |     +-----------+    |         |
+|          | <=> | Work Items | <=> |        | <=  | Result Q  | <= |         |
+|          |     +------------+     |        |     +-----------+    |         |
+|          |     | 6: call()  |     |        |     | ...       |    |         |
+|          |     |    future  |     +--------+     | 4, result |    |         |
+|          |     | ...        |                    | 3, except |    |         |
++----------+     +------------+                    +-----------+    +---------+
+
+Executor.submit() called:
+- creates a uniquely numbered _WorkItem and adds it to the "Work Items" dict
+- adds the id of the _WorkItem to the "Work Ids" queue
+
+Local worker thread:
+- reads work ids from the "Work Ids" queue and looks up the corresponding
+  WorkItem from the "Work Items" dict: if the work item has been cancelled then
+  it is simply removed from the dict, otherwise it is repackaged as a
+  _CallItem and put in the "Call Q". New _CallItems are put in the "Call Q"
+  until "Call Q" is full. NOTE: the size of the "Call Q" is kept small because
+  calls placed in the "Call Q" can no longer be cancelled with Future.cancel().
+- reads _ResultItems from "Result Q", updates the future stored in the
+  "Work Items" dict and deletes the dict entry
+
+Process #1..n:
+- reads _CallItems from "Call Q", executes the calls, and puts the resulting
+  _ResultItems in "Result Q"
+"""
+
+
+__author__ = 'Thomas Moreau (thomas.moreau.2010@gmail.com)'
+
+
+import os
+import sys
+import types
+import weakref
+import warnings
+import itertools
+import traceback
+import threading
+import multiprocessing as mp
+from functools import partial
+from pickle import PicklingError
+from time import time
+import gc
+
+from . import _base
+from .backend import get_context
+from .backend.compat import queue
+from .backend.compat import wait
+from .backend.context import cpu_count
+from .backend.queues import Queue, SimpleQueue, Full
+from .backend.utils import recursive_terminate
+
+try:
+    from concurrent.futures.process import BrokenProcessPool as _BPPException
+except ImportError:
+    _BPPException = RuntimeError
+
+
+# Compatibility for python2.7
+if sys.version_info[0] == 2:
+    ProcessLookupError = OSError
+
+
+# Workers are created as daemon threads and processes. This is done to allow
+# the interpreter to exit when there are still idle processes in a
+# ProcessPoolExecutor's process pool (i.e. shutdown() was not called). However,
+# allowing workers to die with the interpreter has two undesirable properties:
+#   - The workers would still be running during interpreter shutdown,
+#     meaning that they would fail in unpredictable ways.
+#   - The workers could be killed while evaluating a work item, which could
+#     be bad if the callable being evaluated has external side-effects e.g.
+#     writing to a file.
+#
+# To work around this problem, an exit handler is installed which tells the
+# workers to exit when their work queues are empty and then waits until the
+# threads/processes finish.
+
+_threads_wakeups = weakref.WeakKeyDictionary()
+_global_shutdown = False
+
+# Mechanism to prevent infinite process spawning. When a worker of a
+# ProcessPoolExecutor nested in MAX_DEPTH Executor tries to create a new
+# Executor, a LokyRecursionError is raised
+MAX_DEPTH = int(os.environ.get("LOKY_MAX_DEPTH", 10))
+_CURRENT_DEPTH = 0
+
+# Minimum time interval between two consecutive memory usage checks.
+_MEMORY_CHECK_DELAY = 1.
+
+# Number of bytes of memory usage allowed over the reference process size.
+_MAX_MEMORY_LEAK_SIZE = int(1e8)
+
+try:
+    from psutil import Process
+
+    def _get_memory_usage(pid, force_gc=False):
+        if force_gc:
+            gc.collect()
+
+        return Process(pid).memory_info().rss
+
+except ImportError:
+    _get_memory_usage = None
+
+
+class _ThreadWakeup:
+    def __init__(self):
+        self._reader, self._writer = mp.Pipe(duplex=False)
+
+    def close(self):
+        self._writer.close()
+        self._reader.close()
+
+    def wakeup(self):
+        if sys.platform == "win32" and sys.version_info[:2] < (3, 4):
+            # Compat for python2.7 on windows, where poll return false for
+            # b"" messages. Use the slightly larger message b"0".
+            self._writer.send_bytes(b"0")
+        else:
+            self._writer.send_bytes(b"")
+
+    def clear(self):
+        while self._reader.poll():
+            self._reader.recv_bytes()
+
+
+class _ExecutorFlags(object):
+    """necessary references to maintain executor states without preventing gc
+
+    It permits to keep the information needed by queue_management_thread
+    and crash_detection_thread to maintain the pool without preventing the
+    garbage collection of unreferenced executors.
+    """
+    def __init__(self):
+
+        self.shutdown = False
+        self.broken = None
+        self.kill_workers = False
+        self.shutdown_lock = threading.Lock()
+
+    def flag_as_shutting_down(self, kill_workers=False):
+        with self.shutdown_lock:
+            self.shutdown = True
+            self.kill_workers = kill_workers
+
+    def flag_as_broken(self, broken):
+        with self.shutdown_lock:
+            self.shutdown = True
+            self.broken = broken
+
+
+def _python_exit():
+    global _global_shutdown
+    _global_shutdown = True
+    items = list(_threads_wakeups.items())
+    mp.util.debug("Interpreter shutting down. Waking up queue_manager_threads "
+                  "{}".format(items))
+    for thread, thread_wakeup in items:
+        if thread.is_alive():
+            thread_wakeup.wakeup()
+    for thread, _ in items:
+        thread.join()
+
+
+# Module variable to register the at_exit call
+process_pool_executor_at_exit = None
+
+# Controls how many more calls than processes will be queued in the call queue.
+# A smaller number will mean that processes spend more time idle waiting for
+# work while a larger number will make Future.cancel() succeed less frequently
+# (Futures in the call queue cannot be cancelled).
+EXTRA_QUEUED_CALLS = 1
+
+
+class _RemoteTraceback(Exception):
+    """Embed stringification of remote traceback in local traceback
+    """
+    def __init__(self, tb=None):
+        self.tb = tb
+
+    def __str__(self):
+        return self.tb
+
+
+class _ExceptionWithTraceback(BaseException):
+
+    def __init__(self, exc, tb=None):
+        if tb is None:
+            _, _, tb = sys.exc_info()
+        tb = traceback.format_exception(type(exc), exc, tb)
+        tb = ''.join(tb)
+        self.exc = exc
+        self.tb = '\n"""\n%s"""' % tb
+
+    def __reduce__(self):
+        return _rebuild_exc, (self.exc, self.tb)
+
+
+def _rebuild_exc(exc, tb):
+    exc.__cause__ = _RemoteTraceback(tb)
+    return exc
+
+
+class _WorkItem(object):
+
+    __slots__ = ["future", "fn", "args", "kwargs"]
+
+    def __init__(self, future, fn, args, kwargs):
+        self.future = future
+        self.fn = fn
+        self.args = args
+        self.kwargs = kwargs
+
+
+class _ResultItem(object):
+
+    def __init__(self, work_id, exception=None, result=None):
+        self.work_id = work_id
+        self.exception = exception
+        self.result = result
+
+
+class _CallItem(object):
+
+    def __init__(self, work_id, fn, args, kwargs):
+        self.work_id = work_id
+        self.fn = fn
+        self.args = args
+        self.kwargs = kwargs
+
+    def __repr__(self):
+        return "CallItem({}, {}, {}, {})".format(
+            self.work_id, self.fn, self.args, self.kwargs)
+
+    try:
+        # If cloudpickle is present on the system, use it to pickle the
+        # function. This permits to use interactive terminal for loky calls.
+        # TODO: Add option to deactivate, as it increases pickling time.
+        from .backend import LOKY_PICKLER
+        assert LOKY_PICKLER is None or LOKY_PICKLER == ""
+
+        import cloudpickle  # noqa: F401
+
+        def __getstate__(self):
+            from cloudpickle import dumps
+            if isinstance(self.fn, (types.FunctionType,
+                                    types.LambdaType,
+                                    partial)):
+                cp = True
+                fn = dumps(self.fn)
+            else:
+                cp = False
+                fn = self.fn
+            return (self.work_id, self.args, self.kwargs, fn, cp)
+
+        def __setstate__(self, state):
+            self.work_id, self.args, self.kwargs, self.fn, cp = state
+            if cp:
+                from cloudpickle import loads
+                self.fn = loads(self.fn)
+
+    except (ImportError, AssertionError) as e:
+        pass
+
+
+class _SafeQueue(Queue):
+    """Safe Queue set exception to the future object linked to a job"""
+    def __init__(self, max_size=0, ctx=None, pending_work_items=None,
+                 running_work_items=None, thread_wakeup=None, reducers=None):
+        self.thread_wakeup = thread_wakeup
+        self.pending_work_items = pending_work_items
+        self.running_work_items = running_work_items
+        super(_SafeQueue, self).__init__(max_size, reducers=reducers, ctx=ctx)
+
+    def _on_queue_feeder_error(self, e, obj):
+        if isinstance(obj, _CallItem):
+            # fromat traceback only on python3
+            pickling_error = PicklingError(
+                "Could not pickle the task to send it to the workers.")
+            tb = traceback.format_exception(
+                type(e), e, getattr(e, "__traceback__", None))
+            pickling_error.__cause__ = _RemoteTraceback(
+                '\n"""\n{}"""'.format(''.join(tb)))
+            work_item = self.pending_work_items.pop(obj.work_id, None)
+            self.running_work_items.remove(obj.work_id)
+            # work_item can be None if another process terminated. In this
+            # case, the queue_manager_thread fails all work_items with
+            # BrokenProcessPool
+            if work_item is not None:
+                work_item.future.set_exception(pickling_error)
+                del work_item
+            self.thread_wakeup.wakeup()
+        else:
+            super()._on_queue_feeder_error(e, obj)
+
+
+def _get_chunks(chunksize, *iterables):
+    """ Iterates over zip()ed iterables in chunks. """
+    if sys.version_info < (3, 3):
+        it = itertools.izip(*iterables)
+    else:
+        it = zip(*iterables)
+    while True:
+        chunk = tuple(itertools.islice(it, chunksize))
+        if not chunk:
+            return
+        yield chunk
+
+
+def _process_chunk(fn, chunk):
+    """ Processes a chunk of an iterable passed to map.
+
+    Runs the function passed to map() on a chunk of the
+    iterable passed to map.
+
+    This function is run in a separate process.
+
+    """
+    return [fn(*args) for args in chunk]
+
+
+def _sendback_result(result_queue, work_id, result=None, exception=None):
+    """Safely send back the given result or exception"""
+    try:
+        result_queue.put(_ResultItem(work_id, result=result,
+                                     exception=exception))
+    except BaseException as e:
+        exc = _ExceptionWithTraceback(e, getattr(e, "__traceback__", None))
+        result_queue.put(_ResultItem(work_id, exception=exc))
+
+
+def _process_worker(call_queue, result_queue, initializer, initargs,
+                    processes_management_lock, timeout, worker_exit_lock,
+                    current_depth):
+    """Evaluates calls from call_queue and places the results in result_queue.
+
+    This worker is run in a separate process.
+
+    Args:
+        call_queue: A ctx.Queue of _CallItems that will be read and
+            evaluated by the worker.
+        result_queue: A ctx.Queue of _ResultItems that will written
+            to by the worker.
+        initializer: A callable initializer, or None
+        initargs: A tuple of args for the initializer
+        process_management_lock: A ctx.Lock avoiding worker timeout while some
+            workers are being spawned.
+        timeout: maximum time to wait for a new item in the call_queue. If that
+            time is expired, the worker will shutdown.
+        worker_exit_lock: Lock to avoid flagging the executor as broken on
+            workers timeout.
+        current_depth: Nested parallelism level, to avoid infinite spawning.
+    """
+    if initializer is not None:
+        try:
+            initializer(*initargs)
+        except BaseException:
+            _base.LOGGER.critical('Exception in initializer:', exc_info=True)
+            # The parent will notice that the process stopped and
+            # mark the pool broken
+            return
+
+    # set the global _CURRENT_DEPTH mechanism to limit recursive call
+    global _CURRENT_DEPTH
+    _CURRENT_DEPTH = current_depth
+    _REFERENCE_PROCESS_SIZE = None
+    _LAST_MEMORY_CHECK = None
+    pid = os.getpid()
+
+    mp.util.debug('Worker started with timeout=%s' % timeout)
+    while True:
+        try:
+            call_item = call_queue.get(block=True, timeout=timeout)
+            if call_item is None:
+                mp.util.info("Shutting down worker on sentinel")
+        except queue.Empty:
+            mp.util.info("Shutting down worker after timeout %0.3fs"
+                         % timeout)
+            if processes_management_lock.acquire(block=False):
+                processes_management_lock.release()
+                call_item = None
+            else:
+                mp.util.info("Could not acquire processes_management_lock")
+                continue
+        except BaseException as e:
+            traceback.print_exc()
+            sys.exit(1)
+        if call_item is None:
+            # Notify queue management thread about clean worker shutdown
+            result_queue.put(pid)
+            with worker_exit_lock:
+                return
+        try:
+            r = call_item.fn(*call_item.args, **call_item.kwargs)
+        except BaseException as e:
+            exc = _ExceptionWithTraceback(e, getattr(e, "__traceback__", None))
+            result_queue.put(_ResultItem(call_item.work_id, exception=exc))
+        else:
+            _sendback_result(result_queue, call_item.work_id, result=r)
+
+        # Free the resource as soon as possible, to avoid holding onto
+        # open files or shared memory that is not needed anymore
+        del call_item
+
+        if _get_memory_usage is not None:
+            if _REFERENCE_PROCESS_SIZE is None:
+                # Make reference measurement after the first call
+                _REFERENCE_PROCESS_SIZE = _get_memory_usage(pid, force_gc=True)
+                _LAST_MEMORY_CHECK = time()
+                continue
+            if time() - _LAST_MEMORY_CHECK > _MEMORY_CHECK_DELAY:
+                mem_usage = _get_memory_usage(pid)
+                _LAST_MEMORY_CHECK = time()
+                if mem_usage - _REFERENCE_PROCESS_SIZE < _MAX_MEMORY_LEAK_SIZE:
+                    # Memory usage stays within bounds: everything is fine.
+                    continue
+
+                # Check again memory usage; this time take the measurement
+                # after a forced garbage collection to break any reference
+                # cycles.
+                mem_usage = _get_memory_usage(pid, force_gc=True)
+                _LAST_MEMORY_CHECK = time()
+                if mem_usage - _REFERENCE_PROCESS_SIZE < _MAX_MEMORY_LEAK_SIZE:
+                    # The GC managed to free the memory: everything is fine.
+                    continue
+
+                # The process is leaking memory: let the master process
+                # know that we need to start a new worker.
+                mp.util.info("Memory leak detected: shutting down worker")
+                result_queue.put(pid)
+                with worker_exit_lock:
+                    return
+
+
+def _add_call_item_to_queue(pending_work_items,
+                            running_work_items,
+                            work_ids,
+                            call_queue):
+    """Fills call_queue with _WorkItems from pending_work_items.
+
+    This function never blocks.
+
+    Args:
+        pending_work_items: A dict mapping work ids to _WorkItems e.g.
+            {5: <_WorkItem...>, 6: <_WorkItem...>, ...}
+        work_ids: A queue.Queue of work ids e.g. Queue([5, 6, ...]). Work ids
+            are consumed and the corresponding _WorkItems from
+            pending_work_items are transformed into _CallItems and put in
+            call_queue.
+        call_queue: A ctx.Queue that will be filled with _CallItems
+            derived from _WorkItems.
+    """
+    while True:
+        if call_queue.full():
+            return
+        try:
+            work_id = work_ids.get(block=False)
+        except queue.Empty:
+            return
+        else:
+            work_item = pending_work_items[work_id]
+
+            if work_item.future.set_running_or_notify_cancel():
+                running_work_items += [work_id]
+                call_queue.put(_CallItem(work_id,
+                                         work_item.fn,
+                                         work_item.args,
+                                         work_item.kwargs),
+                               block=True)
+            else:
+                del pending_work_items[work_id]
+                continue
+
+
+def _queue_management_worker(executor_reference,
+                             executor_flags,
+                             processes,
+                             pending_work_items,
+                             running_work_items,
+                             work_ids_queue,
+                             call_queue,
+                             result_queue,
+                             thread_wakeup,
+                             processes_management_lock):
+    """Manages the communication between this process and the worker processes.
+
+    This function is run in a local thread.
+
+    Args:
+        executor_reference: A weakref.ref to the ProcessPoolExecutor that owns
+            this thread. Used to determine if the ProcessPoolExecutor has been
+            garbage collected and that this function can exit.
+        executor_flags: A ExecutorFlags holding internal states of the
+            ProcessPoolExecutor. It permits to know if the executor is broken
+            even the object has been gc.
+        process: A list of the ctx.Process instances used as
+            workers.
+        pending_work_items: A dict mapping work ids to _WorkItems e.g.
+            {5: <_WorkItem...>, 6: <_WorkItem...>, ...}
+        work_ids_queue: A queue.Queue of work ids e.g. Queue([5, 6, ...]).
+        call_queue: A ctx.Queue that will be filled with _CallItems
+            derived from _WorkItems for processing by the process workers.
+        result_queue: A ctx.SimpleQueue of _ResultItems generated by the
+            process workers.
+        thread_wakeup: A _ThreadWakeup to allow waking up the
+            queue_manager_thread from the main Thread and avoid deadlocks
+            caused by permanently locked queues.
+    """
+    executor = None
+
+    def is_shutting_down():
+        # No more work items can be added if:
+        #   - The interpreter is shutting down OR
+        #   - The executor that own this worker is not broken AND
+        #        * The executor that owns this worker has been collected OR
+        #        * The executor that owns this worker has been shutdown.
+        # If the executor is broken, it should be detected in the next loop.
+        return (_global_shutdown or
+                ((executor is None or executor_flags.shutdown)
+                 and not executor_flags.broken))
+
+    def shutdown_all_workers():
+        mp.util.debug("queue management thread shutting down")
+        executor_flags.flag_as_shutting_down()
+        # Create a list to avoid RuntimeError due to concurrent modification of
+        # processes. nb_children_alive is thus an upper bound. Also release the
+        # processes' _worker_exit_lock to accelerate the shutdown procedure, as
+        # there is no need for hand-shake here.
+        with processes_management_lock:
+            n_children_alive = 0
+            for p in list(processes.values()):
+                p._worker_exit_lock.release()
+                n_children_alive += 1
+        n_children_to_stop = n_children_alive
+        n_sentinels_sent = 0
+        # Send the right number of sentinels, to make sure all children are
+        # properly terminated.
+        while n_sentinels_sent < n_children_to_stop and n_children_alive > 0:
+            for i in range(n_children_to_stop - n_sentinels_sent):
+                try:
+                    call_queue.put_nowait(None)
+                    n_sentinels_sent += 1
+                except Full:
+                    break
+            with processes_management_lock:
+                n_children_alive = sum(
+                    p.is_alive() for p in list(processes.values())
+                )
+
+        # Release the queue's resources as soon as possible. Flag the feeder
+        # thread for clean exit to avoid having the crash detection thread flag
+        # the Executor as broken during the shutdown. This is safe as either:
+        #  * We don't need to communicate with the workers anymore
+        #  * There is nothing left in the Queue buffer except None sentinels
+        mp.util.debug("closing call_queue")
+        call_queue.close()
+
+        mp.util.debug("joining processes")
+        # If .join() is not called on the created processes then
+        # some ctx.Queue methods may deadlock on Mac OS X.
+        while processes:
+            _, p = processes.popitem()
+            p.join()
+        mp.util.debug("queue management thread clean shutdown of worker "
+                      "processes: {}".format(list(processes)))
+
+    result_reader = result_queue._reader
+    wakeup_reader = thread_wakeup._reader
+    readers = [result_reader, wakeup_reader]
+
+    while True:
+        _add_call_item_to_queue(pending_work_items,
+                                running_work_items,
+                                work_ids_queue,
+                                call_queue)
+        # Wait for a result to be ready in the result_queue while checking
+        # that all worker processes are still running, or for a wake up
+        # signal send. The wake up signals come either from new tasks being
+        # submitted, from the executor being shutdown/gc-ed, or from the
+        # shutdown of the python interpreter.
+        worker_sentinels = [p.sentinel for p in processes.values()]
+        ready = wait(readers + worker_sentinels)
+
+        broken = ("A process in the executor was terminated abruptly", None)
+        if result_reader in ready:
+            try:
+                result_item = result_reader.recv()
+                broken = None
+            except BaseException as e:
+                tb = getattr(e, "__traceback__", None)
+                if tb is None:
+                    _, _, tb = sys.exc_info()
+                broken = ("A result has failed to un-serialize",
+                          traceback.format_exception(type(e), e, tb))
+        elif wakeup_reader in ready:
+            broken = None
+            result_item = None
+        thread_wakeup.clear()
+        if broken:
+            msg, cause = broken
+            # Mark the process pool broken so that submits fail right now.
+            executor_flags.flag_as_broken(
+                msg + ", the pool is not usable anymore.")
+            bpe = BrokenProcessPool(
+                msg + " while the future was running or pending.")
+            if cause is not None:
+                bpe.__cause__ = _RemoteTraceback(
+                    "\n'''\n{}'''".format(''.join(cause)))
+
+            # All futures in flight must be marked failed
+            for work_id, work_item in pending_work_items.items():
+                work_item.future.set_exception(bpe)
+                # Delete references to object. See issue16284
+                del work_item
+            pending_work_items.clear()
+
+            # Terminate remaining workers forcibly: the queues or their
+            # locks may be in a dirty state and block forever.
+            while processes:
+                _, p = processes.popitem()
+                mp.util.debug('terminate process {}'.format(p.name))
+                try:
+                    recursive_terminate(p)
+                except ProcessLookupError:  # pragma: no cover
+                    pass
+
+            shutdown_all_workers()
+            return
+        if isinstance(result_item, int):
+            # Clean shutdown of a worker using its PID, either on request
+            # by the executor.shutdown method or by the timeout of the worker
+            # itself: we should not mark the executor as broken.
+            with processes_management_lock:
+                p = processes.pop(result_item, None)
+
+            # p can be None is the executor is concurrently shutting down.
+            if p is not None:
+                p._worker_exit_lock.release()
+                p.join()
+                del p
+
+            # Make sure the executor have the right number of worker, even if a
+            # worker timeout while some jobs were submitted. If some work is
+            # pending or there is less processes than running items, we need to
+            # start a new Process and raise a warning.
+            n_pending = len(pending_work_items)
+            n_running = len(running_work_items)
+            if (n_pending - n_running > 0 or n_running > len(processes)):
+                executor = executor_reference()
+                if (executor is not None
+                        and len(processes) < executor._max_workers):
+                    warnings.warn(
+                        "A worker stopped while some jobs were given to the "
+                        "executor. This can be caused by a too short worker "
+                        "timeout or by a memory leak.", UserWarning
+                    )
+                    executor._adjust_process_count()
+                    executor = None
+
+        elif result_item is not None:
+            work_item = pending_work_items.pop(result_item.work_id, None)
+            # work_item can be None if another process terminated
+            if work_item is not None:
+                if result_item.exception:
+                    work_item.future.set_exception(result_item.exception)
+                else:
+                    work_item.future.set_result(result_item.result)
+                # Delete references to object. See issue16284
+                del work_item
+                running_work_items.remove(result_item.work_id)
+            # Delete reference to result_item
+            del result_item
+
+        # Check whether we should start shutting down.
+        executor = executor_reference()
+        # No more work items can be added if:
+        #   - The interpreter is shutting down OR
+        #   - The executor that owns this worker has been collected OR
+        #   - The executor that owns this worker has been shutdown.
+        if is_shutting_down():
+            # bpo-33097: Make sure that the executor is flagged as shutting
+            # down even if it is shutdown by the interpreter exiting.
+            with executor_flags.shutdown_lock:
+                executor_flags.shutdown = True
+            if executor_flags.kill_workers:
+                while pending_work_items:
+                    _, work_item = pending_work_items.popitem()
+                    work_item.future.set_exception(ShutdownExecutorError(
+                        "The Executor was shutdown before this job could "
+                        "complete."))
+                    del work_item
+                # Terminate remaining workers forcibly: the queues or their
+                # locks may be in a dirty state and block forever.
+                while processes:
+                    _, p = processes.popitem()
+                    recursive_terminate(p)
+                shutdown_all_workers()
+                return
+            # Since no new work items can be added, it is safe to shutdown
+            # this thread if there are no pending work items.
+            if not pending_work_items:
+                shutdown_all_workers()
+                return
+        elif executor_flags.broken:
+            return
+        executor = None
+
+
+_system_limits_checked = False
+_system_limited = None
+
+
+def _check_system_limits():
+    global _system_limits_checked, _system_limited
+    if _system_limits_checked:
+        if _system_limited:
+            raise NotImplementedError(_system_limited)
+    _system_limits_checked = True
+    try:
+        nsems_max = os.sysconf("SC_SEM_NSEMS_MAX")
+    except (AttributeError, ValueError):
+        # sysconf not available or setting not available
+        return
+    if nsems_max == -1:
+        # undetermined limit, assume that limit is determined
+        # by available memory only
+        return
+    if nsems_max >= 256:
+        # minimum number of semaphores available
+        # according to POSIX
+        return
+    _system_limited = ("system provides too few semaphores (%d available, "
+                       "256 necessary)" % nsems_max)
+    raise NotImplementedError(_system_limited)
+
+
+def _chain_from_iterable_of_lists(iterable):
+    """
+    Specialized implementation of itertools.chain.from_iterable.
+    Each item in *iterable* should be a list.  This function is
+    careful not to keep references to yielded objects.
+    """
+    for element in iterable:
+        element.reverse()
+        while element:
+            yield element.pop()
+
+
+def _check_max_depth(context):
+    # Limit the maxmal recursion level
+    global _CURRENT_DEPTH
+    if context.get_start_method() == "fork" and _CURRENT_DEPTH > 0:
+        raise LokyRecursionError(
+            "Could not spawn extra nested processes at depth superior to "
+            "MAX_DEPTH=1. It is not possible to increase this limit when "
+            "using the 'fork' start method.")
+
+    if 0 < MAX_DEPTH and _CURRENT_DEPTH + 1 > MAX_DEPTH:
+        raise LokyRecursionError(
+            "Could not spawn extra nested processes at depth superior to "
+            "MAX_DEPTH={}. If this is intendend, you can change this limit "
+            "with the LOKY_MAX_DEPTH environment variable.".format(MAX_DEPTH))
+
+
+class LokyRecursionError(RuntimeError):
+    """Raised when a process try to spawn too many levels of nested processes.
+    """
+
+
+class BrokenProcessPool(_BPPException):
+    """
+    Raised when a process in a ProcessPoolExecutor terminated abruptly
+    while a future was in the running state.
+    """
+
+
+# Alias for backward compat (for code written for loky 1.1.4 and earlier). Do
+# not use in new code.
+BrokenExecutor = BrokenProcessPool
+
+
+class ShutdownExecutorError(RuntimeError):
+
+    """
+    Raised when a ProcessPoolExecutor is shutdown while a future was in the
+    running or pending state.
+    """
+
+
+class ProcessPoolExecutor(_base.Executor):
+
+    _at_exit = None
+
+    def __init__(self, max_workers=None, job_reducers=None,
+                 result_reducers=None, timeout=None, context=None,
+                 initializer=None, initargs=()):
+        """Initializes a new ProcessPoolExecutor instance.
+
+        Args:
+            max_workers: int, optional (default: cpu_count())
+                The maximum number of processes that can be used to execute the
+                given calls. If None or not given then as many worker processes
+                will be created as the number of CPUs the current process
+                can use.
+            job_reducers, result_reducers: dict(type: reducer_func)
+                Custom reducer for pickling the jobs and the results from the
+                Executor. If only `job_reducers` is provided, `result_reducer`
+                will use the same reducers
+            timeout: int, optional (default: None)
+                Idle workers exit after timeout seconds. If a new job is
+                submitted after the timeout, the executor will start enough
+                new Python processes to make sure the pool of workers is full.
+            context: A multiprocessing context to launch the workers. This
+                object should provide SimpleQueue, Queue and Process.
+            initializer: An callable used to initialize worker processes.
+            initargs: A tuple of arguments to pass to the initializer.
+        """
+        _check_system_limits()
+
+        if max_workers is None:
+            self._max_workers = cpu_count()
+        else:
+            if max_workers <= 0:
+                raise ValueError("max_workers must be greater than 0")
+            self._max_workers = max_workers
+
+        if context is None:
+            context = get_context()
+        self._context = context
+
+        if initializer is not None and not callable(initializer):
+            raise TypeError("initializer must be a callable")
+        self._initializer = initializer
+        self._initargs = initargs
+
+        _check_max_depth(self._context)
+
+        if result_reducers is None:
+            result_reducers = job_reducers
+
+        # Timeout
+        self._timeout = timeout
+
+        # Internal variables of the ProcessPoolExecutor
+        self._processes = {}
+        self._queue_count = 0
+        self._pending_work_items = {}
+        self._running_work_items = []
+        self._work_ids = queue.Queue()
+        self._processes_management_lock = self._context.Lock()
+        self._queue_management_thread = None
+
+        # _ThreadWakeup is a communication channel used to interrupt the wait
+        # of the main loop of queue_manager_thread from another thread (e.g.
+        # when calling executor.submit or executor.shutdown). We do not use the
+        # _result_queue to send the wakeup signal to the queue_manager_thread
+        # as it could result in a deadlock if a worker process dies with the
+        # _result_queue write lock still acquired.
+        self._queue_management_thread_wakeup = _ThreadWakeup()
+
+        # Flag to hold the state of the Executor. This permits to introspect
+        # the Executor state even once it has been garbage collected.
+        self._flags = _ExecutorFlags()
+
+        # Finally setup the queues for interprocess communication
+        self._setup_queues(job_reducers, result_reducers)
+
+        mp.util.debug('ProcessPoolExecutor is setup')
+
+    def _setup_queues(self, job_reducers, result_reducers, queue_size=None):
+        # Make the call queue slightly larger than the number of processes to
+        # prevent the worker processes from idling. But don't make it too big
+        # because futures in the call queue cannot be cancelled.
+        if queue_size is None:
+            queue_size = 2 * self._max_workers + EXTRA_QUEUED_CALLS
+        self._call_queue = _SafeQueue(
+            max_size=queue_size, pending_work_items=self._pending_work_items,
+            running_work_items=self._running_work_items,
+            thread_wakeup=self._queue_management_thread_wakeup,
+            reducers=job_reducers, ctx=self._context)
+        # Killed worker processes can produce spurious "broken pipe"
+        # tracebacks in the queue's own worker thread. But we detect killed
+        # processes anyway, so silence the tracebacks.
+        self._call_queue._ignore_epipe = True
+
+        self._result_queue = SimpleQueue(reducers=result_reducers,
+                                         ctx=self._context)
+
+    def _start_queue_management_thread(self):
+        if self._queue_management_thread is None:
+            mp.util.debug('_start_queue_management_thread called')
+
+            # When the executor gets garbarge collected, the weakref callback
+            # will wake up the queue management thread so that it can terminate
+            # if there is no pending work item.
+            def weakref_cb(_,
+                           thread_wakeup=self._queue_management_thread_wakeup):
+                mp.util.debug('Executor collected: triggering callback for'
+                              ' QueueManager wakeup')
+                thread_wakeup.wakeup()
+
+            # Start the processes so that their sentinels are known.
+            self._queue_management_thread = threading.Thread(
+                target=_queue_management_worker,
+                args=(weakref.ref(self, weakref_cb),
+                      self._flags,
+                      self._processes,
+                      self._pending_work_items,
+                      self._running_work_items,
+                      self._work_ids,
+                      self._call_queue,
+                      self._result_queue,
+                      self._queue_management_thread_wakeup,
+                      self._processes_management_lock),
+                name="QueueManagerThread")
+            self._queue_management_thread.daemon = True
+            self._queue_management_thread.start()
+
+            # register this executor in a mechanism that ensures it will wakeup
+            # when the interpreter is exiting.
+            _threads_wakeups[self._queue_management_thread] = \
+                self._queue_management_thread_wakeup
+
+            global process_pool_executor_at_exit
+            if process_pool_executor_at_exit is None:
+                # Ensure that the _python_exit function will be called before
+                # the multiprocessing.Queue._close finalizers which have an
+                # exitpriority of 10.
+                process_pool_executor_at_exit = mp.util.Finalize(
+                    None, _python_exit, exitpriority=20)
+
+    def _adjust_process_count(self):
+        for _ in range(len(self._processes), self._max_workers):
+            worker_exit_lock = self._context.BoundedSemaphore(1)
+            worker_exit_lock.acquire()
+            p = self._context.Process(
+                target=_process_worker,
+                args=(self._call_queue,
+                      self._result_queue,
+                      self._initializer,
+                      self._initargs,
+                      self._processes_management_lock,
+                      self._timeout,
+                      worker_exit_lock,
+                      _CURRENT_DEPTH + 1))
+            p._worker_exit_lock = worker_exit_lock
+            p.start()
+            self._processes[p.pid] = p
+        mp.util.debug('Adjust process count : {}'.format(self._processes))
+
+    def _ensure_executor_running(self):
+        """ensures all workers and management thread are running
+        """
+        with self._processes_management_lock:
+            if len(self._processes) != self._max_workers:
+                self._adjust_process_count()
+            self._start_queue_management_thread()
+
+    def submit(self, fn, *args, **kwargs):
+        with self._flags.shutdown_lock:
+            if self._flags.broken:
+                raise BrokenProcessPool(self._flags.broken)
+            if self._flags.shutdown:
+                raise ShutdownExecutorError(
+                    'cannot schedule new futures after shutdown')
+
+            # Cannot submit a new calls once the interpreter is shutting down.
+            # This check avoids spawning new processes at exit.
+            if _global_shutdown:
+                raise RuntimeError('cannot schedule new futures after '
+                                   'interpreter shutdown')
+
+            f = _base.Future()
+            w = _WorkItem(f, fn, args, kwargs)
+
+            self._pending_work_items[self._queue_count] = w
+            self._work_ids.put(self._queue_count)
+            self._queue_count += 1
+            # Wake up queue management thread
+            self._queue_management_thread_wakeup.wakeup()
+
+            self._ensure_executor_running()
+            return f
+    submit.__doc__ = _base.Executor.submit.__doc__
+
+    def map(self, fn, *iterables, **kwargs):
+        """Returns an iterator equivalent to map(fn, iter).
+
+        Args:
+            fn: A callable that will take as many arguments as there are
+                passed iterables.
+            timeout: The maximum number of seconds to wait. If None, then there
+                is no limit on the wait time.
+            chunksize: If greater than one, the iterables will be chopped into
+                chunks of size chunksize and submitted to the process pool.
+                If set to one, the items in the list will be sent one at a
+                time.
+
+        Returns:
+            An iterator equivalent to: map(func, *iterables) but the calls may
+            be evaluated out-of-order.
+
+        Raises:
+            TimeoutError: If the entire result iterator could not be generated
+                before the given timeout.
+            Exception: If fn(*args) raises for any values.
+        """
+        timeout = kwargs.get('timeout', None)
+        chunksize = kwargs.get('chunksize', 1)
+        if chunksize < 1:
+            raise ValueError("chunksize must be >= 1.")
+
+        results = super(ProcessPoolExecutor, self).map(
+            partial(_process_chunk, fn), _get_chunks(chunksize, *iterables),
+            timeout=timeout)
+        return _chain_from_iterable_of_lists(results)
+
+    def shutdown(self, wait=True, kill_workers=False):
+        mp.util.debug('shutting down executor %s' % self)
+
+        self._flags.flag_as_shutting_down(kill_workers)
+        qmt = self._queue_management_thread
+        qmtw = self._queue_management_thread_wakeup
+        if qmt:
+            self._queue_management_thread = None
+            if qmtw:
+                self._queue_management_thread_wakeup = None
+            # Wake up queue management thread
+            if qmtw is not None:
+                try:
+                    qmtw.wakeup()
+                except OSError:
+                    # Can happen in case of concurrent calls to shutdown.
+                    pass
+            if wait:
+                qmt.join()
+
+        cq = self._call_queue
+        if cq:
+            self._call_queue = None
+            cq.close()
+            if wait:
+                cq.join_thread()
+        self._result_queue = None
+        self._processes_management_lock = None
+
+        if qmtw:
+            try:
+                qmtw.close()
+            except OSError:
+                # Can happen in case of concurrent calls to shutdown.
+                pass
+    shutdown.__doc__ = _base.Executor.shutdown.__doc__
diff --git a/sklearn/externals/joblib/externals/loky/reusable_executor.py b/sklearn/externals/joblib/externals/loky/reusable_executor.py
new file mode 100644
index 000000000000..30b217fd4113
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/reusable_executor.py
@@ -0,0 +1,205 @@
+###############################################################################
+# Reusable ProcessPoolExecutor
+#
+# author: Thomas Moreau and Olivier Grisel
+#
+import time
+import warnings
+import threading
+import multiprocessing as mp
+
+from .process_executor import ProcessPoolExecutor, EXTRA_QUEUED_CALLS
+from .backend.context import cpu_count
+from .backend import get_context
+
+__all__ = ['get_reusable_executor']
+
+# Python 2 compat helper
+STRING_TYPE = type("")
+
+# Singleton executor and id management
+_executor_lock = threading.RLock()
+_next_executor_id = 0
+_executor = None
+_executor_args = None
+
+
+def _get_next_executor_id():
+    """Ensure that each successive executor instance has a unique, monotonic id.
+
+    The purpose of this monotonic id is to help debug and test automated
+    instance creation.
+    """
+    global _next_executor_id
+    with _executor_lock:
+        executor_id = _next_executor_id
+        _next_executor_id += 1
+        return executor_id
+
+
+def get_reusable_executor(max_workers=None, context=None, timeout=10,
+                          kill_workers=False, reuse="auto",
+                          job_reducers=None, result_reducers=None,
+                          initializer=None, initargs=()):
+    """Return the current ReusableExectutor instance.
+
+    Start a new instance if it has not been started already or if the previous
+    instance was left in a broken state.
+
+    If the previous instance does not have the requested number of workers, the
+    executor is dynamically resized to adjust the number of workers prior to
+    returning.
+
+    Reusing a singleton instance spares the overhead of starting new worker
+    processes and importing common python packages each time.
+
+    ``max_workers`` controls the maximum number of tasks that can be running in
+    parallel in worker processes. By default this is set to the number of
+    CPUs on the host.
+
+    Setting ``timeout`` (in seconds) makes idle workers automatically shutdown
+    so as to release system resources. New workers are respawn upon submission
+    of new tasks so that ``max_workers`` are available to accept the newly
+    submitted tasks. Setting ``timeout`` to around 100 times the time required
+    to spawn new processes and import packages in them (on the order of 100ms)
+    ensures that the overhead of spawning workers is negligible.
+
+    Setting ``kill_workers=True`` makes it possible to forcibly interrupt
+    previously spawned jobs to get a new instance of the reusable executor
+    with new constructor argument values.
+
+    The ``job_reducers`` and ``result_reducers`` are used to customize the
+    pickling of tasks and results send to the executor.
+
+    When provided, the ``initializer`` is run first in newly spawned
+    processes with argument ``initargs``.
+    """
+    with _executor_lock:
+        global _executor, _executor_kwargs
+        executor = _executor
+
+        if max_workers is None:
+            if reuse is True and executor is not None:
+                max_workers = executor._max_workers
+            else:
+                max_workers = cpu_count()
+        elif max_workers <= 0:
+            raise ValueError(
+                "max_workers must be greater than 0, got {}."
+                .format(max_workers))
+
+        if isinstance(context, STRING_TYPE):
+            context = get_context(context)
+        if context is not None and context.get_start_method() == "fork":
+            raise ValueError("Cannot use reusable executor with the 'fork' "
+                             "context")
+
+        kwargs = dict(context=context, timeout=timeout,
+                      job_reducers=job_reducers,
+                      result_reducers=result_reducers,
+                      initializer=initializer, initargs=initargs)
+        if executor is None:
+            mp.util.debug("Create a executor with max_workers={}."
+                          .format(max_workers))
+            executor_id = _get_next_executor_id()
+            _executor_kwargs = kwargs
+            _executor = executor = _ReusablePoolExecutor(
+                _executor_lock, max_workers=max_workers,
+                executor_id=executor_id, **kwargs)
+        else:
+            if reuse == 'auto':
+                reuse = kwargs == _executor_kwargs
+            if (executor._flags.broken or executor._flags.shutdown
+                    or not reuse):
+                if executor._flags.broken:
+                    reason = "broken"
+                elif executor._flags.shutdown:
+                    reason = "shutdown"
+                else:
+                    reason = "arguments have changed"
+                mp.util.debug(
+                    "Creating a new executor with max_workers={} as the "
+                    "previous instance cannot be reused ({})."
+                    .format(max_workers, reason))
+                executor.shutdown(wait=True, kill_workers=kill_workers)
+                _executor = executor = _executor_kwargs = None
+                # Recursive call to build a new instance
+                return get_reusable_executor(max_workers=max_workers,
+                                             **kwargs)
+            else:
+                mp.util.debug("Reusing existing executor with max_workers={}."
+                              .format(executor._max_workers))
+                executor._resize(max_workers)
+
+    return executor
+
+
+class _ReusablePoolExecutor(ProcessPoolExecutor):
+    def __init__(self, submit_resize_lock, max_workers=None, context=None,
+                 timeout=None, executor_id=0, job_reducers=None,
+                 result_reducers=None, initializer=None, initargs=()):
+        super(_ReusablePoolExecutor, self).__init__(
+            max_workers=max_workers, context=context, timeout=timeout,
+            job_reducers=job_reducers, result_reducers=result_reducers,
+            initializer=initializer, initargs=initargs)
+        self.executor_id = executor_id
+        self._submit_resize_lock = submit_resize_lock
+
+    def submit(self, fn, *args, **kwargs):
+        with self._submit_resize_lock:
+            return super(_ReusablePoolExecutor, self).submit(
+                fn, *args, **kwargs)
+
+    def _resize(self, max_workers):
+        with self._submit_resize_lock:
+            if max_workers is None:
+                raise ValueError("Trying to resize with max_workers=None")
+            elif max_workers == self._max_workers:
+                return
+
+            if self._queue_management_thread is None:
+                # If the queue_management_thread has not been started
+                # then no processes have been spawned and we can just
+                # update _max_workers and return
+                self._max_workers = max_workers
+                return
+
+            self._wait_job_completion()
+
+            # Some process might have returned due to timeout so check how many
+            # children are still alive. Use the _process_management_lock to
+            # ensure that no process are spawned or timeout during the resize.
+            with self._processes_management_lock:
+                processes = list(self._processes.values())
+                nb_children_alive = sum(p.is_alive() for p in processes)
+                self._max_workers = max_workers
+                for _ in range(max_workers, nb_children_alive):
+                    self._call_queue.put(None)
+            while (len(self._processes) > max_workers
+                   and not self._flags.broken):
+                time.sleep(1e-3)
+
+            self._adjust_process_count()
+            processes = list(self._processes.values())
+            while not all([p.is_alive() for p in processes]):
+                time.sleep(1e-3)
+
+    def _wait_job_completion(self):
+        """Wait for the cache to be empty before resizing the pool."""
+        # Issue a warning to the user about the bad effect of this usage.
+        if len(self._pending_work_items) > 0:
+            warnings.warn("Trying to resize an executor with running jobs: "
+                          "waiting for jobs completion before resizing.",
+                          UserWarning)
+            mp.util.debug("Executor {} waiting for jobs completion before"
+                          " resizing".format(self.executor_id))
+        # Wait for the completion of the jobs
+        while len(self._pending_work_items) > 0:
+            time.sleep(1e-3)
+
+    def _setup_queues(self, job_reducers, result_reducers):
+        # As this executor can be resized, use a large queue size to avoid
+        # underestimating capacity and introducing overhead
+        queue_size = 2 * cpu_count() + EXTRA_QUEUED_CALLS
+        super(_ReusablePoolExecutor, self)._setup_queues(
+            job_reducers, result_reducers, queue_size=queue_size)
diff --git a/sklearn/externals/_joblib/format_stack.py b/sklearn/externals/joblib/format_stack.py
similarity index 99%
rename from sklearn/externals/_joblib/format_stack.py
rename to sklearn/externals/joblib/format_stack.py
index 4984ebb08132..949ac7d9575c 100644
--- a/sklearn/externals/_joblib/format_stack.py
+++ b/sklearn/externals/joblib/format_stack.py
@@ -355,7 +355,7 @@ def format_exc(etype, evalue, etb, context=5, tb_offset=0):
     # Get (safely) a string form of the exception info
     try:
         etype_str, evalue_str = map(str, (etype, evalue))
-    except:
+    except BaseException:
         # User exception is improperly defined.
         etype, evalue = str, sys.exc_info()[:2]
         etype_str, evalue_str = map(str, (etype, evalue))
diff --git a/sklearn/externals/_joblib/func_inspect.py b/sklearn/externals/joblib/func_inspect.py
similarity index 93%
rename from sklearn/externals/_joblib/func_inspect.py
rename to sklearn/externals/joblib/func_inspect.py
index 86e3a79bb7f5..e9320b43f182 100644
--- a/sklearn/externals/_joblib/func_inspect.py
+++ b/sklearn/externals/joblib/func_inspect.py
@@ -11,12 +11,17 @@
 import warnings
 import re
 import os
+import collections
 
 from ._compat import _basestring
 from .logger import pformat
 from ._memory_helpers import open_py_source
 from ._compat import PY3_OR_LATER
 
+full_argspec_fields = ('args varargs varkw defaults kwonlyargs '
+                       'kwonlydefaults annotations')
+full_argspec_type = collections.namedtuple('FullArgSpec', full_argspec_fields)
+
 
 def get_func_code(func):
     """ Attempts to retrieve a reliable function code hash.
@@ -169,18 +174,13 @@ def getfullargspec(func):
         return inspect.getfullargspec(func)
     except AttributeError:
         arg_spec = inspect.getargspec(func)
-        import collections
-        tuple_fields = ('args varargs varkw defaults kwonlyargs '
-                        'kwonlydefaults annotations')
-        tuple_type = collections.namedtuple('FullArgSpec', tuple_fields)
-
-        return tuple_type(args=arg_spec.args,
-                          varargs=arg_spec.varargs,
-                          varkw=arg_spec.keywords,
-                          defaults=arg_spec.defaults,
-                          kwonlyargs=[],
-                          kwonlydefaults=None,
-                          annotations={})
+        return full_argspec_type(args=arg_spec.args,
+                                 varargs=arg_spec.varargs,
+                                 varkw=arg_spec.keywords,
+                                 defaults=arg_spec.defaults,
+                                 kwonlyargs=[],
+                                 kwonlydefaults=None,
+                                 annotations={})
 
 
 def _signature_str(function_name, arg_spec):
@@ -240,8 +240,10 @@ def filter_args(func, ignore_lst, args=(), kwargs=dict()):
     arg_spec = getfullargspec(func)
     arg_names = arg_spec.args + arg_spec.kwonlyargs
     arg_defaults = arg_spec.defaults or ()
-    arg_defaults = arg_defaults + tuple(arg_spec.kwonlydefaults[k]
-                                        for k in arg_spec.kwonlyargs)
+    if arg_spec.kwonlydefaults:
+        arg_defaults = arg_defaults + tuple(arg_spec.kwonlydefaults[k]
+                                            for k in arg_spec.kwonlyargs
+                                            if k in arg_spec.kwonlydefaults)
     arg_varargs = arg_spec.varargs
     arg_varkw = arg_spec.varkw
 
diff --git a/sklearn/externals/_joblib/hashing.py b/sklearn/externals/joblib/hashing.py
similarity index 100%
rename from sklearn/externals/_joblib/hashing.py
rename to sklearn/externals/joblib/hashing.py
diff --git a/sklearn/externals/_joblib/logger.py b/sklearn/externals/joblib/logger.py
similarity index 98%
rename from sklearn/externals/_joblib/logger.py
rename to sklearn/externals/joblib/logger.py
index 9721512b6dad..f30efef8535d 100644
--- a/sklearn/externals/_joblib/logger.py
+++ b/sklearn/externals/joblib/logger.py
@@ -81,8 +81,7 @@ def debug(self, msg):
         logging.debug("[%s]: %s" % (self, msg))
 
     def format(self, obj, indent=0):
-        """ Return the formatted representation of the object.
-        """
+        """Return the formatted representation of the object."""
         return pformat(obj, indent=indent, depth=self.depth)
 
 
diff --git a/sklearn/externals/_joblib/memory.py b/sklearn/externals/joblib/memory.py
similarity index 53%
rename from sklearn/externals/_joblib/memory.py
rename to sklearn/externals/joblib/memory.py
index 14d7552535bb..ae187950bcfa 100644
--- a/sklearn/externals/_joblib/memory.py
+++ b/sklearn/externals/joblib/memory.py
@@ -11,7 +11,6 @@
 
 from __future__ import with_statement
 import os
-import shutil
 import time
 import pydoc
 import re
@@ -19,13 +18,7 @@
 import traceback
 import warnings
 import inspect
-import json
 import weakref
-import io
-import operator
-import collections
-import datetime
-import threading
 
 # Local imports
 from . import hashing
@@ -34,15 +27,11 @@
 from .func_inspect import format_signature
 from ._memory_helpers import open_py_source
 from .logger import Logger, format_time, pformat
-from . import numpy_pickle
-from .disk import mkdirp, rm_subdirs, memstr_to_bytes
 from ._compat import _basestring, PY3_OR_LATER
-from .backports import concurrency_safe_rename
+from ._store_backends import StoreBackendBase, FileSystemStoreBackend
 
-FIRST_LINE_TEXT = "# first line:"
 
-CacheItemInfo = collections.namedtuple('CacheItemInfo',
-                                       'path size last_access')
+FIRST_LINE_TEXT = "# first line:"
 
 # TODO: The following object should have a data store object as a sub
 # object, and the interface to persist and query should be separated in
@@ -75,141 +64,106 @@ class JobLibCollisionWarning(UserWarning):
     """
 
 
-def _get_func_fullname(func):
-    """Compute the part of part associated with a function.
+_STORE_BACKENDS = {'local': FileSystemStoreBackend}
+
+
+def register_store_backend(backend_name, backend):
+    """Extend available store backends.
+
+    The Memory, MemorizeResult and MemorizeFunc objects are designed to be
+    agnostic to the type of store used behind. By default, the local file
+    system is used but this function gives the possibility to extend joblib's
+    memory pattern with other types of storage such as cloud storage (S3, GCS,
+    OpenStack, HadoopFS, etc) or blob DBs.
+
+    Parameters
+    ----------
+    backend_name: str
+        The name identifying the store backend being registered. For example,
+        'local' is used with FileSystemStoreBackend.
+    backend: StoreBackendBase subclass
+        The name of a class that implements the StoreBackendBase interface.
 
-    See code of_cache_key_to_dir() for details
     """
+    if not isinstance(backend_name, _basestring):
+        raise ValueError("Store backend name should be a string, "
+                         "'{0}' given.".format(backend_name))
+    if backend is None or not issubclass(backend, StoreBackendBase):
+        raise ValueError("Store backend should inherit "
+                         "StoreBackendBase, "
+                         "'{0}' given.".format(backend))
+
+    _STORE_BACKENDS[backend_name] = backend
+
+
+def _store_backend_factory(backend, location, verbose=0, backend_options={}):
+    """Return the correct store object for the given location."""
+    if isinstance(location, StoreBackendBase):
+        return location
+    elif isinstance(location, _basestring):
+        obj = None
+        location = os.path.expanduser(location)
+        # The location is not a local file system, we look in the
+        # registered backends if there's one matching the given backend
+        # name.
+        for backend_key, backend_obj in _STORE_BACKENDS.items():
+            if backend == backend_key:
+                obj = backend_obj()
+
+        # By default, we assume the FileSystemStoreBackend can be used if no
+        # matching backend could be found.
+        if obj is None:
+            raise TypeError('Unknown location {0} or backend {1}'.format(
+                            location, backend))
+
+        # The store backend is configured with the extra named parameters,
+        # some of them are specific to the underlying store backend.
+        obj.configure(location, verbose=verbose,
+                      backend_options=backend_options)
+        return obj
+
+    return None
+
+
+def _get_func_fullname(func):
+    """Compute the part of part associated with a function."""
     modules, funcname = get_func_name(func)
     modules.append(funcname)
     return os.path.join(*modules)
 
 
-def _cache_key_to_dir(cachedir, func, argument_hash):
-    """Compute directory associated with a given cache key.
-
-    func can be a function or a string as returned by _get_func_fullname().
-    """
-    parts = [cachedir]
+def _build_func_identifier(func):
+    """Build a roughly unique identifier for the cached function."""
+    parts = []
     if isinstance(func, _basestring):
         parts.append(func)
     else:
         parts.append(_get_func_fullname(func))
 
-    if argument_hash is not None:
-        parts.append(argument_hash)
+    # We reuse historical fs-like way of building a function identifier
     return os.path.join(*parts)
 
 
-def _load_output(output_dir, func_name, timestamp=None, metadata=None,
-                 mmap_mode=None, verbose=0):
-    """Load output of a computation."""
-    if verbose > 1:
-        signature = ""
-        try:
-            if metadata is not None:
-                args = ", ".join(['%s=%s' % (name, value)
-                                  for name, value
-                                  in metadata['input_args'].items()])
-                signature = "%s(%s)" % (os.path.basename(func_name),
-                                             args)
-            else:
-                signature = os.path.basename(func_name)
-        except KeyError:
-            pass
-
-        if timestamp is not None:
-            t = "% 16s" % format_time(time.time() - timestamp)
-        else:
-            t = ""
-
-        if verbose < 10:
-            print('[Memory]%s: Loading %s...' % (t, str(signature)))
+def _format_load_msg(func_id, args_id, timestamp=None, metadata=None):
+    """ Helper function to format the message when loading the results.
+    """
+    signature = ""
+    try:
+        if metadata is not None:
+            args = ", ".join(['%s=%s' % (name, value)
+                              for name, value
+                              in metadata['input_args'].items()])
+            signature = "%s(%s)" % (os.path.basename(func_id), args)
         else:
-            print('[Memory]%s: Loading %s from %s' % (
-                    t, str(signature), output_dir))
-
-    filename = os.path.join(output_dir, 'output.pkl')
-    if not os.path.isfile(filename):
-        raise KeyError(
-            "Non-existing cache value (may have been cleared).\n"
-            "File %s does not exist" % filename)
-    result = numpy_pickle.load(filename, mmap_mode=mmap_mode)
-
-    return result
-
-
-def _get_cache_items(root_path):
-    """Get cache information for reducing the size of the cache."""
-    cache_items = []
-
-    for dirpath, dirnames, filenames in os.walk(root_path):
-        is_cache_hash_dir = re.match('[a-f0-9]{32}', os.path.basename(dirpath))
-
-        if is_cache_hash_dir:
-            output_filename = os.path.join(dirpath, 'output.pkl')
-            try:
-                last_access = os.path.getatime(output_filename)
-            except OSError:
-                try:
-                    last_access = os.path.getatime(dirpath)
-                except OSError:
-                    # The directory has already been deleted
-                    continue
-
-            last_access = datetime.datetime.fromtimestamp(last_access)
-            try:
-                full_filenames = [os.path.join(dirpath, fn)
-                                  for fn in filenames]
-                dirsize = sum(os.path.getsize(fn)
-                              for fn in full_filenames)
-            except OSError:
-                # Either output_filename or one of the files in
-                # dirpath does not exist any more. We assume this
-                # directory is being cleaned by another process already
-                continue
-
-            cache_items.append(CacheItemInfo(dirpath, dirsize, last_access))
-
-    return cache_items
-
-
-def _get_cache_items_to_delete(root_path, bytes_limit):
-    """Get cache items to delete to keep the cache under a size limit."""
-    if isinstance(bytes_limit, _basestring):
-        bytes_limit = memstr_to_bytes(bytes_limit)
-
-    cache_items = _get_cache_items(root_path)
-    cache_size = sum(item.size for item in cache_items)
-
-    to_delete_size = cache_size - bytes_limit
-    if to_delete_size < 0:
-        return []
-
-    # We want to delete first the cache items that were accessed a
-    # long time ago
-    cache_items.sort(key=operator.attrgetter('last_access'))
-
-    cache_items_to_delete = []
-    size_so_far = 0
-
-    for item in cache_items:
-        if size_so_far > to_delete_size:
-            break
-
-        cache_items_to_delete.append(item)
-        size_so_far += item.size
-
-    return cache_items_to_delete
-
+            signature = os.path.basename(func_id)
+    except KeyError:
+        pass
 
-def concurrency_safe_write(to_write, filename, write_func):
-    """Writes an object into a file in a concurrency-safe way."""
-    thread_id = id(threading.current_thread())
-    temporary_filename = '{}.thread-{}-pid-{}'.format(
-        filename, thread_id, os.getpid())
-    write_func(to_write, temporary_filename)
-    concurrency_safe_rename(temporary_filename, filename)
+    if timestamp is not None:
+        ts_string = "{0: <16}".format(format_time(time.time() - timestamp))
+    else:
+        ts_string = ""
+    return '[Memory]{0}: Loading {1}'.format(ts_string, str(signature))
 
 
 # An in-memory store to avoid looking at the disk-based function
@@ -225,79 +179,86 @@ class MemorizedResult(Logger):
 
     Attributes
     ----------
-    cachedir: string
-        path to root of joblib cache
+    location: str
+        The location of joblib cache. Depends on the store backend used.
 
-    func: function or string
+    func: function or str
         function whose output is cached. The string case is intended only for
         instanciation based on the output of repr() on another instance.
         (namely eval(repr(memorized_instance)) works).
 
-    argument_hash: string
-        hash of the function arguments
+    argument_hash: str
+        hash of the function arguments.
+
+    backend: str
+        Type of store backend for reading/writing cache files.
+        Default is 'local'.
 
     mmap_mode: {None, 'r+', 'r', 'w+', 'c'}
         The memmapping mode used when loading from cache numpy arrays. See
         numpy.load for the meaning of the different values.
 
     verbose: int
-        verbosity level (0 means no message)
+        verbosity level (0 means no message).
 
     timestamp, metadata: string
-        for internal use only
+        for internal use only.
     """
-    def __init__(self, cachedir, func, argument_hash,
+    def __init__(self, location, func, args_id, backend='local',
                  mmap_mode=None, verbose=0, timestamp=None, metadata=None):
         Logger.__init__(self)
-        if isinstance(func, _basestring):
-            self.func = func
-        else:
-            self.func = _get_func_fullname(func)
-        self.argument_hash = argument_hash
-        self.cachedir = cachedir
+        self.func = func
+        self.func_id = _build_func_identifier(func)
+        self.args_id = args_id
+        self.store_backend = _store_backend_factory(backend, location,
+                                                    verbose=verbose)
         self.mmap_mode = mmap_mode
 
-        self._output_dir = _cache_key_to_dir(cachedir, self.func,
-                                             argument_hash)
-
         if metadata is not None:
             self.metadata = metadata
         else:
-            self.metadata = {}
-            # No error is relevant here.
-            try:
-                with open(os.path.join(self._output_dir, 'metadata.json'),
-                          'rb') as f:
-                    self.metadata = json.load(f)
-            except:
-                pass
+            self.metadata = self.store_backend.get_metadata(
+                [self.func_id, self.args_id])
 
         self.duration = self.metadata.get('duration', None)
         self.verbose = verbose
         self.timestamp = timestamp
 
+    @property
+    def argument_hash(self):
+        warnings.warn(
+            "The 'argument_hash' attribute has been deprecated in version "
+            "0.12 and will be removed in version 0.14.\n"
+            "Use `args_id` attribute instead.",
+            DeprecationWarning, stacklevel=2)
+        return self.args_id
+
     def get(self):
         """Read value from cache and return it."""
-        return _load_output(self._output_dir, _get_func_fullname(self.func),
-                            timestamp=self.timestamp,
-                            metadata=self.metadata, mmap_mode=self.mmap_mode,
-                            verbose=self.verbose)
+        if self.verbose:
+            msg = _format_load_msg(self.func_id, self.args_id,
+                                   timestamp=self.timestamp,
+                                   metadata=self.metadata)
+        else:
+            msg = None
+        return self.store_backend.load_item(
+            [self.func_id, self.args_id], msg=msg, verbose=self.verbose)
 
     def clear(self):
         """Clear value from cache"""
-        shutil.rmtree(self._output_dir, ignore_errors=True)
+        self.store_backend.clear_item([self.func_id, self.args_id])
 
     def __repr__(self):
-        return ('{class_name}(cachedir="{cachedir}", func="{func}", '
-                'argument_hash="{argument_hash}")'.format(
-                    class_name=self.__class__.__name__,
-                    cachedir=self.cachedir,
-                    func=self.func,
-                    argument_hash=self.argument_hash
-                    ))
-
+        return ('{class_name}(location="{location}", func="{func}", '
+                'args_id="{args_id}")'
+                .format(class_name=self.__class__.__name__,
+                        location=self.store_backend,
+                        func=self.func,
+                        args_id=self.args_id
+                        ))
     def __reduce__(self):
-        return (self.__class__, (self.cachedir, self.func, self.argument_hash),
+        return (self.__class__,
+                (self.store_backend, self.func, self.args_id),
                 {'mmap_mode': self.mmap_mode})
 
 
@@ -324,10 +285,9 @@ def clear(self):
 
     def __repr__(self):
         if self.valid:
-            return '{class_name}({value})'.format(
-                class_name=self.__class__.__name__,
-                value=pformat(self.value)
-                )
+            return ('{class_name}({value})'
+                    .format(class_name=self.__class__.__name__,
+                            value=pformat(self.value)))
         else:
             return self.__class__.__name__ + ' with no value'
 
@@ -368,10 +328,7 @@ def __reduce__(self):
         return (self.__class__, (self.func,))
 
     def __repr__(self):
-        return '%s(func=%s)' % (
-                    self.__class__.__name__,
-                    self.func
-            )
+        return '{0}(func={1})'.format(self.__class__.__name__, self.func)
 
     def clear(self, warn=True):
         # Argument "warn" is for compatibility with MemorizedFunc.clear
@@ -382,87 +339,73 @@ def clear(self, warn=True):
 # class `MemorizedFunc`
 ###############################################################################
 class MemorizedFunc(Logger):
-    """ Callable object decorating a function for caching its return value
-        each time it is called.
-
-        All values are cached on the filesystem, in a deep directory
-        structure. Methods are provided to inspect the cache or clean it.
+    """Callable object decorating a function for caching its return value
+    each time it is called.
 
-        Attributes
-        ----------
-        func: callable
-            The original, undecorated, function.
+    Methods are provided to inspect the cache or clean it.
 
-        cachedir: string
-            Path to the base cache directory of the memory context.
+    Attributes
+    ----------
+    func: callable
+        The original, undecorated, function.
 
-        ignore: list or None
-            List of variable names to ignore when choosing whether to
-            recompute.
+    location: string
+        The location of joblib cache. Depends on the store backend used.
 
-        mmap_mode: {None, 'r+', 'r', 'w+', 'c'}
-            The memmapping mode used when loading from cache
-            numpy arrays. See numpy.load for the meaning of the different
-            values.
+    backend: str
+        Type of store backend for reading/writing cache files.
+        Default is 'local', in which case the location is the path to a
+        disk storage.
 
-        compress: boolean, or integer
-            Whether to zip the stored data on disk. If an integer is
-            given, it should be between 1 and 9, and sets the amount
-            of compression. Note that compressed arrays cannot be
-            read by memmapping.
+    ignore: list or None
+        List of variable names to ignore when choosing whether to
+        recompute.
 
-        verbose: int, optional
-            The verbosity flag, controls messages that are issued as
-            the function is evaluated.
+    mmap_mode: {None, 'r+', 'r', 'w+', 'c'}
+        The memmapping mode used when loading from cache
+        numpy arrays. See numpy.load for the meaning of the different
+        values.
+
+    compress: boolean, or integer
+        Whether to zip the stored data on disk. If an integer is
+        given, it should be between 1 and 9, and sets the amount
+        of compression. Note that compressed arrays cannot be
+        read by memmapping.
+
+    verbose: int, optional
+        The verbosity flag, controls messages that are issued as
+        the function is evaluated.
     """
-    #-------------------------------------------------------------------------
+    # ------------------------------------------------------------------------
     # Public interface
-    #-------------------------------------------------------------------------
+    # ------------------------------------------------------------------------
 
-    def __init__(self, func, cachedir, ignore=None, mmap_mode=None,
-                 compress=False, verbose=1, timestamp=None):
-        """
-            Parameters
-            ----------
-            func: callable
-                The function to decorate
-            cachedir: string
-                The path of the base directory to use as a data store
-            ignore: list or None
-                List of variable names to ignore.
-            mmap_mode: {None, 'r+', 'r', 'w+', 'c'}, optional
-                The memmapping mode used when loading from cache
-                numpy arrays. See numpy.load for the meaning of the
-                arguments.
-            compress : boolean, or integer
-                Whether to zip the stored data on disk. If an integer is
-                given, it should be between 1 and 9, and sets the amount
-                of compression. Note that compressed arrays cannot be
-                read by memmapping.
-            verbose: int, optional
-                Verbosity flag, controls the debug messages that are issued
-                as functions are evaluated. The higher, the more verbose
-            timestamp: float, optional
-                The reference time from which times in tracing messages
-                are reported.
-        """
+    def __init__(self, func, location, backend='local', ignore=None,
+                 mmap_mode=None, compress=False, verbose=1, timestamp=None):
         Logger.__init__(self)
         self.mmap_mode = mmap_mode
+        self.compress = compress
         self.func = func
         if ignore is None:
             ignore = []
         self.ignore = ignore
-
         self._verbose = verbose
-        self.cachedir = cachedir
-        self.compress = compress
-        if compress and self.mmap_mode is not None:
-            warnings.warn('Compressed results cannot be memmapped',
-                          stacklevel=2)
+
+        # retrieve store object from backend type and location.
+        self.store_backend = _store_backend_factory(backend, location,
+                                                    verbose=verbose,
+                                                    backend_options=dict(
+                                                        compress=compress,
+                                                        mmap_mode=mmap_mode),
+                                                    )
+        if self.store_backend is not None:
+            # Create func directory on demand.
+            self.store_backend.\
+                store_cached_func_code([_build_func_identifier(self.func)])
+
         if timestamp is None:
             timestamp = time.time()
         self.timestamp = timestamp
-        mkdirp(self.cachedir)
         try:
             functools.update_wrapper(self, func)
         except:
@@ -496,32 +439,38 @@ def _cached_call(self, args, kwargs):
         """
         # Compare the function code with the previous to see if the
         # function code has changed
-        output_dir, argument_hash = self._get_output_dir(*args, **kwargs)
+        func_id, args_id = self._get_output_identifiers(*args, **kwargs)
         metadata = None
-        output_pickle_path = os.path.join(output_dir, 'output.pkl')
+        msg = None
         # FIXME: The statements below should be try/excepted
         if not (self._check_previous_func_code(stacklevel=4) and
-                os.path.isfile(output_pickle_path)):
+                self.store_backend.contains_item([func_id, args_id])):
             if self._verbose > 10:
                 _, name = get_func_name(self.func)
-                self.warn('Computing func %s, argument hash %s in '
-                          'directory %s'
-                        % (name, argument_hash, output_dir))
+                self.warn('Computing func {0}, argument hash {1} '
+                          'in location {2}'
+                          .format(name, args_id,
+                                  self.store_backend.
+                                  get_cached_func_info([func_id])['location']))
             out, metadata = self.call(*args, **kwargs)
             if self.mmap_mode is not None:
                 # Memmap the output at the first call to be consistent with
                 # later calls
-                out = _load_output(output_dir, _get_func_fullname(self.func),
-                                   timestamp=self.timestamp,
-                                   mmap_mode=self.mmap_mode,
-                                   verbose=self._verbose)
+                if self._verbose:
+                    msg = _format_load_msg(func_id, args_id,
+                                           timestamp=self.timestamp,
+                                           metadata=metadata)
+                out = self.store_backend.load_item([func_id, args_id], msg=msg,
+                                                   verbose=self._verbose)
         else:
             try:
                 t0 = time.time()
-                out = _load_output(output_dir, _get_func_fullname(self.func),
-                                   timestamp=self.timestamp,
-                                   metadata=metadata, mmap_mode=self.mmap_mode,
-                                   verbose=self._verbose)
+                if self._verbose:
+                    msg = _format_load_msg(func_id, args_id,
+                                           timestamp=self.timestamp,
+                                           metadata=metadata)
+                out = self.store_backend.load_item([func_id, args_id], msg=msg,
+                                                   verbose=self._verbose)
                 if self._verbose > 4:
                     t = time.time() - t0
                     _, name = get_func_name(self.func)
@@ -531,11 +480,12 @@ def _cached_call(self, args, kwargs):
                 # XXX: Should use an exception logger
                 _, signature = format_signature(self.func, *args, **kwargs)
                 self.warn('Exception while loading results for '
-                          '{}\n {}'.format(
-                              signature, traceback.format_exc()))
+                          '{}\n {}'.format(signature, traceback.format_exc()))
+
                 out, metadata = self.call(*args, **kwargs)
-                argument_hash = None
-        return (out, argument_hash, metadata)
+                args_id = None
+
+        return (out, args_id, metadata)
 
     def call_and_shelve(self, *args, **kwargs):
         """Call wrapped function, cache result and return a reference.
@@ -550,13 +500,12 @@ def call_and_shelve(self, *args, **kwargs):
         cached_result: MemorizedResult or NotMemorizedResult
             reference to the value returned by the wrapped function. The
             class "NotMemorizedResult" is used when there is no cache
-            activated (e.g. cachedir=None in Memory).
+            activated (e.g. location=None in Memory).
         """
-        _, argument_hash, metadata = self._cached_call(args, kwargs)
-
-        return MemorizedResult(self.cachedir, self.func, argument_hash,
-            metadata=metadata, verbose=self._verbose - 1,
-            timestamp=self.timestamp)
+        _, args_id, metadata = self._cached_call(args, kwargs)
+        return MemorizedResult(self.store_backend, self.func, args_id,
+                               metadata=metadata, verbose=self._verbose - 1,
+                               timestamp=self.timestamp)
 
     def __call__(self, *args, **kwargs):
         return self._cached_call(args, kwargs)[0]
@@ -566,44 +515,30 @@ def __reduce__(self):
             depending from it.
             In addition, when unpickling, we run the __init__
         """
-        return (self.__class__, (self.func, self.cachedir, self.ignore,
-                self.mmap_mode, self.compress, self._verbose))
+        return (self.__class__, (self.func, None),
+                {k: v for k, v in vars(self).items()
+                 if k not in ('timestamp', 'func')})
 
-    #-------------------------------------------------------------------------
+    # ------------------------------------------------------------------------
     # Private interface
-    #-------------------------------------------------------------------------
+    # ------------------------------------------------------------------------
 
     def _get_argument_hash(self, *args, **kwargs):
-        return hashing.hash(filter_args(self.func, self.ignore,
-                                         args, kwargs),
-                             coerce_mmap=(self.mmap_mode is not None))
+        return hashing.hash(filter_args(self.func, self.ignore, args, kwargs),
+                            coerce_mmap=(self.mmap_mode is not None))
 
-    def _get_output_dir(self, *args, **kwargs):
-        """ Return the directory in which are persisted the result
-            of the function called with the given arguments.
-        """
+    def _get_output_identifiers(self, *args, **kwargs):
+        """Return the func identifier and input parameter hash of a result."""
+        func_id = _build_func_identifier(self.func)
         argument_hash = self._get_argument_hash(*args, **kwargs)
-        output_dir = os.path.join(self._get_func_dir(self.func),
-                                  argument_hash)
-        return output_dir, argument_hash
-
-    get_output_dir = _get_output_dir  # backward compatibility
-
-    def _get_func_dir(self, mkdir=True):
-        """ Get the directory corresponding to the cache for the
-            function.
-        """
-        func_dir = _cache_key_to_dir(self.cachedir, self.func, None)
-        if mkdir:
-            mkdirp(func_dir)
-        return func_dir
+        return func_id, argument_hash
 
     def _hash_func(self):
         """Hash a function to key the online cache"""
         func_code_h = hash(getattr(self.func, '__code__', None))
         return id(self.func), hash(self.func), func_code_h
 
-    def _write_func_code(self, filename, func_code, first_line):
+    def _write_func_code(self, func_code, first_line):
         """ Write the function code and the filename to a file.
         """
         # We store the first line because the filename and the function
@@ -611,17 +546,18 @@ def _write_func_code(self, filename, func_code, first_line):
         # sometimes have several functions named the same way in a
         # file. This is bad practice, but joblib should be robust to bad
         # practice.
+        func_id = _build_func_identifier(self.func)
         func_code = u'%s %i\n%s' % (FIRST_LINE_TEXT, first_line, func_code)
-        with io.open(filename, 'w', encoding="UTF-8") as out:
-            out.write(func_code)
+        self.store_backend.store_cached_func_code([func_id], func_code)
+
         # Also store in the in-memory store of function hashes
         is_named_callable = False
         if PY3_OR_LATER:
-            is_named_callable = (hasattr(self.func, '__name__')
-                                 and self.func.__name__ != '<lambda>')
+            is_named_callable = (hasattr(self.func, '__name__') and
+                                 self.func.__name__ != '<lambda>')
         else:
-            is_named_callable = (hasattr(self.func, 'func_name')
-                                 and self.func.func_name != '<lambda>')
+            is_named_callable = (hasattr(self.func, 'func_name') and
+                                 self.func.func_name != '<lambda>')
         if is_named_callable:
             # Don't do this for lambda functions or strange callable
             # objects, as it ends up being too fragile
@@ -657,15 +593,14 @@ def _check_previous_func_code(self, stacklevel=2):
         # changing code and collision. We cannot inspect.getsource
         # because it is not reliable when using IPython's magic "%run".
         func_code, source_file, first_line = get_func_code(self.func)
-        func_dir = self._get_func_dir()
-        func_code_file = os.path.join(func_dir, 'func_code.py')
+        func_id = _build_func_identifier(self.func)
 
         try:
-            with io.open(func_code_file, encoding="UTF-8") as infile:
-                old_func_code, old_first_line = \
-                            extract_first_line(infile.read())
-        except IOError:
-                self._write_func_code(func_code_file, func_code, first_line)
+            old_func_code, old_first_line =\
+                extract_first_line(
+                    self.store_backend.get_cached_func_code([func_id]))
+        except (IOError, OSError):  # some backend can also raise OSError
+                self._write_func_code(func_code, first_line)
                 return False
         if old_func_code == func_code:
             return True
@@ -678,13 +613,14 @@ def _check_previous_func_code(self, stacklevel=2):
                                      win_characters=False)
         if old_first_line == first_line == -1 or func_name == '<lambda>':
             if not first_line == -1:
-                func_description = '%s (%s:%i)' % (func_name,
-                                                source_file, first_line)
+                func_description = ("{0} ({1}:{2})"
+                                    .format(func_name, source_file,
+                                            first_line))
             else:
                 func_description = func_name
             warnings.warn(JobLibCollisionWarning(
-                "Cannot detect name collisions for function '%s'"
-                        % func_description), stacklevel=stacklevel)
+                "Cannot detect name collisions for function '{0}'"
+                .format(func_description)), stacklevel=stacklevel)
 
         # Fetch the code at the old location and compare it. If it is the
         # same than the code store, we have a collision: the code in the
@@ -699,52 +635,52 @@ def _check_previous_func_code(self, stacklevel=2):
                     on_disk_func_code = f.readlines()[
                         old_first_line - 1:old_first_line - 1 + num_lines - 1]
                 on_disk_func_code = ''.join(on_disk_func_code)
-                possible_collision = (on_disk_func_code.rstrip()
-                                      == old_func_code.rstrip())
+                possible_collision = (on_disk_func_code.rstrip() ==
+                                      old_func_code.rstrip())
             else:
                 possible_collision = source_file.startswith('<doctest ')
             if possible_collision:
                 warnings.warn(JobLibCollisionWarning(
-                        'Possible name collisions between functions '
-                        "'%s' (%s:%i) and '%s' (%s:%i)" %
-                        (func_name, source_file, old_first_line,
-                        func_name, source_file, first_line)),
-                                stacklevel=stacklevel)
+                    'Possible name collisions between functions '
+                    "'%s' (%s:%i) and '%s' (%s:%i)" %
+                    (func_name, source_file, old_first_line,
+                     func_name, source_file, first_line)),
+                    stacklevel=stacklevel)
 
         # The function has changed, wipe the cache directory.
         # XXX: Should be using warnings, and giving stacklevel
         if self._verbose > 10:
             _, func_name = get_func_name(self.func, resolv_alias=False)
-            self.warn("Function %s (stored in %s) has changed." %
-                        (func_name, func_dir))
+            self.warn("Function {0} (identified by {1}) has changed"
+                      ".".format(func_name, func_id))
         self.clear(warn=True)
         return False
 
     def clear(self, warn=True):
-        """ Empty the function's cache.
-        """
-        func_dir = self._get_func_dir(mkdir=False)
+        """Empty the function's cache."""
+        func_id = _build_func_identifier(self.func)
+
         if self._verbose > 0 and warn:
-            self.warn("Clearing cache %s" % func_dir)
-        if os.path.exists(func_dir):
-            shutil.rmtree(func_dir, ignore_errors=True)
-        mkdirp(func_dir)
+            self.warn("Clearing function cache identified by %s" % func_id)
+        self.store_backend.clear_path([func_id, ])
+
         func_code, _, first_line = get_func_code(self.func)
-        func_code_file = os.path.join(func_dir, 'func_code.py')
-        self._write_func_code(func_code_file, func_code, first_line)
+        self._write_func_code(func_code, first_line)
 
     def call(self, *args, **kwargs):
         """ Force the execution of the function with the given arguments and
             persist the output values.
         """
         start_time = time.time()
-        output_dir, _ = self._get_output_dir(*args, **kwargs)
+        func_id, args_id = self._get_output_identifiers(*args, **kwargs)
         if self._verbose > 0:
             print(format_call(self.func, args, kwargs))
         output = self.func(*args, **kwargs)
-        self._persist_output(output, output_dir)
+        self.store_backend.dump_item(
+            [func_id, args_id], output, verbose=self._verbose)
+
         duration = time.time() - start_time
-        metadata = self._persist_input(output_dir, duration, args, kwargs)
+        metadata = self._persist_input(duration, args, kwargs)
 
         if self._verbose > 0:
             _, name = get_func_name(self.func)
@@ -752,23 +688,7 @@ def call(self, *args, **kwargs):
             print(max(0, (80 - len(msg))) * '_' + msg)
         return output, metadata
 
-    # Make public
-    def _persist_output(self, output, dir):
-        """ Persist the given output tuple in the directory.
-        """
-        try:
-            filename = os.path.join(dir, 'output.pkl')
-            mkdirp(dir)
-            write_func = functools.partial(numpy_pickle.dump,
-                                           compress=self.compress)
-            concurrency_safe_write(output, filename, write_func)
-            if self._verbose > 10:
-                print('Persisting in %s' % dir)
-        except OSError:
-            " Race condition in the creation of the directory "
-
-    def _persist_input(self, output_dir, duration, args, kwargs,
-                       this_duration_limit=0.5):
+    def _persist_input(self, duration, args, kwargs, this_duration_limit=0.5):
         """ Save a small summary of the call using json format in the
             output directory.
 
@@ -793,17 +713,9 @@ def _persist_input(self, output_dir, duration, args, kwargs,
         # This can fail due to race-conditions with multiple
         # concurrent joblibs removing the file or the directory
         metadata = {"duration": duration, "input_args": input_repr}
-        try:
-            mkdirp(output_dir)
-            filename = os.path.join(output_dir, 'metadata.json')
 
-            def write_func(output, dest_filename):
-                with open(dest_filename, 'w') as f:
-                    json.dump(output, f)
-
-            concurrency_safe_write(metadata, filename, write_func)
-        except Exception:
-            pass
+        func_id, args_id = self._get_output_identifiers(*args, **kwargs)
+        self.store_backend.store_metadata([func_id, args_id], metadata)
 
         this_duration = time.time() - start_time
         if this_duration > this_duration_limit:
@@ -828,17 +740,14 @@ def write_func(output, dest_filename):
 
     # XXX: Need a method to check if results are available.
 
-
-    #-------------------------------------------------------------------------
+    # ------------------------------------------------------------------------
     # Private `object` interface
-    #-------------------------------------------------------------------------
+    # ------------------------------------------------------------------------
 
     def __repr__(self):
-        return '%s(func=%s, cachedir=%s)' % (
-                    self.__class__.__name__,
-                    self.func,
-                    repr(self.cachedir),
-                    )
+        return ("{0}(func={1}, location={2})".format(self.__class__.__name__,
+                                                     self.func,
+                                                     self.store_backend,))
 
 
 ###############################################################################
@@ -851,54 +760,105 @@ class Memory(Logger):
         All values are cached on the filesystem, in a deep directory
         structure.
 
-        see :ref:`memory_reference`
+        Read more in the :ref:`User Guide <memory>`.
+
+        Parameters
+        ----------
+        location: str or None
+            The path of the base directory to use as a data store
+            or None. If None is given, no caching is done and
+            the Memory object is completely transparent. This option
+            replaces cachedir since version 0.12.
+
+        backend: str, optional
+            Type of store backend for reading/writing cache files.
+            Default: 'local'.
+            The 'local' backend is using regular filesystem operations to
+            manipulate data (open, mv, etc) in the backend.
+
+        mmap_mode: {None, 'r+', 'r', 'w+', 'c'}, optional
+            The memmapping mode used when loading from cache
+            numpy arrays. See numpy.load for the meaning of the
+            arguments.
+
+        compress: boolean, or integer, optional
+            Whether to zip the stored data on disk. If an integer is
+            given, it should be between 1 and 9, and sets the amount
+            of compression. Note that compressed arrays cannot be
+            read by memmapping.
+
+        verbose: int, optional
+            Verbosity flag, controls the debug messages that are issued
+            as functions are evaluated.
+
+        bytes_limit: int, optional
+            Limit in bytes of the size of the cache.
+
+        backend_options: dict, optional
+            Contains a dictionnary of named parameters used to configure
+            the store backend.
+
+        cachedir: str or None, optional
+
+            .. deprecated: 0.12
+                'cachedir' has been deprecated in 0.12 and will be
+                removed in 0.14. Use the 'location' parameter instead.
     """
-    #-------------------------------------------------------------------------
+    # ------------------------------------------------------------------------
     # Public interface
-    #-------------------------------------------------------------------------
+    # ------------------------------------------------------------------------
 
-    def __init__(self, cachedir, mmap_mode=None, compress=False, verbose=1,
-                 bytes_limit=None):
-        """
-            Parameters
-            ----------
-            cachedir: string or None
-                The path of the base directory to use as a data store
-                or None. If None is given, no caching is done and
-                the Memory object is completely transparent.
-            mmap_mode: {None, 'r+', 'r', 'w+', 'c'}, optional
-                The memmapping mode used when loading from cache
-                numpy arrays. See numpy.load for the meaning of the
-                arguments.
-            compress: boolean, or integer
-                Whether to zip the stored data on disk. If an integer is
-                given, it should be between 1 and 9, and sets the amount
-                of compression. Note that compressed arrays cannot be
-                read by memmapping.
-            verbose: int, optional
-                Verbosity flag, controls the debug messages that are issued
-                as functions are evaluated.
-            bytes_limit: int, optional
-                Limit in bytes of the size of the cache
-        """
+    def __init__(self, location=None, backend='local', mmap_mode=None,
+                 compress=False, verbose=1, bytes_limit=None,
+                 backend_options={}, cachedir=None):
         # XXX: Bad explanation of the None value of cachedir
         Logger.__init__(self)
         self._verbose = verbose
         self.mmap_mode = mmap_mode
         self.timestamp = time.time()
-        self.compress = compress
         self.bytes_limit = bytes_limit
+        self.backend = backend
         if compress and mmap_mode is not None:
             warnings.warn('Compressed results cannot be memmapped',
                           stacklevel=2)
-        if cachedir is None:
-            self.cachedir = None
-        else:
-            self.cachedir = os.path.join(cachedir, 'joblib')
-            mkdirp(self.cachedir)
-
-    def cache(self, func=None, ignore=None, verbose=None,
-                        mmap_mode=False):
+        if cachedir is not None:
+            if location is not None:
+                raise ValueError(
+                    'You set both "location={0!r} and "cachedir={1!r}". '
+                    "'cachedir' has been deprecated in version "
+                    "0.12 and will be removed in version 0.14.\n"
+                    'Please only set "location={0!r}"'.format(
+                        location, cachedir))
+
+            warnings.warn(
+                "The 'cachedir' parameter has been deprecated in version "
+                "0.12 and will be removed in version 0.14.\n"
+                'You provided "cachedir={0!r}", '
+                'use "location={0!r}" instead.'.format(cachedir),
+                DeprecationWarning, stacklevel=2)
+            location = cachedir
+
+        self.location = location
+        if isinstance(location, _basestring):
+            location = os.path.join(location, 'joblib')
+
+        self.store_backend = _store_backend_factory(
+            backend, location, verbose=self._verbose,
+            backend_options=dict(compress=compress, mmap_mode=mmap_mode,
+                                 **backend_options))
+
+    @property
+    def cachedir(self):
+        warnings.warn(
+            "The 'cachedir' attribute has been deprecated in version 0.12 "
+            "and will be removed in version 0.14.\n"
+            "Use os.path.join(memory.location, 'joblib') attribute instead.",
+            DeprecationWarning, stacklevel=2)
+        if self.location is None:
+            return None
+        return os.path.join(self.location, 'joblib')
+
+    def cache(self, func=None, ignore=None, verbose=None, mmap_mode=False):
         """ Decorates the given function func to only compute its return
             value for input arguments not cached on disk.
 
@@ -929,7 +889,7 @@ def cache(self, func=None, ignore=None, verbose=None,
             # arguments in decorators
             return functools.partial(self.cache, ignore=ignore,
                                      verbose=verbose, mmap_mode=mmap_mode)
-        if self.cachedir is None:
+        if self.store_backend is None:
             return NotMemorizedFunc(func)
         if verbose is None:
             verbose = self._verbose
@@ -937,38 +897,22 @@ def cache(self, func=None, ignore=None, verbose=None,
             mmap_mode = self.mmap_mode
         if isinstance(func, MemorizedFunc):
             func = func.func
-        return MemorizedFunc(func, cachedir=self.cachedir,
-                                   mmap_mode=mmap_mode,
-                                   ignore=ignore,
-                                   compress=self.compress,
-                                   verbose=verbose,
-                                   timestamp=self.timestamp)
+        return MemorizedFunc(func, self.store_backend, mmap_mode=mmap_mode,
+                             ignore=ignore, verbose=verbose,
+                             timestamp=self.timestamp)
 
     def clear(self, warn=True):
         """ Erase the complete cache directory.
         """
         if warn:
             self.warn('Flushing completely the cache')
-        if self.cachedir is not None:
-            rm_subdirs(self.cachedir)
+        if self.store_backend is not None:
+            self.store_backend.clear()
 
     def reduce_size(self):
-        """Remove cache folders to make cache size fit in ``bytes_limit``."""
-        if self.cachedir is not None and self.bytes_limit is not None:
-            cache_items_to_delete = _get_cache_items_to_delete(
-                self.cachedir, self.bytes_limit)
-
-            for cache_item in cache_items_to_delete:
-                if self._verbose > 10:
-                    print('Deleting cache item {}'.format(cache_item))
-                try:
-                    shutil.rmtree(cache_item.path, ignore_errors=True)
-                except OSError:
-                    # Even with ignore_errors=True can shutil.rmtree
-                    # can raise OSErrror with [Errno 116] Stale file
-                    # handle if another process has deleted the folder
-                    # already.
-                    pass
+        """Remove cache elements to make cache size fit in ``bytes_limit``."""
+        if self.bytes_limit is not None and self.store_backend is not None:
+            self.store_backend.reduce_store_size(self.bytes_limit)
 
     def eval(self, func, *args, **kwargs):
         """ Eval function func with arguments `*args` and `**kwargs`,
@@ -979,26 +923,23 @@ def eval(self, func, *args, **kwargs):
             up to date.
 
         """
-        if self.cachedir is None:
+        if self.store_backend is None:
             return func(*args, **kwargs)
         return self.cache(func)(*args, **kwargs)
 
-    #-------------------------------------------------------------------------
+    # ------------------------------------------------------------------------
     # Private `object` interface
-    #-------------------------------------------------------------------------
+    # ------------------------------------------------------------------------
 
     def __repr__(self):
-        return '%s(cachedir=%s)' % (
-                    self.__class__.__name__,
-                    repr(self.cachedir),
-                    )
+        return '{0}(location={1})'.format(
+            self.__class__.__name__, (repr(None) if self.store_backend is None
+                                      else repr(self.store_backend)))
 
     def __reduce__(self):
         """ We don't store the timestamp when pickling, to avoid the hash
             depending from it.
             In addition, when unpickling, we run the __init__
         """
-        # We need to remove 'joblib' from the end of cachedir
-        cachedir = self.cachedir[:-7] if self.cachedir is not None else None
-        return (self.__class__, (cachedir,
-                self.mmap_mode, self.compress, self._verbose))
+        return (self.__class__, (), {k: v for k, v in vars(self).items()
+                                     if k != 'timestamp'})
diff --git a/sklearn/externals/_joblib/my_exceptions.py b/sklearn/externals/joblib/my_exceptions.py
similarity index 86%
rename from sklearn/externals/_joblib/my_exceptions.py
rename to sklearn/externals/joblib/my_exceptions.py
index 3bda92f58a6e..765885e33c17 100644
--- a/sklearn/externals/_joblib/my_exceptions.py
+++ b/sklearn/externals/joblib/my_exceptions.py
@@ -4,9 +4,9 @@
 # Author: Gael Varoquaux < gael dot varoquaux at normalesup dot org >
 # Copyright: 2010, Gael Varoquaux
 # License: BSD 3 clause
-
 from ._compat import PY3_OR_LATER
 
+
 class JoblibException(Exception):
     """A simple exception with an error message that you can get to."""
     def __init__(self, *args):
@@ -47,6 +47,17 @@ def __init__(self, message, etype):
         self.message = message
         self.etype = etype
 
+    def unwrap(self, context_message=""):
+        report = """\
+%s
+---------------------------------------------------------------------------
+Joblib worker traceback:
+---------------------------------------------------------------------------
+%s""" % (context_message, self.message)
+        # Unwrap the exception to a JoblibException
+        exception_type = _mk_exception(self.etype)[0]
+        return exception_type(report)
+
 
 class WorkerInterrupt(Exception):
     """ An exception that is not KeyboardInterrupt to allow subprocesses
@@ -59,6 +70,10 @@ class WorkerInterrupt(Exception):
 
 
 def _mk_exception(exception, name=None):
+    if issubclass(exception, JoblibException):
+        # No need to wrap recursively JoblibException
+        return exception, exception.__name__
+
     # Create an exception inheriting from both JoblibException
     # and that exception
     if name is None:
diff --git a/sklearn/externals/_joblib/numpy_pickle.py b/sklearn/externals/joblib/numpy_pickle.py
similarity index 90%
rename from sklearn/externals/_joblib/numpy_pickle.py
rename to sklearn/externals/joblib/numpy_pickle.py
index 87a1a616cd54..496429f50f16 100644
--- a/sklearn/externals/_joblib/numpy_pickle.py
+++ b/sklearn/externals/joblib/numpy_pickle.py
@@ -13,8 +13,11 @@
 except ImportError:
     Path = None
 
-from .numpy_pickle_utils import _COMPRESSORS
-from .numpy_pickle_utils import BinaryZlibFile
+from .compressor import lz4, LZ4_NOT_INSTALLED_ERROR
+from .compressor import _COMPRESSORS, register_compressor, BinaryZlibFile
+from .compressor import (ZlibCompressorWrapper, GzipCompressorWrapper,
+                         BZ2CompressorWrapper, LZMACompressorWrapper,
+                         XZCompressorWrapper, LZ4CompressorWrapper)
 from .numpy_pickle_utils import Unpickler, Pickler
 from .numpy_pickle_utils import _read_fileobject, _write_fileobject
 from .numpy_pickle_utils import _read_bytes, BUFFER_SIZE
@@ -28,6 +31,14 @@
 from ._compat import _basestring, PY3_OR_LATER
 from .backports import make_memmap
 
+# Register supported compressors
+register_compressor('zlib', ZlibCompressorWrapper())
+register_compressor('gzip', GzipCompressorWrapper())
+register_compressor('bz2', BZ2CompressorWrapper())
+register_compressor('lzma', LZMACompressorWrapper())
+register_compressor('xz', XZCompressorWrapper())
+register_compressor('lz4', LZ4CompressorWrapper())
+
 ###############################################################################
 # Utility objects for persistence.
 
@@ -209,7 +220,7 @@ class NumpyPickler(Pickler):
     ----------
     fp: file
         File object handle used for serializing the input object.
-    protocol: int
+    protocol: int, optional
         Pickle protocol used. Default is pickle.DEFAULT_PROTOCOL under
         python 3, pickle.HIGHEST_PROTOCOL otherwise.
     """
@@ -353,14 +364,17 @@ def load_build(self):
 def dump(value, filename, compress=0, protocol=None, cache_size=None):
     """Persist an arbitrary Python object into one file.
 
+    Read more in the :ref:`User Guide <persistence>`.
+
     Parameters
     -----------
     value: any Python object
         The object to store to disk.
-    filename: str or pathlib.Path
-        The path of the file in which it is to be stored. The compression
-        method corresponding to one of the supported filename extensions ('.z',
-        '.gz', '.bz2', '.xz' or '.lzma') will be used automatically.
+    filename: str, pathlib.Path, or file object.
+        The file object or path of the file in which it is to be stored.
+        The compression method corresponding to one of the supported filename
+        extensions ('.z', '.gz', '.bz2', '.xz' or '.lzma') will be used
+        automatically.
     compress: int from 0 to 9 or bool or 2-tuple, optional
         Optional compression level for the data. 0 or False is no compression.
         Higher value means more compression, but also slower read and
@@ -371,7 +385,7 @@ def dump(value, filename, compress=0, protocol=None, cache_size=None):
         between supported compressors (e.g 'zlib', 'gzip', 'bz2', 'lzma'
         'xz'), the second element must be an integer from 0 to 9, corresponding
         to the compression level.
-    protocol: positive int
+    protocol: int, optional
         Pickle protocol, see pickle.dump documentation for more details.
     cache_size: positive int, optional
         This option is deprecated in 0.10 and has no effect.
@@ -403,8 +417,9 @@ def dump(value, filename, compress=0, protocol=None, cache_size=None):
 
     compress_method = 'zlib'  # zlib is the default compression method.
     if compress is True:
-        # By default, if compress is enabled, we want to be using 3 by default
-        compress_level = 3
+        # By default, if compress is enabled, we want the default compress
+        # level of the compressor.
+        compress_level = None
     elif isinstance(compress, tuple):
         # a 2-tuple was set in compress
         if len(compress) != 2:
@@ -413,10 +428,19 @@ def dump(value, filename, compress=0, protocol=None, cache_size=None):
                 '(compress method, compress level), you passed {}'
                 .format(compress))
         compress_method, compress_level = compress
+    elif isinstance(compress, _basestring):
+        compress_method = compress
+        compress_level = None  # Use default compress level
+        compress = (compress_method, compress_level)
     else:
         compress_level = compress
 
-    if compress_level is not False and compress_level not in range(10):
+    if compress_method == 'lz4' and lz4 is None:
+        raise ValueError(LZ4_NOT_INSTALLED_ERROR)
+
+    if (compress_level is not None and
+            compress_level is not False and
+            compress_level not in range(10)):
         # Raising an error if a non valid compress level is given.
         raise ValueError(
             'Non valid compress level given: "{}". Possible values are '
@@ -441,25 +465,17 @@ def dump(value, filename, compress=0, protocol=None, cache_size=None):
         # In case no explicit compression was requested using both compression
         # method and level in a tuple and the filename has an explicit
         # extension, we select the corresponding compressor.
-        if filename.endswith('.z'):
-            compress_method = 'zlib'
-        elif filename.endswith('.gz'):
-            compress_method = 'gzip'
-        elif filename.endswith('.bz2'):
-            compress_method = 'bz2'
-        elif filename.endswith('.lzma'):
-            compress_method = 'lzma'
-        elif filename.endswith('.xz'):
-            compress_method = 'xz'
-        else:
-            # no matching compression method found, we unset the variable to
-            # be sure no compression level is set afterwards.
-            compress_method = None
+
+        # unset the variable to be sure no compression level is set afterwards.
+        compress_method = None
+        for name, compressor in _COMPRESSORS.items():
+            if filename.endswith(compressor.extension):
+                compress_method = name
 
         if compress_method in _COMPRESSORS and compress_level == 0:
-            # we choose a default compress_level of 3 in case it was not given
+            # we choose the default compress_level in case it was not given
             # as an argument (using compress).
-            compress_level = 3
+            compress_level = None
 
     if not PY3_OR_LATER and compress_method in ('lzma', 'xz'):
         raise NotImplementedError("{} compression is only available for "
@@ -530,14 +546,16 @@ def _unpickle(fobj, filename="", mmap_mode=None):
 def load(filename, mmap_mode=None):
     """Reconstruct a Python object from a file persisted with joblib.dump.
 
+    Read more in the :ref:`User Guide <persistence>`.
+
     Parameters
     -----------
-    filename: str or pathlib.Path
-        The path of the file from which to load the object
+    filename: str, pathlib.Path, or file object.
+        The file object or path of the file from which to load the object
     mmap_mode: {None, 'r+', 'r', 'w+', 'c'}, optional
         If not None, the arrays are memory-mapped from the disk. This
         mode has no effect for compressed files. Note that in this
-        case the reconstructed object might not longer match exactly
+        case the reconstructed object might no longer match exactly
         the originally pickled object.
 
     Returns
@@ -556,7 +574,7 @@ def load(filename, mmap_mode=None):
     dump. If the mmap_mode argument is given, it is passed to np.load and
     arrays are loaded as memmaps. As a consequence, the reconstructed
     object might not match the original pickled object. Note that if the
-    file was saved with compression, the arrays cannot be memmaped.
+    file was saved with compression, the arrays cannot be memmapped.
     """
     if Path is not None and isinstance(filename, Path):
         filename = str(filename)
diff --git a/sklearn/externals/_joblib/numpy_pickle_compat.py b/sklearn/externals/joblib/numpy_pickle_compat.py
similarity index 100%
rename from sklearn/externals/_joblib/numpy_pickle_compat.py
rename to sklearn/externals/joblib/numpy_pickle_compat.py
diff --git a/sklearn/externals/joblib/numpy_pickle_utils.py b/sklearn/externals/joblib/numpy_pickle_utils.py
new file mode 100644
index 000000000000..1ebf1aa61bb4
--- /dev/null
+++ b/sklearn/externals/joblib/numpy_pickle_utils.py
@@ -0,0 +1,245 @@
+"""Utilities for fast persistence of big data, with optional compression."""
+
+# Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>
+# Copyright (c) 2009 Gael Varoquaux
+# License: BSD Style, 3 clauses.
+
+import pickle
+import io
+import warnings
+import contextlib
+from contextlib import closing
+
+from ._compat import PY3_OR_LATER, PY27
+from .compressor import _ZFILE_PREFIX
+from .compressor import _COMPRESSORS
+
+if PY3_OR_LATER:
+    Unpickler = pickle._Unpickler
+    Pickler = pickle._Pickler
+    xrange = range
+else:
+    Unpickler = pickle.Unpickler
+    Pickler = pickle.Pickler
+
+try:
+    import numpy as np
+except ImportError:
+    np = None
+
+try:
+    # The python standard library can be built without bz2 so we make bz2
+    # usage optional.
+    # see https://github.com/scikit-learn/scikit-learn/issues/7526 for more
+    # details.
+    import bz2
+except ImportError:
+    bz2 = None
+
+# Buffer size used in io.BufferedReader and io.BufferedWriter
+_IO_BUFFER_SIZE = 1024 ** 2
+
+
+def _is_raw_file(fileobj):
+    """Check if fileobj is a raw file object, e.g created with open."""
+    if PY3_OR_LATER:
+        fileobj = getattr(fileobj, 'raw', fileobj)
+        return isinstance(fileobj, io.FileIO)
+    else:
+        return isinstance(fileobj, file)  # noqa
+
+
+def _get_prefixes_max_len():
+    # Compute the max prefix len of registered compressors.
+    prefixes = [len(compressor.prefix) for compressor in _COMPRESSORS.values()]
+    prefixes += [len(_ZFILE_PREFIX)]
+    return max(prefixes)
+
+
+###############################################################################
+# Cache file utilities
+def _detect_compressor(fileobj):
+    """Return the compressor matching fileobj.
+
+    Parameters
+    ----------
+    fileobj: file object
+
+    Returns
+    -------
+    str in {'zlib', 'gzip', 'bz2', 'lzma', 'xz', 'compat', 'not-compressed'}
+    """
+    # Read the magic number in the first bytes of the file.
+    max_prefix_len = _get_prefixes_max_len()
+    if hasattr(fileobj, 'peek'):
+        # Peek allows to read those bytes without moving the cursor in the
+        # file whic.
+        first_bytes = fileobj.peek(max_prefix_len)
+    else:
+        # Fallback to seek if the fileobject is not peekable.
+        first_bytes = fileobj.read(max_prefix_len)
+        fileobj.seek(0)
+
+    if first_bytes.startswith(_ZFILE_PREFIX):
+        return "compat"
+    else:
+        for name, compressor in _COMPRESSORS.items():
+            if first_bytes.startswith(compressor.prefix):
+                return name
+
+    return "not-compressed"
+
+
+def _buffered_read_file(fobj):
+    """Return a buffered version of a read file object."""
+    if PY27 and bz2 is not None and isinstance(fobj, bz2.BZ2File):
+        # Python 2.7 doesn't work with BZ2File through a buffer: "no
+        # attribute 'readable'" error.
+        return fobj
+    else:
+        return io.BufferedReader(fobj, buffer_size=_IO_BUFFER_SIZE)
+
+
+def _buffered_write_file(fobj):
+    """Return a buffered version of a write file object."""
+    if PY27 and bz2 is not None and isinstance(fobj, bz2.BZ2File):
+        # Python 2.7 doesn't work with BZ2File through a buffer: no attribute
+        # 'writable'.
+        # BZ2File doesn't implement the file object context manager in python 2
+        # so we wrap the fileobj using `closing`.
+        return closing(fobj)
+    else:
+        return io.BufferedWriter(fobj, buffer_size=_IO_BUFFER_SIZE)
+
+
+@contextlib.contextmanager
+def _read_fileobject(fileobj, filename, mmap_mode=None):
+    """Utility function opening the right fileobject from a filename.
+
+    The magic number is used to choose between the type of file object to open:
+    * regular file object (default)
+    * zlib file object
+    * gzip file object
+    * bz2 file object
+    * lzma file object (for xz and lzma compressor)
+
+    Parameters
+    ----------
+    fileobj: file object
+    compressor: str in {'zlib', 'gzip', 'bz2', 'lzma', 'xz', 'compat',
+                        'not-compressed'}
+    filename: str
+        filename path corresponding to the fileobj parameter.
+    mmap_mode: str
+        memory map mode that should be used to open the pickle file. This
+        parameter is useful to verify that the user is not trying to one with
+        compression. Default: None.
+
+    Returns
+    -------
+        a file like object
+
+    """
+    # Detect if the fileobj contains compressed data.
+    compressor = _detect_compressor(fileobj)
+
+    if compressor == 'compat':
+        # Compatibility with old pickle mode: simply return the input
+        # filename "as-is" and let the compatibility function be called by the
+        # caller.
+        warnings.warn("The file '%s' has been generated with a joblib "
+                      "version less than 0.10. "
+                      "Please regenerate this pickle file." % filename,
+                      DeprecationWarning, stacklevel=2)
+        yield filename
+    else:
+        if compressor in _COMPRESSORS:
+            # based on the compressor detected in the file, we open the
+            # correct decompressor file object, wrapped in a buffer.
+            compressor_wrapper = _COMPRESSORS[compressor]
+            inst = compressor_wrapper.decompressor_file(fileobj)
+            fileobj = _buffered_read_file(inst)
+
+        # Checking if incompatible load parameters with the type of file:
+        # mmap_mode cannot be used with compressed file or in memory buffers
+        # such as io.BytesIO.
+        if mmap_mode is not None:
+            if isinstance(fileobj, io.BytesIO):
+                warnings.warn('In memory persistence is not compatible with '
+                              'mmap_mode "%(mmap_mode)s" flag passed. '
+                              'mmap_mode option will be ignored.'
+                              % locals(), stacklevel=2)
+            elif compressor != 'not-compressed':
+                warnings.warn('mmap_mode "%(mmap_mode)s" is not compatible '
+                              'with compressed file %(filename)s. '
+                              '"%(mmap_mode)s" flag will be ignored.'
+                              % locals(), stacklevel=2)
+            elif not _is_raw_file(fileobj):
+                warnings.warn('"%(fileobj)r" is not a raw file, mmap_mode '
+                              '"%(mmap_mode)s" flag will be ignored.'
+                              % locals(), stacklevel=2)
+
+        yield fileobj
+
+
+def _write_fileobject(filename, compress=("zlib", 3)):
+    """Return the right compressor file object in write mode."""
+    compressmethod = compress[0]
+    compresslevel = compress[1]
+
+    if compressmethod in _COMPRESSORS.keys():
+        file_instance = _COMPRESSORS[compressmethod].compressor_file(
+            filename, compresslevel=compresslevel)
+        return _buffered_write_file(file_instance)
+    else:
+        file_instance = _COMPRESSORS['zlib'].compressor_file(
+            filename, compresslevel=compresslevel)
+        return _buffered_write_file(file_instance)
+
+
+# Utility functions/variables from numpy required for writing arrays.
+# We need at least the functions introduced in version 1.9 of numpy. Here,
+# we use the ones from numpy 1.10.2.
+BUFFER_SIZE = 2 ** 18  # size of buffer for reading npz files in bytes
+
+
+def _read_bytes(fp, size, error_template="ran out of data"):
+    """Read from file-like object until size bytes are read.
+
+    Raises ValueError if not EOF is encountered before size bytes are read.
+    Non-blocking objects only supported if they derive from io objects.
+
+    Required as e.g. ZipExtFile in python 2.6 can return less data than
+    requested.
+
+    This function was taken from numpy/lib/format.py in version 1.10.2.
+
+    Parameters
+    ----------
+    fp: file-like object
+    size: int
+    error_template: str
+
+    Returns
+    -------
+    a bytes object
+        The data read in bytes.
+
+    """
+    data = bytes()
+    while True:
+        # io files (default in python3) return None or raise on
+        # would-block, python2 file will truncate, probably nothing can be
+        # done about that.  note that regular files can't be non-blocking
+        try:
+            r = fp.read(size - len(data))
+            data += r
+            if len(r) == 0 or len(data) == size:
+                break
+        except io.BlockingIOError:
+            pass
+    if len(data) != size:
+        msg = "EOF: reading %s, expected %d bytes got %d"
+        raise ValueError(msg % (error_template, size, len(data)))
+    else:
+        return data
diff --git a/sklearn/externals/_joblib/parallel.py b/sklearn/externals/joblib/parallel.py
similarity index 66%
rename from sklearn/externals/_joblib/parallel.py
rename to sklearn/externals/joblib/parallel.py
index 6f2091d91fad..8be0ed2c6ba7 100644
--- a/sklearn/externals/_joblib/parallel.py
+++ b/sklearn/externals/joblib/parallel.py
@@ -12,25 +12,25 @@
 from math import sqrt
 import functools
 import time
+import inspect
 import threading
 import itertools
 from numbers import Integral
-from contextlib import contextmanager
 import warnings
-try:
-    import cPickle as pickle
-except ImportError:
-    import pickle
+from functools import partial
 
 from ._multiprocessing_helpers import mp
 
 from .format_stack import format_outer_frames
 from .logger import Logger, short_format_time
-from .my_exceptions import TransportableException, _mk_exception
+from .my_exceptions import TransportableException
 from .disk import memstr_to_bytes
 from ._parallel_backends import (FallbackToBackend, MultiprocessingBackend,
-                                 ThreadingBackend, SequentialBackend)
+                                 ThreadingBackend, SequentialBackend,
+                                 LokyBackend)
 from ._compat import _basestring
+from .externals.cloudpickle import dumps, loads
+from .externals import loky
 
 # Make sure that those two classes are part of the public joblib.parallel API
 # so that 3rd party backend implementers can import them from here.
@@ -41,37 +41,108 @@
     'multiprocessing': MultiprocessingBackend,
     'threading': ThreadingBackend,
     'sequential': SequentialBackend,
+    'loky': LokyBackend,
 }
-
 # name of the backend used by default by Parallel outside of any context
 # managed by ``parallel_backend``.
-DEFAULT_BACKEND = 'multiprocessing'
+DEFAULT_BACKEND = 'loky'
 DEFAULT_N_JOBS = 1
+DEFAULT_THREAD_BACKEND = 'threading'
 
 # Thread local value that can be overridden by the ``parallel_backend`` context
 # manager
 _backend = threading.local()
 
+VALID_BACKEND_HINTS = ('processes', 'threads', None)
+VALID_BACKEND_CONSTRAINTS = ('sharedmem', None)
 
-def get_active_backend():
-    """Return the active default backend"""
-    active_backend_and_jobs = getattr(_backend, 'backend_and_jobs', None)
-    if active_backend_and_jobs is not None:
-        return active_backend_and_jobs
-    # We are outside of the scope of any parallel_backend context manager,
-    # create the default backend instance now
-    active_backend = BACKENDS[DEFAULT_BACKEND]()
-    return active_backend, DEFAULT_N_JOBS
+
+def _register_dask():
+    """ Register Dask Backend if called with parallel_backend("dask") """
+    try:
+        from ._dask import DaskDistributedBackend
+        register_parallel_backend('dask', DaskDistributedBackend)
+    except ImportError:
+        msg = ("To use the dask.distributed backend you must install both "
+               "the `dask` and distributed modules.\n\n"
+               "See http://dask.pydata.org/en/latest/install.html for more "
+               "information.")
+        raise ImportError(msg)
+
+
+EXTERNAL_BACKENDS = {
+    'dask': _register_dask,
+}
 
 
-@contextmanager
-def parallel_backend(backend, n_jobs=-1, **backend_params):
+def get_active_backend(prefer=None, require=None, verbose=0):
+    """Return the active default backend"""
+    if prefer not in VALID_BACKEND_HINTS:
+        raise ValueError("prefer=%r is not a valid backend hint, "
+                         "expected one of %r" % (prefer, VALID_BACKEND_HINTS))
+    if require not in VALID_BACKEND_CONSTRAINTS:
+        raise ValueError("require=%r is not a valid backend constraint, "
+                         "expected one of %r"
+                         % (require, VALID_BACKEND_CONSTRAINTS))
+
+    if prefer == 'processes' and require == 'sharedmem':
+        raise ValueError("prefer == 'processes' and require == 'sharedmem'"
+                         " are inconsistent settings")
+    backend_and_jobs = getattr(_backend, 'backend_and_jobs', None)
+    if backend_and_jobs is not None:
+        # Try to use the backend set by the user with the context manager.
+        backend, n_jobs = backend_and_jobs
+        supports_sharedmem = getattr(backend, 'supports_sharedmem', False)
+        if require == 'sharedmem' and not supports_sharedmem:
+            # This backend does not match the shared memory constraint:
+            # fallback to the default thead-based backend.
+            sharedmem_backend = BACKENDS[DEFAULT_THREAD_BACKEND]()
+            if verbose >= 10:
+                print("Using %s as joblib.Parallel backend instead of %s "
+                      "as the latter does not provide shared memory semantics."
+                      % (sharedmem_backend.__class__.__name__,
+                         backend.__class__.__name__))
+            return sharedmem_backend, DEFAULT_N_JOBS
+        else:
+            return backend_and_jobs
+
+    # We are outside of the scope of any parallel_backend context manager,
+    # create the default backend instance now.
+    backend = BACKENDS[DEFAULT_BACKEND]()
+    supports_sharedmem = getattr(backend, 'supports_sharedmem', False)
+    uses_threads = getattr(backend, 'uses_threads', False)
+    if ((require == 'sharedmem' and not supports_sharedmem) or
+            (prefer == 'threads' and not uses_threads)):
+        # Make sure the selected default backend match the soft hints and
+        # hard constraints:
+        backend = BACKENDS[DEFAULT_THREAD_BACKEND]()
+    return backend, DEFAULT_N_JOBS
+
+
+class parallel_backend(object):
     """Change the default backend used by Parallel inside a with block.
 
     If ``backend`` is a string it must match a previously registered
     implementation using the ``register_parallel_backend`` function.
 
-    Alternatively backend can be passed directly as an instance.
+    By default the following backends are available:
+
+    - 'loky': single-host, process-based parallelism (used by default),
+    - 'threading': single-host, thread-based parallelism,
+    - 'multiprocessing': legacy single-host, process-based parallelism.
+
+    'loky' is recommended to run functions that manipulate Python objects.
+    'threading' is a low-overhead alternative that is most efficient for
+    functions that release the Global Interpreter Lock: e.g. I/O-bound code or
+    CPU-bound code in a few calls to native code that explicitly releases the
+    GIL.
+
+    In addition, if the `dask` and `distributed` Python packages are installed,
+    it is possible to use the 'dask' backend for better scheduling of nested
+    parallel calls without over-subscription and potentially distribute
+    parallel calls over a networked cluster of several hosts.
+
+    Alternatively the backend can be passed directly as an instance.
 
     By default all available workers will be used (``n_jobs=-1``) unless the
     caller passes an explicit value for the ``n_jobs`` parameter.
@@ -93,19 +164,31 @@ def parallel_backend(backend, n_jobs=-1, **backend_params):
     .. versionadded:: 0.10
 
     """
-    if isinstance(backend, _basestring):
-        backend = BACKENDS[backend](**backend_params)
-    old_backend_and_jobs = getattr(_backend, 'backend_and_jobs', None)
-    try:
+    def __init__(self, backend, n_jobs=-1, **backend_params):
+        if isinstance(backend, _basestring):
+            if backend not in BACKENDS and backend in EXTERNAL_BACKENDS:
+                register = EXTERNAL_BACKENDS[backend]
+                register()
+
+            backend = BACKENDS[backend](**backend_params)
+
+        self.old_backend_and_jobs = getattr(_backend, 'backend_and_jobs', None)
+        self.new_backend_and_jobs = (backend, n_jobs)
+
         _backend.backend_and_jobs = (backend, n_jobs)
-        # return the backend instance to make it easier to write tests
-        yield backend, n_jobs
-    finally:
-        if old_backend_and_jobs is None:
+
+    def __enter__(self):
+        return self.new_backend_and_jobs
+
+    def __exit__(self, type, value, traceback):
+        self.unregister()
+
+    def unregister(self):
+        if self.old_backend_and_jobs is None:
             if getattr(_backend, 'backend_and_jobs', None) is not None:
                 del _backend.backend_and_jobs
         else:
-            _backend.backend_and_jobs = old_backend_and_jobs
+            _backend.backend_and_jobs = self.old_backend_and_jobs
 
 
 # Under Linux or OS X the default start method of multiprocessing
@@ -120,19 +203,96 @@ def parallel_backend(backend, n_jobs=-1, **backend_params):
     DEFAULT_MP_CONTEXT = None
 
 
+class CloudpickledObjectWrapper(object):
+    def __init__(self, obj):
+        self.pickled_obj = dumps(obj)
+
+    def __reduce__(self):
+        return loads, (self.pickled_obj,)
+
+
+def _need_pickle_wrapping(obj):
+    if isinstance(obj, list) and len(obj) >= 1:
+        # Make the assumption that the content of the list is homogeneously
+        # typed.
+        return _need_pickle_wrapping(obj[0])
+    elif isinstance(obj, dict) and len(obj) >= 1:
+        # Make the assumption that the content of the dict is homogeneously
+        # typed.
+        k, v = next(iter(obj.items()))
+        return _need_pickle_wrapping(v) or _need_pickle_wrapping(k)
+    elif isinstance(obj, partial):
+        return _need_pickle_wrapping(obj.func)
+
+    # Warning: obj.__module__ can be defined and set to None
+    module = getattr(obj, "__module__", None)
+    need_wrap = module is not None and "__main__" in module
+    if callable(obj):
+        # Need wrap if the object is a function defined in a local scope of
+        # another function.
+        func_code = getattr(obj, "__code__", "")
+        need_wrap |= getattr(func_code, "co_flags", 0) & inspect.CO_NESTED
+
+        # Need wrap if the obj is a lambda expression
+        func_name = getattr(obj, "__name__", "")
+        need_wrap |= "<lambda>" in func_name
+
+        # Need wrap if obj is a bound method of an instance of an
+        # interactively defined class
+        method_self = getattr(obj, '__self__', None)
+        if not need_wrap and method_self is not None:
+            # Recursively introspect the instanc of the method
+            return _need_pickle_wrapping(method_self)
+    return need_wrap
+
+
 class BatchedCalls(object):
     """Wrap a sequence of (func, args, kwargs) tuples as a single callable"""
 
-    def __init__(self, iterator_slice):
+    def __init__(self, iterator_slice, backend, pickle_cache=None):
         self.items = list(iterator_slice)
         self._size = len(self.items)
+        self._backend = backend
+        self._pickle_cache = pickle_cache if pickle_cache is not None else {}
 
     def __call__(self):
-        return [func(*args, **kwargs) for func, args, kwargs in self.items]
+        with parallel_backend(self._backend):
+            return [func(*args, **kwargs)
+                    for func, args, kwargs in self.items]
 
     def __len__(self):
         return self._size
 
+    @staticmethod
+    def _wrap_non_picklable_objects(obj, pickle_cache):
+        if not _need_pickle_wrapping(obj):
+            return obj
+        try:
+            wrapped_obj = pickle_cache.get(obj)
+            hashable = True
+        except TypeError:
+            # obj is not hashable: cannot be cached
+            wrapped_obj = None
+            hashable = False
+        if wrapped_obj is None:
+            wrapped_obj = CloudpickledObjectWrapper(obj)
+            if hashable:
+                pickle_cache[obj] = wrapped_obj
+        return wrapped_obj
+
+    def __getstate__(self):
+        items = [(self._wrap_non_picklable_objects(func, self._pickle_cache),
+                  [self._wrap_non_picklable_objects(a, self._pickle_cache)
+                   for a in args],
+                  {k: self._wrap_non_picklable_objects(a, self._pickle_cache)
+                   for k, a in kwargs.items()}
+                  )
+                 for func, args, kwargs in self.items]
+        return (items, self._size, self._backend)
+
+    def __setstate__(self, state):
+        self.items, self._size, self._backend = state
+
 
 ###############################################################################
 # CPU count that works also when multiprocessing has been disabled via
@@ -141,7 +301,8 @@ def cpu_count():
     """Return the number of CPUs."""
     if mp is None:
         return 1
-    return mp.cpu_count()
+
+    return loky.cpu_count()
 
 
 ###############################################################################
@@ -166,21 +327,15 @@ def _verbosity_filter(index, verbose):
 
 
 ###############################################################################
-def delayed(function, check_pickle=True):
-    """Decorator used to capture the arguments of a function.
-
-    Pass `check_pickle=False` when:
-
-    - performing a possibly repeated check is too costly and has been done
-      already once outside of the call to delayed.
-
-    - when used in conjunction `Parallel(backend='threading')`.
-
-    """
+def delayed(function, check_pickle=None):
+    """Decorator used to capture the arguments of a function."""
+    if check_pickle is not None:
+        warnings.warn('check_pickle is deprecated in joblib 0.12 and will be'
+                      ' removed in 0.13', DeprecationWarning)
     # Try to pickle the input function, to catch the problems early when
     # using with multiprocessing:
     if check_pickle:
-        pickle.dumps(function)
+        dumps(function)
 
     def delayed_function(*args, **kwargs):
         return function, args, kwargs
@@ -215,8 +370,9 @@ def __call__(self, out):
         self.parallel._backend.batch_completed(self.batch_size,
                                                this_batch_duration)
         self.parallel.print_progress()
-        if self.parallel._original_iterator is not None:
-            self.parallel.dispatch_next()
+        with self.parallel._lock:
+            if self.parallel._original_iterator is not None:
+                self.parallel.dispatch_next()
 
 
 ###############################################################################
@@ -245,18 +401,18 @@ def register_parallel_backend(name, factory, make_default=False):
 def effective_n_jobs(n_jobs=-1):
     """Determine the number of jobs that can actually run in parallel
 
-    n_jobs is the number of workers requested by the callers.
-    Passing n_jobs=-1 means requesting all available workers for instance
-    matching the number of CPU cores on the worker host(s).
+    n_jobs is the number of workers requested by the callers. Passing n_jobs=-1
+    means requesting all available workers for instance matching the number of
+    CPU cores on the worker host(s).
 
     This method should return a guesstimate of the number of workers that can
     actually perform work concurrently with the currently enabled default
     backend. The primary use case is to make it possible for the caller to know
     in how many chunks to slice the work.
 
-    In general working on larger data chunks is more efficient (less
-    scheduling overhead and better use of CPU cache prefetching heuristics)
-    as long as all the workers have enough work to do.
+    In general working on larger data chunks is more efficient (less scheduling
+    overhead and better use of CPU cache prefetching heuristics) as long as all
+    the workers have enough work to do.
 
     Warning: this function is experimental and subject to change in a future
     version of joblib.
@@ -272,9 +428,11 @@ def effective_n_jobs(n_jobs=-1):
 class Parallel(Logger):
     ''' Helper class for readable parallel mapping.
 
+        Read more in the :ref:`User Guide <parallel>`.
+
         Parameters
         -----------
-        n_jobs: int, default: 1
+        n_jobs: int, default: None
             The maximum number of concurrently running jobs, such as the number
             of Python worker processes when backend="multiprocessing"
             or the size of the thread-pool when backend="threading".
@@ -282,14 +440,19 @@ class Parallel(Logger):
             is used at all, which is useful for debugging. For n_jobs below -1,
             (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all
             CPUs but one are used.
-        backend: str, ParallelBackendBase instance or None, \
-                default: 'multiprocessing'
+            None is a marker for 'unset' that will be interpreted as n_jobs=1
+            (sequential execution) unless the call is performed under a
+            parallel_backend context manager that sets another value for
+            n_jobs.
+        backend: str, ParallelBackendBase instance or None, default: 'loky'
             Specify the parallelization backend implementation.
             Supported backends are:
 
-            - "multiprocessing" used by default, can induce some
+            - "loky" used by default, can induce some
               communication and memory overhead when exchanging input and
               output data with the worker Python processes.
+            - "multiprocessing" previous process-based backend based on
+              `multiprocessing.Pool`. Less robust than `loky`.
             - "threading" is a very low-overhead backend but it suffers
               from the Python Global Interpreter Lock if the called function
               relies a lot on Python objects. "threading" is mostly useful
@@ -300,6 +463,22 @@ class Parallel(Logger):
             - finally, you can register backends by calling
               register_parallel_backend. This will allow you to implement
               a backend of your liking.
+
+            It is not recommended to hard-code the backend name in a call to
+            Parallel in a library. Instead it is recommended to set soft hints
+            (prefer) or hard constraints (require) so as to make it possible
+            for library users to change the backend from the outside using the
+            parallel_backend context manager.
+        prefer: str in {'processes', 'threads'} or None, default: None
+            Soft hint to choose the default backend if no specific backend
+            was selected with the parallel_backend context manager. The
+            default process-based backend is 'loky' and the default
+            thread-based backend is 'threading'.
+        require: 'sharedmem' or None, default None
+            Hard constraint to select the backend. If set to 'sharedmem',
+            the selected backend will be single-host and thread-based even
+            if the user asked for a non-thread based backend with
+            parallel_backend.
         verbose: int, optional
             The verbosity level: if non zero, progress messages are
             printed. Above 50, the output is sent to stdout.
@@ -311,12 +490,13 @@ class Parallel(Logger):
         pre_dispatch: {'all', integer, or expression, as in '3*n_jobs'}
             The number of batches (of tasks) to be pre-dispatched.
             Default is '2*n_jobs'. When batch_size="auto" this is reasonable
-            default and the multiprocessing workers should never starve.
+            default and the workers should never starve.
         batch_size: int or 'auto', default: 'auto'
             The number of atomic tasks to dispatch at once to each
-            worker. When individual evaluations are very fast, multiprocessing
-            can be slower than sequential computation because of the overhead.
-            Batching fast computations together can mitigate this.
+            worker. When individual evaluations are very fast, dispatching
+            calls to workers can be slower than sequential computation because
+            of the overhead. Batching fast computations together can mitigate
+            this.
             The ``'auto'`` strategy keeps track of the time it takes for a batch
             to complete, and dynamically adjusts the batch size to keep the time
             on the order of half a second, using a heuristic. The initial batch
@@ -326,26 +506,26 @@ class Parallel(Logger):
             very little overhead and using larger batch size has not proved to
             bring any gain in that case.
         temp_folder: str, optional
-            Folder to be used by the pool for memmaping large arrays
+            Folder to be used by the pool for memmapping large arrays
             for sharing memory with worker processes. If None, this will try in
             order:
 
             - a folder pointed by the JOBLIB_TEMP_FOLDER environment
               variable,
             - /dev/shm if the folder exists and is writable: this is a
-              RAMdisk filesystem available by default on modern Linux
+              RAM disk filesystem available by default on modern Linux
               distributions,
             - the default system temporary folder that can be
               overridden with TMP, TMPDIR or TEMP environment
               variables, typically /tmp under Unix operating systems.
 
-            Only active when backend="multiprocessing".
+            Only active when backend="loky" or "multiprocessing".
         max_nbytes int, str, or None, optional, 1M by default
             Threshold on the size of arrays passed to the workers that
             triggers automated memory mapping in temp_folder. Can be an int
             in Bytes, or a human-readable string, e.g., '1M' for 1 megabyte.
-            Use None to disable memmaping of large arrays.
-            Only active when backend="multiprocessing".
+            Use None to disable memmapping of large arrays.
+            Only active when backend="loky" or "multiprocessing".
         mmap_mode: {None, 'r+', 'r', 'w+', 'c'}
             Memmapping mode for numpy arrays passed to workers.
             See 'max_nbytes' parameter documentation for more details.
@@ -353,10 +533,10 @@ class Parallel(Logger):
         Notes
         -----
 
-        This object uses the multiprocessing module to compute in
-        parallel the application of a function to many different
-        arguments. The main functionality it brings in addition to
-        using the raw multiprocessing API are (see examples for details):
+        This object uses workers to compute in parallel the application of a
+        function to many different arguments. The main functionality it brings
+        in addition to using the raw multiprocessing or concurrent.futures API
+        are (see examples for details):
 
         * More readable code, in particular since it avoids
           constructing list of arguments.
@@ -405,12 +585,10 @@ class Parallel(Logger):
 
         >>> from time import sleep
         >>> from sklearn.externals.joblib import Parallel, delayed
-        >>> r = Parallel(n_jobs=2, verbose=5)(delayed(sleep)(.1) for _ in range(10)) #doctest: +SKIP
-        [Parallel(n_jobs=2)]: Done   1 out of  10 | elapsed:    0.1s remaining:    0.9s
-        [Parallel(n_jobs=2)]: Done   3 out of  10 | elapsed:    0.2s remaining:    0.5s
-        [Parallel(n_jobs=2)]: Done   6 out of  10 | elapsed:    0.3s remaining:    0.2s
-        [Parallel(n_jobs=2)]: Done   9 out of  10 | elapsed:    0.5s remaining:    0.1s
-        [Parallel(n_jobs=2)]: Done  10 out of  10 | elapsed:    0.5s finished
+        >>> r = Parallel(n_jobs=2, verbose=10)(delayed(sleep)(.2) for _ in range(10)) #doctest: +SKIP
+        [Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.6s
+        [Parallel(n_jobs=2)]: Done   4 tasks      | elapsed:    0.8s
+        [Parallel(n_jobs=2)]: Done  10 out of  10 | elapsed:    1.4s finished
 
         Traceback example, note how the line of the error is indicated
         as well as the values of the parameter passed to the function that
@@ -445,8 +623,7 @@ class Parallel(Logger):
         Using pre_dispatch in a producer/consumer situation, where the
         data is generated on the fly. Note how the producer is first
         called 3 times before the parallel loop is initiated, and then
-        called to generate new data on the fly. In this case the total
-        number of iterations cannot be reported in the progress messages:
+        called to generate new data on the fly:
 
         >>> from math import sqrt
         >>> from sklearn.externals.joblib import Parallel, delayed
@@ -466,18 +643,24 @@ class Parallel(Logger):
         [Parallel(n_jobs=2)]: Done 3 jobs     | elapsed:  0.0s
         Produced 5
         [Parallel(n_jobs=2)]: Done 4 jobs     | elapsed:  0.0s
-        [Parallel(n_jobs=2)]: Done 5 out of 6 | elapsed:  0.0s remaining: 0.0s
+        [Parallel(n_jobs=2)]: Done 6 out of 6 | elapsed:  0.0s remaining: 0.0s
         [Parallel(n_jobs=2)]: Done 6 out of 6 | elapsed:  0.0s finished
 
     '''
-    def __init__(self, n_jobs=1, backend=None, verbose=0, timeout=None,
+    def __init__(self, n_jobs=None, backend=None, verbose=0, timeout=None,
                  pre_dispatch='2 * n_jobs', batch_size='auto',
-                 temp_folder=None, max_nbytes='1M', mmap_mode='r'):
-        active_backend, default_n_jobs = get_active_backend()
-        if backend is None and n_jobs == 1:
+                 temp_folder=None, max_nbytes='1M', mmap_mode='r',
+                 prefer=None, require=None):
+        active_backend, context_n_jobs = get_active_backend(
+            prefer=prefer, require=require, verbose=verbose)
+        if backend is None and n_jobs is None:
             # If we are under a parallel_backend context manager, look up
             # the default number of jobs and use that instead:
-            n_jobs = default_n_jobs
+            n_jobs = context_n_jobs
+        if n_jobs is None:
+            # No specific context override and no specific value request:
+            # default to 1.
+            n_jobs = 1
         self.n_jobs = n_jobs
         self.verbose = verbose
         self.timeout = timeout
@@ -490,6 +673,8 @@ def __init__(self, n_jobs=1, backend=None, verbose=0, timeout=None,
             max_nbytes=max_nbytes,
             mmap_mode=mmap_mode,
             temp_folder=temp_folder,
+            prefer=prefer,
+            require=require,
             verbose=max(0, self.verbose - 50),
         )
         if DEFAULT_MP_CONTEXT is not None:
@@ -514,6 +699,11 @@ def __init__(self, n_jobs=1, backend=None, verbose=0, timeout=None,
                                  % (backend, sorted(BACKENDS.keys())))
             backend = backend_factory()
 
+        if (require == 'sharedmem' and
+                not getattr(backend, 'supports_sharedmem', False)):
+            raise ValueError("Backend %s does not support shared memory"
+                             % backend)
+
         if (batch_size == 'auto' or isinstance(batch_size, Integral) and
                 batch_size > 0):
             self.batch_size = batch_size
@@ -529,7 +719,7 @@ def __init__(self, n_jobs=1, backend=None, verbose=0, timeout=None,
 
         # This lock is used coordinate the main thread of this process with
         # the async callback thread of our the pool.
-        self._lock = threading.Lock()
+        self._lock = threading.RLock()
 
     def __enter__(self):
         self._managed_backend = True
@@ -585,8 +775,14 @@ def _dispatch(self, batch):
 
         dispatch_timestamp = time.time()
         cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)
-        job = self._backend.apply_async(batch, callback=cb)
-        self._jobs.append(job)
+        with self._lock:
+            job_idx = len(self._jobs)
+            job = self._backend.apply_async(batch, callback=cb)
+            # A job can complete so quickly than its callback is
+            # called before we get here, causing self._jobs to
+            # grow. To ensure correct results ordering, .insert is
+            # used (rather than .append) in the following line
+            self._jobs.insert(job_idx, job)
 
     def dispatch_next(self):
         """Dispatch more data for parallel processing
@@ -617,7 +813,9 @@ def dispatch_one_batch(self, iterator):
             batch_size = self.batch_size
 
         with self._lock:
-            tasks = BatchedCalls(itertools.islice(iterator, batch_size))
+            tasks = BatchedCalls(itertools.islice(iterator, batch_size),
+                                 self._backend.get_nested_backend(),
+                                 self._pickle_cache)
             if len(tasks) == 0:
                 # No more tasks available in the iterator: tell caller to stop.
                 return False
@@ -720,24 +918,14 @@ def retrieve(self):
                     ensure_ready = self._managed_backend
                     backend.abort_everything(ensure_ready=ensure_ready)
 
-                if not isinstance(exception, TransportableException):
-                    raise
-                else:
+                if isinstance(exception, TransportableException):
                     # Capture exception to add information on the local
                     # stack in addition to the distant stack
                     this_report = format_outer_frames(context=10,
                                                       stack_start=1)
-                    report = """Multiprocessing exception:
-%s
----------------------------------------------------------------------------
-Sub-process traceback:
----------------------------------------------------------------------------
-%s""" % (this_report, exception.message)
-                    # Convert this to a JoblibException
-                    exception_type = _mk_exception(exception.etype)[0]
-                    exception = exception_type(report)
-
-                    raise exception
+                    raise exception.unwrap(this_report)
+                else:
+                    raise
 
     def __call__(self, iterable):
         if self._jobs:
@@ -745,11 +933,15 @@ def __call__(self, iterable):
         # A flag used to abort the dispatching of jobs in case an
         # exception is found
         self._aborting = False
+
         if not self._managed_backend:
             n_jobs = self._initialize_backend()
         else:
             n_jobs = self._effective_n_jobs()
-
+        self._print("Using backend %s with %d concurrent workers.",
+                    (self._backend.__class__.__name__, n_jobs))
+        if hasattr(self._backend, 'start_call'):
+            self._backend.start_call()
         iterator = iter(iterable)
         pre_dispatch = self.pre_dispatch
 
@@ -772,30 +964,46 @@ def __call__(self, iterable):
         self.n_dispatched_batches = 0
         self.n_dispatched_tasks = 0
         self.n_completed_tasks = 0
+        # Use a caching dict for callables that are pickled with cloudpickle to
+        # improve performances. This cache is used only in the case of
+        # functions that are defined in the __main__ module, functions that are
+        # defined locally (inside another function) and lambda expressions.
+        self._pickle_cache = dict()
         try:
             # Only set self._iterating to True if at least a batch
             # was dispatched. In particular this covers the edge
-            # case of Parallel used with an exhausted iterator.
+            # case of Parallel used with an exhausted iterator. If
+            # self._original_iterator is None, then this means either
+            # that pre_dispatch == "all", n_jobs == 1 or that the first batch
+            # was very quick and its callback already dispatched all the
+            # remaining jobs.
+            self._iterating = False
+            if self.dispatch_one_batch(iterator):
+                self._iterating = self._original_iterator is not None
+
             while self.dispatch_one_batch(iterator):
-                self._iterating = True
-            else:
-                self._iterating = False
+                pass
 
             if pre_dispatch == "all" or n_jobs == 1:
                 # The iterable was consumed all at once by the above for loop.
                 # No need to wait for async callbacks to trigger to
                 # consumption.
                 self._iterating = False
-            self.retrieve()
+
+            with self._backend.retrieval_context():
+                self.retrieve()
             # Make sure that we get a last message telling us we are done
             elapsed_time = time.time() - self._start_time
             self._print('Done %3i out of %3i | elapsed: %s finished',
                         (len(self._output), len(self._output),
                          short_format_time(elapsed_time)))
         finally:
+            if hasattr(self._backend, 'stop_call'):
+                self._backend.stop_call()
             if not self._managed_backend:
                 self._terminate_backend()
             self._jobs = list()
+            self._pickle_cache = None
         output = self._output
         self._output = None
         return output
diff --git a/sklearn/externals/joblib/pool.py b/sklearn/externals/joblib/pool.py
new file mode 100644
index 000000000000..396a3dfb4efc
--- /dev/null
+++ b/sklearn/externals/joblib/pool.py
@@ -0,0 +1,329 @@
+"""Custom implementation of multiprocessing.Pool with custom pickler.
+
+This module provides efficient ways of working with data stored in
+shared memory with numpy.memmap arrays without inducing any memory
+copy between the parent and child processes.
+
+This module should not be imported if multiprocessing is not
+available as it implements subclasses of multiprocessing Pool
+that uses a custom alternative to SimpleQueue.
+
+"""
+# Author: Olivier Grisel <olivier.grisel@ensta.org>
+# Copyright: 2012, Olivier Grisel
+# License: BSD 3 clause
+
+import sys
+import warnings
+from time import sleep
+
+try:
+    WindowsError
+except NameError:
+    WindowsError = type(None)
+
+# Customizable pure Python pickler in Python 2
+# customizable C-optimized pickler under Python 3.3+
+from pickle import Pickler
+
+from pickle import HIGHEST_PROTOCOL
+from io import BytesIO
+
+from .disk import delete_folder
+from ._memmapping_reducer import get_memmapping_reducers
+from ._multiprocessing_helpers import mp, assert_spawning
+
+# We need the class definition to derive from it, not the multiprocessing.Pool
+# factory function
+from multiprocessing.pool import Pool
+
+try:
+    import numpy as np
+except ImportError:
+    np = None
+
+if sys.version_info[:2] > (2, 7):
+    import copyreg
+
+
+###############################################################################
+# Enable custom pickling in Pool queues
+
+class CustomizablePickler(Pickler):
+    """Pickler that accepts custom reducers.
+
+    HIGHEST_PROTOCOL is selected by default as this pickler is used
+    to pickle ephemeral datastructures for interprocess communication
+    hence no backward compatibility is required.
+
+    `reducers` is expected to be a dictionary with key/values
+    being `(type, callable)` pairs where `callable` is a function that
+    give an instance of `type` will return a tuple `(constructor,
+    tuple_of_objects)` to rebuild an instance out of the pickled
+    `tuple_of_objects` as would return a `__reduce__` method. See the
+    standard library documentation on pickling for more details.
+
+    """
+
+    # We override the pure Python pickler as its the only way to be able to
+    # customize the dispatch table without side effects in Python 2.7
+    # to 3.2. For Python 3.3+ leverage the new dispatch_table
+    # feature from http://bugs.python.org/issue14166 that makes it possible
+    # to use the C implementation of the Pickler which is faster.
+
+    def __init__(self, writer, reducers=None, protocol=HIGHEST_PROTOCOL):
+        Pickler.__init__(self, writer, protocol=protocol)
+        if reducers is None:
+            reducers = {}
+        if hasattr(Pickler, 'dispatch'):
+            # Make the dispatch registry an instance level attribute instead of
+            # a reference to the class dictionary under Python 2
+            self.dispatch = Pickler.dispatch.copy()
+        else:
+            # Under Python 3 initialize the dispatch table with a copy of the
+            # default registry
+            self.dispatch_table = copyreg.dispatch_table.copy()
+        for type, reduce_func in reducers.items():
+            self.register(type, reduce_func)
+
+    def register(self, type, reduce_func):
+        """Attach a reducer function to a given type in the dispatch table."""
+        if hasattr(Pickler, 'dispatch'):
+            # Python 2 pickler dispatching is not explicitly customizable.
+            # Let us use a closure to workaround this limitation.
+            def dispatcher(self, obj):
+                reduced = reduce_func(obj)
+                self.save_reduce(obj=obj, *reduced)
+            self.dispatch[type] = dispatcher
+        else:
+            self.dispatch_table[type] = reduce_func
+
+
+class CustomizablePicklingQueue(object):
+    """Locked Pipe implementation that uses a customizable pickler.
+
+    This class is an alternative to the multiprocessing implementation
+    of SimpleQueue in order to make it possible to pass custom
+    pickling reducers, for instance to avoid memory copy when passing
+    memory mapped datastructures.
+
+    `reducers` is expected to be a dict with key / values being
+    `(type, callable)` pairs where `callable` is a function that, given an
+    instance of `type`, will return a tuple `(constructor, tuple_of_objects)`
+    to rebuild an instance out of the pickled `tuple_of_objects` as would
+    return a `__reduce__` method.
+
+    See the standard library documentation on pickling for more details.
+    """
+
+    def __init__(self, context, reducers=None):
+        self._reducers = reducers
+        self._reader, self._writer = context.Pipe(duplex=False)
+        self._rlock = context.Lock()
+        if sys.platform == 'win32':
+            self._wlock = None
+        else:
+            self._wlock = context.Lock()
+        self._make_methods()
+
+    def __getstate__(self):
+        assert_spawning(self)
+        return (self._reader, self._writer, self._rlock, self._wlock,
+                self._reducers)
+
+    def __setstate__(self, state):
+        (self._reader, self._writer, self._rlock, self._wlock,
+         self._reducers) = state
+        self._make_methods()
+
+    def empty(self):
+        return not self._reader.poll()
+
+    def _make_methods(self):
+        self._recv = recv = self._reader.recv
+        racquire, rrelease = self._rlock.acquire, self._rlock.release
+
+        def get():
+            racquire()
+            try:
+                return recv()
+            finally:
+                rrelease()
+
+        self.get = get
+
+        if self._reducers:
+            def send(obj):
+                buffer = BytesIO()
+                CustomizablePickler(buffer, self._reducers).dump(obj)
+                self._writer.send_bytes(buffer.getvalue())
+            self._send = send
+        else:
+            self._send = send = self._writer.send
+        if self._wlock is None:
+            # writes to a message oriented win32 pipe are atomic
+            self.put = send
+        else:
+            wlock_acquire, wlock_release = (
+                self._wlock.acquire, self._wlock.release)
+
+            def put(obj):
+                wlock_acquire()
+                try:
+                    return send(obj)
+                finally:
+                    wlock_release()
+
+            self.put = put
+
+
+class PicklingPool(Pool):
+    """Pool implementation with customizable pickling reducers.
+
+    This is useful to control how data is shipped between processes
+    and makes it possible to use shared memory without useless
+    copies induces by the default pickling methods of the original
+    objects passed as arguments to dispatch.
+
+    `forward_reducers` and `backward_reducers` are expected to be
+    dictionaries with key/values being `(type, callable)` pairs where
+    `callable` is a function that, given an instance of `type`, will return a
+    tuple `(constructor, tuple_of_objects)` to rebuild an instance out of the
+    pickled `tuple_of_objects` as would return a `__reduce__` method.
+    See the standard library documentation about pickling for more details.
+
+    """
+
+    def __init__(self, processes=None, forward_reducers=None,
+                 backward_reducers=None, **kwargs):
+        if forward_reducers is None:
+            forward_reducers = dict()
+        if backward_reducers is None:
+            backward_reducers = dict()
+        self._forward_reducers = forward_reducers
+        self._backward_reducers = backward_reducers
+        poolargs = dict(processes=processes)
+        poolargs.update(kwargs)
+        super(PicklingPool, self).__init__(**poolargs)
+
+    def _setup_queues(self):
+        context = getattr(self, '_ctx', mp)
+        self._inqueue = CustomizablePicklingQueue(context,
+                                                  self._forward_reducers)
+        self._outqueue = CustomizablePicklingQueue(context,
+                                                   self._backward_reducers)
+        self._quick_put = self._inqueue._send
+        self._quick_get = self._outqueue._recv
+
+
+class MemmappingPool(PicklingPool):
+    """Process pool that shares large arrays to avoid memory copy.
+
+    This drop-in replacement for `multiprocessing.pool.Pool` makes
+    it possible to work efficiently with shared memory in a numpy
+    context.
+
+    Existing instances of numpy.memmap are preserved: the child
+    suprocesses will have access to the same shared memory in the
+    original mode except for the 'w+' mode that is automatically
+    transformed as 'r+' to avoid zeroing the original data upon
+    instantiation.
+
+    Furthermore large arrays from the parent process are automatically
+    dumped to a temporary folder on the filesystem such as child
+    processes to access their content via memmapping (file system
+    backed shared memory).
+
+    Note: it is important to call the terminate method to collect
+    the temporary folder used by the pool.
+
+    Parameters
+    ----------
+    processes: int, optional
+        Number of worker processes running concurrently in the pool.
+    initializer: callable, optional
+        Callable executed on worker process creation.
+    initargs: tuple, optional
+        Arguments passed to the initializer callable.
+    temp_folder: str, optional
+        Folder to be used by the pool for memmapping large arrays
+        for sharing memory with worker processes. If None, this will try in
+        order:
+        - a folder pointed by the JOBLIB_TEMP_FOLDER environment variable,
+        - /dev/shm if the folder exists and is writable: this is a RAMdisk
+          filesystem available by default on modern Linux distributions,
+        - the default system temporary folder that can be overridden
+          with TMP, TMPDIR or TEMP environment variables, typically /tmp
+          under Unix operating systems.
+    max_nbytes int or None, optional, 1e6 by default
+        Threshold on the size of arrays passed to the workers that
+        triggers automated memory mapping in temp_folder.
+        Use None to disable memmapping of large arrays.
+    mmap_mode: {'r+', 'r', 'w+', 'c'}
+        Memmapping mode for numpy arrays passed to workers.
+        See 'max_nbytes' parameter documentation for more details.
+    forward_reducers: dictionary, optional
+        Reducers used to pickle objects passed from master to worker
+        processes: see below.
+    backward_reducers: dictionary, optional
+        Reducers used to pickle return values from workers back to the
+        master process.
+    verbose: int, optional
+        Make it possible to monitor how the communication of numpy arrays
+        with the subprocess is handled (pickling or memmapping)
+    prewarm: bool or str, optional, "auto" by default.
+        If True, force a read on newly memmapped array to make sure that OS
+        pre-cache it in memory. This can be useful to avoid concurrent disk
+        access when the same data array is passed to different worker
+        processes. If "auto" (by default), prewarm is set to True, unless the
+        Linux shared memory partition /dev/shm is available and used as temp
+        folder.
+
+    `forward_reducers` and `backward_reducers` are expected to be
+    dictionaries with key/values being `(type, callable)` pairs where
+    `callable` is a function that give an instance of `type` will return
+    a tuple `(constructor, tuple_of_objects)` to rebuild an instance out
+    of the pickled `tuple_of_objects` as would return a `__reduce__`
+    method. See the standard library documentation on pickling for more
+    details.
+
+    """
+
+    def __init__(self, processes=None, temp_folder=None, max_nbytes=1e6,
+                 mmap_mode='r', forward_reducers=None, backward_reducers=None,
+                 verbose=0, context_id=None, prewarm=False, **kwargs):
+
+        if context_id is not None:
+            warnings.warn('context_id is deprecated and ignored in joblib'
+                          ' 0.9.4 and will be removed in 0.11',
+                          DeprecationWarning)
+
+        forward_reducers, backward_reducers, self._temp_folder = \
+            get_memmapping_reducers(
+                id(self), temp_folder=temp_folder, max_nbytes=max_nbytes,
+                mmap_mode=mmap_mode, forward_reducers=forward_reducers,
+                backward_reducers=backward_reducers, verbose=verbose,
+                prewarm=prewarm)
+
+        poolargs = dict(
+            processes=processes,
+            forward_reducers=forward_reducers,
+            backward_reducers=backward_reducers)
+        poolargs.update(kwargs)
+        super(MemmappingPool, self).__init__(**poolargs)
+
+    def terminate(self):
+        n_retries = 10
+        for i in range(n_retries):
+            try:
+                super(MemmappingPool, self).terminate()
+                break
+            except OSError as e:
+                if isinstance(e, WindowsError):
+                    # Workaround  occasional "[Error 5] Access is denied" issue
+                    # when trying to terminate a process under windows.
+                    sleep(0.1)
+                    if i + 1 == n_retries:
+                        warnings.warn("Failed to terminate worker processes in"
+                                      " multiprocessing pool: %r" % e)
+        delete_folder(self._temp_folder)
diff --git a/sklearn/externals/joblib/testing.py b/sklearn/externals/joblib/testing.py
new file mode 100644
index 000000000000..5426c6338651
--- /dev/null
+++ b/sklearn/externals/joblib/testing.py
@@ -0,0 +1,79 @@
+"""
+Helper for testing.
+"""
+
+import sys
+import warnings
+import os.path
+import re
+import subprocess
+import threading
+
+import pytest
+import _pytest
+
+from sklearn.externals.joblib._compat import PY3_OR_LATER
+
+
+raises = pytest.raises
+warns = pytest.warns
+SkipTest = _pytest.runner.Skipped
+skipif = pytest.mark.skipif
+fixture = pytest.fixture
+parametrize = pytest.mark.parametrize
+timeout = pytest.mark.timeout
+
+
+def warnings_to_stdout():
+    """ Redirect all warnings to stdout.
+    """
+    showwarning_orig = warnings.showwarning
+
+    def showwarning(msg, cat, fname, lno, file=None, line=0):
+        showwarning_orig(msg, cat, os.path.basename(fname), line, sys.stdout)
+
+    warnings.showwarning = showwarning
+    # warnings.simplefilter('always')
+
+
+def check_subprocess_call(cmd, timeout=5, stdout_regex=None,
+                          stderr_regex=None):
+    """Runs a command in a subprocess with timeout in seconds.
+
+    Also checks returncode is zero, stdout if stdout_regex is set, and
+    stderr if stderr_regex is set.
+    """
+    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE,
+                            stderr=subprocess.PIPE)
+
+    def kill_process():
+        warnings.warn("Timeout running {}".format(cmd))
+        proc.kill()
+
+    timer = threading.Timer(timeout, kill_process)
+    try:
+        timer.start()
+        stdout, stderr = proc.communicate()
+
+        if PY3_OR_LATER:
+            stdout, stderr = stdout.decode(), stderr.decode()
+        if proc.returncode != 0:
+            message = (
+                'Non-zero return code: {}.\nStdout:\n{}\n'
+                'Stderr:\n{}').format(
+                    proc.returncode, stdout, stderr)
+            raise ValueError(message)
+
+        if (stdout_regex is not None and
+                not re.search(stdout_regex, stdout)):
+            raise ValueError(
+                "Unexpected stdout: {!r} does not match:\n{!r}".format(
+                    stdout_regex, stdout))
+        if (stderr_regex is not None and
+                not re.search(stderr_regex, stderr)):
+            raise ValueError(
+                "Unexpected stderr: {!r} does not match:\n{!r}".format(
+                    stderr_regex, stderr))
+
+    finally:
+        timer.cancel()
diff --git a/sklearn/externals/setup.py b/sklearn/externals/setup.py
index d3869f3ed757..452f7d25d071 100644
--- a/sklearn/externals/setup.py
+++ b/sklearn/externals/setup.py
@@ -4,6 +4,10 @@
 def configuration(parent_package='', top_path=None):
     from numpy.distutils.misc_util import Configuration
     config = Configuration('externals', parent_package, top_path)
-    config.add_subpackage('_joblib')
+    config.add_subpackage('joblib')
+    config.add_subpackage('joblib/externals')
+    config.add_subpackage('joblib/externals/loky')
+    config.add_subpackage('joblib/externals/loky/backend')
+    config.add_subpackage('joblib/externals/cloudpickle')
 
     return config
diff --git a/sklearn/feature_extraction/dict_vectorizer.py b/sklearn/feature_extraction/dict_vectorizer.py
index 3ab717d1cf29..d078a325b69a 100644
--- a/sklearn/feature_extraction/dict_vectorizer.py
+++ b/sklearn/feature_extraction/dict_vectorizer.py
@@ -3,7 +3,6 @@
 # License: BSD 3 clause
 
 from array import array
-from collections import Mapping
 from operator import itemgetter
 
 import numpy as np
@@ -13,6 +12,7 @@
 from ..externals import six
 from ..externals.six.moves import xrange
 from ..utils import check_array, tosequence
+from ..utils.fixes import _Mapping as Mapping
 
 
 def _tosequence(X):
diff --git a/sklearn/feature_extraction/hashing.py b/sklearn/feature_extraction/hashing.py
index 9795d30aa675..744a073090ba 100644
--- a/sklearn/feature_extraction/hashing.py
+++ b/sklearn/feature_extraction/hashing.py
@@ -7,9 +7,18 @@
 import numpy as np
 import scipy.sparse as sp
 
-from . import _hashing
+from ..utils import IS_PYPY
 from ..base import BaseEstimator, TransformerMixin
 
+if not IS_PYPY:
+    from ._hashing import transform as _hashing_transform
+else:
+    def _hashing_transform(*args, **kwargs):
+        raise NotImplementedError(
+                'FeatureHasher is not compatible with PyPy (see '
+                'https://github.com/scikit-learn/scikit-learn/issues/11540 '
+                'for the status updates).')
+
 
 def _iteritems(d):
     """Like d.iteritems, but accepts any collections.Mapping."""
@@ -155,7 +164,7 @@ def transform(self, raw_X):
         elif self.input_type == "string":
             raw_X = (((f, 1) for f in x) for x in raw_X)
         indices, indptr, values = \
-            _hashing.transform(raw_X, self.n_features, self.dtype,
+            _hashing_transform(raw_X, self.n_features, self.dtype,
                                self.alternate_sign)
         n_samples = indptr.shape[0] - 1
 
diff --git a/sklearn/feature_extraction/image.py b/sklearn/feature_extraction/image.py
index 8c4f5b268945..b6e68537c091 100644
--- a/sklearn/feature_extraction/image.py
+++ b/sklearn/feature_extraction/image.py
@@ -286,7 +286,7 @@ def extract_patches(arr, patch_shape=8, extraction_step=1):
 
     patch_strides = arr.strides
 
-    slices = [slice(None, None, st) for st in extraction_step]
+    slices = tuple(slice(None, None, st) for st in extraction_step)
     indexing_strides = arr[slices].strides
 
     patch_indices_shape = ((np.array(arr.shape) - np.array(patch_shape)) //
@@ -472,6 +472,11 @@ def fit(self, X, y=None):
 
         This method is just there to implement the usual API and hence
         work in pipelines.
+
+        Parameters
+        ----------
+        X : array-like, shape [n_samples, n_features]
+            Training data.
         """
         return self
 
diff --git a/sklearn/feature_extraction/setup.py b/sklearn/feature_extraction/setup.py
index 7b71dfdcc83d..761ff1ee5a7d 100644
--- a/sklearn/feature_extraction/setup.py
+++ b/sklearn/feature_extraction/setup.py
@@ -1,4 +1,5 @@
 import os
+import platform
 
 
 def configuration(parent_package='', top_path=None):
@@ -10,10 +11,11 @@ def configuration(parent_package='', top_path=None):
     if os.name == 'posix':
         libraries.append('m')
 
-    config.add_extension('_hashing',
-                         sources=['_hashing.pyx'],
-                         include_dirs=[numpy.get_include()],
-                         libraries=libraries)
+    if platform.python_implementation() != 'PyPy':
+        config.add_extension('_hashing',
+                             sources=['_hashing.pyx'],
+                             include_dirs=[numpy.get_include()],
+                             libraries=libraries)
     config.add_subpackage("tests")
 
     return config
diff --git a/sklearn/feature_extraction/tests/test_feature_hasher.py b/sklearn/feature_extraction/tests/test_feature_hasher.py
index 6f0d6b021495..77a21ff4364a 100644
--- a/sklearn/feature_extraction/tests/test_feature_hasher.py
+++ b/sklearn/feature_extraction/tests/test_feature_hasher.py
@@ -5,7 +5,9 @@
 
 from sklearn.feature_extraction import FeatureHasher
 from sklearn.utils.testing import (assert_raises, assert_true, assert_equal,
-                                   ignore_warnings)
+                                   ignore_warnings, fails_if_pypy)
+
+pytestmark = fails_if_pypy
 
 
 def test_feature_hasher_dicts():
diff --git a/sklearn/feature_extraction/tests/test_image.py b/sklearn/feature_extraction/tests/test_image.py
index dc9367980600..516c18c2b928 100644
--- a/sklearn/feature_extraction/tests/test_image.py
+++ b/sklearn/feature_extraction/tests/test_image.py
@@ -11,7 +11,8 @@
 from sklearn.feature_extraction.image import (
     img_to_graph, grid_to_graph, extract_patches_2d,
     reconstruct_from_patches_2d, PatchExtractor, extract_patches)
-from sklearn.utils.testing import assert_equal, assert_true, assert_raises
+from sklearn.utils.testing import (assert_equal, assert_true, assert_raises,
+                                   ignore_warnings)
 
 
 def test_img_to_graph():
@@ -55,6 +56,7 @@ def test_grid_to_graph():
     assert_true(A.dtype == np.float64)
 
 
+@ignore_warnings(category=DeprecationWarning)  # scipy deprecation inside face
 def test_connect_regions():
     try:
         face = sp.face(gray=True)
@@ -68,6 +70,7 @@ def test_connect_regions():
         assert_equal(ndimage.label(mask)[1], connected_components(graph)[0])
 
 
+@ignore_warnings(category=DeprecationWarning)  # scipy deprecation inside face
 def test_connect_regions_with_grid():
     try:
         face = sp.face(gray=True)
@@ -301,9 +304,9 @@ def test_extract_patches_strided():
         ndim = len(image_shape)
 
         assert_true(patches.shape[:ndim] == expected_view)
-        last_patch_slices = [slice(i, i + j, None) for i, j in
-                             zip(last_patch, patch_size)]
-        assert_true((patches[[slice(-1, None, None)] * ndim] ==
+        last_patch_slices = tuple(slice(i, i + j, None) for i, j in
+                                  zip(last_patch, patch_size))
+        assert_true((patches[(-1, None, None) * ndim] ==
                     image[last_patch_slices].squeeze()).all())
 
 
diff --git a/sklearn/feature_extraction/tests/test_text.py b/sklearn/feature_extraction/tests/test_text.py
index 9cbc7004a227..b9431bc5439c 100644
--- a/sklearn/feature_extraction/tests/test_text.py
+++ b/sklearn/feature_extraction/tests/test_text.py
@@ -4,6 +4,7 @@
 import pytest
 from scipy import sparse
 
+from sklearn.externals.six import PY2
 from sklearn.feature_extraction.text import strip_tags
 from sklearn.feature_extraction.text import strip_accents_unicode
 from sklearn.feature_extraction.text import strip_accents_ascii
@@ -26,15 +27,16 @@
 import numpy as np
 from numpy.testing import assert_array_almost_equal
 from numpy.testing import assert_array_equal
+from sklearn.utils import IS_PYPY
 from sklearn.utils.testing import (assert_equal, assert_false, assert_true,
                                    assert_not_equal, assert_almost_equal,
                                    assert_in, assert_less, assert_greater,
                                    assert_warns_message, assert_raise_message,
                                    clean_warning_registry, ignore_warnings,
-                                   SkipTest, assert_raises,
-                                   assert_allclose_dense_sparse)
-
-from collections import defaultdict, Mapping
+                                   SkipTest, assert_raises, assert_no_warnings,
+                                   fails_if_pypy, assert_allclose_dense_sparse)
+from sklearn.utils.fixes import _Mapping as Mapping
+from collections import defaultdict
 from functools import partial
 import pickle
 from io import StringIO
@@ -502,6 +504,7 @@ def test_tfidf_vectorizer_setters():
     assert_true(tv._tfidf.sublinear_tf)
 
 
+@fails_if_pypy
 @ignore_warnings(category=DeprecationWarning)
 def test_hashing_vectorizer():
     v = HashingVectorizer()
@@ -684,6 +687,7 @@ def test_count_binary_occurrences():
     assert_equal(X_sparse.dtype, np.float32)
 
 
+@fails_if_pypy
 @ignore_warnings(category=DeprecationWarning)
 def test_hashed_binary_occurrences():
     # by default multiple occurrences are counted as longs
@@ -730,6 +734,8 @@ def test_vectorizer_inverse_transform(Vectorizer):
         assert_array_equal(np.sort(terms), np.sort(terms2))
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_count_vectorizer_pipeline_grid_selection():
     # raw documents
     data = JUNK_FOOD_DOCS + NOTJUNK_FOOD_DOCS
@@ -766,6 +772,8 @@ def test_count_vectorizer_pipeline_grid_selection():
     assert_equal(best_vectorizer.ngram_range, (1, 1))
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_vectorizer_pipeline_grid_selection():
     # raw documents
     data = JUNK_FOOD_DOCS + NOTJUNK_FOOD_DOCS
@@ -819,6 +827,7 @@ def test_vectorizer_pipeline_cross_validation():
     assert_array_equal(cv_scores, [1., 1., 1.])
 
 
+@fails_if_pypy
 @ignore_warnings(category=DeprecationWarning)
 def test_vectorizer_unicode():
     # tests that the count vectorizer works with cyrillic.
@@ -886,9 +895,12 @@ def test_pickling_vectorizer():
         copy = pickle.loads(s)
         assert_equal(type(copy), orig.__class__)
         assert_equal(copy.get_params(), orig.get_params())
-        assert_array_equal(
-            copy.fit_transform(JUNK_FOOD_DOCS).toarray(),
-            orig.fit_transform(JUNK_FOOD_DOCS).toarray())
+        if IS_PYPY and isinstance(orig, HashingVectorizer):
+            continue
+        else:
+            assert_array_equal(
+                copy.fit_transform(JUNK_FOOD_DOCS).toarray(),
+                orig.fit_transform(JUNK_FOOD_DOCS).toarray())
 
 
 def test_countvectorizer_vocab_sets_when_pickling():
@@ -990,6 +1002,7 @@ def test_non_unique_vocab():
     assert_raises(ValueError, vect.fit, [])
 
 
+@fails_if_pypy
 def test_hashingvectorizer_nan_in_docs():
     # np.nan can appear when using pandas to load text fields from a csv file
     # with missing values.
@@ -1104,3 +1117,26 @@ def test_vectorizers_invalid_ngram_range(vec):
     if isinstance(vec, HashingVectorizer):
         assert_raise_message(
             ValueError, message, vec.transform, ["good news everyone"])
+
+
+def test_vectorizer_stop_words_inconsistent():
+    if PY2:
+        lstr = "[u'and', u'll', u've']"
+    else:
+        lstr = "['and', 'll', 've']"
+    message = ('Your stop_words may be inconsistent with your '
+               'preprocessing. Tokenizing the stop words generated '
+               'tokens %s not in stop_words.' % lstr)
+    for vec in [CountVectorizer(),
+                TfidfVectorizer(), HashingVectorizer()]:
+        vec.set_params(stop_words=["you've", "you", "you'll", 'AND'])
+        assert_warns_message(UserWarning, message, vec.fit_transform,
+                             ['hello world'])
+
+    # Only one warning per stop list
+    assert_no_warnings(vec.fit_transform, ['hello world'])
+
+    # Test caching of inconsistency assessment
+    vec.set_params(stop_words=["you've", "you", "you'll", 'blah', 'AND'])
+    assert_warns_message(UserWarning, message, vec.fit_transform,
+                         ['hello world'])
diff --git a/sklearn/feature_extraction/text.py b/sklearn/feature_extraction/text.py
index e96693599da7..05f60d2805c7 100644
--- a/sklearn/feature_extraction/text.py
+++ b/sklearn/feature_extraction/text.py
@@ -14,7 +14,7 @@
 from __future__ import unicode_literals, division
 
 import array
-from collections import Mapping, defaultdict
+from collections import defaultdict
 import numbers
 from operator import itemgetter
 import re
@@ -32,6 +32,8 @@
 from .stop_words import ENGLISH_STOP_WORDS
 from ..utils.validation import check_is_fitted, check_array, FLOAT_DTYPES
 from ..utils.fixes import sp_version
+from ..utils.fixes import _Mapping as Mapping  # noqa
+
 
 __all__ = ['CountVectorizer',
            'ENGLISH_STOP_WORDS',
@@ -49,6 +51,11 @@ def strip_accents_unicode(s):
     implementation 20 times slower than the strip_accents_ascii basic
     normalization.
 
+    Parameters
+    ----------
+    s : string
+        The string to strip
+
     See also
     --------
     strip_accents_ascii
@@ -68,6 +75,11 @@ def strip_accents_ascii(s):
     Warning: this solution is only suited for languages that have a direct
     transliteration to ASCII symbols.
 
+    Parameters
+    ----------
+    s : string
+        The string to strip
+
     See also
     --------
     strip_accents_unicode
@@ -82,6 +94,11 @@ def strip_tags(s):
 
     For serious HTML/XML preprocessing you should rather use an external
     library such as lxml or BeautifulSoup.
+
+    Parameters
+    ----------
+    s : string
+        The string to strip
     """
     return re.compile(r"<([^>]+)>", flags=re.UNICODE).sub(" ", s)
 
@@ -106,6 +123,11 @@ def decode(self, doc):
         """Decode the input into a string of unicode symbols
 
         The decoding strategy depends on the vectorizer parameters.
+
+        Parameters
+        ----------
+        doc : string
+            The string to decode
         """
         if self.input == 'filename':
             with open(doc, 'rb') as fh:
@@ -246,6 +268,23 @@ def get_stop_words(self):
         """Build or fetch the effective stop words list"""
         return _check_stop_list(self.stop_words)
 
+    def _check_stop_words_consistency(self, stop_words, preprocess, tokenize):
+        # NB: stop_words is validated, unlike self.stop_words
+        if id(self.stop_words) != getattr(self, '_stop_words_id', None):
+            inconsistent = set()
+            for w in stop_words or ():
+                tokens = list(tokenize(preprocess(w)))
+                for token in tokens:
+                    if token not in stop_words:
+                        inconsistent.add(token)
+            self._stop_words_id = id(self.stop_words)
+
+            if inconsistent:
+                warnings.warn('Your stop_words may be inconsistent with your '
+                              'preprocessing. Tokenizing the stop words '
+                              'generated tokens %r not in stop_words.' %
+                              sorted(inconsistent))
+
     def build_analyzer(self):
         """Return a callable that handles preprocessing and tokenization"""
         if callable(self.analyzer):
@@ -263,7 +302,8 @@ def build_analyzer(self):
         elif self.analyzer == 'word':
             stop_words = self.get_stop_words()
             tokenize = self.build_tokenizer()
-
+            self._check_stop_words_consistency(stop_words, preprocess,
+                                               tokenize)
             return lambda doc: self._word_ngrams(
                 tokenize(preprocess(self.decode(doc))), stop_words)
 
@@ -391,13 +431,8 @@ class HashingVectorizer(BaseEstimator, VectorizerMixin, TransformerMixin):
         Both 'ascii' and 'unicode' use NFKD normalization from
         :func:`unicodedata.normalize`.
 
-    analyzer : string, {'word', 'char', 'char_wb'} or callable
-        Whether the feature should be made of word or character n-grams.
-        Option 'char_wb' creates character n-grams only from text inside
-        word boundaries; n-grams at the edges of words are padded with space.
-
-        If a callable is passed it is used to extract the sequence of features
-        out of the raw, unprocessed input.
+    lowercase : boolean, default=True
+        Convert all characters to lowercase before tokenizing.
 
     preprocessor : callable or None (default)
         Override the preprocessing (string transformation) stage while
@@ -408,42 +443,46 @@ class HashingVectorizer(BaseEstimator, VectorizerMixin, TransformerMixin):
         preprocessing and n-grams generation steps.
         Only applies if ``analyzer == 'word'``.
 
-    ngram_range : tuple (min_n, max_n), default=(1, 1)
-        The lower and upper boundary of the range of n-values for different
-        n-grams to be extracted. All values of n such that min_n <= n <= max_n
-        will be used.
-
     stop_words : string {'english'}, list, or None (default)
         If 'english', a built-in stop word list for English is used.
+        There are several known issues with 'english' and you should
+        consider an alternative (see :ref:`stop_words`).
 
         If a list, that list is assumed to contain stop words, all of which
         will be removed from the resulting tokens.
         Only applies if ``analyzer == 'word'``.
 
-    lowercase : boolean, default=True
-        Convert all characters to lowercase before tokenizing.
-
     token_pattern : string
         Regular expression denoting what constitutes a "token", only used
         if ``analyzer == 'word'``. The default regexp selects tokens of 2
         or more alphanumeric characters (punctuation is completely ignored
         and always treated as a token separator).
 
+    ngram_range : tuple (min_n, max_n), default=(1, 1)
+        The lower and upper boundary of the range of n-values for different
+        n-grams to be extracted. All values of n such that min_n <= n <= max_n
+        will be used.
+
+    analyzer : string, {'word', 'char', 'char_wb'} or callable
+        Whether the feature should be made of word or character n-grams.
+        Option 'char_wb' creates character n-grams only from text inside
+        word boundaries; n-grams at the edges of words are padded with space.
+
+        If a callable is passed it is used to extract the sequence of features
+        out of the raw, unprocessed input.
+
     n_features : integer, default=(2 ** 20)
         The number of features (columns) in the output matrices. Small numbers
         of features are likely to cause hash collisions, but large numbers
         will cause larger coefficient dimensions in linear learners.
 
-    norm : 'l1', 'l2' or None, optional
-        Norm used to normalize term vectors. None for no normalization.
-
     binary : boolean, default=False.
         If True, all non zero counts are set to 1. This is useful for discrete
         probabilistic models that model binary events rather than integer
         counts.
 
-    dtype : type, optional
-        Type of the matrix returned by fit_transform() or transform().
+    norm : 'l1', 'l2' or None, optional
+        Norm used to normalize term vectors. None for no normalization.
 
     alternate_sign : boolean, optional, default True
         When True, an alternating sign is added to the features as to
@@ -459,6 +498,22 @@ class HashingVectorizer(BaseEstimator, VectorizerMixin, TransformerMixin):
 
         .. deprecated:: 0.19
             This option will be removed in 0.21.
+    dtype : type, optional
+        Type of the matrix returned by fit_transform() or transform().
+
+    Examples
+    --------
+    >>> from sklearn.feature_extraction.text import HashingVectorizer
+    >>> corpus = [
+    ...     'This is the first document.',
+    ...     'This document is the second document.',
+    ...     'And this is the third one.',
+    ...     'Is this the first document?',
+    ... ]
+    >>> vectorizer = HashingVectorizer(n_features=2**4)
+    >>> X = vectorizer.fit_transform(corpus)
+    >>> print(X.shape)
+    (4, 16)
 
     See also
     --------
@@ -496,11 +551,21 @@ def partial_fit(self, X, y=None):
         This method is just there to mark the fact that this transformer
         can work in a streaming setup.
 
+        Parameters
+        ----------
+        X : array-like, shape [n_samples, n_features]
+            Training data.
         """
         return self
 
     def fit(self, X, y=None):
-        """Does nothing: this transformer is stateless."""
+        """Does nothing: this transformer is stateless.
+
+        Parameters
+        ----------
+        X : array-like, shape [n_samples, n_features]
+            Training data.
+        """
         # triggers a parameter validation
         if isinstance(X, six.string_types):
             raise ValueError(
@@ -623,13 +688,8 @@ class CountVectorizer(BaseEstimator, VectorizerMixin):
         Both 'ascii' and 'unicode' use NFKD normalization from
         :func:`unicodedata.normalize`.
 
-    analyzer : string, {'word', 'char', 'char_wb'} or callable
-        Whether the feature should be made of word or character n-grams.
-        Option 'char_wb' creates character n-grams only from text inside
-        word boundaries; n-grams at the edges of words are padded with space.
-
-        If a callable is passed it is used to extract the sequence of features
-        out of the raw, unprocessed input.
+    lowercase : boolean, True by default
+        Convert all characters to lowercase before tokenizing.
 
     preprocessor : callable or None (default)
         Override the preprocessing (string transformation) stage while
@@ -640,13 +700,10 @@ class CountVectorizer(BaseEstimator, VectorizerMixin):
         preprocessing and n-grams generation steps.
         Only applies if ``analyzer == 'word'``.
 
-    ngram_range : tuple (min_n, max_n)
-        The lower and upper boundary of the range of n-values for different
-        n-grams to be extracted. All values of n such that min_n <= n <= max_n
-        will be used.
-
     stop_words : string {'english'}, list, or None (default)
         If 'english', a built-in stop word list for English is used.
+        There are several known issues with 'english' and you should
+        consider an alternative (see :ref:`stop_words`).
 
         If a list, that list is assumed to contain stop words, all of which
         will be removed from the resulting tokens.
@@ -656,15 +713,25 @@ class CountVectorizer(BaseEstimator, VectorizerMixin):
         in the range [0.7, 1.0) to automatically detect and filter stop
         words based on intra corpus document frequency of terms.
 
-    lowercase : boolean, True by default
-        Convert all characters to lowercase before tokenizing.
-
     token_pattern : string
         Regular expression denoting what constitutes a "token", only used
         if ``analyzer == 'word'``. The default regexp select tokens of 2
         or more alphanumeric characters (punctuation is completely ignored
         and always treated as a token separator).
 
+    ngram_range : tuple (min_n, max_n)
+        The lower and upper boundary of the range of n-values for different
+        n-grams to be extracted. All values of n such that min_n <= n <= max_n
+        will be used.
+
+    analyzer : string, {'word', 'char', 'char_wb'} or callable
+        Whether the feature should be made of word or character n-grams.
+        Option 'char_wb' creates character n-grams only from text inside
+        word boundaries; n-grams at the edges of words are padded with space.
+
+        If a callable is passed it is used to extract the sequence of features
+        out of the raw, unprocessed input.
+
     max_df : float in range [0.0, 1.0] or int, default=1.0
         When building the vocabulary ignore terms that have a document
         frequency strictly higher than the given threshold (corpus-specific
@@ -716,6 +783,25 @@ class CountVectorizer(BaseEstimator, VectorizerMixin):
 
         This is only available if no vocabulary was given.
 
+    Examples
+    --------
+    >>> from sklearn.feature_extraction.text import CountVectorizer
+    >>> corpus = [
+    ...     'This is the first document.',
+    ...     'This document is the second document.',
+    ...     'And this is the third one.',
+    ...     'Is this the first document?',
+    ... ]
+    >>> vectorizer = CountVectorizer()
+    >>> X = vectorizer.fit_transform(corpus)
+    >>> print(vectorizer.get_feature_names())
+    ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']
+    >>> print(X.toarray())  # doctest: +NORMALIZE_WHITESPACE
+    [[0 1 1 1 0 0 1 0 1]
+     [0 2 0 1 0 1 1 0 1]
+     [1 0 0 1 1 0 1 1 1]
+     [0 1 1 1 0 0 1 0 1]]
+
     See also
     --------
     HashingVectorizer, TfidfVectorizer
@@ -1238,11 +1324,8 @@ class TfidfVectorizer(CountVectorizer):
         Both 'ascii' and 'unicode' use NFKD normalization from
         :func:`unicodedata.normalize`.
 
-    analyzer : string, {'word', 'char'} or callable
-        Whether the feature should be made of word or character n-grams.
-
-        If a callable is passed it is used to extract the sequence of features
-        out of the raw, unprocessed input.
+    lowercase : boolean, default True
+        Convert all characters to lowercase before tokenizing.
 
     preprocessor : callable or None (default)
         Override the preprocessing (string transformation) stage while
@@ -1253,15 +1336,18 @@ class TfidfVectorizer(CountVectorizer):
         preprocessing and n-grams generation steps.
         Only applies if ``analyzer == 'word'``.
 
-    ngram_range : tuple (min_n, max_n)
-        The lower and upper boundary of the range of n-values for different
-        n-grams to be extracted. All values of n such that min_n <= n <= max_n
-        will be used.
+    analyzer : string, {'word', 'char'} or callable
+        Whether the feature should be made of word or character n-grams.
+
+        If a callable is passed it is used to extract the sequence of features
+        out of the raw, unprocessed input.
 
     stop_words : string {'english'}, list, or None (default)
         If a string, it is passed to _check_stop_list and the appropriate stop
         list is returned. 'english' is currently the only supported string
         value.
+        There are several known issues with 'english' and you should
+        consider an alternative (see :ref:`stop_words`).
 
         If a list, that list is assumed to contain stop words, all of which
         will be removed from the resulting tokens.
@@ -1271,15 +1357,17 @@ class TfidfVectorizer(CountVectorizer):
         in the range [0.7, 1.0) to automatically detect and filter stop
         words based on intra corpus document frequency of terms.
 
-    lowercase : boolean, default True
-        Convert all characters to lowercase before tokenizing.
-
     token_pattern : string
         Regular expression denoting what constitutes a "token", only used
         if ``analyzer == 'word'``. The default regexp selects tokens of 2
         or more alphanumeric characters (punctuation is completely ignored
         and always treated as a token separator).
 
+    ngram_range : tuple (min_n, max_n)
+        The lower and upper boundary of the range of n-values for different
+        n-grams to be extracted. All values of n such that min_n <= n <= max_n
+        will be used.
+
     max_df : float in range [0.0, 1.0] or int, default=1.0
         When building the vocabulary ignore terms that have a document
         frequency strictly higher than the given threshold (corpus-specific
@@ -1347,6 +1435,22 @@ class TfidfVectorizer(CountVectorizer):
 
         This is only available if no vocabulary was given.
 
+    Examples
+    --------
+    >>> from sklearn.feature_extraction.text import TfidfVectorizer
+    >>> corpus = [
+    ...     'This is the first document.',
+    ...     'This document is the second document.',
+    ...     'And this is the third one.',
+    ...     'Is this the first document?',
+    ... ]
+    >>> vectorizer = TfidfVectorizer()
+    >>> X = vectorizer.fit_transform(corpus)
+    >>> print(vectorizer.get_feature_names())
+    ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']
+    >>> print(X.shape)
+    (4, 9)
+
     See also
     --------
     CountVectorizer
diff --git a/sklearn/feature_selection/base.py b/sklearn/feature_selection/base.py
index 3067d6ef31bc..5bb0b3ea890c 100644
--- a/sklearn/feature_selection/base.py
+++ b/sklearn/feature_selection/base.py
@@ -72,7 +72,7 @@ def transform(self, X):
         X_r : array of shape [n_samples, n_selected_features]
             The input samples with only the selected features.
         """
-        X = check_array(X, accept_sparse='csr')
+        X = check_array(X, dtype=None, accept_sparse='csr')
         mask = self.get_support()
         if not mask.any():
             warn("No features were selected: either the data is"
@@ -111,7 +111,7 @@ def inverse_transform(self, X):
             return Xt
 
         support = self.get_support()
-        X = check_array(X)
+        X = check_array(X, dtype=None)
         if support.sum() != X.shape[1]:
             raise ValueError("X has a different shape than during fitting.")
 
diff --git a/sklearn/feature_selection/from_model.py b/sklearn/feature_selection/from_model.py
index 657259f39ea1..3e2efdbeb1e7 100644
--- a/sklearn/feature_selection/from_model.py
+++ b/sklearn/feature_selection/from_model.py
@@ -2,6 +2,7 @@
 # License: BSD 3 clause
 
 import numpy as np
+import numbers
 
 from .base import SelectorMixin
 from ..base import BaseEstimator, clone, MetaEstimatorMixin
@@ -15,12 +16,13 @@ def _get_feature_importances(estimator, norm_order=1):
     """Retrieve or aggregate feature importances from estimator"""
     importances = getattr(estimator, "feature_importances_", None)
 
-    if importances is None and hasattr(estimator, "coef_"):
+    coef_ = getattr(estimator, "coef_", None)
+    if importances is None and coef_ is not None:
         if estimator.coef_.ndim == 1:
-            importances = np.abs(estimator.coef_)
+            importances = np.abs(coef_)
 
         else:
-            importances = np.linalg.norm(estimator.coef_, axis=0,
+            importances = np.linalg.norm(coef_, axis=0,
                                          ord=norm_order)
 
     elif importances is None:
@@ -113,6 +115,13 @@ class SelectFromModel(BaseEstimator, SelectorMixin, MetaEstimatorMixin):
         ``threshold`` in the case where the ``coef_`` attribute of the
         estimator is of dimension 2.
 
+    max_features : int or None, optional
+        The maximum number of features selected scoring above ``threshold``.
+        To disable ``threshold`` and only select based on ``max_features``,
+        set ``threshold=-np.inf``.
+
+        .. versionadded:: 0.20
+
     Attributes
     ----------
     estimator_ : an estimator
@@ -123,11 +132,13 @@ class SelectFromModel(BaseEstimator, SelectorMixin, MetaEstimatorMixin):
     threshold_ : float
         The threshold value used for feature selection.
     """
-    def __init__(self, estimator, threshold=None, prefit=False, norm_order=1):
+    def __init__(self, estimator, threshold=None, prefit=False,
+                 norm_order=1, max_features=None):
         self.estimator = estimator
         self.threshold = threshold
         self.prefit = prefit
         self.norm_order = norm_order
+        self.max_features = max_features
 
     def _get_support_mask(self):
         # SelectFromModel can directly call on transform.
@@ -136,12 +147,20 @@ def _get_support_mask(self):
         elif hasattr(self, 'estimator_'):
             estimator = self.estimator_
         else:
-            raise ValueError(
-                'Either fit SelectFromModel before transform or set "prefit='
-                'True" and pass a fitted estimator to the constructor.')
+            raise ValueError('Either fit the model before transform or set'
+                             ' "prefit=True" while passing the fitted'
+                             ' estimator to the constructor.')
         scores = _get_feature_importances(estimator, self.norm_order)
         threshold = _calculate_threshold(estimator, scores, self.threshold)
-        return scores >= threshold
+        if self.max_features is not None:
+            mask = np.zeros_like(scores, dtype=bool)
+            candidate_indices = \
+                np.argsort(-scores, kind='mergesort')[:self.max_features]
+            mask[candidate_indices] = True
+        else:
+            mask = np.ones_like(scores, dtype=bool)
+        mask[scores < threshold] = False
+        return mask
 
     def fit(self, X, y=None, **fit_params):
         """Fit the SelectFromModel meta-transformer.
@@ -161,6 +180,16 @@ def fit(self, X, y=None, **fit_params):
         -------
         self : object
         """
+        if self.max_features is not None:
+            if not isinstance(self.max_features, numbers.Integral):
+                raise TypeError("'max_features' should be an integer between"
+                                " 0 and {} features. Got {!r} instead."
+                                .format(X.shape[1], self.max_features))
+            elif self.max_features < 0 or self.max_features > X.shape[1]:
+                raise ValueError("'max_features' should be 0 and {} features."
+                                 "Got {} instead."
+                                 .format(X.shape[1], self.max_features))
+
         if self.prefit:
             raise NotFittedError(
                 "Since 'prefit=True', call transform directly")
diff --git a/sklearn/feature_selection/rfe.py b/sklearn/feature_selection/rfe.py
index 84761451c8f2..6aa4c101747e 100644
--- a/sklearn/feature_selection/rfe.py
+++ b/sklearn/feature_selection/rfe.py
@@ -15,7 +15,7 @@
 from ..base import MetaEstimatorMixin
 from ..base import clone
 from ..base import is_classifier
-from ..externals.joblib import Parallel, delayed
+from ..utils import Parallel, delayed, effective_n_jobs
 from ..model_selection import check_cv
 from ..model_selection._validation import _score
 from ..metrics.scorer import check_scoring
@@ -60,12 +60,12 @@ class RFE(BaseEstimator, MetaEstimatorMixin, SelectorMixin):
         are selected.
 
     step : int or float, optional (default=1)
-        If greater than or equal to 1, then `step` corresponds to the (integer)
-        number of features to remove at each iteration.
-        If within (0.0, 1.0), then `step` corresponds to the percentage
+        If greater than or equal to 1, then ``step`` corresponds to the
+        (integer) number of features to remove at each iteration.
+        If within (0.0, 1.0), then ``step`` corresponds to the percentage
         (rounded down) of features to remove at each iteration.
 
-    verbose : int, default=0
+    verbose : int, (default=0)
         Controls verbosity of output.
 
     Attributes
@@ -262,16 +262,61 @@ def _get_support_mask(self):
 
     @if_delegate_has_method(delegate='estimator')
     def decision_function(self, X):
+        """Compute the decision function of ``X``.
+
+        Parameters
+        ----------
+        X : array-like or sparse matrix, shape = [n_samples, n_features]
+            The input samples. Internally, it will be converted to
+            ``dtype=np.float32`` and if a sparse matrix is provided
+            to a sparse ``csr_matrix``.
+
+        Returns
+        -------
+        score : array, shape = [n_samples, n_classes] or [n_samples]
+            The decision function of the input samples. The order of the
+            classes corresponds to that in the attribute `classes_`.
+            Regression and binary classification produce an array of shape
+            [n_samples].
+        """
         check_is_fitted(self, 'estimator_')
         return self.estimator_.decision_function(self.transform(X))
 
     @if_delegate_has_method(delegate='estimator')
     def predict_proba(self, X):
+        """Predict class probabilities for X.
+
+        Parameters
+        ----------
+        X : array-like or sparse matrix, shape = [n_samples, n_features]
+            The input samples. Internally, it will be converted to
+            ``dtype=np.float32`` and if a sparse matrix is provided
+            to a sparse ``csr_matrix``.
+
+        Returns
+        -------
+        p : array of shape = [n_samples, n_classes]
+            The class probabilities of the input samples. The order of the
+            classes corresponds to that in the attribute `classes_`.
+        """
         check_is_fitted(self, 'estimator_')
         return self.estimator_.predict_proba(self.transform(X))
 
     @if_delegate_has_method(delegate='estimator')
     def predict_log_proba(self, X):
+        """Predict class log-probabilities for X.
+
+        Parameters
+        ----------
+        X : array of shape [n_samples, n_features]
+            The input samples.
+
+        Returns
+        -------
+        p : array of shape = [n_samples, n_classes]
+            The class log-probabilities of the input samples. The order of the
+            classes corresponds to that in the attribute `classes_`.
+        """
         check_is_fitted(self, 'estimator_')
         return self.estimator_.predict_log_proba(self.transform(X))
 
@@ -290,10 +335,18 @@ class RFECV(RFE, MetaEstimatorMixin):
         attribute or through a ``feature_importances_`` attribute.
 
     step : int or float, optional (default=1)
-        If greater than or equal to 1, then `step` corresponds to the (integer)
-        number of features to remove at each iteration.
-        If within (0.0, 1.0), then `step` corresponds to the percentage
+        If greater than or equal to 1, then ``step`` corresponds to the
+        (integer) number of features to remove at each iteration.
+        If within (0.0, 1.0), then ``step`` corresponds to the percentage
         (rounded down) of features to remove at each iteration.
+        Note that the last iteration may remove fewer than ``step`` features in
+        order to reach ``min_features_to_select``.
+
+    min_features_to_select : int, (default=1)
+        The minimum number of features to be selected. This number of features
+        will always be scored, even if the difference between the original
+        feature count and ``min_features_to_select`` isn't divisible by
+        ``step``.
 
     cv : int, cross-validation generator or an iterable, optional
         Determines the cross-validation splitting strategy.
@@ -312,18 +365,23 @@ class RFECV(RFE, MetaEstimatorMixin):
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
-    scoring : string, callable or None, optional, default: None
+        .. versionchanged:: 0.20
+            ``cv`` default value of None will change from 3-fold to 5-fold
+            in v0.22.
+
+    scoring : string, callable or None, optional, (default=None)
         A string (see model evaluation documentation) or
         a scorer callable object / function with signature
         ``scorer(estimator, X, y)``.
 
-    verbose : int, default=0
+    verbose : int, (default=0)
         Controls verbosity of output.
 
-    n_jobs : int, default 1
+    n_jobs : int or None, optional (default=None)
         Number of cores to run in parallel while fitting across folds.
-        Defaults to 1 core. If `n_jobs=-1`, then number of jobs is set
-        to number of cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -350,7 +408,8 @@ class RFECV(RFE, MetaEstimatorMixin):
 
     Notes
     -----
-    The size of ``grid_scores_`` is equal to ceil((n_features - 1) / step) + 1,
+    The size of ``grid_scores_`` is equal to
+    ``ceil((n_features - min_features_to_select) / step) + 1``,
     where step is the number of features removed at each iteration.
 
     Examples
@@ -382,14 +441,15 @@ class RFECV(RFE, MetaEstimatorMixin):
            for cancer classification using support vector machines",
            Mach. Learn., 46(1-3), 389--422, 2002.
     """
-    def __init__(self, estimator, step=1, cv=None, scoring=None, verbose=0,
-                 n_jobs=1):
+    def __init__(self, estimator, step=1, min_features_to_select=1, cv='warn',
+                 scoring=None, verbose=0, n_jobs=None):
         self.estimator = estimator
         self.step = step
         self.cv = cv
         self.scoring = scoring
         self.verbose = verbose
         self.n_jobs = n_jobs
+        self.min_features_to_select = min_features_to_select
 
     def fit(self, X, y, groups=None):
         """Fit the RFE model and automatically tune the number of selected
@@ -415,7 +475,6 @@ def fit(self, X, y, groups=None):
         cv = check_cv(self.cv, y, is_classifier(self.estimator))
         scorer = check_scoring(self.estimator, scoring=self.scoring)
         n_features = X.shape[1]
-        n_features_to_select = 1
 
         if 0.0 < self.step < 1.0:
             step = int(max(1, self.step * n_features))
@@ -424,8 +483,10 @@ def fit(self, X, y, groups=None):
         if step <= 0:
             raise ValueError("Step must be >0")
 
+        # Build an RFE object, which will evaluate and score each possible
+        # feature count, down to self.min_features_to_select
         rfe = RFE(estimator=self.estimator,
-                  n_features_to_select=n_features_to_select,
+                  n_features_to_select=self.min_features_to_select,
                   step=self.step, verbose=self.verbose)
 
         # Determine the number of subsets of features by fitting across
@@ -440,10 +501,11 @@ def fit(self, X, y, groups=None):
         # and provides bound methods as scorers is not broken with the
         # addition of n_jobs parameter in version 0.18.
 
-        if self.n_jobs == 1:
+        if effective_n_jobs(self.n_jobs) == 1:
             parallel, func = list, _rfe_single_fit
         else:
-            parallel, func, = Parallel(n_jobs=self.n_jobs), delayed(_rfe_single_fit)
+            parallel = Parallel(n_jobs=self.n_jobs)
+            func = delayed(_rfe_single_fit)
 
         scores = parallel(
             func(rfe, self.estimator, X, y, train, test, scorer)
@@ -454,7 +516,7 @@ def fit(self, X, y, groups=None):
         argmax_idx = len(scores) - np.argmax(scores_rev) - 1
         n_features_to_select = max(
             n_features - (argmax_idx * step),
-            n_features_to_select)
+            self.min_features_to_select)
 
         # Re-execute an elimination with best_k over the whole set
         rfe = RFE(estimator=self.estimator,
diff --git a/sklearn/feature_selection/tests/test_from_model.py b/sklearn/feature_selection/tests/test_from_model.py
index 6efec43dce37..e6bb76c5e19a 100644
--- a/sklearn/feature_selection/tests/test_from_model.py
+++ b/sklearn/feature_selection/tests/test_from_model.py
@@ -1,3 +1,4 @@
+import pytest
 import numpy as np
 
 from sklearn.utils.testing import assert_true
@@ -8,6 +9,7 @@
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_almost_equal
+from sklearn.utils.testing import assert_allclose
 from sklearn.utils.testing import assert_raises
 from sklearn.utils.testing import skip_if_32bit
 
@@ -17,6 +19,7 @@
 from sklearn.feature_selection import SelectFromModel
 from sklearn.ensemble import RandomForestClassifier
 from sklearn.linear_model import PassiveAggressiveClassifier
+from sklearn.base import BaseEstimator
 
 iris = datasets.load_iris()
 data, y = iris.data, iris.target
@@ -32,6 +35,7 @@ def test_invalid_input():
         assert_raises(ValueError, model.transform, data)
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_input_estimator_unchanged():
     # Test that SelectFromModel fits on a clone of the estimator.
     est = RandomForestClassifier()
@@ -40,6 +44,121 @@ def test_input_estimator_unchanged():
     assert_true(transformer.estimator is est)
 
 
+@pytest.mark.parametrize(
+    "max_features, err_type, err_msg",
+    [(-1, ValueError, "'max_features' should be 0 and"),
+     (data.shape[1] + 1, ValueError, "'max_features' should be 0 and"),
+     ('gobbledigook', TypeError, "should be an integer"),
+     ('all', TypeError, "should be an integer")]
+)
+def test_max_features_error(max_features, err_type, err_msg):
+    clf = RandomForestClassifier(n_estimators=50, random_state=0)
+
+    transformer = SelectFromModel(estimator=clf,
+                                  max_features=max_features,
+                                  threshold=-np.inf)
+    with pytest.raises(err_type, match=err_msg):
+        transformer.fit(data, y)
+
+
+@pytest.mark.parametrize("max_features", [0, 2, data.shape[1]])
+def test_max_features_dim(max_features):
+    clf = RandomForestClassifier(n_estimators=50, random_state=0)
+    transformer = SelectFromModel(estimator=clf,
+                                  max_features=max_features,
+                                  threshold=-np.inf)
+    X_trans = transformer.fit_transform(data, y)
+    assert X_trans.shape[1] == max_features
+
+
+class FixedImportanceEstimator(BaseEstimator):
+    def __init__(self, importances):
+        self.importances = importances
+
+    def fit(self, X, y=None):
+        self.feature_importances_ = np.array(self.importances)
+
+
+def test_max_features():
+    # Test max_features parameter using various values
+    X, y = datasets.make_classification(
+        n_samples=1000, n_features=10, n_informative=3, n_redundant=0,
+        n_repeated=0, shuffle=False, random_state=0)
+    max_features = X.shape[1]
+    est = RandomForestClassifier(n_estimators=50, random_state=0)
+
+    transformer1 = SelectFromModel(estimator=est,
+                                   threshold=-np.inf)
+    transformer2 = SelectFromModel(estimator=est,
+                                   max_features=max_features,
+                                   threshold=-np.inf)
+    X_new1 = transformer1.fit_transform(X, y)
+    X_new2 = transformer2.fit_transform(X, y)
+    assert_allclose(X_new1, X_new2)
+
+    # Test max_features against actual model.
+    transformer1 = SelectFromModel(estimator=Lasso(alpha=0.025,
+                                                   random_state=42))
+    X_new1 = transformer1.fit_transform(X, y)
+    scores1 = np.abs(transformer1.estimator_.coef_)
+    candidate_indices1 = np.argsort(-scores1, kind='mergesort')
+
+    for n_features in range(1, X_new1.shape[1] + 1):
+        transformer2 = SelectFromModel(estimator=Lasso(alpha=0.025,
+                                       random_state=42),
+                                       max_features=n_features,
+                                       threshold=-np.inf)
+        X_new2 = transformer2.fit_transform(X, y)
+        scores2 = np.abs(transformer2.estimator_.coef_)
+        candidate_indices2 = np.argsort(-scores2, kind='mergesort')
+        assert_allclose(X[:, candidate_indices1[:n_features]],
+                        X[:, candidate_indices2[:n_features]])
+    assert_allclose(transformer1.estimator_.coef_,
+                    transformer2.estimator_.coef_)
+
+
+def test_max_features_tiebreak():
+    # Test if max_features can break tie among feature importance
+    X, y = datasets.make_classification(
+        n_samples=1000, n_features=10, n_informative=3, n_redundant=0,
+        n_repeated=0, shuffle=False, random_state=0)
+    max_features = X.shape[1]
+
+    feature_importances = np.array([4, 4, 4, 4, 3, 3, 3, 2, 2, 1])
+    for n_features in range(1, max_features + 1):
+        transformer = SelectFromModel(
+            FixedImportanceEstimator(feature_importances),
+            max_features=n_features,
+            threshold=-np.inf)
+        X_new = transformer.fit_transform(X, y)
+        selected_feature_indices = np.where(transformer._get_support_mask())[0]
+        assert_array_equal(selected_feature_indices, np.arange(n_features))
+        assert X_new.shape[1] == n_features
+
+
+def test_threshold_and_max_features():
+    X, y = datasets.make_classification(
+        n_samples=1000, n_features=10, n_informative=3, n_redundant=0,
+        n_repeated=0, shuffle=False, random_state=0)
+    est = RandomForestClassifier(n_estimators=50, random_state=0)
+
+    transformer1 = SelectFromModel(estimator=est, max_features=3,
+                                   threshold=-np.inf)
+    X_new1 = transformer1.fit_transform(X, y)
+
+    transformer2 = SelectFromModel(estimator=est, threshold=0.04)
+    X_new2 = transformer2.fit_transform(X, y)
+
+    transformer3 = SelectFromModel(estimator=est, max_features=3,
+                                   threshold=0.04)
+    X_new3 = transformer3.fit_transform(X, y)
+    assert X_new3.shape[1] == min(X_new1.shape[1], X_new2.shape[1])
+    selected_indices = transformer3.transform(
+        np.arange(X.shape[1])[np.newaxis, :])
+    assert_allclose(X_new3, X[:, selected_indices[0]])
+
+
+@skip_if_32bit
 def test_feature_importances():
     X, y = datasets.make_classification(
         n_samples=1000, n_features=10, n_informative=3, n_redundant=0,
@@ -87,7 +206,8 @@ def test_coef_default_threshold():
         n_repeated=0, shuffle=False, random_state=0)
 
     # For the Lasso and related models, the threshold defaults to 1e-5
-    transformer = SelectFromModel(estimator=Lasso(alpha=0.1))
+    transformer = SelectFromModel(estimator=Lasso(alpha=0.1,
+                                  random_state=42))
     transformer.fit(X, y)
     X_new = transformer.transform(X)
     mask = np.abs(transformer.estimator_.coef_) > 1e-5
@@ -119,6 +239,7 @@ def test_2d_coef():
             assert_array_almost_equal(X_new, X[:, feature_mask])
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_partial_fit():
     est = PassiveAggressiveClassifier(random_state=0, shuffle=False,
                                       max_iter=5, tol=None)
diff --git a/sklearn/feature_selection/tests/test_rfe.py b/sklearn/feature_selection/tests/test_rfe.py
index 3cee0fa6f605..41b4a9e767c1 100644
--- a/sklearn/feature_selection/tests/test_rfe.py
+++ b/sklearn/feature_selection/tests/test_rfe.py
@@ -1,6 +1,9 @@
 """
 Testing Recursive feature elimination
 """
+from __future__ import division
+
+import pytest
 import numpy as np
 from numpy.testing import assert_array_almost_equal, assert_array_equal
 from scipy import sparse
@@ -228,6 +231,27 @@ def test_rfecv_verbose_output():
     assert_greater(len(verbose_output.readline()), 0)
 
 
+def test_rfecv_grid_scores_size():
+    generator = check_random_state(0)
+    iris = load_iris()
+    X = np.c_[iris.data, generator.normal(size=(len(iris.data), 6))]
+    y = list(iris.target)   # regression test: list should be supported
+
+    # Non-regression test for varying combinations of step and
+    # min_features_to_select.
+    for step, min_features_to_select in [[2, 1], [2, 2], [3, 3]]:
+        rfecv = RFECV(estimator=MockClassifier(), step=step,
+                      min_features_to_select=min_features_to_select, cv=5)
+        rfecv.fit(X, y)
+
+        score_len = np.ceil(
+            (X.shape[1] - min_features_to_select) / step) + 1
+        assert len(rfecv.grid_scores_) == score_len
+        assert len(rfecv.ranking_) == X.shape[1]
+        assert rfecv.n_features_ >= min_features_to_select
+
+
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_rfe_estimator_tags():
     rfe = RFE(SVC(kernel='linear'))
     assert_equal(rfe._estimator_type, "classifier")
@@ -319,6 +343,7 @@ def formula2(n_features, n_features_to_select, step):
                      formula2(n_features, n_features_to_select, step))
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_rfe_cv_n_jobs():
     generator = check_random_state(0)
     iris = load_iris()
@@ -336,6 +361,7 @@ def test_rfe_cv_n_jobs():
     assert_array_almost_equal(rfecv.grid_scores_, rfecv_grid_scores)
 
 
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_rfe_cv_groups():
     generator = check_random_state(0)
     iris = load_iris()
diff --git a/sklearn/feature_selection/univariate_selection.py b/sklearn/feature_selection/univariate_selection.py
index 612f61028e2a..653466733921 100644
--- a/sklearn/feature_selection/univariate_selection.py
+++ b/sklearn/feature_selection/univariate_selection.py
@@ -50,8 +50,9 @@ def f_oneway(*args):
 
     Parameters
     ----------
-    sample1, sample2, ... : array_like, sparse matrices
-        The sample measurements should be given as arguments.
+    *args : array_like, sparse matrices
+        sample1, sample2... The sample measurements should be given as
+        arguments.
 
     Returns
     -------
@@ -388,6 +389,17 @@ class SelectPercentile(_BaseFilter):
     pvalues_ : array-like, shape=(n_features,)
         p-values of feature scores, None if `score_func` returned only scores.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.feature_selection import SelectPercentile, chi2
+    >>> X, y = load_digits(return_X_y=True)
+    >>> X.shape
+    (1797, 64)
+    >>> X_new = SelectPercentile(chi2, percentile=10).fit_transform(X, y)
+    >>> X_new.shape
+    (1797, 7)
+
     Notes
     -----
     Ties between features with equal scores will be broken in an unspecified
@@ -426,8 +438,7 @@ def _get_support_mask(self):
             return np.zeros(len(self.scores_), dtype=np.bool)
 
         scores = _clean_nans(self.scores_)
-        threshold = stats.scoreatpercentile(scores,
-                                            100 - self.percentile)
+        threshold = np.percentile(scores, 100 - self.percentile)
         mask = scores > threshold
         ties = np.where(scores == threshold)[0]
         if len(ties):
@@ -462,6 +473,17 @@ class SelectKBest(_BaseFilter):
     pvalues_ : array-like, shape=(n_features,)
         p-values of feature scores, None if `score_func` returned only scores.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.feature_selection import SelectKBest, chi2
+    >>> X, y = load_digits(return_X_y=True)
+    >>> X.shape
+    (1797, 64)
+    >>> X_new = SelectKBest(chi2, k=20).fit_transform(X, y)
+    >>> X_new.shape
+    (1797, 20)
+
     Notes
     -----
     Ties between features with equal scores will be broken in an unspecified
@@ -535,6 +557,17 @@ class SelectFpr(_BaseFilter):
     pvalues_ : array-like, shape=(n_features,)
         p-values of feature scores.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_breast_cancer
+    >>> from sklearn.feature_selection import SelectFpr, chi2
+    >>> X, y = load_breast_cancer(return_X_y=True)
+    >>> X.shape
+    (569, 30)
+    >>> X_new = SelectFpr(chi2, alpha=0.01).fit_transform(X, y)
+    >>> X_new.shape
+    (569, 16)
+
     See also
     --------
     f_classif: ANOVA F-value between label/feature for classification tasks.
@@ -578,6 +611,16 @@ class SelectFdr(_BaseFilter):
     alpha : float, optional
         The highest uncorrected p-value for features to keep.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_breast_cancer
+    >>> from sklearn.feature_selection import SelectFdr, chi2
+    >>> X, y = load_breast_cancer(return_X_y=True)
+    >>> X.shape
+    (569, 30)
+    >>> X_new = SelectFdr(chi2, alpha=0.01).fit_transform(X, y)
+    >>> X_new.shape
+    (569, 16)
 
     Attributes
     ----------
@@ -637,6 +680,17 @@ class SelectFwe(_BaseFilter):
     alpha : float, optional
         The highest uncorrected p-value for features to keep.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_breast_cancer
+    >>> from sklearn.feature_selection import SelectFwe, chi2
+    >>> X, y = load_breast_cancer(return_X_y=True)
+    >>> X.shape
+    (569, 30)
+    >>> X_new = SelectFwe(chi2, alpha=0.01).fit_transform(X, y)
+    >>> X_new.shape
+    (569, 15)
+
     Attributes
     ----------
     scores_ : array-like, shape=(n_features,)
@@ -699,6 +753,18 @@ class GenericUnivariateSelect(_BaseFilter):
     pvalues_ : array-like, shape=(n_features,)
         p-values of feature scores, None if `score_func` returned scores only.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_breast_cancer
+    >>> from sklearn.feature_selection import GenericUnivariateSelect, chi2
+    >>> X, y = load_breast_cancer(return_X_y=True)
+    >>> X.shape
+    (569, 30)
+    >>> transformer = GenericUnivariateSelect(chi2, 'k_best', param=20)
+    >>> X_new = transformer.fit_transform(X, y)
+    >>> X_new.shape
+    (569, 20)
+
     See also
     --------
     f_classif: ANOVA F-value between label/feature for classification tasks.
diff --git a/sklearn/gaussian_process/correlation_models.py b/sklearn/gaussian_process/correlation_models.py
index 3b954e2582b0..b0ff24749f2c 100644
--- a/sklearn/gaussian_process/correlation_models.py
+++ b/sklearn/gaussian_process/correlation_models.py
@@ -238,7 +238,7 @@ def cubic(theta, d):
     else:
         td = np.abs(d) * theta.reshape(1, n_features)
 
-    td[td > 1.] = 1.
+    np.clip(td, None, 1., out=td)
     ss = 1. - td ** 2. * (3. - 2. * td)
     r = np.prod(ss, 1)
 
@@ -290,7 +290,7 @@ def linear(theta, d):
     else:
         td = np.abs(d) * theta.reshape(1, n_features)
 
-    td[td > 1.] = 1.
+    np.clip(td, None, 1., out=td)
     ss = 1. - td
     r = np.prod(ss, 1)
 
diff --git a/sklearn/gaussian_process/gpc.py b/sklearn/gaussian_process/gpc.py
index e1f37617b6d1..bca6bc506de3 100644
--- a/sklearn/gaussian_process/gpc.py
+++ b/sklearn/gaussian_process/gpc.py
@@ -409,7 +409,7 @@ def _posterior_mode(self, K, return_temporaries=False):
             # Line 10: Compute log marginal likelihood in loop and use as
             #          convergence criterion
             lml = -0.5 * a.T.dot(f) \
-                - np.log(1 + np.exp(-(self.y_train_ * 2 - 1) * f)).sum() \
+                - np.log1p(np.exp(-(self.y_train_ * 2 - 1) * f)).sum() \
                 - np.log(np.diag(L)).sum()
             # Check if we have converged (log marginal likelihood does
             # not decrease)
@@ -534,11 +534,11 @@ def optimizer(obj_func, initial_theta, bounds):
         Note that "one_vs_one" does not support predicting probability
         estimates.
 
-    n_jobs : int, optional, default: 1
-        The number of jobs to use for the computation. If -1 all CPUs are used.
-        If 1 is given, no parallel computing code is used at all, which is
-        useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are
-        used. Thus for n_jobs = -2, all CPUs but one are used.
+    n_jobs : int or None, optional (default=None)
+        The number of jobs to use for the computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -558,12 +558,27 @@ def optimizer(obj_func, initial_theta, bounds):
     n_classes_ : int
         The number of classes in the training data
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_iris
+    >>> from sklearn.gaussian_process import GaussianProcessClassifier
+    >>> from sklearn.gaussian_process.kernels import RBF
+    >>> X, y = load_iris(return_X_y=True)
+    >>> kernel = 1.0 * RBF(1.0)
+    >>> gpc = GaussianProcessClassifier(kernel=kernel,
+    ...         random_state=0).fit(X, y)
+    >>> gpc.score(X, y) # doctest: +ELLIPSIS
+    0.9866...
+    >>> gpc.predict_proba(X[:2,:])
+    array([[0.83548752, 0.03228706, 0.13222543],
+           [0.79064206, 0.06525643, 0.14410151]])
+
     .. versionadded:: 0.18
     """
     def __init__(self, kernel=None, optimizer="fmin_l_bfgs_b",
                  n_restarts_optimizer=0, max_iter_predict=100,
                  warm_start=False, copy_X_train=True, random_state=None,
-                 multi_class="one_vs_rest", n_jobs=1):
+                 multi_class="one_vs_rest", n_jobs=None):
         self.kernel = kernel
         self.optimizer = optimizer
         self.n_restarts_optimizer = n_restarts_optimizer
diff --git a/sklearn/gaussian_process/gpr.py b/sklearn/gaussian_process/gpr.py
index 27043c46ddec..ac2c0a46b686 100644
--- a/sklearn/gaussian_process/gpr.py
+++ b/sklearn/gaussian_process/gpr.py
@@ -132,6 +132,20 @@ def optimizer(obj_func, initial_theta, bounds):
     log_marginal_likelihood_value_ : float
         The log-marginal-likelihood of ``self.kernel_.theta``
 
+    Examples
+    --------
+    >>> from sklearn.datasets import make_friedman2
+    >>> from sklearn.gaussian_process import GaussianProcessRegressor
+    >>> from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel
+    >>> X, y = make_friedman2(n_samples=500, noise=0, random_state=0)
+    >>> kernel = DotProduct() + WhiteKernel()
+    >>> gpr = GaussianProcessRegressor(kernel=kernel,
+    ...         random_state=0).fit(X, y)
+    >>> gpr.score(X, y) # doctest: +ELLIPSIS
+    0.3680...
+    >>> gpr.predict(X[:2,:], return_std=True) # doctest: +ELLIPSIS
+    (array([653.0..., 592.1...]), array([316.6..., 316.6...]))
+
     """
     def __init__(self, kernel=None, alpha=1e-10,
                  optimizer="fmin_l_bfgs_b", n_restarts_optimizer=0,
diff --git a/sklearn/gaussian_process/kernels.py b/sklearn/gaussian_process/kernels.py
index 4581b87ba948..79d913bca1cb 100644
--- a/sklearn/gaussian_process/kernels.py
+++ b/sklearn/gaussian_process/kernels.py
@@ -199,7 +199,13 @@ def set_params(self, **params):
         return self
 
     def clone_with_theta(self, theta):
-        """Returns a clone of self with given hyperparameters theta. """
+        """Returns a clone of self with given hyperparameters theta.
+
+        Parameters
+        ----------
+        theta : array, shape (n_dims,)
+            The hyperparameters
+        """
         cloned = clone(self)
         cloned.theta = theta
         return cloned
@@ -395,6 +401,11 @@ class CompoundKernel(Kernel):
     """Kernel which is composed of a set of other kernels.
 
     .. versionadded:: 0.18
+
+    Parameters
+    ----------
+    kernels : list of Kernel objects
+        The other kernels
     """
 
     def __init__(self, kernels):
@@ -999,11 +1010,13 @@ def __call__(self, X, Y=None, eval_gradient=False):
         elif eval_gradient:
             raise ValueError("Gradient can only be evaluated when Y is None.")
 
-        K = self.constant_value * np.ones((X.shape[0], Y.shape[0]))
+        K = np.full((X.shape[0], Y.shape[0]), self.constant_value,
+                    dtype=np.array(self.constant_value).dtype)
         if eval_gradient:
             if not self.hyperparameter_constant_value.fixed:
-                return (K, self.constant_value
-                        * np.ones((X.shape[0], X.shape[0], 1)))
+                return (K, np.full((X.shape[0], X.shape[0], 1),
+                                   self.constant_value,
+                                   dtype=np.array(self.constant_value).dtype))
             else:
                 return K, np.empty((X.shape[0], X.shape[0], 0))
         else:
@@ -1026,7 +1039,8 @@ def diag(self, X):
         K_diag : array, shape (n_samples_X,)
             Diagonal of kernel k(X, X)
         """
-        return self.constant_value * np.ones(X.shape[0])
+        return np.full(X.shape[0], self.constant_value,
+                       dtype=np.array(self.constant_value).dtype)
 
     def __repr__(self):
         return "{0:.3g}**2".format(np.sqrt(self.constant_value))
@@ -1121,7 +1135,8 @@ def diag(self, X):
         K_diag : array, shape (n_samples_X,)
             Diagonal of kernel k(X, X)
         """
-        return self.noise_level * np.ones(X.shape[0])
+        return np.full(X.shape[0], self.noise_level,
+                       dtype=np.array(self.noise_level).dtype)
 
     def __repr__(self):
         return "{0}(noise_level={1:.3g})".format(self.__class__.__name__,
@@ -1268,7 +1283,7 @@ class Matern(RBF):
     length_scale_bounds : pair of floats >= 0, default: (1e-5, 1e5)
         The lower and upper bound on length_scale
 
-    nu: float, default: 1.5
+    nu : float, default: 1.5
         The parameter nu controlling the smoothness of the learned function.
         The smaller nu, the less smooth the approximated function is.
         For nu=inf, the kernel becomes equivalent to the RBF kernel and for
@@ -1755,7 +1770,7 @@ class PairwiseKernel(Kernel):
 
     Parameters
     ----------
-    gamma: float >= 0, default: 1.0
+    gamma : float >= 0, default: 1.0
         Parameter gamma of the pairwise kernel specified by metric
 
     gamma_bounds : pair of floats >= 0, default: (1e-5, 1e5)
diff --git a/sklearn/impute.py b/sklearn/impute.py
index 836bfb0167ad..e98c425d1b34 100644
--- a/sklearn/impute.py
+++ b/sklearn/impute.py
@@ -3,22 +3,16 @@
 #          Sergey Feldman <sergeyfeldman@gmail.com>
 # License: BSD 3 clause
 
-from __future__ import division
-
 import warnings
-from time import time
 import numbers
 
 import numpy as np
 import numpy.ma as ma
 from scipy import sparse
 from scipy import stats
-from collections import namedtuple
 
 from .base import BaseEstimator, TransformerMixin
-from .base import clone
-from .preprocessing import normalize
-from .utils import check_array, check_random_state, safe_indexing
+from .utils import check_array
 from .utils.sparsefuncs import _get_median
 from .utils.validation import check_is_fitted
 from .utils.validation import FLOAT_DTYPES
@@ -30,19 +24,24 @@
 zip = six.moves.zip
 map = six.moves.map
 
-ImputerTriplet = namedtuple('ImputerTriplet', ['feat_idx',
-                                               'neighbor_feat_idx',
-                                               'predictor'])
-
 __all__ = [
+    'MissingIndicator',
     'SimpleImputer',
-    'ChainedImputer',
 ]
 
 
+def _check_inputs_dtype(X, missing_values):
+    if (X.dtype.kind in ("f", "i", "u") and
+            not isinstance(missing_values, numbers.Real)):
+        raise ValueError("'X' and 'missing_values' types are expected to be"
+                         " both numerical. Got X.dtype={} and "
+                         " type(missing_values)={}."
+                         .format(X.dtype, type(missing_values)))
+
+
 def _get_mask(X, value_to_mask):
     """Compute the boolean mask X == missing_values."""
-    if value_to_mask is np.nan:
+    if is_scalar_nan(value_to_mask):
         if X.dtype.kind == "f":
             return np.isnan(X)
         elif X.dtype.kind in ("i", "u"):
@@ -51,7 +50,6 @@ def _get_mask(X, value_to_mask):
         else:
             # np.isnan does not work on object dtypes.
             return _object_dtype_isnan(X)
-
     else:
         # X == value_to_mask with object dytpes does not always perform
         # element-wise for old versions of numpy
@@ -133,7 +131,6 @@ class SimpleImputer(BaseEstimator, TransformerMixin):
         a new copy will always be made, even if `copy=False`:
 
         - If X is not an array of floating values;
-        - If X is sparse and `missing_values=0`;
         - If X is encoded as a CSR matrix.
 
     Attributes
@@ -141,6 +138,22 @@ class SimpleImputer(BaseEstimator, TransformerMixin):
     statistics_ : array of shape (n_features,)
         The imputation fill value for each feature.
 
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.impute import SimpleImputer
+    >>> imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')
+    >>> imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])
+    ... # doctest: +NORMALIZE_WHITESPACE
+    SimpleImputer(copy=True, fill_value=None, missing_values=nan,
+           strategy='mean', verbose=0)
+    >>> X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]
+    >>> print(imp_mean.transform(X))
+    ... # doctest: +NORMALIZE_WHITESPACE
+    [[ 7.   2.   3. ]
+     [ 4.   3.5  6. ]
+     [10.   3.5  9. ]]
+
     Notes
     -----
     Columns which only contained missing values at `fit` are discarded upon
@@ -183,6 +196,7 @@ def _validate_input(self, X):
             else:
                 raise ve
 
+        _check_inputs_dtype(X, self.missing_values)
         if X.dtype.kind not in ("i", "u", "f", "O"):
             raise ValueError("SimpleImputer does not support data with dtype "
                              "{0}. Please provide either a numeric array (with"
@@ -227,10 +241,17 @@ def fit(self, X, y=None):
                              "data".format(fill_value))
 
         if sparse.issparse(X):
-            self.statistics_ = self._sparse_fit(X,
-                                                self.strategy,
-                                                self.missing_values,
-                                                fill_value)
+            # missing_values = 0 not allowed with sparse data as it would
+            # force densification
+            if self.missing_values == 0:
+                raise ValueError("Imputation not possible when missing_values "
+                                 "== 0 and input is sparse. Provide a dense "
+                                 "array instead.")
+            else:
+                self.statistics_ = self._sparse_fit(X,
+                                                    self.strategy,
+                                                    self.missing_values,
+                                                    fill_value)
         else:
             self.statistics_ = self._dense_fit(X,
                                                self.strategy,
@@ -241,80 +262,41 @@ def fit(self, X, y=None):
 
     def _sparse_fit(self, X, strategy, missing_values, fill_value):
         """Fit the transformer on sparse data."""
-        # Count the zeros
-        if missing_values == 0:
-            n_zeros_axis = np.zeros(X.shape[1], dtype=int)
-        else:
-            n_zeros_axis = X.shape[0] - np.diff(X.indptr)
+        mask_data = _get_mask(X.data, missing_values)
+        n_implicit_zeros = X.shape[0] - np.diff(X.indptr)
 
-        # Mean
-        if strategy == "mean":
-            if missing_values != 0:
-                n_non_missing = n_zeros_axis
-
-                # Mask the missing elements
-                mask_missing_values = _get_mask(X.data, missing_values)
-                mask_valids = np.logical_not(mask_missing_values)
-
-                # Sum only the valid elements
-                new_data = X.data.copy()
-                new_data[mask_missing_values] = 0
-                X = sparse.csc_matrix((new_data, X.indices, X.indptr),
-                                      copy=False)
-                sums = X.sum(axis=0)
-
-                # Count the elements != 0
-                mask_non_zeros = sparse.csc_matrix(
-                    (mask_valids.astype(np.float64),
-                     X.indices,
-                     X.indptr), copy=False)
-                s = mask_non_zeros.sum(axis=0)
-                n_non_missing = np.add(n_non_missing, s)
-
-            else:
-                sums = X.sum(axis=0)
-                n_non_missing = np.diff(X.indptr)
+        statistics = np.empty(X.shape[1])
 
-            # Ignore the error, columns with a np.nan statistics_
-            # are not an error at this point. These columns will
-            # be removed in transform
-            with np.errstate(all="ignore"):
-                return np.ravel(sums) / np.ravel(n_non_missing)
+        if strategy == "constant":
+            # for constant strategy, self.statistcs_ is used to store
+            # fill_value in each column
+            statistics.fill(fill_value)
 
-        # Median + Most frequent + Constant
         else:
-            # Remove the missing values, for each column
-            columns_all = np.hsplit(X.data, X.indptr[1:-1])
-            mask_missing_values = _get_mask(X.data, missing_values)
-            mask_valids = np.hsplit(np.logical_not(mask_missing_values),
-                                    X.indptr[1:-1])
-
-            # astype necessary for bug in numpy.hsplit before v1.9
-            columns = [col[mask.astype(bool, copy=False)]
-                       for col, mask in zip(columns_all, mask_valids)]
-
-            # Median
-            if strategy == "median":
-                median = np.empty(len(columns))
-                for i, column in enumerate(columns):
-                    median[i] = _get_median(column, n_zeros_axis[i])
-
-                return median
-
-            # Most frequent
-            elif strategy == "most_frequent":
-                most_frequent = np.empty(len(columns))
-
-                for i, column in enumerate(columns):
-                    most_frequent[i] = _most_frequent(column,
-                                                      0,
-                                                      n_zeros_axis[i])
-
-                return most_frequent
-
-            # Constant
-            elif strategy == "constant":
-                return np.full(X.shape[1], fill_value)
+            for i in range(X.shape[1]):
+                column = X.data[X.indptr[i]:X.indptr[i + 1]]
+                mask_column = mask_data[X.indptr[i]:X.indptr[i + 1]]
+                column = column[~mask_column]
+
+                # combine explicit and implicit zeros
+                mask_zeros = _get_mask(column, 0)
+                column = column[~mask_zeros]
+                n_explicit_zeros = mask_zeros.sum()
+                n_zeros = n_implicit_zeros[i] + n_explicit_zeros
+
+                if strategy == "mean":
+                    s = column.size + n_zeros
+                    statistics[i] = np.nan if s == 0 else column.sum() / s
+
+                elif strategy == "median":
+                    statistics[i] = _get_median(column,
+                                                n_zeros)
+
+                elif strategy == "most_frequent":
+                    statistics[i] = _most_frequent(column,
+                                                   0,
+                                                   n_zeros)
+        return statistics
 
     def _dense_fit(self, X, strategy, missing_values, fill_value):
         """Fit the transformer on dense data."""
@@ -364,6 +346,8 @@ def _dense_fit(self, X, strategy, missing_values, fill_value):
 
         # Constant
         elif strategy == "constant":
+            # for constant strategy, self.statistcs_ is used to store
+            # fill_value in each column
             return np.full(X.shape[1], fill_value, dtype=X.dtype)
 
     def transform(self, X):
@@ -371,7 +355,7 @@ def transform(self, X):
 
         Parameters
         ----------
-        X : {array-like, sparse matrix}, shape = [n_samples, n_features]
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
             The input data to complete.
         """
         check_is_fitted(self, 'statistics_')
@@ -402,17 +386,19 @@ def transform(self, X):
                 X = X[:, valid_statistics_indexes]
 
         # Do actual imputation
-        if sparse.issparse(X) and self.missing_values != 0:
-            mask = _get_mask(X.data, self.missing_values)
-            indexes = np.repeat(np.arange(len(X.indptr) - 1, dtype=np.int),
-                                np.diff(X.indptr))[mask]
+        if sparse.issparse(X):
+            if self.missing_values == 0:
+                raise ValueError("Imputation not possible when missing_values "
+                                 "== 0 and input is sparse. Provide a dense "
+                                 "array instead.")
+            else:
+                mask = _get_mask(X.data, self.missing_values)
+                indexes = np.repeat(np.arange(len(X.indptr) - 1, dtype=np.int),
+                                    np.diff(X.indptr))[mask]
 
-            X.data[mask] = valid_statistics[indexes].astype(X.dtype,
-                                                            copy=False)
+                X.data[mask] = valid_statistics[indexes].astype(X.dtype,
+                                                                copy=False)
         else:
-            if sparse.issparse(X):
-                X = X.toarray()
-
             mask = _get_mask(X, self.missing_values)
             n_missing = np.sum(mask, axis=0)
             values = np.repeat(valid_statistics, n_missing)
@@ -423,545 +409,223 @@ def transform(self, X):
         return X
 
 
-class ChainedImputer(BaseEstimator, TransformerMixin):
-    """Chained imputer transformer to impute missing values.
-
-    Basic implementation of chained imputer from MICE (Multivariate
-    Imputations by Chained Equations) package from R. This version assumes all
-    of the features are Gaussian.
-
-    Read more in the :ref:`User Guide <mice>`.
+class MissingIndicator(BaseEstimator, TransformerMixin):
+    """Binary indicators for missing values.
 
     Parameters
     ----------
-    missing_values : int, np.nan, optional (default=np.nan)
+    missing_values : number, string, np.nan (default) or None
         The placeholder for the missing values. All occurrences of
-        ``missing_values`` will be imputed.
-
-    imputation_order : str, optional (default="ascending")
-        The order in which the features will be imputed. Possible values:
-
-        "ascending"
-            From features with fewest missing values to most.
-        "descending"
-            From features with most missing values to fewest.
-        "roman"
-            Left to right.
-        "arabic"
-            Right to left.
-        "random"
-            A random order for each round.
-
-    n_imputations : int, optional (default=100)
-        Number of chained imputation rounds to perform, the results of which
-        will be used in the final average.
-
-    n_burn_in : int, optional (default=10)
-        Number of initial imputation rounds to perform the results of which
-        will not be returned.
-
-    predictor : estimator object, default=BayesianRidge()
-        The predictor to use at each step of the round-robin imputation.
-        It must support ``return_std`` in its ``predict`` method.
-
-    n_nearest_features : int, optional (default=None)
-        Number of other features to use to estimate the missing values of
-        the each feature column. Nearness between features is measured using
-        the absolute correlation coefficient between each feature pair (after
-        initial imputation). Can provide significant speed-up when the number
-        of features is huge. If ``None``, all features will be used.
-
-    initial_strategy : str, optional (default="mean")
-        Which strategy to use to initialize the missing values. Same as the
-        ``strategy`` parameter in :class:`sklearn.impute.SimpleImputer`
-        Valid values: {"mean", "median", "most_frequent", or "constant"}.
-
-    min_value : float, optional (default=None)
-        Minimum possible imputed value. Default of ``None`` will set minimum
-        to negative infinity.
-
-    max_value : float, optional (default=None)
-        Maximum possible imputed value. Default of ``None`` will set maximum
-        to positive infinity.
-
-    verbose : int, optional (default=0)
-        Verbosity flag, controls the debug messages that are issued
-        as functions are evaluated. The higher, the more verbose. Can be 0, 1,
-        or 2.
-
-    random_state : int, RandomState instance or None, optional (default=None)
-        The seed of the pseudo random number generator to use when shuffling
-        the data.  If int, random_state is the seed used by the random number
-        generator; If RandomState instance, random_state is the random number
-        generator; If None, the random number generator is the RandomState
-        instance used by ``np.random``.
+        `missing_values` will be imputed.
 
-    Attributes
-    ----------
-    initial_imputer_ : object of class :class:`sklearn.preprocessing.Imputer`'
-        The imputer used to initialize the missing values.
+    features : str, optional
+        Whether the imputer mask should represent all or a subset of
+        features.
 
-    imputation_sequence_ : list of tuples
-        Each tuple has ``(feat_idx, neighbor_feat_idx, predictor)``, where
-        ``feat_idx`` is the current feature to be imputed,
-        ``neighbor_feat_idx`` is the array of other features used to impute the
-        current feature, and ``predictor`` is the trained predictor used for
-        the imputation.
+        - If "missing-only" (default), the imputer mask will only represent
+          features containing missing values during fit time.
+        - If "all", the imputer mask will represent all features.
 
-    Notes
-    -----
-    The R version of MICE does not have inductive functionality, i.e. first
-    fitting on ``X_train`` and then transforming any ``X_test`` without
-    additional fitting. We do this by storing each feature's predictor during
-    the round-robin ``fit`` phase, and predicting without refitting (in order)
-    during the ``transform`` phase.
+    sparse : boolean or "auto", optional
+        Whether the imputer mask format should be sparse or dense.
 
-    Features which contain all missing values at ``fit`` are discarded upon
-    ``transform``.
+        - If "auto" (default), the imputer mask will be of same type as
+          input.
+        - If True, the imputer mask will be a sparse matrix.
+        - If False, the imputer mask will be a numpy array.
 
-    Features with missing values in transform which did not have any missing
-    values in fit will be imputed with the initial imputation method only.
+    error_on_new : boolean, optional
+        If True (default), transform will raise an error when there are
+        features with missing values in transform that have no missing values
+        in fit This is applicable only when ``features="missing-only"``.
 
-    References
+    Attributes
     ----------
-    .. [1] `Stef van Buuren, Karin Groothuis-Oudshoorn (2011). "mice:
-        Multivariate Imputation by Chained Equations in R". Journal of
-        Statistical Software 45: 1-67.
-        <https://www.jstatsoft.org/article/view/v045i03>`_
-    """
+    features_ : ndarray, shape (n_missing_features,) or (n_features,)
+        The features indices which will be returned when calling ``transform``.
+        They are computed during ``fit``. For ``features='all'``, it is
+        to ``range(n_features)``.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.impute import MissingIndicator
+    >>> X1 = np.array([[np.nan, 1, 3],
+    ...                [4, 0, np.nan],
+    ...                [8, 1, 0]])
+    >>> X2 = np.array([[5, 1, np.nan],
+    ...                [np.nan, 2, 3],
+    ...                [2, 4, 0]])
+    >>> indicator = MissingIndicator()
+    >>> indicator.fit(X1)
+    MissingIndicator(error_on_new=True, features='missing-only',
+             missing_values=nan, sparse='auto')
+    >>> X2_tr = indicator.transform(X2)
+    >>> X2_tr
+    array([[False,  True],
+           [ True, False],
+           [False, False]])
 
-    def __init__(self,
-                 missing_values=np.nan,
-                 imputation_order='ascending',
-                 n_imputations=100,
-                 n_burn_in=10,
-                 predictor=None,
-                 n_nearest_features=None,
-                 initial_strategy="mean",
-                 min_value=None,
-                 max_value=None,
-                 verbose=False,
-                 random_state=None):
+    """
 
+    def __init__(self, missing_values=np.nan, features="missing-only",
+                 sparse="auto", error_on_new=True):
         self.missing_values = missing_values
-        self.imputation_order = imputation_order
-        self.n_imputations = n_imputations
-        self.n_burn_in = n_burn_in
-        self.predictor = predictor
-        self.n_nearest_features = n_nearest_features
-        self.initial_strategy = initial_strategy
-        self.min_value = min_value
-        self.max_value = max_value
-        self.verbose = verbose
-        self.random_state = random_state
-
-    def _impute_one_feature(self,
-                            X_filled,
-                            mask_missing_values,
-                            feat_idx,
-                            neighbor_feat_idx,
-                            predictor=None,
-                            fit_mode=True):
-        """Impute a single feature from the others provided.
-
-        This function predicts the missing values of one of the features using
-        the current estimates of all the other features. The ``predictor`` must
-        support ``return_std=True`` in its ``predict`` method for this function
-        to work.
+        self.features = features
+        self.sparse = sparse
+        self.error_on_new = error_on_new
+
+    def _get_missing_features_info(self, X):
+        """Compute the imputer mask and the indices of the features
+        containing missing values.
 
         Parameters
         ----------
-        X_filled : ndarray
-            Input data with the most recent imputations.
-
-        mask_missing_values : ndarray
-            Input data's missing indicator matrix.
-
-        feat_idx : int
-            Index of the feature currently being imputed.
-
-        neighbor_feat_idx : ndarray
-            Indices of the features to be used in imputing ``feat_idx``.
-
-        predictor : object
-            The predictor to use at this step of the round-robin imputation.
-            It must support ``return_std`` in its ``predict`` method.
-            If None, it will be cloned from self._predictor.
-
-        fit_mode : boolean, default=True
-            Whether to fit and predict with the predictor or just predict.
+        X : {ndarray or sparse matrix}, shape (n_samples, n_features)
+            The input data with missing values. Note that ``X`` has been
+            checked in ``fit`` and ``transform`` before to call this function.
 
         Returns
         -------
-        X_filled : ndarray
-            Input data with ``X_filled[missing_row_mask, feat_idx]`` updated.
-
-        predictor : predictor with sklearn API
-            The fitted predictor used to impute
-            ``X_filled[missing_row_mask, feat_idx]``.
-        """
-
-        # if nothing is missing, just return the default
-        # (should not happen at fit time because feat_ids would be excluded)
-        missing_row_mask = mask_missing_values[:, feat_idx]
-        if not np.any(missing_row_mask):
-            return X_filled, predictor
-
-        if predictor is None and fit_mode is False:
-            raise ValueError("If fit_mode is False, then an already-fitted "
-                             "predictor should be passed in.")
-
-        if predictor is None:
-            predictor = clone(self._predictor)
-
-        if fit_mode:
-            X_train = safe_indexing(X_filled[:, neighbor_feat_idx],
-                                    ~missing_row_mask)
-            y_train = safe_indexing(X_filled[:, feat_idx],
-                                    ~missing_row_mask)
-            predictor.fit(X_train, y_train)
-
-        # get posterior samples
-        X_test = safe_indexing(X_filled[:, neighbor_feat_idx],
-                               missing_row_mask)
-        mus, sigmas = predictor.predict(X_test, return_std=True)
-        good_sigmas = sigmas > 0
-        imputed_values = np.zeros(mus.shape, dtype=X_filled.dtype)
-        imputed_values[~good_sigmas] = mus[~good_sigmas]
-        imputed_values[good_sigmas] = self.random_state_.normal(
-            loc=mus[good_sigmas], scale=sigmas[good_sigmas])
-
-        # clip the values
-        imputed_values = np.clip(imputed_values,
-                                 self._min_value,
-                                 self._max_value)
-
-        # update the feature
-        X_filled[missing_row_mask, feat_idx] = imputed_values
-        return X_filled, predictor
-
-    def _get_neighbor_feat_idx(self,
-                               n_features,
-                               feat_idx,
-                               abs_corr_mat):
-        """Get a list of other features to predict ``feat_idx``.
-
-        If self.n_nearest_features is less than or equal to the total
-        number of features, then use a probability proportional to the absolute
-        correlation between ``feat_idx`` and each other feature to randomly
-        choose a subsample of the other features (without replacement).
-
-        Parameters
-        ----------
-        n_features : int
-            Number of features in ``X``.
+        imputer_mask : {ndarray or sparse matrix}, shape \
+(n_samples, n_features) or (n_samples, n_features_with_missing)
+            The imputer mask of the original data.
 
-        feat_idx : int
-            Index of the feature currently being imputed.
+        features_with_missing : ndarray, shape (n_features_with_missing)
+            The features containing missing values.
 
-        abs_corr_mat : ndarray, shape (n_features, n_features)
-            Absolute correlation matrix of ``X``. The diagonal has been zeroed
-            out and each feature has been normalized to sum to 1. Can be None.
-
-        Returns
-        -------
-        neighbor_feat_idx : array-like
-            The features to use to impute ``feat_idx``.
         """
-        if (self.n_nearest_features is not None and
-                self.n_nearest_features < n_features):
-            p = abs_corr_mat[:, feat_idx]
-            neighbor_feat_idx = self.random_state_.choice(
-                np.arange(n_features), self.n_nearest_features, replace=False,
-                p=p)
+        if sparse.issparse(X) and self.missing_values != 0:
+            mask = _get_mask(X.data, self.missing_values)
+
+            # The imputer mask will be constructed with the same sparse format
+            # as X.
+            sparse_constructor = (sparse.csr_matrix if X.format == 'csr'
+                                  else sparse.csc_matrix)
+            imputer_mask = sparse_constructor(
+                (mask, X.indices.copy(), X.indptr.copy()),
+                shape=X.shape, dtype=bool)
+
+            missing_values_mask = imputer_mask.copy()
+            missing_values_mask.eliminate_zeros()
+            features_with_missing = (
+                np.flatnonzero(np.diff(missing_values_mask.indptr))
+                if missing_values_mask.format == 'csc'
+                else np.unique(missing_values_mask.indices))
+
+            if self.sparse is False:
+                imputer_mask = imputer_mask.toarray()
+            elif imputer_mask.format == 'csr':
+                imputer_mask = imputer_mask.tocsc()
         else:
-            inds_left = np.arange(feat_idx)
-            inds_right = np.arange(feat_idx + 1, n_features)
-            neighbor_feat_idx = np.concatenate((inds_left, inds_right))
-        return neighbor_feat_idx
+            if sparse.issparse(X):
+                # case of sparse matrix with 0 as missing values. Implicit and
+                # explicit zeros are considered as missing values.
+                X = X.toarray()
+            imputer_mask = _get_mask(X, self.missing_values)
+            features_with_missing = np.flatnonzero(imputer_mask.sum(axis=0))
 
-    def _get_ordered_idx(self, mask_missing_values):
-        """Decide in what order we will update the features.
+            if self.sparse is True:
+                imputer_mask = sparse.csc_matrix(imputer_mask)
 
-        As a homage to the MICE R package, we will have 4 main options of
-        how to order the updates, and use a random order if anything else
-        is specified.
+        return imputer_mask, features_with_missing
 
-        Also, this function skips features which have no missing values.
+    def fit(self, X, y=None):
+        """Fit the transformer on X.
 
         Parameters
         ----------
-        mask_missing_values : array-like, shape (n_samples, n_features)
-            Input data's missing indicator matrix, where "n_samples" is the
-            number of samples and "n_features" is the number of features.
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
+            Input data, where ``n_samples`` is the number of samples and
+            ``n_features`` is the number of features.
 
         Returns
         -------
-        ordered_idx : ndarray, shape (n_features,)
-            The order in which to impute the features.
+        self : object
+            Returns self.
         """
-        frac_of_missing_values = mask_missing_values.mean(axis=0)
-        missing_values_idx = np.nonzero(frac_of_missing_values)[0]
-        if self.imputation_order == 'roman':
-            ordered_idx = missing_values_idx
-        elif self.imputation_order == 'arabic':
-            ordered_idx = missing_values_idx[::-1]
-        elif self.imputation_order == 'ascending':
-            n = len(frac_of_missing_values) - len(missing_values_idx)
-            ordered_idx = np.argsort(frac_of_missing_values,
-                                     kind='mergesort')[n:][::-1]
-        elif self.imputation_order == 'descending':
-            n = len(frac_of_missing_values) - len(missing_values_idx)
-            ordered_idx = np.argsort(frac_of_missing_values,
-                                     kind='mergesort')[n:]
-        elif self.imputation_order == 'random':
-            ordered_idx = missing_values_idx
-            self.random_state_.shuffle(ordered_idx)
+        if not is_scalar_nan(self.missing_values):
+            force_all_finite = True
         else:
-            raise ValueError("Got an invalid imputation order: '{0}'. It must "
-                             "be one of the following: 'roman', 'arabic', "
-                             "'ascending', 'descending', or "
-                             "'random'.".format(self.imputation_order))
-        return ordered_idx
+            force_all_finite = "allow-nan"
+        X = check_array(X, accept_sparse=('csc', 'csr'),
+                        force_all_finite=force_all_finite)
+        _check_inputs_dtype(X, self.missing_values)
 
-    def _get_abs_corr_mat(self, X_filled, tolerance=1e-6):
-        """Get absolute correlation matrix between features.
+        self._n_features = X.shape[1]
 
-        Parameters
-        ----------
-        X_filled : ndarray, shape (n_samples, n_features)
-            Input data with the most recent imputations.
+        if self.features not in ('missing-only', 'all'):
+            raise ValueError("'features' has to be either 'missing-only' or "
+                             "'all'. Got {} instead.".format(self.features))
 
-        tolerance : float, optional (default=1e-6)
-            ``abs_corr_mat`` can have nans, which will be replaced
-            with ``tolerance``.
+        if not ((isinstance(self.sparse, six.string_types) and
+                self.sparse == "auto") or isinstance(self.sparse, bool)):
+            raise ValueError("'sparse' has to be a boolean or 'auto'. "
+                             "Got {!r} instead.".format(self.sparse))
 
-        Returns
-        -------
-        abs_corr_mat : ndarray, shape (n_features, n_features)
-            Absolute correlation matrix of ``X`` at the beginning of the
-            current round. The diagonal has been zeroed out and each feature's
-            absolute correlations with all others have been normalized to sum
-            to 1.
-        """
-        n_features = X_filled.shape[1]
-        if (self.n_nearest_features is None or
-                self.n_nearest_features >= n_features):
-            return None
-        abs_corr_mat = np.abs(np.corrcoef(X_filled.T))
-        # np.corrcoef is not defined for features with zero std
-        abs_corr_mat[np.isnan(abs_corr_mat)] = tolerance
-        # ensures exploration, i.e. at least some probability of sampling
-        abs_corr_mat[abs_corr_mat < tolerance] = tolerance
-        # features are not their own neighbors
-        np.fill_diagonal(abs_corr_mat, 0)
-        # needs to sum to 1 for np.random.choice sampling
-        abs_corr_mat = normalize(abs_corr_mat, norm='l1', axis=0, copy=False)
-        return abs_corr_mat
-
-    def _initial_imputation(self, X):
-        """Perform initial imputation for input X.
+        self.features_ = (self._get_missing_features_info(X)[1]
+                          if self.features == 'missing-only'
+                          else np.arange(self._n_features))
+
+        return self
+
+    def transform(self, X):
+        """Generate missing values indicator for X.
 
         Parameters
         ----------
-        X : ndarray, shape (n_samples, n_features)
-            Input data, where "n_samples" is the number of samples and
-            "n_features" is the number of features.
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
+            The input data to complete.
 
         Returns
         -------
-        Xt : ndarray, shape (n_samples, n_features)
-            Input data, where "n_samples" is the number of samples and
-            "n_features" is the number of features.
+        Xt : {ndarray or sparse matrix}, shape (n_samples, n_features)
+            The missing indicator for input data. The data type of ``Xt``
+            will be boolean.
 
-        X_filled : ndarray, shape (n_samples, n_features)
-            Input data with the most recent imputations.
-
-        mask_missing_values : ndarray, shape (n_samples, n_features)
-            Input data's missing indicator matrix, where "n_samples" is the
-            number of samples and "n_features" is the number of features.
         """
-        if is_scalar_nan(self.missing_values):
-            force_all_finite = "allow-nan"
-        else:
-            force_all_finite = True
+        check_is_fitted(self, "features_")
 
-        X = check_array(X, dtype=FLOAT_DTYPES, order="F",
-                        force_all_finite=force_all_finite)
-
-        mask_missing_values = _get_mask(X, self.missing_values)
-        if self.initial_imputer_ is None:
-            self.initial_imputer_ = SimpleImputer(
-                                            missing_values=self.missing_values,
-                                            strategy=self.initial_strategy)
-            X_filled = self.initial_imputer_.fit_transform(X)
+        if not is_scalar_nan(self.missing_values):
+            force_all_finite = True
         else:
-            X_filled = self.initial_imputer_.transform(X)
-
-        valid_mask = np.flatnonzero(np.logical_not(
-            np.isnan(self.initial_imputer_.statistics_)))
-        Xt = X[:, valid_mask]
-        mask_missing_values = mask_missing_values[:, valid_mask]
-
-        return Xt, X_filled, mask_missing_values
-
-    def fit_transform(self, X, y=None):
-        """Fits the imputer on X and return the transformed X.
+            force_all_finite = "allow-nan"
+        X = check_array(X, accept_sparse=('csc', 'csr'),
+                        force_all_finite=force_all_finite)
+        _check_inputs_dtype(X, self.missing_values)
 
-        Parameters
-        ----------
-        X : array-like, shape (n_samples, n_features)
-            Input data, where "n_samples" is the number of samples and
-            "n_features" is the number of features.
+        if X.shape[1] != self._n_features:
+            raise ValueError("X has a different number of features "
+                             "than during fitting.")
 
-        y : ignored.
+        imputer_mask, features = self._get_missing_features_info(X)
 
-        Returns
-        -------
-        Xt : array-like, shape (n_samples, n_features)
-             The imputed input data.
-        """
-        self.random_state_ = getattr(self, "random_state_",
-                                     check_random_state(self.random_state))
+        if self.features == "missing-only":
+            features_diff_fit_trans = np.setdiff1d(features, self.features_)
+            if (self.error_on_new and features_diff_fit_trans.size > 0):
+                raise ValueError("The features {} have missing values "
+                                 "in transform but have no missing values "
+                                 "in fit.".format(features_diff_fit_trans))
 
-        if self.predictor is None:
-            from .linear_model import BayesianRidge
-            self._predictor = BayesianRidge()
-        else:
-            self._predictor = clone(self.predictor)
-
-        self._min_value = np.nan if self.min_value is None else self.min_value
-        self._max_value = np.nan if self.max_value is None else self.max_value
-
-        self.initial_imputer_ = None
-        X, X_filled, mask_missing_values = self._initial_imputation(X)
-
-        # edge case: in case the user specifies 0 for n_imputations,
-        # then there is no need to do burn in and the result should be
-        # just the initial imputation (before clipping)
-        if self.n_imputations < 1:
-            return X_filled
-
-        X_filled = np.clip(X_filled, self._min_value, self._max_value)
-
-        # order in which to impute
-        # note this is probably too slow for large feature data (d > 100000)
-        # and a better way would be good.
-        # see: https://goo.gl/KyCNwj and subsequent comments
-        ordered_idx = self._get_ordered_idx(mask_missing_values)
-
-        abs_corr_mat = self._get_abs_corr_mat(X_filled)
-
-        # impute data
-        n_rounds = self.n_burn_in + self.n_imputations
-        n_samples, n_features = X_filled.shape
-        Xt = np.zeros((n_samples, n_features), dtype=X.dtype)
-        self.imputation_sequence_ = []
-        if self.verbose > 0:
-            print("[ChainedImputer] Completing matrix with shape %s"
-                  % (X.shape,))
-        start_t = time()
-        for i_rnd in range(n_rounds):
-            if self.imputation_order == 'random':
-                ordered_idx = self._get_ordered_idx(mask_missing_values)
-
-            for feat_idx in ordered_idx:
-                neighbor_feat_idx = self._get_neighbor_feat_idx(n_features,
-                                                                feat_idx,
-                                                                abs_corr_mat)
-                X_filled, predictor = self._impute_one_feature(
-                    X_filled, mask_missing_values, feat_idx, neighbor_feat_idx,
-                    predictor=None, fit_mode=True)
-                predictor_triplet = ImputerTriplet(feat_idx,
-                                                   neighbor_feat_idx,
-                                                   predictor)
-                self.imputation_sequence_.append(predictor_triplet)
-
-            if i_rnd >= self.n_burn_in:
-                Xt += X_filled
-            if self.verbose > 0:
-                print('[ChainedImputer] Ending imputation round '
-                      '%d/%d, elapsed time %0.2f'
-                      % (i_rnd + 1, n_rounds, time() - start_t))
-
-        Xt /= self.n_imputations
-        Xt[~mask_missing_values] = X[~mask_missing_values]
-        return Xt
+            if (self.features_.size > 0 and
+                    self.features_.size < self._n_features):
+                imputer_mask = imputer_mask[:, self.features_]
 
-    def transform(self, X):
-        """Imputes all missing values in X.
+        return imputer_mask
 
-        Note that this is stochastic, and that if random_state is not fixed,
-        repeated calls, or permuted input, will yield different results.
+    def fit_transform(self, X, y=None):
+        """Generate missing values indicator for X.
 
         Parameters
         ----------
-        X : array-like, shape = [n_samples, n_features]
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
             The input data to complete.
 
         Returns
         -------
-        Xt : array-like, shape (n_samples, n_features)
-             The imputed input data.
-        """
-        check_is_fitted(self, 'initial_imputer_')
-
-        X, X_filled, mask_missing_values = self._initial_imputation(X)
-
-        # edge case: in case the user specifies 0 for n_imputations,
-        # then there is no need to do burn in and the result should be
-        # just the initial imputation (before clipping)
-        if self.n_imputations < 1:
-            return X_filled
-
-        X_filled = np.clip(X_filled, self._min_value, self._max_value)
-
-        n_rounds = self.n_burn_in + self.n_imputations
-        n_imputations = len(self.imputation_sequence_)
-        imputations_per_round = n_imputations // n_rounds
-        i_rnd = 0
-        Xt = np.zeros(X.shape, dtype=X.dtype)
-        if self.verbose > 0:
-            print("[ChainedImputer] Completing matrix with shape %s"
-                  % (X.shape,))
-        start_t = time()
-        for it, predictor_triplet in enumerate(self.imputation_sequence_):
-            X_filled, _ = self._impute_one_feature(
-                X_filled,
-                mask_missing_values,
-                predictor_triplet.feat_idx,
-                predictor_triplet.neighbor_feat_idx,
-                predictor=predictor_triplet.predictor,
-                fit_mode=False
-            )
-            if not (it + 1) % imputations_per_round:
-                if i_rnd >= self.n_burn_in:
-                    Xt += X_filled
-                if self.verbose > 1:
-                    print('[ChainedImputer] Ending imputation round '
-                          '%d/%d, elapsed time %0.2f'
-                          % (i_rnd + 1, n_rounds, time() - start_t))
-                i_rnd += 1
-
-        Xt /= self.n_imputations
-        Xt[~mask_missing_values] = X[~mask_missing_values]
-        return Xt
-
-    def fit(self, X, y=None):
-        """Fits the imputer on X and return self.
-
-        Parameters
-        ----------
-        X : array-like, shape (n_samples, n_features)
-            Input data, where "n_samples" is the number of samples and
-            "n_features" is the number of features.
-
-        y : ignored
+        Xt : {ndarray or sparse matrix}, shape (n_samples, n_features)
+            The missing indicator for input data. The data type of ``Xt``
+            will be boolean.
 
-        Returns
-        -------
-        self : object
-            Returns self.
         """
-        self.fit_transform(X)
-        return self
+        return self.fit(X, y).transform(X)
diff --git a/sklearn/kernel_approximation.py b/sklearn/kernel_approximation.py
index 68b2e82772b1..79d915fa1e2d 100644
--- a/sklearn/kernel_approximation.py
+++ b/sklearn/kernel_approximation.py
@@ -44,6 +44,26 @@ class RBFSampler(BaseEstimator, TransformerMixin):
         If None, the random number generator is the RandomState instance used
         by `np.random`.
 
+    Examples
+    --------
+    >>> from sklearn.kernel_approximation import RBFSampler
+    >>> from sklearn.linear_model import SGDClassifier
+    >>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]
+    >>> y = [0, 0, 1, 1]
+    >>> rbf_feature = RBFSampler(gamma=1, random_state=1)
+    >>> X_features = rbf_feature.fit_transform(X)
+    >>> clf = SGDClassifier(max_iter=5)
+    >>> clf.fit(X_features, y)
+    ... # doctest: +NORMALIZE_WHITESPACE
+    SGDClassifier(alpha=0.0001, average=False, class_weight=None,
+           early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,
+           l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,
+           n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',
+           power_t=0.5, random_state=None, shuffle=True, tol=None,
+           validation_fraction=0.1, verbose=0, warm_start=False)
+    >>> clf.score(X_features, y)
+    1.0
+
     Notes
     -----
     See "Random Features for Large-Scale Kernel Machines" by A. Rahimi and
@@ -132,6 +152,27 @@ class SkewedChi2Sampler(BaseEstimator, TransformerMixin):
         If None, the random number generator is the RandomState instance used
         by `np.random`.
 
+    Examples
+    --------
+    >>> from sklearn.kernel_approximation import SkewedChi2Sampler
+    >>> from sklearn.linear_model import SGDClassifier
+    >>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]
+    >>> y = [0, 0, 1, 1]
+    >>> chi2_feature = SkewedChi2Sampler(skewedness=.01,
+    ...                                  n_components=10,
+    ...                                  random_state=0)
+    >>> X_features = chi2_feature.fit_transform(X, y)
+    >>> clf = SGDClassifier(max_iter=10)
+    >>> clf.fit(X_features, y)
+    SGDClassifier(alpha=0.0001, average=False, class_weight=None,
+           early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,
+           l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=10,
+           n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',
+           power_t=0.5, random_state=None, shuffle=True, tol=None,
+           validation_fraction=0.1, verbose=0, warm_start=False)
+    >>> clf.score(X_features, y)
+    1.0
+
     References
     ----------
     See "Random Fourier Approximations for Skewed Multiplicative Histogram
@@ -233,6 +274,25 @@ class AdditiveChi2Sampler(BaseEstimator, TransformerMixin):
     sample_interval : float, optional
         Sampling interval. Must be specified when sample_steps not in {1,2,3}.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.linear_model import SGDClassifier
+    >>> from sklearn.kernel_approximation import AdditiveChi2Sampler
+    >>> X, y = load_digits(return_X_y=True)
+    >>> chi2sampler = AdditiveChi2Sampler(sample_steps=2)
+    >>> X_transformed = chi2sampler.fit_transform(X, y)
+    >>> clf = SGDClassifier(max_iter=5, random_state=0)
+    >>> clf.fit(X_transformed, y)
+    SGDClassifier(alpha=0.0001, average=False, class_weight=None,
+           early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,
+           l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,
+           n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',
+           power_t=0.5, random_state=0, shuffle=True, tol=None,
+           validation_fraction=0.1, verbose=0, warm_start=False)
+    >>> clf.score(X_transformed, y) # doctest: +ELLIPSIS
+    0.9543...
+
     Notes
     -----
     This estimator approximates a slightly different version of the additive
@@ -261,7 +321,19 @@ def __init__(self, sample_steps=2, sample_interval=None):
         self.sample_interval = sample_interval
 
     def fit(self, X, y=None):
-        """Set parameters."""
+        """Set the parameters
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+            Training data, where n_samples in the number of samples
+            and n_features is the number of features.
+
+        Returns
+        -------
+        self : object
+            Returns the transformer.
+        """
         X = check_array(X, accept_sparse='csr')
         if self.sample_interval is None:
             # See reference, figure 2 c)
@@ -379,27 +451,27 @@ class Nystroem(BaseEstimator, TransformerMixin):
         and the keyword arguments passed to this object as kernel_params, and
         should return a floating point number.
 
-    n_components : int
-        Number of features to construct.
-        How many data points will be used to construct the mapping.
-
     gamma : float, default=None
         Gamma parameter for the RBF, laplacian, polynomial, exponential chi2
         and sigmoid kernels. Interpretation of the default value is left to
         the kernel; see the documentation for sklearn.metrics.pairwise.
         Ignored by other kernels.
 
-    degree : float, default=None
-        Degree of the polynomial kernel. Ignored by other kernels.
-
     coef0 : float, default=None
         Zero coefficient for polynomial and sigmoid kernels.
         Ignored by other kernels.
 
+    degree : float, default=None
+        Degree of the polynomial kernel. Ignored by other kernels.
+
     kernel_params : mapping of string to any, optional
         Additional parameters (keyword arguments) for kernel function passed
         as callable object.
 
+    n_components : int
+        Number of features to construct.
+        How many data points will be used to construct the mapping.
+
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
         If RandomState instance, random_state is the random number generator;
@@ -418,6 +490,25 @@ class Nystroem(BaseEstimator, TransformerMixin):
         Normalization matrix needed for embedding.
         Square root of the kernel matrix on ``components_``.
 
+    Examples
+    --------
+    >>> from sklearn import datasets, svm
+    >>> from sklearn.kernel_approximation import Nystroem
+    >>> digits = datasets.load_digits(n_class=9)
+    >>> data = digits.data / 16.
+    >>> clf = svm.LinearSVC()
+    >>> feature_map_nystroem = Nystroem(gamma=.2,
+    ...                                 random_state=1,
+    ...                                 n_components=300)
+    >>> data_transformed = feature_map_nystroem.fit_transform(data)
+    >>> clf.fit(data_transformed, digits.target)
+    ... # doctest: +NORMALIZE_WHITESPACE
+    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
+         intercept_scaling=1, loss='squared_hinge', max_iter=1000,
+         multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
+         verbose=0)
+    >>> clf.score(data_transformed, digits.target) # doctest: +ELLIPSIS
+    0.9987...
 
     References
     ----------
diff --git a/sklearn/linear_model/base.py b/sklearn/linear_model/base.py
index 09c389cb336d..30a28cd507f6 100644
--- a/sklearn/linear_model/base.py
+++ b/sklearn/linear_model/base.py
@@ -24,7 +24,7 @@
 from scipy import sparse
 
 from ..externals import six
-from ..externals.joblib import Parallel, delayed
+from ..utils import Parallel, delayed
 from ..base import BaseEstimator, ClassifierMixin, RegressorMixin
 from ..utils import check_array, check_X_y
 from ..utils.validation import FLOAT_DTYPES
@@ -50,6 +50,29 @@ def make_dataset(X, y, sample_weight, random_state=None):
 
     This also returns the ``intercept_decay`` which is different
     for sparse datasets.
+
+    Parameters
+    ----------
+    X : array_like, shape (n_samples, n_features)
+        Training data
+
+    y : array_like, shape (n_samples, )
+        Target values.
+
+    sample_weight : numpy array of shape (n_samples,)
+        The weight of each sample
+
+    random_state : int, RandomState instance or None (default)
+        Determines random number generation for dataset shuffling and noise.
+        Pass an int for reproducible output across multiple function calls.
+        See :term:`Glossary <random_state>`.
+
+    Returns
+    -------
+    dataset
+        The ``Dataset`` abstraction
+    intercept_decay
+        The intercept decay
     """
 
     rng = check_random_state(random_state)
@@ -68,7 +91,7 @@ def make_dataset(X, y, sample_weight, random_state=None):
 
 
 def _preprocess_data(X, y, fit_intercept, normalize=False, copy=True,
-                     sample_weight=None, return_mean=False):
+                     sample_weight=None, return_mean=False, check_input=True):
     """
     Centers data to have mean zero along axis 0. If fit_intercept=False or if
     the X is a sparse matrix, no centering is done, but normalization can still
@@ -90,8 +113,15 @@ def _preprocess_data(X, y, fit_intercept, normalize=False, copy=True,
     if isinstance(sample_weight, numbers.Number):
         sample_weight = None
 
-    X = check_array(X, copy=copy, accept_sparse=['csr', 'csc'],
-                    dtype=FLOAT_DTYPES)
+    if check_input:
+        X = check_array(X, copy=copy, accept_sparse=['csr', 'csc'],
+                        dtype=FLOAT_DTYPES)
+    elif copy:
+        if sp.issparse(X):
+            X = X.copy()
+        else:
+            X = X.copy(order='K')
+
     y = np.asarray(y, dtype=X.dtype)
 
     if fit_intercept:
@@ -143,7 +173,8 @@ def _preprocess_data(X, y, fit_intercept, normalize=False, copy=True,
 def _rescale_data(X, y, sample_weight):
     """Rescale data so as to support sample_weight"""
     n_samples = X.shape[0]
-    sample_weight = sample_weight * np.ones(n_samples)
+    sample_weight = np.full(n_samples, sample_weight,
+                            dtype=np.array(sample_weight).dtype)
     sample_weight = np.sqrt(sample_weight)
     sw_matrix = sparse.dia_matrix((sample_weight, 0),
                                   shape=(n_samples, n_samples))
@@ -171,12 +202,12 @@ def predict(self, X):
 
         Parameters
         ----------
-        X : {array-like, sparse matrix}, shape = (n_samples, n_features)
+        X : array_like or sparse matrix, shape (n_samples, n_features)
             Samples.
 
         Returns
         -------
-        C : array, shape = (n_samples,)
+        C : array, shape (n_samples,)
             Returns predicted values.
         """
         return self._decision_function(X)
@@ -209,7 +240,7 @@ def decision_function(self, X):
 
         Parameters
         ----------
-        X : {array-like, sparse matrix}, shape = (n_samples, n_features)
+        X : array_like or sparse matrix, shape (n_samples, n_features)
             Samples.
 
         Returns
@@ -239,12 +270,12 @@ def predict(self, X):
 
         Parameters
         ----------
-        X : {array-like, sparse matrix}, shape = [n_samples, n_features]
+        X : array_like or sparse matrix, shape (n_samples, n_features)
             Samples.
 
         Returns
         -------
-        C : array, shape = [n_samples]
+        C : array, shape [n_samples]
             Predicted class label per sample.
         """
         scores = self.decision_function(X)
@@ -350,10 +381,12 @@ class LinearRegression(LinearModel, RegressorMixin):
     copy_X : boolean, optional, default True
         If True, X will be copied; else, it may be overwritten.
 
-    n_jobs : int, optional, default 1
-        The number of jobs to use for the computation.
-        If -1 all CPUs are used. This will only provide speedup for
-        n_targets > 1 and sufficient large problems.
+    n_jobs : int or None, optional (default=None)
+        The number of jobs to use for the computation. This will only provide
+        speedup for n_targets > 1 and sufficient large problems.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -374,7 +407,7 @@ class LinearRegression(LinearModel, RegressorMixin):
     """
 
     def __init__(self, fit_intercept=True, normalize=False, copy_X=True,
-                 n_jobs=1):
+                 n_jobs=None):
         self.fit_intercept = fit_intercept
         self.normalize = normalize
         self.copy_X = copy_X
@@ -386,10 +419,10 @@ def fit(self, X, y, sample_weight=None):
 
         Parameters
         ----------
-        X : numpy array or sparse matrix of shape [n_samples,n_features]
+        X : array-like or sparse matrix, shape (n_samples, n_features)
             Training data
 
-        y : numpy array of shape [n_samples, n_targets]
+        y : array_like, shape (n_samples, n_targets)
             Target values. Will be cast to X's dtype if necessary
 
         sample_weight : numpy array of shape [n_samples]
@@ -441,7 +474,8 @@ def fit(self, X, y, sample_weight=None):
         return self
 
 
-def _pre_fit(X, y, Xy, precompute, normalize, fit_intercept, copy):
+def _pre_fit(X, y, Xy, precompute, normalize, fit_intercept, copy,
+             check_input=True):
     """Aux function used at beginning of fit in linear models"""
     n_samples, n_features = X.shape
 
@@ -450,11 +484,12 @@ def _pre_fit(X, y, Xy, precompute, normalize, fit_intercept, copy):
         precompute = False
         X, y, X_offset, y_offset, X_scale = _preprocess_data(
             X, y, fit_intercept=fit_intercept, normalize=normalize,
-            copy=False, return_mean=True)
+            copy=False, return_mean=True, check_input=check_input)
     else:
         # copy was done in fit if necessary
         X, y, X_offset, y_offset, X_scale = _preprocess_data(
-            X, y, fit_intercept=fit_intercept, normalize=normalize, copy=copy)
+            X, y, fit_intercept=fit_intercept, normalize=normalize, copy=copy,
+            check_input=check_input)
     if hasattr(precompute, '__array__') and (
             fit_intercept and not np.allclose(X_offset, np.zeros(n_features)) or
             normalize and not np.allclose(X_scale, np.ones(n_features))):
diff --git a/sklearn/linear_model/bayes.py b/sklearn/linear_model/bayes.py
index 7c220a67772c..9e85fd7641b9 100644
--- a/sklearn/linear_model/bayes.py
+++ b/sklearn/linear_model/bayes.py
@@ -212,7 +212,8 @@ def fit(self, X, y, sample_weight=None):
                     U / (eigen_vals_ + lambda_ / alpha_)[None, :], U.T))
                 coef_ = np.dot(coef_, y)
                 if self.compute_score:
-                    logdet_sigma_ = lambda_ * np.ones(n_features)
+                    logdet_sigma_ = np.full(n_features, lambda_,
+                                            dtype=np.array(lambda_).dtype)
                     logdet_sigma_[:n_samples] += alpha_ * eigen_vals_
                     logdet_sigma_ = - np.sum(np.log(logdet_sigma_))
 
diff --git a/sklearn/linear_model/cd_fast.pyx b/sklearn/linear_model/cd_fast.pyx
index 4fbfe8b2489d..9a6c68bd0a62 100644
--- a/sklearn/linear_model/cd_fast.pyx
+++ b/sklearn/linear_model/cd_fast.pyx
@@ -18,11 +18,6 @@ import warnings
 
 ctypedef np.float64_t DOUBLE
 ctypedef np.uint32_t UINT32_t
-ctypedef floating (*DOT)(int N, floating *X, int incX, floating *Y,
-                         int incY) nogil
-ctypedef void (*AXPY)(int N, floating alpha, floating *X, int incX,
-                      floating *Y, int incY) nogil
-ctypedef floating (*ASUM)(int N, floating *X, int incX) nogil
 
 np.import_array()
 
@@ -146,10 +141,10 @@ cdef extern from "cblas.h":
 @cython.boundscheck(False)
 @cython.wraparound(False)
 @cython.cdivision(True)
-def enet_coordinate_descent(np.ndarray[floating, ndim=1] w,
+def enet_coordinate_descent(floating[::1] w,
                             floating alpha, floating beta,
-                            np.ndarray[floating, ndim=2, mode='fortran'] X,
-                            np.ndarray[floating, ndim=1, mode='c'] y,
+                            floating[::1, :] X,
+                            floating[::1] y,
                             int max_iter, floating tol,
                             object rng, bint random=0, bint positive=0):
     """Cython version of the coordinate descent algorithm
@@ -162,10 +157,6 @@ def enet_coordinate_descent(np.ndarray[floating, ndim=1] w,
     """
 
     # fused types version of BLAS functions
-    cdef DOT dot
-    cdef AXPY axpy
-    cdef ASUM asum
-
     if floating is float:
         dtype = np.float32
         dot = sdot
@@ -181,15 +172,12 @@ def enet_coordinate_descent(np.ndarray[floating, ndim=1] w,
     cdef unsigned int n_samples = X.shape[0]
     cdef unsigned int n_features = X.shape[1]
 
-    # get the number of tasks indirectly, using strides
-    cdef unsigned int n_tasks = y.strides[0] / sizeof(floating)
-
     # compute norms of the columns of X
-    cdef np.ndarray[floating, ndim=1] norm_cols_X = (X**2).sum(axis=0)
+    cdef floating[::1] norm_cols_X = np.square(X).sum(axis=0)
 
     # initial value of the residuals
-    cdef np.ndarray[floating, ndim=1] R = np.empty(n_samples, dtype=dtype)
-    cdef np.ndarray[floating, ndim=1] XtA = np.empty(n_features, dtype=dtype)
+    cdef floating[::1] R = np.empty(n_samples, dtype=dtype)
+    cdef floating[::1] XtA = np.empty(n_features, dtype=dtype)
 
     cdef floating tmp
     cdef floating w_ii
@@ -211,12 +199,6 @@ def enet_coordinate_descent(np.ndarray[floating, ndim=1] w,
     cdef UINT32_t rand_r_state_seed = rng.randint(0, RAND_R_MAX)
     cdef UINT32_t* rand_r_state = &rand_r_state_seed
 
-    cdef floating *X_data = <floating*> X.data
-    cdef floating *y_data = <floating*> y.data
-    cdef floating *w_data = <floating*> w.data
-    cdef floating *R_data = <floating*> R.data
-    cdef floating *XtA_data = <floating*> XtA.data
-
     if alpha == 0 and beta == 0:
         warnings.warn("Coordinate descent with no regularization may lead to unexpected"
             " results and is discouraged.")
@@ -224,10 +206,10 @@ def enet_coordinate_descent(np.ndarray[floating, ndim=1] w,
     with nogil:
         # R = y - np.dot(X, w)
         for i in range(n_samples):
-            R[i] = y[i] - dot(n_features, &X_data[i], n_samples, w_data, 1)
+            R[i] = y[i] - dot(n_features, &X[i, 0], n_samples, &w[0], 1)
 
         # tol *= np.dot(y, y)
-        tol *= dot(n_samples, y_data, n_tasks, y_data, n_tasks)
+        tol *= dot(n_samples, &y[0], 1, &y[0], 1)
 
         for n_iter in range(max_iter):
             w_max = 0.0
@@ -245,11 +227,10 @@ def enet_coordinate_descent(np.ndarray[floating, ndim=1] w,
 
                 if w_ii != 0.0:
                     # R += w_ii * X[:,ii]
-                    axpy(n_samples, w_ii, &X_data[ii * n_samples], 1,
-                         R_data, 1)
+                    axpy(n_samples, w_ii, &X[0, ii], 1, &R[0], 1)
 
                 # tmp = (X[:,ii]*R).sum()
-                tmp = dot(n_samples, &X_data[ii * n_samples], 1, R_data, 1)
+                tmp = dot(n_samples, &X[0, ii], 1, &R[0], 1)
 
                 if positive and tmp < 0:
                     w[ii] = 0.0
@@ -259,8 +240,7 @@ def enet_coordinate_descent(np.ndarray[floating, ndim=1] w,
 
                 if w[ii] != 0.0:
                     # R -=  w[ii] * X[:,ii] # Update residual
-                    axpy(n_samples, -w[ii], &X_data[ii * n_samples], 1,
-                         R_data, 1)
+                    axpy(n_samples, -w[ii], &X[0, ii], 1, &R[0], 1)
 
                 # update the maximum absolute coefficient update
                 d_w_ii = fabs(w[ii] - w_ii)
@@ -279,19 +259,19 @@ def enet_coordinate_descent(np.ndarray[floating, ndim=1] w,
 
                 # XtA = np.dot(X.T, R) - beta * w
                 for i in range(n_features):
-                    XtA[i] = dot(n_samples, &X_data[i * n_samples],
-                                 1, R_data, 1) - beta * w[i]
+                    XtA[i] = (dot(n_samples, &X[0, i], 1, &R[0], 1)
+                              - beta * w[i])
 
                 if positive:
-                    dual_norm_XtA = max(n_features, XtA_data)
+                    dual_norm_XtA = max(n_features, &XtA[0])
                 else:
-                    dual_norm_XtA = abs_max(n_features, XtA_data)
+                    dual_norm_XtA = abs_max(n_features, &XtA[0])
 
                 # R_norm2 = np.dot(R, R)
-                R_norm2 = dot(n_samples, R_data, 1, R_data, 1)
+                R_norm2 = dot(n_samples, &R[0], 1, &R[0], 1)
 
                 # w_norm2 = np.dot(w, w)
-                w_norm2 = dot(n_features, w_data, 1, w_data, 1)
+                w_norm2 = dot(n_features, &w[0], 1, &w[0], 1)
 
                 if (dual_norm_XtA > alpha):
                     const = alpha / dual_norm_XtA
@@ -301,11 +281,11 @@ def enet_coordinate_descent(np.ndarray[floating, ndim=1] w,
                     const = 1.0
                     gap = R_norm2
 
-                l1_norm = asum(n_features, w_data, 1)
+                l1_norm = asum(n_features, &w[0], 1)
 
                 # np.dot(R.T, y)
                 gap += (alpha * l1_norm
-                        - const * dot(n_samples, R_data, 1, y_data, n_tasks)
+                        - const * dot(n_samples, &R[0], 1, &y[0], 1)
                         + 0.5 * beta * (1 + const ** 2) * (w_norm2))
 
                 if gap < tol:
@@ -317,7 +297,7 @@ def enet_coordinate_descent(np.ndarray[floating, ndim=1] w,
 @cython.boundscheck(False)
 @cython.wraparound(False)
 @cython.cdivision(True)
-def sparse_enet_coordinate_descent(floating [:] w,
+def sparse_enet_coordinate_descent(floating [::1] w,
                             floating alpha, floating beta,
                             np.ndarray[floating, ndim=1, mode='c'] X_data,
                             np.ndarray[int, ndim=1, mode='c'] X_indices,
@@ -345,9 +325,6 @@ def sparse_enet_coordinate_descent(floating [:] w,
     cdef unsigned int startptr = X_indptr[0]
     cdef unsigned int endptr
 
-    # get the number of tasks indirectly, using strides
-    cdef unsigned int n_tasks
-
     # initial value of the residuals
     cdef floating[:] R = y.copy()
 
@@ -355,17 +332,12 @@ def sparse_enet_coordinate_descent(floating [:] w,
     cdef floating[:] XtA
 
     # fused types version of BLAS functions
-    cdef DOT dot
-    cdef ASUM asum
-
     if floating is float:
         dtype = np.float32
-        n_tasks = y.strides[0] / sizeof(float)
         dot = sdot
         asum = sasum
     else:
         dtype = np.float64
-        n_tasks = y.strides[0] / sizeof(DOUBLE)
         dot = ddot
         asum = dasum
 
@@ -526,7 +498,7 @@ def sparse_enet_coordinate_descent(floating [:] w,
                 gap += (alpha * l1_norm - const * dot(
                             n_samples,
                             &R[0], 1,
-                            &y[0], n_tasks
+                            &y[0], 1
                             )
                         + 0.5 * beta * (1 + const ** 2) * w_norm2)
 
@@ -540,7 +512,8 @@ def sparse_enet_coordinate_descent(floating [:] w,
 @cython.boundscheck(False)
 @cython.wraparound(False)
 @cython.cdivision(True)
-def enet_coordinate_descent_gram(floating[:] w, floating alpha, floating beta,
+def enet_coordinate_descent_gram(floating[::1] w,
+                                 floating alpha, floating beta,
                                  np.ndarray[floating, ndim=2, mode='c'] Q,
                                  np.ndarray[floating, ndim=1, mode='c'] q,
                                  np.ndarray[floating, ndim=1] y,
@@ -559,10 +532,6 @@ def enet_coordinate_descent_gram(floating[:] w, floating alpha, floating beta,
     """
 
     # fused types version of BLAS functions
-    cdef DOT dot
-    cdef AXPY axpy
-    cdef ASUM asum
-
     if floating is float:
         dtype = np.float32
         dot = sdot
@@ -713,10 +682,6 @@ def enet_coordinate_descent_multi_task(floating[::1, :] W, floating l1_reg,
 
     """
     # fused types version of BLAS functions
-    cdef DOT dot
-    cdef AXPY axpy
-    cdef ASUM asum
-
     if floating is float:
         dtype = np.float32
         dot = sdot
diff --git a/sklearn/linear_model/coordinate_descent.py b/sklearn/linear_model/coordinate_descent.py
index bdad75bc6197..6fa71f2dddcf 100644
--- a/sklearn/linear_model/coordinate_descent.py
+++ b/sklearn/linear_model/coordinate_descent.py
@@ -18,7 +18,7 @@
 from ..utils import check_array, check_X_y
 from ..utils.validation import check_random_state
 from ..model_selection import check_cv
-from ..externals.joblib import Parallel, delayed
+from ..utils import Parallel, delayed, effective_n_jobs
 from ..externals import six
 from ..externals.six.moves import xrange
 from ..utils.extmath import safe_sparse_dot
@@ -98,8 +98,8 @@ def _alpha_grid(X, y, Xy=None, l1_ratio=1.0, fit_intercept=True,
             # Workaround to find alpha_max for sparse matrices.
             # since we should not destroy the sparsity of such matrices.
             _, _, X_offset, _, X_scale = _preprocess_data(X, y, fit_intercept,
-                                                      normalize,
-                                                      return_mean=True)
+                                                          normalize,
+                                                          return_mean=True)
             mean_dot = X_offset * np.sum(y)
 
     if Xy.ndim == 1:
@@ -418,7 +418,7 @@ def enet_path(X, y, l1_ratio=0.5, eps=1e-3, n_alphas=100, alphas=None,
     if check_input:
         X, y, X_offset, y_offset, X_scale, precompute, Xy = \
             _pre_fit(X, y, Xy, precompute, normalize=False,
-                     fit_intercept=False, copy=False)
+                     fit_intercept=False, copy=False, check_input=check_input)
     if alphas is None:
         # No need to normalize of fit_intercept: it has been done
         # above
@@ -447,7 +447,7 @@ def enet_path(X, y, l1_ratio=0.5, eps=1e-3, n_alphas=100, alphas=None,
                          dtype=X.dtype)
 
     if coef_init is None:
-        coef_ = np.asfortranarray(np.zeros(coefs.shape[:-1], dtype=X.dtype))
+        coef_ = np.zeros(coefs.shape[:-1], dtype=X.dtype, order='F')
     else:
         coef_ = np.asfortranarray(coef_init, dtype=X.dtype)
 
@@ -717,7 +717,8 @@ def fit(self, X, y, check_input=True):
         should_copy = self.copy_X and not X_copied
         X, y, X_offset, y_offset, X_scale, precompute, Xy = \
             _pre_fit(X, y, None, self.precompute, self.normalize,
-                     self.fit_intercept, copy=should_copy)
+                     self.fit_intercept, copy=should_copy,
+                     check_input=check_input)
         if y.ndim == 1:
             y = y[:, np.newaxis]
         if Xy is not None and Xy.ndim == 1:
@@ -752,8 +753,9 @@ def fit(self, X, y, check_input=True):
                           precompute=precompute, Xy=this_Xy,
                           fit_intercept=False, normalize=False, copy_X=True,
                           verbose=False, tol=self.tol, positive=self.positive,
-                          X_offset=X_offset, X_scale=X_scale, return_n_iter=True,
-                          coef_init=coef_[k], max_iter=self.max_iter,
+                          X_offset=X_offset, X_scale=X_scale,
+                          return_n_iter=True, coef_init=coef_[k],
+                          max_iter=self.max_iter,
                           random_state=self.random_state,
                           selection=self.selection,
                           check_input=False)
@@ -826,9 +828,9 @@ class Lasso(ElasticNet):
         reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.
         Given this, you should use the :class:`LinearRegression` object.
 
-    fit_intercept : boolean
-        whether to calculate the intercept for this model. If set
-        to false, no intercept will be used in calculations
+    fit_intercept : boolean, optional, default True
+        Whether to calculate the intercept for this model. If set
+        to False, no intercept will be used in calculations
         (e.g. data is expected to be already centered).
 
     normalize : boolean, optional, default False
@@ -1052,7 +1054,7 @@ class LinearModelCV(six.with_metaclass(ABCMeta, LinearModel)):
     @abstractmethod
     def __init__(self, eps=1e-3, n_alphas=100, alphas=None, fit_intercept=True,
                  normalize=False, precompute='auto', max_iter=1000, tol=1e-4,
-                 copy_X=True, cv=None, verbose=False, n_jobs=1,
+                 copy_X=True, cv='warn', verbose=False, n_jobs=None,
                  positive=False, random_state=None, selection='cyclic'):
         self.eps = eps
         self.n_alphas = n_alphas
@@ -1182,7 +1184,7 @@ def fit(self, X, y):
         path_params['copy_X'] = copy_X
         # We are not computing in parallel, we can modify X
         # inplace in the folds
-        if not (self.n_jobs == 1 or self.n_jobs is None):
+        if effective_n_jobs(self.n_jobs) > 1:
             path_params['copy_X'] = False
 
         # init cross-validation generator
@@ -1201,7 +1203,7 @@ def fit(self, X, y):
                 for this_l1_ratio, this_alphas in zip(l1_ratios, alphas)
                 for train, test in folds)
         mse_paths = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
-                             backend="threading")(jobs)
+                             prefer="threads")(jobs)
         mse_paths = np.reshape(mse_paths, (n_l1_ratio, len(folds), -1))
         mean_mse = np.mean(mse_paths, axis=1)
         self.mse_path_ = np.squeeze(np.rollaxis(mse_paths, 2, 1))
@@ -1311,12 +1313,18 @@ class LassoCV(LinearModelCV, RegressorMixin):
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     verbose : bool or integer
         Amount of verbosity.
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the cross validation. If ``-1``, use
-        all the CPUs.
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the cross validation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     positive : bool, optional
         If positive, restrict regression coefficients to be positive
@@ -1381,7 +1389,7 @@ class LassoCV(LinearModelCV, RegressorMixin):
 
     def __init__(self, eps=1e-3, n_alphas=100, alphas=None, fit_intercept=True,
                  normalize=False, precompute='auto', max_iter=1000, tol=1e-4,
-                 copy_X=True, cv=None, verbose=False, n_jobs=1,
+                 copy_X=True, cv='warn', verbose=False, n_jobs=None,
                  positive=False, random_state=None, selection='cyclic'):
         super(LassoCV, self).__init__(
             eps=eps, n_alphas=n_alphas, alphas=alphas,
@@ -1464,15 +1472,21 @@ class ElasticNetCV(LinearModelCV, RegressorMixin):
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     copy_X : boolean, optional, default True
         If ``True``, X will be copied; else, it may be overwritten.
 
     verbose : bool or integer
         Amount of verbosity.
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the cross validation. If ``-1``, use
-        all the CPUs.
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the cross validation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     positive : bool, optional
         When set to ``True``, forces the coefficients to be positive.
@@ -1526,7 +1540,7 @@ class ElasticNetCV(LinearModelCV, RegressorMixin):
     >>> regr = ElasticNetCV(cv=5, random_state=0)
     >>> regr.fit(X, y)
     ElasticNetCV(alphas=None, copy_X=True, cv=5, eps=0.001, fit_intercept=True,
-           l1_ratio=0.5, max_iter=1000, n_alphas=100, n_jobs=1,
+           l1_ratio=0.5, max_iter=1000, n_alphas=100, n_jobs=None,
            normalize=False, positive=False, precompute='auto', random_state=0,
            selection='cyclic', tol=0.0001, verbose=0)
     >>> print(regr.alpha_) # doctest: +ELLIPSIS
@@ -1573,8 +1587,8 @@ class ElasticNetCV(LinearModelCV, RegressorMixin):
 
     def __init__(self, l1_ratio=0.5, eps=1e-3, n_alphas=100, alphas=None,
                  fit_intercept=True, normalize=False, precompute='auto',
-                 max_iter=1000, tol=1e-4, cv=None, copy_X=True,
-                 verbose=0, n_jobs=1, positive=False, random_state=None,
+                 max_iter=1000, tol=1e-4, cv='warn', copy_X=True,
+                 verbose=0, n_jobs=None, positive=False, random_state=None,
                  selection='cyclic'):
         self.l1_ratio = l1_ratio
         self.eps = eps
@@ -1603,13 +1617,13 @@ class MultiTaskElasticNet(Lasso):
 
     The optimization objective for MultiTaskElasticNet is::
 
-        (1 / (2 * n_samples)) * ||Y - XW||^Fro_2
+        (1 / (2 * n_samples)) * ||Y - XW||_Fro^2
         + alpha * l1_ratio * ||W||_21
         + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2
 
     Where::
 
-        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}
+        ||W||_21 = sum_i sqrt(sum_j w_ij ^ 2)
 
     i.e. the sum of norm of each row.
 
@@ -1994,16 +2008,22 @@ class MultiTaskElasticNetCV(LinearModelCV, RegressorMixin):
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     copy_X : boolean, optional, default True
         If ``True``, X will be copied; else, it may be overwritten.
 
     verbose : bool or integer
         Amount of verbosity.
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the cross validation. If ``-1``, use
-        all the CPUs. Note that this is used only if multiple values for
-        l1_ratio are given.
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the cross validation. Note that this is
+        used only if multiple values for l1_ratio are given.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional, default None
         The seed of the pseudo random number generator that selects a random
@@ -2048,13 +2068,13 @@ class MultiTaskElasticNetCV(LinearModelCV, RegressorMixin):
     Examples
     --------
     >>> from sklearn import linear_model
-    >>> clf = linear_model.MultiTaskElasticNetCV()
+    >>> clf = linear_model.MultiTaskElasticNetCV(cv=3)
     >>> clf.fit([[0,0], [1, 1], [2, 2]],
     ...         [[0, 0], [1, 1], [2, 2]])
     ... #doctest: +NORMALIZE_WHITESPACE
-    MultiTaskElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001,
+    MultiTaskElasticNetCV(alphas=None, copy_X=True, cv=3, eps=0.001,
            fit_intercept=True, l1_ratio=0.5, max_iter=1000, n_alphas=100,
-           n_jobs=1, normalize=False, random_state=None, selection='cyclic',
+           n_jobs=None, normalize=False, random_state=None, selection='cyclic',
            tol=0.0001, verbose=0)
     >>> print(clf.coef_)
     [[0.52875032 0.46958558]
@@ -2079,8 +2099,9 @@ class MultiTaskElasticNetCV(LinearModelCV, RegressorMixin):
 
     def __init__(self, l1_ratio=0.5, eps=1e-3, n_alphas=100, alphas=None,
                  fit_intercept=True, normalize=False,
-                 max_iter=1000, tol=1e-4, cv=None, copy_X=True,
-                 verbose=0, n_jobs=1, random_state=None, selection='cyclic'):
+                 max_iter=1000, tol=1e-4, cv='warn', copy_X=True,
+                 verbose=0, n_jobs=None, random_state=None,
+                 selection='cyclic'):
         self.l1_ratio = l1_ratio
         self.eps = eps
         self.n_alphas = n_alphas
@@ -2164,13 +2185,19 @@ class MultiTaskLassoCV(LinearModelCV, RegressorMixin):
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     verbose : bool or integer
         Amount of verbosity.
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the cross validation. If ``-1``, use
-        all the CPUs. Note that this is used only if multiple values for
-        l1_ratio are given.
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the cross validation. Note that this is
+        used only if multiple values for l1_ratio are given.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional, default None
         The seed of the pseudo random number generator that selects a random
@@ -2225,7 +2252,7 @@ class MultiTaskLassoCV(LinearModelCV, RegressorMixin):
 
     def __init__(self, eps=1e-3, n_alphas=100, alphas=None, fit_intercept=True,
                  normalize=False, max_iter=1000, tol=1e-4, copy_X=True,
-                 cv=None, verbose=False, n_jobs=1, random_state=None,
+                 cv='warn', verbose=False, n_jobs=None, random_state=None,
                  selection='cyclic'):
         super(MultiTaskLassoCV, self).__init__(
             eps=eps, n_alphas=n_alphas, alphas=alphas,
diff --git a/sklearn/linear_model/least_angle.py b/sklearn/linear_model/least_angle.py
index cd10edcc4e94..d139560260a8 100644
--- a/sklearn/linear_model/least_angle.py
+++ b/sklearn/linear_model/least_angle.py
@@ -23,7 +23,7 @@
 from ..utils import arrayfuncs, as_float_array, check_X_y, deprecated
 from ..model_selection import check_cv
 from ..exceptions import ConvergenceWarning
-from ..externals.joblib import Parallel, delayed
+from ..utils import Parallel, delayed
 from ..externals.six.moves import xrange
 from ..externals.six import string_types
 
@@ -606,7 +606,8 @@ def __init__(self, fit_intercept=True, verbose=False, normalize=True,
         self.copy_X = copy_X
         self.fit_path = fit_path
 
-    def _get_gram(self, precompute, X, y):
+    @staticmethod
+    def _get_gram(precompute, X, y):
         if (not hasattr(precompute, '__array__')) and (
                 (precompute is True) or
                 (precompute == 'auto' and X.shape[0] > X.shape[1]) or
@@ -1013,13 +1014,19 @@ class LarsCV(Lars):
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     max_n_alphas : integer, optional
         The maximum number of points on the path used to compute the
         residuals in the cross-validation
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the cross validation. If ``-1``, use
-        all the CPUs
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the cross validation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     eps : float, optional
         The machine-precision regularization in the computation of the
@@ -1071,8 +1078,8 @@ class LarsCV(Lars):
     method = 'lar'
 
     def __init__(self, fit_intercept=True, verbose=False, max_iter=500,
-                 normalize=True, precompute='auto', cv=None,
-                 max_n_alphas=1000, n_jobs=1, eps=np.finfo(np.float).eps,
+                 normalize=True, precompute='auto', cv='warn',
+                 max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
                  copy_X=True, positive=False):
         self.max_iter = max_iter
         self.cv = cv
@@ -1222,13 +1229,19 @@ class LassoLarsCV(LarsCV):
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     max_n_alphas : integer, optional
         The maximum number of points on the path used to compute the
         residuals in the cross-validation
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the cross validation. If ``-1``, use
-        all the CPUs
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the cross validation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     eps : float, optional
         The machine-precision regularization in the computation of the
@@ -1297,8 +1310,8 @@ class LassoLarsCV(LarsCV):
     method = 'lasso'
 
     def __init__(self, fit_intercept=True, verbose=False, max_iter=500,
-                 normalize=True, precompute='auto', cv=None,
-                 max_n_alphas=1000, n_jobs=1, eps=np.finfo(np.float).eps,
+                 normalize=True, precompute='auto', cv='warn',
+                 max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
                  copy_X=True, positive=False):
         self.fit_intercept = fit_intercept
         self.verbose = verbose
diff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py
index e4ea696ce714..85a531dacf3d 100644
--- a/sklearn/linear_model/logistic.py
+++ b/sklearn/linear_model/logistic.py
@@ -32,7 +32,7 @@
 from ..exceptions import (NotFittedError, ConvergenceWarning,
                           ChangedBehaviorWarning)
 from ..utils.multiclass import check_classification_targets
-from ..externals.joblib import Parallel, delayed
+from ..utils import Parallel, delayed, effective_n_jobs
 from ..model_selection import check_cv
 from ..externals import six
 from ..metrics import get_scorer
@@ -61,8 +61,8 @@ def _intercept_dot(w, X, y):
         Coefficient vector without the intercept weight (w[-1]) if the
         intercept should be fit. Unchanged otherwise.
 
-    X : {array-like, sparse matrix}, shape (n_samples, n_features)
-        Training data. Unchanged.
+    c : float
+        The intercept.
 
     yz : float
         y * np.dot(X, w).
@@ -572,7 +572,9 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
     coefs : ndarray, shape (n_cs, n_features) or (n_cs, n_features + 1)
         List of coefficients for the Logistic Regression model. If
         fit_intercept is set to True then the second dimension will be
-        n_features + 1, where the last item represents the intercept.
+        n_features + 1, where the last item represents the intercept. For
+        ``multiclass='multinomial'``, the shape is (n_classes, n_cs,
+        n_features) or (n_classes, n_cs, n_features + 1).
 
     Cs : ndarray
         Grid of Cs used for cross-validation.
@@ -684,7 +686,6 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
             else:
                 w0[:, :coef.shape[1]] = coef
 
-
     if multi_class == 'multinomial':
         # fmin_l_bfgs_b and newton-cg accepts only ravelled parameters.
         if solver in ['lbfgs', 'newton-cg']:
@@ -711,10 +712,12 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
     n_iter = np.zeros(len(Cs), dtype=np.int32)
     for i, C in enumerate(Cs):
         if solver == 'lbfgs':
+            iprint = [-1, 50, 1, 100, 101][
+                np.searchsorted(np.array([0, 1, 2, 3]), verbose)]
             w0, loss, info = optimize.fmin_l_bfgs_b(
                 func, w0, fprime=None,
                 args=(X, target, 1. / C, sample_weight),
-                iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
+                iprint=iprint, pgtol=tol, maxiter=max_iter)
             if info["warnflag"] == 1:
                 warnings.warn("lbfgs failed to converge. Increase the number "
                               "of iterations.", ConvergenceWarning)
@@ -761,13 +764,13 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
             multi_w0 = np.reshape(w0, (classes.size, -1))
             if classes.size == 2:
                 multi_w0 = multi_w0[1][np.newaxis, :]
-            coefs.append(multi_w0)
+            coefs.append(multi_w0.copy())
         else:
             coefs.append(w0.copy())
 
         n_iter[i] = n_iter_i
 
-    return coefs, np.array(Cs), n_iter
+    return np.array(coefs), np.array(Cs), n_iter
 
 
 # helper function for LogisticCV
@@ -922,7 +925,7 @@ def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,
         check_input=False, max_squared_sum=max_squared_sum,
         sample_weight=sample_weight)
 
-    log_reg = LogisticRegression(fit_intercept=fit_intercept)
+    log_reg = LogisticRegression(multi_class=multi_class)
 
     # The score method of Logistic Regression has a classes_ attribute.
     if multi_class == 'ovr':
@@ -1042,8 +1045,9 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,
         instance used by `np.random`. Used when ``solver`` == 'sag' or
         'liblinear'.
 
-    solver : str, {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},
-        default: 'liblinear'
+    solver : str, {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, \
+             default: 'liblinear'
+
         Algorithm to use in the optimization problem.
 
         - For small datasets, 'liblinear' is a good choice, whereas 'sag' and
@@ -1089,11 +1093,13 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,
         .. versionadded:: 0.17
            *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.
 
-    n_jobs : int, default: 1
+    n_jobs : int or None, optional (default=None)
         Number of CPU cores used when parallelizing over classes if
         multi_class='ovr'". This parameter is ignored when the ``solver`` is
         set to 'liblinear' regardless of whether 'multi_class' is specified or
-        not. If given a value of -1, all cores are used.
+        not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`
+        context. ``-1`` means using all processors.
+        See :term:`Glossary <n_jobs>` for more details.
 
     Attributes
     ----------
@@ -1124,6 +1130,20 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,
             In SciPy <= 1.0.0 the number of lbfgs iterations may exceed
             ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_iris
+    >>> from sklearn.linear_model import LogisticRegression
+    >>> X, y = load_iris(return_X_y=True)
+    >>> clf = LogisticRegression(random_state=0).fit(X, y)
+    >>> clf.predict(X[:2, :])
+    array([0, 0])
+    >>> clf.predict_proba(X[:2, :]) # doctest: +ELLIPSIS
+    array([[8.78...e-01, 1.21...e-01, 1.079...e-05],
+           [7.97...e-01, 2.02...e-01, 3.029...e-05]])
+    >>> clf.score(X, y)
+    0.96
+
     See also
     --------
     SGDClassifier : incrementally trained logistic regression (when given
@@ -1166,7 +1186,7 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,
     def __init__(self, penalty='l2', dual=False, tol=1e-4, C=1.0,
                  fit_intercept=True, intercept_scaling=1, class_weight=None,
                  random_state=None, solver='liblinear', max_iter=100,
-                 multi_class='ovr', verbose=0, warm_start=False, n_jobs=1):
+                 multi_class='ovr', verbose=0, warm_start=False, n_jobs=None):
 
         self.penalty = penalty
         self.dual = dual
@@ -1231,10 +1251,10 @@ def fit(self, X, y, sample_weight=None):
                              self.dual)
 
         if self.solver == 'liblinear':
-            if self.n_jobs != 1:
+            if effective_n_jobs(self.n_jobs) != 1:
                 warnings.warn("'n_jobs' > 1 does not have any effect when"
                               " 'solver' is set to 'liblinear'. Got 'n_jobs'"
-                              " = {}.".format(self.n_jobs))
+                              " = {}.".format(effective_n_jobs(self.n_jobs)))
             self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                 X, y, self.C, self.fit_intercept, self.intercept_scaling,
                 self.class_weight, self.penalty, self.dual, self.verbose,
@@ -1283,11 +1303,11 @@ def fit(self, X, y, sample_weight=None):
         # The SAG solver releases the GIL so it's more efficient to use
         # threads for this solver.
         if self.solver in ['sag', 'saga']:
-            backend = 'threading'
+            prefer = 'threads'
         else:
-            backend = 'multiprocessing'
+            prefer = 'processes'
         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
-                               backend=backend)(
+                               prefer=prefer)(
             path_func(X, y, pos_class=class_, Cs=[self.C],
                       fit_intercept=self.fit_intercept, tol=self.tol,
                       verbose=self.verbose, solver=self.solver,
@@ -1407,12 +1427,16 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,
         Specifies if a constant (a.k.a. bias or intercept) should be
         added to the decision function.
 
-    cv : integer or cross-validation generator
+    cv : integer or cross-validation generator, default: None
         The default cross-validation generator used is Stratified K-Folds.
         If an integer is provided, then it is the number of folds used.
         See the module :mod:`sklearn.model_selection` module for the
         list of possible cross-validation objects.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     dual : bool
         Dual or primal formulation. Dual formulation is only implemented for
         l2 penalty with liblinear solver. Prefer dual=False when
@@ -1429,8 +1453,9 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,
         that can be used, look at :mod:`sklearn.metrics`. The
         default scoring option used is 'accuracy'.
 
-    solver : str, {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},
-        default: 'lbfgs'
+    solver : str, {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, \
+             default: 'lbfgs'
+
         Algorithm to use in the optimization problem.
 
         - For small datasets, 'liblinear' is a good choice, whereas 'sag' and
@@ -1472,9 +1497,11 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,
         .. versionadded:: 0.17
            class_weight == 'balanced'
 
-    n_jobs : int, optional
-        Number of CPU cores used during the cross-validation loop. If given
-        a value of -1, all cores are used.
+    n_jobs : int or None, optional (default=None)
+        Number of CPU cores used during the cross-validation loop.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : int
         For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any
@@ -1563,15 +1590,29 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,
         Actual number of iterations for all classes, folds and Cs.
         In the binary or multinomial cases, the first dimension is equal to 1.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_iris
+    >>> from sklearn.linear_model import LogisticRegressionCV
+    >>> X, y = load_iris(return_X_y=True)
+    >>> clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)
+    >>> clf.predict(X[:2, :])
+    array([0, 0])
+    >>> clf.predict_proba(X[:2, :]) # doctest: +ELLIPSIS
+    array([[8.72...e-01, 1.27...e-01, 5.50...e-14],
+           [6.76...e-01, 3.23...e-01, 2.11...e-13]])
+    >>> clf.score(X, y) # doctest: +ELLIPSIS
+    0.9266...
+
     See also
     --------
     LogisticRegression
 
     """
 
-    def __init__(self, Cs=10, fit_intercept=True, cv=None, dual=False,
+    def __init__(self, Cs=10, fit_intercept=True, cv='warn', dual=False,
                  penalty='l2', scoring=None, solver='lbfgs', tol=1e-4,
-                 max_iter=100, class_weight=None, n_jobs=1, verbose=0,
+                 max_iter=100, class_weight=None, n_jobs=None, verbose=0,
                  refit=True, intercept_scaling=1., multi_class='ovr',
                  random_state=None):
         self.Cs = Cs
@@ -1683,11 +1724,11 @@ def fit(self, X, y, sample_weight=None):
         # The SAG solver releases the GIL so it's more efficient to use
         # threads for this solver.
         if self.solver in ['sag', 'saga']:
-            backend = 'threading'
+            prefer = 'threads'
         else:
-            backend = 'multiprocessing'
+            prefer = 'processes'
         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
-                               backend=backend)(
+                               prefer=prefer)(
             path_func(X, y, train, test, pos_class=label, Cs=self.Cs,
                       fit_intercept=self.fit_intercept, penalty=self.penalty,
                       dual=self.dual, solver=self.solver, tol=self.tol,
diff --git a/sklearn/linear_model/omp.py b/sklearn/linear_model/omp.py
index 777b915d0339..a0f6d4949094 100644
--- a/sklearn/linear_model/omp.py
+++ b/sklearn/linear_model/omp.py
@@ -16,7 +16,7 @@
 from ..base import RegressorMixin
 from ..utils import as_float_array, check_array, check_X_y
 from ..model_selection import check_cv
-from ..externals.joblib import Parallel, delayed
+from ..utils import Parallel, delayed
 
 premature = """ Orthogonal matching pursuit ended prematurely due to linear
 dependence in the dictionary. The requested precision might not have been met.
@@ -785,9 +785,15 @@ class OrthogonalMatchingPursuitCV(LinearModel, RegressorMixin):
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the cross validation. If ``-1``, use
-        all the CPUs
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the cross validation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : boolean or integer, optional
         Sets the verbosity amount
@@ -822,7 +828,7 @@ class OrthogonalMatchingPursuitCV(LinearModel, RegressorMixin):
 
     """
     def __init__(self, copy=True, fit_intercept=True, normalize=True,
-                 max_iter=None, cv=None, n_jobs=1, verbose=False):
+                 max_iter=None, cv='warn', n_jobs=None, verbose=False):
         self.copy = copy
         self.fit_intercept = fit_intercept
         self.normalize = normalize
diff --git a/sklearn/linear_model/passive_aggressive.py b/sklearn/linear_model/passive_aggressive.py
index e803840279ad..22f1c0fbba12 100644
--- a/sklearn/linear_model/passive_aggressive.py
+++ b/sklearn/linear_model/passive_aggressive.py
@@ -36,6 +36,27 @@ class PassiveAggressiveClassifier(BaseSGDClassifier):
 
         .. versionadded:: 0.19
 
+    early_stopping : bool, default=False
+        Whether to use early stopping to terminate training when validation.
+        score is not improving. If set to True, it will automatically set aside
+        a fraction of training data as validation and terminate training when
+        validation score is not improving by at least tol for
+        n_iter_no_change consecutive epochs.
+
+        .. versionadded:: 0.20
+
+    validation_fraction : float, default=0.1
+        The proportion of training data to set aside as validation set for
+        early stopping. Must be between 0 and 1.
+        Only used if early_stopping is True.
+
+        .. versionadded:: 0.20
+
+    n_iter_no_change : int, default=5
+        Number of iterations with no improvement to wait before early stopping.
+
+        .. versionadded:: 0.20
+
     shuffle : bool, default=True
         Whether or not the training data should be shuffled after each epoch.
 
@@ -47,10 +68,12 @@ class PassiveAggressiveClassifier(BaseSGDClassifier):
         hinge: equivalent to PA-I in the reference paper.
         squared_hinge: equivalent to PA-II in the reference paper.
 
-    n_jobs : integer, optional
+    n_jobs : int or None, optional (default=None)
         The number of CPUs to use to do the OVA (One Versus All, for
-        multi-class problems) computation. -1 means 'all CPUs'. Defaults
-        to 1.
+        multi-class problems) computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional, default=None
         The seed of the pseudo random number generator to use when shuffling
@@ -116,16 +139,17 @@ class PassiveAggressiveClassifier(BaseSGDClassifier):
     >>> from sklearn.datasets import make_classification
     >>>
     >>> X, y = make_classification(n_features=4, random_state=0)
-    >>> clf = PassiveAggressiveClassifier(random_state=0)
+    >>> clf = PassiveAggressiveClassifier(max_iter=1000, random_state=0)
     >>> clf.fit(X, y)
     PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,
-                  fit_intercept=True, loss='hinge', max_iter=None, n_iter=None,
-                  n_jobs=1, random_state=0, shuffle=True, tol=None, verbose=0,
-                  warm_start=False)
+                  early_stopping=False, fit_intercept=True, loss='hinge',
+                  max_iter=1000, n_iter=None, n_iter_no_change=5, n_jobs=None,
+                  random_state=0, shuffle=True, tol=None,
+                  validation_fraction=0.1, verbose=0, warm_start=False)
     >>> print(clf.coef_)
-    [[0.49324685 1.0552176  1.49519589 1.33798314]]
+    [[0.29509834 0.33711843 0.56127352 0.60105546]]
     >>> print(clf.intercept_)
-    [2.18438388]
+    [2.54153383]
     >>> print(clf.predict([[0, 0, 0, 0]]))
     [1]
 
@@ -143,14 +167,18 @@ class PassiveAggressiveClassifier(BaseSGDClassifier):
 
     """
     def __init__(self, C=1.0, fit_intercept=True, max_iter=None, tol=None,
-                 shuffle=True, verbose=0, loss="hinge", n_jobs=1,
-                 random_state=None, warm_start=False, class_weight=None,
-                 average=False, n_iter=None):
+                 early_stopping=False, validation_fraction=0.1,
+                 n_iter_no_change=5, shuffle=True, verbose=0, loss="hinge",
+                 n_jobs=None, random_state=None, warm_start=False,
+                 class_weight=None, average=False, n_iter=None):
         super(PassiveAggressiveClassifier, self).__init__(
             penalty=None,
             fit_intercept=fit_intercept,
             max_iter=max_iter,
             tol=tol,
+            early_stopping=early_stopping,
+            validation_fraction=validation_fraction,
+            n_iter_no_change=n_iter_no_change,
             shuffle=shuffle,
             verbose=verbose,
             random_state=random_state,
@@ -187,6 +215,7 @@ def partial_fit(self, X, y, classes=None):
         -------
         self : returns an instance of self.
         """
+        self._validate_params(for_partial_fit=True)
         if self.class_weight == 'balanced':
             raise ValueError("class_weight 'balanced' is not supported for "
                              "partial_fit. For 'balanced' weights, use "
@@ -224,6 +253,7 @@ def fit(self, X, y, coef_init=None, intercept_init=None):
         -------
         self : returns an instance of self.
         """
+        self._validate_params()
         lr = "pa1" if self.loss == "hinge" else "pa2"
         return self._fit(X, y, alpha=1.0, C=self.C,
                          loss="hinge", learning_rate=lr,
@@ -260,6 +290,27 @@ class PassiveAggressiveRegressor(BaseSGDRegressor):
 
         .. versionadded:: 0.19
 
+    early_stopping : bool, default=False
+        Whether to use early stopping to terminate training when validation.
+        score is not improving. If set to True, it will automatically set aside
+        a fraction of training data as validation and terminate training when
+        validation score is not improving by at least tol for
+        n_iter_no_change consecutive epochs.
+
+        .. versionadded:: 0.20
+
+    validation_fraction : float, default=0.1
+        The proportion of training data to set aside as validation set for
+        early stopping. Must be between 0 and 1.
+        Only used if early_stopping is True.
+
+        .. versionadded:: 0.20
+
+    n_iter_no_change : int, default=5
+        Number of iterations with no improvement to wait before early stopping.
+
+        .. versionadded:: 0.20
+
     shuffle : bool, default=True
         Whether or not the training data should be shuffled after each epoch.
 
@@ -326,12 +377,13 @@ class PassiveAggressiveRegressor(BaseSGDRegressor):
     >>> from sklearn.datasets import make_regression
     >>>
     >>> X, y = make_regression(n_features=4, random_state=0)
-    >>> regr = PassiveAggressiveRegressor(random_state=0)
+    >>> regr = PassiveAggressiveRegressor(max_iter=100, random_state=0)
     >>> regr.fit(X, y)
-    PassiveAggressiveRegressor(C=1.0, average=False, epsilon=0.1,
-                  fit_intercept=True, loss='epsilon_insensitive',
-                  max_iter=None, n_iter=None, random_state=0, shuffle=True,
-                  tol=None, verbose=0, warm_start=False)
+    PassiveAggressiveRegressor(C=1.0, average=False, early_stopping=False,
+                  epsilon=0.1, fit_intercept=True, loss='epsilon_insensitive',
+                  max_iter=100, n_iter=None, n_iter_no_change=5,
+                  random_state=0, shuffle=True, tol=None,
+                  validation_fraction=0.1, verbose=0, warm_start=False)
     >>> print(regr.coef_)
     [20.48736655 34.18818427 67.59122734 87.94731329]
     >>> print(regr.intercept_)
@@ -352,8 +404,10 @@ class PassiveAggressiveRegressor(BaseSGDRegressor):
 
     """
     def __init__(self, C=1.0, fit_intercept=True, max_iter=None, tol=None,
-                 shuffle=True, verbose=0, loss="epsilon_insensitive",
-                 epsilon=DEFAULT_EPSILON, random_state=None, warm_start=False,
+                 early_stopping=False, validation_fraction=0.1,
+                 n_iter_no_change=5, shuffle=True, verbose=0,
+                 loss="epsilon_insensitive", epsilon=DEFAULT_EPSILON,
+                 random_state=None, warm_start=False,
                  average=False, n_iter=None):
         super(PassiveAggressiveRegressor, self).__init__(
             penalty=None,
@@ -363,6 +417,9 @@ def __init__(self, C=1.0, fit_intercept=True, max_iter=None, tol=None,
             fit_intercept=fit_intercept,
             max_iter=max_iter,
             tol=tol,
+            early_stopping=early_stopping,
+            validation_fraction=validation_fraction,
+            n_iter_no_change=n_iter_no_change,
             shuffle=shuffle,
             verbose=verbose,
             random_state=random_state,
@@ -387,7 +444,7 @@ def partial_fit(self, X, y):
         -------
         self : returns an instance of self.
         """
-        self._validate_params()
+        self._validate_params(for_partial_fit=True)
         lr = "pa1" if self.loss == "epsilon_insensitive" else "pa2"
         return self._partial_fit(X, y, alpha=1.0, C=self.C,
                                  loss="epsilon_insensitive",
@@ -416,6 +473,7 @@ def fit(self, X, y, coef_init=None, intercept_init=None):
         -------
         self : returns an instance of self.
         """
+        self._validate_params()
         lr = "pa1" if self.loss == "epsilon_insensitive" else "pa2"
         return self._fit(X, y, alpha=1.0, C=self.C,
                          loss="epsilon_insensitive",
diff --git a/sklearn/linear_model/perceptron.py b/sklearn/linear_model/perceptron.py
index a09663e4873f..1bc06f4f1727 100644
--- a/sklearn/linear_model/perceptron.py
+++ b/sklearn/linear_model/perceptron.py
@@ -47,10 +47,12 @@ class Perceptron(BaseSGDClassifier):
     eta0 : double
         Constant by which the updates are multiplied. Defaults to 1.
 
-    n_jobs : integer, optional
+    n_jobs : int or None, optional (default=None)
         The number of CPUs to use to do the OVA (One Versus All, for
-        multi-class problems) computation. -1 means 'all CPUs'. Defaults
-        to 1.
+        multi-class problems) computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional, default None
         The seed of the pseudo random number generator to use when shuffling
@@ -59,6 +61,27 @@ class Perceptron(BaseSGDClassifier):
         generator; If None, the random number generator is the RandomState
         instance used by `np.random`.
 
+    early_stopping : bool, default=False
+        Whether to use early stopping to terminate training when validation.
+        score is not improving. If set to True, it will automatically set aside
+        a fraction of training data as validation and terminate training when
+        validation score is not improving by at least tol for
+        n_iter_no_change consecutive epochs.
+
+        .. versionadded:: 0.20
+
+    validation_fraction : float, default=0.1
+        The proportion of training data to set aside as validation set for
+        early stopping. Must be between 0 and 1.
+        Only used if early_stopping is True.
+
+        .. versionadded:: 0.20
+
+    n_iter_no_change : int, default=5
+        Number of iterations with no improvement to wait before early stopping.
+
+        .. versionadded:: 0.20
+
     class_weight : dict, {class_label: weight} or "balanced" or None, optional
         Preset for the class_weight fit parameter.
 
@@ -102,6 +125,20 @@ class Perceptron(BaseSGDClassifier):
     ``Perceptron()`` is equivalent to `SGDClassifier(loss="perceptron",
     eta0=1, learning_rate="constant", penalty=None)`.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.linear_model import Perceptron
+    >>> X, y = load_digits(return_X_y=True)
+    >>> clf = Perceptron(tol=1e-3, random_state=0)
+    >>> clf.fit(X, y)
+    Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,
+          fit_intercept=True, max_iter=None, n_iter=None, n_iter_no_change=5,
+          n_jobs=None, penalty=None, random_state=0, shuffle=True, tol=0.001,
+          validation_fraction=0.1, verbose=0, warm_start=False)
+    >>> clf.score(X, y) # doctest: +ELLIPSIS
+    0.946...
+
     See also
     --------
 
@@ -114,21 +151,15 @@ class Perceptron(BaseSGDClassifier):
     """
     def __init__(self, penalty=None, alpha=0.0001, fit_intercept=True,
                  max_iter=None, tol=None, shuffle=True, verbose=0, eta0=1.0,
-                 n_jobs=1, random_state=0, class_weight=None,
-                 warm_start=False, n_iter=None):
-        super(Perceptron, self).__init__(loss="perceptron",
-                                         penalty=penalty,
-                                         alpha=alpha, l1_ratio=0,
-                                         fit_intercept=fit_intercept,
-                                         max_iter=max_iter,
-                                         tol=tol,
-                                         shuffle=shuffle,
-                                         verbose=verbose,
-                                         random_state=random_state,
-                                         learning_rate="constant",
-                                         eta0=eta0,
-                                         power_t=0.5,
-                                         warm_start=warm_start,
-                                         class_weight=class_weight,
-                                         n_jobs=n_jobs,
-                                         n_iter=n_iter)
+                 n_jobs=None, random_state=0, early_stopping=False,
+                 validation_fraction=0.1, n_iter_no_change=5,
+                 class_weight=None, warm_start=False, n_iter=None):
+        super(Perceptron, self).__init__(
+            loss="perceptron", penalty=penalty, alpha=alpha, l1_ratio=0,
+            fit_intercept=fit_intercept, max_iter=max_iter, tol=tol,
+            shuffle=shuffle, verbose=verbose, random_state=random_state,
+            learning_rate="constant", eta0=eta0, early_stopping=early_stopping,
+            validation_fraction=validation_fraction,
+            n_iter_no_change=n_iter_no_change, power_t=0.5,
+            warm_start=warm_start, class_weight=class_weight, n_jobs=n_jobs,
+            n_iter=n_iter)
diff --git a/sklearn/linear_model/randomized_l1.py b/sklearn/linear_model/randomized_l1.py
index 1b8cb567b661..cc87276964a8 100644
--- a/sklearn/linear_model/randomized_l1.py
+++ b/sklearn/linear_model/randomized_l1.py
@@ -19,7 +19,7 @@
 from .base import _preprocess_data
 from ..base import BaseEstimator
 from ..externals import six
-from ..externals.joblib import Memory, Parallel, delayed
+from ..utils import Memory, Parallel, delayed
 from ..feature_selection.base import SelectorMixin
 from ..utils import (as_float_array, check_random_state, check_X_y, safe_mask,
                      deprecated)
@@ -33,7 +33,7 @@
 # Randomized linear model: feature selection
 
 def _resample_model(estimator_func, X, y, scaling=.5, n_resampling=200,
-                    n_jobs=1, verbose=False, pre_dispatch='3*n_jobs',
+                    n_jobs=None, verbose=False, pre_dispatch='3*n_jobs',
                     random_state=None, sample_fraction=.75, **params):
     random_state = check_random_state(random_state)
     # We are generating 1 - weights, and not weights
@@ -109,7 +109,7 @@ def fit(self, X, y):
             memory = Memory(cachedir=memory, verbose=0)
         elif not isinstance(memory, Memory):
             raise ValueError("'memory' should either be a string or"
-                             " a sklearn.externals.joblib.Memory"
+                             " a sklearn.utils.Memory"
                              " instance, got 'memory={!r}' instead.".format(
                                  type(memory)))
 
@@ -257,9 +257,11 @@ class RandomizedLasso(BaseRandomizedLinearModel):
         If None, the random number generator is the RandomState instance used
         by `np.random`.
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the resampling. If '-1', use
-        all the CPUs
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the resampling.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     pre_dispatch : int, or string, optional
         Controls the number of jobs that get dispatched during parallel
@@ -296,7 +298,7 @@ class RandomizedLasso(BaseRandomizedLinearModel):
     Examples
     --------
     >>> from sklearn.linear_model import RandomizedLasso
-    >>> randomized_lasso = RandomizedLasso()
+    >>> randomized_lasso = RandomizedLasso() # doctest: +SKIP
 
     References
     ----------
@@ -316,7 +318,7 @@ def __init__(self, alpha='aic', scaling=.5, sample_fraction=.75,
                  normalize=True, precompute='auto',
                  max_iter=500,
                  eps=np.finfo(np.float).eps, random_state=None,
-                 n_jobs=1, pre_dispatch='3*n_jobs',
+                 n_jobs=None, pre_dispatch='3*n_jobs',
                  memory=None):
         self.alpha = alpha
         self.scaling = scaling
@@ -451,9 +453,11 @@ class RandomizedLogisticRegression(BaseRandomizedLinearModel):
         If None, the random number generator is the RandomState instance used
         by `np.random`.
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the resampling. If '-1', use
-        all the CPUs
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the resampling.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     pre_dispatch : int, or string, optional
         Controls the number of jobs that get dispatched during parallel
@@ -490,7 +494,7 @@ class RandomizedLogisticRegression(BaseRandomizedLinearModel):
     Examples
     --------
     >>> from sklearn.linear_model import RandomizedLogisticRegression
-    >>> randomized_logistic = RandomizedLogisticRegression()
+    >>> randomized_logistic = RandomizedLogisticRegression() # doctest: +SKIP
 
     References
     ----------
@@ -510,7 +514,7 @@ def __init__(self, C=1, scaling=.5, sample_fraction=.75,
                  fit_intercept=True, verbose=False,
                  normalize=True,
                  random_state=None,
-                 n_jobs=1, pre_dispatch='3*n_jobs',
+                 n_jobs=None, pre_dispatch='3*n_jobs',
                  memory=None):
         self.C = C
         self.scaling = scaling
@@ -572,7 +576,7 @@ def _lasso_stability_path(X, y, mask, weights, eps):
 def lasso_stability_path(X, y, scaling=0.5, random_state=None,
                          n_resampling=200, n_grid=100,
                          sample_fraction=0.75,
-                         eps=4 * np.finfo(np.float).eps, n_jobs=1,
+                         eps=4 * np.finfo(np.float).eps, n_jobs=None,
                          verbose=False):
     """Stability path based on randomized Lasso estimates
 
@@ -608,9 +612,11 @@ def lasso_stability_path(X, y, scaling=0.5, random_state=None,
     eps : float, optional
         Smallest value of alpha / alpha_max considered
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the resampling. If '-1', use
-        all the CPUs
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the resampling.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : boolean or integer, optional
         Sets the verbosity amount
diff --git a/sklearn/linear_model/ransac.py b/sklearn/linear_model/ransac.py
index f9c372d8ed46..9dcd044d1f3e 100644
--- a/sklearn/linear_model/ransac.py
+++ b/sklearn/linear_model/ransac.py
@@ -74,6 +74,8 @@ class RANSACRegressor(BaseEstimator, MetaEstimatorMixin, RegressorMixin):
            which is used for the stop criterion defined by `stop_score`.
            Additionally, the score is used to decide which of two equally
            large consensus sets is chosen as the better one.
+         * `predict(X)`: Returns predicted values using the linear model,
+           which is used to compute residual error using loss function.
 
         If `base_estimator` is None, then
         ``base_estimator=sklearn.linear_model.LinearRegression()`` is used for
diff --git a/sklearn/linear_model/ridge.py b/sklearn/linear_model/ridge.py
index 80778132bb24..089540efed0f 100644
--- a/sklearn/linear_model/ridge.py
+++ b/sklearn/linear_model/ridge.py
@@ -59,7 +59,12 @@ def _mv(x):
             # w = X.T * inv(X X^t + alpha*Id) y
             C = sp_linalg.LinearOperator(
                 (n_samples, n_samples), matvec=mv, dtype=X.dtype)
-            coef, info = sp_linalg.cg(C, y_column, tol=tol)
+            # FIXME atol
+            try:
+                coef, info = sp_linalg.cg(C, y_column, tol=tol, atol='legacy')
+            except TypeError:
+                # old scipy
+                coef, info = sp_linalg.cg(C, y_column, tol=tol)
             coefs[i] = X1.rmatvec(coef)
         else:
             # linear ridge
@@ -67,8 +72,15 @@ def _mv(x):
             y_column = X1.rmatvec(y_column)
             C = sp_linalg.LinearOperator(
                 (n_features, n_features), matvec=mv, dtype=X.dtype)
-            coefs[i], info = sp_linalg.cg(C, y_column, maxiter=max_iter,
-                                          tol=tol)
+            # FIXME atol
+            try:
+                coefs[i], info = sp_linalg.cg(C, y_column, maxiter=max_iter,
+                                              tol=tol, atol='legacy')
+            except TypeError:
+                # old scipy
+                coefs[i], info = sp_linalg.cg(C, y_column, maxiter=max_iter,
+                                              tol=tol)
+
         if info < 0:
             raise ValueError("Failed with error code %d" % info)
 
@@ -767,6 +779,15 @@ class RidgeClassifier(LinearClassifierMixin, _BaseRidge):
         Actual number of iterations for each target. Available only for
         sag and lsqr solvers. Other solvers will return None.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_breast_cancer
+    >>> from sklearn.linear_model import RidgeClassifier
+    >>> X, y = load_breast_cancer(return_X_y=True)
+    >>> clf = RidgeClassifier().fit(X, y)
+    >>> clf.score(X, y) # doctest: +ELLIPSIS
+    0.9595...
+
     See also
     --------
     Ridge : Ridge regression
@@ -1237,6 +1258,15 @@ class RidgeCV(_BaseRidgeCV, RegressorMixin):
     alpha_ : float
         Estimated regularization parameter.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_diabetes
+    >>> from sklearn.linear_model import RidgeCV
+    >>> X, y = load_diabetes(return_X_y=True)
+    >>> clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)
+    >>> clf.score(X, y) # doctest: +ELLIPSIS
+    0.5166...
+
     See also
     --------
     Ridge : Ridge regression
@@ -1327,6 +1357,15 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):
     alpha_ : float
         Estimated regularization parameter
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_breast_cancer
+    >>> from sklearn.linear_model import RidgeClassifierCV
+    >>> X, y = load_breast_cancer(return_X_y=True)
+    >>> clf = RidgeClassifierCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)
+    >>> clf.score(X, y) # doctest: +ELLIPSIS
+    0.9630...
+
     See also
     --------
     Ridge : Ridge regression
diff --git a/sklearn/linear_model/sag.py b/sklearn/linear_model/sag.py
index 39b817da1b0e..06a72d47b4c9 100644
--- a/sklearn/linear_model/sag.py
+++ b/sklearn/linear_model/sag.py
@@ -217,7 +217,7 @@ def sag_solver(X, y, sample_weight=None, loss='log', alpha=1., beta=0.,
     ... #doctest: +NORMALIZE_WHITESPACE
     LogisticRegression(C=1.0, class_weight=None, dual=False,
         fit_intercept=True, intercept_scaling=1, max_iter=100,
-        multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
+        multi_class='ovr', n_jobs=None, penalty='l2', random_state=None,
         solver='sag', tol=0.0001, verbose=0, warm_start=False)
 
     References
diff --git a/sklearn/linear_model/sgd_fast.pyx b/sklearn/linear_model/sgd_fast.pyx
index 384ad25673be..7724e6e305d5 100644
--- a/sklearn/linear_model/sgd_fast.pyx
+++ b/sklearn/linear_model/sgd_fast.pyx
@@ -36,8 +36,10 @@ DEF ELASTICNET = 3
 DEF CONSTANT = 1
 DEF OPTIMAL = 2
 DEF INVSCALING = 3
-DEF PA1 = 4
-DEF PA2 = 5
+DEF ADAPTIVE = 4
+DEF PA1 = 5
+DEF PA2 = 6
+
 
 
 # ----------------------------------------
@@ -337,6 +339,9 @@ def plain_sgd(np.ndarray[double, ndim=1, mode='c'] weights,
               double alpha, double C,
               double l1_ratio,
               SequentialDataset dataset,
+              np.ndarray[unsigned char, ndim=1, mode='c'] validation_mask,
+              bint early_stopping, estimator,
+              int n_iter_no_change,
               int max_iter, double tol, int fit_intercept,
               int verbose, bint shuffle, np.uint32_t seed,
               double weight_pos, double weight_neg,
@@ -365,10 +370,19 @@ def plain_sgd(np.ndarray[double, ndim=1, mode='c'] weights,
         l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.
     dataset : SequentialDataset
         A concrete ``SequentialDataset`` object.
+    validation_mask : ndarray[unsigned char, ndim=1]
+        Equal to True on the validation set.
+    early_stopping : boolean
+        Whether to use a stopping criterion based on the validation set.
+    estimator : BaseSGD
+        A concrete object inheriting from ``BaseSGD``.
+        Used only if early_stopping is True.
+    n_iter_no_change : int
+        Number of iteration with no improvement to wait before stopping.
     max_iter : int
         The maximum number of iterations (epochs).
     tol: double
-        The tolerance for the stopping criterion
+        The tolerance for the stopping criterion.
     fit_intercept : int
         Whether or not to fit the intercept (1 or 0).
     verbose : int
@@ -386,8 +400,9 @@ def plain_sgd(np.ndarray[double, ndim=1, mode='c'] weights,
         (1) constant, eta = eta0
         (2) optimal, eta = 1.0/(alpha * t).
         (3) inverse scaling, eta = eta0 / pow(t, power_t)
-        (4) Passive Aggressive-I, eta = min(alpha, loss/norm(x))
-        (5) Passive Aggressive-II, eta = 1.0 / (norm(x) + 0.5*alpha)
+        (4) adaptive decrease
+        (5) Passive Aggressive-I, eta = min(alpha, loss/norm(x))
+        (6) Passive Aggressive-II, eta = 1.0 / (norm(x) + 0.5*alpha)
     eta0 : double
         The initial learning rate.
     power_t : double
@@ -418,6 +433,10 @@ def plain_sgd(np.ndarray[double, ndim=1, mode='c'] weights,
                                    alpha, C,
                                    l1_ratio,
                                    dataset,
+                                   validation_mask,
+                                   early_stopping,
+                                   estimator,
+                                   n_iter_no_change,
                                    max_iter, tol, fit_intercept,
                                    verbose, shuffle, seed,
                                    weight_pos, weight_neg,
@@ -438,6 +457,9 @@ def average_sgd(np.ndarray[double, ndim=1, mode='c'] weights,
                 double alpha, double C,
                 double l1_ratio,
                 SequentialDataset dataset,
+                np.ndarray[unsigned char, ndim=1, mode='c'] validation_mask,
+                bint early_stopping, estimator,
+                int n_iter_no_change,
                 int max_iter, double tol, int fit_intercept,
                 int verbose, bint shuffle, np.uint32_t seed,
                 double weight_pos, double weight_neg,
@@ -471,6 +493,15 @@ def average_sgd(np.ndarray[double, ndim=1, mode='c'] weights,
         l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.
     dataset : SequentialDataset
         A concrete ``SequentialDataset`` object.
+    validation_mask : ndarray[unsigned char, ndim=1]
+        Equal to True on the validation set.
+    early_stopping : boolean
+        Whether to use a stopping criterion based on the validation set.
+    estimator : BaseSGD
+        A concrete object inheriting from ``BaseSGD``.
+        Used only if early_stopping is True.
+    n_iter_no_change : int
+        Number of iteration with no improvement to wait before stopping.
     max_iter : int
         The maximum number of iterations (epochs).
     tol: double
@@ -492,8 +523,9 @@ def average_sgd(np.ndarray[double, ndim=1, mode='c'] weights,
         (1) constant, eta = eta0
         (2) optimal, eta = 1.0/(alpha * t).
         (3) inverse scaling, eta = eta0 / pow(t, power_t)
-        (4) Passive Aggressive-I, eta = min(alpha, loss/norm(x))
-        (5) Passive Aggressive-II, eta = 1.0 / (norm(x) + 0.5*alpha)
+        (4) adaptive decrease
+        (5) Passive Aggressive-I, eta = min(alpha, loss/norm(x))
+        (6) Passive Aggressive-II, eta = 1.0 / (norm(x) + 0.5*alpha)
     eta0 : double
         The initial learning rate.
     power_t : double
@@ -528,6 +560,10 @@ def average_sgd(np.ndarray[double, ndim=1, mode='c'] weights,
                       alpha, C,
                       l1_ratio,
                       dataset,
+                      validation_mask,
+                      early_stopping,
+                      estimator,
+                      n_iter_no_change,
                       max_iter, tol, fit_intercept,
                       verbose, shuffle, seed,
                       weight_pos, weight_neg,
@@ -547,6 +583,9 @@ def _plain_sgd(np.ndarray[double, ndim=1, mode='c'] weights,
                double alpha, double C,
                double l1_ratio,
                SequentialDataset dataset,
+               np.ndarray[unsigned char, ndim=1, mode='c'] validation_mask,
+               bint early_stopping, estimator,
+               int n_iter_no_change,
                int max_iter, double tol, int fit_intercept,
                int verbose, bint shuffle, np.uint32_t seed,
                double weight_pos, double weight_neg,
@@ -567,13 +606,16 @@ def _plain_sgd(np.ndarray[double, ndim=1, mode='c'] weights,
     cdef double* ps_ptr = NULL
 
     # helper variables
+    cdef int no_improvement_count = 0
     cdef bint infinity = False
     cdef int xnnz
     cdef double eta = 0.0
     cdef double p = 0.0
     cdef double update = 0.0
     cdef double sumloss = 0.0
-    cdef double previous_loss = np.inf
+    cdef double score = 0.0
+    cdef double best_loss = INFINITY
+    cdef double best_score = -INFINITY
     cdef double y = 0.0
     cdef double sample_weight
     cdef double class_weight = 1.0
@@ -587,6 +629,9 @@ def _plain_sgd(np.ndarray[double, ndim=1, mode='c'] weights,
     cdef double max_change = 0.0
     cdef double max_weight = 0.0
 
+    cdef long long sample_index
+    cdef unsigned char [:] validation_mask_view = validation_mask
+
     # q vector is only used for L1 regularization
     cdef np.ndarray[double, ndim = 1, mode = "c"] q = None
     cdef double * q_data_ptr = NULL
@@ -622,13 +667,19 @@ def _plain_sgd(np.ndarray[double, ndim=1, mode='c'] weights,
                 dataset.next(&x_data_ptr, &x_ind_ptr, &xnnz,
                              &y, &sample_weight)
 
+                sample_index = dataset.index_data_ptr[dataset.current_index]
+                if validation_mask_view[sample_index]:
+                    # do not learn on the validation set
+                    continue
+
                 p = w.dot(x_data_ptr, x_ind_ptr, xnnz) + intercept
                 if learning_rate == OPTIMAL:
                     eta = 1.0 / (alpha * (optimal_init + t - 1))
                 elif learning_rate == INVSCALING:
                     eta = eta0 / pow(t, power_t)
 
-                sumloss += loss.loss(p, y)
+                if verbose or not early_stopping:
+                    sumloss += loss.loss(p, y)
 
                 if y > 0.0:
                     class_weight = weight_pos
@@ -705,13 +756,36 @@ def _plain_sgd(np.ndarray[double, ndim=1, mode='c'] weights,
                 infinity = True
                 break
 
-            if tol > -INFINITY and sumloss > previous_loss - tol * n_samples:
-                if verbose:
-                    with gil:
-                        print("Convergence after %d epochs took %.2f seconds"
-                              % (epoch + 1, time() - t_start))
-                break
-            previous_loss = sumloss
+            # evaluate the score on the validation set
+            if early_stopping:
+                with gil:
+                    score = estimator._validation_score(weights, intercept)
+                if tol > -INFINITY and score < best_score + tol:
+                    no_improvement_count += 1
+                else:
+                    no_improvement_count = 0
+                if score > best_score:
+                    best_score = score
+            # or evaluate the loss on the training set
+            else:
+                if tol > -INFINITY and sumloss > best_loss - tol * n_samples:
+                    no_improvement_count += 1
+                else:
+                    no_improvement_count = 0
+                if sumloss < best_loss:
+                    best_loss = sumloss
+
+            # if there is no improvement several times in a row
+            if no_improvement_count >= n_iter_no_change:
+                if learning_rate == ADAPTIVE and eta > 1e-6:
+                    eta = eta / 5
+                    no_improvement_count = 0
+                else:
+                    if verbose:
+                        with gil:
+                            print("Convergence after %d epochs took %.2f "
+                                  "seconds" % (epoch + 1, time() - t_start))
+                    break
 
     if infinity:
         raise ValueError(("Floating-point under-/overflow occurred at epoch"
diff --git a/sklearn/linear_model/stochastic_gradient.py b/sklearn/linear_model/stochastic_gradient.py
index 35551dfc39a9..93a2a6c91209 100644
--- a/sklearn/linear_model/stochastic_gradient.py
+++ b/sklearn/linear_model/stochastic_gradient.py
@@ -9,7 +9,7 @@
 
 from abc import ABCMeta, abstractmethod
 
-from ..externals.joblib import Parallel, delayed
+from ..utils import Parallel, delayed
 
 from .base import LinearClassifierMixin, SparseCoefMixin
 from .base import make_dataset
@@ -20,6 +20,7 @@
 from ..utils.validation import check_is_fitted
 from ..exceptions import ConvergenceWarning
 from ..externals import six
+from ..model_selection import train_test_split
 
 from .sgd_fast import plain_sgd, average_sgd
 from ..utils import compute_class_weight
@@ -33,9 +34,8 @@
 from .sgd_fast import EpsilonInsensitive
 from .sgd_fast import SquaredEpsilonInsensitive
 
-
 LEARNING_RATE_TYPES = {"constant": 1, "optimal": 2, "invscaling": 3,
-                       "pa1": 4, "pa2": 5}
+                       "adaptive": 4, "pa1": 5, "pa2": 6}
 
 PENALTY_TYPES = {"none": 0, "l2": 2, "l1": 1, "elasticnet": 3}
 
@@ -50,7 +50,9 @@ def __init__(self, loss, penalty='l2', alpha=0.0001, C=1.0,
                  l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None,
                  shuffle=True, verbose=0, epsilon=0.1, random_state=None,
                  learning_rate="optimal", eta0=0.0, power_t=0.5,
-                 warm_start=False, average=False, n_iter=None):
+                 early_stopping=False, validation_fraction=0.1,
+                 n_iter_no_change=5, warm_start=False, average=False,
+                 n_iter=None):
         self.loss = loss
         self.penalty = penalty
         self.learning_rate = learning_rate
@@ -64,6 +66,9 @@ def __init__(self, loss, penalty='l2', alpha=0.0001, C=1.0,
         self.verbose = verbose
         self.eta0 = eta0
         self.power_t = power_t
+        self.early_stopping = early_stopping
+        self.validation_fraction = validation_fraction
+        self.n_iter_no_change = n_iter_no_change
         self.warm_start = warm_start
         self.average = average
         self.n_iter = n_iter
@@ -86,13 +91,21 @@ def _validate_params(self, set_max_iter=True, for_partial_fit=False):
         """Validate input params. """
         if not isinstance(self.shuffle, bool):
             raise ValueError("shuffle must be either True or False")
+        if not isinstance(self.early_stopping, bool):
+            raise ValueError("early_stopping must be either True or False")
+        if self.early_stopping and for_partial_fit:
+            raise ValueError("early_stopping should be False with partial_fit")
         if self.max_iter is not None and self.max_iter <= 0:
             raise ValueError("max_iter must be > zero. Got %f" % self.max_iter)
         if not (0.0 <= self.l1_ratio <= 1.0):
             raise ValueError("l1_ratio must be in [0, 1]")
         if self.alpha < 0.0:
             raise ValueError("alpha must be >= 0")
-        if self.learning_rate in ("constant", "invscaling"):
+        if self.n_iter_no_change < 1:
+            raise ValueError("n_iter_no_change must be >= 1")
+        if not (0.0 < self.validation_fraction < 1.0):
+            raise ValueError("validation_fraction must be in ]0, 1[")
+        if self.learning_rate in ("constant", "invscaling", "adaptive"):
             if self.eta0 <= 0.0:
                 raise ValueError("eta0 must be > 0")
         if self.learning_rate == "optimal" and self.alpha == 0:
@@ -235,11 +248,77 @@ def _allocate_parameter_mem(self, n_classes, n_features, coef_init=None,
                                                dtype=np.float64,
                                                order="C")
 
+    def _make_validation_split(self, X, y, sample_weight):
+        """Split the dataset between training set and validation set.
+
+        Parameters
+        ----------
+        X : {array, sparse matrix}, shape (n_samples, n_features)
+            Training data.
+
+        y : array, shape (n_samples, )
+            Target values.
+
+        sample_weight : array, shape (n_samples, )
+            Weights applied to individual samples.
+
+        Returns
+        -------
+        validation_mask : array, shape (n_samples, )
+            Equal to 1 on the validation set, 0 on the training set.
+        """
+        n_samples = X.shape[0]
+        validation_mask = np.zeros(n_samples, dtype=np.uint8)
+        if not self.early_stopping:
+            # use the full set for training, with an empty validation set
+            return validation_mask
+
+        tmp = train_test_split(X, y, np.arange(n_samples), sample_weight,
+                               test_size=self.validation_fraction,
+                               random_state=self.random_state)
+        X_train, X_val, y_train, y_val = tmp[:4]
+        idx_train, idx_val, sample_weight_train, sample_weight_val = tmp[4:8]
+        if X_train.shape[0] == 0 or X_val.shape[0] == 0:
+            raise ValueError(
+                "Splitting %d samples into a train set and a validation set "
+                "with validation_fraction=%r led to an empty set (%d and %d "
+                "samples). Please either change validation_fraction, increase "
+                "number of samples, or disable early_stopping."
+                % (n_samples, self.validation_fraction, X_train.shape[0],
+                   X_val.shape[0]))
+
+        self._X_val = X_val
+        self._y_val = y_val
+        self._sample_weight_val = sample_weight_val
+        validation_mask[idx_val] = 1
+        return validation_mask
+
+    def _delete_validation_split(self):
+        if self.early_stopping:
+            del self._X_val
+            del self._y_val
+            del self._sample_weight_val
+
+    def _validation_score(self, coef, intercept):
+        """Compute the score on the validation set. Used for early stopping."""
+        # store attributes
+        old_coefs, old_intercept = self.coef_, self.intercept_
+
+        # replace them with current coefficients for scoring
+        self.coef_ = coef.reshape(1, -1)
+        self.intercept_ = np.atleast_1d(intercept)
+        score = self.score(self._X_val, self._y_val, self._sample_weight_val)
+
+        # restore old attributes
+        self.coef_, self.intercept_ = old_coefs, old_intercept
+
+        return score
+
 
 def _prepare_fit_binary(est, y, i):
     """Initialization for fit_binary.
 
-    Returns y, coef, intercept.
+    Returns y, coef, intercept, average_coef, average_intercept.
     """
     y_i = np.ones(y.shape, dtype=np.float64, order="C")
     y_i[y != est.classes_[i]] = -1.0
@@ -273,6 +352,42 @@ def fit_binary(est, i, X, y, alpha, C, learning_rate, max_iter,
     """Fit a single binary classifier.
 
     The i'th class is considered the "positive" class.
+
+    Parameters
+    ----------
+    est : Estimator object
+        The estimator to fit
+
+    i : int
+        Index of the positive class
+
+    X : numpy array or sparse matrix of shape [n_samples,n_features]
+        Training data
+
+    y : numpy array of shape [n_samples, ]
+        Target values
+
+    alpha : float
+        The regularization parameter
+
+    C : float
+        Maximum step size for passive aggressive
+
+    learning_rate : string
+        The learning rate. Accepted values are 'constant', 'optimal',
+        'invscaling', 'pa1' and 'pa2'.
+
+    max_iter : int
+        The maximum number of iterations (epochs)
+
+    pos_weight : float
+        The weight of the positive class
+
+    neg_weight : float
+        The weight of the negative class
+
+    sample_weight : numpy array of shape [n_samples, ]
+        The weight of each sample
     """
     # if average is not true, average_coef, and average_intercept will be
     # unused
@@ -284,6 +399,8 @@ def fit_binary(est, i, X, y, alpha, C, learning_rate, max_iter,
     penalty_type = est._get_penalty_type(est.penalty)
     learning_rate_type = est._get_learning_rate_type(learning_rate)
 
+    validation_mask = est._make_validation_split(X, y, sample_weight)
+
     # XXX should have random_state_!
     random_state = check_random_state(est.random_state)
     # numpy mtrand expects a C long which is a signed 32 bit integer under
@@ -293,20 +410,24 @@ def fit_binary(est, i, X, y, alpha, C, learning_rate, max_iter,
     tol = est.tol if est.tol is not None else -np.inf
 
     if not est.average:
-        return plain_sgd(coef, intercept, est.loss_function_,
-                         penalty_type, alpha, C, est.l1_ratio,
-                         dataset, max_iter, tol, int(est.fit_intercept),
-                         int(est.verbose), int(est.shuffle), seed,
-                         pos_weight, neg_weight,
-                         learning_rate_type, est.eta0,
-                         est.power_t, est.t_, intercept_decay)
+        result = plain_sgd(coef, intercept, est.loss_function_,
+                           penalty_type, alpha, C, est.l1_ratio,
+                           dataset, validation_mask, est.early_stopping, est,
+                           int(est.n_iter_no_change),
+                           max_iter, tol, int(est.fit_intercept),
+                           int(est.verbose), int(est.shuffle), seed,
+                           pos_weight, neg_weight,
+                           learning_rate_type, est.eta0,
+                           est.power_t, est.t_, intercept_decay)
 
     else:
         standard_coef, standard_intercept, average_coef, average_intercept, \
             n_iter_ = average_sgd(coef, intercept, average_coef,
                                   average_intercept, est.loss_function_,
                                   penalty_type, alpha, C, est.l1_ratio,
-                                  dataset, max_iter, tol,
+                                  dataset, validation_mask, est.early_stopping,
+                                  est, int(est.n_iter_no_change),
+                                  max_iter, tol,
                                   int(est.fit_intercept), int(est.verbose),
                                   int(est.shuffle), seed, pos_weight,
                                   neg_weight, learning_rate_type, est.eta0,
@@ -318,7 +439,10 @@ def fit_binary(est, i, X, y, alpha, C, learning_rate, max_iter,
         else:
             est.average_intercept_[i] = average_intercept
 
-        return standard_coef, standard_intercept, n_iter_
+        result = standard_coef, standard_intercept, n_iter_
+
+    est._delete_validation_split()
+    return result
 
 
 class BaseSGDClassifier(six.with_metaclass(ABCMeta, BaseSGD,
@@ -340,26 +464,24 @@ class BaseSGDClassifier(six.with_metaclass(ABCMeta, BaseSGD,
     @abstractmethod
     def __init__(self, loss="hinge", penalty='l2', alpha=0.0001,
                  l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None,
-                 shuffle=True, verbose=0, epsilon=DEFAULT_EPSILON, n_jobs=1,
+                 shuffle=True, verbose=0, epsilon=DEFAULT_EPSILON, n_jobs=None,
                  random_state=None, learning_rate="optimal", eta0=0.0,
-                 power_t=0.5, class_weight=None, warm_start=False,
-                 average=False, n_iter=None):
+                 power_t=0.5, early_stopping=False,
+                 validation_fraction=0.1, n_iter_no_change=5,
+                 class_weight=None, warm_start=False, average=False,
+                 n_iter=None):
 
-        super(BaseSGDClassifier, self).__init__(loss=loss, penalty=penalty,
-                                                alpha=alpha, l1_ratio=l1_ratio,
-                                                fit_intercept=fit_intercept,
-                                                max_iter=max_iter, tol=tol,
-                                                shuffle=shuffle,
-                                                verbose=verbose,
-                                                epsilon=epsilon,
-                                                random_state=random_state,
-                                                learning_rate=learning_rate,
-                                                eta0=eta0, power_t=power_t,
-                                                warm_start=warm_start,
-                                                average=average,
-                                                n_iter=n_iter)
+        super(BaseSGDClassifier, self).__init__(
+            loss=loss, penalty=penalty, alpha=alpha, l1_ratio=l1_ratio,
+            fit_intercept=fit_intercept, max_iter=max_iter, tol=tol,
+            shuffle=shuffle, verbose=verbose, epsilon=epsilon,
+            random_state=random_state, learning_rate=learning_rate, eta0=eta0,
+            power_t=power_t, early_stopping=early_stopping,
+            validation_fraction=validation_fraction,
+            n_iter_no_change=n_iter_no_change, warm_start=warm_start,
+            average=average, n_iter=n_iter)
         self.class_weight = class_weight
-        self.n_jobs = int(n_jobs)
+        self.n_jobs = n_jobs
 
     @property
     @deprecated("Attribute loss_function was deprecated in version 0.19 and "
@@ -491,7 +613,7 @@ def _fit_multiclass(self, X, y, alpha, C, learning_rate,
         strategy is called OVA: One Versus All.
         """
         # Use joblib to fit OvA in parallel.
-        result = Parallel(n_jobs=self.n_jobs, backend="threading",
+        result = Parallel(n_jobs=self.n_jobs, prefer="threads",
                           verbose=self.verbose)(
             delayed(fit_binary)(self, i, X, y, alpha, C, learning_rate,
                                 max_iter, self._expanded_class_weight[i],
@@ -684,10 +806,12 @@ class SGDClassifier(BaseSGDClassifier):
         For epsilon-insensitive, any differences between the current prediction
         and the correct label are ignored if they are less than this threshold.
 
-    n_jobs : integer, optional
+    n_jobs : int or None, optional (default=None)
         The number of CPUs to use to do the OVA (One Versus All, for
-        multi-class problems) computation. -1 means 'all CPUs'. Defaults
-        to 1.
+        multi-class problems) computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional (default=None)
         The seed of the pseudo random number generator to use when shuffling
@@ -699,20 +823,48 @@ class SGDClassifier(BaseSGDClassifier):
     learning_rate : string, optional
         The learning rate schedule:
 
-        - 'constant': eta = eta0
-        - 'optimal': eta = 1.0 / (alpha * (t + t0)) [default]
-        - 'invscaling': eta = eta0 / pow(t, power_t)
-
-        where t0 is chosen by a heuristic proposed by Leon Bottou.
+        'constant':
+            eta = eta0
+        'optimal': [default]
+            eta = 1.0 / (alpha * (t + t0))
+            where t0 is chosen by a heuristic proposed by Leon Bottou.
+        'invscaling':
+            eta = eta0 / pow(t, power_t)
+        'adaptive':
+            eta = eta0, as long as the training keeps decreasing.
+            Each time n_iter_no_change consecutive epochs fail to decrease the
+            training loss by tol or fail to increase validation score by tol if
+            early_stopping is True, the current learning rate is divided by 5.
 
     eta0 : double
-        The initial learning rate for the 'constant' or 'invscaling'
-        schedules. The default value is 0.0 as eta0 is not used by the
-        default schedule 'optimal'.
+        The initial learning rate for the 'constant', 'invscaling' or
+        'adaptive' schedules. The default value is 0.0 as eta0 is not used by
+        the default schedule 'optimal'.
 
     power_t : double
         The exponent for inverse scaling learning rate [default 0.5].
 
+    early_stopping : bool, default=False
+        Whether to use early stopping to terminate training when validation.
+        score is not improving. If set to True, it will automatically set aside
+        a fraction of training data as validation and terminate training when
+        validation score is not improving by at least tol for
+        n_iter_no_change consecutive epochs.
+
+        .. versionadded:: 0.20
+
+    validation_fraction : float, default=0.1
+        The proportion of training data to set aside as validation set for
+        early stopping. Must be between 0 and 1.
+        Only used if early_stopping is True.
+
+        .. versionadded:: 0.20
+
+    n_iter_no_change : int, default=5
+        Number of iterations with no improvement to wait before early stopping.
+
+        .. versionadded:: 0.20
+
     class_weight : dict, {class_label: weight} or "balanced" or None, optional
         Preset for the class_weight fit parameter.
 
@@ -771,14 +923,15 @@ class SGDClassifier(BaseSGDClassifier):
     >>> from sklearn import linear_model
     >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
     >>> Y = np.array([1, 1, 2, 2])
-    >>> clf = linear_model.SGDClassifier()
+    >>> clf = linear_model.SGDClassifier(max_iter=1000)
     >>> clf.fit(X, Y)
     ... #doctest: +NORMALIZE_WHITESPACE
-    SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,
-           eta0=0.0, fit_intercept=True, l1_ratio=0.15,
-           learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,
-           n_jobs=1, penalty='l2', power_t=0.5, random_state=None,
-           shuffle=True, tol=None, verbose=0, warm_start=False)
+    SGDClassifier(alpha=0.0001, average=False, class_weight=None,
+           early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,
+           l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=1000,
+           n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',
+           power_t=0.5, random_state=None, shuffle=True, tol=None,
+           validation_fraction=0.1, verbose=0, warm_start=False)
 
     >>> print(clf.predict([[-0.8, -1]]))
     [1]
@@ -791,17 +944,20 @@ class SGDClassifier(BaseSGDClassifier):
 
     def __init__(self, loss="hinge", penalty='l2', alpha=0.0001, l1_ratio=0.15,
                  fit_intercept=True, max_iter=None, tol=None, shuffle=True,
-                 verbose=0, epsilon=DEFAULT_EPSILON, n_jobs=1,
+                 verbose=0, epsilon=DEFAULT_EPSILON, n_jobs=None,
                  random_state=None, learning_rate="optimal", eta0=0.0,
-                 power_t=0.5, class_weight=None, warm_start=False,
+                 power_t=0.5, early_stopping=False, validation_fraction=0.1,
+                 n_iter_no_change=5, class_weight=None, warm_start=False,
                  average=False, n_iter=None):
         super(SGDClassifier, self).__init__(
             loss=loss, penalty=penalty, alpha=alpha, l1_ratio=l1_ratio,
             fit_intercept=fit_intercept, max_iter=max_iter, tol=tol,
             shuffle=shuffle, verbose=verbose, epsilon=epsilon, n_jobs=n_jobs,
             random_state=random_state, learning_rate=learning_rate, eta0=eta0,
-            power_t=power_t, class_weight=class_weight, warm_start=warm_start,
-            average=average, n_iter=n_iter)
+            power_t=power_t, early_stopping=early_stopping,
+            validation_fraction=validation_fraction,
+            n_iter_no_change=n_iter_no_change, class_weight=class_weight,
+            warm_start=warm_start, average=average, n_iter=n_iter)
 
     def _check_proba(self):
         if self.loss not in ("log", "modified_huber"):
@@ -934,20 +1090,18 @@ def __init__(self, loss="squared_loss", penalty="l2", alpha=0.0001,
                  l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None,
                  shuffle=True, verbose=0, epsilon=DEFAULT_EPSILON,
                  random_state=None, learning_rate="invscaling", eta0=0.01,
-                 power_t=0.25, warm_start=False, average=False, n_iter=None):
-        super(BaseSGDRegressor, self).__init__(loss=loss, penalty=penalty,
-                                               alpha=alpha, l1_ratio=l1_ratio,
-                                               fit_intercept=fit_intercept,
-                                               max_iter=max_iter, tol=tol,
-                                               shuffle=shuffle,
-                                               verbose=verbose,
-                                               epsilon=epsilon,
-                                               random_state=random_state,
-                                               learning_rate=learning_rate,
-                                               eta0=eta0, power_t=power_t,
-                                               warm_start=warm_start,
-                                               average=average,
-                                               n_iter=n_iter)
+                 power_t=0.25, early_stopping=False, validation_fraction=0.1,
+                 n_iter_no_change=5, warm_start=False, average=False,
+                 n_iter=None):
+        super(BaseSGDRegressor, self).__init__(
+            loss=loss, penalty=penalty, alpha=alpha, l1_ratio=l1_ratio,
+            fit_intercept=fit_intercept, max_iter=max_iter, tol=tol,
+            shuffle=shuffle, verbose=verbose, epsilon=epsilon,
+            random_state=random_state, learning_rate=learning_rate, eta0=eta0,
+            power_t=power_t, early_stopping=early_stopping,
+            validation_fraction=validation_fraction,
+            n_iter_no_change=n_iter_no_change, warm_start=warm_start,
+            average=average, n_iter=n_iter)
 
     def _partial_fit(self, X, y, alpha, C, loss, learning_rate,
                      max_iter, sample_weight, coef_init, intercept_init):
@@ -1115,6 +1269,8 @@ def _fit_regressor(self, X, y, alpha, C, loss, learning_rate,
         if not hasattr(self, "t_"):
             self.t_ = 1.0
 
+        validation_mask = self._make_validation_split(X, y, sample_weight)
+
         random_state = check_random_state(self.random_state)
         # numpy mtrand expects a C long which is a signed 32 bit integer under
         # Windows
@@ -1134,6 +1290,8 @@ def _fit_regressor(self, X, y, alpha, C, loss, learning_rate,
                             alpha, C,
                             self.l1_ratio,
                             dataset,
+                            validation_mask, self.early_stopping, self,
+                            int(self.n_iter_no_change),
                             max_iter, tol,
                             int(self.fit_intercept),
                             int(self.verbose),
@@ -1164,6 +1322,8 @@ def _fit_regressor(self, X, y, alpha, C, loss, learning_rate,
                           alpha, C,
                           self.l1_ratio,
                           dataset,
+                          validation_mask, self.early_stopping, self,
+                          int(self.n_iter_no_change),
                           max_iter, tol,
                           int(self.fit_intercept),
                           int(self.verbose),
@@ -1177,6 +1337,8 @@ def _fit_regressor(self, X, y, alpha, C, loss, learning_rate,
             self.t_ += self.n_iter_ * X.shape[0]
             self.intercept_ = np.atleast_1d(self.intercept_)
 
+        self._delete_validation_split()
+
 
 class SGDRegressor(BaseSGDRegressor):
     """Linear model fitted by minimizing a regularized empirical loss with SGD
@@ -1270,17 +1432,47 @@ class SGDRegressor(BaseSGDRegressor):
     learning_rate : string, optional
         The learning rate schedule:
 
-        - 'constant': eta = eta0
-        - 'optimal': eta = 1.0 / (alpha * (t + t0))
-        - 'invscaling': eta = eta0 / pow(t, power_t) [default]
+        'constant':
+            eta = eta0
+        'optimal':
+            eta = 1.0 / (alpha * (t + t0))
+            where t0 is chosen by a heuristic proposed by Leon Bottou.
+        'invscaling': [default]
+            eta = eta0 / pow(t, power_t)
+        'adaptive':
+            eta = eta0, as long as the training keeps decreasing.
+            Each time n_iter_no_change consecutive epochs fail to decrease the
+            training loss by tol or fail to increase validation score by tol if
+            early_stopping is True, the current learning rate is divided by 5.
+
+    eta0 : double
+        The initial learning rate for the 'constant', 'invscaling' or
+        'adaptive' schedules. The default value is 0.0 as eta0 is not used by
+        the default schedule 'optimal'.
+
+    power_t : double
+        The exponent for inverse scaling learning rate [default 0.5].
+
+    early_stopping : bool, default=False
+        Whether to use early stopping to terminate training when validation.
+        score is not improving. If set to True, it will automatically set aside
+        a fraction of training data as validation and terminate training when
+        validation score is not improving by at least tol for
+        n_iter_no_change consecutive epochs.
+
+        .. versionadded:: 0.20
 
-        where t0 is chosen by a heuristic proposed by Leon Bottou.
+    validation_fraction : float, default=0.1
+        The proportion of training data to set aside as validation set for
+        early stopping. Must be between 0 and 1.
+        Only used if early_stopping is True.
 
-    eta0 : double, optional
-        The initial learning rate [default 0.01].
+        .. versionadded:: 0.20
 
-    power_t : double, optional
-        The exponent for inverse scaling learning rate [default 0.25].
+    n_iter_no_change : int, default=5
+        Number of iterations with no improvement to wait before early stopping.
+
+        .. versionadded:: 0.20
 
     warm_start : bool, optional
         When set to True, reuse the solution of the previous call to fit as
@@ -1334,16 +1526,16 @@ class SGDRegressor(BaseSGDRegressor):
     >>> np.random.seed(0)
     >>> y = np.random.randn(n_samples)
     >>> X = np.random.randn(n_samples, n_features)
-    >>> clf = linear_model.SGDRegressor()
+    >>> clf = linear_model.SGDRegressor(max_iter=1000)
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
-    SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,
-           fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',
-           loss='squared_loss', max_iter=None, n_iter=None, penalty='l2',
-           power_t=0.25, random_state=None, shuffle=True, tol=None,
+    SGDRegressor(alpha=0.0001, average=False, early_stopping=False,
+           epsilon=0.1, eta0=0.01, fit_intercept=True, l1_ratio=0.15,
+           learning_rate='invscaling', loss='squared_loss', max_iter=1000,
+           n_iter=None, n_iter_no_change=5, penalty='l2', power_t=0.25,
+           random_state=None, shuffle=True, tol=None, validation_fraction=0.1,
            verbose=0, warm_start=False)
 
-
     See also
     --------
     Ridge, ElasticNet, Lasso, sklearn.svm.SVR
@@ -1353,16 +1545,15 @@ def __init__(self, loss="squared_loss", penalty="l2", alpha=0.0001,
                  l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None,
                  shuffle=True, verbose=0, epsilon=DEFAULT_EPSILON,
                  random_state=None, learning_rate="invscaling", eta0=0.01,
-                 power_t=0.25, warm_start=False, average=False, n_iter=None):
-        super(SGDRegressor, self).__init__(loss=loss, penalty=penalty,
-                                           alpha=alpha, l1_ratio=l1_ratio,
-                                           fit_intercept=fit_intercept,
-                                           max_iter=max_iter, tol=tol,
-                                           shuffle=shuffle,
-                                           verbose=verbose,
-                                           epsilon=epsilon,
-                                           random_state=random_state,
-                                           learning_rate=learning_rate,
-                                           eta0=eta0, power_t=power_t,
-                                           warm_start=warm_start,
-                                           average=average, n_iter=n_iter)
+                 power_t=0.25, early_stopping=False, validation_fraction=0.1,
+                 n_iter_no_change=5, warm_start=False, average=False,
+                 n_iter=None):
+        super(SGDRegressor, self).__init__(
+            loss=loss, penalty=penalty, alpha=alpha, l1_ratio=l1_ratio,
+            fit_intercept=fit_intercept, max_iter=max_iter, tol=tol,
+            shuffle=shuffle, verbose=verbose, epsilon=epsilon,
+            random_state=random_state, learning_rate=learning_rate, eta0=eta0,
+            power_t=power_t, early_stopping=early_stopping,
+            validation_fraction=validation_fraction,
+            n_iter_no_change=n_iter_no_change, warm_start=warm_start,
+            average=average, n_iter=n_iter)
diff --git a/sklearn/linear_model/tests/test_base.py b/sklearn/linear_model/tests/test_base.py
index 30e4cfdcced4..bcabe12ed35f 100644
--- a/sklearn/linear_model/tests/test_base.py
+++ b/sklearn/linear_model/tests/test_base.py
@@ -3,6 +3,8 @@
 #
 # License: BSD 3 clause
 
+import pytest
+
 import numpy as np
 from scipy import sparse
 from scipy import linalg
@@ -321,6 +323,28 @@ def test_csr_preprocess_data():
     assert_equal(csr_.getformat(), 'csr')
 
 
+@pytest.mark.parametrize('is_sparse', (True, False))
+@pytest.mark.parametrize('to_copy', (True, False))
+def test_preprocess_copy_data_no_checks(is_sparse, to_copy):
+    X, y = make_regression()
+    X[X < 2.5] = 0.0
+
+    if is_sparse:
+        X = sparse.csr_matrix(X)
+
+    X_, y_, _, _, _ = _preprocess_data(X, y, True,
+                                       copy=to_copy, check_input=False)
+
+    if to_copy and is_sparse:
+        assert not np.may_share_memory(X_.data, X.data)
+    elif to_copy:
+        assert not np.may_share_memory(X_, X)
+    elif is_sparse:
+        assert np.may_share_memory(X_.data, X.data)
+    else:
+        assert np.may_share_memory(X_, X)
+
+
 def test_dtype_preprocess_data():
     n_samples = 200
     n_features = 2
diff --git a/sklearn/linear_model/tests/test_bayes.py b/sklearn/linear_model/tests/test_bayes.py
index 443f856fa728..61ac7395cb52 100644
--- a/sklearn/linear_model/tests/test_bayes.py
+++ b/sklearn/linear_model/tests/test_bayes.py
@@ -86,8 +86,10 @@ def test_prediction_bayesian_ridge_ard_with_constant_input():
     random_state = check_random_state(42)
     constant_value = random_state.rand()
     X = random_state.random_sample((n_samples, n_features))
-    y = np.full(n_samples, constant_value)
-    expected = np.full(n_samples, constant_value)
+    y = np.full(n_samples, constant_value,
+                dtype=np.array(constant_value).dtype)
+    expected = np.full(n_samples, constant_value,
+                       dtype=np.array(constant_value).dtype)
 
     for clf in [BayesianRidge(), ARDRegression()]:
         y_pred = clf.fit(X, y).predict(X)
@@ -103,7 +105,8 @@ def test_std_bayesian_ridge_ard_with_constant_input():
     random_state = check_random_state(42)
     constant_value = random_state.rand()
     X = random_state.random_sample((n_samples, n_features))
-    y = np.full(n_samples, constant_value)
+    y = np.full(n_samples, constant_value,
+                dtype=np.array(constant_value).dtype)
     expected_upper_boundary = 0.01
 
     for clf in [BayesianRidge(), ARDRegression()]:
diff --git a/sklearn/linear_model/tests/test_coordinate_descent.py b/sklearn/linear_model/tests/test_coordinate_descent.py
index fb65d800e78b..834d685f5b23 100644
--- a/sklearn/linear_model/tests/test_coordinate_descent.py
+++ b/sklearn/linear_model/tests/test_coordinate_descent.py
@@ -146,6 +146,7 @@ def build_dataset(n_samples=50, n_features=200, n_informative_features=10,
     return X, y, X_test, y_test
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_lasso_cv():
     X, y, X_test, y_test = build_dataset()
     max_iter = 150
@@ -232,6 +233,7 @@ def test_lasso_path_return_models_vs_new_return_gives_same_coefficients():
         decimal=1)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_enet_path():
     # We use a large number of samples and of informative features so that
     # the l1_ratio selected is more toward ridge than lasso
@@ -289,6 +291,7 @@ def test_enet_path():
     assert_almost_equal(clf1.alpha_, clf2.alpha_)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_path_parameters():
     X, y, _, _ = build_dataset()
     max_iter = 100
@@ -360,6 +363,7 @@ def test_enet_cv_positive_constraint():
     assert_true(min(enetcv_constrained.coef_) >= 0)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_uniform_targets():
     enet = ElasticNetCV(fit_intercept=True, n_alphas=3)
     m_enet = MultiTaskElasticNetCV(fit_intercept=True, n_alphas=3)
@@ -454,6 +458,7 @@ def test_multioutput_enetcv_error():
     assert_raises(ValueError, clf.fit, X, y)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_multitask_enet_and_lasso_cv():
     X, y, _, _ = build_dataset(n_features=50, n_targets=3)
     clf = MultiTaskElasticNetCV().fit(X, y)
@@ -480,6 +485,7 @@ def test_multitask_enet_and_lasso_cv():
     assert_equal(10, len(clf.alphas_))
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_1d_multioutput_enet_and_multitask_enet_cv():
     X, y, _, _ = build_dataset(n_features=10)
     y = y[:, np.newaxis]
@@ -493,6 +499,7 @@ def test_1d_multioutput_enet_and_multitask_enet_cv():
     assert_almost_equal(clf.intercept_, clf1.intercept_[0])
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_1d_multioutput_lasso_and_multitask_lasso_cv():
     X, y, _, _ = build_dataset(n_features=10)
     y = y[:, np.newaxis]
@@ -505,6 +512,7 @@ def test_1d_multioutput_lasso_and_multitask_lasso_cv():
     assert_almost_equal(clf.intercept_, clf1.intercept_[0])
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_sparse_input_dtype_enet_and_lassocv():
     X, y, _, _ = build_dataset(n_features=10)
     clf = ElasticNetCV(n_alphas=5)
@@ -522,6 +530,7 @@ def test_sparse_input_dtype_enet_and_lassocv():
     assert_almost_equal(clf.coef_, clf1.coef_, decimal=6)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_precompute_invalid_argument():
     X, y, _, _ = build_dataset()
     for clf in [ElasticNetCV(precompute="invalid"),
diff --git a/sklearn/linear_model/tests/test_least_angle.py b/sklearn/linear_model/tests/test_least_angle.py
index 630559fe4fef..9c9a883f9638 100644
--- a/sklearn/linear_model/tests/test_least_angle.py
+++ b/sklearn/linear_model/tests/test_least_angle.py
@@ -1,5 +1,7 @@
 import warnings
 
+from distutils.version import LooseVersion
+
 import numpy as np
 from scipy import linalg
 
@@ -78,7 +80,6 @@ def test_simple_precomputed():
 
 def test_all_precomputed():
     # Test that lars_path with precomputed Gram and Xy gives the right answer
-    X, y = diabetes.data, diabetes.target
     G = np.dot(X.T, X)
     Xy = np.dot(X.T, y)
     for method in 'lar', 'lasso':
@@ -88,16 +89,22 @@ def test_all_precomputed():
             assert_array_almost_equal(expected, got)
 
 
+@pytest.mark.filterwarnings('ignore: `rcond` parameter will change')
+# numpy deprecation
 def test_lars_lstsq():
     # Test that Lars gives least square solution at the end
     # of the path
     X1 = 3 * diabetes.data  # use un-normalized dataset
     clf = linear_model.LassoLars(alpha=0.)
     clf.fit(X1, y)
-    coef_lstsq = np.linalg.lstsq(X1, y)[0]
+    # Avoid FutureWarning about default value change when numpy >= 1.14
+    rcond = None if LooseVersion(np.__version__) >= '1.14' else -1
+    coef_lstsq = np.linalg.lstsq(X1, y, rcond=rcond)[0]
     assert_array_almost_equal(clf.coef_, coef_lstsq)
 
 
+@pytest.mark.filterwarnings('ignore:`rcond` parameter will change')
+# numpy deprecation
 def test_lasso_gives_lstsq_solution():
     # Test that Lars Lasso gives least square solution at the end
     # of the path
@@ -174,12 +181,12 @@ def test_no_path_all_precomputed():
     assert_true(alpha_ == alphas_[-1])
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 @pytest.mark.parametrize(
         'classifier',
         [linear_model.Lars, linear_model.LarsCV, linear_model.LassoLarsIC])
 def test_lars_precompute(classifier):
     # Check for different values of precompute
-    X, y = diabetes.data, diabetes.target
     G = np.dot(X.T, X)
 
     clf = classifier(precompute=G)
@@ -410,6 +417,7 @@ def test_multitarget():
             assert_array_almost_equal(Y_pred[:, k], y_pred)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_lars_cv():
     # Test the LassoLarsCV object by checking that the optimal alpha
     # increases as the number of samples increases.
@@ -426,6 +434,7 @@ def test_lars_cv():
     assert_false(hasattr(lars_cv, 'n_nonzero_coefs'))
 
 
+@pytest.mark.filterwarnings('ignore::FutureWarning')
 def test_lars_cv_max_iter():
     with warnings.catch_warnings(record=True) as w:
         X = diabetes.data
@@ -473,6 +482,7 @@ def test_lars_path_readonly_data():
         _lars_path_residues(X_train, y_train, X_test, y_test, copy=False)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
 def test_lars_path_positive_constraint():
     # this is the main test for the positive parameter on the lars_path method
     # the estimator classes just make use of this function
@@ -487,12 +497,10 @@ def test_lars_path_positive_constraint():
     # assert_raises(ValueError, linear_model.lars_path, diabetes['data'],
     #               diabetes['target'], method='lar', positive=True)
 
-    with warnings.catch_warnings(record=True) as w:
+    with pytest.warns(DeprecationWarning, match="broken"):
         linear_model.lars_path(diabetes['data'], diabetes['target'],
                                return_path=True, method='lar',
                                positive=True)
-    assert_true(len(w) == 1)
-    assert "broken" in str(w[0].message)
 
     method = 'lasso'
     alpha, active, coefs = \
@@ -517,6 +525,7 @@ def test_lars_path_positive_constraint():
                            'LassoLarsIC': {}}
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_estimatorclasses_positive_constraint():
     # testing the transmissibility for the positive option of all estimator
     # classes in this same function here
diff --git a/sklearn/linear_model/tests/test_logistic.py b/sklearn/linear_model/tests/test_logistic.py
index 56be87f71015..9fc6d6ec5f04 100644
--- a/sklearn/linear_model/tests/test_logistic.py
+++ b/sklearn/linear_model/tests/test_logistic.py
@@ -6,6 +6,7 @@
 
 from sklearn.datasets import load_iris, make_classification
 from sklearn.metrics import log_loss
+from sklearn.metrics.scorer import get_scorer
 from sklearn.model_selection import StratifiedKFold
 from sklearn.preprocessing import LabelEncoder
 from sklearn.utils import compute_class_weight
@@ -29,7 +30,7 @@
     logistic_regression_path, LogisticRegressionCV,
     _logistic_loss_and_grad, _logistic_grad_hess,
     _multinomial_grad_hess, _logistic_loss,
-)
+    _log_reg_scoring_path)
 
 X = [[-1, 0], [0, 1], [1, 1]]
 X_sp = sp.csr_matrix(X)
@@ -428,7 +429,7 @@ def test_logistic_grad_hess():
     X_sp[X_sp < .1] = 0
     X_sp = sp.csr_matrix(X_sp)
     for X in (X_ref, X_sp):
-        w = .1 * np.ones(n_features)
+        w = np.full(n_features, .1)
 
         # First check that _logistic_grad_hess is consistent
         # with _logistic_loss_and_grad
@@ -466,6 +467,7 @@ def test_logistic_grad_hess():
         assert_array_almost_equal(grad_interp, grad_interp_2)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_logistic_cv():
     # test for LogisticRegressionCV object
     n_samples, n_features = 50, 5
@@ -492,6 +494,41 @@ def test_logistic_cv():
     assert_array_equal(scores.shape, (1, 3, 1))
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
+@pytest.mark.parametrize('scoring, multiclass_agg_list',
+                         [('accuracy', ['']),
+                          ('precision', ['_macro', '_weighted']),
+                          # no need to test for micro averaging because it
+                          # is the same as accuracy for f1, precision,
+                          # and recall (see https://github.com/
+                          # scikit-learn/scikit-learn/pull/
+                          # 11578#discussion_r203250062)
+                          ('f1', ['_macro', '_weighted']),
+                          ('neg_log_loss', ['']),
+                          ('recall', ['_macro', '_weighted'])])
+def test_logistic_cv_multinomial_score(scoring, multiclass_agg_list):
+    # test that LogisticRegressionCV uses the right score to compute its
+    # cross-validation scores when using a multinomial scoring
+    # see https://github.com/scikit-learn/scikit-learn/issues/8720
+    X, y = make_classification(n_samples=100, random_state=0, n_classes=3,
+                               n_informative=6)
+    train, test = np.arange(80), np.arange(80, 100)
+    lr = LogisticRegression(C=1., solver='lbfgs', multi_class='multinomial')
+    # we use lbfgs to support multinomial
+    params = lr.get_params()
+    # we store the params to set them further in _log_reg_scoring_path
+    for key in ['C', 'n_jobs', 'warm_start']:
+        del params[key]
+    lr.fit(X[train], y[train])
+    for averaging in multiclass_agg_list:
+        scorer = get_scorer(scoring + averaging)
+        assert_array_almost_equal(
+            _log_reg_scoring_path(X, y, train, test, Cs=[1.],
+                                  scoring=scorer, **params)[2][0],
+            scorer(lr, X[test], y[test]))
+
+
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_multinomial_logistic_regression_string_inputs():
     # Test with string labels for LogisticRegression(CV)
     n_samples, n_features, n_classes = 50, 5, 3
@@ -531,6 +568,7 @@ def test_multinomial_logistic_regression_string_inputs():
     assert_equal(sorted(np.unique(lr_cv_str.predict(X_ref))), ['bar', 'baz'])
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_logistic_cv_sparse():
     X, y = make_classification(n_samples=50, n_features=5,
                                random_state=0)
@@ -694,6 +732,7 @@ def test_logistic_regression_solvers_multiclass():
     assert_array_almost_equal(saga.coef_, lib.coef_, decimal=4)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_logistic_regressioncv_class_weights():
     for weight in [{0: 0.1, 1: 0.2}, {0: 0.1, 1: 0.2, 2: 0.5}]:
         n_classes = len(weight)
@@ -733,6 +772,7 @@ def test_logistic_regressioncv_class_weights():
             assert_array_almost_equal(clf_saga.coef_, clf_lbf.coef_, decimal=4)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_logistic_regression_sample_weights():
     X, y = make_classification(n_samples=20, n_features=5, n_informative=3,
                                n_classes=2, random_state=0)
@@ -848,6 +888,7 @@ def test_logistic_regression_class_weights():
         assert_array_almost_equal(clf1.coef_, clf2.coef_, decimal=6)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_logistic_regression_multinomial():
     # Tests for the multinomial option in logistic regression
 
@@ -941,6 +982,7 @@ def test_liblinear_decision_function_zero():
     assert_array_equal(clf.predict(X), np.zeros(5))
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_liblinear_logregcv_sparse():
     # Test LogRegCV with solver='liblinear' works for sparse matrices
 
@@ -949,6 +991,7 @@ def test_liblinear_logregcv_sparse():
     clf.fit(sparse.csr_matrix(X), y)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_saga_sparse():
     # Test LogRegCV with solver='liblinear' works for sparse matrices
 
@@ -1041,6 +1084,7 @@ def test_logreg_l1_sparse_data():
     assert_array_almost_equal(lr_saga.coef_, lr_saga_dense.coef_)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_logreg_cv_penalty():
     # Test that the correct penalty is passed to the final fit.
     X, y = make_classification(n_samples=50, n_features=20, random_state=0)
@@ -1224,18 +1268,21 @@ def test_dtype_match():
         for multi_class in ['ovr', 'multinomial']:
 
             # Check type consistency
-            lr_32 = LogisticRegression(solver=solver, multi_class=multi_class)
+            lr_32 = LogisticRegression(solver=solver, multi_class=multi_class,
+                                       random_state=42)
             lr_32.fit(X_32, y_32)
             assert_equal(lr_32.coef_.dtype, X_32.dtype)
 
             # check consistency with sparsity
             lr_32_sparse = LogisticRegression(solver=solver,
-                                              multi_class=multi_class)
+                                              multi_class=multi_class,
+                                              random_state=42)
             lr_32_sparse.fit(X_sparse_32, y_32)
             assert_equal(lr_32_sparse.coef_.dtype, X_sparse_32.dtype)
 
             # Check accuracy consistency
-            lr_64 = LogisticRegression(solver=solver, multi_class=multi_class)
+            lr_64 = LogisticRegression(solver=solver, multi_class=multi_class,
+                                       random_state=42)
             lr_64.fit(X_64, y_64)
             assert_equal(lr_64.coef_.dtype, X_64.dtype)
             assert_almost_equal(lr_32.coef_, lr_64.coef_.astype(np.float32))
@@ -1260,3 +1307,23 @@ def test_warm_start_converge_LR():
         lr_ws.fit(X, y)
     lr_ws_loss = log_loss(y, lr_ws.predict_proba(X))
     assert_allclose(lr_no_ws_loss, lr_ws_loss, rtol=1e-5)
+
+
+def test_logistic_regression_path_coefs_multinomial():
+    # Make sure that the returned coefs by logistic_regression_path when
+    # multi_class='multinomial' don't override each other (used to be a
+    # bug).
+    X, y = make_classification(n_samples=200, n_classes=3, n_informative=2,
+                               n_redundant=0, n_clusters_per_class=1,
+                               random_state=0, n_features=2)
+    Cs = [.00001, 1, 10000]
+    coefs, _, _ = logistic_regression_path(X, y, penalty='l1', Cs=Cs,
+                                           solver='saga', random_state=0,
+                                           multi_class='multinomial')
+
+    with pytest.raises(AssertionError):
+        assert_array_almost_equal(coefs[0], coefs[1], decimal=1)
+    with pytest.raises(AssertionError):
+        assert_array_almost_equal(coefs[0], coefs[2], decimal=1)
+    with pytest.raises(AssertionError):
+        assert_array_almost_equal(coefs[1], coefs[2], decimal=1)
diff --git a/sklearn/linear_model/tests/test_randomized_l1.py b/sklearn/linear_model/tests/test_randomized_l1.py
index c783bfc7d493..564fbd4e7827 100644
--- a/sklearn/linear_model/tests/test_randomized_l1.py
+++ b/sklearn/linear_model/tests/test_randomized_l1.py
@@ -1,5 +1,6 @@
 # Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>
 # License: BSD 3 clause
+
 from tempfile import mkdtemp
 import shutil
 
@@ -15,8 +16,8 @@
 from sklearn.utils.testing import assert_warns_message
 
 from sklearn.linear_model.randomized_l1 import(lasso_stability_path,
-                                                RandomizedLasso,
-                                                RandomizedLogisticRegression)
+                                               RandomizedLasso,
+                                               RandomizedLogisticRegression)
 
 from sklearn.datasets import load_diabetes, load_iris
 from sklearn.feature_selection import f_regression, f_classif
@@ -56,7 +57,7 @@ def test_randomized_lasso_error_memory():
                           selection_threshold=selection_threshold,
                           memory=tempdir)
     assert_raises_regex(ValueError, "'memory' should either be a string or"
-                        " a sklearn.externals.joblib.Memory instance",
+                        " a sklearn.utils.Memory instance",
                         clf.fit, X, y)
 
 
@@ -111,6 +112,7 @@ def test_randomized_lasso():
     assert_raises(ValueError, clf.fit, X, y)
 
 
+@ignore_warnings(category=DeprecationWarning)
 def test_randomized_lasso_precompute():
     # Check randomized lasso for different values of precompute
     n_resampling = 20
diff --git a/sklearn/linear_model/tests/test_ridge.py b/sklearn/linear_model/tests/test_ridge.py
index 2f574b88ba7b..d42e0f874300 100644
--- a/sklearn/linear_model/tests/test_ridge.py
+++ b/sklearn/linear_model/tests/test_ridge.py
@@ -172,7 +172,8 @@ def test_ridge_sample_weights():
         for (alpha, intercept, solver) in param_grid:
 
             # Ridge with explicit sample_weight
-            est = Ridge(alpha=alpha, fit_intercept=intercept, solver=solver)
+            est = Ridge(alpha=alpha, fit_intercept=intercept,
+                        solver=solver, tol=1e-6)
             est.fit(X, y, sample_weight=sample_weight)
             coefs = est.coef_
             inter = est.intercept_
@@ -488,6 +489,8 @@ def check_dense_sparse(test_func):
         assert_array_almost_equal(ret_dense, ret_sparse, decimal=3)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 @pytest.mark.parametrize(
         'test_func',
         (_test_ridge_loo, _test_ridge_cv, _test_ridge_cv_normalize,
@@ -546,6 +549,7 @@ def test_class_weights():
     assert_array_almost_equal(reg.intercept_, rega.intercept_)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 @pytest.mark.parametrize('reg', (RidgeClassifier, RidgeClassifierCV))
 def test_class_weight_vs_sample_weight(reg):
     """Check class_weights resemble sample_weights behavior."""
@@ -575,6 +579,7 @@ def test_class_weight_vs_sample_weight(reg):
     assert_almost_equal(reg1.coef_, reg2.coef_)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_class_weights_cv():
     # Test class weights for cross validated ridge classifier.
     X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],
@@ -591,6 +596,7 @@ def test_class_weights_cv():
     assert_array_equal(reg.predict([[-.2, 2]]), np.array([-1]))
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_ridgecv_store_cv_values():
     rng = np.random.RandomState(42)
 
@@ -600,7 +606,7 @@ def test_ridgecv_store_cv_values():
     alphas = [1e-1, 1e0, 1e1]
     n_alphas = len(alphas)
 
-    r = RidgeCV(alphas=alphas, store_cv_values=True)
+    r = RidgeCV(alphas=alphas, cv=None, store_cv_values=True)
 
     # with len(y.shape) == 1
     y = rng.randn(n_samples)
@@ -614,6 +620,7 @@ def test_ridgecv_store_cv_values():
     assert r.cv_values_.shape == (n_samples, n_targets, n_alphas)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_ridge_classifier_cv_store_cv_values():
     x = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],
                   [1.0, 1.0], [1.0, 0.0]])
@@ -623,7 +630,7 @@ def test_ridge_classifier_cv_store_cv_values():
     alphas = [1e-1, 1e0, 1e1]
     n_alphas = len(alphas)
 
-    r = RidgeClassifierCV(alphas=alphas, store_cv_values=True)
+    r = RidgeClassifierCV(alphas=alphas, cv=None, store_cv_values=True)
 
     # with len(y.shape) == 1
     n_targets = 1
@@ -639,6 +646,7 @@ def test_ridge_classifier_cv_store_cv_values():
     assert r.cv_values_.shape == (n_samples, n_targets, n_alphas)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
 def test_ridgecv_sample_weight():
     rng = np.random.RandomState(0)
     alphas = (0.1, 1.0, 10.0)
@@ -733,6 +741,7 @@ def test_sparse_design_with_sample_weights():
                                       decimal=6)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_ridgecv_int_alphas():
     X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],
                   [1.0, 1.0], [1.0, 0.0]])
@@ -743,6 +752,7 @@ def test_ridgecv_int_alphas():
     ridge.fit(X, y)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_ridgecv_negative_alphas():
     X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],
                   [1.0, 1.0], [1.0, 0.0]])
diff --git a/sklearn/linear_model/tests/test_sag.py b/sklearn/linear_model/tests/test_sag.py
index 02a557d56ef7..ca99a81a7396 100644
--- a/sklearn/linear_model/tests/test_sag.py
+++ b/sklearn/linear_model/tests/test_sag.py
@@ -4,6 +4,7 @@
 # License: BSD 3 clause
 
 import math
+import pytest
 import numpy as np
 import scipy.sparse as sp
 
@@ -17,9 +18,9 @@
 from sklearn.utils.extmath import row_norms
 from sklearn.utils.testing import assert_almost_equal
 from sklearn.utils.testing import assert_array_almost_equal
+from sklearn.utils.testing import assert_allclose
 from sklearn.utils.testing import assert_greater
 from sklearn.utils.testing import assert_raise_message
-from sklearn.utils.testing import ignore_warnings
 from sklearn.utils import compute_class_weight
 from sklearn.utils import check_random_state
 from sklearn.preprocessing import LabelEncoder, LabelBinarizer
@@ -230,7 +231,6 @@ def get_step_size(X, alpha, fit_intercept, classification=True):
         return 1.0 / (np.max(np.sum(X * X, axis=1)) + fit_intercept + alpha)
 
 
-@ignore_warnings
 def test_classifier_matching():
     n_samples = 20
     X, y = make_blobs(n_samples=n_samples, centers=2, random_state=0,
@@ -269,7 +269,6 @@ def test_classifier_matching():
         assert_array_almost_equal(intercept2, clf.intercept_, decimal=9)
 
 
-@ignore_warnings
 def test_regressor_matching():
     n_samples = 10
     n_features = 5
@@ -295,13 +294,13 @@ def test_regressor_matching():
                                dloss=squared_dloss,
                                fit_intercept=fit_intercept)
 
-    assert_array_almost_equal(weights1, clf.coef_, decimal=10)
-    assert_array_almost_equal(intercept1, clf.intercept_, decimal=10)
-    assert_array_almost_equal(weights2, clf.coef_, decimal=10)
-    assert_array_almost_equal(intercept2, clf.intercept_, decimal=10)
+    assert_allclose(weights1, clf.coef_)
+    assert_allclose(intercept1, clf.intercept_)
+    assert_allclose(weights2, clf.coef_)
+    assert_allclose(intercept2, clf.intercept_)
 
 
-@ignore_warnings
+@pytest.mark.filterwarnings('ignore:The max_iter was reached')
 def test_sag_pobj_matches_logistic_regression():
     """tests if the sag pobj matches log reg"""
     n_samples = 100
@@ -331,7 +330,7 @@ def test_sag_pobj_matches_logistic_regression():
     assert_array_almost_equal(pobj3, pobj1, decimal=4)
 
 
-@ignore_warnings
+@pytest.mark.filterwarnings('ignore:The max_iter was reached')
 def test_sag_pobj_matches_ridge_regression():
     """tests if the sag pobj matches ridge reg"""
     n_samples = 100
@@ -363,7 +362,7 @@ def test_sag_pobj_matches_ridge_regression():
     assert_array_almost_equal(pobj3, pobj2, decimal=4)
 
 
-@ignore_warnings
+@pytest.mark.filterwarnings('ignore:The max_iter was reached')
 def test_sag_regressor_computed_correctly():
     """tests if the sag regressor is computed correctly"""
     alpha = .1
@@ -407,7 +406,6 @@ def test_sag_regressor_computed_correctly():
     # assert_almost_equal(clf2.intercept_, spintercept2, decimal=1)'''
 
 
-@ignore_warnings
 def test_get_auto_step_size():
     X = np.array([[1, 2, 3], [2, 3, 4], [2, 3, 2]], dtype=np.float64)
     alpha = 1.2
@@ -452,7 +450,7 @@ def test_get_auto_step_size():
                          max_squared_sum_, alpha, "wrong", fit_intercept)
 
 
-@ignore_warnings
+@pytest.mark.filterwarnings('ignore:The max_iter was reached')
 def test_sag_regressor():
     """tests if the sag regressor performs well"""
     xmin, xmax = -5, 5
@@ -491,7 +489,7 @@ def test_sag_regressor():
     assert_greater(score2, 0.5)
 
 
-@ignore_warnings
+@pytest.mark.filterwarnings('ignore:The max_iter was reached')
 def test_sag_classifier_computed_correctly():
     """tests if the binary classifier is computed correctly"""
     alpha = .1
@@ -534,7 +532,7 @@ def test_sag_classifier_computed_correctly():
     assert_almost_equal(clf2.intercept_, spintercept2, decimal=1)
 
 
-@ignore_warnings
+@pytest.mark.filterwarnings('ignore:The max_iter was reached')
 def test_sag_multiclass_computed_correctly():
     """tests if the multiclass classifier is computed correctly"""
     alpha = .1
@@ -593,7 +591,6 @@ def test_sag_multiclass_computed_correctly():
         assert_almost_equal(clf2.intercept_[i], intercept2[i], decimal=1)
 
 
-@ignore_warnings
 def test_classifier_results():
     """tests if classifier results match target"""
     alpha = .1
@@ -618,7 +615,7 @@ def test_classifier_results():
     assert_almost_equal(pred2, y, decimal=12)
 
 
-@ignore_warnings
+@pytest.mark.filterwarnings('ignore:The max_iter was reached')
 def test_binary_classifier_class_weight():
     """tests binary classifier with classweights for each class"""
     alpha = .1
@@ -668,7 +665,7 @@ def test_binary_classifier_class_weight():
     assert_almost_equal(clf2.intercept_, spintercept2, decimal=1)
 
 
-@ignore_warnings
+@pytest.mark.filterwarnings('ignore:The max_iter was reached')
 def test_multiclass_classifier_class_weight():
     """tests multiclass with classweights for each class"""
     alpha = .1
diff --git a/sklearn/linear_model/tests/test_sgd.py b/sklearn/linear_model/tests/test_sgd.py
index 18bc07313965..15e29ce4d41d 100644
--- a/sklearn/linear_model/tests/test_sgd.py
+++ b/sklearn/linear_model/tests/test_sgd.py
@@ -1,3 +1,4 @@
+
 import pickle
 import unittest
 import pytest
@@ -25,7 +26,7 @@
 from sklearn.preprocessing import LabelEncoder, scale, MinMaxScaler
 from sklearn.preprocessing import StandardScaler
 from sklearn.exceptions import ConvergenceWarning
-
+from sklearn.model_selection import train_test_split
 from sklearn.linear_model import sgd_fast
 
 
@@ -101,7 +102,8 @@ def decision_function(self, X, *args, **kw):
 true_result5 = [0, 1, 1]
 
 
-# Classification Test Case
+###############################################################################
+# Tests common to classification and regression
 
 class CommonTest(object):
 
@@ -187,6 +189,9 @@ def test_warm_start_invscaling(self):
     def test_warm_start_optimal(self):
         self._test_warm_start(X, Y, "optimal")
 
+    def test_warm_start_adaptive(self):
+        self._test_warm_start(X, Y, "adaptive")
+
     def test_input_format(self):
         # Input format tests.
         clf = self.factory(alpha=0.01, shuffle=False)
@@ -272,6 +277,68 @@ def test_sgd_bad_alpha_for_optimal_learning_rate(self):
         assert_raises(ValueError, self.factory,
                       alpha=0, learning_rate="optimal")
 
+    def test_early_stopping(self):
+        for early_stopping in [True, False]:
+            max_iter = 1000
+            clf = self.factory(early_stopping=early_stopping, tol=1e-3,
+                               max_iter=max_iter).fit(X, Y)
+            assert clf.n_iter_ < max_iter
+            assert not hasattr(clf, '_X_val')
+            assert not hasattr(clf, '_y_val')
+            assert not hasattr(clf, '_sample_weight_val')
+
+    def test_adaptive_longer_than_constant(self):
+        clf1 = self.factory(learning_rate="adaptive", eta0=0.01, tol=1e-3,
+                            max_iter=100)
+        clf1.fit(iris.data, iris.target)
+        clf2 = self.factory(learning_rate="constant", eta0=0.01, tol=1e-3,
+                            max_iter=100)
+        clf2.fit(iris.data, iris.target)
+        assert clf1.n_iter_ > clf2.n_iter_
+
+    def test_validation_set_not_used_for_training(self):
+        X, Y = iris.data, iris.target
+        validation_fraction = 0.4
+        random_state = 42
+        shuffle = False
+        clf1 = self.factory(early_stopping=True, random_state=random_state,
+                            validation_fraction=validation_fraction,
+                            learning_rate='constant', eta0=0.01,
+                            tol=None, max_iter=1000, shuffle=shuffle)
+        clf1.fit(X, Y)
+
+        idx_train, idx_val = train_test_split(
+            np.arange(X.shape[0]), test_size=validation_fraction,
+            random_state=random_state)
+        clf2 = self.factory(early_stopping=False,
+                            random_state=random_state,
+                            learning_rate='constant', eta0=0.01,
+                            tol=None, max_iter=1000, shuffle=shuffle)
+        idx_train = np.sort(idx_train)  # remove shuffling
+        clf2.fit(X[idx_train], np.array(Y)[idx_train])
+
+        assert_array_equal(clf1.coef_, clf2.coef_)
+
+    @ignore_warnings(category=ConvergenceWarning)
+    def test_n_iter_no_change(self):
+        # test that n_iter_ increases monotonically with n_iter_no_change
+        for early_stopping in [True, False]:
+            n_iter_list = [self.factory(early_stopping=early_stopping,
+                                        n_iter_no_change=n_iter_no_change,
+                                        tol=1e-4, max_iter=1000
+                                        ).fit(X, Y).n_iter_
+                           for n_iter_no_change in [2, 3, 10]]
+            assert_array_equal(n_iter_list, sorted(n_iter_list))
+
+    def test_not_enough_sample_for_early_stopping(self):
+        # test an error is raised if the training or validation set is empty
+        clf = self.factory(early_stopping=True, validation_fraction=0.99)
+        with pytest.raises(ValueError):
+            clf.fit(X3, Y3)
+
+
+###############################################################################
+# Classification Test Case
 
 class DenseSGDClassifierTestCase(unittest.TestCase, CommonTest):
     """Test suite for the dense representation variant of SGD"""
@@ -321,6 +388,18 @@ def test_sgd_shuffle_param(self):
         # Test parameter validity check
         assert_raises(ValueError, self.factory, shuffle="false")
 
+    def test_sgd_early_stopping_param(self):
+        # Test parameter validity check
+        assert_raises(ValueError, self.factory, early_stopping="false")
+
+    def test_sgd_validation_fraction(self):
+        # Test parameter validity check
+        assert_raises(ValueError, self.factory, validation_fraction=-.1)
+
+    def test_sgd_n_iter_no_change(self):
+        # Test parameter validity check
+        assert_raises(ValueError, self.factory, n_iter_no_change=0)
+
     def test_argument_coef(self):
         # Checks coef_init not allowed as model argument (only fit)
         # Provided coef_ does not match dataset
@@ -338,6 +417,11 @@ def test_set_intercept(self):
         assert_raises(ValueError, self.factory().fit,
                       X, Y, intercept_init=np.zeros((3,)))
 
+    def test_sgd_early_stopping_with_partial_fit(self):
+        # Test parameter validity check
+        assert_raises(ValueError,
+                      self.factory(early_stopping=True).partial_fit, X, Y)
+
     def test_set_intercept_binary(self):
         # Checks intercept_ shape for the warm starts in binary case
         self.factory().fit(X5, Y5, intercept_init=0)
@@ -809,6 +893,9 @@ def test_partial_fit_equal_fit_optimal(self):
     def test_partial_fit_equal_fit_invscaling(self):
         self._test_partial_fit_equal_fit("invscaling")
 
+    def test_partial_fit_equal_fit_adaptive(self):
+        self._test_partial_fit_equal_fit("adaptive")
+
     def test_regression_losses(self):
         clf = self.factory(alpha=0.01, learning_rate="constant",
                            eta0=0.1, loss="epsilon_insensitive")
@@ -1093,6 +1180,9 @@ def test_partial_fit_equal_fit_optimal(self):
     def test_partial_fit_equal_fit_invscaling(self):
         self._test_partial_fit_equal_fit("invscaling")
 
+    def test_partial_fit_equal_fit_adaptive(self):
+        self._test_partial_fit_equal_fit("adaptive")
+
     def test_loss_function_epsilon(self):
         clf = self.factory(epsilon=0.9)
         clf.set_params(epsilon=0.1)
diff --git a/sklearn/linear_model/theil_sen.py b/sklearn/linear_model/theil_sen.py
index 544f79f9df05..0f3b19164b14 100644
--- a/sklearn/linear_model/theil_sen.py
+++ b/sklearn/linear_model/theil_sen.py
@@ -20,8 +20,8 @@
 from .base import LinearModel
 from ..base import RegressorMixin
 from ..utils import check_random_state
-from ..utils import check_X_y, _get_n_jobs
-from ..externals.joblib import Parallel, delayed
+from ..utils import check_X_y, effective_n_jobs
+from ..utils import Parallel, delayed
 from ..externals.six.moves import xrange as range
 from ..exceptions import ConvergenceWarning
 
@@ -249,9 +249,11 @@ class TheilSenRegressor(LinearModel, RegressorMixin):
         random number generator; If None, the random number generator is the
         RandomState instance used by `np.random`.
 
-    n_jobs : integer, optional, default 1
-        Number of CPUs to use during the cross validation. If ``-1``, use
-        all the CPUs.
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the cross validation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : boolean, optional, default False
         Verbose mode when fitting the model.
@@ -283,7 +285,7 @@ class TheilSenRegressor(LinearModel, RegressorMixin):
 
     def __init__(self, fit_intercept=True, copy_X=True,
                  max_subpopulation=1e4, n_subsamples=None, max_iter=300,
-                 tol=1.e-3, random_state=None, n_jobs=1, verbose=False):
+                 tol=1.e-3, random_state=None, n_jobs=None, verbose=False):
         self.fit_intercept = fit_intercept
         self.copy_X = copy_X
         self.max_subpopulation = int(max_subpopulation)
@@ -368,7 +370,7 @@ def fit(self, X, y):
                                            replace=False)
                        for _ in range(self.n_subpopulation_)]
 
-        n_jobs = _get_n_jobs(self.n_jobs)
+        n_jobs = effective_n_jobs(self.n_jobs)
         index_list = np.array_split(indices, n_jobs)
         weights = Parallel(n_jobs=n_jobs,
                            verbose=self.verbose)(
diff --git a/sklearn/manifold/isomap.py b/sklearn/manifold/isomap.py
index 6ac431929d30..bbb83a5ed81f 100644
--- a/sklearn/manifold/isomap.py
+++ b/sklearn/manifold/isomap.py
@@ -58,9 +58,11 @@ class Isomap(BaseEstimator, TransformerMixin):
         Algorithm to use for nearest neighbors search,
         passed to neighbors.NearestNeighbors instance.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -80,6 +82,18 @@ class Isomap(BaseEstimator, TransformerMixin):
     dist_matrix_ : array-like, shape (n_samples, n_samples)
         Stores the geodesic distance matrix of training data.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.manifold import Isomap
+    >>> X, _ = load_digits(return_X_y=True)
+    >>> X.shape
+    (1797, 64)
+    >>> embedding = Isomap(n_components=2)
+    >>> X_transformed = embedding.fit_transform(X[:100])
+    >>> X_transformed.shape
+    (100, 2)
+
     References
     ----------
 
@@ -89,7 +103,7 @@ class Isomap(BaseEstimator, TransformerMixin):
 
     def __init__(self, n_neighbors=5, n_components=2, eigen_solver='auto',
                  tol=0, max_iter=None, path_method='auto',
-                 neighbors_algorithm='auto', n_jobs=1):
+                 neighbors_algorithm='auto', n_jobs=None):
         self.n_neighbors = n_neighbors
         self.n_components = n_components
         self.eigen_solver = eigen_solver
@@ -157,7 +171,7 @@ def fit(self, X, y=None):
             numpy array, precomputed tree, or NearestNeighbors
             object.
 
-        y: Ignored
+        y : Ignored
 
         Returns
         -------
@@ -175,7 +189,7 @@ def fit_transform(self, X, y=None):
             Training vector, where n_samples in the number of samples
             and n_features is the number of features.
 
-        y: Ignored
+        y : Ignored
 
         Returns
         -------
diff --git a/sklearn/manifold/locally_linear.py b/sklearn/manifold/locally_linear.py
index 661af1d4166a..c3afdac2cba0 100644
--- a/sklearn/manifold/locally_linear.py
+++ b/sklearn/manifold/locally_linear.py
@@ -64,7 +64,7 @@ def barycenter_weights(X, Z, reg=1e-3):
     return B
 
 
-def barycenter_kneighbors_graph(X, n_neighbors, reg=1e-3, n_jobs=1):
+def barycenter_kneighbors_graph(X, n_neighbors, reg=1e-3, n_jobs=None):
     """Computes the barycenter weighted graph of k-Neighbors for points in X
 
     Parameters
@@ -81,9 +81,11 @@ def barycenter_kneighbors_graph(X, n_neighbors, reg=1e-3, n_jobs=1):
         problem. Only relevant if mode='barycenter'. If None, use the
         default.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Returns
     -------
@@ -186,7 +188,7 @@ def null_space(M, k, k_skip=1, eigen_solver='arpack', tol=1E-6, max_iter=100,
 def locally_linear_embedding(
         X, n_neighbors, n_components, reg=1e-3, eigen_solver='auto', tol=1e-6,
         max_iter=100, method='standard', hessian_tol=1E-4, modified_tol=1E-12,
-        random_state=None, n_jobs=1):
+        random_state=None, n_jobs=None):
     """Perform a Locally Linear Embedding analysis on the data.
 
     Read more in the :ref:`User Guide <locally_linear_embedding>`.
@@ -253,9 +255,11 @@ def locally_linear_embedding(
         If None, the random number generator is the RandomState instance used
         by `np.random`. Used when ``solver`` == 'arpack'.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Returns
     -------
@@ -448,7 +452,7 @@ def locally_linear_embedding(
             # compute Householder matrix which satisfies
             #  Hi*Vi.T*ones(n_neighbors) = alpha_i*ones(s)
             # using prescription from paper
-            h = alpha_i * np.ones(s_i) - np.dot(Vi.T, np.ones(n_neighbors))
+            h = np.full(s_i, alpha_i) - np.dot(Vi.T, np.ones(n_neighbors))
 
             norm_h = np.linalg.norm(h)
             if norm_h < modified_tol:
@@ -582,9 +586,11 @@ class LocallyLinearEmbedding(BaseEstimator, TransformerMixin):
         If None, the random number generator is the RandomState instance used
         by `np.random`. Used when ``eigen_solver`` == 'arpack'.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -598,6 +604,18 @@ class LocallyLinearEmbedding(BaseEstimator, TransformerMixin):
         Stores nearest neighbors instance, including BallTree or KDtree
         if applicable.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.manifold import LocallyLinearEmbedding
+    >>> X, _ = load_digits(return_X_y=True)
+    >>> X.shape
+    (1797, 64)
+    >>> embedding = LocallyLinearEmbedding(n_components=2)
+    >>> X_transformed = embedding.fit_transform(X[:100])
+    >>> X_transformed.shape
+    (100, 2)
+
     References
     ----------
 
@@ -617,7 +635,7 @@ class LocallyLinearEmbedding(BaseEstimator, TransformerMixin):
     def __init__(self, n_neighbors=5, n_components=2, reg=1E-3,
                  eigen_solver='auto', tol=1E-6, max_iter=100,
                  method='standard', hessian_tol=1E-4, modified_tol=1E-12,
-                 neighbors_algorithm='auto', random_state=None, n_jobs=1):
+                 neighbors_algorithm='auto', random_state=None, n_jobs=None):
         self.n_neighbors = n_neighbors
         self.n_components = n_components
         self.reg = reg
@@ -655,7 +673,7 @@ def fit(self, X, y=None):
         X : array-like of shape [n_samples, n_features]
             training set.
 
-        y: Ignored
+        y : Ignored
 
         Returns
         -------
@@ -672,7 +690,7 @@ def fit_transform(self, X, y=None):
         X : array-like of shape [n_samples, n_features]
             training set.
 
-        y: Ignored
+        y : Ignored
 
         Returns
         -------
diff --git a/sklearn/manifold/mds.py b/sklearn/manifold/mds.py
index 3890c4e40bff..ecfe22af28ab 100644
--- a/sklearn/manifold/mds.py
+++ b/sklearn/manifold/mds.py
@@ -12,8 +12,9 @@
 from ..base import BaseEstimator
 from ..metrics import euclidean_distances
 from ..utils import check_random_state, check_array, check_symmetric
-from ..externals.joblib import Parallel
-from ..externals.joblib import delayed
+from ..utils import Parallel
+from ..utils import delayed
+from ..utils import effective_n_jobs
 from ..isotonic import IsotonicRegression
 
 
@@ -132,7 +133,7 @@ def _smacof_single(dissimilarities, metric=True, n_components=2, init=None,
 
 
 def smacof(dissimilarities, metric=True, n_components=2, init=None, n_init=8,
-           n_jobs=1, max_iter=300, verbose=0, eps=1e-3, random_state=None,
+           n_jobs=None, max_iter=300, verbose=0, eps=1e-3, random_state=None,
            return_n_iter=False):
     """Computes multidimensional scaling using the SMACOF algorithm.
 
@@ -177,15 +178,14 @@ def smacof(dissimilarities, metric=True, n_components=2, init=None, n_init=8,
         determined by the run with the smallest final stress. If ``init`` is
         provided, this option is overridden and a single run is performed.
 
-    n_jobs : int, optional, default: 1
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. If multiple
         initializations are used (``n_init``), each run of the algorithm is
         computed in parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For ``n_jobs`` below -1,
-        (``n_cpus + 1 + n_jobs``) are used. Thus for ``n_jobs = -2``, all CPUs
-        but one are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     max_iter : int, optional, default: 300
         Maximum number of iterations of the SMACOF algorithm for a single run.
@@ -245,7 +245,7 @@ def smacof(dissimilarities, metric=True, n_components=2, init=None, n_init=8,
 
     best_pos, best_stress = None, None
 
-    if n_jobs == 1:
+    if effective_n_jobs(n_jobs) == 1:
         for it in range(n_init):
             pos, stress, n_iter_ = _smacof_single(
                 dissimilarities, metric=metric,
@@ -304,15 +304,14 @@ class MDS(BaseEstimator):
         Relative tolerance with respect to stress at which to declare
         convergence.
 
-    n_jobs : int, optional, default: 1
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. If multiple
         initializations are used (``n_init``), each run of the algorithm is
         computed in parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For ``n_jobs`` below -1,
-        (``n_cpus + 1 + n_jobs``) are used. Thus for ``n_jobs = -2``, all CPUs
-        but one are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional, default: None
         The generator used to initialize the centers.  If int, random_state is
@@ -332,13 +331,24 @@ class MDS(BaseEstimator):
 
     Attributes
     ----------
-    embedding_ : array-like, shape (n_components, n_samples)
+    embedding_ : array-like, shape (n_samples, n_components)
         Stores the position of the dataset in the embedding space.
 
     stress_ : float
         The final value of the stress (sum of squared distance of the
         disparities and the distances for all constrained points).
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.manifold import MDS
+    >>> X, _ = load_digits(return_X_y=True)
+    >>> X.shape
+    (1797, 64)
+    >>> embedding = MDS(n_components=2)
+    >>> X_transformed = embedding.fit_transform(X[:100])
+    >>> X_transformed.shape
+    (100, 2)
 
     References
     ----------
@@ -353,7 +363,7 @@ class MDS(BaseEstimator):
 
     """
     def __init__(self, n_components=2, metric=True, n_init=4,
-                 max_iter=300, verbose=0, eps=1e-3, n_jobs=1,
+                 max_iter=300, verbose=0, eps=1e-3, n_jobs=None,
                  random_state=None, dissimilarity="euclidean"):
         self.n_components = n_components
         self.dissimilarity = dissimilarity
@@ -379,7 +389,7 @@ def fit(self, X, y=None, init=None):
             Input data. If ``dissimilarity=='precomputed'``, the input should
             be the dissimilarity matrix.
 
-        y: Ignored
+        y : Ignored
 
         init : ndarray, shape (n_samples,), optional, default: None
             Starting configuration of the embedding to initialize the SMACOF
@@ -399,7 +409,7 @@ def fit_transform(self, X, y=None, init=None):
             Input data. If ``dissimilarity=='precomputed'``, the input should
             be the dissimilarity matrix.
 
-        y: Ignored
+        y : Ignored
 
         init : ndarray, shape (n_samples,), optional, default: None
             Starting configuration of the embedding to initialize the SMACOF
diff --git a/sklearn/manifold/spectral_embedding_.py b/sklearn/manifold/spectral_embedding_.py
index bc367b4e5af9..71c822e3b435 100644
--- a/sklearn/manifold/spectral_embedding_.py
+++ b/sklearn/manifold/spectral_embedding_.py
@@ -382,9 +382,11 @@ class SpectralEmbedding(BaseEstimator):
     n_neighbors : int, default : max(n_samples/10 , 1)
         Number of nearest neighbors for nearest_neighbors graph building.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -395,6 +397,18 @@ class SpectralEmbedding(BaseEstimator):
     affinity_matrix_ : array, shape = (n_samples, n_samples)
         Affinity_matrix constructed from samples or precomputed.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.manifold import SpectralEmbedding
+    >>> X, _ = load_digits(return_X_y=True)
+    >>> X.shape
+    (1797, 64)
+    >>> embedding = SpectralEmbedding(n_components=2)
+    >>> X_transformed = embedding.fit_transform(X[:100])
+    >>> X_transformed.shape
+    (100, 2)
+
     References
     ----------
 
@@ -413,7 +427,7 @@ class SpectralEmbedding(BaseEstimator):
 
     def __init__(self, n_components=2, affinity="nearest_neighbors",
                  gamma=None, random_state=None, eigen_solver=None,
-                 n_neighbors=None, n_jobs=1):
+                 n_neighbors=None, n_jobs=None):
         self.n_components = n_components
         self.affinity = affinity
         self.gamma = gamma
@@ -487,8 +501,6 @@ def fit(self, X, y=None):
             Interpret X as precomputed adjacency graph computed from
             samples.
 
-        Y: Ignored
-
         Returns
         -------
         self : object
@@ -529,8 +541,6 @@ def fit_transform(self, X, y=None):
             Interpret X as precomputed adjacency graph computed from
             samples.
 
-        Y: Ignored
-
         Returns
         -------
         X_new : array-like, shape (n_samples, n_components)
diff --git a/sklearn/manifold/tests/test_spectral_embedding.py b/sklearn/manifold/tests/test_spectral_embedding.py
index bc32b58c6e7f..d236c17e5dbb 100644
--- a/sklearn/manifold/tests/test_spectral_embedding.py
+++ b/sklearn/manifold/tests/test_spectral_embedding.py
@@ -86,6 +86,8 @@ def test_sparse_graph_connected_component():
         assert_array_equal(component_1, component_2)
 
 
+@pytest.mark.filterwarnings("ignore:the behavior of nmi will "
+                            "change in version 0.22")
 def test_spectral_embedding_two_components(seed=36):
     # Test spectral embedding with two components
     random_state = np.random.RandomState(seed)
@@ -180,6 +182,8 @@ def test_spectral_embedding_amg_solver(seed=36):
     assert_true(_check_with_col_sign_flipping(embed_amg, embed_arpack, 0.05))
 
 
+@pytest.mark.filterwarnings("ignore:the behavior of nmi will "
+                            "change in version 0.22")
 def test_pipeline_spectral_clustering(seed=36):
     # Test using pipeline to do spectral clustering
     random_state = np.random.RandomState(seed)
diff --git a/sklearn/metrics/base.py b/sklearn/metrics/base.py
index b8bbab30930b..1877ee4e43f7 100644
--- a/sklearn/metrics/base.py
+++ b/sklearn/metrics/base.py
@@ -49,6 +49,8 @@ def _average_binary_score(binary_metric, y_true, y_score, average,
         ``'samples'``:
             Calculate metrics for each instance, and find their average.
 
+        Will be ignored when ``y_true`` is binary.
+
     sample_weight : array-like of shape = [n_samples], optional
         Sample weights.
 
@@ -94,7 +96,7 @@ def _average_binary_score(binary_metric, y_true, y_score, average,
                 y_true, np.reshape(score_weight, (-1, 1))), axis=0)
         else:
             average_weight = np.sum(y_true, axis=0)
-        if average_weight.sum() == 0:
+        if np.isclose(average_weight.sum(), 0.0):
             return 0
 
     elif average == 'samples':
diff --git a/sklearn/metrics/classification.py b/sklearn/metrics/classification.py
index f372461873f6..29a2115e2eeb 100644
--- a/sklearn/metrics/classification.py
+++ b/sklearn/metrics/classification.py
@@ -675,7 +675,7 @@ def f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary',
             Calculate metrics for each label, and find their unweighted
             mean.  This does not take label imbalance into account.
         ``'weighted'``:
-            Calculate metrics for each label, and find their average, weighted
+            Calculate metrics for each label, and find their average weighted
             by support (the number of true instances for each label). This
             alters 'macro' to account for label imbalance; it can result in an
             F-score that is not between precision and recall.
@@ -778,7 +778,7 @@ def fbeta_score(y_true, y_pred, beta, labels=None, pos_label=1,
             Calculate metrics for each label, and find their unweighted
             mean.  This does not take label imbalance into account.
         ``'weighted'``:
-            Calculate metrics for each label, and find their average, weighted
+            Calculate metrics for each label, and find their average weighted
             by support (the number of true instances for each label). This
             alters 'macro' to account for label imbalance; it can result in an
             F-score that is not between precision and recall.
@@ -950,7 +950,7 @@ def precision_recall_fscore_support(y_true, y_pred, beta=1.0, labels=None,
             Calculate metrics for each label, and find their unweighted
             mean.  This does not take label imbalance into account.
         ``'weighted'``:
-            Calculate metrics for each label, and find their average, weighted
+            Calculate metrics for each label, and find their average weighted
             by support (the number of true instances for each label). This
             alters 'macro' to account for label imbalance; it can result in an
             F-score that is not between precision and recall.
@@ -1224,7 +1224,7 @@ def precision_score(y_true, y_pred, labels=None, pos_label=1,
             Calculate metrics for each label, and find their unweighted
             mean.  This does not take label imbalance into account.
         ``'weighted'``:
-            Calculate metrics for each label, and find their average, weighted
+            Calculate metrics for each label, and find their average weighted
             by support (the number of true instances for each label). This
             alters 'macro' to account for label imbalance; it can result in an
             F-score that is not between precision and recall.
@@ -1323,7 +1323,7 @@ def recall_score(y_true, y_pred, labels=None, pos_label=1, average='binary',
             Calculate metrics for each label, and find their unweighted
             mean.  This does not take label imbalance into account.
         ``'weighted'``:
-            Calculate metrics for each label, and find their average, weighted
+            Calculate metrics for each label, and find their average weighted
             by support (the number of true instances for each label). This
             alters 'macro' to account for label imbalance; it can result in an
             F-score that is not between precision and recall.
@@ -1367,16 +1367,15 @@ def recall_score(y_true, y_pred, labels=None, pos_label=1, average='binary',
     return r
 
 
-def balanced_accuracy_score(y_true, y_pred, sample_weight=None):
+def balanced_accuracy_score(y_true, y_pred, sample_weight=None,
+                            adjusted=False):
     """Compute the balanced accuracy
 
-    The balanced accuracy is used in binary classification problems to deal
-    with imbalanced datasets. It is defined as the arithmetic mean of
-    sensitivity (true positive rate) and specificity (true negative rate),
-    or the average recall obtained on either class. It is also equal to the
-    ROC AUC score given binary inputs.
+    The balanced accuracy in binary and multiclass classification problems to
+    deal with imbalanced datasets. It is defined as the average of recall
+    obtained on each class.
 
-    The best value is 1 and the worst value is 0.
+    The best value is 1 and the worst value is 0 when ``adjusted=False``.
 
     Read more in the :ref:`User Guide <balanced_accuracy_score>`.
 
@@ -1391,10 +1390,13 @@ def balanced_accuracy_score(y_true, y_pred, sample_weight=None):
     sample_weight : array-like of shape = [n_samples], optional
         Sample weights.
 
+    adjusted : bool, default=False
+        When true, the result is adjusted for chance, so that random
+        performance would score 0, and perfect performance scores 1.
+
     Returns
     -------
-    balanced_accuracy : float.
-        The average of sensitivity and specificity
+    balanced_accuracy : float
 
     See also
     --------
@@ -1406,6 +1408,10 @@ def balanced_accuracy_score(y_true, y_pred, sample_weight=None):
            The balanced accuracy and its posterior distribution.
            Proceedings of the 20th International Conference on Pattern
            Recognition, 3121-24.
+    .. [2] John. D. Kelleher, Brian Mac Namee, Aoife D'Arcy, (2015).
+           `Fundamentals of Machine Learning for Predictive Data Analytics:
+           Algorithms, Worked Examples, and Case Studies
+           <https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics>`_.
 
     Examples
     --------
@@ -1416,16 +1422,19 @@ def balanced_accuracy_score(y_true, y_pred, sample_weight=None):
     0.625
 
     """
-    y_type, y_true, y_pred = _check_targets(y_true, y_pred)
-
-    if y_type != 'binary':
-        raise ValueError('Balanced accuracy is only meaningful '
-                         'for binary classification problems.')
-    # simply wrap the ``recall_score`` function
-    return recall_score(y_true, y_pred,
-                        pos_label=None,
-                        average='macro',
-                        sample_weight=sample_weight)
+    C = confusion_matrix(y_true, y_pred, sample_weight=sample_weight)
+    with np.errstate(divide='ignore', invalid='ignore'):
+        per_class = np.diag(C) / C.sum(axis=1)
+    if np.any(np.isnan(per_class)):
+        warnings.warn('y_pred contains classes not in y_true')
+        per_class = per_class[~np.isnan(per_class)]
+    score = np.mean(per_class)
+    if adjusted:
+        n_classes = len(per_class)
+        chance = 1 / n_classes
+        score -= chance
+        score /= 1 - chance
+    return score
 
 
 def classification_report(y_true, y_pred, labels=None, target_names=None,
@@ -1452,9 +1461,11 @@ def classification_report(y_true, y_pred, labels=None, target_names=None,
         Sample weights.
 
     digits : int
-        Number of digits for formatting output floating point values
+        Number of digits for formatting output floating point values.
+        When ``output_dict`` is ``True``, this will be ignored and the
+        returned values will not be rounded.
 
-    output_dict: bool (default = False)
+    output_dict : bool (default = False)
         If True, return output as dict
 
     Returns
@@ -1472,9 +1483,12 @@ def classification_report(y_true, y_pred, labels=None, target_names=None,
               ...
             }
 
-        The reported averages are a prevalence-weighted macro-average across
-        classes (equivalent to :func:`precision_recall_fscore_support` with
-        ``average='weighted'``).
+        The reported averages include micro average (averaging the
+        total true positives, false negatives and false positives), macro
+        average (averaging the unweighted mean per label), weighted average
+        (averaging the support-weighted mean per label) and sample average
+        (only for multilabel classification). See also
+        :func:`precision_recall_fscore_support` for more details on averages.
 
         Note that in binary classification, recall of the positive class
         is also known as "sensitivity"; recall of the negative class is
@@ -1487,17 +1501,20 @@ def classification_report(y_true, y_pred, labels=None, target_names=None,
     >>> y_pred = [0, 0, 2, 2, 1]
     >>> target_names = ['class 0', 'class 1', 'class 2']
     >>> print(classification_report(y_true, y_pred, target_names=target_names))
-                 precision    recall  f1-score   support
+                  precision    recall  f1-score   support
     <BLANKLINE>
-        class 0       0.50      1.00      0.67         1
-        class 1       0.00      0.00      0.00         1
-        class 2       1.00      0.67      0.80         3
+         class 0       0.50      1.00      0.67         1
+         class 1       0.00      0.00      0.00         1
+         class 2       1.00      0.67      0.80         3
     <BLANKLINE>
-    avg / total       0.70      0.60      0.61         5
+       micro avg       0.60      0.60      0.60         5
+       macro avg       0.50      0.56      0.49         5
+    weighted avg       0.70      0.60      0.61         5
     <BLANKLINE>
-
     """
 
+    y_type, y_true, y_pred = _check_targets(y_true, y_pred)
+
     labels_given = True
     if labels is None:
         labels = unique_labels(y_true, y_pred)
@@ -1517,53 +1534,59 @@ class 2       1.00      0.67      0.80         3
                 "target_names, {1}. Try specifying the labels "
                 "parameter".format(len(labels), len(target_names))
             )
-
-    last_line_heading = 'avg / total'
-
     if target_names is None:
         target_names = [u'%s' % l for l in labels]
-    name_width = max(len(cn) for cn in target_names)
-    width = max(name_width, len(last_line_heading), digits)
 
     headers = ["precision", "recall", "f1-score", "support"]
-    head_fmt = u'{:>{width}s} ' + u' {:>9}' * len(headers)
-    report = head_fmt.format(u'', *headers, width=width)
-    report += u'\n\n'
-
+    # compute per-class results without averaging
     p, r, f1, s = precision_recall_fscore_support(y_true, y_pred,
                                                   labels=labels,
                                                   average=None,
                                                   sample_weight=sample_weight)
-
-    row_fmt = u'{:>{width}s} ' + u' {:>9.{digits}f}' * 3 + u' {:>9}\n'
     rows = zip(target_names, p, r, f1, s)
 
-    avg_total = [np.average(p, weights=s),
-                 np.average(r, weights=s),
-                 np.average(f1, weights=s),
-                 np.sum(s)]
+    if y_type.startswith('multilabel'):
+        average_options = ('micro', 'macro', 'weighted', 'samples')
+    else:
+        average_options = ('micro', 'macro', 'weighted')
 
     if output_dict:
         report_dict = {label[0]: label[1:] for label in rows}
-
         for label, scores in report_dict.items():
-            report_dict[label] = dict(zip(headers, scores))
-
-        report_dict['avg / total'] = dict(zip(headers, avg_total))
+            report_dict[label] = dict(zip(headers,
+                                          [i.item() for i in scores]))
+    else:
+        longest_last_line_heading = 'weighted avg'
+        name_width = max(len(cn) for cn in target_names)
+        width = max(name_width, len(longest_last_line_heading), digits)
+        head_fmt = u'{:>{width}s} ' + u' {:>9}' * len(headers)
+        report = head_fmt.format(u'', *headers, width=width)
+        report += u'\n\n'
+        row_fmt = u'{:>{width}s} ' + u' {:>9.{digits}f}' * 3 + u' {:>9}\n'
+        for row in rows:
+            report += row_fmt.format(*row, width=width, digits=digits)
+        report += u'\n'
+
+    # compute all applicable averages
+    for average in average_options:
+        line_heading = average + ' avg'
+        # compute averages with specified averaging method
+        avg_p, avg_r, avg_f1, _ = precision_recall_fscore_support(
+            y_true, y_pred, labels=labels,
+            average=average, sample_weight=sample_weight)
+        avg = [avg_p, avg_r, avg_f1, np.sum(s)]
+
+        if output_dict:
+            report_dict[line_heading] = dict(
+                zip(headers, [i.item() for i in avg]))
+        else:
+            report += row_fmt.format(line_heading, *avg,
+                                     width=width, digits=digits)
 
+    if output_dict:
         return report_dict
-
-    for row in rows:
-        report += row_fmt.format(*row, width=width, digits=digits)
-
-    report += u'\n'
-
-    # append averages
-    report += row_fmt.format(last_line_heading,
-                             *avg_total,
-                             width=width, digits=digits)
-
-    return report
+    else:
+        return report
 
 
 def hamming_loss(y_true, y_pred, labels=None, sample_weight=None):
@@ -1910,7 +1933,7 @@ def hinge_loss(y_true, pred_decision, labels=None, sample_weight=None):
 
     losses = 1 - margin
     # The hinge_loss doesn't penalize good enough predictions.
-    losses[losses <= 0] = 0
+    np.clip(losses, 0, None, out=losses)
     return np.average(losses, weights=sample_weight)
 
 
diff --git a/sklearn/metrics/cluster/supervised.py b/sklearn/metrics/cluster/supervised.py
index db73380fafbf..8483ef11f8d0 100644
--- a/sklearn/metrics/cluster/supervised.py
+++ b/sklearn/metrics/cluster/supervised.py
@@ -11,11 +11,13 @@
 #          Thierry Guillemot <thierry.guillemot.work@gmail.com>
 #          Gregory Stupp <stuppie@gmail.com>
 #          Joel Nothman <joel.nothman@gmail.com>
+#          Arya McCarthy <arya@jhu.edu>
 # License: BSD 3 clause
 
 from __future__ import division
 
 from math import log
+import warnings
 
 import numpy as np
 from scipy import sparse as sp
@@ -25,14 +27,23 @@
 from ...utils.fixes import comb
 
 
-def comb2(n):
+def _comb2(n):
     # the exact version is faster for k == 2: use it by default globally in
     # this module instead of the float approximate variant
     return comb(n, 2, exact=1)
 
 
 def check_clusterings(labels_true, labels_pred):
-    """Check that the two clusterings matching 1D integer arrays."""
+    """Check that the labels arrays are 1D and of same dimension.
+
+    Parameters
+    ----------
+    labels_true : int array, shape = [n_samples]
+        The true labels
+
+    labels_pred : int array, shape = [n_samples]
+        The predicted labels
+    """
     labels_true = np.asarray(labels_true)
     labels_pred = np.asarray(labels_pred)
 
@@ -50,6 +61,21 @@ def check_clusterings(labels_true, labels_pred):
     return labels_true, labels_pred
 
 
+def _generalized_average(U, V, average_method):
+    """Return a particular mean of two numbers."""
+    if average_method == "min":
+        return min(U, V)
+    elif average_method == "geometric":
+        return np.sqrt(U * V)
+    elif average_method == "arithmetic":
+        return np.mean([U, V])
+    elif average_method == "max":
+        return max(U, V)
+    else:
+        raise ValueError("'average_method' must be 'min', 'geometric', "
+                         "'arithmetic', or 'max'")
+
+
 def contingency_matrix(labels_true, labels_pred, eps=None, sparse=False):
     """Build a contingency matrix describing the relationship between labels.
 
@@ -205,11 +231,11 @@ def adjusted_rand_score(labels_true, labels_pred):
 
     # Compute the ARI using the contingency data
     contingency = contingency_matrix(labels_true, labels_pred, sparse=True)
-    sum_comb_c = sum(comb2(n_c) for n_c in np.ravel(contingency.sum(axis=1)))
-    sum_comb_k = sum(comb2(n_k) for n_k in np.ravel(contingency.sum(axis=0)))
-    sum_comb = sum(comb2(n_ij) for n_ij in contingency.data)
+    sum_comb_c = sum(_comb2(n_c) for n_c in np.ravel(contingency.sum(axis=1)))
+    sum_comb_k = sum(_comb2(n_k) for n_k in np.ravel(contingency.sum(axis=0)))
+    sum_comb = sum(_comb2(n_ij) for n_ij in contingency.data)
 
-    prod_comb = (sum_comb_c * sum_comb_k) / comb2(n_samples)
+    prod_comb = (sum_comb_c * sum_comb_k) / _comb2(n_samples)
     mean_comb = (sum_comb_k + sum_comb_c) / 2.
     return (sum_comb - prod_comb) / (mean_comb - prod_comb)
 
@@ -236,7 +262,9 @@ def homogeneity_completeness_v_measure(labels_true, labels_pred):
 
     V-Measure is furthermore symmetric: swapping ``labels_true`` and
     ``label_pred`` will give the same score. This does not hold for
-    homogeneity and completeness.
+    homogeneity and completeness. V-Measure is identical to
+    :func:`normalized_mutual_info_score` with the arithmetic averaging
+    method.
 
     Read more in the :ref:`User Guide <homogeneity_completeness>`.
 
@@ -435,7 +463,8 @@ def completeness_score(labels_true, labels_pred):
 def v_measure_score(labels_true, labels_pred):
     """V-measure cluster labeling given a ground truth.
 
-    This score is identical to :func:`normalized_mutual_info_score`.
+    This score is identical to :func:`normalized_mutual_info_score` with
+    the ``'arithmetic'`` option for averaging.
 
     The V-measure is the harmonic mean between homogeneity and completeness::
 
@@ -450,6 +479,7 @@ def v_measure_score(labels_true, labels_pred):
     measure the agreement of two independent label assignments strategies
     on the same dataset when the real ground truth is not known.
 
+
     Read more in the :ref:`User Guide <homogeneity_completeness>`.
 
     Parameters
@@ -476,6 +506,7 @@ def v_measure_score(labels_true, labels_pred):
     --------
     homogeneity_score
     completeness_score
+    normalized_mutual_info_score
 
     Examples
     --------
@@ -608,7 +639,8 @@ def mutual_info_score(labels_true, labels_pred, contingency=None):
     return mi.sum()
 
 
-def adjusted_mutual_info_score(labels_true, labels_pred):
+def adjusted_mutual_info_score(labels_true, labels_pred,
+                               average_method='warn'):
     """Adjusted Mutual Information between two clusterings.
 
     Adjusted Mutual Information (AMI) is an adjustment of the Mutual
@@ -617,7 +649,7 @@ def adjusted_mutual_info_score(labels_true, labels_pred):
     clusters, regardless of whether there is actually more information shared.
     For two clusterings :math:`U` and :math:`V`, the AMI is given as::
 
-        AMI(U, V) = [MI(U, V) - E(MI(U, V))] / [max(H(U), H(V)) - E(MI(U, V))]
+        AMI(U, V) = [MI(U, V) - E(MI(U, V))] / [avg(H(U), H(V)) - E(MI(U, V))]
 
     This metric is independent of the absolute values of the labels:
     a permutation of the class or cluster label values won't change the
@@ -641,9 +673,17 @@ def adjusted_mutual_info_score(labels_true, labels_pred):
     labels_pred : array, shape = [n_samples]
         A clustering of the data into disjoint subsets.
 
+    average_method : string, optional (default: 'warn')
+        How to compute the normalizer in the denominator. Possible options
+        are 'min', 'geometric', 'arithmetic', and 'max'.
+        If 'warn', 'max' will be used. The default will change to
+        'arithmetic' in version 0.22.
+
+        .. versionadded:: 0.20
+
     Returns
     -------
-    ami: float(upperlimited by 1.0)
+    ami: float (upperlimited by 1.0)
        The AMI returns a value of 1 when the two partitions are identical
        (ie perfectly matched). Random partitions (independent labellings) have
        an expected AMI around 0 on average hence can be negative.
@@ -661,14 +701,17 @@ def adjusted_mutual_info_score(labels_true, labels_pred):
 
       >>> from sklearn.metrics.cluster import adjusted_mutual_info_score
       >>> adjusted_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])
+      ... # doctest: +SKIP
       1.0
       >>> adjusted_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])
+      ... # doctest: +SKIP
       1.0
 
     If classes members are completely split across different clusters,
     the assignment is totally in-complete, hence the AMI is null::
 
       >>> adjusted_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])
+      ... # doctest: +SKIP
       0.0
 
     References
@@ -682,6 +725,12 @@ def adjusted_mutual_info_score(labels_true, labels_pred):
        <https://en.wikipedia.org/wiki/Adjusted_Mutual_Information>`_
 
     """
+    if average_method == 'warn':
+        warnings.warn("The behavior of AMI will change in version 0.22. "
+                      "To match the behavior of 'v_measure_score', AMI will "
+                      "use average_method='arithmetic' by default.",
+                      FutureWarning)
+        average_method = 'max'
     labels_true, labels_pred = check_clusterings(labels_true, labels_pred)
     n_samples = labels_true.shape[0]
     classes = np.unique(labels_true)
@@ -700,20 +749,32 @@ def adjusted_mutual_info_score(labels_true, labels_pred):
     emi = expected_mutual_information(contingency, n_samples)
     # Calculate entropy for each labeling
     h_true, h_pred = entropy(labels_true), entropy(labels_pred)
-    ami = (mi - emi) / (max(h_true, h_pred) - emi)
+    normalizer = _generalized_average(h_true, h_pred, average_method)
+    denominator = normalizer - emi
+    # Avoid 0.0 / 0.0 when expectation equals maximum, i.e a perfect match.
+    # normalizer should always be >= emi, but because of floating-point
+    # representation, sometimes emi is slightly larger. Correct this
+    # by preserving the sign.
+    if denominator < 0:
+        denominator = min(denominator, -np.finfo('float64').eps)
+    else:
+        denominator = max(denominator, np.finfo('float64').eps)
+    ami = (mi - emi) / denominator
     return ami
 
 
-def normalized_mutual_info_score(labels_true, labels_pred):
+def normalized_mutual_info_score(labels_true, labels_pred,
+                                 average_method='warn'):
     """Normalized Mutual Information between two clusterings.
 
-    Normalized Mutual Information (NMI) is an normalization of the Mutual
+    Normalized Mutual Information (NMI) is a normalization of the Mutual
     Information (MI) score to scale the results between 0 (no mutual
     information) and 1 (perfect correlation). In this function, mutual
-    information is normalized by ``sqrt(H(labels_true) * H(labels_pred))``.
+    information is normalized by some generalized mean of ``H(labels_true)``
+    and ``H(labels_pred))``, defined by the `average_method`.
 
     This measure is not adjusted for chance. Therefore
-    :func:`adjusted_mustual_info_score` might be preferred.
+    :func:`adjusted_mutual_info_score` might be preferred.
 
     This metric is independent of the absolute values of the labels:
     a permutation of the class or cluster label values won't change the
@@ -734,6 +795,14 @@ def normalized_mutual_info_score(labels_true, labels_pred):
     labels_pred : array, shape = [n_samples]
         A clustering of the data into disjoint subsets.
 
+    average_method : string, optional (default: 'warn')
+        How to compute the normalizer in the denominator. Possible options
+        are 'min', 'geometric', 'arithmetic', and 'max'.
+        If 'warn', 'geometric' will be used. The default will change to
+        'arithmetic' in version 0.22.
+
+        .. versionadded:: 0.20
+
     Returns
     -------
     nmi : float
@@ -741,6 +810,7 @@ def normalized_mutual_info_score(labels_true, labels_pred):
 
     See also
     --------
+    v_measure_score: V-Measure (NMI with arithmetic mean option.)
     adjusted_rand_score: Adjusted Rand Index
     adjusted_mutual_info_score: Adjusted Mutual Information (adjusted
         against chance)
@@ -753,17 +823,26 @@ def normalized_mutual_info_score(labels_true, labels_pred):
 
       >>> from sklearn.metrics.cluster import normalized_mutual_info_score
       >>> normalized_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])
+      ... # doctest: +SKIP
       1.0
       >>> normalized_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])
+      ... # doctest: +SKIP
       1.0
 
     If classes members are completely split across different clusters,
     the assignment is totally in-complete, hence the NMI is null::
 
-      >>> normalized_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])
+      >>> normalized_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])i
+      ... # doctest: +SKIP
       0.0
 
     """
+    if average_method == 'warn':
+        warnings.warn("The behavior of NMI will change in version 0.22. "
+                      "To match the behavior of 'v_measure_score', NMI will "
+                      "use average_method='arithmetic' by default.",
+                      FutureWarning)
+        average_method = 'geometric'
     labels_true, labels_pred = check_clusterings(labels_true, labels_pred)
     classes = np.unique(labels_true)
     clusters = np.unique(labels_pred)
@@ -780,7 +859,10 @@ def normalized_mutual_info_score(labels_true, labels_pred):
     # Calculate the expected value for the mutual information
     # Calculate entropy for each labeling
     h_true, h_pred = entropy(labels_true), entropy(labels_pred)
-    nmi = mi / max(np.sqrt(h_true * h_pred), 1e-10)
+    normalizer = _generalized_average(h_true, h_pred, average_method)
+    # Avoid 0.0 / 0.0 when either entropy is zero.
+    normalizer = max(normalizer, np.finfo('float64').eps)
+    nmi = mi / normalizer
     return nmi
 
 
@@ -861,7 +943,13 @@ def fowlkes_mallows_score(labels_true, labels_pred, sparse=False):
 
 
 def entropy(labels):
-    """Calculates the entropy for a labeling."""
+    """Calculates the entropy for a labeling.
+
+    Parameters
+    ----------
+    labels : int array, shape = [n_samples]
+        The labels
+    """
     if len(labels) == 0:
         return 1.0
     label_idx = np.unique(labels, return_inverse=True)[1]
diff --git a/sklearn/metrics/cluster/tests/test_common.py b/sklearn/metrics/cluster/tests/test_common.py
index a7e54d22cc7c..c186a3ad5a4c 100644
--- a/sklearn/metrics/cluster/tests/test_common.py
+++ b/sklearn/metrics/cluster/tests/test_common.py
@@ -15,7 +15,7 @@
 from sklearn.metrics.cluster import calinski_harabaz_score
 from sklearn.metrics.cluster import davies_bouldin_score
 
-from sklearn.utils.testing import assert_allclose
+from sklearn.utils.testing import assert_allclose, ignore_warnings
 
 
 # Dictionaries of metrics
@@ -83,6 +83,8 @@ def test_symmetric_non_symmetric_union():
             sorted(SUPERVISED_METRICS))
 
 
+# 0.22 AMI and NMI changes
+@pytest.mark.filterwarnings('ignore::FutureWarning')
 @pytest.mark.parametrize(
     'metric_name, y1, y2',
     [(name, y1, y2) for name in SYMMETRIC_METRICS]
@@ -101,6 +103,8 @@ def test_non_symmetry(metric_name, y1, y2):
     assert metric(y1, y2) != pytest.approx(metric(y2, y1))
 
 
+# 0.22 AMI and NMI changes
+@pytest.mark.filterwarnings('ignore::FutureWarning')
 @pytest.mark.parametrize("metric_name", NORMALIZED_METRICS)
 def test_normalized_output(metric_name):
     upper_bound_1 = [0, 0, 0, 1, 1, 1]
@@ -119,13 +123,15 @@ def test_normalized_output(metric_name):
     assert not (score < 0).any()
 
 
-# All clustering metrics do not change score due to permutations of labels
-# that is when 0 and 1 exchanged.
+# 0.22 AMI and NMI changes
+@pytest.mark.filterwarnings('ignore::FutureWarning')
 @pytest.mark.parametrize(
     "metric_name",
     dict(SUPERVISED_METRICS, **UNSUPERVISED_METRICS)
 )
 def test_permute_labels(metric_name):
+    # All clustering metrics do not change score due to permutations of labels
+    # that is when 0 and 1 exchanged.
     y_label = np.array([0, 0, 0, 1, 1, 0, 1])
     y_pred = np.array([1, 0, 1, 0, 1, 1, 0])
     if metric_name in SUPERVISED_METRICS:
@@ -141,11 +147,13 @@ def test_permute_labels(metric_name):
         assert_allclose(score_1, metric(X, 1 - y_pred))
 
 
-# For all clustering metrics Input parameters can be both
+# 0.22 AMI and NMI changes
+@pytest.mark.filterwarnings('ignore::FutureWarning')
 @pytest.mark.parametrize(
     "metric_name",
     dict(SUPERVISED_METRICS, **UNSUPERVISED_METRICS)
 )
+# For all clustering metrics Input parameters can be both
 # in the form of arrays lists, positive, negetive or string
 def test_format_invariance(metric_name):
     y_true = [0, 0, 0, 0, 1, 1, 1, 1]
diff --git a/sklearn/metrics/cluster/tests/test_supervised.py b/sklearn/metrics/cluster/tests/test_supervised.py
index 8be39cd220d2..46b95cfd8fda 100644
--- a/sklearn/metrics/cluster/tests/test_supervised.py
+++ b/sklearn/metrics/cluster/tests/test_supervised.py
@@ -12,10 +12,12 @@
 from sklearn.metrics.cluster import mutual_info_score
 from sklearn.metrics.cluster import normalized_mutual_info_score
 from sklearn.metrics.cluster import v_measure_score
+from sklearn.metrics.cluster.supervised import _generalized_average
 
 from sklearn.utils import assert_all_finite
 from sklearn.utils.testing import (
         assert_equal, assert_almost_equal, assert_raise_message,
+        assert_warns_message, ignore_warnings
 )
 from numpy.testing import assert_array_almost_equal
 
@@ -30,6 +32,18 @@
 ]
 
 
+def test_future_warning():
+    score_funcs_with_changing_means = [
+        normalized_mutual_info_score,
+        adjusted_mutual_info_score,
+    ]
+    warning_msg = "The behavior of "
+    args = [0, 0, 0], [0, 0, 0]
+    for score_func in score_funcs_with_changing_means:
+        assert_warns_message(FutureWarning, warning_msg, score_func, *args)
+
+
+@ignore_warnings(category=FutureWarning)
 def test_error_messages_on_wrong_input():
     for score_func in score_funcs:
         expected = ('labels_true and labels_pred must have same size,'
@@ -46,6 +60,17 @@ def test_error_messages_on_wrong_input():
                              [0, 1, 0], [[1, 1], [0, 0]])
 
 
+def test_generalized_average():
+    a, b = 1, 2
+    methods = ["min", "geometric", "arithmetic", "max"]
+    means = [_generalized_average(a, b, method) for method in methods]
+    assert means[0] <= means[1] <= means[2] <= means[3]
+    c, d = 12, 12
+    means = [_generalized_average(c, d, method) for method in methods]
+    assert means[0] == means[1] == means[2] == means[3]
+
+
+@ignore_warnings(category=FutureWarning)
 def test_perfect_matches():
     for score_func in score_funcs:
         assert_equal(score_func([], []), 1.0)
@@ -55,6 +80,20 @@ def test_perfect_matches():
         assert_equal(score_func([0., 1., 0.], [42., 7., 42.]), 1.0)
         assert_equal(score_func([0., 1., 2.], [42., 7., 2.]), 1.0)
         assert_equal(score_func([0, 1, 2], [42, 7, 2]), 1.0)
+    score_funcs_with_changing_means = [
+        normalized_mutual_info_score,
+        adjusted_mutual_info_score,
+    ]
+    means = {"min", "geometric", "arithmetic", "max"}
+    for score_func in score_funcs_with_changing_means:
+        for mean in means:
+            assert score_func([], [], mean) == 1.0
+            assert score_func([0], [1], mean) == 1.0
+            assert score_func([0, 0, 0], [0, 0, 0], mean) == 1.0
+            assert score_func([0, 1, 0], [42, 7, 42], mean) == 1.0
+            assert score_func([0., 1., 0.], [42., 7., 42.], mean) == 1.0
+            assert score_func([0., 1., 2.], [42., 7., 2.], mean) == 1.0
+            assert score_func([0, 1, 2], [42, 7, 2], mean) == 1.0
 
 
 def test_homogeneous_but_not_complete_labeling():
@@ -87,7 +126,7 @@ def test_not_complete_and_not_homogeneous_labeling():
     assert_almost_equal(v, 0.52, 2)
 
 
-def test_non_consicutive_labels():
+def test_non_consecutive_labels():
     # regression tests for labels with gaps
     h, c, v = homogeneity_completeness_v_measure(
         [0, 0, 0, 2, 2, 2],
@@ -109,6 +148,7 @@ def test_non_consicutive_labels():
     assert_almost_equal(ari_2, 0.24, 2)
 
 
+@ignore_warnings(category=FutureWarning)
 def uniform_labelings_scores(score_func, n_samples, k_range, n_runs=10,
                              seed=42):
     # Compute score for random uniform cluster labelings
@@ -122,6 +162,7 @@ def uniform_labelings_scores(score_func, n_samples, k_range, n_runs=10,
     return scores
 
 
+@ignore_warnings(category=FutureWarning)
 def test_adjustment_for_chance():
     # Check that adjusted scores are almost zero on random labels
     n_clusters_range = [2, 10, 50, 90]
@@ -135,6 +176,7 @@ def test_adjustment_for_chance():
     assert_array_almost_equal(max_abs_scores, [0.02, 0.03, 0.03, 0.02], 2)
 
 
+@ignore_warnings(category=FutureWarning)
 def test_adjusted_mutual_info_score():
     # Compute the Adjusted Mutual Information and test against known values
     labels_a = np.array([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3])
@@ -215,6 +257,7 @@ def test_contingency_matrix_sparse():
                                     eps=1e-10, sparse=True)
 
 
+@ignore_warnings(category=FutureWarning)
 def test_exactly_zero_info_score():
     # Check numerical stability when information is exactly zero
     for i in np.logspace(1, 4, 4).astype(np.int):
@@ -224,6 +267,11 @@ def test_exactly_zero_info_score():
         assert_equal(v_measure_score(labels_a, labels_b), 0.0)
         assert_equal(adjusted_mutual_info_score(labels_a, labels_b), 0.0)
         assert_equal(normalized_mutual_info_score(labels_a, labels_b), 0.0)
+        for method in ["min", "geometric", "arithmetic", "max"]:
+            assert adjusted_mutual_info_score(labels_a, labels_b,
+                                              method) == 0.0
+            assert normalized_mutual_info_score(labels_a, labels_b,
+                                                method) == 0.0
 
 
 def test_v_measure_and_mutual_information(seed=36):
@@ -235,6 +283,11 @@ def test_v_measure_and_mutual_information(seed=36):
         assert_almost_equal(v_measure_score(labels_a, labels_b),
                             2.0 * mutual_info_score(labels_a, labels_b) /
                             (entropy(labels_a) + entropy(labels_b)), 0)
+        avg = 'arithmetic'
+        assert_almost_equal(v_measure_score(labels_a, labels_b),
+                            normalized_mutual_info_score(labels_a, labels_b,
+                                                         average_method=avg)
+                            )
 
 
 def test_fowlkes_mallows_score():
diff --git a/sklearn/metrics/cluster/unsupervised.py b/sklearn/metrics/cluster/unsupervised.py
index 7c954acea518..4e34cd6cab70 100644
--- a/sklearn/metrics/cluster/unsupervised.py
+++ b/sklearn/metrics/cluster/unsupervised.py
@@ -20,6 +20,16 @@
 
 
 def check_number_of_labels(n_labels, n_samples):
+    """Check that number of labels are valid.
+
+    Parameters
+    ----------
+    n_labels : int
+        Number of labels
+
+    n_samples : int
+        Number of samples
+    """
     if not 1 < n_labels < n_samples:
         raise ValueError("Number of labels is %d. Valid values are 2 "
                          "to n_samples - 1 (inclusive)" % n_labels)
diff --git a/sklearn/metrics/pairwise.py b/sklearn/metrics/pairwise.py
index b4928ed7492f..2e56255af001 100644
--- a/sklearn/metrics/pairwise.py
+++ b/sklearn/metrics/pairwise.py
@@ -24,9 +24,9 @@
 from ..utils import gen_batches, get_chunk_n_rows
 from ..utils.extmath import row_norms, safe_sparse_dot
 from ..preprocessing import normalize
-from ..externals.joblib import Parallel
-from ..externals.joblib import delayed
-from ..externals.joblib import cpu_count
+from ..utils import Parallel
+from ..utils import delayed
+from ..utils import effective_n_jobs
 
 from .pairwise_fast import _chi2_kernel_fast, _sparse_manhattan
 
@@ -491,7 +491,7 @@ def manhattan_distances(X, Y=None, sum_over_features=True,
            [4., 4.]])
     >>> import numpy as np
     >>> X = np.ones((1, 2))
-    >>> y = 2 * np.ones((2, 2))
+    >>> y = np.full((2, 2), 2.)
     >>> manhattan_distances(X, y, sum_over_features=False)#doctest:+ELLIPSIS
     array([[1., 1.],
            [1., 1.]])
@@ -1061,21 +1061,18 @@ def distance_metrics():
 def _parallel_pairwise(X, Y, func, n_jobs, **kwds):
     """Break the pairwise matrix in n_jobs even slices
     and compute them in parallel"""
-    if n_jobs < 0:
-        n_jobs = max(cpu_count() + 1 + n_jobs, 1)
 
     if Y is None:
         Y = X
 
-    if n_jobs == 1:
-        # Special case to avoid picklability checks in delayed
+    if effective_n_jobs(n_jobs) == 1:
         return func(X, Y, **kwds)
 
     # TODO: in some cases, backend='threading' may be appropriate
     fd = delayed(func)
     ret = Parallel(n_jobs=n_jobs, verbose=0)(
         fd(X, Y[s], **kwds)
-        for s in gen_even_slices(Y.shape[0], n_jobs))
+        for s in gen_even_slices(_num_samples(Y), effective_n_jobs(n_jobs)))
 
     return np.hstack(ret)
 
@@ -1142,7 +1139,7 @@ def _check_chunk_size(reduced, chunk_size):
 
 
 def pairwise_distances_chunked(X, Y=None, reduce_func=None,
-                               metric='euclidean', n_jobs=1,
+                               metric='euclidean', n_jobs=None,
                                working_memory=None, **kwds):
     """Generate a distance matrix chunk by chunk with optional reduction
 
@@ -1184,15 +1181,14 @@ def pairwise_distances_chunked(X, Y=None, reduce_func=None,
         should take two arrays from X as input and return a value indicating
         the distance between them.
 
-    n_jobs : int
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. This works by breaking
         down the pairwise matrix into n_jobs even slices and computing them in
         parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For n_jobs below -1,
-        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
-        are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     working_memory : int, optional
         The sought maximum memory for temporary distance matrix chunks.
@@ -1291,7 +1287,7 @@ def pairwise_distances_chunked(X, Y=None, reduce_func=None,
         yield D_chunk
 
 
-def pairwise_distances(X, Y=None, metric="euclidean", n_jobs=1, **kwds):
+def pairwise_distances(X, Y=None, metric="euclidean", n_jobs=None, **kwds):
     """ Compute the distance matrix from a vector array X and optional Y.
 
     This method takes either a vector array or a distance matrix, and returns
@@ -1347,15 +1343,14 @@ def pairwise_distances(X, Y=None, metric="euclidean", n_jobs=1, **kwds):
         should take two arrays from X as input and return a value indicating
         the distance between them.
 
-    n_jobs : int
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. This works by breaking
         down the pairwise matrix into n_jobs even slices and computing them in
         parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For n_jobs below -1,
-        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
-        are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     **kwds : optional keyword parameters
         Any further parameters are passed directly to the distance function.
@@ -1372,9 +1367,9 @@ def pairwise_distances(X, Y=None, metric="euclidean", n_jobs=1, **kwds):
 
     See also
     --------
-    pairwise_distances_chunked : performs the same calculation as this funtion,
-        but returns a generator of chunks of the distance matrix, in order to
-        limit memory usage.
+    pairwise_distances_chunked : performs the same calculation as this
+        function, but returns a generator of chunks of the distance matrix, in
+        order to limit memory usage.
     paired_distances : Computes the distances between corresponding
                        elements of two arrays
     """
@@ -1397,10 +1392,9 @@ def pairwise_distances(X, Y=None, metric="euclidean", n_jobs=1, **kwds):
                             " support sparse matrices.")
 
         dtype = bool if metric in PAIRWISE_BOOLEAN_FUNCTIONS else None
-
         X, Y = check_pairwise_arrays(X, Y, dtype=dtype)
 
-        if n_jobs == 1 and X is Y:
+        if effective_n_jobs(n_jobs) == 1 and X is Y:
             return distance.squareform(distance.pdist(X, metric=metric,
                                                       **kwds))
         func = partial(distance.cdist, metric=metric, **kwds)
@@ -1478,7 +1472,7 @@ def kernel_metrics():
 
 
 def pairwise_kernels(X, Y=None, metric="linear", filter_params=False,
-                     n_jobs=1, **kwds):
+                     n_jobs=None, **kwds):
     """Compute the kernel between arrays X and optional array Y.
 
     This method takes either a vector array or a kernel matrix, and returns
@@ -1519,15 +1513,14 @@ def pairwise_kernels(X, Y=None, metric="linear", filter_params=False,
     filter_params : boolean
         Whether to filter invalid parameters or not.
 
-    n_jobs : int
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. This works by breaking
         down the pairwise matrix into n_jobs even slices and computing them in
         parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For n_jobs below -1,
-        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
-        are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     **kwds : optional keyword parameters
         Any further parameters are passed directly to the kernel function.
diff --git a/sklearn/metrics/ranking.py b/sklearn/metrics/ranking.py
index 5039c5f874a5..2037f4237478 100644
--- a/sklearn/metrics/ranking.py
+++ b/sklearn/metrics/ranking.py
@@ -20,6 +20,8 @@
 from __future__ import division
 
 import warnings
+from functools import partial
+
 import numpy as np
 from scipy.sparse import csr_matrix
 from scipy.stats import rankdata
@@ -125,7 +127,7 @@ def auc(x, y, reorder='deprecated'):
     return area
 
 
-def average_precision_score(y_true, y_score, average="macro",
+def average_precision_score(y_true, y_score, average="macro", pos_label=1,
                             sample_weight=None):
     """Compute average precision (AP) from prediction scores
 
@@ -150,7 +152,7 @@ def average_precision_score(y_true, y_score, average="macro",
     Parameters
     ----------
     y_true : array, shape = [n_samples] or [n_samples, n_classes]
-        True binary labels (either {0, 1} or {-1, 1}).
+        True binary labels or binary label indicators.
 
     y_score : array, shape = [n_samples] or [n_samples, n_classes]
         Target scores, can either be probability estimates of the positive
@@ -173,6 +175,12 @@ def average_precision_score(y_true, y_score, average="macro",
         ``'samples'``:
             Calculate metrics for each instance, and find their average.
 
+        Will be ignored when ``y_true`` is binary.
+
+    pos_label : int or str (default=1)
+        The label of the positive class. Only applied to binary ``y_true``.
+        For multilabel-indicator ``y_true``, ``pos_label`` is fixed to 1.
+
     sample_weight : array-like of shape = [n_samples], optional
         Sample weights.
 
@@ -209,17 +217,23 @@ def average_precision_score(y_true, y_score, average="macro",
       are weighted by the change in recall since the last operating point.
     """
     def _binary_uninterpolated_average_precision(
-            y_true, y_score, sample_weight=None):
+            y_true, y_score, pos_label=1, sample_weight=None):
         precision, recall, _ = precision_recall_curve(
-            y_true, y_score, sample_weight=sample_weight)
+            y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)
         # Return the step function integral
         # The following works because the last entry of precision is
         # guaranteed to be 1, as returned by precision_recall_curve
         return -np.sum(np.diff(recall) * np.array(precision)[:-1])
 
-    return _average_binary_score(_binary_uninterpolated_average_precision,
-                                 y_true, y_score, average,
-                                 sample_weight=sample_weight)
+    y_type = type_of_target(y_true)
+    if y_type == "multilabel-indicator" and pos_label != 1:
+        raise ValueError("Parameter pos_label is fixed to 1 for "
+                         "multilabel-indicator y_true. Do not set "
+                         "pos_label or set pos_label to 1.")
+    average_precision = partial(_binary_uninterpolated_average_precision,
+                                pos_label=pos_label)
+    return _average_binary_score(average_precision, y_true, y_score,
+                                 average, sample_weight=sample_weight)
 
 
 def roc_auc_score(y_true, y_score, average="macro", sample_weight=None,
@@ -260,6 +274,8 @@ def roc_auc_score(y_true, y_score, average="macro", sample_weight=None,
         ``'samples'``:
             Calculate metrics for each instance, and find their average.
 
+        Will be ignored when ``y_true`` is binary.
+
     sample_weight : array-like of shape = [n_samples], optional
         Sample weights.
 
@@ -501,6 +517,7 @@ def precision_recall_curve(y_true, probas_pred, pos_label=None,
                                              sample_weight=sample_weight)
 
     precision = tps / (tps + fps)
+    precision[np.isnan(precision)] = 0
     recall = tps / tps[-1]
 
     # stop when full recall attained
diff --git a/sklearn/metrics/regression.py b/sklearn/metrics/regression.py
index 4bc88561a73f..e9084a4276e1 100644
--- a/sklearn/metrics/regression.py
+++ b/sklearn/metrics/regression.py
@@ -314,7 +314,7 @@ def mean_squared_log_error(y_true, y_pred,
         raise ValueError("Mean Squared Logarithmic Error cannot be used when "
                          "targets contain negative values.")
 
-    return mean_squared_error(np.log(y_true + 1), np.log(y_pred + 1),
+    return mean_squared_error(np.log1p(y_true), np.log1p(y_pred),
                               sample_weight, multioutput)
 
 
diff --git a/sklearn/metrics/scorer.py b/sklearn/metrics/scorer.py
index 590b9826b4ef..2661a379b4e5 100644
--- a/sklearn/metrics/scorer.py
+++ b/sklearn/metrics/scorer.py
@@ -19,7 +19,6 @@
 # License: Simplified BSD
 
 from abc import ABCMeta
-from collections import Iterable
 
 import numpy as np
 
@@ -40,6 +39,7 @@
 from .cluster import fowlkes_mallows_score
 
 from ..utils.multiclass import type_of_target
+from ..utils.fixes import _Iterable as Iterable
 from ..externals import six
 from ..base import is_regressor
 
@@ -216,8 +216,8 @@ def get_scorer(scoring):
             scorer = SCORERS[scoring]
         except KeyError:
             raise ValueError('%r is not a valid scoring value. '
-                             'Valid options are %s'
-                             % (scoring, sorted(SCORERS.keys())))
+                             'Use sorted(sklearn.metrics.SCORERS.keys()) '
+                             'to get valid options.' % (scoring))
     else:
         scorer = scoring
     return scorer
diff --git a/sklearn/metrics/tests/test_classification.py b/sklearn/metrics/tests/test_classification.py
index be4545eb0df1..3a91a612631d 100644
--- a/sklearn/metrics/tests/test_classification.py
+++ b/sklearn/metrics/tests/test_classification.py
@@ -1,11 +1,11 @@
 from __future__ import division, print_function
 
-import numpy as np
-from scipy import linalg
 from functools import partial
 from itertools import product
 import warnings
 
+import numpy as np
+from scipy import linalg
 import pytest
 
 from sklearn import datasets
@@ -31,6 +31,7 @@
 
 from sklearn.metrics import accuracy_score
 from sklearn.metrics import average_precision_score
+from sklearn.metrics import balanced_accuracy_score
 from sklearn.metrics import classification_report
 from sklearn.metrics import cohen_kappa_score
 from sklearn.metrics import confusion_matrix
@@ -122,16 +123,28 @@ def test_classification_report_dictionary_output():
                                      'recall': 0.90000000000000002,
                                      'f1-score': 0.57142857142857151,
                                      'support': 20},
-                       'avg / total': {'precision': 0.51375351084147847,
-                                       'recall': 0.53333333333333333,
-                                       'f1-score': 0.47310435663627154,
-                                       'support': 75}}
+                       'macro avg': {'f1-score': 0.5099797365754813,
+                                     'precision': 0.5260083136726211,
+                                     'recall': 0.596146953405018,
+                                     'support': 75},
+                       'micro avg': {'f1-score': 0.5333333333333333,
+                                     'precision': 0.5333333333333333,
+                                     'recall': 0.5333333333333333,
+                                     'support': 75},
+                       'weighted avg': {'f1-score': 0.47310435663627154,
+                                        'precision': 0.5137535108414785,
+                                        'recall': 0.5333333333333333,
+                                        'support': 75}}
 
     report = classification_report(
         y_true, y_pred, labels=np.arange(len(iris.target_names)),
         target_names=iris.target_names, output_dict=True)
 
     assert_dict_equal(report, expected_report)
+    assert type(expected_report['setosa']['precision']) == float
+    assert type(expected_report['macro avg']['precision']) == float
+    assert type(expected_report['setosa']['support']) == int
+    assert type(expected_report['macro avg']['support']) == int
 
 
 def test_multilabel_accuracy_score_subset_accuracy():
@@ -708,13 +721,13 @@ def test_confusion_matrix_dtype():
         assert_equal(cm.dtype, np.float64)
 
     # np.iinfo(np.uint32).max should be accumulated correctly
-    weight = np.ones(len(y), dtype=np.uint32) * 4294967295
+    weight = np.full(len(y), 4294967295, dtype=np.uint32)
     cm = confusion_matrix(y, y, sample_weight=weight)
     assert_equal(cm[0, 0], 4294967295)
     assert_equal(cm[1, 1], 8589934590)
 
     # np.iinfo(np.int64).max should cause an overflow
-    weight = np.ones(len(y), dtype=np.int64) * 9223372036854775807
+    weight = np.full(len(y), 9223372036854775807, dtype=np.int64)
     cm = confusion_matrix(y, y, sample_weight=weight)
     assert_equal(cm[0, 0], 9223372036854775807)
     assert_equal(cm[1, 1], -2)
@@ -727,27 +740,55 @@ def test_classification_report_multiclass():
 
     # print classification report with class names
     expected_report = """\
-             precision    recall  f1-score   support
+              precision    recall  f1-score   support
 
-     setosa       0.83      0.79      0.81        24
- versicolor       0.33      0.10      0.15        31
-  virginica       0.42      0.90      0.57        20
+      setosa       0.83      0.79      0.81        24
+  versicolor       0.33      0.10      0.15        31
+   virginica       0.42      0.90      0.57        20
 
-avg / total       0.51      0.53      0.47        75
+   micro avg       0.53      0.53      0.53        75
+   macro avg       0.53      0.60      0.51        75
+weighted avg       0.51      0.53      0.47        75
 """
     report = classification_report(
         y_true, y_pred, labels=np.arange(len(iris.target_names)),
         target_names=iris.target_names)
     assert_equal(report, expected_report)
+
+
+def test_classification_report_multiclass_balanced():
+    y_true, y_pred = [0, 0, 0, 1, 1, 1, 2, 2, 2], [0, 1, 2, 0, 1, 2, 0, 1, 2]
+
+    expected_report = """\
+              precision    recall  f1-score   support
+
+           0       0.33      0.33      0.33         3
+           1       0.33      0.33      0.33         3
+           2       0.33      0.33      0.33         3
+
+   micro avg       0.33      0.33      0.33         9
+   macro avg       0.33      0.33      0.33         9
+weighted avg       0.33      0.33      0.33         9
+"""
+    report = classification_report(y_true, y_pred)
+    assert_equal(report, expected_report)
+
+
+def test_classification_report_multiclass_with_label_detection():
+    iris = datasets.load_iris()
+    y_true, y_pred, _ = make_prediction(dataset=iris, binary=False)
+
     # print classification report with label detection
     expected_report = """\
-             precision    recall  f1-score   support
+              precision    recall  f1-score   support
 
-          0       0.83      0.79      0.81        24
-          1       0.33      0.10      0.15        31
-          2       0.42      0.90      0.57        20
+           0       0.83      0.79      0.81        24
+           1       0.33      0.10      0.15        31
+           2       0.42      0.90      0.57        20
 
-avg / total       0.51      0.53      0.47        75
+   micro avg       0.53      0.53      0.53        75
+   macro avg       0.53      0.60      0.51        75
+weighted avg       0.51      0.53      0.47        75
 """
     report = classification_report(y_true, y_pred)
     assert_equal(report, expected_report)
@@ -760,30 +801,20 @@ def test_classification_report_multiclass_with_digits():
 
     # print classification report with class names
     expected_report = """\
-             precision    recall  f1-score   support
+              precision    recall  f1-score   support
 
-     setosa    0.82609   0.79167   0.80851        24
- versicolor    0.33333   0.09677   0.15000        31
-  virginica    0.41860   0.90000   0.57143        20
+      setosa    0.82609   0.79167   0.80851        24
+  versicolor    0.33333   0.09677   0.15000        31
+   virginica    0.41860   0.90000   0.57143        20
 
-avg / total    0.51375   0.53333   0.47310        75
+   micro avg    0.53333   0.53333   0.53333        75
+   macro avg    0.52601   0.59615   0.50998        75
+weighted avg    0.51375   0.53333   0.47310        75
 """
     report = classification_report(
         y_true, y_pred, labels=np.arange(len(iris.target_names)),
         target_names=iris.target_names, digits=5)
     assert_equal(report, expected_report)
-    # print classification report with label detection
-    expected_report = """\
-             precision    recall  f1-score   support
-
-          0       0.83      0.79      0.81        24
-          1       0.33      0.10      0.15        31
-          2       0.42      0.90      0.57        20
-
-avg / total       0.51      0.53      0.47        75
-"""
-    report = classification_report(y_true, y_pred)
-    assert_equal(report, expected_report)
 
 
 def test_classification_report_multiclass_with_string_label():
@@ -793,25 +824,29 @@ def test_classification_report_multiclass_with_string_label():
     y_pred = np.array(["blue", "green", "red"])[y_pred]
 
     expected_report = """\
-             precision    recall  f1-score   support
+              precision    recall  f1-score   support
 
-       blue       0.83      0.79      0.81        24
-      green       0.33      0.10      0.15        31
-        red       0.42      0.90      0.57        20
+        blue       0.83      0.79      0.81        24
+       green       0.33      0.10      0.15        31
+         red       0.42      0.90      0.57        20
 
-avg / total       0.51      0.53      0.47        75
+   micro avg       0.53      0.53      0.53        75
+   macro avg       0.53      0.60      0.51        75
+weighted avg       0.51      0.53      0.47        75
 """
     report = classification_report(y_true, y_pred)
     assert_equal(report, expected_report)
 
     expected_report = """\
-             precision    recall  f1-score   support
+              precision    recall  f1-score   support
 
-          a       0.83      0.79      0.81        24
-          b       0.33      0.10      0.15        31
-          c       0.42      0.90      0.57        20
+           a       0.83      0.79      0.81        24
+           b       0.33      0.10      0.15        31
+           c       0.42      0.90      0.57        20
 
-avg / total       0.51      0.53      0.47        75
+   micro avg       0.53      0.53      0.53        75
+   macro avg       0.53      0.60      0.51        75
+weighted avg       0.51      0.53      0.47        75
 """
     report = classification_report(y_true, y_pred,
                                    target_names=["a", "b", "c"])
@@ -826,13 +861,15 @@ def test_classification_report_multiclass_with_unicode_label():
     y_pred = labels[y_pred]
 
     expected_report = u"""\
-             precision    recall  f1-score   support
+              precision    recall  f1-score   support
 
-      blue\xa2       0.83      0.79      0.81        24
-     green\xa2       0.33      0.10      0.15        31
-       red\xa2       0.42      0.90      0.57        20
+       blue\xa2       0.83      0.79      0.81        24
+      green\xa2       0.33      0.10      0.15        31
+        red\xa2       0.42      0.90      0.57        20
 
-avg / total       0.51      0.53      0.47        75
+   micro avg       0.53      0.53      0.53        75
+   macro avg       0.53      0.60      0.51        75
+weighted avg       0.51      0.53      0.47        75
 """
     report = classification_report(y_true, y_pred)
     assert_equal(report, expected_report)
@@ -841,7 +878,7 @@ def test_classification_report_multiclass_with_unicode_label():
 def test_classification_report_multiclass_with_long_string_label():
     y_true, y_pred, _ = make_prediction(binary=False)
 
-    labels = np.array(["blue", "green"*5, "red"])
+    labels = np.array(["blue", "green" * 5, "red"])
     y_true = labels[y_true]
     y_pred = labels[y_pred]
 
@@ -852,7 +889,9 @@ def test_classification_report_multiclass_with_long_string_label():
 greengreengreengreengreen       0.33      0.10      0.15        31
                       red       0.42      0.90      0.57        20
 
-              avg / total       0.51      0.53      0.47        75
+                micro avg       0.53      0.53      0.53        75
+                macro avg       0.53      0.60      0.51        75
+             weighted avg       0.51      0.53      0.47        75
 """
 
     report = classification_report(y_true, y_pred)
@@ -900,14 +939,17 @@ def test_multilabel_classification_report():
                                                random_state=1)
 
     expected_report = """\
-             precision    recall  f1-score   support
+              precision    recall  f1-score   support
 
-          0       0.50      0.67      0.57        24
-          1       0.51      0.74      0.61        27
-          2       0.29      0.08      0.12        26
-          3       0.52      0.56      0.54        27
+           0       0.50      0.67      0.57        24
+           1       0.51      0.74      0.61        27
+           2       0.29      0.08      0.12        26
+           3       0.52      0.56      0.54        27
 
-avg / total       0.45      0.51      0.46       104
+   micro avg       0.50      0.51      0.50       104
+   macro avg       0.45      0.51      0.46       104
+weighted avg       0.45      0.51      0.46       104
+ samples avg       0.46      0.42      0.40       104
 """
 
     report = classification_report(y_true, y_pred)
@@ -1460,7 +1502,7 @@ def test_hinge_loss_multiclass():
         1 - pred_decision[4][3] + pred_decision[4][2],
         1 - pred_decision[5][2] + pred_decision[5][3]
     ])
-    dummy_losses[dummy_losses <= 0] = 0
+    np.clip(dummy_losses, 0, None, out=dummy_losses)
     dummy_hinge_loss = np.mean(dummy_losses)
     assert_equal(hinge_loss(y_true, pred_decision),
                  dummy_hinge_loss)
@@ -1498,7 +1540,7 @@ def test_hinge_loss_multiclass_with_missing_labels():
         1 - pred_decision[3][1] + pred_decision[3][2],
         1 - pred_decision[4][2] + pred_decision[4][3]
     ])
-    dummy_losses[dummy_losses <= 0] = 0
+    np.clip(dummy_losses, 0, None, out=dummy_losses)
     dummy_hinge_loss = np.mean(dummy_losses)
     assert_equal(hinge_loss(y_true, pred_decision, labels=labels),
                  dummy_hinge_loss)
@@ -1525,7 +1567,7 @@ def test_hinge_loss_multiclass_invariance_lists():
         1 - pred_decision[4][3] + pred_decision[4][2],
         1 - pred_decision[5][2] + pred_decision[5][3]
     ])
-    dummy_losses[dummy_losses <= 0] = 0
+    np.clip(dummy_losses, 0, None, out=dummy_losses)
     dummy_hinge_loss = np.mean(dummy_losses)
     assert_equal(hinge_loss(y_true, pred_decision),
                  dummy_hinge_loss)
@@ -1630,3 +1672,26 @@ def test_brier_score_loss():
     # calculate even if only single class in y_true (#6980)
     assert_almost_equal(brier_score_loss([0], [0.5]), 0.25)
     assert_almost_equal(brier_score_loss([1], [0.5]), 0.25)
+
+
+def test_balanced_accuracy_score_unseen():
+    assert_warns_message(UserWarning, 'y_pred contains classes not in y_true',
+                         balanced_accuracy_score, [0, 0, 0], [0, 0, 1])
+
+
+@pytest.mark.parametrize('y_true,y_pred',
+                         [
+                             (['a', 'b', 'a', 'b'], ['a', 'a', 'a', 'b']),
+                             (['a', 'b', 'c', 'b'], ['a', 'a', 'a', 'b']),
+                             (['a', 'a', 'a', 'b'], ['a', 'b', 'c', 'b']),
+                         ])
+def test_balanced_accuracy_score(y_true, y_pred):
+    macro_recall = recall_score(y_true, y_pred, average='macro',
+                                labels=np.unique(y_true))
+    with ignore_warnings():
+        # Warnings are tested in test_balanced_accuracy_score_unseen
+        balanced = balanced_accuracy_score(y_true, y_pred)
+    assert balanced == pytest.approx(macro_recall)
+    adjusted = balanced_accuracy_score(y_true, y_pred, adjusted=True)
+    chance = balanced_accuracy_score(y_true, np.full_like(y_true, y_true[0]))
+    assert adjusted == (balanced - chance) / (1 - chance)
diff --git a/sklearn/metrics/tests/test_common.py b/sklearn/metrics/tests/test_common.py
index b858868a7454..16e4f5d4c76d 100644
--- a/sklearn/metrics/tests/test_common.py
+++ b/sklearn/metrics/tests/test_common.py
@@ -99,6 +99,8 @@
 CLASSIFICATION_METRICS = {
     "accuracy_score": accuracy_score,
     "balanced_accuracy_score": balanced_accuracy_score,
+    "adjusted_balanced_accuracy_score": partial(balanced_accuracy_score,
+                                                adjusted=True),
     "unnormalized_accuracy_score": partial(accuracy_score, normalize=False),
 
     # `confusion_matrix` returns absolute values and hence behaves unnormalized
@@ -198,22 +200,20 @@ def precision_recall_curve_padded_thresholds(*args, **kwargs):
 
     "brier_score_loss": brier_score_loss,
 
-    "roc_auc_score": roc_auc_score,
+    "roc_auc_score": roc_auc_score,  # default: average="macro"
     "weighted_roc_auc": partial(roc_auc_score, average="weighted"),
     "samples_roc_auc": partial(roc_auc_score, average="samples"),
     "micro_roc_auc": partial(roc_auc_score, average="micro"),
-    "macro_roc_auc": partial(roc_auc_score, average="macro"),
     "partial_roc_auc": partial(roc_auc_score, max_fpr=0.5),
 
-    "average_precision_score": average_precision_score,
+    "average_precision_score":
+    average_precision_score,  # default: average="macro"
     "weighted_average_precision_score":
     partial(average_precision_score, average="weighted"),
     "samples_average_precision_score":
     partial(average_precision_score, average="samples"),
     "micro_average_precision_score":
     partial(average_precision_score, average="micro"),
-    "macro_average_precision_score":
-    partial(average_precision_score, average="macro"),
     "label_ranking_average_precision_score":
     label_ranking_average_precision_score,
 }
@@ -241,13 +241,6 @@ def precision_recall_curve_padded_thresholds(*args, **kwargs):
     "samples_precision_score",
     "samples_recall_score",
     "coverage_error",
-
-    "average_precision_score",
-    "weighted_average_precision_score",
-    "micro_average_precision_score",
-    "macro_average_precision_score",
-    "samples_average_precision_score",
-
     "label_ranking_loss",
     "label_ranking_average_precision_score",
 }
@@ -255,15 +248,18 @@ def precision_recall_curve_padded_thresholds(*args, **kwargs):
 # Those metrics don't support multiclass inputs
 METRIC_UNDEFINED_MULTICLASS = {
     "brier_score_loss",
-    "balanced_accuracy_score",
 
     "roc_auc_score",
     "micro_roc_auc",
     "weighted_roc_auc",
-    "macro_roc_auc",
     "samples_roc_auc",
     "partial_roc_auc",
 
+    "average_precision_score",
+    "weighted_average_precision_score",
+    "micro_average_precision_score",
+    "samples_average_precision_score",
+
     # with default average='binary', multiclass is prohibited
     "precision_score",
     "recall_score",
@@ -299,6 +295,11 @@ def precision_recall_curve_padded_thresholds(*args, **kwargs):
 
     "precision_score", "recall_score", "f1_score", "f2_score", "f0.5_score",
 
+    "average_precision_score",
+    "weighted_average_precision_score",
+    "micro_average_precision_score",
+    "samples_average_precision_score",
+
     # pos_label support deprecated; to be removed in 0.18:
     "weighted_f0.5_score", "weighted_f1_score", "weighted_f2_score",
     "weighted_precision_score", "weighted_recall_score",
@@ -348,11 +349,10 @@ def precision_recall_curve_padded_thresholds(*args, **kwargs):
     "unnormalized_log_loss",
 
     "roc_auc_score", "weighted_roc_auc", "samples_roc_auc",
-    "micro_roc_auc", "macro_roc_auc", "partial_roc_auc",
+    "micro_roc_auc", "partial_roc_auc",
 
     "average_precision_score", "weighted_average_precision_score",
     "samples_average_precision_score", "micro_average_precision_score",
-    "macro_average_precision_score",
 
     "coverage_error", "label_ranking_loss",
     "label_ranking_average_precision_score",
@@ -408,6 +408,7 @@ def precision_recall_curve_padded_thresholds(*args, **kwargs):
 # metric(y_true, y_pred) != metric(y_pred, y_true).
 NOT_SYMMETRIC_METRICS = {
     "balanced_accuracy_score",
+    "adjusted_balanced_accuracy_score",
     "explained_variance_score",
     "r2_score",
     "unnormalized_confusion_matrix",
@@ -667,7 +668,7 @@ def test_thresholded_invariance_string_vs_numbers_labels(name):
                                err_msg="{0} failed string vs number "
                                        "invariance test".format(name))
 
-            measure_with_strobj = metric(y1_str.astype('O'), y2)
+            measure_with_strobj = metric_str(y1_str.astype('O'), y2)
             assert_array_equal(measure_with_number, measure_with_strobj,
                                err_msg="{0} failed string object vs number "
                                        "invariance test".format(name))
diff --git a/sklearn/metrics/tests/test_ranking.py b/sklearn/metrics/tests/test_ranking.py
index 5e9a8a0c847a..b921fb1124ae 100644
--- a/sklearn/metrics/tests/test_ranking.py
+++ b/sklearn/metrics/tests/test_ranking.py
@@ -276,8 +276,7 @@ def test_roc_curve_one_label():
     w = UndefinedMetricWarning
     fpr, tpr, thresholds = assert_warns(w, roc_curve, y_true, y_pred)
     # all true labels, all fpr should be nan
-    assert_array_equal(fpr,
-                       np.nan * np.ones(len(thresholds)))
+    assert_array_equal(fpr, np.full(len(thresholds), np.nan))
     assert_equal(fpr.shape, tpr.shape)
     assert_equal(fpr.shape, thresholds.shape)
 
@@ -286,8 +285,7 @@ def test_roc_curve_one_label():
                                         [1 - x for x in y_true],
                                         y_pred)
     # all negative labels, all tpr should be nan
-    assert_array_equal(tpr,
-                       np.nan * np.ones(len(thresholds)))
+    assert_array_equal(tpr, np.full(len(thresholds), np.nan))
     assert_equal(fpr.shape, tpr.shape)
     assert_equal(fpr.shape, thresholds.shape)
 
@@ -430,6 +428,7 @@ def test_auc():
     assert_array_almost_equal(auc(x, y), 0.5)
 
 
+@pytest.mark.filterwarnings("ignore: The 'reorder' parameter")  # 0.22
 def test_auc_duplicate_values():
     # Test Area Under Curve (AUC) computation with duplicate values
 
@@ -437,6 +436,8 @@ def test_auc_duplicate_values():
     # from numpy.argsort(x), which was reordering the tied 0's in this example
     # and resulting in an incorrect area computation. This test detects the
     # error.
+
+    # This will not work again in the future! so regression?
     x = [-2.0, 0.0, 0.0, 0.0, 1.0]
     y1 = [2.0, 0.0, 0.5, 1.0, 1.0]
     y2 = [2.0, 1.0, 0.0, 0.5, 1.0]
@@ -482,7 +483,7 @@ def test_auc_score_non_binary_class():
     y_true = np.ones(10, dtype="int")
     assert_raise_message(ValueError, "ROC AUC score is not defined",
                          roc_auc_score, y_true, y_pred)
-    y_true = -np.ones(10, dtype="int")
+    y_true = np.full(10, -1, dtype="int")
     assert_raise_message(ValueError, "ROC AUC score is not defined",
                          roc_auc_score, y_true, y_pred)
     # y_true contains three different class values
@@ -501,7 +502,7 @@ def test_auc_score_non_binary_class():
         y_true = np.ones(10, dtype="int")
         assert_raise_message(ValueError, "ROC AUC score is not defined",
                              roc_auc_score, y_true, y_pred)
-        y_true = -np.ones(10, dtype="int")
+        y_true = np.full(10, -1, dtype="int")
         assert_raise_message(ValueError, "ROC AUC score is not defined",
                              roc_auc_score, y_true, y_pred)
 
@@ -681,6 +682,18 @@ def test_average_precision_constant_values():
     assert_equal(average_precision_score(y_true, y_score), .25)
 
 
+def test_average_precision_score_pos_label_multilabel_indicator():
+    # Raise an error for multilabel-indicator y_true with
+    # pos_label other than 1
+    y_true = np.array([[1, 0], [0, 1], [0, 1], [1, 0]])
+    y_pred = np.array([[0.9, 0.1], [0.1, 0.9], [0.8, 0.2], [0.2, 0.8]])
+    erorr_message = ("Parameter pos_label is fixed to 1 for multilabel"
+                     "-indicator y_true. Do not set pos_label or set "
+                     "pos_label to 1.")
+    assert_raise_message(ValueError, erorr_message, average_precision_score,
+                         y_true, y_pred, pos_label=0)
+
+
 def test_score_scale_invariance():
     # Test that average_precision_score and roc_auc_score are invariant by
     # the scaling or shifting of probabilities
diff --git a/sklearn/metrics/tests/test_score_objects.py b/sklearn/metrics/tests/test_score_objects.py
index 6ce2955a127d..f418a9375d99 100644
--- a/sklearn/metrics/tests/test_score_objects.py
+++ b/sklearn/metrics/tests/test_score_objects.py
@@ -250,6 +250,7 @@ def test_check_scoring_and_check_multimetric_scoring():
                              scoring=scoring)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_check_scoring_gridsearchcv():
     # test that check_scoring works on GridSearchCV and pipeline.
     # slightly redundant non-regression test.
@@ -413,6 +414,8 @@ def test_thresholded_scorers_multilabel_indicator_data():
     assert_almost_equal(score1, score2)
 
 
+@pytest.mark.filterwarnings("ignore:the behavior of ")
+# AMI and NMI changes for 0.22
 def test_supervised_cluster_scorers():
     # Test clustering scorers against gold standard labeling.
     X, y = make_blobs(random_state=0, centers=2)
diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py
index a9f66740f2b8..8f16bf6c0ab4 100644
--- a/sklearn/mixture/base.py
+++ b/sklearn/mixture/base.py
@@ -172,11 +172,14 @@ def _initialize(self, X, resp):
     def fit(self, X, y=None):
         """Estimate model parameters with the EM algorithm.
 
-        The method fit the model `n_init` times and set the parameters with
+        The method fits the model ``n_init`` times and sets the parameters with
         which the model has the largest likelihood or lower bound. Within each
-        trial, the method iterates between E-step and M-step for `max_iter`
+        trial, the method iterates between E-step and M-step for ``max_iter``
         times until the change of likelihood or lower bound is less than
-        `tol`, otherwise, a `ConvergenceWarning` is raised.
+        ``tol``, otherwise, a ``ConvergenceWarning`` is raised.
+        If ``warm_start`` is ``True``, then ``n_init`` is ignored and a single
+        initialization is performed upon the first call. Upon consecutive
+        calls, training starts where it left off.
 
         Parameters
         ----------
@@ -188,6 +191,32 @@ def fit(self, X, y=None):
         -------
         self
         """
+        self.fit_predict(X, y)
+        return self
+
+    def fit_predict(self, X, y=None):
+        """Estimate model parameters using X and predict the labels for X.
+
+        The method fits the model n_init times and sets the parameters with
+        which the model has the largest likelihood or lower bound. Within each
+        trial, the method iterates between E-step and M-step for `max_iter`
+        times until the change of likelihood or lower bound is less than
+        `tol`, otherwise, a `ConvergenceWarning` is raised. After fitting, it
+        predicts the most probable label for the input data points.
+
+        .. versionadded:: 0.20
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+            List of n_features-dimensional data points. Each row
+            corresponds to a single data point.
+
+        Returns
+        -------
+        labels : array, shape (n_samples,)
+            Component labels.
+        """
         X = _check_X(X, self.n_components, ensure_min_samples=2)
         self._check_initial_parameters(X)
 
@@ -206,27 +235,28 @@ def fit(self, X, y=None):
 
             if do_init:
                 self._initialize_parameters(X, random_state)
-                self.lower_bound_ = -np.infty
+
+            lower_bound = (-np.infty if do_init else self.lower_bound_)
 
             for n_iter in range(1, self.max_iter + 1):
-                prev_lower_bound = self.lower_bound_
+                prev_lower_bound = lower_bound
 
                 log_prob_norm, log_resp = self._e_step(X)
                 self._m_step(X, log_resp)
-                self.lower_bound_ = self._compute_lower_bound(
+                lower_bound = self._compute_lower_bound(
                     log_resp, log_prob_norm)
 
-                change = self.lower_bound_ - prev_lower_bound
+                change = lower_bound - prev_lower_bound
                 self._print_verbose_msg_iter_end(n_iter, change)
 
                 if abs(change) < self.tol:
                     self.converged_ = True
                     break
 
-            self._print_verbose_msg_init_end(self.lower_bound_)
+            self._print_verbose_msg_init_end(lower_bound)
 
-            if self.lower_bound_ > max_lower_bound:
-                max_lower_bound = self.lower_bound_
+            if lower_bound > max_lower_bound:
+                max_lower_bound = lower_bound
                 best_params = self._get_parameters()
                 best_n_iter = n_iter
 
@@ -239,8 +269,9 @@ def fit(self, X, y=None):
 
         self._set_parameters(best_params)
         self.n_iter_ = best_n_iter
+        self.lower_bound_ = max_lower_bound
 
-        return self
+        return log_resp.argmax(axis=1)
 
     def _e_step(self, X):
         """E step.
@@ -404,7 +435,7 @@ def sample(self, n_samples=1):
                 for (mean, covariance, sample) in zip(
                     self.means_, self.covariances_, n_samples_comp)])
 
-        y = np.concatenate([j * np.ones(sample, dtype=int)
+        y = np.concatenate([np.full(sample, j, dtype=int)
                            for j, sample in enumerate(n_samples_comp)])
 
         return (X, y)
diff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py
index 5673db5f98a0..2c5f9b6cf151 100644
--- a/sklearn/mixture/gaussian_mixture.py
+++ b/sklearn/mixture/gaussian_mixture.py
@@ -512,6 +512,8 @@ class GaussianMixture(BaseMixture):
         If 'warm_start' is True, the solution of the last fitting is used as
         initialization for the next call of fit(). This can speed up
         convergence when fit is called several times on similar problems.
+        In that case, 'n_init' is ignored and only a single initialization
+        occurs upon the first call.
         See :term:`the Glossary <warm_start>`.
 
     verbose : int, default to 0.
@@ -575,7 +577,8 @@ class GaussianMixture(BaseMixture):
         Number of step used by the best fit of EM to reach the convergence.
 
     lower_bound_ : float
-        Log-likelihood of the best fit of EM.
+        Lower bound value on the log-likelihood (of the training data with
+        respect to the model) of the best fit of EM.
 
     See Also
     --------
diff --git a/sklearn/mixture/tests/test_bayesian_mixture.py b/sklearn/mixture/tests/test_bayesian_mixture.py
index e678c07d9236..540b6265ca0e 100644
--- a/sklearn/mixture/tests/test_bayesian_mixture.py
+++ b/sklearn/mixture/tests/test_bayesian_mixture.py
@@ -1,12 +1,16 @@
 # Author: Wei Xue <xuewei4d@gmail.com>
 #         Thierry Guillemot <thierry.guillemot.work@gmail.com>
 # License: BSD 3 clause
+import copy
 
 import numpy as np
 from scipy.special import gammaln
 
 from sklearn.utils.testing import assert_raise_message
 from sklearn.utils.testing import assert_almost_equal
+from sklearn.utils.testing import assert_array_equal
+
+from sklearn.metrics.cluster import adjusted_rand_score
 
 from sklearn.mixture.bayesian_mixture import _log_dirichlet_norm
 from sklearn.mixture.bayesian_mixture import _log_wishart_norm
@@ -14,7 +18,7 @@
 from sklearn.mixture import BayesianGaussianMixture
 
 from sklearn.mixture.tests.test_gaussian_mixture import RandomData
-from sklearn.exceptions import ConvergenceWarning
+from sklearn.exceptions import ConvergenceWarning, NotFittedError
 from sklearn.utils.testing import assert_greater_equal, ignore_warnings
 
 
@@ -419,3 +423,49 @@ def test_invariant_translation():
             assert_almost_equal(bgmm1.means_, bgmm2.means_ - 100)
             assert_almost_equal(bgmm1.weights_, bgmm2.weights_)
             assert_almost_equal(bgmm1.covariances_, bgmm2.covariances_)
+
+
+def test_bayesian_mixture_fit_predict():
+    rng = np.random.RandomState(0)
+    rand_data = RandomData(rng, scale=7)
+    n_components = 2 * rand_data.n_components
+
+    for covar_type in COVARIANCE_TYPE:
+        bgmm1 = BayesianGaussianMixture(n_components=n_components,
+                                        max_iter=100, random_state=rng,
+                                        tol=1e-3, reg_covar=0)
+        bgmm1.covariance_type = covar_type
+        bgmm2 = copy.deepcopy(bgmm1)
+        X = rand_data.X[covar_type]
+
+        Y_pred1 = bgmm1.fit(X).predict(X)
+        Y_pred2 = bgmm2.fit_predict(X)
+        assert_array_equal(Y_pred1, Y_pred2)
+
+
+def test_bayesian_mixture_predict_predict_proba():
+    # this is the same test as test_gaussian_mixture_predict_predict_proba()
+    rng = np.random.RandomState(0)
+    rand_data = RandomData(rng)
+    for prior_type in PRIOR_TYPE:
+        for covar_type in COVARIANCE_TYPE:
+            X = rand_data.X[covar_type]
+            Y = rand_data.Y
+            bgmm = BayesianGaussianMixture(
+                n_components=rand_data.n_components,
+                random_state=rng,
+                weight_concentration_prior_type=prior_type,
+                covariance_type=covar_type)
+
+            # Check a warning message arrive if we don't do fit
+            assert_raise_message(NotFittedError,
+                                 "This BayesianGaussianMixture instance"
+                                 " is not fitted yet. Call 'fit' with "
+                                 "appropriate arguments before using "
+                                 "this method.", bgmm.predict, X)
+
+            bgmm.fit(X)
+            Y_pred = bgmm.predict(X)
+            Y_pred_proba = bgmm.predict_proba(X).argmax(axis=1)
+            assert_array_equal(Y_pred, Y_pred_proba)
+            assert_greater_equal(adjusted_rand_score(Y, Y_pred), .95)
diff --git a/sklearn/mixture/tests/test_gaussian_mixture.py b/sklearn/mixture/tests/test_gaussian_mixture.py
index 08a083abf71e..cabe4b67d7f2 100644
--- a/sklearn/mixture/tests/test_gaussian_mixture.py
+++ b/sklearn/mixture/tests/test_gaussian_mixture.py
@@ -3,6 +3,7 @@
 # License: BSD 3 clause
 
 import sys
+import copy
 import warnings
 
 import numpy as np
@@ -95,7 +96,8 @@ def __init__(self, rng, n_samples=500, n_components=2, n_features=2,
         self.X = dict(zip(COVARIANCE_TYPE, [generate_data(
             n_samples, n_features, self.weights, self.means, self.covariances,
             covar_type) for covar_type in COVARIANCE_TYPE]))
-        self.Y = np.hstack([k * np.ones(int(np.round(w * n_samples)))
+        self.Y = np.hstack([np.full(int(np.round(w * n_samples)), k,
+                                    dtype=np.int)
                             for k, w in enumerate(self.weights)])
 
 
@@ -286,8 +288,8 @@ def test_check_precisions():
     precisions_not_positive = {
         'full': precisions_not_pos,
         'tied': precisions_not_pos[0],
-        'diag': -1. * np.ones((n_components, n_features)),
-        'spherical': -1. * np.ones(n_components)}
+        'diag': np.full((n_components, n_features), -1.),
+        'spherical': np.full(n_components, -1.)}
 
     not_positive_errors = {
         'full': 'symmetric, positive-definite',
@@ -569,6 +571,26 @@ def test_gaussian_mixture_predict_predict_proba():
         assert_greater(adjusted_rand_score(Y, Y_pred), .95)
 
 
+def test_gaussian_mixture_fit_predict():
+    rng = np.random.RandomState(0)
+    rand_data = RandomData(rng)
+    for covar_type in COVARIANCE_TYPE:
+        X = rand_data.X[covar_type]
+        Y = rand_data.Y
+        g = GaussianMixture(n_components=rand_data.n_components,
+                            random_state=rng, weights_init=rand_data.weights,
+                            means_init=rand_data.means,
+                            precisions_init=rand_data.precisions[covar_type],
+                            covariance_type=covar_type)
+
+        # check if fit_predict(X) is equivalent to fit(X).predict(X)
+        f = copy.deepcopy(g)
+        Y_pred1 = f.fit(X).predict(X)
+        Y_pred2 = g.fit_predict(X)
+        assert_array_equal(Y_pred1, Y_pred2)
+        assert_greater(adjusted_rand_score(Y, Y_pred2), .95)
+
+
 def test_gaussian_mixture_fit():
     # recover the ground truth
     rng = np.random.RandomState(0)
@@ -743,7 +765,6 @@ def test_gaussian_mixture_verbose():
 
 
 def test_warm_start():
-
     random_state = 0
     rng = np.random.RandomState(random_state)
     n_samples, n_features, n_components = 500, 2, 2
@@ -785,6 +806,25 @@ def test_warm_start():
     assert_true(h.converged_)
 
 
+@ignore_warnings(category=ConvergenceWarning)
+def test_convergence_detected_with_warm_start():
+    # We check that convergence is detected when warm_start=True
+    rng = np.random.RandomState(0)
+    rand_data = RandomData(rng)
+    n_components = rand_data.n_components
+    X = rand_data.X['full']
+
+    for max_iter in (1, 2, 50):
+        gmm = GaussianMixture(n_components=n_components, warm_start=True,
+                              max_iter=max_iter, random_state=rng)
+        for _ in range(100):
+            gmm.fit(X)
+            if gmm.converged_:
+                break
+        assert gmm.converged_
+        assert max_iter >= gmm.n_iter_
+
+
 def test_score():
     covar_type = 'full'
     rng = np.random.RandomState(0)
@@ -970,14 +1010,14 @@ def test_sample():
 @ignore_warnings(category=ConvergenceWarning)
 def test_init():
     # We check that by increasing the n_init number we have a better solution
-    random_state = 0
-    rand_data = RandomData(np.random.RandomState(random_state), scale=1)
-    n_components = rand_data.n_components
-    X = rand_data.X['full']
+    for random_state in range(25):
+        rand_data = RandomData(np.random.RandomState(random_state), scale=1)
+        n_components = rand_data.n_components
+        X = rand_data.X['full']
 
-    gmm1 = GaussianMixture(n_components=n_components, n_init=1,
-                           max_iter=1, random_state=random_state).fit(X)
-    gmm2 = GaussianMixture(n_components=n_components, n_init=100,
-                           max_iter=1, random_state=random_state).fit(X)
+        gmm1 = GaussianMixture(n_components=n_components, n_init=1,
+                               max_iter=1, random_state=random_state).fit(X)
+        gmm2 = GaussianMixture(n_components=n_components, n_init=10,
+                               max_iter=1, random_state=random_state).fit(X)
 
-    assert_greater(gmm2.lower_bound_, gmm1.lower_bound_)
+        assert gmm2.lower_bound_ >= gmm1.lower_bound_
diff --git a/sklearn/model_selection/_search.py b/sklearn/model_selection/_search.py
index a45a4bf5b4e6..92cb3a50ca72 100644
--- a/sklearn/model_selection/_search.py
+++ b/sklearn/model_selection/_search.py
@@ -13,7 +13,7 @@
 # License: BSD 3 clause
 
 from abc import ABCMeta, abstractmethod
-from collections import Mapping, namedtuple, defaultdict, Sequence, Iterable
+from collections import namedtuple, defaultdict
 from functools import partial, reduce
 from itertools import product
 import operator
@@ -29,11 +29,13 @@
 from ._validation import _fit_and_score
 from ._validation import _aggregate_score_dicts
 from ..exceptions import NotFittedError
-from ..externals.joblib import Parallel, delayed
+from ..utils import Parallel, delayed
 from ..externals import six
 from ..utils import check_random_state
 from ..utils.fixes import sp_version
 from ..utils.fixes import MaskedArray
+from ..utils.fixes import _Mapping as Mapping, _Sequence as Sequence
+from ..utils.fixes import _Iterable as Iterable
 from ..utils.random import sample_without_replacement
 from ..utils.validation import indexable, check_is_fitted
 from ..utils.metaestimators import if_delegate_has_method
@@ -406,12 +408,13 @@ def __repr__(self):
 
 class BaseSearchCV(six.with_metaclass(ABCMeta, BaseEstimator,
                                       MetaEstimatorMixin)):
-    """Base class for hyper parameter search with cross-validation."""
+    """Abstract base class for hyper parameter search with cross-validation.
+    """
 
     @abstractmethod
     def __init__(self, estimator, scoring=None,
-                 fit_params=None, n_jobs=1, iid='warn',
-                 refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs',
+                 fit_params=None, n_jobs=None, iid='warn',
+                 refit=True, cv='warn', verbose=0, pre_dispatch='2*n_jobs',
                  error_score='raise-deprecating', return_train_score=True):
 
         self.scoring = scoring
@@ -464,7 +467,7 @@ def _check_is_fitted(self, method_name):
                                  'with refit=False. %s is '
                                  'available only after refitting on the best '
                                  'parameters. You can refit an estimator '
-                                 'manually using the ``best_parameters_`` '
+                                 'manually using the ``best_params_`` '
                                  'attribute'
                                  % (type(self).__name__, method_name))
         else:
@@ -577,6 +580,40 @@ def classes_(self):
         self._check_is_fitted("classes_")
         return self.best_estimator_.classes_
 
+    @abstractmethod
+    def _run_search(self, evaluate_candidates):
+        """Repeatedly calls `evaluate_candidates` to conduct a search.
+
+        This method, implemented in sub-classes, makes it is possible to
+        customize the the scheduling of evaluations: GridSearchCV and
+        RandomizedSearchCV schedule evaluations for their whole parameter
+        search space at once but other more sequential approaches are also
+        possible: for instance is possible to iteratively schedule evaluations
+        for new regions of the parameter search space based on previously
+        collected evaluation results. This makes it possible to implement
+        Bayesian optimization or more generally sequential model-based
+        optimization by deriving from the BaseSearchCV abstract base class.
+
+        Parameters
+        ----------
+        evaluate_candidates : callable
+            This callback accepts a list of candidates, where each candidate is
+            a dict of parameter settings. It returns a dict of all results so
+            far, formatted like ``cv_results_``.
+
+        Examples
+        --------
+
+        ::
+
+            def _run_search(self, evaluate_candidates):
+                'Try C=0.1 only if C=1 is better than C=10'
+                all_results = evaluate_candidates([{'C': 1}, {'C': 10}])
+                score = all_results['mean_test_score']
+                if score[0] < score[1]:
+                    evaluate_candidates([{'C': 0.1}])
+        """
+
     def fit(self, X, y=None, groups=None, **fit_params):
         """Run fit with all sets of parameters.
 
@@ -636,29 +673,86 @@ def fit(self, X, y=None, groups=None, **fit_params):
 
         X, y, groups = indexable(X, y, groups)
         n_splits = cv.get_n_splits(X, y, groups)
-        # Regenerate parameter iterable for each fit
-        candidate_params = list(self._get_param_iterator())
-        n_candidates = len(candidate_params)
-        if self.verbose > 0:
-            print("Fitting {0} folds for each of {1} candidates, totalling"
-                  " {2} fits".format(n_splits, n_candidates,
-                                     n_candidates * n_splits))
 
         base_estimator = clone(self.estimator)
-        pre_dispatch = self.pre_dispatch
-
-        out = Parallel(
-            n_jobs=self.n_jobs, verbose=self.verbose,
-            pre_dispatch=pre_dispatch
-        )(delayed(_fit_and_score)(clone(base_estimator), X, y, scorers, train,
-                                  test, self.verbose, parameters,
-                                  fit_params=fit_params,
-                                  return_train_score=self.return_train_score,
-                                  return_n_test_samples=True,
-                                  return_times=True, return_parameters=False,
-                                  error_score=self.error_score)
-          for parameters, (train, test) in product(candidate_params,
-                                                   cv.split(X, y, groups)))
+
+        parallel = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
+                            pre_dispatch=self.pre_dispatch)
+
+        fit_and_score_kwargs = dict(scorer=scorers,
+                                    fit_params=fit_params,
+                                    return_train_score=self.return_train_score,
+                                    return_n_test_samples=True,
+                                    return_times=True,
+                                    return_parameters=False,
+                                    error_score=self.error_score,
+                                    verbose=self.verbose)
+        results_container = [{}]
+        with parallel:
+            all_candidate_params = []
+            all_out = []
+
+            def evaluate_candidates(candidate_params):
+                candidate_params = list(candidate_params)
+                n_candidates = len(candidate_params)
+
+                if self.verbose > 0:
+                    print("Fitting {0} folds for each of {1} candidates,"
+                          " totalling {2} fits".format(
+                              n_splits, n_candidates, n_candidates * n_splits))
+
+                out = parallel(delayed(_fit_and_score)(clone(base_estimator),
+                                                       X, y,
+                                                       train=train, test=test,
+                                                       parameters=parameters,
+                                                       **fit_and_score_kwargs)
+                               for parameters, (train, test)
+                               in product(candidate_params,
+                                          cv.split(X, y, groups)))
+
+                all_candidate_params.extend(candidate_params)
+                all_out.extend(out)
+
+                # XXX: When we drop Python 2 support, we can use nonlocal
+                # instead of results_container
+                results_container[0] = self._format_results(
+                    all_candidate_params, scorers, n_splits, all_out)
+                return results_container[0]
+
+            self._run_search(evaluate_candidates)
+
+        results = results_container[0]
+
+        # For multi-metric evaluation, store the best_index_, best_params_ and
+        # best_score_ iff refit is one of the scorer names
+        # In single metric evaluation, refit_metric is "score"
+        if self.refit or not self.multimetric_:
+            self.best_index_ = results["rank_test_%s" % refit_metric].argmin()
+            self.best_params_ = results["params"][self.best_index_]
+            self.best_score_ = results["mean_test_%s" % refit_metric][
+                self.best_index_]
+
+        if self.refit:
+            self.best_estimator_ = clone(base_estimator).set_params(
+                **self.best_params_)
+            refit_start_time = time.time()
+            if y is not None:
+                self.best_estimator_.fit(X, y, **fit_params)
+            else:
+                self.best_estimator_.fit(X, **fit_params)
+            refit_end_time = time.time()
+            self.refit_time_ = refit_end_time - refit_start_time
+
+        # Store the only scorer not as a dict for single metric evaluation
+        self.scorer_ = scorers if self.multimetric_ else scorers['score']
+
+        self.cv_results_ = results
+        self.n_splits_ = n_splits
+
+        return self
+
+    def _format_results(self, candidate_params, scorers, n_splits, out):
+        n_candidates = len(candidate_params)
 
         # if one choose to see train score, "out" will contain train score info
         if self.return_train_score:
@@ -744,7 +838,6 @@ def _store(key_name, array, weights=None, splits=False, rank=False):
                 prev_keys = set(results.keys())
                 _store('train_%s' % scorer_name, train_scores[scorer_name],
                        splits=True)
-
                 if self.return_train_score == 'warn':
                     for key in set(results.keys()) - prev_keys:
                         message = (
@@ -755,33 +848,7 @@ def _store(key_name, array, weights=None, splits=False, rank=False):
                         # warn on key access
                         results.add_warning(key, message, FutureWarning)
 
-        # For multi-metric evaluation, store the best_index_, best_params_ and
-        # best_score_ iff refit is one of the scorer names
-        # In single metric evaluation, refit_metric is "score"
-        if self.refit or not self.multimetric_:
-            self.best_index_ = results["rank_test_%s" % refit_metric].argmin()
-            self.best_params_ = candidate_params[self.best_index_]
-            self.best_score_ = results["mean_test_%s" % refit_metric][
-                self.best_index_]
-
-        if self.refit:
-            self.best_estimator_ = clone(base_estimator).set_params(
-                **self.best_params_)
-            refit_start_time = time.time()
-            if y is not None:
-                self.best_estimator_.fit(X, y, **fit_params)
-            else:
-                self.best_estimator_.fit(X, **fit_params)
-            refit_end_time = time.time()
-            self.refit_time_ = refit_end_time - refit_start_time
-
-        # Store the only scorer not as a dict for single metric evaluation
-        self.scorer_ = scorers if self.multimetric_ else scorers['score']
-
-        self.cv_results_ = results
-        self.n_splits_ = n_splits
-
-        return self
+        return results
 
 
 class GridSearchCV(BaseSearchCV):
@@ -836,8 +903,11 @@ class GridSearchCV(BaseSearchCV):
            0.19 and will be removed in version 0.21. Pass fit parameters to
            the ``fit`` method instead.
 
-    n_jobs : int, default=1
+    n_jobs : int or None, optional (default=None)
         Number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     pre_dispatch : int, or string, optional
         Controls the number of jobs that get dispatched during parallel
@@ -885,6 +955,10 @@ class GridSearchCV(BaseSearchCV):
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     refit : boolean, or string, default=True
         Refit an estimator using the best found parameters on the whole
         dataset.
@@ -898,7 +972,7 @@ class GridSearchCV(BaseSearchCV):
         ``GridSearchCV`` instance.
 
         Also for multiple metric evaluation, the attributes ``best_index_``,
-        ``best_score_`` and ``best_parameters_`` will only be available if
+        ``best_score_`` and ``best_params_`` will only be available if
         ``refit`` is set and all of them will be determined w.r.t this specific
         scorer.
 
@@ -936,16 +1010,16 @@ class GridSearchCV(BaseSearchCV):
     >>> iris = datasets.load_iris()
     >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
     >>> svc = svm.SVC(gamma="scale")
-    >>> clf = GridSearchCV(svc, parameters)
+    >>> clf = GridSearchCV(svc, parameters, cv=5)
     >>> clf.fit(iris.data, iris.target)
     ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
-    GridSearchCV(cv=None, error_score=...,
+    GridSearchCV(cv=5, error_score=...,
            estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,
                          decision_function_shape='ovr', degree=..., gamma=...,
                          kernel='rbf', max_iter=-1, probability=False,
                          random_state=None, shrinking=True, tol=...,
                          verbose=False),
-           fit_params=None, iid=..., n_jobs=1,
+           fit_params=None, iid=..., n_jobs=None,
            param_grid=..., pre_dispatch=..., refit=..., return_train_score=...,
            scoring=..., verbose=...)
     >>> sorted(clf.cv_results_.keys())
@@ -1089,7 +1163,7 @@ class GridSearchCV(BaseSearchCV):
     """
 
     def __init__(self, estimator, param_grid, scoring=None, fit_params=None,
-                 n_jobs=1, iid='warn', refit=True, cv=None, verbose=0,
+                 n_jobs=None, iid='warn', refit=True, cv='warn', verbose=0,
                  pre_dispatch='2*n_jobs', error_score='raise-deprecating',
                  return_train_score="warn"):
         super(GridSearchCV, self).__init__(
@@ -1100,9 +1174,9 @@ def __init__(self, estimator, param_grid, scoring=None, fit_params=None,
         self.param_grid = param_grid
         _check_param_grid(param_grid)
 
-    def _get_param_iterator(self):
-        """Return ParameterGrid instance for the given param_grid"""
-        return ParameterGrid(self.param_grid)
+    def _run_search(self, evaluate_candidates):
+        """Search all candidates in param_grid"""
+        evaluate_candidates(ParameterGrid(self.param_grid))
 
 
 class RandomizedSearchCV(BaseSearchCV):
@@ -1176,8 +1250,11 @@ class RandomizedSearchCV(BaseSearchCV):
            0.19 and will be removed in version 0.21. Pass fit parameters to
            the ``fit`` method instead.
 
-    n_jobs : int, default=1
+    n_jobs : int or None, optional (default=None)
         Number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     pre_dispatch : int, or string, optional
         Controls the number of jobs that get dispatched during parallel
@@ -1225,6 +1302,10 @@ class RandomizedSearchCV(BaseSearchCV):
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     refit : boolean, or string default=True
         Refit an estimator using the best found parameters on the whole
         dataset.
@@ -1238,7 +1319,7 @@ class RandomizedSearchCV(BaseSearchCV):
         ``RandomizedSearchCV`` instance.
 
         Also for multiple metric evaluation, the attributes ``best_index_``,
-        ``best_score_`` and ``best_parameters_`` will only be available if
+        ``best_score_`` and ``best_params_`` will only be available if
         ``refit`` is set and all of them will be determined w.r.t this specific
         scorer.
 
@@ -1402,9 +1483,10 @@ class RandomizedSearchCV(BaseSearchCV):
     """
 
     def __init__(self, estimator, param_distributions, n_iter=10, scoring=None,
-                 fit_params=None, n_jobs=1, iid='warn', refit=True, cv=None,
-                 verbose=0, pre_dispatch='2*n_jobs', random_state=None,
-                 error_score='raise-deprecating', return_train_score="warn"):
+                 fit_params=None, n_jobs=None, iid='warn', refit=True,
+                 cv='warn', verbose=0, pre_dispatch='2*n_jobs',
+                 random_state=None, error_score='raise-deprecating',
+                 return_train_score="warn"):
         self.param_distributions = param_distributions
         self.n_iter = n_iter
         self.random_state = random_state
@@ -1414,8 +1496,8 @@ def __init__(self, estimator, param_distributions, n_iter=10, scoring=None,
             pre_dispatch=pre_dispatch, error_score=error_score,
             return_train_score=return_train_score)
 
-    def _get_param_iterator(self):
-        """Return ParameterSampler instance for the given distributions"""
-        return ParameterSampler(
+    def _run_search(self, evaluate_candidates):
+        """Search n_iter candidates from param_distributions"""
+        evaluate_candidates(ParameterSampler(
             self.param_distributions, self.n_iter,
-            random_state=self.random_state)
+            random_state=self.random_state))
diff --git a/sklearn/model_selection/_split.py b/sklearn/model_selection/_split.py
index c0367a2349e8..da4d274a642b 100644
--- a/sklearn/model_selection/_split.py
+++ b/sklearn/model_selection/_split.py
@@ -15,7 +15,6 @@
 
 import warnings
 from itertools import chain, combinations
-from collections import Iterable
 from math import ceil, floor
 import numbers
 from abc import ABCMeta, abstractmethod
@@ -29,6 +28,7 @@
 from ..externals.six import with_metaclass
 from ..externals.six.moves import zip
 from ..utils.fixes import signature, comb
+from ..utils.fixes import _Iterable as Iterable
 from ..base import _pprint
 
 __all__ = ['BaseCrossValidator',
@@ -49,6 +49,17 @@
            'check_cv']
 
 
+NSPLIT_WARNING = (
+    "You should specify a value for 'n_splits' instead of relying on the "
+    "default value. The default value will change from 3 to 5 "
+    "in version 0.22.")
+
+CV_WARNING = (
+    "You should specify a value for 'cv' instead of relying on the "
+    "default value. The default value will change from 3 to 5 "
+    "in version 0.22.")
+
+
 class BaseCrossValidator(with_metaclass(ABCMeta)):
     """Base class for all cross-validators
 
@@ -358,6 +369,9 @@ class KFold(_BaseKFold):
     n_splits : int, default=3
         Number of folds. Must be at least 2.
 
+        .. versionchanged:: 0.20
+            ``n_splits`` default value will change from 3 to 5 in v0.22.
+
     shuffle : boolean, optional
         Whether to shuffle the data before splitting into batches.
 
@@ -406,8 +420,11 @@ class KFold(_BaseKFold):
     RepeatedKFold: Repeats K-Fold n times.
     """
 
-    def __init__(self, n_splits=3, shuffle=False,
+    def __init__(self, n_splits='warn', shuffle=False,
                  random_state=None):
+        if n_splits is 'warn':
+            warnings.warn(NSPLIT_WARNING, FutureWarning)
+            n_splits = 3
         super(KFold, self).__init__(n_splits, shuffle, random_state)
 
     def _iter_test_indices(self, X, y=None, groups=None):
@@ -417,7 +434,7 @@ def _iter_test_indices(self, X, y=None, groups=None):
             check_random_state(self.random_state).shuffle(indices)
 
         n_splits = self.n_splits
-        fold_sizes = (n_samples // n_splits) * np.ones(n_splits, dtype=np.int)
+        fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
         fold_sizes[:n_samples % n_splits] += 1
         current = 0
         for fold_size in fold_sizes:
@@ -440,6 +457,9 @@ class GroupKFold(_BaseKFold):
     n_splits : int, default=3
         Number of folds. Must be at least 2.
 
+        .. versionchanged:: 0.20
+            ``n_splits`` default value will change from 3 to 5 in v0.22.
+
     Examples
     --------
     >>> from sklearn.model_selection import GroupKFold
@@ -472,7 +492,10 @@ class GroupKFold(_BaseKFold):
         For splitting the data according to explicit domain-specific
         stratification of the dataset.
     """
-    def __init__(self, n_splits=3):
+    def __init__(self, n_splits='warn'):
+        if n_splits is 'warn':
+            warnings.warn(NSPLIT_WARNING, FutureWarning)
+            n_splits = 3
         super(GroupKFold, self).__init__(n_splits, shuffle=False,
                                          random_state=None)
 
@@ -530,6 +553,9 @@ class StratifiedKFold(_BaseKFold):
     n_splits : int, default=3
         Number of folds. Must be at least 2.
 
+        .. versionchanged:: 0.20
+            ``n_splits`` default value will change from 3 to 5 in v0.22.
+
     shuffle : boolean, optional
         Whether to shuffle each stratification of the data before splitting
         into batches.
@@ -567,7 +593,10 @@ class StratifiedKFold(_BaseKFold):
     RepeatedStratifiedKFold: Repeats Stratified K-Fold n times.
     """
 
-    def __init__(self, n_splits=3, shuffle=False, random_state=None):
+    def __init__(self, n_splits='warn', shuffle=False, random_state=None):
+        if n_splits is 'warn':
+            warnings.warn(NSPLIT_WARNING, FutureWarning)
+            n_splits = 3
         super(StratifiedKFold, self).__init__(n_splits, shuffle, random_state)
 
     def _make_test_folds(self, X, y=None):
@@ -687,17 +716,20 @@ class TimeSeriesSplit(_BaseKFold):
     n_splits : int, default=3
         Number of splits. Must be at least 1.
 
+        .. versionchanged:: 0.20
+            ``n_splits`` default value will change from 3 to 5 in v0.22.
+
     max_train_size : int, optional
         Maximum size for a single training set.
 
     Examples
     --------
     >>> from sklearn.model_selection import TimeSeriesSplit
-    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
-    >>> y = np.array([1, 2, 3, 4])
-    >>> tscv = TimeSeriesSplit(n_splits=3)
+    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])
+    >>> y = np.array([1, 2, 3, 4, 5, 6])
+    >>> tscv = TimeSeriesSplit(n_splits=5)
     >>> print(tscv)  # doctest: +NORMALIZE_WHITESPACE
-    TimeSeriesSplit(max_train_size=None, n_splits=3)
+    TimeSeriesSplit(max_train_size=None, n_splits=5)
     >>> for train_index, test_index in tscv.split(X):
     ...    print("TRAIN:", train_index, "TEST:", test_index)
     ...    X_train, X_test = X[train_index], X[test_index]
@@ -705,6 +737,8 @@ class TimeSeriesSplit(_BaseKFold):
     TRAIN: [0] TEST: [1]
     TRAIN: [0 1] TEST: [2]
     TRAIN: [0 1 2] TEST: [3]
+    TRAIN: [0 1 2 3] TEST: [4]
+    TRAIN: [0 1 2 3 4] TEST: [5]
 
     Notes
     -----
@@ -713,7 +747,10 @@ class TimeSeriesSplit(_BaseKFold):
     with a test set of size ``n_samples//(n_splits + 1)``,
     where ``n_samples`` is the number of samples.
     """
-    def __init__(self, n_splits=3, max_train_size=None):
+    def __init__(self, n_splits='warn', max_train_size=None):
+        if n_splits is 'warn':
+            warnings.warn(NSPLIT_WARNING, FutureWarning)
+            n_splits = 3
         super(TimeSeriesSplit, self).__init__(n_splits,
                                               shuffle=False,
                                               random_state=None)
@@ -1270,27 +1307,31 @@ class ShuffleSplit(BaseShuffleSplit):
     Examples
     --------
     >>> from sklearn.model_selection import ShuffleSplit
-    >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
-    >>> y = np.array([1, 2, 1, 2])
-    >>> rs = ShuffleSplit(n_splits=3, test_size=.25, random_state=0)
+    >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [3, 4], [5, 6]])
+    >>> y = np.array([1, 2, 1, 2, 1, 2])
+    >>> rs = ShuffleSplit(n_splits=5, test_size=.25, random_state=0)
     >>> rs.get_n_splits(X)
-    3
+    5
     >>> print(rs)
-    ShuffleSplit(n_splits=3, random_state=0, test_size=0.25, train_size=None)
+    ShuffleSplit(n_splits=5, random_state=0, test_size=0.25, train_size=None)
     >>> for train_index, test_index in rs.split(X):
     ...    print("TRAIN:", train_index, "TEST:", test_index)
     ...  # doctest: +ELLIPSIS
-    TRAIN: [3 1 0] TEST: [2]
-    TRAIN: [2 1 3] TEST: [0]
-    TRAIN: [0 2 1] TEST: [3]
-    >>> rs = ShuffleSplit(n_splits=3, train_size=0.5, test_size=.25,
+    TRAIN: [1 3 0 4] TEST: [5 2]
+    TRAIN: [4 0 2 5] TEST: [1 3]
+    TRAIN: [1 2 4 0] TEST: [3 5]
+    TRAIN: [3 4 1 0] TEST: [5 2]
+    TRAIN: [3 5 1 0] TEST: [2 4]
+    >>> rs = ShuffleSplit(n_splits=5, train_size=0.5, test_size=.25,
     ...                   random_state=0)
     >>> for train_index, test_index in rs.split(X):
     ...    print("TRAIN:", train_index, "TEST:", test_index)
     ...  # doctest: +ELLIPSIS
-    TRAIN: [3 1] TEST: [2]
-    TRAIN: [2 1] TEST: [0]
-    TRAIN: [0 2] TEST: [3]
+    TRAIN: [1 3 0] TEST: [5 2]
+    TRAIN: [4 0 2] TEST: [1 3]
+    TRAIN: [1 2 4] TEST: [3 5]
+    TRAIN: [3 4 1] TEST: [5 2]
+    TRAIN: [3 5 1] TEST: [2 4]
     """
 
     def _iter_indices(self, X, y=None, groups=None):
@@ -1502,20 +1543,22 @@ class StratifiedShuffleSplit(BaseShuffleSplit):
     Examples
     --------
     >>> from sklearn.model_selection import StratifiedShuffleSplit
-    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
-    >>> y = np.array([0, 0, 1, 1])
-    >>> sss = StratifiedShuffleSplit(n_splits=3, test_size=0.5, random_state=0)
+    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])
+    >>> y = np.array([0, 0, 0, 1, 1, 1])
+    >>> sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)
     >>> sss.get_n_splits(X, y)
-    3
+    5
     >>> print(sss)       # doctest: +ELLIPSIS
-    StratifiedShuffleSplit(n_splits=3, random_state=0, ...)
+    StratifiedShuffleSplit(n_splits=5, random_state=0, ...)
     >>> for train_index, test_index in sss.split(X, y):
     ...    print("TRAIN:", train_index, "TEST:", test_index)
     ...    X_train, X_test = X[train_index], X[test_index]
     ...    y_train, y_test = y[train_index], y[test_index]
-    TRAIN: [1 2] TEST: [3 0]
-    TRAIN: [0 2] TEST: [1 3]
-    TRAIN: [0 2] TEST: [3 1]
+    TRAIN: [5 2 3] TEST: [4 1 0]
+    TRAIN: [5 1 4] TEST: [0 2 3]
+    TRAIN: [5 0 2] TEST: [4 3 1]
+    TRAIN: [4 1 0] TEST: [2 3 5]
+    TRAIN: [0 5 1] TEST: [3 4 2]
     """
 
     def __init__(self, n_splits=10, test_size="default", train_size=None,
@@ -1859,7 +1902,7 @@ def split(self, X=None, y=None, groups=None):
             yield train, test
 
 
-def check_cv(cv=3, y=None, classifier=False):
+def check_cv(cv='warn', y=None, classifier=False):
     """Input checker utility for building a cross-validator
 
     Parameters
@@ -1880,6 +1923,9 @@ def check_cv(cv=3, y=None, classifier=False):
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value will change from 3-fold to 5-fold in v0.22.
+
     y : array-like, optional
         The target variable for supervised learning problems.
 
@@ -1893,7 +1939,8 @@ def check_cv(cv=3, y=None, classifier=False):
         The return value is a cross-validator which generates the train/test
         splits via the ``split`` method.
     """
-    if cv is None:
+    if cv is None or cv is 'warn':
+        warnings.warn(CV_WARNING, FutureWarning)
         cv = 3
 
     if isinstance(cv, numbers.Integral):
diff --git a/sklearn/model_selection/_validation.py b/sklearn/model_selection/_validation.py
index 9241d05fd432..4ddfc5edac6a 100644
--- a/sklearn/model_selection/_validation.py
+++ b/sklearn/model_selection/_validation.py
@@ -25,7 +25,8 @@
 from ..utils.deprecation import DeprecationDict
 from ..utils.validation import _is_arraylike, _num_samples
 from ..utils.metaestimators import _safe_split
-from ..externals.joblib import Parallel, delayed, logger
+from ..utils import Parallel, delayed
+from ..utils._joblib import logger
 from ..externals.six.moves import zip
 from ..metrics.scorer import check_scoring, _check_multimetric_scoring
 from ..exceptions import FitFailedWarning
@@ -37,10 +38,10 @@
            'permutation_test_score', 'learning_curve', 'validation_curve']
 
 
-def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv=None,
-                   n_jobs=1, verbose=0, fit_params=None,
+def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv='warn',
+                   n_jobs=None, verbose=0, fit_params=None,
                    pre_dispatch='2*n_jobs', return_train_score="warn",
-                   return_estimator=False):
+                   return_estimator=False, error_score='raise-deprecating'):
     """Evaluate metric(s) by cross-validation and also record fit/score times.
 
     Read more in the :ref:`User Guide <multimetric_cross_validation>`.
@@ -92,9 +93,15 @@ def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv=None,
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
-    n_jobs : integer, optional
-        The number of CPUs to use to do the computation. -1 means
-        'all CPUs'.
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
+    n_jobs : int or None, optional (default=None)
+        The number of CPUs to use to do the computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : integer, optional
         The verbosity level.
@@ -134,6 +141,16 @@ def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv=None,
     return_estimator : boolean, default False
         Whether to return the estimators fitted on each split.
 
+    error_score : 'raise' | 'raise-deprecating' or numeric
+        Value to assign to the score if an error occurs in estimator fitting.
+        If set to 'raise', the error is raised.
+        If set to 'raise-deprecating', a FutureWarning is printed before the
+        error is raised.
+        If a numeric value is given, FitFailedWarning is raised. This parameter
+        does not affect the refit step, which will always raise the error.
+        Default is 'raise-deprecating' but from version 0.22 it will change
+        to np.nan.
+
     Returns
     -------
     scores : dict of float arrays of shape=(n_splits,)
@@ -174,7 +191,8 @@ def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv=None,
 
     Single metric evaluation using ``cross_validate``
 
-    >>> cv_results = cross_validate(lasso, X, y, return_train_score=False)
+    >>> cv_results = cross_validate(lasso, X, y, cv=3,
+    ...                             return_train_score=False)
     >>> sorted(cv_results.keys())                         # doctest: +ELLIPSIS
     ['fit_time', 'score_time', 'test_score']
     >>> cv_results['test_score']    # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
@@ -183,8 +201,9 @@ def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv=None,
     Multiple metric evaluation using ``cross_validate``
     (please refer the ``scoring`` parameter doc for more information)
 
-    >>> scores = cross_validate(lasso, X, y,
-    ...                         scoring=('r2', 'neg_mean_squared_error'))
+    >>> scores = cross_validate(lasso, X, y, cv=3,
+    ...                         scoring=('r2', 'neg_mean_squared_error'),
+    ...                         return_train_score=True)
     >>> print(scores['test_neg_mean_squared_error'])      # doctest: +ELLIPSIS
     [-3635.5... -3573.3... -6114.7...]
     >>> print(scores['train_r2'])                         # doctest: +ELLIPSIS
@@ -216,7 +235,8 @@ def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv=None,
         delayed(_fit_and_score)(
             clone(estimator), X, y, scorers, train, test, verbose, None,
             fit_params, return_train_score=return_train_score,
-            return_times=True, return_estimator=return_estimator)
+            return_times=True, return_estimator=return_estimator,
+            error_score=error_score)
         for train, test in cv.split(X, y, groups))
 
     zipped_scores = list(zip(*scores))
@@ -253,9 +273,9 @@ def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv=None,
     return ret
 
 
-def cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv=None,
-                    n_jobs=1, verbose=0, fit_params=None,
-                    pre_dispatch='2*n_jobs'):
+def cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv='warn',
+                    n_jobs=None, verbose=0, fit_params=None,
+                    pre_dispatch='2*n_jobs', error_score='raise-deprecating'):
     """Evaluate a score by cross-validation
 
     Read more in the :ref:`User Guide <cross_validation>`.
@@ -297,9 +317,15 @@ def cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv=None,
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
-    n_jobs : integer, optional
-        The number of CPUs to use to do the computation. -1 means
-        'all CPUs'.
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
+    n_jobs : int or None, optional (default=None)
+        The number of CPUs to use to do the computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : integer, optional
         The verbosity level.
@@ -324,6 +350,16 @@ def cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv=None,
             - A string, giving an expression as a function of n_jobs,
               as in '2*n_jobs'
 
+    error_score : 'raise' | 'raise-deprecating' or numeric
+        Value to assign to the score if an error occurs in estimator fitting.
+        If set to 'raise', the error is raised.
+        If set to 'raise-deprecating', a FutureWarning is printed before the
+        error is raised.
+        If a numeric value is given, FitFailedWarning is raised. This parameter
+        does not affect the refit step, which will always raise the error.
+        Default is 'raise-deprecating' but from version 0.22 it will change
+        to np.nan.
+
     Returns
     -------
     scores : array of float, shape=(len(list(cv)),)
@@ -337,7 +373,7 @@ def cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv=None,
     >>> X = diabetes.data[:150]
     >>> y = diabetes.target[:150]
     >>> lasso = linear_model.Lasso()
-    >>> print(cross_val_score(lasso, X, y))  # doctest: +ELLIPSIS
+    >>> print(cross_val_score(lasso, X, y, cv=3))  # doctest: +ELLIPSIS
     [0.33150734 0.08022311 0.03531764]
 
     See Also
@@ -362,7 +398,8 @@ def cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv=None,
                                 return_train_score=False,
                                 n_jobs=n_jobs, verbose=verbose,
                                 fit_params=fit_params,
-                                pre_dispatch=pre_dispatch)
+                                pre_dispatch=pre_dispatch,
+                                error_score=error_score)
     return cv_results['test_score']
 
 
@@ -404,12 +441,15 @@ def _fit_and_score(estimator, X, y, scorer, train, test, verbose,
     verbose : integer
         The verbosity level.
 
-    error_score : 'raise' or numeric
+    error_score : 'raise' | 'raise-deprecating' or numeric
         Value to assign to the score if an error occurs in estimator fitting.
-        If set to 'raise', the error is raised. If a numeric value is given,
-        FitFailedWarning is raised. This parameter does not affect the refit
-        step, which will always raise the error. Default is 'raise' but from
-        version 0.22 it will change to np.nan.
+        If set to 'raise', the error is raised.
+        If set to 'raise-deprecating', a FutureWarning is printed before the
+        error is raised.
+        If a numeric value is given, FitFailedWarning is raised. This parameter
+        does not affect the refit step, which will always raise the error.
+        Default is 'raise-deprecating' but from version 0.22 it will change
+        to np.nan.
 
     parameters : dict or None
         Parameters to be set on the estimator.
@@ -610,9 +650,9 @@ def _multimetric_score(estimator, X_test, y_test, scorers):
     return scores
 
 
-def cross_val_predict(estimator, X, y=None, groups=None, cv=None, n_jobs=1,
-                      verbose=0, fit_params=None, pre_dispatch='2*n_jobs',
-                      method='predict'):
+def cross_val_predict(estimator, X, y=None, groups=None, cv='warn',
+                      n_jobs=None, verbose=0, fit_params=None,
+                      pre_dispatch='2*n_jobs', method='predict'):
     """Generate cross-validated estimates for each input data point
 
     It is not appropriate to pass these predictions into an evaluation
@@ -652,9 +692,15 @@ def cross_val_predict(estimator, X, y=None, groups=None, cv=None, n_jobs=1,
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
-    n_jobs : integer, optional
-        The number of CPUs to use to do the computation. -1 means
-        'all CPUs'.
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
+    n_jobs : int or None, optional (default=None)
+        The number of CPUs to use to do the computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : integer, optional
         The verbosity level.
@@ -712,7 +758,7 @@ def cross_val_predict(estimator, X, y=None, groups=None, cv=None, n_jobs=1,
     >>> X = diabetes.data[:150]
     >>> y = diabetes.target[:150]
     >>> lasso = linear_model.Lasso()
-    >>> y_pred = cross_val_predict(lasso, X, y)
+    >>> y_pred = cross_val_predict(lasso, X, y, cv=3)
     """
     X, y, groups = indexable(X, y, groups)
 
@@ -886,8 +932,8 @@ def _index_param_value(X, v, indices):
     return safe_indexing(v, indices)
 
 
-def permutation_test_score(estimator, X, y, groups=None, cv=None,
-                           n_permutations=100, n_jobs=1, random_state=0,
+def permutation_test_score(estimator, X, y, groups=None, cv='warn',
+                           n_permutations=100, n_jobs=None, random_state=0,
                            verbose=0, scoring=None):
     """Evaluate the significance of a cross-validated score with permutations
 
@@ -937,12 +983,18 @@ def permutation_test_score(estimator, X, y, groups=None, cv=None,
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     n_permutations : integer, optional
         Number of times to permute ``y``.
 
-    n_jobs : integer, optional
-        The number of CPUs to use to do the computation. -1 means
-        'all CPUs'.
+    n_jobs : int or None, optional (default=None)
+        The number of CPUs to use to do the computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional (default=0)
         If int, random_state is the seed used by the random number generator;
@@ -1023,10 +1075,10 @@ def _shuffle(y, groups, random_state):
 
 
 def learning_curve(estimator, X, y, groups=None,
-                   train_sizes=np.linspace(0.1, 1.0, 5), cv=None, scoring=None,
-                   exploit_incremental_learning=False, n_jobs=1,
-                   pre_dispatch="all", verbose=0, shuffle=False,
-                   random_state=None):
+                   train_sizes=np.linspace(0.1, 1.0, 5), cv='warn',
+                   scoring=None, exploit_incremental_learning=False,
+                   n_jobs=None, pre_dispatch="all", verbose=0, shuffle=False,
+                   random_state=None,  error_score='raise-deprecating'):
     """Learning curve.
 
     Determines cross-validated training and test scores for different training
@@ -1083,6 +1135,10 @@ def learning_curve(estimator, X, y, groups=None,
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     scoring : string, callable or None, optional, default: None
         A string (see model evaluation documentation) or
         a scorer callable object / function with signature
@@ -1092,8 +1148,11 @@ def learning_curve(estimator, X, y, groups=None,
         If the estimator supports incremental learning, this will be
         used to speed up fitting for different training set sizes.
 
-    n_jobs : integer, optional
-        Number of jobs to run in parallel (default 1).
+    n_jobs : int or None, optional (default=None)
+        Number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     pre_dispatch : integer or string, optional
         Number of predispatched jobs for parallel execution (default is
@@ -1113,6 +1172,16 @@ def learning_curve(estimator, X, y, groups=None,
         If None, the random number generator is the RandomState instance used
         by `np.random`. Used when ``shuffle`` is True.
 
+    error_score : 'raise' | 'raise-deprecating' or numeric
+        Value to assign to the score if an error occurs in estimator fitting.
+        If set to 'raise', the error is raised.
+        If set to 'raise-deprecating', a FutureWarning is printed before the
+        error is raised.
+        If a numeric value is given, FitFailedWarning is raised. This parameter
+        does not affect the refit step, which will always raise the error.
+        Default is 'raise-deprecating' but from version 0.22 it will change
+        to np.nan.
+
     Returns
     -------
     train_sizes_abs : array, shape (n_unique_ticks,), dtype int
@@ -1171,8 +1240,9 @@ def learning_curve(estimator, X, y, groups=None,
                 train_test_proportions.append((train[:n_train_samples], test))
 
         out = parallel(delayed(_fit_and_score)(
-            clone(estimator), X, y, scorer, train, test,
-            verbose, parameters=None, fit_params=None, return_train_score=True)
+            clone(estimator), X, y, scorer, train, test, verbose,
+            parameters=None, fit_params=None, return_train_score=True,
+            error_score=error_score)
             for train, test in train_test_proportions)
         out = np.array(out)
         n_cv_folds = out.shape[0] // n_unique_ticks
@@ -1264,8 +1334,8 @@ def _incremental_fit_estimator(estimator, X, y, classes, train, test,
 
 
 def validation_curve(estimator, X, y, param_name, param_range, groups=None,
-                     cv=None, scoring=None, n_jobs=1, pre_dispatch="all",
-                     verbose=0):
+                     cv='warn', scoring=None, n_jobs=None, pre_dispatch="all",
+                     verbose=0, error_score='raise-deprecating'):
     """Validation curve.
 
     Determine training and test scores for varying parameter values.
@@ -1316,13 +1386,20 @@ def validation_curve(estimator, X, y, param_name, param_range, groups=None,
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     scoring : string, callable or None, optional, default: None
         A string (see model evaluation documentation) or
         a scorer callable object / function with signature
         ``scorer(estimator, X, y)``.
 
-    n_jobs : integer, optional
-        Number of jobs to run in parallel (default 1).
+    n_jobs : int or None, optional (default=None)
+        Number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     pre_dispatch : integer or string, optional
         Number of predispatched jobs for parallel execution (default is
@@ -1332,6 +1409,16 @@ def validation_curve(estimator, X, y, param_name, param_range, groups=None,
     verbose : integer, optional
         Controls the verbosity: the higher, the more messages.
 
+    error_score : 'raise' | 'raise-deprecating' or numeric
+        Value to assign to the score if an error occurs in estimator fitting.
+        If set to 'raise', the error is raised.
+        If set to 'raise-deprecating', a FutureWarning is printed before the
+        error is raised.
+        If a numeric value is given, FitFailedWarning is raised. This parameter
+        does not affect the refit step, which will always raise the error.
+        Default is 'raise-deprecating' but from version 0.22 it will change
+        to np.nan.
+
     Returns
     -------
     train_scores : array, shape (n_ticks, n_cv_folds)
@@ -1354,7 +1441,8 @@ def validation_curve(estimator, X, y, param_name, param_range, groups=None,
                         verbose=verbose)
     out = parallel(delayed(_fit_and_score)(
         clone(estimator), X, y, scorer, train, test, verbose,
-        parameters={param_name: v}, fit_params=None, return_train_score=True)
+        parameters={param_name: v}, fit_params=None, return_train_score=True,
+        error_score=error_score)
         # NOTE do not change order of iteration to allow one time cv splitters
         for train, test in cv.split(X, y, groups) for v in param_range)
     out = np.asarray(out)
diff --git a/sklearn/model_selection/tests/test_search.py b/sklearn/model_selection/tests/test_search.py
index d80e39f9f0c4..0409794bf08e 100644
--- a/sklearn/model_selection/tests/test_search.py
+++ b/sklearn/model_selection/tests/test_search.py
@@ -1,6 +1,5 @@
 """Test the search module"""
 
-from collections import Iterable, Sized
 from sklearn.externals.six.moves import cStringIO as StringIO
 from sklearn.externals.six.moves import xrange
 from itertools import chain, product
@@ -8,6 +7,7 @@
 import sys
 from types import GeneratorType
 import re
+import warnings
 
 import numpy as np
 import scipy.sparse as sp
@@ -15,6 +15,7 @@
 
 from sklearn.utils.fixes import sp_version
 from sklearn.utils.fixes import PY3_OR_LATER
+from sklearn.utils.fixes import _Iterable as Iterable, _Sized as Sized
 from sklearn.utils.testing import assert_equal
 from sklearn.utils.testing import assert_not_equal
 from sklearn.utils.testing import assert_raises
@@ -25,6 +26,7 @@
 from sklearn.utils.testing import assert_false, assert_true
 from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_array_almost_equal
+from sklearn.utils.testing import assert_allclose
 from sklearn.utils.testing import assert_almost_equal
 from sklearn.utils.testing import assert_greater_equal
 from sklearn.utils.testing import ignore_warnings
@@ -53,6 +55,7 @@
 from sklearn.model_selection import RandomizedSearchCV
 from sklearn.model_selection import ParameterGrid
 from sklearn.model_selection import ParameterSampler
+from sklearn.model_selection._search import BaseSearchCV
 
 from sklearn.model_selection._validation import FitFailedWarning
 
@@ -177,6 +180,9 @@ def test_parameter_grid():
     assert_grid_iter_equals_getitem(has_empty)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
+
 def test_grid_search():
     # Test that the best estimator contains the right value for foo_param
     clf = MockClassifier()
@@ -220,14 +226,19 @@ def check_hyperparameter_searcher_with_fit_params(klass, **klass_kwargs):
     searcher.fit(X, y, spam=np.ones(10), eggs=np.zeros(10))
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
 def test_grid_search_with_fit_params():
-    check_hyperparameter_searcher_with_fit_params(GridSearchCV)
+    check_hyperparameter_searcher_with_fit_params(GridSearchCV,
+                                                  error_score='raise')
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
 def test_random_search_with_fit_params():
-    check_hyperparameter_searcher_with_fit_params(RandomizedSearchCV, n_iter=1)
+    check_hyperparameter_searcher_with_fit_params(RandomizedSearchCV, n_iter=1,
+                                                  error_score='raise')
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
 def test_grid_search_fit_params_deprecation():
     # NOTE: Remove this test in v0.21
 
@@ -241,6 +252,8 @@ def test_grid_search_fit_params_deprecation():
     assert_warns(DeprecationWarning, grid_search.fit, X, y)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_grid_search_fit_params_two_places():
     # NOTE: Remove this test in v0.21
 
@@ -264,10 +277,16 @@ def test_grid_search_fit_params_two_places():
 
     # Verify that `fit` prefers its own kwargs by giving valid
     # kwargs in the constructor and invalid in the method call
-    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]},
-                               fit_params={'spam': np.ones(10)})
-    assert_raise_message(AssertionError, "Fit parameter spam has length 1",
-                         grid_search.fit, X, y, spam=np.ones(1))
+    with warnings.catch_warnings():
+        # JvR: As passing fit params to the constructor is deprecated, this
+        # unit test raises a warning (unit test can be removed after version
+        # 0.22)
+        warnings.filterwarnings("ignore", category=DeprecationWarning)
+        grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]},
+                                   fit_params={'spam': np.ones(10)},
+                                   error_score='raise')
+        assert_raise_message(AssertionError, "Fit parameter spam has length 1",
+                             grid_search.fit, X, y, spam=np.ones(1))
 
 
 @ignore_warnings
@@ -296,6 +315,8 @@ def test_grid_search_no_score():
                          [[1]])
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_grid_search_score_method():
     X, y = make_classification(n_samples=100, n_classes=2, flip_y=.2,
                                random_state=0)
@@ -305,7 +326,8 @@ def test_grid_search_score_method():
     search_no_scoring = GridSearchCV(clf, grid, scoring=None).fit(X, y)
     search_accuracy = GridSearchCV(clf, grid, scoring='accuracy').fit(X, y)
     search_no_score_method_auc = GridSearchCV(LinearSVCNoScore(), grid,
-                                              scoring='roc_auc').fit(X, y)
+                                              scoring='roc_auc'
+                                              ).fit(X, y)
     search_auc = GridSearchCV(clf, grid, scoring='roc_auc').fit(X, y)
 
     # Check warning only occurs in situation where behavior changed:
@@ -324,6 +346,8 @@ def test_grid_search_score_method():
     assert_almost_equal(score_auc, score_no_score_auc)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_grid_search_groups():
     # Check if ValueError (when groups is None) propagates to GridSearchCV
     # And also check if groups is correctly passed to the cv object
@@ -358,9 +382,10 @@ def test_return_train_score_warn():
     y = np.array([0] * 5 + [1] * 5)
     grid = {'C': [1, 2]}
 
-    estimators = [GridSearchCV(LinearSVC(random_state=0), grid, iid=False),
+    estimators = [GridSearchCV(LinearSVC(random_state=0), grid,
+                               iid=False, cv=3),
                   RandomizedSearchCV(LinearSVC(random_state=0), grid,
-                                     n_iter=2, iid=False)]
+                                     n_iter=2, iid=False, cv=3)]
 
     result = {}
     for estimator in estimators:
@@ -388,6 +413,8 @@ def test_return_train_score_warn():
             assert_no_warnings(result['warn'].get, key)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_classes__property():
     # Test that classes_ property matches best_estimator_.classes_
     X = np.arange(100).reshape(10, 10)
@@ -415,6 +442,8 @@ def test_classes__property():
     assert_false(hasattr(grid_search, 'classes_'))
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_trivial_cv_results_attr():
     # Test search over a "grid" with only one point.
     clf = MockClassifier()
@@ -427,6 +456,8 @@ def test_trivial_cv_results_attr():
     assert_true(hasattr(grid_search, "cv_results_"))
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_no_refit():
     # Test that GSCV can be used for model selection alone without refitting
     clf = MockClassifier()
@@ -452,10 +483,13 @@ def test_no_refit():
                              "parameter refit must be set to a scorer key",
                              GridSearchCV(clf, {}, refit=refit,
                                           scoring={'acc': 'accuracy',
-                                                   'prec': 'precision'}).fit,
+                                                   'prec': 'precision'}
+                                          ).fit,
                              X, y)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_grid_search_error():
     # Test that grid search will capture errors on data with different length
     X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
@@ -465,11 +499,13 @@ def test_grid_search_error():
     assert_raises(ValueError, cv.fit, X_[:180], y_)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_grid_search_one_grid_point():
     X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
     param_dict = {"C": [1.0], "kernel": ["rbf"], "gamma": [0.1]}
 
-    clf = SVC()
+    clf = SVC(gamma='auto')
     cv = GridSearchCV(clf, param_dict)
     cv.fit(X_, y_)
 
@@ -479,6 +515,8 @@ def test_grid_search_one_grid_point():
     assert_array_equal(clf.dual_coef_, cv.best_estimator_.dual_coef_)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_grid_search_when_param_grid_includes_range():
     # Test that the best estimator contains the right value for foo_param
     clf = MockClassifier()
@@ -491,9 +529,11 @@ def test_grid_search_when_param_grid_includes_range():
     assert_equal(grid_search.best_estimator_.foo_param, 2)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_grid_search_bad_param_grid():
     param_dict = {"C": 1.0}
-    clf = SVC()
+    clf = SVC(gamma='auto')
     assert_raise_message(
         ValueError,
         "Parameter values for parameter (C) need to be a sequence"
@@ -508,18 +548,20 @@ def test_grid_search_bad_param_grid():
         GridSearchCV, clf, param_dict)
 
     param_dict = {"C": "1,2,3"}
-    clf = SVC()
+    clf = SVC(gamma='auto')
     assert_raise_message(
         ValueError,
         "Parameter values for parameter (C) need to be a sequence"
         "(but not a string) or np.ndarray.",
         GridSearchCV, clf, param_dict)
 
-    param_dict = {"C": np.ones(6).reshape(3, 2)}
+    param_dict = {"C": np.ones((3, 2))}
     clf = SVC(gamma="scale")
     assert_raises(ValueError, GridSearchCV, clf, param_dict)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_grid_search_sparse():
     # Test that grid search works with both dense and sparse matrices
     X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
@@ -541,6 +583,8 @@ def test_grid_search_sparse():
     assert_equal(C, C2)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_grid_search_sparse_scoring():
     X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
 
@@ -576,6 +620,8 @@ def f1_loss(y_true_, y_pred_):
     assert_array_equal(y_pred, y_pred3)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_grid_search_precomputed_kernel():
     # Test that grid search works when the input features are given in the
     # form of a precomputed kernel matrix
@@ -604,6 +650,8 @@ def test_grid_search_precomputed_kernel():
     assert_raises(ValueError, cv.fit, K_train.tolist(), y_train)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_grid_search_precomputed_kernel_error_nonsquare():
     # Test that grid search returns an error with a non-square precomputed
     # training kernel matrix
@@ -641,6 +689,8 @@ def test_refit():
     clf.fit(X, y)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_gridsearch_nd():
     # Pass X as list in GridSearchCV
     X_4d = np.arange(10 * 5 * 3 * 2).reshape(10, 5, 3, 2)
@@ -653,6 +703,7 @@ def test_gridsearch_nd():
     assert_true(hasattr(grid_search, "cv_results_"))
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
 def test_X_as_list():
     # Pass X as list in GridSearchCV
     X = np.arange(100).reshape(10, 10)
@@ -665,6 +716,7 @@ def test_X_as_list():
     assert_true(hasattr(grid_search, "cv_results_"))
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
 def test_y_as_list():
     # Pass y as list in GridSearchCV
     X = np.arange(100).reshape(10, 10)
@@ -708,6 +760,8 @@ def check_series(x):
         assert_true(hasattr(grid_search, "cv_results_"))
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_unsupervised_grid_search():
     # test grid-search with unsupervised estimator
     X, y = make_blobs(random_state=0)
@@ -734,6 +788,8 @@ def test_unsupervised_grid_search():
     assert_equal(grid_search.best_params_["n_clusters"], 4)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_gridsearch_no_predict():
     # test grid-search with an estimator without predict.
     # slight duplication of a test from KDE
@@ -822,7 +878,7 @@ def test_grid_search_cv_results():
 
     for iid in (False, True):
         search = GridSearchCV(SVC(gamma='scale'), cv=n_splits, iid=iid,
-                              param_grid=params)
+                              param_grid=params, return_train_score=True)
         search.fit(X, y)
         assert_equal(iid, search.iid)
         cv_results = search.cv_results_
@@ -873,7 +929,8 @@ def test_random_search_cv_results():
     for iid in (False, True):
         search = RandomizedSearchCV(SVC(gamma='scale'), n_iter=n_search_iter,
                                     cv=n_splits, iid=iid,
-                                    param_distributions=params)
+                                    param_distributions=params,
+                                    return_train_score=True)
         search.fit(X, y)
         assert_equal(iid, search.iid)
         cv_results = search.cv_results_
@@ -881,8 +938,8 @@ def test_random_search_cv_results():
         check_cv_results_array_types(search, param_keys, score_keys)
         check_cv_results_keys(cv_results, param_keys, score_keys, n_cand)
         # For random_search, all the param array vals should be unmasked
-        assert_false(any(cv_results['param_C'].mask) or
-                     any(cv_results['param_gamma'].mask))
+        assert_false(any(np.ma.getmaskarray(cv_results['param_C'])) or
+                     any(np.ma.getmaskarray(cv_results['param_gamma'])))
 
 
 @ignore_warnings(category=DeprecationWarning)
@@ -901,11 +958,12 @@ def test_search_iid_param():
     # create "cv" for splits
     cv = [[mask, ~mask], [~mask, mask]]
     # once with iid=True (default)
-    grid_search = GridSearchCV(SVC(), param_grid={'C': [1, 10]},
-                               cv=cv)
-    random_search = RandomizedSearchCV(SVC(), n_iter=2,
+    grid_search = GridSearchCV(SVC(gamma='auto'), param_grid={'C': [1, 10]},
+                               cv=cv, return_train_score=True)
+    random_search = RandomizedSearchCV(SVC(gamma='auto'), n_iter=2,
                                        param_distributions={'C': [1, 10]},
-                                       cv=cv)
+                                       cv=cv, iid=True,
+                                       return_train_score=True)
     for search in (grid_search, random_search):
         search.fit(X, y)
         assert_true(search.iid or search.iid is None)
@@ -936,7 +994,7 @@ def test_search_iid_param():
         assert_almost_equal(test_mean, expected_test_mean)
         assert_almost_equal(test_std, expected_test_std)
         assert_array_almost_equal(test_cv_scores,
-                                  cross_val_score(SVC(C=1), X,
+                                  cross_val_score(SVC(C=1, gamma='auto'), X,
                                                   y, cv=cv))
 
         # For the train scores, we do not take a weighted mean irrespective of
@@ -945,12 +1003,13 @@ def test_search_iid_param():
         assert_almost_equal(train_std, 0)
 
     # once with iid=False
-    grid_search = GridSearchCV(SVC(),
+    grid_search = GridSearchCV(SVC(gamma='auto'),
                                param_grid={'C': [1, 10]},
-                               cv=cv, iid=False)
-    random_search = RandomizedSearchCV(SVC(), n_iter=2,
+                               cv=cv, iid=False, return_train_score=True)
+    random_search = RandomizedSearchCV(SVC(gamma='auto'), n_iter=2,
                                        param_distributions={'C': [1, 10]},
-                                       cv=cv, iid=False)
+                                       cv=cv, iid=False,
+                                       return_train_score=True)
 
     for search in (grid_search, random_search):
         search.fit(X, y)
@@ -1083,6 +1142,8 @@ def compare_refit_methods_when_refit_with_acc(search_multi, search_acc, refit):
         assert_equal(getattr(search_multi, key), getattr(search_acc, key))
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_search_cv_results_rank_tie_breaking():
     X, y = make_blobs(n_samples=50, random_state=42)
 
@@ -1090,9 +1151,11 @@ def test_search_cv_results_rank_tie_breaking():
     # which would result in a tie of their mean cv-scores
     param_grid = {'C': [1, 1.001, 0.001]}
 
-    grid_search = GridSearchCV(SVC(gamma="scale"), param_grid=param_grid)
+    grid_search = GridSearchCV(SVC(gamma="scale"), param_grid=param_grid,
+                               return_train_score=True)
     random_search = RandomizedSearchCV(SVC(gamma="scale"), n_iter=3,
-                                       param_distributions=param_grid)
+                                       param_distributions=param_grid,
+                                       return_train_score=True)
 
     for search in (grid_search, random_search):
         search.fit(X, y)
@@ -1112,6 +1175,8 @@ def test_search_cv_results_rank_tie_breaking():
         assert_almost_equal(search.cv_results_['rank_test_score'], [1, 1, 3])
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_search_cv_results_none_param():
     X, y = [[1], [2], [3], [4], [5]], [0, 0, 0, 0, 1]
     estimators = (DecisionTreeRegressor(), DecisionTreeClassifier())
@@ -1119,7 +1184,8 @@ def test_search_cv_results_none_param():
     cv = KFold(random_state=0)
 
     for est in estimators:
-        grid_search = GridSearchCV(est, est_parameters, cv=cv).fit(X, y)
+        grid_search = GridSearchCV(est, est_parameters, cv=cv,
+                                   ).fit(X, y)
         assert_array_equal(grid_search.cv_results_['param_random_state'],
                            [0, None])
 
@@ -1152,6 +1218,7 @@ def test_search_cv_timing():
         assert_greater_equal(search.refit_time_, 0)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
 def test_grid_search_correct_score_results():
     # test that correct scores are used
     n_splits = 3
@@ -1187,6 +1254,7 @@ def test_grid_search_correct_score_results():
                 assert_almost_equal(correct_score, cv_scores[i])
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_fit_grid_point():
     X, y = make_classification(random_state=0)
     cv = StratifiedKFold(random_state=0)
@@ -1215,6 +1283,8 @@ def test_fit_grid_point():
                          {'score': scorer}, verbose=True)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_pickle():
     # Test that a fit search can be pickled
     clf = MockClassifier()
@@ -1232,6 +1302,8 @@ def test_pickle():
                               random_search_pickled.predict(X))
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_grid_search_with_multioutput_data():
     # Test search with multi-output estimator
 
@@ -1277,6 +1349,7 @@ def test_grid_search_with_multioutput_data():
                                               % i][cand_i])
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
 def test_predict_proba_disabled():
     # Test predict_proba when disabled on estimator.
     X = np.arange(20).reshape(5, -1)
@@ -1286,6 +1359,7 @@ def test_predict_proba_disabled():
     assert_false(hasattr(gs, "predict_proba"))
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
 def test_grid_search_allows_nans():
     # Test GridSearchCV with SimpleImputer
     X = np.arange(20, dtype=np.float64).reshape(5, -1)
@@ -1313,7 +1387,11 @@ def fit(self, X, y=None):
     def predict(self, X):
         return np.zeros(X.shape[0])
 
+    def score(self, X=None, Y=None):
+        return 0.
+
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
 def test_grid_search_failing_classifier():
     # GridSearchCV with on_error != 'raise'
     # Ensures that a warning is raised and score reset where appropriate.
@@ -1361,6 +1439,8 @@ def get_cand_scores(i):
     assert gs.best_index_ != clf.FAILING_PARAMETER
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_grid_search_failing_classifier_raise():
     # GridSearchCV with on_error == 'raise' raises the error
 
@@ -1412,6 +1492,8 @@ def test_parameters_sampler_replacement():
     assert_equal(len(samples), 7)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_stochastic_gradient_loss_param():
     # Make sure the predict_proba works when loss is specified
     # as one of the parameters in the param_grid.
@@ -1442,6 +1524,8 @@ def test_stochastic_gradient_loss_param():
     assert_false(hasattr(clf, "predict_proba"))
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_search_train_scores_set_to_false():
     X = np.arange(6).reshape(6, -1)
     y = [0, 0, 0, 1, 1, 1]
@@ -1452,6 +1536,7 @@ def test_search_train_scores_set_to_false():
     gs.fit(X, y)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
 def test_grid_search_cv_splits_consistency():
     # Check if a one time iterable is accepted as a cv parameter.
     n_samples = 100
@@ -1461,12 +1546,13 @@ def test_grid_search_cv_splits_consistency():
     gs = GridSearchCV(LinearSVC(random_state=0),
                       param_grid={'C': [0.1, 0.2, 0.3]},
                       cv=OneTimeSplitter(n_splits=n_splits,
-                                         n_samples=n_samples))
+                                         n_samples=n_samples),
+                      return_train_score=True)
     gs.fit(X, y)
 
     gs2 = GridSearchCV(LinearSVC(random_state=0),
                        param_grid={'C': [0.1, 0.2, 0.3]},
-                       cv=KFold(n_splits=n_splits))
+                       cv=KFold(n_splits=n_splits), return_train_score=True)
     gs2.fit(X, y)
 
     # Give generator as a cv parameter
@@ -1476,13 +1562,14 @@ def test_grid_search_cv_splits_consistency():
     gs3 = GridSearchCV(LinearSVC(random_state=0),
                        param_grid={'C': [0.1, 0.2, 0.3]},
                        cv=KFold(n_splits=n_splits, shuffle=True,
-                                random_state=0).split(X, y))
+                                random_state=0).split(X, y),
+                       return_train_score=True)
     gs3.fit(X, y)
 
     gs4 = GridSearchCV(LinearSVC(random_state=0),
                        param_grid={'C': [0.1, 0.2, 0.3]},
                        cv=KFold(n_splits=n_splits, shuffle=True,
-                                random_state=0))
+                                random_state=0), return_train_score=True)
     gs4.fit(X, y)
 
     def _pop_time_keys(cv_results):
@@ -1510,7 +1597,8 @@ def _pop_time_keys(cv_results):
     # Check consistency of folds across the parameters
     gs = GridSearchCV(LinearSVC(random_state=0),
                       param_grid={'C': [0.1, 0.1, 0.2, 0.2]},
-                      cv=KFold(n_splits=n_splits, shuffle=True))
+                      cv=KFold(n_splits=n_splits, shuffle=True),
+                      return_train_score=True)
     gs.fit(X, y)
 
     # As the first two param settings (C=0.1) and the next two param
@@ -1530,6 +1618,8 @@ def _pop_time_keys(cv_results):
                                   per_param_scores[3])
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_transform_inverse_transform_round_trip():
     clf = MockClassifier()
     grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, verbose=3)
@@ -1539,6 +1629,55 @@ def test_transform_inverse_transform_round_trip():
     assert_array_equal(X, X_round_trip)
 
 
+def test_custom_run_search():
+    def check_results(results, gscv):
+        exp_results = gscv.cv_results_
+        assert sorted(results.keys()) == sorted(exp_results)
+        for k in results:
+            if not k.endswith('_time'):
+                # XXX: results['params'] is a list :|
+                results[k] = np.asanyarray(results[k])
+                if results[k].dtype.kind == 'O':
+                    assert_array_equal(exp_results[k], results[k],
+                                       err_msg='Checking ' + k)
+                else:
+                    assert_allclose(exp_results[k], results[k],
+                                    err_msg='Checking ' + k)
+
+    def fit_grid(param_grid):
+        return GridSearchCV(clf, param_grid, cv=5,
+                            return_train_score=True).fit(X, y)
+
+    class CustomSearchCV(BaseSearchCV):
+        def __init__(self, estimator, **kwargs):
+            super(CustomSearchCV, self).__init__(estimator, **kwargs)
+
+        def _run_search(self, evaluate):
+            results = evaluate([{'max_depth': 1}, {'max_depth': 2}])
+            check_results(results, fit_grid({'max_depth': [1, 2]}))
+            results = evaluate([{'min_samples_split': 5},
+                                {'min_samples_split': 10}])
+            check_results(results, fit_grid([{'max_depth': [1, 2]},
+                                             {'min_samples_split': [5, 10]}]))
+
+    # Using regressor to make sure each score differs
+    clf = DecisionTreeRegressor(random_state=0)
+    X, y = make_classification(n_samples=100, n_informative=4,
+                               random_state=0)
+    mycv = CustomSearchCV(clf, cv=5, return_train_score=True).fit(X, y)
+    gscv = fit_grid([{'max_depth': [1, 2]},
+                     {'min_samples_split': [5, 10]}])
+
+    results = mycv.cv_results_
+    check_results(results, gscv)
+    for attr in dir(gscv):
+        if attr[0].islower() and attr[-1:] == '_' and \
+           attr not in {'cv_results_', 'best_estimator_',
+                        'refit_time_'}:
+            assert getattr(gscv, attr) == getattr(mycv, attr), \
+                   "Attribute %s not equal" % attr
+
+
 def test_deprecated_grid_search_iid():
     depr_message = ("The default of the `iid` parameter will change from True "
                     "to False in version 0.22")
diff --git a/sklearn/model_selection/tests/test_split.py b/sklearn/model_selection/tests/test_split.py
index 0071129d8ce7..710c194cfc69 100644
--- a/sklearn/model_selection/tests/test_split.py
+++ b/sklearn/model_selection/tests/test_split.py
@@ -1,7 +1,7 @@
 """Test the split module"""
 from __future__ import division
 import warnings
-
+import pytest
 import numpy as np
 from scipy.sparse import coo_matrix, csc_matrix, csr_matrix
 from scipy import stats
@@ -23,6 +23,7 @@
 from sklearn.utils.testing import assert_warns
 from sklearn.utils.testing import assert_raise_message
 from sklearn.utils.testing import ignore_warnings
+from sklearn.utils.testing import assert_no_warnings
 from sklearn.utils.validation import _num_samples
 from sklearn.utils.mocking import MockDataFrame
 
@@ -50,6 +51,8 @@
 from sklearn.model_selection._split import _validate_shuffle_split
 from sklearn.model_selection._split import _CVIterableWrapper
 from sklearn.model_selection._split import _build_repr
+from sklearn.model_selection._split import CV_WARNING
+from sklearn.model_selection._split import NSPLIT_WARNING
 
 from sklearn.datasets import load_digits
 from sklearn.datasets import make_classification
@@ -199,6 +202,7 @@ def test_cross_validator_with_default_params():
                          lpo.get_n_splits, None, y, groups)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_2d_y():
     # smoke test for 2d y and multi-label
     n_samples = 30
@@ -473,16 +477,18 @@ def test_shuffle_kfold_stratifiedkfold_reproducibility():
 
     for cv in (kf, skf):
         for data in zip((X, X2), (y, y2)):
-            # Test if the two splits are different
-            # numpy's assert_equal properly compares nested lists
-            try:
-                np.testing.assert_array_equal(list(cv.split(*data)),
-                                              list(cv.split(*data)))
-            except AssertionError:
-                pass
-            else:
-                raise AssertionError("The splits for data, %s, are same even "
-                                     "when random state is not set" % data)
+            # Test if the two splits are different cv
+            for (_, test_a), (_, test_b) in zip(cv.split(*data),
+                                                cv.split(*data)):
+                # cv.split(...) returns an array of tuples, each tuple
+                # consisting of an array with train indices and test indices
+                try:
+                    np.testing.assert_array_equal(test_a, test_b)
+                except AssertionError:
+                    pass
+                else:
+                    raise AssertionError("The splits for data, are same even "
+                                         "when random state is not set")
 
 
 def test_shuffle_stratifiedkfold():
@@ -751,7 +757,7 @@ def test_stratified_shuffle_split_multilabel_many_labels():
 
 def test_predefinedsplit_with_kfold_split():
     # Check that PredefinedSplit can reproduce a split generated by Kfold.
-    folds = -1 * np.ones(10)
+    folds = np.full(10, -1.)
     kf_train = []
     kf_test = []
     for i, (train_ind, test_ind) in enumerate(KFold(5, shuffle=True).split(X)):
@@ -1004,7 +1010,12 @@ def test_repeated_stratified_kfold_determinstic_split():
 
 def test_train_test_split_errors():
     assert_raises(ValueError, train_test_split)
-    assert_raises(ValueError, train_test_split, range(3), train_size=1.1)
+    with warnings.catch_warnings():
+        # JvR: Currently, a future warning is raised if test_size is not
+        # given. As that is the point of this test, ignore the future warning
+        warnings.filterwarnings("ignore", category=FutureWarning)
+        assert_raises(ValueError, train_test_split, range(3), train_size=1.1)
+
     assert_raises(ValueError, train_test_split, range(3), test_size=0.6,
                   train_size=0.6)
     assert_raises(ValueError, train_test_split, range(3),
@@ -1391,6 +1402,7 @@ def test_time_series_max_train_size():
     _check_time_series_max_train_size(splits, check_splits, max_train_size=2)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_nested_cv():
     # Test if nested cross validation works with different combinations of cv
     rng = np.random.RandomState(0)
@@ -1403,7 +1415,7 @@ def test_nested_cv():
 
     for inner_cv, outer_cv in combinations_with_replacement(cvs, 2):
         gs = GridSearchCV(Ridge(), param_grid={'alpha': [1, .1]},
-                          cv=inner_cv)
+                          cv=inner_cv, error_score='raise', iid=False)
         cross_val_score(gs, X=X, y=y, groups=groups, cv=outer_cv,
                         fit_params={'groups': groups})
 
@@ -1416,6 +1428,26 @@ def test_train_test_default_warning():
                  train_size=0.75)
 
 
+def test_nsplit_default_warn():
+    # Test that warnings are raised. Will be removed in 0.22
+    assert_warns_message(FutureWarning, NSPLIT_WARNING, KFold)
+    assert_warns_message(FutureWarning, NSPLIT_WARNING, GroupKFold)
+    assert_warns_message(FutureWarning, NSPLIT_WARNING, StratifiedKFold)
+    assert_warns_message(FutureWarning, NSPLIT_WARNING, TimeSeriesSplit)
+
+    assert_no_warnings(KFold, n_splits=5)
+    assert_no_warnings(GroupKFold, n_splits=5)
+    assert_no_warnings(StratifiedKFold, n_splits=5)
+    assert_no_warnings(TimeSeriesSplit, n_splits=5)
+
+
+def test_check_cv_default_warn():
+    # Test that warnings are raised. Will be removed in 0.22
+    assert_warns_message(FutureWarning, CV_WARNING, check_cv)
+    assert_warns_message(FutureWarning, CV_WARNING, check_cv, None)
+    assert_no_warnings(check_cv, cv=5)
+
+
 def test_build_repr():
     class MockSplitter:
         def __init__(self, a, b=0, c=None):
diff --git a/sklearn/model_selection/tests/test_validation.py b/sklearn/model_selection/tests/test_validation.py
index f2aff6b82bea..bb7e736eb3c3 100644
--- a/sklearn/model_selection/tests/test_validation.py
+++ b/sklearn/model_selection/tests/test_validation.py
@@ -239,6 +239,7 @@ def get_params(self, deep=False):
 P_sparse = coo_matrix(np.eye(5))
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_score():
     clf = MockClassifier()
 
@@ -276,9 +277,11 @@ def test_cross_val_score():
     scores = cross_val_score(clf, X_3d, y2)
 
     clf = MockClassifier(allow_nd=False)
-    assert_raises(ValueError, cross_val_score, clf, X_3d, y2)
+    assert_raises(ValueError, cross_val_score, clf, X_3d, y2,
+                  error_score='raise')
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_validate_invalid_scoring_param():
     X, y = make_classification(random_state=0)
     estimator = MockClassifier()
@@ -400,7 +403,7 @@ def test_cross_validate_return_train_score_warn():
     result = {}
     for val in [False, True, 'warn']:
         result[val] = assert_no_warnings(cross_validate, estimator, X, y,
-                                         return_train_score=val)
+                                         return_train_score=val, cv=5)
 
     msg = (
         'You are accessing a training score ({!r}), '
@@ -420,9 +423,10 @@ def check_cross_validate_single_metric(clf, X, y, scores):
     for (return_train_score, dict_len) in ((True, 4), (False, 3)):
         # Single metric passed as a string
         if return_train_score:
-            # It must be True by default
+            # It must be True by default - deprecated
             mse_scores_dict = cross_validate(clf, X, y, cv=5,
-                                             scoring='neg_mean_squared_error')
+                                             scoring='neg_mean_squared_error',
+                                             return_train_score=True)
             assert_array_almost_equal(mse_scores_dict['train_score'],
                                       train_mse_scores)
         else:
@@ -436,10 +440,11 @@ def check_cross_validate_single_metric(clf, X, y, scores):
 
         # Single metric passed as a list
         if return_train_score:
-            # It must be True by default
-            r2_scores_dict = cross_validate(clf, X, y, cv=5, scoring=['r2'])
+            # It must be True by default - deprecated
+            r2_scores_dict = cross_validate(clf, X, y, cv=5, scoring=['r2'],
+                                            return_train_score=True)
             assert_array_almost_equal(r2_scores_dict['train_r2'],
-                                      train_r2_scores)
+                                      train_r2_scores, True)
         else:
             r2_scores_dict = cross_validate(clf, X, y, cv=5, scoring=['r2'],
                                             return_train_score=False)
@@ -472,8 +477,9 @@ def check_cross_validate_multi_metric(clf, X, y, scores):
     for return_train_score in (True, False):
         for scoring in all_scoring:
             if return_train_score:
-                # return_train_score must be True by default
-                cv_results = cross_validate(clf, X, y, cv=5, scoring=scoring)
+                # return_train_score must be True by default - deprecated
+                cv_results = cross_validate(clf, X, y, cv=5, scoring=scoring,
+                                            return_train_score=True)
                 assert_array_almost_equal(cv_results['train_r2'],
                                           train_r2_scores)
                 assert_array_almost_equal(
@@ -504,6 +510,7 @@ def check_cross_validate_multi_metric(clf, X, y, scores):
             assert np.all(cv_results['score_time'] < 10)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_score_predict_groups():
     # Check if ValueError (when groups is None) propagates to cross_val_score
     # and cross_val_predict
@@ -523,6 +530,8 @@ def test_cross_val_score_predict_groups():
                              cross_val_predict, estimator=clf, X=X, y=y, cv=cv)
 
 
+@pytest.mark.filterwarnings('ignore: Using or importing the ABCs from')
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_score_pandas():
     # check cross_val_score doesn't destroy pandas dataframe
     types = [(MockDataFrame, MockDataFrame)]
@@ -560,6 +569,7 @@ def test_cross_val_score_mask():
     assert_array_equal(scores_indices, scores_masks)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_score_precomputed():
     # test for svm with precomputed kernel
     svm = SVC(kernel="precomputed")
@@ -586,6 +596,7 @@ def test_cross_val_score_precomputed():
                   linear_kernel.tolist(), y)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_score_fit_params():
     clf = MockClassifier()
     n_samples = X.shape[0]
@@ -608,7 +619,7 @@ def assert_fit_params(clf):
         assert_equal(clf.dummy_obj, DUMMY_OBJ)
 
     fit_params = {'sample_weight': np.ones(n_samples),
-                  'class_prior': np.ones(n_classes) / n_classes,
+                  'class_prior': np.full(n_classes, 1. / n_classes),
                   'sparse_sample_weight': W_sparse,
                   'sparse_param': P_sparse,
                   'dummy_int': DUMMY_INT,
@@ -779,6 +790,7 @@ def test_cross_val_score_multilabel():
     assert_almost_equal(score_samples, [1, 1 / 2, 3 / 4, 1 / 2, 1 / 4])
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict():
     boston = load_boston()
     X, y = boston.data, boston.target
@@ -828,6 +840,7 @@ def split(self, X, y=None, groups=None):
                          X, y, method='predict_proba', cv=KFold(2))
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict_decision_function_shape():
     X, y = make_classification(n_classes=2, n_samples=50, random_state=0)
 
@@ -874,6 +887,7 @@ def test_cross_val_predict_decision_function_shape():
                         cv=KFold(n_splits=3), method='decision_function')
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict_predict_proba_shape():
     X, y = make_classification(n_classes=2, n_samples=50, random_state=0)
 
@@ -888,6 +902,7 @@ def test_cross_val_predict_predict_proba_shape():
     assert_equal(preds.shape, (150, 3))
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict_predict_log_proba_shape():
     X, y = make_classification(n_classes=2, n_samples=50, random_state=0)
 
@@ -902,6 +917,7 @@ def test_cross_val_predict_predict_log_proba_shape():
     assert_equal(preds.shape, (150, 3))
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict_input_types():
     iris = load_iris()
     X, y = iris.data, iris.target
@@ -947,6 +963,9 @@ def test_cross_val_predict_input_types():
     assert_array_equal(predictions.shape, (150,))
 
 
+@pytest.mark.filterwarnings('ignore: Using or importing the ABCs from')
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
+# python3.7 deprecation warnings in pandas via matplotlib :-/
 def test_cross_val_predict_pandas():
     # check cross_val_score doesn't destroy pandas dataframe
     types = [(MockDataFrame, MockDataFrame)]
@@ -964,6 +983,7 @@ def test_cross_val_predict_pandas():
         cross_val_predict(clf, X_df, y_ser)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_score_sparse_fit_params():
     iris = load_iris()
     X, y = iris.data, iris.target
@@ -1172,7 +1192,8 @@ def test_learning_curve_with_shuffle():
     assert_array_almost_equal(test_scores_batch.mean(axis=1),
                               np.array([0.36111111, 0.25, 0.25]))
     assert_raises(ValueError, learning_curve, estimator, X, y, cv=cv, n_jobs=1,
-                  train_sizes=np.linspace(0.3, 1.0, 3), groups=groups)
+                  train_sizes=np.linspace(0.3, 1.0, 3), groups=groups,
+                  error_score='raise')
 
     train_sizes_inc, train_scores_inc, test_scores_inc = learning_curve(
         estimator, X, y, cv=cv, n_jobs=1, train_sizes=np.linspace(0.3, 1.0, 3),
@@ -1315,10 +1336,13 @@ def check_cross_val_predict_with_method(est):
         assert_array_equal(predictions, predictions_ystr)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict_with_method():
     check_cross_val_predict_with_method(LogisticRegression())
 
 
+@pytest.mark.filterwarnings('ignore: max_iter and tol parameters')
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict_method_checking():
     # Regression test for issue #9639. Tests that cross_val_predict does not
     # check estimator methods (e.g. predict_proba) before fitting
@@ -1326,6 +1350,8 @@ def test_cross_val_predict_method_checking():
     check_cross_val_predict_with_method(est)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_gridsearchcv_cross_val_predict_with_method():
     est = GridSearchCV(LogisticRegression(random_state=42),
                        {'C': [0.1, 1]},
@@ -1353,6 +1379,7 @@ def get_expected_predictions(X, y, cv, classes, est, method):
     return expected_predictions
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict_class_subset():
 
     X = np.arange(200).reshape(100, 2)
@@ -1394,6 +1421,7 @@ def test_cross_val_predict_class_subset():
         assert_array_almost_equal(expected_predictions, predictions)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_score_memmap():
     # Ensure a scalar score of memmap type is accepted
     iris = load_iris()
@@ -1421,6 +1449,8 @@ def test_score_memmap():
                 sleep(1.)
 
 
+@pytest.mark.filterwarnings('ignore: Using or importing the ABCs from')
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_permutation_test_score_pandas():
     # check permutation_test_score doesn't destroy pandas dataframe
     types = [(MockDataFrame, MockDataFrame)]
@@ -1445,6 +1475,7 @@ def test_fit_and_score():
     failing_clf = FailingClassifier(FailingClassifier.FAILING_PARAMETER)
     # dummy X data
     X = np.arange(1, 10)
+    y = np.ones(9)
     fit_and_score_args = [failing_clf, X, None, dict(), None, None, 0,
                           None, None]
     # passing error score to trigger the warning message
@@ -1463,10 +1494,6 @@ def test_fit_and_score():
     assert_warns_message(FitFailedWarning, warning_message, _fit_and_score,
                          *fit_and_score_args, **fit_and_score_kwargs)
 
-    # check if exception is raised, with default error_score argument
-    assert_raise_message(ValueError, "Failing classifier failed as required",
-                         _fit_and_score, *fit_and_score_args)
-
     # check if warning was raised, with default error_score argument
     warning_message = ("From version 0.22, errors during fit will result "
                        "in a cross validation score of NaN by default. Use "
@@ -1482,3 +1509,24 @@ def test_fit_and_score():
     assert_raise_message(ValueError, "Failing classifier failed as required",
                          _fit_and_score, *fit_and_score_args,
                          **fit_and_score_kwargs)
+
+    # check that functions upstream pass error_score param to _fit_and_score
+    error_message = ("error_score must be the string 'raise' or a"
+                     " numeric value. (Hint: if using 'raise', please"
+                     " make sure that it has been spelled correctly.)")
+
+    assert_raise_message(ValueError, error_message, cross_validate,
+                         failing_clf, X, cv=3, error_score='unvalid-string')
+
+    assert_raise_message(ValueError, error_message, cross_val_score,
+                         failing_clf, X, cv=3, error_score='unvalid-string')
+
+    assert_raise_message(ValueError, error_message, learning_curve,
+                         failing_clf, X, y, cv=3, error_score='unvalid-string')
+
+    assert_raise_message(ValueError, error_message, validation_curve,
+                         failing_clf, X, y, 'parameter',
+                         [FailingClassifier.FAILING_PARAMETER], cv=3,
+                         error_score='unvalid-string')
+
+    assert_equal(failing_clf.score(), 0.)  # FailingClassifier coverage
diff --git a/sklearn/multiclass.py b/sklearn/multiclass.py
index 0fc2907fe481..0361dfa4d4f3 100644
--- a/sklearn/multiclass.py
+++ b/sklearn/multiclass.py
@@ -52,8 +52,8 @@
                                _ovr_decision_function)
 from .utils.metaestimators import _safe_split, if_delegate_has_method
 
-from .externals.joblib import Parallel
-from .externals.joblib import delayed
+from .utils import Parallel
+from .utils import delayed
 from .externals.six.moves import zip as izip
 
 __all__ = [
@@ -157,11 +157,11 @@ class OneVsRestClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):
         An estimator object implementing `fit` and one of `decision_function`
         or `predict_proba`.
 
-    n_jobs : int, optional, default: 1
-        The number of jobs to use for the computation. If -1 all CPUs are used.
-        If 1 is given, no parallel computing code is used at all, which is
-        useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are
-        used. Thus for n_jobs = -2, all CPUs but one are used.
+    n_jobs : int or None, optional (default=None)
+        The number of jobs to use for the computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -176,7 +176,7 @@ class OneVsRestClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):
     multilabel_ : boolean
         Whether a OneVsRestClassifier is a multilabel classifier.
     """
-    def __init__(self, estimator, n_jobs=1):
+    def __init__(self, estimator, n_jobs=None):
         self.estimator = estimator
         self.n_jobs = n_jobs
 
@@ -456,11 +456,11 @@ class OneVsOneClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):
         An estimator object implementing `fit` and one of `decision_function`
         or `predict_proba`.
 
-    n_jobs : int, optional, default: 1
-        The number of jobs to use for the computation. If -1 all CPUs are used.
-        If 1 is given, no parallel computing code is used at all, which is
-        useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are
-        used. Thus for n_jobs = -2, all CPUs but one are used.
+    n_jobs : int or None, optional (default=None)
+        The number of jobs to use for the computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -471,7 +471,7 @@ class OneVsOneClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):
         Array containing labels.
     """
 
-    def __init__(self, estimator, n_jobs=1):
+    def __init__(self, estimator, n_jobs=None):
         self.estimator = estimator
         self.n_jobs = n_jobs
 
@@ -662,11 +662,11 @@ class OutputCodeClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):
         random_state is the random number generator; If None, the random number
         generator is the RandomState instance used by `np.random`.
 
-    n_jobs : int, optional, default: 1
-        The number of jobs to use for the computation. If -1 all CPUs are used.
-        If 1 is given, no parallel computing code is used at all, which is
-        useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are
-        used. Thus for n_jobs = -2, all CPUs but one are used.
+    n_jobs : int or None, optional (default=None)
+        The number of jobs to use for the computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -698,7 +698,8 @@ class OutputCodeClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):
        2008.
     """
 
-    def __init__(self, estimator, code_size=1.5, random_state=None, n_jobs=1):
+    def __init__(self, estimator, code_size=1.5, random_state=None,
+                 n_jobs=None):
         self.estimator = estimator
         self.code_size = code_size
         self.random_state = random_state
diff --git a/sklearn/multioutput.py b/sklearn/multioutput.py
index 4d9b9e10f4fb..1b0fdd19e14a 100644
--- a/sklearn/multioutput.py
+++ b/sklearn/multioutput.py
@@ -25,7 +25,7 @@
 from .utils.metaestimators import if_delegate_has_method
 from .utils.validation import check_is_fitted, has_fit_parameter
 from .utils.multiclass import check_classification_targets
-from .externals.joblib import Parallel, delayed
+from .utils import Parallel, delayed
 from .externals import six
 
 __all__ = ["MultiOutputRegressor", "MultiOutputClassifier",
@@ -63,7 +63,7 @@ def _partial_fit_estimator(estimator, X, y, classes=None, sample_weight=None,
 class MultiOutputEstimator(six.with_metaclass(ABCMeta, BaseEstimator,
                                               MetaEstimatorMixin)):
     @abstractmethod
-    def __init__(self, estimator, n_jobs=1):
+    def __init__(self, estimator, n_jobs=None):
         self.estimator = estimator
         self.n_jobs = n_jobs
 
@@ -209,15 +209,18 @@ class MultiOutputRegressor(MultiOutputEstimator, RegressorMixin):
     estimator : estimator object
         An estimator object implementing `fit` and `predict`.
 
-    n_jobs : int, optional, default=1
-        The number of jobs to run in parallel for `fit`. If -1,
-        then the number of jobs is set to the number of cores.
+    n_jobs : int or None, optional (default=None)
+        The number of jobs to run in parallel for `fit`.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
+
         When individual estimators are fast to train or predict
         using `n_jobs>1` can result in slower performance due
         to the overhead of spawning processes.
     """
 
-    def __init__(self, estimator, n_jobs=1):
+    def __init__(self, estimator, n_jobs=None):
         super(MultiOutputRegressor, self).__init__(estimator, n_jobs)
 
     @if_delegate_has_method('estimator')
@@ -295,13 +298,12 @@ class MultiOutputClassifier(MultiOutputEstimator, ClassifierMixin):
     estimator : estimator object
         An estimator object implementing `fit`, `score` and `predict_proba`.
 
-    n_jobs : int, optional, default=1
-        The number of jobs to use for the computation. If -1 all CPUs are used.
-        If 1 is given, no parallel computing code is used at all, which is
-        useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are
-        used. Thus for n_jobs = -2, all CPUs but one are used.
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation.
         It does each target variable in y in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -309,7 +311,7 @@ class MultiOutputClassifier(MultiOutputEstimator, ClassifierMixin):
         Estimators used for predictions.
     """
 
-    def __init__(self, estimator, n_jobs=1):
+    def __init__(self, estimator, n_jobs=None):
         super(MultiOutputClassifier, self).__init__(estimator, n_jobs)
 
     def predict_proba(self, X):
diff --git a/sklearn/naive_bayes.py b/sklearn/naive_bayes.py
index ddbc7e7a5467..cf2c65b3acc0 100644
--- a/sklearn/naive_bayes.py
+++ b/sklearn/naive_bayes.py
@@ -465,7 +465,7 @@ def _update_class_log_prior(self, class_prior=None):
             self.class_log_prior_ = (np.log(self.class_count_) -
                                      np.log(self.class_count_.sum()))
         else:
-            self.class_log_prior_ = np.zeros(n_classes) - np.log(n_classes)
+            self.class_log_prior_ = np.full(n_classes, -np.log(n_classes))
 
     def _check_alpha(self):
         if np.min(self.alpha) < 0:
diff --git a/sklearn/neighbors/approximate.py b/sklearn/neighbors/approximate.py
index 6a3fd571b321..650af47e0d81 100644
--- a/sklearn/neighbors/approximate.py
+++ b/sklearn/neighbors/approximate.py
@@ -82,16 +82,53 @@ def _to_hash(projected):
         return out.reshape(projected.shape[0], -1)
 
     def fit_transform(self, X, y=None):
+        """
+        Parameters
+        ----------
+        X : array-like, shape = [n_samples, n_features]
+            Training vectors, where n_samples is the number of samples and
+            n_features is the number of predictors.
+        """
+
         self.fit(X)
         return self.transform(X)
 
     def transform(self, X):
+        """
+        Parameters
+        ----------
+        X : array-like, shape = [n_samples, n_features]
+            Training vectors, where n_samples is the number of samples and
+            n_features is the number of predictors.
+        """
         return self._to_hash(super(ProjectionToHashMixin, self).transform(X))
 
 
 class GaussianRandomProjectionHash(ProjectionToHashMixin,
                                    GaussianRandomProjection):
-    """Use GaussianRandomProjection to produce a cosine LSH fingerprint"""
+    """Use GaussianRandomProjection to produce a cosine LSH fingerprint
+
+    Parameters
+    ----------
+
+    n_components : int or 'auto', optional (default = 32)
+        Dimensionality of the target projection space.
+
+        n_components can be automatically adjusted according to the
+        number of samples in the dataset and the bound given by the
+        Johnson-Lindenstrauss lemma. In that case the quality of the
+        embedding is controlled by the ``eps`` parameter.
+
+        It should be noted that Johnson-Lindenstrauss lemma can yield
+        very conservative estimated of the required number of components
+        as it makes no assumption on the structure of the dataset.
+
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
+    """
     def __init__(self,
                  n_components=32,
                  random_state=None):
@@ -187,17 +224,18 @@ class LSHForest(BaseEstimator, KNeighborsMixin, RadiusNeighborsMixin):
 
       >>> X_train = [[5, 5, 2], [21, 5, 5], [1, 1, 1], [8, 9, 1], [6, 10, 2]]
       >>> X_test = [[9, 1, 6], [3, 1, 10], [7, 10, 3]]
-      >>> lshf = LSHForest(random_state=42)
-      >>> lshf.fit(X_train)  # doctest: +NORMALIZE_WHITESPACE
+      >>> lshf = LSHForest(random_state=42)  # doctest: +SKIP
+      >>> lshf.fit(X_train)  # doctest: +SKIP
       LSHForest(min_hash_match=4, n_candidates=50, n_estimators=10,
                 n_neighbors=5, radius=1.0, radius_cutoff_ratio=0.9,
                 random_state=42)
       >>> distances, indices = lshf.kneighbors(X_test, n_neighbors=2)
-      >>> distances                                        # doctest: +ELLIPSIS
+      ... # doctest: +SKIP
+      >>> distances                                        # doctest: +SKIP
       array([[0.069..., 0.149...],
              [0.229..., 0.481...],
              [0.004..., 0.014...]])
-      >>> indices
+      >>> indices  # doctest: +SKIP
       array([[1, 2],
              [2, 0],
              [4, 0]])
diff --git a/sklearn/neighbors/base.py b/sklearn/neighbors/base.py
index 14810e65b016..fcb221b037a8 100644
--- a/sklearn/neighbors/base.py
+++ b/sklearn/neighbors/base.py
@@ -7,6 +7,7 @@
 #
 # License: BSD 3 clause (C) INRIA, University of Amsterdam
 from functools import partial
+from distutils.version import LooseVersion
 
 import warnings
 from abc import ABCMeta, abstractmethod
@@ -19,12 +20,12 @@
 from ..base import BaseEstimator
 from ..metrics import pairwise_distances_chunked
 from ..metrics.pairwise import PAIRWISE_DISTANCE_FUNCTIONS
-from ..utils import check_X_y, check_array, _get_n_jobs, gen_even_slices
+from ..utils import check_X_y, check_array, gen_even_slices
 from ..utils.multiclass import check_classification_targets
 from ..utils.validation import check_is_fitted
 from ..externals import six
-from ..externals.joblib import Parallel, delayed
-from ..exceptions import NotFittedError
+from ..utils import Parallel, delayed, effective_n_jobs
+from ..utils._joblib import __version__ as joblib_version
 from ..exceptions import DataConversionWarning
 
 VALID_METRICS = dict(ball_tree=BallTree.valid_metrics,
@@ -107,7 +108,7 @@ class NeighborsBase(six.with_metaclass(ABCMeta, BaseEstimator)):
     @abstractmethod
     def __init__(self, n_neighbors=None, radius=None,
                  algorithm='auto', leaf_size=30, metric='minkowski',
-                 p=2, metric_params=None, n_jobs=1):
+                 p=2, metric_params=None, n_jobs=None):
 
         self.n_neighbors = n_neighbors
         self.radius = radius
@@ -401,7 +402,7 @@ class from an array representing our data set and ask who's
         n_samples, _ = X.shape
         sample_range = np.arange(n_samples)[:, None]
 
-        n_jobs = _get_n_jobs(self.n_jobs)
+        n_jobs = effective_n_jobs(self.n_jobs)
         if self._fit_method == 'brute':
 
             reduce_func = partial(self._kneighbors_reduce_func,
@@ -422,8 +423,16 @@ class from an array representing our data set and ask who's
                 raise ValueError(
                     "%s does not work with sparse matrices. Densify the data, "
                     "or set algorithm='brute'" % self._fit_method)
-            result = Parallel(n_jobs, backend='threading')(
-                delayed(self._tree.query, check_pickle=False)(
+            if LooseVersion(joblib_version) < LooseVersion('0.12'):
+                # Deal with change of API in joblib
+                delayed_query = delayed(self._tree.query,
+                                        check_pickle=False)
+                parallel_kwargs = {"backend": "threading"}
+            else:
+                delayed_query = delayed(self._tree.query)
+                parallel_kwargs = {"prefer": "threads"}
+            result = Parallel(n_jobs, **parallel_kwargs)(
+                delayed_query(
                     X[s], n_neighbors, return_distance)
                 for s in gen_even_slices(X.shape[0], n_jobs)
             )
@@ -697,10 +706,17 @@ class from an array representing our data set and ask who's
                     "%s does not work with sparse matrices. Densify the data, "
                     "or set algorithm='brute'" % self._fit_method)
 
-            n_jobs = _get_n_jobs(self.n_jobs)
-            results = Parallel(n_jobs, backend='threading')(
-                delayed(self._tree.query_radius, check_pickle=False)(
-                    X[s], radius, return_distance)
+            n_jobs = effective_n_jobs(self.n_jobs)
+            if LooseVersion(joblib_version) < LooseVersion('0.12'):
+                # Deal with change of API in joblib
+                delayed_query = delayed(self._tree.query_radius,
+                                        check_pickle=False)
+                parallel_kwargs = {"backend": "threading"}
+            else:
+                delayed_query = delayed(self._tree.query_radius)
+                parallel_kwargs = {"prefer": "threads"}
+            results = Parallel(n_jobs, **parallel_kwargs)(
+                delayed_query(X[s], radius, return_distance)
                 for s in gen_even_slices(X.shape[0], n_jobs)
             )
             if return_distance:
diff --git a/sklearn/neighbors/binary_tree.pxi b/sklearn/neighbors/binary_tree.pxi
index 6b736d39fab8..51520cd1606c 100755
--- a/sklearn/neighbors/binary_tree.pxi
+++ b/sklearn/neighbors/binary_tree.pxi
@@ -591,8 +591,8 @@ cdef class NeighborsHeap:
         self.indices = get_memview_ITYPE_2D(self.indices_arr)
 
     def __init__(self, n_pts, n_nbrs):
-        self.distances_arr = np.inf + np.zeros((n_pts, n_nbrs), dtype=DTYPE,
-                                               order='C')
+        self.distances_arr = np.full((n_pts, n_nbrs), np.inf, dtype=DTYPE,
+                                     order='C')
         self.indices_arr = np.zeros((n_pts, n_nbrs), dtype=ITYPE, order='C')
         self.distances = get_memview_DTYPE_2D(self.distances_arr)
         self.indices = get_memview_ITYPE_2D(self.indices_arr)
@@ -1119,7 +1119,7 @@ cdef class BinaryTree:
         """
         reduce method used for pickling
         """
-        return (newObj, (BinaryTree,), self.__getstate__())
+        return (newObj, (type(self),), self.__getstate__())
 
     def __getstate__(self):
         """
@@ -1136,7 +1136,8 @@ cdef class BinaryTree:
                 int(self.n_leaves),
                 int(self.n_splits),
                 int(self.n_calls),
-                self.dist_metric)
+                self.dist_metric,
+                self.sample_weight)
 
     def __setstate__(self, state):
         """
@@ -1162,6 +1163,7 @@ cdef class BinaryTree:
         self.dist_metric = state[11]
         self.euclidean = (self.dist_metric.__class__.__name__
                           == 'EuclideanDistance')
+        self.sample_weight = state[12]
 
     def get_tree_stats(self):
         return (self.n_trims, self.n_leaves, self.n_splits)
@@ -1336,7 +1338,7 @@ cdef class BinaryTree:
                 self._query_dual_breadthfirst(other, heap, nodeheap)
             else:
                 reduced_dist_LB = min_rdist_dual(self, 0, other, 0)
-                bounds = np.inf + np.zeros(other.node_data.shape[0])
+                bounds = np.full(other.node_data.shape[0], np.inf)
                 self._query_dual_depthfirst(0, other, 0, bounds,
                                             heap, reduced_dist_LB)
 
@@ -1446,7 +1448,7 @@ cdef class BinaryTree:
         r = np.asarray(r, dtype=DTYPE, order='C')
         r = np.atleast_1d(r)
         if r.shape == (1,):
-            r = r[0] + np.zeros(X.shape[:X.ndim - 1], dtype=DTYPE)
+            r = np.full(X.shape[:X.ndim - 1], r[0], dtype=DTYPE)
         else:
             if r.shape != X.shape[:X.ndim - 1]:
                 raise ValueError("r must be broadcastable to X.shape")
@@ -1654,7 +1656,7 @@ cdef class BinaryTree:
         #       this is difficult because of the need to cache values
         #       computed between node pairs.
         if breadth_first:
-            node_log_min_bounds_arr = -np.inf + np.zeros(self.n_nodes)
+            node_log_min_bounds_arr = np.full(self.n_nodes, -np.inf)
             node_log_min_bounds = get_memview_DTYPE_1D(node_log_min_bounds_arr)
             node_bound_widths_arr = np.zeros(self.n_nodes)
             node_bound_widths = get_memview_DTYPE_1D(node_bound_widths_arr)
@@ -1970,7 +1972,7 @@ cdef class BinaryTree:
         """Non-recursive dual-tree k-neighbors query, breadth-first"""
         cdef ITYPE_t i, i1, i2, i_node1, i_node2, i_pt
         cdef DTYPE_t dist_pt, reduced_dist_LB
-        cdef DTYPE_t[::1] bounds = np.inf + np.zeros(other.node_data.shape[0])
+        cdef DTYPE_t[::1] bounds = np.full(other.node_data.shape[0], np.inf)
         cdef NodeData_t* node_data1 = &self.node_data[0]
         cdef NodeData_t* node_data2 = &other.node_data[0]
         cdef NodeData_t node_info1, node_info2
diff --git a/sklearn/neighbors/classification.py b/sklearn/neighbors/classification.py
index 83ee27cccd4b..e84f9751d008 100644
--- a/sklearn/neighbors/classification.py
+++ b/sklearn/neighbors/classification.py
@@ -75,9 +75,11 @@ class KNeighborsClassifier(NeighborsBase, KNeighborsMixin,
     metric_params : dict, optional (default = None)
         Additional keyword arguments for the metric function.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
         Doesn't affect :meth:`fit` method.
 
     Examples
@@ -117,7 +119,7 @@ class KNeighborsClassifier(NeighborsBase, KNeighborsMixin,
 
     def __init__(self, n_neighbors=5,
                  weights='uniform', algorithm='auto', leaf_size=30,
-                 p=2, metric='minkowski', metric_params=None, n_jobs=1,
+                 p=2, metric='minkowski', metric_params=None, n_jobs=None,
                  **kwargs):
 
         super(KNeighborsClassifier, self).__init__(
@@ -288,9 +290,11 @@ class RadiusNeighborsClassifier(NeighborsBase, RadiusNeighborsMixin,
     metric_params : dict, optional (default = None)
         Additional keyword arguments for the metric function.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Examples
     --------
@@ -320,7 +324,8 @@ class RadiusNeighborsClassifier(NeighborsBase, RadiusNeighborsMixin,
 
     def __init__(self, radius=1.0, weights='uniform',
                  algorithm='auto', leaf_size=30, p=2, metric='minkowski',
-                 outlier_label=None, metric_params=None, n_jobs=1, **kwargs):
+                 outlier_label=None, metric_params=None, n_jobs=None,
+                 **kwargs):
         super(RadiusNeighborsClassifier, self).__init__(
               radius=radius,
               algorithm=algorithm,
@@ -378,10 +383,10 @@ def predict(self, X):
                 mode = np.array([stats.mode(pl)[0]
                                  for pl in pred_labels[inliers]], dtype=np.int)
             else:
-                mode = np.array([weighted_mode(pl, w)[0]
-                                 for (pl, w)
-                                 in zip(pred_labels[inliers], weights[inliers])],
-                                dtype=np.int)
+                mode = np.array(
+                    [weighted_mode(pl, w)[0]
+                     for (pl, w) in zip(pred_labels[inliers], weights[inliers])
+                     ], dtype=np.int)
 
             mode = mode.ravel()
 
diff --git a/sklearn/neighbors/graph.py b/sklearn/neighbors/graph.py
index b794e2059a7b..3999ff458e12 100644
--- a/sklearn/neighbors/graph.py
+++ b/sklearn/neighbors/graph.py
@@ -32,7 +32,7 @@ def _query_include_self(X, include_self):
 
 
 def kneighbors_graph(X, n_neighbors, mode='connectivity', metric='minkowski',
-                     p=2, metric_params=None, include_self=False, n_jobs=1):
+                     p=2, metric_params=None, include_self=False, n_jobs=None):
     """Computes the (weighted) graph of k-Neighbors for points in X
 
     Read more in the :ref:`User Guide <unsupervised_neighbors>`.
@@ -70,9 +70,11 @@ def kneighbors_graph(X, n_neighbors, mode='connectivity', metric='minkowski',
         itself. If `None`, then True is used for mode='connectivity' and False
         for mode='distance' as this will preserve backwards compatibility.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Returns
     -------
@@ -104,7 +106,8 @@ def kneighbors_graph(X, n_neighbors, mode='connectivity', metric='minkowski',
 
 
 def radius_neighbors_graph(X, radius, mode='connectivity', metric='minkowski',
-                           p=2, metric_params=None, include_self=False, n_jobs=1):
+                           p=2, metric_params=None, include_self=False,
+                           n_jobs=None):
     """Computes the (weighted) graph of Neighbors for points in X
 
     Neighborhoods are restricted the points at a distance lower than
@@ -145,9 +148,11 @@ def radius_neighbors_graph(X, radius, mode='connectivity', metric='minkowski',
         itself. If `None`, then True is used for mode='connectivity' and False
         for mode='distance' as this will preserve backwards compatibility.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Returns
     -------
@@ -158,7 +163,8 @@ def radius_neighbors_graph(X, radius, mode='connectivity', metric='minkowski',
     --------
     >>> X = [[0], [3], [1]]
     >>> from sklearn.neighbors import radius_neighbors_graph
-    >>> A = radius_neighbors_graph(X, 1.5, mode='connectivity', include_self=True)
+    >>> A = radius_neighbors_graph(X, 1.5, mode='connectivity',
+    ...                            include_self=True)
     >>> A.toarray()
     array([[1., 0., 1.],
            [0., 1., 0.],
diff --git a/sklearn/neighbors/kd_tree.pyx b/sklearn/neighbors/kd_tree.pyx
index 08d2ff8ef0e3..4e713f846a5e 100644
--- a/sklearn/neighbors/kd_tree.pyx
+++ b/sklearn/neighbors/kd_tree.pyx
@@ -69,6 +69,8 @@ cdef int init_node(BinaryTree tree, ITYPE_t i_node,
         for j in range(n_features):
             lower_bounds[j] = fmin(lower_bounds[j], data_row[j])
             upper_bounds[j] = fmax(upper_bounds[j], data_row[j])
+
+    for j in range(n_features):
         if tree.dist_metric.p == INF:
             rad = fmax(rad, 0.5 * (upper_bounds[j] - lower_bounds[j]))
         else:
diff --git a/sklearn/neighbors/kde.py b/sklearn/neighbors/kde.py
index 140c72b526fa..ff5920b68ea5 100644
--- a/sklearn/neighbors/kde.py
+++ b/sklearn/neighbors/kde.py
@@ -121,7 +121,7 @@ def fit(self, X, y=None, sample_weight=None):
         X : array_like, shape (n_samples, n_features)
             List of n_features-dimensional data points.  Each row
             corresponds to a single data point.
-        sample_weight: array_like, shape (n_samples,), optional
+        sample_weight : array_like, shape (n_samples,), optional
             List of sample weights attached to the data X.
         """
         algorithm = self._choose_algorithm(self.algorithm, self.metric)
diff --git a/sklearn/neighbors/lof.py b/sklearn/neighbors/lof.py
index a2589f792331..68fe777b3c48 100644
--- a/sklearn/neighbors/lof.py
+++ b/sklearn/neighbors/lof.py
@@ -4,7 +4,6 @@
 
 import numpy as np
 import warnings
-from scipy.stats import scoreatpercentile
 
 from .base import NeighborsBase
 from .base import KNeighborsMixin
@@ -100,9 +99,22 @@ class LocalOutlierFactor(NeighborsBase, KNeighborsMixin, UnsupervisedMixin,
         threshold on the decision function. If "auto", the decision function
         threshold is determined as in the original paper.
 
-    n_jobs : int, optional (default=1)
+        .. versionchanged:: 0.20
+           The default value of ``contamination`` will change from 0.1 in 0.20
+           to ``'auto'`` in 0.22.
+
+    novelty : boolean, default False
+        By default, LocalOutlierFactor is only meant to be used for outlier
+        detection (novelty=False). Set novelty to True if you want to use
+        LocalOutlierFactor for novelty detection. In this case be aware that
+        that you should only use predict, decision_function and score_samples
+        on new unseen data and not on the training set.
+
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
         Affects only :meth:`kneighbors` and :meth:`kneighbors_graph` methods.
 
 
@@ -110,7 +122,7 @@ class LocalOutlierFactor(NeighborsBase, KNeighborsMixin, UnsupervisedMixin,
     ----------
     negative_outlier_factor_ : numpy array, shape (n_samples,)
         The opposite LOF of the training samples. The higher, the more normal.
-        Inliers tend to have a LOF score close to 1 (`negative_outlier_factor_`
+        Inliers tend to have a LOF score close to 1 (``negative_outlier_factor_``
         close to -1), while outliers tend to have a larger LOF score.
 
         The local outlier factor (LOF) of a sample captures its
@@ -137,25 +149,49 @@ class LocalOutlierFactor(NeighborsBase, KNeighborsMixin, UnsupervisedMixin,
     """
     def __init__(self, n_neighbors=20, algorithm='auto', leaf_size=30,
                  metric='minkowski', p=2, metric_params=None,
-                 contamination="legacy", n_jobs=1):
+                 contamination="legacy", novelty=False, n_jobs=None):
         super(LocalOutlierFactor, self).__init__(
-              n_neighbors=n_neighbors,
-              algorithm=algorithm,
-              leaf_size=leaf_size, metric=metric, p=p,
-              metric_params=metric_params, n_jobs=n_jobs)
-
-        if contamination == "legacy":
-            warnings.warn('default contamination parameter 0.1 will change '
-                          'in version 0.22 to "auto". This will change the '
-                          'predict method behavior.',
-                          DeprecationWarning)
+            n_neighbors=n_neighbors,
+            algorithm=algorithm,
+            leaf_size=leaf_size, metric=metric, p=p,
+            metric_params=metric_params, n_jobs=n_jobs)
         self.contamination = contamination
+        self.novelty = novelty
+
+    @property
+    def fit_predict(self):
+        """"Fits the model to the training set X and returns the labels.
+
+        Label is 1 for an inlier and -1 for an outlier according to the LOF
+        score and the contamination parameter.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features), default=None
+            The query sample or samples to compute the Local Outlier Factor
+            w.r.t. to the training samples.
+
+        Returns
+        -------
+        is_inlier : array, shape (n_samples,)
+            Returns -1 for anomalies/outliers and 1 for inliers.
+        """
+
+        # As fit_predict would be different from fit.predict, fit_predict is
+        # only available for outlier detection (novelty=False)
+
+        if self.novelty:
+            msg = ('fit_predict is not available when novelty=True. Use '
+                   'novelty=False if you want to predict on the training set.')
+            raise AttributeError(msg)
+
+        return self._fit_predict
 
-    def fit_predict(self, X, y=None):
-        """"Fits the model to the training set X and returns the labels
-        (1 inlier, -1 outlier) on the training set according to the LOF score
-        and the contamination parameter.
+    def _fit_predict(self, X, y=None):
+        """"Fits the model to the training set X and returns the labels.
 
+        Label is 1 for an inlier and -1 for an outlier according to the LOF
+        score and the contamination parameter.
 
         Parameters
         ----------
@@ -169,6 +205,9 @@ def fit_predict(self, X, y=None):
             Returns -1 for anomalies/outliers and 1 for inliers.
         """
 
+        # As fit_predict would be different from fit.predict, fit_predict is
+        # only available for outlier detection (novelty=False)
+
         return self.fit(X)._predict()
 
     def fit(self, X, y=None):
@@ -184,10 +223,19 @@ def fit(self, X, y=None):
         -------
         self : object
         """
-        if self.contamination not in ["auto", "legacy"]:  # rm legacy in 0.22
-            if not(0. < self.contamination <= .5):
+        if self.contamination == "legacy":
+            warnings.warn('default contamination parameter 0.1 will change '
+                          'in version 0.22 to "auto". This will change the '
+                          'predict method behavior.',
+                          FutureWarning)
+            self._contamination = 0.1
+        else:
+            self._contamination = self.contamination
+
+        if self._contamination != 'auto':
+            if not(0. < self._contamination <= .5):
                 raise ValueError("contamination must be in (0, 0.5], "
-                                 "got: %f" % self.contamination)
+                                 "got: %f" % self._contamination)
 
         super(LocalOutlierFactor, self).fit(X)
 
@@ -211,26 +259,47 @@ def fit(self, X, y=None):
 
         self.negative_outlier_factor_ = -np.mean(lrd_ratios_array, axis=1)
 
-        if self.contamination == "auto":
+        if self._contamination == "auto":
             # inliers score around -1 (the higher, the less abnormal).
             self.offset_ = -1.5
-        elif self.contamination == "legacy":  # to rm in 0.22
-            self.offset_ = scoreatpercentile(
-                self.negative_outlier_factor_, 100. * 0.1)
         else:
-            self.offset_ = scoreatpercentile(
-                self.negative_outlier_factor_, 100. * self.contamination)
+            self.offset_ = np.percentile(self.negative_outlier_factor_,
+                                         100. * self._contamination)
 
         return self
 
+    @property
+    def predict(self):
+        """Predict the labels (1 inlier, -1 outlier) of X according to LOF.
+
+        This method allows to generalize prediction to *new observations* (not
+        in the training set). Only available for novelty detection (when
+        novelty is set to True).
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+            The query sample or samples to compute the Local Outlier Factor
+            w.r.t. to the training samples.
+
+        Returns
+        -------
+        is_inlier : array, shape (n_samples,)
+            Returns -1 for anomalies/outliers and +1 for inliers.
+        """
+        if not self.novelty:
+            msg = ('predict is not available when novelty=False, use '
+                   'fit_predict if you want to predict on training data. Use '
+                   'novelty=True if you want to use LOF for novelty detection '
+                   'and predict on new unseen data.')
+            raise AttributeError(msg)
+
+        return self._predict
+
     def _predict(self, X=None):
         """Predict the labels (1 inlier, -1 outlier) of X according to LOF.
 
         If X is None, returns the same as fit_predict(X_train).
-        This method allows to generalize prediction to new observations (not
-        in the training set). As LOF originally does not deal with new data,
-        this method is kept private. In particular, fit(X)._predict(X) is not
-        the same as fit_predict(X).
 
         Parameters
         ----------
@@ -250,26 +319,61 @@ def _predict(self, X=None):
         if X is not None:
             X = check_array(X, accept_sparse='csr')
             is_inlier = np.ones(X.shape[0], dtype=int)
-            is_inlier[self._decision_function(X) < 0] = -1
+            is_inlier[self.decision_function(X) < 0] = -1
         else:
             is_inlier = np.ones(self._fit_X.shape[0], dtype=int)
             is_inlier[self.negative_outlier_factor_ < self.offset_] = -1
 
         return is_inlier
 
+    @property
+    def decision_function(self):
+        """Shifted opposite of the Local Outlier Factor of X.
+
+        Bigger is better, i.e. large values correspond to inliers.
+
+        The shift offset allows a zero threshold for being an outlier.
+        Only available for novelty detection (when novelty is set to True).
+        The argument X is supposed to contain *new data*: if X contains a
+        point from training, it considers the later in its own neighborhood.
+        Also, the samples in X are not considered in the neighborhood of any
+        point.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+            The query sample or samples to compute the Local Outlier Factor
+            w.r.t. the training samples.
+
+        Returns
+        -------
+        shifted_opposite_lof_scores : array, shape (n_samples,)
+            The shifted opposite of the Local Outlier Factor of each input
+            samples. The lower, the more abnormal. Negative scores represent
+            outliers, positive scores represent inliers.
+        """
+        if not self.novelty:
+            msg = ('decision_function is not available when novelty=False. '
+                   'Use novelty=True if you want to use LOF for novelty '
+                   'detection and compute decision_function for new unseen '
+                   'data. Note that the opposite LOF of the training samples '
+                   'is always available by considering the '
+                   'negative_outlier_factor_ attribute.')
+            raise AttributeError(msg)
+
+        return self._decision_function
+
     def _decision_function(self, X):
-        """Shifted opposite of the Local Outlier Factor of X
+        """Shifted opposite of the Local Outlier Factor of X.
 
         Bigger is better, i.e. large values correspond to inliers.
 
         The shift offset allows a zero threshold for being an outlier.
+        Only available for novelty detection (when novelty is set to True).
         The argument X is supposed to contain *new data*: if X contains a
-        point from training, it consider the later in its own neighborhood.
+        point from training, it considers the later in its own neighborhood.
         Also, the samples in X are not considered in the neighborhood of any
         point.
-        This method is kept private as the predict method is.
-        The decision function on training data is available by considering the
-        the negative_outlier_factor_ attribute.
 
         Parameters
         ----------
@@ -284,17 +388,59 @@ def _decision_function(self, X):
             samples. The lower, the more abnormal. Negative scores represent
             outliers, positive scores represent inliers.
         """
+
         return self._score_samples(X) - self.offset_
 
+    @property
+    def score_samples(self):
+        """Opposite of the Local Outlier Factor of X.
+
+        It is the opposite as as bigger is better, i.e. large values correspond
+        to inliers.
+
+        Only available for novelty detection (when novelty is set to True).
+        The argument X is supposed to contain *new data*: if X contains a
+        point from training, it considers the later in its own neighborhood.
+        Also, the samples in X are not considered in the neighborhood of any
+        point.
+        The score_samples on training data is available by considering the
+        the ``negative_outlier_factor_`` attribute.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+            The query sample or samples to compute the Local Outlier Factor
+            w.r.t. the training samples.
+
+        Returns
+        -------
+        opposite_lof_scores : array, shape (n_samples,)
+            The opposite of the Local Outlier Factor of each input samples.
+            The lower, the more abnormal.
+        """
+        if not self.novelty:
+            msg = ('score_samples is not available when novelty=False. The '
+                   'scores of the training samples are always available '
+                   'through the negative_outlier_factor_ attribute. Use '
+                   'novelty=True if you want to use LOF for novelty detection '
+                   'and compute score_samples for new unseen data.')
+            raise AttributeError(msg)
+
+        return self._score_samples
+
     def _score_samples(self, X):
-        """Opposite of the Local Outlier Factor of X (as bigger is
-        better, i.e. large values correspond to inliers).
+        """Opposite of the Local Outlier Factor of X.
+
+        It is the opposite as as bigger is better, i.e. large values correspond
+        to inliers.
 
+        Only available for novelty detection (when novelty is set to True).
         The argument X is supposed to contain *new data*: if X contains a
-        point from training, it consider the later in its own neighborhood.
+        point from training, it considers the later in its own neighborhood.
         Also, the samples in X are not considered in the neighborhood of any
         point.
-        This method is kept private as the predict method is.
+        The score_samples on training data is available by considering the
+        the ``negative_outlier_factor_`` attribute.
 
         Parameters
         ----------
diff --git a/sklearn/neighbors/nearest_centroid.py b/sklearn/neighbors/nearest_centroid.py
index 48cd7a18fef9..73705bf64942 100644
--- a/sklearn/neighbors/nearest_centroid.py
+++ b/sklearn/neighbors/nearest_centroid.py
@@ -161,7 +161,7 @@ def fit(self, X, y):
             # it becomes zero.
             signs = np.sign(deviation)
             deviation = (np.abs(deviation) - self.shrink_threshold)
-            deviation[deviation < 0] = 0
+            np.clip(deviation, 0, None, out=deviation)
             deviation *= signs
             # Now adjust the centroids using the deviation
             msd = ms * deviation
diff --git a/sklearn/neighbors/regression.py b/sklearn/neighbors/regression.py
index d9f5682d49bf..be4f835ff2a2 100644
--- a/sklearn/neighbors/regression.py
+++ b/sklearn/neighbors/regression.py
@@ -82,9 +82,11 @@ class KNeighborsRegressor(NeighborsBase, KNeighborsMixin,
     metric_params : dict, optional (default = None)
         Additional keyword arguments for the metric function.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
         Doesn't affect :meth:`fit` method.
 
     Examples
@@ -122,7 +124,7 @@ class KNeighborsRegressor(NeighborsBase, KNeighborsMixin,
 
     def __init__(self, n_neighbors=5, weights='uniform',
                  algorithm='auto', leaf_size=30,
-                 p=2, metric='minkowski', metric_params=None, n_jobs=1,
+                 p=2, metric='minkowski', metric_params=None, n_jobs=None,
                  **kwargs):
         super(KNeighborsRegressor, self).__init__(
               n_neighbors=n_neighbors,
@@ -238,9 +240,11 @@ class RadiusNeighborsRegressor(NeighborsBase, RadiusNeighborsMixin,
     metric_params : dict, optional (default = None)
         Additional keyword arguments for the metric function.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Examples
     --------
@@ -270,7 +274,7 @@ class RadiusNeighborsRegressor(NeighborsBase, RadiusNeighborsMixin,
 
     def __init__(self, radius=1.0, weights='uniform',
                  algorithm='auto', leaf_size=30,
-                 p=2, metric='minkowski', metric_params=None, n_jobs=1,
+                 p=2, metric='minkowski', metric_params=None, n_jobs=None,
                  **kwargs):
         super(RadiusNeighborsRegressor, self).__init__(
               radius=radius,
diff --git a/sklearn/neighbors/tests/test_ball_tree.py b/sklearn/neighbors/tests/test_ball_tree.py
index de0d166fb889..52ed63ae416b 100644
--- a/sklearn/neighbors/tests/test_ball_tree.py
+++ b/sklearn/neighbors/tests/test_ball_tree.py
@@ -228,6 +228,8 @@ def check_pickle_protocol(protocol):
         assert_array_almost_equal(ind1_pyfunc, ind2_pyfunc)
         assert_array_almost_equal(dist1_pyfunc, dist2_pyfunc)
 
+        assert isinstance(bt2, BallTree)
+
     for protocol in (0, 1, 2):
         check_pickle_protocol(protocol)
 
diff --git a/sklearn/neighbors/tests/test_kd_tree.py b/sklearn/neighbors/tests/test_kd_tree.py
index 46cddc711e76..18d213802160 100644
--- a/sklearn/neighbors/tests/test_kd_tree.py
+++ b/sklearn/neighbors/tests/test_kd_tree.py
@@ -187,6 +187,7 @@ def check_pickle_protocol(protocol):
         ind2, dist2 = kdt2.query(X)
         assert_array_almost_equal(ind1, ind2)
         assert_array_almost_equal(dist1, dist2)
+        assert isinstance(kdt2, KDTree)
 
     check_pickle_protocol(protocol)
 
diff --git a/sklearn/neighbors/tests/test_kde.py b/sklearn/neighbors/tests/test_kde.py
index f4a8be244889..990942c9efdc 100644
--- a/sklearn/neighbors/tests/test_kde.py
+++ b/sklearn/neighbors/tests/test_kde.py
@@ -10,6 +10,7 @@
 from sklearn.datasets import make_blobs
 from sklearn.model_selection import GridSearchCV
 from sklearn.preprocessing import StandardScaler
+from sklearn.externals import joblib
 
 
 def compute_kernel_slow(Y, X, kernel, h):
@@ -159,7 +160,7 @@ def test_kde_pipeline_gridsearch():
 def test_kde_sample_weights():
     n_samples = 400
     size_test = 20
-    weights_neutral = 3 * np.ones(n_samples)
+    weights_neutral = np.full(n_samples, 3.)
     for d in [1, 2, 10]:
         rng = np.random.RandomState(0)
         X = rng.rand(n_samples, d)
@@ -202,3 +203,23 @@ def test_kde_sample_weights():
                     kde.fit(X, sample_weight=(scale_factor * weights))
                     scores_scaled_weight = kde.score_samples(test_points)
                     assert_allclose(scores_scaled_weight, scores_weight)
+
+
+def test_pickling(tmpdir):
+    # Make sure that predictions are the same before and after pickling. Used
+    # to be a bug because sample_weights wasn't pickled and the resulting tree
+    # would miss some info.
+
+    kde = KernelDensity()
+    data = np.reshape([1., 2., 3.], (-1, 1))
+    kde.fit(data)
+
+    X = np.reshape([1.1, 2.1], (-1, 1))
+    scores = kde.score_samples(X)
+
+    file_path = str(tmpdir.join('dump.pkl'))
+    joblib.dump(kde, file_path)
+    kde = joblib.load(file_path)
+    scores_pickled = kde.score_samples(X)
+
+    assert_allclose(scores, scores_pickled)
diff --git a/sklearn/neighbors/tests/test_lof.py b/sklearn/neighbors/tests/test_lof.py
index b472ab3d833b..ed57a1d0fba2 100644
--- a/sklearn/neighbors/tests/test_lof.py
+++ b/sklearn/neighbors/tests/test_lof.py
@@ -3,6 +3,8 @@
 # License: BSD 3 clause
 
 from math import sqrt
+
+import pytest
 import numpy as np
 from sklearn import neighbors
 
@@ -12,10 +14,13 @@
 from sklearn.metrics import roc_auc_score
 
 from sklearn.utils import check_random_state
-from sklearn.utils.testing import assert_greater
+from sklearn.utils.testing import assert_greater, ignore_warnings
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_equal
-from sklearn.utils.testing import assert_warns_message, assert_raises
+from sklearn.utils.testing import assert_warns_message
+from sklearn.utils.testing import assert_raises
+from sklearn.utils.testing import assert_raises_regex
+from sklearn.utils.estimator_checks import check_estimator
 
 from sklearn.datasets import load_iris
 
@@ -29,6 +34,9 @@
 iris.target = iris.target[perm]
 
 
+@pytest.mark.filterwarnings(
+    'ignore:default contamination parameter 0.1:FutureWarning')
+# XXX: Remove in 0.22
 def test_lof():
     # Toy sample (the last two samples are outliers):
     X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], [5, 3], [-4, 2]]
@@ -45,8 +53,12 @@ def test_lof():
     clf = neighbors.LocalOutlierFactor(contamination=0.25,
                                        n_neighbors=5).fit(X)
     assert_array_equal(clf._predict(), 6 * [1] + 2 * [-1])
+    assert_array_equal(clf.fit_predict(X), 6 * [1] + 2 * [-1])
 
 
+@pytest.mark.filterwarnings(
+    'ignore:default contamination parameter 0.1:FutureWarning')
+# XXX: Remove in 0.22
 def test_lof_performance():
     # Generate train/test data
     rng = check_random_state(2)
@@ -58,35 +70,43 @@ def test_lof_performance():
     X_test = np.r_[X[100:], X_outliers]
     y_test = np.array([0] * 20 + [1] * 20)
 
-    # fit the model
-    clf = neighbors.LocalOutlierFactor().fit(X_train)
+    # fit the model for novelty detection
+    clf = neighbors.LocalOutlierFactor(novelty=True).fit(X_train)
 
     # predict scores (the lower, the more normal)
-    y_pred = -clf._decision_function(X_test)
+    y_pred = -clf.decision_function(X_test)
 
     # check that roc_auc is good
     assert_greater(roc_auc_score(y_test, y_pred), .99)
 
 
+@pytest.mark.filterwarnings(
+    'ignore:default contamination parameter 0.1:FutureWarning')
+# XXX: Remove in 0.22
 def test_lof_values():
     # toy samples:
     X_train = [[1, 1], [1, 2], [2, 1]]
     clf1 = neighbors.LocalOutlierFactor(n_neighbors=2,
-                                        contamination=0.1).fit(X_train)
-    clf2 = neighbors.LocalOutlierFactor(n_neighbors=2).fit(X_train)
+                                        contamination=0.1,
+                                        novelty=True).fit(X_train)
+    clf2 = neighbors.LocalOutlierFactor(n_neighbors=2,
+                                        novelty=True).fit(X_train)
     s_0 = 2. * sqrt(2.) / (1. + sqrt(2.))
     s_1 = (1. + sqrt(2)) * (1. / (4. * sqrt(2.)) + 1. / (2. + 2. * sqrt(2)))
     # check predict()
     assert_array_almost_equal(-clf1.negative_outlier_factor_, [s_0, s_1, s_1])
     assert_array_almost_equal(-clf2.negative_outlier_factor_, [s_0, s_1, s_1])
     # check predict(one sample not in train)
-    assert_array_almost_equal(-clf1._score_samples([[2., 2.]]), [s_0])
-    assert_array_almost_equal(-clf2._score_samples([[2., 2.]]), [s_0])
+    assert_array_almost_equal(-clf1.score_samples([[2., 2.]]), [s_0])
+    assert_array_almost_equal(-clf2.score_samples([[2., 2.]]), [s_0])
     # check predict(one sample already in train)
-    assert_array_almost_equal(-clf1._score_samples([[1., 1.]]), [s_1])
-    assert_array_almost_equal(-clf2._score_samples([[1., 1.]]), [s_1])
+    assert_array_almost_equal(-clf1.score_samples([[1., 1.]]), [s_1])
+    assert_array_almost_equal(-clf2.score_samples([[1., 1.]]), [s_1])
 
 
+@pytest.mark.filterwarnings(
+    'ignore:default contamination parameter 0.1:FutureWarning')
+# XXX: Remove in 0.22
 def test_lof_precomputed(random_state=42):
     """Tests LOF with a distance matrix."""
     # Note: smaller samples may result in spurious test success
@@ -96,22 +116,25 @@ def test_lof_precomputed(random_state=42):
     DXX = metrics.pairwise_distances(X, metric='euclidean')
     DYX = metrics.pairwise_distances(Y, X, metric='euclidean')
     # As a feature matrix (n_samples by n_features)
-    lof_X = neighbors.LocalOutlierFactor(n_neighbors=3)
+    lof_X = neighbors.LocalOutlierFactor(n_neighbors=3, novelty=True)
     lof_X.fit(X)
     pred_X_X = lof_X._predict()
-    pred_X_Y = lof_X._predict(Y)
+    pred_X_Y = lof_X.predict(Y)
 
     # As a dense distance matrix (n_samples by n_samples)
     lof_D = neighbors.LocalOutlierFactor(n_neighbors=3, algorithm='brute',
-                                         metric='precomputed')
+                                         metric='precomputed', novelty=True)
     lof_D.fit(DXX)
     pred_D_X = lof_D._predict()
-    pred_D_Y = lof_D._predict(DYX)
+    pred_D_Y = lof_D.predict(DYX)
 
     assert_array_almost_equal(pred_X_X, pred_D_X)
     assert_array_almost_equal(pred_X_Y, pred_D_Y)
 
 
+@pytest.mark.filterwarnings(
+    'ignore:default contamination parameter 0.1:FutureWarning')
+# XXX: Remove in 0.22
 def test_n_neighbors_attribute():
     X = iris.data
     clf = neighbors.LocalOutlierFactor(n_neighbors=500).fit(X)
@@ -124,17 +147,22 @@ def test_n_neighbors_attribute():
     assert_equal(clf.n_neighbors_, X.shape[0] - 1)
 
 
+@pytest.mark.filterwarnings(
+    'ignore:default contamination parameter 0.1:FutureWarning')
+# XXX: Remove in 0.22
 def test_score_samples():
     X_train = [[1, 1], [1, 2], [2, 1]]
     clf1 = neighbors.LocalOutlierFactor(n_neighbors=2,
-                                        contamination=0.1).fit(X_train)
-    clf2 = neighbors.LocalOutlierFactor(n_neighbors=2).fit(X_train)
-    assert_array_equal(clf1._score_samples([[2., 2.]]),
-                       clf1._decision_function([[2., 2.]]) + clf1.offset_)
-    assert_array_equal(clf2._score_samples([[2., 2.]]),
-                       clf2._decision_function([[2., 2.]]) + clf2.offset_)
-    assert_array_equal(clf1._score_samples([[2., 2.]]),
-                       clf2._score_samples([[2., 2.]]))
+                                        contamination=0.1,
+                                        novelty=True).fit(X_train)
+    clf2 = neighbors.LocalOutlierFactor(n_neighbors=2,
+                                        novelty=True).fit(X_train)
+    assert_array_equal(clf1.score_samples([[2., 2.]]),
+                       clf1.decision_function([[2., 2.]]) + clf1.offset_)
+    assert_array_equal(clf2.score_samples([[2., 2.]]),
+                       clf2.decision_function([[2., 2.]]) + clf2.offset_)
+    assert_array_equal(clf1.score_samples([[2., 2.]]),
+                       clf2.score_samples([[2., 2.]]))
 
 
 def test_contamination():
@@ -143,8 +171,84 @@ def test_contamination():
     assert_raises(ValueError, clf.fit, X)
 
 
-def test_deprecation():
-    assert_warns_message(DeprecationWarning,
+@pytest.mark.filterwarnings(
+    'ignore:default contamination parameter 0.1:FutureWarning')
+# XXX: Remove in 0.22
+def test_novelty_errors():
+    X = iris.data
+
+    # check errors for novelty=False
+    clf = neighbors.LocalOutlierFactor()
+    clf.fit(X)
+    # predict, decision_function and score_samples raise ValueError
+    for method in ['predict', 'decision_function', 'score_samples']:
+        msg = ('{} is not available when novelty=False'.format(method))
+        assert_raises_regex(AttributeError, msg, getattr, clf, method)
+
+    # check errors for novelty=True
+    clf = neighbors.LocalOutlierFactor(novelty=True)
+    msg = 'fit_predict is not available when novelty=True'
+    assert_raises_regex(AttributeError, msg, getattr, clf, 'fit_predict')
+
+
+@pytest.mark.filterwarnings(
+    'ignore:default contamination parameter 0.1:FutureWarning')
+# XXX: Remove in 0.22
+def test_novelty_training_scores():
+    # check that the scores of the training samples are still accessible
+    # when novelty=True through the negative_outlier_factor_ attribute
+    X = iris.data
+
+    # fit with novelty=False
+    clf_1 = neighbors.LocalOutlierFactor()
+    clf_1.fit(X)
+    scores_1 = clf_1.negative_outlier_factor_
+
+    # fit with novelty=True
+    clf_2 = neighbors.LocalOutlierFactor(novelty=True)
+    clf_2.fit(X)
+    scores_2 = clf_2.negative_outlier_factor_
+
+    assert_array_almost_equal(scores_1, scores_2)
+
+
+@pytest.mark.filterwarnings(
+    'ignore:default contamination parameter 0.1:FutureWarning')
+# XXX: Remove in 0.22
+def test_hasattr_prediction():
+    # check availability of prediction methods depending on novelty value.
+    X = [[1, 1], [1, 2], [2, 1]]
+
+    # when novelty=True
+    clf = neighbors.LocalOutlierFactor(novelty=True)
+    clf.fit(X)
+    assert hasattr(clf, 'predict')
+    assert hasattr(clf, 'decision_function')
+    assert hasattr(clf, 'score_samples')
+    assert not hasattr(clf, 'fit_predict')
+
+    # when novelty=False
+    clf = neighbors.LocalOutlierFactor(novelty=False)
+    clf.fit(X)
+    assert hasattr(clf, 'fit_predict')
+    assert not hasattr(clf, 'predict')
+    assert not hasattr(clf, 'decision_function')
+    assert not hasattr(clf, 'score_samples')
+
+
+@pytest.mark.filterwarnings(
+    'ignore:default contamination parameter 0.1:FutureWarning')
+# XXX: Remove in 0.22
+def test_novelty_true_common_tests():
+
+    # the common tests are run for the default LOF (novelty=False).
+    # here we run these common tests for LOF when novelty=True
+    check_estimator(neighbors.LocalOutlierFactor(novelty=True))
+
+
+def test_contamination_future_warning():
+    X = [[1, 1], [1, 2], [2, 1]]
+    assert_warns_message(FutureWarning,
                          'default contamination parameter 0.1 will change '
                          'in version 0.22 to "auto"',
-                         neighbors.LocalOutlierFactor, )
+                         neighbors.LocalOutlierFactor().fit, X)
diff --git a/sklearn/neighbors/tests/test_neighbors.py b/sklearn/neighbors/tests/test_neighbors.py
index e1acaa4c6f13..9b244cde0953 100644
--- a/sklearn/neighbors/tests/test_neighbors.py
+++ b/sklearn/neighbors/tests/test_neighbors.py
@@ -181,6 +181,7 @@ def test_precomputed(random_state=42):
         assert_array_almost_equal(pred_X, pred_D)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_precomputed_cross_validation():
     # Ensure array is split correctly
     rng = np.random.RandomState(0)
@@ -683,7 +684,7 @@ def test_radius_neighbors_regressor(n_samples=40,
                                                    weights=weights,
                                                    algorithm='auto')
         neigh.fit(X, y)
-        X_test_nan = np.ones((1, n_features))*-1
+        X_test_nan = np.full((1, n_features), -1.)
         empty_warning_msg = ("One or more samples have no neighbors "
                              "within specified radius; predicting NaN.")
         pred = assert_warns_message(UserWarning,
diff --git a/sklearn/neighbors/unsupervised.py b/sklearn/neighbors/unsupervised.py
index db19e8df6b9f..9d41b640f9e1 100644
--- a/sklearn/neighbors/unsupervised.py
+++ b/sklearn/neighbors/unsupervised.py
@@ -74,9 +74,11 @@ class NearestNeighbors(NeighborsBase, KNeighborsMixin,
     metric_params : dict, optional (default = None)
         Additional keyword arguments for the metric function.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Examples
     --------
@@ -114,7 +116,7 @@ class NearestNeighbors(NeighborsBase, KNeighborsMixin,
 
     def __init__(self, n_neighbors=5, radius=1.0,
                  algorithm='auto', leaf_size=30, metric='minkowski',
-                 p=2, metric_params=None, n_jobs=1, **kwargs):
+                 p=2, metric_params=None, n_jobs=None, **kwargs):
         super(NearestNeighbors, self).__init__(
               n_neighbors=n_neighbors,
               radius=radius,
diff --git a/sklearn/neural_network/_stochastic_optimizers.py b/sklearn/neural_network/_stochastic_optimizers.py
index 4d2895653ca5..a741ca7695ee 100644
--- a/sklearn/neural_network/_stochastic_optimizers.py
+++ b/sklearn/neural_network/_stochastic_optimizers.py
@@ -140,22 +140,22 @@ def iteration_ends(self, time_step):
                                   (time_step + 1) ** self.power_t)
 
     def trigger_stopping(self, msg, verbose):
-        if self.lr_schedule == 'adaptive':
-            if self.learning_rate > 1e-6:
-                self.learning_rate /= 5.
-                if verbose:
-                    print(msg + " Setting learning rate to %f" %
-                          self.learning_rate)
-                return False
-            else:
-                if verbose:
-                    print(msg + " Learning rate too small. Stopping.")
-                return True
-        else:
+        if self.lr_schedule != 'adaptive':
             if verbose:
                 print(msg + " Stopping.")
             return True
 
+        if self.learning_rate <= 1e-6:
+            if verbose:
+                print(msg + " Learning rate too small. Stopping.")
+            return True
+
+        self.learning_rate /= 5.
+        if verbose:
+            print(msg + " Setting learning rate to %f" %
+                  self.learning_rate)
+        return False
+
     def _get_updates(self, grads):
         """Get the values used to update params with given gradients
 
diff --git a/sklearn/pipeline.py b/sklearn/pipeline.py
index 1e99dd54615a..87d7180e0c23 100644
--- a/sklearn/pipeline.py
+++ b/sklearn/pipeline.py
@@ -15,7 +15,7 @@
 from scipy import sparse
 
 from .base import clone, TransformerMixin
-from .externals.joblib import Parallel, delayed
+from .utils import Parallel, delayed
 from .externals import six
 from .utils.metaestimators import if_delegate_has_method
 from .utils import Bunch
@@ -206,10 +206,22 @@ def _fit(self, X, y=None, **fit_params):
             if transformer is None:
                 pass
             else:
-                if hasattr(memory, 'cachedir') and memory.cachedir is None:
-                    # we do not clone when caching is disabled to preserve
-                    # backward compatibility
-                    cloned_transformer = transformer
+                if hasattr(memory, 'location'):
+                    # joblib >= 0.12
+                    if memory.location is None:
+                        # we do not clone when caching is disabled to
+                        # preserve backward compatibility
+                        cloned_transformer = transformer
+                    else:
+                        cloned_transformer = clone(transformer)
+                elif hasattr(memory, 'cachedir'):
+                    # joblib < 0.11
+                    if memory.cachedir is None:
+                        # we do not clone when caching is disabled to
+                        # preserve backward compatibility
+                        cloned_transformer = transformer
+                    else:
+                        cloned_transformer = clone(transformer)
                 else:
                     cloned_transformer = clone(transformer)
                 # Fit or load from cache the current transfomer
@@ -628,8 +640,11 @@ class FeatureUnion(_BaseComposition, TransformerMixin):
         List of transformer objects to be applied to the data. The first
         half of each tuple is the name of the transformer.
 
-    n_jobs : int, optional
-        Number of jobs to run in parallel (default 1).
+    n_jobs : int or None, optional (default=None)
+        Number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     transformer_weights : dict, optional
         Multiplicative weights for features per transformer.
@@ -651,7 +666,8 @@ class FeatureUnion(_BaseComposition, TransformerMixin):
     array([[ 1.5       ,  3.0...,  0.8...],
            [-1.5       ,  5.7..., -0.4...]])
     """
-    def __init__(self, transformer_list, n_jobs=1, transformer_weights=None):
+    def __init__(self, transformer_list, n_jobs=None,
+                 transformer_weights=None):
         self.transformer_list = transformer_list
         self.n_jobs = n_jobs
         self.transformer_weights = transformer_weights
@@ -831,8 +847,11 @@ def make_union(*transformers, **kwargs):
     ----------
     *transformers : list of estimators
 
-    n_jobs : int, optional
-        Number of jobs to run in parallel (default 1).
+    n_jobs : int or None, optional (default=None)
+        Number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Returns
     -------
@@ -848,7 +867,7 @@ def make_union(*transformers, **kwargs):
     >>> from sklearn.decomposition import PCA, TruncatedSVD
     >>> from sklearn.pipeline import make_union
     >>> make_union(PCA(), TruncatedSVD())    # doctest: +NORMALIZE_WHITESPACE
-    FeatureUnion(n_jobs=1,
+    FeatureUnion(n_jobs=None,
            transformer_list=[('pca',
                               PCA(copy=True, iterated_power='auto',
                                   n_components=None, random_state=None,
@@ -859,7 +878,7 @@ def make_union(*transformers, **kwargs):
                               random_state=None, tol=0.0))],
            transformer_weights=None)
     """
-    n_jobs = kwargs.pop('n_jobs', 1)
+    n_jobs = kwargs.pop('n_jobs', None)
     if kwargs:
         # We do not currently support `transformer_weights` as we may want to
         # change its type spec in make_union
diff --git a/sklearn/preprocessing/__init__.py b/sklearn/preprocessing/__init__.py
index 85bade9b81c1..15905bf37d2e 100644
--- a/sklearn/preprocessing/__init__.py
+++ b/sklearn/preprocessing/__init__.py
@@ -33,6 +33,8 @@
 from .label import LabelEncoder
 from .label import MultiLabelBinarizer
 
+from ._discretization import KBinsDiscretizer
+
 from .imputation import Imputer
 
 # stub, remove in version 0.21
@@ -42,6 +44,7 @@
     'Binarizer',
     'FunctionTransformer',
     'Imputer',
+    'KBinsDiscretizer',
     'KernelCenterer',
     'LabelBinarizer',
     'LabelEncoder',
diff --git a/sklearn/preprocessing/_discretization.py b/sklearn/preprocessing/_discretization.py
new file mode 100644
index 000000000000..e10aa51d31a1
--- /dev/null
+++ b/sklearn/preprocessing/_discretization.py
@@ -0,0 +1,304 @@
+# -*- coding: utf-8 -*-
+
+# Author: Henry Lin <hlin117@gmail.com>
+#         Tom Dupré la Tour
+
+# License: BSD
+
+from __future__ import division, absolute_import
+
+import numbers
+import numpy as np
+import warnings
+
+from . import OneHotEncoder
+
+from ..base import BaseEstimator, TransformerMixin
+from ..utils.validation import check_array
+from ..utils.validation import check_is_fitted
+from ..utils.validation import FLOAT_DTYPES
+from ..utils.fixes import np_version
+
+
+class KBinsDiscretizer(BaseEstimator, TransformerMixin):
+    """Bin continuous data into intervals.
+
+    Read more in the :ref:`User Guide <preprocessing_discretization>`.
+
+    Parameters
+    ----------
+    n_bins : int or array-like, shape (n_features,) (default=5)
+        The number of bins to produce. The intervals for the bins are
+        determined by the minimum and maximum of the input data.
+        Raises ValueError if ``n_bins < 2``.
+
+        If ``n_bins`` is an array, and there is an ignored feature at
+        index ``i``, ``n_bins[i]`` will be ignored.
+
+    encode : {'onehot', 'onehot-dense', 'ordinal'}, (default='onehot')
+        Method used to encode the transformed result.
+
+        onehot
+            Encode the transformed result with one-hot encoding
+            and return a sparse matrix. Ignored features are always
+            stacked to the right.
+        onehot-dense
+            Encode the transformed result with one-hot encoding
+            and return a dense array. Ignored features are always
+            stacked to the right.
+        ordinal
+            Return the bin identifier encoded as an integer value.
+
+    strategy : {'uniform', 'quantile', 'kmeans'}, (default='quantile')
+        Strategy used to define the widths of the bins.
+
+        uniform
+            All bins in each feature have identical widths.
+        quantile
+            All bins in each feature have the same number of points.
+        kmeans
+            Values in each bin have the same nearest center of a 1D k-means
+            cluster.
+
+    Attributes
+    ----------
+    n_bins_ : int array, shape (n_features,)
+        Number of bins per feature. An ignored feature at index ``i``
+        will have ``n_bins_[i] == 0``.
+
+    bin_edges_ : array of arrays, shape (n_features, )
+        The edges of each bin. Contain arrays of varying shapes ``(n_bins_, )``
+        Ignored features will have empty arrays.
+
+    Examples
+    --------
+    >>> X = [[-2, 1, -4,   -1],
+    ...      [-1, 2, -3, -0.5],
+    ...      [ 0, 3, -2,  0.5],
+    ...      [ 1, 4, -1,    2]]
+    >>> est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')
+    >>> est.fit(X)  # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE
+    KBinsDiscretizer(...)
+    >>> Xt = est.transform(X)
+    >>> Xt  # doctest: +SKIP
+    array([[ 0., 0., 0., 0.],
+           [ 1., 1., 1., 0.],
+           [ 2., 2., 2., 1.],
+           [ 2., 2., 2., 2.]])
+
+    Sometimes it may be useful to convert the data back into the original
+    feature space. The ``inverse_transform`` function converts the binned
+    data into the original feature space. Each value will be equal to the mean
+    of the two bin edges.
+
+    >>> est.bin_edges_[0]
+    array([-2., -1.,  0.,  1.])
+    >>> est.inverse_transform(Xt)
+    array([[-1.5,  1.5, -3.5, -0.5],
+           [-0.5,  2.5, -2.5, -0.5],
+           [ 0.5,  3.5, -1.5,  0.5],
+           [ 0.5,  3.5, -1.5,  1.5]])
+
+    Notes
+    -----
+    In bin edges for feature ``i``, the first and last values are used only for
+    ``inverse_transform``. During transform, bin edges are extended to::
+
+      np.concatenate([-np.inf, bin_edges_[i][1:-1], np.inf])
+
+    You can combine ``KBinsDiscretizer`` with
+    :class:`sklearn.compose.ColumnTransformer` if you only want to preprocess
+    part of the features.
+
+    See also
+    --------
+     sklearn.preprocessing.Binarizer : class used to bin values as ``0`` or
+        ``1`` based on a parameter ``threshold``.
+    """
+
+    def __init__(self, n_bins=5, encode='onehot', strategy='quantile'):
+        self.n_bins = n_bins
+        self.encode = encode
+        self.strategy = strategy
+
+    def fit(self, X, y=None):
+        """Fits the estimator.
+
+        Parameters
+        ----------
+        X : numeric array-like, shape (n_samples, n_features)
+            Data to be discretized.
+
+        y : ignored
+
+        Returns
+        -------
+        self
+        """
+        X = check_array(X, dtype='numeric')
+
+        valid_encode = ('onehot', 'onehot-dense', 'ordinal')
+        if self.encode not in valid_encode:
+            raise ValueError("Valid options for 'encode' are {}. "
+                             "Got encode={!r} instead."
+                             .format(valid_encode, self.encode))
+        valid_strategy = ('uniform', 'quantile', 'kmeans')
+        if self.strategy not in valid_strategy:
+            raise ValueError("Valid options for 'strategy' are {}. "
+                             "Got strategy={!r} instead."
+                             .format(valid_strategy, self.strategy))
+
+        n_features = X.shape[1]
+        n_bins = self._validate_n_bins(n_features)
+
+        bin_edges = np.zeros(n_features, dtype=object)
+        for jj in range(n_features):
+            column = X[:, jj]
+            col_min, col_max = column.min(), column.max()
+
+            if col_min == col_max:
+                warnings.warn("Feature %d is constant and will be "
+                              "replaced with 0." % jj)
+                n_bins[jj] = 1
+                bin_edges[jj] = np.array([-np.inf, np.inf])
+                continue
+
+            if self.strategy == 'uniform':
+                bin_edges[jj] = np.linspace(col_min, col_max, n_bins[jj] + 1)
+
+            elif self.strategy == 'quantile':
+                quantiles = np.linspace(0, 100, n_bins[jj] + 1)
+                if np_version < (1, 9):
+                    quantiles = list(quantiles)
+                bin_edges[jj] = np.asarray(np.percentile(column, quantiles))
+
+            elif self.strategy == 'kmeans':
+                from ..cluster import KMeans  # fixes import loops
+
+                # Deterministic initialization with uniform spacing
+                uniform_edges = np.linspace(col_min, col_max, n_bins[jj] + 1)
+                init = (uniform_edges[1:] + uniform_edges[:-1])[:, None] * 0.5
+
+                # 1D k-means procedure
+                km = KMeans(n_clusters=n_bins[jj], init=init, n_init=1)
+                centers = km.fit(column[:, None]).cluster_centers_[:, 0]
+                bin_edges[jj] = (centers[1:] + centers[:-1]) * 0.5
+                bin_edges[jj] = np.r_[col_min, bin_edges[jj], col_max]
+
+        self.bin_edges_ = bin_edges
+        self.n_bins_ = n_bins
+
+        if 'onehot' in self.encode:
+            self._encoder = OneHotEncoder(
+                categories=[np.arange(i) for i in self.n_bins_],
+                sparse=self.encode == 'onehot')
+
+        return self
+
+    def _validate_n_bins(self, n_features):
+        """Returns n_bins_, the number of bins per feature.
+
+        Also ensures that ignored bins are zero.
+        """
+        orig_bins = self.n_bins
+        if isinstance(orig_bins, numbers.Number):
+            if not isinstance(orig_bins, (numbers.Integral, np.integer)):
+                raise ValueError("{} received an invalid n_bins type. "
+                                 "Received {}, expected int."
+                                 .format(KBinsDiscretizer.__name__,
+                                         type(orig_bins).__name__))
+            if orig_bins < 2:
+                raise ValueError("{} received an invalid number "
+                                 "of bins. Received {}, expected at least 2."
+                                 .format(KBinsDiscretizer.__name__, orig_bins))
+            return np.full(n_features, orig_bins, dtype=np.int)
+
+        n_bins = check_array(orig_bins, dtype=np.int, copy=True,
+                             ensure_2d=False)
+
+        if n_bins.ndim > 1 or n_bins.shape[0] != n_features:
+            raise ValueError("n_bins must be a scalar or array "
+                             "of shape (n_features,).")
+
+        bad_nbins_value = (n_bins < 2) | (n_bins != orig_bins)
+
+        violating_indices = np.where(bad_nbins_value)[0]
+        if violating_indices.shape[0] > 0:
+            indices = ", ".join(str(i) for i in violating_indices)
+            raise ValueError("{} received an invalid number "
+                             "of bins at indices {}. Number of bins "
+                             "must be at least 2, and must be an int."
+                             .format(KBinsDiscretizer.__name__, indices))
+        return n_bins
+
+    def transform(self, X):
+        """Discretizes the data.
+
+        Parameters
+        ----------
+        X : numeric array-like, shape (n_samples, n_features)
+            Data to be discretized.
+
+        Returns
+        -------
+        Xt : numeric array-like or sparse matrix
+            Data in the binned space.
+        """
+        check_is_fitted(self, ["bin_edges_"])
+
+        Xt = check_array(X, copy=True, dtype=FLOAT_DTYPES)
+        n_features = self.n_bins_.shape[0]
+        if Xt.shape[1] != n_features:
+            raise ValueError("Incorrect number of features. Expecting {}, "
+                             "received {}.".format(n_features, Xt.shape[1]))
+
+        bin_edges = self.bin_edges_
+        for jj in range(Xt.shape[1]):
+            # Values which are close to a bin edge are susceptible to numeric
+            # instability. Add eps to X so these values are binned correctly
+            # with respect to their decimal truncation. See documentation of
+            # numpy.isclose for an explanation of ``rtol`` and ``atol``.
+            rtol = 1.e-5
+            atol = 1.e-8
+            eps = atol + rtol * np.abs(Xt[:, jj])
+            Xt[:, jj] = np.digitize(Xt[:, jj] + eps, bin_edges[jj][1:])
+        np.clip(Xt, 0, self.n_bins_ - 1, out=Xt)
+
+        if self.encode == 'ordinal':
+            return Xt
+
+        return self._encoder.fit_transform(Xt)
+
+    def inverse_transform(self, Xt):
+        """Transforms discretized data back to original feature space.
+
+        Note that this function does not regenerate the original data
+        due to discretization rounding.
+
+        Parameters
+        ----------
+        Xt : numeric array-like, shape (n_sample, n_features)
+            Transformed data in the binned space.
+
+        Returns
+        -------
+        Xinv : numeric array-like
+            Data in the original feature space.
+        """
+        check_is_fitted(self, ["bin_edges_"])
+
+        if 'onehot' in self.encode:
+            Xt = self._encoder.inverse_transform(Xt)
+
+        Xinv = check_array(Xt, copy=True, dtype=FLOAT_DTYPES)
+        n_features = self.n_bins_.shape[0]
+        if Xinv.shape[1] != n_features:
+            raise ValueError("Incorrect number of features. Expecting {}, "
+                             "received {}.".format(n_features, Xinv.shape[1]))
+
+        for jj in range(n_features):
+            bin_edges = self.bin_edges_[jj]
+            bin_centers = (bin_edges[1:] + bin_edges[:-1]) * 0.5
+            Xinv[:, jj] = bin_centers[np.int_(Xinv[:, jj])]
+
+        return Xinv
diff --git a/sklearn/preprocessing/_encoders.py b/sklearn/preprocessing/_encoders.py
index eee30948e8db..bd6e10fb6281 100644
--- a/sklearn/preprocessing/_encoders.py
+++ b/sklearn/preprocessing/_encoders.py
@@ -15,8 +15,10 @@
 from ..utils import check_array
 from ..utils import deprecated
 from ..utils.fixes import _argmax
-from ..utils.validation import check_is_fitted, FLOAT_DTYPES
-from .label import LabelEncoder
+from ..utils.validation import check_is_fitted
+
+from .base import _transform_selected
+from .label import _encode, _encode_check_unknown
 
 
 range = six.moves.range
@@ -28,64 +30,6 @@
 ]
 
 
-def _transform_selected(X, transform, dtype, selected="all", copy=True):
-    """Apply a transform function to portion of selected features
-
-    Parameters
-    ----------
-    X : {array-like, sparse matrix}, shape [n_samples, n_features]
-        Dense array or sparse matrix.
-
-    transform : callable
-        A callable transform(X) -> X_transformed
-
-    dtype : number type
-        Desired dtype of output.
-
-    copy : boolean, optional
-        Copy X even if it could be avoided.
-
-    selected: "all" or array of indices or mask
-        Specify which features to apply the transform to.
-
-    Returns
-    -------
-    X : array or sparse matrix, shape=(n_samples, n_features_new)
-    """
-    X = check_array(X, accept_sparse='csc', copy=copy, dtype=FLOAT_DTYPES)
-
-    if isinstance(selected, six.string_types) and selected == "all":
-        return transform(X)
-
-    if len(selected) == 0:
-        return X
-
-    n_features = X.shape[1]
-    ind = np.arange(n_features)
-    sel = np.zeros(n_features, dtype=bool)
-    sel[np.asarray(selected)] = True
-    not_sel = np.logical_not(sel)
-    n_selected = np.sum(sel)
-
-    if n_selected == 0:
-        # No features selected.
-        return X
-    elif n_selected == n_features:
-        # All features selected.
-        return transform(X)
-    else:
-        X_sel = transform(X[:, ind[sel]])
-        # The columns of X which are not transformed need
-        # to be casted to the desire dtype before concatenation.
-        # Otherwise, the stacking will cast to the higher-precision dtype.
-        X_not_sel = X[:, ind[not_sel]].astype(dtype)
-
-        if sparse.issparse(X_sel) or sparse.issparse(X_not_sel):
-            return sparse.hstack((X_sel, X_not_sel))
-        else:
-            return np.hstack((X_sel, X_not_sel))
-
-
 class _BaseEncoder(BaseEstimator, TransformerMixin):
     """
     Base class for encoders that includes the code to categorize and
@@ -104,32 +48,30 @@ def _fit(self, X, handle_unknown='error'):
         n_samples, n_features = X.shape
 
         if self._categories != 'auto':
-            for cats in self._categories:
-                if not np.all(np.sort(cats) == np.array(cats)):
-                    raise ValueError("Unsorted categories are not yet "
-                                     "supported")
+            if X.dtype != object:
+                for cats in self._categories:
+                    if not np.all(np.sort(cats) == np.array(cats)):
+                        raise ValueError("Unsorted categories are not "
+                                         "supported for numerical categories")
             if len(self._categories) != n_features:
                 raise ValueError("Shape mismatch: if n_values is an array,"
                                  " it has to be of shape (n_features,).")
 
-        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]
+        self.categories_ = []
 
         for i in range(n_features):
-            le = self._label_encoders_[i]
             Xi = X[:, i]
             if self._categories == 'auto':
-                le.fit(Xi)
+                cats = _encode(Xi)
             else:
-                if handle_unknown == 'error':
-                    valid_mask = np.in1d(Xi, self._categories[i])
-                    if not np.all(valid_mask):
-                        diff = np.unique(Xi[~valid_mask])
+                cats = np.array(self._categories[i], dtype=X.dtype)
+                if self.handle_unknown == 'error':
+                    diff = _encode_check_unknown(Xi, cats)
+                    if diff:
                         msg = ("Found unknown categories {0} in column {1}"
                                " during fit".format(diff, i))
                         raise ValueError(msg)
-                le.classes_ = np.array(self._categories[i], dtype=X.dtype)
-
-        self.categories_ = [le.classes_ for le in self._label_encoders_]
+            self.categories_.append(cats)
 
     def _transform(self, X, handle_unknown='error'):
 
@@ -145,11 +87,11 @@ def _transform(self, X, handle_unknown='error'):
 
         for i in range(n_features):
             Xi = X[:, i]
-            valid_mask = np.in1d(Xi, self.categories_[i])
+            diff, valid_mask = _encode_check_unknown(Xi, self.categories_[i],
+                                                     return_mask=True)
 
             if not np.all(valid_mask):
                 if handle_unknown == 'error':
-                    diff = np.unique(X[~valid_mask, i])
                     msg = ("Found unknown categories {0} in column {1}"
                            " during transform".format(diff, i))
                     raise ValueError(msg)
@@ -160,7 +102,8 @@ def _transform(self, X, handle_unknown='error'):
                     X_mask[:, i] = valid_mask
                     Xi = Xi.copy()
                     Xi[~valid_mask] = self.categories_[i][0]
-            X_int[:, i] = self._label_encoders_[i].transform(Xi)
+            _, encoded = _encode(Xi, self.categories_[i], encode=True)
+            X_int[:, i] = encoded
 
         return X_int, X_mask
 
@@ -195,8 +138,9 @@ class OneHotEncoder(_BaseEncoder):
 
         - 'auto' : Determine categories automatically from the training data.
         - list : ``categories[i]`` holds the categories expected in the ith
-          column. The passed categories must be sorted and should not mix
-          strings and numeric values.
+          column. The passed categories should not mix strings and numeric
+          values within a single feature, and should be sorted in case of
+          numeric values.
 
         The used categories can be found in the ``categories_`` attribute.
 
@@ -254,24 +198,24 @@ class OneHotEncoder(_BaseEncoder):
         in the training set. Only available when n_values is ``'auto'``.
 
         .. deprecated:: 0.20
-            The `active_features_` attribute was deprecated in version
+            The ``active_features_`` attribute was deprecated in version
             0.20 and will be removed in 0.22.
 
     feature_indices_ : array of shape (n_features,)
         Indices to feature ranges.
         Feature ``i`` in the original data is mapped to features
         from ``feature_indices_[i]`` to ``feature_indices_[i+1]``
-        (and then potentially masked by `active_features_` afterwards)
+        (and then potentially masked by ``active_features_`` afterwards)
 
         .. deprecated:: 0.20
-            The `feature_indices_` attribute was deprecated in version
+            The ``feature_indices_`` attribute was deprecated in version
             0.20 and will be removed in 0.22.
 
     n_values_ : array of shape (n_features,)
         Maximum number of values per feature.
 
         .. deprecated:: 0.20
-            The `n_values_` attribute was deprecated in version
+            The ``n_values_`` attribute was deprecated in version
             0.20 and will be removed in 0.22.
 
     Examples
@@ -296,6 +240,8 @@ class OneHotEncoder(_BaseEncoder):
     >>> enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])
     array([['Male', 1],
            [None, 2]], dtype=object)
+    >>> enc.get_feature_names()
+    array(['x0_Female', 'x0_Male', 'x1_1', 'x1_2', 'x1_3'], dtype=object)
 
     See also
     --------
@@ -325,21 +271,21 @@ def __init__(self, n_values=None, categorical_features=None,
     # Deprecated attributes
 
     @property
-    @deprecated("The 'active_features_' attribute was deprecated in version "
+    @deprecated("The ``active_features_`` attribute was deprecated in version "
                 "0.20 and will be removed 0.22.")
     def active_features_(self):
         check_is_fitted(self, 'categories_')
         return self._active_features_
 
     @property
-    @deprecated("The 'feature_indices_' attribute was deprecated in version "
+    @deprecated("The ``feature_indices_`` attribute was deprecated in version "
                 "0.20 and will be removed 0.22.")
     def feature_indices_(self):
         check_is_fitted(self, 'categories_')
         return self._feature_indices_
 
     @property
-    @deprecated("The 'n_values_' attribute was deprecated in version "
+    @deprecated("The ``n_values_`` attribute was deprecated in version "
                 "0.20 and will be removed 0.22.")
     def n_values_(self):
         check_is_fitted(self, 'categories_')
@@ -695,6 +641,38 @@ def inverse_transform(self, X):
 
         return X_tr
 
+    def get_feature_names(self, input_features=None):
+        """Return feature names for output features.
+
+        Parameters
+        ----------
+        input_features : list of string, length n_features, optional
+            String names for input features if available. By default,
+            "x0", "x1", ... "xn_features" is used.
+
+        Returns
+        -------
+        output_feature_names : array of string, length n_output_features
+
+        """
+        check_is_fitted(self, 'categories_')
+        cats = self.categories_
+        if input_features is None:
+            input_features = ['x%d' % i for i in range(len(cats))]
+        elif(len(input_features) != len(self.categories_)):
+            raise ValueError(
+                "input_features should have length equal to number of "
+                "features ({}), got {}".format(len(self.categories_),
+                                               len(input_features)))
+
+        feature_names = []
+        for i in range(len(cats)):
+            names = [
+                input_features[i] + '_' + six.text_type(t) for t in cats[i]]
+            feature_names.extend(names)
+
+        return np.array(feature_names, dtype=object)
+
 
 class OrdinalEncoder(_BaseEncoder):
     """Encode categorical features as an integer array.
@@ -713,8 +691,8 @@ class OrdinalEncoder(_BaseEncoder):
 
         - 'auto' : Determine categories automatically from the training data.
         - list : ``categories[i]`` holds the categories expected in the ith
-          column. The passed categories must be sorted and should not mix
-          strings and numeric values.
+          column. The passed categories should not mix strings and numeric
+          values, and should be sorted in case of numeric values.
 
         The used categories can be found in the ``categories_`` attribute.
 
diff --git a/sklearn/preprocessing/_function_transformer.py b/sklearn/preprocessing/_function_transformer.py
index f2a129068599..0c7954333821 100644
--- a/sklearn/preprocessing/_function_transformer.py
+++ b/sklearn/preprocessing/_function_transformer.py
@@ -42,10 +42,16 @@ class FunctionTransformer(BaseEstimator, TransformerMixin):
 
     validate : bool, optional default=True
         Indicate that the input X array should be checked before calling
-        func. If validate is false, there will be no input validation.
-        If it is true, then X will be converted to a 2-dimensional NumPy
-        array or sparse matrix. If this conversion is not possible or X
-        contains NaN or infinity, an exception is raised.
+        ``func``. The possibilities are:
+
+        - If False, there is no input validation.
+        - If True, then X will be converted to a 2-dimensional NumPy array or
+          sparse matrix. If the conversion is not possible an exception is
+          raised.
+
+        .. deprecated:: 0.20
+           ``validate=True`` as default will be replaced by
+           ``validate=False`` in 0.22.
 
     accept_sparse : boolean, optional
         Indicate that func accepts a sparse matrix as input. If validate is
@@ -72,7 +78,7 @@ class FunctionTransformer(BaseEstimator, TransformerMixin):
         Dictionary of additional keyword arguments to pass to inverse_func.
 
     """
-    def __init__(self, func=None, inverse_func=None, validate=True,
+    def __init__(self, func=None, inverse_func=None, validate=None,
                  accept_sparse=False, pass_y='deprecated', check_inverse=True,
                  kw_args=None, inv_kw_args=None):
         self.func = func
@@ -84,6 +90,19 @@ def __init__(self, func=None, inverse_func=None, validate=True,
         self.kw_args = kw_args
         self.inv_kw_args = inv_kw_args
 
+    def _check_input(self, X):
+        # FIXME: Future warning to be removed in 0.22
+        if self.validate is None:
+            self._validate = True
+            warnings.warn("The default validate=True will be replaced by "
+                          "validate=False in 0.22.", FutureWarning)
+        else:
+            self._validate = self.validate
+
+        if self._validate:
+            return check_array(X, accept_sparse=self.accept_sparse)
+        return X
+
     def _check_inverse_transform(self, X):
         """Check that func and inverse_func are the inverse."""
         idx_selected = slice(None, None, max(1, X.shape[0] // 100))
@@ -111,8 +130,7 @@ def fit(self, X, y=None):
         -------
         self
         """
-        if self.validate:
-            X = check_array(X, self.accept_sparse)
+        X = self._check_input(X)
         if (self.check_inverse and not (self.func is None or
                                         self.inverse_func is None)):
             self._check_inverse_transform(X)
@@ -165,8 +183,7 @@ def inverse_transform(self, X, y='deprecated'):
                                kw_args=self.inv_kw_args)
 
     def _transform(self, X, y=None, func=None, kw_args=None):
-        if self.validate:
-            X = check_array(X, self.accept_sparse)
+        X = self._check_input(X)
 
         if func is None:
             func = _identity
diff --git a/sklearn/preprocessing/base.py b/sklearn/preprocessing/base.py
new file mode 100644
index 000000000000..4b0cdbc35e1e
--- /dev/null
+++ b/sklearn/preprocessing/base.py
@@ -0,0 +1,90 @@
+"""Helpers for preprocessing"""
+
+import numpy as np
+from scipy import sparse
+
+from ..utils import check_array
+from ..utils.validation import FLOAT_DTYPES
+from ..externals import six
+
+
+def _transform_selected(X, transform, dtype, selected="all", copy=True,
+                        retain_order=False):
+    """Apply a transform function to portion of selected features.
+
+    Returns an array Xt, where the non-selected features appear on the right
+    side (largest column indices) of Xt.
+
+    Parameters
+    ----------
+    X : {array-like, sparse matrix}, shape [n_samples, n_features]
+        Dense array or sparse matrix.
+
+    transform : callable
+        A callable transform(X) -> X_transformed
+
+    dtype : number type
+        Desired dtype of output.
+
+    copy : boolean, default=True
+        Copy X even if it could be avoided.
+
+    selected : "all" or array of indices or mask
+        Specify which features to apply the transform to.
+
+    retain_order : boolean, default=False
+        If True, the non-selected features will not be displaced to the right
+        side of the transformed array. The number of features in Xt must
+        match the number of features in X. Furthermore, X and Xt cannot be
+        sparse.
+
+    Returns
+    -------
+    Xt : array or sparse matrix, shape=(n_samples, n_features_new)
+    """
+    X = check_array(X, accept_sparse='csc', copy=copy, dtype=FLOAT_DTYPES)
+
+    if sparse.issparse(X) and retain_order:
+        raise ValueError("The retain_order option can only be set to True "
+                         "for dense matrices.")
+
+    if isinstance(selected, six.string_types) and selected == "all":
+        return transform(X)
+
+    if len(selected) == 0:
+        return X
+
+    n_features = X.shape[1]
+    ind = np.arange(n_features)
+    sel = np.zeros(n_features, dtype=bool)
+    sel[np.asarray(selected)] = True
+    not_sel = np.logical_not(sel)
+    n_selected = np.sum(sel)
+
+    if n_selected == 0:
+        # No features selected.
+        return X
+    elif n_selected == n_features:
+        # All features selected.
+        return transform(X)
+    else:
+        X_sel = transform(X[:, ind[sel]])
+        # The columns of X which are not transformed need
+        # to be casted to the desire dtype before concatenation.
+        # Otherwise, the stacking will cast to the higher-precision dtype.
+        X_not_sel = X[:, ind[not_sel]].astype(dtype)
+
+    if retain_order:
+        if X_sel.shape[1] + X_not_sel.shape[1] != n_features:
+            raise ValueError("The retain_order option can only be set to True "
+                             "if the dimensions of the input array match the "
+                             "dimensions of the transformed array.")
+
+        # Fancy indexing not supported for sparse matrices
+        X[:, ind[sel]] = X_sel
+        return X
+
+    if sparse.issparse(X_sel) or sparse.issparse(X_not_sel):
+        return sparse.hstack((X_sel, X_not_sel))
+    else:
+        return np.hstack((X_sel, X_not_sel))
diff --git a/sklearn/preprocessing/data.py b/sklearn/preprocessing/data.py
index 7c014a07481b..0a33f9140f90 100644
--- a/sklearn/preprocessing/data.py
+++ b/sklearn/preprocessing/data.py
@@ -17,6 +17,7 @@
 import numpy as np
 from scipy import sparse
 from scipy import stats
+from scipy import optimize
 
 from ..base import BaseEstimator, TransformerMixin
 from ..externals import six
@@ -24,7 +25,7 @@
 from ..utils import check_array
 from ..utils.extmath import row_norms
 from ..utils.extmath import _incremental_mean_and_var
-from ..utils.fixes import boxcox, nanpercentile
+from ..utils.fixes import boxcox, nanpercentile, nanmedian
 from ..utils.sparsefuncs_fast import (inplace_csr_row_normalize_l1,
                                       inplace_csr_row_normalize_l2)
 from ..utils.sparsefuncs import (inplace_column_scale,
@@ -830,6 +831,20 @@ class MaxAbsScaler(BaseEstimator, TransformerMixin):
         The number of samples processed by the estimator. Will be reset on
         new calls to fit, but increments across ``partial_fit`` calls.
 
+    Examples
+    --------
+    >>> from sklearn.preprocessing import MaxAbsScaler
+    >>> X = [[ 1., -1.,  2.],
+    ...      [ 2.,  0.,  0.],
+    ...      [ 0.,  1., -1.]]
+    >>> transformer = MaxAbsScaler().fit(X)
+    >>> transformer
+    MaxAbsScaler(copy=True)
+    >>> transformer.transform(X)
+    array([[ 0.5, -1. ,  1. ],
+           [ 1. ,  0. ,  0. ],
+           [ 0. ,  1. , -0.5]])
+
     See also
     --------
     maxabs_scale: Equivalent function without the estimator API.
@@ -1067,6 +1082,21 @@ class RobustScaler(BaseEstimator, TransformerMixin):
         .. versionadded:: 0.17
            *scale_* attribute.
 
+    Examples
+    --------
+    >>> from sklearn.preprocessing import RobustScaler
+    >>> X = [[ 1., -2.,  2.],
+    ...      [ -2.,  1.,  3.],
+    ...      [ 4.,  1., -2.]]
+    >>> transformer = RobustScaler().fit(X)
+    >>> transformer
+    RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,
+           with_scaling=True)
+    >>> transformer.transform(X)
+    array([[ 0. , -2. ,  0. ],
+           [-1. ,  0. ,  0.4],
+           [ 1. ,  0. , -1.6]])
+
     See also
     --------
     robust_scale: Equivalent function without the estimator API.
@@ -1092,18 +1122,6 @@ def __init__(self, with_centering=True, with_scaling=True,
         self.quantile_range = quantile_range
         self.copy = copy
 
-    def _check_array(self, X, copy):
-        """Makes sure centering is not enabled for sparse matrices."""
-        X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,
-                        estimator=self, dtype=FLOAT_DTYPES)
-
-        if sparse.issparse(X):
-            if self.with_centering:
-                raise ValueError(
-                    "Cannot center sparse matrices: use `with_centering=False`"
-                    " instead. See docstring for motivation and alternatives.")
-        return X
-
     def fit(self, X, y=None):
         """Compute the median and quantiles to be used for scaling.
 
@@ -1113,39 +1131,60 @@ def fit(self, X, y=None):
             The data used to compute the median and quantiles
             used for later scaling along the features axis.
         """
-        if sparse.issparse(X):
-            raise TypeError("RobustScaler cannot be fitted on sparse inputs")
-        X = self._check_array(X, self.copy)
+        # at fit, convert sparse matrices to csc for optimized computation of
+        # the quantiles
+        X = check_array(X, accept_sparse='csc', copy=self.copy, estimator=self,
+                        dtype=FLOAT_DTYPES, force_all_finite='allow-nan')
+
+        q_min, q_max = self.quantile_range
+        if not 0 <= q_min <= q_max <= 100:
+            raise ValueError("Invalid quantile range: %s" %
+                             str(self.quantile_range))
+
         if self.with_centering:
-            self.center_ = np.median(X, axis=0)
+            if sparse.issparse(X):
+                raise ValueError(
+                    "Cannot center sparse matrices: use `with_centering=False`"
+                    " instead. See docstring for motivation and alternatives.")
+            self.center_ = nanmedian(X, axis=0)
+        else:
+            self.center_ = None
 
         if self.with_scaling:
-            q_min, q_max = self.quantile_range
-            if not 0 <= q_min <= q_max <= 100:
-                raise ValueError("Invalid quantile range: %s" %
-                                 str(self.quantile_range))
+            quantiles = []
+            for feature_idx in range(X.shape[1]):
+                if sparse.issparse(X):
+                    column_nnz_data = X.data[X.indptr[feature_idx]:
+                                             X.indptr[feature_idx + 1]]
+                    column_data = np.zeros(shape=X.shape[0], dtype=X.dtype)
+                    column_data[:len(column_nnz_data)] = column_nnz_data
+                else:
+                    column_data = X[:, feature_idx]
+
+                quantiles.append(nanpercentile(column_data,
+                                               self.quantile_range))
 
-            q = np.percentile(X, self.quantile_range, axis=0)
-            self.scale_ = (q[1] - q[0])
+            quantiles = np.transpose(quantiles)
+
+            self.scale_ = quantiles[1] - quantiles[0]
             self.scale_ = _handle_zeros_in_scale(self.scale_, copy=False)
+        else:
+            self.scale_ = None
+
         return self
 
     def transform(self, X):
         """Center and scale the data.
 
-        Can be called on sparse input, provided that ``RobustScaler`` has been
-        fitted to dense input and ``with_centering=False``.
-
         Parameters
         ----------
         X : {array-like, sparse matrix}
             The data used to scale along the specified axis.
         """
-        if self.with_centering:
-            check_is_fitted(self, 'center_')
-        if self.with_scaling:
-            check_is_fitted(self, 'scale_')
-        X = self._check_array(X, self.copy)
+        check_is_fitted(self, 'center_', 'scale_')
+        X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,
+                        estimator=self, dtype=FLOAT_DTYPES,
+                        force_all_finite='allow-nan')
 
         if sparse.issparse(X):
             if self.with_scaling:
@@ -1165,11 +1204,10 @@ def inverse_transform(self, X):
         X : array-like
             The data used to scale along the specified axis.
         """
-        if self.with_centering:
-            check_is_fitted(self, 'center_')
-        if self.with_scaling:
-            check_is_fitted(self, 'scale_')
-        X = self._check_array(X, self.copy)
+        check_is_fitted(self, 'center_', 'scale_')
+        X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,
+                        estimator=self, dtype=FLOAT_DTYPES,
+                        force_all_finite='allow-nan')
 
         if sparse.issparse(X):
             if self.with_scaling:
@@ -1242,7 +1280,8 @@ def robust_scale(X, axis=0, with_centering=True, with_scaling=True,
         (e.g. as part of a preprocessing :class:`sklearn.pipeline.Pipeline`).
     """
     X = check_array(X, accept_sparse=('csr', 'csc'), copy=False,
-                    ensure_2d=False, dtype=FLOAT_DTYPES)
+                    ensure_2d=False, dtype=FLOAT_DTYPES,
+                    force_all_finite='allow-nan')
     original_ndim = X.ndim
 
     if original_ndim == 1:
@@ -1570,6 +1609,20 @@ class Normalizer(BaseEstimator, TransformerMixin):
         copy (if the input is already a numpy array or a scipy.sparse
         CSR matrix).
 
+    Examples
+    --------
+    >>> from sklearn.preprocessing import Normalizer
+    >>> X = [[4, 1, 2, 2],
+    ...      [1, 3, 9, 3],
+    ...      [5, 7, 5, 1]]
+    >>> transformer = Normalizer().fit(X) # fit does nothing.
+    >>> transformer
+    Normalizer(copy=True, norm='l2')
+    >>> transformer.transform(X)
+    array([[0.8, 0.2, 0.4, 0.4],
+           [0.1, 0.3, 0.9, 0.3],
+           [0.5, 0.7, 0.5, 0.1]])
+
     Notes
     -----
     This estimator is stateless (besides constructor parameters), the
@@ -1697,6 +1750,20 @@ class Binarizer(BaseEstimator, TransformerMixin):
         set to False to perform inplace binarization and avoid a copy (if
         the input is already a numpy array or a scipy.sparse CSR matrix).
 
+    Examples
+    --------
+    >>> from sklearn.preprocessing import Binarizer
+    >>> X = [[ 1., -1.,  2.],
+    ...      [ 2.,  0.,  0.],
+    ...      [ 0.,  1., -1.]]
+    >>> transformer = Binarizer().fit(X) # fit does nothing.
+    >>> transformer
+    Binarizer(copy=True, threshold=0.0)
+    >>> transformer.transform(X)
+    array([[1., 0., 1.],
+           [1., 0., 0.],
+           [0., 1., 0.]])
+
     Notes
     -----
     If the input is a sparse matrix, only the non-zero values are subject
@@ -1761,8 +1828,32 @@ class KernelCenterer(BaseEstimator, TransformerMixin):
     sklearn.preprocessing.StandardScaler(with_std=False).
 
     Read more in the :ref:`User Guide <kernel_centering>`.
+
+    Examples
+    --------
+    >>> from sklearn.preprocessing import KernelCenterer
+    >>> from sklearn.metrics.pairwise import pairwise_kernels
+    >>> X = [[ 1., -2.,  2.],
+    ...      [ -2.,  1.,  3.],
+    ...      [ 4.,  1., -2.]]
+    >>> K = pairwise_kernels(X, metric='linear')
+    >>> K
+    array([[  9.,   2.,  -2.],
+           [  2.,  14., -13.],
+           [ -2., -13.,  21.]])
+    >>> transformer = KernelCenterer().fit(K)
+    >>> transformer
+    KernelCenterer()
+    >>> transformer.transform(K)
+    array([[  5.,   0.,  -5.],
+           [  0.,  14., -14.],
+           [ -5., -14.,  19.]])
     """
 
+    def __init__(self):
+        # Needed for backported inspect.signature compatibility with PyPy
+        pass
+
     def fit(self, K, y=None):
         """Fit KernelCenterer
 
@@ -1861,7 +1952,7 @@ def add_dummy_feature(X, value=1.0):
             # Row indices of dummy feature are 0, ..., n_samples-1.
             row = np.concatenate((np.arange(n_samples), X.row))
             # Prepend the dummy feature n_samples times.
-            data = np.concatenate((np.ones(n_samples) * value, X.data))
+            data = np.concatenate((np.full(n_samples, value), X.data))
             return sparse.coo_matrix((data, (row, col)), shape)
         elif sparse.isspmatrix_csc(X):
             # Shift index pointers since we need to add n_samples elements.
@@ -1871,13 +1962,13 @@ def add_dummy_feature(X, value=1.0):
             # Row indices of dummy feature are 0, ..., n_samples-1.
             indices = np.concatenate((np.arange(n_samples), X.indices))
             # Prepend the dummy feature n_samples times.
-            data = np.concatenate((np.ones(n_samples) * value, X.data))
+            data = np.concatenate((np.full(n_samples, value), X.data))
             return sparse.csc_matrix((data, indices, indptr), shape)
         else:
             klass = X.__class__
             return klass(add_dummy_feature(X.tocoo(), value))
     else:
-        return np.hstack((np.ones((n_samples, 1)) * value, X))
+        return np.hstack((np.full((n_samples, 1), value), X))
 
 
 class QuantileTransformer(BaseEstimator, TransformerMixin):
@@ -2388,10 +2479,12 @@ class PowerTransformer(BaseEstimator, TransformerMixin):
     modeling issues related to heteroscedasticity (non-constant variance),
     or other situations where normality is desired.
 
-    Currently, PowerTransformer supports the Box-Cox transform. Box-Cox
-    requires input data to be strictly positive. The optimal parameter
-    for stabilizing variance and minimizing skewness is estimated through
-    maximum likelihood.
+    Currently, PowerTransformer supports the Box-Cox transform and the
+    Yeo-Johson transform. The optimal parameter for stabilizing variance and
+    minimizing skewness is estimated through maximum likelihood.
+
+    Box-Cox requires input data to be strictly positive, while Yeo-Johnson
+    supports both positive or negative data.
 
     By default, zero-mean, unit-variance normalization is applied to the
     transformed data.
@@ -2400,9 +2493,11 @@ class PowerTransformer(BaseEstimator, TransformerMixin):
 
     Parameters
     ----------
-    method : str, (default='box-cox')
-        The power transform method. Currently, 'box-cox' (Box-Cox transform)
-        is the only option available.
+    method : str, (default='yeo-johnson')
+        The power transform method. Available methods are:
+
+        - 'yeo-johnson' [1]_, works with positive and negative values
+        - 'box-cox' [2]_, only works with strictly positive values
 
     standardize : boolean, default=True
         Set to True to apply zero-mean, unit-variance normalization to the
@@ -2423,13 +2518,13 @@ class PowerTransformer(BaseEstimator, TransformerMixin):
     >>> pt = PowerTransformer()
     >>> data = [[1, 2], [3, 2], [4, 5]]
     >>> print(pt.fit(data))
-    PowerTransformer(copy=True, method='box-cox', standardize=True)
-    >>> print(pt.lambdas_)  # doctest: +ELLIPSIS
-    [ 1.051... -2.345...]
-    >>> print(pt.transform(data))  # doctest: +ELLIPSIS
-    [[-1.332... -0.707...]
-     [ 0.256... -0.707...]
-     [ 1.076...  1.414...]]
+    PowerTransformer(copy=True, method='yeo-johnson', standardize=True)
+    >>> print(pt.lambdas_)
+    [1.38668178e+00 5.93926346e-09]
+    >>> print(pt.transform(data))
+    [[-1.31616039 -0.70710678]
+     [ 0.20998268 -0.70710678]
+     [ 1.1061777   1.41421356]]
 
     See also
     --------
@@ -2449,21 +2544,24 @@ class PowerTransformer(BaseEstimator, TransformerMixin):
 
     References
     ----------
-    G.E.P. Box and D.R. Cox, "An Analysis of Transformations", Journal of the
-    Royal Statistical Society B, 26, 211-252 (1964).
 
+    .. [1] I.K. Yeo and R.A. Johnson, "A new family of power transformations to
+           improve normality or symmetry." Biometrika, 87(4), pp.954-959,
+           (2000).
+
+    .. [2] G.E.P. Box and D.R. Cox, "An Analysis of Transformations", Journal
+           of the Royal Statistical Society B, 26, 211-252 (1964).
     """
-    def __init__(self, method='box-cox', standardize=True, copy=True):
+    def __init__(self, method='yeo-johnson', standardize=True, copy=True):
         self.method = method
         self.standardize = standardize
         self.copy = copy
 
     def fit(self, X, y=None):
-        """Estimate the optimal parameter for each feature.
+        """Estimate the optimal parameter lambda for each feature.
 
-        The optimal parameter for minimizing skewness is estimated
-        on each feature independently. If the method is Box-Cox,
-        the lambdas are estimated using maximum likelihood.
+        The optimal lambda parameter for minimizing skewness is estimated on
+        each feature independently using maximum likelihood.
 
         Parameters
         ----------
@@ -2476,27 +2574,44 @@ def fit(self, X, y=None):
         -------
         self : object
         """
+        self._fit(X, y=y, force_transform=False)
+        return self
+
+    def fit_transform(self, X, y=None):
+        return self._fit(X, y, force_transform=True)
+
+    def _fit(self, X, y=None, force_transform=False):
         X = self._check_input(X, check_positive=True, check_method=True)
 
-        self.lambdas_ = []
-        transformed = []
+        if not self.copy and not force_transform:  # if call from fit()
+            X = X.copy()  # force copy so that fit does not change X inplace
 
+        optim_function = {'box-cox': self._box_cox_optimize,
+                          'yeo-johnson': self._yeo_johnson_optimize
+                          }[self.method]
+        self.lambdas_ = []
         for col in X.T:
-            # the computation of lambda is influenced by NaNs and we need to
-            # get rid of them to compute them.
-            _, lmbda = stats.boxcox(col[~np.isnan(col)], lmbda=None)
-            col_trans = boxcox(col, lmbda)
-            self.lambdas_.append(lmbda)
-            transformed.append(col_trans)
-
+            with np.errstate(invalid='ignore'):  # hide NaN warnings
+                lmbda = optim_function(col)
+                self.lambdas_.append(lmbda)
         self.lambdas_ = np.array(self.lambdas_)
-        transformed = np.array(transformed)
+
+        if self.standardize or force_transform:
+            transform_function = {'box-cox': boxcox,
+                                  'yeo-johnson': self._yeo_johnson_transform
+                                  }[self.method]
+            for i, lmbda in enumerate(self.lambdas_):
+                with np.errstate(invalid='ignore'):  # hide NaN warnings
+                    X[:, i] = transform_function(X[:, i], lmbda)
 
         if self.standardize:
-            self._scaler = StandardScaler()
-            self._scaler.fit(X=transformed.T)
+            self._scaler = StandardScaler(copy=False)
+            if force_transform:
+                X = self._scaler.fit_transform(X)
+            else:
+                self._scaler.fit(X)
 
-        return self
+        return X
 
     def transform(self, X):
         """Apply the power transform to each feature using the fitted lambdas.
@@ -2505,12 +2620,21 @@ def transform(self, X):
         ----------
         X : array-like, shape (n_samples, n_features)
             The data to be transformed using a power transformation.
+
+        Returns
+        -------
+        X_trans : array-like, shape (n_samples, n_features)
+            The transformed data.
         """
         check_is_fitted(self, 'lambdas_')
         X = self._check_input(X, check_positive=True, check_shape=True)
 
+        transform_function = {'box-cox': boxcox,
+                              'yeo-johnson': self._yeo_johnson_transform
+                              }[self.method]
         for i, lmbda in enumerate(self.lambdas_):
-            X[:, i] = boxcox(X[:, i], lmbda)
+            with np.errstate(invalid='ignore'):  # hide NaN warnings
+                X[:, i] = transform_function(X[:, i], lmbda)
 
         if self.standardize:
             X = self._scaler.transform(X)
@@ -2527,10 +2651,26 @@ def inverse_transform(self, X):
             else:
                 X = (X_trans * lambda + 1) ** (1 / lambda)
 
+        The inverse of the Yeo-Johnson transformation is given by::
+
+            if X >= 0 and lambda == 0:
+                X = exp(X_trans) - 1
+            elif X >= 0 and lambda != 0:
+                X = (X_trans * lambda + 1) ** (1 / lambda) - 1
+            elif X < 0 and lambda != 2:
+                X = 1 - (-(2 - lambda) * X_trans + 1) ** (1 / (2 - lambda))
+            elif X < 0 and lambda == 2:
+                X = 1 - exp(-X_trans)
+
         Parameters
         ----------
         X : array-like, shape (n_samples, n_features)
             The transformed data.
+
+        Returns
+        -------
+        X : array-like, shape (n_samples, n_features)
+            The original data
         """
         check_is_fitted(self, 'lambdas_')
         X = self._check_input(X, check_shape=True)
@@ -2538,16 +2678,120 @@ def inverse_transform(self, X):
         if self.standardize:
             X = self._scaler.inverse_transform(X)
 
+        inv_fun = {'box-cox': self._box_cox_inverse_tranform,
+                   'yeo-johnson': self._yeo_johnson_inverse_transform
+                   }[self.method]
         for i, lmbda in enumerate(self.lambdas_):
-            x = X[:, i]
-            if lmbda == 0:
-                x_inv = np.exp(x)
-            else:
-                x_inv = (x * lmbda + 1) ** (1 / lmbda)
-            X[:, i] = x_inv
+            with np.errstate(invalid='ignore'):  # hide NaN warnings
+                X[:, i] = inv_fun(X[:, i], lmbda)
 
         return X
 
+    def _box_cox_inverse_tranform(self, x, lmbda):
+        """Return inverse-transformed input x following Box-Cox inverse
+        transform with parameter lambda.
+        """
+        if lmbda == 0:
+            x_inv = np.exp(x)
+        else:
+            x_inv = (x * lmbda + 1) ** (1 / lmbda)
+
+        return x_inv
+
+    def _yeo_johnson_inverse_transform(self, x, lmbda):
+        """Return inverse-transformed input x following Yeo-Johnson inverse
+        transform with parameter lambda.
+
+        Notes
+        -----
+        We're comparing lmbda to 1e-19 instead of strict equality to 0. See
+        scipy/special/_boxcox.pxd for a rationale behind this
+        """
+        x_inv = np.zeros(x.shape, dtype=x.dtype)
+        pos = x >= 0
+
+        # when x >= 0
+        if lmbda < 1e-19:
+            x_inv[pos] = np.exp(x[pos]) - 1
+        else:  # lmbda != 0
+            x_inv[pos] = np.power(x[pos] * lmbda + 1, 1 / lmbda) - 1
+
+        # when x < 0
+        if lmbda < 2 - 1e-19:
+            x_inv[~pos] = 1 - np.power(-(2 - lmbda) * x[~pos] + 1,
+                                       1 / (2 - lmbda))
+        else:  # lmbda == 2
+            x_inv[~pos] = 1 - np.exp(-x[~pos])
+
+        return x_inv
+
+    def _yeo_johnson_transform(self, x, lmbda):
+        """Return transformed input x following Yeo-Johnson transform with
+        parameter lambda.
+
+        Notes
+        -----
+        We're comparing lmbda to 1e-19 instead of strict equality to 0. See
+        scipy/special/_boxcox.pxd for a rationale behind this
+        """
+
+        out = np.zeros(shape=x.shape, dtype=x.dtype)
+        pos = x >= 0  # binary mask
+
+        # when x >= 0
+        if lmbda < 1e-19:
+            out[pos] = np.log(x[pos] + 1)
+        else:  # lmbda != 0
+            out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda
+
+        # when x < 0
+        if lmbda < 2 - 1e-19:
+            out[~pos] = -(np.power(-x[~pos] + 1, 2 - lmbda) - 1) / (2 - lmbda)
+        else:  # lmbda == 2
+            out[~pos] = -np.log(-x[~pos] + 1)
+
+        return out
+
+    def _box_cox_optimize(self, x):
+        """Find and return optimal lambda parameter of the Box-Cox transform by
+        MLE, for observed data x.
+
+        We here use scipy builtins which uses the brent optimizer.
+        """
+        # the computation of lambda is influenced by NaNs so we need to
+        # get rid of them
+        _, lmbda = stats.boxcox(x[~np.isnan(x)], lmbda=None)
+
+        return lmbda
+
+    def _yeo_johnson_optimize(self, x):
+        """Find and return optimal lambda parameter of the Yeo-Johnson
+        transform by MLE, for observed data x.
+
+        Like for Box-Cox, MLE is done via the brent optimizer.
+        """
+
+        def _neg_log_likelihood(lmbda):
+            """Return the negative log likelihood of the observed data x as a
+            function of lambda."""
+            x_trans = self._yeo_johnson_transform(x, lmbda)
+            n_samples = x.shape[0]
+
+            # Estimated mean and variance of the normal distribution
+            est_mean = x_trans.sum() / n_samples
+            est_var = np.power(x_trans - est_mean, 2).sum() / n_samples
+
+            loglike = -n_samples / 2 * np.log(est_var)
+            loglike += (lmbda - 1) * (np.sign(x) * np.log(np.abs(x) + 1)).sum()
+
+            return -loglike
+
+        # the computation of lambda is influenced by NaNs so we need to
+        # get rid of them
+        x = x[~np.isnan(x)]
+        # choosing bracket -2, 2 like for boxcox
+        return optimize.brent(_neg_log_likelihood, brack=(-2, 2))
+
     def _check_input(self, X, check_positive=False, check_shape=False,
                      check_method=False):
         """Validate the input before fit and transform.
@@ -2557,7 +2801,8 @@ def _check_input(self, X, check_positive=False, check_shape=False,
         X : array-like, shape (n_samples, n_features)
 
         check_positive : bool
-            If True, check that all data is positive and non-zero.
+            If True, check that all data is positive and non-zero (only if
+            ``self.method=='box-cox'``).
 
         check_shape : bool
             If True, check that n_features matches the length of self.lambdas_
@@ -2581,7 +2826,7 @@ def _check_input(self, X, check_positive=False, check_shape=False,
                              "than fitting data. Should have {n}, data has {m}"
                              .format(n=len(self.lambdas_), m=X.shape[1]))
 
-        valid_methods = ('box-cox',)
+        valid_methods = ('box-cox', 'yeo-johnson')
         if check_method and self.method not in valid_methods:
             raise ValueError("'method' must be one of {}, "
                              "got {} instead."
diff --git a/sklearn/preprocessing/label.py b/sklearn/preprocessing/label.py
index 043067fa37a8..51faccf1a30a 100644
--- a/sklearn/preprocessing/label.py
+++ b/sklearn/preprocessing/label.py
@@ -37,6 +37,129 @@
 ]
 
 
+def _encode_numpy(values, uniques=None, encode=False):
+    # only used in _encode below, see docstring there for details
+    if uniques is None:
+        if encode:
+            uniques, encoded = np.unique(values, return_inverse=True)
+            return uniques, encoded
+        else:
+            # unique sorts
+            return np.unique(values)
+    if encode:
+        diff = _encode_check_unknown(values, uniques)
+        if diff:
+            raise ValueError("y contains previously unseen labels: %s"
+                             % str(diff))
+        encoded = np.searchsorted(uniques, values)
+        return uniques, encoded
+    else:
+        return uniques
+
+
+def _encode_python(values, uniques=None, encode=False):
+    # only used in _encode below, see docstring there for details
+    if uniques is None:
+        uniques = sorted(set(values))
+        uniques = np.array(uniques, dtype=values.dtype)
+    if encode:
+        table = {val: i for i, val in enumerate(uniques)}
+        try:
+            encoded = np.array([table[v] for v in values])
+        except KeyError as e:
+            raise ValueError("y contains previously unseen labels: %s"
+                             % str(e))
+        return uniques, encoded
+    else:
+        return uniques
+
+
+def _encode(values, uniques=None, encode=False):
+    """Helper function to factorize (find uniques) and encode values.
+
+    Uses pure python method for object dtype, and numpy method for
+    all other dtypes.
+    The numpy method has the limitation that the `uniques` need to
+    be sorted. Importantly, this is not checked but assumed to already be
+    the case. The calling method needs to ensure this for all non-object
+    values.
+
+    Parameters
+    ----------
+    values : array
+        Values to factorize or encode.
+    uniques : array, optional
+        If passed, uniques are not determined from passed values (this
+        can be because the user specified categories, or because they
+        already have been determined in fit).
+    encode : bool, default False
+        If True, also encode the values into integer codes based on `uniques`.
+
+    Returns
+    -------
+    uniques
+        If ``encode=False``. The unique values are sorted if the `uniques`
+        parameter was None (and thus inferred from the data).
+    (uniques, encoded)
+        If ``encode=True``.
+
+    """
+    if values.dtype == object:
+        return _encode_python(values, uniques, encode)
+    else:
+        return _encode_numpy(values, uniques, encode)
+
+
+def _encode_check_unknown(values, uniques, return_mask=False):
+    """
+    Helper function to check for unknowns in values to be encoded.
+
+    Uses pure python method for object dtype, and numpy method for
+    all other dtypes.
+
+    Parameters
+    ----------
+    values : array
+        Values to check for unknowns.
+    uniques : array
+        Allowed uniques values.
+    return_mask : bool, default False
+        If True, return a mask of the same shape as `values` indicating
+        the valid values.
+
+    Returns
+    -------
+    diff : list
+        The unique values present in `values` and not in `uniques` (the
+        unknown values).
+    valid_mask : boolean array
+        Additionally returned if ``return_mask=True``.
+
+    """
+    if values.dtype == object:
+        uniques_set = set(uniques)
+        diff = list(set(values) - uniques_set)
+        if return_mask:
+            if diff:
+                valid_mask = np.array([val in uniques_set for val in values])
+            else:
+                valid_mask = np.ones(len(values), dtype=bool)
+            return diff, valid_mask
+        else:
+            return diff
+    else:
+        unique_values = np.unique(values)
+        diff = list(np.setdiff1d(unique_values, uniques, assume_unique=True))
+        if return_mask:
+            if diff:
+                valid_mask = np.in1d(values, uniques)
+            else:
+                valid_mask = np.ones(len(values), dtype=bool)
+            return diff, valid_mask
+        else:
+            return diff
+
+
 class LabelEncoder(BaseEstimator, TransformerMixin):
     """Encode labels with value between 0 and n_classes-1.
 
@@ -94,7 +217,7 @@ def fit(self, y):
         self : returns an instance of self.
         """
         y = column_or_1d(y, warn=True)
-        self.classes_ = np.unique(y)
+        self.classes_ = _encode(y)
         return self
 
     def fit_transform(self, y):
@@ -110,7 +233,7 @@ def fit_transform(self, y):
         y : array-like of shape [n_samples]
         """
         y = column_or_1d(y, warn=True)
-        self.classes_, y = np.unique(y, return_inverse=True)
+        self.classes_, y = _encode(y, encode=True)
         return y
 
     def transform(self, y):
@@ -131,12 +254,8 @@ def transform(self, y):
         if _num_samples(y) == 0:
             return np.array([])
 
-        classes = np.unique(y)
-        if len(np.intersect1d(classes, self.classes_)) < len(classes):
-            diff = np.setdiff1d(classes, self.classes_)
-            raise ValueError(
-                    "y contains previously unseen labels: %s" % str(diff))
-        return np.searchsorted(self.classes_, y)
+        _, y = _encode(y, uniques=self.classes_, encode=True)
+        return y
 
     def inverse_transform(self, y):
         """Transform labels back to original encoding.
diff --git a/sklearn/preprocessing/tests/test_base.py b/sklearn/preprocessing/tests/test_base.py
new file mode 100644
index 000000000000..530d9174a16d
--- /dev/null
+++ b/sklearn/preprocessing/tests/test_base.py
@@ -0,0 +1,89 @@
+import numpy as np
+import pytest
+from scipy import sparse
+
+from sklearn.utils.testing import assert_array_equal
+from sklearn.utils.testing import assert_raise_message
+from sklearn.preprocessing._encoders import _transform_selected
+from sklearn.preprocessing.data import Binarizer
+
+
+def toarray(a):
+    if hasattr(a, "toarray"):
+        a = a.toarray()
+    return a
+
+
+def _check_transform_selected(X, X_expected, dtype, sel):
+    for M in (X, sparse.csr_matrix(X)):
+        Xtr = _transform_selected(M, Binarizer().transform, dtype, sel)
+        assert_array_equal(toarray(Xtr), X_expected)
+
+
+@pytest.mark.parametrize("output_dtype", [np.int32, np.float32, np.float64])
+@pytest.mark.parametrize("input_dtype", [np.int32, np.float32, np.float64])
+def test_transform_selected(output_dtype, input_dtype):
+    X = np.asarray([[3, 2, 1], [0, 1, 1]], dtype=input_dtype)
+
+    X_expected = np.asarray([[1, 2, 1], [0, 1, 1]], dtype=output_dtype)
+    _check_transform_selected(X, X_expected, output_dtype, [0])
+    _check_transform_selected(X, X_expected, output_dtype,
+                              [True, False, False])
+
+    X_expected = np.asarray([[1, 1, 1], [0, 1, 1]], dtype=output_dtype)
+    _check_transform_selected(X, X_expected, output_dtype, [0, 1, 2])
+    _check_transform_selected(X, X_expected, output_dtype, [True, True, True])
+    _check_transform_selected(X, X_expected, output_dtype, "all")
+
+    _check_transform_selected(X, X, output_dtype, [])
+    _check_transform_selected(X, X, output_dtype, [False, False, False])
+
+
+@pytest.mark.parametrize("output_dtype", [np.int32, np.float32, np.float64])
+@pytest.mark.parametrize("input_dtype", [np.int32, np.float32, np.float64])
+def test_transform_selected_copy_arg(output_dtype, input_dtype):
+    # transformer that alters X
+    def _mutating_transformer(X):
+        X[0, 0] = X[0, 0] + 1
+        return X
+
+    original_X = np.asarray([[1, 2], [3, 4]], dtype=input_dtype)
+    expected_Xtr = np.asarray([[2, 2], [3, 4]], dtype=output_dtype)
+
+    X = original_X.copy()
+    Xtr = _transform_selected(X, _mutating_transformer, output_dtype,
+                              copy=True, selected='all')
+
+    assert_array_equal(toarray(X), toarray(original_X))
+    assert_array_equal(toarray(Xtr), expected_Xtr)
+
+
+def test_transform_selected_retain_order():
+    X = [[-1, 1], [2, -2]]
+
+    assert_raise_message(ValueError,
+                         "The retain_order option can only be set to True "
+                         "for dense matrices.",
+                         _transform_selected, sparse.csr_matrix(X),
+                         Binarizer().transform, dtype=np.int, selected=[0],
+                         retain_order=True)
+
+    def transform(X):
+        return np.hstack((X, [[0], [0]]))
+
+    assert_raise_message(ValueError,
+                         "The retain_order option can only be set to True "
+                         "if the dimensions of the input array match the "
+                         "dimensions of the transformed array.",
+                         _transform_selected, X, transform, dtype=np.int,
+                         selected=[0], retain_order=True)
+
+    X_expected = [[-1, 1], [2, 0]]
+    Xtr = _transform_selected(X, Binarizer().transform, dtype=np.int,
+                              selected=[1], retain_order=True)
+    assert_array_equal(toarray(Xtr), X_expected)
+
+    X_expected = [[0, 1], [1, -2]]
+    Xtr = _transform_selected(X, Binarizer().transform, dtype=np.int,
+                              selected=[0], retain_order=True)
+    assert_array_equal(toarray(Xtr), X_expected)
diff --git a/sklearn/preprocessing/tests/test_common.py b/sklearn/preprocessing/tests/test_common.py
index b3c8b7aed7a3..ac904d99e8af 100644
--- a/sklearn/preprocessing/tests/test_common.py
+++ b/sklearn/preprocessing/tests/test_common.py
@@ -15,12 +15,14 @@
 from sklearn.preprocessing import scale
 from sklearn.preprocessing import power_transform
 from sklearn.preprocessing import quantile_transform
+from sklearn.preprocessing import robust_scale
 
 from sklearn.preprocessing import MaxAbsScaler
 from sklearn.preprocessing import MinMaxScaler
 from sklearn.preprocessing import StandardScaler
 from sklearn.preprocessing import PowerTransformer
 from sklearn.preprocessing import QuantileTransformer
+from sklearn.preprocessing import RobustScaler
 
 from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_allclose
@@ -39,8 +41,11 @@ def _get_valid_samples_by_column(X, col):
      (MinMaxScaler(), minmax_scale, False, False),
      (StandardScaler(), scale, False, False),
      (StandardScaler(with_mean=False), scale, True, False),
-     (PowerTransformer(), power_transform, False, True),
-     (QuantileTransformer(n_quantiles=10), quantile_transform, True, False)]
+     (PowerTransformer('yeo-johnson'), power_transform, False, False),
+     (PowerTransformer('box-cox'), power_transform, False, True),
+     (QuantileTransformer(n_quantiles=10), quantile_transform, True, False),
+     (RobustScaler(), robust_scale, False, False),
+     (RobustScaler(with_centering=False), robust_scale, True, False)]
 )
 def test_missing_value_handling(est, func, support_sparse, strictly_positive):
     # check that the preprocessing method let pass nan
diff --git a/sklearn/preprocessing/tests/test_data.py b/sklearn/preprocessing/tests/test_data.py
index f90fbee278c0..f4d0b5af9799 100644
--- a/sklearn/preprocessing/tests/test_data.py
+++ b/sklearn/preprocessing/tests/test_data.py
@@ -62,6 +62,7 @@
 from sklearn.pipeline import Pipeline
 from sklearn.model_selection import cross_val_predict
 from sklearn.svm import SVR
+from sklearn.utils import shuffle
 
 from sklearn import datasets
 
@@ -209,7 +210,7 @@ def test_standard_scaler_1d():
         assert_array_almost_equal(X_scaled_back, X)
 
     # Constant feature
-    X = np.ones(5).reshape(5, 1)
+    X = np.ones((5, 1))
     scaler = StandardScaler()
     X_scaled = scaler.fit(X).transform(X, copy=True)
     assert_almost_equal(scaler.mean_, 1.)
@@ -237,7 +238,7 @@ def test_standard_scaler_numerical_stability():
     # np.log(1e-5) is taken because of its floating point representation
     # was empirically found to cause numerical problems with np.mean & np.std.
 
-    x = np.zeros(8, dtype=np.float64) + np.log(1e-5, dtype=np.float64)
+    x = np.full(8, np.log(1e-5), dtype=np.float64)
     if LooseVersion(np.__version__) >= LooseVersion('1.9'):
         # This does not raise a warning as the number of samples is too low
         # to trigger the problem in recent numpy
@@ -249,17 +250,17 @@ def test_standard_scaler_numerical_stability():
         assert_array_almost_equal(x_scaled, np.zeros(8))
 
     # with 2 more samples, the std computation run into numerical issues:
-    x = np.zeros(10, dtype=np.float64) + np.log(1e-5, dtype=np.float64)
+    x = np.full(10, np.log(1e-5), dtype=np.float64)
     w = "standard deviation of the data is probably very close to 0"
     x_scaled = assert_warns_message(UserWarning, w, scale, x)
     assert_array_almost_equal(x_scaled, np.zeros(10))
 
-    x = np.ones(10, dtype=np.float64) * 1e-100
+    x = np.full(10, 1e-100, dtype=np.float64)
     x_small_scaled = assert_no_warnings(scale, x)
     assert_array_almost_equal(x_small_scaled, np.zeros(10))
 
     # Large values can cause (often recoverable) numerical stability issues:
-    x_big = np.ones(10, dtype=np.float64) * 1e100
+    x_big = np.full(10, 1e100, dtype=np.float64)
     w = "Dataset may contain too large values"
     x_big_scaled = assert_warns_message(UserWarning, w, scale, x_big)
     assert_array_almost_equal(x_big_scaled, np.zeros(10))
@@ -510,7 +511,7 @@ def test_standard_scaler_trasform_with_partial_fit():
         assert_array_almost_equal(X_sofar, right_input)
 
         zero = np.zeros(X.shape[1])
-        epsilon = np.nextafter(0, 1)
+        epsilon = np.finfo(float).eps
         assert_array_less(zero, scaler_incr.var_ + epsilon)  # as less or equal
         assert_array_less(zero, scaler_incr.scale_ + epsilon)
         # (i+1) because the Scaler has been already fitted
@@ -621,7 +622,7 @@ def test_min_max_scaler_1d():
         assert_array_almost_equal(X_scaled_back, X)
 
     # Constant feature
-    X = np.ones(5).reshape(5, 1)
+    X = np.ones((5, 1))
     scaler = MinMaxScaler()
     X_scaled = scaler.fit(X).transform(X)
     assert_greater_equal(X_scaled.min(), 0.)
@@ -906,6 +907,52 @@ def test_scale_input_finiteness_validation():
                         scale, X)
 
 
+def test_robust_scaler_error_sparse():
+    X_sparse = sparse.rand(1000, 10)
+    scaler = RobustScaler(with_centering=True)
+    err_msg = "Cannot center sparse matrices"
+    with pytest.raises(ValueError, match=err_msg):
+        scaler.fit(X_sparse)
+
+
+@pytest.mark.parametrize("with_centering", [True, False])
+@pytest.mark.parametrize("with_scaling", [True, False])
+@pytest.mark.parametrize("X", [np.random.randn(10, 3),
+                               sparse.rand(10, 3, density=0.5)])
+def test_robust_scaler_attributes(X, with_centering, with_scaling):
+    # check consistent type of attributes
+    if with_centering and sparse.issparse(X):
+        pytest.skip("RobustScaler cannot center sparse matrix")
+
+    scaler = RobustScaler(with_centering=with_centering,
+                          with_scaling=with_scaling)
+    scaler.fit(X)
+
+    if with_centering:
+        assert isinstance(scaler.center_, np.ndarray)
+    else:
+        assert scaler.center_ is None
+    if with_scaling:
+        assert isinstance(scaler.scale_, np.ndarray)
+    else:
+        assert scaler.scale_ is None
+
+
+def test_robust_scaler_col_zero_sparse():
+    # check that the scaler is working when there is not data materialized in a
+    # column of a sparse matrix
+    X = np.random.randn(10, 5)
+    X[:, 0] = 0
+    X = sparse.csr_matrix(X)
+
+    scaler = RobustScaler(with_centering=False)
+    scaler.fit(X)
+    assert scaler.scale_[0] == pytest.approx(1)
+
+    X_trans = scaler.transform(X)
+    assert_allclose(X[:, 0].toarray(), X_trans[:, 0].toarray())
+
+
 def test_robust_scaler_2d_arrays():
     # Test robust scaling of 2d array along first axis
     rng = np.random.RandomState(0)
@@ -919,6 +966,29 @@ def test_robust_scaler_2d_arrays():
     assert_array_almost_equal(X_scaled.std(axis=0)[0], 0)
 
 
+@pytest.mark.parametrize("density", [0, 0.05, 0.1, 0.5, 1])
+@pytest.mark.parametrize("strictly_signed",
+                         ['positive', 'negative', 'zeros', None])
+def test_robust_scaler_equivalence_dense_sparse(density, strictly_signed):
+    # Check the equivalence of the fitting with dense and sparse matrices
+    X_sparse = sparse.rand(1000, 5, density=density).tocsc()
+    if strictly_signed == 'positive':
+        X_sparse.data = np.abs(X_sparse.data)
+    elif strictly_signed == 'negative':
+        X_sparse.data = - np.abs(X_sparse.data)
+    elif strictly_signed == 'zeros':
+        X_sparse.data = np.zeros(X_sparse.data.shape, dtype=np.float64)
+    X_dense = X_sparse.toarray()
+
+    scaler_sparse = RobustScaler(with_centering=False)
+    scaler_dense = RobustScaler(with_centering=False)
+
+    scaler_sparse.fit(X_sparse)
+    scaler_dense.fit(X_dense)
+
+    assert_allclose(scaler_sparse.scale_, scaler_dense.scale_)
+
+
 def test_robust_scaler_transform_one_row_csr():
     # Check RobustScaler on transforming csr matrix with one row
     rng = np.random.RandomState(0)
@@ -1508,7 +1578,7 @@ def test_maxabs_scaler_1d():
         assert_array_almost_equal(X_scaled_back, X)
 
     # Constant feature
-    X = np.ones(5).reshape(5, 1)
+    X = np.ones((5, 1))
     scaler = MaxAbsScaler()
     X_scaled = scaler.fit(X).transform(X)
     assert_array_almost_equal(np.abs(X_scaled.max(axis=0)), 1.)
@@ -1934,13 +2004,26 @@ def test_quantile_transform_valid_axis():
                         ". Got axis=2", quantile_transform, X.T, axis=2)
 
 
-def test_power_transformer_notfitted():
-    pt = PowerTransformer(method='box-cox')
+@pytest.mark.parametrize("method", ['box-cox', 'yeo-johnson'])
+def test_power_transformer_notfitted(method):
+    pt = PowerTransformer(method=method)
     X = np.abs(X_1col)
     assert_raises(NotFittedError, pt.transform, X)
     assert_raises(NotFittedError, pt.inverse_transform, X)
 
 
+@pytest.mark.parametrize('method', ['box-cox', 'yeo-johnson'])
+@pytest.mark.parametrize('standardize', [True, False])
+@pytest.mark.parametrize('X', [X_1col, X_2d])
+def test_power_transformer_inverse(method, standardize, X):
+    # Make sure we get the original input when applying transform and then
+    # inverse transform
+    X = np.abs(X) if method == 'box-cox' else X
+    pt = PowerTransformer(method=method, standardize=standardize)
+    X_trans = pt.fit_transform(X)
+    assert_almost_equal(X, pt.inverse_transform(X_trans))
+
+
 def test_power_transformer_1d():
     X = np.abs(X_1col)
 
@@ -1992,11 +2075,12 @@ def test_power_transformer_2d():
         assert isinstance(pt.lambdas_, np.ndarray)
 
 
-def test_power_transformer_strictly_positive_exception():
+def test_power_transformer_boxcox_strictly_positive_exception():
+    # Exceptions should be raised for negative arrays and zero arrays when
+    # method is boxcox
+
     pt = PowerTransformer(method='box-cox')
     pt.fit(np.abs(X_2d))
-
-    # Exceptions should be raised for negative arrays and zero arrays
     X_with_negatives = X_2d
     not_positive_message = 'strictly positive'
 
@@ -2007,7 +2091,7 @@ def test_power_transformer_strictly_positive_exception():
                          pt.fit, X_with_negatives)
 
     assert_raise_message(ValueError, not_positive_message,
-                         power_transform, X_with_negatives)
+                         power_transform, X_with_negatives, 'box-cox')
 
     assert_raise_message(ValueError, not_positive_message,
                          pt.transform, np.zeros(X_2d.shape))
@@ -2016,11 +2100,19 @@ def test_power_transformer_strictly_positive_exception():
                          pt.fit, np.zeros(X_2d.shape))
 
     assert_raise_message(ValueError, not_positive_message,
-                         power_transform, np.zeros(X_2d.shape))
+                         power_transform, np.zeros(X_2d.shape), 'box-cox')
 
 
-def test_power_transformer_shape_exception():
-    pt = PowerTransformer(method='box-cox')
+@pytest.mark.parametrize('X', [X_2d, np.abs(X_2d), -np.abs(X_2d),
+                               np.zeros(X_2d.shape)])
+def test_power_transformer_yeojohnson_any_input(X):
+    # Yeo-Johnson method should support any kind of input
+    power_transform(X, method='yeo-johnson')
+
+
+@pytest.mark.parametrize("method", ['box-cox', 'yeo-johnson'])
+def test_power_transformer_shape_exception(method):
+    pt = PowerTransformer(method=method)
     X = np.abs(X_2d)
     pt.fit(X)
 
@@ -2053,3 +2145,136 @@ def test_power_transformer_lambda_zero():
     pt.lambdas_ = np.array([0])
     X_trans = pt.transform(X)
     assert_array_almost_equal(pt.inverse_transform(X_trans), X)
+
+
+def test_power_transformer_lambda_one():
+    # Make sure lambda = 1 corresponds to the identity for yeo-johnson
+    pt = PowerTransformer(method='yeo-johnson', standardize=False)
+    X = np.abs(X_2d)[:, 0:1]
+
+    pt.lambdas_ = np.array([1])
+    X_trans = pt.transform(X)
+    assert_array_almost_equal(X_trans, X)
+
+
+@pytest.mark.parametrize("method, lmbda", [('box-cox', .1),
+                                           ('box-cox', .5),
+                                           ('yeo-johnson', .1),
+                                           ('yeo-johnson', .5),
+                                           ('yeo-johnson', 1.),
+                                           ])
+def test_optimization_power_transformer(method, lmbda):
+    # Test the optimization procedure:
+    # - set a predefined value for lambda
+    # - apply inverse_transform to a normal dist (we get X_inv)
+    # - apply fit_transform to X_inv (we get X_inv_trans)
+    # - check that X_inv_trans is roughly equal to X
+
+    rng = np.random.RandomState(0)
+    n_samples = 20000
+    X = rng.normal(loc=0, scale=1, size=(n_samples, 1))
+
+    pt = PowerTransformer(method=method, standardize=False)
+    pt.lambdas_ = [lmbda]
+    X_inv = pt.inverse_transform(X)
+
+    pt = PowerTransformer(method=method, standardize=False)
+    X_inv_trans = pt.fit_transform(X_inv)
+
+    assert_almost_equal(0, np.linalg.norm(X - X_inv_trans) / n_samples,
+                        decimal=2)
+    assert_almost_equal(0, X_inv_trans.mean(), decimal=1)
+    assert_almost_equal(1, X_inv_trans.std(), decimal=1)
+
+
+@pytest.mark.parametrize('method', ['box-cox', 'yeo-johnson'])
+def test_power_transformer_nans(method):
+    # Make sure lambda estimation is not influenced by NaN values
+    # and that transform() supports NaN silently
+
+    X = np.abs(X_1col)
+    pt = PowerTransformer(method=method)
+    pt.fit(X)
+    lmbda_no_nans = pt.lambdas_[0]
+
+    # concat nans at the end and check lambda stays the same
+    X = np.concatenate([X, np.full_like(X, np.nan)])
+    X = shuffle(X, random_state=0)
+
+    pt.fit(X)
+    lmbda_nans = pt.lambdas_[0]
+
+    assert_almost_equal(lmbda_no_nans, lmbda_nans, decimal=5)
+
+    X_trans = pt.transform(X)
+    assert_array_equal(np.isnan(X_trans), np.isnan(X))
+
+
+@pytest.mark.parametrize('method', ['box-cox', 'yeo-johnson'])
+@pytest.mark.parametrize('standardize', [True, False])
+def test_power_transformer_fit_transform(method, standardize):
+    # check that fit_transform() and fit().transform() return the same values
+    X = X_1col
+    if method == 'box-cox':
+        X = np.abs(X)
+
+    pt = PowerTransformer(method, standardize)
+    assert_array_almost_equal(pt.fit(X).transform(X), pt.fit_transform(X))
+
+
+@pytest.mark.parametrize('method', ['box-cox', 'yeo-johnson'])
+@pytest.mark.parametrize('standardize', [True, False])
+def test_power_transformer_copy_True(method, standardize):
+    # Check that neither fit, transform, fit_transform nor inverse_transform
+    # modify X inplace when copy=True
+    X = X_1col
+    if method == 'box-cox':
+        X = np.abs(X)
+
+    X_original = X.copy()
+    assert X is not X_original  # sanity checks
+    assert_array_almost_equal(X, X_original)
+
+    pt = PowerTransformer(method, standardize, copy=True)
+
+    pt.fit(X)
+    assert_array_almost_equal(X, X_original)
+    X_trans = pt.transform(X)
+    assert X_trans is not X
+
+    X_trans = pt.fit_transform(X)
+    assert_array_almost_equal(X, X_original)
+    assert X_trans is not X
+
+    X_inv_trans = pt.inverse_transform(X_trans)
+    assert X_trans is not X_inv_trans
+
+
+@pytest.mark.parametrize('method', ['box-cox', 'yeo-johnson'])
+@pytest.mark.parametrize('standardize', [True, False])
+def test_power_transformer_copy_False(method, standardize):
+    # check that when copy=False fit doesn't change X inplace but transform,
+    # fit_transform and inverse_transform do.
+    X = X_1col
+    if method == 'box-cox':
+        X = np.abs(X)
+
+    X_original = X.copy()
+    assert X is not X_original  # sanity checks
+    assert_array_almost_equal(X, X_original)
+
+    pt = PowerTransformer(method, standardize, copy=False)
+
+    pt.fit(X)
+    assert_array_almost_equal(X, X_original)  # fit didn't change X
+
+    X_trans = pt.transform(X)
+    assert X_trans is X
+
+    if method == 'box-cox':
+        X = np.abs(X)
+    X_trans = pt.fit_transform(X)
+    assert X_trans is X
+
+    X_inv_trans = pt.inverse_transform(X_trans)
+    assert X_trans is X_inv_trans
diff --git a/sklearn/preprocessing/tests/test_discretization.py b/sklearn/preprocessing/tests/test_discretization.py
new file mode 100644
index 000000000000..052061dfd7c2
--- /dev/null
+++ b/sklearn/preprocessing/tests/test_discretization.py
@@ -0,0 +1,251 @@
+from __future__ import absolute_import
+
+import pytest
+import numpy as np
+import scipy.sparse as sp
+import warnings
+
+from sklearn.externals.six.moves import xrange as range
+from sklearn.preprocessing import KBinsDiscretizer
+from sklearn.preprocessing import OneHotEncoder
+from sklearn.utils.testing import (
+    assert_array_equal,
+    assert_raises,
+    assert_raise_message,
+    assert_warns_message
+)
+
+X = [[-2, 1.5, -4, -1],
+     [-1, 2.5, -3, -0.5],
+     [0, 3.5, -2, 0.5],
+     [1, 4.5, -1, 2]]
+
+
+@pytest.mark.parametrize(
+    'strategy, expected',
+    [('uniform', [[0, 0, 0, 0], [1, 1, 1, 0], [2, 2, 2, 1], [2, 2, 2, 2]]),
+     ('kmeans', [[0, 0, 0, 0], [0, 0, 0, 0], [1, 1, 1, 1], [2, 2, 2, 2]]),
+     ('quantile', [[0, 0, 0, 0], [1, 1, 1, 1], [2, 2, 2, 2], [2, 2, 2, 2]])])
+def test_fit_transform(strategy, expected):
+    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy=strategy)
+    est.fit(X)
+    assert_array_equal(expected, est.transform(X))
+
+
+def test_valid_n_bins():
+    KBinsDiscretizer(n_bins=2).fit_transform(X)
+    KBinsDiscretizer(n_bins=np.array([2])[0]).fit_transform(X)
+    assert KBinsDiscretizer(n_bins=2).fit(X).n_bins_.dtype == np.dtype(np.int)
+
+
+def test_invalid_n_bins():
+    est = KBinsDiscretizer(n_bins=1)
+    assert_raise_message(ValueError, "KBinsDiscretizer received an invalid "
+                         "number of bins. Received 1, expected at least 2.",
+                         est.fit_transform, X)
+
+    est = KBinsDiscretizer(n_bins=1.1)
+    assert_raise_message(ValueError, "KBinsDiscretizer received an invalid "
+                         "n_bins type. Received float, expected int.",
+                         est.fit_transform, X)
+
+
+def test_invalid_n_bins_array():
+    # Bad shape
+    n_bins = np.full((2, 4), 2.)
+    est = KBinsDiscretizer(n_bins=n_bins)
+    assert_raise_message(ValueError,
+                         "n_bins must be a scalar or array of shape "
+                         "(n_features,).", est.fit_transform, X)
+
+    # Incorrect number of features
+    n_bins = [1, 2, 2]
+    est = KBinsDiscretizer(n_bins=n_bins)
+    assert_raise_message(ValueError,
+                         "n_bins must be a scalar or array of shape "
+                         "(n_features,).", est.fit_transform, X)
+
+    # Bad bin values
+    n_bins = [1, 2, 2, 1]
+    est = KBinsDiscretizer(n_bins=n_bins)
+    assert_raise_message(ValueError,
+                         "KBinsDiscretizer received an invalid number of bins "
+                         "at indices 0, 3. Number of bins must be at least 2, "
+                         "and must be an int.",
+                         est.fit_transform, X)
+
+    # Float bin values
+    n_bins = [2.1, 2, 2.1, 2]
+    est = KBinsDiscretizer(n_bins=n_bins)
+    assert_raise_message(ValueError,
+                         "KBinsDiscretizer received an invalid number of bins "
+                         "at indices 0, 2. Number of bins must be at least 2, "
+                         "and must be an int.",
+                         est.fit_transform, X)
+
+
+@pytest.mark.parametrize(
+    'strategy, expected',
+    [('uniform', [[0, 0, 0, 0], [0, 1, 1, 0], [1, 2, 2, 1], [1, 2, 2, 2]]),
+     ('kmeans', [[0, 0, 0, 0], [0, 0, 0, 0], [1, 1, 1, 1], [1, 2, 2, 2]]),
+     ('quantile', [[0, 0, 0, 0], [0, 1, 1, 1], [1, 2, 2, 2], [1, 2, 2, 2]])])
+def test_fit_transform_n_bins_array(strategy, expected):
+    est = KBinsDiscretizer(n_bins=[2, 3, 3, 3], encode='ordinal',
+                           strategy=strategy).fit(X)
+    assert_array_equal(expected, est.transform(X))
+
+    # test the shape of bin_edges_
+    n_features = np.array(X).shape[1]
+    assert est.bin_edges_.shape == (n_features, )
+    for bin_edges, n_bins in zip(est.bin_edges_, est.n_bins_):
+        assert bin_edges.shape == (n_bins + 1, )
+
+
+def test_invalid_n_features():
+    est = KBinsDiscretizer(n_bins=3).fit(X)
+    bad_X = np.arange(25).reshape(5, -1)
+    assert_raise_message(ValueError,
+                         "Incorrect number of features. Expecting 4, "
+                         "received 5", est.transform, bad_X)
+
+
+@pytest.mark.parametrize('strategy', ['uniform', 'kmeans', 'quantile'])
+def test_same_min_max(strategy):
+    warnings.simplefilter("always")
+    X = np.array([[1, -2],
+                  [1, -1],
+                  [1, 0],
+                  [1, 1]])
+    est = KBinsDiscretizer(strategy=strategy, n_bins=3, encode='ordinal')
+    assert_warns_message(UserWarning,
+                         "Feature 0 is constant and will be replaced "
+                         "with 0.", est.fit, X)
+    assert est.n_bins_[0] == 1
+    # replace the feature with zeros
+    Xt = est.transform(X)
+    assert_array_equal(Xt[:, 0], np.zeros(X.shape[0]))
+
+
+def test_transform_1d_behavior():
+    X = np.arange(4)
+    est = KBinsDiscretizer(n_bins=2)
+    assert_raises(ValueError, est.fit, X)
+
+    est = KBinsDiscretizer(n_bins=2)
+    est.fit(X.reshape(-1, 1))
+    assert_raises(ValueError, est.transform, X)
+
+
+def test_numeric_stability():
+    X_init = np.array([2., 4., 6., 8., 10.]).reshape(-1, 1)
+    Xt_expected = np.array([0, 0, 1, 1, 1]).reshape(-1, 1)
+
+    # Test up to discretizing nano units
+    for i in range(1, 9):
+        X = X_init / 10**i
+        Xt = KBinsDiscretizer(n_bins=2, encode='ordinal').fit_transform(X)
+        assert_array_equal(Xt_expected, Xt)
+
+
+def test_invalid_encode_option():
+    est = KBinsDiscretizer(n_bins=[2, 3, 3, 3], encode='invalid-encode')
+    assert_raise_message(ValueError, "Valid options for 'encode' are "
+                         "('onehot', 'onehot-dense', 'ordinal'). "
+                         "Got encode='invalid-encode' instead.",
+                         est.fit, X)
+
+
+def test_encode_options():
+    est = KBinsDiscretizer(n_bins=[2, 3, 3, 3],
+                           encode='ordinal').fit(X)
+    Xt_1 = est.transform(X)
+    est = KBinsDiscretizer(n_bins=[2, 3, 3, 3],
+                           encode='onehot-dense').fit(X)
+    Xt_2 = est.transform(X)
+    assert not sp.issparse(Xt_2)
+    assert_array_equal(OneHotEncoder(
+                           categories=[np.arange(i) for i in [2, 3, 3, 3]],
+                           sparse=False)
+                       .fit_transform(Xt_1), Xt_2)
+    est = KBinsDiscretizer(n_bins=[2, 3, 3, 3],
+                           encode='onehot').fit(X)
+    Xt_3 = est.transform(X)
+    assert sp.issparse(Xt_3)
+    assert_array_equal(OneHotEncoder(
+                           categories=[np.arange(i) for i in [2, 3, 3, 3]],
+                           sparse=True)
+                       .fit_transform(Xt_1).toarray(),
+                       Xt_3.toarray())
+
+
+def test_invalid_strategy_option():
+    est = KBinsDiscretizer(n_bins=[2, 3, 3, 3], strategy='invalid-strategy')
+    assert_raise_message(ValueError, "Valid options for 'strategy' are "
+                         "('uniform', 'quantile', 'kmeans'). "
+                         "Got strategy='invalid-strategy' instead.",
+                         est.fit, X)
+
+
+@pytest.mark.parametrize(
+    'strategy, expected_2bins, expected_3bins',
+    [('uniform', [0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 2, 2]),
+     ('kmeans', [0, 0, 0, 0, 1, 1], [0, 1, 1, 1, 2, 2]),
+     ('quantile', [0, 0, 0, 1, 1, 1], [0, 0, 1, 1, 2, 2])])
+def test_nonuniform_strategies(strategy, expected_2bins, expected_3bins):
+    X = np.array([0, 1, 2, 3, 9, 10]).reshape(-1, 1)
+
+    # with 2 bins
+    est = KBinsDiscretizer(n_bins=2, strategy=strategy, encode='ordinal')
+    Xt = est.fit_transform(X)
+    assert_array_equal(expected_2bins, Xt.ravel())
+
+    # with 3 bins
+    est = KBinsDiscretizer(n_bins=3, strategy=strategy, encode='ordinal')
+    Xt = est.fit_transform(X)
+    assert_array_equal(expected_3bins, Xt.ravel())
+
+
+@pytest.mark.parametrize('strategy', ['uniform', 'kmeans', 'quantile'])
+@pytest.mark.parametrize('encode', ['ordinal', 'onehot', 'onehot-dense'])
+def test_inverse_transform(strategy, encode):
+    X = np.random.RandomState(0).randn(100, 3)
+    kbd = KBinsDiscretizer(n_bins=3, strategy=strategy, encode=encode)
+    Xt = kbd.fit_transform(X)
+    X2 = kbd.inverse_transform(Xt)
+    X2t = kbd.fit_transform(X2)
+    if encode == 'onehot':
+        assert_array_equal(Xt.todense(), X2t.todense())
+    else:
+        assert_array_equal(Xt, X2t)
+    if 'onehot' in encode:
+        Xt = kbd._encoder.inverse_transform(Xt)
+        X2t = kbd._encoder.inverse_transform(X2t)
+
+    assert_array_equal(Xt.max(axis=0) + 1, kbd.n_bins_)
+    assert_array_equal(X2t.max(axis=0) + 1, kbd.n_bins_)
+
+
+@pytest.mark.parametrize('strategy', ['uniform', 'kmeans', 'quantile'])
+def test_transform_outside_fit_range(strategy):
+    X = np.array([0, 1, 2, 3])[:, None]
+    kbd = KBinsDiscretizer(n_bins=4, strategy=strategy, encode='ordinal')
+    kbd.fit(X)
+
+    X2 = np.array([-2, 5])[:, None]
+    X2t = kbd.transform(X2)
+    assert_array_equal(X2t.max(axis=0) + 1, kbd.n_bins_)
+    assert_array_equal(X2t.min(axis=0), [0])
+
+
+def test_overwrite():
+    X = np.array([0, 1, 2, 3])[:, None]
+    X_before = X.copy()
+
+    est = KBinsDiscretizer(n_bins=3, encode="ordinal")
+    Xt = est.fit_transform(X)
+    assert_array_equal(X, X_before)
+
+    Xt_before = Xt.copy()
+    Xinv = est.inverse_transform(Xt)
+    assert_array_equal(Xt, Xt_before)
+    assert_array_equal(Xinv, np.array([[0.5], [1.5], [2.5], [2.5]]))
diff --git a/sklearn/preprocessing/tests/test_encoders.py b/sklearn/preprocessing/tests/test_encoders.py
index d5acd110e286..9ec16b85df60 100644
--- a/sklearn/preprocessing/tests/test_encoders.py
+++ b/sklearn/preprocessing/tests/test_encoders.py
@@ -1,3 +1,4 @@
+# -*- coding: utf-8 -*-
 from __future__ import division
 
 import re
@@ -16,8 +17,6 @@
 from sklearn.utils.testing import assert_warns_message
 from sklearn.utils.testing import assert_no_warnings
 
-from sklearn.preprocessing._encoders import _transform_selected
-from sklearn.preprocessing.data import Binarizer
 from sklearn.preprocessing import OneHotEncoder
 from sklearn.preprocessing import OrdinalEncoder
 
@@ -177,50 +176,6 @@ def test_one_hot_encoder_force_new_behaviour():
     assert_raises(ValueError, enc.transform, X2)
 
 
-def _check_transform_selected(X, X_expected, dtype, sel):
-    for M in (X, sparse.csr_matrix(X)):
-        Xtr = _transform_selected(M, Binarizer().transform, dtype, sel)
-        assert_array_equal(toarray(Xtr), X_expected)
-
-
-@pytest.mark.parametrize("output_dtype", [np.int32, np.float32, np.float64])
-@pytest.mark.parametrize("input_dtype", [np.int32, np.float32, np.float64])
-def test_transform_selected(output_dtype, input_dtype):
-    X = np.asarray([[3, 2, 1], [0, 1, 1]], dtype=input_dtype)
-
-    X_expected = np.asarray([[1, 2, 1], [0, 1, 1]], dtype=output_dtype)
-    _check_transform_selected(X, X_expected, output_dtype, [0])
-    _check_transform_selected(X, X_expected, output_dtype,
-                              [True, False, False])
-
-    X_expected = np.asarray([[1, 1, 1], [0, 1, 1]], dtype=output_dtype)
-    _check_transform_selected(X, X_expected, output_dtype, [0, 1, 2])
-    _check_transform_selected(X, X_expected, output_dtype, [True, True, True])
-    _check_transform_selected(X, X_expected, output_dtype, "all")
-
-    _check_transform_selected(X, X, output_dtype, [])
-    _check_transform_selected(X, X, output_dtype, [False, False, False])
-
-
-@pytest.mark.parametrize("output_dtype", [np.int32, np.float32, np.float64])
-@pytest.mark.parametrize("input_dtype", [np.int32, np.float32, np.float64])
-def test_transform_selected_copy_arg(output_dtype, input_dtype):
-    # transformer that alters X
-    def _mutating_transformer(X):
-        X[0, 0] = X[0, 0] + 1
-        return X
-
-    original_X = np.asarray([[1, 2], [3, 4]], dtype=input_dtype)
-    expected_Xtr = np.asarray([[2, 2], [3, 4]], dtype=output_dtype)
-
-    X = original_X.copy()
-    Xtr = _transform_selected(X, _mutating_transformer, output_dtype,
-                              copy=True, selected='all')
-
-    assert_array_equal(toarray(X), toarray(original_X))
-    assert_array_equal(toarray(Xtr), expected_Xtr)
-
-
 def _run_one_hot(X, X2, cat):
     # enc = assert_warns(
     #     DeprecationWarning,
@@ -339,10 +294,10 @@ def test_one_hot_encoder_set_params():
 
 
 def check_categorical_onehot(X):
-    enc = OneHotEncoder()
+    enc = OneHotEncoder(categories='auto')
     Xtr1 = enc.fit_transform(X)
 
-    enc = OneHotEncoder(sparse=False)
+    enc = OneHotEncoder(categories='auto', sparse=False)
     Xtr2 = enc.fit_transform(X)
 
     assert_allclose(Xtr1.toarray(), Xtr2)
@@ -351,17 +306,20 @@ def check_categorical_onehot(X):
     return Xtr1.toarray()
 
 
-def test_one_hot_encoder():
-    X = [['abc', 1, 55], ['def', 2, 55]]
-
+@pytest.mark.parametrize("X", [
+    [['def', 1, 55], ['abc', 2, 55]],
+    np.array([[10, 1, 55], [5, 2, 55]]),
+    np.array([['b', 'A', 'cat'], ['a', 'B', 'cat']], dtype=object)
+    ], ids=['mixed', 'numeric', 'object'])
+def test_one_hot_encoder(X):
     Xtr = check_categorical_onehot(np.array(X)[:, [0]])
-    assert_allclose(Xtr, [[1, 0], [0, 1]])
+    assert_allclose(Xtr, [[0, 1], [1, 0]])
 
     Xtr = check_categorical_onehot(np.array(X)[:, [0, 1]])
-    assert_allclose(Xtr, [[1, 0, 1, 0], [0, 1, 0, 1]])
+    assert_allclose(Xtr, [[0, 1, 1, 0], [1, 0, 0, 1]])
 
-    Xtr = OneHotEncoder().fit_transform(X)
-    assert_allclose(Xtr.toarray(), [[1, 0, 1, 0,  1], [0, 1, 0, 1, 1]])
+    Xtr = OneHotEncoder(categories='auto').fit_transform(X)
+    assert_allclose(Xtr.toarray(), [[0, 1, 1, 0,  1], [1, 0, 0, 1, 1]])
 
 
 def test_one_hot_encoder_inverse():
@@ -449,7 +407,8 @@ def test_one_hot_encoder_specified_categories(X, X2, cats, cat_dtype):
     # when specifying categories manually, unknown categories should already
     # raise when fitting
     enc = OneHotEncoder(categories=cats)
-    assert_raises(ValueError, enc.fit, X2)
+    with pytest.raises(ValueError, match="Found unknown categories"):
+        enc.fit(X2)
     enc = OneHotEncoder(categories=cats, handle_unknown='ignore')
     exp = np.array([[1., 0., 0.], [0., 0., 0.]])
     assert_array_equal(enc.fit(X2).transform(X2).toarray(), exp)
@@ -458,10 +417,20 @@ def test_one_hot_encoder_specified_categories(X, X2, cats, cat_dtype):
 def test_one_hot_encoder_unsorted_categories():
     X = np.array([['a', 'b']], dtype=object).T
 
-    # unsorted passed categories raises for now
-    enc = OneHotEncoder(categories=[['c', 'b', 'a']])
-    msg = re.escape('Unsorted categories are not yet supported')
-    assert_raises_regex(ValueError, msg, enc.fit_transform, X)
+    enc = OneHotEncoder(categories=[['b', 'a', 'c']])
+    exp = np.array([[0., 1., 0.],
+                    [1., 0., 0.]])
+    assert_array_equal(enc.fit(X).transform(X).toarray(), exp)
+    assert_array_equal(enc.fit_transform(X).toarray(), exp)
+    assert enc.categories_[0].tolist() == ['b', 'a', 'c']
+    assert np.issubdtype(enc.categories_[0].dtype, np.object_)
+
+    # unsorted passed categories still raise for numerical values
+    X = np.array([[1, 2]]).T
+    enc = OneHotEncoder(categories=[[2, 1, 3]])
+    msg = 'Unsorted categories are not supported'
+    with pytest.raises(ValueError, match=msg):
+        enc.fit_transform(X)
 
 
 def test_one_hot_encoder_specified_categories_mixed_columns():
@@ -487,9 +456,53 @@ def test_one_hot_encoder_pandas():
     assert_allclose(Xtr, [[1, 0, 1, 0], [0, 1, 0, 1]])
 
 
-def test_ordinal_encoder():
-    X = [['abc', 2, 55], ['def', 1, 55]]
+def test_one_hot_encoder_feature_names():
+    enc = OneHotEncoder()
+    X = [['Male', 1, 'girl', 2, 3],
+         ['Female', 41, 'girl', 1, 10],
+         ['Male', 51, 'boy', 12, 3],
+         ['Male', 91, 'girl', 21, 30]]
 
+    enc.fit(X)
+    feature_names = enc.get_feature_names()
+    assert isinstance(feature_names, np.ndarray)
+
+    assert_array_equal(['x0_Female', 'x0_Male',
+                        'x1_1', 'x1_41', 'x1_51', 'x1_91',
+                        'x2_boy', 'x2_girl',
+                        'x3_1', 'x3_2', 'x3_12', 'x3_21',
+                        'x4_3',
+                        'x4_10', 'x4_30'], feature_names)
+
+    feature_names2 = enc.get_feature_names(['one', 'two',
+                                            'three', 'four', 'five'])
+
+    assert_array_equal(['one_Female', 'one_Male',
+                        'two_1', 'two_41', 'two_51', 'two_91',
+                        'three_boy', 'three_girl',
+                        'four_1', 'four_2', 'four_12', 'four_21',
+                        'five_3', 'five_10', 'five_30'], feature_names2)
+
+    with pytest.raises(ValueError, match="input_features should have length"):
+        enc.get_feature_names(['one', 'two'])
+
+
+def test_one_hot_encoder_feature_names_unicode():
+    enc = OneHotEncoder()
+    X = np.array([[u'c❤t1', u'dat2']], dtype=object).T
+    enc.fit(X)
+    feature_names = enc.get_feature_names()
+    assert_array_equal([u'x0_c❤t1', u'x0_dat2'], feature_names)
+    feature_names = enc.get_feature_names(input_features=[u'n👍me'])
+    assert_array_equal([u'n👍me_c❤t1', u'n👍me_dat2'], feature_names)
+
+
+@pytest.mark.parametrize("X", [
+    [['abc', 2, 55], ['def', 1, 55]],
+    np.array([[10, 2, 55], [20, 1, 55]]),
+    np.array([['a', 'B', 'cat'], ['b', 'A', 'cat']], dtype=object)
+    ], ids=['mixed', 'numeric', 'object'])
+def test_ordinal_encoder(X):
     enc = OrdinalEncoder()
     exp = np.array([[0, 1, 0],
                     [1, 0, 0]], dtype='int64')
diff --git a/sklearn/preprocessing/tests/test_function_transformer.py b/sklearn/preprocessing/tests/test_function_transformer.py
index 4d166457777c..464581e5e9c2 100644
--- a/sklearn/preprocessing/tests/test_function_transformer.py
+++ b/sklearn/preprocessing/tests/test_function_transformer.py
@@ -1,3 +1,4 @@
+import pytest
 import numpy as np
 from scipy import sparse
 
@@ -5,6 +6,7 @@
 from sklearn.utils.testing import (assert_equal, assert_array_equal,
                                    assert_allclose_dense_sparse)
 from sklearn.utils.testing import assert_warns_message, assert_no_warnings
+from sklearn.utils.testing import ignore_warnings
 
 
 def _make_func(args_store, kwargs_store, func=lambda X, *a, **k: X):
@@ -24,7 +26,8 @@ def test_delegate_to_func():
     kwargs_store = {}
     X = np.arange(10).reshape((5, 2))
     assert_array_equal(
-        FunctionTransformer(_make_func(args_store, kwargs_store)).transform(X),
+        FunctionTransformer(_make_func(args_store, kwargs_store),
+                            validate=False).transform(X),
         X, 'transform should have returned X unchanged',
     )
 
@@ -52,7 +55,7 @@ def test_delegate_to_func():
         DeprecationWarning, "pass_y is deprecated",
         FunctionTransformer(
             _make_func(args_store, kwargs_store),
-            pass_y=True).transform, X, y)
+            pass_y=True, validate=False).transform, X, y)
 
     assert_array_equal(transformed, X,
                        err_msg='transform should have returned X unchanged')
@@ -74,6 +77,8 @@ def test_delegate_to_func():
     )
 
 
+@ignore_warnings(category=FutureWarning)
+# ignore warning for validate=False 0.22
 def test_np_log():
     X = np.arange(10).reshape((5, 2))
 
@@ -84,6 +89,8 @@ def test_np_log():
     )
 
 
+@ignore_warnings(category=FutureWarning)
+# ignore warning for validate=False 0.22
 def test_kw_arg():
     X = np.linspace(0, 1, num=10).reshape((5, 2))
 
@@ -94,6 +101,8 @@ def test_kw_arg():
                        np.around(X, decimals=3))
 
 
+@ignore_warnings(category=FutureWarning)
+# ignore warning for validate=False 0.22
 def test_kw_arg_update():
     X = np.linspace(0, 1, num=10).reshape((5, 2))
 
@@ -105,6 +114,8 @@ def test_kw_arg_update():
     assert_array_equal(F.transform(X), np.around(X, decimals=1))
 
 
+@ignore_warnings(category=FutureWarning)
+# ignore warning for validate=False 0.22
 def test_kw_arg_reset():
     X = np.linspace(0, 1, num=10).reshape((5, 2))
 
@@ -116,6 +127,8 @@ def test_kw_arg_reset():
     assert_array_equal(F.transform(X), np.around(X, decimals=1))
 
 
+@ignore_warnings(category=FutureWarning)
+# ignore warning for validate=False 0.22
 def test_inverse_transform():
     X = np.array([1, 4, 9, 16]).reshape((2, 2))
 
@@ -130,6 +143,8 @@ def test_inverse_transform():
     )
 
 
+@ignore_warnings(category=FutureWarning)
+# ignore warning for validate=False 0.22
 def test_check_inverse():
     X_dense = np.array([1, 4, 9, 16], dtype=np.float64).reshape((2, 2))
 
@@ -145,7 +160,8 @@ def test_check_inverse():
         trans = FunctionTransformer(func=np.sqrt,
                                     inverse_func=np.around,
                                     accept_sparse=accept_sparse,
-                                    check_inverse=True)
+                                    check_inverse=True,
+                                    validate=True)
         assert_warns_message(UserWarning,
                              "The provided functions are not strictly"
                              " inverse of each other. If you are sure you"
@@ -156,15 +172,38 @@ def test_check_inverse():
         trans = FunctionTransformer(func=np.expm1,
                                     inverse_func=np.log1p,
                                     accept_sparse=accept_sparse,
-                                    check_inverse=True)
+                                    check_inverse=True,
+                                    validate=True)
         Xt = assert_no_warnings(trans.fit_transform, X)
         assert_allclose_dense_sparse(X, trans.inverse_transform(Xt))
 
     # check that we don't check inverse when one of the func or inverse is not
     # provided.
     trans = FunctionTransformer(func=np.expm1, inverse_func=None,
-                                check_inverse=True)
+                                check_inverse=True, validate=True)
     assert_no_warnings(trans.fit, X_dense)
     trans = FunctionTransformer(func=None, inverse_func=np.expm1,
-                                check_inverse=True)
+                                check_inverse=True, validate=True)
     assert_no_warnings(trans.fit, X_dense)
+
+
+@pytest.mark.parametrize("validate, expected_warning",
+                         [(None, FutureWarning),
+                          (True, None),
+                          (False, None)])
+def test_function_transformer_future_warning(validate, expected_warning):
+    # FIXME: to be removed in 0.22
+    X = np.random.randn(100, 10)
+    transformer = FunctionTransformer(validate=validate)
+    with pytest.warns(expected_warning) as results:
+        transformer.fit_transform(X)
+    if expected_warning is None:
+        assert len(results) == 0
+
+
+def test_function_transformer_frame():
+    pd = pytest.importorskip('pandas')
+    X_df = pd.DataFrame(np.random.randn(100, 10))
+    transformer = FunctionTransformer(validate=False)
+    X_df_trans = transformer.fit_transform(X_df)
+    assert hasattr(X_df_trans, 'loc')
diff --git a/sklearn/preprocessing/tests/test_label.py b/sklearn/preprocessing/tests/test_label.py
index faa0cc3ce275..f8f4ee4870ac 100644
--- a/sklearn/preprocessing/tests/test_label.py
+++ b/sklearn/preprocessing/tests/test_label.py
@@ -1,5 +1,7 @@
 import numpy as np
 
+import pytest
+
 from scipy.sparse import issparse
 from scipy.sparse import coo_matrix
 from scipy.sparse import csc_matrix
@@ -24,6 +26,7 @@
 
 from sklearn.preprocessing.label import _inverse_binarize_thresholding
 from sklearn.preprocessing.label import _inverse_binarize_multiclass
+from sklearn.preprocessing.label import _encode
 
 from sklearn import datasets
 
@@ -169,8 +172,33 @@ def test_label_binarizer_errors():
                   [1, 2, 3])
 
 
-def test_label_encoder():
-    # Test LabelEncoder's transform and inverse_transform methods
+@pytest.mark.parametrize(
+        "values, classes, unknown",
+        [(np.array([2, 1, 3, 1, 3], dtype='int64'),
+          np.array([1, 2, 3], dtype='int64'), np.array([4], dtype='int64')),
+         (np.array(['b', 'a', 'c', 'a', 'c'], dtype=object),
+          np.array(['a', 'b', 'c'], dtype=object),
+          np.array(['d'], dtype=object)),
+         (np.array(['b', 'a', 'c', 'a', 'c']),
+          np.array(['a', 'b', 'c']), np.array(['d']))],
+        ids=['int64', 'object', 'str'])
+def test_label_encoder(values, classes, unknown):
+    # Test LabelEncoder's transform, fit_transform and
+    # inverse_transform methods
+    le = LabelEncoder()
+    le.fit(values)
+    assert_array_equal(le.classes_, classes)
+    assert_array_equal(le.transform(values), [1, 0, 2, 0, 2])
+    assert_array_equal(le.inverse_transform([1, 0, 2, 0, 2]), values)
+    le = LabelEncoder()
+    ret = le.fit_transform(values)
+    assert_array_equal(ret, [1, 0, 2, 0, 2])
+
+    with pytest.raises(ValueError, match="unseen labels"):
+        le.transform(unknown)
+
+
+def test_label_encoder_negative_ints():
     le = LabelEncoder()
     le.fit([1, 1, 4, 5, -1, 0])
     assert_array_equal(le.classes_, [-1, 0, 1, 4, 5])
@@ -180,20 +208,13 @@ def test_label_encoder():
                        [0, 1, 4, 4, 5, -1, -1])
     assert_raises(ValueError, le.transform, [0, 6])
 
-    le.fit(["apple", "orange"])
-    msg = "bad input shape"
-    assert_raise_message(ValueError, msg, le.transform, "apple")
-
-
-def test_label_encoder_fit_transform():
-    # Test fit_transform
-    le = LabelEncoder()
-    ret = le.fit_transform([1, 1, 4, 5, -1, 0])
-    assert_array_equal(ret, [2, 2, 3, 4, 0, 1])
 
+@pytest.mark.parametrize("dtype", ['str', 'object'])
+def test_label_encoder_str_bad_shape(dtype):
     le = LabelEncoder()
-    ret = le.fit_transform(["paris", "paris", "tokyo", "amsterdam"])
-    assert_array_equal(ret, [1, 1, 2, 0])
+    le.fit(np.array(["apple", "orange"], dtype=dtype))
+    msg = "bad input shape"
+    assert_raise_message(ValueError, msg, le.transform, "apple")
 
 
 def test_label_encoder_errors():
@@ -214,9 +235,15 @@ def test_label_encoder_errors():
     assert_raise_message(ValueError, msg, le.inverse_transform, "")
 
 
-def test_label_encoder_empty_array():
+@pytest.mark.parametrize(
+        "values",
+        [np.array([2, 1, 3, 1, 3], dtype='int64'),
+         np.array(['b', 'a', 'c', 'a', 'c'], dtype=object),
+         np.array(['b', 'a', 'c', 'a', 'c'])],
+        ids=['int64', 'object', 'str'])
+def test_label_encoder_empty_array(values):
     le = LabelEncoder()
-    le.fit(np.array(["1", "2", "1", "2", "2"]))
+    le.fit(values)
     # test empty transform
     transformed = le.transform([])
     assert_array_equal(np.array([]), transformed)
@@ -536,3 +563,22 @@ def test_inverse_binarize_multiclass():
                                                    [0, 0, 0]]),
                                        np.arange(3))
     assert_array_equal(got, np.array([1, 1, 0]))
+
+
+@pytest.mark.parametrize(
+        "values, expected",
+        [(np.array([2, 1, 3, 1, 3], dtype='int64'),
+          np.array([1, 2, 3], dtype='int64')),
+         (np.array(['b', 'a', 'c', 'a', 'c'], dtype=object),
+          np.array(['a', 'b', 'c'], dtype=object)),
+         (np.array(['b', 'a', 'c', 'a', 'c']),
+          np.array(['a', 'b', 'c']))],
+        ids=['int64', 'object', 'str'])
+def test_encode_util(values, expected):
+    uniques = _encode(values)
+    assert_array_equal(uniques, expected)
+    uniques, encoded = _encode(values, encode=True)
+    assert_array_equal(uniques, expected)
+    assert_array_equal(encoded, np.array([1, 0, 2, 0, 2]))
+    _, encoded = _encode(values, uniques, encode=True)
+    assert_array_equal(encoded, np.array([1, 0, 2, 0, 2]))
diff --git a/sklearn/random_projection.py b/sklearn/random_projection.py
index daa3c0243d89..4a2edada4c1d 100644
--- a/sklearn/random_projection.py
+++ b/sklearn/random_projection.py
@@ -150,7 +150,7 @@ def _check_input_size(n_components, n_features):
                          n_components)
     if n_features <= 0:
         raise ValueError("n_features must be strictly positive, got %d" %
-                         n_components)
+                         n_features)
 
 
 def gaussian_random_matrix(n_components, n_features, random_state=None):
@@ -465,6 +465,16 @@ class GaussianRandomProjection(BaseRandomProjection):
     components_ : numpy array of shape [n_components, n_features]
         Random matrix used for the projection.
 
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.random_projection import GaussianRandomProjection
+    >>> X = np.random.rand(100, 10000)
+    >>> transformer = GaussianRandomProjection()
+    >>> X_new = transformer.fit_transform(X)
+    >>> X_new.shape
+    (100, 3947)
+
     See Also
     --------
     SparseRandomProjection
@@ -577,6 +587,20 @@ class SparseRandomProjection(BaseRandomProjection):
     density_ : float in range 0.0 - 1.0
         Concrete density computed from when density = "auto".
 
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.random_projection import SparseRandomProjection
+    >>> np.random.seed(42)
+    >>> X = np.random.rand(100, 10000)
+    >>> transformer = SparseRandomProjection()
+    >>> X_new = transformer.fit_transform(X)
+    >>> X_new.shape
+    (100, 3947)
+    >>> # very few components are non-zero
+    >>> np.mean(transformer.components_ != 0) # doctest: +ELLIPSIS
+    0.0100...
+
     See Also
     --------
     GaussianRandomProjection
diff --git a/sklearn/semi_supervised/label_propagation.py b/sklearn/semi_supervised/label_propagation.py
index 778686f53da3..ff32005399fe 100644
--- a/sklearn/semi_supervised/label_propagation.py
+++ b/sklearn/semi_supervised/label_propagation.py
@@ -101,13 +101,15 @@ class BaseLabelPropagation(six.with_metaclass(ABCMeta, BaseEstimator,
         Convergence tolerance: threshold to consider the system at steady
         state
 
-    n_jobs : int, optional (default = 1)
+   n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
     """
 
     def __init__(self, kernel='rbf', gamma=20, n_neighbors=7,
-                 alpha=1, max_iter=30, tol=1e-3, n_jobs=1):
+                 alpha=1, max_iter=30, tol=1e-3, n_jobs=None):
 
         self.max_iter = max_iter
         self.tol = tol
@@ -334,9 +336,11 @@ class LabelPropagation(BaseLabelPropagation):
         Convergence tolerance: threshold to consider the system at steady
         state
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -384,7 +388,7 @@ class LabelPropagation(BaseLabelPropagation):
     _variant = 'propagation'
 
     def __init__(self, kernel='rbf', gamma=20, n_neighbors=7,
-                 alpha=None, max_iter=1000, tol=1e-3, n_jobs=1):
+                 alpha=None, max_iter=1000, tol=1e-3, n_jobs=None):
         super(LabelPropagation, self).__init__(
             kernel=kernel, gamma=gamma, n_neighbors=n_neighbors, alpha=alpha,
             max_iter=max_iter, tol=tol, n_jobs=n_jobs)
@@ -452,9 +456,11 @@ class LabelSpreading(BaseLabelPropagation):
       Convergence tolerance: threshold to consider the system at steady
       state
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -502,7 +508,7 @@ class LabelSpreading(BaseLabelPropagation):
     _variant = 'spreading'
 
     def __init__(self, kernel='rbf', gamma=20, n_neighbors=7, alpha=0.2,
-                 max_iter=30, tol=1e-3, n_jobs=1):
+                 max_iter=30, tol=1e-3, n_jobs=None):
 
         # this one has different base parameters
         super(LabelSpreading, self).__init__(kernel=kernel, gamma=gamma,
diff --git a/sklearn/semi_supervised/tests/test_label_propagation.py b/sklearn/semi_supervised/tests/test_label_propagation.py
index 8cd0cce41d7e..51b725030cb6 100644
--- a/sklearn/semi_supervised/tests/test_label_propagation.py
+++ b/sklearn/semi_supervised/tests/test_label_propagation.py
@@ -113,8 +113,10 @@ def test_label_propagation_closed_form():
     clf.fit(X, y)
     # adopting notation from Zhu et al 2002
     T_bar = clf._build_graph()
-    Tuu = T_bar[np.meshgrid(unlabelled_idx, unlabelled_idx, indexing='ij')]
-    Tul = T_bar[np.meshgrid(unlabelled_idx, labelled_idx, indexing='ij')]
+    Tuu = T_bar[tuple(np.meshgrid(unlabelled_idx, unlabelled_idx,
+                      indexing='ij'))]
+    Tul = T_bar[tuple(np.meshgrid(unlabelled_idx, labelled_idx,
+                                  indexing='ij'))]
     Y = Y[:, :-1]
     Y_l = Y[labelled_idx, :]
     Y_u = np.dot(np.dot(np.linalg.inv(np.eye(Tuu.shape[0]) - Tuu), Tul), Y_l)
diff --git a/sklearn/svm/bounds.py b/sklearn/svm/bounds.py
index f477ded6e627..d745212fcd0c 100644
--- a/sklearn/svm/bounds.py
+++ b/sklearn/svm/bounds.py
@@ -51,8 +51,8 @@ def l1_min_c(X, y, loss='squared_hinge', fit_intercept=True,
     l1_min_c : float
         minimum value for C
     """
-    if loss not in ('squared_hinge', 'log', 'l2'):
-        raise ValueError('loss type not in ("squared_hinge", "log", "l2")')
+    if loss not in ('squared_hinge', 'log'):
+        raise ValueError('loss type not in ("squared_hinge", "log")')
 
     X = check_array(X, accept_sparse='csc')
     check_consistent_length(X, y)
@@ -61,7 +61,8 @@ def l1_min_c(X, y, loss='squared_hinge', fit_intercept=True,
     # maximum absolute value over classes and features
     den = np.max(np.abs(safe_sparse_dot(Y, X)))
     if fit_intercept:
-        bias = intercept_scaling * np.ones((np.size(y), 1))
+        bias = np.full((np.size(y), 1), intercept_scaling,
+                       dtype=np.array(intercept_scaling).dtype)
         den = max(den, abs(np.dot(Y, bias)).max())
 
     if den == 0.0:
diff --git a/sklearn/svm/classes.py b/sklearn/svm/classes.py
index 7b34a40f25b9..4bb423e79048 100644
--- a/sklearn/svm/classes.py
+++ b/sklearn/svm/classes.py
@@ -116,16 +116,15 @@ class LinearSVC(BaseEstimator, LinearClassifierMixin,
     >>> from sklearn.svm import LinearSVC
     >>> from sklearn.datasets import make_classification
     >>> X, y = make_classification(n_features=4, random_state=0)
-    >>> clf = LinearSVC(random_state=0)
+    >>> clf = LinearSVC(random_state=0, tol=1e-5)
     >>> clf.fit(X, y)
     LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
-         multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
-         verbose=0)
+         multi_class='ovr', penalty='l2', random_state=0, tol=1e-05, verbose=0)
     >>> print(clf.coef_)
-    [[0.08551385 0.39414796 0.49847831 0.37513797]]
+    [[0.085... 0.394... 0.498... 0.375...]]
     >>> print(clf.intercept_)
-    [0.28418066]
+    [0.284...]
     >>> print(clf.predict([[0, 0, 0, 0]]))
     [1]
 
@@ -260,26 +259,23 @@ class LinearSVR(LinearModel, RegressorMixin):
 
     Parameters
     ----------
-    C : float, optional (default=1.0)
-        Penalty parameter C of the error term. The penalty is a squared
-        l2 penalty. The bigger this parameter, the less regularization is used.
-
-    loss : string, 'epsilon_insensitive' or 'squared_epsilon_insensitive' (default='epsilon_insensitive')
-        Specifies the loss function. 'l1' is the epsilon-insensitive loss
-        (standard SVR) while 'l2' is the squared epsilon-insensitive loss.
-
     epsilon : float, optional (default=0.1)
         Epsilon parameter in the epsilon-insensitive loss function. Note
         that the value of this parameter depends on the scale of the target
         variable y. If unsure, set ``epsilon=0``.
 
-    dual : bool, (default=True)
-        Select the algorithm to either solve the dual or primal
-        optimization problem. Prefer dual=False when n_samples > n_features.
-
     tol : float, optional (default=1e-4)
         Tolerance for stopping criteria.
 
+    C : float, optional (default=1.0)
+        Penalty parameter C of the error term. The penalty is a squared
+        l2 penalty. The bigger this parameter, the less regularization is used.
+
+    loss : string, optional (default='epsilon_insensitive')
+        Specifies the loss function. The epsilon-insensitive loss
+        (standard SVR) is the L1 loss, while the squared epsilon-insensitive
+        loss ('squared_epsilon_insensitive') is the L2 loss.
+
     fit_intercept : boolean, optional (default=True)
         Whether to calculate the intercept for this model. If set
         to false, no intercept will be used in calculations
@@ -296,6 +292,10 @@ class LinearSVR(LinearModel, RegressorMixin):
         To lessen the effect of regularization on synthetic feature weight
         (and therefore on the intercept) intercept_scaling has to be increased.
 
+    dual : bool, (default=True)
+        Select the algorithm to either solve the dual or primal
+        optimization problem. Prefer dual=False when n_samples > n_features.
+
     verbose : int, (default=0)
         Enable verbose output. Note that this setting takes advantage of a
         per-process runtime setting in liblinear that, if enabled, may not work
@@ -328,17 +328,17 @@ class LinearSVR(LinearModel, RegressorMixin):
     >>> from sklearn.svm import LinearSVR
     >>> from sklearn.datasets import make_regression
     >>> X, y = make_regression(n_features=4, random_state=0)
-    >>> regr = LinearSVR(random_state=0)
+    >>> regr = LinearSVR(random_state=0, tol=1e-5)
     >>> regr.fit(X, y)
     LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,
          intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,
-         random_state=0, tol=0.0001, verbose=0)
+         random_state=0, tol=1e-05, verbose=0)
     >>> print(regr.coef_)
-    [16.35750999 26.91499923 42.30652207 60.47843124]
+    [16.35... 26.91... 42.30... 60.47...]
     >>> print(regr.intercept_)
-    [-4.29756543]
+    [-4.29...]
     >>> print(regr.predict([[0, 0, 0, 0]]))
-    [-4.29756543]
+    [-4.29...]
 
     See also
     --------
@@ -473,13 +473,13 @@ class SVC(BaseSVC):
         Independent term in kernel function.
         It is only significant in 'poly' and 'sigmoid'.
 
+    shrinking : boolean, optional (default=True)
+        Whether to use the shrinking heuristic.
+
     probability : boolean, optional (default=False)
         Whether to enable probability estimates. This must be enabled prior
         to calling `fit`, and will slow down that method.
 
-    shrinking : boolean, optional (default=True)
-        Whether to use the shrinking heuristic.
-
     tol : float, optional (default=1e-3)
         Tolerance for stopping criterion.
 
@@ -543,7 +543,7 @@ class SVC(BaseSVC):
         non-trivial. See the section about multi-class classification in the
         SVM section of the User Guide for details.
 
-    coef_ : array, shape = [n_class-1, n_features]
+    coef_ : array, shape = [n_class * (n_class-1) / 2, n_features]
         Weights assigned to the features (coefficients in the primal
         problem). This is only available in the case of a linear kernel.
 
@@ -639,13 +639,13 @@ class NuSVC(BaseSVC):
         Independent term in kernel function.
         It is only significant in 'poly' and 'sigmoid'.
 
+    shrinking : boolean, optional (default=True)
+        Whether to use the shrinking heuristic.
+
     probability : boolean, optional (default=False)
         Whether to enable probability estimates. This must be enabled prior
         to calling `fit`, and will slow down that method.
 
-    shrinking : boolean, optional (default=True)
-        Whether to use the shrinking heuristic.
-
     tol : float, optional (default=1e-3)
         Tolerance for stopping criterion.
 
@@ -707,7 +707,7 @@ class NuSVC(BaseSVC):
         non-trivial. See the section about multi-class classification in
         the SVM section of the User Guide for details.
 
-    coef_ : array, shape = [n_class-1, n_features]
+    coef_ : array, shape = [n_class * (n_class-1) / 2, n_features]
         Weights assigned to the features (coefficients in the primal
         problem). This is only available in the case of a linear kernel.
 
@@ -769,15 +769,6 @@ class SVR(BaseLibSVM, RegressorMixin):
 
     Parameters
     ----------
-    C : float, optional (default=1.0)
-        Penalty parameter C of the error term.
-
-    epsilon : float, optional (default=0.1)
-         Epsilon in the epsilon-SVR model. It specifies the epsilon-tube
-         within which no penalty is associated in the training loss function
-         with points predicted within a distance epsilon from the actual
-         value.
-
     kernel : string, optional (default='rbf')
          Specifies the kernel type to be used in the algorithm.
          It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or
@@ -803,12 +794,21 @@ class SVR(BaseLibSVM, RegressorMixin):
         Independent term in kernel function.
         It is only significant in 'poly' and 'sigmoid'.
 
-    shrinking : boolean, optional (default=True)
-        Whether to use the shrinking heuristic.
-
     tol : float, optional (default=1e-3)
         Tolerance for stopping criterion.
 
+    C : float, optional (default=1.0)
+        Penalty parameter C of the error term.
+
+    epsilon : float, optional (default=0.1)
+         Epsilon in the epsilon-SVR model. It specifies the epsilon-tube
+         within which no penalty is associated in the training loss function
+         with points predicted within a distance epsilon from the actual
+         value.
+
+    shrinking : boolean, optional (default=True)
+        Whether to use the shrinking heuristic.
+
     cache_size : float, optional
         Specify the size of the kernel cache (in MB).
 
@@ -894,14 +894,14 @@ class NuSVR(BaseLibSVM, RegressorMixin):
 
     Parameters
     ----------
-    C : float, optional (default=1.0)
-        Penalty parameter C of the error term.
-
     nu : float, optional
         An upper bound on the fraction of training errors and a lower bound of
         the fraction of support vectors. Should be in the interval (0, 1].  By
         default 0.5 will be taken.
 
+    C : float, optional (default=1.0)
+        Penalty parameter C of the error term.
+
     kernel : string, optional (default='rbf')
          Specifies the kernel type to be used in the algorithm.
          It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or
@@ -1009,7 +1009,7 @@ class OneClassSVM(BaseLibSVM, OutlierMixin):
 
     The implementation is based on libsvm.
 
-    Read more in the :ref:`User Guide <svm_outlier_detection>`.
+    Read more in the :ref:`User Guide <outlier_detection>`.
 
     Parameters
     ----------
@@ -1020,12 +1020,6 @@ class OneClassSVM(BaseLibSVM, OutlierMixin):
          If none is given, 'rbf' will be used. If a callable is given it is
          used to precompute the kernel matrix.
 
-    nu : float, optional
-        An upper bound on the fraction of training
-        errors and a lower bound of the fraction of support
-        vectors. Should be in the interval (0, 1]. By default 0.5
-        will be taken.
-
     degree : int, optional (default=3)
         Degree of the polynomial kernel function ('poly').
         Ignored by all other kernels.
@@ -1047,6 +1041,12 @@ class OneClassSVM(BaseLibSVM, OutlierMixin):
     tol : float, optional
         Tolerance for stopping criterion.
 
+    nu : float, optional
+        An upper bound on the fraction of training
+        errors and a lower bound of the fraction of support
+        vectors. Should be in the interval (0, 1]. By default 0.5
+        will be taken.
+
     shrinking : boolean, optional
         Whether to use the shrinking heuristic.
 
diff --git a/sklearn/svm/tests/test_svm.py b/sklearn/svm/tests/test_svm.py
index ead2d1cd27fd..6187a08f7b75 100644
--- a/sklearn/svm/tests/test_svm.py
+++ b/sklearn/svm/tests/test_svm.py
@@ -22,7 +22,7 @@
 from sklearn.utils.testing import ignore_warnings, assert_raises
 from sklearn.utils.testing import assert_no_warnings
 from sklearn.exceptions import ConvergenceWarning
-from sklearn.exceptions import NotFittedError
+from sklearn.exceptions import NotFittedError, UndefinedMetricWarning
 from sklearn.multiclass import OneVsRestClassifier
 from sklearn.externals import six
 
@@ -283,7 +283,7 @@ def test_oneclass_decision_function():
 
 def test_oneclass_score_samples():
     X_train = [[1, 1], [1, 2], [2, 1]]
-    clf = svm.OneClassSVM().fit(X_train)
+    clf = svm.OneClassSVM(gamma=1).fit(X_train)
     assert_array_equal(clf.score_samples([[2., 2.]]),
                        clf.decision_function([[2., 2.]]) + clf.offset_)
 
@@ -442,13 +442,15 @@ def test_sample_weights():
     assert_array_almost_equal(dual_coef_no_weight, clf.dual_coef_)
 
 
+@ignore_warnings(category=UndefinedMetricWarning)
 def test_auto_weight():
     # Test class weights for imbalanced data
     from sklearn.linear_model import LogisticRegression
     # We take as dataset the two-dimensional projection of iris so
     # that it is not separable and remove half of predictors from
     # class 1.
-    # We add one to the targets as a non-regression test: class_weight="balanced"
+    # We add one to the targets as a non-regression test:
+    # class_weight="balanced"
     # used to work only when the labels where a range [0..K).
     from sklearn.utils import compute_class_weight
     X, y = iris.data[:, :2], iris.target + 1
diff --git a/sklearn/tests/test_calibration.py b/sklearn/tests/test_calibration.py
index 404c3a797c1d..e454633a3a29 100644
--- a/sklearn/tests/test_calibration.py
+++ b/sklearn/tests/test_calibration.py
@@ -2,6 +2,7 @@
 # License: BSD 3 clause
 
 from __future__ import division
+import pytest
 import numpy as np
 from scipy import sparse
 from sklearn.model_selection import LeaveOneOut
@@ -24,7 +25,8 @@
 from sklearn.calibration import calibration_curve
 
 
-@ignore_warnings
+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_calibration():
     """Test calibration objects with isotonic and sigmoid"""
     n_samples = 100
@@ -100,6 +102,7 @@ def test_calibration():
         assert_raises(RuntimeError, clf_base_regressor.fit, X_train, y_train)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_sample_weight():
     n_samples = 100
     X, y = make_classification(n_samples=2 * n_samples, n_features=6,
diff --git a/sklearn/tests/test_common.py b/sklearn/tests/test_common.py
index 1f4c41ec8285..8e5f020985b1 100644
--- a/sklearn/tests/test_common.py
+++ b/sklearn/tests/test_common.py
@@ -28,6 +28,7 @@
 from sklearn.cluster.bicluster import BiclusterMixin
 
 from sklearn.linear_model.base import LinearClassifierMixin
+from sklearn.utils import IS_PYPY
 from sklearn.utils.estimator_checks import (
     _yield_all_checks,
     set_checking_parameters,
@@ -110,6 +111,8 @@ def test_no_attributes_set_in_init(name, Estimator):
         check_no_attributes_set_in_init(name, estimator)
 
 
+@ignore_warnings(category=DeprecationWarning)
+# ignore deprecated open(.., 'U') in numpy distutils
 def test_configure():
     # Smoke test the 'configure' step of setup, this tests all the
     # 'configure' functions in the setup.pys in scikit-learn
@@ -161,6 +164,9 @@ def test_import_all_consistency():
     for modname in submods + ['sklearn']:
         if ".tests." in modname:
             continue
+        if IS_PYPY and ('_svmlight_format' in modname or
+                        'feature_extraction._hashing' in modname):
+            continue
         package = __import__(modname, fromlist="dummy")
         for name in getattr(package, '__all__', ()):
             if getattr(package, name, None) is None:
diff --git a/sklearn/tests/test_discriminant_analysis.py b/sklearn/tests/test_discriminant_analysis.py
index 8eb5da1908ba..6e509949b0a8 100644
--- a/sklearn/tests/test_discriminant_analysis.py
+++ b/sklearn/tests/test_discriminant_analysis.py
@@ -324,9 +324,9 @@ def test_qda_deprecation():
                          "removed in 0.21.", clf.fit, X, y)
 
     # check that covariance_ (and covariances_ with warning) is stored
-    assert_warns_message(DeprecationWarning, "Attribute covariances_ was "
+    assert_warns_message(DeprecationWarning, "Attribute ``covariances_`` was "
                          "deprecated in version 0.19 and will be removed "
-                         "in 0.21. Use covariance_ instead", getattr, clf,
+                         "in 0.21. Use ``covariance_`` instead", getattr, clf,
                          'covariances_')
 
 
diff --git a/sklearn/tests/test_docstring_parameters.py b/sklearn/tests/test_docstring_parameters.py
index cb7217e3ef04..648de6b6e6ca 100644
--- a/sklearn/tests/test_docstring_parameters.py
+++ b/sklearn/tests/test_docstring_parameters.py
@@ -12,6 +12,7 @@
 
 import sklearn
 from sklearn.base import signature
+from sklearn.utils import IS_PYPY
 from sklearn.utils.testing import SkipTest
 from sklearn.utils.testing import check_docstring_parameters
 from sklearn.utils.testing import _get_func_name
@@ -22,36 +23,6 @@
                                                         path=sklearn.__path__)
                       if not ("._" in pckg[1] or ".tests." in pckg[1])])
 
-# TODO Uncomment all modules and fix doc inconsistencies everywhere
-# The list of modules that are not tested for now
-IGNORED_MODULES = (
-    'cross_decomposition',
-    'covariance',
-    'cluster',
-    'datasets',
-    'decomposition',
-    'feature_extraction',
-    'gaussian_process',
-    'linear_model',
-    'manifold',
-    'metrics',
-    'discriminant_analysis',
-    'ensemble',
-    'feature_selection',
-    'kernel_approximation',
-    'model_selection',
-    'multioutput',
-    'random_projection',
-    'setup',
-    'svm',
-    'utils',
-    'neighbors',
-    # Deprecated modules
-    'cross_validation',
-    'grid_search',
-    'learning_curve',
-)
-
 
 # functions to ignore args / docstring of
 _DOCSTRING_IGNORES = [
@@ -72,6 +43,9 @@
 ]
 
 
+# numpydoc 0.8.0's docscrape tool raises because of collections.abc under
+# Python 3.7
+@ignore_warnings(category=DeprecationWarning)
 def test_docstring_parameters():
     # Test module docstring formatting
 
@@ -87,8 +61,6 @@ def test_docstring_parameters():
 
     incorrect = []
     for name in PUBLIC_MODULES:
-        if name.startswith('_') or name.split(".")[1] in IGNORED_MODULES:
-            continue
         with warnings.catch_warnings(record=True):
             module = importlib.import_module(name)
         classes = inspect.getmembers(module, inspect.isclass)
@@ -155,6 +127,11 @@ def test_tabs():
     # Test that there are no tabs in our source files
     for importer, modname, ispkg in walk_packages(sklearn.__path__,
                                                   prefix='sklearn.'):
+
+        if IS_PYPY and ('_svmlight_format' in modname or
+                        'feature_extraction._hashing' in modname):
+            continue
+
         # because we don't import
         mod = importlib.import_module(modname)
         try:
diff --git a/sklearn/tests/test_impute.py b/sklearn/tests/test_impute.py
index f5c42f744348..f25f76e611d7 100644
--- a/sklearn/tests/test_impute.py
+++ b/sklearn/tests/test_impute.py
@@ -1,5 +1,3 @@
-from __future__ import division
-
 import pytest
 
 import numpy as np
@@ -13,9 +11,8 @@
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_false
 
-from sklearn.impute import SimpleImputer, ChainedImputer
-from sklearn.dummy import DummyRegressor
-from sklearn.linear_model import BayesianRidge, ARDRegression
+from sklearn.impute import MissingIndicator
+from sklearn.impute import SimpleImputer
 from sklearn.pipeline import Pipeline
 from sklearn.model_selection import GridSearchCV
 from sklearn import tree
@@ -72,10 +69,6 @@ def test_imputation_shape():
         X_imputed = imputer.fit_transform(X)
         assert X_imputed.shape == (10, 2)
 
-        chained_imputer = ChainedImputer(initial_strategy=strategy)
-        X_imputed = chained_imputer.fit_transform(X)
-        assert X_imputed.shape == (10, 2)
-
 
 @pytest.mark.parametrize("strategy", ["const", 101, None])
 def test_imputation_error_invalid_strategy(strategy):
@@ -97,6 +90,23 @@ def test_imputation_deletion_warning(strategy):
         imputer.fit_transform(X)
 
 
+@pytest.mark.parametrize("strategy", ["mean", "median",
+                                      "most_frequent", "constant"])
+def test_imputation_error_sparse_0(strategy):
+    # check that error are raised when missing_values = 0 and input is sparse
+    X = np.ones((3, 5))
+    X[0] = 0
+    X = sparse.csc_matrix(X)
+
+    imputer = SimpleImputer(strategy=strategy, missing_values=0)
+    with pytest.raises(ValueError, match="Provide a dense array"):
+        imputer.fit(X)
+
+    imputer.fit(X.toarray())
+    with pytest.raises(ValueError, match="Provide a dense array"):
+        imputer.transform(X)
+
+
 def safe_median(arr, *args, **kwargs):
     # np.median([]) raises a TypeError for numpy >= 1.10.1
     length = arr.size if hasattr(arr, 'size') else len(arr)
@@ -123,10 +133,8 @@ def test_imputation_mean_median():
     values[4::2] = - values[4::2]
 
     tests = [("mean", np.nan, lambda z, v, p: safe_mean(np.hstack((z, v)))),
-             ("mean", 0, lambda z, v, p: np.mean(v)),
              ("median", np.nan,
-              lambda z, v, p: safe_median(np.hstack((z, v)))),
-             ("median", 0, lambda z, v, p: np.median(v))]
+              lambda z, v, p: safe_median(np.hstack((z, v))))]
 
     for strategy, test_missing_values, true_value_fun in tests:
         X = np.empty(shape)
@@ -315,7 +323,7 @@ def test_imputation_most_frequent_pandas(dtype):
 @pytest.mark.parametrize("X_data, missing_value", [(1, 0), (1., np.nan)])
 def test_imputation_constant_error_invalid_type(X_data, missing_value):
     # Verify that exceptions are raised on invalid fill_value type
-    X = np.full((3, 5), X_data)
+    X = np.full((3, 5), X_data, dtype=float)
     X[0, 0] = missing_value
 
     with pytest.raises(ValueError, match="imputing numerical"):
@@ -425,16 +433,22 @@ def test_imputation_constant_pandas(dtype):
     assert_array_equal(X_trans, X_true)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_imputation_pipeline_grid_search():
     # Test imputation within a pipeline + gridsearch.
-    pipeline = Pipeline([('imputer', SimpleImputer(missing_values=0)),
-                         ('tree', tree.DecisionTreeRegressor(random_state=0))])
+    X = sparse_random_matrix(100, 100, density=0.10)
+    missing_values = X.data[0]
+
+    pipeline = Pipeline([('imputer',
+                          SimpleImputer(missing_values=missing_values)),
+                         ('tree',
+                          tree.DecisionTreeRegressor(random_state=0))])
 
     parameters = {
         'imputer__strategy': ["mean", "median", "most_frequent"]
     }
 
-    X = sparse_random_matrix(100, 100, density=0.10)
     Y = sparse_random_matrix(100, 1, density=0.10).toarray()
     gs = GridSearchCV(pipeline, parameters)
     gs.fit(X, Y)
@@ -486,222 +500,138 @@ def test_imputation_copy():
     # made, even if copy=False.
 
 
-def test_chained_imputer_rank_one():
-    rng = np.random.RandomState(0)
-    d = 100
-    A = rng.rand(d, 1)
-    B = rng.rand(1, d)
-    X = np.dot(A, B)
-    nan_mask = rng.rand(d, d) < 0.5
-    X_missing = X.copy()
-    X_missing[nan_mask] = np.nan
-
-    imputer = ChainedImputer(n_imputations=5,
-                             n_burn_in=5,
-                             verbose=True,
-                             random_state=rng)
-    X_filled = imputer.fit_transform(X_missing)
-    assert_allclose(X_filled, X, atol=0.001)
-
-
 @pytest.mark.parametrize(
-    "imputation_order",
-    ['random', 'roman', 'ascending', 'descending', 'arabic']
+    "X_fit, X_trans, params, msg_err",
+    [(np.array([[-1, 1], [1, 2]]), np.array([[-1, 1], [1, -1]]),
+      {'features': 'missing-only', 'sparse': 'auto'},
+      'have missing values in transform but have no missing values in fit'),
+     (np.array([[-1, 1], [1, 2]]), np.array([[-1, 1], [1, 2]]),
+      {'features': 'random', 'sparse': 'auto'},
+      "'features' has to be either 'missing-only' or 'all'"),
+     (np.array([[-1, 1], [1, 2]]), np.array([[-1, 1], [1, 2]]),
+      {'features': 'all', 'sparse': 'random'},
+      "'sparse' has to be a boolean or 'auto'")]
 )
-def test_chained_imputer_imputation_order(imputation_order):
-    rng = np.random.RandomState(0)
-    n = 100
-    d = 10
-    X = sparse_random_matrix(n, d, density=0.10, random_state=rng).toarray()
-    X[:, 0] = 1  # this column should not be discarded by ChainedImputer
-
-    imputer = ChainedImputer(missing_values=0,
-                             n_imputations=1,
-                             n_burn_in=1,
-                             n_nearest_features=5,
-                             min_value=0,
-                             max_value=1,
-                             verbose=False,
-                             imputation_order=imputation_order,
-                             random_state=rng)
-    imputer.fit_transform(X)
-    ordered_idx = [i.feat_idx for i in imputer.imputation_sequence_]
-    if imputation_order == 'roman':
-        assert np.all(ordered_idx[:d-1] == np.arange(1, d))
-    elif imputation_order == 'arabic':
-        assert np.all(ordered_idx[:d-1] == np.arange(d-1, 0, -1))
-    elif imputation_order == 'random':
-        ordered_idx_round_1 = ordered_idx[:d-1]
-        ordered_idx_round_2 = ordered_idx[d-1:]
-        assert ordered_idx_round_1 != ordered_idx_round_2
-    elif 'ending' in imputation_order:
-        assert len(ordered_idx) == 2 * (d - 1)
+def test_missing_indicator_error(X_fit, X_trans, params, msg_err):
+    indicator = MissingIndicator(missing_values=-1)
+    indicator.set_params(**params)
+    with pytest.raises(ValueError, match=msg_err):
+        indicator.fit(X_fit).transform(X_trans)
 
 
 @pytest.mark.parametrize(
-    "predictor",
-    [DummyRegressor(), BayesianRidge(), ARDRegression()]
-)
-def test_chained_imputer_predictors(predictor):
-    rng = np.random.RandomState(0)
-
-    n = 100
-    d = 10
-    X = sparse_random_matrix(n, d, density=0.10, random_state=rng).toarray()
-
-    imputer = ChainedImputer(missing_values=0,
-                             n_imputations=1,
-                             n_burn_in=1,
-                             predictor=predictor,
-                             random_state=rng)
-    imputer.fit_transform(X)
-
-    # check that types are correct for predictors
-    hashes = []
-    for triplet in imputer.imputation_sequence_:
-        assert triplet.predictor
-        hashes.append(id(triplet.predictor))
-
-    # check that each predictor is unique
-    assert len(set(hashes)) == len(hashes)
-
-
-def test_chained_imputer_clip():
-    rng = np.random.RandomState(0)
-    n = 100
-    d = 10
-    X = sparse_random_matrix(n, d, density=0.10,
-                             random_state=rng).toarray()
-
-    imputer = ChainedImputer(missing_values=0,
-                             n_imputations=1,
-                             n_burn_in=1,
-                             min_value=0.1,
-                             max_value=0.2,
-                             random_state=rng)
-
-    Xt = imputer.fit_transform(X)
-    assert_allclose(np.min(Xt[X == 0]), 0.1)
-    assert_allclose(np.max(Xt[X == 0]), 0.2)
-    assert_allclose(Xt[X != 0], X[X != 0])
-
-
+    "missing_values, dtype",
+    [(np.nan, np.float64),
+     (0, np.int32),
+     (-1, np.int32)])
 @pytest.mark.parametrize(
-    "strategy",
-    ["mean", "median", "most_frequent"]
-)
-def test_chained_imputer_missing_at_transform(strategy):
-    rng = np.random.RandomState(0)
-    n = 100
-    d = 10
-    X_train = rng.randint(low=0, high=3, size=(n, d))
-    X_test = rng.randint(low=0, high=3, size=(n, d))
-
-    X_train[:, 0] = 1  # definitely no missing values in 0th column
-    X_test[0, 0] = 0  # definitely missing value in 0th column
-
-    imputer = ChainedImputer(missing_values=0,
-                             n_imputations=1,
-                             n_burn_in=1,
-                             initial_strategy=strategy,
-                             random_state=rng).fit(X_train)
-    initial_imputer = SimpleImputer(missing_values=0,
-                                    strategy=strategy).fit(X_train)
-
-    # if there were no missing values at time of fit, then imputer will
-    # only use the initial imputer for that feature at transform
-    assert np.all(imputer.transform(X_test)[:, 0] ==
-                  initial_imputer.transform(X_test)[:, 0])
-
-
-def test_chained_imputer_transform_stochasticity():
-    rng = np.random.RandomState(0)
-    n = 100
-    d = 10
-    X = sparse_random_matrix(n, d, density=0.10,
-                             random_state=rng).toarray()
-
-    imputer = ChainedImputer(missing_values=0,
-                             n_imputations=1,
-                             n_burn_in=1,
-                             random_state=rng)
-    imputer.fit(X)
-
-    X_fitted_1 = imputer.transform(X)
-    X_fitted_2 = imputer.transform(X)
-
-    # sufficient to assert that the means are not the same
-    assert np.mean(X_fitted_1) != pytest.approx(np.mean(X_fitted_2))
+    "arr_type",
+    [np.array, sparse.csc_matrix, sparse.csr_matrix, sparse.coo_matrix,
+     sparse.lil_matrix, sparse.bsr_matrix])
+@pytest.mark.parametrize(
+    "param_features, n_features, features_indices",
+    [('missing-only', 2, np.array([0, 1])),
+     ('all', 3, np.array([0, 1, 2]))])
+def test_missing_indicator_new(missing_values, arr_type, dtype, param_features,
+                               n_features, features_indices):
+    X_fit = np.array([[missing_values, missing_values, 1],
+                      [4, missing_values, 2]])
+    X_trans = np.array([[missing_values, missing_values, 1],
+                        [4, 12, 10]])
+    X_fit_expected = np.array([[1, 1, 0], [0, 1, 0]])
+    X_trans_expected = np.array([[1, 1, 0], [0, 0, 0]])
+
+    # convert the input to the right array format and right dtype
+    X_fit = arr_type(X_fit).astype(dtype)
+    X_trans = arr_type(X_trans).astype(dtype)
+    X_fit_expected = X_fit_expected.astype(dtype)
+    X_trans_expected = X_trans_expected.astype(dtype)
+
+    indicator = MissingIndicator(missing_values=missing_values,
+                                 features=param_features,
+                                 sparse=False)
+    X_fit_mask = indicator.fit_transform(X_fit)
+    X_trans_mask = indicator.transform(X_trans)
+
+    assert X_fit_mask.shape[1] == n_features
+    assert X_trans_mask.shape[1] == n_features
+
+    assert_array_equal(indicator.features_, features_indices)
+    assert_allclose(X_fit_mask, X_fit_expected[:, features_indices])
+    assert_allclose(X_trans_mask, X_trans_expected[:, features_indices])
+
+    assert X_fit_mask.dtype == bool
+    assert X_trans_mask.dtype == bool
+    assert isinstance(X_fit_mask, np.ndarray)
+    assert isinstance(X_trans_mask, np.ndarray)
+
+    indicator.set_params(sparse=True)
+    X_fit_mask_sparse = indicator.fit_transform(X_fit)
+    X_trans_mask_sparse = indicator.transform(X_trans)
+
+    assert X_fit_mask_sparse.dtype == bool
+    assert X_trans_mask_sparse.dtype == bool
+    assert X_fit_mask_sparse.format == 'csc'
+    assert X_trans_mask_sparse.format == 'csc'
+    assert_allclose(X_fit_mask_sparse.toarray(), X_fit_mask)
+    assert_allclose(X_trans_mask_sparse.toarray(), X_trans_mask)
+
+
+@pytest.mark.parametrize("param_sparse", [True, False, 'auto'])
+@pytest.mark.parametrize("missing_values", [np.nan, 0])
+@pytest.mark.parametrize(
+    "arr_type",
+    [np.array, sparse.csc_matrix, sparse.csr_matrix, sparse.coo_matrix])
+def test_missing_indicator_sparse_param(arr_type, missing_values,
+                                        param_sparse):
+    # check the format of the output with different sparse parameter
+    X_fit = np.array([[missing_values, missing_values, 1],
+                      [4, missing_values, 2]])
+    X_trans = np.array([[missing_values, missing_values, 1],
+                        [4, 12, 10]])
+    X_fit = arr_type(X_fit).astype(np.float64)
+    X_trans = arr_type(X_trans).astype(np.float64)
+
+    indicator = MissingIndicator(missing_values=missing_values,
+                                 sparse=param_sparse)
+    X_fit_mask = indicator.fit_transform(X_fit)
+    X_trans_mask = indicator.transform(X_trans)
+
+    if param_sparse is True:
+        assert X_fit_mask.format == 'csc'
+        assert X_trans_mask.format == 'csc'
+    elif param_sparse == 'auto' and missing_values == 0:
+        assert isinstance(X_fit_mask, np.ndarray)
+        assert isinstance(X_trans_mask, np.ndarray)
+    elif param_sparse is False:
+        assert isinstance(X_fit_mask, np.ndarray)
+        assert isinstance(X_trans_mask, np.ndarray)
+    else:
+        if sparse.issparse(X_fit):
+            assert X_fit_mask.format == 'csc'
+            assert X_trans_mask.format == 'csc'
+        else:
+            assert isinstance(X_fit_mask, np.ndarray)
+            assert isinstance(X_trans_mask, np.ndarray)
 
 
-def test_chained_imputer_no_missing():
-    rng = np.random.RandomState(0)
-    X = rng.rand(100, 100)
-    X[:, 0] = np.nan
-    m1 = ChainedImputer(n_imputations=10, random_state=rng)
-    m2 = ChainedImputer(n_imputations=10, random_state=rng)
-    pred1 = m1.fit(X).transform(X)
-    pred2 = m2.fit_transform(X)
-    # should exclude the first column entirely
-    assert_allclose(X[:, 1:], pred1)
-    # fit and fit_transform should both be identical
-    assert_allclose(pred1, pred2)
+@pytest.mark.parametrize("imputer_constructor",
+                         [SimpleImputer])
+@pytest.mark.parametrize(
+    "imputer_missing_values, missing_value, err_msg",
+    [("NaN", np.nan, "Input contains NaN"),
+     ("-1", -1, "types are expected to be both numerical.")])
+def test_inconsistent_dtype_X_missing_values(imputer_constructor,
+                                             imputer_missing_values,
+                                             missing_value,
+                                             err_msg):
+    # regression test for issue #11390. Comparison between incoherent dtype
+    # for X and missing_values was not raising a proper error.
+    rng = np.random.RandomState(42)
+    X = rng.randn(10, 10)
+    X[0, 0] = missing_value
 
+    imputer = imputer_constructor(missing_values=imputer_missing_values)
 
-@pytest.mark.parametrize(
-    "rank",
-    [3, 5]
-)
-def test_chained_imputer_transform_recovery(rank):
-    rng = np.random.RandomState(0)
-    n = 100
-    d = 100
-    A = rng.rand(n, rank)
-    B = rng.rand(rank, d)
-    X_filled = np.dot(A, B)
-    # half is randomly missing
-    nan_mask = rng.rand(n, d) < 0.5
-    X_missing = X_filled.copy()
-    X_missing[nan_mask] = np.nan
-
-    # split up data in half
-    n = n // 2
-    X_train = X_missing[:n]
-    X_test_filled = X_filled[n:]
-    X_test = X_missing[n:]
-
-    imputer = ChainedImputer(n_imputations=10,
-                             n_burn_in=10,
-                             verbose=True,
-                             random_state=rng).fit(X_train)
-    X_test_est = imputer.transform(X_test)
-    assert_allclose(X_test_filled, X_test_est, rtol=1e-5, atol=0.1)
-
-
-def test_chained_imputer_additive_matrix():
-    rng = np.random.RandomState(0)
-    n = 100
-    d = 10
-    A = rng.randn(n, d)
-    B = rng.randn(n, d)
-    X_filled = np.zeros(A.shape)
-    for i in range(d):
-        for j in range(d):
-            X_filled[:, (i+j) % d] += (A[:, i] + B[:, j]) / 2
-    # a quarter is randomly missing
-    nan_mask = rng.rand(n, d) < 0.25
-    X_missing = X_filled.copy()
-    X_missing[nan_mask] = np.nan
-
-    # split up data
-    n = n // 2
-    X_train = X_missing[:n]
-    X_test_filled = X_filled[n:]
-    X_test = X_missing[n:]
-
-    imputer = ChainedImputer(n_imputations=25,
-                             n_burn_in=10,
-                             verbose=True,
-                             random_state=rng).fit(X_train)
-    X_test_est = imputer.transform(X_test)
-    assert_allclose(X_test_filled, X_test_est, atol=0.01)
+    with pytest.raises(ValueError, match=err_msg):
+        imputer.fit_transform(X)
diff --git a/sklearn/tests/test_init.py b/sklearn/tests/test_init.py
index 56dbcaafba2c..75c9dd92129f 100644
--- a/sklearn/tests/test_init.py
+++ b/sklearn/tests/test_init.py
@@ -1,11 +1,18 @@
 # Basic unittests to test functioning of module's top-level
 
-__author__ = 'Yaroslav Halchenko'
-__license__ = 'BSD'
+import subprocess
+
+import pkgutil
 
+import pytest
 
+import sklearn
 from sklearn.utils.testing import assert_equal
 
+__author__ = 'Yaroslav Halchenko'
+__license__ = 'BSD'
+
+
 try:
     from sklearn import *  # noqa
     _top_import_error = None
@@ -18,3 +25,37 @@ def test_import_skl():
     # "import *" is discouraged outside of the module level, hence we
     # rely on setting up the variable above
     assert_equal(_top_import_error, None)
+
+
+def test_import_sklearn_no_warnings():
+    # Test that importing scikit-learn main modules doesn't raise any warnings.
+
+    try:
+        pkgs = pkgutil.iter_modules(path=sklearn.__path__, prefix='sklearn.')
+        import_modules = '; '.join(['import ' + modname
+                                    for _, modname, _ in pkgs
+                                    if (not modname.startswith('_') and
+                                        # add deprecated top level modules
+                                        # below to ignore them
+                                        modname not in [])])
+
+        message = subprocess.check_output(['python', '-Wdefault',
+                                           '-c', import_modules],
+                                          stderr=subprocess.STDOUT)
+        message = message.decode("utf-8")
+        message = '\n'.join([line for line in message.splitlines()
+                             if not (
+                                     # ignore ImportWarning due to Cython
+                                     "ImportWarning" in line or
+                                     # ignore DeprecationWarning due to pytest
+                                     "pytest" in line or
+                                     # ignore DeprecationWarnings due to
+                                     # numpy.oldnumeric
+                                     "oldnumeric" in line
+                                     )])
+        assert 'Warning' not in message
+        assert 'Error' not in message
+
+    except Exception as e:
+        pytest.skip('soft-failed test_import_sklearn_no_warnings.\n'
+                    ' %s, \n %s' % (e, message))
diff --git a/sklearn/tests/test_metaestimators.py b/sklearn/tests/test_metaestimators.py
index 36885ee8229d..1c2d5a0873cd 100644
--- a/sklearn/tests/test_metaestimators.py
+++ b/sklearn/tests/test_metaestimators.py
@@ -1,5 +1,5 @@
 """Common tests for metaestimators"""
-
+import pytest
 import functools
 
 import numpy as np
@@ -47,6 +47,7 @@ def __init__(self, name, construct, skip_methods=(),
 ]
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_metaestimator_delegation():
     # Ensures specified metaestimators have methods iff subestimator does
     def hides(method):
diff --git a/sklearn/tests/test_multiclass.py b/sklearn/tests/test_multiclass.py
index 78a1fd617ccf..08c3b9f01e16 100644
--- a/sklearn/tests/test_multiclass.py
+++ b/sklearn/tests/test_multiclass.py
@@ -1,3 +1,5 @@
+import pytest
+
 import numpy as np
 import scipy.sparse as sp
 
@@ -329,6 +331,8 @@ def test_ovr_multilabel_dataset():
                             decimal=2)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_ovr_multilabel_predict_proba():
     base_clf = MultinomialNB(alpha=1)
     for au in (False, True):
@@ -421,6 +425,8 @@ def test_ovr_single_label_decision_function():
                        clf.predict(X_test))
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_ovr_gridsearch():
     ovr = OneVsRestClassifier(LinearSVC(random_state=0))
     Cs = [0.1, 0.5, 0.8]
@@ -597,6 +603,8 @@ def test_ovo_decision_function():
         assert_greater(len(np.unique(decisions[:, class_idx])), 146)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_ovo_gridsearch():
     ovo = OneVsOneClassifier(LinearSVC(random_state=0))
     Cs = [0.1, 0.5, 0.8]
@@ -691,6 +699,8 @@ def test_ecoc_fit_predict():
     assert_equal(len(ecoc.estimators_), n_classes * 2)
 
 
+@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_ecoc_gridsearch():
     ecoc = OutputCodeClassifier(LinearSVC(random_state=0),
                                 random_state=0)
@@ -741,6 +751,7 @@ def test_pairwise_attribute():
         assert_true(ovr_true._pairwise)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_pairwise_cross_val_score():
     clf_precomputed = svm.SVC(kernel='precomputed')
     clf_notprecomputed = svm.SVC(kernel='linear')
diff --git a/sklearn/tests/test_multioutput.py b/sklearn/tests/test_multioutput.py
index 717d680fa6fd..83e3794d7887 100644
--- a/sklearn/tests/test_multioutput.py
+++ b/sklearn/tests/test_multioutput.py
@@ -18,7 +18,7 @@
 from sklearn.datasets import make_classification
 from sklearn.ensemble import GradientBoostingRegressor, RandomForestClassifier
 from sklearn.exceptions import NotFittedError
-from sklearn.externals.joblib import cpu_count
+from sklearn.utils import cpu_count
 from sklearn.linear_model import Lasso
 from sklearn.linear_model import LogisticRegression
 from sklearn.linear_model import Ridge
diff --git a/sklearn/tests/test_pipeline.py b/sklearn/tests/test_pipeline.py
index beb4be61d0b0..a36d3e17e31e 100644
--- a/sklearn/tests/test_pipeline.py
+++ b/sklearn/tests/test_pipeline.py
@@ -1,7 +1,7 @@
 """
 Test the pipeline module.
 """
-
+from distutils.version import LooseVersion
 from tempfile import mkdtemp
 import shutil
 import time
@@ -33,7 +33,8 @@
 from sklearn.datasets import load_iris
 from sklearn.preprocessing import StandardScaler
 from sklearn.feature_extraction.text import CountVectorizer
-from sklearn.externals.joblib import Memory
+from sklearn.utils import Memory
+from sklearn.utils._joblib import __version__ as joblib_version
 
 
 JUNK_FOOD_DOCS = (
@@ -908,7 +909,7 @@ def test_pipeline_wrong_memory():
                             ('svc', SVC())], memory=memory)
     assert_raises_regex(ValueError, "'memory' should be None, a string or"
                         " have the same interface as "
-                        "sklearn.externals.joblib.Memory."
+                        "sklearn.utils.Memory."
                         " Got memory='1' instead.", cached_pipe.fit, X, y)
 
 
@@ -931,7 +932,7 @@ def test_pipeline_with_cache_attribute():
                     memory=dummy)
     assert_raises_regex(ValueError, "'memory' should be None, a string or"
                         " have the same interface as "
-                        "sklearn.externals.joblib.Memory."
+                        "sklearn.utils.Memory."
                         " Got memory='{}' instead.".format(dummy), pipe.fit, X)
 
 
@@ -941,7 +942,11 @@ def test_pipeline_memory():
     y = iris.target
     cachedir = mkdtemp()
     try:
-        memory = Memory(cachedir=cachedir, verbose=10)
+        if LooseVersion(joblib_version) < LooseVersion('0.12'):
+            # Deal with change of API in joblib
+            memory = Memory(cachedir=cachedir, verbose=10)
+        else:
+            memory = Memory(location=cachedir, verbose=10)
         # Test with Transformer + SVC
         clf = SVC(gamma='scale', probability=True, random_state=0)
         transf = DummyTransf()
@@ -999,7 +1004,11 @@ def test_pipeline_memory():
 
 def test_make_pipeline_memory():
     cachedir = mkdtemp()
-    memory = Memory(cachedir=cachedir)
+    if LooseVersion(joblib_version) < LooseVersion('0.12'):
+        # Deal with change of API in joblib
+        memory = Memory(cachedir=cachedir, verbose=10)
+    else:
+        memory = Memory(location=cachedir, verbose=10)
     pipeline = make_pipeline(DummyTransf(), SVC(), memory=memory)
     assert_true(pipeline.memory is memory)
     pipeline = make_pipeline(DummyTransf(), SVC())
diff --git a/sklearn/tests/test_random_projection.py b/sklearn/tests/test_random_projection.py
index 975922a34116..bac639bb199f 100644
--- a/sklearn/tests/test_random_projection.py
+++ b/sklearn/tests/test_random_projection.py
@@ -77,7 +77,7 @@ def test_input_size_jl_min_dim():
                   2 * [0.9])
 
     johnson_lindenstrauss_min_dim(np.random.randint(1, 10, size=(10, 10)),
-                                  0.5 * np.ones((10, 10)))
+                                  np.full((10, 10), 0.5))
 
 
 ###############################################################################
diff --git a/sklearn/tests/test_site_joblib.py b/sklearn/tests/test_site_joblib.py
new file mode 100644
index 000000000000..7ceb80a28166
--- /dev/null
+++ b/sklearn/tests/test_site_joblib.py
@@ -0,0 +1,42 @@
+import os
+from sklearn.externals import joblib as joblib_vendored
+from sklearn.utils import Parallel, delayed, Memory, parallel_backend
+
+if os.environ.get('SKLEARN_SITE_JOBLIB', False):
+    import joblib as joblib_site
+else:
+    joblib_site = None
+
+
+def test_old_pickle(tmpdir):
+    # Check that a pickle that references sklearn.external.joblib can load
+    f = tmpdir.join('foo.pkl')
+    f.write(b'\x80\x02csklearn.externals.joblib.numpy_pickle\nNumpyArrayWrappe'
+            b'r\nq\x00)\x81q\x01}q\x02(U\x05dtypeq\x03cnumpy\ndtype\nq\x04U'
+            b'\x02i8q\x05K\x00K\x01\x87q\x06Rq\x07(K\x03U\x01<q\x08NNNJ\xff'
+            b'\xff\xff\xffJ\xff\xff\xff\xffK\x00tq\tbU\x05shapeq\nK\x01\x85q'
+            b'\x0bU\x05orderq\x0cU\x01Cq\rU\x08subclassq\x0ecnumpy\nndarray\nq'
+            b'\x0fU\nallow_mmapq\x10\x88ub\x01\x00\x00\x00\x00\x00\x00\x00.',
+            mode='wb')
+
+    joblib_vendored.load(str(f))
+
+
+def test_site_joblib_dispatch():
+    if os.environ.get('SKLEARN_SITE_JOBLIB', False):
+        assert Parallel is joblib_site.Parallel
+        assert delayed is joblib_site.delayed
+        assert parallel_backend is joblib_site.parallel_backend
+        assert Memory is joblib_site.Memory
+
+        assert joblib_vendored.Parallel is not joblib_site.Parallel
+        assert joblib_vendored.delayed is not joblib_site.delayed
+        assert joblib_vendored.parallel_backend is not \
+            joblib_site.parallel_backend
+        assert joblib_vendored.Memory is not joblib_site.Memory
+
+    else:
+        assert Parallel is joblib_vendored.Parallel
+        assert delayed is joblib_vendored.delayed
+        assert parallel_backend is joblib_vendored.parallel_backend
+        assert Memory is joblib_vendored.Memory
diff --git a/sklearn/tree/_criterion.pyx b/sklearn/tree/_criterion.pyx
index f47aa73e6e79..a2b362334de5 100644
--- a/sklearn/tree/_criterion.pyx
+++ b/sklearn/tree/_criterion.pyx
@@ -842,7 +842,7 @@ cdef class RegressionCriterion(Criterion):
         #           sum_left[x] +  sum_right[x] = sum_total[x]
         # and that sum_total is known, we are going to update
         # sum_left from the direction that require the least amount
-        # of computations, i.e. from pos to new_pos or from end to new_po.
+        # of computations, i.e. from pos to new_pos or from end to new_pos.
 
         if (new_pos - pos) <= (end - new_pos):
             for p in range(pos, new_pos):
@@ -1238,9 +1238,8 @@ cdef class MAE(RegressionCriterion):
         cdef SIZE_t* samples = self.samples
         cdef SIZE_t i, p, k
         cdef DOUBLE_t y_ik
-        cdef DOUBLE_t w_y_ik
-
-        cdef double impurity = 0.0
+        cdef DOUBLE_t w = 1.0
+        cdef DOUBLE_t impurity = 0.0
 
         for k in range(self.n_outputs):
             for p in range(self.start, self.end):
@@ -1248,11 +1247,15 @@ cdef class MAE(RegressionCriterion):
 
                 y_ik = y[i * self.y_stride + k]
 
-                impurity += <double> fabs((<double> y_ik) - <double> self.node_medians[k])
+                if sample_weight != NULL:
+                    w = sample_weight[i]
+
+                impurity += fabs(y_ik - self.node_medians[k]) * w
+
         return impurity / (self.weighted_n_node_samples * self.n_outputs)
 
-    cdef void children_impurity(self, double* impurity_left,
-                                double* impurity_right) nogil:
+    cdef void children_impurity(self, double* p_impurity_left,
+                                double* p_impurity_right) nogil:
         """Evaluate the impurity in children nodes, i.e. the impurity of the
            left child (samples[start:pos]) and the impurity the right child
            (samples[pos:end]).
@@ -1269,13 +1272,13 @@ cdef class MAE(RegressionCriterion):
         cdef SIZE_t i, p, k
         cdef DOUBLE_t y_ik
         cdef DOUBLE_t median
+        cdef DOUBLE_t w = 1.0
+        cdef DOUBLE_t impurity_left = 0.0
+        cdef DOUBLE_t impurity_right = 0.0
 
         cdef void** left_child = <void**> self.left_child.data
         cdef void** right_child = <void**> self.right_child.data
 
-        impurity_left[0] = 0.0
-        impurity_right[0] = 0.0
-
         for k in range(self.n_outputs):
             median = (<WeightedMedianCalculator> left_child[k]).get_median()
             for p in range(start, pos):
@@ -1283,9 +1286,12 @@ cdef class MAE(RegressionCriterion):
 
                 y_ik = y[i * self.y_stride + k]
 
-                impurity_left[0] += <double>fabs((<double> y_ik) -
-                                                 <double> median)
-        impurity_left[0] /= <double>((self.weighted_n_left) * self.n_outputs)
+                if sample_weight != NULL:
+                    w = sample_weight[i]
+
+                impurity_left += fabs(y_ik - median) * w
+        p_impurity_left[0] = impurity_left / (self.weighted_n_left * 
+                                              self.n_outputs)
 
         for k in range(self.n_outputs):
             median = (<WeightedMedianCalculator> right_child[k]).get_median()
@@ -1294,10 +1300,12 @@ cdef class MAE(RegressionCriterion):
 
                 y_ik = y[i * self.y_stride + k]
 
-                impurity_right[0] += <double>fabs((<double> y_ik) -
-                                                  <double> median)
-        impurity_right[0] /= <double>((self.weighted_n_right) *
-                                      self.n_outputs)
+                if sample_weight != NULL:
+                    w = sample_weight[i]
+
+                impurity_right += fabs(y_ik - median) * w
+        p_impurity_right[0] = impurity_right / (self.weighted_n_right * 
+                                                self.n_outputs)
 
 
 cdef class FriedmanMSE(MSE):
diff --git a/sklearn/tree/tests/test_tree.py b/sklearn/tree/tests/test_tree.py
index bb117d8a2986..37eb6582c702 100644
--- a/sklearn/tree/tests/test_tree.py
+++ b/sklearn/tree/tests/test_tree.py
@@ -18,6 +18,7 @@
 from sklearn.metrics import accuracy_score
 from sklearn.metrics import mean_squared_error
 
+from sklearn.utils.testing import assert_allclose
 from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_almost_equal
@@ -214,7 +215,7 @@ def test_weighted_classification_toy():
         assert_array_equal(clf.predict(T), true_result,
                            "Failed with {0}".format(name))
 
-        clf.fit(X, y, sample_weight=np.ones(len(X)) * 0.5)
+        clf.fit(X, y, sample_weight=np.full(len(X), 0.5))
         assert_array_equal(clf.predict(T), true_result,
                            "Failed with {0}".format(name))
 
@@ -573,7 +574,7 @@ def test_error():
 
 def test_min_samples_split():
     """Test min_samples_split parameter"""
-    X = np.asfortranarray(iris.data.astype(tree._tree.DTYPE))
+    X = np.asfortranarray(iris.data, dtype=tree._tree.DTYPE)
     y = iris.target
 
     # test both DepthFirstTreeBuilder and BestFirstTreeBuilder
@@ -606,7 +607,7 @@ def test_min_samples_split():
 
 def test_min_samples_leaf():
     # Test if leaves contain more than leaf_count training examples
-    X = np.asfortranarray(iris.data.astype(tree._tree.DTYPE))
+    X = np.asfortranarray(iris.data, dtype=tree._tree.DTYPE)
     y = iris.target
 
     # test both DepthFirstTreeBuilder and BestFirstTreeBuilder
@@ -791,7 +792,7 @@ def test_min_impurity_split():
     # test if min_impurity_split creates leaves with impurity
     # [0, min_impurity_split) when min_samples_leaf = 1 and
     # min_samples_split = 2.
-    X = np.asfortranarray(iris.data.astype(tree._tree.DTYPE))
+    X = np.asfortranarray(iris.data, dtype=tree._tree.DTYPE)
     y = iris.target
 
     # test both DepthFirstTreeBuilder and BestFirstTreeBuilder
@@ -1280,13 +1281,13 @@ def test_with_only_one_non_constant_features():
         est = TreeEstimator(random_state=0, max_features=1)
         est.fit(X, y)
         assert_equal(est.tree_.max_depth, 1)
-        assert_array_equal(est.predict_proba(X), 0.5 * np.ones((4, 2)))
+        assert_array_equal(est.predict_proba(X), np.full((4, 2), 0.5))
 
     for name, TreeEstimator in REG_TREES.items():
         est = TreeEstimator(random_state=0, max_features=1)
         est.fit(X, y)
         assert_equal(est.tree_.max_depth, 1)
-        assert_array_equal(est.predict(X), 0.5 * np.ones((4, )))
+        assert_array_equal(est.predict(X), np.full((4, ), 0.5))
 
 
 def test_big_input():
@@ -1693,19 +1694,101 @@ def test_no_sparse_y_support(name):
 
 
 def test_mae():
-    # check MAE criterion produces correct results
-    # on small toy dataset
+    """Check MAE criterion produces correct results on small toy dataset:
+
+    ------------------
+    | X | y | weight |
+    ------------------
+    | 3 | 3 |  0.1   |
+    | 5 | 3 |  0.3   |
+    | 8 | 4 |  1.0   |
+    | 3 | 6 |  0.6   |
+    | 5 | 7 |  0.3   |
+    ------------------
+    |sum wt:|  2.3   |
+    ------------------
+
+    Because we are dealing with sample weights, we cannot find the median by
+    simply choosing/averaging the centre value(s), instead we consider the
+    median where 50% of the cumulative weight is found (in a y sorted data set)
+    . Therefore with regards to this test data, the cumulative weight is >= 50%
+    when y = 4.  Therefore:
+    Median = 4
+
+    For all the samples, we can get the total error by summing:
+    Absolute(Median - y) * weight
+
+    I.e., total error = (Absolute(4 - 3) * 0.1)
+                      + (Absolute(4 - 3) * 0.3)
+                      + (Absolute(4 - 4) * 1.0)
+                      + (Absolute(4 - 6) * 0.6)
+                      + (Absolute(4 - 7) * 0.3)
+                      = 2.5
+
+    Impurity = Total error / total weight
+             = 2.5 / 2.3
+             = 1.08695652173913
+             ------------------
+
+    From this root node, the next best split is between X values of 3 and 5.
+    Thus, we have left and right child nodes:
+
+    LEFT                    RIGHT
+    ------------------      ------------------
+    | X | y | weight |      | X | y | weight |
+    ------------------      ------------------
+    | 3 | 3 |  0.1   |      | 5 | 3 |  0.3   |
+    | 3 | 6 |  0.6   |      | 8 | 4 |  1.0   |
+    ------------------      | 5 | 7 |  0.3   |
+    |sum wt:|  0.7   |      ------------------
+    ------------------      |sum wt:|  1.6   |
+                            ------------------
+
+    Impurity is found in the same way:
+    Left node Median = 6
+    Total error = (Absolute(6 - 3) * 0.1)
+                + (Absolute(6 - 6) * 0.6)
+                = 0.3
+
+    Left Impurity = Total error / total weight
+            = 0.3 / 0.7
+            = 0.428571428571429
+            -------------------
+
+    Likewise for Right node:
+    Right node Median = 4
+    Total error = (Absolute(4 - 3) * 0.3)
+                + (Absolute(4 - 4) * 1.0)
+                + (Absolute(4 - 7) * 0.3)
+                = 1.2
+
+    Right Impurity = Total error / total weight
+            = 1.2 / 1.6
+            = 0.75
+            ------
+    """
     dt_mae = DecisionTreeRegressor(random_state=0, criterion="mae",
                                    max_leaf_nodes=2)
-    dt_mae.fit([[3], [5], [3], [8], [5]], [6, 7, 3, 4, 3])
-    assert_array_equal(dt_mae.tree_.impurity, [1.4, 1.5, 4.0/3.0])
-    assert_array_equal(dt_mae.tree_.value.flat, [4, 4.5, 4.0])
 
-    dt_mae.fit([[3], [5], [3], [8], [5]], [6, 7, 3, 4, 3],
-               [0.6, 0.3, 0.1, 1.0, 0.3])
-    assert_array_equal(dt_mae.tree_.impurity, [7.0/2.3, 3.0/0.7, 4.0/1.6])
+    # Test MAE where sample weights are non-uniform (as illustrated above):
+    dt_mae.fit(X=[[3], [5], [3], [8], [5]], y=[6, 7, 3, 4, 3],
+               sample_weight=[0.6, 0.3, 0.1, 1.0, 0.3])
+    assert_allclose(dt_mae.tree_.impurity, [2.5 / 2.3, 0.3 / 0.7, 1.2 / 1.6])
     assert_array_equal(dt_mae.tree_.value.flat, [4.0, 6.0, 4.0])
 
+    # Test MAE where all sample weights are uniform:
+    dt_mae.fit(X=[[3], [5], [3], [8], [5]], y=[6, 7, 3, 4, 3],
+               sample_weight=np.ones(5))
+    assert_array_equal(dt_mae.tree_.impurity, [1.4, 1.5, 4.0 / 3.0])
+    assert_array_equal(dt_mae.tree_.value.flat, [4, 4.5, 4.0])
+
+    # Test MAE where a `sample_weight` is not explicitly provided.
+    # This is equivalent to providing uniform sample weights, though
+    # the internal logic is different:
+    dt_mae.fit(X=[[3], [5], [3], [8], [5]], y=[6, 7, 3, 4, 3])
+    assert_array_equal(dt_mae.tree_.impurity, [1.4, 1.5, 4.0 / 3.0])
+    assert_array_equal(dt_mae.tree_.value.flat, [4, 4.5, 4.0])
+
 
 def test_criterion_copy():
     # Let's check whether copy of our criterion has the same type
diff --git a/sklearn/tree/tree.py b/sklearn/tree/tree.py
index af216f1906eb..7105a86ce05f 100644
--- a/sklearn/tree/tree.py
+++ b/sklearn/tree/tree.py
@@ -1152,7 +1152,7 @@ class ExtraTreeClassifier(DecisionTreeClassifier):
         The function to measure the quality of a split. Supported criteria are
         "gini" for the Gini impurity and "entropy" for the information gain.
 
-    splitter : string, optional (default="best")
+    splitter : string, optional (default="random")
         The strategy used to choose the split at each node. Supported
         strategies are "best" to choose the best split and "random" to choose
         the best random split.
@@ -1189,7 +1189,7 @@ class ExtraTreeClassifier(DecisionTreeClassifier):
         the input samples) required to be at a leaf node. Samples have
         equal weight when sample_weight is not provided.
 
-    max_features : int, float, string or None, optional (default=None)
+    max_features : int, float, string or None, optional (default="auto")
         The number of features to consider when looking for the best split:
 
             - If int, then consider `max_features` features at each split.
@@ -1336,7 +1336,7 @@ class ExtraTreeRegressor(DecisionTreeRegressor):
         .. versionadded:: 0.18
            Mean Absolute Error (MAE) criterion.
 
-    splitter : string, optional (default="best")
+    splitter : string, optional (default="random")
         The strategy used to choose the split at each node. Supported
         strategies are "best" to choose the best split and "random" to choose
         the best random split.
@@ -1373,7 +1373,7 @@ class ExtraTreeRegressor(DecisionTreeRegressor):
         the input samples) required to be at a leaf node. Samples have
         equal weight when sample_weight is not provided.
 
-    max_features : int, float, string or None, optional (default=None)
+    max_features : int, float, string or None, optional (default="auto")
         The number of features to consider when looking for the best split:
 
         - If int, then consider `max_features` features at each split.
diff --git a/sklearn/utils/__init__.py b/sklearn/utils/__init__.py
index bb1f383505fe..8af32c145d7b 100644
--- a/sklearn/utils/__init__.py
+++ b/sklearn/utils/__init__.py
@@ -1,8 +1,8 @@
 """
 The :mod:`sklearn.utils` module includes various utilities.
 """
-from collections import Sequence
 import numbers
+import platform
 
 import numpy as np
 from scipy.sparse import issparse
@@ -15,8 +15,11 @@
                          check_consistent_length, check_X_y, indexable,
                          check_symmetric)
 from .class_weight import compute_class_weight, compute_sample_weight
-from ..externals.joblib import cpu_count
+from ._joblib import cpu_count, Parallel, Memory, delayed, hash
+from ._joblib import parallel_backend, register_parallel_backend
+from ._joblib import effective_n_jobs
 from ..exceptions import DataConversionWarning
+from ..utils.fixes import _Sequence as Sequence
 from .deprecation import deprecated
 from .. import get_config
 
@@ -26,7 +29,11 @@
            "compute_class_weight", "compute_sample_weight",
            "column_or_1d", "safe_indexing",
            "check_consistent_length", "check_X_y", 'indexable',
-           "check_symmetric", "indices_to_mask", "deprecated"]
+           "check_symmetric", "indices_to_mask", "deprecated",
+           "cpu_count", "Parallel", "Memory", "delayed", "parallel_backend",
+           "register_parallel_backend", "hash", "effective_n_jobs"]
+
+IS_PYPY = platform.python_implementation() == 'PyPy'
 
 
 class Bunch(dict):
@@ -114,6 +121,21 @@ def axis0_safe_slice(X, mask, len_mask):
     is not going to be the bottleneck, since the number of outliers
     and non_outliers are typically non-zero and it makes the code
     tougher to follow.
+
+    Parameters
+    ----------
+    X : {array-like, sparse matrix}
+        Data on which to apply mask.
+
+    mask : array
+        Mask to be used on X.
+
+    len_mask : int
+        The length of the mask.
+
+    Returns
+    -------
+        mask
     """
     if len_mask != 0:
         return X[safe_mask(X, mask), :]
@@ -177,6 +199,8 @@ def resample(*arrays, **options):
         Indexable data-structures can be arrays, lists, dataframes or scipy
         sparse matrices with consistent first dimension.
 
+    Other Parameters
+    ----------------
     replace : boolean, True by default
         Implements resampling with replacement. If False, this will implement
         (sliced) random permutations.
@@ -287,6 +311,8 @@ def shuffle(*arrays, **options):
         Indexable data-structures can be arrays, lists, dataframes or scipy
         sparse matrices with consistent first dimension.
 
+    Other Parameters
+    ----------------
     random_state : int, RandomState instance or None, optional (default=None)
         The seed of the pseudo random number generator to use when shuffling
         the data.  If int, random_state is the seed used by the random number
@@ -378,6 +404,16 @@ def gen_batches(n, batch_size):
     The last slice may contain less than batch_size elements, when batch_size
     does not divide n.
 
+    Parameters
+    ----------
+    n : int
+    batch_size : int
+        Number of element in each batch
+
+    Yields
+    ------
+    slice of batch_size elements
+
     Examples
     --------
     >>> from sklearn.utils import gen_batches
@@ -400,8 +436,19 @@ def gen_batches(n, batch_size):
 def gen_even_slices(n, n_packs, n_samples=None):
     """Generator to create n_packs slices going up to n.
 
-    Pass n_samples when the slices are to be used for sparse matrix indexing;
-    slicing off-the-end raises an exception, while it works for NumPy arrays.
+    Parameters
+    ----------
+    n : int
+    n_packs : int
+        Number of slices to generate.
+    n_samples : int or None (default = None)
+        Number of samples. Pass n_samples when the slices are to be used for
+        sparse matrix indexing; slicing off-the-end raises an exception, while
+        it works for NumPy arrays.
+
+    Yields
+    ------
+    slice
 
     Examples
     --------
@@ -431,45 +478,6 @@ def gen_even_slices(n, n_packs, n_samples=None):
             start = end
 
 
-def _get_n_jobs(n_jobs):
-    """Get number of jobs for the computation.
-
-    This function reimplements the logic of joblib to determine the actual
-    number of jobs depending on the cpu count. If -1 all CPUs are used.
-    If 1 is given, no parallel computing code is used at all, which is useful
-    for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used.
-    Thus for n_jobs = -2, all CPUs but one are used.
-
-    Parameters
-    ----------
-    n_jobs : int
-        Number of jobs stated in joblib convention.
-
-    Returns
-    -------
-    n_jobs : int
-        The actual number of jobs as positive integer.
-
-    Examples
-    --------
-    >>> from sklearn.utils import _get_n_jobs
-    >>> _get_n_jobs(4)
-    4
-    >>> jobs = _get_n_jobs(-2)
-    >>> assert jobs == max(cpu_count() - 1, 1)
-    >>> _get_n_jobs(0)
-    Traceback (most recent call last):
-    ...
-    ValueError: Parameter n_jobs == 0 has no meaning.
-    """
-    if n_jobs < 0:
-        return max(cpu_count() + 1 + n_jobs, 1)
-    elif n_jobs == 0:
-        raise ValueError('Parameter n_jobs == 0 has no meaning.')
-    else:
-        return n_jobs
-
-
 def tosequence(x):
     """Cast iterable x to a Sequence, avoiding a copy if possible.
 
diff --git a/sklearn/utils/_joblib.py b/sklearn/utils/_joblib.py
new file mode 100644
index 000000000000..e1c39a40119a
--- /dev/null
+++ b/sklearn/utils/_joblib.py
@@ -0,0 +1,21 @@
+# We need the absolute_import to avoid the local joblib to override the
+# site one
+from __future__ import absolute_import
+import os as _os
+import warnings as _warnings
+
+# An environment variable to use the site joblib
+if _os.environ.get('SKLEARN_SITE_JOBLIB', False):
+    with _warnings.catch_warnings():
+        _warnings.simplefilter("ignore")
+        # joblib imports may raise DeprecationWarning on certain Python
+        # versions
+        from joblib import __all__
+        from joblib import *  # noqa
+        from joblib import __version__
+        from joblib import logger
+else:
+    from ..externals.joblib import __all__   # noqa
+    from ..externals.joblib import *  # noqa
+    from ..externals.joblib import __version__  # noqa
+    from ..externals.joblib import logger  # noqa
diff --git a/sklearn/utils/_show_versions.py b/sklearn/utils/_show_versions.py
new file mode 100644
index 000000000000..5973e8afb823
--- /dev/null
+++ b/sklearn/utils/_show_versions.py
@@ -0,0 +1,119 @@
+"""
+Utility methods to print system info for debugging
+
+adapted from :func:`pandas.show_versions`
+"""
+# License: BSD 3 clause
+
+import platform
+import sys
+import importlib
+
+
+def _get_sys_info():
+    """System information
+
+    Return
+    ------
+    sys_info : dict
+        system and Python version information
+
+    """
+    python = sys.version.replace('\n', ' ')
+
+    blob = [
+        ("python", python),
+        ('executable', sys.executable),
+        ("machine", platform.platform()),
+    ]
+
+    return dict(blob)
+
+
+def _get_deps_info():
+    """Overview of the installed version of main dependencies
+
+    Returns
+    -------
+    deps_info: dict
+        version information on relevant Python libraries
+
+    """
+    deps = [
+        "pip",
+        "setuptools",
+        "sklearn",
+        "numpy",
+        "scipy",
+        "Cython",
+        "pandas",
+    ]
+
+    def get_version(module):
+        return module.__version__
+
+    deps_info = {}
+
+    for modname in deps:
+        try:
+            if modname in sys.modules:
+                mod = sys.modules[modname]
+            else:
+                mod = importlib.import_module(modname)
+            ver = get_version(mod)
+            deps_info[modname] = ver
+        except ImportError:
+            deps_info[modname] = None
+
+    return deps_info
+
+
+def _get_blas_info():
+    """Information on system BLAS
+
+    Uses the `scikit-learn` builtin method
+    :func:`sklearn._build_utils.get_blas_info` which may fail from time to time
+
+    Returns
+    -------
+    blas_info: dict
+        system BLAS information
+
+    """
+    from .._build_utils import get_blas_info
+
+    cblas_libs, blas_dict = get_blas_info()
+
+    macros = ['{key}={val}'.format(key=a, val=b)
+              for (a, b) in blas_dict.get('define_macros', [])]
+
+    blas_blob = [
+        ('macros', ', '.join(macros)),
+        ('lib_dirs', ':'.join(blas_dict.get('library_dirs', ''))),
+        ('cblas_libs', ', '.join(cblas_libs)),
+    ]
+
+    return dict(blas_blob)
+
+
+def show_versions():
+    "Print useful debugging information"
+
+    sys_info = _get_sys_info()
+    deps_info = _get_deps_info()
+    blas_info = _get_blas_info()
+
+    print('\nSystem')
+    print('------')
+    for k, stat in sys_info.items():
+        print("{k:>10}: {stat}".format(k=k, stat=stat))
+
+    print('\nBLAS')
+    print('----')
+    for k, stat in blas_info.items():
+        print("{k:>10}: {stat}".format(k=k, stat=stat))
+
+    print('\nPython deps')
+    print('-----------')
+    for k, stat in deps_info.items():
+        print("{k:>10}: {stat}".format(k=k, stat=stat))
diff --git a/sklearn/utils/_unittest_backport.py b/sklearn/utils/_unittest_backport.py
index 919217f67e3c..a7cfe267280e 100644
--- a/sklearn/utils/_unittest_backport.py
+++ b/sklearn/utils/_unittest_backport.py
@@ -149,7 +149,7 @@ def __exit__(self, exc_type, exc_value, tb):
 
 
 class TestCase(unittest.TestCase):
-    longMessage = False
+    longMessage = True
     failureException = AssertionError
 
     def _formatMessage(self, msg, standardMsg):
diff --git a/sklearn/utils/bench.py b/sklearn/utils/bench.py
index 82267d00e65b..1a04ed2bb9f8 100644
--- a/sklearn/utils/bench.py
+++ b/sklearn/utils/bench.py
@@ -10,6 +10,15 @@ def total_seconds(delta):
 
     http://docs.python.org/library/datetime.html\
 #datetime.timedelta.total_seconds
+
+    Parameters
+    ----------
+    delta : datetime object
+
+    Returns
+    -------
+    int
+        The number of seconds contained in delta
     """
 
     mu_sec = 1e-6  # number of seconds in one microseconds
diff --git a/sklearn/utils/deprecation.py b/sklearn/utils/deprecation.py
index fc06f9bc84d3..b84e0bd9b4fa 100644
--- a/sklearn/utils/deprecation.py
+++ b/sklearn/utils/deprecation.py
@@ -126,6 +126,15 @@ def __getitem__(self, key):
         return super(DeprecationDict, self).__getitem__(key)
 
     def get(self, key, default=None):
+        """Return the value corresponding to key, else default.
+
+        Parameters
+        ----------
+        key : any hashable object
+            The key
+        default : object, optional
+            The default returned when key is not in dict
+        """
         # dict does not implement it like this, hence it needs to be overridden
         try:
             return self[key]
@@ -133,5 +142,11 @@ def get(self, key, default=None):
             return default
 
     def add_warning(self, key, *args, **kwargs):
-        """Add a warning to be triggered when the specified key is read"""
+        """Add a warning to be triggered when the specified key is read
+
+        Parameters
+        ----------
+        key : any hashable object
+            The key
+        """
         self._deprecations[key] = (args, kwargs)
diff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py
index 6c8fffd103d4..dac884a317bc 100644
--- a/sklearn/utils/estimator_checks.py
+++ b/sklearn/utils/estimator_checks.py
@@ -14,7 +14,8 @@
 from scipy.stats import rankdata
 
 from sklearn.externals.six.moves import zip
-from sklearn.externals.joblib import hash, Memory
+from sklearn.utils import IS_PYPY
+from sklearn.utils._joblib import hash, Memory
 from sklearn.utils.testing import assert_raises, _get_args
 from sklearn.utils.testing import assert_raises_regex
 from sklearn.utils.testing import assert_raise_message
@@ -51,7 +52,6 @@
 from sklearn.svm.base import BaseLibSVM
 from sklearn.linear_model.stochastic_gradient import BaseSGD
 from sklearn.pipeline import make_pipeline
-from sklearn.exceptions import ConvergenceWarning
 from sklearn.exceptions import DataConversionWarning
 from sklearn.exceptions import SkipTestWarning
 from sklearn.model_selection import train_test_split
@@ -78,8 +78,8 @@
                 'RANSACRegressor', 'RadiusNeighborsRegressor',
                 'RandomForestRegressor', 'Ridge', 'RidgeCV']
 
-ALLOW_NAN = ['Imputer', 'SimpleImputer', 'ChainedImputer',
-             'MaxAbsScaler', 'MinMaxScaler', 'StandardScaler',
+ALLOW_NAN = ['Imputer', 'SimpleImputer', 'MissingIndicator',
+             'MaxAbsScaler', 'MinMaxScaler', 'RobustScaler', 'StandardScaler',
              'PowerTransformer', 'QuantileTransformer']
 
 
@@ -89,6 +89,7 @@ def _yield_non_meta_checks(name, estimator):
     yield check_dtype_object
     yield check_sample_weights_pandas_series
     yield check_sample_weights_list
+    yield check_sample_weights_invariance
     yield check_estimators_fit_returns_self
     yield partial(check_estimators_fit_returns_self, readonly_memmap=True)
     yield check_complex_data
@@ -154,7 +155,7 @@ def check_supervised_y_no_nan(name, estimator_orig):
     estimator = clone(estimator_orig)
     rng = np.random.RandomState(888)
     X = rng.randn(10, 5)
-    y = np.ones(10) * np.inf
+    y = np.full(10, np.inf)
     y = multioutput_estimator_convert_y_2d(estimator, y)
 
     errmsg = "Input contains NaN, infinity or a value too large for " \
@@ -225,8 +226,9 @@ def _yield_clustering_checks(name, clusterer):
 
 def _yield_outliers_checks(name, estimator):
 
-    # checks for all outlier detectors
-    yield check_outliers_fit_predict
+    # checks for outlier detectors that have a fit_predict method
+    if hasattr(estimator, 'fit_predict'):
+        yield check_outliers_fit_predict
 
     # checks for estimators that can be used on a test set
     if hasattr(estimator, 'predict'):
@@ -262,6 +264,7 @@ def _yield_all_checks(name, estimator):
     yield check_fit2d_1feature
     yield check_fit1d
     yield check_get_params_invariance
+    yield check_set_params
     yield check_dict_unchanged
     yield check_dont_overwrite_parameters
 
@@ -341,7 +344,13 @@ def set_checking_parameters(estimator):
         estimator.set_params(n_resampling=5)
     if "n_estimators" in params:
         # especially gradient boosting with default 100
-        estimator.set_params(n_estimators=min(5, estimator.n_estimators))
+        # FIXME: The default number of trees was changed and is set to 'warn'
+        # for some of the ensemble methods. We need to catch this case to avoid
+        # an error during the comparison. To be reverted in 0.22.
+        if estimator.n_estimators == 'warn':
+            estimator.set_params(n_estimators=5)
+        else:
+            estimator.set_params(n_estimators=min(5, estimator.n_estimators))
     if "max_trials" in params:
         # RANSAC
         estimator.set_params(max_trials=10)
@@ -359,6 +368,13 @@ def set_checking_parameters(estimator):
     if estimator.__class__.__name__ == "TheilSenRegressor":
         estimator.max_subpopulation = 100
 
+    if estimator.__class__.__name__ == "IsolationForest":
+        # XXX to be removed in 0.22.
+        # this is used because the old IsolationForest does not
+        # respect the outlier detection API and thus and does not
+        # pass the outlier detection common tests.
+        estimator.set_params(behaviour='new')
+
     if isinstance(estimator, BaseRandomProjection):
         # Due to the jl lemma and often very few samples, the number
         # of components of the random matrix projection will be probably
@@ -373,7 +389,13 @@ def set_checking_parameters(estimator):
 
 
 class NotAnArray(object):
-    " An object that is convertable to an array"
+    """An object that is convertible to an array
+
+    Parameters
+    ----------
+    data : array_like
+        The data.
+    """
 
     def __init__(self, data):
         self.data = data
@@ -416,7 +438,7 @@ def _is_pairwise_metric(estimator):
     out : bool
         True if _pairwise is set to True and False otherwise.
     """
-    metric = getattr(estimator,  "metric", None)
+    metric = getattr(estimator, "metric", None)
 
     return bool(metric == 'precomputed')
 
@@ -554,6 +576,40 @@ def check_sample_weights_list(name, estimator_orig):
         estimator.fit(X, y, sample_weight=sample_weight)
 
 
+@ignore_warnings(category=(DeprecationWarning, FutureWarning))
+def check_sample_weights_invariance(name, estimator_orig):
+    # check that the estimators yield same results for
+    # unit weights and no weights
+    if (has_fit_parameter(estimator_orig, "sample_weight") and
+            not (hasattr(estimator_orig, "_pairwise")
+                 and estimator_orig._pairwise)):
+        # We skip pairwise because the data is not pairwise
+
+        estimator1 = clone(estimator_orig)
+        estimator2 = clone(estimator_orig)
+        set_random_state(estimator1, random_state=0)
+        set_random_state(estimator2, random_state=0)
+
+        X = np.array([[1, 3], [1, 3], [1, 3], [1, 3],
+                      [2, 1], [2, 1], [2, 1], [2, 1],
+                      [3, 3], [3, 3], [3, 3], [3, 3],
+                      [4, 1], [4, 1], [4, 1], [4, 1]], dtype=np.dtype('float'))
+        y = np.array([1, 1, 1, 1, 2, 2, 2, 2,
+                      1, 1, 1, 1, 2, 2, 2, 2], dtype=np.dtype('int'))
+
+        estimator1.fit(X, y=y, sample_weight=np.ones(shape=len(y)))
+        estimator2.fit(X, y=y, sample_weight=None)
+
+        for method in ["predict", "transform"]:
+            if hasattr(estimator_orig, method):
+                X_pred1 = getattr(estimator1, method)(X)
+                X_pred2 = getattr(estimator2, method)(X)
+                assert_allclose(X_pred1, X_pred2,
+                                err_msg="For %s sample_weight=None is not"
+                                        " equivalent to sample_weight=ones"
+                                        % name)
+
+
 @ignore_warnings(category=(DeprecationWarning, FutureWarning, UserWarning))
 def check_dtype_object(name, estimator_orig):
     # check that estimators treat dtype object as numeric if possible
@@ -853,9 +909,6 @@ def check_transformer_general(name, transformer, readonly_memmap=False):
                       random_state=0, n_features=2, cluster_std=0.1)
     X = StandardScaler().fit_transform(X)
     X -= X.min()
-    if name == 'PowerTransformer':
-        # Box-Cox requires positive, non-zero data
-        X += 1
 
     if readonly_memmap:
         X, y = create_memmap_backed_data([X, y])
@@ -981,9 +1034,6 @@ def check_pipeline_consistency(name, estimator_orig):
     X, y = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],
                       random_state=0, n_features=2, cluster_std=0.1)
     X -= X.min()
-    if name == 'PowerTransformer':
-        # Box-Cox requires positive, non-zero data
-        X += 1
     X = pairwise_estimator_convert_X(X, estimator_orig, kernel=rbf_kernel)
     estimator = clone(estimator_orig)
     y = multioutput_estimator_convert_y_2d(estimator, y)
@@ -1045,9 +1095,6 @@ def check_estimators_dtypes(name, estimator_orig):
     methods = ["predict", "transform", "decision_function", "predict_proba"]
 
     for X_train in [X_train_32, X_train_64, X_train_int_64, X_train_int_32]:
-        if name == 'PowerTransformer':
-            # Box-Cox requires positive, non-zero data
-            X_train = np.abs(X_train) + 1
         estimator = clone(estimator_orig)
         set_random_state(estimator, 1)
         estimator.fit(X_train, y)
@@ -1162,11 +1209,15 @@ def check_estimators_pickle(name, estimator_orig):
 
     # some estimators can't do features less than 0
     X -= X.min()
-    if name == 'PowerTransformer':
-        # Box-Cox requires positive, non-zero data
-        X += 1
     X = pairwise_estimator_convert_X(X, estimator_orig, kernel=rbf_kernel)
 
+    # include NaN values when the estimator should deal with them
+    if name in ALLOW_NAN:
+        # set randomly 10 elements to np.nan
+        rng = np.random.RandomState(42)
+        mask = rng.choice(X.size, 10, replace=False)
+        X.reshape(-1)[mask] = np.nan
+
     estimator = clone(estimator_orig)
 
     # some estimators only take multioutputs
@@ -1186,6 +1237,11 @@ def check_estimators_pickle(name, estimator_orig):
         assert_true(b"version" in pickled_estimator)
     unpickled_estimator = pickle.loads(pickled_estimator)
 
+    result = dict()
+    for method in check_methods:
+        if hasattr(estimator, method):
+            result[method] = getattr(estimator, method)(X)
+
     for method in result:
         unpickled_result = getattr(unpickled_estimator, method)(X)
         assert_allclose_dense_sparse(result[method], unpickled_result)
@@ -1495,9 +1551,12 @@ def check_outliers_train(name, estimator_orig, readonly_memmap=True):
     assert_raises(ValueError, estimator.score_samples, X.T)
 
     # contamination parameter (not for OneClassSVM which has the nu parameter)
-    if hasattr(estimator, "contamination"):
+    if (hasattr(estimator, 'contamination')
+            and not hasattr(estimator, 'novelty')):
         # proportion of outliers equal to contamination parameter when not
-        # set to 'auto'
+        # set to 'auto'. This is true for the training set and cannot thus be
+        # checked as follows for estimators with a novelty parameter such as
+        # LocalOutlierFactor (tested in check_outliers_fit_predict)
         contamination = 0.1
         estimator.set_params(contamination=contamination)
         estimator.fit(X)
@@ -1517,9 +1576,6 @@ def check_estimators_fit_returns_self(name, estimator_orig,
     X, y = make_blobs(random_state=0, n_samples=9, n_features=4)
     # some want non-negative input
     X -= X.min()
-    if name == 'PowerTransformer':
-        # Box-Cox requires positive, non-zero data
-        X += 1
     X = pairwise_estimator_convert_X(X, estimator_orig)
 
     estimator = clone(estimator_orig)
@@ -1544,24 +1600,25 @@ def check_estimators_unfitted(name, estimator_orig):
     # Common test for Regressors, Classifiers and Outlier detection estimators
     X, y = _boston_subset()
 
-    est = clone(estimator_orig)
+    estimator = clone(estimator_orig)
 
     msg = "fit"
-    if hasattr(est, 'predict'):
+
+    if hasattr(estimator, 'predict'):
         assert_raise_message((AttributeError, ValueError), msg,
-                             est.predict, X)
+                             estimator.predict, X)
 
-    if hasattr(est, 'decision_function'):
+    if hasattr(estimator, 'decision_function'):
         assert_raise_message((AttributeError, ValueError), msg,
-                             est.decision_function, X)
+                             estimator.decision_function, X)
 
-    if hasattr(est, 'predict_proba'):
+    if hasattr(estimator, 'predict_proba'):
         assert_raise_message((AttributeError, ValueError), msg,
-                             est.predict_proba, X)
+                             estimator.predict_proba, X)
 
-    if hasattr(est, 'predict_log_proba'):
+    if hasattr(estimator, 'predict_log_proba'):
         assert_raise_message((AttributeError, ValueError), msg,
-                             est.predict_log_proba, X)
+                             estimator.predict_log_proba, X)
 
 
 @ignore_warnings(category=(DeprecationWarning, FutureWarning))
@@ -1880,9 +1937,6 @@ def check_estimators_overwrite_params(name, estimator_orig):
     X, y = make_blobs(random_state=0, n_samples=9)
     # some want non-negative input
     X -= X.min()
-    if name == 'PowerTransformer':
-        # Box-Cox requires positive, non-zero data
-        X += 1
     X = pairwise_estimator_convert_X(X, estimator_orig, kernel=rbf_kernel)
     estimator = clone(estimator_orig)
     y = multioutput_estimator_convert_y_2d(estimator, y)
@@ -1921,6 +1975,11 @@ def check_no_attributes_set_in_init(name, estimator):
         return
 
     init_params = _get_args(type(estimator).__init__)
+    if IS_PYPY:
+        # __init__ signature has additional objects in PyPy
+        for key in ['obj']:
+            if key in init_params:
+                init_params.remove(key)
     parents_init_params = [param for params_parent in
                            (_get_args(parent) for parent in
                             type(estimator).__mro__)
@@ -2174,6 +2233,59 @@ def transform(self, X):
                     shallow_params.items()))
 
 
+@ignore_warnings(category=(DeprecationWarning, FutureWarning))
+def check_set_params(name, estimator_orig):
+    # Check that get_params() returns the same thing
+    # before and after set_params() with some fuzz
+    estimator = clone(estimator_orig)
+
+    orig_params = estimator.get_params(deep=False)
+    msg = ("get_params result does not match what was passed to set_params")
+
+    estimator.set_params(**orig_params)
+    curr_params = estimator.get_params(deep=False)
+    assert_equal(set(orig_params.keys()), set(curr_params.keys()), msg)
+    for k, v in curr_params.items():
+        assert orig_params[k] is v, msg
+
+    # some fuzz values
+    test_values = [-np.inf, np.inf, None]
+
+    test_params = deepcopy(orig_params)
+    for param_name in orig_params.keys():
+        default_value = orig_params[param_name]
+        for value in test_values:
+            test_params[param_name] = value
+            try:
+                estimator.set_params(**test_params)
+            except (TypeError, ValueError) as e:
+                e_type = e.__class__.__name__
+                # Exception occurred, possibly parameter validation
+                warnings.warn("{} occurred during set_params. "
+                              "It is recommended to delay parameter "
+                              "validation until fit.".format(e_type))
+
+                change_warning_msg = "Estimator's parameters changed after " \
+                                     "set_params raised {}".format(e_type)
+                params_before_exception = curr_params
+                curr_params = estimator.get_params(deep=False)
+                try:
+                    assert_equal(set(params_before_exception.keys()),
+                                 set(curr_params.keys()))
+                    for k, v in curr_params.items():
+                        assert params_before_exception[k] is v
+                except AssertionError:
+                    warnings.warn(change_warning_msg)
+            else:
+                curr_params = estimator.get_params(deep=False)
+                assert_equal(set(test_params.keys()),
+                             set(curr_params.keys()),
+                             msg)
+                for k, v in curr_params.items():
+                    assert test_params[k] is v, msg
+        test_params[param_name] = default_value
+
+
 @ignore_warnings(category=(DeprecationWarning, FutureWarning))
 def check_classifiers_regression_target(name, estimator_orig):
     # Check if classifier throws an exception when fed regression targets
@@ -2220,7 +2332,9 @@ def check_outliers_fit_predict(name, estimator_orig):
     assert y_pred.dtype.kind == 'i'
     assert_array_equal(np.unique(y_pred), np.array([-1, 1]))
 
-    # check fit_predict = fit.predict when possible
+    # check fit_predict = fit.predict when the estimator has both a predict and
+    # a fit_predict method. recall that it is already assumed here that the
+    # estimator has a fit_predict method
     if hasattr(estimator, 'predict'):
         y_pred_2 = estimator.fit(X).predict(X)
         assert_array_equal(y_pred, y_pred_2)
diff --git a/sklearn/utils/extmath.py b/sklearn/utils/extmath.py
index 218733145a0d..95e464f07164 100644
--- a/sklearn/utils/extmath.py
+++ b/sklearn/utils/extmath.py
@@ -40,8 +40,17 @@ def norm(x):
 def squared_norm(x):
     """Squared Euclidean or Frobenius norm of x.
 
-    Returns the Euclidean norm when x is a vector, the Frobenius norm when x
-    is a matrix (2-d array). Faster than norm(x) ** 2.
+    Faster than norm(x) ** 2.
+
+    Parameters
+    ----------
+    x : array_like
+
+    Returns
+    -------
+    float
+        The Euclidean norm when x is a vector, the Frobenius norm when x
+        is a matrix (2-d array).
     """
     x = np.ravel(x, order='K')
     if np.issubdtype(x.dtype, np.integer):
@@ -58,6 +67,18 @@ def row_norms(X, squared=False):
     matrices and does not create an X.shape-sized temporary.
 
     Performs no input validation.
+
+    Parameters
+    ----------
+    X : array_like
+        The input array
+    squared : bool, optional (default = False)
+        If True, return squared norms.
+
+    Returns
+    -------
+    array_like
+        The row-wise (squared) Euclidean norm of X.
     """
     if sparse.issparse(X):
         if not isinstance(X, sparse.csr_matrix):
@@ -76,6 +97,11 @@ def fast_logdet(A):
 
     Equivalent to : np.log(nl.det(A)) but more robust.
     It returns -Inf if det(A) is non positive or is not defined.
+
+    Parameters
+    ----------
+    A : array_like
+        The matrix
     """
     sign, ld = np.linalg.slogdet(A)
     if not sign > 0:
@@ -102,7 +128,15 @@ def fast_dot(a, b, out=None):
 def density(w, **kwargs):
     """Compute density of a sparse vector
 
-    Return a value between 0 and 1
+    Parameters
+    ----------
+    w : array_like
+        The sparse vector
+
+    Returns
+    -------
+    float
+        The density of w, between 0 and 1
     """
     if hasattr(w, "toarray"):
         d = float(w.nnz) / (w.shape[0] * w.shape[1])
@@ -367,7 +401,7 @@ def logsumexp(arr, axis=0):
     >>> a = np.arange(10)
     >>> np.log(np.sum(np.exp(a)))
     9.458...
-    >>> logsumexp(a)
+    >>> logsumexp(a)  # doctest: +SKIP
     9.458...
     """
     return scipy_logsumexp(arr, axis)
@@ -428,7 +462,7 @@ def weighted_mode(a, w, axis=0):
         w = np.asarray(w)
 
     if a.shape != w.shape:
-        w = np.zeros(a.shape, dtype=w.dtype) + w
+        w = np.full(a.shape, w, dtype=w.dtype)
 
     scores = np.unique(np.ravel(a))       # get ALL unique values
     testshape = list(a.shape)
@@ -509,7 +543,12 @@ def svd_flip(u, v, u_based_decision=True):
 
     Parameters
     ----------
-    u, v : ndarray
+    u : ndarray
+        u and v are the output of `linalg.svd` or
+        `sklearn.utils.extmath.randomized_svd`, with matching inner dimensions
+        so one can compute `np.dot(u * s, v)`.
+
+    v : ndarray
         u and v are the output of `linalg.svd` or
         `sklearn.utils.extmath.randomized_svd`, with matching inner dimensions
         so one can compute `np.dot(u * s, v)`.
@@ -624,6 +663,15 @@ def safe_min(X):
 
     Adapated from http://stackoverflow.com/q/13426580
 
+    Parameters
+    ----------
+    X : array_like
+        The input array or sparse matrix
+
+    Returns
+    -------
+    Float
+        The min value of X
     """
     if sparse.issparse(X):
         if len(X.data) == 0:
@@ -635,7 +683,25 @@ def safe_min(X):
 
 
 def make_nonnegative(X, min_value=0):
-    """Ensure `X.min()` >= `min_value`."""
+    """Ensure `X.min()` >= `min_value`.
+
+    Parameters
+    ----------
+    X : array_like
+        The matrix to make non-negative
+    min_value : float
+        The threshold value
+
+    Returns
+    -------
+    array_like
+        The thresholded array
+
+    Raises
+    ------
+    ValueError
+        When X is sparse
+    """
     min_ = safe_min(X)
     if min_ < min_value:
         if sparse.issparse(X):
diff --git a/sklearn/utils/fixes.py b/sklearn/utils/fixes.py
index 0bc55c782989..070afbdbb952 100644
--- a/sklearn/utils/fixes.py
+++ b/sklearn/utils/fixes.py
@@ -199,7 +199,16 @@ def _argmax(arr_or_matrix, axis=None):
 
 
 def parallel_helper(obj, methodname, *args, **kwargs):
-    """Workaround for Python 2 limitations of pickling instance methods"""
+    """Workaround for Python 2 limitations of pickling instance methods
+
+    Parameters
+    ----------
+    obj
+    methodname
+    *args
+    **kwargs
+
+    """
     return getattr(obj, methodname)(*args, **kwargs)
 
 
@@ -282,6 +291,19 @@ def nanpercentile(a, q):
     from numpy import nanpercentile  # noqa
 
 
+if np_version < (1, 9):
+    def nanmedian(a, axis=None):
+        if axis is None:
+            data = a.reshape(-1)
+            return np.median(np.compress(~np.isnan(data), data))
+        else:
+            data = a.T if not axis else a
+            return np.array([np.median(np.compress(~np.isnan(row), row))
+                             for row in data])
+else:
+    from numpy import nanmedian  # noqa
+
+
 # Fix for behavior inconsistency on numpy.equal for object dtypes.
 # For numpy versions < 1.13, numpy.equal tests element-wise identity of objects
 # instead of equality. This fix returns the mask of NaNs in an array of
@@ -297,3 +319,16 @@ def _object_dtype_isnan(X):
 else:
     def _object_dtype_isnan(X):
         return np.frompyfunc(lambda x: x != x, 1, 1)(X).astype(bool)
+
+
+# To be removed once this fix is included in six
+try:
+    from collections.abc import Sequence as _Sequence  # noqa
+    from collections.abc import Iterable as _Iterable  # noqa
+    from collections.abc import Mapping as _Mapping  # noqa
+    from collections.abc import Sized as _Sized  # noqa
+except ImportError:  # python <3.3
+    from collections import Sequence as _Sequence  # noqa
+    from collections import Iterable as _Iterable  # noqa
+    from collections import Mapping as _Mapping  # noqa
+    from collections import Sized as _Sized  # noqa
diff --git a/sklearn/utils/metaestimators.py b/sklearn/utils/metaestimators.py
index f20f51889191..49b059b32459 100644
--- a/sklearn/utils/metaestimators.py
+++ b/sklearn/utils/metaestimators.py
@@ -41,7 +41,10 @@ def _set_params(self, attr, **params):
         if attr in params:
             setattr(self, attr, params.pop(attr))
         # 2. Step replacement
-        names, _ = zip(*getattr(self, attr))
+        items = getattr(self, attr)
+        names = []
+        if items:
+            names, _ = zip(*items)
         for name in list(six.iterkeys(params)):
             if '__' not in name and name in names:
                 self._replace_estimator(attr, name, params.pop(name))
diff --git a/sklearn/utils/mocking.py b/sklearn/utils/mocking.py
index 06d5a7cbd367..db2e2ef31936 100644
--- a/sklearn/utils/mocking.py
+++ b/sklearn/utils/mocking.py
@@ -6,6 +6,11 @@
 
 
 class ArraySlicingWrapper(object):
+    """
+    Parameters
+    ----------
+    array
+    """
     def __init__(self, array):
         self.array = array
 
@@ -14,8 +19,12 @@ def __getitem__(self, aslice):
 
 
 class MockDataFrame(object):
-
-    # have shape an length but don't support indexing.
+    """
+    Parameters
+    ----------
+    array
+    """
+    # have shape and length but don't support indexing.
     def __init__(self, array):
         self.array = array
         self.values = array
@@ -46,6 +55,13 @@ class CheckingClassifier(BaseEstimator, ClassifierMixin):
     Checks some property of X and y in fit / predict.
     This allows testing whether pipelines / cross-validation or metaestimators
     changed the input.
+
+    Parameters
+    ----------
+    check_y
+    check_X
+    foo_param
+    expected_fit_params
     """
     def __init__(self, check_y=None, check_X=None, foo_param=0,
                  expected_fit_params=None):
@@ -55,6 +71,22 @@ def __init__(self, check_y=None, check_X=None, foo_param=0,
         self.expected_fit_params = expected_fit_params
 
     def fit(self, X, y, **fit_params):
+        """
+        Fit classifier
+
+        Parameters
+        ----------
+        X : array-like, shape = [n_samples, n_features]
+            Training vector, where n_samples is the number of samples and
+            n_features is the number of features.
+
+        y : array-like, shape = [n_samples] or [n_samples, n_output], optional
+            Target relative to X for classification or regression;
+            None for unsupervised learning.
+
+        **fit_params : dict of string -> object
+            Parameters passed to the ``fit`` method of the estimator
+        """
         assert_true(len(X) == len(y))
         if self.check_X is not None:
             assert_true(self.check_X(X))
@@ -74,11 +106,27 @@ def fit(self, X, y, **fit_params):
         return self
 
     def predict(self, T):
+        """
+        Parameters
+        -----------
+        T : indexable, length n_samples
+        """
         if self.check_X is not None:
             assert_true(self.check_X(T))
         return self.classes_[np.zeros(_num_samples(T), dtype=np.int)]
 
     def score(self, X=None, Y=None):
+        """
+        Parameters
+        ----------
+        X : array-like, shape = [n_samples, n_features]
+            Input data, where n_samples is the number of samples and
+            n_features is the number of features.
+
+        Y : array-like, shape = [n_samples] or [n_samples, n_output], optional
+            Target relative to X for classification or regression;
+            None for unsupervised learning.
+        """
         if self.foo_param > 1:
             score = 1.
         else:
diff --git a/sklearn/utils/multiclass.py b/sklearn/utils/multiclass.py
index 4adefd4ff04f..f4d28ec227ba 100644
--- a/sklearn/utils/multiclass.py
+++ b/sklearn/utils/multiclass.py
@@ -7,7 +7,6 @@
 
 """
 from __future__ import division
-from collections import Sequence
 from itertools import chain
 
 from scipy.sparse import issparse
@@ -18,10 +17,10 @@
 import numpy as np
 
 from ..externals.six import string_types
+from ..utils.fixes import _Sequence as Sequence
 from .validation import check_array
 
 
-
 def _unique_multiclass(y):
     if hasattr(y, '__array__'):
         return np.unique(np.asarray(y))
diff --git a/sklearn/utils/random.py b/sklearn/utils/random.py
index 044b8c70d8b7..24ddf4680c74 100644
--- a/sklearn/utils/random.py
+++ b/sklearn/utils/random.py
@@ -158,7 +158,7 @@ def random_choice_csc(n_samples, classes, class_probability=None,
         else:
             class_prob_j = np.asarray(class_probability[j])
 
-        if np.sum(class_prob_j) != 1.0:
+        if not np.isclose(np.sum(class_prob_j), 1.0):
             raise ValueError("Probability array at index {0} does not sum to "
                              "one".format(j))
 
diff --git a/sklearn/utils/seq_dataset.pyx b/sklearn/utils/seq_dataset.pyx
index 94e4868eef5f..b4e099774493 100644
--- a/sklearn/utils/seq_dataset.pyx
+++ b/sklearn/utils/seq_dataset.pyx
@@ -15,12 +15,44 @@ np.import_array()
 
 
 cdef class SequentialDataset:
-    """Base class for datasets with sequential data access. """
+    """Base class for datasets with sequential data access.
+
+    SequentialDataset is used to iterate over the rows of a matrix X and
+    corresponding target values y, i.e. to iterate over samples.
+    There are two methods to get the next sample:
+        - next : Iterate sequentially (optionally randomized)
+        - random : Iterate randomly (with replacement)
+
+    Attributes
+    ----------
+    index : np.ndarray
+        Index array for fast shuffling.
+
+    index_data_ptr : int
+        Pointer to the index array.
+
+    current_index : int
+        Index of current sample in ``index``.
+        The index of current sample in the data is given by
+        index_data_ptr[current_index].
+
+    n_samples : Py_ssize_t
+        Number of samples in the dataset.
+
+    seed : np.uint32_t
+        Seed used for random sampling.
+
+    """
 
     cdef void next(self, double **x_data_ptr, int **x_ind_ptr,
                    int *nnz, double *y, double *sample_weight) nogil:
         """Get the next example ``x`` from the dataset.
 
+        This method gets the next sample looping sequentially over all samples.
+        The order can be shuffled with the method ``shuffle``.
+        Shuffling once before iterating over all samples corresponds to a
+        random draw without replacement. It is used for instance in SGD solver.
+
         Parameters
         ----------
         x_data_ptr : double**
@@ -49,6 +81,10 @@ cdef class SequentialDataset:
                     int *nnz, double *y, double *sample_weight) nogil:
         """Get a random example ``x`` from the dataset.
 
+        This method gets next sample chosen randomly over a uniform
+        distribution. It corresponds to a random draw with replacement.
+        It is used for instance in SAG solver.
+
         Parameters
         ----------
         x_data_ptr : double**
@@ -71,8 +107,8 @@ cdef class SequentialDataset:
 
         Returns
         -------
-        index : int
-            The index sampled
+        current_index : int
+            Index of current sample.
         """
         cdef int current_index = self._get_random_index()
         self._sample(x_data_ptr, x_ind_ptr, nnz, y, sample_weight,
diff --git a/sklearn/utils/sparsefuncs_fast.pyx b/sklearn/utils/sparsefuncs_fast.pyx
index 7de906cdaa14..d49c0683ae86 100644
--- a/sklearn/utils/sparsefuncs_fast.pyx
+++ b/sklearn/utils/sparsefuncs_fast.pyx
@@ -308,7 +308,7 @@ def _incr_mean_variance_axis0(np.ndarray[floating, ndim=1] X_data,
         np.ndarray[np.int64_t, ndim=1] counts_nan
 
     # Obtain new stats first
-    new_n = np.ones(n_features, dtype=np.int64) * n_samples
+    new_n = np.full(n_features, n_samples, dtype=np.int64)
     updated_n = np.zeros_like(new_n, dtype=np.int64)
     last_over_new_n = np.zeros_like(new_n, dtype=dtype)
 
diff --git a/sklearn/utils/stats.py b/sklearn/utils/stats.py
index 43f37bb95a6b..82b8912b7882 100644
--- a/sklearn/utils/stats.py
+++ b/sklearn/utils/stats.py
@@ -22,4 +22,6 @@ def _weighted_percentile(array, sample_weight, percentile=50):
     weight_cdf = stable_cumsum(sample_weight[sorted_idx])
     percentile_idx = np.searchsorted(
         weight_cdf, (percentile / 100.) * weight_cdf[-1])
+    # in rare cases, percentile_idx equals to len(sorted_idx)
+    percentile_idx = np.clip(percentile_idx, 0, len(sorted_idx)-1)
     return array[sorted_idx[percentile_idx]]
diff --git a/sklearn/utils/testing.py b/sklearn/utils/testing.py
index c67a314e2fc5..c28d729883b4 100644
--- a/sklearn/utils/testing.py
+++ b/sklearn/utils/testing.py
@@ -47,7 +47,7 @@
 from sklearn.base import BaseEstimator
 from sklearn.externals import joblib
 from sklearn.utils.fixes import signature
-from sklearn.utils import deprecated
+from sklearn.utils import deprecated, IS_PYPY
 
 
 additional_names_in_all = []
@@ -233,6 +233,12 @@ def assert_warns_div0(func, *args, **kw):
     """Assume that numpy's warning for divide by zero is raised
 
     Handles the case of platforms that do not support warning on divide by zero
+
+    Parameters
+    ----------
+    func
+    *args
+    **kw
     """
 
     with np.errstate(divide='warn', invalid='warn'):
@@ -248,6 +254,13 @@ def assert_warns_div0(func, *args, **kw):
 
 # To remove when we support numpy 1.7
 def assert_no_warnings(func, *args, **kw):
+    """
+    Parameters
+    ----------
+    func
+    *args
+    **kw
+    """
     # very important to avoid uncontrolled state propagation
     clean_warning_registry()
     with warnings.catch_warnings(record=True) as w:
@@ -275,6 +288,8 @@ def ignore_warnings(obj=None, category=Warning):
 
     Parameters
     ----------
+    obj : callable or None
+        callable where you want to ignore the warnings.
     category : warning class, defaults to Warning.
         The category to filter. If Warning, all categories will be muted.
 
@@ -290,7 +305,17 @@ def ignore_warnings(obj=None, category=Warning):
     >>> ignore_warnings(nasty_warn)()
     42
     """
-    if callable(obj):
+    if isinstance(obj, type) and issubclass(obj, Warning):
+        # Avoid common pitfall of passing category as the first positional
+        # argument which result in the test not being run
+        warning_name = obj.__name__
+        raise ValueError(
+            "'obj' should be a callable where you want to ignore warnings. "
+            "You passed a warning class instead: 'obj={warning_name}'. "
+            "If you want to pass a warning class to ignore_warnings, "
+            "you should use 'category={warning_name}'".format(
+                warning_name=warning_name))
+    elif callable(obj):
         return _IgnoreWarnings(category=category)(obj)
     else:
         return _IgnoreWarnings(category=category)
@@ -443,9 +468,13 @@ def assert_allclose_dense_sparse(x, y, rtol=1e-07, atol=1e-9, err_msg=''):
                          " not a sparse matrix and an array.")
 
 
+@deprecated('deprecated in version 0.20 to be removed in version 0.22')
 def fake_mldata(columns_dict, dataname, matfile, ordering=None):
     """Create a fake mldata data set.
 
+    .. deprecated:: 0.20
+        Will be removed in version 0.22
+
     Parameters
     ----------
     columns_dict : dict, keys=str, values=ndarray
@@ -483,6 +512,7 @@ def fake_mldata(columns_dict, dataname, matfile, ordering=None):
     scipy.io.savemat(matfile, datasets, oned_as='column')
 
 
+@deprecated('deprecated in version 0.20 to be removed in version 0.22')
 class mock_mldata_urlopen(object):
     """Object that mocks the urlopen function to fake requests to mldata.
 
@@ -490,6 +520,9 @@ class mock_mldata_urlopen(object):
     creates a fake dataset in a StringIO object and returns it. Otherwise, it
     raises an HTTPError.
 
+    .. deprecated:: 0.20
+        Will be removed in version 0.22
+
     Parameters
     ----------
     mock_datasets : dict
@@ -503,6 +536,12 @@ def __init__(self, mock_datasets):
         self.mock_datasets = mock_datasets
 
     def __call__(self, urlname):
+        """
+        Parameters
+        ----------
+        urlname : string
+            The url
+        """
         dataset_name = urlname.split('/')[-1]
         if dataset_name in self.mock_datasets:
             resource_name = '_' + dataset_name
@@ -523,6 +562,16 @@ def __call__(self, urlname):
 
 
 def install_mldata_mock(mock_datasets):
+    """
+    Parameters
+    ----------
+    mock_datasets : dict
+        A dictionary of {dataset_name: data_dict}, or
+        {dataset_name: (data_dict, ordering). `data_dict` itself is a
+        dictionary of {column_name: data_array}, and `ordering` is a list of
+        column_names to determine the ordering in the data set (see
+        :func:`fake_mldata` for details).
+    """
     # Lazy import to avoid mutually recursive imports
     from sklearn import datasets
     datasets.mldata.urlopen = mock_mldata_urlopen(mock_datasets)
@@ -541,7 +590,8 @@ def uninstall_mldata_mock():
                    "RFE", "RFECV", "BaseEnsemble", "ClassifierChain",
                    "RegressorChain"]
 # estimators that there is no way to default-construct sensibly
-OTHER = ["Pipeline", "FeatureUnion", "GridSearchCV", "RandomizedSearchCV",
+OTHER = ["Pipeline", "FeatureUnion",
+         "GridSearchCV", "RandomizedSearchCV",
          "SelectFromModel", "ColumnTransformer"]
 
 # some strange ones
@@ -585,9 +635,6 @@ def all_estimators(include_meta_estimators=False,
         not be default-constructed sensibly. These are currently
         Pipeline, FeatureUnion and GridSearchCV
 
-    include_dont_test : boolean, default=False
-        Whether to include "special" label estimator or test processors.
-
     type_filter : string, list of string,  or None, default=None
         Which kind of estimators should be returned. If None, no filter is
         applied and all estimators are returned.  Possible values are
@@ -595,6 +642,9 @@ def all_estimators(include_meta_estimators=False,
         estimators only of these specific types, or a list of these to
         get the estimators that fit at least one of the types.
 
+    include_dont_test : boolean, default=False
+        Whether to include "special" label estimator or test processors.
+
     Returns
     -------
     estimators : list of tuples
@@ -613,7 +663,10 @@ def is_abstract(c):
     path = sklearn.__path__
     for importer, modname, ispkg in pkgutil.walk_packages(
             path=path, prefix='sklearn.', onerror=lambda x: None):
-        if (".tests." in modname):
+        if ".tests." in modname:
+            continue
+        if IS_PYPY and ('_svmlight_format' in modname or
+                        'feature_extraction._hashing' in modname):
             continue
         module = __import__(modname, fromlist="dummy")
         classes = inspect.getmembers(module, inspect.isclass)
@@ -665,13 +718,28 @@ def is_abstract(c):
 
 def set_random_state(estimator, random_state=0):
     """Set random state of an estimator if it has the `random_state` param.
+
+    Parameters
+    ----------
+    estimator : object
+        The estimator
+    random_state : int, RandomState instance or None, optional, default=0
+        Pseudo random number generator state.  If int, random_state is the seed
+        used by the random number generator; If RandomState instance,
+        random_state is the random number generator; If None, the random number
+        generator is the RandomState instance used by `np.random`.
     """
     if "random_state" in estimator.get_params():
         estimator.set_params(random_state=random_state)
 
 
 def if_matplotlib(func):
-    """Test decorator that skips test if matplotlib not installed."""
+    """Test decorator that skips test if matplotlib not installed.
+
+    Parameters
+    ----------
+    func
+    """
     @wraps(func)
     def run_test(*args, **kwargs):
         try:
@@ -694,6 +762,8 @@ def run_test(*args, **kwargs):
                                        reason='skipped on 32bit platforms')
     skip_travis = pytest.mark.skipif(os.environ.get('TRAVIS') == 'true',
                                      reason='skip on travis')
+    fails_if_pypy = pytest.mark.xfail(IS_PYPY, raises=NotImplementedError,
+                                      reason='not compatible with PyPy')
 
     #  Decorator for tests involving both BLAS calls and multiprocessing.
     #
@@ -758,6 +828,12 @@ def _delete_folder(folder_path, warn=False):
 
 
 class TempMemmap(object):
+    """
+    Parameters
+    ----------
+    data
+    mmap_mode
+    """
     def __init__(self, data, mmap_mode='r'):
         self.mmap_mode = mmap_mode
         self.data = data
@@ -772,6 +848,13 @@ def __exit__(self, exc_type, exc_val, exc_tb):
 
 
 def create_memmap_backed_data(data, mmap_mode='r', return_folder=False):
+    """
+    Parameters
+    ----------
+    data
+    mmap_mode
+    return_folder
+    """
     temp_folder = tempfile.mkdtemp(prefix='sklearn_testing_')
     atexit.register(functools.partial(_delete_folder, temp_folder, warn=True))
     filename = op.join(temp_folder, 'data.pkl')
@@ -865,6 +948,12 @@ def check_docstring_parameters(func, doc=None, ignore=None, class_name=None):
     # Don't check docstring for property-functions
     if inspect.isdatadescriptor(func):
         return incorrect
+    # Don't check docstring for setup / teardown pytest functions
+    if func_name.split('.')[-1] in ('setup_module', 'teardown_module'):
+        return incorrect
+    # Dont check estimator_checks module
+    if func_name.split('.')[2] == 'estimator_checks':
+        return incorrect
     args = list(filter(lambda x: x not in ignore, _get_args(func)))
     # drop self
     if len(args) > 0 and args[0] == 'self':
diff --git a/sklearn/utils/tests/test_estimator_checks.py b/sklearn/utils/tests/test_estimator_checks.py
index 9b4a9c4f87c1..bf8412b3e527 100644
--- a/sklearn/utils/tests/test_estimator_checks.py
+++ b/sklearn/utils/tests/test_estimator_checks.py
@@ -10,7 +10,8 @@
 from sklearn.base import BaseEstimator, ClassifierMixin
 from sklearn.utils import deprecated
 from sklearn.utils.testing import (assert_raises_regex, assert_true,
-                                   assert_equal, ignore_warnings)
+                                   assert_equal, ignore_warnings,
+                                   assert_warns)
 from sklearn.utils.estimator_checks import check_estimator
 from sklearn.utils.estimator_checks import set_random_state
 from sklearn.utils.estimator_checks import set_checking_parameters
@@ -86,6 +87,61 @@ def fit(self, X, y=None):
         return self
 
 
+class RaisesErrorInSetParams(BaseEstimator):
+    def __init__(self, p=0):
+        self.p = p
+
+    def set_params(self, **kwargs):
+        if 'p' in kwargs:
+            p = kwargs.pop('p')
+            if p < 0:
+                raise ValueError("p can't be less than 0")
+            self.p = p
+        return super(RaisesErrorInSetParams, self).set_params(**kwargs)
+
+    def fit(self, X, y=None):
+        X, y = check_X_y(X, y)
+        return self
+
+
+class ModifiesValueInsteadOfRaisingError(BaseEstimator):
+    def __init__(self, p=0):
+        self.p = p
+
+    def set_params(self, **kwargs):
+        if 'p' in kwargs:
+            p = kwargs.pop('p')
+            if p < 0:
+                p = 0
+            self.p = p
+        return super(ModifiesValueInsteadOfRaisingError,
+                     self).set_params(**kwargs)
+
+    def fit(self, X, y=None):
+        X, y = check_X_y(X, y)
+        return self
+
+
+class ModifiesAnotherValue(BaseEstimator):
+    def __init__(self, a=0, b='method1'):
+        self.a = a
+        self.b = b
+
+    def set_params(self, **kwargs):
+        if 'a' in kwargs:
+            a = kwargs.pop('a')
+            self.a = a
+            if a is None:
+                kwargs.pop('b')
+                self.b = 'method2'
+        return super(ModifiesAnotherValue,
+                     self).set_params(**kwargs)
+
+    def fit(self, X, y=None):
+        X, y = check_X_y(X, y)
+        return self
+
+
 class NoCheckinPredict(BaseBadClassifier):
     def fit(self, X, y):
         X, y = check_X_y(X, y)
@@ -219,6 +275,13 @@ def test_check_estimator():
     msg = "it does not implement a 'get_params' methods"
     assert_raises_regex(TypeError, msg, check_estimator, object)
     assert_raises_regex(TypeError, msg, check_estimator, object())
+    # check that values returned by get_params match set_params
+    msg = "get_params result does not match what was passed to set_params"
+    assert_raises_regex(AssertionError, msg, check_estimator,
+                        ModifiesValueInsteadOfRaisingError())
+    assert_warns(UserWarning, check_estimator, RaisesErrorInSetParams())
+    assert_raises_regex(AssertionError, msg, check_estimator,
+                        ModifiesAnotherValue())
     # check that we have a fit method
     msg = "object has no attribute 'fit'"
     assert_raises_regex(AttributeError, msg, check_estimator, BaseEstimator)
@@ -317,25 +380,25 @@ def test_check_estimator_clones():
     for Estimator in [GaussianMixture, LinearRegression,
                       RandomForestClassifier, NMF, SGDClassifier,
                       MiniBatchKMeans]:
-        with ignore_warnings(category=FutureWarning):
+        with ignore_warnings(category=(FutureWarning, DeprecationWarning)):
             # when 'est = SGDClassifier()'
             est = Estimator()
-        set_checking_parameters(est)
-        set_random_state(est)
-        # without fitting
-        old_hash = joblib.hash(est)
-        check_estimator(est)
+            set_checking_parameters(est)
+            set_random_state(est)
+            # without fitting
+            old_hash = joblib.hash(est)
+            check_estimator(est)
         assert_equal(old_hash, joblib.hash(est))
 
-        with ignore_warnings(category=FutureWarning):
+        with ignore_warnings(category=(FutureWarning, DeprecationWarning)):
             # when 'est = SGDClassifier()'
             est = Estimator()
-        set_checking_parameters(est)
-        set_random_state(est)
-        # with fitting
-        est.fit(iris.data + 10, iris.target)
-        old_hash = joblib.hash(est)
-        check_estimator(est)
+            set_checking_parameters(est)
+            set_random_state(est)
+            # with fitting
+            est.fit(iris.data + 10, iris.target)
+            old_hash = joblib.hash(est)
+            check_estimator(est)
         assert_equal(old_hash, joblib.hash(est))
 
 
diff --git a/sklearn/utils/tests/test_extmath.py b/sklearn/utils/tests/test_extmath.py
index ee08e016abe6..3de67e5a2130 100644
--- a/sklearn/utils/tests/test_extmath.py
+++ b/sklearn/utils/tests/test_extmath.py
@@ -500,7 +500,7 @@ def test_incremental_variance_update_formulas():
 
     old_means = X1.mean(axis=0)
     old_variances = X1.var(axis=0)
-    old_sample_count = np.ones(X1.shape[1], dtype=np.int32) * X1.shape[0]
+    old_sample_count = np.full(X1.shape[1], X1.shape[0], dtype=np.int32)
     final_means, final_variances, final_count = \
         _incremental_mean_and_var(X2, old_means, old_variances,
                                   old_sample_count)
@@ -575,8 +575,8 @@ def naive_mean_variance_update(x, last_mean, last_variance,
     n_samples = 10000
     x1 = np.array(1e8, dtype=np.float64)
     x2 = np.log(1e-5, dtype=np.float64)
-    A0 = x1 * np.ones((n_samples // 2, n_features), dtype=np.float64)
-    A1 = x2 * np.ones((n_samples // 2, n_features), dtype=np.float64)
+    A0 = np.full((n_samples // 2, n_features), x1, dtype=np.float64)
+    A1 = np.full((n_samples // 2, n_features), x2, dtype=np.float64)
     A = np.vstack((A0, A1))
 
     # Older versions of numpy have different precision
@@ -603,7 +603,7 @@ def naive_mean_variance_update(x, last_mean, last_variance,
 
     # Robust implementation: <tol (177)
     mean, var = A0[0, :], np.zeros(n_features)
-    n = np.ones(n_features, dtype=np.int32) * (n_samples // 2)
+    n = np.full(n_features, n_samples // 2, dtype=np.int32)
     for i in range(A1.shape[0]):
         mean, var, n = \
             _incremental_mean_and_var(A1[i, :].reshape((1, A1.shape[1])),
@@ -630,8 +630,8 @@ def test_incremental_variance_ddof():
                 incremental_variances = batch.var(axis=0)
                 # Assign this twice so that the test logic is consistent
                 incremental_count = batch.shape[0]
-                sample_count = (np.ones(batch.shape[1], dtype=np.int32) *
-                                batch.shape[0])
+                sample_count = np.full(batch.shape[1], batch.shape[0],
+                                       dtype=np.int32)
             else:
                 result = _incremental_mean_and_var(
                     batch, incremental_means, incremental_variances,
diff --git a/sklearn/utils/tests/test_fixes.py b/sklearn/utils/tests/test_fixes.py
index 8a55f74a4f6e..92f954439f79 100644
--- a/sklearn/utils/tests/test_fixes.py
+++ b/sklearn/utils/tests/test_fixes.py
@@ -14,6 +14,7 @@
 
 from sklearn.utils.fixes import divide
 from sklearn.utils.fixes import MaskedArray
+from sklearn.utils.fixes import nanmedian
 from sklearn.utils.fixes import nanpercentile
 
 
@@ -31,6 +32,22 @@ def test_masked_array_obj_dtype_pickleable():
         assert_array_equal(marr.mask, marr_pickled.mask)
 
 
+@pytest.mark.parametrize(
+    "axis, expected_median",
+    [(None, 4.0),
+     (0, np.array([1., 3.5, 3.5, 4., 7., np.nan])),
+     (1, np.array([1., 6.]))]
+)
+def test_nanmedian(axis, expected_median):
+    X = np.array([[1, 1, 1, 2, np.nan, np.nan],
+                  [np.nan, 6, 6, 6, 7, np.nan]])
+    median = nanmedian(X, axis=axis)
+    if axis is None:
+        assert median == pytest.approx(expected_median)
+    else:
+        assert_allclose(median, expected_median)
+
+
 @pytest.mark.parametrize(
     "a, q, expected_percentile",
     [(np.array([1, 2, 3, np.nan]), [0, 50, 100], np.array([1., 2., 3.])),
diff --git a/sklearn/utils/tests/test_murmurhash.py b/sklearn/utils/tests/test_murmurhash.py
index f51c5f7e26c3..d59ec6cecad7 100644
--- a/sklearn/utils/tests/test_murmurhash.py
+++ b/sklearn/utils/tests/test_murmurhash.py
@@ -75,6 +75,6 @@ def test_uniform_distribution():
         bins[murmurhash3_32(i, positive=True) % n_bins] += 1
 
     means = bins / n_samples
-    expected = np.ones(n_bins) / n_bins
+    expected = np.full(n_bins, 1. / n_bins)
 
     assert_array_almost_equal(means / expected, np.ones(n_bins), 2)
diff --git a/sklearn/utils/tests/test_seq_dataset.py b/sklearn/utils/tests/test_seq_dataset.py
index aaa3e43fc993..45435371b8d4 100644
--- a/sklearn/utils/tests/test_seq_dataset.py
+++ b/sklearn/utils/tests/test_seq_dataset.py
@@ -1,4 +1,4 @@
-# Author: Tom Dupre la Tour <tom.dupre-la-tour@m4x.org>
+# Author: Tom Dupre la Tour
 #
 # License: BSD 3 clause
 
@@ -81,4 +81,3 @@ def test_seq_dataset_shuffle():
         _, _, _, idx1 = dataset1._random_py()
         _, _, _, idx2 = dataset2._random_py()
         assert_equal(idx1, idx2)
-
diff --git a/sklearn/utils/tests/test_show_versions.py b/sklearn/utils/tests/test_show_versions.py
new file mode 100644
index 000000000000..f55bc8b945b6
--- /dev/null
+++ b/sklearn/utils/tests/test_show_versions.py
@@ -0,0 +1,32 @@
+
+from sklearn.utils._show_versions import _get_sys_info
+from sklearn.utils._show_versions import _get_deps_info
+from sklearn.utils._show_versions import show_versions
+
+
+def test_get_sys_info():
+    sys_info = _get_sys_info()
+
+    assert 'python' in sys_info
+    assert 'executable' in sys_info
+    assert 'machine' in sys_info
+
+
+def test_get_deps_info():
+    deps_info = _get_deps_info()
+
+    assert 'pip' in deps_info
+    assert 'setuptools' in deps_info
+    assert 'sklearn' in deps_info
+    assert 'numpy' in deps_info
+    assert 'scipy' in deps_info
+    assert 'Cython' in deps_info
+    assert 'pandas' in deps_info
+
+
+def test_show_versions_with_blas(capsys):
+    show_versions()
+    out, err = capsys.readouterr()
+    assert 'python' in out
+    assert 'numpy' in out
+    assert 'BLAS' in out
diff --git a/sklearn/utils/tests/test_testing.py b/sklearn/utils/tests/test_testing.py
index eb9512f177ed..729b5ef81c68 100644
--- a/sklearn/utils/tests/test_testing.py
+++ b/sklearn/utils/tests/test_testing.py
@@ -8,6 +8,8 @@
 
 from scipy import sparse
 
+import pytest
+
 from sklearn.utils.deprecation import deprecated
 from sklearn.utils.metaestimators import if_delegate_has_method
 from sklearn.utils.testing import (
@@ -210,6 +212,20 @@ def context_manager_no_user_multiple_warning():
     assert_warns(UserWarning, context_manager_no_deprecation_multiple_warning)
     assert_warns(DeprecationWarning, context_manager_no_user_multiple_warning)
 
+    # Check that passing warning class as first positional argument
+    warning_class = UserWarning
+    match = "'obj' should be a callable.+you should use 'category=UserWarning'"
+
+    with pytest.raises(ValueError, match=match):
+        silence_warnings_func = ignore_warnings(warning_class)(
+            _warning_function)
+        silence_warnings_func()
+
+    with pytest.raises(ValueError, match=match):
+        @ignore_warnings(warning_class)
+        def test():
+            pass
+
 
 class TestWarns(unittest.TestCase):
     def test_warn(self):
diff --git a/sklearn/utils/tests/test_validation.py b/sklearn/utils/tests/test_validation.py
index deec9a50179b..3e577ebaa8ee 100644
--- a/sklearn/utils/tests/test_validation.py
+++ b/sklearn/utils/tests/test_validation.py
@@ -291,40 +291,17 @@ def test_check_array():
     assert_true(isinstance(result, np.ndarray))
 
     # deprecation warning if string-like array with dtype="numeric"
-    X_str = [['a', 'b'], ['c', 'd']]
-    assert_warns_message(
-        FutureWarning,
-        "arrays of strings will be interpreted as decimal numbers if "
-        "parameter 'dtype' is 'numeric'. It is recommended that you convert "
-        "the array to type np.float64 before passing it to check_array.",
-        check_array, X_str, "numeric")
-    assert_warns_message(
-        FutureWarning,
-        "arrays of strings will be interpreted as decimal numbers if "
-        "parameter 'dtype' is 'numeric'. It is recommended that you convert "
-        "the array to type np.float64 before passing it to check_array.",
-        check_array, np.array(X_str, dtype='U'), "numeric")
-    assert_warns_message(
-        FutureWarning,
-        "arrays of strings will be interpreted as decimal numbers if "
-        "parameter 'dtype' is 'numeric'. It is recommended that you convert "
-        "the array to type np.float64 before passing it to check_array.",
-        check_array, np.array(X_str, dtype='S'), "numeric")
+    expected_warn_regex = r"converted to decimal numbers if dtype='numeric'"
+    X_str = [['11', '12'], ['13', 'xx']]
+    for X in [X_str, np.array(X_str, dtype='U'), np.array(X_str, dtype='S')]:
+        with pytest.warns(FutureWarning, match=expected_warn_regex):
+            check_array(X, dtype="numeric")
 
     # deprecation warning if byte-like array with dtype="numeric"
     X_bytes = [[b'a', b'b'], [b'c', b'd']]
-    assert_warns_message(
-        FutureWarning,
-        "arrays of strings will be interpreted as decimal numbers if "
-        "parameter 'dtype' is 'numeric'. It is recommended that you convert "
-        "the array to type np.float64 before passing it to check_array.",
-        check_array, X_bytes, "numeric")
-    assert_warns_message(
-        FutureWarning,
-        "arrays of strings will be interpreted as decimal numbers if "
-        "parameter 'dtype' is 'numeric'. It is recommended that you convert "
-        "the array to type np.float64 before passing it to check_array.",
-        check_array, np.array(X_bytes, dtype='V1'), "numeric")
+    for X in [X_bytes, np.array(X_bytes, dtype='V1')]:
+        with pytest.warns(FutureWarning, match=expected_warn_regex):
+            check_array(X, dtype="numeric")
 
 
 def test_check_array_pandas_dtype_object_conversion():
@@ -436,8 +413,9 @@ def test_check_array_accept_sparse_type_exception():
            "Use X.toarray() to convert to a dense numpy array.")
     assert_raise_message(TypeError, msg,
                          check_array, X_csr, accept_sparse=False)
-    assert_raise_message(TypeError, msg,
-                         check_array, X_csr, accept_sparse=None)
+    with pytest.warns(DeprecationWarning):
+        assert_raise_message(TypeError, msg,
+                             check_array, X_csr, accept_sparse=None)
 
     msg = ("Parameter 'accept_sparse' should be a string, "
            "boolean or list of strings. You provided 'accept_sparse={}'.")
@@ -755,6 +733,7 @@ class WrongDummyMemory(object):
     pass
 
 
+@pytest.mark.filterwarnings("ignore:The 'cachedir' attribute")
 def test_check_memory():
     memory = check_memory("cache_directory")
     assert_equal(memory.cachedir, os.path.join('cache_directory', 'joblib'))
@@ -765,12 +744,12 @@ def test_check_memory():
     assert memory is dummy
     assert_raises_regex(ValueError, "'memory' should be None, a string or"
                         " have the same interface as "
-                        "sklearn.externals.joblib.Memory."
+                        "sklearn.utils.Memory."
                         " Got memory='1' instead.", check_memory, 1)
     dummy = WrongDummyMemory()
     assert_raises_regex(ValueError, "'memory' should be None, a string or"
                         " have the same interface as "
-                        "sklearn.externals.joblib.Memory. Got memory='{}' "
+                        "sklearn.utils.Memory. Got memory='{}' "
                         "instead.".format(dummy), check_memory, dummy)
 
 
diff --git a/sklearn/utils/validation.py b/sklearn/utils/validation.py
index a000d935624c..facc51e2c565 100644
--- a/sklearn/utils/validation.py
+++ b/sklearn/utils/validation.py
@@ -24,8 +24,8 @@
 from ..exceptions import NonBLASDotWarning
 from ..exceptions import NotFittedError
 from ..exceptions import DataConversionWarning
-from ..externals.joblib import Memory
-
+from ..utils._joblib import Memory
+from ..utils._joblib import __version__ as joblib_version
 
 FLOAT_DTYPES = (np.float64, np.float32, np.float16)
 
@@ -183,7 +183,7 @@ def check_memory(memory):
     """Check that ``memory`` is joblib.Memory-like.
 
     joblib.Memory-like means that ``memory`` can be converted into a
-    sklearn.externals.joblib.Memory instance (typically a str denoting the
+    sklearn.utils.Memory instance (typically a str denoting the
     ``cachedir``) or has the same interface (has a ``cache`` method).
 
     Parameters
@@ -201,10 +201,13 @@ def check_memory(memory):
     """
 
     if memory is None or isinstance(memory, six.string_types):
-        memory = Memory(cachedir=memory, verbose=0)
+        if LooseVersion(joblib_version) < '0.12':
+            memory = Memory(cachedir=memory, verbose=0)
+        else:
+            memory = Memory(location=memory, verbose=0)
     elif not hasattr(memory, 'cache'):
         raise ValueError("'memory' should be None, a string or have the same"
-                         " interface as sklearn.externals.joblib.Memory."
+                         " interface as sklearn.utils.Memory."
                          " Got memory='{}' instead.".format(memory))
     return memory
 
@@ -546,10 +549,12 @@ def check_array(array, accept_sparse=False, accept_large_sparse=True,
         # in the future np.flexible dtypes will be handled like object dtypes
         if dtype_numeric and np.issubdtype(array.dtype, np.flexible):
             warnings.warn(
-                "Beginning in version 0.22, arrays of strings will be "
-                "interpreted as decimal numbers if parameter 'dtype' is "
-                "'numeric'. It is recommended that you convert the array to "
-                "type np.float64 before passing it to check_array.",
+                "Beginning in version 0.22, arrays of bytes/strings will be "
+                "converted to decimal numbers if dtype='numeric'. "
+                "It is recommended that you convert the array to "
+                "a float dtype before using it in scikit-learn, "
+                "for example by using "
+                "your_array = your_array.astype(np.float64).",
                 FutureWarning)
 
         # make sure we actually converted to numeric:
@@ -812,7 +817,7 @@ def has_fit_parameter(estimator, parameter):
     estimator : object
         An estimator to inspect.
 
-    parameter: str
+    parameter : str
         The searched parameter.
 
     Returns
