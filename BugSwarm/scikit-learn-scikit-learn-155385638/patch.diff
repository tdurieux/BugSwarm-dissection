diff --git a/doc/conf.py b/doc/conf.py
index 57d385a881..9c75922b2a 100755
--- a/doc/conf.py
+++ b/doc/conf.py
@@ -32,11 +32,20 @@
 # extensions coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
 extensions = [
     'sphinx.ext.autodoc', 'sphinx.ext.autosummary',
-    'sphinx.ext.pngmath', 'numpy_ext.numpydoc',
+    'numpy_ext.numpydoc',
     'sphinx.ext.linkcode', 'sphinx.ext.doctest',
     'sphinx_gallery.gen_gallery',
 ]
 
+# pngmath / imgmath compatibility layer for different sphinx versions
+import sphinx
+from distutils.version import LooseVersion
+if LooseVersion(sphinx.__version__) < LooseVersion('1.4'):
+    extensions.append('sphinx.ext.pngmath')
+else:
+    extensions.append('sphinx.ext.imgmath')
+
+
 autodoc_default_flags = ['members', 'inherited-members']
 
 # Add any paths that contain templates here, relative to this directory.
diff --git a/doc/datasets/rcv1_fixture.py b/doc/datasets/rcv1_fixture.py
index c409f2f937..75d93d7ec5 100755
--- a/doc/datasets/rcv1_fixture.py
+++ b/doc/datasets/rcv1_fixture.py
@@ -1,7 +1,7 @@
 """Fixture module to skip the datasets loading when offline
 
 The RCV1 data is rather large and some CI workers such as travis are
-stateless hence will not cache the dataset as regular sklearn users would do.
+stateless hence will not cache the dataset as regular scikit-learn users would do.
 
 The following will skip the execution of the rcv1.rst doctests
 if the proper environment variable is configured (see the source code of
diff --git a/doc/developers/advanced_installation.rst b/doc/developers/advanced_installation.rst
index c086a3deb3..9029f5991a 100755
--- a/doc/developers/advanced_installation.rst
+++ b/doc/developers/advanced_installation.rst
@@ -61,7 +61,7 @@ installing build dependencies
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 installing from source requires you to have installed the scikit-learn runtime
-dependencies, python development headers and a working c/c++ compiler.
+dependencies, python development headers and a working C/C++ compiler.
 under debian-based operating systems, which include ubuntu, if you have
 python 2 you can install all these requirements by issuing::
 
@@ -257,7 +257,7 @@ or::
 building on windows
 -------------------
 
-to build scikit-learn on windows you need a working c/c++ compiler in
+to build scikit-learn on windows you need a working C/C++ compiler in
 addition to numpy, scipy and setuptools.
 
 picking the right compiler depends on the version of python (2 or 3)
@@ -364,7 +364,7 @@ bleeding edge
 =============
 
 see section :ref:`git_repo` on how to get the development version. then follow
-the previous instructions to build from source depending on your platform. 
+the previous instructions to build from source depending on your platform.
 You will also require Cython >=0.23 in order to build the development version.
 
 
diff --git a/doc/developers/contributing.rst b/doc/developers/contributing.rst
index c2b75d6630..c705d8dd93 100755
--- a/doc/developers/contributing.rst
+++ b/doc/developers/contributing.rst
@@ -871,9 +871,9 @@ an integer called ``n_iter``.
 Rolling your own estimator
 ==========================
 If you want to implement a new estimator that is scikit-learn-compatible,
-whether it is just for you or for contributing it to sklearn, there are several
-internals of scikit-learn that you should be aware of in addition to the
-sklearn API outlined above. You can check whether your estimator
+whether it is just for you or for contributing it to scikit-learn, there are
+several internals of scikit-learn that you should be aware of in addition to
+the scikit-learn API outlined above. You can check whether your estimator
 adheres to the scikit-learn interface and standards by running
 :func:`utils.estimator_checks.check_estimator` on the class::
 
@@ -929,7 +929,7 @@ E.g., below is a custom classifier. For more information on this example, see
 
 get_params and set_params
 -------------------------
-All sklearn estimator have ``get_params`` and ``set_params`` functions.
+All scikit-learn estimators have ``get_params`` and ``set_params`` functions.
 The ``get_params`` function takes no arguments and returns a dict of the
 ``__init__`` parameters of the estimator, together with their values.
 It must take one keyword argument, ``deep``,
diff --git a/doc/faq.rst b/doc/faq.rst
index eaf674d70d..2b911cf009 100755
--- a/doc/faq.rst
+++ b/doc/faq.rst
@@ -250,12 +250,14 @@ You can find more default on the new start methods in the `multiprocessing
 documentation <https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods>`_.
 
 
-Why is there no support for deep learning / Will there be support for deep learning in scikit-learn?
-----------------------------------------------------------------------------------------------------
-Deep learning requires a rich vocabulary to define an architecture and the
-use of GPUs for efficient computing. However, neither of these fit within
-the design constraints of scikit-learn. As a result, deep learning is
-currently out of scope for what scikit-learn seeks to achieve.
+Why is there no support for deep or reinforcement learning / Will there be support for deep or reinforcement learning in scikit-learn?
+--------------------------------------------------------------------------------------------------------------------------------------
+Deep learning and reinforcement learning both require a rich vocabulary to
+define an architecture, with deep learning additionally requiring
+GPUs for efficient computing. However, neither of these fit within
+the design constraints of scikit-learn; as a result, deep learning
+and reinforcement learning are currently out of scope for what
+scikit-learn seeks to achieve.
 
 
 Why is my pull request not getting any attention?
diff --git a/doc/modules/gaussian_process.rst b/doc/modules/gaussian_process.rst
index 52211a154d..fb408c4acd 100755
--- a/doc/modules/gaussian_process.rst
+++ b/doc/modules/gaussian_process.rst
@@ -66,7 +66,7 @@ WhiteKernel component into the kernel, which can estimate the global noise
 level from the data (see example below).
 
 The implementation is based on Algorithm 2.1 of [RW2006]_. In addition to
-the API of standard sklearn estimators, GaussianProcessRegressor:
+the API of standard scikit-learn estimators, GaussianProcessRegressor:
 
 * allows prediction without prior fitting (based on the GP prior)
 
@@ -164,7 +164,7 @@ than just predicting the mean.
 GPR on Mauna Loa CO2 data
 -------------------------
 
-This example is based on Section 5.4.3 of [RW2006]_. 
+This example is based on Section 5.4.3 of [RW2006]_.
 It illustrates an example of complex kernel engineering and
 hyperparameter optimization using gradient ascent on the
 log-marginal-likelihood. The data consists of the monthly average atmospheric
@@ -602,11 +602,11 @@ References
 ----------
 
     * `[RW2006]
-      <http://www.gaussianprocess.org/gpml/chapters/>`_ 
-      **Gaussian Processes for Machine Learning**, 
-      Carl Eduard Rasmussen and Christopher K.I. Williams, MIT Press 2006. 
-      Link to an official complete PDF version of the book 
-      `here <http://www.gaussianprocess.org/gpml/chapters/RW.pdf>`_ . 
+      <http://www.gaussianprocess.org/gpml/chapters/>`_
+      **Gaussian Processes for Machine Learning**,
+      Carl Eduard Rasmussen and Christopher K.I. Williams, MIT Press 2006.
+      Link to an official complete PDF version of the book
+      `here <http://www.gaussianprocess.org/gpml/chapters/RW.pdf>`_ .
 
 .. currentmodule:: sklearn.gaussian_process
 
@@ -616,9 +616,9 @@ References
 Legacy Gaussian Processes
 =========================
 
-In this section, the implementation of Gaussian processes used in sklearn until
-release 0.16.1 is described. Note that this implementation is deprecated and
-will be removed in version 0.18.
+In this section, the implementation of Gaussian processes used in scikit-learn
+until release 0.16.1 is described. Note that this implementation is deprecated
+and will be removed in version 0.18.
 
 An introductory regression example
 ----------------------------------
diff --git a/doc/modules/manifold.rst b/doc/modules/manifold.rst
index 4fc05bb05d..3c003e0d0c 100755
--- a/doc/modules/manifold.rst
+++ b/doc/modules/manifold.rst
@@ -59,10 +59,10 @@ interesting structure within the data will be lost.
 
 To address this concern, a number of supervised and unsupervised linear
 dimensionality reduction frameworks have been designed, such as Principal
-Component Analysis (PCA), Independent Component Analysis, Linear 
-Discriminant Analysis, and others.  These algorithms define specific 
+Component Analysis (PCA), Independent Component Analysis, Linear
+Discriminant Analysis, and others.  These algorithms define specific
 rubrics to choose an "interesting" linear projection of the data.
-These methods can be powerful, but often miss important non-linear 
+These methods can be powerful, but often miss important non-linear
 structure in the data.
 
 
@@ -91,7 +91,7 @@ from the data itself, without the use of predetermined classifications.
     * See :ref:`sphx_glr_auto_examples_manifold_plot_compare_methods.py` for an example of
       dimensionality reduction on a toy "S-curve" dataset.
 
-The manifold learning implementations available in sklearn are
+The manifold learning implementations available in scikit-learn are
 summarized below
 
 .. _isomap:
@@ -121,13 +121,13 @@ The Isomap algorithm comprises three stages:
    nearest neighbors of :math:`N` points in :math:`D` dimensions.
 
 2. **Shortest-path graph search.**  The most efficient known algorithms
-   for this are *Dijkstra's Algorithm*, which is approximately 
+   for this are *Dijkstra's Algorithm*, which is approximately
    :math:`O[N^2(k + \log(N))]`, or the *Floyd-Warshall algorithm*, which
    is :math:`O[N^3]`.  The algorithm can be selected by the user with
    the ``path_method`` keyword of ``Isomap``.  If unspecified, the code
    attempts to choose the best algorithm for the input data.
 
-3. **Partial eigenvalue decomposition.**  The embedding is encoded in the 
+3. **Partial eigenvalue decomposition.**  The embedding is encoded in the
    eigenvectors corresponding to the :math:`d` largest eigenvalues of the
    :math:`N \times N` isomap kernel.  For a dense solver, the cost is
    approximately :math:`O[d N^2]`.  This cost can often be improved using
@@ -191,7 +191,7 @@ The overall complexity of standard LLE is
 * :math:`d` : output dimension
 
 .. topic:: References:
-   
+
    * `"Nonlinear dimensionality reduction by locally linear embedding"
      <http://www.sciencemag.org/content/290/5500/2323.full>`_
      Roweis, S. & Saul, L.  Science 290:2323 (2000)
@@ -221,7 +221,7 @@ It requires ``n_neighbors > n_components``.
    :target: ../auto_examples/manifold/plot_lle_digits.html
    :align: center
    :scale: 50
-   
+
 Complexity
 ----------
 
@@ -232,7 +232,7 @@ The MLLE algorithm comprises three stages:
 2. **Weight Matrix Construction**. Approximately
    :math:`O[D N k^3] + O[N (k-D) k^2]`.  The first term is exactly equivalent
    to that of standard LLE.  The second term has to do with constructing the
-   weight matrix from multiple weights.  In practice, the added cost of 
+   weight matrix from multiple weights.  In practice, the added cost of
    constructing the MLLE weight matrix is relatively small compared to the
    cost of steps 1 and 3.
 
@@ -247,7 +247,7 @@ The overall complexity of MLLE is
 * :math:`d` : output dimension
 
 .. topic:: References:
-     
+
    * `"MLLE: Modified Locally Linear Embedding Using Multiple Weights"
      <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382>`_
      Zhang, Z. & Wang, J.
@@ -271,7 +271,7 @@ It requires ``n_neighbors > n_components * (n_components + 3) / 2``.
    :target: ../auto_examples/manifold/plot_lle_digits.html
    :align: center
    :scale: 50
-   
+
 Complexity
 ----------
 
@@ -308,10 +308,10 @@ Spectral Embedding
 Spectral Embedding (also known as Laplacian Eigenmaps) is one method
 to calculate non-linear embedding. It finds a low dimensional representation
 of the data using a spectral decomposition of the graph Laplacian.
-The graph generated can be considered as a discrete approximation of the 
-low dimensional manifold in the high dimensional space. Minimization of a 
-cost function based on the graph ensures that points close to each other on 
-the manifold are mapped close to each other in the low dimensional space, 
+The graph generated can be considered as a discrete approximation of the
+low dimensional manifold in the high dimensional space. Minimization of a
+cost function based on the graph ensures that points close to each other on
+the manifold are mapped close to each other in the low dimensional space,
 preserving local distances. Spectral embedding can be  performed with the
 function :func:`spectral_embedding` or its object-oriented counterpart
 :class:`SpectralEmbedding`.
@@ -326,9 +326,9 @@ The Spectral Embedding algorithm comprises three stages:
 
 2. **Graph Laplacian Construction**. unnormalized Graph Laplacian
    is constructed as :math:`L = D - A` for and normalized one as
-   :math:`L = D^{-\frac{1}{2}} (D - A) D^{-\frac{1}{2}}`.  
+   :math:`L = D^{-\frac{1}{2}} (D - A) D^{-\frac{1}{2}}`.
 
-3. **Partial Eigenvalue Decomposition**. Eigenvalue decomposition is 
+3. **Partial Eigenvalue Decomposition**. Eigenvalue decomposition is
    done on graph Laplacian
 
 The overall complexity of spectral embedding is
@@ -342,7 +342,7 @@ The overall complexity of spectral embedding is
 .. topic:: References:
 
    * `"Laplacian Eigenmaps for Dimensionality Reduction
-     and Data Representation" 
+     and Data Representation"
      <http://web.cse.ohio-state.edu/~mbelkin/papers/LEM_NC_03.pdf>`_
      M. Belkin, P. Niyogi, Neural Computation, June 2003; 15 (6):1373-1396
 
@@ -354,7 +354,7 @@ Though not technically a variant of LLE, Local tangent space alignment (LTSA)
 is algorithmically similar enough to LLE that it can be put in this category.
 Rather than focusing on preserving neighborhood distances as in LLE, LTSA
 seeks to characterize the local geometry at each neighborhood via its
-tangent space, and performs a global optimization to align these local 
+tangent space, and performs a global optimization to align these local
 tangent spaces to learn the embedding.  LTSA can be performed with function
 :func:`locally_linear_embedding` or its object-oriented counterpart
 :class:`LocallyLinearEmbedding`, with the keyword ``method = 'ltsa'``.
@@ -421,7 +421,7 @@ space and the similarities/dissimilarities.
    :target: ../auto_examples/manifold/plot_lle_digits.html
    :align: center
    :scale: 50
- 
+
 
 Let :math:`S` be the similarity matrix, and :math:`X` the coordinates of the
 :math:`n` input points. Disparities :math:`\hat{d}_{ij}` are transformation of
@@ -456,7 +456,7 @@ order to avoid that, the disparities :math:`\hat{d}_{ij}` are normalized.
    :target: ../auto_examples/manifold/plot_mds.html
    :align: center
    :scale: 60
-  
+
 
 .. topic:: References:
 
@@ -499,7 +499,7 @@ probabilities in the original space and the embedded space will be minimized
 by gradient descent. Note that the KL divergence is not convex, i.e.
 multiple restarts with different initializations will end up in local minima
 of the KL divergence. Hence, it is sometimes useful to try different seeds
-and select the embedding with the lowest KL divergence. 
+and select the embedding with the lowest KL divergence.
 
 The disadvantages to using t-SNE are roughly:
 
@@ -552,7 +552,7 @@ divergence will increase during optimization. More tips can be found in
 Laurens van der Maaten's FAQ (see references). The last parameter, angle,
 is a tradeoff between performance and accuracy. Larger angles imply that we
 can approximate larger regions by a single point,leading to better speed
-but less accurate results. 
+but less accurate results.
 
 Barnes-Hut t-SNE
 ----------------
@@ -560,8 +560,8 @@ Barnes-Hut t-SNE
 The Barnes-Hut t-SNE that has been implemented here is usually much slower than
 other manifold learning algorithms. The optimization is quite difficult
 and the computation of the gradient is :math:`O[d N log(N)]`, where :math:`d`
-is the number of output dimensions and :math:`N` is the number of samples. The 
-Barnes-Hut method improves on the exact method where t-SNE complexity is 
+is the number of output dimensions and :math:`N` is the number of samples. The
+Barnes-Hut method improves on the exact method where t-SNE complexity is
 :math:`O[d N^2]`, but has several other notable differences:
 
 * The Barnes-Hut implementation only works when the target dimensionality is 3
diff --git a/doc/modules/pipeline.rst b/doc/modules/pipeline.rst
index 344d3faa19..ddbcc022d7 100755
--- a/doc/modules/pipeline.rst
+++ b/doc/modules/pipeline.rst
@@ -37,17 +37,16 @@ is an estimator object::
     >>> from sklearn.pipeline import Pipeline
     >>> from sklearn.svm import SVC
     >>> from sklearn.decomposition import PCA
-    >>> estimators = [('reduce_dim', PCA()), ('svm', SVC())]
-    >>> clf = Pipeline(estimators)
-    >>> clf # doctest: +NORMALIZE_WHITESPACE
+    >>> estimators = [('reduce_dim', PCA()), ('clf', SVC())]
+    >>> pipe = Pipeline(estimators)
+    >>> pipe # doctest: +NORMALIZE_WHITESPACE
     Pipeline(steps=[('reduce_dim', PCA(copy=True, iterated_power=4,
     n_components=None, random_state=None, svd_solver='auto', tol=0.0,
-    whiten=False)), ('svm', SVC(C=1.0, cache_size=200, class_weight=None,
+    whiten=False)), ('clf', SVC(C=1.0, cache_size=200, class_weight=None,
     coef0=0.0, decision_function_shape=None, degree=3, gamma='auto',
     kernel='rbf', max_iter=-1, probability=False, random_state=None,
     shrinking=True, tol=0.001, verbose=False))])
 
-
 The utility function :func:`make_pipeline` is a shorthand
 for constructing pipelines;
 it takes a variable number of estimators and returns a pipeline,
@@ -64,23 +63,23 @@ filling in the names automatically::
 
 The estimators of a pipeline are stored as a list in the ``steps`` attribute::
 
-    >>> clf.steps[0]
+    >>> pipe.steps[0]
     ('reduce_dim', PCA(copy=True, iterated_power=4, n_components=None, random_state=None,
       svd_solver='auto', tol=0.0, whiten=False))
 
 and as a ``dict`` in ``named_steps``::
 
-    >>> clf.named_steps['reduce_dim']
+    >>> pipe.named_steps['reduce_dim']
     PCA(copy=True, iterated_power=4, n_components=None, random_state=None,
       svd_solver='auto', tol=0.0, whiten=False)
 
 Parameters of the estimators in the pipeline can be accessed using the
 ``<estimator>__<parameter>`` syntax::
 
-    >>> clf.set_params(svm__C=10) # doctest: +NORMALIZE_WHITESPACE
+    >>> pipe.set_params(clf__C=10) # doctest: +NORMALIZE_WHITESPACE
     Pipeline(steps=[('reduce_dim', PCA(copy=True, iterated_power=4,
         n_components=None, random_state=None, svd_solver='auto', tol=0.0,
-        whiten=False)), ('svm', SVC(C=10, cache_size=200, class_weight=None,
+        whiten=False)), ('clf', SVC(C=10, cache_size=200, class_weight=None,
         coef0=0.0, decision_function_shape=None, degree=3, gamma='auto',
         kernel='rbf', max_iter=-1, probability=False, random_state=None,
         shrinking=True, tol=0.001, verbose=False))])
@@ -90,9 +89,17 @@ This is particularly important for doing grid searches::
 
     >>> from sklearn.model_selection import GridSearchCV
     >>> params = dict(reduce_dim__n_components=[2, 5, 10],
-    ...               svm__C=[0.1, 10, 100])
-    >>> grid_search = GridSearchCV(clf, param_grid=params)
+    ...               clf__C=[0.1, 10, 100])
+    >>> grid_search = GridSearchCV(pipe, param_grid=params)
+
+Individual steps may also be replaced as parameters, and non-final steps may be
+ignored by setting them to ``None``::
 
+    >>> from sklearn.linear_model import LogisticRegression
+    >>> params = dict(reduce_dim=[None, PCA(5), PCA(10)],
+    ...               clf=[SVC(), LogisticRegression()],
+    ...               clf__C=[0.1, 10, 100])
+    >>> grid_search = GridSearchCV(pipe, param_grid=params)
 
 .. topic:: Examples:
 
@@ -172,6 +179,15 @@ Like pipelines, feature unions have a shorthand constructor called
 :func:`make_union` that does not require explicit naming of the components.
 
 
+Like ``Pipeline``, individual steps may be replaced using ``set_params``,
+and ignored by setting to ``None``::
+
+    >>> combined.set_params(kernel_pca=None) # doctest: +NORMALIZE_WHITESPACE
+    FeatureUnion(n_jobs=1, transformer_list=[('linear_pca', PCA(copy=True,
+          iterated_power=4, n_components=None, random_state=None,
+          svd_solver='auto', tol=0.0, whiten=False)), ('kernel_pca', None)],
+        transformer_weights=None)
+
 .. topic:: Examples:
 
  * :ref:`sphx_glr_auto_examples_feature_stacker.py`
diff --git a/doc/themes/scikit-learn/static/nature.css_t b/doc/themes/scikit-learn/static/nature.css_t
index 5e4c00783d..c9c228fc7b 100755
--- a/doc/themes/scikit-learn/static/nature.css_t
+++ b/doc/themes/scikit-learn/static/nature.css_t
@@ -116,6 +116,7 @@ div.navbar div.nav-icon {
         background: #ff9c34;
         padding: 5px 10px;
         border-radius: 5px;
+        z-index: 10;
     }
     div.navbar ul li {
         display: none;
@@ -137,6 +138,7 @@ div.navbar div.nav-icon {
         top: 10px;
         margin-right: 10px;
         background: #ff9c34;
+        z-index: 9;
     }
     div.navbar.responsive > ul li {
         display: flex;
@@ -1186,6 +1188,7 @@ div.navbar ul.dropdown-menu li a.btn {
   vertical-align: middle;
   padding: 0;
   padding-left: 4px;
+  font-size: 8px;
 }
 
 .navbar .btn-group .btn {
@@ -1197,10 +1200,6 @@ div.navbar ul.dropdown-menu li a.btn {
   box-shadow: inset 0 0px 0 rgba(255,255,255,.2), 0 0px 0px rgba(0,0,0,.05);
 }
 
-a.btn.dropdown-toggle,  a.btn.dropdown-toggle:hover{
-  vertical-align: baseline;
-}
-
 li#other-versions {
   position: absolute;
   left: inherit;
@@ -1268,7 +1267,7 @@ li#other-versions {
   color: #fff;
   vertical-align: middle;
   border-top-color: rgb(255, 255, 255);
-  padding-bottom: 8px;
+  padding-bottom: 5px;
 }
 
 .navbar .dropdown-menu .divider {
diff --git a/doc/tutorial/statistical_inference/model_selection.rst b/doc/tutorial/statistical_inference/model_selection.rst
index 475e1c5e5b..ef3568ffb0 100755
--- a/doc/tutorial/statistical_inference/model_selection.rst
+++ b/doc/tutorial/statistical_inference/model_selection.rst
@@ -207,7 +207,7 @@ Grid-search
 
 .. currentmodule:: sklearn.model_selection
 
-The sklearn provides an object that, given data, computes the score
+scikit-learn provides an object that, given data, computes the score
 during the fit of an estimator on a parameter grid and chooses the
 parameters to maximize the cross-validation score. This object takes an
 estimator during the construction and exposes an estimator API::
@@ -257,9 +257,9 @@ Cross-validated estimators
 ----------------------------
 
 Cross-validation to set a parameter can be done more efficiently on an
-algorithm-by-algorithm basis. This is why for certain estimators the
-sklearn exposes :ref:`cross_validation` estimators that set their parameter
-automatically by cross-validation::
+algorithm-by-algorithm basis. This is why, for certain estimators,
+scikit-learn exposes :ref:`cross_validation` estimators that set their
+parameter automatically by cross-validation::
 
     >>> from sklearn import linear_model, datasets
     >>> lasso = linear_model.LassoCV()
diff --git a/doc/tutorial/text_analytics/working_with_text_data_fixture.py b/doc/tutorial/text_analytics/working_with_text_data_fixture.py
index c10f5c4f4d..d620986d07 100755
--- a/doc/tutorial/text_analytics/working_with_text_data_fixture.py
+++ b/doc/tutorial/text_analytics/working_with_text_data_fixture.py
@@ -1,7 +1,7 @@
 """Fixture module to skip the datasets loading when offline
 
 The 20 newsgroups data is rather large and some CI workers such as travis are
-stateless hence will not cache the dataset as regular sklearn users would do.
+stateless hence will not cache the dataset as regular scikit-learn users would.
 
 The following will skip the execution of the working_with_text_data.rst doctests
 if the proper environment variable is configured (see the source code of
diff --git a/doc/whats_new.rst b/doc/whats_new.rst
index 408dc7eef3..b49e0a3348 100755
--- a/doc/whats_new.rst
+++ b/doc/whats_new.rst
@@ -286,6 +286,12 @@ Enhancements
      (`#5805 <https://github.com/scikit-learn/scikit-learn/pull/5805>`_)
      By `Ibraim Ganiev`_.
 
+   - Added support for substituting or disabling :class:`pipeline.Pipeline`
+     and :class:`pipeline.FeatureUnion` components using the ``set_params``
+     interface that powers :mod:`sklearn.grid_search`.
+     See :ref:`example_plot_compare_reduction.py`. By `Joel Nothman`_ and
+     `Robert McGibbon`_.
+
 Bug fixes
 .........
 
diff --git a/examples/hetero_feature_union.py b/examples/hetero_feature_union.py
index 0ec565a8f5..0af24a3c05 100755
--- a/examples/hetero_feature_union.py
+++ b/examples/hetero_feature_union.py
@@ -51,7 +51,7 @@ class ItemSelector(BaseEstimator, TransformerMixin):
 
     >> len(data[key]) == n_samples
 
-    Please note that this is the opposite convention to sklearn feature
+    Please note that this is the opposite convention to scikit-learn feature
     matrixes (where the first index corresponds to sample).
 
     ItemSelector only requires that the collection implement getitem
diff --git a/examples/plot_compare_reduction.py b/examples/plot_compare_reduction.py
new file mode 100755
index 0000000000..0f578722ee
--- /dev/null
+++ b/examples/plot_compare_reduction.py
@@ -0,0 +1,75 @@
+#!/usr/bin/python
+# -*- coding: utf-8 -*-
+"""
+=================================================================
+Selecting dimensionality reduction with Pipeline and GridSearchCV
+=================================================================
+
+This example constructs a pipeline that does dimensionality
+reduction followed by prediction with a support vector
+classifier. It demonstrates the use of GridSearchCV and
+Pipeline to optimize over different classes of estimators in a
+single CV run -- unsupervised PCA and NMF dimensionality
+reductions are compared to univariate feature selection during
+the grid search.
+"""
+# Authors: Robert McGibbon, Joel Nothman
+
+from __future__ import print_function, division
+
+import numpy as np
+import matplotlib.pyplot as plt
+from sklearn.datasets import load_digits
+from sklearn.model_selection import GridSearchCV
+from sklearn.pipeline import Pipeline
+from sklearn.svm import LinearSVC
+from sklearn.decomposition import PCA, NMF
+from sklearn.feature_selection import SelectKBest, chi2
+
+print(__doc__)
+
+pipe = Pipeline([
+    ('reduce_dim', PCA()),
+    ('classify', LinearSVC())
+])
+
+N_FEATURES_OPTIONS = [2, 4, 8]
+C_OPTIONS = [1, 10, 100, 1000]
+param_grid = [
+    {
+        'reduce_dim': [PCA(iterated_power=7), NMF()],
+        'reduce_dim__n_components': N_FEATURES_OPTIONS,
+        'classify__C': C_OPTIONS
+    },
+    {
+        'reduce_dim': [SelectKBest(chi2)],
+        'reduce_dim__k': N_FEATURES_OPTIONS,
+        'classify__C': C_OPTIONS
+    },
+]
+reducer_labels = ['PCA', 'NMF', 'KBest(chi2)']
+
+grid = GridSearchCV(pipe, cv=3, n_jobs=2, param_grid=param_grid)
+digits = load_digits()
+grid.fit(digits.data, digits.target)
+
+mean_scores = np.array(grid.results_['test_mean_score'])
+# scores are in the order of param_grid iteration, which is alphabetical
+mean_scores = mean_scores.reshape(len(C_OPTIONS), -1, len(N_FEATURES_OPTIONS))
+# select score for best C
+mean_scores = mean_scores.max(axis=0)
+bar_offsets = (np.arange(len(N_FEATURES_OPTIONS)) *
+               (len(reducer_labels) + 1) + .5)
+
+plt.figure()
+COLORS = 'bgrcmyk'
+for i, (label, reducer_scores) in enumerate(zip(reducer_labels, mean_scores)):
+    plt.bar(bar_offsets + i, reducer_scores, label=label, color=COLORS[i])
+
+plt.title("Comparing feature reduction techniques")
+plt.xlabel('Reduced number of features')
+plt.xticks(bar_offsets + len(reducer_labels) / 2, N_FEATURES_OPTIONS)
+plt.ylabel('Digit classification accuracy')
+plt.ylim((0, 1))
+plt.legend(loc='upper left')
+plt.show()
diff --git a/sklearn/cluster/k_means_.py b/sklearn/cluster/k_means_.py
index 6e2a84b2d9..4037cfa255 100755
--- a/sklearn/cluster/k_means_.py
+++ b/sklearn/cluster/k_means_.py
@@ -792,6 +792,22 @@ class KMeans(BaseEstimator, ClusterMixin, TransformerMixin):
     inertia_ : float
         Sum of distances of samples to their closest cluster center.
 
+    Examples
+    --------
+
+    >>> from sklearn.cluster import KMeans
+    >>> import numpy as np
+    >>> X = np.array([[1, 2], [1, 4], [1, 0],
+    ...               [4, 2], [4, 4], [4, 0]])
+    >>> kmeans = KMeans(n_clusters=2, random_state=0).fit(X)
+    >>> kmeans.labels_
+    array([0, 0, 0, 1, 1, 1], dtype=int32)
+    >>> kmeans.predict([[0, 0], [4, 4]])
+    array([0, 1], dtype=int32)
+    >>> kmeans.cluster_centers_
+    array([[ 1.,  2.],
+           [ 4.,  2.]])
+
     See also
     --------
 
diff --git a/sklearn/cluster/tests/test_k_means.py b/sklearn/cluster/tests/test_k_means.py
index 9ee3ef616b..b00d91eb95 100755
--- a/sklearn/cluster/tests/test_k_means.py
+++ b/sklearn/cluster/tests/test_k_means.py
@@ -669,7 +669,7 @@ def test_int_input():
 
         expected_labels = [0, 1, 1, 0, 0, 1]
         scores = np.array([v_measure_score(expected_labels, km.labels_)
-                        for km in fitted_models])
+                           for km in fitted_models])
         assert_array_equal(scores, np.ones(scores.shape[0]))
 
 
@@ -793,24 +793,28 @@ def test_float_precision():
                 if is_sparse:
                     X_test = sp.csr_matrix(X_csr, dtype=dtype)
                 else:
-                    X_test = dtype(X)
+                    X_test = X.astype(dtype)
                 estimator.fit(X_test)
-                # dtype of cluster centers has to be the dtype of the input data
+                # dtype of cluster centers has to be the dtype of the input
+                # data
                 assert_equal(estimator.cluster_centers_.dtype, dtype)
                 inertia[dtype] = estimator.inertia_
                 X_new[dtype] = estimator.transform(X_test)
                 centers[dtype] = estimator.cluster_centers_
-                # make sure predictions correspond to the correct label
-                assert_equal(estimator.predict(X_test[0]), estimator.labels_[0])
+                # ensure the extracted row is a 2d array
+                assert_equal(estimator.predict(X_test[:1]),
+                             estimator.labels_[0])
                 if hasattr(estimator, 'partial_fit'):
                     estimator.partial_fit(X_test[0:3])
-                    # dtype of cluster centers has to stay the same after partial_fit
+                    # dtype of cluster centers has to stay the same after
+                    # partial_fit
                     assert_equal(estimator.cluster_centers_.dtype, dtype)
 
             # compare arrays with low precision since the difference between
-            # 32 and 64 bit sometimes makes a difference up to the 4th decimal place
+            # 32 and 64 bit sometimes makes a difference up to the 4th decimal
+            # place
             assert_array_almost_equal(inertia[np.float32], inertia[np.float64],
-                                    decimal=4)
+                                      decimal=4)
             assert_array_almost_equal(X_new[np.float32], X_new[np.float64],
                                       decimal=4)
             assert_array_almost_equal(centers[np.float32], centers[np.float64],
@@ -818,14 +822,14 @@ def test_float_precision():
 
 
 def test_KMeans_init_centers():
-    # This test is used to check KMeans won't mutate the user provided input array silently
-    # even if input data and init centers have the same type
+    # This test is used to check KMeans won't mutate the user provided input
+    # array silently even if input data and init centers have the same type
     X_small = np.array([[1.1, 1.1], [-7.5, -7.5], [-1.1, -1.1], [7.5, 7.5]])
     init_centers = np.array([[0.0, 0.0], [5.0, 5.0], [-5.0, -5.0]])
     for dtype in [np.int32, np.int64, np.float32, np.float64]:
         X_test = dtype(X_small)
         init_centers_test = dtype(init_centers)
         assert_array_equal(init_centers, init_centers_test)
-        km = KMeans(init=init_centers_test, n_clusters=3)
+        km = KMeans(init=init_centers_test, n_clusters=3, n_init=1)
         km.fit(X_test)
         assert_equal(False, np.may_share_memory(km.cluster_centers_, init_centers))
diff --git a/sklearn/covariance/tests/test_graph_lasso.py b/sklearn/covariance/tests/test_graph_lasso.py
index f8da99ce3a..bc2c8339da 100755
--- a/sklearn/covariance/tests/test_graph_lasso.py
+++ b/sklearn/covariance/tests/test_graph_lasso.py
@@ -61,8 +61,8 @@ def test_graph_lasso(random_state=0):
 
 def test_graph_lasso_iris():
     # Hard-coded solution from R glasso package for alpha=1.0
-    # The iris datasets in R and sklearn do not match in a few places, these
-    # values are for the sklearn version
+    # The iris datasets in R and scikit-learn do not match in a few places,
+    # these values are for the scikit-learn version.
     cov_R = np.array([
         [0.68112222, 0.0, 0.2651911, 0.02467558],
         [0.00, 0.1867507, 0.0, 0.00],
diff --git a/sklearn/datasets/mldata.py b/sklearn/datasets/mldata.py
index 3768435fe4..1ab3edea91 100755
--- a/sklearn/datasets/mldata.py
+++ b/sklearn/datasets/mldata.py
@@ -103,7 +103,7 @@ def fetch_mldata(dataname, target_name='label', data_name='data',
     (150, 4)
 
     Load the 'leukemia' dataset from mldata.org, which needs to be transposed
-    to respects the sklearn axes convention:
+    to respects the scikit-learn axes convention:
 
     >>> leuk = fetch_mldata('leukemia', transpose_data=True,
     ...                     data_home=test_data_home)
@@ -205,7 +205,7 @@ def fetch_mldata(dataname, target_name='label', data_name='data',
             del dataset[col_names[1]]
             dataset['data'] = matlab_dict[col_names[1]]
 
-    # set axes to sklearn conventions
+    # set axes to scikit-learn conventions
     if transpose_data:
         dataset['data'] = dataset['data'].T
     if 'target' in dataset:
diff --git a/sklearn/ensemble/forest.py b/sklearn/ensemble/forest.py
index 65522064df..4d4f04bc12 100755
--- a/sklearn/ensemble/forest.py
+++ b/sklearn/ensemble/forest.py
@@ -160,9 +160,9 @@ def apply(self, X):
         Parameters
         ----------
         X : array-like or sparse matrix, shape = [n_samples, n_features]
-            The input samples. Internally, it will be converted to
-            ``dtype=np.float32`` and if a sparse matrix is provided
-            to a sparse ``csr_matrix``.
+            The input samples. Internally, its dtype will be converted to
+            ``dtype=np.float32``. If a sparse matrix is provided, it will be
+            converted into a sparse ``csr_matrix``.
 
         Returns
         -------
@@ -184,9 +184,9 @@ def decision_path(self, X):
         Parameters
         ----------
         X : array-like or sparse matrix, shape = [n_samples, n_features]
-            The input samples. Internally, it will be converted to
-            ``dtype=np.float32`` and if a sparse matrix is provided
-            to a sparse ``csr_matrix``.
+            The input samples. Internally, its dtype will be converted to
+            ``dtype=np.float32``. If a sparse matrix is provided, it will be
+            converted into a sparse ``csr_matrix``.
 
         Returns
         -------
@@ -217,9 +217,9 @@ def fit(self, X, y, sample_weight=None):
         Parameters
         ----------
         X : array-like or sparse matrix of shape = [n_samples, n_features]
-            The training input samples. Internally, it will be converted to
-            ``dtype=np.float32`` and if a sparse matrix is provided
-            to a sparse ``csc_matrix``.
+            The training input samples. Internally, its dtype will be converted to
+            ``dtype=np.float32``. If a sparse matrix is provided, it will be
+            converted into a sparse ``csc_matrix``.
 
         y : array-like, shape = [n_samples] or [n_samples, n_outputs]
             The target values (class labels in classification, real numbers in
@@ -516,9 +516,9 @@ def predict(self, X):
         Parameters
         ----------
         X : array-like or sparse matrix of shape = [n_samples, n_features]
-            The input samples. Internally, it will be converted to
-            ``dtype=np.float32`` and if a sparse matrix is provided
-            to a sparse ``csr_matrix``.
+            The input samples. Internally, its dtype will be converted to
+            ``dtype=np.float32``. If a sparse matrix is provided, it will be
+            converted into a sparse ``csr_matrix``.
 
         Returns
         -------
@@ -552,9 +552,9 @@ class in a leaf.
         Parameters
         ----------
         X : array-like or sparse matrix of shape = [n_samples, n_features]
-            The input samples. Internally, it will be converted to
-            ``dtype=np.float32`` and if a sparse matrix is provided
-            to a sparse ``csr_matrix``.
+            The input samples. Internally, its dtype will be converted to
+            ``dtype=np.float32``. If a sparse matrix is provided, it will be
+            converted into a sparse ``csr_matrix``.
 
         Returns
         -------
@@ -605,9 +605,9 @@ def predict_log_proba(self, X):
         Parameters
         ----------
         X : array-like or sparse matrix of shape = [n_samples, n_features]
-            The input samples. Internally, it will be converted to
-            ``dtype=np.float32`` and if a sparse matrix is provided
-            to a sparse ``csr_matrix``.
+            The input samples. Internally, its dtype will be converted to
+            ``dtype=np.float32``. If a sparse matrix is provided, it will be
+            converted into a sparse ``csr_matrix``.
 
         Returns
         -------
@@ -666,9 +666,9 @@ def predict(self, X):
         Parameters
         ----------
         X : array-like or sparse matrix of shape = [n_samples, n_features]
-            The input samples. Internally, it will be converted to
-            ``dtype=np.float32`` and if a sparse matrix is provided
-            to a sparse ``csr_matrix``.
+            The input samples. Internally, its dtype will be converted to
+            ``dtype=np.float32``. If a sparse matrix is provided, it will be
+            converted into a sparse ``csr_matrix``.
 
         Returns
         -------
diff --git a/sklearn/ensemble/gradient_boosting.py b/sklearn/ensemble/gradient_boosting.py
index 02e3765cfc..4ea8ef8e4e 100755
--- a/sklearn/ensemble/gradient_boosting.py
+++ b/sklearn/ensemble/gradient_boosting.py
@@ -1234,9 +1234,9 @@ def apply(self, X):
         Parameters
         ----------
         X : array-like or sparse matrix, shape = [n_samples, n_features]
-            The input samples. Internally, it will be converted to
-            ``dtype=np.float32`` and if a sparse matrix is provided
-            to a sparse ``csr_matrix``.
+            The input samples. Internally, its dtype will be converted to
+            ``dtype=np.float32``. If a sparse matrix is provided, it will
+            be converted to a sparse ``csr_matrix``.
 
         Returns
         -------
@@ -1865,9 +1865,9 @@ def apply(self, X):
         Parameters
         ----------
         X : array-like or sparse matrix, shape = [n_samples, n_features]
-            The input samples. Internally, it will be converted to
-            ``dtype=np.float32`` and if a sparse matrix is provided
-            to a sparse ``csr_matrix``.
+            The input samples. Internally, its dtype will be converted to
+            ``dtype=np.float32``. If a sparse matrix is provided, it will
+            be converted to a sparse ``csr_matrix``.
 
         Returns
         -------
diff --git a/sklearn/feature_extraction/image.py b/sklearn/feature_extraction/image.py
index 7b9c5c94e0..f4bfd7e533 100755
--- a/sklearn/feature_extraction/image.py
+++ b/sklearn/feature_extraction/image.py
@@ -152,8 +152,8 @@ def img_to_graph(img, mask=None, return_as=sparse.coo_matrix, dtype=None):
 
     Notes
     -----
-    For sklearn versions 0.14.1 and prior, return_as=np.ndarray was handled
-    by returning a dense np.matrix instance.  Going forward, np.ndarray
+    For scikit-learn versions 0.14.1 and prior, return_as=np.ndarray was
+    handled by returning a dense np.matrix instance.  Going forward, np.ndarray
     returns an np.ndarray, as expected.
 
     For compatibility, user code relying on this method should wrap its
@@ -188,8 +188,8 @@ def grid_to_graph(n_x, n_y, n_z=1, mask=None, return_as=sparse.coo_matrix,
 
     Notes
     -----
-    For sklearn versions 0.14.1 and prior, return_as=np.ndarray was handled
-    by returning a dense np.matrix instance.  Going forward, np.ndarray
+    For scikit-learn versions 0.14.1 and prior, return_as=np.ndarray was
+    handled by returning a dense np.matrix instance.  Going forward, np.ndarray
     returns an np.ndarray, as expected.
 
     For compatibility, user code relying on this method should wrap its
diff --git a/sklearn/gaussian_process/gpr.py b/sklearn/gaussian_process/gpr.py
index e04f2a6798..24ff1b058a 100755
--- a/sklearn/gaussian_process/gpr.py
+++ b/sklearn/gaussian_process/gpr.py
@@ -23,7 +23,8 @@ class GaussianProcessRegressor(BaseEstimator, RegressorMixin):
     The implementation is based on Algorithm 2.1 of Gaussian Processes
     for Machine Learning (GPML) by Rasmussen and Williams.
 
-    In addition to standard sklearn estimator API, GaussianProcessRegressor:
+    In addition to standard scikit-learn estimator API,
+    GaussianProcessRegressor:
 
        * allows prediction without prior fitting (based on the GP prior)
        * provides an additional method sample_y(X), which evaluates samples
diff --git a/sklearn/linear_model/cd_fast.pyx b/sklearn/linear_model/cd_fast.pyx
index 1dcd10feb1..63916c61f7 100755
--- a/sklearn/linear_model/cd_fast.pyx
+++ b/sklearn/linear_model/cd_fast.pyx
@@ -528,11 +528,11 @@ def sparse_enet_coordinate_descent(floating [:] w,
 @cython.boundscheck(False)
 @cython.wraparound(False)
 @cython.cdivision(True)
-def enet_coordinate_descent_gram(double[:] w, double alpha, double beta,
-                                 np.ndarray[double, ndim=2, mode='c'] Q,
-                                 np.ndarray[double, ndim=1, mode='c'] q,
-                                 np.ndarray[double, ndim=1] y,
-                                 int max_iter, double tol, object rng,
+def enet_coordinate_descent_gram(floating[:] w, floating alpha, floating beta,
+                                 np.ndarray[floating, ndim=2, mode='c'] Q,
+                                 np.ndarray[floating, ndim=1, mode='c'] q,
+                                 np.ndarray[floating, ndim=1] y,
+                                 int max_iter, floating tol, object rng,
                                  bint random=0, bint positive=0):
     """Cython version of the coordinate descent algorithm
         for Elastic-Net regression
@@ -546,34 +546,52 @@ def enet_coordinate_descent_gram(double[:] w, double alpha, double beta,
         q = X^T y
     """
 
+    # fused types version of BLAS functions
+    cdef DOT dot
+    cdef AXPY axpy
+    cdef ASUM asum
+
+    if floating is float:
+        dtype = np.float32
+        dot = sdot
+        axpy = saxpy
+        asum = sasum
+    else:
+        dtype = np.float64
+        dot = ddot
+        axpy = daxpy
+        asum = dasum
+
     # get the data information into easy vars
     cdef unsigned int n_samples = y.shape[0]
     cdef unsigned int n_features = Q.shape[0]
 
     # initial value "Q w" which will be kept of up to date in the iterations
-    cdef double[:] H = np.dot(Q, w)
+    cdef floating[:] H = np.dot(Q, w)
 
-    cdef double[:] XtA = np.zeros(n_features)
-    cdef double tmp
-    cdef double w_ii
-    cdef double d_w_max
-    cdef double w_max
-    cdef double d_w_ii
-    cdef double gap = tol + 1.0
-    cdef double d_w_tol = tol
-    cdef double dual_norm_XtA
+    cdef floating[:] XtA = np.zeros(n_features, dtype=dtype)
+    cdef floating tmp
+    cdef floating w_ii
+    cdef floating d_w_max
+    cdef floating w_max
+    cdef floating d_w_ii
+    cdef floating q_dot_w
+    cdef floating w_norm2
+    cdef floating gap = tol + 1.0
+    cdef floating d_w_tol = tol
+    cdef floating dual_norm_XtA
     cdef unsigned int ii
     cdef unsigned int n_iter = 0
     cdef unsigned int f_iter
     cdef UINT32_t rand_r_state_seed = rng.randint(0, RAND_R_MAX)
     cdef UINT32_t* rand_r_state = &rand_r_state_seed
 
-    cdef double y_norm2 = np.dot(y, y)
-    cdef double* w_ptr = <double*>&w[0]
-    cdef double* Q_ptr = &Q[0, 0]
-    cdef double* q_ptr = <double*>q.data
-    cdef double* H_ptr = &H[0]
-    cdef double* XtA_ptr = &XtA[0]
+    cdef floating y_norm2 = np.dot(y, y)
+    cdef floating* w_ptr = <floating*>&w[0]
+    cdef floating* Q_ptr = &Q[0, 0]
+    cdef floating* q_ptr = <floating*>q.data
+    cdef floating* H_ptr = &H[0]
+    cdef floating* XtA_ptr = &XtA[0]
     tol = tol * y_norm2
 
     if alpha == 0:
@@ -597,8 +615,8 @@ def enet_coordinate_descent_gram(double[:] w, double alpha, double beta,
 
                 if w_ii != 0.0:
                     # H -= w_ii * Q[ii]
-                    daxpy(n_features, -w_ii, Q_ptr + ii * n_features, 1,
-                          H_ptr, 1)
+                    axpy(n_features, -w_ii, Q_ptr + ii * n_features, 1,
+                         H_ptr, 1)
 
                 tmp = q[ii] - H[ii]
 
@@ -610,8 +628,8 @@ def enet_coordinate_descent_gram(double[:] w, double alpha, double beta,
 
                 if w[ii] != 0.0:
                     # H +=  w[ii] * Q[ii] # Update H = X.T X w
-                    daxpy(n_features, w[ii], Q_ptr + ii * n_features, 1,
-                          H_ptr, 1)
+                    axpy(n_features, w[ii], Q_ptr + ii * n_features, 1,
+                         H_ptr, 1)
 
                 # update the maximum absolute coefficient update
                 d_w_ii = fabs(w[ii] - w_ii)
@@ -627,7 +645,7 @@ def enet_coordinate_descent_gram(double[:] w, double alpha, double beta,
                 # criterion
 
                 # q_dot_w = np.dot(w, q)
-                q_dot_w = ddot(n_features, w_ptr, 1, q_ptr, 1)
+                q_dot_w = dot(n_features, w_ptr, 1, q_ptr, 1)
 
                 for ii in range(n_features):
                     XtA[ii] = q[ii] - H[ii] - beta * w[ii]
@@ -643,7 +661,7 @@ def enet_coordinate_descent_gram(double[:] w, double alpha, double beta,
                 R_norm2 = y_norm2 + tmp - 2.0 * q_dot_w
 
                 # w_norm2 = np.dot(w, w)
-                w_norm2 = ddot(n_features, &w[0], 1, &w[0], 1)
+                w_norm2 = dot(n_features, &w[0], 1, &w[0], 1)
 
                 if (dual_norm_XtA > alpha):
                     const = alpha / dual_norm_XtA
@@ -654,7 +672,7 @@ def enet_coordinate_descent_gram(double[:] w, double alpha, double beta,
                     gap = R_norm2
 
                 # The call to dasum is equivalent to the L1 norm of w
-                gap += (alpha * dasum(n_features, &w[0], 1) -
+                gap += (alpha * asum(n_features, &w[0], 1) -
                         const * y_norm2 +  const * q_dot_w +
                         0.5 * beta * (1 + const ** 2) * w_norm2)
 
diff --git a/sklearn/linear_model/stochastic_gradient.py b/sklearn/linear_model/stochastic_gradient.py
index 67bbd68e30..232ca90a77 100755
--- a/sklearn/linear_model/stochastic_gradient.py
+++ b/sklearn/linear_model/stochastic_gradient.py
@@ -734,7 +734,10 @@ def predict_proba(self):
         Elkan.
 
         Binary probability estimates for loss="modified_huber" are given by
-        (clip(decision_function(X), -1, 1) + 1) / 2.
+        (clip(decision_function(X), -1, 1) + 1) / 2. For other loss functions
+        it is necessary to perform proper probability calibration by wrapping
+        the classifier with
+        :class:`sklearn.calibration.CalibratedClassifierCV` instead.
 
         Parameters
         ----------
diff --git a/sklearn/linear_model/tests/test_coordinate_descent.py b/sklearn/linear_model/tests/test_coordinate_descent.py
index 9065c5b97d..41d687b6b3 100755
--- a/sklearn/linear_model/tests/test_coordinate_descent.py
+++ b/sklearn/linear_model/tests/test_coordinate_descent.py
@@ -682,9 +682,11 @@ def test_enet_float_precision():
         for fit_intercept in [True, False]:
             coef = {}
             intercept = {}
-            clf = ElasticNet(alpha=0.5, max_iter=100, precompute=False,
-                            fit_intercept=fit_intercept, normalize=normalize)
             for dtype in [np.float64, np.float32]:
+                clf = ElasticNet(alpha=0.5, max_iter=100, precompute=False,
+                                 fit_intercept=fit_intercept,
+                                 normalize=normalize)
+
                 X = dtype(X)
                 y = dtype(y)
                 ignore_warnings(clf.fit)(X, y)
@@ -694,8 +696,19 @@ def test_enet_float_precision():
 
                 assert_equal(clf.coef_.dtype, dtype)
 
+                # test precompute Gram array
+                Gram = X.T.dot(X)
+                clf_precompute = ElasticNet(alpha=0.5, max_iter=100,
+                                            precompute=Gram,
+                                            fit_intercept=fit_intercept,
+                                            normalize=normalize)
+                ignore_warnings(clf_precompute.fit)(X, y)
+                assert_array_almost_equal(clf.coef_, clf_precompute.coef_)
+                assert_array_almost_equal(clf.intercept_,
+                                          clf_precompute.intercept_)
+
             assert_array_almost_equal(coef[np.float32], coef[np.float64],
-                                    decimal=4)
+                                      decimal=4)
             assert_array_almost_equal(intercept[np.float32],
-                                    intercept[np.float64],
-                                    decimal=4)
+                                      intercept[np.float64],
+                                      decimal=4)
diff --git a/sklearn/metrics/cluster/expected_mutual_info_fast.pyx b/sklearn/metrics/cluster/expected_mutual_info_fast.pyx
index 2ef60701d6..d0c08be8d2 100755
--- a/sklearn/metrics/cluster/expected_mutual_info_fast.pyx
+++ b/sklearn/metrics/cluster/expected_mutual_info_fast.pyx
@@ -13,21 +13,21 @@ cimport cython
 from sklearn.utils.lgamma cimport lgamma
 
 np.import_array()
-
+ctypedef np.float64_t DOUBLE
 
 @cython.boundscheck(False)
 @cython.wraparound(False)
 def expected_mutual_information(contingency, int n_samples):
     """Calculate the expected mutual information for two labelings."""
     cdef int R, C
-    cdef float N, gln_N, emi, term2, term3, gln
-    cdef np.ndarray[double] gln_a, gln_b, gln_Na, gln_Nb, gln_nij, log_Nnij
-    cdef np.ndarray[double] nijs, term1
-    cdef np.ndarray[double, ndim=2] log_ab_outer
+    cdef DOUBLE N, gln_N, emi, term2, term3, gln
+    cdef np.ndarray[DOUBLE] gln_a, gln_b, gln_Na, gln_Nb, gln_nij, log_Nnij
+    cdef np.ndarray[DOUBLE] nijs, term1
+    cdef np.ndarray[DOUBLE, ndim=2] log_ab_outer
     cdef np.ndarray[np.int32_t] a, b
     #cdef np.ndarray[int, ndim=2] start, end
     R, C = contingency.shape
-    N = float(n_samples)
+    N = <DOUBLE>n_samples
     a = np.sum(contingency, axis=1).astype(np.int32)
     b = np.sum(contingency, axis=0).astype(np.int32)
     # There are three major terms to the EMI equation, which are multiplied to
diff --git a/sklearn/metrics/tests/test_pairwise.py b/sklearn/metrics/tests/test_pairwise.py
index 530dc6068a..0186316695 100755
--- a/sklearn/metrics/tests/test_pairwise.py
+++ b/sklearn/metrics/tests/test_pairwise.py
@@ -61,7 +61,8 @@ def test_pairwise_distances():
     Y_tuples = tuple([tuple([v for v in row]) for row in Y])
     S2 = pairwise_distances(X_tuples, Y_tuples, metric="euclidean")
     assert_array_almost_equal(S, S2)
-    # "cityblock" uses sklearn metric, cityblock (function) is scipy.spatial.
+    # "cityblock" uses scikit-learn metric, cityblock (function) is
+    # scipy.spatial.
     S = pairwise_distances(X, metric="cityblock")
     S2 = pairwise_distances(X, metric=cityblock)
     assert_equal(S.shape[0], S.shape[1])
@@ -78,7 +79,8 @@ def test_pairwise_distances():
     S3 = manhattan_distances(X, Y, size_threshold=10)
     assert_array_almost_equal(S, S3)
     # Test cosine as a string metric versus cosine callable
-    # "cosine" uses sklearn metric, cosine (function) is scipy.spatial
+    # The string "cosine" uses sklearn.metric,
+    # while the function cosine is scipy.spatial
     S = pairwise_distances(X, Y, metric="cosine")
     S2 = pairwise_distances(X, Y, metric=cosine)
     assert_equal(S.shape[0], X.shape[0])
@@ -330,7 +332,7 @@ def test_pairwise_distances_argmin_min():
     assert_equal(type(Dsp), np.ndarray)
     assert_equal(type(Esp), np.ndarray)
 
-    # Non-euclidean sklearn metric
+    # Non-euclidean scikit-learn metric
     D, E = pairwise_distances_argmin_min(X, Y, metric="manhattan")
     D2 = pairwise_distances_argmin(X, Y, metric="manhattan")
     assert_array_almost_equal(D, [0, 1])
diff --git a/sklearn/mixture/tests/test_gaussian_mixture.py b/sklearn/mixture/tests/test_gaussian_mixture.py
index 1ea55140d5..fab41933d0 100755
--- a/sklearn/mixture/tests/test_gaussian_mixture.py
+++ b/sklearn/mixture/tests/test_gaussian_mixture.py
@@ -902,7 +902,6 @@ def test_property():
                               covariance_type=covar_type, random_state=rng,
                               n_init=5)
         gmm.fit(X)
-        print(covar_type)
         if covar_type == 'full':
             for prec, covar in zip(gmm.precisions_, gmm.covariances_):
 
diff --git a/sklearn/naive_bayes.py b/sklearn/naive_bayes.py
index 9e3e0fb6f4..6b0623843c 100755
--- a/sklearn/naive_bayes.py
+++ b/sklearn/naive_bayes.py
@@ -388,7 +388,7 @@ def _partial_fit(self, X, y, classes=None, _refit=False,
         if not np.all(unique_y_in_classes):
             raise ValueError("The target label(s) %s in y do not exist in the "
                              "initial classes %s" %
-                             (y[~unique_y_in_classes], classes))
+                             (unique_y[~unique_y_in_classes], classes))
 
         for y_i in unique_y:
             i = classes.searchsorted(y_i)
diff --git a/sklearn/neural_network/rbm.py b/sklearn/neural_network/rbm.py
index da872c8b89..bdbc04e6be 100755
--- a/sklearn/neural_network/rbm.py
+++ b/sklearn/neural_network/rbm.py
@@ -243,7 +243,7 @@ def partial_fit(self, X, y=None):
                     0.01,
                     (self.n_components, X.shape[1])
                 ),
-                order='fortran')
+                order='F')
         if not hasattr(self, 'intercept_hidden_'):
             self.intercept_hidden_ = np.zeros(self.n_components, )
         if not hasattr(self, 'intercept_visible_'):
@@ -340,7 +340,7 @@ def fit(self, X, y=None):
 
         self.components_ = np.asarray(
             rng.normal(0, 0.01, (self.n_components, X.shape[1])),
-            order='fortran')
+            order='F')
         self.intercept_hidden_ = np.zeros(self.n_components, )
         self.intercept_visible_ = np.zeros(X.shape[1], )
         self.h_samples_ = np.zeros((self.batch_size, self.n_components))
diff --git a/sklearn/neural_network/tests/test_mlp.py b/sklearn/neural_network/tests/test_mlp.py
index bdf79ba197..17522f146b 100755
--- a/sklearn/neural_network/tests/test_mlp.py
+++ b/sklearn/neural_network/tests/test_mlp.py
@@ -458,7 +458,7 @@ def test_tolerance():
     # It should force the algorithm to exit the loop when it converges.
     X = [[3, 2], [1, 6]]
     y = [1, 0]
-    clf = MLPClassifier(tol=0.5, max_iter=3000, algorithm='sgd', verbose=10)
+    clf = MLPClassifier(tol=0.5, max_iter=3000, algorithm='sgd')
     clf.fit(X, y)
     assert_greater(clf.max_iter, clf.n_iter_)
 
@@ -499,7 +499,7 @@ def test_adaptive_learning_rate():
     X = [[3, 2], [1, 6]]
     y = [1, 0]
     clf = MLPClassifier(tol=0.5, max_iter=3000, algorithm='sgd',
-                        learning_rate='adaptive', verbose=10)
+                        learning_rate='adaptive')
     clf.fit(X, y)
     assert_greater(clf.max_iter, clf.n_iter_)
     assert_greater(1e-6, clf._optimizer.learning_rate)
diff --git a/sklearn/pipeline.py b/sklearn/pipeline.py
index 8777094738..91e4fef0ec 100755
--- a/sklearn/pipeline.py
+++ b/sklearn/pipeline.py
@@ -11,6 +11,7 @@
 
 from collections import defaultdict
 from warnings import warn
+from abc import ABCMeta, abstractmethod
 
 import numpy as np
 from scipy import sparse
@@ -20,12 +21,69 @@
 from .externals import six
 from .utils import tosequence
 from .utils.metaestimators import if_delegate_has_method
-from .externals.six import iteritems
 
 __all__ = ['Pipeline', 'FeatureUnion']
 
 
-class Pipeline(BaseEstimator):
+class _BasePipeline(six.with_metaclass(ABCMeta, BaseEstimator)):
+    """Handles parameter management for classifiers composed of named steps.
+    """
+
+    @abstractmethod
+    def __init__(self):
+        pass
+
+    def _replace_step(self, steps_attr, name, new_val):
+        # assumes `name` is a valid step name
+        new_steps = getattr(self, steps_attr)[:]
+        for i, (step_name, _) in enumerate(new_steps):
+            if step_name == name:
+                new_steps[i] = (name, new_val)
+                break
+        setattr(self, steps_attr, new_steps)
+
+    def _get_params(self, steps_attr, deep=True):
+        out = super(_BasePipeline, self).get_params(deep=False)
+        if not deep:
+            return out
+        steps = getattr(self, steps_attr)
+        out.update(steps)
+        for name, estimator in steps:
+            if estimator is None:
+                continue
+            for key, value in six.iteritems(estimator.get_params(deep=True)):
+                out['%s__%s' % (name, key)] = value
+        return out
+
+    def _set_params(self, steps_attr, **params):
+        # Ensure strict ordering of parameter setting:
+        # 1. All steps
+        if steps_attr in params:
+            setattr(self, steps_attr, params.pop(steps_attr))
+        # 2. Step replacement
+        step_names, _ = zip(*getattr(self, steps_attr))
+        for name in list(six.iterkeys(params)):
+            if '__' not in name and name in step_names:
+                self._replace_step(steps_attr, name, params.pop(name))
+        # 3. Step parameters and other initilisation arguments
+        super(_BasePipeline, self).set_params(**params)
+        return self
+
+    def _validate_names(self, names):
+        if len(set(names)) != len(names):
+            raise ValueError('Names provided are not unique: '
+                             '{0!r}'.format(list(names)))
+        invalid_names = set(names).intersection(self.get_params(deep=False))
+        if invalid_names:
+            raise ValueError('Step names conflict with constructor arguments: '
+                             '{0!r}'.format(sorted(invalid_names)))
+        invalid_names = [name for name in names if '__' in name]
+        if invalid_names:
+            raise ValueError('Step names must not contain __: got '
+                             '{0!r}'.format(invalid_names))
+
+
+class Pipeline(_BasePipeline):
     """Pipeline of transforms with a final estimator.
 
     Sequentially apply a list of transforms and a final estimator.
@@ -37,6 +95,9 @@ class Pipeline(BaseEstimator):
     cross-validated together while setting different parameters.
     For this, it enables setting parameters of the various steps using their
     names and the parameter name separated by a '__', as in the example below.
+    A step's estimator may be replaced entirely by setting the parameter
+    with its name to another estimator, or a transformer removed by setting
+    to None.
 
     Read more in the :ref:`User Guide <pipeline>`.
 
@@ -87,44 +148,67 @@ class Pipeline(BaseEstimator):
     # BaseEstimator interface
 
     def __init__(self, steps):
-        names, estimators = zip(*steps)
-        if len(dict(steps)) != len(steps):
-            raise ValueError("Provided step names are not unique: %s"
-                             % (names,))
-
         # shallow copy of steps
         self.steps = tosequence(steps)
-        transforms = estimators[:-1]
+        self._validate_steps()
+
+    def get_params(self, deep=True):
+        """Get parameters for this estimator.
+
+        Parameters
+        ----------
+        deep: boolean, optional
+            If True, will return the parameters for this estimator and
+            contained subobjects that are estimators.
+
+        Returns
+        -------
+        params : mapping of string to any
+            Parameter names mapped to their values.
+        """
+        return self._get_params('steps', deep=deep)
+
+    def set_params(self, **kwargs):
+        """Set the parameters of this estimator.
+
+        Valid parameter keys can be listed with ``get_params()``.
+
+        Returns
+        -------
+        self
+        """
+        self._set_params('steps', **kwargs)
+        return self
+
+    def _validate_steps(self):
+        names, estimators = zip(*self.steps)
+
+        # validate names
+        self._validate_names(names)
+
+        # validate estimators
+        transformers = estimators[:-1]
         estimator = estimators[-1]
 
-        for t in transforms:
+        for t in transformers:
+            if t is None:
+                continue
             if (not (hasattr(t, "fit") or hasattr(t, "fit_transform")) or not
                     hasattr(t, "transform")):
-                raise TypeError("All intermediate steps of the chain should "
-                                "be transforms and implement fit and transform"
-                                " '%s' (type %s) doesn't)" % (t, type(t)))
-
-        if not hasattr(estimator, "fit"):
-            raise TypeError("Last step of chain should implement fit "
-                            "'%s' (type %s) doesn't)"
+                raise TypeError("All intermediate steps should be "
+                                "transformers and implement fit and transform."
+                                " '%s' (type %s) doesn't" % (t, type(t)))
+
+        # We allow last estimator to be None as an identity transformation
+        if estimator is not None and not hasattr(estimator, "fit"):
+            raise TypeError("Last step of Pipeline should implement fit. "
+                            "'%s' (type %s) doesn't"
                             % (estimator, type(estimator)))
 
     @property
     def _estimator_type(self):
         return self.steps[-1][1]._estimator_type
 
-    def get_params(self, deep=True):
-        if not deep:
-            return super(Pipeline, self).get_params(deep=False)
-        else:
-            out = self.named_steps
-            for name, step in six.iteritems(self.named_steps):
-                for key, value in six.iteritems(step.get_params(deep=True)):
-                    out['%s__%s' % (name, key)] = value
-
-            out.update(super(Pipeline, self).get_params(deep=False))
-            return out
-
     @property
     def named_steps(self):
         return dict(self.steps)
@@ -135,22 +219,30 @@ def _final_estimator(self):
 
     # Estimator interface
 
-    def _pre_transform(self, X, y=None, **fit_params):
-        fit_params_steps = dict((step, {}) for step, _ in self.steps)
+    def _fit(self, X, y=None, **fit_params):
+        self._validate_steps()
+        fit_params_steps = dict((name, {}) for name, step in self.steps
+                                if step is not None)
         for pname, pval in six.iteritems(fit_params):
             step, param = pname.split('__', 1)
             fit_params_steps[step][param] = pval
         Xt = X
         for name, transform in self.steps[:-1]:
-            if hasattr(transform, "fit_transform"):
+            if transform is None:
+                pass
+            elif hasattr(transform, "fit_transform"):
                 Xt = transform.fit_transform(Xt, y, **fit_params_steps[name])
             else:
                 Xt = transform.fit(Xt, y, **fit_params_steps[name]) \
                               .transform(Xt)
+        if self._final_estimator is None:
+            return Xt, {}
         return Xt, fit_params_steps[self.steps[-1][0]]
 
     def fit(self, X, y=None, **fit_params):
-        """Fit all the transforms one after the other and transform the
+        """Fit the model
+
+        Fit all the transforms one after the other and transform the
         data, then fit the transformed data using the final estimator.
 
         Parameters
@@ -158,17 +250,31 @@ def fit(self, X, y=None, **fit_params):
         X : iterable
             Training data. Must fulfill input requirements of first step of the
             pipeline.
+
         y : iterable, default=None
             Training targets. Must fulfill label requirements for all steps of
             the pipeline.
+
+        **fit_params : dict of string -> object
+            Parameters passed to the ``fit`` method of each step, where
+            each parameter name is prefixed such that parameter ``p`` for step
+            ``s`` has key ``s__p``.
+
+        Returns
+        -------
+        self : Pipeline
+            This estimator
         """
-        Xt, fit_params = self._pre_transform(X, y, **fit_params)
-        self.steps[-1][-1].fit(Xt, y, **fit_params)
+        Xt, fit_params = self._fit(X, y, **fit_params)
+        if self._final_estimator is not None:
+            self._final_estimator.fit(Xt, y, **fit_params)
         return self
 
     def fit_transform(self, X, y=None, **fit_params):
-        """Fit all the transforms one after the other and transform the
-        data, then use fit_transform on transformed data using the final
+        """Fit the model and transform with the final estimator
+
+        Fits all the transforms one after the other and transforms the
+        data, then uses fit_transform on transformed data with the final
         estimator.
 
         Parameters
@@ -180,28 +286,44 @@ def fit_transform(self, X, y=None, **fit_params):
         y : iterable, default=None
             Training targets. Must fulfill label requirements for all steps of
             the pipeline.
+
+        **fit_params : dict of string -> object
+            Parameters passed to the ``fit`` method of each step, where
+            each parameter name is prefixed such that parameter ``p`` for step
+            ``s`` has key ``s__p``.
+
+        Returns
+        -------
+        Xt : array-like, shape = [n_samples, n_transformed_features]
+            Transformed samples
         """
-        Xt, fit_params = self._pre_transform(X, y, **fit_params)
-        if hasattr(self.steps[-1][-1], 'fit_transform'):
-            return self.steps[-1][-1].fit_transform(Xt, y, **fit_params)
+        last_step = self._final_estimator
+        Xt, fit_params = self._fit(X, y, **fit_params)
+        if hasattr(last_step, 'fit_transform'):
+            return last_step.fit_transform(Xt, y, **fit_params)
+        elif last_step is None:
+            return Xt
         else:
-            return self.steps[-1][-1].fit(Xt, y, **fit_params).transform(Xt)
+            return last_step.fit(Xt, y, **fit_params).transform(Xt)
 
     @if_delegate_has_method(delegate='_final_estimator')
     def predict(self, X):
-        """Applies transforms to the data, and the predict method of the
-        final estimator. Valid only if the final estimator implements
-        predict.
+        """Apply transforms to the data, and predict with the final estimator
 
         Parameters
         ----------
         X : iterable
             Data to predict on. Must fulfill input requirements of first step
             of the pipeline.
+
+        Returns
+        -------
+        y_pred : array-like
         """
         Xt = X
         for name, transform in self.steps[:-1]:
-            Xt = transform.transform(Xt)
+            if transform is not None:
+                Xt = transform.transform(Xt)
         return self.steps[-1][-1].predict(Xt)
 
     @if_delegate_has_method(delegate='_final_estimator')
@@ -217,122 +339,172 @@ def fit_predict(self, X, y=None, **fit_params):
         X : iterable
             Training data. Must fulfill input requirements of first step of
             the pipeline.
+
         y : iterable, default=None
             Training targets. Must fulfill label requirements for all steps
             of the pipeline.
+
+        **fit_params : dict of string -> object
+            Parameters passed to the ``fit`` method of each step, where
+            each parameter name is prefixed such that parameter ``p`` for step
+            ``s`` has key ``s__p``.
+
+        Returns
+        -------
+        y_pred : array-like
         """
-        Xt, fit_params = self._pre_transform(X, y, **fit_params)
+        Xt = X
+        for name, transform in self.steps[:-1]:
+            if transform is not None:
+                Xt = transform.transform(Xt)
         return self.steps[-1][-1].fit_predict(Xt, y, **fit_params)
 
     @if_delegate_has_method(delegate='_final_estimator')
     def predict_proba(self, X):
-        """Applies transforms to the data, and the predict_proba method of the
-        final estimator. Valid only if the final estimator implements
-        predict_proba.
+        """Apply transforms, and predict_proba of the final estimator
 
         Parameters
         ----------
         X : iterable
             Data to predict on. Must fulfill input requirements of first step
             of the pipeline.
+
+        Returns
+        -------
+        y_proba : array-like, shape = [n_samples, n_classes]
         """
         Xt = X
         for name, transform in self.steps[:-1]:
-            Xt = transform.transform(Xt)
+            if transform is not None:
+                Xt = transform.transform(Xt)
         return self.steps[-1][-1].predict_proba(Xt)
 
     @if_delegate_has_method(delegate='_final_estimator')
     def decision_function(self, X):
-        """Applies transforms to the data, and the decision_function method of
-        the final estimator. Valid only if the final estimator implements
-        decision_function.
+        """Apply transforms, and decision_function of the final estimator
 
         Parameters
         ----------
         X : iterable
             Data to predict on. Must fulfill input requirements of first step
             of the pipeline.
+
+        Returns
+        -------
+        y_score : array-like, shape = [n_samples, n_classes]
         """
         Xt = X
         for name, transform in self.steps[:-1]:
-            Xt = transform.transform(Xt)
+            if transform is not None:
+                Xt = transform.transform(Xt)
         return self.steps[-1][-1].decision_function(Xt)
 
     @if_delegate_has_method(delegate='_final_estimator')
     def predict_log_proba(self, X):
-        """Applies transforms to the data, and the predict_log_proba method of
-        the final estimator. Valid only if the final estimator implements
-        predict_log_proba.
+        """Apply transforms, and predict_log_proba of the final estimator
 
         Parameters
         ----------
         X : iterable
             Data to predict on. Must fulfill input requirements of first step
             of the pipeline.
+
+        Returns
+        -------
+        y_score : array-like, shape = [n_samples, n_classes]
         """
         Xt = X
         for name, transform in self.steps[:-1]:
-            Xt = transform.transform(Xt)
+            if transform is not None:
+                Xt = transform.transform(Xt)
         return self.steps[-1][-1].predict_log_proba(Xt)
 
-    @if_delegate_has_method(delegate='_final_estimator')
-    def transform(self, X):
-        """Applies transforms to the data, and the transform method of the
-        final estimator. Valid only if the final estimator implements
-        transform.
+    @property
+    def transform(self):
+        """Apply transforms, and transform with the final estimator
+
+        This also works where final estimator is ``None``: all prior
+        transformations are applied.
 
         Parameters
         ----------
         X : iterable
-            Data to predict on. Must fulfill input requirements of first step
+            Data to transform. Must fulfill input requirements of first step
             of the pipeline.
+
+        Returns
+        -------
+        Xt : array-like, shape = [n_samples, n_transformed_features]
         """
+        # _final_estimator is None or has transform, otherwise attribute error
+        if self._final_estimator is not None:
+            self._final_estimator.transform
+        return self._transform
+
+    def _transform(self, X):
         Xt = X
         for name, transform in self.steps:
-            Xt = transform.transform(Xt)
+            if transform is not None:
+                Xt = transform.transform(Xt)
         return Xt
 
-    @if_delegate_has_method(delegate='_final_estimator')
-    def inverse_transform(self, X):
-        """Applies inverse transform to the data.
-        Starts with the last step of the pipeline and applies
-        ``inverse_transform`` in inverse order of the pipeline steps.
-        Valid only if all steps of the pipeline implement inverse_transform.
+    @property
+    def inverse_transform(self):
+        """Apply inverse transformations in reverse order
+
+        All estimators in the pipeline must support ``inverse_transform``.
 
         Parameters
         ----------
-        X : iterable
-            Data to inverse transform. Must fulfill output requirements of the
-            last step of the pipeline.
+        Xt : array-like, shape = [n_samples, n_transformed_features]
+            Data samples, where ``n_samples`` is the number of samples and
+            ``n_features`` is the number of features. Must fulfill
+            input requirements of last step of pipeline's
+            ``inverse_transform`` method.
+
+        Returns
+        -------
+        Xt : array-like, shape = [n_samples, n_features]
         """
-        if X.ndim == 1:
+        # raise AttributeError if necessary for hasattr behaviour
+        for name, transform in self.steps:
+            if transform is not None:
+                transform.inverse_transform
+        return self._inverse_transform
+
+    def _inverse_transform(self, X):
+        if hasattr(X, 'ndim') and X.ndim == 1:
             warn("From version 0.19, a 1d X will not be reshaped in"
                  " pipeline.inverse_transform any more.", FutureWarning)
             X = X[None, :]
         Xt = X
-        for name, step in self.steps[::-1]:
-            Xt = step.inverse_transform(Xt)
+        for name, transform in self.steps[::-1]:
+            if transform is not None:
+                Xt = transform.inverse_transform(Xt)
         return Xt
 
     @if_delegate_has_method(delegate='_final_estimator')
     def score(self, X, y=None):
-        """Applies transforms to the data, and the score method of the
-        final estimator. Valid only if the final estimator implements
-        score.
+        """Apply transforms, and score with the final estimator
 
         Parameters
         ----------
         X : iterable
-            Data to score. Must fulfill input requirements of first step of the
-            pipeline.
+            Data to predict on. Must fulfill input requirements of first step
+            of the pipeline.
 
         y : iterable, default=None
             Targets used for scoring. Must fulfill label requirements for all
             steps of the pipeline.
+
+        Returns
+        -------
+        score : float
         """
         Xt = X
         for name, transform in self.steps[:-1]:
-            Xt = transform.transform(Xt)
+            if transform is not None:
+                Xt = transform.transform(Xt)
         return self.steps[-1][-1].score(Xt, y)
 
     @property
@@ -377,7 +549,8 @@ def make_pipeline(*steps):
     --------
     >>> from sklearn.naive_bayes import GaussianNB
     >>> from sklearn.preprocessing import StandardScaler
-    >>> make_pipeline(StandardScaler(), GaussianNB(priors=None))    # doctest: +NORMALIZE_WHITESPACE
+    >>> make_pipeline(StandardScaler(), GaussianNB(priors=None))
+    ...     # doctest: +NORMALIZE_WHITESPACE
     Pipeline(steps=[('standardscaler',
                      StandardScaler(copy=True, with_mean=True, with_std=True)),
                     ('gaussiannb', GaussianNB(priors=None))])
@@ -393,38 +566,38 @@ def _fit_one_transformer(transformer, X, y):
     return transformer.fit(X, y)
 
 
-def _transform_one(transformer, name, X, transformer_weights):
-    if transformer_weights is not None and name in transformer_weights:
-        # if we have a weight for this transformer, multiply output
-        return transformer.transform(X) * transformer_weights[name]
-    return transformer.transform(X)
+def _transform_one(transformer, name, weight, X):
+    res = transformer.transform(X)
+    # if we have a weight for this transformer, multiply output
+    if weight is None:
+        return res
+    return res * weight
 
 
-def _fit_transform_one(transformer, name, X, y, transformer_weights,
+def _fit_transform_one(transformer, name, weight, X, y,
                        **fit_params):
-    if transformer_weights is not None and name in transformer_weights:
-        # if we have a weight for this transformer, multiply output
-        if hasattr(transformer, 'fit_transform'):
-            X_transformed = transformer.fit_transform(X, y, **fit_params)
-            return X_transformed * transformer_weights[name], transformer
-        else:
-            X_transformed = transformer.fit(X, y, **fit_params).transform(X)
-            return X_transformed * transformer_weights[name], transformer
     if hasattr(transformer, 'fit_transform'):
-        X_transformed = transformer.fit_transform(X, y, **fit_params)
-        return X_transformed, transformer
+        res = transformer.fit_transform(X, y, **fit_params)
     else:
-        X_transformed = transformer.fit(X, y, **fit_params).transform(X)
-        return X_transformed, transformer
+        res = transformer.fit(X, y, **fit_params).transform(X)
+    # if we have a weight for this transformer, multiply output
+    if weight is None:
+        return res, transformer
+    return res * weight, transformer
 
 
-class FeatureUnion(BaseEstimator, TransformerMixin):
+class FeatureUnion(_BasePipeline, TransformerMixin):
     """Concatenates results of multiple transformer objects.
 
     This estimator applies a list of transformer objects in parallel to the
     input data, then concatenates the results. This is useful to combine
     several feature extraction mechanisms into a single transformer.
 
+    Parameters of the transformers may be set using its name and the parameter
+    name separated by a '__'. A transformer may be replaced entirely by
+    setting the parameter with its name to another transformer,
+    or removed by setting to ``None``.
+
     Read more in the :ref:`User Guide <feature_union>`.
 
     Parameters
@@ -442,9 +615,62 @@ class FeatureUnion(BaseEstimator, TransformerMixin):
 
     """
     def __init__(self, transformer_list, n_jobs=1, transformer_weights=None):
-        self.transformer_list = transformer_list
+        self.transformer_list = tosequence(transformer_list)
         self.n_jobs = n_jobs
         self.transformer_weights = transformer_weights
+        self._validate_transformers()
+
+    def get_params(self, deep=True):
+        """Get parameters for this estimator.
+
+        Parameters
+        ----------
+        deep: boolean, optional
+            If True, will return the parameters for this estimator and
+            contained subobjects that are estimators.
+
+        Returns
+        -------
+        params : mapping of string to any
+            Parameter names mapped to their values.
+        """
+        return self._get_params('transformer_list', deep=deep)
+
+    def set_params(self, **kwargs):
+        """Set the parameters of this estimator.
+
+        Valid parameter keys can be listed with ``get_params()``.
+
+        Returns
+        -------
+        self
+        """
+        self._set_params('transformer_list', **kwargs)
+        return self
+
+    def _validate_transformers(self):
+        names, transformers = zip(*self.transformer_list)
+
+        # validate names
+        self._validate_names(names)
+
+        # validate estimators
+        for t in transformers:
+            if t is None:
+                continue
+            if (not (hasattr(t, "fit") or hasattr(t, "fit_transform")) or not
+                    hasattr(t, "transform")):
+                raise TypeError("All estimators should implement fit and "
+                                "transform. '%s' (type %s) doesn't" %
+                                (t, type(t)))
+
+    def _iter(self):
+        """Generate (name, est, weight) tuples excluding None transformers
+        """
+        get_weight = (self.transformer_weights or {}).get
+        return ((name, trans, get_weight(name))
+                for name, trans in self.transformer_list
+                if trans is not None)
 
     def get_feature_names(self):
         """Get feature names from all transformers.
@@ -455,10 +681,11 @@ def get_feature_names(self):
             Names of the features produced by transform.
         """
         feature_names = []
-        for name, trans in self.transformer_list:
+        for name, trans, weight in self._iter():
             if not hasattr(trans, 'get_feature_names'):
-                raise AttributeError("Transformer %s does not provide"
-                                     " get_feature_names." % str(name))
+                raise AttributeError("Transformer %s (type %s) does not "
+                                     "provide get_feature_names."
+                                     % (str(name), type(trans).__name__))
             feature_names.extend([name + "__" + f for f in
                                   trans.get_feature_names()])
         return feature_names
@@ -468,35 +695,50 @@ def fit(self, X, y=None):
 
         Parameters
         ----------
-        X : array-like or sparse matrix, shape (n_samples, n_features)
+        X : iterable or array-like, depending on transformers
             Input data, used to fit transformers.
+
+        y : array-like, shape (n_samples, ...), optional
+            Targets for supervised learning.
+
+        Returns
+        -------
+        self : FeatureUnion
+            This estimator
         """
+        self._validate_transformers()
         transformers = Parallel(n_jobs=self.n_jobs)(
             delayed(_fit_one_transformer)(trans, X, y)
-            for name, trans in self.transformer_list)
+            for _, trans, _ in self._iter())
         self._update_transformer_list(transformers)
         return self
 
     def fit_transform(self, X, y=None, **fit_params):
-        """Fit all transformers using X, transform the data and concatenate
-        results.
+        """Fit all transformers, transform the data and concatenate results.
 
         Parameters
         ----------
-        X : array-like or sparse matrix, shape (n_samples, n_features)
+        X : iterable or array-like, depending on transformers
             Input data to be transformed.
 
+        y : array-like, shape (n_samples, ...), optional
+            Targets for supervised learning.
+
         Returns
         -------
         X_t : array-like or sparse matrix, shape (n_samples, sum_n_components)
             hstack of results of transformers. sum_n_components is the
             sum of n_components (output dimension) over transformers.
         """
+        self._validate_transformers()
         result = Parallel(n_jobs=self.n_jobs)(
-            delayed(_fit_transform_one)(trans, name, X, y,
-                                        self.transformer_weights, **fit_params)
-            for name, trans in self.transformer_list)
+            delayed(_fit_transform_one)(trans, name, weight, X, y,
+                                        **fit_params)
+            for name, trans, weight in self._iter())
 
+        if not result:
+            # All transformers are None
+            return np.zeros((X.shape[0], 0))
         Xs, transformers = zip(*result)
         self._update_transformer_list(transformers)
         if any(sparse.issparse(f) for f in Xs):
@@ -510,7 +752,7 @@ def transform(self, X):
 
         Parameters
         ----------
-        X : array-like or sparse matrix, shape (n_samples, n_features)
+        X : iterable or array-like, depending on transformers
             Input data to be transformed.
 
         Returns
@@ -520,29 +762,22 @@ def transform(self, X):
             sum of n_components (output dimension) over transformers.
         """
         Xs = Parallel(n_jobs=self.n_jobs)(
-            delayed(_transform_one)(trans, name, X, self.transformer_weights)
-            for name, trans in self.transformer_list)
+            delayed(_transform_one)(trans, name, weight, X)
+            for name, trans, weight in self._iter())
+        if not Xs:
+            # All transformers are None
+            return np.zeros((X.shape[0], 0))
         if any(sparse.issparse(f) for f in Xs):
             Xs = sparse.hstack(Xs).tocsr()
         else:
             Xs = np.hstack(Xs)
         return Xs
 
-    def get_params(self, deep=True):
-        if not deep:
-            return super(FeatureUnion, self).get_params(deep=False)
-        else:
-            out = dict(self.transformer_list)
-            for name, trans in self.transformer_list:
-                for key, value in iteritems(trans.get_params(deep=True)):
-                    out['%s__%s' % (name, key)] = value
-            out.update(super(FeatureUnion, self).get_params(deep=False))
-            return out
-
     def _update_transformer_list(self, transformers):
+        transformers = iter(transformers)
         self.transformer_list[:] = [
-            (name, new)
-            for ((name, old), new) in zip(self.transformer_list, transformers)
+            (name, None if old is None else next(transformers))
+            for name, old in self.transformer_list
         ]
 
 
diff --git a/sklearn/preprocessing/tests/test_data.py b/sklearn/preprocessing/tests/test_data.py
index 398c1541a8..39360ebdd7 100755
--- a/sklearn/preprocessing/tests/test_data.py
+++ b/sklearn/preprocessing/tests/test_data.py
@@ -202,7 +202,7 @@ def test_scale_1d():
 
 @skip_if_32bit
 def test_standard_scaler_numerical_stability():
-    """Test numerical stability of scaling"""
+    # Test numerical stability of scaling
     # np.log(1e-5) is taken because of its floating point representation
     # was empirically found to cause numerical problems with np.mean & np.std.
 
@@ -802,7 +802,7 @@ def test_scale_input_finiteness_validation():
 
 
 def test_robust_scaler_2d_arrays():
-    """Test robust scaling of 2d array along first axis"""
+    # Test robust scaling of 2d array along first axis
     rng = np.random.RandomState(0)
     X = rng.randn(4, 5)
     X[:, 0] = 0.0  # first feature is always of zero
@@ -910,7 +910,7 @@ def test_robust_scale_axis1():
 
 
 def test_robust_scaler_zero_variance_features():
-    """Check RobustScaler on toy data with zero variance features"""
+    # Check RobustScaler on toy data with zero variance features
     X = [[0., 1., +0.5],
          [0., 1., -0.1],
          [0., 1., +1.1]]
@@ -943,7 +943,7 @@ def test_robust_scaler_zero_variance_features():
 
 
 def test_maxabs_scaler_zero_variance_features():
-    """Check MaxAbsScaler on toy data with zero variance features"""
+    # Check MaxAbsScaler on toy data with zero variance features
     X = [[0., 1., +0.5],
          [0., 1., -0.3],
          [0., 1., +1.5],
@@ -1400,9 +1400,9 @@ def test_center_kernel():
 
 
 def test_cv_pipeline_precomputed():
-    """Cross-validate a regression on four coplanar points with the same
-    value. Use precomputed kernel to ensure Pipeline with KernelCenterer
-    is treated as a _pairwise operation."""
+    # Cross-validate a regression on four coplanar points with the same
+    # value. Use precomputed kernel to ensure Pipeline with KernelCenterer
+    # is treated as a _pairwise operation.
     X = np.array([[3, 0, 0], [0, 3, 0], [0, 0, 3], [1, 1, 1]])
     y_true = np.ones((4,))
     K = X.dot(X.T)
diff --git a/sklearn/tests/test_base.py b/sklearn/tests/test_base.py
index 63b4283628..acda133980 100755
--- a/sklearn/tests/test_base.py
+++ b/sklearn/tests/test_base.py
@@ -82,7 +82,7 @@ def predict(self, X=None):
 
 
 class VargEstimator(BaseEstimator):
-    """Sklearn estimators shouldn't have vargs."""
+    """scikit-learn estimators shouldn't have vargs."""
     def __init__(self, *vargs):
         pass
 
@@ -260,6 +260,11 @@ def test_score_sample_weight():
                              "are unexpectedly equal")
 
 
+class TreeNoVersion(DecisionTreeClassifier):
+    def __getstate__(self):
+        return self.__dict__
+
+
 def test_pickle_version_warning():
     # check that warnings are raised when unpickling in a different version
 
@@ -267,6 +272,7 @@ def test_pickle_version_warning():
     iris = datasets.load_iris()
     tree = DecisionTreeClassifier().fit(iris.data, iris.target)
     tree_pickle = pickle.dumps(tree)
+    assert_true(b"version" in tree_pickle)
     assert_no_warnings(pickle.loads, tree_pickle)
 
     # check that warning is raised on different version
@@ -279,18 +285,16 @@ def test_pickle_version_warning():
     assert_warns_message(UserWarning, message, pickle.loads, tree_pickle_other)
 
     # check that not including any version also works:
-    # don't do this at home, kids
-    from sklearn.base import BaseEstimator
-    old_getstate = BaseEstimator.__getstate__
-    del BaseEstimator.__getstate__
-    # tree lost its getstate, like pre-0.18
-    assert_false(hasattr(tree, "__getstate__"))
+    # TreeNoVersion has no getstate, like pre-0.18
+    tree = TreeNoVersion().fit(iris.data, iris.target)
+
     tree_pickle_noversion = pickle.dumps(tree)
+    assert_false(b"version" in tree_pickle_noversion)
     message = message.replace("something", "pre-0.18")
+    message = message.replace("DecisionTreeClassifier", "TreeNoVersion")
     # check we got the warning about using pre-0.18 pickle
     assert_warns_message(UserWarning, message, pickle.loads, tree_pickle_noversion)
-    BaseEstimator.__getstate__ = old_getstate
 
     # check that no warning is raised for external estimators
-    DecisionTreeClassifier.__module__ = "notsklearn"
+    TreeNoVersion.__module__ = "notsklearn"
     assert_no_warnings(pickle.loads, tree_pickle_noversion)
diff --git a/sklearn/tests/test_pipeline.py b/sklearn/tests/test_pipeline.py
index a8c5fff4ef..518c966169 100755
--- a/sklearn/tests/test_pipeline.py
+++ b/sklearn/tests/test_pipeline.py
@@ -14,8 +14,9 @@
 from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_warns_message
+from sklearn.utils.testing import assert_dict_equal
 
-from sklearn.base import clone
+from sklearn.base import clone, BaseEstimator
 from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline, make_union
 from sklearn.svm import SVC
 from sklearn.linear_model import LogisticRegression
@@ -38,7 +39,7 @@
 )
 
 
-class IncorrectT(object):
+class NoFit(object):
     """Small class to test parameter dispatching.
     """
 
@@ -47,7 +48,7 @@ def __init__(self, a=None, b=None):
         self.b = b
 
 
-class T(IncorrectT):
+class NoTrans(NoFit):
 
     def fit(self, X, y):
         return self
@@ -60,8 +61,12 @@ def set_params(self, **params):
         return self
 
 
-class TransfT(T):
+class NoInvTransf(NoTrans):
+    def transform(self, X, y=None):
+        return X
+
 
+class Transf(NoInvTransf):
     def transform(self, X, y=None):
         return X
 
@@ -69,7 +74,29 @@ def inverse_transform(self, X):
         return X
 
 
-class FitParamT(object):
+class Mult(BaseEstimator):
+    def __init__(self, mult=1):
+        self.mult = mult
+
+    def fit(self, X, y):
+        return self
+
+    def transform(self, X):
+        return np.asarray(X) * self.mult
+
+    def inverse_transform(self, X):
+        return np.asarray(X) / self.mult
+
+    def predict(self, X):
+        return (np.asarray(X) * self.mult).sum(axis=1)
+
+    predict_proba = predict_log_proba = decision_function = predict
+
+    def score(self, X, y=None):
+        return np.sum(X)
+
+
+class FitParamT(BaseEstimator):
     """Mock classifier
     """
 
@@ -88,9 +115,12 @@ def test_pipeline_init():
     assert_raises(TypeError, Pipeline)
     # Check that we can't instantiate pipelines with objects without fit
     # method
-    pipe = assert_raises(TypeError, Pipeline, [('svc', IncorrectT)])
+    assert_raises_regex(TypeError,
+                        'Last step of Pipeline should implement fit. '
+                        '.*NoFit.*',
+                        Pipeline, [('clf', NoFit())])
     # Smoke test with only an estimator
-    clf = T()
+    clf = NoTrans()
     pipe = Pipeline([('svc', clf)])
     assert_equal(pipe.get_params(deep=True),
                  dict(svc__a=None, svc__b=None, svc=clf,
@@ -108,8 +138,12 @@ def test_pipeline_init():
     filter1 = SelectKBest(f_classif)
     pipe = Pipeline([('anova', filter1), ('svc', clf)])
 
-    # Check that we can't use the same stage name twice
-    assert_raises(ValueError, Pipeline, [('svc', SVC()), ('svc', SVC())])
+    # Check that we can't instantiate with non-transformers on the way
+    # Note that NoTrans implements fit, but not transform
+    assert_raises_regex(TypeError,
+                        'All intermediate steps should be transformers'
+                        '.*\\bNoTrans\\b.*',
+                        Pipeline, [('t', NoTrans()), ('svc', clf)])
 
     # Check that params are set
     pipe.set_params(svc__C=0.1)
@@ -160,7 +194,7 @@ def test_pipeline_methods_anova():
 
 def test_pipeline_fit_params():
     # Test that the pipeline can take fit parameters
-    pipe = Pipeline([('transf', TransfT()), ('clf', FitParamT())])
+    pipe = Pipeline([('transf', Transf()), ('clf', FitParamT())])
     pipe.fit(X=None, y=None, clf__should_succeed=True)
     # classifier should return True
     assert_true(pipe.predict(None))
@@ -297,17 +331,24 @@ def test_feature_union():
     assert_equal(fs.fit_transform(X, y).shape, (X.shape[0], 4))
 
     # test it works with transformers missing fit_transform
-    fs = FeatureUnion([("mock", TransfT()), ("svd", svd), ("select", select)])
+    fs = FeatureUnion([("mock", Transf()), ("svd", svd), ("select", select)])
     X_transformed = fs.fit_transform(X, y)
     assert_equal(X_transformed.shape, (X.shape[0], 8))
 
+    # test error if some elements do not support transform
+    assert_raises_regex(TypeError,
+                        'All estimators should implement fit and '
+                        'transform.*\\bNoTrans\\b',
+                        FeatureUnion,
+                        [("transform", Transf()), ("no_transform", NoTrans())])
+
 
 def test_make_union():
     pca = PCA(svd_solver='full')
-    mock = TransfT()
+    mock = Transf()
     fu = make_union(pca, mock)
     names, transformers = zip(*fu.transformer_list)
-    assert_equal(names, ("pca", "transft"))
+    assert_equal(names, ("pca", "transf"))
     assert_equal(transformers, (pca, mock))
 
 
@@ -336,28 +377,149 @@ def test_pipeline_fit_transform():
     iris = load_iris()
     X = iris.data
     y = iris.target
-    transft = TransfT()
-    pipeline = Pipeline([('mock', transft)])
+    transf = Transf()
+    pipeline = Pipeline([('mock', transf)])
 
     # test fit_transform:
     X_trans = pipeline.fit_transform(X, y)
-    X_trans2 = transft.fit(X, y).transform(X)
+    X_trans2 = transf.fit(X, y).transform(X)
     assert_array_almost_equal(X_trans, X_trans2)
 
 
-def test_make_pipeline():
-    t1 = TransfT()
-    t2 = TransfT()
+def test_set_pipeline_steps():
+    transf1 = Transf()
+    transf2 = Transf()
+    pipeline = Pipeline([('mock', transf1)])
+    assert_true(pipeline.named_steps['mock'] is transf1)
+
+    # Directly setting attr
+    pipeline.steps = [('mock2', transf2)]
+    assert_true('mock' not in pipeline.named_steps)
+    assert_true(pipeline.named_steps['mock2'] is transf2)
+    assert_equal([('mock2', transf2)], pipeline.steps)
+
+    # Using set_params
+    pipeline.set_params(steps=[('mock', transf1)])
+    assert_equal([('mock', transf1)], pipeline.steps)
+
+    # Using set_params to replace single step
+    pipeline.set_params(mock=transf2)
+    assert_equal([('mock', transf2)], pipeline.steps)
+
+    # With invalid data
+    pipeline.set_params(steps=[('junk', ())])
+    assert_raises(TypeError, pipeline.fit, [[1]], [1])
+    assert_raises(TypeError, pipeline.fit_transform, [[1]], [1])
+
+
+def test_set_pipeline_step_none():
+    # Test setting Pipeline steps to None
+    X = np.array([[1]])
+    y = np.array([1])
+    mult2 = Mult(mult=2)
+    mult3 = Mult(mult=3)
+    mult5 = Mult(mult=5)
+
+    def make():
+        return Pipeline([('m2', mult2), ('m3', mult3), ('last', mult5)])
+
+    pipeline = make()
+
+    exp = 2 * 3 * 5
+    assert_array_equal([[exp]], pipeline.fit_transform(X, y))
+    assert_array_equal([exp], pipeline.fit(X).predict(X))
+    assert_array_equal(X, pipeline.inverse_transform([[exp]]))
+
+    pipeline.set_params(m3=None)
+    exp = 2 * 5
+    assert_array_equal([[exp]], pipeline.fit_transform(X, y))
+    assert_array_equal([exp], pipeline.fit(X).predict(X))
+    assert_array_equal(X, pipeline.inverse_transform([[exp]]))
+    assert_dict_equal(pipeline.get_params(deep=True),
+                      {'steps': pipeline.steps,
+                       'm2': mult2,
+                       'm3': None,
+                       'last': mult5,
+                       'm2__mult': 2,
+                       'last__mult': 5,
+                       })
+
+    pipeline.set_params(m2=None)
+    exp = 5
+    assert_array_equal([[exp]], pipeline.fit_transform(X, y))
+    assert_array_equal([exp], pipeline.fit(X).predict(X))
+    assert_array_equal(X, pipeline.inverse_transform([[exp]]))
+
+    # for other methods, ensure no AttributeErrors on None:
+    other_methods = ['predict_proba', 'predict_log_proba',
+                     'decision_function', 'transform', 'score']
+    for method in other_methods:
+        getattr(pipeline, method)(X)
+
+    pipeline.set_params(m2=mult2)
+    exp = 2 * 5
+    assert_array_equal([[exp]], pipeline.fit_transform(X, y))
+    assert_array_equal([exp], pipeline.fit(X).predict(X))
+    assert_array_equal(X, pipeline.inverse_transform([[exp]]))
+
+    pipeline = make()
+    pipeline.set_params(last=None)
+    # mult2 and mult3 are active
+    exp = 6
+    assert_array_equal([[exp]], pipeline.fit(X, y).transform(X))
+    assert_array_equal([[exp]], pipeline.fit_transform(X, y))
+    assert_array_equal(X, pipeline.inverse_transform([[exp]]))
+    assert_raise_message(AttributeError,
+                         "'NoneType' object has no attribute 'predict'",
+                         getattr, pipeline, 'predict')
+
+    # Check None step at construction time
+    exp = 2 * 5
+    pipeline = Pipeline([('m2', mult2), ('m3', None), ('last', mult5)])
+    assert_array_equal([[exp]], pipeline.fit_transform(X, y))
+    assert_array_equal([exp], pipeline.fit(X).predict(X))
+    assert_array_equal(X, pipeline.inverse_transform([[exp]]))
+
+
+def test_pipeline_ducktyping():
+    pipeline = make_pipeline(Mult(5))
+    pipeline.predict
+    pipeline.transform
+    pipeline.inverse_transform
+
+    pipeline = make_pipeline(Transf())
+    assert_false(hasattr(pipeline, 'predict'))
+    pipeline.transform
+    pipeline.inverse_transform
+
+    pipeline = make_pipeline(None)
+    assert_false(hasattr(pipeline, 'predict'))
+    pipeline.transform
+    pipeline.inverse_transform
+
+    pipeline = make_pipeline(Transf(), NoInvTransf())
+    assert_false(hasattr(pipeline, 'predict'))
+    pipeline.transform
+    assert_false(hasattr(pipeline, 'inverse_transform'))
+
+    pipeline = make_pipeline(NoInvTransf(), Transf())
+    assert_false(hasattr(pipeline, 'predict'))
+    pipeline.transform
+    assert_false(hasattr(pipeline, 'inverse_transform'))
 
+
+def test_make_pipeline():
+    t1 = Transf()
+    t2 = Transf()
     pipe = make_pipeline(t1, t2)
     assert_true(isinstance(pipe, Pipeline))
-    assert_equal(pipe.steps[0][0], "transft-1")
-    assert_equal(pipe.steps[1][0], "transft-2")
+    assert_equal(pipe.steps[0][0], "transf-1")
+    assert_equal(pipe.steps[1][0], "transf-2")
 
     pipe = make_pipeline(t1, t2, FitParamT())
     assert_true(isinstance(pipe, Pipeline))
-    assert_equal(pipe.steps[0][0], "transft-1")
-    assert_equal(pipe.steps[1][0], "transft-2")
+    assert_equal(pipe.steps[0][0], "transf-1")
+    assert_equal(pipe.steps[1][0], "transf-2")
     assert_equal(pipe.steps[2][0], "fitparamt")
 
 
@@ -378,7 +540,7 @@ def test_feature_union_weights():
                       transformer_weights={"pca": 10})
     X_fit_transformed = fs.fit_transform(X, y)
     # test it works with transformers missing fit_transform
-    fs = FeatureUnion([("mock", TransfT()), ("pca", pca), ("select", select)],
+    fs = FeatureUnion([("mock", Transf()), ("pca", pca), ("select", select)],
                       transformer_weights={"mock": 10})
     X_fit_transformed_wo_method = fs.fit_transform(X, y)
     # check against expected result
@@ -450,6 +612,11 @@ def test_feature_union_feature_names():
         assert_true("chars__" in feat or "words__" in feat)
     assert_equal(len(feature_names), 35)
 
+    ft = FeatureUnion([("tr1", Transf())]).fit([[1]])
+    assert_raise_message(AttributeError,
+                         'Transformer tr1 (type Transf) does not provide '
+                         'get_feature_names', ft.get_feature_names)
+
 
 def test_classes_property():
     iris = load_iris()
@@ -467,8 +634,96 @@ def test_classes_property():
 
 
 def test_X1d_inverse_transform():
-    transformer = TransfT()
+    transformer = Transf()
     pipeline = make_pipeline(transformer)
     X = np.ones(10)
     msg = "1d X will not be reshaped in pipeline.inverse_transform"
     assert_warns_message(FutureWarning, msg, pipeline.inverse_transform, X)
+
+
+def test_set_feature_union_steps():
+    mult2 = Mult(2)
+    mult2.get_feature_names = lambda: ['x2']
+    mult3 = Mult(3)
+    mult3.get_feature_names = lambda: ['x3']
+    mult5 = Mult(5)
+    mult5.get_feature_names = lambda: ['x5']
+
+    ft = FeatureUnion([('m2', mult2), ('m3', mult3)])
+    assert_array_equal([[2, 3]], ft.transform(np.asarray([[1]])))
+    assert_equal(['m2__x2', 'm3__x3'], ft.get_feature_names())
+
+    # Directly setting attr
+    ft.transformer_list = [('m5', mult5)]
+    assert_array_equal([[5]], ft.transform(np.asarray([[1]])))
+    assert_equal(['m5__x5'], ft.get_feature_names())
+
+    # Using set_params
+    ft.set_params(transformer_list=[('mock', mult3)])
+    assert_array_equal([[3]], ft.transform(np.asarray([[1]])))
+    assert_equal(['mock__x3'], ft.get_feature_names())
+
+    # Using set_params to replace single step
+    ft.set_params(mock=mult5)
+    assert_array_equal([[5]], ft.transform(np.asarray([[1]])))
+    assert_equal(['mock__x5'], ft.get_feature_names())
+
+
+def test_set_feature_union_step_none():
+    mult2 = Mult(2)
+    mult2.get_feature_names = lambda: ['x2']
+    mult3 = Mult(3)
+    mult3.get_feature_names = lambda: ['x3']
+    X = np.asarray([[1]])
+
+    ft = FeatureUnion([('m2', mult2), ('m3', mult3)])
+    assert_array_equal([[2, 3]], ft.fit(X).transform(X))
+    assert_array_equal([[2, 3]], ft.fit_transform(X))
+    assert_equal(['m2__x2', 'm3__x3'], ft.get_feature_names())
+
+    ft.set_params(m2=None)
+    assert_array_equal([[3]], ft.fit(X).transform(X))
+    assert_array_equal([[3]], ft.fit_transform(X))
+    assert_equal(['m3__x3'], ft.get_feature_names())
+
+    ft.set_params(m3=None)
+    assert_array_equal([[]], ft.fit(X).transform(X))
+    assert_array_equal([[]], ft.fit_transform(X))
+    assert_equal([], ft.get_feature_names())
+
+    # check we can change back
+    ft.set_params(m3=mult3)
+    assert_array_equal([[3]], ft.fit(X).transform(X))
+
+
+def test_step_name_validation():
+    bad_steps1 = [('a__q', Mult(2)), ('b', Mult(3))]
+    bad_steps2 = [('a', Mult(2)), ('a', Mult(3))]
+    for cls, param in [(Pipeline, 'steps'),
+                       (FeatureUnion, 'transformer_list')]:
+        # we validate in construction (despite scikit-learn convention)
+        bad_steps3 = [('a', Mult(2)), (param, Mult(3))]
+        for bad_steps, message in [
+            (bad_steps1, "Step names must not contain __: got ['a__q']"),
+            (bad_steps2, "Names provided are not unique: ['a', 'a']"),
+            (bad_steps3, "Step names conflict with constructor "
+                         "arguments: ['%s']" % param),
+        ]:
+            # three ways to make invalid:
+            # - construction
+            assert_raise_message(ValueError, message, cls,
+                                 **{param: bad_steps})
+
+            # - setattr
+            est = cls(**{param: [('a', Mult(1))]})
+            setattr(est, param, bad_steps)
+            assert_raise_message(ValueError, message, est.fit, [[1]], [1])
+            assert_raise_message(ValueError, message, est.fit_transform,
+                                 [[1]], [1])
+
+            # - set_params
+            est = cls(**{param: [('a', Mult(1))]})
+            est.set_params(**{param: bad_steps})
+            assert_raise_message(ValueError, message, est.fit, [[1]], [1])
+            assert_raise_message(ValueError, message, est.fit_transform,
+                                 [[1]], [1])
diff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py
index aaf174906f..db27550d97 100755
--- a/sklearn/utils/estimator_checks.py
+++ b/sklearn/utils/estimator_checks.py
@@ -220,7 +220,7 @@ def _yield_all_checks(name, Estimator):
 
 
 def check_estimator(Estimator):
-    """Check if estimator adheres to sklearn conventions.
+    """Check if estimator adheres to scikit-learn conventions.
 
     This estimator will run an extensive test-suite for input validation,
     shapes, etc.
@@ -1529,6 +1529,9 @@ def __init__(self):
         def fit(self, X, y):
             return self
 
+        def transform(self, X):
+            return X
+
     if name in ('FeatureUnion', 'Pipeline'):
         e = estimator([('clf', T())])
 
diff --git a/sklearn/utils/mocking.py b/sklearn/utils/mocking.py
index 1fd9e03d3d..507f08c8e7 100755
--- a/sklearn/utils/mocking.py
+++ b/sklearn/utils/mocking.py
@@ -26,7 +26,7 @@ def __init__(self, array):
     def __len__(self):
         return len(self.array)
 
-    def __array__(self):
+    def __array__(self, dtype=None):
         # Pandas data frames also are array-like: we want to make sure that
         # input validation in cross-validation does not try to call that
         # method.
diff --git a/sklearn/utils/testing.py b/sklearn/utils/testing.py
index 1035c4e7b9..2eeead9711 100755
--- a/sklearn/utils/testing.py
+++ b/sklearn/utils/testing.py
@@ -54,6 +54,11 @@
 from nose.tools import assert_false
 from nose.tools import assert_raises
 from nose.tools import raises
+try:
+    from nose.tools import assert_dict_equal
+except ImportError:
+    # Not in old versions of nose, but is only for formatting anyway
+    assert_dict_equal = assert_equal
 from nose import SkipTest
 from nose import with_setup
 
diff --git a/sklearn/utils/tests/test_multiclass.py b/sklearn/utils/tests/test_multiclass.py
index f6c45e1015..d103a177d7 100755
--- a/sklearn/utils/tests/test_multiclass.py
+++ b/sklearn/utils/tests/test_multiclass.py
@@ -36,7 +36,7 @@ class NotAnArray(object):
     def __init__(self, data):
         self.data = data
 
-    def __array__(self):
+    def __array__(self, dtype=None):
         return self.data
 
 
diff --git a/sklearn/utils/tests/test_validation.py b/sklearn/utils/tests/test_validation.py
index d577864fb7..a6268b08d1 100755
--- a/sklearn/utils/tests/test_validation.py
+++ b/sklearn/utils/tests/test_validation.py
@@ -236,6 +236,16 @@ def test_check_array_pandas_dtype_object_conversion():
     assert_equal(check_array(X_df, ensure_2d=False).dtype.kind, "f")
 
 
+def test_check_array_on_mock_dataframe():
+    arr = np.array([[0.2, 0.7], [0.6, 0.5], [0.4, 0.1], [0.7, 0.2]])
+    mock_df = MockDataFrame(arr)
+    checked_arr = check_array(mock_df)
+    assert_equal(checked_arr.dtype,
+                 arr.dtype)
+    checked_arr = check_array(mock_df, dtype=np.float32)
+    assert_equal(checked_arr.dtype, np.dtype(np.float32))
+
+
 def test_check_array_dtype_stability():
     # test that lists with ints don't get converted to floats
     X = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
