diff --git a/sklearn/__init__.py b/sklearn/__init__.py
index 8adf598c3a..3d1f4fc93a 100755
--- a/sklearn/__init__.py
+++ b/sklearn/__init__.py
@@ -37,7 +37,7 @@
 # Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
 # 'X.Y.dev0' is the canonical version of 'X.Y.dev'
 #
-__version__ = '0.18.dev0'
+__version__ = '0.19.dev0'
 
 
 try:
diff --git a/sklearn/gaussian_process/tests/test_gpc.py b/sklearn/gaussian_process/tests/test_gpc.py
index d429018cb3..16b2507e45 100755
--- a/sklearn/gaussian_process/tests/test_gpc.py
+++ b/sklearn/gaussian_process/tests/test_gpc.py
@@ -34,8 +34,7 @@ def f(x):
 
 
 def test_predict_consistent():
-    """ Check binary predict decision has also predicted probability above 0.5.
-    """
+    # Check binary predict decision has also predicted probability above 0.5.
     for kernel in kernels:
         gpc = GaussianProcessClassifier(kernel=kernel).fit(X, y)
         assert_array_equal(gpc.predict(X),
@@ -43,7 +42,7 @@ def test_predict_consistent():
 
 
 def test_lml_improving():
-    """ Test that hyperparameter-tuning improves log-marginal likelihood. """
+    # Test that hyperparameter-tuning improves log-marginal likelihood.
     for kernel in kernels:
         if kernel == fixed_kernel:
             continue
@@ -53,7 +52,7 @@ def test_lml_improving():
 
 
 def test_lml_precomputed():
-    """ Test that lml of optimized kernel is stored correctly. """
+    # Test that lml of optimized kernel is stored correctly.
     for kernel in kernels:
         gpc = GaussianProcessClassifier(kernel=kernel).fit(X, y)
         assert_almost_equal(gpc.log_marginal_likelihood(gpc.kernel_.theta),
@@ -61,7 +60,7 @@ def test_lml_precomputed():
 
 
 def test_converged_to_local_maximum():
-    """ Test that we are in local maximum after hyperparameter-optimization."""
+    # Test that we are in local maximum after hyperparameter-optimization.
     for kernel in kernels:
         if kernel == fixed_kernel:
             continue
@@ -76,7 +75,7 @@ def test_converged_to_local_maximum():
 
 
 def test_lml_gradient():
-    """ Compare analytic and numeric gradient of log marginal likelihood. """
+    # Compare analytic and numeric gradient of log marginal likelihood.
     for kernel in kernels:
         gpc = GaussianProcessClassifier(kernel=kernel).fit(X, y)
 
@@ -91,10 +90,8 @@ def test_lml_gradient():
 
 
 def test_random_starts():
-    """
-    Test that an increasing number of random-starts of GP fitting only
-    increases the log marginal likelihood of the chosen theta.
-    """
+    # Test that an increasing number of random-starts of GP fitting only
+    # increases the log marginal likelihood of the chosen theta.
     n_samples, n_features = 25, 2
     np.random.seed(0)
     rng = np.random.RandomState(0)
@@ -115,7 +112,7 @@ def test_random_starts():
 
 
 def test_custom_optimizer():
-    """ Test that GPC can use externally defined optimizers. """
+    # Test that GPC can use externally defined optimizers.
     # Define a dummy optimizer that simply tests 50 random hyperparameters
     def optimizer(obj_func, initial_theta, bounds):
         rng = np.random.RandomState(0)
@@ -140,7 +137,7 @@ def optimizer(obj_func, initial_theta, bounds):
 
 
 def test_multi_class():
-    """ Test GPC for multi-class classification problems. """
+    # Test GPC for multi-class classification problems.
     for kernel in kernels:
         gpc = GaussianProcessClassifier(kernel=kernel)
         gpc.fit(X, y_mc)
@@ -153,7 +150,7 @@ def test_multi_class():
 
 
 def test_multi_class_n_jobs():
-    """ Test that multi-class GPC produces identical results with n_jobs>1. """
+    # Test that multi-class GPC produces identical results with n_jobs>1.
     for kernel in kernels:
         gpc = GaussianProcessClassifier(kernel=kernel)
         gpc.fit(X, y_mc)
diff --git a/sklearn/gaussian_process/tests/test_gpr.py b/sklearn/gaussian_process/tests/test_gpr.py
index 98b2d63f6d..e62a2c1b14 100755
--- a/sklearn/gaussian_process/tests/test_gpr.py
+++ b/sklearn/gaussian_process/tests/test_gpr.py
@@ -36,7 +36,7 @@ def f(x):
 
 
 def test_gpr_interpolation():
-    """Test the interpolating property for different kernels."""
+    # Test the interpolating property for different kernels.
     for kernel in kernels:
         gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
         y_pred, y_cov = gpr.predict(X, return_cov=True)
@@ -46,7 +46,7 @@ def test_gpr_interpolation():
 
 
 def test_lml_improving():
-    """ Test that hyperparameter-tuning improves log-marginal likelihood. """
+    # Test that hyperparameter-tuning improves log-marginal likelihood.
     for kernel in kernels:
         if kernel == fixed_kernel:
             continue
@@ -56,7 +56,7 @@ def test_lml_improving():
 
 
 def test_lml_precomputed():
-    """ Test that lml of optimized kernel is stored correctly. """
+    # Test that lml of optimized kernel is stored correctly.
     for kernel in kernels:
         gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
         assert_equal(gpr.log_marginal_likelihood(gpr.kernel_.theta),
@@ -64,7 +64,7 @@ def test_lml_precomputed():
 
 
 def test_converged_to_local_maximum():
-    """ Test that we are in local maximum after hyperparameter-optimization."""
+    # Test that we are in local maximum after hyperparameter-optimization.
     for kernel in kernels:
         if kernel == fixed_kernel:
             continue
@@ -79,7 +79,7 @@ def test_converged_to_local_maximum():
 
 
 def test_solution_inside_bounds():
-    """ Test that hyperparameter-optimization remains in bounds"""
+    # Test that hyperparameter-optimization remains in bounds#
     for kernel in kernels:
         if kernel == fixed_kernel:
             continue
@@ -95,7 +95,7 @@ def test_solution_inside_bounds():
 
 
 def test_lml_gradient():
-    """ Compare analytic and numeric gradient of log marginal likelihood. """
+    # Compare analytic and numeric gradient of log marginal likelihood.
     for kernel in kernels:
         gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
 
@@ -110,7 +110,7 @@ def test_lml_gradient():
 
 
 def test_prior():
-    """ Test that GP prior has mean 0 and identical variances."""
+    # Test that GP prior has mean 0 and identical variances.
     for kernel in kernels:
         gpr = GaussianProcessRegressor(kernel=kernel)
 
@@ -125,7 +125,7 @@ def test_prior():
 
 
 def test_sample_statistics():
-    """ Test that statistics of samples drawn from GP are correct."""
+    # Test that statistics of samples drawn from GP are correct.
     for kernel in kernels:
         gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
 
@@ -140,14 +140,14 @@ def test_sample_statistics():
 
 
 def test_no_optimizer():
-    """ Test that kernel parameters are unmodified when optimizer is None."""
+    # Test that kernel parameters are unmodified when optimizer is None.
     kernel = RBF(1.0)
     gpr = GaussianProcessRegressor(kernel=kernel, optimizer=None).fit(X, y)
     assert_equal(np.exp(gpr.kernel_.theta), 1.0)
 
 
 def test_predict_cov_vs_std():
-    """ Test that predicted std.-dev. is consistent with cov's diagonal."""
+    # Test that predicted std.-dev. is consistent with cov's diagonal.
     for kernel in kernels:
         gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
         y_mean, y_cov = gpr.predict(X2, return_cov=True)
@@ -156,7 +156,7 @@ def test_predict_cov_vs_std():
 
 
 def test_anisotropic_kernel():
-    """ Test that GPR can identify meaningful anisotropic length-scales. """
+    # Test that GPR can identify meaningful anisotropic length-scales.
     # We learn a function which varies in one dimension ten-times slower
     # than in the other. The corresponding length-scales should differ by at
     # least a factor 5
@@ -171,10 +171,8 @@ def test_anisotropic_kernel():
 
 
 def test_random_starts():
-    """
-    Test that an increasing number of random-starts of GP fitting only
-    increases the log marginal likelihood of the chosen theta.
-    """
+    # Test that an increasing number of random-starts of GP fitting only
+    # increases the log marginal likelihood of the chosen theta.
     n_samples, n_features = 25, 2
     np.random.seed(0)
     rng = np.random.RandomState(0)
@@ -197,11 +195,10 @@ def test_random_starts():
 
 
 def test_y_normalization():
-    """ Test normalization of the target values in GP
+    # Test normalization of the target values in GP
 
-    Fitting non-normalizing GP on normalized y and fitting normalizing GP
-    on unnormalized y should yield identical results
-    """
+    # Fitting non-normalizing GP on normalized y and fitting normalizing GP
+    # on unnormalized y should yield identical results
     y_mean = y.mean(0)
     y_norm = y - y_mean
     for kernel in kernels:
@@ -226,7 +223,7 @@ def test_y_normalization():
 
 
 def test_y_multioutput():
-    """ Test that GPR can deal with multi-dimensional target values"""
+    # Test that GPR can deal with multi-dimensional target values
     y_2d = np.vstack((y, y * 2)).T
 
     # Test for fixed kernel that first dimension of 2d GP equals the output
@@ -269,7 +266,7 @@ def test_y_multioutput():
 
 
 def test_custom_optimizer():
-    """ Test that GPR can use externally defined optimizers. """
+    # Test that GPR can use externally defined optimizers.
     # Define a dummy optimizer that simply tests 50 random hyperparameters
     def optimizer(obj_func, initial_theta, bounds):
         rng = np.random.RandomState(0)
@@ -294,7 +291,7 @@ def optimizer(obj_func, initial_theta, bounds):
 
 
 def test_duplicate_input():
-    """ Test GPR can handle two different output-values for the same input. """
+    # Test GPR can handle two different output-values for the same input.
     for kernel in kernels:
         gpr_equal_inputs = \
             GaussianProcessRegressor(kernel=kernel, alpha=1e-2)
diff --git a/sklearn/gaussian_process/tests/test_kernels.py b/sklearn/gaussian_process/tests/test_kernels.py
index 116fad8dda..c51b89eeaa 100755
--- a/sklearn/gaussian_process/tests/test_kernels.py
+++ b/sklearn/gaussian_process/tests/test_kernels.py
@@ -3,7 +3,6 @@
 # Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
 # License: BSD 3 clause
 
-from collections import Hashable
 from sklearn.externals.funcsigs import signature
 
 import numpy as np
@@ -50,7 +49,7 @@
 
 
 def test_kernel_gradient():
-    """ Compare analytic and numeric gradient of kernels. """
+    # Compare analytic and numeric gradient of kernels.
     for kernel in kernels:
         K, K_gradient = kernel(X, eval_gradient=True)
 
@@ -70,7 +69,7 @@ def eval_kernel_for_theta(theta):
 
 
 def test_kernel_theta():
-    """ Check that parameter vector theta of kernel is set correctly. """
+    # Check that parameter vector theta of kernel is set correctly.
     for kernel in kernels:
         if isinstance(kernel, KernelOperator) \
            or isinstance(kernel, Exponentiation):  # skip non-basic kernels
@@ -111,8 +110,8 @@ def test_kernel_theta():
                 assert_array_equal(K_gradient[..., :i],
                                    K_gradient_new[..., :i])
             if i + 1 < len(kernel.hyperparameters):
-                assert_equal(theta[i+1:], new_kernel.theta[i:])
-                assert_array_equal(K_gradient[..., i+1:],
+                assert_equal(theta[i + 1:], new_kernel.theta[i:])
+                assert_array_equal(K_gradient[..., i + 1:],
                                    K_gradient_new[..., i:])
 
         # Check that values of theta are modified correctly
@@ -126,7 +125,7 @@ def test_kernel_theta():
 
 
 def test_auto_vs_cross():
-    """ Auto-correlation and cross-correlation should be consistent. """
+    # Auto-correlation and cross-correlation should be consistent.
     for kernel in kernels:
         if kernel == kernel_white:
             continue  # Identity is not satisfied on diagonal
@@ -136,7 +135,7 @@ def test_auto_vs_cross():
 
 
 def test_kernel_diag():
-    """ Test that diag method of kernel returns consistent results. """
+    # Test that diag method of kernel returns consistent results.
     for kernel in kernels:
         K_call_diag = np.diag(kernel(X))
         K_diag = kernel.diag(X)
@@ -144,7 +143,7 @@ def test_kernel_diag():
 
 
 def test_kernel_operator_commutative():
-    """ Adding kernels and multiplying kernels should be commutative. """
+    # Adding kernels and multiplying kernels should be commutative.
     # Check addition
     assert_almost_equal((RBF(2.0) + 1.0)(X),
                         (1.0 + RBF(2.0))(X))
@@ -155,7 +154,7 @@ def test_kernel_operator_commutative():
 
 
 def test_kernel_anisotropic():
-    """ Anisotropic kernel should be consistent with isotropic kernels."""
+    # Anisotropic kernel should be consistent with isotropic kernels.
     kernel = 3.0 * RBF([0.5, 2.0])
 
     K = kernel(X)
@@ -176,7 +175,7 @@ def test_kernel_anisotropic():
 
 
 def test_kernel_stationary():
-    """ Test stationarity of kernels."""
+    # Test stationarity of kernels.
     for kernel in kernels:
         if not kernel.is_stationary():
             continue
@@ -185,7 +184,7 @@ def test_kernel_stationary():
 
 
 def check_hyperparameters_equal(kernel1, kernel2):
-    """Check that hyperparameters of two kernels are equal"""
+    # Check that hyperparameters of two kernels are equal
     for attr in set(dir(kernel1) + dir(kernel2)):
         if attr.startswith("hyperparameter_"):
             attr_value1 = getattr(kernel1, attr)
@@ -194,7 +193,7 @@ def check_hyperparameters_equal(kernel1, kernel2):
 
 
 def test_kernel_clone():
-    """ Test that sklearn's clone works correctly on kernels. """
+    # Test that sklearn's clone works correctly on kernels.
     bounds = (1e-5, 1e5)
     for kernel in kernels:
         kernel_cloned = clone(kernel)
@@ -219,7 +218,8 @@ def test_kernel_clone():
         params = kernel.get_params()
         # RationalQuadratic kernel is isotropic.
         isotropic_kernels = (ExpSineSquared, RationalQuadratic)
-        if 'length_scale' in params and not isinstance(kernel, isotropic_kernels):
+        if 'length_scale' in params and not isinstance(kernel,
+                                                       isotropic_kernels):
             length_scale = params['length_scale']
             if np.iterable(length_scale):
                 params['length_scale'] = length_scale[0]
@@ -232,11 +232,12 @@ def test_kernel_clone():
             assert_equal(kernel_cloned_clone.get_params(),
                          kernel_cloned.get_params())
             assert_not_equal(id(kernel_cloned_clone), id(kernel_cloned))
-            yield check_hyperparameters_equal, kernel_cloned, kernel_cloned_clone
+            yield (check_hyperparameters_equal, kernel_cloned,
+                   kernel_cloned_clone)
 
 
 def test_matern_kernel():
-    """ Test consistency of Matern kernel for special values of nu. """
+    # Test consistency of Matern kernel for special values of nu.
     K = Matern(nu=1.5, length_scale=1.0)(X)
     # the diagonal elements of a matern kernel are 1
     assert_array_almost_equal(np.diag(K), np.ones(X.shape[0]))
@@ -255,7 +256,7 @@ def test_matern_kernel():
 
 
 def test_kernel_versus_pairwise():
-    """Check that GP kernels can also be used as pairwise kernels."""
+    # Check that GP kernels can also be used as pairwise kernels.
     for kernel in kernels:
         # Test auto-kernel
         if kernel != kernel_white:
@@ -272,7 +273,7 @@ def test_kernel_versus_pairwise():
 
 
 def test_set_get_params():
-    """Check that set_params()/get_params() is consistent with kernel.theta."""
+    # Check that set_params()/get_params() is consistent with kernel.theta.
     for kernel in kernels:
         # Test get_params()
         index = 0
@@ -282,7 +283,7 @@ def test_set_get_params():
                 continue
             size = hyperparameter.n_elements
             if size > 1:  # anisotropic kernels
-                assert_almost_equal(np.exp(kernel.theta[index:index+size]),
+                assert_almost_equal(np.exp(kernel.theta[index:index + size]),
                                     params[hyperparameter.name])
                 index += size
             else:
@@ -297,9 +298,9 @@ def test_set_get_params():
                 continue
             size = hyperparameter.n_elements
             if size > 1:  # anisotropic kernels
-                kernel.set_params(**{hyperparameter.name: [value]*size})
-                assert_almost_equal(np.exp(kernel.theta[index:index+size]),
-                                    [value]*size)
+                kernel.set_params(**{hyperparameter.name: [value] * size})
+                assert_almost_equal(np.exp(kernel.theta[index:index + size]),
+                                    [value] * size)
                 index += size
             else:
                 kernel.set_params(**{hyperparameter.name: value})
@@ -308,7 +309,7 @@ def test_set_get_params():
 
 
 def test_repr_kernels():
-    """Smoke-test for repr in kernels."""
+    # Smoke-test for repr in kernels.
 
     for kernel in kernels:
         repr(kernel)
diff --git a/sklearn/kernel_ridge.py b/sklearn/kernel_ridge.py
index 682811cb61..c782886c73 100755
--- a/sklearn/kernel_ridge.py
+++ b/sklearn/kernel_ridge.py
@@ -69,8 +69,8 @@ class KernelRidge(BaseEstimator, RegressorMixin):
 
     Attributes
     ----------
-    dual_coef_ : array, shape = [n_features] or [n_targets, n_features]
-        Weight vector(s) in kernel space
+    dual_coef_ : array, shape = [n_samples] or [n_samples, n_targets]
+        Representation of weight vector(s) in kernel space
 
     X_fit_ : {array-like, sparse matrix}, shape = [n_samples, n_features]
         Training data, which is also required for prediction
diff --git a/sklearn/linear_model/tests/test_ridge.py b/sklearn/linear_model/tests/test_ridge.py
index d961509733..c6f076483e 100755
--- a/sklearn/linear_model/tests/test_ridge.py
+++ b/sklearn/linear_model/tests/test_ridge.py
@@ -297,8 +297,8 @@ def _test_ridge_loo(filter_):
 
     ret = []
 
-    fit_intercept = filter_ == DENSE_FILTER 
-    if fit_intercept: 
+    fit_intercept = filter_ == DENSE_FILTER
+    if fit_intercept:
         X_diabetes_ = X_diabetes - X_diabetes.mean(0)
     else:
         X_diabetes_ = X_diabetes
diff --git a/sklearn/neural_network/multilayer_perceptron.py b/sklearn/neural_network/multilayer_perceptron.py
index fc184974a3..87ea951533 100755
--- a/sklearn/neural_network/multilayer_perceptron.py
+++ b/sklearn/neural_network/multilayer_perceptron.py
@@ -24,7 +24,8 @@
 from ..exceptions import ConvergenceWarning
 from ..utils.extmath import safe_sparse_dot
 from ..utils.validation import check_is_fitted
-from ..utils.multiclass import _check_partial_fit_first_call
+from ..utils.multiclass import _check_partial_fit_first_call, unique_labels
+from ..utils.multiclass import type_of_target
 
 
 _STOCHASTIC_SOLVERS = ['sgd', 'adam']
@@ -268,7 +269,7 @@ def _initialize(self, y, layer_units):
         if not isinstance(self, ClassifierMixin):
             self.out_activation_ = 'identity'
         # Output for multi class
-        elif self.label_binarizer_.y_type_ == 'multiclass':
+        elif self._label_binarizer.y_type_ == 'multiclass':
             self.out_activation_ = 'softmax'
         # Output for binary class and multi-label
         else:
@@ -489,7 +490,7 @@ def _fit_stochastic(self, X, y, activations, deltas, coef_grads,
                 X, y, random_state=self._random_state,
                 test_size=self.validation_fraction)
             if isinstance(self, ClassifierMixin):
-                y_val = self.label_binarizer_.inverse_transform(y_val)
+                y_val = self._label_binarizer.inverse_transform(y_val)
         else:
             X_val = None
             y_val = None
@@ -819,9 +820,6 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
     `loss_` : float
         The current loss computed with the loss function.
 
-    `label_binarizer_` : LabelBinarizer
-        A LabelBinarizer object trained on the training set.
-
     `coefs_` : list, length n_layers - 1
         The ith element in the list represents the weight matrix corresponding
         to layer i.
@@ -894,25 +892,24 @@ def __init__(self, hidden_layer_sizes=(100,), activation="relu",
                      validation_fraction=validation_fraction,
                      beta_1=beta_1, beta_2=beta_2, epsilon=epsilon)
 
-        self.label_binarizer_ = LabelBinarizer()
-
     def _validate_input(self, X, y, incremental):
         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],
                          multi_output=True)
         if y.ndim == 2 and y.shape[1] == 1:
             y = column_or_1d(y, warn=True)
-        self.label_binarizer_.fit(y)
 
-        if not hasattr(self, 'classes_') or not incremental:
-            self.classes_ = self.label_binarizer_.classes_
+        if not incremental:
+            self._label_binarizer = LabelBinarizer()
+            self._label_binarizer.fit(y)
+            self.classes_ = self._label_binarizer.classes_
         else:
-            classes = self.label_binarizer_.classes_
-            if not np.all(np.in1d(classes, self.classes_)):
+            classes = unique_labels(y)
+            if np.setdiff1d(classes, self.classes_, assume_unique=True):
                 raise ValueError("`y` has classes not in `self.classes_`."
                                  " `self.classes_` has %s. 'y' has %s." %
                                  (self.classes_, classes))
 
-        y = self.label_binarizer_.transform(y)
+        y = self._label_binarizer.transform(y)
         return X, y
 
     def predict(self, X):
@@ -934,7 +931,7 @@ def predict(self, X):
         if self.n_outputs_ == 1:
             y_pred = y_pred.ravel()
 
-        return self.label_binarizer_.inverse_transform(y_pred)
+        return self._label_binarizer.inverse_transform(y_pred)
 
     @property
     def partial_fit(self):
@@ -967,7 +964,12 @@ def partial_fit(self):
         return self._partial_fit
 
     def _partial_fit(self, X, y, classes=None):
-        _check_partial_fit_first_call(self, classes)
+        if _check_partial_fit_first_call(self, classes):
+            self._label_binarizer = LabelBinarizer()
+            if type_of_target(y).startswith('multilabel'):
+                self._label_binarizer.fit(y)
+            else:
+                self._label_binarizer.fit(classes)
 
         super(MLPClassifier, self)._partial_fit(X, y)
 
diff --git a/sklearn/neural_network/tests/test_mlp.py b/sklearn/neural_network/tests/test_mlp.py
index 931e29f696..b8552246ce 100755
--- a/sklearn/neural_network/tests/test_mlp.py
+++ b/sklearn/neural_network/tests/test_mlp.py
@@ -80,7 +80,6 @@ def test_fit():
     # set weights
     mlp.coefs_ = [0] * 2
     mlp.intercepts_ = [0] * 2
-    mlp.classes_ = [0, 1]
     mlp.n_outputs_ = 1
     mlp.coefs_[0] = np.array([[0.1, 0.2], [0.3, 0.1], [0.5, 0]])
     mlp.coefs_[1] = np.array([[0.1], [0.2]])
@@ -89,8 +88,6 @@ def test_fit():
     mlp._coef_grads = [] * 2
     mlp._intercept_grads = [] * 2
 
-    mlp.label_binarizer_.y_type_ = 'binary'
-
     # Initialize parameters
     mlp.n_iter_ = 0
     mlp.learning_rate_ = 0.1
@@ -345,6 +342,17 @@ def test_partial_fit_classification():
         assert_greater(mlp.score(X, y), 0.95)
 
 
+def test_partial_fit_unseen_classes():
+    # Non regression test for bug 6994
+    # Tests for labeling errors in partial fit
+
+    clf = MLPClassifier(random_state=0)
+    clf.partial_fit([[1], [2], [3]], ["a", "b", "c"],
+                    classes=["a", "b", "c", "d"])
+    clf.partial_fit([[4]], ["d"])
+    assert_greater(clf.score([[1], [2], [3], [4]], ["a", "b", "c", "d"]), 0)
+
+
 def test_partial_fit_regression():
     # Test partial_fit on regression.
     # `partial_fit` should yield the same results as 'fit' for regression.
diff --git a/sklearn/tests/test_base.py b/sklearn/tests/test_base.py
index 1c5c9bb744..d0df21c780 100755
--- a/sklearn/tests/test_base.py
+++ b/sklearn/tests/test_base.py
@@ -324,6 +324,11 @@ def __getstate__(self):
         return self.__dict__
 
 
+class TreeBadVersion(DecisionTreeClassifier):
+    def __getstate__(self):
+        return dict(self.__dict__.items(), _sklearn_version="something")
+
+
 def test_pickle_version_warning():
     # check that warnings are raised when unpickling in a different version
 
@@ -335,9 +340,9 @@ def test_pickle_version_warning():
     assert_no_warnings(pickle.loads, tree_pickle)
 
     # check that warning is raised on different version
-    tree_pickle_other = tree_pickle.replace(sklearn.__version__.encode(),
-                                            b"something")
-    message = ("Trying to unpickle estimator DecisionTreeClassifier from "
+    tree = TreeBadVersion().fit(iris.data, iris.target)
+    tree_pickle_other = pickle.dumps(tree)
+    message = ("Trying to unpickle estimator TreeBadVersion from "
                "version {0} when using version {1}. This might lead to "
                "breaking code or invalid results. "
                "Use at your own risk.".format("something",
@@ -351,7 +356,7 @@ def test_pickle_version_warning():
     tree_pickle_noversion = pickle.dumps(tree)
     assert_false(b"version" in tree_pickle_noversion)
     message = message.replace("something", "pre-0.18")
-    message = message.replace("DecisionTreeClassifier", "TreeNoVersion")
+    message = message.replace("TreeBadVersion", "TreeNoVersion")
     # check we got the warning about using pre-0.18 pickle
     assert_warns_message(UserWarning, message, pickle.loads,
                          tree_pickle_noversion)
