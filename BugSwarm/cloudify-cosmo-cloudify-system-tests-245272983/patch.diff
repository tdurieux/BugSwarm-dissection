diff --git a/cosmo_tester/resources/hostpool/service-blueprint.yaml b/cosmo_tester/resources/hostpool/service-blueprint.yaml
index 8b700c4045..07aef5c2cc 100755
--- a/cosmo_tester/resources/hostpool/service-blueprint.yaml
+++ b/cosmo_tester/resources/hostpool/service-blueprint.yaml
@@ -2,7 +2,7 @@ tosca_definitions_version: cloudify_dsl_1_3
 
 imports:
   - http://www.getcloudify.org/spec/cloudify/4.1/types.yaml
-  - http://www.getcloudify.org/spec/fabric-plugin/1.4.2/plugin.yaml
+  - http://www.getcloudify.org/spec/fabric-plugin/1.5/plugin.yaml
   - http://www.getcloudify.org/spec/openstack-plugin/2.0.1/plugin.yaml
   - https://raw.githubusercontent.com/cloudify-cosmo/cloudify-host-pool-service/1.1/host-pool-service.yaml
 
diff --git a/cosmo_tester/test_suites/ha/ha_cluster_scenarios_test.py b/cosmo_tester/test_suites/ha/ha_cluster_scenarios_test.py
index 5a346d42a9..d9d20cc57d 100755
--- a/cosmo_tester/test_suites/ha/ha_cluster_scenarios_test.py
+++ b/cosmo_tester/test_suites/ha/ha_cluster_scenarios_test.py
@@ -40,17 +40,19 @@ def cluster(
 
     try:
         manager1 = cluster.managers[0]
+        ha_helper.delete_active_profile()
         manager1.use()
+
         cfy.cluster.start(timeout=600,
                           cluster_host_ip=manager1.private_ip_address,
-                          cluster_node_name=manager1.index)
+                          cluster_node_name=manager1.ip_address)
 
         for manager in cluster.managers[1:]:
             manager.use()
             cfy.cluster.join(manager1.ip_address,
                              timeout=600,
                              cluster_host_ip=manager.private_ip_address,
-                             cluster_node_name=manager.index)
+                             cluster_node_name=manager.ip_address)
 
         cfy.cluster.nodes.list()
 
@@ -80,42 +82,44 @@ def hello_world(cfy, cluster, attributes, ssh_key, tmpdir, logger):
 def test_data_replication(cfy, cluster, hello_world,
                           logger):
     manager1 = cluster.managers[0]
+    ha_helper.delete_active_profile()
     manager1.use()
     ha_helper.verify_nodes_status(manager1, cfy, logger)
     hello_world.upload_blueprint()
     hello_world.create_deployment()
     hello_world.install()
 
-    logger.info('Manager %s resources', manager1.index)
+    logger.info('Manager %s resources', manager1.ip_address)
     m1_blueprints_list = cfy.blueprints.list()
     m1_deployments_list = cfy.deployments.list()
     m1_plugins_list = cfy.plugins.list()
 
     for manager in cluster.managers[1:]:
         ha_helper.set_active(manager, cfy, logger)
-        ha_helper.wait_leader_election(logger)
+        ha_helper.delete_active_profile()
         manager.use()
         ha_helper.verify_nodes_status(manager, cfy, logger)
 
-        logger.info('Manager %s resources', manager.index)
+        logger.info('Manager %s resources', manager.ip_address)
         assert m1_blueprints_list == cfy.blueprints.list()
         assert m1_deployments_list == cfy.deployments.list()
         assert m1_plugins_list == cfy.plugins.list()
 
     ha_helper.set_active(manager1, cfy, logger)
-    ha_helper.wait_leader_election(logger)
+    ha_helper.delete_active_profile()
     manager1.use()
 
 
 def test_set_active(cfy, cluster,
                     logger):
     manager1 = cluster.managers[0]
+    ha_helper.delete_active_profile()
     manager1.use()
     ha_helper.verify_nodes_status(manager1, cfy, logger)
 
     for manager in cluster.managers[1:]:
         ha_helper.set_active(manager, cfy, logger)
-        ha_helper.wait_leader_election(logger)
+        ha_helper.delete_active_profile()
         manager.use()
         ha_helper.verify_nodes_status(manager, cfy, logger)
 
@@ -123,16 +127,14 @@ def test_set_active(cfy, cluster,
 def test_delete_manager_node(cfy, cluster, hello_world,
                              logger):
     ha_helper.set_active(cluster.managers[1], cfy, logger)
-    ha_helper.wait_leader_election(logger)
+    expected_master = cluster.managers[0]
 
     for manager in cluster.managers[1:]:
-        logger.info('Deleting manager %s', manager.private_ip_address)
+        logger.info('Deleting manager %s', manager.ip_address)
         manager.delete()
         ha_helper.wait_leader_election(logger)
 
-    expected_master = cluster.managers[0]
-    expected_master.use()
-
+    logger.info('Expected leader %s', expected_master)
     ha_helper.verify_nodes_status(expected_master, cfy, logger)
     hello_world.upload_blueprint()
 
@@ -141,30 +143,32 @@ def test_failover(cfy, cluster, hello_world,
                   logger):
     for manager in cluster.managers[:-1]:
         logger.info('Simulating manager %s failure by stopping'
-                    ' nginx service', manager.private_ip_address)
+                    ' nginx service', manager.ip_address)
         with manager.ssh() as fabric:
             fabric.run('sudo systemctl stop nginx')
         ha_helper.wait_leader_election(logger)
         cfy.cluster.nodes.list()
 
     expected_master = cluster.managers[-1]
+    ha_helper.delete_active_profile()
     expected_master.use()
     ha_helper.verify_nodes_status(expected_master, cfy, logger)
 
     with expected_master.ssh() as fabric:
         logger.info('Simulating manager %s failure by stopping '
                     'cloudify-mgmtworker service',
-                    expected_master.private_ip_address)
+                    expected_master.ip_address)
         fabric.run('sudo systemctl stop cloudify-mgmtworker')
     ha_helper.wait_leader_election(logger)
     cfy.cluster.nodes.list()
 
     expected_master = cluster.managers[0]
     logger.info('Starting nginx service on manager %s',
-                expected_master.private_ip_address)
+                expected_master.ip_address)
     with expected_master.ssh() as fabric:
         fabric.run('sudo systemctl start nginx')
     ha_helper.wait_leader_election(logger)
+    ha_helper.delete_active_profile()
     expected_master.use()
     ha_helper.verify_nodes_status(expected_master, cfy, logger)
     hello_world.upload_blueprint()
@@ -173,15 +177,17 @@ def test_failover(cfy, cluster, hello_world,
 def test_remove_manager_from_cluster(cfy, cluster, hello_world,
                                      logger):
     ha_helper.set_active(cluster.managers[1], cfy, logger)
-    ha_helper.wait_leader_election(logger)
+    ha_helper.delete_active_profile()
 
     for manager in cluster.managers[1:]:
+        manager.use()
         logger.info('Removing the manager %s from HA cluster',
-                    manager.private_ip_address)
-        cfy.cluster.nodes.remove(manager.index)
+                    manager.ip_address)
+        cfy.cluster.nodes.remove(manager.ip_address)
         ha_helper.wait_leader_election(logger)
 
     expected_master = cluster.managers[0]
+    ha_helper.delete_active_profile()
     expected_master.use()
 
     ha_helper.verify_nodes_status(expected_master, cfy, logger)
diff --git a/cosmo_tester/test_suites/ha/ha_helper.py b/cosmo_tester/test_suites/ha/ha_helper.py
index f36446a1e8..5fd029c87f 100755
--- a/cosmo_tester/test_suites/ha/ha_helper.py
+++ b/cosmo_tester/test_suites/ha/ha_helper.py
@@ -16,7 +16,6 @@
 
 import time
 import os
-from base64 import standard_b64encode
 
 
 class HighAvailabilityHelper(object):
@@ -24,8 +23,8 @@ class HighAvailabilityHelper(object):
     def set_active(manager, cfy, logger):
         try:
             logger.info('Setting active manager %s',
-                        manager.private_ip_address)
-            cfy.cluster('set-active', manager.index)
+                        manager.ip_address)
+            cfy.cluster('set-active', manager.ip_address)
         except Exception as e:
             logger.info('Setting active manager error message: %s', e.message)
         finally:
@@ -38,17 +37,22 @@ def wait_leader_election(logger):
 
     @staticmethod
     def verify_nodes_status(manager, cfy, logger):
-        logger.info('Verifying that manager %s is a master '
-                    'and others are standby', manager.private_ip_address)
+        logger.info('Verifying that manager %s is a leader '
+                    'and others are replicas', manager.ip_address)
         cfy.cluster.nodes.list()
         nodes = manager.client.cluster.nodes.list()
 
         for node in nodes:
-            if node.name == str(manager.index):
+            if node.name == str(manager.ip_address):
                 assert node.master is True
+                logger.info('Manager %s is a leader ', node.name)
             else:
-                assert node.master is False
+                assert node.master is not True
+                logger.info('Manager %s is a replica ', node.name)
 
     @staticmethod
-    def create_encription_key():
-        return standard_b64encode(os.urandom(16))
+    def delete_active_profile():
+        active_profile_path = os.path.join(os.environ['CFY_WORKDIR'],
+                                           '.cloudify/active.profile')
+        if os.path.exists(active_profile_path):
+            os.remove(active_profile_path)
diff --git a/cosmo_tester/test_suites/ha/ha_negative_test.py b/cosmo_tester/test_suites/ha/ha_negative_test.py
index 1e9b50f64b..510a78da77 100755
--- a/cosmo_tester/test_suites/ha/ha_negative_test.py
+++ b/cosmo_tester/test_suites/ha/ha_negative_test.py
@@ -42,16 +42,17 @@ def cluster(
         manager1 = cluster.managers[0]
         manager2 = cluster.managers[1]
 
+        ha_helper.delete_active_profile()
         manager1.use()
         cfy.cluster.start(timeout=600,
                           cluster_host_ip=manager1.private_ip_address,
-                          cluster_node_name=manager1.index)
+                          cluster_node_name=manager1.ip_address)
 
         manager2.use()
         cfy.cluster.join(manager1.ip_address,
                          timeout=600,
                          cluster_host_ip=manager2.private_ip_address,
-                         cluster_node_name=manager2.index)
+                         cluster_node_name=manager2.ip_address)
         cfy.cluster.nodes.list()
 
         yield cluster
@@ -82,13 +83,15 @@ def test_nonempty_manager_join_cluster_negative(cfy,
         manager1 = cluster.managers[0]
         manager2 = cluster.managers[1]
 
+        ha_helper.delete_active_profile()
         manager1.use()
         cfy.cluster.start(timeout=600,
                           cluster_host_ip=manager1.private_ip_address,
-                          cluster_node_name=manager1.index)
+                          cluster_node_name=manager1.ip_address)
 
         cfy.cluster.nodes.list()
 
+        ha_helper.delete_active_profile()
         manager2.use()
         hello_world = HelloWorldExample(
             cfy, manager2, attributes, ssh_key, logger, tmpdir)
@@ -105,7 +108,7 @@ def test_nonempty_manager_join_cluster_negative(cfy,
             cfy.cluster.join(manager1.ip_address,
                              timeout=600,
                              cluster_host_ip=manager2.private_ip_address,
-                             cluster_node_name=manager2.index)
+                             cluster_node_name=manager2.ip_address)
 
     finally:
         cluster.destroy()
@@ -116,15 +119,17 @@ def test_remove_from_cluster_and_use_negative(cfy,
     manager1 = cluster.managers[0]
     manager2 = cluster.managers[1]
 
+    ha_helper.delete_active_profile()
     manager1.use()
     logger.info('Removing the standby manager %s from the HA cluster',
-                manager2.private_ip_address)
-    cfy.cluster.nodes.remove(manager2.index)
+                manager2.ip_address)
+    cfy.cluster.nodes.remove(manager2.ip_address)
     cfy.cluster.nodes.list()
 
     logger.info('Trying to use a manager previously removed'
                 ' from HA cluster')
     with pytest.raises(Exception):
+        ha_helper.delete_active_profile()
         manager2.use()
         cfy('--version')
 
@@ -134,10 +139,11 @@ def test_remove_from_cluster_and_rejoin_negative(cfy,
     manager1 = cluster.managers[0]
     manager2 = cluster.managers[1]
 
+    ha_helper.delete_active_profile()
     manager1.use()
     logger.info('Removing the standby manager %s from the HA cluster',
-                manager2.private_ip_address)
-    cfy.cluster.nodes.remove(manager2.index)
+                manager2.ip_address)
+    cfy.cluster.nodes.remove(manager2.ip_address)
     cfy.cluster.nodes.list()
 
     logger.info('Trying to rejoin HA cluster with a manager previously'
@@ -146,7 +152,7 @@ def test_remove_from_cluster_and_rejoin_negative(cfy,
         cfy.cluster.join(manager1.ip_address,
                          timeout=600,
                          cluster_host_ip=manager2.private_ip_address,
-                         cluster_node_name=manager2.index)
+                         cluster_node_name=manager2.ip_address)
     assert 'is already part of a Cloudify Manager cluster' \
            not in str(exinfo.value)
 
@@ -156,11 +162,13 @@ def test_manager_already_in_cluster_join_cluster_negative(cfy,
     manager1 = cluster.managers[0]
     manager2 = cluster.managers[1]
 
+    ha_helper.set_active(manager2, cfy, logger)
+    ha_helper.delete_active_profile()
     manager2.use()
     logger.info('Joining HA cluster with the manager %s that is already'
-                ' a part of the cluster', manager2.private_ip_address)
+                ' a part of the cluster', manager2.ip_address)
     with pytest.raises(Exception):
         cfy.cluster.join(manager1.ip_address,
                          timeout=600,
                          cluster_host_ip=manager2.private_ip_address,
-                         cluster_node_name=manager2.index)
+                         cluster_node_name=manager2.ip_address)
diff --git a/cosmo_tester/test_suites/snapshots/snapshot_upgrade_test.py b/cosmo_tester/test_suites/snapshots/snapshot_upgrade_test.py
index 5b04294931..40993f90b1 100755
--- a/cosmo_tester/test_suites/snapshots/snapshot_upgrade_test.py
+++ b/cosmo_tester/test_suites/snapshots/snapshot_upgrade_test.py
@@ -24,15 +24,13 @@
     CloudifyCluster,
     MANAGERS,
 )
-from cosmo_tester.framework.util import (
-    create_rest_client,
-    assert_snapshot_created,
-)
+from cosmo_tester.framework.util import assert_snapshot_created
 
 # CFY-6912
 from cloudify_cli.commands.executions import (
     _get_deployment_environment_creation_execution,
     )
+from cloudify_cli.constants import CLOUDIFY_TENANT_HEADER
 
 
 HELLO_WORLD_URL = 'https://github.com/cloudify-cosmo/cloudify-hello-world-example/archive/4.0.zip'  # noqa
@@ -59,8 +57,8 @@ def cluster(request, cfy, ssh_key, module_tmpdir, attributes, logger):
     if request.param.startswith('3'):
         # Install dev tools & python headers
         with cluster.managers[1].ssh() as fabric_ssh:
-            fabric_ssh.sudo('yum -y groupinstall "Development Tools"')
-            fabric_ssh.sudo('yum -y install python-devel')
+            fabric_ssh.sudo('yum -y -q groupinstall "Development Tools"')
+            fabric_ssh.sudo('yum -y -q install python-devel')
 
     yield cluster
 
@@ -100,24 +98,34 @@ def _hello_world_example(cluster, attributes, logger, tmpdir):
 
 def test_restore_snapshot_and_agents_upgrade(
         cfy, cluster, attributes, logger, tmpdir):
-    manager1 = cluster.managers[0]
-    manager2 = cluster.managers[1]
+    manager0 = cluster.managers[0]
+    manager1 = cluster.managers[1]
 
     snapshot_id = str(uuid.uuid4())
 
-    logger.info('Creating snapshot on manager1..')
-    manager1.client.snapshots.create(snapshot_id, False, False, False)
-    assert_snapshot_created(manager1, snapshot_id, attributes)
+    logger.info('Creating snapshot on old manager..')
+    manager0.client.snapshots.create(snapshot_id, False, False, False)
+    assert_snapshot_created(manager0, snapshot_id, attributes)
 
     local_snapshot_path = str(tmpdir / 'snapshot.zip')
 
     logger.info('Downloading snapshot from old manager..')
-    manager1.client.snapshots.list()
-    manager1.client.snapshots.download(snapshot_id, local_snapshot_path)
+    manager0.client.snapshots.list()
+    manager0.client.snapshots.download(snapshot_id, local_snapshot_path)
+
+    manager1.use()
+    if '3.4' in manager0.branch_name:
+        # When working with a 3.x snapshot, we need to create a new tenant
+        # into which we'll restore the snapshot
+        tenant_name = manager0.restore_tenant_name
+        manager1.client.tenants.create(tenant_name)
+
+        # Update the tenant in the manager's client and CLI
+        manager1.client._client.headers[CLOUDIFY_TENANT_HEADER] = tenant_name
+        cfy.profiles.set(['-t', tenant_name])
 
-    manager2.use()
     logger.info('Uploading snapshot to latest manager..')
-    snapshot = manager2.client.snapshots.upload(local_snapshot_path,
+    snapshot = manager1.client.snapshots.upload(local_snapshot_path,
                                                 snapshot_id)
     logger.info('Uploaded snapshot:%s%s',
                 os.linesep,
@@ -126,10 +134,7 @@ def test_restore_snapshot_and_agents_upgrade(
     cfy.snapshots.list()
 
     logger.info('Restoring snapshot on latest manager..')
-    restore_execution = manager2.client.snapshots.restore(
-        snapshot_id,
-        tenant_name=manager1.restore_tenant_name,
-        )
+    restore_execution = manager1.client.snapshots.restore(snapshot_id)
     logger.info('Snapshot restore execution:%s%s',
                 os.linesep,
                 json.dumps(restore_execution, indent=2))
@@ -137,32 +142,23 @@ def test_restore_snapshot_and_agents_upgrade(
     cfy.executions.list(['--include-system-workflows'])
 
     restore_execution = wait_for_execution(
-        manager2.client,
+        manager1.client,
         restore_execution,
         logger)
     assert restore_execution.status == 'terminated'
 
     cfy.executions.list(['--include-system-workflows'])
 
-    manager2.use(tenant=manager1.restore_tenant_name)
-    client = create_rest_client(
-        manager2.ip_address,
-        username=cluster._attributes.cloudify_username,
-        password=cluster._attributes.cloudify_password,
-        tenant=manager1.tenant_name,
-        api_version=manager2.api_version,
-        )
-
     cfy.deployments.list()
-    deployments = client.deployments.list()
+    deployments = manager1.client.deployments.list()
     assert 1 == len(deployments)
 
     logger.info('Upgrading agents..')
     cfy.agents.install()
 
     logger.info('Deleting original {version} manager..'.format(
-        version=manager1.branch_name))
-    manager1.delete()
+        version=manager0.branch_name))
+    manager0.delete()
 
     logger.info('Uninstalling deployment from latest manager..')
     cfy.executions.start.uninstall(['-d', deployment_id])
diff --git a/suites/jenkins-slave-blueprint/openstack-blueprint.yaml b/suites/jenkins-slave-blueprint/openstack-blueprint.yaml
index 26bc5dec2d..5bd686992d 100755
--- a/suites/jenkins-slave-blueprint/openstack-blueprint.yaml
+++ b/suites/jenkins-slave-blueprint/openstack-blueprint.yaml
@@ -2,7 +2,7 @@ tosca_definitions_version: cloudify_dsl_1_2
 
 imports:
   - http://www.getcloudify.org/spec/cloudify/4.1/types.yaml
-  - http://www.getcloudify.org/spec/fabric-plugin/1.4.2/plugin.yaml
+  - http://www.getcloudify.org/spec/fabric-plugin/1.5/plugin.yaml
   - http://www.getcloudify.org/spec/openstack-plugin/2.0.1/plugin.yaml
   - types/jenkins-types.yaml
 
diff --git a/suites/jenkins-slave-blueprint/singlehost-blueprint.yaml b/suites/jenkins-slave-blueprint/singlehost-blueprint.yaml
index 6186501b1d..ba8afa5960 100755
--- a/suites/jenkins-slave-blueprint/singlehost-blueprint.yaml
+++ b/suites/jenkins-slave-blueprint/singlehost-blueprint.yaml
@@ -2,7 +2,7 @@ tosca_definitions_version: cloudify_dsl_1_3
 
 imports:
   - http://www.getcloudify.org/spec/cloudify/4.1/types.yaml
-  - http://www.getcloudify.org/spec/fabric-plugin/1.4.2/plugin.yaml
+  - http://www.getcloudify.org/spec/fabric-plugin/1.5/plugin.yaml
   - types/jenkins-types.yaml
 
 inputs:
diff --git a/test-requirements.txt b/test-requirements.txt
index 85f706701d..e4bc46b38f 100755
--- a/test-requirements.txt
+++ b/test-requirements.txt
@@ -1,6 +1,6 @@
 git+https://github.com/cloudify-cosmo/cloudify-dsl-parser@4.1#egg=cloudify-dsl-parser==4.1
 git+https://github.com/cloudify-cosmo/cloudify-rest-client@4.1#egg=cloudify-rest-client==4.1
 git+https://github.com/cloudify-cosmo/cloudify-plugins-common@4.1#egg=cloudify-plugins-common==4.1
-git+https://github.com/cloudify-cosmo/cloudify-script-plugin@master#egg=cloudify-script-plugin==1.4
+git+https://github.com/cloudify-cosmo/cloudify-script-plugin@master#egg=cloudify-script-plugin==1.5
 git+https://github.com/cloudify-cosmo/cloudify-cli@4.1#egg=cloudify==4.1
 mock==2.0.0
