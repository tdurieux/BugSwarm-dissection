diff --git a/doc/developers/advanced_installation.rst b/doc/developers/advanced_installation.rst
index 9029f5991a..c21e1b6bf1 100755
--- a/doc/developers/advanced_installation.rst
+++ b/doc/developers/advanced_installation.rst
@@ -52,31 +52,31 @@ Linux
 -----
 
 At this time scikit-learn does not provide official binary packages for Linux
-so you have to build from source if you want the lastest version.
+so you have to build from source if you want the latest version.
 If you don't need the newest version, consider using your package manager to
-install scikit-learn. it is usually the easiest way, but might not provide the
+install scikit-learn. It is usually the easiest way, but might not provide the
 newest version.
 
-installing build dependencies
+Installing build dependencies
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
-installing from source requires you to have installed the scikit-learn runtime
-dependencies, python development headers and a working C/C++ compiler.
-under debian-based operating systems, which include ubuntu, if you have
-python 2 you can install all these requirements by issuing::
+Installing from source requires you to have installed the scikit-learn runtime
+dependencies, Python development headers and a working C/C++ compiler.
+Under Debian-based operating systems, which include Ubuntu, if you have
+Python 2 you can install all these requirements by issuing::
 
     sudo apt-get install build-essential python-dev python-setuptools \
                          python-numpy python-scipy \
                          libatlas-dev libatlas3gf-base
 
-if you have python 3::
+If you have Python 3::
 
     sudo apt-get install build-essential python3-dev python3-setuptools \
                          python3-numpy python3-scipy \
                          libatlas-dev libatlas3gf-base
 
-on recent debian and ubuntu (e.g. ubuntu 13.04 or later) make sure that atlas
-is used to provide the implementation of the blas and lapack linear algebra
+On recent Debian and Ubuntu (e.g. Ubuntu 13.04 or later) make sure that ATLAS
+is used to provide the implementation of the BLAS and LAPACK linear algebra
 routines::
 
     sudo update-alternatives --set libblas.so.3 \
@@ -86,24 +86,24 @@ routines::
 
 .. note::
 
-    in order to build the documentation and run the example code contains in
+    In order to build the documentation and run the example code contains in
     this documentation you will need matplotlib::
 
         sudo apt-get install python-matplotlib
 
 .. note::
 
-    the above installs the atlas implementation of blas
-    (the basic linear algebra subprograms library).
-    ubuntu 11.10 and later, and recent (testing) versions of debian,
-    offer an alternative implementation called openblas.
+    The above installs the ATLAS implementation of BLAS
+    (the Basic Linear Algebra Subprograms library).
+    Ubuntu 11.10 and later, and recent (testing) versions of Debian,
+    offer an alternative implementation called OpenBLAS.
 
-    using openblas can give speedups in some scikit-learn modules,
-    but can freeze joblib/multiprocessing prior to openblas version 0.2.8-4,
+    Using OpenBLAS can give speedups in some scikit-learn modules,
+    but can freeze joblib/multiprocessing prior to OpenBLAS version 0.2.8-4,
     so using it is not recommended unless you know what you're doing.
 
-    if you do want to use openblas, then replacing atlas only requires a couple
-    of commands. atlas has to be removed, otherwise numpy may not work::
+    If you do want to use OpenBLAS, then replacing ATLAS only requires a couple
+    of commands. ATLAS has to be removed, otherwise NumPy may not work::
 
         sudo apt-get remove libatlas3gf-base libatlas-dev
         sudo apt-get install libopenblas-dev
@@ -113,30 +113,30 @@ routines::
         sudo update-alternatives --set liblapack.so.3 \
             /usr/lib/lapack/liblapack.so.3
 
-on red hat and clones (e.g. centos), install the dependencies using::
+On Red Hat and clones (e.g. CentOS), install the dependencies using::
 
     sudo yum -y install gcc gcc-c++ numpy python-devel scipy
 
 
-building scikit-learn with pip
+Building scikit-learn with pip
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
-this is usually the fastest way to install or upgrade to the latest stable
+This is usually the fastest way to install or upgrade to the latest stable
 release::
 
-    pip install --user --install-option="--prefix=" -u scikit-learn
+    pip install --user --install-option="--prefix=" -U scikit-learn
 
-the ``--user`` flag asks pip to install scikit-learn in the ``$home/.local``
-folder therefore not requiring root permission. this flag should make pip
+The ``--user`` flag asks pip to install scikit-learn in the ``$HOME/.local``
+folder therefore not requiring root permission. This flag should make pip
 ignore any old version of scikit-learn previously installed on the system while
-benefiting from system packages for numpy and scipy. those dependencies can
+benefiting from system packages for numpy and scipy. Those dependencies can
 be long and complex to build correctly from source.
 
-the ``--install-option="--prefix="`` flag is only required if python has a
+The ``--install-option="--prefix="`` flag is only required if Python has a
 ``distutils.cfg`` configuration with a predefined ``prefix=`` entry.
 
 
-from source package
+From source package
 ~~~~~~~~~~~~~~~~~~~
 
 download the source package from
@@ -144,7 +144,7 @@ download the source package from
 cd into the source directory.
 
 This packages uses distutils, which is the default way of installing
-python modules. the install command is::
+python modules. The install command is::
 
     python setup.py install
 
@@ -154,27 +154,27 @@ or alternatively (also from within the scikit-learn source folder)::
 
 .. warning::
 
-   packages installed with the ``python setup.py install`` command cannot
-   be uninstalled nor upgraded by ``pip`` later. to properly uninstall
+   Packages installed with the ``python setup.py install`` command cannot
+   be uninstalled nor upgraded by ``pip`` later. To properly uninstall
    scikit-learn in that case it is necessary to delete the ``sklearn`` folder
-   from your python ``site-packages`` directory.
+   from your Python ``site-packages`` directory.
 
 
-windows
+Windows
 -------
 
-first, you need to install `numpy <http://www.numpy.org/>`_ and `scipy
+First, you need to install `numpy <http://www.numpy.org/>`_ and `scipy
 <http://www.scipy.org/>`_ from their own official installers.
 
-wheel packages (.whl files) for scikit-learn from `pypi
+Wheel packages (.whl files) for scikit-learn from `pypi
 <https://pypi.python.org/pypi/scikit-learn/>`_ can be installed with the `pip
 <https://pip.readthedocs.io/en/stable/installing/>`_ utility.
-open a console and type the following to install or upgrade scikit-learn to the
+Open a console and type the following to install or upgrade scikit-learn to the
 latest stable release::
 
-    pip install -u scikit-learn
+    pip install -U scikit-learn
 
-if there are no binary packages matching your python, version you might
+If there are no binary packages matching your python, version you might
 to try to install scikit-learn and its dependencies from `christoph gohlke
 unofficial windows installers
 <http://www.lfd.uci.edu/~gohlke/pythonlibs/#scikit-learn>`_
@@ -186,23 +186,23 @@ or from a :ref:`python distribution <install_by_distribution>` instead.
 Third party distributions of scikit-learn
 =========================================
 
-some third-party distributions are now providing versions of
+Some third-party distributions are now providing versions of
 scikit-learn integrated with their package-management systems.
 
-these can make installation and upgrading much easier for users since
+These can make installation and upgrading much easier for users since
 the integration includes the ability to automatically install
 dependencies (numpy, scipy) that scikit-learn requires.
 
-the following is an incomplete list of python and os distributions
+The following is an incomplete list of python and os distributions
 that provide their own version of scikit-learn.
 
 
-macports for mac osx
+MacPorts for Mac OSX
 --------------------
 
-the macports package is named ``py<xy>-scikits-learn``,
-where ``xy`` denotes the python version.
-it can be installed by typing the following
+The MacPorts package is named ``py<XY>-scikits-learn``,
+where ``XY`` denotes the Python version.
+It can be installed by typing the following
 command::
 
     sudo port install py26-scikit-learn
@@ -212,39 +212,39 @@ or::
     sudo port install py27-scikit-learn
 
 
-arch linux
+Arch Linux
 ----------
 
-arch linux's package is provided through the `official repositories
+Arch Linux's package is provided through the `official repositories
 <https://www.archlinux.org/packages/?q=scikit-learn>`_ as
-``python-scikit-learn`` for python 3 and ``python2-scikit-learn`` for python 2.
-it can be installed by typing the following command:
+``python-scikit-learn`` for Python 3 and ``python2-scikit-learn`` for Python 2.
+It can be installed by typing the following command:
 
 .. code-block:: none
 
-     # pacman -s python-scikit-learn
+     # pacman -S python-scikit-learn
 
 or:
 
 .. code-block:: none
 
-     # pacman -s python2-scikit-learn
+     # pacman -S python2-scikit-learn
 
-depending on the version of python you use.
+depending on the version of Python you use.
 
 
-netbsd
+NetBSD
 ------
 
 scikit-learn is available via `pkgsrc-wip <http://pkgsrc-wip.sourceforge.net/>`_:
 
     http://pkgsrc.se/wip/py-scikit_learn
 
-fedora
+Fedora
 ------
 
-the fedora package is called ``python-scikit-learn`` for the python 2 version
-and ``python3-scikit-learn`` for the python 3 version. both versions can
+The Fedora package is called ``python-scikit-learn`` for the Python 2 version
+and ``python3-scikit-learn`` for the Python 3 version. Both versions can
 be installed using ``yum``::
 
     $ sudo yum install python-scikit-learn
@@ -254,103 +254,103 @@ or::
     $ sudo yum install python3-scikit-learn
 
 
-building on windows
+Building on windows
 -------------------
 
-to build scikit-learn on windows you need a working C/C++ compiler in
+To build scikit-learn on Windows you need a working C/C++ compiler in
 addition to numpy, scipy and setuptools.
 
-picking the right compiler depends on the version of python (2 or 3)
-and the architecture of the python interpreter, 32-bit or 64-bit.
-you can check the python version by running the following in ``cmd`` or
+Picking the right compiler depends on the version of Python (2 or 3)
+and the architecture of the Python interpreter, 32-bit or 64-bit.
+You can check the Python version by running the following in ``cmd`` or
 ``powershell`` console::
 
     python --version
 
 and the architecture with::
 
-    python -c "import struct; print(struct.calcsize('p') * 8)"
+    python -c "import struct; print(struct.calcsize('P') * 8)"
 
-the above commands assume that you have the python installation folder in your
-path environment variable.
+The above commands assume that you have the Python installation folder in your
+PATH environment variable.
 
 
-32-bit python
+32-bit Python
 -------------
 
-for 32-bit python it is possible use the standalone installers for
+For 32-bit python it is possible use the standalone installers for
 `microsoft visual c++ express 2008 <http://download.microsoft.com/download/A/5/4/A54BADB6-9C3F-478D-8657-93B3FC9FE62D/vcsetup.exe>`_
-for python 2 or microsoft visual c++ express 2010 for python 3.
+for Python 2 or Microsoft Visual C++ Express 2010 for Python 3.
 
-once installed you should be able to build scikit-learn without any
+Once installed you should be able to build scikit-learn without any
 particular configuration by running the following command in the scikit-learn
 folder::
 
    python setup.py install
 
 
-64-bit python
+64-bit Python
 -------------
 
-for the 64-bit architecture, you either need the full visual studio or
-the free windows sdks that can be downloaded from the links below.
+For the 64-bit architecture, you either need the full Visual Studio or
+the free Windows SDKs that can be downloaded from the links below.
 
-the windows sdks include the msvc compilers both for 32 and 64-bit
-architectures. they come as a ``grmsdkx_en_dvd.iso`` file that can be mounted
+The Windows SDKs include the MSVC compilers both for 32 and 64-bit
+architectures. They come as a ``GRMSDKX_EN_DVD.iso`` file that can be mounted
 as a new drive with a ``setup.exe`` installer in it.
 
-- for python 2 you need sdk **v7.0**: `ms windows sdk for windows 7 and .net
-  framework 3.5 sp1
+- For Python 2 you need SDK **v7.0**: `MS Windows SDK for Windows 7 and .NET
+  Framework 3.5 SP1
   <https://www.microsoft.com/en-us/download/details.aspx?id=18950>`_
 
-- for python 3 you need sdk **v7.1**: `ms windows sdk for windows 7 and .net
-  framework 4
+- For Python 3 you need SDK **v7.1**: `MS Windows SDK for Windows 7 and .NET
+  Framework 4
   <https://www.microsoft.com/en-us/download/details.aspx?id=8442>`_
 
-both sdks can be installed in parallel on the same host. to use the windows
-sdks, you need to setup the environment of a ``cmd`` console launched with the
-following flags (at least for sdk v7.0)::
+Both SDKs can be installed in parallel on the same host. To use the Windows
+SDKs, you need to setup the environment of a ``cmd`` console launched with the
+following flags (at least for SDK v7.0)::
 
-    cmd /e:on /v:on /k
+    cmd /E:ON /V:ON /K
 
-then configure the build environment with::
+Then configure the build environment with::
 
-    set distutils_use_sdk=1
-    set mssdk=1
-    "c:\program files\microsoft sdks\windows\v7.0\setup\windowssdkver.exe" -q -version:v7.0
-    "c:\program files\microsoft sdks\windows\v7.0\bin\setenv.cmd" /x64 /release
+    SET DISTUTILS_USE_SDK=1
+    SET MSSdk=1
+    "C:\Program Files\Microsoft SDKs\Windows\v7.0\Setup\WindowsSdkVer.exe" -q -version:v7.0
+    "C:\Program Files\Microsoft SDKs\Windows\v7.0\Bin\SetEnv.cmd" /x64 /release
 
-finally you can build scikit-learn in the same ``cmd`` console::
+Finally you can build scikit-learn in the same ``cmd`` console::
 
     python setup.py install
 
-replace ``v7.0`` by the ``v7.1`` in the above commands to do the same for
-python 3 instead of python 2.
+Replace ``v7.0`` by the ``v7.1`` in the above commands to do the same for
+Python 3 instead of Python 2.
 
-replace ``/x64`` by ``/x86``  to build for 32-bit python instead of 64-bit
-python.
+Replace ``/x64`` by ``/x86``  to build for 32-bit Python instead of 64-bit
+Python.
 
 
-building binary packages and installers
+Building binary packages and installers
 ---------------------------------------
 
-the ``.whl`` package and ``.exe`` installers can be built with::
+The ``.whl`` package and ``.exe`` installers can be built with::
 
     pip install wheel
     python setup.py bdist_wheel bdist_wininst -b doc/logos/scikit-learn-logo.bmp
 
-the resulting packages are generated in the ``dist/`` folder.
+The resulting packages are generated in the ``dist/`` folder.
 
 
-using an alternative compiler
+Using an alternative compiler
 -----------------------------
 
-it is possible to use `mingw <http://www.mingw.org>`_ (a port of gcc to windows
-os) as an alternative to msvc for 32-bit python. not that extensions built with
-mingw32 can be redistributed as reusable packages as they depend on gcc runtime
+It is possible to use `MinGW <http://www.mingw.org>`_ (a port of GCC to Windows
+OS) as an alternative to MSVC for 32-bit Python. Not that extensions built with
+mingw32 can be redistributed as reusable packages as they depend on GCC runtime
 libraries typically not installed on end-users environment.
 
-to force the use of a particular compiler, pass the ``--compiler`` flag to the
+To force the use of a particular compiler, pass the ``--compiler`` flag to the
 build step::
 
     python setup.py build --compiler=my_compiler install
@@ -360,62 +360,62 @@ where ``my_compiler`` should be one of ``mingw32`` or ``msvc``.
 
 .. _install_bleeding_edge:
 
-bleeding edge
+Bleeding Edge
 =============
 
-see section :ref:`git_repo` on how to get the development version. then follow
+See section :ref:`git_repo` on how to get the development version. Then follow
 the previous instructions to build from source depending on your platform.
 You will also require Cython >=0.23 in order to build the development version.
 
 
 .. _testing:
 
-testing
+Testing
 =======
 
-testing scikit-learn once installed
+Testing scikit-learn once installed
 -----------------------------------
 
-testing requires having the `nose
-<https://nose.readthedocs.io/en/latest/>`_ library. after
+Testing requires having the `nose
+<https://somethingaboutorange.com/mrl/projects/nose/>`_ library. After
 installation, the package can be tested by executing *from outside* the
 source directory::
 
     $ nosetests -v sklearn
 
-under windows, it is recommended to use the following command (adjust the path
+Under Windows, it is recommended to use the following command (adjust the path
 to the ``python.exe`` program) as using the ``nosetests.exe`` program can badly
 interact with tests that use ``multiprocessing``::
 
-    c:\python34\python.exe -c "import nose; nose.main()" -v sklearn
+    C:\Python34\python.exe -c "import nose; nose.main()" -v sklearn
 
-this should give you a lot of output (and some warnings) but
+This should give you a lot of output (and some warnings) but
 eventually should finish with a message similar to::
 
-    ran 3246 tests in 260.618s
-    ok (skip=20)
+    Ran 3246 tests in 260.618s
+    OK (SKIP=20)
 
-otherwise, please consider posting an issue into the `bug tracker
+Otherwise, please consider posting an issue into the `bug tracker
 <https://github.com/scikit-learn/scikit-learn/issues>`_ or to the
 :ref:`mailing_lists` including the traceback of the individual failures
-and errors. please include your operation system, your version of numpy, scipy
+and errors. Please include your operating system, your version of NumPy, SciPy
 and scikit-learn, and how you installed scikit-learn.
 
 
-testing scikit-learn from within the source folder
+Testing scikit-learn from within the source folder
 --------------------------------------------------
 
-scikit-learn can also be tested without having the package
-installed. for this you must compile the sources inplace from the
+Scikit-learn can also be tested without having the package
+installed. For this you must compile the sources inplace from the
 source directory::
 
     python setup.py build_ext --inplace
 
-test can now be run using nosetests::
+Test can now be run using nosetests::
 
     nosetests -v sklearn/
 
-this is automated by the commands::
+This is automated by the commands::
 
     make in
 
@@ -424,7 +424,7 @@ and::
     make test
 
 
-you can also install a symlink named ``site-packages/scikit-learn.egg-link``
+You can also install a symlink named ``site-packages/scikit-learn.egg-link``
 to the development folder of scikit-learn with::
 
     pip install --editable .
diff --git a/doc/modules/neural_networks_supervised.rst b/doc/modules/neural_networks_supervised.rst
index 6f083ab4a8..f94ecd72d4 100755
--- a/doc/modules/neural_networks_supervised.rst
+++ b/doc/modules/neural_networks_supervised.rst
@@ -86,14 +86,16 @@ training samples::
     >>> from sklearn.neural_network import MLPClassifier
     >>> X = [[0., 0.], [1., 1.]]
     >>> y = [0, 1]
-    >>> clf = MLPClassifier(algorithm='l-bfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)
-    >>> clf.fit(X, y) # doctest: +NORMALIZE_WHITESPACE
-    MLPClassifier(activation='relu', algorithm='l-bfgs', alpha=1e-05,
-           batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,
+    >>> clf = MLPClassifier(solver='lbgfs', alpha=1e-5,
+    ...                     hidden_layer_sizes=(5, 2), random_state=1)
+    ...
+    >>> clf.fit(X, y)                         # doctest: +NORMALIZE_WHITESPACE
+    MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',
+           beta_1=0.9, beta_2=0.999, early_stopping=False,
            epsilon=1e-08, hidden_layer_sizes=(5, 2), learning_rate='constant',
            learning_rate_init=0.001, max_iter=200, momentum=0.9,
            nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,
-           tol=0.0001, validation_fraction=0.1, verbose=False,
+           solver='lbgfs', tol=0.0001, validation_fraction=0.1, verbose=False,
            warm_start=False)
 
 After fitting (training), the model can predict labels for new samples::
@@ -124,7 +126,7 @@ of probability estimates :math:`P(y|x)` per sample :math:`x`::
 applying `Softmax <https://en.wikipedia.org/wiki/Softmax_activation_function>`_
 as the output function.
 
-Further, the algorithm supports :ref:`multi-label classification <multiclass>`
+Further, the model supports :ref:`multi-label classification <multiclass>`
 in which a sample can belong to more than one class. For each class, the raw
 output passes through the logistic function. Values larger or equal to `0.5`
 are rounded to `1`, otherwise to `0`. For a predicted output of a sample, the
@@ -132,15 +134,16 @@ indices where the value is `1` represents the assigned classes of that sample::
 
     >>> X = [[0., 0.], [1., 1.]]
     >>> y = [[0, 1], [1, 1]]
-    >>> clf = MLPClassifier(algorithm='l-bfgs', alpha=1e-5,
+    >>> clf = MLPClassifier(solver='lbgfs', alpha=1e-5,
     ...                     hidden_layer_sizes=(15,), random_state=1)
-    >>> clf.fit(X, y)
-    MLPClassifier(activation='relu', algorithm='l-bfgs', alpha=1e-05,
-           batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,
+    ...
+    >>> clf.fit(X, y)                         # doctest: +NORMALIZE_WHITESPACE
+    MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',
+           beta_1=0.9, beta_2=0.999, early_stopping=False,
            epsilon=1e-08, hidden_layer_sizes=(15,), learning_rate='constant',
            learning_rate_init=0.001, max_iter=200, momentum=0.9,
            nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,
-           tol=0.0001, validation_fraction=0.1, verbose=False,
+           solver='lbgfs', tol=0.0001, validation_fraction=0.1, verbose=False,
            warm_start=False)
     >>> clf.predict([1., 2.])
     array([[1, 1]])
@@ -208,19 +211,19 @@ for the network.
 More details can be found in the documentation of
 `SGD <http://scikit-learn.org/stable/modules/sgd.html>`_
 
-Adam is similar to SGD in a sense that it is a stochastic optimization
-algorithm, but it can automatically adjust the amount to update parameters
-based on adaptive estimates of lower-order moments.
+Adam is similar to SGD in a sense that it is a stochastic optimizer, but it can
+automatically adjust the amount to update parameters based on adaptive estimates
+of lower-order moments.
 
 With SGD or Adam, training supports online and mini-batch learning.
 
-L-BFGS is a fast learning algorithm that approximates the Hessian matrix which
-represents the second-order partial derivative of a function. Further it
-approximates the inverse of the Hessian matrix to perform parameter updates.
-The implementation uses the Scipy version of
-`L-BFGS <http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html>`__..
+L-BFGS is a solver that approximates the Hessian matrix which represents the
+second-order partial derivative of a function. Further it approximates the
+inverse of the Hessian matrix to perform parameter updates. The implementation
+uses the Scipy version of `L-BFGS
+<http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html>`_.
 
-If the selected algorithm is 'L-BFGS', training does not support online nor
+If the selected solver is 'L-BFGS', training does not support online nor
 mini-batch learning.
 
 
diff --git a/doc/whats_new.rst b/doc/whats_new.rst
index 9e4e9bfebd..c3452e5c08 100755
--- a/doc/whats_new.rst
+++ b/doc/whats_new.rst
@@ -432,6 +432,13 @@ Bug fixes
       problems for users with very small differences in scores (`#7353
       <https://github.com/scikit-learn/scikit-learn/pull/7353>`_).
 
+    - Fix incomplete ``predict_proba`` method delegation from
+      :class:`model_selection.GridSearchCV` to
+      :class:`linear_model.SGDClassifier` (`#7159
+      <https://github.com/scikit-learn/scikit-learn/pull/7159>`_)
+      by `Yichuan Liu <https://github.com/yl565>`_.
+
+
 API changes summary
 -------------------
 
diff --git a/examples/neural_networks/plot_mlp_training_curves.py b/examples/neural_networks/plot_mlp_training_curves.py
index c6456ab12c..89ca2747bd 100755
--- a/examples/neural_networks/plot_mlp_training_curves.py
+++ b/examples/neural_networks/plot_mlp_training_curves.py
@@ -8,6 +8,9 @@
 use several small datasets, for which L-BFGS might be more suitable. The
 general trend shown in these examples seems to carry over to larger datasets,
 however.
+
+Note that those results can be highly dependent on the value of
+``learning_rate_init``.
 """
 
 print(__doc__)
@@ -17,19 +20,19 @@
 from sklearn import datasets
 
 # different learning rate schedules and momentum parameters
-params = [{'algorithm': 'sgd', 'learning_rate': 'constant', 'momentum': 0,
+params = [{'solver': 'sgd', 'learning_rate': 'constant', 'momentum': 0,
            'learning_rate_init': 0.2},
-          {'algorithm': 'sgd', 'learning_rate': 'constant', 'momentum': .9,
+          {'solver': 'sgd', 'learning_rate': 'constant', 'momentum': .9,
            'nesterovs_momentum': False, 'learning_rate_init': 0.2},
-          {'algorithm': 'sgd', 'learning_rate': 'constant', 'momentum': .9,
+          {'solver': 'sgd', 'learning_rate': 'constant', 'momentum': .9,
            'nesterovs_momentum': True, 'learning_rate_init': 0.2},
-          {'algorithm': 'sgd', 'learning_rate': 'invscaling', 'momentum': 0,
+          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': 0,
            'learning_rate_init': 0.2},
-          {'algorithm': 'sgd', 'learning_rate': 'invscaling', 'momentum': .9,
+          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': .9,
            'nesterovs_momentum': True, 'learning_rate_init': 0.2},
-          {'algorithm': 'sgd', 'learning_rate': 'invscaling', 'momentum': .9,
+          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': .9,
            'nesterovs_momentum': False, 'learning_rate_init': 0.2},
-          {'algorithm': 'adam'}]
+          {'solver': 'adam', 'learning_rate_init': 0.01}]
 
 labels = ["constant learning-rate", "constant with momentum",
           "constant with Nesterov's momentum",
diff --git a/examples/neural_networks/plot_mnist_filters.py b/examples/neural_networks/plot_mnist_filters.py
index dc63f1caeb..6c3b8b2284 100755
--- a/examples/neural_networks/plot_mnist_filters.py
+++ b/examples/neural_networks/plot_mnist_filters.py
@@ -33,9 +33,9 @@
 y_train, y_test = y[:60000], y[60000:]
 
 # mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,
-#                     algorithm='sgd', verbose=10, tol=1e-4, random_state=1)
+#                     solver='sgd', verbose=10, tol=1e-4, random_state=1)
 mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4,
-                    algorithm='sgd', verbose=10, tol=1e-4, random_state=1,
+                    solver='sgd', verbose=10, tol=1e-4, random_state=1,
                     learning_rate_init=.1)
 
 mlp.fit(X_train, y_train)
diff --git a/sklearn/gaussian_process/tests/test_gpc.py b/sklearn/gaussian_process/tests/test_gpc.py
index d429018cb3..16b2507e45 100755
--- a/sklearn/gaussian_process/tests/test_gpc.py
+++ b/sklearn/gaussian_process/tests/test_gpc.py
@@ -34,8 +34,7 @@ def f(x):
 
 
 def test_predict_consistent():
-    """ Check binary predict decision has also predicted probability above 0.5.
-    """
+    # Check binary predict decision has also predicted probability above 0.5.
     for kernel in kernels:
         gpc = GaussianProcessClassifier(kernel=kernel).fit(X, y)
         assert_array_equal(gpc.predict(X),
@@ -43,7 +42,7 @@ def test_predict_consistent():
 
 
 def test_lml_improving():
-    """ Test that hyperparameter-tuning improves log-marginal likelihood. """
+    # Test that hyperparameter-tuning improves log-marginal likelihood.
     for kernel in kernels:
         if kernel == fixed_kernel:
             continue
@@ -53,7 +52,7 @@ def test_lml_improving():
 
 
 def test_lml_precomputed():
-    """ Test that lml of optimized kernel is stored correctly. """
+    # Test that lml of optimized kernel is stored correctly.
     for kernel in kernels:
         gpc = GaussianProcessClassifier(kernel=kernel).fit(X, y)
         assert_almost_equal(gpc.log_marginal_likelihood(gpc.kernel_.theta),
@@ -61,7 +60,7 @@ def test_lml_precomputed():
 
 
 def test_converged_to_local_maximum():
-    """ Test that we are in local maximum after hyperparameter-optimization."""
+    # Test that we are in local maximum after hyperparameter-optimization.
     for kernel in kernels:
         if kernel == fixed_kernel:
             continue
@@ -76,7 +75,7 @@ def test_converged_to_local_maximum():
 
 
 def test_lml_gradient():
-    """ Compare analytic and numeric gradient of log marginal likelihood. """
+    # Compare analytic and numeric gradient of log marginal likelihood.
     for kernel in kernels:
         gpc = GaussianProcessClassifier(kernel=kernel).fit(X, y)
 
@@ -91,10 +90,8 @@ def test_lml_gradient():
 
 
 def test_random_starts():
-    """
-    Test that an increasing number of random-starts of GP fitting only
-    increases the log marginal likelihood of the chosen theta.
-    """
+    # Test that an increasing number of random-starts of GP fitting only
+    # increases the log marginal likelihood of the chosen theta.
     n_samples, n_features = 25, 2
     np.random.seed(0)
     rng = np.random.RandomState(0)
@@ -115,7 +112,7 @@ def test_random_starts():
 
 
 def test_custom_optimizer():
-    """ Test that GPC can use externally defined optimizers. """
+    # Test that GPC can use externally defined optimizers.
     # Define a dummy optimizer that simply tests 50 random hyperparameters
     def optimizer(obj_func, initial_theta, bounds):
         rng = np.random.RandomState(0)
@@ -140,7 +137,7 @@ def optimizer(obj_func, initial_theta, bounds):
 
 
 def test_multi_class():
-    """ Test GPC for multi-class classification problems. """
+    # Test GPC for multi-class classification problems.
     for kernel in kernels:
         gpc = GaussianProcessClassifier(kernel=kernel)
         gpc.fit(X, y_mc)
@@ -153,7 +150,7 @@ def test_multi_class():
 
 
 def test_multi_class_n_jobs():
-    """ Test that multi-class GPC produces identical results with n_jobs>1. """
+    # Test that multi-class GPC produces identical results with n_jobs>1.
     for kernel in kernels:
         gpc = GaussianProcessClassifier(kernel=kernel)
         gpc.fit(X, y_mc)
diff --git a/sklearn/gaussian_process/tests/test_gpr.py b/sklearn/gaussian_process/tests/test_gpr.py
index 98b2d63f6d..e62a2c1b14 100755
--- a/sklearn/gaussian_process/tests/test_gpr.py
+++ b/sklearn/gaussian_process/tests/test_gpr.py
@@ -36,7 +36,7 @@ def f(x):
 
 
 def test_gpr_interpolation():
-    """Test the interpolating property for different kernels."""
+    # Test the interpolating property for different kernels.
     for kernel in kernels:
         gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
         y_pred, y_cov = gpr.predict(X, return_cov=True)
@@ -46,7 +46,7 @@ def test_gpr_interpolation():
 
 
 def test_lml_improving():
-    """ Test that hyperparameter-tuning improves log-marginal likelihood. """
+    # Test that hyperparameter-tuning improves log-marginal likelihood.
     for kernel in kernels:
         if kernel == fixed_kernel:
             continue
@@ -56,7 +56,7 @@ def test_lml_improving():
 
 
 def test_lml_precomputed():
-    """ Test that lml of optimized kernel is stored correctly. """
+    # Test that lml of optimized kernel is stored correctly.
     for kernel in kernels:
         gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
         assert_equal(gpr.log_marginal_likelihood(gpr.kernel_.theta),
@@ -64,7 +64,7 @@ def test_lml_precomputed():
 
 
 def test_converged_to_local_maximum():
-    """ Test that we are in local maximum after hyperparameter-optimization."""
+    # Test that we are in local maximum after hyperparameter-optimization.
     for kernel in kernels:
         if kernel == fixed_kernel:
             continue
@@ -79,7 +79,7 @@ def test_converged_to_local_maximum():
 
 
 def test_solution_inside_bounds():
-    """ Test that hyperparameter-optimization remains in bounds"""
+    # Test that hyperparameter-optimization remains in bounds#
     for kernel in kernels:
         if kernel == fixed_kernel:
             continue
@@ -95,7 +95,7 @@ def test_solution_inside_bounds():
 
 
 def test_lml_gradient():
-    """ Compare analytic and numeric gradient of log marginal likelihood. """
+    # Compare analytic and numeric gradient of log marginal likelihood.
     for kernel in kernels:
         gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
 
@@ -110,7 +110,7 @@ def test_lml_gradient():
 
 
 def test_prior():
-    """ Test that GP prior has mean 0 and identical variances."""
+    # Test that GP prior has mean 0 and identical variances.
     for kernel in kernels:
         gpr = GaussianProcessRegressor(kernel=kernel)
 
@@ -125,7 +125,7 @@ def test_prior():
 
 
 def test_sample_statistics():
-    """ Test that statistics of samples drawn from GP are correct."""
+    # Test that statistics of samples drawn from GP are correct.
     for kernel in kernels:
         gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
 
@@ -140,14 +140,14 @@ def test_sample_statistics():
 
 
 def test_no_optimizer():
-    """ Test that kernel parameters are unmodified when optimizer is None."""
+    # Test that kernel parameters are unmodified when optimizer is None.
     kernel = RBF(1.0)
     gpr = GaussianProcessRegressor(kernel=kernel, optimizer=None).fit(X, y)
     assert_equal(np.exp(gpr.kernel_.theta), 1.0)
 
 
 def test_predict_cov_vs_std():
-    """ Test that predicted std.-dev. is consistent with cov's diagonal."""
+    # Test that predicted std.-dev. is consistent with cov's diagonal.
     for kernel in kernels:
         gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
         y_mean, y_cov = gpr.predict(X2, return_cov=True)
@@ -156,7 +156,7 @@ def test_predict_cov_vs_std():
 
 
 def test_anisotropic_kernel():
-    """ Test that GPR can identify meaningful anisotropic length-scales. """
+    # Test that GPR can identify meaningful anisotropic length-scales.
     # We learn a function which varies in one dimension ten-times slower
     # than in the other. The corresponding length-scales should differ by at
     # least a factor 5
@@ -171,10 +171,8 @@ def test_anisotropic_kernel():
 
 
 def test_random_starts():
-    """
-    Test that an increasing number of random-starts of GP fitting only
-    increases the log marginal likelihood of the chosen theta.
-    """
+    # Test that an increasing number of random-starts of GP fitting only
+    # increases the log marginal likelihood of the chosen theta.
     n_samples, n_features = 25, 2
     np.random.seed(0)
     rng = np.random.RandomState(0)
@@ -197,11 +195,10 @@ def test_random_starts():
 
 
 def test_y_normalization():
-    """ Test normalization of the target values in GP
+    # Test normalization of the target values in GP
 
-    Fitting non-normalizing GP on normalized y and fitting normalizing GP
-    on unnormalized y should yield identical results
-    """
+    # Fitting non-normalizing GP on normalized y and fitting normalizing GP
+    # on unnormalized y should yield identical results
     y_mean = y.mean(0)
     y_norm = y - y_mean
     for kernel in kernels:
@@ -226,7 +223,7 @@ def test_y_normalization():
 
 
 def test_y_multioutput():
-    """ Test that GPR can deal with multi-dimensional target values"""
+    # Test that GPR can deal with multi-dimensional target values
     y_2d = np.vstack((y, y * 2)).T
 
     # Test for fixed kernel that first dimension of 2d GP equals the output
@@ -269,7 +266,7 @@ def test_y_multioutput():
 
 
 def test_custom_optimizer():
-    """ Test that GPR can use externally defined optimizers. """
+    # Test that GPR can use externally defined optimizers.
     # Define a dummy optimizer that simply tests 50 random hyperparameters
     def optimizer(obj_func, initial_theta, bounds):
         rng = np.random.RandomState(0)
@@ -294,7 +291,7 @@ def optimizer(obj_func, initial_theta, bounds):
 
 
 def test_duplicate_input():
-    """ Test GPR can handle two different output-values for the same input. """
+    # Test GPR can handle two different output-values for the same input.
     for kernel in kernels:
         gpr_equal_inputs = \
             GaussianProcessRegressor(kernel=kernel, alpha=1e-2)
diff --git a/sklearn/gaussian_process/tests/test_kernels.py b/sklearn/gaussian_process/tests/test_kernels.py
index 116fad8dda..c51b89eeaa 100755
--- a/sklearn/gaussian_process/tests/test_kernels.py
+++ b/sklearn/gaussian_process/tests/test_kernels.py
@@ -3,7 +3,6 @@
 # Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
 # License: BSD 3 clause
 
-from collections import Hashable
 from sklearn.externals.funcsigs import signature
 
 import numpy as np
@@ -50,7 +49,7 @@
 
 
 def test_kernel_gradient():
-    """ Compare analytic and numeric gradient of kernels. """
+    # Compare analytic and numeric gradient of kernels.
     for kernel in kernels:
         K, K_gradient = kernel(X, eval_gradient=True)
 
@@ -70,7 +69,7 @@ def eval_kernel_for_theta(theta):
 
 
 def test_kernel_theta():
-    """ Check that parameter vector theta of kernel is set correctly. """
+    # Check that parameter vector theta of kernel is set correctly.
     for kernel in kernels:
         if isinstance(kernel, KernelOperator) \
            or isinstance(kernel, Exponentiation):  # skip non-basic kernels
@@ -111,8 +110,8 @@ def test_kernel_theta():
                 assert_array_equal(K_gradient[..., :i],
                                    K_gradient_new[..., :i])
             if i + 1 < len(kernel.hyperparameters):
-                assert_equal(theta[i+1:], new_kernel.theta[i:])
-                assert_array_equal(K_gradient[..., i+1:],
+                assert_equal(theta[i + 1:], new_kernel.theta[i:])
+                assert_array_equal(K_gradient[..., i + 1:],
                                    K_gradient_new[..., i:])
 
         # Check that values of theta are modified correctly
@@ -126,7 +125,7 @@ def test_kernel_theta():
 
 
 def test_auto_vs_cross():
-    """ Auto-correlation and cross-correlation should be consistent. """
+    # Auto-correlation and cross-correlation should be consistent.
     for kernel in kernels:
         if kernel == kernel_white:
             continue  # Identity is not satisfied on diagonal
@@ -136,7 +135,7 @@ def test_auto_vs_cross():
 
 
 def test_kernel_diag():
-    """ Test that diag method of kernel returns consistent results. """
+    # Test that diag method of kernel returns consistent results.
     for kernel in kernels:
         K_call_diag = np.diag(kernel(X))
         K_diag = kernel.diag(X)
@@ -144,7 +143,7 @@ def test_kernel_diag():
 
 
 def test_kernel_operator_commutative():
-    """ Adding kernels and multiplying kernels should be commutative. """
+    # Adding kernels and multiplying kernels should be commutative.
     # Check addition
     assert_almost_equal((RBF(2.0) + 1.0)(X),
                         (1.0 + RBF(2.0))(X))
@@ -155,7 +154,7 @@ def test_kernel_operator_commutative():
 
 
 def test_kernel_anisotropic():
-    """ Anisotropic kernel should be consistent with isotropic kernels."""
+    # Anisotropic kernel should be consistent with isotropic kernels.
     kernel = 3.0 * RBF([0.5, 2.0])
 
     K = kernel(X)
@@ -176,7 +175,7 @@ def test_kernel_anisotropic():
 
 
 def test_kernel_stationary():
-    """ Test stationarity of kernels."""
+    # Test stationarity of kernels.
     for kernel in kernels:
         if not kernel.is_stationary():
             continue
@@ -185,7 +184,7 @@ def test_kernel_stationary():
 
 
 def check_hyperparameters_equal(kernel1, kernel2):
-    """Check that hyperparameters of two kernels are equal"""
+    # Check that hyperparameters of two kernels are equal
     for attr in set(dir(kernel1) + dir(kernel2)):
         if attr.startswith("hyperparameter_"):
             attr_value1 = getattr(kernel1, attr)
@@ -194,7 +193,7 @@ def check_hyperparameters_equal(kernel1, kernel2):
 
 
 def test_kernel_clone():
-    """ Test that sklearn's clone works correctly on kernels. """
+    # Test that sklearn's clone works correctly on kernels.
     bounds = (1e-5, 1e5)
     for kernel in kernels:
         kernel_cloned = clone(kernel)
@@ -219,7 +218,8 @@ def test_kernel_clone():
         params = kernel.get_params()
         # RationalQuadratic kernel is isotropic.
         isotropic_kernels = (ExpSineSquared, RationalQuadratic)
-        if 'length_scale' in params and not isinstance(kernel, isotropic_kernels):
+        if 'length_scale' in params and not isinstance(kernel,
+                                                       isotropic_kernels):
             length_scale = params['length_scale']
             if np.iterable(length_scale):
                 params['length_scale'] = length_scale[0]
@@ -232,11 +232,12 @@ def test_kernel_clone():
             assert_equal(kernel_cloned_clone.get_params(),
                          kernel_cloned.get_params())
             assert_not_equal(id(kernel_cloned_clone), id(kernel_cloned))
-            yield check_hyperparameters_equal, kernel_cloned, kernel_cloned_clone
+            yield (check_hyperparameters_equal, kernel_cloned,
+                   kernel_cloned_clone)
 
 
 def test_matern_kernel():
-    """ Test consistency of Matern kernel for special values of nu. """
+    # Test consistency of Matern kernel for special values of nu.
     K = Matern(nu=1.5, length_scale=1.0)(X)
     # the diagonal elements of a matern kernel are 1
     assert_array_almost_equal(np.diag(K), np.ones(X.shape[0]))
@@ -255,7 +256,7 @@ def test_matern_kernel():
 
 
 def test_kernel_versus_pairwise():
-    """Check that GP kernels can also be used as pairwise kernels."""
+    # Check that GP kernels can also be used as pairwise kernels.
     for kernel in kernels:
         # Test auto-kernel
         if kernel != kernel_white:
@@ -272,7 +273,7 @@ def test_kernel_versus_pairwise():
 
 
 def test_set_get_params():
-    """Check that set_params()/get_params() is consistent with kernel.theta."""
+    # Check that set_params()/get_params() is consistent with kernel.theta.
     for kernel in kernels:
         # Test get_params()
         index = 0
@@ -282,7 +283,7 @@ def test_set_get_params():
                 continue
             size = hyperparameter.n_elements
             if size > 1:  # anisotropic kernels
-                assert_almost_equal(np.exp(kernel.theta[index:index+size]),
+                assert_almost_equal(np.exp(kernel.theta[index:index + size]),
                                     params[hyperparameter.name])
                 index += size
             else:
@@ -297,9 +298,9 @@ def test_set_get_params():
                 continue
             size = hyperparameter.n_elements
             if size > 1:  # anisotropic kernels
-                kernel.set_params(**{hyperparameter.name: [value]*size})
-                assert_almost_equal(np.exp(kernel.theta[index:index+size]),
-                                    [value]*size)
+                kernel.set_params(**{hyperparameter.name: [value] * size})
+                assert_almost_equal(np.exp(kernel.theta[index:index + size]),
+                                    [value] * size)
                 index += size
             else:
                 kernel.set_params(**{hyperparameter.name: value})
@@ -308,7 +309,7 @@ def test_set_get_params():
 
 
 def test_repr_kernels():
-    """Smoke-test for repr in kernels."""
+    # Smoke-test for repr in kernels.
 
     for kernel in kernels:
         repr(kernel)
diff --git a/sklearn/grid_search.py b/sklearn/grid_search.py
index 202327574e..0de08ee9e8 100755
--- a/sklearn/grid_search.py
+++ b/sklearn/grid_search.py
@@ -426,7 +426,7 @@ def score(self, X, y=None):
                           ChangedBehaviorWarning)
         return self.scorer_(self.best_estimator_, X, y)
 
-    @if_delegate_has_method(delegate='estimator')
+    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
     def predict(self, X):
         """Call predict on the estimator with the best found parameters.
 
@@ -442,7 +442,7 @@ def predict(self, X):
         """
         return self.best_estimator_.predict(X)
 
-    @if_delegate_has_method(delegate='estimator')
+    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
     def predict_proba(self, X):
         """Call predict_proba on the estimator with the best found parameters.
 
@@ -458,7 +458,7 @@ def predict_proba(self, X):
         """
         return self.best_estimator_.predict_proba(X)
 
-    @if_delegate_has_method(delegate='estimator')
+    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
     def predict_log_proba(self, X):
         """Call predict_log_proba on the estimator with the best found parameters.
 
@@ -474,7 +474,7 @@ def predict_log_proba(self, X):
         """
         return self.best_estimator_.predict_log_proba(X)
 
-    @if_delegate_has_method(delegate='estimator')
+    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
     def decision_function(self, X):
         """Call decision_function on the estimator with the best found parameters.
 
@@ -490,7 +490,7 @@ def decision_function(self, X):
         """
         return self.best_estimator_.decision_function(X)
 
-    @if_delegate_has_method(delegate='estimator')
+    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
     def transform(self, X):
         """Call transform on the estimator with the best found parameters.
 
@@ -506,7 +506,7 @@ def transform(self, X):
         """
         return self.best_estimator_.transform(X)
 
-    @if_delegate_has_method(delegate='estimator')
+    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
     def inverse_transform(self, Xt):
         """Call inverse_transform on the estimator with the best found parameters.
 
diff --git a/sklearn/kernel_ridge.py b/sklearn/kernel_ridge.py
index 682811cb61..c782886c73 100755
--- a/sklearn/kernel_ridge.py
+++ b/sklearn/kernel_ridge.py
@@ -69,8 +69,8 @@ class KernelRidge(BaseEstimator, RegressorMixin):
 
     Attributes
     ----------
-    dual_coef_ : array, shape = [n_features] or [n_targets, n_features]
-        Weight vector(s) in kernel space
+    dual_coef_ : array, shape = [n_samples] or [n_samples, n_targets]
+        Representation of weight vector(s) in kernel space
 
     X_fit_ : {array-like, sparse matrix}, shape = [n_samples, n_features]
         Training data, which is also required for prediction
diff --git a/sklearn/metrics/tests/test_ranking.py b/sklearn/metrics/tests/test_ranking.py
index 0ba1d858ab..ce9ad40f94 100755
--- a/sklearn/metrics/tests/test_ranking.py
+++ b/sklearn/metrics/tests/test_ranking.py
@@ -452,21 +452,6 @@ def test_precision_recall_curve():
     assert_equal(p.size, t.size + 1)
 
 
-def test_precision_recall_curve_pos_label():
-    y_true, _, probas_pred = make_prediction(binary=False)
-    pos_label = 2
-    p, r, thresholds = precision_recall_curve(y_true,
-                                              probas_pred[:, pos_label],
-                                              pos_label=pos_label)
-    p2, r2, thresholds2 = precision_recall_curve(y_true == pos_label,
-                                                 probas_pred[:, pos_label])
-    assert_array_almost_equal(p, p2)
-    assert_array_almost_equal(r, r2)
-    assert_array_almost_equal(thresholds, thresholds2)
-    assert_equal(p.size, r.size)
-    assert_equal(p.size, thresholds.size + 1)
-
-
 def _test_precision_recall_curve(y_true, probas_pred):
     # Test Precision-Recall and aread under PR curve
     p, r, thresholds = precision_recall_curve(y_true, probas_pred)
diff --git a/sklearn/model_selection/_search.py b/sklearn/model_selection/_search.py
index f1880555df..7c6344c02c 100755
--- a/sklearn/model_selection/_search.py
+++ b/sklearn/model_selection/_search.py
@@ -426,7 +426,7 @@ def _check_is_fitted(self, method_name):
         else:
             check_is_fitted(self, 'best_estimator_')
 
-    @if_delegate_has_method(delegate='estimator')
+    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
     def predict(self, X):
         """Call predict on the estimator with the best found parameters.
 
@@ -443,7 +443,7 @@ def predict(self, X):
         self._check_is_fitted('predict')
         return self.best_estimator_.predict(X)
 
-    @if_delegate_has_method(delegate='estimator')
+    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
     def predict_proba(self, X):
         """Call predict_proba on the estimator with the best found parameters.
 
@@ -460,7 +460,7 @@ def predict_proba(self, X):
         self._check_is_fitted('predict_proba')
         return self.best_estimator_.predict_proba(X)
 
-    @if_delegate_has_method(delegate='estimator')
+    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
     def predict_log_proba(self, X):
         """Call predict_log_proba on the estimator with the best found parameters.
 
@@ -477,7 +477,7 @@ def predict_log_proba(self, X):
         self._check_is_fitted('predict_log_proba')
         return self.best_estimator_.predict_log_proba(X)
 
-    @if_delegate_has_method(delegate='estimator')
+    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
     def decision_function(self, X):
         """Call decision_function on the estimator with the best found parameters.
 
@@ -494,7 +494,7 @@ def decision_function(self, X):
         self._check_is_fitted('decision_function')
         return self.best_estimator_.decision_function(X)
 
-    @if_delegate_has_method(delegate='estimator')
+    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
     def transform(self, X):
         """Call transform on the estimator with the best found parameters.
 
@@ -511,7 +511,7 @@ def transform(self, X):
         self._check_is_fitted('transform')
         return self.best_estimator_.transform(X)
 
-    @if_delegate_has_method(delegate='estimator')
+    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
     def inverse_transform(self, Xt):
         """Call inverse_transform on the estimator with the best found params.
 
diff --git a/sklearn/model_selection/tests/test_search.py b/sklearn/model_selection/tests/test_search.py
index 141c1a21b4..bb21a386d3 100755
--- a/sklearn/model_selection/tests/test_search.py
+++ b/sklearn/model_selection/tests/test_search.py
@@ -58,6 +58,7 @@
 from sklearn.metrics import roc_auc_score
 from sklearn.preprocessing import Imputer
 from sklearn.pipeline import Pipeline
+from sklearn.linear_model import SGDClassifier
 
 
 # Neither of the following two estimators inherit from BaseEstimator,
@@ -967,11 +968,13 @@ def test_grid_search_failing_classifier():
                       refit=False, error_score=0.0)
     assert_warns(FitFailedWarning, gs.fit, X, y)
     n_candidates = len(gs.cv_results_['params'])
+
     # Ensure that grid scores were set to zero as required for those fits
     # that are expected to fail.
-    get_cand_scores = lambda i: np.array(list(
-        gs.cv_results_['split%d_test_score' % s][i]
-        for s in range(gs.n_splits_)))
+    def get_cand_scores(i):
+        return np.array(list(gs.cv_results_['split%d_test_score' % s][i]
+                             for s in range(gs.n_splits_)))
+
     assert all((np.all(get_cand_scores(cand_i) == 0.0)
                 for cand_i in range(n_candidates)
                 if gs.cv_results_['param_parameter'][cand_i] ==
@@ -1028,3 +1031,33 @@ def test_parameters_sampler_replacement():
     sampler = ParameterSampler(params_distribution, n_iter=7)
     samples = list(sampler)
     assert_equal(len(samples), 7)
+
+
+def test_stochastic_gradient_loss_param():
+    # Make sure the predict_proba works when loss is specified
+    # as one of the parameters in the param_grid.
+    param_grid = {
+        'loss': ['log'],
+    }
+    X = np.arange(20).reshape(5, -1)
+    y = [0, 0, 1, 1, 1]
+    clf = GridSearchCV(estimator=SGDClassifier(loss='hinge'),
+                       param_grid=param_grid)
+
+    # When the estimator is not fitted, `predict_proba` is not available as the
+    # loss is 'hinge'.
+    assert_false(hasattr(clf, "predict_proba"))
+    clf.fit(X, y)
+    clf.predict_proba(X)
+    clf.predict_log_proba(X)
+
+    # Make sure `predict_proba` is not available when setting loss=['hinge']
+    # in param_grid
+    param_grid = {
+        'loss': ['hinge'],
+    }
+    clf = GridSearchCV(estimator=SGDClassifier(loss='hinge'),
+                       param_grid=param_grid)
+    assert_false(hasattr(clf, "predict_proba"))
+    clf.fit(X, y)
+    assert_false(hasattr(clf, "predict_proba"))
diff --git a/sklearn/neighbors/base.py b/sklearn/neighbors/base.py
index 164543333f..a2339aeb2d 100755
--- a/sklearn/neighbors/base.py
+++ b/sklearn/neighbors/base.py
@@ -583,11 +583,12 @@ class from an array representing our data set and ask who's
             # for efficiency, use squared euclidean distances
             if self.effective_metric_ == 'euclidean':
                 dist = pairwise_distances(X, self._fit_X, 'euclidean',
-                                          squared=True)
+                                          n_jobs=self.n_jobs, squared=True)
                 radius *= radius
             else:
                 dist = pairwise_distances(X, self._fit_X,
                                           self.effective_metric_,
+                                          n_jobs=self.n_jobs,
                                           **self.effective_metric_params_)
 
             neigh_ind_list = [np.where(d <= radius)[0] for d in dist]
diff --git a/sklearn/neural_network/multilayer_perceptron.py b/sklearn/neural_network/multilayer_perceptron.py
index f6cfabbd05..87ea951533 100755
--- a/sklearn/neural_network/multilayer_perceptron.py
+++ b/sklearn/neural_network/multilayer_perceptron.py
@@ -24,10 +24,11 @@
 from ..exceptions import ConvergenceWarning
 from ..utils.extmath import safe_sparse_dot
 from ..utils.validation import check_is_fitted
-from ..utils.multiclass import _check_partial_fit_first_call
+from ..utils.multiclass import _check_partial_fit_first_call, unique_labels
+from ..utils.multiclass import type_of_target
 
 
-_STOCHASTIC_ALGOS = ['sgd', 'adam']
+_STOCHASTIC_SOLVERS = ['sgd', 'adam']
 
 
 def _pack(coefs_, intercepts_):
@@ -43,13 +44,13 @@ class BaseMultilayerPerceptron(six.with_metaclass(ABCMeta, BaseEstimator)):
     """
 
     @abstractmethod
-    def __init__(self, hidden_layer_sizes, activation, algorithm,
+    def __init__(self, hidden_layer_sizes, activation, solver,
                  alpha, batch_size, learning_rate, learning_rate_init, power_t,
                  max_iter, loss, shuffle, random_state, tol, verbose,
                  warm_start, momentum, nesterovs_momentum, early_stopping,
                  validation_fraction, beta_1, beta_2, epsilon):
         self.activation = activation
-        self.algorithm = algorithm
+        self.solver = solver
         self.alpha = alpha
         self.batch_size = batch_size
         self.learning_rate = learning_rate
@@ -133,7 +134,7 @@ def _loss_grad_lbfgs(self, packed_coef_inter, X, y, activations, deltas,
         with respect to the different parameters given in the initialization.
 
         Returned gradients are packed in a single vector so it can be used
-        in l-bfgs
+        in lbgfs
 
         Parameters
         ----------
@@ -268,7 +269,7 @@ def _initialize(self, y, layer_units):
         if not isinstance(self, ClassifierMixin):
             self.out_activation_ = 'identity'
         # Output for multi class
-        elif self.label_binarizer_.y_type_ == 'multiclass':
+        elif self._label_binarizer.y_type_ == 'multiclass':
             self.out_activation_ = 'softmax'
         # Output for binary class and multi-label
         else:
@@ -284,7 +285,7 @@ def _initialize(self, y, layer_units):
             self.coefs_.append(coef_init)
             self.intercepts_.append(intercept_init)
 
-        if self.algorithm in _STOCHASTIC_ALGOS:
+        if self.solver in _STOCHASTIC_SOLVERS:
             self.loss_curve_ = []
             self._no_improvement_count = 0
             if self.early_stopping:
@@ -344,8 +345,8 @@ def _fit(self, X, y, incremental=False):
             # First time training the model
             self._initialize(y, layer_units)
 
-        # l-bfgs does not support mini-batches
-        if self.algorithm == 'l-bfgs':
+        # lbgfs does not support mini-batches
+        if self.solver == 'lbgfs':
             batch_size = n_samples
         elif self.batch_size == 'auto':
             batch_size = min(200, n_samples)
@@ -368,13 +369,13 @@ def _fit(self, X, y, incremental=False):
         intercept_grads = [np.empty(n_fan_out_) for n_fan_out_ in
                            layer_units[1:]]
 
-        # Run the Stochastic optimization algorithm
-        if self.algorithm in _STOCHASTIC_ALGOS:
+        # Run the Stochastic optimization solver
+        if self.solver in _STOCHASTIC_SOLVERS:
             self._fit_stochastic(X, y, activations, deltas, coef_grads,
                                  intercept_grads, layer_units, incremental)
 
-        # Run the LBFGS algorithm
-        elif self.algorithm == 'l-bfgs':
+        # Run the LBFGS solver
+        elif self.solver == 'lbgfs':
             self._fit_lbfgs(X, y, activations, deltas, coef_grads,
                             intercept_grads, layer_units)
         return self
@@ -421,11 +422,11 @@ def _validate_hyperparameters(self):
         if self.learning_rate not in ["constant", "invscaling", "adaptive"]:
             raise ValueError("learning rate %s is not supported. " %
                              self.learning_rate)
-        supported_algorithms = _STOCHASTIC_ALGOS + ["l-bfgs"]
-        if self.algorithm not in supported_algorithms:
-            raise ValueError("The algorithm %s is not supported. "
+        supported_solvers = _STOCHASTIC_SOLVERS + ["lbgfs"]
+        if self.solver not in supported_solvers:
+            raise ValueError("The solver %s is not supported. "
                              " Expected one of: %s" %
-                             (self.algorithm, ", ".join(supported_algorithms)))
+                             (self.solver, ", ".join(supported_solvers)))
 
     def _fit_lbfgs(self, X, y, activations, deltas, coef_grads,
                    intercept_grads, layer_units):
@@ -473,11 +474,11 @@ def _fit_stochastic(self, X, y, activations, deltas, coef_grads,
         if not incremental or not hasattr(self, '_optimizer'):
             params = self.coefs_ + self.intercepts_
 
-            if self.algorithm == 'sgd':
+            if self.solver == 'sgd':
                 self._optimizer = SGDOptimizer(
                     params, self.learning_rate_init, self.learning_rate,
                     self.momentum, self.nesterovs_momentum, self.power_t)
-            elif self.algorithm == 'adam':
+            elif self.solver == 'adam':
                 self._optimizer = AdamOptimizer(
                     params, self.learning_rate_init, self.beta_1, self.beta_2,
                     self.epsilon)
@@ -489,7 +490,7 @@ def _fit_stochastic(self, X, y, activations, deltas, coef_grads,
                 X, y, random_state=self._random_state,
                 test_size=self.validation_fraction)
             if isinstance(self, ClassifierMixin):
-                y_val = self.label_binarizer_.inverse_transform(y_val)
+                y_val = self._label_binarizer.inverse_transform(y_val)
         else:
             X_val = None
             y_val = None
@@ -630,10 +631,10 @@ def partial_fit(self):
         -------
         self : returns a trained MLP model.
         """
-        if self.algorithm not in _STOCHASTIC_ALGOS:
+        if self.solver not in _STOCHASTIC_SOLVERS:
             raise AttributeError("partial_fit is only available for stochastic"
-                                 " optimization algorithms. %s is not"
-                                 " stochastic" % self.algorithm)
+                                 " optimizers. %s is not stochastic."
+                                 % self.solver)
         return self._partial_fit
 
     def _partial_fit(self, X, y, classes=None):
@@ -679,8 +680,8 @@ def _predict(self, X):
 class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
     """Multi-layer Perceptron classifier.
 
-    This algorithm optimizes the log-loss function using l-bfgs or gradient
-    descent.
+    This model optimizes the log-loss function using LBFGS or stochastic
+    gradient descent.
 
     Parameters
     ----------
@@ -703,21 +704,20 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
         - 'relu', the rectified linear unit function,
           returns f(x) = max(0, x)
 
-    algorithm : {'l-bfgs', 'sgd', 'adam'}, default 'adam'
-        The algorithm for weight optimization.
+    solver : {'lbgfs', 'sgd', 'adam'}, default 'adam'
+        The solver for weight optimization.
 
-        - 'l-bfgs' is an optimization algorithm in the family of
-          quasi-Newton methods.
+        - 'lbgfs' is an optimizer in the family of quasi-Newton methods.
 
         - 'sgd' refers to stochastic gradient descent.
 
-        - 'adam' refers to a stochastic gradient-based optimization algorithm
-          proposed by Kingma, Diederik, and Jimmy Ba
+        - 'adam' refers to a stochastic gradient-based optimizer proposed
+          by Kingma, Diederik, and Jimmy Ba
 
-        Note: The default algorithm 'adam' works pretty well on relatively
+        Note: The default solver 'adam' works pretty well on relatively
         large datasets (with thousands of training samples or more) in terms of
         both training time and validation score.
-        For small datasets, however, 'l-bfgs' can converge faster and perform
+        For small datasets, however, 'lbgfs' can converge faster and perform
         better.
 
     alpha : float, optional, default 0.0001
@@ -725,7 +725,7 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
 
     batch_size : int, optional, default 'auto'
         Size of minibatches for stochastic optimizers.
-        If the algorithm is 'l-bfgs', the classifier will not use minibatch.
+        If the solver is 'lbgfs', the classifier will not use minibatch.
         When set to "auto", `batch_size=min(200, n_samples)`
 
     learning_rate : {'constant', 'invscaling', 'adaptive'}, default 'constant'
@@ -744,10 +744,10 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
           least tol, or fail to increase validation score by at least tol if
           'early_stopping' is on, the current learning rate is divided by 5.
 
-        Only used when ``algorithm='sgd'``.
+        Only used when ``solver='sgd'``.
 
     max_iter : int, optional, default 200
-        Maximum number of iterations. The algorithm iterates until convergence
+        Maximum number of iterations. The solver iterates until convergence
         (determined by 'tol') or this number of iterations.
 
     random_state : int or RandomState, optional, default None
@@ -755,7 +755,7 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
 
     shuffle : bool, optional, default True
         Whether to shuffle samples in each iteration. Only used when
-        algorithm='sgd' or 'adam'.
+        solver='sgd' or 'adam'.
 
     tol : float, optional, default 1e-4
         Tolerance for the optimization. When the loss or score is not improving
@@ -765,12 +765,12 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
 
     learning_rate_init : double, optional, default 0.001
         The initial learning rate used. It controls the step-size
-        in updating the weights. Only used when algorithm='sgd' or 'adam'.
+        in updating the weights. Only used when solver='sgd' or 'adam'.
 
     power_t : double, optional, default 0.5
         The exponent for inverse scaling learning rate.
         It is used in updating effective learning rate when the learning_rate
-        is set to 'invscaling'. Only used when algorithm='sgd'.
+        is set to 'invscaling'. Only used when solver='sgd'.
 
     verbose : bool, optional, default False
         Whether to print progress messages to stdout.
@@ -782,10 +782,10 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
 
     momentum : float, default 0.9
         Momentum for gradient descent update. Should be between 0 and 1. Only
-        used when algorithm='sgd'.
+        used when solver='sgd'.
 
     nesterovs_momentum : boolean, default True
-        Whether to use Nesterov's momentum. Only used when algorithm='sgd' and
+        Whether to use Nesterov's momentum. Only used when solver='sgd' and
         momentum > 0.
 
     early_stopping : bool, default False
@@ -794,7 +794,7 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
         aside 10% of training data as validation and terminate training when
         validation score is not improving by at least tol for two consecutive
         epochs.
-        Only effective when algorithm='sgd' or 'adam'
+        Only effective when solver='sgd' or 'adam'
 
     validation_fraction : float, optional, default 0.1
         The proportion of training data to set aside as validation set for
@@ -803,14 +803,14 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
 
     beta_1 : float, optional, default 0.9
         Exponential decay rate for estimates of first moment vector in adam,
-        should be in [0, 1). Only used when algorithm='adam'
+        should be in [0, 1). Only used when solver='adam'
 
     beta_2 : float, optional, default 0.999
         Exponential decay rate for estimates of second moment vector in adam,
-        should be in [0, 1). Only used when algorithm='adam'
+        should be in [0, 1). Only used when solver='adam'
 
     epsilon : float, optional, default 1e-8
-        Value for numerical stability in adam. Only used when algorithm='adam'
+        Value for numerical stability in adam. Only used when solver='adam'
 
     Attributes
     ----------
@@ -820,9 +820,6 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
     `loss_` : float
         The current loss computed with the loss function.
 
-    `label_binarizer_` : LabelBinarizer
-        A LabelBinarizer object trained on the training set.
-
     `coefs_` : list, length n_layers - 1
         The ith element in the list represents the weight matrix corresponding
         to layer i.
@@ -832,7 +829,7 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
         layer i + 1.
 
     n_iter_ : int,
-        The number of iterations the algorithm has ran.
+        The number of iterations the solver has ran.
 
     n_layers_ : int
         Number of layers.
@@ -873,7 +870,7 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
         optimization." arXiv preprint arXiv:1412.6980 (2014).
     """
     def __init__(self, hidden_layer_sizes=(100,), activation="relu",
-                 algorithm='adam', alpha=0.0001,
+                 solver='adam', alpha=0.0001,
                  batch_size='auto', learning_rate="constant",
                  learning_rate_init=0.001, power_t=0.5, max_iter=200,
                  shuffle=True, random_state=None, tol=1e-4,
@@ -884,7 +881,7 @@ def __init__(self, hidden_layer_sizes=(100,), activation="relu",
 
         sup = super(MLPClassifier, self)
         sup.__init__(hidden_layer_sizes=hidden_layer_sizes,
-                     activation=activation, algorithm=algorithm, alpha=alpha,
+                     activation=activation, solver=solver, alpha=alpha,
                      batch_size=batch_size, learning_rate=learning_rate,
                      learning_rate_init=learning_rate_init, power_t=power_t,
                      max_iter=max_iter, loss='log_loss', shuffle=shuffle,
@@ -895,25 +892,24 @@ def __init__(self, hidden_layer_sizes=(100,), activation="relu",
                      validation_fraction=validation_fraction,
                      beta_1=beta_1, beta_2=beta_2, epsilon=epsilon)
 
-        self.label_binarizer_ = LabelBinarizer()
-
     def _validate_input(self, X, y, incremental):
         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],
                          multi_output=True)
         if y.ndim == 2 and y.shape[1] == 1:
             y = column_or_1d(y, warn=True)
-        self.label_binarizer_.fit(y)
 
-        if not hasattr(self, 'classes_') or not incremental:
-            self.classes_ = self.label_binarizer_.classes_
+        if not incremental:
+            self._label_binarizer = LabelBinarizer()
+            self._label_binarizer.fit(y)
+            self.classes_ = self._label_binarizer.classes_
         else:
-            classes = self.label_binarizer_.classes_
-            if not np.all(np.in1d(classes, self.classes_)):
+            classes = unique_labels(y)
+            if np.setdiff1d(classes, self.classes_, assume_unique=True):
                 raise ValueError("`y` has classes not in `self.classes_`."
                                  " `self.classes_` has %s. 'y' has %s." %
                                  (self.classes_, classes))
 
-        y = self.label_binarizer_.transform(y)
+        y = self._label_binarizer.transform(y)
         return X, y
 
     def predict(self, X):
@@ -935,7 +931,7 @@ def predict(self, X):
         if self.n_outputs_ == 1:
             y_pred = y_pred.ravel()
 
-        return self.label_binarizer_.inverse_transform(y_pred)
+        return self._label_binarizer.inverse_transform(y_pred)
 
     @property
     def partial_fit(self):
@@ -961,14 +957,19 @@ def partial_fit(self):
         -------
         self : returns a trained MLP model.
         """
-        if self.algorithm not in _STOCHASTIC_ALGOS:
+        if self.solver not in _STOCHASTIC_SOLVERS:
             raise AttributeError("partial_fit is only available for stochastic"
-                                 " optimization algorithms. %s is not"
-                                 " stochastic" % self.algorithm)
+                                 " optimizer. %s is not stochastic"
+                                 % self.solver)
         return self._partial_fit
 
     def _partial_fit(self, X, y, classes=None):
-        _check_partial_fit_first_call(self, classes)
+        if _check_partial_fit_first_call(self, classes):
+            self._label_binarizer = LabelBinarizer()
+            if type_of_target(y).startswith('multilabel'):
+                self._label_binarizer.fit(y)
+            else:
+                self._label_binarizer.fit(classes)
 
         super(MLPClassifier, self)._partial_fit(X, y)
 
@@ -1021,7 +1022,8 @@ def predict_proba(self, X):
 class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):
     """Multi-layer Perceptron regressor.
 
-    This algorithm optimizes the squared-loss using l-bfgs or gradient descent.
+    This model optimizes the squared-loss using LBFGS or stochastic gradient
+    descent.
 
     Parameters
     ----------
@@ -1044,21 +1046,20 @@ class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):
         - 'relu', the rectified linear unit function,
           returns f(x) = max(0, x)
 
-    algorithm : {'l-bfgs', 'sgd', 'adam'}, default 'adam'
-        The algorithm for weight optimization.
+    solver : {'lbgfs', 'sgd', 'adam'}, default 'adam'
+        The solver for weight optimization.
 
-        - 'l-bfgs' is an optimization algorithm in the family of
-          quasi-Newton methods.
+        - 'lbgfs' is an optimizer in the family of quasi-Newton methods.
 
         - 'sgd' refers to stochastic gradient descent.
 
-        - 'adam' refers to a stochastic gradient-based optimization algorithm
-          proposed by Kingma, Diederik, and Jimmy Ba
+        - 'adam' refers to a stochastic gradient-based optimizer proposed by
+          Kingma, Diederik, and Jimmy Ba
 
-        Note: The default algorithm 'adam' works pretty well on relatively
+        Note: The default solver 'adam' works pretty well on relatively
         large datasets (with thousands of training samples or more) in terms of
         both training time and validation score.
-        For small datasets, however, 'l-bfgs' can converge faster and perform
+        For small datasets, however, 'lbgfs' can converge faster and perform
         better.
 
     alpha : float, optional, default 0.0001
@@ -1066,7 +1067,7 @@ class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):
 
     batch_size : int, optional, default 'auto'
         Size of minibatches for stochastic optimizers.
-        If the algorithm is 'l-bfgs', the classifier will not use minibatch.
+        If the solver is 'lbgfs', the classifier will not use minibatch.
         When set to "auto", `batch_size=min(200, n_samples)`
 
     learning_rate : {'constant', 'invscaling', 'adaptive'}, default 'constant'
@@ -1085,10 +1086,10 @@ class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):
           least tol, or fail to increase validation score by at least tol if
           'early_stopping' is on, the current learning rate is divided by 5.
 
-        Only used when algorithm='sgd'.
+        Only used when solver='sgd'.
 
     max_iter : int, optional, default 200
-        Maximum number of iterations. The algorithm iterates until convergence
+        Maximum number of iterations. The solver iterates until convergence
         (determined by 'tol') or this number of iterations.
 
     random_state : int or RandomState, optional, default None
@@ -1096,7 +1097,7 @@ class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):
 
     shuffle : bool, optional, default True
         Whether to shuffle samples in each iteration. Only used when
-        algorithm='sgd' or 'adam'.
+        solver='sgd' or 'adam'.
 
     tol : float, optional, default 1e-4
         Tolerance for the optimization. When the loss or score is not improving
@@ -1106,12 +1107,12 @@ class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):
 
     learning_rate_init : double, optional, default 0.001
         The initial learning rate used. It controls the step-size
-        in updating the weights. Only used when algorithm='sgd' or 'adam'.
+        in updating the weights. Only used when solver='sgd' or 'adam'.
 
     power_t : double, optional, default 0.5
         The exponent for inverse scaling learning rate.
         It is used in updating effective learning rate when the learning_rate
-        is set to 'invscaling'. Only used when algorithm='sgd'.
+        is set to 'invscaling'. Only used when solver='sgd'.
 
     verbose : bool, optional, default False
         Whether to print progress messages to stdout.
@@ -1123,10 +1124,10 @@ class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):
 
     momentum : float, default 0.9
         Momentum for gradient descent update.  Should be between 0 and 1. Only
-        used when algorithm='sgd'.
+        used when solver='sgd'.
 
     nesterovs_momentum : boolean, default True
-        Whether to use Nesterov's momentum. Only used when algorithm='sgd' and
+        Whether to use Nesterov's momentum. Only used when solver='sgd' and
         momentum > 0.
 
     early_stopping : bool, default False
@@ -1135,7 +1136,7 @@ class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):
         aside 10% of training data as validation and terminate training when
         validation score is not improving by at least tol for two consecutive
         epochs.
-        Only effective when algorithm='sgd' or 'adam'
+        Only effective when solver='sgd' or 'adam'
 
     validation_fraction : float, optional, default 0.1
         The proportion of training data to set aside as validation set for
@@ -1144,14 +1145,14 @@ class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):
 
     beta_1 : float, optional, default 0.9
         Exponential decay rate for estimates of first moment vector in adam,
-        should be in [0, 1). Only used when algorithm='adam'
+        should be in [0, 1). Only used when solver='adam'
 
     beta_2 : float, optional, default 0.999
         Exponential decay rate for estimates of second moment vector in adam,
-        should be in [0, 1). Only used when algorithm='adam'
+        should be in [0, 1). Only used when solver='adam'
 
     epsilon : float, optional, default 1e-8
-        Value for numerical stability in adam. Only used when algorithm='adam'
+        Value for numerical stability in adam. Only used when solver='adam'
 
     Attributes
     ----------
@@ -1167,7 +1168,7 @@ class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):
         layer i + 1.
 
     n_iter_ : int,
-        The number of iterations the algorithm has ran.
+        The number of iterations the solver has ran.
 
     n_layers_ : int
         Number of layers.
@@ -1208,7 +1209,7 @@ class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):
         optimization." arXiv preprint arXiv:1412.6980 (2014).
     """
     def __init__(self, hidden_layer_sizes=(100,), activation="relu",
-                 algorithm='adam', alpha=0.0001,
+                 solver='adam', alpha=0.0001,
                  batch_size='auto', learning_rate="constant",
                  learning_rate_init=0.001,
                  power_t=0.5, max_iter=200, shuffle=True,
@@ -1220,7 +1221,7 @@ def __init__(self, hidden_layer_sizes=(100,), activation="relu",
 
         sup = super(MLPRegressor, self)
         sup.__init__(hidden_layer_sizes=hidden_layer_sizes,
-                     activation=activation, algorithm=algorithm, alpha=alpha,
+                     activation=activation, solver=solver, alpha=alpha,
                      batch_size=batch_size, learning_rate=learning_rate,
                      learning_rate_init=learning_rate_init, power_t=power_t,
                      max_iter=max_iter, loss='squared_loss', shuffle=shuffle,
diff --git a/sklearn/neural_network/tests/test_mlp.py b/sklearn/neural_network/tests/test_mlp.py
index 484eee2d12..b8552246ce 100755
--- a/sklearn/neural_network/tests/test_mlp.py
+++ b/sklearn/neural_network/tests/test_mlp.py
@@ -74,13 +74,12 @@ def test_fit():
     # Test that the algorithm solution is equal to a worked out example.
     X = np.array([[0.6, 0.8, 0.7]])
     y = np.array([0])
-    mlp = MLPClassifier(algorithm='sgd', learning_rate_init=0.1, alpha=0.1,
+    mlp = MLPClassifier(solver='sgd', learning_rate_init=0.1, alpha=0.1,
                         activation='logistic', random_state=1, max_iter=1,
                         hidden_layer_sizes=2, momentum=0)
     # set weights
     mlp.coefs_ = [0] * 2
     mlp.intercepts_ = [0] * 2
-    mlp.classes_ = [0, 1]
     mlp.n_outputs_ = 1
     mlp.coefs_[0] = np.array([[0.1, 0.2], [0.3, 0.1], [0.5, 0]])
     mlp.coefs_[1] = np.array([[0.1], [0.2]])
@@ -89,8 +88,6 @@ def test_fit():
     mlp._coef_grads = [] * 2
     mlp._intercept_grads = [] * 2
 
-    mlp.label_binarizer_.y_type_ = 'binary'
-
     # Initialize parameters
     mlp.n_iter_ = 0
     mlp.learning_rate_ = 0.1
@@ -179,7 +176,7 @@ def test_gradient():
 
         for activation in ACTIVATION_TYPES:
             mlp = MLPClassifier(activation=activation, hidden_layer_sizes=10,
-                                algorithm='l-bfgs', alpha=1e-5,
+                                solver='lbgfs', alpha=1e-5,
                                 learning_rate_init=0.2, max_iter=1,
                                 random_state=1)
             mlp.fit(X, y)
@@ -238,7 +235,7 @@ def test_lbfgs_classification():
         expected_shape_dtype = (X_test.shape[0], y_train.dtype.kind)
 
         for activation in ACTIVATION_TYPES:
-            mlp = MLPClassifier(algorithm='l-bfgs', hidden_layer_sizes=50,
+            mlp = MLPClassifier(solver='lbgfs', hidden_layer_sizes=50,
                                 max_iter=150, shuffle=True, random_state=1,
                                 activation=activation)
             mlp.fit(X_train, y_train)
@@ -253,7 +250,7 @@ def test_lbfgs_regression():
     X = Xboston
     y = yboston
     for activation in ACTIVATION_TYPES:
-        mlp = MLPRegressor(algorithm='l-bfgs', hidden_layer_sizes=50,
+        mlp = MLPRegressor(solver='lbgfs', hidden_layer_sizes=50,
                            max_iter=150, shuffle=True, random_state=1,
                            activation=activation)
         mlp.fit(X, y)
@@ -269,7 +266,7 @@ def test_learning_rate_warmstart():
     X = [[3, 2], [1, 6], [5, 6], [-2, -4]]
     y = [1, 1, 1, 0]
     for learning_rate in ["invscaling", "constant"]:
-        mlp = MLPClassifier(algorithm='sgd', hidden_layer_sizes=4,
+        mlp = MLPClassifier(solver='sgd', hidden_layer_sizes=4,
                             learning_rate=learning_rate, max_iter=1,
                             power_t=0.25, warm_start=True)
         with ignore_warnings(category=ConvergenceWarning):
@@ -290,14 +287,14 @@ def test_multilabel_classification():
     # test fit method
     X, y = make_multilabel_classification(n_samples=50, random_state=0,
                                           return_indicator=True)
-    mlp = MLPClassifier(algorithm='l-bfgs', hidden_layer_sizes=50, alpha=1e-5,
+    mlp = MLPClassifier(solver='lbgfs', hidden_layer_sizes=50, alpha=1e-5,
                         max_iter=150, random_state=0, activation='logistic',
                         learning_rate_init=0.2)
     mlp.fit(X, y)
     assert_equal(mlp.score(X, y), 1)
 
     # test partial fit method
-    mlp = MLPClassifier(algorithm='sgd', hidden_layer_sizes=50, max_iter=150,
+    mlp = MLPClassifier(solver='sgd', hidden_layer_sizes=50, max_iter=150,
                         random_state=0, activation='logistic', alpha=1e-5,
                         learning_rate_init=0.2)
     for i in range(100):
@@ -308,7 +305,7 @@ def test_multilabel_classification():
 def test_multioutput_regression():
     # Test that multi-output regression works as expected
     X, y = make_regression(n_samples=200, n_targets=5)
-    mlp = MLPRegressor(algorithm='l-bfgs', hidden_layer_sizes=50, max_iter=200,
+    mlp = MLPRegressor(solver='lbgfs', hidden_layer_sizes=50, max_iter=200,
                        random_state=1)
     mlp.fit(X, y)
     assert_greater(mlp.score(X, y), 0.9)
@@ -318,7 +315,7 @@ def test_partial_fit_classes_error():
     # Tests that passing different classes to partial_fit raises an error
     X = [[3, 2]]
     y = [0]
-    clf = MLPClassifier(algorithm='sgd')
+    clf = MLPClassifier(solver='sgd')
     clf.partial_fit(X, y, classes=[0, 1])
     assert_raises(ValueError, clf.partial_fit, X, y, classes=[1, 2])
 
@@ -330,13 +327,13 @@ def test_partial_fit_classification():
     for X, y in classification_datasets:
         X = X
         y = y
-        mlp = MLPClassifier(algorithm='sgd', max_iter=100, random_state=1,
+        mlp = MLPClassifier(solver='sgd', max_iter=100, random_state=1,
                             tol=0, alpha=1e-5, learning_rate_init=0.2)
 
         with ignore_warnings(category=ConvergenceWarning):
             mlp.fit(X, y)
         pred1 = mlp.predict(X)
-        mlp = MLPClassifier(algorithm='sgd', random_state=1, alpha=1e-5,
+        mlp = MLPClassifier(solver='sgd', random_state=1, alpha=1e-5,
                             learning_rate_init=0.2)
         for i in range(100):
             mlp.partial_fit(X, y, classes=np.unique(y))
@@ -345,6 +342,17 @@ def test_partial_fit_classification():
         assert_greater(mlp.score(X, y), 0.95)
 
 
+def test_partial_fit_unseen_classes():
+    # Non regression test for bug 6994
+    # Tests for labeling errors in partial fit
+
+    clf = MLPClassifier(random_state=0)
+    clf.partial_fit([[1], [2], [3]], ["a", "b", "c"],
+                    classes=["a", "b", "c", "d"])
+    clf.partial_fit([[4]], ["d"])
+    assert_greater(clf.score([[1], [2], [3], [4]], ["a", "b", "c", "d"]), 0)
+
+
 def test_partial_fit_regression():
     # Test partial_fit on regression.
     # `partial_fit` should yield the same results as 'fit' for regression.
@@ -352,14 +360,14 @@ def test_partial_fit_regression():
     y = yboston
 
     for momentum in [0, .9]:
-        mlp = MLPRegressor(algorithm='sgd', max_iter=100, activation='relu',
+        mlp = MLPRegressor(solver='sgd', max_iter=100, activation='relu',
                            random_state=1, learning_rate_init=0.01,
                            batch_size=X.shape[0], momentum=momentum)
         with warnings.catch_warnings(record=True):
             # catch convergence warning
             mlp.fit(X, y)
         pred1 = mlp.predict(X)
-        mlp = MLPRegressor(algorithm='sgd', activation='relu',
+        mlp = MLPRegressor(solver='sgd', activation='relu',
                            learning_rate_init=0.01, random_state=1,
                            batch_size=X.shape[0], momentum=momentum)
         for i in range(100):
@@ -378,13 +386,10 @@ def test_partial_fit_errors():
 
     # no classes passed
     assert_raises(ValueError,
-                  MLPClassifier(
-                      algorithm='sgd').partial_fit,
-                  X, y,
-                  classes=[2])
+                  MLPClassifier(solver='sgd').partial_fit, X, y, classes=[2])
 
-    # l-bfgs doesn't support partial_fit
-    assert_false(hasattr(MLPClassifier(algorithm='l-bfgs'), 'partial_fit'))
+    # lbgfs doesn't support partial_fit
+    assert_false(hasattr(MLPClassifier(solver='lbgfs'), 'partial_fit'))
 
 
 def test_params_errors():
@@ -410,7 +415,7 @@ def test_params_errors():
     assert_raises(ValueError, clf(beta_2=-0.5).fit, X, y)
     assert_raises(ValueError, clf(epsilon=-0.5).fit, X, y)
 
-    assert_raises(ValueError, clf(algorithm='hadoken').fit, X, y)
+    assert_raises(ValueError, clf(solver='hadoken').fit, X, y)
     assert_raises(ValueError, clf(learning_rate='converge').fit, X, y)
     assert_raises(ValueError, clf(activation='cloak').fit, X, y)
 
@@ -466,7 +471,7 @@ def test_predict_proba_multilabel():
                                           return_indicator=True)
     n_samples, n_classes = Y.shape
 
-    clf = MLPClassifier(algorithm='l-bfgs', hidden_layer_sizes=30,
+    clf = MLPClassifier(solver='lbgfs', hidden_layer_sizes=30,
                         random_state=0)
     clf.fit(X, Y)
     y_proba = clf.predict_proba(X)
@@ -488,7 +493,7 @@ def test_sparse_matrices():
     X = X_digits_binary[:50]
     y = y_digits_binary[:50]
     X_sparse = csr_matrix(X)
-    mlp = MLPClassifier(algorithm='l-bfgs', hidden_layer_sizes=15,
+    mlp = MLPClassifier(solver='lbgfs', hidden_layer_sizes=15,
                         random_state=1)
     mlp.fit(X, y)
     pred1 = mlp.predict(X)
@@ -502,10 +507,10 @@ def test_sparse_matrices():
 
 def test_tolerance():
     # Test tolerance.
-    # It should force the algorithm to exit the loop when it converges.
+    # It should force the solver to exit the loop when it converges.
     X = [[3, 2], [1, 6]]
     y = [1, 0]
-    clf = MLPClassifier(tol=0.5, max_iter=3000, algorithm='sgd')
+    clf = MLPClassifier(tol=0.5, max_iter=3000, solver='sgd')
     clf.fit(X, y)
     assert_greater(clf.max_iter, clf.n_iter_)
 
@@ -514,7 +519,7 @@ def test_verbose_sgd():
     # Test verbose.
     X = [[3, 2], [1, 6]]
     y = [1, 0]
-    clf = MLPClassifier(algorithm='sgd', max_iter=2, verbose=10,
+    clf = MLPClassifier(solver='sgd', max_iter=2, verbose=10,
                         hidden_layer_sizes=2)
     old_stdout = sys.stdout
     sys.stdout = output = StringIO()
@@ -531,7 +536,7 @@ def test_early_stopping():
     X = X_digits_binary[:100]
     y = y_digits_binary[:100]
     tol = 0.2
-    clf = MLPClassifier(tol=tol, max_iter=3000, algorithm='sgd',
+    clf = MLPClassifier(tol=tol, max_iter=3000, solver='sgd',
                         early_stopping=True)
     clf.fit(X, y)
     assert_greater(clf.max_iter, clf.n_iter_)
@@ -546,7 +551,7 @@ def test_early_stopping():
 def test_adaptive_learning_rate():
     X = [[3, 2], [1, 6]]
     y = [1, 0]
-    clf = MLPClassifier(tol=0.5, max_iter=3000, algorithm='sgd',
+    clf = MLPClassifier(tol=0.5, max_iter=3000, solver='sgd',
                         learning_rate='adaptive')
     clf.fit(X, y)
     assert_greater(clf.max_iter, clf.n_iter_)
diff --git a/sklearn/tests/test_metaestimators.py b/sklearn/tests/test_metaestimators.py
index 4c6ace3d3a..f0f30cb91a 100755
--- a/sklearn/tests/test_metaestimators.py
+++ b/sklearn/tests/test_metaestimators.py
@@ -39,7 +39,8 @@ def __init__(self, name, construct, skip_methods=(),
                   skip_methods=['transform', 'inverse_transform', 'score']),
     DelegatorData('BaggingClassifier', BaggingClassifier,
                   skip_methods=['transform', 'inverse_transform', 'score',
-                                'predict_proba', 'predict_log_proba', 'predict'])
+                                'predict_proba', 'predict_log_proba',
+                                'predict'])
 ]
 
 
diff --git a/sklearn/utils/metaestimators.py b/sklearn/utils/metaestimators.py
index 9850ea50ea..346064448b 100755
--- a/sklearn/utils/metaestimators.py
+++ b/sklearn/utils/metaestimators.py
@@ -14,16 +14,22 @@ class _IffHasAttrDescriptor(object):
     """Implements a conditional property using the descriptor protocol.
 
     Using this class to create a decorator will raise an ``AttributeError``
-    if the ``attribute_name`` is not present on the base object.
+    if none of the delegates (specified in ``delegate_names``) is an attribute
+    of the base object or the first found delegate does not have an attribute
+    ``attribute_name``.
 
-    This allows ducktyping of the decorated method based on ``attribute_name``.
+    This allows ducktyping of the decorated method based on
+    ``delegate.attribute_name``. Here ``delegate`` is the first item in
+    ``delegate_names`` for which ``hasattr(object, delegate) is True``.
 
     See https://docs.python.org/3/howto/descriptor.html for an explanation of
     descriptors.
     """
-    def __init__(self, fn, attribute_name):
+    def __init__(self, fn, delegate_names, attribute_name):
         self.fn = fn
-        self.get_attribute = attrgetter(attribute_name)
+        self.delegate_names = delegate_names
+        self.attribute_name = attribute_name
+
         # update the docstring of the descriptor
         update_wrapper(self, fn)
 
@@ -32,7 +38,17 @@ def __get__(self, obj, type=None):
         if obj is not None:
             # delegate only on instances, not the classes.
             # this is to allow access to the docstrings.
-            self.get_attribute(obj)
+            for delegate_name in self.delegate_names:
+                try:
+                    delegate = attrgetter(delegate_name)(obj)
+                except AttributeError:
+                    continue
+                else:
+                    getattr(delegate, self.attribute_name)
+                    break
+            else:
+                attrgetter(self.delegate_names[-1])(obj)
+
         # lambda, but not partial, allows help() to work with update_wrapper
         out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)
         # update the docstring of the returned function
@@ -46,27 +62,18 @@ def if_delegate_has_method(delegate):
     This enables ducktyping by hasattr returning True according to the
     sub-estimator.
 
-    >>> from sklearn.utils.metaestimators import if_delegate_has_method
-    >>>
-    >>>
-    >>> class MetaEst(object):
-    ...     def __init__(self, sub_est):
-    ...         self.sub_est = sub_est
-    ...
-    ...     @if_delegate_has_method(delegate='sub_est')
-    ...     def predict(self, X):
-    ...         return self.sub_est.predict(X)
-    ...
-    >>> class HasPredict(object):
-    ...     def predict(self, X):
-    ...         return X.sum(axis=1)
-    ...
-    >>> class HasNoPredict(object):
-    ...     pass
-    ...
-    >>> hasattr(MetaEst(HasPredict()), 'predict')
-    True
-    >>> hasattr(MetaEst(HasNoPredict()), 'predict')
-    False
+    Parameters
+    ----------
+    delegate : string, list of strings or tuple of strings
+        Name of the sub-estimator that can be accessed as an attribute of the
+        base object. If a list or a tuple of names are provided, the first
+        sub-estimator that is an attribute of the base object  will be used.
+
     """
-    return lambda fn: _IffHasAttrDescriptor(fn, '%s.%s' % (delegate, fn.__name__))
+    if isinstance(delegate, list):
+        delegate = tuple(delegate)
+    if not isinstance(delegate, tuple):
+        delegate = (delegate,)
+
+    return lambda fn: _IffHasAttrDescriptor(fn, delegate,
+                                            attribute_name=fn.__name__)
diff --git a/sklearn/utils/tests/test_metaestimators.py b/sklearn/utils/tests/test_metaestimators.py
index cb1f46ef80..d73c67d0d1 100755
--- a/sklearn/utils/tests/test_metaestimators.py
+++ b/sklearn/utils/tests/test_metaestimators.py
@@ -1,5 +1,5 @@
+from nose.tools import assert_true, assert_false
 from sklearn.utils.metaestimators import if_delegate_has_method
-from nose.tools import assert_true
 
 
 class Prefix(object):
@@ -24,3 +24,57 @@ def test_delegated_docstring():
                 in str(MockMetaEstimator.func.__doc__))
     assert_true("This is a mock delegated function"
                 in str(MockMetaEstimator().func.__doc__))
+
+
+class MetaEst(object):
+    """A mock meta estimator"""
+    def __init__(self, sub_est, better_sub_est=None):
+        self.sub_est = sub_est
+        self.better_sub_est = better_sub_est
+
+    @if_delegate_has_method(delegate='sub_est')
+    def predict(self):
+        pass
+
+
+class MetaEstTestTuple(MetaEst):
+    """A mock meta estimator to test passing a tuple of delegates"""
+
+    @if_delegate_has_method(delegate=('sub_est', 'better_sub_est'))
+    def predict(self):
+        pass
+
+
+class MetaEstTestList(MetaEst):
+    """A mock meta estimator to test passing a list of delegates"""
+
+    @if_delegate_has_method(delegate=['sub_est', 'better_sub_est'])
+    def predict(self):
+        pass
+
+
+class HasPredict(object):
+    """A mock sub-estimator with predict method"""
+
+    def predict(self):
+        pass
+
+
+class HasNoPredict(object):
+    """A mock sub-estimator with no predict method"""
+    pass
+
+
+def test_if_delegate_has_method():
+    assert_true(hasattr(MetaEst(HasPredict()), 'predict'))
+    assert_false(hasattr(MetaEst(HasNoPredict()), 'predict'))
+    assert_false(
+        hasattr(MetaEstTestTuple(HasNoPredict(), HasNoPredict()), 'predict'))
+    assert_true(
+        hasattr(MetaEstTestTuple(HasPredict(), HasNoPredict()), 'predict'))
+    assert_false(
+        hasattr(MetaEstTestTuple(HasNoPredict(), HasPredict()), 'predict'))
+    assert_false(
+        hasattr(MetaEstTestList(HasNoPredict(), HasPredict()), 'predict'))
+    assert_true(
+        hasattr(MetaEstTestList(HasPredict(), HasPredict()), 'predict'))
diff --git a/sklearn/utils/tests/test_multiclass.py b/sklearn/utils/tests/test_multiclass.py
index a137ba63d5..fe96650447 100755
--- a/sklearn/utils/tests/test_multiclass.py
+++ b/sklearn/utils/tests/test_multiclass.py
@@ -292,12 +292,11 @@ def test_type_of_target():
 
     try:
         from pandas import SparseSeries
+        y = SparseSeries([1, 0, 0, 1, 0])
+        msg = "y cannot be class 'SparseSeries'."
+        assert_raises_regex(ValueError, msg, type_of_target, y)
     except ImportError:
         pass
-    y = SparseSeries([1, 0, 0, 1, 0])
-    msg = "y cannot be class 'SparseSeries'."
-    assert_raises_regex(ValueError, msg, type_of_target, y)
-
 
 def test_class_distribution():
     y = np.array([[1, 0, 0, 1],
