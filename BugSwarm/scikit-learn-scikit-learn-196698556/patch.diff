diff --git a/build_tools/circle/build_doc.sh b/build_tools/circle/build_doc.sh
index 35d3456f263d..84b4b8759608 100755
--- a/build_tools/circle/build_doc.sh
+++ b/build_tools/circle/build_doc.sh
@@ -107,18 +107,16 @@ then
    wget https://repo.continuum.io/miniconda/Miniconda-latest-Linux-x86_64.sh \
    -O miniconda.sh
 fi
-chmod +x miniconda.sh && ./miniconda.sh -b -p $HOME/miniconda
+chmod +x miniconda.sh && ./miniconda.sh -b -p $MINICONDA_PATH
 cd ..
-export PATH="$HOME/miniconda/bin:$PATH"
+export PATH="$MINICONDA_PATH/bin:$PATH"
 conda update --yes --quiet conda
 popd
 
 # Configure the conda environment and put it in the path using the
 # provided versions
-# Using sphinx 1.4 for now until sphinx-gallery has a fix for sphinx 1.5
-# See https://github.com/sphinx-gallery/sphinx-gallery/pull/178 for more details
-conda create -n testenv --yes --quiet python numpy scipy \
-  cython nose coverage matplotlib sphinx=1.4 pillow
+conda create -n $CONDA_ENV_NAME --yes --quiet python numpy scipy \
+  cython nose coverage matplotlib sphinx pillow
 source activate testenv
 
 # Build and install scikit-learn in dev mode
diff --git a/build_tools/circle/checkout_merge_commit.sh b/build_tools/circle/checkout_merge_commit.sh
new file mode 100755
index 000000000000..0d82d172e894
--- /dev/null
+++ b/build_tools/circle/checkout_merge_commit.sh
@@ -0,0 +1,29 @@
+#!/bin/bash
+
+
+# Add `master` branch to the update list.
+# Otherwise CircleCI will give us a cached one.
+FETCH_REFS="+master:master"
+
+# Update PR refs for testing.
+if [[ -n "${CIRCLE_PR_NUMBER}" ]]
+then
+    FETCH_REFS="${FETCH_REFS} +refs/pull/${CIRCLE_PR_NUMBER}/head:pr/${CIRCLE_PR_NUMBER}/head"
+    FETCH_REFS="${FETCH_REFS} +refs/pull/${CIRCLE_PR_NUMBER}/merge:pr/${CIRCLE_PR_NUMBER}/merge"
+fi
+
+# Retrieve the refs.
+git fetch -u origin ${FETCH_REFS}
+
+# Checkout the PR merge ref.
+if [[ -n "${CIRCLE_PR_NUMBER}" ]]
+then
+    git checkout -qf "pr/${CIRCLE_PR_NUMBER}/merge"
+fi
+
+# Check for merge conflicts.
+if [[ -n "${CIRCLE_PR_NUMBER}" ]]
+then
+    git branch --merged | grep master > /dev/null
+    git branch --merged | grep "pr/${CIRCLE_PR_NUMBER}/head" > /dev/null
+fi
diff --git a/build_tools/travis/test_script.sh b/build_tools/travis/test_script.sh
index 3e9bb9978161..6ab342b932cf 100755
--- a/build_tools/travis/test_script.sh
+++ b/build_tools/travis/test_script.sh
@@ -41,7 +41,7 @@ run_tests() {
 
     # Test doc
     cd $OLDPWD
-    make test-doc test-sphinxext
+    make test-doc
 }
 
 if [[ "$RUN_FLAKE8" == "true" ]]; then
diff --git a/circle.yml b/circle.yml
index a65067a06177..5b791aeb1102 100644
--- a/circle.yml
+++ b/circle.yml
@@ -1,3 +1,12 @@
+checkout:
+  post:
+    - ./build_tools/circle/checkout_merge_commit.sh
+
+machine:
+  environment:
+    MINICONDA_PATH: $HOME/miniconda
+    CONDA_ENV_NAME: testenv
+
 dependencies:
   cache_directories:
     - "~/scikit_learn_data"
@@ -8,10 +17,13 @@ dependencies:
   override:
     - ./build_tools/circle/build_doc.sh:
         timeout: 3600 # seconds
+
 test:
   override:
-    # override is needed otherwise nosetests is run by default
-    - echo "Documentation has been built in the 'dependencies' step. No additional test to run"
+    - |
+      export PATH="$MINICONDA_PATH/bin:$PATH"
+      source activate $CONDA_ENV_NAME
+      make test-sphinxext
 deployment:
  push:
    branch: /^master$|^[0-9]+\.[0-9]+\.X$/
diff --git a/doc/faq.rst b/doc/faq.rst
index 612a662ab2d8..519a46f0a79a 100644
--- a/doc/faq.rst
+++ b/doc/faq.rst
@@ -282,6 +282,9 @@ the design constraints of scikit-learn; as a result, deep learning
 and reinforcement learning are currently out of scope for what
 scikit-learn seeks to achieve.
 
+You can find more information about addition of gpu support at
+`Will you add GPU support?`_.
+
 Why is my pull request not getting any attention?
 -------------------------------------------------
 
diff --git a/doc/modules/manifold.rst b/doc/modules/manifold.rst
index 3c003e0d0cb5..ddc82fed31b1 100644
--- a/doc/modules/manifold.rst
+++ b/doc/modules/manifold.rst
@@ -305,11 +305,11 @@ The overall complexity of standard HLLE is
 Spectral Embedding
 ====================
 
-Spectral Embedding (also known as Laplacian Eigenmaps) is one method
-to calculate non-linear embedding. It finds a low dimensional representation
-of the data using a spectral decomposition of the graph Laplacian.
-The graph generated can be considered as a discrete approximation of the
-low dimensional manifold in the high dimensional space. Minimization of a
+Spectral Embedding is an approach to calculating a non-linear embedding.
+Scikit-learn implements Laplacian Eigenmaps, which finds a low dimensional
+representation of the data using a spectral decomposition of the graph
+Laplacian. The graph generated can be considered as a discrete approximation of
+the low dimensional manifold in the high dimensional space. Minimization of a
 cost function based on the graph ensures that points close to each other on
 the manifold are mapped close to each other in the low dimensional space,
 preserving local distances. Spectral embedding can be  performed with the
@@ -319,7 +319,7 @@ function :func:`spectral_embedding` or its object-oriented counterpart
 Complexity
 ----------
 
-The Spectral Embedding algorithm comprises three stages:
+The Spectral Embedding (Laplacian Eigenmaps) algorithm comprises three stages:
 
 1. **Weighted Graph Construction**. Transform the raw input data into
    graph representation using affinity (adjacency) matrix representation.
@@ -640,4 +640,3 @@ Tips on practical use
    :ref:`random_trees_embedding` can also be useful to derive non-linear
    representations of feature space, also it does not perform
    dimensionality reduction.
-
diff --git a/doc/related_projects.rst b/doc/related_projects.rst
index 8280ccc9a50a..29e0a3337e4b 100644
--- a/doc/related_projects.rst
+++ b/doc/related_projects.rst
@@ -12,19 +12,12 @@ Interoperability and framework enhancements
 These tools adapt scikit-learn for use with other technologies or otherwise
 enhance the functionality of scikit-learn's estimators.
 
-- `ML Frontend <https://github.com/jeff1evesque/machine-learning>`_ provides
-  dataset management and SVM fitting/prediction through
-  `web-based <https://github.com/jeff1evesque/machine-learning#web-interface>`_
-  and `programmatic <https://github.com/jeff1evesque/machine-learning#programmatic-interface>`_
-  interfaces.
+**Data formats**
 
 - `sklearn_pandas <https://github.com/paulgb/sklearn-pandas/>`_ bridge for
   scikit-learn pipelines and pandas data frame with dedicated transformers.
 
-- `Scikit-Learn Laboratory
-  <https://skll.readthedocs.io/en/latest/index.html>`_  A command-line
-  wrapper around scikit-learn that makes it easy to run machine learning
-  experiments with multiple learners and large feature sets.
+**Auto-ML**
 
 - `auto-sklearn <https://github.com/automl/auto-sklearn/>`_
   An automated machine learning toolkit and a drop-in replacement for a
@@ -36,6 +29,37 @@ enhance the functionality of scikit-learn's estimators.
   preprocessors as well as the estimators. Works as a drop-in replacement for a
   scikit-learn estimator.
 
+**Experimentation frameworks**
+
+- `PyMC <http://pymc-devs.github.io/pymc/>`_ Bayesian statistical models and
+  fitting algorithms.
+
+- `REP <https://github.com/yandex/REP>`_ Environment for conducting data-driven
+  research in a consistent and reproducible way
+
+- `ML Frontend <https://github.com/jeff1evesque/machine-learning>`_ provides
+  dataset management and SVM fitting/prediction through
+  `web-based <https://github.com/jeff1evesque/machine-learning#web-interface>`_
+  and `programmatic <https://github.com/jeff1evesque/machine-learning#programmatic-interface>`_
+  interfaces.
+
+- `Scikit-Learn Laboratory
+  <https://skll.readthedocs.io/en/latest/index.html>`_  A command-line
+  wrapper around scikit-learn that makes it easy to run machine learning
+  experiments with multiple learners and large feature sets.
+
+**Model inspection and visualisation**
+
+- `eli5 <https://github.com/TeamHG-Memex/eli5/>`_ A library for
+  debugging/inspecting machine learning models and explaining their
+  predictions.
+
+- `mlxtend <https://github.com/rasbt/mlxtend>`_ Includes model visualization
+  utilities.
+
+
+**Model export for production**
+
 - `sklearn-pmml <https://github.com/alex-pirozhenko/sklearn-pmml>`_
   Serialization of (some) scikit-learn estimators into PMML.
 
@@ -47,6 +71,12 @@ enhance the functionality of scikit-learn's estimators.
 - `sklearn-porter <https://github.com/nok/sklearn-porter>`_
   Transpile trained scikit-learn models to C, Java, Javascript and others.
 
+- `sklearn-compiledtrees <https://github.com/ajtulloch/sklearn-compiledtrees/>`_
+  Generate a C++ implementation of the predict function for decision trees (and
+  ensembles) trained by sklearn. Useful for latency-sensitive production
+  environments.
+
+
 Other estimators and tasks
 --------------------------
 
@@ -55,14 +85,7 @@ project. The following are projects providing interfaces similar to
 scikit-learn for additional learning algorithms, infrastructures
 and tasks.
 
-- `pylearn2 <http://deeplearning.net/software/pylearn2/>`_ A deep learning and
-  neural network library build on theano with scikit-learn like interface.
-
-- `sklearn_theano <http://sklearn-theano.github.io/>`_ scikit-learn compatible
-  estimators, transformers, and datasets which use Theano internally
-
-- `lightning <https://github.com/scikit-learn-contrib/lightning>`_ Fast state-of-the-art
-  linear model solvers (SDCA, AdaGrad, SVRG, SAG, etc...).
+**Structured learning**
 
 - `Seqlearn <https://github.com/larsmans/seqlearn>`_  Sequence classification
   using HMMs or structured perceptron.
@@ -81,25 +104,41 @@ and tasks.
   (`CRFsuite <http://www.chokkan.org/software/crfsuite/>`_ wrapper with
   sklearn-like API).
 
-- `py-earth <https://github.com/scikit-learn-contrib/py-earth>`_ Multivariate adaptive
-  regression splines
+**Deep neural networks etc.**
 
-- `sklearn-compiledtrees <https://github.com/ajtulloch/sklearn-compiledtrees/>`_
-  Generate a C++ implementation of the predict function for decision trees (and
-  ensembles) trained by sklearn. Useful for latency-sensitive production
-  environments.
+- `pylearn2 <http://deeplearning.net/software/pylearn2/>`_ A deep learning and
+  neural network library build on theano with scikit-learn like interface.
 
-- `lda <https://github.com/ariddell/lda/>`_: Fast implementation of latent
-  Dirichlet allocation in Cython which uses `Gibbs sampling
-  <https://en.wikipedia.org/wiki/Gibbs_sampling>`_ to sample from the true
-  posterior distribution. (scikit-learn's
-  :class:`sklearn.decomposition.LatentDirichletAllocation` implementation uses
-  `variational inference
-  <https://en.wikipedia.org/wiki/Variational_Bayesian_methods>`_ to sample from
-  a tractable approximation of a topic model's posterior distribution.)
+- `sklearn_theano <http://sklearn-theano.github.io/>`_ scikit-learn compatible
+  estimators, transformers, and datasets which use Theano internally
 
-- `Sparse Filtering <https://github.com/jmetzen/sparse-filtering>`_
-  Unsupervised feature learning based on sparse-filtering
+- `nolearn <https://github.com/dnouri/nolearn>`_ A number of wrappers and
+  abstractions around existing neural network libraries
+
+- `keras <https://github.com/fchollet/keras>`_ Deep Learning library capable of
+  running on top of either TensorFlow or Theano.
+
+- `lasagne <https://github.com/Lasagne/Lasagne>`_ A lightweight library to
+  build and train neural networks in Theano.
+
+**Broad scope**
+
+- `mlxtend <https://github.com/rasbt/mlxtend>`_ Includes a number of additional
+  estimators as well as model visualization utilities.
+
+- `sparkit-learn <https://github.com/lensacom/sparkit-learn>`_ Scikit-learn
+  API and functionality for PySpark's distributed modelling.
+
+**Other regression and classification**
+
+- `xgboost <https://github.com/dmlc/xgboost>`_ Optimised gradient boosted decision
+  tree library.
+
+- `lightning <https://github.com/scikit-learn-contrib/lightning>`_ Fast
+  state-of-the-art linear model solvers (SDCA, AdaGrad, SVRG, SAG, etc...).
+
+- `py-earth <https://github.com/scikit-learn-contrib/py-earth>`_ Multivariate
+  adaptive regression splines
 
 - `Kernel Regression <https://github.com/jmetzen/kernel_regression>`_
   Implementation of Nadaraya-Watson kernel regression with automatic bandwidth
@@ -108,28 +147,32 @@ and tasks.
 - `gplearn <https://github.com/trevorstephens/gplearn>`_ Genetic Programming
   for symbolic regression tasks.
 
-- `nolearn <https://github.com/dnouri/nolearn>`_ A number of wrappers and
-  abstractions around existing neural network libraries
+- `multiisotonic <https://github.com/alexfields/multiisotonic>`_ Isotonic
+  regression on multidimensional features.
 
-- `sparkit-learn <https://github.com/lensacom/sparkit-learn>`_ Scikit-learn functionality and API on PySpark.
+**Decomposition and clustering**
 
-- `keras <https://github.com/fchollet/keras>`_ Deep Learning library capable of
-  running on top of either TensorFlow or Theano.
-
-- `mlxtend <https://github.com/rasbt/mlxtend>`_ Includes a number of additional
-  estimators as well as model visualization utilities.
-
-- `kmodes <https://github.com/nicodv/kmodes>`_ k-modes clustering algorithm for categorical data, and
-  several of its variations.
+- `lda <https://github.com/ariddell/lda/>`_: Fast implementation of latent
+  Dirichlet allocation in Cython which uses `Gibbs sampling
+  <https://en.wikipedia.org/wiki/Gibbs_sampling>`_ to sample from the true
+  posterior distribution. (scikit-learn's
+  :class:`sklearn.decomposition.LatentDirichletAllocation` implementation uses
+  `variational inference
+  <https://en.wikipedia.org/wiki/Variational_Bayesian_methods>`_ to sample from
+  a tractable approximation of a topic model's posterior distribution.)
 
-- `hdbscan <https://github.com/lmcinnes/hdbscan>`_ HDBSCAN and Robust Single Linkage clustering algorithms
-  for robust variable density clustering.
+- `Sparse Filtering <https://github.com/jmetzen/sparse-filtering>`_
+  Unsupervised feature learning based on sparse-filtering
 
-- `lasagne <https://github.com/Lasagne/Lasagne>`_ A lightweight library to build and train neural networks in Theano.
+- `kmodes <https://github.com/nicodv/kmodes>`_ k-modes clustering algorithm for
+  categorical data, and several of its variations.
 
-- `multiisotonic <https://github.com/alexfields/multiisotonic>`_ Isotonic regression on multidimensional features.
+- `hdbscan <https://github.com/lmcinnes/hdbscan>`_ HDBSCAN and Robust Single
+  Linkage clustering algorithms for robust variable density clustering.
 
-- `spherecluster <https://github.com/clara-labs/spherecluster>`_ Spherical K-means and mixture of von Mises Fisher clustering routines for data on the unit hypersphere.
+- `spherecluster <https://github.com/clara-labs/spherecluster>`_ Spherical
+  K-means and mixture of von Mises Fisher clustering routines for data on the
+  unit hypersphere.
 
 Statistical learning with Python
 --------------------------------
@@ -145,12 +188,6 @@ Other packages useful for data analysis and machine learning.
   statistical models. More focused on statistical tests and less on prediction
   than scikit-learn.
 
-- `PyMC <http://pymc-devs.github.io/pymc/>`_ Bayesian statistical models and
-  fitting algorithms.
-
-- `REP <https://github.com/yandex/REP>`_ Environment for conducting data-driven
-  research in a consistent and reproducible way
-
 - `Sacred <https://github.com/IDSIA/Sacred>`_ Tool to help you configure,
   organize, log and reproduce experiments
 
diff --git a/doc/sphinxext/sphinx_gallery/__init__.py b/doc/sphinxext/sphinx_gallery/__init__.py
index 94a22d34ce56..80a27d3e7f2d 100644
--- a/doc/sphinxext/sphinx_gallery/__init__.py
+++ b/doc/sphinxext/sphinx_gallery/__init__.py
@@ -5,7 +5,7 @@
 
 """
 import os
-__version__ = '0.1.4'
+__version__ = '0.1.7'
 
 
 def glr_path_static():
diff --git a/doc/sphinxext/sphinx_gallery/_static/gallery.css b/doc/sphinxext/sphinx_gallery/_static/gallery.css
index ac10f8bd69eb..a7e28908aa25 100644
--- a/doc/sphinxext/sphinx_gallery/_static/gallery.css
+++ b/doc/sphinxext/sphinx_gallery/_static/gallery.css
@@ -103,9 +103,15 @@ thumbnail with its default link Background color */
 blockquote.sphx-glr-script-out {
   margin-left: 0pt;
 }
+
+div.sphx-glr-footer {
+    text-align: center;
+}
+
 div.sphx-glr-download {
   display: inline-block;
   margin: 1em auto 1ex 2ex;
+  vertical-align: middle;
 }
 
 div.sphx-glr-download a {
@@ -115,14 +121,45 @@ div.sphx-glr-download a {
   border: 1px solid #c2c22d;
   color: #000;
   display: inline-block;
+  /* Not valid in old browser, hence we keep the line above to override */
+  display: table-caption;
   font-weight: bold;
-  max-width: 45ex;
   padding: 1ex;
   text-align: center;
 }
 
+/* The last child of a download button is the file name */
+div.sphx-glr-download a span:last-child {
+    font-size: smaller;
+}
+
+@media (min-width: 20em) {
+    div.sphx-glr-download a {
+	min-width: 10em;
+    }
+}
+
+@media (min-width: 30em) {
+    div.sphx-glr-download a {
+	min-width: 13em;
+    }
+}
+
+@media (min-width: 40em) {
+    div.sphx-glr-download a {
+	min-width: 16em;
+    }
+}
+
+
 div.sphx-glr-download code.download {
   display: inline-block;
+  white-space: normal;
+  word-break: normal;
+  overflow-wrap: break-word;
+  /* border and background are given by the enclosing 'a' */
+  border: none;
+  background: none;
 }
 
 div.sphx-glr-download a:hover {
@@ -144,10 +181,34 @@ ul.sphx-glr-horizontal img {
 }
 
 p.sphx-glr-signature a.reference.external {
-  background-color: #EBECED;
   -moz-border-radius: 5px;
   -webkit-border-radius: 5px;
   border-radius: 5px;
   padding: 3px;
   font-size: 75%;
+  text-align: right;
+  margin-left: auto;
+  display: table;
+}
+
+a.sphx-glr-code-links:hover{
+    text-decoration: none;
+}
+
+a.sphx-glr-code-links[tooltip]:hover:before{
+    background: rgba(0,0,0,.8);
+    border-radius: 5px;
+    color: white;
+    content: attr(tooltip);
+    padding: 5px 15px;
+    position: absolute;
+    z-index: 98;
+    width: 16em;
+    word-break: normal;
+    white-space: normal;
+    display: inline-block;
+    text-align: center;
+    text-indent: 0;
+    margin-left: 0; /* Use zero to avoid overlapping with sidebar */
+    margin-top: 1.2em;
 }
diff --git a/doc/sphinxext/sphinx_gallery/backreferences.py b/doc/sphinxext/sphinx_gallery/backreferences.py
index 4fe579c6ad47..52f1b1209c32 100644
--- a/doc/sphinxext/sphinx_gallery/backreferences.py
+++ b/doc/sphinxext/sphinx_gallery/backreferences.py
@@ -103,7 +103,13 @@ def identify_names(code):
     for name, full_name in finder.get_mapping():
         # name is as written in file (e.g. np.asarray)
         # full_name includes resolved import path (e.g. numpy.asarray)
-        module, attribute = full_name.rsplit('.', 1)
+        splitted = full_name.rsplit('.', 1)
+        if len(splitted) == 1:
+            # module without attribute. This is not useful for
+            # backreferences
+            continue
+
+        module, attribute = splitted
         # get shortened module name
         module_short = get_short_module_name(module, attribute)
         cobj = {'name': attribute, 'module': module,
@@ -127,6 +133,8 @@ def scan_used_functions(example_file, gallery_conf):
     return backrefs
 
 
+# XXX This figure:: uses a forward slash even on Windows, but the op.join's
+# elsewhere will use backslashes...
 THUMBNAIL_TEMPLATE = """
 .. raw:: html
 
diff --git a/doc/sphinxext/sphinx_gallery/docs_resolv.py b/doc/sphinxext/sphinx_gallery/docs_resolv.py
index 0675a131d50d..b2757d2ef3b3 100644
--- a/doc/sphinxext/sphinx_gallery/docs_resolv.py
+++ b/doc/sphinxext/sphinx_gallery/docs_resolv.py
@@ -232,22 +232,34 @@ def _get_link(self, cobj):
                 fname_idx = value[cobj['name']][0]
 
         if fname_idx is not None:
-            fname = self._searchindex['filenames'][fname_idx] + '.html'
-
-            if self._is_windows:
-                fname = fname.replace('/', '\\')
-                link = os.path.join(self.doc_url, fname)
-            else:
-                link = posixpath.join(self.doc_url, fname)
-
-            if hasattr(link, 'decode'):
-                link = link.decode('utf-8', 'replace')
-
-            if link in self._page_cache:
-                html = self._page_cache[link]
+            fname = self._searchindex['filenames'][fname_idx]
+            # In 1.5+ Sphinx seems to have changed from .rst.html to only
+            # .html extension in converted files. But URLs could be
+            # built with < 1.5 or >= 1.5 regardless of what we're currently
+            # building with, so let's just check both :(
+            fnames = [fname + '.html', os.path.splitext(fname)[0] + '.html']
+            for fname in fnames:
+                try:
+                    if self._is_windows:
+                        fname = fname.replace('/', '\\')
+                        link = os.path.join(self.doc_url, fname)
+                    else:
+                        link = posixpath.join(self.doc_url, fname)
+
+                    if hasattr(link, 'decode'):
+                        link = link.decode('utf-8', 'replace')
+
+                    if link in self._page_cache:
+                        html = self._page_cache[link]
+                    else:
+                        html = get_data(link, self.gallery_dir)
+                        self._page_cache[link] = html
+                except (HTTPError, URLError, IOError):
+                    pass
+                else:
+                    break
             else:
-                html = get_data(link, self.gallery_dir)
-                self._page_cache[link] = html
+                raise
 
             # test if cobj appears in page
             comb_names = [cobj['module_short'] + '.' + cobj['name']]
@@ -345,7 +357,8 @@ def _embed_code_links(app, gallery_conf, gallery_dir):
                                                     gallery_dir))
 
     # patterns for replacement
-    link_pattern = '<a href="%s">%s</a>'
+    link_pattern = ('<a href="%s" class="sphx-glr-code-links" '
+       'tooltip="Link to documentation for %s">%s</a>')
     orig_pattern = '<span class="n">%s</span>'
     period = '<span class="o">.</span>'
 
@@ -374,15 +387,22 @@ def _embed_code_links(app, gallery_conf, gallery_dir):
                         link = doc_resolvers[this_module].resolve(cobj,
                                                                   full_fname)
                     except (HTTPError, URLError) as e:
-                        print("The following error has occurred:\n")
-                        print(repr(e))
+                        if isinstance(e, HTTPError):
+                            extra = e.code
+                        else:
+                            extra = e.reason
+                        print("\t\tError resolving %s.%s: %r (%s)"
+                              % (cobj['module'], cobj['name'], e, extra))
                         continue
 
                     if link is not None:
                         parts = name.split('.')
                         name_html = period.join(orig_pattern % part
                                                 for part in parts)
-                        str_repl[name_html] = link_pattern % (link, name_html)
+                        full_function_name = '%s.%s' % (
+                            cobj['module'], cobj['name'])
+                        str_repl[name_html] = link_pattern % (
+                            link, full_function_name, name_html)
                 # do the replacement in the html file
 
                 # ensure greediness
diff --git a/doc/sphinxext/sphinx_gallery/downloads.py b/doc/sphinxext/sphinx_gallery/downloads.py
index d67ad54e0d02..b962952316f3 100644
--- a/doc/sphinxext/sphinx_gallery/downloads.py
+++ b/doc/sphinxext/sphinx_gallery/downloads.py
@@ -13,22 +13,26 @@
 import zipfile
 
 CODE_DOWNLOAD = """
-\n.. container:: sphx-glr-download
+\n.. container:: sphx-glr-footer
 
-    :download:`Download Python source code: {0} <{0}>`\n
+\n  .. container:: sphx-glr-download
 
-\n.. container:: sphx-glr-download
+     :download:`Download Python source code: {0} <{0}>`\n
 
-    :download:`Download Jupyter notebook: {1} <{1}>`\n"""
+\n  .. container:: sphx-glr-download
+
+     :download:`Download Jupyter notebook: {1} <{1}>`\n"""
 
 CODE_ZIP_DOWNLOAD = """
-\n.. container:: sphx-glr-download
+\n.. container:: sphx-glr-footer
+
+\n  .. container:: sphx-glr-download
 
     :download:`Download all examples in Python source code: {0} </{1}>`\n
 
-\n.. container:: sphx-glr-download
+\n  .. container:: sphx-glr-download
 
-    :download:`Download all examples in Jupyter notebook files: {2} </{3}>`\n"""
+    :download:`Download all examples in Jupyter notebooks: {2} </{3}>`\n"""
 
 
 def python_zip(file_list, gallery_path, extension='.py'):
diff --git a/doc/sphinxext/sphinx_gallery/gen_gallery.py b/doc/sphinxext/sphinx_gallery/gen_gallery.py
index 352a71879747..bf4f22b5398f 100644
--- a/doc/sphinxext/sphinx_gallery/gen_gallery.py
+++ b/doc/sphinxext/sphinx_gallery/gen_gallery.py
@@ -20,6 +20,12 @@
 from .docs_resolv import embed_code_links
 from .downloads import generate_zipfiles
 
+try:
+    FileNotFoundError
+except NameError:
+    # Python2
+    FileNotFoundError = IOError
+
 DEFAULT_GALLERY_CONF = {
     'filename_pattern': re.escape(os.sep) + 'plot',
     'examples_dirs': os.path.join('..', 'examples'),
@@ -117,6 +123,10 @@ def generate_gallery_rst(app):
         this_fhindex, this_computation_times = \
             generate_dir_rst(examples_dir, gallery_dir, gallery_conf,
                              seen_backrefs)
+        if this_fhindex == "":
+            raise FileNotFoundError("Main example directory {0} does not "
+                                    "have a README.txt file. Please write "
+                                    "one to introduce your gallery.".format(examples_dir))
 
         computation_times += this_computation_times
 
@@ -178,8 +188,10 @@ def sumarize_failing_examples(app, exception):
         return
 
     gallery_conf = app.config.sphinx_gallery_conf
-    failing_examples = set(gallery_conf['failing_examples'])
-    expected_failing_examples = set(gallery_conf['expected_failing_examples'])
+    failing_examples = set([os.path.normpath(path) for path in
+                            gallery_conf['failing_examples']])
+    expected_failing_examples = set([os.path.normpath(path) for path in
+                                     gallery_conf['expected_failing_examples']])
 
     examples_expected_to_fail = failing_examples.intersection(
         expected_failing_examples)
@@ -204,7 +216,7 @@ def sumarize_failing_examples(app, exception):
         failing_examples)
     if examples_not_expected_to_pass:
         fail_msgs.append("Examples expected to fail, but not failling:\n" +
-                         "Please remove this examples from\n" +
+                         "Please remove these examples from\n" +
                          "sphinx_gallery_conf['expected_failing_examples']\n" +
                          "in your conf.py file"
                          "\n".join(examples_not_expected_to_pass))
diff --git a/doc/sphinxext/sphinx_gallery/gen_rst.py b/doc/sphinxext/sphinx_gallery/gen_rst.py
index 329f4623b9d9..cbc92f26b2ae 100644
--- a/doc/sphinxext/sphinx_gallery/gen_rst.py
+++ b/doc/sphinxext/sphinx_gallery/gen_rst.py
@@ -16,7 +16,6 @@
 # tricky errors come up with exec(code_blocks, ...) calls
 from __future__ import division, print_function, absolute_import
 from time import time
-import ast
 import codecs
 import hashlib
 import os
@@ -27,11 +26,8 @@
 import traceback
 import warnings
 
-from .downloads import CODE_DOWNLOAD
-
 
 # Try Python 2 first, otherwise load from Python 3
-from textwrap import dedent
 try:
     # textwrap indent only exists in python 3
     from textwrap import indent
@@ -59,7 +55,22 @@ def prefixed_lines():
     # make sure that the Agg backend is set before importing any
     # matplotlib
     import matplotlib
-    matplotlib.use('Agg')
+    matplotlib.use('agg')
+    matplotlib_backend = matplotlib.get_backend()
+
+    if matplotlib_backend != 'agg':
+        mpl_backend_msg = (
+            "Sphinx-Gallery relies on the matplotlib 'agg' backend to "
+            "render figures and write them to files. You are "
+            "currently using the {} backend. Sphinx-Gallery will "
+            "terminate the build now, because changing backends is "
+            "not well supported by matplotlib. We advise you to move "
+            "sphinx_gallery imports before any matplotlib-dependent "
+            "import. Moving sphinx_gallery imports at the top of "
+            "your conf.py file should fix this issue")
+
+        raise ValueError(mpl_backend_msg.format(matplotlib_backend))
+
     import matplotlib.pyplot as plt
 except ImportError:
     # this script can be imported by nosetest to find tests to run: we should
@@ -68,7 +79,11 @@ def prefixed_lines():
 
 from . import glr_path_static
 from .backreferences import write_backreferences, _thumbnail_div
-from .notebook import Notebook
+from .downloads import CODE_DOWNLOAD
+from .py_source_parser import (get_docstring_and_rest,
+                               split_code_and_text_blocks)
+
+from .notebook import jupyter_notebook, text2string, save_notebook
 
 try:
     basestring
@@ -144,80 +159,6 @@ def write(self, data):
     `Generated by Sphinx-Gallery <http://sphinx-gallery.readthedocs.io>`_\n"""
 
 
-def get_docstring_and_rest(filename):
-    """Separate `filename` content between docstring and the rest
-
-    Strongly inspired from ast.get_docstring.
-
-    Returns
-    -------
-    docstring: str
-        docstring of `filename`
-    rest: str
-        `filename` content without the docstring
-    """
-    # can't use codecs.open(filename, 'r', 'utf-8') here b/c ast doesn't
-    # seem to work with unicode strings in Python2.7
-    # "SyntaxError: encoding declaration in Unicode string"
-    with open(filename, 'rb') as f:
-        content = f.read()
-
-    node = ast.parse(content)
-    if not isinstance(node, ast.Module):
-        raise TypeError("This function only supports modules. "
-                        "You provided {0}".format(node.__class__.__name__))
-    if node.body and isinstance(node.body[0], ast.Expr) and \
-       isinstance(node.body[0].value, ast.Str):
-        docstring_node = node.body[0]
-        docstring = docstring_node.value.s
-        if hasattr(docstring, 'decode'):  # python2.7
-            docstring = docstring.decode('utf-8')
-        # This get the content of the file after the docstring last line
-        # Note: 'maxsplit' argument is not a keyword argument in python2
-        rest = content.decode('utf-8').split('\n', docstring_node.lineno)[-1]
-        return docstring, rest
-    else:
-        raise ValueError(('Could not find docstring in file "{0}". '
-                          'A docstring is required by sphinx-gallery')
-                         .format(filename))
-
-
-def split_code_and_text_blocks(source_file):
-    """Return list with source file separated into code and text blocks.
-
-    Returns
-    -------
-    blocks : list of (label, content)
-        List where each element is a tuple with the label ('text' or 'code'),
-        and content string of block.
-    """
-    docstring, rest_of_content = get_docstring_and_rest(source_file)
-    blocks = [('text', docstring)]
-
-    pattern = re.compile(
-        r'(?P<header_line>^#{20,}.*)\s(?P<text_content>(?:^#.*\s)*)',
-        flags=re.M)
-
-    pos_so_far = 0
-    for match in re.finditer(pattern, rest_of_content):
-        match_start_pos, match_end_pos = match.span()
-        code_block_content = rest_of_content[pos_so_far:match_start_pos]
-        text_content = match.group('text_content')
-        sub_pat = re.compile('^#', flags=re.M)
-        text_block_content = dedent(re.sub(sub_pat, '', text_content)).lstrip()
-        if code_block_content.strip():
-            blocks.append(('code', code_block_content))
-        if text_block_content.strip():
-            blocks.append(('text', text_block_content))
-        pos_so_far = match_end_pos
-
-    remaining_content = rest_of_content[pos_so_far:]
-    if remaining_content.strip():
-        blocks.append(('code', remaining_content))
-
-    return blocks
-
-
 def codestr2rst(codestr, lang='python'):
     """Return reStructuredText code block from code string"""
     code_directive = "\n.. code-block:: {0}\n\n".format(lang)
@@ -225,20 +166,13 @@ def codestr2rst(codestr, lang='python'):
     return code_directive + indented_block
 
 
-def text2string(content):
-    """Returns a string without the extra triple quotes"""
-    try:
-        return ast.literal_eval(content) + '\n'
-    except Exception:
-        return content + '\n'
-
-
 def extract_thumbnail_number(text):
     """ Pull out the thumbnail image number specified in the docstring. """
 
     # check whether the user has specified a specific thumbnail image
     pattr = re.compile(
-        r"^\s*#\s*sphinx_gallery_thumbnail_number\s*=\s*([0-9]+)\s*$", flags=re.MULTILINE)
+        r"^\s*#\s*sphinx_gallery_thumbnail_number\s*=\s*([0-9]+)\s*$",
+        flags=re.MULTILINE)
     match = pattr.search(text)
 
     if match is None:
@@ -273,19 +207,15 @@ def extract_intro(filename):
 def get_md5sum(src_file):
     """Returns md5sum of file"""
 
-    with open(src_file, 'r') as src_data:
+    with open(src_file, 'rb') as src_data:
         src_content = src_data.read()
 
-        # data needs to be encoded in python3 before hashing
-        if sys.version_info[0] == 3:
-            src_content = src_content.encode('utf-8')
-
         src_md5 = hashlib.md5(src_content).hexdigest()
     return src_md5
 
 
 def md5sum_is_current(src_file):
-    """Returns True if src_file has the same md5 hash as the one stored on disk"""
+    """Checks whether src_file has the same md5 hash as the one on disk"""
 
     src_md5 = get_md5sum(src_file)
 
@@ -320,11 +250,11 @@ def save_figures(image_path, fig_count, gallery_conf):
     """
     figure_list = []
 
-    fig_managers = matplotlib._pylab_helpers.Gcf.get_all_fig_managers()
-    for fig_mngr in fig_managers:
+    fig_numbers = plt.get_fignums()
+    for fig_num in fig_numbers:
         # Set the fig_num figure as the current figure as we can't
         # save a figure that's not the current figure.
-        fig = plt.figure(fig_mngr.num)
+        fig = plt.figure(fig_num)
         kwargs = {}
         to_rgba = matplotlib.colors.colorConverter.to_rgba
         for attr in ['facecolor', 'edgecolor']:
@@ -333,7 +263,7 @@ def save_figures(image_path, fig_count, gallery_conf):
             if to_rgba(fig_attr) != to_rgba(default_attr):
                 kwargs[attr] = fig_attr
 
-        current_fig = image_path.format(fig_count + fig_mngr.num)
+        current_fig = image_path.format(fig_count + fig_num)
         fig.savefig(current_fig, **kwargs)
         figure_list.append(current_fig)
 
@@ -452,6 +382,8 @@ def generate_dir_rst(src_dir, target_dir, gallery_conf, seen_backrefs):
         return "", []  # because string is an expected return type
 
     fhindex = open(os.path.join(src_dir, 'README.txt')).read()
+    # Add empty lines to avoid bug in issue #165
+    fhindex += "\n\n"
 
     if not os.path.exists(target_dir):
         os.makedirs(target_dir)
@@ -609,7 +541,6 @@ def generate_file_rst(fname, target_dir, src_dir, gallery_conf):
 
     ref_fname = example_file.replace(os.path.sep, '_')
     example_rst = """\n\n.. _sphx_glr_{0}:\n\n""".format(ref_fname)
-    example_nb = Notebook(fname, target_dir)
 
     filename_pattern = gallery_conf.get('filename_pattern')
     execute_script = re.search(filename_pattern, src_file) and gallery_conf[
@@ -642,7 +573,6 @@ def generate_file_rst(fname, target_dir, src_dir, gallery_conf):
                                                     gallery_conf)
 
             time_elapsed += rtime
-            example_nb.add_code_cell(bcontent)
 
             if is_example_notebook_like:
                 example_rst += codestr2rst(bcontent) + '\n'
@@ -656,7 +586,6 @@ def generate_file_rst(fname, target_dir, src_dir, gallery_conf):
 
         else:
             example_rst += text2string(bcontent) + '\n'
-            example_nb.add_markdown_cell(text2string(bcontent))
 
     clean_modules()
 
@@ -669,13 +598,15 @@ def generate_file_rst(fname, target_dir, src_dir, gallery_conf):
     save_thumbnail(image_path_template, src_file, gallery_conf)
 
     time_m, time_s = divmod(time_elapsed, 60)
-    example_nb.save_file()
+    example_nb = jupyter_notebook(script_blocks)
+    save_notebook(example_nb, example_file.replace('.py', '.ipynb'))
     with codecs.open(os.path.join(target_dir, base_image_name + '.rst'),
                      mode='w', encoding='utf-8') as f:
         example_rst += "**Total running time of the script:**" \
                        " ({0: .0f} minutes {1: .3f} seconds)\n\n".format(
                            time_m, time_s)
-        example_rst += CODE_DOWNLOAD.format(fname, example_nb.file_name)
+        example_rst += CODE_DOWNLOAD.format(fname,
+                                            fname.replace('.py', '.ipynb'))
         example_rst += SPHX_GLR_SIG
         f.write(example_rst)
 
diff --git a/doc/sphinxext/sphinx_gallery/notebook.py b/doc/sphinxext/sphinx_gallery/notebook.py
index fc0fccfb6b98..31d29279ec01 100644
--- a/doc/sphinxext/sphinx_gallery/notebook.py
+++ b/doc/sphinxext/sphinx_gallery/notebook.py
@@ -12,13 +12,22 @@
 
 from __future__ import division, absolute_import, print_function
 from functools import partial
+import argparse
 import json
-import os
 import re
 import sys
+from .py_source_parser import split_code_and_text_blocks
 
 
-def ipy_notebook_skeleton():
+def text2string(content):
+    """Returns a string without the extra triple quotes"""
+    try:
+        return ast.literal_eval(content) + '\n'
+    except Exception:
+        return content + '\n'
+
+
+def jupyter_notebook_skeleton():
     """Returns a dictionary with the elements of a Jupyter notebook"""
     py_version = sys.version_info
     notebook_skeleton = {
@@ -98,61 +107,96 @@ def rst2md(text):
     return text
 
 
-class Notebook(object):
-    """Jupyter notebook object
+def jupyter_notebook(script_blocks):
+    """Generate a Jupyter notebook file cell-by-cell
 
-    Constructs the file cell-by-cell and writes it at the end"""
+    Parameters
+    ----------
+    script_blocks: list
+        script execution cells
+    """
 
-    def __init__(self, file_name, target_dir):
-        """Declare the skeleton of the notebook
+    work_notebook = jupyter_notebook_skeleton()
+    add_code_cell(work_notebook, "%matplotlib inline")
+    fill_notebook(work_notebook, script_blocks)
 
-        Parameters
-        ----------
-        file_name : str
-            original script file name, .py extension will be renamed
-        target_dir: str
-            directory where notebook file is to be saved
-        """
+    return work_notebook
 
-        self.file_name = file_name.replace('.py', '.ipynb')
-        self.write_file = os.path.join(target_dir, self.file_name)
-        self.work_notebook = ipy_notebook_skeleton()
-        self.add_code_cell("%matplotlib inline")
 
-    def add_code_cell(self, code):
-        """Add a code cell to the notebook
+def add_code_cell(work_notebook, code):
+    """Add a code cell to the notebook
 
-        Parameters
-        ----------
-        code : str
-            Cell content
-        """
+    Parameters
+    ----------
+    code : str
+        Cell content
+    """
 
-        code_cell = {
-            "cell_type": "code",
-            "execution_count": None,
-            "metadata": {"collapsed": False},
-            "outputs": [],
-            "source": [code.strip()]
-            }
-        self.work_notebook["cells"].append(code_cell)
-
-    def add_markdown_cell(self, text):
-        """Add a markdown cell to the notebook
-
-        Parameters
-        ----------
-        code : str
-            Cell content
-        """
-        markdown_cell = {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [rst2md(text)]
-        }
-        self.work_notebook["cells"].append(markdown_cell)
-
-    def save_file(self):
-        """Saves the notebook to a file"""
-        with open(self.write_file, 'w') as out_nb:
-            json.dump(self.work_notebook, out_nb, indent=2)
+    code_cell = {
+        "cell_type": "code",
+        "execution_count": None,
+        "metadata": {"collapsed": False},
+        "outputs": [],
+        "source": [code.strip()]
+    }
+    work_notebook["cells"].append(code_cell)
+
+
+def add_markdown_cell(work_notebook, text):
+    """Add a markdown cell to the notebook
+
+    Parameters
+    ----------
+    code : str
+        Cell content
+    """
+    markdown_cell = {
+        "cell_type": "markdown",
+        "metadata": {},
+        "source": [rst2md(text)]
+    }
+    work_notebook["cells"].append(markdown_cell)
+
+
+def fill_notebook(work_notebook, script_blocks):
+    """Writes the Jupyter notebook cells
+
+    Parameters
+    ----------
+    script_blocks : list of tuples
+    """
+
+    for blabel, bcontent in script_blocks:
+        if blabel == 'code':
+            add_code_cell(work_notebook, bcontent)
+        else:
+            add_markdown_cell(work_notebook, text2string(bcontent))
+
+
+def save_notebook(work_notebook, write_file):
+    """Saves the Jupyter work_notebook to write_file"""
+    with open(write_file, 'w') as out_nb:
+        json.dump(work_notebook, out_nb, indent=2)
+
+
+###############################################################################
+# Notebook shell utility
+
+def python_to_jupyter_cli(args=None, namespace=None):
+    """Exposes the jupyter notebook renderer to the command line
+
+    Takes the same arguments as ArgumentParser.parse_args
+    """
+    parser = argparse.ArgumentParser(
+        description='Sphinx-Gallery Notebook converter')
+    parser.add_argument('python_src_file', nargs='+',
+                        help='Input Python file script to convert. '
+                        'Supports multiple files and shell wildcards'
+                        ' (e.g. *.py)')
+    args = parser.parse_args(args, namespace)
+
+    for src_file in args.python_src_file:
+        blocks = split_code_and_text_blocks(src_file)
+        print('Converting {0}'.format(src_file))
+        example_nb = jupyter_notebook(blocks)
+        save_notebook(example_nb, src_file.replace('.py', '.ipynb'))
diff --git a/doc/sphinxext/sphinx_gallery/py_source_parser.py b/doc/sphinxext/sphinx_gallery/py_source_parser.py
new file mode 100644
index 000000000000..6d85e75d43dd
--- /dev/null
+++ b/doc/sphinxext/sphinx_gallery/py_source_parser.py
@@ -0,0 +1,88 @@
+# -*- coding: utf-8 -*-
+r"""
+Parser for python source files
+==============================
+"""
+# Created Sun Nov 27 14:03:07 2016
+# Author: Óscar Nájera
+
+from __future__ import division, absolute_import, print_function
+import ast
+import re
+from textwrap import dedent
+
+
+def get_docstring_and_rest(filename):
+    """Separate `filename` content between docstring and the rest
+
+    Strongly inspired from ast.get_docstring.
+
+    Returns
+    -------
+    docstring: str
+        docstring of `filename`
+    rest: str
+        `filename` content without the docstring
+    """
+    # can't use codecs.open(filename, 'r', 'utf-8') here b/c ast doesn't
+    # seem to work with unicode strings in Python2.7
+    # "SyntaxError: encoding declaration in Unicode string"
+    with open(filename, 'rb') as f:
+        content = f.read()
+    # change from Windows format to UNIX for uniformity
+    content = content.replace(b'\r\n', b'\n')
+
+    node = ast.parse(content)
+    if not isinstance(node, ast.Module):
+        raise TypeError("This function only supports modules. "
+                        "You provided {0}".format(node.__class__.__name__))
+    if node.body and isinstance(node.body[0], ast.Expr) and \
+       isinstance(node.body[0].value, ast.Str):
+        docstring_node = node.body[0]
+        docstring = docstring_node.value.s
+        if hasattr(docstring, 'decode'):  # python2.7
+            docstring = docstring.decode('utf-8')
+        # This get the content of the file after the docstring last line
+        # Note: 'maxsplit' argument is not a keyword argument in python2
+        rest = content.decode('utf-8').split('\n', docstring_node.lineno)[-1]
+        return docstring, rest
+    else:
+        raise ValueError(('Could not find docstring in file "{0}". '
+                          'A docstring is required by sphinx-gallery')
+                         .format(filename))
+
+
+def split_code_and_text_blocks(source_file):
+    """Return list with source file separated into code and text blocks.
+
+    Returns
+    -------
+    blocks : list of (label, content)
+        List where each element is a tuple with the label ('text' or 'code'),
+        and content string of block.
+    """
+    docstring, rest_of_content = get_docstring_and_rest(source_file)
+    blocks = [('text', docstring)]
+
+    pattern = re.compile(
+        r'(?P<header_line>^#{20,}.*)\s(?P<text_content>(?:^#.*\s)*)',
+        flags=re.M)
+
+    pos_so_far = 0
+    for match in re.finditer(pattern, rest_of_content):
+        match_start_pos, match_end_pos = match.span()
+        code_block_content = rest_of_content[pos_so_far:match_start_pos]
+        text_content = match.group('text_content')
+        sub_pat = re.compile('^#', flags=re.M)
+        text_block_content = dedent(re.sub(sub_pat, '', text_content)).lstrip()
+        if code_block_content.strip():
+            blocks.append(('code', code_block_content))
+        if text_block_content.strip():
+            blocks.append(('text', text_block_content))
+        pos_so_far = match_end_pos
+
+    remaining_content = rest_of_content[pos_so_far:]
+    if remaining_content.strip():
+        blocks.append(('code', remaining_content))
+
+    return blocks
diff --git a/doc/sphinxext/sphinx_issues.py b/doc/sphinxext/sphinx_issues.py
index 63ebb60f312d..f4b8c9346b56 100644
--- a/doc/sphinxext/sphinx_issues.py
+++ b/doc/sphinxext/sphinx_issues.py
@@ -1,6 +1,6 @@
 # -*- coding: utf-8 -*-
-"""A Sphinx extension for linking to your project's issue tracker."""
-"""
+"""A Sphinx extension for linking to your project's issue tracker.
+
 Copyright 2014 Steven Loria
 
 Permission is hereby granted, free of charge, to any person obtaining a copy
@@ -22,12 +22,8 @@
 THE SOFTWARE.
 """
 
-try:
-    from docutils import nodes, utils
-    from sphinx.util.nodes import split_explicit_title
-except ImportError:
-    # Load lazily so that test-sphinxext does not require docutils dependency
-    pass
+from docutils import nodes, utils
+from sphinx.util.nodes import split_explicit_title
 
 __version__ = '0.2.0'
 __author__ = 'Steven Loria'
diff --git a/doc/whats_new.rst b/doc/whats_new.rst
index a0e2509957b3..9d76f5377c0e 100644
--- a/doc/whats_new.rst
+++ b/doc/whats_new.rst
@@ -41,14 +41,18 @@ New features
      Kullback-Leibler divergence and the Itakura-Saito divergence.
      By `Tom Dupre la Tour`_.
 
-   - Added :func:`metrics.mean_squared_log_error`, which computes 
-     the mean square error of the logarithmic transformation of targets, 
+   - Added :func:`metrics.mean_squared_log_error`, which computes
+     the mean square error of the logarithmic transformation of targets,
      particularly useful for targets with an exponential trend.
      :issue:`7655` by :user:`Karan Desai <karandesai-96>`.
 
 Enhancements
 ............
 
+   - Update Sphinx-Gallery from 0.1.4 to 0.1.7 for resolving links in
+     documentation build with Sphinx>1.5 :issue:`8010`, :issue:`7986`
+     :user:`Oscar Najera <Titan-C>`
+
    - :class:`multioutput.MultiOutputRegressor` and :class:`multioutput.MultiOutputClassifier`
      now support online learning using `partial_fit`.
      issue: `8053` by :user:`Peng Yu <yupbank>`.
@@ -159,8 +163,8 @@ Bug fixes
    - Fix estimators to accept a ``sample_weight`` parameter of type
      ``pandas.Series`` in their ``fit`` function. :issue:`7825` by
      `Kathleen Chen`_.
-  
-   - Fixed a bug where :class:`sklearn.ensemble.IsolationForest` fails when 
+
+   - Fixed a bug where :class:`sklearn.ensemble.IsolationForest` fails when
      ``max_features`` is less than 1.
      :issue:`5732` by :user:`Ishank Gulati <IshankGulati>`.
 
@@ -190,6 +194,10 @@ Bug fixes
      would be raised on trying to stack matrices with different dimensions.
      :issue:`8093` by :user:`Peter Bull <pjbull>`.
 
+   - Fix a bug where :func:`sklearn.linear_model.LassoLars.fit` sometimes
+     left `coef_` as a list, rather than an ndarray.
+     :issue:`8160` by :user:`CJ Carey <perimosocordiae>`.
+
 API changes summary
 -------------------
 
diff --git a/examples/covariance/plot_sparse_cov.py b/examples/covariance/plot_sparse_cov.py
index 664e974b331c..d9b7f0808fd7 100644
--- a/examples/covariance/plot_sparse_cov.py
+++ b/examples/covariance/plot_sparse_cov.py
@@ -126,7 +126,7 @@
 # plot the model selection metric
 plt.figure(figsize=(4, 3))
 plt.axes([.2, .15, .75, .7])
-plt.plot(model.cv_alphas_, np.mean(model.grid_scores, axis=1), 'o-')
+plt.plot(model.cv_alphas_, np.mean(model.grid_scores_, axis=1), 'o-')
 plt.axvline(model.alpha_, color='.5')
 plt.title('Model selection')
 plt.ylabel('Cross-validation score')
diff --git a/examples/linear_model/plot_ransac.py b/examples/linear_model/plot_ransac.py
index e9a6d910ecf8..0bafe4ee4a39 100644
--- a/examples/linear_model/plot_ransac.py
+++ b/examples/linear_model/plot_ransac.py
@@ -27,32 +27,33 @@
 y[:n_outliers] = -3 + 10 * np.random.normal(size=n_outliers)
 
 # Fit line using all data
-model = linear_model.LinearRegression()
-model.fit(X, y)
+lr = linear_model.LinearRegression()
+lr.fit(X, y)
 
 # Robustly fit linear model with RANSAC algorithm
-model_ransac = linear_model.RANSACRegressor(linear_model.LinearRegression())
-model_ransac.fit(X, y)
-inlier_mask = model_ransac.inlier_mask_
+ransac = linear_model.RANSACRegressor()
+ransac.fit(X, y)
+inlier_mask = ransac.inlier_mask_
 outlier_mask = np.logical_not(inlier_mask)
 
 # Predict data of estimated models
-line_X = np.arange(-5, 5)
-line_y = model.predict(line_X[:, np.newaxis])
-line_y_ransac = model_ransac.predict(line_X[:, np.newaxis])
+line_X = np.arange(X.min(), X.max())[:, np.newaxis]
+line_y = lr.predict(line_X)
+line_y_ransac = ransac.predict(line_X)
 
 # Compare estimated coefficients
-print("Estimated coefficients (true, normal, RANSAC):")
-print(coef, model.coef_, model_ransac.estimator_.coef_)
+print("Estimated coefficients (true, linear regression, RANSAC):")
+print(coef, lr.coef_, ransac.estimator_.coef_)
 
 lw = 2
 plt.scatter(X[inlier_mask], y[inlier_mask], color='yellowgreen', marker='.',
             label='Inliers')
 plt.scatter(X[outlier_mask], y[outlier_mask], color='gold', marker='.',
             label='Outliers')
-plt.plot(line_X, line_y, color='navy', linestyle='-', linewidth=lw,
-         label='Linear regressor')
-plt.plot(line_X, line_y_ransac, color='cornflowerblue', linestyle='-',
-         linewidth=lw, label='RANSAC regressor')
+plt.plot(line_X, line_y, color='navy', linewidth=lw, label='Linear regressor')
+plt.plot(line_X, line_y_ransac, color='cornflowerblue', linewidth=lw,
+         label='RANSAC regressor')
 plt.legend(loc='lower right')
+plt.xlabel("Input")
+plt.ylabel("Response")
 plt.show()
diff --git a/examples/linear_model/plot_ridge_path.py b/examples/linear_model/plot_ridge_path.py
index 52f816d34227..1f2c475f78b7 100644
--- a/examples/linear_model/plot_ridge_path.py
+++ b/examples/linear_model/plot_ridge_path.py
@@ -44,13 +44,12 @@
 
 n_alphas = 200
 alphas = np.logspace(-10, -2, n_alphas)
-clf = linear_model.Ridge(fit_intercept=False)
 
 coefs = []
 for a in alphas:
-    clf.set_params(alpha=a)
-    clf.fit(X, y)
-    coefs.append(clf.coef_)
+    ridge = linear_model.Ridge(alpha=a, fit_intercept=False)
+    ridge.fit(X, y)
+    coefs.append(ridge.coef_)
 
 ###############################################################################
 # Display results
diff --git a/examples/model_selection/plot_roc_crossval.py b/examples/model_selection/plot_roc_crossval.py
index 6678dcb1af8b..366aa0acbee0 100644
--- a/examples/model_selection/plot_roc_crossval.py
+++ b/examples/model_selection/plot_roc_crossval.py
@@ -62,32 +62,39 @@
 classifier = svm.SVC(kernel='linear', probability=True,
                      random_state=random_state)
 
-mean_tpr = 0.0
+tprs = []
+aucs = []
 mean_fpr = np.linspace(0, 1, 100)
 
-colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange'])
-lw = 2
-
 i = 0
-for (train, test), color in zip(cv.split(X, y), colors):
+for train, test in cv.split(X, y):
     probas_ = classifier.fit(X[train], y[train]).predict_proba(X[test])
     # Compute ROC curve and area the curve
     fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])
-    mean_tpr += interp(mean_fpr, fpr, tpr)
-    mean_tpr[0] = 0.0
+    tprs.append(interp(mean_fpr, fpr, tpr))
+    tprs[-1][0] = 0.0
     roc_auc = auc(fpr, tpr)
-    plt.plot(fpr, tpr, lw=lw, color=color,
-             label='ROC fold %d (area = %0.2f)' % (i, roc_auc))
+    aucs.append(roc_auc)
+    plt.plot(fpr, tpr, lw=1, alpha=0.3,
+             label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))
 
     i += 1
-plt.plot([0, 1], [0, 1], linestyle='--', lw=lw, color='k',
-         label='Luck')
+plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',
+         label='Luck', alpha=.8)
 
-mean_tpr /= cv.get_n_splits(X, y)
+mean_tpr = np.mean(tprs, axis=0)
 mean_tpr[-1] = 1.0
 mean_auc = auc(mean_fpr, mean_tpr)
-plt.plot(mean_fpr, mean_tpr, color='g', linestyle='--',
-         label='Mean ROC (area = %0.2f)' % mean_auc, lw=lw)
+std_auc = np.std(aucs)
+plt.plot(mean_fpr, mean_tpr, color='b',
+         label=r'Mean ROC (AUC = %0.2f $\pm$ %0.2f)' % (mean_auc, std_auc),
+         lw=2, alpha=.8)
+
+std_tpr = np.std(tprs, axis=0)
+tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
+tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
+plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,
+                 label=r'$\pm$ 1 std. dev.')
 
 plt.xlim([-0.05, 1.05])
 plt.ylim([-0.05, 1.05])
diff --git a/examples/semi_supervised/plot_label_propagation_digits_active_learning.py b/examples/semi_supervised/plot_label_propagation_digits_active_learning.py
index 5fc5a7141363..5c8543937beb 100644
--- a/examples/semi_supervised/plot_label_propagation_digits_active_learning.py
+++ b/examples/semi_supervised/plot_label_propagation_digits_active_learning.py
@@ -9,7 +9,10 @@
 We start by training a label propagation model with only 10 labeled points,
 then we select the top five most uncertain points to label. Next, we train
 with 15 labeled points (original 10 + 5 new ones). We repeat this process
-four times to have a model trained with 30 labeled examples.
+four times to have a model trained with 30 labeled examples. Note you can
+increase this to label more than 30 by changing `max_iterations`. Labeling
+more than 30 can be useful to get a sense for the speed of convergence of
+this active learning technique.
 
 A plot will appear showing the top 5 most uncertain digits for each iteration
 of training. These may or may not contain mistakes, but we will train the next
@@ -39,11 +42,15 @@
 
 n_total_samples = len(y)
 n_labeled_points = 10
+max_iterations = 5
 
 unlabeled_indices = np.arange(n_total_samples)[n_labeled_points:]
 f = plt.figure()
 
-for i in range(5):
+for i in range(max_iterations):
+    if len(unlabeled_indices) == 0:
+        print("No unlabeled items left to label.")
+        break
     y_train = np.copy(y)
     y_train[unlabeled_indices] = -1
 
@@ -56,7 +63,7 @@
     cm = confusion_matrix(true_labels, predicted_labels,
                           labels=lp_model.classes_)
 
-    print('Iteration %i %s' % (i, 70 * '_'))
+    print("Iteration %i %s" % (i, 70 * "_"))
     print("Label Spreading model: %d labeled & %d unlabeled (%d total)"
           % (n_labeled_points, n_total_samples - n_labeled_points, n_total_samples))
 
@@ -69,29 +76,36 @@
     pred_entropies = stats.distributions.entropy(
         lp_model.label_distributions_.T)
 
-    # select five digit examples that the classifier is most uncertain about
-    uncertainty_index = uncertainty_index = np.argsort(pred_entropies)[-5:]
+    # select up to 5 digit examples that the classifier is most uncertain about
+    uncertainty_index = np.argsort(pred_entropies)[::-1]
+    uncertainty_index = uncertainty_index[
+        np.in1d(uncertainty_index, unlabeled_indices)][:5]
 
     # keep track of indices that we get labels for
     delete_indices = np.array([])
 
-    f.text(.05, (1 - (i + 1) * .183),
-           "model %d\n\nfit with\n%d labels" % ((i + 1), i * 5 + 10), size=10)
+    # for more than 5 iterations, visualize the gain only on the first 5
+    if i < 5:
+        f.text(.05, (1 - (i + 1) * .183),
+               "model %d\n\nfit with\n%d labels" %
+               ((i + 1), i * 5 + 10), size=10)
     for index, image_index in enumerate(uncertainty_index):
         image = images[image_index]
 
-        sub = f.add_subplot(5, 5, index + 1 + (5 * i))
-        sub.imshow(image, cmap=plt.cm.gray_r)
-        sub.set_title('predict: %i\ntrue: %i' % (
-            lp_model.transduction_[image_index], y[image_index]), size=10)
-        sub.axis('off')
+        # for more than 5 iterations, visualize the gain only on the first 5
+        if i < 5:
+            sub = f.add_subplot(5, 5, index + 1 + (5 * i))
+            sub.imshow(image, cmap=plt.cm.gray_r)
+            sub.set_title("predict: %i\ntrue: %i" % (
+                lp_model.transduction_[image_index], y[image_index]), size=10)
+            sub.axis('off')
 
         # labeling 5 points, remote from labeled set
         delete_index, = np.where(unlabeled_indices == image_index)
         delete_indices = np.concatenate((delete_indices, delete_index))
 
     unlabeled_indices = np.delete(unlabeled_indices, delete_indices)
-    n_labeled_points += 5
+    n_labeled_points += len(uncertainty_index)
 
 f.suptitle("Active learning with Label Propagation.\nRows show 5 most "
            "uncertain labels to learn with the next model.")
diff --git a/setup.py b/setup.py
index fb427498cf8a..ff7527ef04be 100755
--- a/setup.py
+++ b/setup.py
@@ -203,6 +203,7 @@ def setup_package():
                                  'Programming Language :: Python :: 3',
                                  'Programming Language :: Python :: 3.4',
                                  'Programming Language :: Python :: 3.5',
+                                 'Programming Language :: Python :: 3.6',
                                  ],
                     cmdclass=cmdclass,
                     **extra_setuptools_args)
diff --git a/sklearn/calibration.py b/sklearn/calibration.py
index 37928817fd5e..35cdab45a1b5 100644
--- a/sklearn/calibration.py
+++ b/sklearn/calibration.py
@@ -19,7 +19,7 @@
 from .base import BaseEstimator, ClassifierMixin, RegressorMixin, clone
 from .preprocessing import label_binarize, LabelBinarizer
 from .utils import check_X_y, check_array, indexable, column_or_1d
-from .utils.validation import check_is_fitted
+from .utils.validation import check_is_fitted, check_consistent_length
 from .utils.fixes import signature
 from .isotonic import IsotonicRegression
 from .svm import LinearSVC
@@ -167,6 +167,9 @@ def fit(self, X, y, sample_weight=None):
                               " itself." % estimator_name)
                 base_estimator_sample_weight = None
             else:
+                if sample_weight is not None:
+                    sample_weight = check_array(sample_weight, ensure_2d=False)
+                    check_consistent_length(y, sample_weight)
                 base_estimator_sample_weight = sample_weight
             for train, test in cv.split(X, y):
                 this_estimator = clone(base_estimator)
diff --git a/sklearn/covariance/graph_lasso_.py b/sklearn/covariance/graph_lasso_.py
index bc9b935c69dc..ae2f108eed45 100644
--- a/sklearn/covariance/graph_lasso_.py
+++ b/sklearn/covariance/graph_lasso_.py
@@ -19,6 +19,7 @@
 from ..exceptions import ConvergenceWarning
 from ..utils.extmath import pinvh
 from ..utils.validation import check_random_state, check_array
+from ..utils import deprecated
 from ..linear_model import lars_path
 from ..linear_model import cd_fast
 from ..model_selection import check_cv, cross_val_score
@@ -525,7 +526,7 @@ class GraphLassoCV(GraphLasso):
     cv_alphas_ : list of float
         All penalization parameters explored.
 
-    `grid_scores`: 2D numpy.ndarray (n_alphas, n_folds)
+    grid_scores_ : 2D numpy.ndarray (n_alphas, n_folds)
         Log-likelihood score on left-out data across folds.
 
     n_iter_ : int
@@ -564,6 +565,12 @@ def __init__(self, alphas=4, n_refinements=4, cv=None, tol=1e-4,
         # The base class needs this for the score method
         self.store_precision = True
 
+    @property
+    @deprecated("Attribute grid_scores was deprecated in version 0.19 and "
+                "will be removed in 0.21. Use 'grid_scores_' instead")
+    def grid_scores(self):
+        return self.grid_scores_
+
     def fit(self, X, y=None):
         """Fits the GraphLasso covariance model to X.
 
@@ -680,7 +687,7 @@ def fit(self, X, y=None):
         grid_scores.append(cross_val_score(EmpiricalCovariance(), X,
                                            cv=cv, n_jobs=self.n_jobs,
                                            verbose=inner_verbose))
-        self.grid_scores = np.array(grid_scores)
+        self.grid_scores_ = np.array(grid_scores)
         best_alpha = alphas[best_index]
         self.alpha_ = best_alpha
         self.cv_alphas_ = alphas
diff --git a/sklearn/covariance/tests/test_graph_lasso.py b/sklearn/covariance/tests/test_graph_lasso.py
index bc2c8339da21..c46e060c43c4 100644
--- a/sklearn/covariance/tests/test_graph_lasso.py
+++ b/sklearn/covariance/tests/test_graph_lasso.py
@@ -7,6 +7,7 @@
 
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_array_less
+from sklearn.utils.testing import assert_warns_message
 
 from sklearn.covariance import (graph_lasso, GraphLasso, GraphLassoCV,
                                 empirical_covariance)
@@ -15,6 +16,8 @@
 from sklearn.utils import check_random_state
 from sklearn import datasets
 
+from numpy.testing import assert_equal
+
 
 def test_graph_lasso(random_state=0):
     # Sample data from a sparse multivariate normal
@@ -131,3 +134,23 @@ def test_graph_lasso_cv(random_state=1):
 
     # Smoke test with specified alphas
     GraphLassoCV(alphas=[0.8, 0.5], tol=1e-1, n_jobs=1).fit(X)
+
+
+def test_deprecated_grid_scores(random_state=1):
+    dim = 5
+    n_samples = 6
+    random_state = check_random_state(random_state)
+    prec = make_sparse_spd_matrix(dim, alpha=.96,
+                                  random_state=random_state)
+    cov = linalg.inv(prec)
+    X = random_state.multivariate_normal(np.zeros(dim), cov, size=n_samples)
+    graph_lasso = GraphLassoCV(alphas=[0.8, 0.5], tol=1e-1, n_jobs=1)
+    graph_lasso.fit(X)
+
+    depr_message = ("Attribute grid_scores was deprecated in version "
+                    "0.19 and will be removed in 0.21. Use "
+                    "'grid_scores_' instead")
+
+    assert_warns_message(DeprecationWarning, depr_message,
+                         lambda: graph_lasso.grid_scores)
+    assert_equal(graph_lasso.grid_scores, graph_lasso.grid_scores_)
diff --git a/sklearn/datasets/lfw.py b/sklearn/datasets/lfw.py
index d155cfe47859..13aaed805b4f 100644
--- a/sklearn/datasets/lfw.py
+++ b/sklearn/datasets/lfw.py
@@ -196,7 +196,7 @@ def _fetch_lfw_people(data_folder_path, slice_=None, color=False, resize=None,
         folder_path = join(data_folder_path, person_name)
         if not isdir(folder_path):
             continue
-        paths = [join(folder_path, f) for f in listdir(folder_path)]
+        paths = [join(folder_path, f) for f in sorted(listdir(folder_path))]
         n_pictures = len(paths)
         if n_pictures >= min_faces_per_person:
             person_name = person_name.replace('_', ' ')
diff --git a/sklearn/ensemble/bagging.py b/sklearn/ensemble/bagging.py
index bdc863794490..711d089d057d 100644
--- a/sklearn/ensemble/bagging.py
+++ b/sklearn/ensemble/bagging.py
@@ -20,7 +20,7 @@
 from ..utils import check_random_state, check_X_y, check_array, column_or_1d
 from ..utils.random import sample_without_replacement
 from ..utils.validation import has_fit_parameter, check_is_fitted
-from ..utils import indices_to_mask
+from ..utils import indices_to_mask, check_consistent_length
 from ..utils.fixes import bincount
 from ..utils.metaestimators import if_delegate_has_method
 from ..utils.multiclass import check_classification_targets
@@ -82,8 +82,8 @@ def _parallel_build_estimators(n_estimators, ensemble, X, y, sample_weight,
 
     for i in range(n_estimators):
         if verbose > 1:
-            print("Building estimator %d of %d for this parallel run (total %d)..." %
-                  (i + 1, n_estimators, total_n_estimators))
+            print("Building estimator %d of %d for this parallel run "
+                  "(total %d)..." % (i + 1, n_estimators, total_n_estimators))
 
         random_state = np.random.RandomState(seeds[i])
         estimator = ensemble._make_estimator(append=False,
@@ -282,6 +282,9 @@ def _fit(self, X, y, max_samples=None, max_depth=None, sample_weight=None):
 
         # Convert data
         X, y = check_X_y(X, y, ['csr', 'csc'])
+        if sample_weight is not None:
+            sample_weight = check_array(sample_weight, ensure_2d=False)
+            check_consistent_length(y, sample_weight)
 
         # Remap output
         n_samples, self.n_features_ = X.shape
diff --git a/sklearn/ensemble/gradient_boosting.py b/sklearn/ensemble/gradient_boosting.py
index 37ba1ccf9293..49a3fd1a9e34 100644
--- a/sklearn/ensemble/gradient_boosting.py
+++ b/sklearn/ensemble/gradient_boosting.py
@@ -57,6 +57,7 @@
 from ..utils.extmath import logsumexp
 from ..utils.fixes import expit
 from ..utils.fixes import bincount
+from ..utils import deprecated
 from ..utils.stats import _weighted_percentile
 from ..utils.validation import check_is_fitted
 from ..utils.multiclass import check_classification_targets
@@ -846,25 +847,26 @@ def _check_params(self):
             if self.max_features == "auto":
                 # if is_classification
                 if self.n_classes_ > 1:
-                    max_features = max(1, int(np.sqrt(self.n_features)))
+                    max_features = max(1, int(np.sqrt(self.n_features_)))
                 else:
                     # is regression
-                    max_features = self.n_features
+                    max_features = self.n_features_
             elif self.max_features == "sqrt":
-                max_features = max(1, int(np.sqrt(self.n_features)))
+                max_features = max(1, int(np.sqrt(self.n_features_)))
             elif self.max_features == "log2":
-                max_features = max(1, int(np.log2(self.n_features)))
+                max_features = max(1, int(np.log2(self.n_features_)))
             else:
                 raise ValueError("Invalid value for max_features: %r. "
                                  "Allowed string values are 'auto', 'sqrt' "
                                  "or 'log2'." % self.max_features)
         elif self.max_features is None:
-            max_features = self.n_features
+            max_features = self.n_features_
         elif isinstance(self.max_features, (numbers.Integral, np.integer)):
             max_features = self.max_features
         else:  # float
             if 0. < self.max_features <= 1.:
-                max_features = max(int(self.max_features * self.n_features), 1)
+                max_features = max(int(self.max_features *
+                                       self.n_features_), 1)
             else:
                 raise ValueError("max_features must be in (0, n_features]")
 
@@ -924,6 +926,12 @@ def _check_initialized(self):
         """Check that the estimator is initialized, raising an error if not."""
         check_is_fitted(self, 'estimators_')
 
+    @property
+    @deprecated("Attribute n_features was deprecated in version 0.19 and "
+                "will be removed in 0.21.")
+    def n_features(self):
+        return self.n_features_
+
     def fit(self, X, y, sample_weight=None, monitor=None):
         """Fit the gradient boosting model.
 
@@ -965,7 +973,7 @@ def fit(self, X, y, sample_weight=None, monitor=None):
 
         # Check input
         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)
-        n_samples, self.n_features = X.shape
+        n_samples, self.n_features_ = X.shape
         if sample_weight is None:
             sample_weight = np.ones(n_samples, dtype=np.float32)
         else:
@@ -1106,9 +1114,9 @@ def _init_decision_function(self, X):
         """Check input and compute prediction of ``init``. """
         self._check_initialized()
         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
-        if X.shape[1] != self.n_features:
+        if X.shape[1] != self.n_features_:
             raise ValueError("X.shape[1] should be {0:d}, not {1:d}.".format(
-                self.n_features, X.shape[1]))
+                self.n_features_, X.shape[1]))
         score = self.init_.predict(X).astype(np.float64)
         return score
 
@@ -1158,7 +1166,7 @@ def feature_importances_(self):
         """
         self._check_initialized()
 
-        total_sum = np.zeros((self.n_features, ), dtype=np.float64)
+        total_sum = np.zeros((self.n_features_, ), dtype=np.float64)
         for stage in self.estimators_:
             stage_sum = sum(tree.feature_importances_
                             for tree in stage) / len(stage)
diff --git a/sklearn/ensemble/partial_dependence.py b/sklearn/ensemble/partial_dependence.py
index 44818435877e..d4ed3233f44e 100644
--- a/sklearn/ensemble/partial_dependence.py
+++ b/sklearn/ensemble/partial_dependence.py
@@ -129,9 +129,9 @@ def partial_dependence(gbrt, target_variables, grid=None, X=None,
     target_variables = np.asarray(target_variables, dtype=np.int32,
                                   order='C').ravel()
 
-    if any([not (0 <= fx < gbrt.n_features) for fx in target_variables]):
+    if any([not (0 <= fx < gbrt.n_features_) for fx in target_variables]):
         raise ValueError('target_variables must be in [0, %d]'
-                         % (gbrt.n_features - 1))
+                         % (gbrt.n_features_ - 1))
 
     if X is not None:
         X = check_array(X, dtype=DTYPE, order='C')
@@ -258,8 +258,8 @@ def plot_partial_dependence(gbrt, X, features, feature_names=None,
         label_idx = 0
 
     X = check_array(X, dtype=DTYPE, order='C')
-    if gbrt.n_features != X.shape[1]:
-        raise ValueError('X.shape[1] does not match gbrt.n_features')
+    if gbrt.n_features_ != X.shape[1]:
+        raise ValueError('X.shape[1] does not match gbrt.n_features_')
 
     if line_kw is None:
         line_kw = {'color': 'green'}
@@ -269,7 +269,7 @@ def plot_partial_dependence(gbrt, X, features, feature_names=None,
     # convert feature_names to list
     if feature_names is None:
         # if not feature_names use fx indices as name
-        feature_names = [str(i) for i in range(gbrt.n_features)]
+        feature_names = [str(i) for i in range(gbrt.n_features_)]
     elif isinstance(feature_names, np.ndarray):
         feature_names = feature_names.tolist()
 
diff --git a/sklearn/feature_selection/mutual_info_.py b/sklearn/feature_selection/mutual_info_.py
index 463cc12e6d20..b72e884704c5 100644
--- a/sklearn/feature_selection/mutual_info_.py
+++ b/sklearn/feature_selection/mutual_info_.py
@@ -281,7 +281,7 @@ def _estimate_mi(X, y, discrete_features='auto', discrete_target=False,
         y = scale(y, with_mean=False)
         y += 1e-10 * np.maximum(1, np.mean(np.abs(y))) * rng.randn(n_samples)
 
-    mi = [_compute_mi(x, y, discrete_feature, discrete_target) for
+    mi = [_compute_mi(x, y, discrete_feature, discrete_target, n_neighbors) for
           x, discrete_feature in moves.zip(_iterate_columns(X), discrete_mask)]
 
     return np.array(mi)
diff --git a/sklearn/feature_selection/tests/test_mutual_info.py b/sklearn/feature_selection/tests/test_mutual_info.py
index f9b86777dcbe..c4486c937f17 100644
--- a/sklearn/feature_selection/tests/test_mutual_info.py
+++ b/sklearn/feature_selection/tests/test_mutual_info.py
@@ -5,7 +5,8 @@
 from scipy.sparse import csr_matrix
 
 from sklearn.utils.testing import (assert_array_equal, assert_almost_equal,
-                                   assert_false, assert_raises, assert_equal)
+                                   assert_false, assert_raises, assert_equal,
+                                   assert_allclose, assert_greater)
 from sklearn.feature_selection.mutual_info_ import (
     mutual_info_regression, mutual_info_classif, _compute_mi)
 
@@ -158,8 +159,19 @@ def test_mutual_info_classif_mixed():
     y = ((0.5 * X[:, 0] + X[:, 2]) > 0.5).astype(int)
     X[:, 2] = X[:, 2] > 0.5
 
-    mi = mutual_info_classif(X, y, discrete_features=[2], random_state=0)
+    mi = mutual_info_classif(X, y, discrete_features=[2], n_neighbors=3,
+                             random_state=0)
     assert_array_equal(np.argsort(-mi), [2, 0, 1])
+    for n_neighbors in [5, 7, 9]:
+        mi_nn = mutual_info_classif(X, y, discrete_features=[2],
+                                    n_neighbors=n_neighbors, random_state=0)
+        # Check that the continuous values have an higher MI with greater
+        # n_neighbors
+        assert_greater(mi_nn[0], mi[0])
+        assert_greater(mi_nn[1], mi[1])
+        # The n_neighbors should not have any effect on the discrete value
+        # The MI should be the same
+        assert_equal(mi_nn[2], mi[2])
 
 
 def test_mutual_info_options():
diff --git a/sklearn/gaussian_process/gpr.py b/sklearn/gaussian_process/gpr.py
index ac1b1f6d6254..a0f8ff249f0c 100644
--- a/sklearn/gaussian_process/gpr.py
+++ b/sklearn/gaussian_process/gpr.py
@@ -15,6 +15,7 @@
 from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
 from sklearn.utils import check_random_state
 from sklearn.utils.validation import check_X_y, check_array
+from sklearn.utils.deprecation import deprecated
 
 
 class GaussianProcessRegressor(BaseEstimator, RegressorMixin):
@@ -140,8 +141,20 @@ def __init__(self, kernel=None, alpha=1e-10,
         self.copy_X_train = copy_X_train
         self.random_state = random_state
 
+    @property
+    @deprecated("Attribute rng was deprecated in version 0.19 and "
+                "will be removed in 0.21.")
+    def rng(self):
+        return self._rng
+
+    @property
+    @deprecated("Attribute y_train_mean was deprecated in version 0.19 and "
+                "will be removed in 0.21.")
+    def y_train_mean(self):
+        return self._y_train_mean
+
     def fit(self, X, y):
-        """Fit Gaussian process regression model
+        """Fit Gaussian process regression model.
 
         Parameters
         ----------
@@ -161,17 +174,17 @@ def fit(self, X, y):
         else:
             self.kernel_ = clone(self.kernel)
 
-        self.rng = check_random_state(self.random_state)
+        self._rng = check_random_state(self.random_state)
 
         X, y = check_X_y(X, y, multi_output=True, y_numeric=True)
 
         # Normalize target value
         if self.normalize_y:
-            self.y_train_mean = np.mean(y, axis=0)
+            self._y_train_mean = np.mean(y, axis=0)
             # demean y
-            y = y - self.y_train_mean
+            y = y - self._y_train_mean
         else:
-            self.y_train_mean = np.zeros(1)
+            self._y_train_mean = np.zeros(1)
 
         if np.iterable(self.alpha) \
            and self.alpha.shape[0] != y.shape[0]:
@@ -211,7 +224,7 @@ def obj_func(theta, eval_gradient=True):
                 bounds = self.kernel_.bounds
                 for iteration in range(self.n_restarts_optimizer):
                     theta_initial = \
-                        self.rng.uniform(bounds[:, 0], bounds[:, 1])
+                        self._rng.uniform(bounds[:, 0], bounds[:, 1])
                     optima.append(
                         self._constrained_optimization(obj_func, theta_initial,
                                                        bounds))
@@ -287,7 +300,7 @@ def predict(self, X, return_std=False, return_cov=False):
         else:  # Predict based on GP posterior
             K_trans = self.kernel_(X, self.X_train_)
             y_mean = K_trans.dot(self.alpha_)  # Line 4 (y_mean = f_star)
-            y_mean = self.y_train_mean + y_mean  # undo normal.
+            y_mean = self._y_train_mean + y_mean  # undo normal.
             if return_cov:
                 v = cho_solve((self.L_, True), K_trans.T)  # Line 5
                 y_cov = self.kernel_(X) - K_trans.dot(v)  # Line 6
diff --git a/sklearn/gaussian_process/tests/test_kernels.py b/sklearn/gaussian_process/tests/test_kernels.py
index e670e5330889..003c5727da09 100644
--- a/sklearn/gaussian_process/tests/test_kernels.py
+++ b/sklearn/gaussian_process/tests/test_kernels.py
@@ -80,7 +80,7 @@ def test_kernel_theta():
         # Determine kernel parameters that contribute to theta
         init_sign = signature(kernel.__class__.__init__).parameters.values()
         args = [p.name for p in init_sign if p.name != 'self']
-        theta_vars = map(lambda s: s.rstrip("_bounds"),
+        theta_vars = map(lambda s: s[0:-len("_bounds")],
                          filter(lambda s: s.endswith("_bounds"), args))
         assert_equal(
             set(hyperparameter.name
diff --git a/sklearn/linear_model/least_angle.py b/sklearn/linear_model/least_angle.py
index 1b040df898b8..c06720a25299 100644
--- a/sklearn/linear_model/least_angle.py
+++ b/sklearn/linear_model/least_angle.py
@@ -587,6 +587,8 @@ class Lars(LinearModel, RegressorMixin):
     sklearn.decomposition.sparse_encode
 
     """
+    method = 'lar'
+
     def __init__(self, fit_intercept=True, verbose=False, normalize=True,
                  precompute='auto', n_nonzero_coefs=500,
                  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
@@ -594,7 +596,6 @@ def __init__(self, fit_intercept=True, verbose=False, normalize=True,
         self.fit_intercept = fit_intercept
         self.verbose = verbose
         self.normalize = normalize
-        self.method = 'lar'
         self.precompute = precompute
         self.n_nonzero_coefs = n_nonzero_coefs
         self.positive = positive
@@ -665,9 +666,9 @@ def fit(self, X, y, Xy=None):
 
         self.alphas_ = []
         self.n_iter_ = []
+        self.coef_ = np.empty((n_targets, n_features))
 
         if self.fit_path:
-            self.coef_ = []
             self.active_ = []
             self.coef_path_ = []
             for k in xrange(n_targets):
@@ -682,7 +683,7 @@ def fit(self, X, y, Xy=None):
                 self.active_.append(active)
                 self.n_iter_.append(n_iter_)
                 self.coef_path_.append(coef_path)
-                self.coef_.append(coef_path[:, -1])
+                self.coef_[k] = coef_path[:, -1]
 
             if n_targets == 1:
                 self.alphas_, self.active_, self.coef_path_, self.coef_ = [
@@ -690,7 +691,6 @@ def fit(self, X, y, Xy=None):
                                    self.coef_)]
                 self.n_iter_ = self.n_iter_[0]
         else:
-            self.coef_ = np.empty((n_targets, n_features))
             for k in xrange(n_targets):
                 this_Xy = None if Xy is None else Xy[:, k]
                 alphas, _, self.coef_[k], n_iter_ = lars_path(
@@ -828,6 +828,7 @@ class LassoLars(Lars):
     sklearn.decomposition.sparse_encode
 
     """
+    method = 'lasso'
 
     def __init__(self, alpha=1.0, fit_intercept=True, verbose=False,
                  normalize=True, precompute='auto', max_iter=500,
@@ -838,7 +839,6 @@ def __init__(self, alpha=1.0, fit_intercept=True, verbose=False,
         self.max_iter = max_iter
         self.verbose = verbose
         self.normalize = normalize
-        self.method = 'lasso'
         self.positive = positive
         self.precompute = precompute
         self.copy_X = copy_X
@@ -1076,17 +1076,16 @@ def __init__(self, fit_intercept=True, verbose=False, max_iter=500,
                  normalize=True, precompute='auto', cv=None,
                  max_n_alphas=1000, n_jobs=1, eps=np.finfo(np.float).eps,
                  copy_X=True, positive=False):
-        self.fit_intercept = fit_intercept
-        self.positive = positive
         self.max_iter = max_iter
-        self.verbose = verbose
-        self.normalize = normalize
-        self.precompute = precompute
-        self.copy_X = copy_X
         self.cv = cv
         self.max_n_alphas = max_n_alphas
         self.n_jobs = n_jobs
-        self.eps = eps
+        super(LarsCV, self).__init__(fit_intercept=fit_intercept,
+                                     verbose=verbose, normalize=normalize,
+                                     precompute=precompute,
+                                     n_nonzero_coefs=500,
+                                     eps=eps, copy_X=copy_X, fit_path=True,
+                                     positive=positive)
 
     def fit(self, X, y):
         """Fit the model using X, y as training data.
@@ -1104,7 +1103,6 @@ def fit(self, X, y):
         self : object
             returns an instance of self.
         """
-        self.fit_path = True
         X, y = check_X_y(X, y, y_numeric=True)
         X = as_float_array(X, copy=self.copy_X)
         y = as_float_array(y, copy=self.copy_X)
@@ -1429,6 +1427,7 @@ def __init__(self, criterion='aic', fit_intercept=True, verbose=False,
         self.copy_X = copy_X
         self.precompute = precompute
         self.eps = eps
+        self.fit_path = True
 
     def fit(self, X, y, copy_X=True):
         """Fit the model using X, y as training data.
@@ -1449,7 +1448,6 @@ def fit(self, X, y, copy_X=True):
         self : object
             returns an instance of self.
         """
-        self.fit_path = True
         X, y = check_X_y(X, y, y_numeric=True)
 
         X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(
diff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py
index 565acba536c5..ca20c9dbc64f 100644
--- a/sklearn/linear_model/logistic.py
+++ b/sklearn/linear_model/logistic.py
@@ -888,6 +888,9 @@ def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,
     y_test = y[test]
 
     if sample_weight is not None:
+        sample_weight = check_array(sample_weight, ensure_2d=False)
+        check_consistent_length(y, sample_weight)
+
         sample_weight = sample_weight[train]
 
     coefs, Cs, n_iter = logistic_regression_path(
diff --git a/sklearn/linear_model/stochastic_gradient.py b/sklearn/linear_model/stochastic_gradient.py
index 112f7406abae..c234b8eb94f0 100644
--- a/sklearn/linear_model/stochastic_gradient.py
+++ b/sklearn/linear_model/stochastic_gradient.py
@@ -22,6 +22,7 @@
 from .sgd_fast import plain_sgd, average_sgd
 from ..utils.fixes import astype
 from ..utils import compute_class_weight
+from ..utils import deprecated
 from .sgd_fast import Hinge
 from .sgd_fast import SquaredHinge
 from .sgd_fast import Log
@@ -258,7 +259,7 @@ def fit_binary(est, i, X, y, alpha, C, learning_rate, n_iter,
     seed = random_state.randint(0, np.iinfo(np.int32).max)
 
     if not est.average:
-        return plain_sgd(coef, intercept, est.loss_function,
+        return plain_sgd(coef, intercept, est.loss_function_,
                          penalty_type, alpha, C, est.l1_ratio,
                          dataset, n_iter, int(est.fit_intercept),
                          int(est.verbose), int(est.shuffle), seed,
@@ -270,7 +271,7 @@ def fit_binary(est, i, X, y, alpha, C, learning_rate, n_iter,
         standard_coef, standard_intercept, average_coef, \
             average_intercept = average_sgd(coef, intercept, average_coef,
                                             average_intercept,
-                                            est.loss_function, penalty_type,
+                                            est.loss_function_, penalty_type,
                                             alpha, C, est.l1_ratio, dataset,
                                             n_iter, int(est.fit_intercept),
                                             int(est.verbose), int(est.shuffle),
@@ -325,6 +326,12 @@ def __init__(self, loss="hinge", penalty='l2', alpha=0.0001, l1_ratio=0.15,
         self.class_weight = class_weight
         self.n_jobs = int(n_jobs)
 
+    @property
+    @deprecated("Attribute loss_function was deprecated in version 0.19 and "
+                "will be removed in 0.21. Use 'loss_function_' instead")
+    def loss_function(self):
+        return self.loss_function_
+
     def _partial_fit(self, X, y, alpha, C,
                      loss, learning_rate, n_iter,
                      classes, sample_weight,
@@ -350,7 +357,7 @@ def _partial_fit(self, X, y, alpha, C,
             raise ValueError("Number of features %d does not match previous "
                              "data %d." % (n_features, self.coef_.shape[-1]))
 
-        self.loss_function = self._get_loss_function(loss)
+        self.loss_function_ = self._get_loss_function(loss)
         if not hasattr(self, "t_"):
             self.t_ = 1.0
 
@@ -668,6 +675,8 @@ class SGDClassifier(BaseSGDClassifier):
     intercept_ : array, shape (1,) if n_classes == 2 else (n_classes,)
         Constants in decision function.
 
+    loss_function_ : concrete ``LossFunction``
+
     Examples
     --------
     >>> import numpy as np
diff --git a/sklearn/linear_model/tests/test_least_angle.py b/sklearn/linear_model/tests/test_least_angle.py
index fbd559695e3b..9651b1239d08 100644
--- a/sklearn/linear_model/tests/test_least_angle.py
+++ b/sklearn/linear_model/tests/test_least_angle.py
@@ -366,8 +366,15 @@ def test_multitarget():
     X = diabetes.data
     Y = np.vstack([diabetes.target, diabetes.target ** 2]).T
     n_targets = Y.shape[1]
-
-    for estimator in (linear_model.LassoLars(), linear_model.Lars()):
+    estimators = [
+        linear_model.LassoLars(),
+        linear_model.Lars(),
+        # regression test for gh-1615
+        linear_model.LassoLars(fit_intercept=False),
+        linear_model.Lars(fit_intercept=False),
+    ]
+
+    for estimator in estimators:
         estimator.fit(X, Y)
         Y_pred = estimator.predict(X)
         alphas, active, coef, path = (estimator.alphas_, estimator.active_,
diff --git a/sklearn/manifold/spectral_embedding_.py b/sklearn/manifold/spectral_embedding_.py
index c2fc878693c9..39a7355cc8f5 100644
--- a/sklearn/manifold/spectral_embedding_.py
+++ b/sklearn/manifold/spectral_embedding_.py
@@ -149,6 +149,8 @@ def spectral_embedding(adjacency, n_components=8, eigen_solver=None,
     However care must taken to always make the affinity matrix symmetric
     so that the eigenvector decomposition works as expected.
 
+    Note : Laplacian Eigenmaps is the actual algorithm implemented here.
+
     Read more in the :ref:`User Guide <spectral_embedding>`.
 
     Parameters
@@ -189,9 +191,9 @@ def spectral_embedding(adjacency, n_components=8, eigen_solver=None,
 
     Notes
     -----
-    Spectral embedding is most useful when the graph has one connected
-    component. If there graph has many components, the first few eigenvectors
-    will simply uncover the connected components of the graph.
+    Spectral Embedding (Laplacian Eigenmaps) is most useful when the graph
+    has one connected component. If there graph has many components, the first
+    few eigenvectors will simply uncover the connected components of the graph.
 
     References
     ----------
@@ -329,6 +331,8 @@ class SpectralEmbedding(BaseEstimator):
     The resulting transformation is given by the value of the
     eigenvectors for each data point.
 
+    Note : Laplacian Eigenmaps is the actual algorithm implemented here.
+
     Read more in the :ref:`User Guide <spectral_embedding>`.
 
     Parameters
diff --git a/sklearn/manifold/t_sne.py b/sklearn/manifold/t_sne.py
index 5797ee7a6725..1ce77f163896 100644
--- a/sklearn/manifold/t_sne.py
+++ b/sklearn/manifold/t_sne.py
@@ -24,6 +24,7 @@
 from . import _barnes_hut_tsne
 from ..utils.fixes import astype
 from ..externals.six import string_types
+from ..utils import deprecated
 
 
 MACHINE_EPSILON = np.finfo(np.double).eps
@@ -616,6 +617,9 @@ class TSNE(BaseEstimator):
     kl_divergence_ : float
         Kullback-Leibler divergence after optimization.
 
+    n_iter_ : int
+        Number of iterations run.
+
     Examples
     --------
 
@@ -787,6 +791,12 @@ def _fit(self, X, skip_num_points=0):
                           neighbors=neighbors_nn,
                           skip_num_points=skip_num_points)
 
+    @property
+    @deprecated("Attribute n_iter_final was deprecated in version 0.19 and "
+                "will be removed in 0.21. Use 'n_iter_' instead")
+    def n_iter_final(self):
+        return self.n_iter_
+
     def _tsne(self, P, degrees_of_freedom, n_samples, random_state,
               X_embedded=None, neighbors=None, skip_num_points=0):
         """Runs t-SNE."""
@@ -848,7 +858,7 @@ def _tsne(self, P, degrees_of_freedom, n_samples, random_state,
             print("[t-SNE] KL divergence after %d iterations with early "
                   "exaggeration: %f" % (it + 1, kl_divergence))
         # Save the final number of iterations
-        self.n_iter_final = it
+        self.n_iter_ = it
 
         # Final optimization
         P /= self.early_exaggeration
diff --git a/sklearn/metrics/classification.py b/sklearn/metrics/classification.py
index 7c74f0f2bfca..e94e70a7c108 100644
--- a/sklearn/metrics/classification.py
+++ b/sklearn/metrics/classification.py
@@ -243,9 +243,9 @@ def top_n_accuracy_score(y_true, y_pred, n=5, normalize=True):
     num_obs, num_labels = y_pred.shape
     idx = num_labels - n - 1
     counter = 0
-    parted = np.argpartition(y_pred, kth=idx, axis=1)
+    argsorted = np.argsort(y_pred, axis=1)
     for i in range(num_obs):
-        if y_true[i] in parted[i, idx+1:]:
+        if y_true[i] in argsorted[i, idx+1:]:
             counter += 1
     if normalize:
         return counter / num_obs
@@ -1470,6 +1470,12 @@ class 2       1.00      0.67      0.80         3
     else:
         labels = np.asarray(labels)
 
+    if target_names is not None and len(labels) != len(target_names):
+        warnings.warn(
+            "labels size, {0}, does not match size of target_names, {1}"
+            .format(len(labels), len(target_names))
+        )
+
     last_line_heading = 'avg / total'
 
     if target_names is None:
diff --git a/sklearn/metrics/tests/test_classification.py b/sklearn/metrics/tests/test_classification.py
index b23d7dffd045..80a51e86f076 100644
--- a/sklearn/metrics/tests/test_classification.py
+++ b/sklearn/metrics/tests/test_classification.py
@@ -738,6 +738,18 @@ def test_classification_report_multiclass_with_long_string_label():
     assert_equal(report, expected_report)
 
 
+def test_classification_report_labels_target_names_unequal_length():
+    y_true = [0, 0, 2, 0, 0]
+    y_pred = [0, 2, 2, 0, 0]
+    target_names = ['class 0', 'class 1', 'class 2']
+
+    assert_warns_message(UserWarning,
+                         "labels size, 2, does not "
+                         "match size of target_names, 3",
+                         classification_report,
+                         y_true, y_pred, target_names=target_names)
+
+
 def test_multilabel_classification_report():
     n_classes = 4
     n_samples = 50
diff --git a/sklearn/naive_bayes.py b/sklearn/naive_bayes.py
index 6b0623843cec..843bf9ce126c 100644
--- a/sklearn/naive_bayes.py
+++ b/sklearn/naive_bayes.py
@@ -25,7 +25,7 @@
 from .preprocessing import binarize
 from .preprocessing import LabelBinarizer
 from .preprocessing import label_binarize
-from .utils import check_X_y, check_array
+from .utils import check_X_y, check_array, check_consistent_length
 from .utils.extmath import safe_sparse_dot, logsumexp
 from .utils.multiclass import _check_partial_fit_first_call
 from .utils.fixes import in1d
@@ -333,6 +333,9 @@ def _partial_fit(self, X, y, classes=None, _refit=False,
             Returns self.
         """
         X, y = check_X_y(X, y)
+        if sample_weight is not None:
+            sample_weight = check_array(sample_weight, ensure_2d=False)
+            check_consistent_length(y, sample_weight)
 
         # If the ratio of data variance between dimensions is too small, it
         # will cause numerical errors. To address this, we artificially
diff --git a/sklearn/preprocessing/data.py b/sklearn/preprocessing/data.py
index ee160a1a8c87..093137d07800 100644
--- a/sklearn/preprocessing/data.py
+++ b/sklearn/preprocessing/data.py
@@ -1735,7 +1735,7 @@ class OneHotEncoder(BaseEstimator, TransformerMixin):
 
     Examples
     --------
-    Given a dataset with three features and two samples, we let the encoder
+    Given a dataset with three features and four samples, we let the encoder
     find the maximum value per feature and transform the data to a binary
     one-hot encoding.
 
diff --git a/sklearn/tree/_criterion.pxd b/sklearn/tree/_criterion.pxd
index 57dacc0726c0..229a6bc2874e 100644
--- a/sklearn/tree/_criterion.pxd
+++ b/sklearn/tree/_criterion.pxd
@@ -53,12 +53,12 @@ cdef class Criterion:
     # statistics correspond to samples[start:pos] and samples[pos:end].
 
     # Methods
-    cdef void init(self, DOUBLE_t* y, SIZE_t y_stride, DOUBLE_t* sample_weight,
-                   double weighted_n_samples, SIZE_t* samples, SIZE_t start,
-                   SIZE_t end) nogil
-    cdef void reset(self) nogil
-    cdef void reverse_reset(self) nogil
-    cdef void update(self, SIZE_t new_pos) nogil
+    cdef int init(self, DOUBLE_t* y, SIZE_t y_stride, DOUBLE_t* sample_weight,
+                  double weighted_n_samples, SIZE_t* samples, SIZE_t start,
+                  SIZE_t end) nogil except -1
+    cdef int reset(self) nogil except -1
+    cdef int reverse_reset(self) nogil except -1
+    cdef int update(self, SIZE_t new_pos) nogil except -1
     cdef double node_impurity(self) nogil
     cdef void children_impurity(self, double* impurity_left,
                                 double* impurity_right) nogil
diff --git a/sklearn/tree/_criterion.pyx b/sklearn/tree/_criterion.pyx
index 3d71818846b9..5187a5066bb2 100644
--- a/sklearn/tree/_criterion.pyx
+++ b/sklearn/tree/_criterion.pyx
@@ -51,11 +51,14 @@ cdef class Criterion:
     def __setstate__(self, d):
         pass
 
-    cdef void init(self, DOUBLE_t* y, SIZE_t y_stride, DOUBLE_t* sample_weight,
-                   double weighted_n_samples, SIZE_t* samples, SIZE_t start,
-                   SIZE_t end) nogil:
+    cdef int init(self, DOUBLE_t* y, SIZE_t y_stride, DOUBLE_t* sample_weight,
+                  double weighted_n_samples, SIZE_t* samples, SIZE_t start,
+                  SIZE_t end) nogil except -1:
         """Placeholder for a method which will initialize the criterion.
 
+        Returns -1 in case of failure to allocate memory (and raise MemoryError)
+        or 0 otherwise.
+
         Parameters
         ----------
         y : array-like, dtype=DOUBLE_t
@@ -79,7 +82,7 @@ cdef class Criterion:
 
         pass
 
-    cdef void reset(self) nogil:
+    cdef int reset(self) nogil except -1:
         """Reset the criterion at pos=start.
 
         This method must be implemented by the subclass.
@@ -87,14 +90,14 @@ cdef class Criterion:
 
         pass
 
-    cdef void reverse_reset(self) nogil:
+    cdef int reverse_reset(self) nogil except -1:
         """Reset the criterion at pos=end.
 
         This method must be implemented by the subclass.
         """
         pass
 
-    cdef void update(self, SIZE_t new_pos) nogil:
+    cdef int update(self, SIZE_t new_pos) nogil except -1:
         """Updated statistics by moving samples[pos:new_pos] to the left child.
 
         This updates the collected statistics by moving samples[pos:new_pos]
@@ -281,12 +284,15 @@ cdef class ClassificationCriterion(Criterion):
                  sizet_ptr_to_ndarray(self.n_classes, self.n_outputs)),
                 self.__getstate__())
 
-    cdef void init(self, DOUBLE_t* y, SIZE_t y_stride,
-                   DOUBLE_t* sample_weight, double weighted_n_samples,
-                   SIZE_t* samples, SIZE_t start, SIZE_t end) nogil:
+    cdef int init(self, DOUBLE_t* y, SIZE_t y_stride,
+                  DOUBLE_t* sample_weight, double weighted_n_samples,
+                  SIZE_t* samples, SIZE_t start, SIZE_t end) nogil except -1:
         """Initialize the criterion at node samples[start:end] and
         children samples[start:start] and samples[start:end].
 
+        Returns -1 in case of failure to allocate memory (and raise MemoryError)
+        or 0 otherwise.
+
         Parameters
         ----------
         y : array-like, dtype=DOUBLE_t
@@ -347,10 +353,14 @@ cdef class ClassificationCriterion(Criterion):
 
         # Reset to pos=start
         self.reset()
+        return 0
 
-    cdef void reset(self) nogil:
-        """Reset the criterion at pos=start."""
+    cdef int reset(self) nogil except -1:
+        """Reset the criterion at pos=start
 
+        Returns -1 in case of failure to allocate memory (and raise MemoryError)
+        or 0 otherwise.
+        """
         self.pos = self.start
 
         self.weighted_n_left = 0.0
@@ -370,9 +380,14 @@ cdef class ClassificationCriterion(Criterion):
             sum_total += self.sum_stride
             sum_left += self.sum_stride
             sum_right += self.sum_stride
+        return 0
 
-    cdef void reverse_reset(self) nogil:
-        """Reset the criterion at pos=end."""
+    cdef int reverse_reset(self) nogil except -1:
+        """Reset the criterion at pos=end
+
+        Returns -1 in case of failure to allocate memory (and raise MemoryError)
+        or 0 otherwise.
+        """
         self.pos = self.end
 
         self.weighted_n_left = self.weighted_n_node_samples
@@ -392,10 +407,14 @@ cdef class ClassificationCriterion(Criterion):
             sum_total += self.sum_stride
             sum_left += self.sum_stride
             sum_right += self.sum_stride
+        return 0
 
-    cdef void update(self, SIZE_t new_pos) nogil:
+    cdef int update(self, SIZE_t new_pos) nogil except -1:
         """Updated statistics by moving samples[pos:new_pos] to the left child.
 
+        Returns -1 in case of failure to allocate memory (and raise MemoryError)
+        or 0 otherwise.
+
         Parameters
         ----------
         new_pos : SIZE_t
@@ -470,6 +489,7 @@ cdef class ClassificationCriterion(Criterion):
             sum_total += self.sum_stride
 
         self.pos = new_pos
+        return 0
 
     cdef double node_impurity(self) nogil:
         pass
@@ -736,9 +756,9 @@ cdef class RegressionCriterion(Criterion):
     def __reduce__(self):
         return (type(self), (self.n_outputs, self.n_samples), self.__getstate__())
 
-    cdef void init(self, DOUBLE_t* y, SIZE_t y_stride, DOUBLE_t* sample_weight,
-                   double weighted_n_samples, SIZE_t* samples, SIZE_t start,
-                   SIZE_t end) nogil:
+    cdef int init(self, DOUBLE_t* y, SIZE_t y_stride, DOUBLE_t* sample_weight,
+                  double weighted_n_samples, SIZE_t* samples, SIZE_t start,
+                  SIZE_t end) nogil except -1:
         """Initialize the criterion at node samples[start:end] and
            children samples[start:start] and samples[start:end]."""
         # Initialize fields
@@ -778,8 +798,9 @@ cdef class RegressionCriterion(Criterion):
 
         # Reset to pos=start
         self.reset()
+        return 0
 
-    cdef void reset(self) nogil:
+    cdef int reset(self) nogil except -1:
         """Reset the criterion at pos=start."""
         cdef SIZE_t n_bytes = self.n_outputs * sizeof(double)
         memset(self.sum_left, 0, n_bytes)
@@ -788,8 +809,9 @@ cdef class RegressionCriterion(Criterion):
         self.weighted_n_left = 0.0
         self.weighted_n_right = self.weighted_n_node_samples
         self.pos = self.start
+        return 0
 
-    cdef void reverse_reset(self) nogil:
+    cdef int reverse_reset(self) nogil except -1:
         """Reset the criterion at pos=end."""
         cdef SIZE_t n_bytes = self.n_outputs * sizeof(double)
         memset(self.sum_right, 0, n_bytes)
@@ -798,8 +820,9 @@ cdef class RegressionCriterion(Criterion):
         self.weighted_n_right = 0.0
         self.weighted_n_left = self.weighted_n_node_samples
         self.pos = self.end
+        return 0
 
-    cdef void update(self, SIZE_t new_pos) nogil:
+    cdef int update(self, SIZE_t new_pos) nogil except -1:
         """Updated statistics by moving samples[pos:new_pos] to the left."""
 
         cdef double* sum_left = self.sum_left
@@ -859,6 +882,7 @@ cdef class RegressionCriterion(Criterion):
             sum_right[k] = sum_total[k] - sum_left[k]
 
         self.pos = new_pos
+        return 0
 
     cdef double node_impurity(self) nogil:
         pass
@@ -1018,9 +1042,6 @@ cdef class MAE(RegressionCriterion):
         # Allocate memory for the accumulators
         safe_realloc(&self.node_medians, n_outputs)
 
-        if (self.node_medians == NULL):
-            raise MemoryError()
-
         self.left_child = np.empty(n_outputs, dtype='object')
         self.right_child = np.empty(n_outputs, dtype='object')
         # initialize WeightedMedianCalculators
@@ -1028,9 +1049,9 @@ cdef class MAE(RegressionCriterion):
             self.left_child[k] = WeightedMedianCalculator(n_samples)
             self.right_child[k] = WeightedMedianCalculator(n_samples)
 
-    cdef void init(self, DOUBLE_t* y, SIZE_t y_stride, DOUBLE_t* sample_weight,
-                   double weighted_n_samples, SIZE_t* samples, SIZE_t start,
-                   SIZE_t end) nogil:
+    cdef int init(self, DOUBLE_t* y, SIZE_t y_stride, DOUBLE_t* sample_weight,
+                  double weighted_n_samples, SIZE_t* samples, SIZE_t start,
+                  SIZE_t end) nogil except -1:
         """Initialize the criterion at node samples[start:end] and
            children samples[start:start] and samples[start:end]."""
 
@@ -1068,6 +1089,7 @@ cdef class MAE(RegressionCriterion):
             for k in range(self.n_outputs):
                 y_ik = y[i * y_stride + k]
 
+                # push method ends up calling safe_realloc, hence `except -1`
                 # push all values to the right side,
                 # since pos = start initially anyway
                 (<WeightedMedianCalculator> right_child[k]).push(y_ik, w)
@@ -1079,9 +1101,14 @@ cdef class MAE(RegressionCriterion):
 
         # Reset to pos=start
         self.reset()
+        return 0
 
-    cdef void reset(self) nogil:
-        """Reset the criterion at pos=start."""
+    cdef int reset(self) nogil except -1:
+        """Reset the criterion at pos=start
+
+        Returns -1 in case of failure to allocate memory (and raise MemoryError)
+        or 0 otherwise.
+        """
 
         cdef SIZE_t i, k
         cdef DOUBLE_t value
@@ -1103,11 +1130,17 @@ cdef class MAE(RegressionCriterion):
                 # remove everything from left and put it into right
                 (<WeightedMedianCalculator> left_child[k]).pop(&value,
                                                                &weight)
+                # push method ends up calling safe_realloc, hence `except -1`
                 (<WeightedMedianCalculator> right_child[k]).push(value,
                                                                  weight)
+        return 0
 
-    cdef void reverse_reset(self) nogil:
-        """Reset the criterion at pos=end."""
+    cdef int reverse_reset(self) nogil except -1:
+        """Reset the criterion at pos=end
+
+        Returns -1 in case of failure to allocate memory (and raise MemoryError)
+        or 0 otherwise.
+        """
 
         self.weighted_n_right = 0.0
         self.weighted_n_left = self.weighted_n_node_samples
@@ -1126,11 +1159,17 @@ cdef class MAE(RegressionCriterion):
                 # remove everything from right and put it into left
                 (<WeightedMedianCalculator> right_child[k]).pop(&value,
                                                                 &weight)
+                # push method ends up calling safe_realloc, hence `except -1`
                 (<WeightedMedianCalculator> left_child[k]).push(value,
                                                                 weight)
+        return 0
 
-    cdef void update(self, SIZE_t new_pos) nogil:
-        """Updated statistics by moving samples[pos:new_pos] to the left."""
+    cdef int update(self, SIZE_t new_pos) nogil except -1:
+        """Updated statistics by moving samples[pos:new_pos] to the left
+
+        Returns -1 in case of failure to allocate memory (and raise MemoryError)
+        or 0 otherwise.
+        """
 
         cdef DOUBLE_t* sample_weight = self.sample_weight
         cdef SIZE_t* samples = self.samples
@@ -1162,6 +1201,7 @@ cdef class MAE(RegressionCriterion):
                     y_ik = y[i * self.y_stride + k]
                     # remove y_ik and its weight w from right and add to left
                     (<WeightedMedianCalculator> right_child[k]).remove(y_ik, w)
+                    # push method ends up calling safe_realloc, hence except -1
                     (<WeightedMedianCalculator> left_child[k]).push(y_ik, w)
 
                 self.weighted_n_left += w
@@ -1185,6 +1225,7 @@ cdef class MAE(RegressionCriterion):
         self.weighted_n_right = (self.weighted_n_node_samples -
                                  self.weighted_n_left)
         self.pos = new_pos
+        return 0
 
     cdef void node_value(self, double* dest) nogil:
         """Computes the node value of samples[start:end] into dest."""
diff --git a/sklearn/tree/_splitter.pxd b/sklearn/tree/_splitter.pxd
index 4b97d01614b9..4d5c5ae46bce 100644
--- a/sklearn/tree/_splitter.pxd
+++ b/sklearn/tree/_splitter.pxd
@@ -81,18 +81,18 @@ cdef class Splitter:
     # This allows optimization with depth-based tree building.
 
     # Methods
-    cdef void init(self, object X, np.ndarray y,
-                   DOUBLE_t* sample_weight,
-                   np.ndarray X_idx_sorted=*) except *
+    cdef int init(self, object X, np.ndarray y,
+                  DOUBLE_t* sample_weight,
+                  np.ndarray X_idx_sorted=*) except -1
 
-    cdef void node_reset(self, SIZE_t start, SIZE_t end,
-                         double* weighted_n_node_samples) nogil
+    cdef int node_reset(self, SIZE_t start, SIZE_t end,
+                        double* weighted_n_node_samples) nogil except -1
 
-    cdef void node_split(self,
-                         double impurity,   # Impurity of the node
-                         SplitRecord* split,
-                         SIZE_t* n_constant_features) nogil
+    cdef int node_split(self,
+                        double impurity,   # Impurity of the node
+                        SplitRecord* split,
+                        SIZE_t* n_constant_features) nogil except -1
 
     cdef void node_value(self, double* dest) nogil
 
-    cdef double node_impurity(self) nogil
\ No newline at end of file
+    cdef double node_impurity(self) nogil
diff --git a/sklearn/tree/_splitter.pyx b/sklearn/tree/_splitter.pyx
index 5fa7ee553fe2..06dfab587493 100644
--- a/sklearn/tree/_splitter.pyx
+++ b/sklearn/tree/_splitter.pyx
@@ -116,15 +116,18 @@ cdef class Splitter:
     def __setstate__(self, d):
         pass
 
-    cdef void init(self,
+    cdef int init(self,
                    object X,
                    np.ndarray[DOUBLE_t, ndim=2, mode="c"] y,
                    DOUBLE_t* sample_weight,
-                   np.ndarray X_idx_sorted=None) except *:
+                   np.ndarray X_idx_sorted=None) except -1:
         """Initialize the splitter.
 
         Take in the input data X, the target Y, and optional sample weights.
 
+        Returns -1 in case of failure to allocate memory (and raise MemoryError)
+        or 0 otherwise.
+
         Parameters
         ----------
         X : object
@@ -180,11 +183,15 @@ cdef class Splitter:
         self.y_stride = <SIZE_t> y.strides[0] / <SIZE_t> y.itemsize
 
         self.sample_weight = sample_weight
+        return 0
 
-    cdef void node_reset(self, SIZE_t start, SIZE_t end,
-                         double* weighted_n_node_samples) nogil:
+    cdef int node_reset(self, SIZE_t start, SIZE_t end,
+                        double* weighted_n_node_samples) nogil except -1:
         """Reset splitter on node samples[start:end].
 
+        Returns -1 in case of failure to allocate memory (and raise MemoryError)
+        or 0 otherwise.
+
         Parameters
         ----------
         start : SIZE_t
@@ -207,13 +214,16 @@ cdef class Splitter:
                             end)
 
         weighted_n_node_samples[0] = self.criterion.weighted_n_node_samples
+        return 0
 
-    cdef void node_split(self, double impurity, SplitRecord* split,
-                         SIZE_t* n_constant_features) nogil:
+    cdef int node_split(self, double impurity, SplitRecord* split,
+                        SIZE_t* n_constant_features) nogil except -1:
         """Find the best split on node samples[start:end].
 
         This is a placeholder method. The majority of computation will be done
         here.
+
+        It should return -1 upon errors.
         """
 
         pass
@@ -257,12 +267,16 @@ cdef class BaseDenseSplitter(Splitter):
         if self.presort == 1:
             free(self.sample_mask)
 
-    cdef void init(self,
-                   object X,
-                   np.ndarray[DOUBLE_t, ndim=2, mode="c"] y,
-                   DOUBLE_t* sample_weight,
-                   np.ndarray X_idx_sorted=None) except *:
-        """Initialize the splitter."""
+    cdef int init(self,
+                  object X,
+                  np.ndarray[DOUBLE_t, ndim=2, mode="c"] y,
+                  DOUBLE_t* sample_weight,
+                  np.ndarray X_idx_sorted=None) except -1:
+        """Initialize the splitter
+
+        Returns -1 in case of failure to allocate memory (and raise MemoryError)
+        or 0 otherwise.
+        """
 
         # Call parent init
         Splitter.init(self, X, y, sample_weight)
@@ -284,6 +298,8 @@ cdef class BaseDenseSplitter(Splitter):
             safe_realloc(&self.sample_mask, self.n_total_samples)
             memset(self.sample_mask, 0, self.n_total_samples*sizeof(SIZE_t))
 
+        return 0
+
 
 cdef class BestSplitter(BaseDenseSplitter):
     """Splitter for finding the best split."""
@@ -295,9 +311,13 @@ cdef class BestSplitter(BaseDenseSplitter):
                                self.random_state,
                                self.presort), self.__getstate__())
 
-    cdef void node_split(self, double impurity, SplitRecord* split,
-                         SIZE_t* n_constant_features) nogil:
-        """Find the best split on node samples[start:end]."""
+    cdef int node_split(self, double impurity, SplitRecord* split,
+                        SIZE_t* n_constant_features) nogil except -1:
+        """Find the best split on node samples[start:end]
+
+        Returns -1 in case of failure to allocate memory (and raise MemoryError)
+        or 0 otherwise.
+        """
         # Find the best split
         cdef SIZE_t* samples = self.samples
         cdef SIZE_t start = self.start
@@ -509,6 +529,7 @@ cdef class BestSplitter(BaseDenseSplitter):
         # Return values
         split[0] = best
         n_constant_features[0] = n_total_constants
+        return 0
 
 
 # Sort n-element arrays pointed to by Xf and samples, simultaneously,
@@ -518,7 +539,8 @@ cdef inline void sort(DTYPE_t* Xf, SIZE_t* samples, SIZE_t n) nogil:
     introsort(Xf, samples, n, maxd)
 
 
-cdef inline void swap(DTYPE_t* Xf, SIZE_t* samples, SIZE_t i, SIZE_t j) nogil:
+cdef inline void swap(DTYPE_t* Xf, SIZE_t* samples,
+        SIZE_t i, SIZE_t j) nogil:
     # Helper for sort
     Xf[i], Xf[j] = Xf[j], Xf[i]
     samples[i], samples[j] = samples[j], samples[i]
@@ -546,7 +568,8 @@ cdef inline DTYPE_t median3(DTYPE_t* Xf, SIZE_t n) nogil:
 
 # Introsort with median of 3 pivot selection and 3-way partition function
 # (robust to repeated elements, e.g. lots of zero features).
-cdef void introsort(DTYPE_t* Xf, SIZE_t *samples, SIZE_t n, int maxd) nogil:
+cdef void introsort(DTYPE_t* Xf, SIZE_t *samples,
+                    SIZE_t n, int maxd) nogil:
     cdef DTYPE_t pivot
     cdef SIZE_t i, l, r
 
@@ -631,9 +654,13 @@ cdef class RandomSplitter(BaseDenseSplitter):
                                  self.random_state,
                                  self.presort), self.__getstate__())
 
-    cdef void node_split(self, double impurity, SplitRecord* split,
-                         SIZE_t* n_constant_features) nogil:
-        """Find the best random split on node samples[start:end]."""
+    cdef int node_split(self, double impurity, SplitRecord* split,
+                        SIZE_t* n_constant_features) nogil except -1:
+        """Find the best random split on node samples[start:end]
+
+        Returns -1 in case of failure to allocate memory (and raise MemoryError)
+        or 0 otherwise.
+        """
         # Draw random splits and pick the best
         cdef SIZE_t* samples = self.samples
         cdef SIZE_t start = self.start
@@ -833,6 +860,7 @@ cdef class RandomSplitter(BaseDenseSplitter):
         # Return values
         split[0] = best
         n_constant_features[0] = n_total_constants
+        return 0
 
 
 cdef class BaseSparseSplitter(Splitter):
@@ -865,13 +893,16 @@ cdef class BaseSparseSplitter(Splitter):
         free(self.index_to_samples)
         free(self.sorted_samples)
 
-    cdef void init(self,
-                   object X,
-                   np.ndarray[DOUBLE_t, ndim=2, mode="c"] y,
-                   DOUBLE_t* sample_weight,
-                   np.ndarray X_idx_sorted=None) except *:
-        """Initialize the splitter."""
+    cdef int init(self,
+                  object X,
+                  np.ndarray[DOUBLE_t, ndim=2, mode="c"] y,
+                  DOUBLE_t* sample_weight,
+                  np.ndarray X_idx_sorted=None) except -1:
+        """Initialize the splitter
 
+        Returns -1 in case of failure to allocate memory (and raise MemoryError)
+        or 0 otherwise.
+        """
         # Call parent init
         Splitter.init(self, X, y, sample_weight)
 
@@ -903,6 +934,7 @@ cdef class BaseSparseSplitter(Splitter):
 
         for p in range(n_samples):
             index_to_samples[samples[p]] = p
+        return 0
 
     cdef inline SIZE_t _partition(self, double threshold,
                                   SIZE_t end_negative, SIZE_t start_positive,
@@ -1148,7 +1180,7 @@ cdef inline void extract_nnz_binary_search(INT32_t* X_indices,
 
 
 cdef inline void sparse_swap(SIZE_t* index_to_samples, SIZE_t* samples,
-                             SIZE_t pos_1, SIZE_t pos_2) nogil  :
+                             SIZE_t pos_1, SIZE_t pos_2) nogil:
     """Swap sample pos_1 and pos_2 preserving sparse invariant."""
     samples[pos_1], samples[pos_2] =  samples[pos_2], samples[pos_1]
     index_to_samples[samples[pos_1]] = pos_1
@@ -1166,10 +1198,12 @@ cdef class BestSparseSplitter(BaseSparseSplitter):
                                      self.random_state,
                                      self.presort), self.__getstate__())
 
-    cdef void node_split(self, double impurity, SplitRecord* split,
-                         SIZE_t* n_constant_features) nogil:
-        """Find the best split on node samples[start:end], using sparse
-           features.
+    cdef int node_split(self, double impurity, SplitRecord* split,
+                        SIZE_t* n_constant_features) nogil except -1:
+        """Find the best split on node samples[start:end], using sparse features
+
+        Returns -1 in case of failure to allocate memory (and raise MemoryError)
+        or 0 otherwise.
         """
         # Find the best split
         cdef SIZE_t* samples = self.samples
@@ -1380,6 +1414,7 @@ cdef class BestSparseSplitter(BaseSparseSplitter):
         # Return values
         split[0] = best
         n_constant_features[0] = n_total_constants
+        return 0
 
 
 cdef class RandomSparseSplitter(BaseSparseSplitter):
@@ -1393,10 +1428,12 @@ cdef class RandomSparseSplitter(BaseSparseSplitter):
                                        self.random_state,
                                        self.presort), self.__getstate__())
 
-    cdef void node_split(self, double impurity, SplitRecord* split,
-                         SIZE_t* n_constant_features) nogil:
-        """Find a random split on node samples[start:end], using sparse
-           features.
+    cdef int node_split(self, double impurity, SplitRecord* split,
+                        SIZE_t* n_constant_features) nogil except -1:
+        """Find a random split on node samples[start:end], using sparse features
+
+        Returns -1 in case of failure to allocate memory (and raise MemoryError)
+        or 0 otherwise.
         """
         # Find the best split
         cdef SIZE_t* samples = self.samples
@@ -1608,3 +1645,4 @@ cdef class RandomSparseSplitter(BaseSparseSplitter):
         # Return values
         split[0] = best
         n_constant_features[0] = n_total_constants
+        return 0
diff --git a/sklearn/tree/_tree.pxd b/sklearn/tree/_tree.pxd
index dbf0545b1e1d..4f9f35972564 100644
--- a/sklearn/tree/_tree.pxd
+++ b/sklearn/tree/_tree.pxd
@@ -58,9 +58,9 @@ cdef class Tree:
     cdef SIZE_t _add_node(self, SIZE_t parent, bint is_left, bint is_leaf,
                           SIZE_t feature, double threshold, double impurity,
                           SIZE_t n_node_samples,
-                          double weighted_n_samples) nogil
-    cdef void _resize(self, SIZE_t capacity) except *
-    cdef int _resize_c(self, SIZE_t capacity=*) nogil
+                          double weighted_n_samples) nogil except -1
+    cdef int _resize(self, SIZE_t capacity) nogil except -1
+    cdef int _resize_c(self, SIZE_t capacity=*) nogil except -1
 
     cdef np.ndarray _get_value_ndarray(self)
     cdef np.ndarray _get_node_ndarray(self)
diff --git a/sklearn/tree/_tree.pyx b/sklearn/tree/_tree.pyx
index f8632ab1640d..69ab8572d2ae 100644
--- a/sklearn/tree/_tree.pyx
+++ b/sklearn/tree/_tree.pyx
@@ -19,7 +19,6 @@
 from cpython cimport Py_INCREF, PyObject
 
 from libc.stdlib cimport free
-from libc.stdlib cimport realloc
 from libc.string cimport memcpy
 from libc.string cimport memset
 
@@ -272,9 +271,12 @@ cdef class DepthFirstTreeBuilder(TreeBuilder):
 # Best first builder ----------------------------------------------------------
 
 cdef inline int _add_to_frontier(PriorityHeapRecord* rec,
-                                 PriorityHeap frontier) nogil:
-    """Adds record ``rec`` to the priority queue ``frontier``; returns -1
-    on memory-error. """
+                                 PriorityHeap frontier) nogil except -1:
+    """Adds record ``rec`` to the priority queue ``frontier``
+
+    Returns -1 in case of failure to allocate memory (and raise MemoryError)
+    or 0 otherwise.
+    """
     return frontier.push(rec.node_id, rec.start, rec.end, rec.pos, rec.depth,
                          rec.is_leaf, rec.improvement, rec.impurity,
                          rec.impurity_left, rec.impurity_right)
@@ -417,7 +419,7 @@ cdef class BestFirstTreeBuilder(TreeBuilder):
                                     SIZE_t start, SIZE_t end, double impurity,
                                     bint is_first, bint is_left, Node* parent,
                                     SIZE_t depth,
-                                    PriorityHeapRecord* res) nogil:
+                                    PriorityHeapRecord* res) nogil except -1:
         """Adds node w/ partition ``[start, end)`` to the frontier. """
         cdef SplitRecord split
         cdef SIZE_t node_id
@@ -657,16 +659,26 @@ cdef class Tree:
         value = memcpy(self.value, (<np.ndarray> value_ndarray).data,
                        self.capacity * self.value_stride * sizeof(double))
 
-    cdef void _resize(self, SIZE_t capacity) except *:
+    cdef int _resize(self, SIZE_t capacity) nogil except -1:
         """Resize all inner arrays to `capacity`, if `capacity` == -1, then
-           double the size of the inner arrays."""
+           double the size of the inner arrays.
+
+        Returns -1 in case of failure to allocate memory (and raise MemoryError)
+        or 0 otherwise.
+        """
         if self._resize_c(capacity) != 0:
-            raise MemoryError()
+            # Acquire gil only if we need to raise
+            with gil:
+                raise MemoryError()
 
     # XXX using (size_t)(-1) is ugly, but SIZE_MAX is not available in C89
     # (i.e., older MSVC).
-    cdef int _resize_c(self, SIZE_t capacity=<SIZE_t>(-1)) nogil:
-        """Guts of _resize. Returns 0 for success, -1 for error."""
+    cdef int _resize_c(self, SIZE_t capacity=<SIZE_t>(-1)) nogil except -1:
+        """Guts of _resize
+
+        Returns -1 in case of failure to allocate memory (and raise MemoryError)
+        or 0 otherwise.
+        """
         if capacity == self.capacity and self.nodes != NULL:
             return 0
 
@@ -676,16 +688,8 @@ cdef class Tree:
             else:
                 capacity = 2 * self.capacity
 
-        # XXX no safe_realloc here because we need to grab the GIL
-        cdef void* ptr = realloc(self.nodes, capacity * sizeof(Node))
-        if ptr == NULL:
-            return -1
-        self.nodes = <Node*> ptr
-        ptr = realloc(self.value,
-                      capacity * self.value_stride * sizeof(double))
-        if ptr == NULL:
-            return -1
-        self.value = <double*> ptr
+        safe_realloc(&self.nodes, capacity)
+        safe_realloc(&self.value, capacity * self.value_stride)
 
         # value memory is initialised to 0 to enable classifier argmax
         if capacity > self.capacity:
@@ -702,7 +706,8 @@ cdef class Tree:
 
     cdef SIZE_t _add_node(self, SIZE_t parent, bint is_left, bint is_leaf,
                           SIZE_t feature, double threshold, double impurity,
-                          SIZE_t n_node_samples, double weighted_n_node_samples) nogil:
+                          SIZE_t n_node_samples,
+                          double weighted_n_node_samples) nogil except -1:
         """Add a node to the tree.
 
         The new node registers itself as the child of its parent.
diff --git a/sklearn/tree/_utils.pxd b/sklearn/tree/_utils.pxd
index cc9649030ed6..017888ab41db 100644
--- a/sklearn/tree/_utils.pxd
+++ b/sklearn/tree/_utils.pxd
@@ -40,19 +40,21 @@ ctypedef fused realloc_ptr:
     (DOUBLE_t**)
     (Node*)
     (Node**)
+    (StackRecord*)
+    (PriorityHeapRecord*)
 
-cdef realloc_ptr safe_realloc(realloc_ptr* p, size_t nelems) except *
+cdef realloc_ptr safe_realloc(realloc_ptr* p, size_t nelems) nogil except *
 
 
 cdef np.ndarray sizet_ptr_to_ndarray(SIZE_t* data, SIZE_t size)
 
 
 cdef SIZE_t rand_int(SIZE_t low, SIZE_t high,
-                            UINT32_t* random_state) nogil
+                     UINT32_t* random_state) nogil
 
 
 cdef double rand_uniform(double low, double high,
-                                UINT32_t* random_state) nogil
+                         UINT32_t* random_state) nogil
 
 
 cdef double log(double x) nogil
@@ -79,7 +81,7 @@ cdef class Stack:
     cdef bint is_empty(self) nogil
     cdef int push(self, SIZE_t start, SIZE_t end, SIZE_t depth, SIZE_t parent,
                   bint is_left, double impurity,
-                  SIZE_t n_constant_features) nogil
+                  SIZE_t n_constant_features) nogil except -1
     cdef int pop(self, StackRecord* res) nogil
 
 
@@ -111,7 +113,7 @@ cdef class PriorityHeap:
     cdef int push(self, SIZE_t node_id, SIZE_t start, SIZE_t end, SIZE_t pos,
                   SIZE_t depth, bint is_leaf, double improvement,
                   double impurity, double impurity_left,
-                  double impurity_right) nogil
+                  double impurity_right) nogil except -1
     cdef int pop(self, PriorityHeapRecord* res) nogil
 
 # =============================================================================
@@ -129,9 +131,9 @@ cdef class WeightedPQueue:
     cdef WeightedPQueueRecord* array_
 
     cdef bint is_empty(self) nogil
-    cdef void reset(self) nogil
+    cdef int reset(self) nogil except -1
     cdef SIZE_t size(self) nogil
-    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil
+    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil except -1
     cdef int remove(self, DOUBLE_t data, DOUBLE_t weight) nogil
     cdef int pop(self, DOUBLE_t* data, DOUBLE_t* weight) nogil
     cdef int peek(self, DOUBLE_t* data, DOUBLE_t* weight) nogil
@@ -152,14 +154,14 @@ cdef class WeightedMedianCalculator:
                                        # = w[0] + w[1] + ... + w[k-1]
 
     cdef SIZE_t size(self) nogil
-    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil
-    cdef void reset(self) nogil
-    cdef int update_median_parameters_post_push(self, DOUBLE_t data,
-                                                DOUBLE_t weight,
-                                                DOUBLE_t original_median) nogil
+    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil except -1
+    cdef int reset(self) nogil except -1
+    cdef int update_median_parameters_post_push(
+        self, DOUBLE_t data, DOUBLE_t weight,
+        DOUBLE_t original_median) nogil
     cdef int remove(self, DOUBLE_t data, DOUBLE_t weight) nogil
     cdef int pop(self, DOUBLE_t* data, DOUBLE_t* weight) nogil
-    cdef int update_median_parameters_post_remove(self, DOUBLE_t data,
-                                                  DOUBLE_t weight,
-                                                  DOUBLE_t original_median) nogil
+    cdef int update_median_parameters_post_remove(
+        self, DOUBLE_t data, DOUBLE_t weight,
+        DOUBLE_t original_median) nogil
     cdef DOUBLE_t get_median(self) nogil
diff --git a/sklearn/tree/_utils.pyx b/sklearn/tree/_utils.pyx
index 465afebc99ff..faf2e5b77744 100644
--- a/sklearn/tree/_utils.pyx
+++ b/sklearn/tree/_utils.pyx
@@ -13,7 +13,6 @@
 
 from libc.stdlib cimport free
 from libc.stdlib cimport malloc
-from libc.stdlib cimport calloc
 from libc.stdlib cimport realloc
 from libc.math cimport log as ln
 
@@ -25,17 +24,19 @@ np.import_array()
 # Helper functions
 # =============================================================================
 
-cdef realloc_ptr safe_realloc(realloc_ptr* p, size_t nelems) except *:
+cdef realloc_ptr safe_realloc(realloc_ptr* p, size_t nelems) nogil except *:
     # sizeof(realloc_ptr[0]) would be more like idiomatic C, but causes Cython
     # 0.20.1 to crash.
     cdef size_t nbytes = nelems * sizeof(p[0][0])
     if nbytes / sizeof(p[0][0]) != nelems:
         # Overflow in the multiplication
-        raise MemoryError("could not allocate (%d * %d) bytes"
-                          % (nelems, sizeof(p[0][0])))
+        with gil:
+            raise MemoryError("could not allocate (%d * %d) bytes"
+                              % (nelems, sizeof(p[0][0])))
     cdef realloc_ptr tmp = <realloc_ptr>realloc(p[0], nbytes)
     if tmp == NULL:
-        raise MemoryError("could not allocate %d bytes" % nbytes)
+        with gil:
+            raise MemoryError("could not allocate %d bytes" % nbytes)
 
     p[0] = tmp
     return tmp  # for convenience
@@ -109,8 +110,6 @@ cdef class Stack:
         self.capacity = capacity
         self.top = 0
         self.stack_ = <StackRecord*> malloc(capacity * sizeof(StackRecord))
-        if self.stack_ == NULL:
-            raise MemoryError()
 
     def __dealloc__(self):
         free(self.stack_)
@@ -120,10 +119,11 @@ cdef class Stack:
 
     cdef int push(self, SIZE_t start, SIZE_t end, SIZE_t depth, SIZE_t parent,
                   bint is_left, double impurity,
-                  SIZE_t n_constant_features) nogil:
+                  SIZE_t n_constant_features) nogil except -1:
         """Push a new element onto the stack.
 
-        Returns 0 if successful; -1 on out of memory error.
+        Return -1 in case of failure to allocate memory (and raise MemoryError)
+        or 0 otherwise.
         """
         cdef SIZE_t top = self.top
         cdef StackRecord* stack = NULL
@@ -131,12 +131,8 @@ cdef class Stack:
         # Resize if capacity not sufficient
         if top >= self.capacity:
             self.capacity *= 2
-            stack = <StackRecord*> realloc(self.stack_,
-                                           self.capacity * sizeof(StackRecord))
-            if stack == NULL:
-                # no free; __dealloc__ handles that
-                return -1
-            self.stack_ = stack
+            # Since safe_realloc can raise MemoryError, use `except -1`
+            safe_realloc(&self.stack_, self.capacity)
 
         stack = self.stack_
         stack[top].start = start
@@ -196,9 +192,7 @@ cdef class PriorityHeap:
     def __cinit__(self, SIZE_t capacity):
         self.capacity = capacity
         self.heap_ptr = 0
-        self.heap_ = <PriorityHeapRecord*> malloc(capacity * sizeof(PriorityHeapRecord))
-        if self.heap_ == NULL:
-            raise MemoryError()
+        safe_realloc(&self.heap_, capacity)
 
     def __dealloc__(self):
         free(self.heap_)
@@ -241,10 +235,11 @@ cdef class PriorityHeap:
     cdef int push(self, SIZE_t node_id, SIZE_t start, SIZE_t end, SIZE_t pos,
                   SIZE_t depth, bint is_leaf, double improvement,
                   double impurity, double impurity_left,
-                  double impurity_right) nogil:
+                  double impurity_right) nogil except -1:
         """Push record on the priority heap.
 
-        Returns 0 if successful; -1 on out of memory error.
+        Return -1 in case of failure to allocate memory (and raise MemoryError)
+        or 0 otherwise.
         """
         cdef SIZE_t heap_ptr = self.heap_ptr
         cdef PriorityHeapRecord* heap = NULL
@@ -252,13 +247,8 @@ cdef class PriorityHeap:
         # Resize if capacity not sufficient
         if heap_ptr >= self.capacity:
             self.capacity *= 2
-            heap = <PriorityHeapRecord*> realloc(self.heap_,
-                                                 self.capacity *
-                                                 sizeof(PriorityHeapRecord))
-            if heap == NULL:
-                # no free; __dealloc__ handles that
-                return -1
-            self.heap_ = heap
+            # Since safe_realloc can raise MemoryError, use `except -1`
+            safe_realloc(&self.heap_, self.capacity)
 
         # Put element as last element of heap
         heap = self.heap_
@@ -330,17 +320,19 @@ cdef class WeightedPQueue:
         self.array_ptr = 0
         safe_realloc(&self.array_, capacity)
 
-        if self.array_ == NULL:
-            raise MemoryError()
-
     def __dealloc__(self):
         free(self.array_)
 
-    cdef void reset(self) nogil:
-        """Reset the WeightedPQueue to its state at construction"""
+    cdef int reset(self) nogil except -1:
+        """Reset the WeightedPQueue to its state at construction
+
+        Return -1 in case of failure to allocate memory (and raise MemoryError)
+        or 0 otherwise.
+        """
         self.array_ptr = 0
-        self.array_ = <WeightedPQueueRecord*> calloc(self.capacity,
-                                                     sizeof(WeightedPQueueRecord))
+        # Since safe_realloc can raise MemoryError, use `except *`
+        safe_realloc(&self.array_, self.capacity)
+        return 0
 
     cdef bint is_empty(self) nogil:
         return self.array_ptr <= 0
@@ -348,9 +340,11 @@ cdef class WeightedPQueue:
     cdef SIZE_t size(self) nogil:
         return self.array_ptr
 
-    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil:
+    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil except -1:
         """Push record on the array.
-        Returns 0 if successful; -1 on out of memory error.
+
+        Return -1 in case of failure to allocate memory (and raise MemoryError)
+        or 0 otherwise.
         """
         cdef SIZE_t array_ptr = self.array_ptr
         cdef WeightedPQueueRecord* array = NULL
@@ -359,14 +353,8 @@ cdef class WeightedPQueue:
         # Resize if capacity not sufficient
         if array_ptr >= self.capacity:
             self.capacity *= 2
-            array = <WeightedPQueueRecord*> realloc(self.array_,
-                                                    self.capacity *
-                                                    sizeof(WeightedPQueueRecord))
-
-            if array == NULL:
-                # no free; __dealloc__ handles that
-                return -1
-            self.array_ = array
+            # Since safe_realloc can raise MemoryError, use `except -1`
+            safe_realloc(&self.array_, self.capacity)
 
         # Put element as last element of array
         array = self.array_
@@ -510,31 +498,40 @@ cdef class WeightedMedianCalculator:
         WeightedMedianCalculator"""
         return self.samples.size()
 
-    cdef void reset(self) nogil:
-        """Reset the WeightedMedianCalculator to its state at construction"""
+    cdef int reset(self) nogil except -1:
+        """Reset the WeightedMedianCalculator to its state at construction
+
+        Return -1 in case of failure to allocate memory (and raise MemoryError)
+        or 0 otherwise.
+        """
+        # samples.reset (WeightedPQueue.reset) uses safe_realloc, hence
+        # except -1
         self.samples.reset()
         self.total_weight = 0
         self.k = 0
         self.sum_w_0_k = 0
+        return 0
+
+    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil except -1:
+        """Push a value and its associated weight to the WeightedMedianCalculator
 
-    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil:
-        """Push a value and its associated weight
-        to the WeightedMedianCalculator to be considered
-        in the median calculation.
+        Return -1 in case of failure to allocate memory (and raise MemoryError)
+        or 0 otherwise.
         """
         cdef int return_value
         cdef DOUBLE_t original_median
 
         if self.size() != 0:
             original_median = self.get_median()
+        # samples.push (WeightedPQueue.push) uses safe_realloc, hence except -1
         return_value = self.samples.push(data, weight)
         self.update_median_parameters_post_push(data, weight,
                                                 original_median)
         return return_value
 
-    cdef int update_median_parameters_post_push(self, DOUBLE_t data,
-                                                DOUBLE_t weight,
-                                                DOUBLE_t original_median) nogil:
+    cdef int update_median_parameters_post_push(
+            self, DOUBLE_t data, DOUBLE_t weight,
+            DOUBLE_t original_median) nogil:
         """Update the parameters used in the median calculation,
         namely `k` and `sum_w_0_k` after an insertion"""
 
@@ -609,9 +606,9 @@ cdef class WeightedMedianCalculator:
                                                   original_median)
         return return_value
 
-    cdef int update_median_parameters_post_remove(self, DOUBLE_t data,
-                                                  DOUBLE_t weight,
-                                                  double original_median) nogil:
+    cdef int update_median_parameters_post_remove(
+            self, DOUBLE_t data, DOUBLE_t weight,
+            double original_median) nogil:
         """Update the parameters used in the median calculation,
         namely `k` and `sum_w_0_k` after a removal"""
         # reset parameters because it there are no elements
diff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py
index 9fab2f6fbef9..cb23e0ba8a31 100644
--- a/sklearn/utils/estimator_checks.py
+++ b/sklearn/utils/estimator_checks.py
@@ -72,6 +72,7 @@ def _yield_non_meta_checks(name, Estimator):
     yield check_fit_score_takes_y
     yield check_dtype_object
     yield check_sample_weights_pandas_series
+    yield check_sample_weights_list
     yield check_estimators_fit_returns_self
 
     # Check that all estimator yield informative messages when
@@ -223,6 +224,7 @@ def _yield_all_checks(name, Estimator):
     yield check_get_params_invariance
     yield check_dict_unchanged
     yield check_no_fit_attributes_set_in_init
+    yield check_dont_overwrite_parameters
 
 
 def check_estimator(Estimator):
@@ -395,6 +397,21 @@ def check_sample_weights_pandas_series(name, Estimator):
                            "input of type pandas.Series to class weight.")
 
 
+@ignore_warnings(category=DeprecationWarning)
+def check_sample_weights_list(name, Estimator):
+    # check that estimators will accept a 'sample_weight' parameter of
+    # type list in the 'fit' function.
+    estimator = Estimator()
+    if has_fit_parameter(estimator, "sample_weight"):
+        rnd = np.random.RandomState(0)
+        X = rnd.uniform(size=(10, 3))
+        y = np.arange(10) % 3
+        y = multioutput_estimator_convert_y_2d(name, y)
+        sample_weight = [3] * 10
+        # Test that estimators don't raise any exception
+        estimator.fit(X, y, sample_weight=sample_weight)
+
+
 @ignore_warnings(category=(DeprecationWarning, UserWarning))
 def check_dtype_object(name, Estimator):
     # check that estimators treat dtype object as numeric if possible
@@ -467,8 +484,62 @@ def check_dict_unchanged(name, Estimator):
                               'Estimator changes __dict__ during %s' % method)
 
 
+def is_public_parameter(attr):
+    return not (attr.startswith('_') or attr.endswith('_'))
+
+
+def check_dont_overwrite_parameters(name, Estimator):
+    # check that fit method only changes or sets private attributes
+    if hasattr(Estimator.__init__, "deprecated_original"):
+        # to not check deprecated classes
+        return
+    rnd = np.random.RandomState(0)
+    X = 3 * rnd.uniform(size=(20, 3))
+    y = X[:, 0].astype(np.int)
+    y = multioutput_estimator_convert_y_2d(name, y)
+    estimator = Estimator()
+    set_testing_parameters(estimator)
+
+    if hasattr(estimator, "n_components"):
+        estimator.n_components = 1
+    if hasattr(estimator, "n_clusters"):
+        estimator.n_clusters = 1
+
+    set_random_state(estimator, 1)
+    dict_before_fit = estimator.__dict__.copy()
+    estimator.fit(X, y)
+
+    dict_after_fit = estimator.__dict__
+
+    public_keys_after_fit = [key for key in dict_after_fit.keys()
+                             if is_public_parameter(key)]
+
+    attrs_added_by_fit = [key for key in public_keys_after_fit
+                          if key not in dict_before_fit.keys()]
+
+    # check that fit doesn't add any public attribute
+    assert_true(not attrs_added_by_fit,
+                ('Estimator adds public attribute(s) during'
+                 ' the fit method.'
+                 ' Estimators are only allowed to add private attributes'
+                 ' either started with _ or ended'
+                 ' with _ but %s added' % ', '.join(attrs_added_by_fit)))
+
+    # check that fit doesn't change any public attribute
+    attrs_changed_by_fit = [key for key in public_keys_after_fit
+                            if (dict_before_fit[key]
+                                is not dict_after_fit[key])]
+
+    assert_true(not attrs_changed_by_fit,
+                ('Estimator changes public attribute(s) during'
+                 ' the fit method. Estimators are only allowed'
+                 ' to change attributes started'
+                 ' or ended with _, but'
+                 ' %s changed' % ', '.join(attrs_changed_by_fit)))
+
+
 def check_fit2d_predict1d(name, Estimator):
-    # check by fitting a 2d array and prediting with a 1d array
+    # check by fitting a 2d array and predicting with a 1d array
     rnd = np.random.RandomState(0)
     X = 3 * rnd.uniform(size=(20, 3))
     y = X[:, 0].astype(np.int)
diff --git a/sklearn/utils/tests/test_estimator_checks.py b/sklearn/utils/tests/test_estimator_checks.py
index 1d57d0b797d0..c84604ef9265 100644
--- a/sklearn/utils/tests/test_estimator_checks.py
+++ b/sklearn/utils/tests/test_estimator_checks.py
@@ -43,6 +43,26 @@ def predict(self, X):
         return np.ones(X.shape[0])
 
 
+class SetsWrongAttribute(BaseEstimator):
+    def __init__(self):
+        self.acceptable_key = 0
+
+    def fit(self, X, y=None):
+        self.wrong_attribute = 0
+        X, y = check_X_y(X, y)
+        return self
+
+
+class ChangesWrongAttribute(BaseEstimator):
+    def __init__(self):
+        self.wrong_attribute = 0
+
+    def fit(self, X, y=None):
+        self.wrong_attribute = 1
+        X, y = check_X_y(X, y)
+        return self
+
+
 class NoCheckinPredict(BaseBadClassifier):
     def fit(self, X, y):
         X, y = check_X_y(X, y)
@@ -122,7 +142,20 @@ def test_check_estimator():
     # at transform/predict/predict_proba time
     msg = 'Estimator changes __dict__ during predict'
     assert_raises_regex(AssertionError, msg, check_estimator, ChangesDict)
-
+    # check that `fit` only changes attribures that
+    # are private (start with an _ or end with a _).
+    msg = ('Estimator changes public attribute\(s\) during the fit method.'
+           ' Estimators are only allowed to change attributes started'
+           ' or ended with _, but wrong_attribute changed')
+    assert_raises_regex(AssertionError, msg,
+                        check_estimator, ChangesWrongAttribute)
+    # check that `fit` doesn't add any public attribute
+    msg = ('Estimator adds public attribute\(s\) during the fit method.'
+           ' Estimators are only allowed to add private attributes'
+           ' either started with _ or ended'
+           ' with _ but wrong_attribute added')
+    assert_raises_regex(AssertionError, msg,
+                        check_estimator, SetsWrongAttribute)
     # check for sparse matrix input handling
     name = NoSparseClassifier.__name__
     msg = "Estimator " + name + " doesn't seem to fail gracefully on sparse data"
