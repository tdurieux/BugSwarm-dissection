diff --git a/doc/related_projects.rst b/doc/related_projects.rst
index 8280ccc9a5..29e0a3337e 100755
--- a/doc/related_projects.rst
+++ b/doc/related_projects.rst
@@ -12,19 +12,12 @@ Interoperability and framework enhancements
 These tools adapt scikit-learn for use with other technologies or otherwise
 enhance the functionality of scikit-learn's estimators.
 
-- `ML Frontend <https://github.com/jeff1evesque/machine-learning>`_ provides
-  dataset management and SVM fitting/prediction through
-  `web-based <https://github.com/jeff1evesque/machine-learning#web-interface>`_
-  and `programmatic <https://github.com/jeff1evesque/machine-learning#programmatic-interface>`_
-  interfaces.
+**Data formats**
 
 - `sklearn_pandas <https://github.com/paulgb/sklearn-pandas/>`_ bridge for
   scikit-learn pipelines and pandas data frame with dedicated transformers.
 
-- `Scikit-Learn Laboratory
-  <https://skll.readthedocs.io/en/latest/index.html>`_  A command-line
-  wrapper around scikit-learn that makes it easy to run machine learning
-  experiments with multiple learners and large feature sets.
+**Auto-ML**
 
 - `auto-sklearn <https://github.com/automl/auto-sklearn/>`_
   An automated machine learning toolkit and a drop-in replacement for a
@@ -36,6 +29,37 @@ enhance the functionality of scikit-learn's estimators.
   preprocessors as well as the estimators. Works as a drop-in replacement for a
   scikit-learn estimator.
 
+**Experimentation frameworks**
+
+- `PyMC <http://pymc-devs.github.io/pymc/>`_ Bayesian statistical models and
+  fitting algorithms.
+
+- `REP <https://github.com/yandex/REP>`_ Environment for conducting data-driven
+  research in a consistent and reproducible way
+
+- `ML Frontend <https://github.com/jeff1evesque/machine-learning>`_ provides
+  dataset management and SVM fitting/prediction through
+  `web-based <https://github.com/jeff1evesque/machine-learning#web-interface>`_
+  and `programmatic <https://github.com/jeff1evesque/machine-learning#programmatic-interface>`_
+  interfaces.
+
+- `Scikit-Learn Laboratory
+  <https://skll.readthedocs.io/en/latest/index.html>`_  A command-line
+  wrapper around scikit-learn that makes it easy to run machine learning
+  experiments with multiple learners and large feature sets.
+
+**Model inspection and visualisation**
+
+- `eli5 <https://github.com/TeamHG-Memex/eli5/>`_ A library for
+  debugging/inspecting machine learning models and explaining their
+  predictions.
+
+- `mlxtend <https://github.com/rasbt/mlxtend>`_ Includes model visualization
+  utilities.
+
+
+**Model export for production**
+
 - `sklearn-pmml <https://github.com/alex-pirozhenko/sklearn-pmml>`_
   Serialization of (some) scikit-learn estimators into PMML.
 
@@ -47,6 +71,12 @@ enhance the functionality of scikit-learn's estimators.
 - `sklearn-porter <https://github.com/nok/sklearn-porter>`_
   Transpile trained scikit-learn models to C, Java, Javascript and others.
 
+- `sklearn-compiledtrees <https://github.com/ajtulloch/sklearn-compiledtrees/>`_
+  Generate a C++ implementation of the predict function for decision trees (and
+  ensembles) trained by sklearn. Useful for latency-sensitive production
+  environments.
+
+
 Other estimators and tasks
 --------------------------
 
@@ -55,14 +85,7 @@ project. The following are projects providing interfaces similar to
 scikit-learn for additional learning algorithms, infrastructures
 and tasks.
 
-- `pylearn2 <http://deeplearning.net/software/pylearn2/>`_ A deep learning and
-  neural network library build on theano with scikit-learn like interface.
-
-- `sklearn_theano <http://sklearn-theano.github.io/>`_ scikit-learn compatible
-  estimators, transformers, and datasets which use Theano internally
-
-- `lightning <https://github.com/scikit-learn-contrib/lightning>`_ Fast state-of-the-art
-  linear model solvers (SDCA, AdaGrad, SVRG, SAG, etc...).
+**Structured learning**
 
 - `Seqlearn <https://github.com/larsmans/seqlearn>`_  Sequence classification
   using HMMs or structured perceptron.
@@ -81,25 +104,41 @@ and tasks.
   (`CRFsuite <http://www.chokkan.org/software/crfsuite/>`_ wrapper with
   sklearn-like API).
 
-- `py-earth <https://github.com/scikit-learn-contrib/py-earth>`_ Multivariate adaptive
-  regression splines
+**Deep neural networks etc.**
 
-- `sklearn-compiledtrees <https://github.com/ajtulloch/sklearn-compiledtrees/>`_
-  Generate a C++ implementation of the predict function for decision trees (and
-  ensembles) trained by sklearn. Useful for latency-sensitive production
-  environments.
+- `pylearn2 <http://deeplearning.net/software/pylearn2/>`_ A deep learning and
+  neural network library build on theano with scikit-learn like interface.
 
-- `lda <https://github.com/ariddell/lda/>`_: Fast implementation of latent
-  Dirichlet allocation in Cython which uses `Gibbs sampling
-  <https://en.wikipedia.org/wiki/Gibbs_sampling>`_ to sample from the true
-  posterior distribution. (scikit-learn's
-  :class:`sklearn.decomposition.LatentDirichletAllocation` implementation uses
-  `variational inference
-  <https://en.wikipedia.org/wiki/Variational_Bayesian_methods>`_ to sample from
-  a tractable approximation of a topic model's posterior distribution.)
+- `sklearn_theano <http://sklearn-theano.github.io/>`_ scikit-learn compatible
+  estimators, transformers, and datasets which use Theano internally
 
-- `Sparse Filtering <https://github.com/jmetzen/sparse-filtering>`_
-  Unsupervised feature learning based on sparse-filtering
+- `nolearn <https://github.com/dnouri/nolearn>`_ A number of wrappers and
+  abstractions around existing neural network libraries
+
+- `keras <https://github.com/fchollet/keras>`_ Deep Learning library capable of
+  running on top of either TensorFlow or Theano.
+
+- `lasagne <https://github.com/Lasagne/Lasagne>`_ A lightweight library to
+  build and train neural networks in Theano.
+
+**Broad scope**
+
+- `mlxtend <https://github.com/rasbt/mlxtend>`_ Includes a number of additional
+  estimators as well as model visualization utilities.
+
+- `sparkit-learn <https://github.com/lensacom/sparkit-learn>`_ Scikit-learn
+  API and functionality for PySpark's distributed modelling.
+
+**Other regression and classification**
+
+- `xgboost <https://github.com/dmlc/xgboost>`_ Optimised gradient boosted decision
+  tree library.
+
+- `lightning <https://github.com/scikit-learn-contrib/lightning>`_ Fast
+  state-of-the-art linear model solvers (SDCA, AdaGrad, SVRG, SAG, etc...).
+
+- `py-earth <https://github.com/scikit-learn-contrib/py-earth>`_ Multivariate
+  adaptive regression splines
 
 - `Kernel Regression <https://github.com/jmetzen/kernel_regression>`_
   Implementation of Nadaraya-Watson kernel regression with automatic bandwidth
@@ -108,28 +147,32 @@ and tasks.
 - `gplearn <https://github.com/trevorstephens/gplearn>`_ Genetic Programming
   for symbolic regression tasks.
 
-- `nolearn <https://github.com/dnouri/nolearn>`_ A number of wrappers and
-  abstractions around existing neural network libraries
+- `multiisotonic <https://github.com/alexfields/multiisotonic>`_ Isotonic
+  regression on multidimensional features.
 
-- `sparkit-learn <https://github.com/lensacom/sparkit-learn>`_ Scikit-learn functionality and API on PySpark.
+**Decomposition and clustering**
 
-- `keras <https://github.com/fchollet/keras>`_ Deep Learning library capable of
-  running on top of either TensorFlow or Theano.
-
-- `mlxtend <https://github.com/rasbt/mlxtend>`_ Includes a number of additional
-  estimators as well as model visualization utilities.
-
-- `kmodes <https://github.com/nicodv/kmodes>`_ k-modes clustering algorithm for categorical data, and
-  several of its variations.
+- `lda <https://github.com/ariddell/lda/>`_: Fast implementation of latent
+  Dirichlet allocation in Cython which uses `Gibbs sampling
+  <https://en.wikipedia.org/wiki/Gibbs_sampling>`_ to sample from the true
+  posterior distribution. (scikit-learn's
+  :class:`sklearn.decomposition.LatentDirichletAllocation` implementation uses
+  `variational inference
+  <https://en.wikipedia.org/wiki/Variational_Bayesian_methods>`_ to sample from
+  a tractable approximation of a topic model's posterior distribution.)
 
-- `hdbscan <https://github.com/lmcinnes/hdbscan>`_ HDBSCAN and Robust Single Linkage clustering algorithms
-  for robust variable density clustering.
+- `Sparse Filtering <https://github.com/jmetzen/sparse-filtering>`_
+  Unsupervised feature learning based on sparse-filtering
 
-- `lasagne <https://github.com/Lasagne/Lasagne>`_ A lightweight library to build and train neural networks in Theano.
+- `kmodes <https://github.com/nicodv/kmodes>`_ k-modes clustering algorithm for
+  categorical data, and several of its variations.
 
-- `multiisotonic <https://github.com/alexfields/multiisotonic>`_ Isotonic regression on multidimensional features.
+- `hdbscan <https://github.com/lmcinnes/hdbscan>`_ HDBSCAN and Robust Single
+  Linkage clustering algorithms for robust variable density clustering.
 
-- `spherecluster <https://github.com/clara-labs/spherecluster>`_ Spherical K-means and mixture of von Mises Fisher clustering routines for data on the unit hypersphere.
+- `spherecluster <https://github.com/clara-labs/spherecluster>`_ Spherical
+  K-means and mixture of von Mises Fisher clustering routines for data on the
+  unit hypersphere.
 
 Statistical learning with Python
 --------------------------------
@@ -145,12 +188,6 @@ Other packages useful for data analysis and machine learning.
   statistical models. More focused on statistical tests and less on prediction
   than scikit-learn.
 
-- `PyMC <http://pymc-devs.github.io/pymc/>`_ Bayesian statistical models and
-  fitting algorithms.
-
-- `REP <https://github.com/yandex/REP>`_ Environment for conducting data-driven
-  research in a consistent and reproducible way
-
 - `Sacred <https://github.com/IDSIA/Sacred>`_ Tool to help you configure,
   organize, log and reproduce experiments
 
diff --git a/examples/linear_model/plot_ransac.py b/examples/linear_model/plot_ransac.py
index e9a6d910ec..0bafe4ee4a 100755
--- a/examples/linear_model/plot_ransac.py
+++ b/examples/linear_model/plot_ransac.py
@@ -27,32 +27,33 @@
 y[:n_outliers] = -3 + 10 * np.random.normal(size=n_outliers)
 
 # Fit line using all data
-model = linear_model.LinearRegression()
-model.fit(X, y)
+lr = linear_model.LinearRegression()
+lr.fit(X, y)
 
 # Robustly fit linear model with RANSAC algorithm
-model_ransac = linear_model.RANSACRegressor(linear_model.LinearRegression())
-model_ransac.fit(X, y)
-inlier_mask = model_ransac.inlier_mask_
+ransac = linear_model.RANSACRegressor()
+ransac.fit(X, y)
+inlier_mask = ransac.inlier_mask_
 outlier_mask = np.logical_not(inlier_mask)
 
 # Predict data of estimated models
-line_X = np.arange(-5, 5)
-line_y = model.predict(line_X[:, np.newaxis])
-line_y_ransac = model_ransac.predict(line_X[:, np.newaxis])
+line_X = np.arange(X.min(), X.max())[:, np.newaxis]
+line_y = lr.predict(line_X)
+line_y_ransac = ransac.predict(line_X)
 
 # Compare estimated coefficients
-print("Estimated coefficients (true, normal, RANSAC):")
-print(coef, model.coef_, model_ransac.estimator_.coef_)
+print("Estimated coefficients (true, linear regression, RANSAC):")
+print(coef, lr.coef_, ransac.estimator_.coef_)
 
 lw = 2
 plt.scatter(X[inlier_mask], y[inlier_mask], color='yellowgreen', marker='.',
             label='Inliers')
 plt.scatter(X[outlier_mask], y[outlier_mask], color='gold', marker='.',
             label='Outliers')
-plt.plot(line_X, line_y, color='navy', linestyle='-', linewidth=lw,
-         label='Linear regressor')
-plt.plot(line_X, line_y_ransac, color='cornflowerblue', linestyle='-',
-         linewidth=lw, label='RANSAC regressor')
+plt.plot(line_X, line_y, color='navy', linewidth=lw, label='Linear regressor')
+plt.plot(line_X, line_y_ransac, color='cornflowerblue', linewidth=lw,
+         label='RANSAC regressor')
 plt.legend(loc='lower right')
+plt.xlabel("Input")
+plt.ylabel("Response")
 plt.show()
diff --git a/examples/linear_model/plot_ridge_path.py b/examples/linear_model/plot_ridge_path.py
index 52f816d342..1f2c475f78 100755
--- a/examples/linear_model/plot_ridge_path.py
+++ b/examples/linear_model/plot_ridge_path.py
@@ -44,13 +44,12 @@
 
 n_alphas = 200
 alphas = np.logspace(-10, -2, n_alphas)
-clf = linear_model.Ridge(fit_intercept=False)
 
 coefs = []
 for a in alphas:
-    clf.set_params(alpha=a)
-    clf.fit(X, y)
-    coefs.append(clf.coef_)
+    ridge = linear_model.Ridge(alpha=a, fit_intercept=False)
+    ridge.fit(X, y)
+    coefs.append(ridge.coef_)
 
 ###############################################################################
 # Display results
diff --git a/setup.py b/setup.py
index fb427498cf..ff7527ef04 100755
--- a/setup.py
+++ b/setup.py
@@ -203,6 +203,7 @@ def setup_package():
                                  'Programming Language :: Python :: 3',
                                  'Programming Language :: Python :: 3.4',
                                  'Programming Language :: Python :: 3.5',
+                                 'Programming Language :: Python :: 3.6',
                                  ],
                     cmdclass=cmdclass,
                     **extra_setuptools_args)
diff --git a/sklearn/calibration.py b/sklearn/calibration.py
index 37928817fd..35cdab45a1 100755
--- a/sklearn/calibration.py
+++ b/sklearn/calibration.py
@@ -19,7 +19,7 @@
 from .base import BaseEstimator, ClassifierMixin, RegressorMixin, clone
 from .preprocessing import label_binarize, LabelBinarizer
 from .utils import check_X_y, check_array, indexable, column_or_1d
-from .utils.validation import check_is_fitted
+from .utils.validation import check_is_fitted, check_consistent_length
 from .utils.fixes import signature
 from .isotonic import IsotonicRegression
 from .svm import LinearSVC
@@ -167,6 +167,9 @@ def fit(self, X, y, sample_weight=None):
                               " itself." % estimator_name)
                 base_estimator_sample_weight = None
             else:
+                if sample_weight is not None:
+                    sample_weight = check_array(sample_weight, ensure_2d=False)
+                    check_consistent_length(y, sample_weight)
                 base_estimator_sample_weight = sample_weight
             for train, test in cv.split(X, y):
                 this_estimator = clone(base_estimator)
diff --git a/sklearn/ensemble/bagging.py b/sklearn/ensemble/bagging.py
index bdc8637944..711d089d05 100755
--- a/sklearn/ensemble/bagging.py
+++ b/sklearn/ensemble/bagging.py
@@ -20,7 +20,7 @@
 from ..utils import check_random_state, check_X_y, check_array, column_or_1d
 from ..utils.random import sample_without_replacement
 from ..utils.validation import has_fit_parameter, check_is_fitted
-from ..utils import indices_to_mask
+from ..utils import indices_to_mask, check_consistent_length
 from ..utils.fixes import bincount
 from ..utils.metaestimators import if_delegate_has_method
 from ..utils.multiclass import check_classification_targets
@@ -82,8 +82,8 @@ def _parallel_build_estimators(n_estimators, ensemble, X, y, sample_weight,
 
     for i in range(n_estimators):
         if verbose > 1:
-            print("Building estimator %d of %d for this parallel run (total %d)..." %
-                  (i + 1, n_estimators, total_n_estimators))
+            print("Building estimator %d of %d for this parallel run "
+                  "(total %d)..." % (i + 1, n_estimators, total_n_estimators))
 
         random_state = np.random.RandomState(seeds[i])
         estimator = ensemble._make_estimator(append=False,
@@ -282,6 +282,9 @@ def _fit(self, X, y, max_samples=None, max_depth=None, sample_weight=None):
 
         # Convert data
         X, y = check_X_y(X, y, ['csr', 'csc'])
+        if sample_weight is not None:
+            sample_weight = check_array(sample_weight, ensure_2d=False)
+            check_consistent_length(y, sample_weight)
 
         # Remap output
         n_samples, self.n_features_ = X.shape
diff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py
index 565acba536..ca20c9dbc6 100755
--- a/sklearn/linear_model/logistic.py
+++ b/sklearn/linear_model/logistic.py
@@ -888,6 +888,9 @@ def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,
     y_test = y[test]
 
     if sample_weight is not None:
+        sample_weight = check_array(sample_weight, ensure_2d=False)
+        check_consistent_length(y, sample_weight)
+
         sample_weight = sample_weight[train]
 
     coefs, Cs, n_iter = logistic_regression_path(
diff --git a/sklearn/metrics/classification.py b/sklearn/metrics/classification.py
index 6ad2e81fcb..e94e70a7c1 100755
--- a/sklearn/metrics/classification.py
+++ b/sklearn/metrics/classification.py
@@ -243,9 +243,9 @@ def top_n_accuracy_score(y_true, y_pred, n=5, normalize=True):
     num_obs, num_labels = y_pred.shape
     idx = num_labels - n - 1
     counter = 0
-    parted = np.argpartition(y_pred, kth=idx, axis=1)
+    argsorted = np.argsort(y_pred, axis=1)
     for i in range(num_obs):
-        if y_true[i] in parted[i, idx+1:]:
+        if y_true[i] in argsorted[i, idx+1:]:
             counter += 1
     if normalize:
         return counter / num_obs
diff --git a/sklearn/naive_bayes.py b/sklearn/naive_bayes.py
index 6b0623843c..843bf9ce12 100755
--- a/sklearn/naive_bayes.py
+++ b/sklearn/naive_bayes.py
@@ -25,7 +25,7 @@
 from .preprocessing import binarize
 from .preprocessing import LabelBinarizer
 from .preprocessing import label_binarize
-from .utils import check_X_y, check_array
+from .utils import check_X_y, check_array, check_consistent_length
 from .utils.extmath import safe_sparse_dot, logsumexp
 from .utils.multiclass import _check_partial_fit_first_call
 from .utils.fixes import in1d
@@ -333,6 +333,9 @@ def _partial_fit(self, X, y, classes=None, _refit=False,
             Returns self.
         """
         X, y = check_X_y(X, y)
+        if sample_weight is not None:
+            sample_weight = check_array(sample_weight, ensure_2d=False)
+            check_consistent_length(y, sample_weight)
 
         # If the ratio of data variance between dimensions is too small, it
         # will cause numerical errors. To address this, we artificially
diff --git a/sklearn/preprocessing/data.py b/sklearn/preprocessing/data.py
index ee160a1a8c..093137d078 100755
--- a/sklearn/preprocessing/data.py
+++ b/sklearn/preprocessing/data.py
@@ -1735,7 +1735,7 @@ class OneHotEncoder(BaseEstimator, TransformerMixin):
 
     Examples
     --------
-    Given a dataset with three features and two samples, we let the encoder
+    Given a dataset with three features and four samples, we let the encoder
     find the maximum value per feature and transform the data to a binary
     one-hot encoding.
 
diff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py
index b5a59631c5..cb23e0ba8a 100755
--- a/sklearn/utils/estimator_checks.py
+++ b/sklearn/utils/estimator_checks.py
@@ -72,6 +72,7 @@ def _yield_non_meta_checks(name, Estimator):
     yield check_fit_score_takes_y
     yield check_dtype_object
     yield check_sample_weights_pandas_series
+    yield check_sample_weights_list
     yield check_estimators_fit_returns_self
 
     # Check that all estimator yield informative messages when
@@ -396,6 +397,21 @@ def check_sample_weights_pandas_series(name, Estimator):
                            "input of type pandas.Series to class weight.")
 
 
+@ignore_warnings(category=DeprecationWarning)
+def check_sample_weights_list(name, Estimator):
+    # check that estimators will accept a 'sample_weight' parameter of
+    # type list in the 'fit' function.
+    estimator = Estimator()
+    if has_fit_parameter(estimator, "sample_weight"):
+        rnd = np.random.RandomState(0)
+        X = rnd.uniform(size=(10, 3))
+        y = np.arange(10) % 3
+        y = multioutput_estimator_convert_y_2d(name, y)
+        sample_weight = [3] * 10
+        # Test that estimators don't raise any exception
+        estimator.fit(X, y, sample_weight=sample_weight)
+
+
 @ignore_warnings(category=(DeprecationWarning, UserWarning))
 def check_dtype_object(name, Estimator):
     # check that estimators treat dtype object as numeric if possible
