diff --git a/.circleci/config.yml b/.circleci/config.yml
index fcc43270c668..effb37d6b8fa 100644
--- a/.circleci/config.yml
+++ b/.circleci/config.yml
@@ -65,6 +65,22 @@ jobs:
           path: ~/log.txt
           destination: log.txt
 
+  pypy3:
+    docker:
+      - image: pypy:3-6.0.0
+    steps:
+      - restore_cache:
+          keys:
+            - pypy3-ccache-{{ .Branch }}
+            - pypy3-ccache
+      - checkout
+      - run: ./build_tools/circle/build_test_pypy.sh
+      - save_cache:
+          key: pypy3-ccache-{{ .Branch }}-{{ .BuildNum }}
+          paths:
+            - ~/.ccache
+            - ~/.cache/pip
+
   deploy:
     docker:
       - image: circleci/python:3.6.1
@@ -88,6 +104,7 @@ workflows:
     jobs:
       - python3
       - python2
+      - pypy3
       - deploy:
           requires:
             - python3
diff --git a/build_tools/circle/build_test_pypy.sh b/build_tools/circle/build_test_pypy.sh
new file mode 100755
index 000000000000..18fa361821d1
--- /dev/null
+++ b/build_tools/circle/build_test_pypy.sh
@@ -0,0 +1,30 @@
+#!/usr/bin/env bash
+set -x
+set -e
+
+apt-get -yq update
+apt-get -yq install libatlas-dev libatlas-base-dev liblapack-dev gfortran ccache
+
+pip install virtualenv
+
+if command -v pypy3; then
+    virtualenv -p $(command -v pypy3) pypy-env
+elif command -v pypy; then
+    virtualenv -p $(command -v pypy) pypy-env
+fi
+
+source pypy-env/bin/activate
+
+python --version
+which python
+
+pip install --extra-index https://antocuni.github.io/pypy-wheels/ubuntu numpy==1.14.4 Cython pytest
+pip install "scipy>=1.1.0" sphinx numpydoc docutils
+
+ccache -M 512M
+export CCACHE_COMPRESS=1
+export PATH=/usr/lib/ccache:$PATH
+
+pip install -e .
+
+make test
diff --git a/conftest.py b/conftest.py
index c2b9ae203887..621097bfc47a 100644
--- a/conftest.py
+++ b/conftest.py
@@ -5,6 +5,7 @@
 # doc/modules/clustering.rst and use sklearn from the local folder rather than
 # the one from site-packages.
 
+import platform
 from distutils.version import LooseVersion
 
 import pytest
@@ -12,6 +13,15 @@
 
 
 def pytest_collection_modifyitems(config, items):
+
+    # FeatureHasher is not compatible with PyPy
+    if platform.python_implementation() == 'PyPy':
+        skip_marker = pytest.mark.skip(
+            reason='FeatureHasher is not compatible with PyPy')
+        for item in items:
+            if item.name == 'sklearn.feature_extraction.hashing.FeatureHasher':
+                item.add_marker(skip_marker)
+
     # numpy changed the str/repr formatting of numpy arrays in 1.14. We want to
     # run doctests only for numpy >= 1.14.
     skip_doctests = True
diff --git a/doc/conftest.py b/doc/conftest.py
index 11b190d8f66f..7e229781cd32 100644
--- a/doc/conftest.py
+++ b/doc/conftest.py
@@ -1,9 +1,11 @@
+import os
 from os.path import exists
 from os.path import join
 import warnings
 
 import numpy as np
 
+from sklearn.utils import IS_PYPY
 from sklearn.utils.testing import SkipTest
 from sklearn.utils.testing import check_skip_network
 from sklearn.datasets import get_data_home
@@ -56,6 +58,8 @@ def setup_twenty_newsgroups():
 
 
 def setup_working_with_text_data():
+    if IS_PYPY and os.environ.get('CI', None):
+        raise SkipTest('Skipping too slow test with PyPy on CI')
     check_skip_network()
     cache_path = _pkl_filepath(get_data_home(), CACHE_NAME)
     if not exists(cache_path):
@@ -98,6 +102,8 @@ def pytest_runtest_setup(item):
         setup_working_with_text_data()
     elif fname.endswith('modules/compose.rst') or is_index:
         setup_compose()
+    elif IS_PYPY and fname.endswith('modules/feature_extraction.rst'):
+        raise SkipTest('FeatureHasher is not compatible with PyPy')
     elif fname.endswith('modules/impute.rst'):
         setup_impute()
     elif fname.endswith('statistical_inference/unsupervised_learning.rst'):
diff --git a/doc/developers/advanced_installation.rst b/doc/developers/advanced_installation.rst
index 19b6c2e45557..720c11ed98f4 100644
--- a/doc/developers/advanced_installation.rst
+++ b/doc/developers/advanced_installation.rst
@@ -38,6 +38,12 @@ Scikit-learn requires:
 - NumPy (>= 1.8.2),
 - SciPy (>= 0.13.3).
 
+.. note::
+
+   For installing on PyPy, PyPy3-v5.10+, Numpy 1.14.0+, and scipy 1.1.0+
+   are required. For PyPy, only installation instructions with pip apply.
+
+
 Building Scikit-learn also requires
 
 - Cython >=0.23 
diff --git a/doc/developers/contributing.rst b/doc/developers/contributing.rst
index 1b43400f4a7a..a27bae14ba25 100644
--- a/doc/developers/contributing.rst
+++ b/doc/developers/contributing.rst
@@ -352,7 +352,7 @@ and Cython optimizations.
 
    * Travis is used for testing on Linux platforms
    * Appveyor is used for testing on Windows platforms
-   * CircleCI is used to build the docs for viewing
+   * CircleCI is used to build the docs for viewing and for testing with PyPy on Linux
 
    Please note that if one of the following markers appear in the latest commit
    message, the following actions are taken.
diff --git a/doc/faq.rst b/doc/faq.rst
index 85ec39e45ba3..bef75f58e179 100644
--- a/doc/faq.rst
+++ b/doc/faq.rst
@@ -179,12 +179,10 @@ careful choice of algorithms.
 Do you support PyPy?
 --------------------
 
-In case you didn't know, `PyPy <http://pypy.org/>`_ is the new, fast,
-just-in-time compiling Python implementation. We don't support it.
-When the `NumPy support <http://buildbot.pypy.org/numpy-status/latest.html>`_
-in PyPy is complete or near-complete, and SciPy is ported over as well,
-we can start thinking of a port.
-We use too much of NumPy to work with a partial implementation.
+In case you didn't know, `PyPy <http://pypy.org/>`_ is an alternative
+Python implementation with a built-in just-in-time compiler. Experimental
+support for PyPy3-v5.10+ has been added, which requires Numpy 1.14.0+,
+and scipy 1.1.0+.
 
 How do I deal with string data (or trees, graphs...)?
 -----------------------------------------------------
diff --git a/doc/glossary.rst b/doc/glossary.rst
index 6a3780ba0c05..533a8eac63d0 100644
--- a/doc/glossary.rst
+++ b/doc/glossary.rst
@@ -1414,7 +1414,8 @@ functions or non-estimator constructors.
         - An iterable yielding train/test splits.
 
         With some exceptions (especially where not using cross validation at
-        all is an option), the default is 3-fold.
+        all is an option), the default is 3-fold and will change to 5-fold
+        in version 0.22.
 
         ``cv`` values are validated and interpreted with :func:`utils.check_cv`.
 
diff --git a/doc/install.rst b/doc/install.rst
index 89c1aca455c7..7dbb2287c406 100644
--- a/doc/install.rst
+++ b/doc/install.rst
@@ -52,6 +52,12 @@ it as ``scikit-learn[alldeps]``. The most common use case for this is in a
 application or a Docker image. This option is not intended for manual
 installation from the command line.
 
+.. note::
+
+   For installing on PyPy, PyPy3-v5.10+, Numpy 1.14.0+, and scipy 1.1.0+
+   are required.
+
+
 For installation instructions for more distributions see
 :ref:`other distributions <install_by_distribution>`.
 For compiling the development version from source, or building the package
diff --git a/doc/modules/cross_validation.rst b/doc/modules/cross_validation.rst
index c6d301f8edb6..328270b086ed 100644
--- a/doc/modules/cross_validation.rst
+++ b/doc/modules/cross_validation.rst
@@ -138,11 +138,9 @@ validation iterator instead, for instance::
 
   >>> from sklearn.model_selection import ShuffleSplit
   >>> n_samples = iris.data.shape[0]
-  >>> cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=0)
-  >>> cross_val_score(clf, iris.data, iris.target, cv=cv)
-  ...                                                     # doctest: +ELLIPSIS
-  array([0.97..., 0.97..., 1.        ])
-
+  >>> cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)
+  >>> cross_val_score(clf, iris.data, iris.target, cv=cv)  # doctest: +ELLIPSIS
+  array([0.977..., 0.977..., 1.  ..., 0.955..., 1.        ])
 
 .. topic:: Data transformation with held out data
 
@@ -168,7 +166,7 @@ validation iterator instead, for instance::
       >>> clf = make_pipeline(preprocessing.StandardScaler(), svm.SVC(C=1))
       >>> cross_val_score(clf, iris.data, iris.target, cv=cv)
       ...                                                 # doctest: +ELLIPSIS
-      array([0.97..., 0.93..., 0.95...])
+      array([0.977..., 0.933..., 0.955..., 0.933..., 0.977...])
 
     See :ref:`combining_estimators`.
 
@@ -230,7 +228,7 @@ Or as a dict mapping scorer name to a predefined or custom scoring function::
 Here is an example of ``cross_validate`` using a single metric::
 
     >>> scores = cross_validate(clf, iris.data, iris.target,
-    ...                         scoring='precision_macro',
+    ...                         scoring='precision_macro', cv=5,
     ...                         return_estimator=True)
     >>> sorted(scores.keys())
     ['estimator', 'fit_time', 'score_time', 'test_score', 'train_score']
@@ -462,15 +460,16 @@ generator.
 Here is a usage example::
 
   >>> from sklearn.model_selection import ShuffleSplit
-  >>> X = np.arange(5)
-  >>> ss = ShuffleSplit(n_splits=3, test_size=0.25,
+  >>> X = np.arange(10)
+  >>> ss = ShuffleSplit(n_splits=5, test_size=0.25,
   ...     random_state=0)
   >>> for train_index, test_index in ss.split(X):
   ...     print("%s %s" % (train_index, test_index))
-  ...
-  [1 3 4] [2 0]
-  [1 4 3] [0 2]
-  [4 0 2] [1 3]
+  [9 1 6 7 3 0 5] [2 8 4]
+  [2 9 8 0 6 7 4] [3 5 1]
+  [4 5 1 0 6 9 7] [2 3 8]
+  [2 7 5 8 0 3 4] [6 1 9]
+  [4 1 0 6 8 9 3] [5 2 7]
 
 :class:`ShuffleSplit` is thus a good alternative to :class:`KFold` cross
 validation that allows a finer control on the number of iterations and
diff --git a/doc/modules/ensemble.rst b/doc/modules/ensemble.rst
index 19e46d3eab25..85fd8a30ba03 100644
--- a/doc/modules/ensemble.rst
+++ b/doc/modules/ensemble.rst
@@ -167,19 +167,19 @@ in bias::
 
     >>> clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,
     ...     random_state=0)
-    >>> scores = cross_val_score(clf, X, y)
-    >>> scores.mean()                             # doctest: +ELLIPSIS
-    0.97...
+    >>> scores = cross_val_score(clf, X, y, cv=5)
+    >>> scores.mean()                               # doctest: +ELLIPSIS
+    0.98...
 
     >>> clf = RandomForestClassifier(n_estimators=10, max_depth=None,
     ...     min_samples_split=2, random_state=0)
-    >>> scores = cross_val_score(clf, X, y)
-    >>> scores.mean()                             # doctest: +ELLIPSIS
+    >>> scores = cross_val_score(clf, X, y, cv=5)
+    >>> scores.mean()                               # doctest: +ELLIPSIS
     0.999...
 
     >>> clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,
     ...     min_samples_split=2, random_state=0)
-    >>> scores = cross_val_score(clf, X, y)
+    >>> scores = cross_val_score(clf, X, y, cv=5)
     >>> scores.mean() > 0.999
     True
 
@@ -257,8 +257,8 @@ Feature importance evaluation
 The relative rank (i.e. depth) of a feature used as a decision node in a
 tree can be used to assess the relative importance of that feature with
 respect to the predictability of the target variable. Features used at
-the top of the tree contribute to the final prediction decision of a 
-larger fraction of the input samples. The **expected fraction of the 
+the top of the tree contribute to the final prediction decision of a
+larger fraction of the input samples. The **expected fraction of the
 samples** they contribute to can thus be used as an estimate of the
 **relative importance of the features**.
 
@@ -373,7 +373,7 @@ learners::
 
     >>> iris = load_iris()
     >>> clf = AdaBoostClassifier(n_estimators=100)
-    >>> scores = cross_val_score(clf, iris.data, iris.target)
+    >>> scores = cross_val_score(clf, iris.data, iris.target, cv=5)
     >>> scores.mean()                             # doctest: +ELLIPSIS
     0.9...
 
diff --git a/doc/modules/feature_extraction.rst b/doc/modules/feature_extraction.rst
index 68d4295bd658..97001f37fb04 100644
--- a/doc/modules/feature_extraction.rst
+++ b/doc/modules/feature_extraction.rst
@@ -509,7 +509,7 @@ v{_2}^2 + \dots + v{_n}^2}}`
 For example, we can compute the tf-idf of the first term in the first
 document in the `counts` array as follows:
 
-:math:`n_{d, {\text{term1}}} = 6`
+:math:`n_{d} = 6`
 
 :math:`\text{df}(d, t)_{\text{term1}} = 6`
 
diff --git a/doc/modules/learning_curve.rst b/doc/modules/learning_curve.rst
index 6ae5ac4a9b53..8656ee0c90a0 100644
--- a/doc/modules/learning_curve.rst
+++ b/doc/modules/learning_curve.rst
@@ -81,15 +81,16 @@ The function :func:`validation_curve` can help in this case::
   >>> X, y = X[indices], y[indices]
 
   >>> train_scores, valid_scores = validation_curve(Ridge(), X, y, "alpha",
-  ...                                               np.logspace(-7, 3, 3))
-  >>> train_scores           # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE
-  array([[0.94..., 0.92..., 0.92...],
-         [0.94..., 0.92..., 0.92...],
-         [0.47..., 0.45..., 0.42...]])
+  ...                                               np.logspace(-7, 3, 3),
+  ...                                               cv=5)
+  >>> train_scores            # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE
+  array([[0.93..., 0.94..., 0.92..., 0.91..., 0.92...],
+         [0.93..., 0.94..., 0.92..., 0.91..., 0.92...],
+         [0.51..., 0.52..., 0.49..., 0.47..., 0.49...]])
   >>> valid_scores           # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE
-  array([[0.90..., 0.92..., 0.94...],
-         [0.90..., 0.92..., 0.94...],
-         [0.44..., 0.39..., 0.45...]])
+  array([[0.90..., 0.84..., 0.94..., 0.96..., 0.93...],
+         [0.90..., 0.84..., 0.94..., 0.96..., 0.93...],
+         [0.46..., 0.25..., 0.50..., 0.49..., 0.52...]])
 
 If the training score and the validation score are both low, the estimator will
 be underfitting. If the training score is high and the validation score is low,
diff --git a/doc/modules/linear_model.rst b/doc/modules/linear_model.rst
index bb4a9e4e57f3..dada7db1095c 100644
--- a/doc/modules/linear_model.rst
+++ b/doc/modules/linear_model.rst
@@ -138,9 +138,9 @@ as GridSearchCV except that it defaults to Generalized Cross-Validation
 (GCV), an efficient form of leave-one-out cross-validation::
 
     >>> from sklearn import linear_model
-    >>> reg = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])
+    >>> reg = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0], cv=3)
     >>> reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])       # doctest: +SKIP
-    RidgeCV(alphas=[0.1, 1.0, 10.0], cv=None, fit_intercept=True, scoring=None,
+    RidgeCV(alphas=[0.1, 1.0, 10.0], cv=3, fit_intercept=True, scoring=None,
         normalize=False)
     >>> reg.alpha_                                      # doctest: +SKIP
     0.1
diff --git a/doc/modules/model_evaluation.rst b/doc/modules/model_evaluation.rst
index 7638bcd7b955..c1fd9cb5fa7f 100644
--- a/doc/modules/model_evaluation.rst
+++ b/doc/modules/model_evaluation.rst
@@ -99,10 +99,11 @@ Usage examples:
     >>> iris = datasets.load_iris()
     >>> X, y = iris.data, iris.target
     >>> clf = svm.SVC(gamma='scale', random_state=0)
-    >>> cross_val_score(clf, X, y, scoring='recall_macro') # doctest: +ELLIPSIS
-    array([0.980..., 0.960..., 0.979...])
+    >>> cross_val_score(clf, X, y, scoring='recall_macro',
+    ...                 cv=5)  # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE
+    array([0.96..., 1.  ..., 0.96..., 0.96..., 1.        ])
     >>> model = svm.SVC()
-    >>> cross_val_score(model, X, y, scoring='wrong_choice')
+    >>> cross_val_score(model, X, y, cv=5, scoring='wrong_choice')
     Traceback (most recent call last):
     ValueError: 'wrong_choice' is not a valid scoring value. Valid options are ['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'balanced_accuracy', 'brier_score_loss', 'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'mutual_info_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'v_measure_score']
 
@@ -150,7 +151,8 @@ the :func:`fbeta_score` function::
     >>> ftwo_scorer = make_scorer(fbeta_score, beta=2)
     >>> from sklearn.model_selection import GridSearchCV
     >>> from sklearn.svm import LinearSVC
-    >>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]}, scoring=ftwo_scorer)
+    >>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},
+    ...                     scoring=ftwo_scorer, cv=5)
 
 The second use case is to build a completely custom scorer object
 from a simple python function using :func:`make_scorer`, which can
@@ -250,13 +252,14 @@ permitted and will require a wrapper to return a single metric::
     >>> def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 1]
     >>> scoring = {'tp' : make_scorer(tp), 'tn' : make_scorer(tn),
     ...            'fp' : make_scorer(fp), 'fn' : make_scorer(fn)}
-    >>> cv_results = cross_validate(svm.fit(X, y), X, y, scoring=scoring)
+    >>> cv_results = cross_validate(svm.fit(X, y), X, y,
+    ...                             scoring=scoring, cv=5)
     >>> # Getting the test set true positive scores
-    >>> print(cv_results['test_tp'])          # doctest: +NORMALIZE_WHITESPACE
-    [16 14  9]
+    >>> print(cv_results['test_tp'])  # doctest: +NORMALIZE_WHITESPACE
+    [10  9  8  7  8]
     >>> # Getting the test set false negative scores
-    >>> print(cv_results['test_fn'])          # doctest: +NORMALIZE_WHITESPACE
-    [1 3 7]
+    >>> print(cv_results['test_fn'])  # doctest: +NORMALIZE_WHITESPACE
+    [0 1 2 3 2]
 
 .. _classification_metrics:
 
diff --git a/doc/modules/outlier_detection.rst b/doc/modules/outlier_detection.rst
index b287f3d7bc73..6f7f65852197 100644
--- a/doc/modules/outlier_detection.rst
+++ b/doc/modules/outlier_detection.rst
@@ -12,13 +12,27 @@ belongs to the same distribution as existing observations (it is an
 Often, this ability is used to clean real data sets. Two important
 distinction must be made:
 
-:novelty detection:
-  The training data is not polluted by outliers, and we are interested in
-  detecting anomalies in new observations.
-
 :outlier detection:
-  The training data contains outliers, and we need to fit the central
-  mode of the training data, ignoring the deviant observations.
+  The training data contains outliers which are defined as observations that
+  are far from the others. Outlier detection estimators thus try to fit the
+  regions where the training data is the most concentrated, ignoring the
+  deviant observations.
+
+:novelty detection:
+  The training data is not polluted by outliers and we are interested in
+  detecting whether a **new** observation is an outlier. In this context an
+  outlier is also called a novelty.
+
+Outlier detection and novelty detection are both used for anomaly
+detection, where one is interested in detecting abnormal or unusual
+observations. Outlier detection is then also known as unsupervised anomaly
+detection and novelty detection as semi-supervised anomaly detection. In the
+context of outlier detection, the outliers/anomalies cannot form a
+dense cluster as available estimators assume that the outliers/anomalies are
+located in low density regions. On the contrary, in the context of novelty
+detection, novelties/anomalies can form a dense cluster as long as they are in
+a low density region of the training data, considered as normal in this
+context.
 
 The scikit-learn project provides a set of machine learning tools that
 can be used both for novelty or outliers detection. This strategy is
@@ -44,19 +58,55 @@ inliers::
     estimator.decision_function(X_test)
 
 Note that :class:`neighbors.LocalOutlierFactor` does not support
-``predict`` and ``decision_function`` methods, as this algorithm is
-purely transductive and is thus not designed to deal with new data.
+``predict``, ``decision_function`` and ``score_samples`` methods by default
+but only a ``fit_predict`` method, as this estimator was originally meant to
+be applied for outlier detection. The scores of abnormality of the training
+samples are accessible through the ``negative_outlier_factor_`` attribute.
+
+If you really want to use :class:`neighbors.LocalOutlierFactor` for novelty
+detection, i.e. predict labels or compute the score of abnormality of new
+unseen data, you can instantiate the estimator with the ``novelty`` parameter
+set to ``True`` before fitting the estimator. In this case, ``fit_predict`` is
+not available.
+
+.. warning:: **Novelty detection with Local Outlier Factor**
+
+  When ``novelty`` is set to ``True`` be aware that you must only use
+  ``predict``, ``decision_function`` and ``score_samples`` on new unseen data
+  and not on the training samples as this would lead to wrong results.
+  The scores of abnormality of the training samples are always accessible
+  through the ``negative_outlier_factor_`` attribute.
+
 
 Overview of outlier detection methods
 =====================================
 
+A comparison of the outlier detection algorithms in scikit-learn. Local
+Outlier Factor (LOF) does not show a decision boundary in black as it
+has no predict method to be applied on new data when it is used for outlier
+detection.
+
 .. figure:: ../auto_examples/images/sphx_glr_plot_anomaly_comparison_001.png
    :target: ../auto_examples/plot_anomaly_comparison.html
    :align: center
    :scale: 50
 
-   A comparison of the outlier detection algorithms in scikit-learn
+:class:`ensemble.IsolationForest` and :class:`neighbors.LocalOutlierFactor`
+perform reasonably well on the data sets considered here.
+The :class:`svm.OneClassSVM` is known to be sensitive to outliers and thus
+does not perform very well for outlier detection. Finally,
+:class:`covariance.EllipticEnvelope` assumes the data is Gaussian and learns
+an ellipse. For more details on the different estimators refer to the example
+:ref:`sphx_glr_auto_examples_plot_anomaly_comparison.py` and the sections
+hereunder.
+
+.. topic:: Examples:
 
+  * See :ref:`sphx_glr_auto_examples_plot_anomaly_comparison.py`
+    for a comparison of the :class:`svm.OneClassSVM`, the
+    :class:`ensemble.IsolationForest`, the
+    :class:`neighbors.LocalOutlierFactor` and
+    :class:`covariance.EllipticEnvelope`.
 
 Novelty Detection
 =================
@@ -189,7 +239,7 @@ This strategy is illustrated below.
    * See :ref:`sphx_glr_auto_examples_ensemble_plot_isolation_forest.py` for
      an illustration of the use of IsolationForest.
 
-   * See :ref:`sphx_glr_auto_examples_covariance_plot_outlier_detection.py` for a
+   * See :ref:`sphx_glr_auto_examples_plot_anomaly_comparison.py` for a
      comparison of :class:`ensemble.IsolationForest` with
      :class:`neighbors.LocalOutlierFactor`,
      :class:`svm.OneClassSVM` (tuned to perform like an outlier detection
@@ -237,20 +287,29 @@ where abnormal samples have different underlying densities.
 The question is not, how isolated the sample is, but how isolated it is
 with respect to the surrounding neighborhood.
 
+When applying LOF for outlier detection, there are no ``predict``,
+``decision_function`` and ``score_samples`` methods but only a ``fit_predict``
+method. The scores of abnormality of the training samples are accessible
+through the ``negative_outlier_factor_`` attribute.
+Note that ``predict``, ``decision_function`` and ``score_samples`` can be used
+on new unseen data when LOF is applied for novelty detection, i.e. when the
+``novelty`` parameter is set to ``True``. See :ref:`novelty_with_lof`.
+
+
 This strategy is illustrated below.
 
-.. figure:: ../auto_examples/neighbors/images/sphx_glr_plot_lof_001.png
-   :target: ../auto_examples/neighbors/plot_lof.html
+.. figure:: ../auto_examples/neighbors/images/sphx_glr_plot_lof_outlier_detection_001.png
+   :target: ../auto_examples/neighbors/sphx_glr_plot_lof_outlier_detection.html
    :align: center
    :scale: 75%
 
 .. topic:: Examples:
 
-   * See :ref:`sphx_glr_auto_examples_neighbors_plot_lof.py` for
-     an illustration of the use of :class:`neighbors.LocalOutlierFactor`.
+   * See :ref:`sphx_glr_auto_examples_neighbors_plot_lof_outlier_detection.py`
+   for an illustration of the use of :class:`neighbors.LocalOutlierFactor`.
 
-   * See :ref:`sphx_glr_auto_examples_covariance_plot_outlier_detection.py` for a
-     comparison with other anomaly detection methods.
+   * See :ref:`sphx_glr_auto_examples_plot_anomaly_comparison.py` for a
+   comparison with other anomaly detection methods.
 
 .. topic:: References:
 
@@ -259,72 +318,45 @@ This strategy is illustrated below.
       <http://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf>`_
       Proc. ACM SIGMOD
 
-One-class SVM versus Elliptic Envelope versus Isolation Forest versus LOF
--------------------------------------------------------------------------
-
-Strictly-speaking, the One-class SVM is not an outlier-detection method,
-but a novelty-detection method: its training set should not be
-contaminated by outliers as it may fit them. That said, outlier detection
-in high-dimension, or without any assumptions on the distribution of the
-inlying data is very challenging, and a One-class SVM gives useful
-results in these situations.
-
-The examples below illustrate how the performance of the
-:class:`covariance.EllipticEnvelope` degrades as the data is less and
-less unimodal. The :class:`svm.OneClassSVM` works better on data with
-multiple modes and :class:`ensemble.IsolationForest` and
-:class:`neighbors.LocalOutlierFactor` perform well in every cases.
-
-.. |outlier1| image:: ../auto_examples/covariance/images/sphx_glr_plot_outlier_detection_001.png
-   :target: ../auto_examples/covariance/plot_outlier_detection.html
-   :scale: 50%
-
-.. |outlier2| image:: ../auto_examples/covariance/images/sphx_glr_plot_outlier_detection_002.png
-   :target: ../auto_examples/covariance/plot_outlier_detection.html
-   :scale: 50%
-
-.. |outlier3| image:: ../auto_examples/covariance/images/sphx_glr_plot_outlier_detection_003.png
-   :target: ../auto_examples/covariance/plot_outlier_detection.html
-   :scale: 50%
-
-.. list-table:: **Comparing One-class SVM, Isolation Forest, LOF, and Elliptic Envelope**
-   :widths: 40 60
-
-   *
-      - For a inlier mode well-centered and elliptic, the
-        :class:`svm.OneClassSVM` is not able to benefit from the
-        rotational symmetry of the inlier population. In addition, it
-        fits a bit the outliers present in the training set. On the
-        opposite, the decision rule based on fitting an
-        :class:`covariance.EllipticEnvelope` learns an ellipse, which
-        fits well the inlier distribution. The :class:`ensemble.IsolationForest`
-        and :class:`neighbors.LocalOutlierFactor` perform as well.
-      - |outlier1| 
-
-   *
-      - As the inlier distribution becomes bimodal, the
-        :class:`covariance.EllipticEnvelope` does not fit well the
-        inliers. However, we can see that :class:`ensemble.IsolationForest`,
-        :class:`svm.OneClassSVM` and :class:`neighbors.LocalOutlierFactor`
-        have difficulties to detect the two modes,
-        and that the :class:`svm.OneClassSVM`
-        tends to overfit: because it has no model of inliers, it
-        interprets a region where, by chance some outliers are
-        clustered, as inliers.
-      - |outlier2|
-
-   *
-      - If the inlier distribution is strongly non Gaussian, the
-        :class:`svm.OneClassSVM` is able to recover a reasonable
-        approximation as well as :class:`ensemble.IsolationForest`
-        and :class:`neighbors.LocalOutlierFactor`,
-        whereas the :class:`covariance.EllipticEnvelope` completely fails.
-      - |outlier3|
+.. _novelty_with_lof:
 
-.. topic:: Examples:
+Novelty detection with Local Outlier Factor
+===========================================
+
+To use :class:`neighbors.LocalOutlierFactor` for novelty detection, i.e.
+predict labels or compute the score of abnormality of new unseen data, you
+need to instantiate the estimator with the ``novelty`` parameter
+set to ``True`` before fitting the estimator::
+
+  lof = LocalOutlierFactor(novelty=True)
+  lof.fit(X_train)
+
+Note that ``fit_predict`` is not available in this case.
+
+.. warning:: **Novelty detection with Local Outlier Factor`**
+
+  When ``novelty`` is set to ``True`` be aware that you must only use
+  ``predict``, ``decision_function`` and ``score_samples`` on new unseen data
+  and not on the training samples as this would lead to wrong results.
+  The scores of abnormality of the training samples are always accessible
+  through the ``negative_outlier_factor_`` attribute.
+
+The behavior of LOF is summarized in the following table.
+
+====================  ================================  =====================
+Method                Outlier detection                 Novelty detection
+====================  ================================  =====================
+`fit_predict`         OK                                Not available
+`predict`             Not available                     Use only on test data
+`decision_function`   Not available                     Use only on test data
+`score_samples`       Use `negative_outlier_factor_`    Use only on test data
+====================  ================================  =====================
+
+
+This strategy is illustrated below.
+
+  .. figure:: ../auto_examples/neighbors/images/sphx_glr_plot_lof_novelty_detection_001.png
+     :target: ../auto_examples/neighbors/sphx_glr_plot_lof_novelty_detection.html
+     :align: center
+     :scale: 75%
 
-   * See :ref:`sphx_glr_auto_examples_covariance_plot_outlier_detection.py` for a
-     comparison of the :class:`svm.OneClassSVM` (tuned to perform like
-     an outlier detection method), the :class:`ensemble.IsolationForest`,
-     the :class:`neighbors.LocalOutlierFactor`
-     and a covariance-based outlier detection :class:`covariance.EllipticEnvelope`.
diff --git a/doc/modules/random_projection.rst b/doc/modules/random_projection.rst
index 5585be6f6b21..d3e2c8023089 100644
--- a/doc/modules/random_projection.rst
+++ b/doc/modules/random_projection.rst
@@ -152,11 +152,11 @@ projection transformer::
 
  * D. Achlioptas. 2003.
    `Database-friendly random projections: Johnson-Lindenstrauss  with binary
-   coins <www.cs.ucsc.edu/~optas/papers/jl.pdf>`_.
+   coins <http://www.cs.ucsc.edu/~optas/papers/jl.pdf>`_.
    Journal of Computer and System Sciences 66 (2003) 671–687
 
  * Ping Li, Trevor J. Hastie, and Kenneth W. Church. 2006.
-   `Very sparse random projections. <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.62.585&rep=rep1&type=pdf>`_
+   `Very sparse random projections. <https://web.stanford.edu/~hastie/Papers/Ping/KDD06_rp.pdf>`_
    In Proceedings of the 12th ACM SIGKDD international conference on
    Knowledge discovery and data mining (KDD '06). ACM, New York, NY, USA,
    287-296.
diff --git a/doc/tutorial/statistical_inference/model_selection.rst b/doc/tutorial/statistical_inference/model_selection.rst
index 2146c62b9af8..f9fc2e2505c5 100644
--- a/doc/tutorial/statistical_inference/model_selection.rst
+++ b/doc/tutorial/statistical_inference/model_selection.rst
@@ -60,19 +60,21 @@ of the chosen cross-validation strategy.
 This example shows an example usage of the ``split`` method.
 
     >>> from sklearn.model_selection import KFold, cross_val_score
-    >>> X = ["a", "a", "b", "c", "c", "c"]
-    >>> k_fold = KFold(n_splits=3)
+    >>> X = ["a", "a", "a", "b", "b", "c", "c", "c", "c", "c"]
+    >>> k_fold = KFold(n_splits=5)
     >>> for train_indices, test_indices in k_fold.split(X):
     ...      print('Train: %s | test: %s' % (train_indices, test_indices))
-    Train: [2 3 4 5] | test: [0 1]
-    Train: [0 1 4 5] | test: [2 3]
-    Train: [0 1 2 3] | test: [4 5]
+    Train: [2 3 4 5 6 7 8 9] | test: [0 1]
+    Train: [0 1 4 5 6 7 8 9] | test: [2 3]
+    Train: [0 1 2 3 6 7 8 9] | test: [4 5]
+    Train: [0 1 2 3 4 5 8 9] | test: [6 7]
+    Train: [0 1 2 3 4 5 6 7] | test: [8 9]
 
 The cross-validation can then be performed easily::
 
     >>> [svc.fit(X_digits[train], y_digits[train]).score(X_digits[test], y_digits[test])
     ...          for train, test in k_fold.split(X_digits)]  # doctest: +ELLIPSIS
-    [0.934..., 0.956..., 0.939...]
+    [0.963..., 0.922..., 0.963..., 0.963..., 0.930...]
 
 The cross-validation score can be directly calculated using the
 :func:`cross_val_score` helper. Given an estimator, the cross-validation object
@@ -86,7 +88,7 @@ Refer the :ref:`metrics module <metrics>` to learn more on the available scoring
 methods.
 
     >>> cross_val_score(svc, X_digits, y_digits, cv=k_fold, n_jobs=-1)
-    array([0.93489149, 0.95659432, 0.93989983])
+    array([0.96388889, 0.92222222, 0.9637883 , 0.9637883 , 0.93036212])
 
 `n_jobs=-1` means that the computation will be dispatched on all the CPUs
 of the computer.
@@ -96,7 +98,7 @@ scoring method.
 
     >>> cross_val_score(svc, X_digits, y_digits, cv=k_fold,
     ...                 scoring='precision_macro')
-    array([0.93969761, 0.95911415, 0.94041254])
+    array([0.96578289, 0.92708922, 0.96681476, 0.96362897, 0.93192644])
 
    **Cross-validation generators**
 
@@ -229,7 +231,8 @@ estimator during the construction and exposes an estimator API::
 
 By default, the :class:`GridSearchCV` uses a 3-fold cross-validation. However,
 if it detects that a classifier is passed, rather than a regressor, it uses
-a stratified 3-fold.
+a stratified 3-fold. The default will change to a 5-fold cross-validation in
+version 0.22.
 
 .. topic:: Nested cross-validation
 
@@ -260,12 +263,12 @@ scikit-learn exposes :ref:`cross_validation` estimators that set their
 parameter automatically by cross-validation::
 
     >>> from sklearn import linear_model, datasets
-    >>> lasso = linear_model.LassoCV()
+    >>> lasso = linear_model.LassoCV(cv=3)
     >>> diabetes = datasets.load_diabetes()
     >>> X_diabetes = diabetes.data
     >>> y_diabetes = diabetes.target
     >>> lasso.fit(X_diabetes, y_diabetes)
-    LassoCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,
+    LassoCV(alphas=None, copy_X=True, cv=3, eps=0.001, fit_intercept=True,
         max_iter=1000, n_alphas=100, n_jobs=1, normalize=False, positive=False,
         precompute='auto', random_state=None, selection='cyclic', tol=0.0001,
         verbose=False)
diff --git a/doc/whats_new/v0.20.rst b/doc/whats_new/v0.20.rst
index 014eefb35234..0a8c0eec1c6a 100644
--- a/doc/whats_new/v0.20.rst
+++ b/doc/whats_new/v0.20.rst
@@ -199,7 +199,7 @@ Metrics
   :func:`metrics.roc_auc_score`. :issue:`3273` by
   :user:`Alexander Niederbühl <Alexander-N>`.
 
-- Added control over the normalization in 
+- Added control over the normalization in
   :func:`metrics.normalized_mutual_information_score` and
   :func:`metrics.adjusted_mutual_information_score` via the ``average_method``
   parameter. In version 0.22, the default normalizer for each will become
@@ -338,7 +338,7 @@ Preprocessing
 - :class:`preprocessing.RobustScaler` and :func:`preprocessing.robust_scale`
   can be fitted using sparse matrices.
   :issue:`11308` by :user:`Guillaume Lemaitre <glemaitre>`.
-  
+
 Model evaluation and meta-estimators
 
 - A scorer based on :func:`metrics.brier_score_loss` is also available.
@@ -435,6 +435,12 @@ Miscellaneous
   :issue:`9101` by :user:`alex-33 <alex-33>`
   and :user:`Maskani Filali Mohamed <maskani-moh>`.
 
+- Add almost complete PyPy 3 support. Known unsupported functionalities are
+  :func:`datasets.load_svmlight_file`, :class:`feature_extraction.FeatureHasher` and
+  :class:`feature_extraction.text.HashingVectorizer`.  For running on PyPy, PyPy3-v5.10+,
+  Numpy 1.14.0+, and scipy 1.1.0+ are required.
+  :issue:`11010` by :user:`Ronan Lamy <rlamy>` and `Roman Yurchak`_.
+
 Bug fixes
 .........
 
@@ -552,12 +558,12 @@ Classifiers and regressors
   per-tree basis. The previous behavior over-weighted the Gini importance of
   features that appear in later stages. This issue only affected feature
   importances. :issue:`11176` by :user:`Gil Forsyth <gforsyth>`.
-  
-- Fixed a bug in :class:`tree.MAE` to ensure sample weights are being used 
-  during the calculation of tree MAE impurity. Previous behaviour could 
-  cause suboptimal splits to be chosen since the impurity calculation 
+
+- Fixed a bug in :class:`tree.MAE` to ensure sample weights are being used
+  during the calculation of tree MAE impurity. Previous behaviour could
+  cause suboptimal splits to be chosen since the impurity calculation
   considered all samples to be of equal weight importance.
-  :issue:`11464` by :user:`John Stott <JohnStott>`.  
+  :issue:`11464` by :user:`John Stott <JohnStott>`.
 
 Decomposition, manifold learning and clustering
 
@@ -658,6 +664,13 @@ Metrics
   :issue:`9515` by :user:`Alan Liddell <aliddell>` and
   :user:`Manh Dao <manhdao>`.
 
+Ensemble
+
+- Fix allowing to obtain deterministic with :class:`BaseBagging` estimator,
+  when comparing results generated at fit time with the one using the object
+  attributes when ``random_state`` is set. :issue:`9723` by :user:`Guillaume
+  Lemaitre <glemaitre>`.
+
 Neighbors
 
 - Fixed a bug so ``predict`` in :class:`neighbors.RadiusNeighborsRegressor` can
@@ -743,10 +756,10 @@ API changes summary
 
 Classifiers and regressors
 
-- The default value of the ``n_estimators`` parameter of 
-  :class:`ensemble.RandomForestClassifier`, :class:`ensemble.RandomForestRegressor`, 
-  :class:`ensemble.ExtraTreesClassifier`, :class:`ensemble.ExtraTreesRegressor`, 
-  and :class:`ensemble.RandomTreesEmbedding` will change from 10 in version 0.20 
+- The default value of the ``n_estimators`` parameter of
+  :class:`ensemble.RandomForestClassifier`, :class:`ensemble.RandomForestRegressor`,
+  :class:`ensemble.ExtraTreesClassifier`, :class:`ensemble.ExtraTreesRegressor`,
+  and :class:`ensemble.RandomTreesEmbedding` will change from 10 in version 0.20
   to 100 in 0.22. A FutureWarning is raised when the default value is used.
   :issue:`11542` by :user:`Anna Ayzenshtat <annaayzenshtat>`.
 
@@ -810,9 +823,9 @@ Metrics
   :issue:`9851` by :user:`Hanmin Qin <qinhanmin2014>`.
 
 - In :func:`metrics.normalized_mutual_information_score` and
-  :func:`metrics.adjusted_mutual_information_score`, 
+  :func:`metrics.adjusted_mutual_information_score`,
   warn that ``average_method``
-  will have a new default value. In version 0.22, the default normalizer for each 
+  will have a new default value. In version 0.22, the default normalizer for each
   will become the *arithmetic* mean of the entropies of each clustering. Currently,
   :func:`metrics.normalized_mutual_information_score` uses the default of
   ``average_method='geometric'``, and :func:`metrics.adjusted_mutual_information_score`
@@ -832,6 +845,15 @@ Cluster
   :class:`cluster.AgglomerativeClustering`. :issue:`9875` by :user:`Kumar Ashutosh
   <thechargedneutron>`.
 
+Ensemble
+
+- Classes derived from :class:`ensemble.BaseBagging`. The attribute
+  ``estimators_samples_`` will return a list of arrays containing the indices
+  selected for each bootstrap instead of a list of arrays containing the mask
+  of the samples selected for each bootstrap. Indices allows to repeat samples
+  while mask does not allow this functionality. :issue:`9524` by
+  :user:`Guillaume Lemaitre <glemaitre>`.
+
 Imputer
 
 - Deprecate :class:`preprocessing.Imputer` and move the corresponding module to
@@ -874,6 +896,15 @@ Outlier Detection models
   ``raw_values`` parameter is deprecated as the shifted Mahalanobis distance
   will be always returned in 0.22. :issue:`9015` by `Nicolas Goix`_.
 
+- Novelty detection with :class:`neighbors.LocalOutlierFactor`:
+  Add a ``novelty`` parameter to :class:`neighbors.LocalOutlierFactor`. When
+  ``novelty`` is set to True, :class:`neighbors.LocalOutlierFactor` can then 
+  be used for novelty detection, i.e. predict on new unseen data. Available
+  prediction methods are ``predict``, ``decision_function`` and
+  ``score_samples``. By default, ``novelty`` is set to ``False``, and only
+  the ``fit_predict`` method is avaiable.
+  By :user:`Albert Thomas <albertcthomas>`.
+
 Covariance
 
 - The :func:`covariance.graph_lasso`, :class:`covariance.GraphLasso` and
@@ -914,6 +945,13 @@ Preprocessing
   will be from ``True`` to ``False`` in 0.22.
   :issue:`10655` by :user:`Guillaume Lemaitre <glemaitre>`.
 
+Model selection
+
+- The default number of cross-validation folds ``cv`` and the default number of
+  splits ``n_splits`` in the :class:`model_selection.KFold`-like splitters will change
+  from 3 to 5 in 0.22 as 3-fold has a lot of variance.
+  :issue:`11557` by :user:`Alexandre Boucaud <aboucaud>`.
+
 Changes to estimator checks
 ---------------------------
 
@@ -931,7 +969,7 @@ These changes mostly affect library developers.
   ``check_set_params`` test which checks that ``set_params`` is equivalent to
   passing parameters in ``__init__`` and warns if it encounters parameter
   validation. :issue:`7738` by :user:`Alvin Chiang <absolutelyNoWarranty>`
-  
+
 - Add invariance tests for clustering metrics. :issue:`8102` by :user:`Ankita
   Sinha <anki08>` and :user:`Guillaume Lemaitre <glemaitre>`.
 
diff --git a/examples/compose/plot_compare_reduction.py b/examples/compose/plot_compare_reduction.py
index 89d4d417290e..1eca8e40a072 100755
--- a/examples/compose/plot_compare_reduction.py
+++ b/examples/compose/plot_compare_reduction.py
@@ -63,7 +63,7 @@
 ]
 reducer_labels = ['PCA', 'NMF', 'KBest(chi2)']
 
-grid = GridSearchCV(pipe, cv=3, n_jobs=1, param_grid=param_grid)
+grid = GridSearchCV(pipe, cv=5, n_jobs=1, param_grid=param_grid)
 digits = load_digits()
 grid.fit(digits.data, digits.target)
 
@@ -114,7 +114,7 @@
                        memory=memory)
 
 # This time, a cached pipeline will be used within the grid search
-grid = GridSearchCV(cached_pipe, cv=3, n_jobs=1, param_grid=param_grid)
+grid = GridSearchCV(cached_pipe, cv=5, n_jobs=1, param_grid=param_grid)
 digits = load_digits()
 grid.fit(digits.data, digits.target)
 
diff --git a/examples/covariance/plot_outlier_detection.py b/examples/covariance/plot_outlier_detection.py
deleted file mode 100644
index 4c6ea43418b8..000000000000
--- a/examples/covariance/plot_outlier_detection.py
+++ /dev/null
@@ -1,129 +0,0 @@
-"""
-==========================================
-Outlier detection with several methods.
-==========================================
-
-When the amount of contamination is known, this example illustrates three
-different ways of performing :ref:`outlier_detection`:
-
-- based on a robust estimator of covariance, which is assuming that the
-  data are Gaussian distributed and performs better than the One-Class SVM
-  in that case.
-
-- using the One-Class SVM and its ability to capture the shape of the
-  data set, hence performing better when the data is strongly
-  non-Gaussian, i.e. with two well-separated clusters;
-
-- using the Isolation Forest algorithm, which is based on random forests and
-  hence more adapted to large-dimensional settings, even if it performs
-  quite well in the examples below.
-
-- using the Local Outlier Factor to measure the local deviation of a given
-  data point with respect to its neighbors by comparing their local density.
-
-The ground truth about inliers and outliers is given by the points colors
-while the orange-filled area indicates which points are reported as inliers
-by each method.
-
-Here, we assume that we know the fraction of outliers in the datasets.
-Thus rather than using the 'predict' method of the objects, we set the
-threshold on the decision_function to separate out the corresponding
-fraction.
-"""
-
-import numpy as np
-from scipy import stats
-import matplotlib.pyplot as plt
-import matplotlib.font_manager
-
-from sklearn import svm
-from sklearn.covariance import EllipticEnvelope
-from sklearn.ensemble import IsolationForest
-from sklearn.neighbors import LocalOutlierFactor
-
-print(__doc__)
-
-SEED = 42
-GRID_PRECISION = 100
-
-rng = np.random.RandomState(SEED)
-
-# Example settings
-n_samples = 200
-outliers_fraction = 0.25
-clusters_separation = (0, 1, 2)
-
-# define two outlier detection tools to be compared
-classifiers = {
-    "One-Class SVM": svm.OneClassSVM(nu=0.95 * outliers_fraction + 0.05,
-                                     kernel="rbf", gamma=0.1),
-    "Robust covariance": EllipticEnvelope(contamination=outliers_fraction),
-    "Isolation Forest": IsolationForest(max_samples=n_samples,
-                                        contamination=outliers_fraction,
-                                        random_state=rng),
-    "Local Outlier Factor": LocalOutlierFactor(
-        n_neighbors=35,
-        contamination=outliers_fraction)}
-
-# Compare given classifiers under given settings
-xx, yy = np.meshgrid(np.linspace(-7, 7, GRID_PRECISION),
-                     np.linspace(-7, 7, GRID_PRECISION))
-n_outliers = int(outliers_fraction * n_samples)
-n_inliers = n_samples - n_outliers
-ground_truth = np.ones(n_samples, dtype=int)
-ground_truth[-n_outliers:] = -1
-
-# Fit the problem with varying cluster separation
-for _, offset in enumerate(clusters_separation):
-    np.random.seed(SEED)
-    # Data generation
-    X1 = 0.3 * np.random.randn(n_inliers // 2, 2) - offset
-    X2 = 0.3 * np.random.randn(n_inliers // 2, 2) + offset
-    X = np.concatenate([X1, X2], axis=0)
-    # Add outliers
-    X = np.concatenate([X, np.random.uniform(low=-6, high=6,
-                       size=(n_outliers, 2))], axis=0)
-
-    # Fit the model
-    plt.figure(figsize=(9, 7))
-    for i, (clf_name, clf) in enumerate(classifiers.items()):
-        # fit the data and tag outliers
-        if clf_name == "Local Outlier Factor":
-            y_pred = clf.fit_predict(X)
-            scores_pred = clf.negative_outlier_factor_
-        else:
-            clf.fit(X)
-            scores_pred = clf.decision_function(X)
-            y_pred = clf.predict(X)
-        n_errors = (y_pred != ground_truth).sum()
-        # plot the levels lines and the points
-        if clf_name == "Local Outlier Factor":
-            # decision_function is private for LOF
-            Z = clf._decision_function(np.c_[xx.ravel(), yy.ravel()])
-        else:
-            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
-        Z = Z.reshape(xx.shape)
-        subplot = plt.subplot(2, 2, i + 1)
-        subplot.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7),
-                         cmap=plt.cm.Blues_r)
-        a = subplot.contour(xx, yy, Z, levels=[0],
-                            linewidths=2, colors='red')
-        subplot.contourf(xx, yy, Z, levels=[0, Z.max()],
-                         colors='orange')
-        b = subplot.scatter(X[:-n_outliers, 0], X[:-n_outliers, 1], c='white',
-                            s=20, edgecolor='k')
-        c = subplot.scatter(X[-n_outliers:, 0], X[-n_outliers:, 1], c='black',
-                            s=20, edgecolor='k')
-        subplot.axis('tight')
-        subplot.legend(
-            [a.collections[0], b, c],
-            ['learned decision function', 'true inliers', 'true outliers'],
-            prop=matplotlib.font_manager.FontProperties(size=10),
-            loc='lower right')
-        subplot.set_xlabel("%d. %s (errors: %d)" % (i + 1, clf_name, n_errors))
-        subplot.set_xlim((-7, 7))
-        subplot.set_ylim((-7, 7))
-    plt.subplots_adjust(0.04, 0.1, 0.96, 0.94, 0.1, 0.26)
-    plt.suptitle("Outlier detection")
-
-plt.show()
diff --git a/examples/ensemble/plot_gradient_boosting_oob.py b/examples/ensemble/plot_gradient_boosting_oob.py
index dfae1ad9b8a9..ea38b326ce5c 100644
--- a/examples/ensemble/plot_gradient_boosting_oob.py
+++ b/examples/ensemble/plot_gradient_boosting_oob.py
@@ -74,7 +74,7 @@ def heldout_score(clf, X_test, y_test):
     return score
 
 
-def cv_estimate(n_splits=3):
+def cv_estimate(n_splits=None):
     cv = KFold(n_splits=n_splits)
     cv_clf = ensemble.GradientBoostingClassifier(**params)
     val_scores = np.zeros((n_estimators,), dtype=np.float64)
diff --git a/examples/neighbors/plot_lof.py b/examples/neighbors/plot_lof.py
deleted file mode 100644
index f48e6b619694..000000000000
--- a/examples/neighbors/plot_lof.py
+++ /dev/null
@@ -1,59 +0,0 @@
-"""
-=================================================
-Anomaly detection with Local Outlier Factor (LOF)
-=================================================
-
-This example presents the Local Outlier Factor (LOF) estimator. The LOF
-algorithm is an unsupervised outlier detection method which computes the local
-density deviation of a given data point with respect to its neighbors.
-It considers as outlier samples that have a substantially lower density than
-their neighbors.
-
-The number of neighbors considered, (parameter n_neighbors) is typically
-chosen 1) greater than the minimum number of objects a cluster has to contain,
-so that other objects can be local outliers relative to this cluster, and 2)
-smaller than the maximum number of close by objects that can potentially be
-local outliers.
-In practice, such informations are generally not available, and taking
-n_neighbors=20 appears to work well in general.
-"""
-print(__doc__)
-
-import numpy as np
-import matplotlib.pyplot as plt
-from sklearn.neighbors import LocalOutlierFactor
-
-np.random.seed(42)
-
-# Generate train data
-X_inliers = 0.3 * np.random.randn(100, 2)
-X_inliers = np.r_[X_inliers + 2, X_inliers - 2]
-
-# Generate some abnormal novel observations
-X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))
-X = np.r_[X_inliers, X_outliers]
-
-# fit the model
-clf = LocalOutlierFactor(n_neighbors=20, contamination='auto')
-y_pred = clf.fit_predict(X)
-
-# plot the level sets of the decision function
-xx, yy = np.meshgrid(np.linspace(-5, 5, 50), np.linspace(-5, 5, 50))
-Z = clf._decision_function(np.c_[xx.ravel(), yy.ravel()])
-Z = Z.reshape(xx.shape)
-
-plt.title("Local Outlier Factor (LOF)")
-plt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)
-
-a = plt.scatter(X_inliers[:, 0], X_inliers[:, 1], c='white',
-                edgecolor='k', s=20)
-b = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='red',
-                edgecolor='k', s=20)
-plt.axis('tight')
-plt.xlim((-5, 5))
-plt.ylim((-5, 5))
-plt.legend([a, b],
-           ["normal observations",
-            "abnormal observations"],
-           loc="upper left")
-plt.show()
diff --git a/examples/neighbors/plot_lof_novelty_detection.py b/examples/neighbors/plot_lof_novelty_detection.py
new file mode 100644
index 000000000000..71c0736a256a
--- /dev/null
+++ b/examples/neighbors/plot_lof_novelty_detection.py
@@ -0,0 +1,83 @@
+"""
+=================================================
+Novelty detection with Local Outlier Factor (LOF)
+=================================================
+
+The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection
+method which computes the local density deviation of a given data point with
+respect to its neighbors. It considers as outliers the samples that have a
+substantially lower density than their neighbors. This example shows how to
+use LOF for novelty detection. Note that when LOF is used for novelty
+detection you MUST not use predict, decision_function and score_samples on the
+training set as this would lead to wrong results. You must only use these
+methods on new unseen data (which are not in the training set). See
+:ref:`User Guide <outlier_detection>`: for details on the difference between
+outlier detection and novelty detection and how to use LOF for outlier
+detection.
+
+The number of neighbors considered, (parameter n_neighbors) is typically
+set 1) greater than the minimum number of samples a cluster has to contain,
+so that other samples can be local outliers relative to this cluster, and 2)
+smaller than the maximum number of close by samples that can potentially be
+local outliers.
+In practice, such informations are generally not available, and taking
+n_neighbors=20 appears to work well in general.
+"""
+
+import numpy as np
+import matplotlib
+import matplotlib.pyplot as plt
+from sklearn.neighbors import LocalOutlierFactor
+
+print(__doc__)
+
+np.random.seed(42)
+
+xx, yy = np.meshgrid(np.linspace(-5, 5, 500), np.linspace(-5, 5, 500))
+# Generate normal (not abnormal) training observations
+X = 0.3 * np.random.randn(100, 2)
+X_train = np.r_[X + 2, X - 2]
+# Generate new normal (not abnormal) observations
+X = 0.3 * np.random.randn(20, 2)
+X_test = np.r_[X + 2, X - 2]
+# Generate some abnormal novel observations
+X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))
+
+# fit the model for novelty detection (novelty=True)
+clf = LocalOutlierFactor(n_neighbors=20, novelty=True, contamination=0.1)
+clf.fit(X_train)
+# DO NOT use predict, decision_function and score_samples on X_train as this
+# would give wrong results but only on new unseen data (not used in X_train),
+# e.g. X_test, X_outliers or the meshgrid
+y_pred_test = clf.predict(X_test)
+y_pred_outliers = clf.predict(X_outliers)
+n_error_test = y_pred_test[y_pred_test == -1].size
+n_error_outliers = y_pred_outliers[y_pred_outliers == 1].size
+
+# plot the learned frontier, the points, and the nearest vectors to the plane
+Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
+Z = Z.reshape(xx.shape)
+
+plt.title("Novelty Detection with LOF")
+plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu)
+a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')
+plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='palevioletred')
+
+s = 40
+b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white', s=s, edgecolors='k')
+b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='blueviolet', s=s,
+                 edgecolors='k')
+c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='gold', s=s,
+                edgecolors='k')
+plt.axis('tight')
+plt.xlim((-5, 5))
+plt.ylim((-5, 5))
+plt.legend([a.collections[0], b1, b2, c],
+           ["learned frontier", "training observations",
+            "new regular observations", "new abnormal observations"],
+           loc="upper left",
+           prop=matplotlib.font_manager.FontProperties(size=11))
+plt.xlabel(
+    "errors novel regular: %d/40 ; errors novel abnormal: %d/40"
+    % (n_error_test, n_error_outliers))
+plt.show()
diff --git a/examples/neighbors/plot_lof_outlier_detection.py b/examples/neighbors/plot_lof_outlier_detection.py
new file mode 100644
index 000000000000..6f0e5bb490b9
--- /dev/null
+++ b/examples/neighbors/plot_lof_outlier_detection.py
@@ -0,0 +1,68 @@
+"""
+=================================================
+Outlier detection with Local Outlier Factor (LOF)
+=================================================
+
+The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection
+method which computes the local density deviation of a given data point with
+respect to its neighbors. It considers as outliers the samples that have a
+substantially lower density than their neighbors. This example shows how to
+use LOF for outlier detection which is the default use case of this estimator
+in scikit-learn. Note that when LOF is used for outlier detection it has no
+predict, decision_function and score_samples methods. See
+:ref:`User Guide <outlier_detection>`: for details on the difference between
+outlier detection and novelty detection and how to use LOF for novelty
+detection.
+
+The number of neighbors considered (parameter n_neighbors) is typically
+set 1) greater than the minimum number of samples a cluster has to contain,
+so that other samples can be local outliers relative to this cluster, and 2)
+smaller than the maximum number of close by samples that can potentially be
+local outliers.
+In practice, such informations are generally not available, and taking
+n_neighbors=20 appears to work well in general.
+"""
+
+import numpy as np
+import matplotlib.pyplot as plt
+from sklearn.neighbors import LocalOutlierFactor
+
+print(__doc__)
+
+np.random.seed(42)
+
+# Generate train data
+X_inliers = 0.3 * np.random.randn(100, 2)
+X_inliers = np.r_[X_inliers + 2, X_inliers - 2]
+
+# Generate some outliers
+X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))
+X = np.r_[X_inliers, X_outliers]
+
+n_outliers = len(X_outliers)
+ground_truth = np.ones(len(X), dtype=int)
+ground_truth[-n_outliers:] = -1
+
+# fit the model for outlier detection (default)
+clf = LocalOutlierFactor(n_neighbors=20, contamination=0.1)
+# use fit_predict to compute the predicted labels of the training samples
+# (when LOF is used for outlier detection, the estimator has no predict,
+# decision_function and score_samples methods).
+y_pred = clf.fit_predict(X)
+n_errors = (y_pred != ground_truth).sum()
+X_scores = clf.negative_outlier_factor_
+
+plt.title("Local Outlier Factor (LOF)")
+plt.scatter(X[:, 0], X[:, 1], color='k', s=3., label='Data points')
+# plot circles with radius proportional to the outlier scores
+radius = (X_scores.max() - X_scores) / (X_scores.max() - X_scores.min())
+plt.scatter(X[:, 0], X[:, 1], s=1000 * radius, edgecolors='r',
+            facecolors='none', label='Outlier scores')
+plt.axis('tight')
+plt.xlim((-5, 5))
+plt.ylim((-5, 5))
+plt.xlabel("prediction errors: %d" % (n_errors))
+legend = plt.legend(loc='upper left')
+legend.legendHandles[0]._sizes = [10]
+legend.legendHandles[1]._sizes = [20]
+plt.show()
diff --git a/examples/plot_anomaly_comparison.py b/examples/plot_anomaly_comparison.py
index 2248d9a91cd7..201c466db71d 100644
--- a/examples/plot_anomaly_comparison.py
+++ b/examples/plot_anomaly_comparison.py
@@ -10,10 +10,36 @@
 For each dataset, 15% of samples are generated as random uniform noise. This
 proportion is the value given to the nu parameter of the OneClassSVM and the
 contamination parameter of the other outlier detection algorithms.
-Decision boundaries between inliers and outliers are displayed in black.
-
-Local Outlier Factor (LOF) does not show a decision boundary in black as it
-has no predict method to be applied on new data.
+Decision boundaries between inliers and outliers are displayed in black
+except for Local Outlier Factor (LOF) as it has no predict method to be applied
+on new data when it is used for outlier detection.
+
+The :class:`svm.OneClassSVM` is known to be sensitive to outliers and thus does
+not perform very well for outlier detection. This estimator is best suited for
+novelty detection when the training set is not contaminated by outliers.
+That said, outlier detection in high-dimension, or without any assumptions on
+the distribution of the inlying data is very challenging, and a One-class SVM
+might give useful results in these situations depending on the value of its
+hyperparameters.
+
+:class:`covariance.EllipticEnvelope` assumes the data is Gaussian and learns
+an ellipse. It thus degrades when the data is not unimodal. Notice however
+that this estimator is robust to outliers.
+
+:class:`ensemble.IsolationForest` and :class:`neighbors.LocalOutlierFactor`
+seem to perform reasonably well for multi-modal data sets. The advantage of
+:class:`neighbors.LocalOutlierFactor` over the other estimators is shown for
+the third data set, where the two modes have different densities. This
+advantage is explained by the local aspect of LOF, meaning that it only
+compares the score of abnormality of one sample with the scores of its
+neighbors.
+
+Finally, for the last data set, it is hard to say that one sample is more
+abnormal than another sample as they are uniformly distributed in a
+hypercube. Except for the :class:`svm.OneClassSVM` which overfits a little, all
+estimators present decent solutions for this situation. In such a case, it
+would be wise to look more closely at the scores of abnormality of the samples
+as a good estimator should assign similar scores to all the samples.
 
 While these examples give some intuition about the algorithms, this
 intuition might not apply to very high dimensional data.
@@ -64,6 +90,8 @@
 datasets = [
     make_blobs(centers=[[0, 0], [0, 0]], cluster_std=0.5,
                **blobs_params)[0],
+    make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[0.5, 0.5],
+               **blobs_params)[0],
     make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[1.5, .3],
                **blobs_params)[0],
     4. * (make_moons(n_samples=n_samples, noise=.05, random_state=0)[0] -
diff --git a/setup.py b/setup.py
index 206cd645afec..530ec899dc40 100755
--- a/setup.py
+++ b/setup.py
@@ -3,10 +3,10 @@
 # Copyright (C) 2007-2009 Cournapeau David <cournape@gmail.com>
 #               2010 Fabian Pedregosa <fabian.pedregosa@inria.fr>
 # License: 3-clause BSD
-descr = """A set of python modules for machine learning and data mining"""
 
 import sys
 import os
+import platform
 import shutil
 from distutils.command.clean import clean as Clean
 from pkg_resources import parse_version
@@ -41,8 +41,12 @@
 
 VERSION = sklearn.__version__
 
-SCIPY_MIN_VERSION = '0.13.3'
-NUMPY_MIN_VERSION = '1.8.2'
+if platform.python_implementation() == 'PyPy':
+    SCIPY_MIN_VERSION = '1.1.0'
+    NUMPY_MIN_VERSION = '1.14.0'
+else:
+    SCIPY_MIN_VERSION = '0.13.3'
+    NUMPY_MIN_VERSION = '1.8.2'
 
 
 # Optional setuptools features
@@ -185,6 +189,10 @@ def setup_package():
                                  'Programming Language :: Python :: 3.4',
                                  'Programming Language :: Python :: 3.5',
                                  'Programming Language :: Python :: 3.6',
+                                 ('Programming Language :: Python :: '
+                                  'Implementation :: CPython'),
+                                 ('Programming Language :: Python :: '
+                                  'Implementation :: PyPy')
                                  ],
                     cmdclass=cmdclass,
                     install_requires=[
diff --git a/sklearn/calibration.py b/sklearn/calibration.py
index 4bc376cc506a..ba7dcd0cb54f 100644
--- a/sklearn/calibration.py
+++ b/sklearn/calibration.py
@@ -75,6 +75,10 @@ class CalibratedClassifierCV(BaseEstimator, ClassifierMixin):
         If "prefit" is passed, it is assumed that base_estimator has been
         fitted already and all data is used for calibration.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     Attributes
     ----------
     classes_ : array, shape (n_classes)
@@ -99,7 +103,7 @@ class CalibratedClassifierCV(BaseEstimator, ClassifierMixin):
     .. [4] Predicting Good Probabilities with Supervised Learning,
            A. Niculescu-Mizil & R. Caruana, ICML 2005
     """
-    def __init__(self, base_estimator=None, method='sigmoid', cv=3):
+    def __init__(self, base_estimator=None, method='sigmoid', cv='warn'):
         self.base_estimator = base_estimator
         self.method = method
         self.cv = cv
diff --git a/sklearn/compose/tests/test_column_transformer.py b/sklearn/compose/tests/test_column_transformer.py
index c1363992c0da..02a3b21d4c72 100644
--- a/sklearn/compose/tests/test_column_transformer.py
+++ b/sklearn/compose/tests/test_column_transformer.py
@@ -794,6 +794,11 @@ def test_column_transformer_no_estimators():
     assert ct.transformers_[-1][2] == [0, 1, 2]
 
 
+def test_column_transformer_no_estimators_set_params():
+    ct = ColumnTransformer([]).set_params(n_jobs=2)
+    assert ct.n_jobs == 2
+
+
 def test_column_transformer_callable_specifier():
     # assert that function gets the full array / dataframe
     X_array = np.array([[0, 1, 2], [2, 4, 6]]).T
diff --git a/sklearn/covariance/graph_lasso_.py b/sklearn/covariance/graph_lasso_.py
index 2ed9e86c7829..0837acf4a364 100644
--- a/sklearn/covariance/graph_lasso_.py
+++ b/sklearn/covariance/graph_lasso_.py
@@ -497,6 +497,10 @@ class GraphicalLassoCV(GraphicalLasso):
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     tol : positive float, optional
         The tolerance to declare convergence: if the dual gap goes below
         this value, iterations are stopped.
@@ -566,7 +570,7 @@ class GraphicalLassoCV(GraphicalLasso):
     be close to these missing values.
     """
 
-    def __init__(self, alphas=4, n_refinements=4, cv=None, tol=1e-4,
+    def __init__(self, alphas=4, n_refinements=4, cv='warn', tol=1e-4,
                  enet_tol=1e-4, max_iter=100, mode='cd', n_jobs=1,
                  verbose=False, assume_centered=False):
         super(GraphicalLassoCV, self).__init__(
@@ -900,6 +904,10 @@ class GraphLassoCV(GraphicalLassoCV):
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     tol : positive float, optional
         The tolerance to declare convergence: if the dual gap goes below
         this value, iterations are stopped.
diff --git a/sklearn/covariance/tests/test_graph_lasso.py b/sklearn/covariance/tests/test_graph_lasso.py
index 69dee9a883fe..8c0753636361 100644
--- a/sklearn/covariance/tests/test_graph_lasso.py
+++ b/sklearn/covariance/tests/test_graph_lasso.py
@@ -2,6 +2,8 @@
 """
 import sys
 
+import pytest
+
 import numpy as np
 from scipy import linalg
 
@@ -118,6 +120,7 @@ def test_graph_lasso_iris_singular():
 
 
 @ignore_warnings(category=DeprecationWarning)
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_graph_lasso_cv(random_state=1):
     # Sample data from a sparse multivariate normal
     dim = 5
@@ -141,6 +144,7 @@ def test_graph_lasso_cv(random_state=1):
 
 
 @ignore_warnings(category=DeprecationWarning)
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_deprecated_grid_scores(random_state=1):
     dim = 5
     n_samples = 6
diff --git a/sklearn/covariance/tests/test_graphical_lasso.py b/sklearn/covariance/tests/test_graphical_lasso.py
index 2c1b604e5cf7..f1d6aab6a9b2 100644
--- a/sklearn/covariance/tests/test_graphical_lasso.py
+++ b/sklearn/covariance/tests/test_graphical_lasso.py
@@ -116,6 +116,7 @@ def test_graphical_lasso_iris_singular():
         assert_array_almost_equal(icov, icov_R, decimal=5)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_graphical_lasso_cv(random_state=1):
     # Sample data from a sparse multivariate normal
     dim = 5
@@ -138,6 +139,7 @@ def test_graphical_lasso_cv(random_state=1):
     GraphicalLassoCV(alphas=[0.8, 0.5], tol=1e-1, n_jobs=1).fit(X)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 @pytest.mark.skipif(not PY3_OR_LATER,
                     reason='On Python 2 DeprecationWarning is not issued for some unkown reason.')
 def test_deprecated_grid_scores(random_state=1):
diff --git a/sklearn/datasets/setup.py b/sklearn/datasets/setup.py
index a1def76c1bfc..3a8936bedffe 100644
--- a/sklearn/datasets/setup.py
+++ b/sklearn/datasets/setup.py
@@ -1,6 +1,7 @@
 
 import numpy
 import os
+import platform
 
 
 def configuration(parent_package='', top_path=None):
@@ -10,9 +11,10 @@ def configuration(parent_package='', top_path=None):
     config.add_data_dir('descr')
     config.add_data_dir('images')
     config.add_data_dir(os.path.join('tests', 'data'))
-    config.add_extension('_svmlight_format',
-                         sources=['_svmlight_format.pyx'],
-                         include_dirs=[numpy.get_include()])
+    if platform.python_implementation() != 'PyPy':
+        config.add_extension('_svmlight_format',
+                             sources=['_svmlight_format.pyx'],
+                             include_dirs=[numpy.get_include()])
     config.add_subpackage('tests')
     return config
 
diff --git a/sklearn/datasets/svmlight_format.py b/sklearn/datasets/svmlight_format.py
index 357b257e542b..42de5943b6d5 100644
--- a/sklearn/datasets/svmlight_format.py
+++ b/sklearn/datasets/svmlight_format.py
@@ -22,12 +22,21 @@
 import numpy as np
 import scipy.sparse as sp
 
-from ._svmlight_format import _load_svmlight_file
 from .. import __version__
 from ..externals import six
 from ..externals.six import u, b
 from ..externals.six.moves import range, zip
-from ..utils import check_array
+from ..utils import check_array, IS_PYPY
+
+if not IS_PYPY:
+    from ._svmlight_format import _load_svmlight_file
+else:
+    def _load_svmlight_file(*args, **kwargs):
+        raise NotImplementedError(
+                'load_svmlight_file is currently not '
+                'compatible with PyPy (see '
+                'https://github.com/scikit-learn/scikit-learn/issues/11543 '
+                'for the status updates).')
 
 
 def load_svmlight_file(f, n_features=None, dtype=np.float64,
diff --git a/sklearn/datasets/tests/test_svmlight_format.py b/sklearn/datasets/tests/test_svmlight_format.py
index 3eab1d7c37eb..ca1f7ddae8ec 100644
--- a/sklearn/datasets/tests/test_svmlight_format.py
+++ b/sklearn/datasets/tests/test_svmlight_format.py
@@ -18,6 +18,7 @@
 from sklearn.utils.testing import assert_raises
 from sklearn.utils.testing import assert_raises_regex
 from sklearn.utils.testing import assert_in
+from sklearn.utils.testing import fails_if_pypy
 from sklearn.utils.fixes import sp_version
 
 import sklearn
@@ -30,6 +31,8 @@
 invalidfile = os.path.join(currdir, "data", "svmlight_invalid.txt")
 invalidfile2 = os.path.join(currdir, "data", "svmlight_invalid_order.txt")
 
+pytestmark = fails_if_pypy
+
 
 def test_load_svmlight_file():
     X, y = load_svmlight_file(datafile)
@@ -119,7 +122,8 @@ def test_load_compressed():
     with NamedTemporaryFile(prefix="sklearn-test", suffix=".gz") as tmp:
         tmp.close()  # necessary under windows
         with open(datafile, "rb") as f:
-            shutil.copyfileobj(f, gzip.open(tmp.name, "wb"))
+            with gzip.open(tmp.name, "wb") as fh_out:
+                shutil.copyfileobj(f, fh_out)
         Xgz, ygz = load_svmlight_file(tmp.name)
         # because we "close" it manually and write to it,
         # we need to remove it manually.
@@ -130,7 +134,8 @@ def test_load_compressed():
     with NamedTemporaryFile(prefix="sklearn-test", suffix=".bz2") as tmp:
         tmp.close()  # necessary under windows
         with open(datafile, "rb") as f:
-            shutil.copyfileobj(f, BZ2File(tmp.name, "wb"))
+            with BZ2File(tmp.name, "wb") as fh_out:
+                shutil.copyfileobj(f, fh_out)
         Xbz, ybz = load_svmlight_file(tmp.name)
         # because we "close" it manually and write to it,
         # we need to remove it manually.
diff --git a/sklearn/ensemble/bagging.py b/sklearn/ensemble/bagging.py
index 777e25edec06..f82221c959b2 100644
--- a/sklearn/ensemble/bagging.py
+++ b/sklearn/ensemble/bagging.py
@@ -110,7 +110,6 @@ def _parallel_build_estimators(n_estimators, ensemble, X, y, sample_weight,
 
             estimator.fit(X[:, features], y, sample_weight=curr_sample_weight)
 
-        # Draw samples, using a mask, and then fit
         else:
             estimator.fit((X[indices])[:, features], y[indices])
 
@@ -412,7 +411,7 @@ def _get_estimators_indices(self):
     def estimators_samples_(self):
         """The subset of drawn samples for each base estimator.
 
-        Returns a dynamically generated list of boolean masks identifying
+        Returns a dynamically generated list of indices identifying
         the samples used for fitting each member of the ensemble, i.e.,
         the in-bag samples.
 
@@ -420,12 +419,8 @@ def estimators_samples_(self):
         to reduce the object memory footprint by not storing the sampling
         data. Thus fetching the property may be slower than expected.
         """
-        sample_masks = []
-        for _, sample_indices in self._get_estimators_indices():
-            mask = indices_to_mask(sample_indices, self._n_samples)
-            sample_masks.append(mask)
-
-        return sample_masks
+        return [sample_indices
+                for _, sample_indices in self._get_estimators_indices()]
 
 
 class BaggingClassifier(BaseBagging, ClassifierMixin):
@@ -512,7 +507,7 @@ class BaggingClassifier(BaseBagging, ClassifierMixin):
 
     estimators_samples_ : list of arrays
         The subset of drawn samples (i.e., the in-bag samples) for each base
-        estimator. Each subset is defined by a boolean mask.
+        estimator. Each subset is defined by an array of the indices selected.
 
     estimators_features_ : list of arrays
         The subset of drawn features for each base estimator.
@@ -590,7 +585,7 @@ def _set_oob_score(self, X, y):
                                                 self.estimators_samples_,
                                                 self.estimators_features_):
             # Create mask for OOB samples
-            mask = ~samples
+            mask = ~indices_to_mask(samples, n_samples)
 
             if hasattr(estimator, "predict_proba"):
                 predictions[mask, :] += estimator.predict_proba(
@@ -885,7 +880,7 @@ class BaggingRegressor(BaseBagging, RegressorMixin):
 
     estimators_samples_ : list of arrays
         The subset of drawn samples (i.e., the in-bag samples) for each base
-        estimator. Each subset is defined by a boolean mask.
+        estimator. Each subset is defined by an array of the indices selected.
 
     estimators_features_ : list of arrays
         The subset of drawn features for each base estimator.
@@ -996,7 +991,7 @@ def _set_oob_score(self, X, y):
                                                 self.estimators_samples_,
                                                 self.estimators_features_):
             # Create mask for OOB samples
-            mask = ~samples
+            mask = ~indices_to_mask(samples, n_samples)
 
             predictions[mask] += estimator.predict((X[mask, :])[:, features])
             n_predictions[mask] += 1
diff --git a/sklearn/ensemble/gradient_boosting.py b/sklearn/ensemble/gradient_boosting.py
index f32ac7699035..ad25a1965b26 100644
--- a/sklearn/ensemble/gradient_boosting.py
+++ b/sklearn/ensemble/gradient_boosting.py
@@ -1336,12 +1336,14 @@ def _resize_state(self):
             raise ValueError('resize with smaller n_estimators %d < %d' %
                              (total_n_estimators, self.estimators_[0]))
 
-        self.estimators_.resize((total_n_estimators, self.loss_.K))
-        self.train_score_.resize(total_n_estimators)
+        self.estimators_ = np.resize(self.estimators_,
+                                     (total_n_estimators, self.loss_.K))
+        self.train_score_ = np.resize(self.train_score_, total_n_estimators)
         if (self.subsample < 1 or hasattr(self, 'oob_improvement_')):
             # if do oob resize arrays or create new if not available
             if hasattr(self, 'oob_improvement_'):
-                self.oob_improvement_.resize(total_n_estimators)
+                self.oob_improvement_ = np.resize(self.oob_improvement_,
+                                                  total_n_estimators)
             else:
                 self.oob_improvement_ = np.zeros((total_n_estimators,),
                                                  dtype=np.float64)
diff --git a/sklearn/ensemble/tests/test_bagging.py b/sklearn/ensemble/tests/test_bagging.py
index 396bda20159f..505ec2f17b24 100644
--- a/sklearn/ensemble/tests/test_bagging.py
+++ b/sklearn/ensemble/tests/test_bagging.py
@@ -29,11 +29,12 @@
 from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
 from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
 from sklearn.svm import SVC, SVR
+from sklearn.random_projection import SparseRandomProjection
 from sklearn.pipeline import make_pipeline
 from sklearn.feature_selection import SelectKBest
 from sklearn.model_selection import train_test_split
 from sklearn.datasets import load_boston, load_iris, make_hastie_10_2
-from sklearn.utils import check_random_state
+from sklearn.utils import check_random_state, hash
 from sklearn.preprocessing import FunctionTransformer
 
 from scipy.sparse import csc_matrix, csr_matrix
@@ -222,6 +223,13 @@ def fit(self, X, y):
             assert_array_almost_equal(sparse_results, dense_results)
 
 
+class DummySizeEstimator(BaseEstimator):
+
+    def fit(self, X, y):
+        self.training_size_ = X.shape[0]
+        self.training_hash_ = hash(X)
+
+
 def test_bootstrap_samples():
     # Test that bootstrapping samples generate non-perfect base estimators.
     rng = check_random_state(0)
@@ -249,6 +257,17 @@ def test_bootstrap_samples():
     assert_greater(base_estimator.score(X_train, y_train),
                    ensemble.score(X_train, y_train))
 
+    # check that each sampling correspond to a complete bootstrap resample.
+    # the size of each bootstrap should be the same as the input data but
+    # the data should be different (checked using the hash of the data).
+    ensemble = BaggingRegressor(base_estimator=DummySizeEstimator(),
+                                bootstrap=True).fit(X_train, y_train)
+    training_hash = []
+    for estimator in ensemble.estimators_:
+        assert estimator.training_size_ == X_train.shape[0]
+        training_hash.append(estimator.training_hash_)
+    assert len(set(training_hash)) == len(training_hash)
+
 
 def test_bootstrap_features():
     # Test that bootstrapping features may generate duplicate features.
@@ -498,6 +517,7 @@ def test_parallel_regression():
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_gridsearch():
     # Check that bagging ensembles can be grid-searched.
     # Transform iris into a binary classification task
@@ -709,8 +729,8 @@ def test_estimators_samples():
 
     # Test for correct formatting
     assert_equal(len(estimators_samples), len(estimators))
-    assert_equal(len(estimators_samples[0]), len(X))
-    assert_equal(estimators_samples[0].dtype.kind, 'b')
+    assert_equal(len(estimators_samples[0]), len(X) // 2)
+    assert_equal(estimators_samples[0].dtype.kind, 'i')
 
     # Re-fit single estimator to test for consistent sampling
     estimator_index = 0
@@ -728,6 +748,34 @@ def test_estimators_samples():
     assert_array_almost_equal(orig_coefs, new_coefs)
 
 
+def test_estimators_samples_deterministic():
+    # This test is a regression test to check that with a random step
+    # (e.g. SparseRandomProjection) and a given random state, the results
+    # generated at fit time can be identically reproduced at a later time using
+    # data saved in object attributes. Check issue #9524 for full discussion.
+
+    iris = load_iris()
+    X, y = iris.data, iris.target
+
+    base_pipeline = make_pipeline(SparseRandomProjection(n_components=2),
+                                  LogisticRegression())
+    clf = BaggingClassifier(base_estimator=base_pipeline,
+                            max_samples=0.5,
+                            random_state=0)
+    clf.fit(X, y)
+    pipeline_estimator_coef = clf.estimators_[0].steps[-1][1].coef_.copy()
+
+    estimator = clf.estimators_[0]
+    estimator_sample = clf.estimators_samples_[0]
+    estimator_feature = clf.estimators_features_[0]
+
+    X_train = (X[estimator_sample])[:, estimator_feature]
+    y_train = y[estimator_sample]
+
+    estimator.fit(X_train, y_train)
+    assert_array_equal(estimator.steps[-1][1].coef_, pipeline_estimator_coef)
+
+
 def test_max_samples_consistency():
     # Make sure validated max_samples and original max_samples are identical
     # when valid integer max_samples supplied by user
diff --git a/sklearn/ensemble/tests/test_forest.py b/sklearn/ensemble/tests/test_forest.py
index cd7626b74759..0a14476da89a 100644
--- a/sklearn/ensemble/tests/test_forest.py
+++ b/sklearn/ensemble/tests/test_forest.py
@@ -446,6 +446,7 @@ def check_gridsearch(name):
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 @pytest.mark.parametrize('name', FOREST_CLASSIFIERS)
 def test_gridsearch(name):
     # Check that base trees can be grid-searched.
diff --git a/sklearn/ensemble/tests/test_weight_boosting.py b/sklearn/ensemble/tests/test_weight_boosting.py
index 8a0beba2207f..e6a6c9d36f44 100755
--- a/sklearn/ensemble/tests/test_weight_boosting.py
+++ b/sklearn/ensemble/tests/test_weight_boosting.py
@@ -197,6 +197,7 @@ def test_staged_predict():
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_gridsearch():
     # Check that base trees can be grid-searched.
     # AdaBoost classification
diff --git a/sklearn/feature_extraction/hashing.py b/sklearn/feature_extraction/hashing.py
index 9795d30aa675..744a073090ba 100644
--- a/sklearn/feature_extraction/hashing.py
+++ b/sklearn/feature_extraction/hashing.py
@@ -7,9 +7,18 @@
 import numpy as np
 import scipy.sparse as sp
 
-from . import _hashing
+from ..utils import IS_PYPY
 from ..base import BaseEstimator, TransformerMixin
 
+if not IS_PYPY:
+    from ._hashing import transform as _hashing_transform
+else:
+    def _hashing_transform(*args, **kwargs):
+        raise NotImplementedError(
+                'FeatureHasher is not compatible with PyPy (see '
+                'https://github.com/scikit-learn/scikit-learn/issues/11540 '
+                'for the status updates).')
+
 
 def _iteritems(d):
     """Like d.iteritems, but accepts any collections.Mapping."""
@@ -155,7 +164,7 @@ def transform(self, raw_X):
         elif self.input_type == "string":
             raw_X = (((f, 1) for f in x) for x in raw_X)
         indices, indptr, values = \
-            _hashing.transform(raw_X, self.n_features, self.dtype,
+            _hashing_transform(raw_X, self.n_features, self.dtype,
                                self.alternate_sign)
         n_samples = indptr.shape[0] - 1
 
diff --git a/sklearn/feature_extraction/setup.py b/sklearn/feature_extraction/setup.py
index 7b71dfdcc83d..761ff1ee5a7d 100644
--- a/sklearn/feature_extraction/setup.py
+++ b/sklearn/feature_extraction/setup.py
@@ -1,4 +1,5 @@
 import os
+import platform
 
 
 def configuration(parent_package='', top_path=None):
@@ -10,10 +11,11 @@ def configuration(parent_package='', top_path=None):
     if os.name == 'posix':
         libraries.append('m')
 
-    config.add_extension('_hashing',
-                         sources=['_hashing.pyx'],
-                         include_dirs=[numpy.get_include()],
-                         libraries=libraries)
+    if platform.python_implementation() != 'PyPy':
+        config.add_extension('_hashing',
+                             sources=['_hashing.pyx'],
+                             include_dirs=[numpy.get_include()],
+                             libraries=libraries)
     config.add_subpackage("tests")
 
     return config
diff --git a/sklearn/feature_extraction/tests/test_feature_hasher.py b/sklearn/feature_extraction/tests/test_feature_hasher.py
index 6f0d6b021495..77a21ff4364a 100644
--- a/sklearn/feature_extraction/tests/test_feature_hasher.py
+++ b/sklearn/feature_extraction/tests/test_feature_hasher.py
@@ -5,7 +5,9 @@
 
 from sklearn.feature_extraction import FeatureHasher
 from sklearn.utils.testing import (assert_raises, assert_true, assert_equal,
-                                   ignore_warnings)
+                                   ignore_warnings, fails_if_pypy)
+
+pytestmark = fails_if_pypy
 
 
 def test_feature_hasher_dicts():
diff --git a/sklearn/feature_extraction/tests/test_text.py b/sklearn/feature_extraction/tests/test_text.py
index db7abc0e756e..b9431bc5439c 100644
--- a/sklearn/feature_extraction/tests/test_text.py
+++ b/sklearn/feature_extraction/tests/test_text.py
@@ -27,13 +27,14 @@
 import numpy as np
 from numpy.testing import assert_array_almost_equal
 from numpy.testing import assert_array_equal
+from sklearn.utils import IS_PYPY
 from sklearn.utils.testing import (assert_equal, assert_false, assert_true,
                                    assert_not_equal, assert_almost_equal,
                                    assert_in, assert_less, assert_greater,
                                    assert_warns_message, assert_raise_message,
                                    clean_warning_registry, ignore_warnings,
                                    SkipTest, assert_raises, assert_no_warnings,
-                                   assert_allclose_dense_sparse)
+                                   fails_if_pypy, assert_allclose_dense_sparse)
 from sklearn.utils.fixes import _Mapping as Mapping
 from collections import defaultdict
 from functools import partial
@@ -503,6 +504,7 @@ def test_tfidf_vectorizer_setters():
     assert_true(tv._tfidf.sublinear_tf)
 
 
+@fails_if_pypy
 @ignore_warnings(category=DeprecationWarning)
 def test_hashing_vectorizer():
     v = HashingVectorizer()
@@ -685,6 +687,7 @@ def test_count_binary_occurrences():
     assert_equal(X_sparse.dtype, np.float32)
 
 
+@fails_if_pypy
 @ignore_warnings(category=DeprecationWarning)
 def test_hashed_binary_occurrences():
     # by default multiple occurrences are counted as longs
@@ -732,6 +735,7 @@ def test_vectorizer_inverse_transform(Vectorizer):
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_count_vectorizer_pipeline_grid_selection():
     # raw documents
     data = JUNK_FOOD_DOCS + NOTJUNK_FOOD_DOCS
@@ -769,6 +773,7 @@ def test_count_vectorizer_pipeline_grid_selection():
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_vectorizer_pipeline_grid_selection():
     # raw documents
     data = JUNK_FOOD_DOCS + NOTJUNK_FOOD_DOCS
@@ -822,6 +827,7 @@ def test_vectorizer_pipeline_cross_validation():
     assert_array_equal(cv_scores, [1., 1., 1.])
 
 
+@fails_if_pypy
 @ignore_warnings(category=DeprecationWarning)
 def test_vectorizer_unicode():
     # tests that the count vectorizer works with cyrillic.
@@ -889,9 +895,12 @@ def test_pickling_vectorizer():
         copy = pickle.loads(s)
         assert_equal(type(copy), orig.__class__)
         assert_equal(copy.get_params(), orig.get_params())
-        assert_array_equal(
-            copy.fit_transform(JUNK_FOOD_DOCS).toarray(),
-            orig.fit_transform(JUNK_FOOD_DOCS).toarray())
+        if IS_PYPY and isinstance(orig, HashingVectorizer):
+            continue
+        else:
+            assert_array_equal(
+                copy.fit_transform(JUNK_FOOD_DOCS).toarray(),
+                orig.fit_transform(JUNK_FOOD_DOCS).toarray())
 
 
 def test_countvectorizer_vocab_sets_when_pickling():
@@ -993,6 +1002,7 @@ def test_non_unique_vocab():
     assert_raises(ValueError, vect.fit, [])
 
 
+@fails_if_pypy
 def test_hashingvectorizer_nan_in_docs():
     # np.nan can appear when using pandas to load text fields from a csv file
     # with missing values.
diff --git a/sklearn/feature_selection/rfe.py b/sklearn/feature_selection/rfe.py
index 31c0133b2c69..b02ae49c0041 100644
--- a/sklearn/feature_selection/rfe.py
+++ b/sklearn/feature_selection/rfe.py
@@ -262,16 +262,61 @@ def _get_support_mask(self):
 
     @if_delegate_has_method(delegate='estimator')
     def decision_function(self, X):
+        """Compute the decision function of ``X``.
+
+        Parameters
+        ----------
+        X : array-like or sparse matrix, shape = [n_samples, n_features]
+            The input samples. Internally, it will be converted to
+            ``dtype=np.float32`` and if a sparse matrix is provided
+            to a sparse ``csr_matrix``.
+
+        Returns
+        -------
+        score : array, shape = [n_samples, n_classes] or [n_samples]
+            The decision function of the input samples. The order of the
+            classes corresponds to that in the attribute `classes_`.
+            Regression and binary classification produce an array of shape
+            [n_samples].
+        """
         check_is_fitted(self, 'estimator_')
         return self.estimator_.decision_function(self.transform(X))
 
     @if_delegate_has_method(delegate='estimator')
     def predict_proba(self, X):
+        """Predict class probabilities for X.
+
+        Parameters
+        ----------
+        X : array-like or sparse matrix, shape = [n_samples, n_features]
+            The input samples. Internally, it will be converted to
+            ``dtype=np.float32`` and if a sparse matrix is provided
+            to a sparse ``csr_matrix``.
+
+        Returns
+        -------
+        p : array of shape = [n_samples, n_classes]
+            The class probabilities of the input samples. The order of the
+            classes corresponds to that in the attribute `classes_`.
+        """
         check_is_fitted(self, 'estimator_')
         return self.estimator_.predict_proba(self.transform(X))
 
     @if_delegate_has_method(delegate='estimator')
     def predict_log_proba(self, X):
+        """Predict class log-probabilities for X.
+
+        Parameters
+        ----------
+        X : array of shape [n_samples, n_features]
+            The input samples.
+
+        Returns
+        -------
+        p : array of shape = [n_samples, n_classes]
+            The class log-probabilities of the input samples. The order of the
+            classes corresponds to that in the attribute `classes_`.
+        """
         check_is_fitted(self, 'estimator_')
         return self.estimator_.predict_log_proba(self.transform(X))
 
@@ -312,6 +357,10 @@ class RFECV(RFE, MetaEstimatorMixin):
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     scoring : string, callable or None, optional, default: None
         A string (see model evaluation documentation) or
         a scorer callable object / function with signature
@@ -382,7 +431,7 @@ class RFECV(RFE, MetaEstimatorMixin):
            for cancer classification using support vector machines",
            Mach. Learn., 46(1-3), 389--422, 2002.
     """
-    def __init__(self, estimator, step=1, cv=None, scoring=None, verbose=0,
+    def __init__(self, estimator, step=1, cv='warn', scoring=None, verbose=0,
                  n_jobs=1):
         self.estimator = estimator
         self.step = step
diff --git a/sklearn/feature_selection/tests/test_rfe.py b/sklearn/feature_selection/tests/test_rfe.py
index e8533d808daf..29854bb1df57 100644
--- a/sklearn/feature_selection/tests/test_rfe.py
+++ b/sklearn/feature_selection/tests/test_rfe.py
@@ -229,6 +229,7 @@ def test_rfecv_verbose_output():
     assert_greater(len(verbose_output.readline()), 0)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_rfe_estimator_tags():
     rfe = RFE(SVC(kernel='linear'))
     assert_equal(rfe._estimator_type, "classifier")
@@ -320,6 +321,7 @@ def formula2(n_features, n_features_to_select, step):
                      formula2(n_features, n_features_to_select, step))
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_rfe_cv_n_jobs():
     generator = check_random_state(0)
     iris = load_iris()
diff --git a/sklearn/feature_selection/univariate_selection.py b/sklearn/feature_selection/univariate_selection.py
index 612f61028e2a..360c312b5587 100644
--- a/sklearn/feature_selection/univariate_selection.py
+++ b/sklearn/feature_selection/univariate_selection.py
@@ -50,8 +50,9 @@ def f_oneway(*args):
 
     Parameters
     ----------
-    sample1, sample2, ... : array_like, sparse matrices
-        The sample measurements should be given as arguments.
+    *args : array_like, sparse matrices
+        sample1, sample2... The sample measurements should be given as
+        arguments.
 
     Returns
     -------
diff --git a/sklearn/kernel_approximation.py b/sklearn/kernel_approximation.py
index 4044a8f6a996..cbd8dcf51a94 100644
--- a/sklearn/kernel_approximation.py
+++ b/sklearn/kernel_approximation.py
@@ -302,7 +302,19 @@ def __init__(self, sample_steps=2, sample_interval=None):
         self.sample_interval = sample_interval
 
     def fit(self, X, y=None):
-        """Set parameters."""
+        """Set the parameters
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+            Training data, where n_samples in the number of samples
+            and n_features is the number of features.
+
+        Returns
+        -------
+        self : object
+            Returns the transformer.
+        """
         X = check_array(X, accept_sparse='csr')
         if self.sample_interval is None:
             # See reference, figure 2 c)
@@ -420,27 +432,27 @@ class Nystroem(BaseEstimator, TransformerMixin):
         and the keyword arguments passed to this object as kernel_params, and
         should return a floating point number.
 
-    n_components : int
-        Number of features to construct.
-        How many data points will be used to construct the mapping.
-
     gamma : float, default=None
         Gamma parameter for the RBF, laplacian, polynomial, exponential chi2
         and sigmoid kernels. Interpretation of the default value is left to
         the kernel; see the documentation for sklearn.metrics.pairwise.
         Ignored by other kernels.
 
-    degree : float, default=None
-        Degree of the polynomial kernel. Ignored by other kernels.
-
     coef0 : float, default=None
         Zero coefficient for polynomial and sigmoid kernels.
         Ignored by other kernels.
 
+    degree : float, default=None
+        Degree of the polynomial kernel. Ignored by other kernels.
+
     kernel_params : mapping of string to any, optional
         Additional parameters (keyword arguments) for kernel function passed
         as callable object.
 
+    n_components : int
+        Number of features to construct.
+        How many data points will be used to construct the mapping.
+
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
         If RandomState instance, random_state is the random number generator;
diff --git a/sklearn/linear_model/coordinate_descent.py b/sklearn/linear_model/coordinate_descent.py
index 38650b887f5b..13e3a3e09ddf 100644
--- a/sklearn/linear_model/coordinate_descent.py
+++ b/sklearn/linear_model/coordinate_descent.py
@@ -1052,7 +1052,7 @@ class LinearModelCV(six.with_metaclass(ABCMeta, LinearModel)):
     @abstractmethod
     def __init__(self, eps=1e-3, n_alphas=100, alphas=None, fit_intercept=True,
                  normalize=False, precompute='auto', max_iter=1000, tol=1e-4,
-                 copy_X=True, cv=None, verbose=False, n_jobs=1,
+                 copy_X=True, cv='warn', verbose=False, n_jobs=1,
                  positive=False, random_state=None, selection='cyclic'):
         self.eps = eps
         self.n_alphas = n_alphas
@@ -1311,6 +1311,10 @@ class LassoCV(LinearModelCV, RegressorMixin):
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     verbose : bool or integer
         Amount of verbosity.
 
@@ -1381,7 +1385,7 @@ class LassoCV(LinearModelCV, RegressorMixin):
 
     def __init__(self, eps=1e-3, n_alphas=100, alphas=None, fit_intercept=True,
                  normalize=False, precompute='auto', max_iter=1000, tol=1e-4,
-                 copy_X=True, cv=None, verbose=False, n_jobs=1,
+                 copy_X=True, cv='warn', verbose=False, n_jobs=1,
                  positive=False, random_state=None, selection='cyclic'):
         super(LassoCV, self).__init__(
             eps=eps, n_alphas=n_alphas, alphas=alphas,
@@ -1464,6 +1468,10 @@ class ElasticNetCV(LinearModelCV, RegressorMixin):
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     copy_X : boolean, optional, default True
         If ``True``, X will be copied; else, it may be overwritten.
 
@@ -1573,7 +1581,7 @@ class ElasticNetCV(LinearModelCV, RegressorMixin):
 
     def __init__(self, l1_ratio=0.5, eps=1e-3, n_alphas=100, alphas=None,
                  fit_intercept=True, normalize=False, precompute='auto',
-                 max_iter=1000, tol=1e-4, cv=None, copy_X=True,
+                 max_iter=1000, tol=1e-4, cv='warn', copy_X=True,
                  verbose=0, n_jobs=1, positive=False, random_state=None,
                  selection='cyclic'):
         self.l1_ratio = l1_ratio
@@ -1994,6 +2002,10 @@ class MultiTaskElasticNetCV(LinearModelCV, RegressorMixin):
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     copy_X : boolean, optional, default True
         If ``True``, X will be copied; else, it may be overwritten.
 
@@ -2048,11 +2060,11 @@ class MultiTaskElasticNetCV(LinearModelCV, RegressorMixin):
     Examples
     --------
     >>> from sklearn import linear_model
-    >>> clf = linear_model.MultiTaskElasticNetCV()
+    >>> clf = linear_model.MultiTaskElasticNetCV(cv=3)
     >>> clf.fit([[0,0], [1, 1], [2, 2]],
     ...         [[0, 0], [1, 1], [2, 2]])
     ... #doctest: +NORMALIZE_WHITESPACE
-    MultiTaskElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001,
+    MultiTaskElasticNetCV(alphas=None, copy_X=True, cv=3, eps=0.001,
            fit_intercept=True, l1_ratio=0.5, max_iter=1000, n_alphas=100,
            n_jobs=1, normalize=False, random_state=None, selection='cyclic',
            tol=0.0001, verbose=0)
@@ -2079,7 +2091,7 @@ class MultiTaskElasticNetCV(LinearModelCV, RegressorMixin):
 
     def __init__(self, l1_ratio=0.5, eps=1e-3, n_alphas=100, alphas=None,
                  fit_intercept=True, normalize=False,
-                 max_iter=1000, tol=1e-4, cv=None, copy_X=True,
+                 max_iter=1000, tol=1e-4, cv='warn', copy_X=True,
                  verbose=0, n_jobs=1, random_state=None, selection='cyclic'):
         self.l1_ratio = l1_ratio
         self.eps = eps
@@ -2164,6 +2176,10 @@ class MultiTaskLassoCV(LinearModelCV, RegressorMixin):
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     verbose : bool or integer
         Amount of verbosity.
 
@@ -2225,7 +2241,7 @@ class MultiTaskLassoCV(LinearModelCV, RegressorMixin):
 
     def __init__(self, eps=1e-3, n_alphas=100, alphas=None, fit_intercept=True,
                  normalize=False, max_iter=1000, tol=1e-4, copy_X=True,
-                 cv=None, verbose=False, n_jobs=1, random_state=None,
+                 cv='warn', verbose=False, n_jobs=1, random_state=None,
                  selection='cyclic'):
         super(MultiTaskLassoCV, self).__init__(
             eps=eps, n_alphas=n_alphas, alphas=alphas,
diff --git a/sklearn/linear_model/least_angle.py b/sklearn/linear_model/least_angle.py
index f0409545a26b..bae65fc88937 100644
--- a/sklearn/linear_model/least_angle.py
+++ b/sklearn/linear_model/least_angle.py
@@ -1013,6 +1013,10 @@ class LarsCV(Lars):
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     max_n_alphas : integer, optional
         The maximum number of points on the path used to compute the
         residuals in the cross-validation
@@ -1071,7 +1075,7 @@ class LarsCV(Lars):
     method = 'lar'
 
     def __init__(self, fit_intercept=True, verbose=False, max_iter=500,
-                 normalize=True, precompute='auto', cv=None,
+                 normalize=True, precompute='auto', cv='warn',
                  max_n_alphas=1000, n_jobs=1, eps=np.finfo(np.float).eps,
                  copy_X=True, positive=False):
         self.max_iter = max_iter
@@ -1222,6 +1226,10 @@ class LassoLarsCV(LarsCV):
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     max_n_alphas : integer, optional
         The maximum number of points on the path used to compute the
         residuals in the cross-validation
@@ -1297,7 +1305,7 @@ class LassoLarsCV(LarsCV):
     method = 'lasso'
 
     def __init__(self, fit_intercept=True, verbose=False, max_iter=500,
-                 normalize=True, precompute='auto', cv=None,
+                 normalize=True, precompute='auto', cv='warn',
                  max_n_alphas=1000, n_jobs=1, eps=np.finfo(np.float).eps,
                  copy_X=True, positive=False):
         self.fit_intercept = fit_intercept
diff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py
index 4cfd8eeb5229..895d5faa00b1 100644
--- a/sklearn/linear_model/logistic.py
+++ b/sklearn/linear_model/logistic.py
@@ -1408,12 +1408,16 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,
         Specifies if a constant (a.k.a. bias or intercept) should be
         added to the decision function.
 
-    cv : integer or cross-validation generator
+    cv : integer or cross-validation generator, default: None
         The default cross-validation generator used is Stratified K-Folds.
         If an integer is provided, then it is the number of folds used.
         See the module :mod:`sklearn.model_selection` module for the
         list of possible cross-validation objects.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     dual : bool
         Dual or primal formulation. Dual formulation is only implemented for
         l2 penalty with liblinear solver. Prefer dual=False when
@@ -1570,7 +1574,7 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,
 
     """
 
-    def __init__(self, Cs=10, fit_intercept=True, cv=None, dual=False,
+    def __init__(self, Cs=10, fit_intercept=True, cv='warn', dual=False,
                  penalty='l2', scoring=None, solver='lbfgs', tol=1e-4,
                  max_iter=100, class_weight=None, n_jobs=1, verbose=0,
                  refit=True, intercept_scaling=1., multi_class='ovr',
diff --git a/sklearn/linear_model/omp.py b/sklearn/linear_model/omp.py
index 09d58e72b886..ec3cb7efc8ed 100644
--- a/sklearn/linear_model/omp.py
+++ b/sklearn/linear_model/omp.py
@@ -785,6 +785,10 @@ class OrthogonalMatchingPursuitCV(LinearModel, RegressorMixin):
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     n_jobs : integer, optional
         Number of CPUs to use during the cross validation. If ``-1``, use
         all the CPUs
@@ -822,7 +826,7 @@ class OrthogonalMatchingPursuitCV(LinearModel, RegressorMixin):
 
     """
     def __init__(self, copy=True, fit_intercept=True, normalize=True,
-                 max_iter=None, cv=None, n_jobs=1, verbose=False):
+                 max_iter=None, cv='warn', n_jobs=1, verbose=False):
         self.copy = copy
         self.fit_intercept = fit_intercept
         self.normalize = normalize
diff --git a/sklearn/linear_model/tests/test_coordinate_descent.py b/sklearn/linear_model/tests/test_coordinate_descent.py
index fb65d800e78b..834d685f5b23 100644
--- a/sklearn/linear_model/tests/test_coordinate_descent.py
+++ b/sklearn/linear_model/tests/test_coordinate_descent.py
@@ -146,6 +146,7 @@ def build_dataset(n_samples=50, n_features=200, n_informative_features=10,
     return X, y, X_test, y_test
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_lasso_cv():
     X, y, X_test, y_test = build_dataset()
     max_iter = 150
@@ -232,6 +233,7 @@ def test_lasso_path_return_models_vs_new_return_gives_same_coefficients():
         decimal=1)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_enet_path():
     # We use a large number of samples and of informative features so that
     # the l1_ratio selected is more toward ridge than lasso
@@ -289,6 +291,7 @@ def test_enet_path():
     assert_almost_equal(clf1.alpha_, clf2.alpha_)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_path_parameters():
     X, y, _, _ = build_dataset()
     max_iter = 100
@@ -360,6 +363,7 @@ def test_enet_cv_positive_constraint():
     assert_true(min(enetcv_constrained.coef_) >= 0)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_uniform_targets():
     enet = ElasticNetCV(fit_intercept=True, n_alphas=3)
     m_enet = MultiTaskElasticNetCV(fit_intercept=True, n_alphas=3)
@@ -454,6 +458,7 @@ def test_multioutput_enetcv_error():
     assert_raises(ValueError, clf.fit, X, y)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_multitask_enet_and_lasso_cv():
     X, y, _, _ = build_dataset(n_features=50, n_targets=3)
     clf = MultiTaskElasticNetCV().fit(X, y)
@@ -480,6 +485,7 @@ def test_multitask_enet_and_lasso_cv():
     assert_equal(10, len(clf.alphas_))
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_1d_multioutput_enet_and_multitask_enet_cv():
     X, y, _, _ = build_dataset(n_features=10)
     y = y[:, np.newaxis]
@@ -493,6 +499,7 @@ def test_1d_multioutput_enet_and_multitask_enet_cv():
     assert_almost_equal(clf.intercept_, clf1.intercept_[0])
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_1d_multioutput_lasso_and_multitask_lasso_cv():
     X, y, _, _ = build_dataset(n_features=10)
     y = y[:, np.newaxis]
@@ -505,6 +512,7 @@ def test_1d_multioutput_lasso_and_multitask_lasso_cv():
     assert_almost_equal(clf.intercept_, clf1.intercept_[0])
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_sparse_input_dtype_enet_and_lassocv():
     X, y, _, _ = build_dataset(n_features=10)
     clf = ElasticNetCV(n_alphas=5)
@@ -522,6 +530,7 @@ def test_sparse_input_dtype_enet_and_lassocv():
     assert_almost_equal(clf.coef_, clf1.coef_, decimal=6)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_precompute_invalid_argument():
     X, y, _, _ = build_dataset()
     for clf in [ElasticNetCV(precompute="invalid"),
diff --git a/sklearn/linear_model/tests/test_least_angle.py b/sklearn/linear_model/tests/test_least_angle.py
index 15df84177e66..8545ecd98839 100644
--- a/sklearn/linear_model/tests/test_least_angle.py
+++ b/sklearn/linear_model/tests/test_least_angle.py
@@ -182,6 +182,7 @@ def test_no_path_all_precomputed():
     assert_true(alpha_ == alphas_[-1])
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 @pytest.mark.parametrize(
         'classifier',
         [linear_model.Lars, linear_model.LarsCV, linear_model.LassoLarsIC])
@@ -418,6 +419,7 @@ def test_multitarget():
             assert_array_almost_equal(Y_pred[:, k], y_pred)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_lars_cv():
     # Test the LassoLarsCV object by checking that the optimal alpha
     # increases as the number of samples increases.
@@ -434,6 +436,7 @@ def test_lars_cv():
     assert_false(hasattr(lars_cv, 'n_nonzero_coefs'))
 
 
+@pytest.mark.filterwarnings('ignore::FutureWarning')
 def test_lars_cv_max_iter():
     with warnings.catch_warnings(record=True) as w:
         X = diabetes.data
@@ -524,6 +527,7 @@ def test_lars_path_positive_constraint():
                            'LassoLarsIC': {}}
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_estimatorclasses_positive_constraint():
     # testing the transmissibility for the positive option of all estimator
     # classes in this same function here
diff --git a/sklearn/linear_model/tests/test_logistic.py b/sklearn/linear_model/tests/test_logistic.py
index 343d8211b1ef..201fda6af936 100644
--- a/sklearn/linear_model/tests/test_logistic.py
+++ b/sklearn/linear_model/tests/test_logistic.py
@@ -467,6 +467,7 @@ def test_logistic_grad_hess():
         assert_array_almost_equal(grad_interp, grad_interp_2)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_logistic_cv():
     # test for LogisticRegressionCV object
     n_samples, n_features = 50, 5
@@ -493,6 +494,7 @@ def test_logistic_cv():
     assert_array_equal(scores.shape, (1, 3, 1))
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 @pytest.mark.parametrize('scoring, multiclass_agg_list',
                          [('accuracy', ['']),
                           ('precision', ['_macro', '_weighted']),
@@ -526,6 +528,7 @@ def test_logistic_cv_multinomial_score(scoring, multiclass_agg_list):
             scorer(lr, X[test], y[test]))
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_multinomial_logistic_regression_string_inputs():
     # Test with string labels for LogisticRegression(CV)
     n_samples, n_features, n_classes = 50, 5, 3
@@ -565,6 +568,7 @@ def test_multinomial_logistic_regression_string_inputs():
     assert_equal(sorted(np.unique(lr_cv_str.predict(X_ref))), ['bar', 'baz'])
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_logistic_cv_sparse():
     X, y = make_classification(n_samples=50, n_features=5,
                                random_state=0)
@@ -728,6 +732,7 @@ def test_logistic_regression_solvers_multiclass():
     assert_array_almost_equal(saga.coef_, lib.coef_, decimal=4)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_logistic_regressioncv_class_weights():
     for weight in [{0: 0.1, 1: 0.2}, {0: 0.1, 1: 0.2, 2: 0.5}]:
         n_classes = len(weight)
@@ -767,6 +772,7 @@ def test_logistic_regressioncv_class_weights():
             assert_array_almost_equal(clf_saga.coef_, clf_lbf.coef_, decimal=4)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_logistic_regression_sample_weights():
     X, y = make_classification(n_samples=20, n_features=5, n_informative=3,
                                n_classes=2, random_state=0)
@@ -882,6 +888,7 @@ def test_logistic_regression_class_weights():
         assert_array_almost_equal(clf1.coef_, clf2.coef_, decimal=6)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_logistic_regression_multinomial():
     # Tests for the multinomial option in logistic regression
 
@@ -975,6 +982,7 @@ def test_liblinear_decision_function_zero():
     assert_array_equal(clf.predict(X), np.zeros(5))
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_liblinear_logregcv_sparse():
     # Test LogRegCV with solver='liblinear' works for sparse matrices
 
@@ -983,6 +991,7 @@ def test_liblinear_logregcv_sparse():
     clf.fit(sparse.csr_matrix(X), y)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_saga_sparse():
     # Test LogRegCV with solver='liblinear' works for sparse matrices
 
@@ -1075,6 +1084,7 @@ def test_logreg_l1_sparse_data():
     assert_array_almost_equal(lr_saga.coef_, lr_saga_dense.coef_)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_logreg_cv_penalty():
     # Test that the correct penalty is passed to the final fit.
     X, y = make_classification(n_samples=50, n_features=20, random_state=0)
diff --git a/sklearn/linear_model/tests/test_ridge.py b/sklearn/linear_model/tests/test_ridge.py
index fc0c0a943b70..d42e0f874300 100644
--- a/sklearn/linear_model/tests/test_ridge.py
+++ b/sklearn/linear_model/tests/test_ridge.py
@@ -490,6 +490,7 @@ def check_dense_sparse(test_func):
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 @pytest.mark.parametrize(
         'test_func',
         (_test_ridge_loo, _test_ridge_cv, _test_ridge_cv_normalize,
@@ -548,6 +549,7 @@ def test_class_weights():
     assert_array_almost_equal(reg.intercept_, rega.intercept_)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 @pytest.mark.parametrize('reg', (RidgeClassifier, RidgeClassifierCV))
 def test_class_weight_vs_sample_weight(reg):
     """Check class_weights resemble sample_weights behavior."""
@@ -577,6 +579,7 @@ def test_class_weight_vs_sample_weight(reg):
     assert_almost_equal(reg1.coef_, reg2.coef_)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_class_weights_cv():
     # Test class weights for cross validated ridge classifier.
     X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],
@@ -593,6 +596,7 @@ def test_class_weights_cv():
     assert_array_equal(reg.predict([[-.2, 2]]), np.array([-1]))
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_ridgecv_store_cv_values():
     rng = np.random.RandomState(42)
 
@@ -602,7 +606,7 @@ def test_ridgecv_store_cv_values():
     alphas = [1e-1, 1e0, 1e1]
     n_alphas = len(alphas)
 
-    r = RidgeCV(alphas=alphas, store_cv_values=True)
+    r = RidgeCV(alphas=alphas, cv=None, store_cv_values=True)
 
     # with len(y.shape) == 1
     y = rng.randn(n_samples)
@@ -616,6 +620,7 @@ def test_ridgecv_store_cv_values():
     assert r.cv_values_.shape == (n_samples, n_targets, n_alphas)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_ridge_classifier_cv_store_cv_values():
     x = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],
                   [1.0, 1.0], [1.0, 0.0]])
@@ -625,7 +630,7 @@ def test_ridge_classifier_cv_store_cv_values():
     alphas = [1e-1, 1e0, 1e1]
     n_alphas = len(alphas)
 
-    r = RidgeClassifierCV(alphas=alphas, store_cv_values=True)
+    r = RidgeClassifierCV(alphas=alphas, cv=None, store_cv_values=True)
 
     # with len(y.shape) == 1
     n_targets = 1
@@ -736,6 +741,7 @@ def test_sparse_design_with_sample_weights():
                                       decimal=6)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_ridgecv_int_alphas():
     X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],
                   [1.0, 1.0], [1.0, 0.0]])
@@ -746,6 +752,7 @@ def test_ridgecv_int_alphas():
     ridge.fit(X, y)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_ridgecv_negative_alphas():
     X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],
                   [1.0, 1.0], [1.0, 0.0]])
diff --git a/sklearn/linear_model/tests/test_sag.py b/sklearn/linear_model/tests/test_sag.py
index 81193d1b92c2..ca99a81a7396 100644
--- a/sklearn/linear_model/tests/test_sag.py
+++ b/sklearn/linear_model/tests/test_sag.py
@@ -4,6 +4,7 @@
 # License: BSD 3 clause
 
 import math
+import pytest
 import numpy as np
 import scipy.sparse as sp
 
@@ -20,7 +21,6 @@
 from sklearn.utils.testing import assert_allclose
 from sklearn.utils.testing import assert_greater
 from sklearn.utils.testing import assert_raise_message
-from sklearn.utils.testing import ignore_warnings
 from sklearn.utils import compute_class_weight
 from sklearn.utils import check_random_state
 from sklearn.preprocessing import LabelEncoder, LabelBinarizer
@@ -231,7 +231,6 @@ def get_step_size(X, alpha, fit_intercept, classification=True):
         return 1.0 / (np.max(np.sum(X * X, axis=1)) + fit_intercept + alpha)
 
 
-@ignore_warnings
 def test_classifier_matching():
     n_samples = 20
     X, y = make_blobs(n_samples=n_samples, centers=2, random_state=0,
@@ -301,7 +300,7 @@ def test_regressor_matching():
     assert_allclose(intercept2, clf.intercept_)
 
 
-@ignore_warnings
+@pytest.mark.filterwarnings('ignore:The max_iter was reached')
 def test_sag_pobj_matches_logistic_regression():
     """tests if the sag pobj matches log reg"""
     n_samples = 100
@@ -331,7 +330,7 @@ def test_sag_pobj_matches_logistic_regression():
     assert_array_almost_equal(pobj3, pobj1, decimal=4)
 
 
-@ignore_warnings
+@pytest.mark.filterwarnings('ignore:The max_iter was reached')
 def test_sag_pobj_matches_ridge_regression():
     """tests if the sag pobj matches ridge reg"""
     n_samples = 100
@@ -363,7 +362,7 @@ def test_sag_pobj_matches_ridge_regression():
     assert_array_almost_equal(pobj3, pobj2, decimal=4)
 
 
-@ignore_warnings
+@pytest.mark.filterwarnings('ignore:The max_iter was reached')
 def test_sag_regressor_computed_correctly():
     """tests if the sag regressor is computed correctly"""
     alpha = .1
@@ -407,7 +406,6 @@ def test_sag_regressor_computed_correctly():
     # assert_almost_equal(clf2.intercept_, spintercept2, decimal=1)'''
 
 
-@ignore_warnings
 def test_get_auto_step_size():
     X = np.array([[1, 2, 3], [2, 3, 4], [2, 3, 2]], dtype=np.float64)
     alpha = 1.2
@@ -452,7 +450,7 @@ def test_get_auto_step_size():
                          max_squared_sum_, alpha, "wrong", fit_intercept)
 
 
-@ignore_warnings
+@pytest.mark.filterwarnings('ignore:The max_iter was reached')
 def test_sag_regressor():
     """tests if the sag regressor performs well"""
     xmin, xmax = -5, 5
@@ -491,7 +489,7 @@ def test_sag_regressor():
     assert_greater(score2, 0.5)
 
 
-@ignore_warnings
+@pytest.mark.filterwarnings('ignore:The max_iter was reached')
 def test_sag_classifier_computed_correctly():
     """tests if the binary classifier is computed correctly"""
     alpha = .1
@@ -534,7 +532,7 @@ def test_sag_classifier_computed_correctly():
     assert_almost_equal(clf2.intercept_, spintercept2, decimal=1)
 
 
-@ignore_warnings
+@pytest.mark.filterwarnings('ignore:The max_iter was reached')
 def test_sag_multiclass_computed_correctly():
     """tests if the multiclass classifier is computed correctly"""
     alpha = .1
@@ -593,7 +591,6 @@ def test_sag_multiclass_computed_correctly():
         assert_almost_equal(clf2.intercept_[i], intercept2[i], decimal=1)
 
 
-@ignore_warnings
 def test_classifier_results():
     """tests if classifier results match target"""
     alpha = .1
@@ -618,7 +615,7 @@ def test_classifier_results():
     assert_almost_equal(pred2, y, decimal=12)
 
 
-@ignore_warnings
+@pytest.mark.filterwarnings('ignore:The max_iter was reached')
 def test_binary_classifier_class_weight():
     """tests binary classifier with classweights for each class"""
     alpha = .1
@@ -668,7 +665,7 @@ def test_binary_classifier_class_weight():
     assert_almost_equal(clf2.intercept_, spintercept2, decimal=1)
 
 
-@ignore_warnings
+@pytest.mark.filterwarnings('ignore:The max_iter was reached')
 def test_multiclass_classifier_class_weight():
     """tests multiclass with classweights for each class"""
     alpha = .1
diff --git a/sklearn/metrics/tests/test_score_objects.py b/sklearn/metrics/tests/test_score_objects.py
index 33a0fc110939..f418a9375d99 100644
--- a/sklearn/metrics/tests/test_score_objects.py
+++ b/sklearn/metrics/tests/test_score_objects.py
@@ -250,6 +250,7 @@ def test_check_scoring_and_check_multimetric_scoring():
                              scoring=scoring)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_check_scoring_gridsearchcv():
     # test that check_scoring works on GridSearchCV and pipeline.
     # slightly redundant non-regression test.
diff --git a/sklearn/model_selection/_search.py b/sklearn/model_selection/_search.py
index effee28cb20c..2da6c3f6ec9c 100644
--- a/sklearn/model_selection/_search.py
+++ b/sklearn/model_selection/_search.py
@@ -413,7 +413,7 @@ class BaseSearchCV(six.with_metaclass(ABCMeta, BaseEstimator,
     @abstractmethod
     def __init__(self, estimator, scoring=None,
                  fit_params=None, n_jobs=1, iid='warn',
-                 refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs',
+                 refit=True, cv='warn', verbose=0, pre_dispatch='2*n_jobs',
                  error_score='raise-deprecating', return_train_score=True):
 
         self.scoring = scoring
@@ -887,6 +887,10 @@ class GridSearchCV(BaseSearchCV):
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     refit : boolean, or string, default=True
         Refit an estimator using the best found parameters on the whole
         dataset.
@@ -938,10 +942,10 @@ class GridSearchCV(BaseSearchCV):
     >>> iris = datasets.load_iris()
     >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
     >>> svc = svm.SVC(gamma="scale")
-    >>> clf = GridSearchCV(svc, parameters) # doctest: +SKIP
-    >>> clf.fit(iris.data, iris.target) # doctest: +SKIP
+    >>> clf = GridSearchCV(svc, parameters, cv=5)
+    >>> clf.fit(iris.data, iris.target)
     ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
-    GridSearchCV(cv=None, error_score=...,
+    GridSearchCV(cv=5, error_score=...,
            estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,
                          decision_function_shape='ovr', degree=..., gamma=...,
                          kernel='rbf', max_iter=-1, probability=False,
@@ -950,7 +954,7 @@ class GridSearchCV(BaseSearchCV):
            fit_params=None, iid=..., n_jobs=1,
            param_grid=..., pre_dispatch=..., refit=..., return_train_score=...,
            scoring=..., verbose=...)
-    >>> sorted(clf.cv_results_.keys()) # doctest: +SKIP
+    >>> sorted(clf.cv_results_.keys())
     ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
     ['mean_fit_time', 'mean_score_time', 'mean_test_score',...
      'mean_train_score', 'param_C', 'param_kernel', 'params',...
@@ -1091,7 +1095,7 @@ class GridSearchCV(BaseSearchCV):
     """
 
     def __init__(self, estimator, param_grid, scoring=None, fit_params=None,
-                 n_jobs=1, iid='warn', refit=True, cv=None, verbose=0,
+                 n_jobs=1, iid='warn', refit=True, cv='warn', verbose=0,
                  pre_dispatch='2*n_jobs', error_score='raise-deprecating',
                  return_train_score="warn"):
         super(GridSearchCV, self).__init__(
@@ -1227,6 +1231,10 @@ class RandomizedSearchCV(BaseSearchCV):
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     refit : boolean, or string default=True
         Refit an estimator using the best found parameters on the whole
         dataset.
@@ -1404,7 +1412,7 @@ class RandomizedSearchCV(BaseSearchCV):
     """
 
     def __init__(self, estimator, param_distributions, n_iter=10, scoring=None,
-                 fit_params=None, n_jobs=1, iid='warn', refit=True, cv=None,
+                 fit_params=None, n_jobs=1, iid='warn', refit=True, cv='warn',
                  verbose=0, pre_dispatch='2*n_jobs', random_state=None,
                  error_score='raise-deprecating', return_train_score="warn"):
         self.param_distributions = param_distributions
diff --git a/sklearn/model_selection/_split.py b/sklearn/model_selection/_split.py
index ded8b66b3af3..f815c29987e4 100644
--- a/sklearn/model_selection/_split.py
+++ b/sklearn/model_selection/_split.py
@@ -49,6 +49,17 @@
            'check_cv']
 
 
+NSPLIT_WARNING = (
+    "You should specify a value for 'n_splits' instead of relying on the "
+    "default value. The default value will change from 3 to 5 "
+    "in version 0.22.")
+
+CV_WARNING = (
+    "You should specify a value for 'cv' instead of relying on the "
+    "default value. The default value will change from 3 to 5 "
+    "in version 0.22.")
+
+
 class BaseCrossValidator(with_metaclass(ABCMeta)):
     """Base class for all cross-validators
 
@@ -358,6 +369,9 @@ class KFold(_BaseKFold):
     n_splits : int, default=3
         Number of folds. Must be at least 2.
 
+        .. versionchanged:: 0.20
+            ``n_splits`` default value will change from 3 to 5 in v0.22.
+
     shuffle : boolean, optional
         Whether to shuffle the data before splitting into batches.
 
@@ -406,8 +420,11 @@ class KFold(_BaseKFold):
     RepeatedKFold: Repeats K-Fold n times.
     """
 
-    def __init__(self, n_splits=3, shuffle=False,
+    def __init__(self, n_splits='warn', shuffle=False,
                  random_state=None):
+        if n_splits is 'warn':
+            warnings.warn(NSPLIT_WARNING, FutureWarning)
+            n_splits = 3
         super(KFold, self).__init__(n_splits, shuffle, random_state)
 
     def _iter_test_indices(self, X, y=None, groups=None):
@@ -440,6 +457,9 @@ class GroupKFold(_BaseKFold):
     n_splits : int, default=3
         Number of folds. Must be at least 2.
 
+        .. versionchanged:: 0.20
+            ``n_splits`` default value will change from 3 to 5 in v0.22.
+
     Examples
     --------
     >>> from sklearn.model_selection import GroupKFold
@@ -472,7 +492,10 @@ class GroupKFold(_BaseKFold):
         For splitting the data according to explicit domain-specific
         stratification of the dataset.
     """
-    def __init__(self, n_splits=3):
+    def __init__(self, n_splits='warn'):
+        if n_splits is 'warn':
+            warnings.warn(NSPLIT_WARNING, FutureWarning)
+            n_splits = 3
         super(GroupKFold, self).__init__(n_splits, shuffle=False,
                                          random_state=None)
 
@@ -530,6 +553,9 @@ class StratifiedKFold(_BaseKFold):
     n_splits : int, default=3
         Number of folds. Must be at least 2.
 
+        .. versionchanged:: 0.20
+            ``n_splits`` default value will change from 3 to 5 in v0.22.
+
     shuffle : boolean, optional
         Whether to shuffle each stratification of the data before splitting
         into batches.
@@ -567,7 +593,10 @@ class StratifiedKFold(_BaseKFold):
     RepeatedStratifiedKFold: Repeats Stratified K-Fold n times.
     """
 
-    def __init__(self, n_splits=3, shuffle=False, random_state=None):
+    def __init__(self, n_splits='warn', shuffle=False, random_state=None):
+        if n_splits is 'warn':
+            warnings.warn(NSPLIT_WARNING, FutureWarning)
+            n_splits = 3
         super(StratifiedKFold, self).__init__(n_splits, shuffle, random_state)
 
     def _make_test_folds(self, X, y=None):
@@ -687,17 +716,20 @@ class TimeSeriesSplit(_BaseKFold):
     n_splits : int, default=3
         Number of splits. Must be at least 1.
 
+        .. versionchanged:: 0.20
+            ``n_splits`` default value will change from 3 to 5 in v0.22.
+
     max_train_size : int, optional
         Maximum size for a single training set.
 
     Examples
     --------
     >>> from sklearn.model_selection import TimeSeriesSplit
-    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
-    >>> y = np.array([1, 2, 3, 4])
-    >>> tscv = TimeSeriesSplit(n_splits=3)
+    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])
+    >>> y = np.array([1, 2, 3, 4, 5, 6])
+    >>> tscv = TimeSeriesSplit(n_splits=5)
     >>> print(tscv)  # doctest: +NORMALIZE_WHITESPACE
-    TimeSeriesSplit(max_train_size=None, n_splits=3)
+    TimeSeriesSplit(max_train_size=None, n_splits=5)
     >>> for train_index, test_index in tscv.split(X):
     ...    print("TRAIN:", train_index, "TEST:", test_index)
     ...    X_train, X_test = X[train_index], X[test_index]
@@ -705,6 +737,8 @@ class TimeSeriesSplit(_BaseKFold):
     TRAIN: [0] TEST: [1]
     TRAIN: [0 1] TEST: [2]
     TRAIN: [0 1 2] TEST: [3]
+    TRAIN: [0 1 2 3] TEST: [4]
+    TRAIN: [0 1 2 3 4] TEST: [5]
 
     Notes
     -----
@@ -713,7 +747,10 @@ class TimeSeriesSplit(_BaseKFold):
     with a test set of size ``n_samples//(n_splits + 1)``,
     where ``n_samples`` is the number of samples.
     """
-    def __init__(self, n_splits=3, max_train_size=None):
+    def __init__(self, n_splits='warn', max_train_size=None):
+        if n_splits is 'warn':
+            warnings.warn(NSPLIT_WARNING, FutureWarning)
+            n_splits = 3
         super(TimeSeriesSplit, self).__init__(n_splits,
                                               shuffle=False,
                                               random_state=None)
@@ -1270,27 +1307,31 @@ class ShuffleSplit(BaseShuffleSplit):
     Examples
     --------
     >>> from sklearn.model_selection import ShuffleSplit
-    >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
-    >>> y = np.array([1, 2, 1, 2])
-    >>> rs = ShuffleSplit(n_splits=3, test_size=.25, random_state=0)
+    >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [3, 4], [5, 6]])
+    >>> y = np.array([1, 2, 1, 2, 1, 2])
+    >>> rs = ShuffleSplit(n_splits=5, test_size=.25, random_state=0)
     >>> rs.get_n_splits(X)
-    3
+    5
     >>> print(rs)
-    ShuffleSplit(n_splits=3, random_state=0, test_size=0.25, train_size=None)
+    ShuffleSplit(n_splits=5, random_state=0, test_size=0.25, train_size=None)
     >>> for train_index, test_index in rs.split(X):
     ...    print("TRAIN:", train_index, "TEST:", test_index)
     ...  # doctest: +ELLIPSIS
-    TRAIN: [3 1 0] TEST: [2]
-    TRAIN: [2 1 3] TEST: [0]
-    TRAIN: [0 2 1] TEST: [3]
-    >>> rs = ShuffleSplit(n_splits=3, train_size=0.5, test_size=.25,
+    TRAIN: [1 3 0 4] TEST: [5 2]
+    TRAIN: [4 0 2 5] TEST: [1 3]
+    TRAIN: [1 2 4 0] TEST: [3 5]
+    TRAIN: [3 4 1 0] TEST: [5 2]
+    TRAIN: [3 5 1 0] TEST: [2 4]
+    >>> rs = ShuffleSplit(n_splits=5, train_size=0.5, test_size=.25,
     ...                   random_state=0)
     >>> for train_index, test_index in rs.split(X):
     ...    print("TRAIN:", train_index, "TEST:", test_index)
     ...  # doctest: +ELLIPSIS
-    TRAIN: [3 1] TEST: [2]
-    TRAIN: [2 1] TEST: [0]
-    TRAIN: [0 2] TEST: [3]
+    TRAIN: [1 3 0] TEST: [5 2]
+    TRAIN: [4 0 2] TEST: [1 3]
+    TRAIN: [1 2 4] TEST: [3 5]
+    TRAIN: [3 4 1] TEST: [5 2]
+    TRAIN: [3 5 1] TEST: [2 4]
     """
 
     def _iter_indices(self, X, y=None, groups=None):
@@ -1502,20 +1543,22 @@ class StratifiedShuffleSplit(BaseShuffleSplit):
     Examples
     --------
     >>> from sklearn.model_selection import StratifiedShuffleSplit
-    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
-    >>> y = np.array([0, 0, 1, 1])
-    >>> sss = StratifiedShuffleSplit(n_splits=3, test_size=0.5, random_state=0)
+    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])
+    >>> y = np.array([0, 0, 0, 1, 1, 1])
+    >>> sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)
     >>> sss.get_n_splits(X, y)
-    3
+    5
     >>> print(sss)       # doctest: +ELLIPSIS
-    StratifiedShuffleSplit(n_splits=3, random_state=0, ...)
+    StratifiedShuffleSplit(n_splits=5, random_state=0, ...)
     >>> for train_index, test_index in sss.split(X, y):
     ...    print("TRAIN:", train_index, "TEST:", test_index)
     ...    X_train, X_test = X[train_index], X[test_index]
     ...    y_train, y_test = y[train_index], y[test_index]
-    TRAIN: [1 2] TEST: [3 0]
-    TRAIN: [0 2] TEST: [1 3]
-    TRAIN: [0 2] TEST: [3 1]
+    TRAIN: [5 2 3] TEST: [4 1 0]
+    TRAIN: [5 1 4] TEST: [0 2 3]
+    TRAIN: [5 0 2] TEST: [4 3 1]
+    TRAIN: [4 1 0] TEST: [2 3 5]
+    TRAIN: [0 5 1] TEST: [3 4 2]
     """
 
     def __init__(self, n_splits=10, test_size="default", train_size=None,
@@ -1859,7 +1902,7 @@ def split(self, X=None, y=None, groups=None):
             yield train, test
 
 
-def check_cv(cv=3, y=None, classifier=False):
+def check_cv(cv='warn', y=None, classifier=False):
     """Input checker utility for building a cross-validator
 
     Parameters
@@ -1880,6 +1923,9 @@ def check_cv(cv=3, y=None, classifier=False):
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value will change from 3-fold to 5-fold in v0.22.
+
     y : array-like, optional
         The target variable for supervised learning problems.
 
@@ -1893,7 +1939,8 @@ def check_cv(cv=3, y=None, classifier=False):
         The return value is a cross-validator which generates the train/test
         splits via the ``split`` method.
     """
-    if cv is None:
+    if cv is 'warn':
+        warnings.warn(CV_WARNING, FutureWarning)
         cv = 3
 
     if isinstance(cv, numbers.Integral):
diff --git a/sklearn/model_selection/_validation.py b/sklearn/model_selection/_validation.py
index 5a46ca0bf792..0933c6a6aab0 100644
--- a/sklearn/model_selection/_validation.py
+++ b/sklearn/model_selection/_validation.py
@@ -38,7 +38,7 @@
            'permutation_test_score', 'learning_curve', 'validation_curve']
 
 
-def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv=None,
+def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv='warn',
                    n_jobs=1, verbose=0, fit_params=None,
                    pre_dispatch='2*n_jobs', return_train_score="warn",
                    return_estimator=False):
@@ -93,6 +93,10 @@ def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv=None,
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     n_jobs : integer, optional
         The number of CPUs to use to do the computation. -1 means
         'all CPUs'.
@@ -175,7 +179,8 @@ def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv=None,
 
     Single metric evaluation using ``cross_validate``
 
-    >>> cv_results = cross_validate(lasso, X, y, return_train_score=False)
+    >>> cv_results = cross_validate(lasso, X, y, cv=3,
+    ...                             return_train_score=False)
     >>> sorted(cv_results.keys())                         # doctest: +ELLIPSIS
     ['fit_time', 'score_time', 'test_score']
     >>> cv_results['test_score']    # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
@@ -184,7 +189,7 @@ def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv=None,
     Multiple metric evaluation using ``cross_validate``
     (please refer the ``scoring`` parameter doc for more information)
 
-    >>> scores = cross_validate(lasso, X, y,
+    >>> scores = cross_validate(lasso, X, y, cv=3,
     ...                         scoring=('r2', 'neg_mean_squared_error'),
     ...                         return_train_score=True)
     >>> print(scores['test_neg_mean_squared_error'])      # doctest: +ELLIPSIS
@@ -255,7 +260,7 @@ def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv=None,
     return ret
 
 
-def cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv=None,
+def cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv='warn',
                     n_jobs=1, verbose=0, fit_params=None,
                     pre_dispatch='2*n_jobs'):
     """Evaluate a score by cross-validation
@@ -299,6 +304,10 @@ def cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv=None,
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     n_jobs : integer, optional
         The number of CPUs to use to do the computation. -1 means
         'all CPUs'.
@@ -339,7 +348,7 @@ def cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv=None,
     >>> X = diabetes.data[:150]
     >>> y = diabetes.target[:150]
     >>> lasso = linear_model.Lasso()
-    >>> print(cross_val_score(lasso, X, y))  # doctest: +ELLIPSIS
+    >>> print(cross_val_score(lasso, X, y, cv=3))  # doctest: +ELLIPSIS
     [0.33150734 0.08022311 0.03531764]
 
     See Also
@@ -612,7 +621,7 @@ def _multimetric_score(estimator, X_test, y_test, scorers):
     return scores
 
 
-def cross_val_predict(estimator, X, y=None, groups=None, cv=None, n_jobs=1,
+def cross_val_predict(estimator, X, y=None, groups=None, cv='warn', n_jobs=1,
                       verbose=0, fit_params=None, pre_dispatch='2*n_jobs',
                       method='predict'):
     """Generate cross-validated estimates for each input data point
@@ -654,6 +663,10 @@ def cross_val_predict(estimator, X, y=None, groups=None, cv=None, n_jobs=1,
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     n_jobs : integer, optional
         The number of CPUs to use to do the computation. -1 means
         'all CPUs'.
@@ -714,7 +727,7 @@ def cross_val_predict(estimator, X, y=None, groups=None, cv=None, n_jobs=1,
     >>> X = diabetes.data[:150]
     >>> y = diabetes.target[:150]
     >>> lasso = linear_model.Lasso()
-    >>> y_pred = cross_val_predict(lasso, X, y)
+    >>> y_pred = cross_val_predict(lasso, X, y, cv=3)
     """
     X, y, groups = indexable(X, y, groups)
 
@@ -888,7 +901,7 @@ def _index_param_value(X, v, indices):
     return safe_indexing(v, indices)
 
 
-def permutation_test_score(estimator, X, y, groups=None, cv=None,
+def permutation_test_score(estimator, X, y, groups=None, cv='warn',
                            n_permutations=100, n_jobs=1, random_state=0,
                            verbose=0, scoring=None):
     """Evaluate the significance of a cross-validated score with permutations
@@ -939,6 +952,10 @@ def permutation_test_score(estimator, X, y, groups=None, cv=None,
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     n_permutations : integer, optional
         Number of times to permute ``y``.
 
@@ -1025,8 +1042,8 @@ def _shuffle(y, groups, random_state):
 
 
 def learning_curve(estimator, X, y, groups=None,
-                   train_sizes=np.linspace(0.1, 1.0, 5), cv=None, scoring=None,
-                   exploit_incremental_learning=False, n_jobs=1,
+                   train_sizes=np.linspace(0.1, 1.0, 5), cv='warn',
+                   scoring=None, exploit_incremental_learning=False, n_jobs=1,
                    pre_dispatch="all", verbose=0, shuffle=False,
                    random_state=None):
     """Learning curve.
@@ -1085,6 +1102,10 @@ def learning_curve(estimator, X, y, groups=None,
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     scoring : string, callable or None, optional, default: None
         A string (see model evaluation documentation) or
         a scorer callable object / function with signature
@@ -1266,7 +1287,7 @@ def _incremental_fit_estimator(estimator, X, y, classes, train, test,
 
 
 def validation_curve(estimator, X, y, param_name, param_range, groups=None,
-                     cv=None, scoring=None, n_jobs=1, pre_dispatch="all",
+                     cv='warn', scoring=None, n_jobs=1, pre_dispatch="all",
                      verbose=0):
     """Validation curve.
 
@@ -1318,6 +1339,10 @@ def validation_curve(estimator, X, y, param_name, param_range, groups=None,
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.
 
+        .. versionchanged:: 0.20
+            ``cv`` default value if None will change from 3-fold to 5-fold
+            in v0.22.
+
     scoring : string, callable or None, optional, default: None
         A string (see model evaluation documentation) or
         a scorer callable object / function with signature
diff --git a/sklearn/model_selection/tests/test_search.py b/sklearn/model_selection/tests/test_search.py
index 9bd4c475d190..dd415cee7d2a 100644
--- a/sklearn/model_selection/tests/test_search.py
+++ b/sklearn/model_selection/tests/test_search.py
@@ -179,6 +179,8 @@ def test_parameter_grid():
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
+
 def test_grid_search():
     # Test that the best estimator contains the right value for foo_param
     clf = MockClassifier()
@@ -249,6 +251,7 @@ def test_grid_search_fit_params_deprecation():
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_grid_search_fit_params_two_places():
     # NOTE: Remove this test in v0.21
 
@@ -311,6 +314,7 @@ def test_grid_search_no_score():
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_grid_search_score_method():
     X, y = make_classification(n_samples=100, n_classes=2, flip_y=.2,
                                random_state=0)
@@ -341,6 +345,7 @@ def test_grid_search_score_method():
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_grid_search_groups():
     # Check if ValueError (when groups is None) propagates to GridSearchCV
     # And also check if groups is correctly passed to the cv object
@@ -375,9 +380,10 @@ def test_return_train_score_warn():
     y = np.array([0] * 5 + [1] * 5)
     grid = {'C': [1, 2]}
 
-    estimators = [GridSearchCV(LinearSVC(random_state=0), grid, iid=False),
+    estimators = [GridSearchCV(LinearSVC(random_state=0), grid,
+                               iid=False, cv=3),
                   RandomizedSearchCV(LinearSVC(random_state=0), grid,
-                                     n_iter=2, iid=False)]
+                                     n_iter=2, iid=False, cv=3)]
 
     result = {}
     for estimator in estimators:
@@ -406,6 +412,7 @@ def test_return_train_score_warn():
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_classes__property():
     # Test that classes_ property matches best_estimator_.classes_
     X = np.arange(100).reshape(10, 10)
@@ -434,6 +441,7 @@ def test_classes__property():
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_trivial_cv_results_attr():
     # Test search over a "grid" with only one point.
     clf = MockClassifier()
@@ -447,6 +455,7 @@ def test_trivial_cv_results_attr():
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_no_refit():
     # Test that GSCV can be used for model selection alone without refitting
     clf = MockClassifier()
@@ -478,6 +487,7 @@ def test_no_refit():
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_grid_search_error():
     # Test that grid search will capture errors on data with different length
     X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
@@ -488,6 +498,7 @@ def test_grid_search_error():
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_grid_search_one_grid_point():
     X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
     param_dict = {"C": [1.0], "kernel": ["rbf"], "gamma": [0.1]}
@@ -503,6 +514,7 @@ def test_grid_search_one_grid_point():
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_grid_search_when_param_grid_includes_range():
     # Test that the best estimator contains the right value for foo_param
     clf = MockClassifier()
@@ -516,6 +528,7 @@ def test_grid_search_when_param_grid_includes_range():
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_grid_search_bad_param_grid():
     param_dict = {"C": 1.0}
     clf = SVC(gamma='auto')
@@ -546,6 +559,7 @@ def test_grid_search_bad_param_grid():
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_grid_search_sparse():
     # Test that grid search works with both dense and sparse matrices
     X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
@@ -568,6 +582,7 @@ def test_grid_search_sparse():
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_grid_search_sparse_scoring():
     X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
 
@@ -604,6 +619,7 @@ def f1_loss(y_true_, y_pred_):
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_grid_search_precomputed_kernel():
     # Test that grid search works when the input features are given in the
     # form of a precomputed kernel matrix
@@ -633,6 +649,7 @@ def test_grid_search_precomputed_kernel():
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_grid_search_precomputed_kernel_error_nonsquare():
     # Test that grid search returns an error with a non-square precomputed
     # training kernel matrix
@@ -671,6 +688,7 @@ def test_refit():
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_gridsearch_nd():
     # Pass X as list in GridSearchCV
     X_4d = np.arange(10 * 5 * 3 * 2).reshape(10, 5, 3, 2)
@@ -741,6 +759,7 @@ def check_series(x):
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_unsupervised_grid_search():
     # test grid-search with unsupervised estimator
     X, y = make_blobs(random_state=0)
@@ -768,6 +787,7 @@ def test_unsupervised_grid_search():
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_gridsearch_no_predict():
     # test grid-search with an estimator without predict.
     # slight duplication of a test from KDE
@@ -1121,6 +1141,7 @@ def compare_refit_methods_when_refit_with_acc(search_multi, search_acc, refit):
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_search_cv_results_rank_tie_breaking():
     X, y = make_blobs(n_samples=50, random_state=42)
 
@@ -1153,6 +1174,7 @@ def test_search_cv_results_rank_tie_breaking():
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_search_cv_results_none_param():
     X, y = [[1], [2], [3], [4], [5]], [0, 0, 0, 0, 1]
     estimators = (DecisionTreeRegressor(), DecisionTreeClassifier())
@@ -1230,6 +1252,7 @@ def test_grid_search_correct_score_results():
                 assert_almost_equal(correct_score, cv_scores[i])
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_fit_grid_point():
     X, y = make_classification(random_state=0)
     cv = StratifiedKFold(random_state=0)
@@ -1259,6 +1282,7 @@ def test_fit_grid_point():
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_pickle():
     # Test that a fit search can be pickled
     clf = MockClassifier()
@@ -1277,6 +1301,7 @@ def test_pickle():
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_grid_search_with_multioutput_data():
     # Test search with multi-output estimator
 
@@ -1410,6 +1435,7 @@ def get_cand_scores(i):
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_grid_search_failing_classifier_raise():
     # GridSearchCV with on_error == 'raise' raises the error
 
@@ -1462,6 +1488,7 @@ def test_parameters_sampler_replacement():
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_stochastic_gradient_loss_param():
     # Make sure the predict_proba works when loss is specified
     # as one of the parameters in the param_grid.
@@ -1493,6 +1520,7 @@ def test_stochastic_gradient_loss_param():
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_search_train_scores_set_to_false():
     X = np.arange(6).reshape(6, -1)
     y = [0, 0, 0, 1, 1, 1]
@@ -1586,6 +1614,7 @@ def _pop_time_keys(cv_results):
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_transform_inverse_transform_round_trip():
     clf = MockClassifier()
     grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, verbose=3)
diff --git a/sklearn/model_selection/tests/test_split.py b/sklearn/model_selection/tests/test_split.py
index 98a1f808b4d7..5d63701e625f 100644
--- a/sklearn/model_selection/tests/test_split.py
+++ b/sklearn/model_selection/tests/test_split.py
@@ -1,7 +1,7 @@
 """Test the split module"""
 from __future__ import division
 import warnings
-
+import pytest
 import numpy as np
 from scipy.sparse import coo_matrix, csc_matrix, csr_matrix
 from scipy import stats
@@ -23,6 +23,7 @@
 from sklearn.utils.testing import assert_warns
 from sklearn.utils.testing import assert_raise_message
 from sklearn.utils.testing import ignore_warnings
+from sklearn.utils.testing import assert_no_warnings
 from sklearn.utils.validation import _num_samples
 from sklearn.utils.mocking import MockDataFrame
 
@@ -50,6 +51,8 @@
 from sklearn.model_selection._split import _validate_shuffle_split
 from sklearn.model_selection._split import _CVIterableWrapper
 from sklearn.model_selection._split import _build_repr
+from sklearn.model_selection._split import CV_WARNING
+from sklearn.model_selection._split import NSPLIT_WARNING
 
 from sklearn.datasets import load_digits
 from sklearn.datasets import make_classification
@@ -199,6 +202,7 @@ def test_cross_validator_with_default_params():
                          lpo.get_n_splits, None, y, groups)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_2d_y():
     # smoke test for 2d y and multi-label
     n_samples = 30
@@ -1398,6 +1402,7 @@ def test_time_series_max_train_size():
     _check_time_series_max_train_size(splits, check_splits, max_train_size=2)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_nested_cv():
     # Test if nested cross validation works with different combinations of cv
     rng = np.random.RandomState(0)
@@ -1423,6 +1428,26 @@ def test_train_test_default_warning():
                  train_size=0.75)
 
 
+def test_nsplit_default_warn():
+    # Test that warnings are raised. Will be removed in 0.22
+    assert_warns_message(FutureWarning, NSPLIT_WARNING, KFold)
+    assert_warns_message(FutureWarning, NSPLIT_WARNING, GroupKFold)
+    assert_warns_message(FutureWarning, NSPLIT_WARNING, StratifiedKFold)
+    assert_warns_message(FutureWarning, NSPLIT_WARNING, TimeSeriesSplit)
+
+    assert_no_warnings(KFold, n_splits=5)
+    assert_no_warnings(GroupKFold, n_splits=5)
+    assert_no_warnings(StratifiedKFold, n_splits=5)
+    assert_no_warnings(TimeSeriesSplit, n_splits=5)
+
+
+def test_check_cv_default_warn():
+    # Test that warnings are raised. Will be removed in 0.22
+    assert_warns_message(FutureWarning, CV_WARNING, check_cv)
+
+    assert_no_warnings(check_cv, cv=5)
+
+
 def test_build_repr():
     class MockSplitter:
         def __init__(self, a, b=0, c=None):
diff --git a/sklearn/model_selection/tests/test_validation.py b/sklearn/model_selection/tests/test_validation.py
index f0a6564270b7..7e7c0d398620 100644
--- a/sklearn/model_selection/tests/test_validation.py
+++ b/sklearn/model_selection/tests/test_validation.py
@@ -240,6 +240,7 @@ def get_params(self, deep=False):
 
 
 @pytest.mark.filterwarnings('ignore: From version 0.22, errors during fit')
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 # FIXME issue in error_score parameter
 def test_cross_val_score():
     clf = MockClassifier()
@@ -281,6 +282,7 @@ def test_cross_val_score():
     assert_raises(ValueError, cross_val_score, clf, X_3d, y2)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_validate_invalid_scoring_param():
     X, y = make_classification(random_state=0)
     estimator = MockClassifier()
@@ -402,7 +404,7 @@ def test_cross_validate_return_train_score_warn():
     result = {}
     for val in [False, True, 'warn']:
         result[val] = assert_no_warnings(cross_validate, estimator, X, y,
-                                         return_train_score=val)
+                                         return_train_score=val, cv=5)
 
     msg = (
         'You are accessing a training score ({!r}), '
@@ -509,6 +511,7 @@ def check_cross_validate_multi_metric(clf, X, y, scores):
             assert np.all(cv_results['score_time'] < 10)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_score_predict_groups():
     # Check if ValueError (when groups is None) propagates to cross_val_score
     # and cross_val_predict
@@ -529,6 +532,7 @@ def test_cross_val_score_predict_groups():
 
 
 @pytest.mark.filterwarnings('ignore: Using or importing the ABCs from')
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_score_pandas():
     # check cross_val_score doesn't destroy pandas dataframe
     types = [(MockDataFrame, MockDataFrame)]
@@ -566,6 +570,7 @@ def test_cross_val_score_mask():
     assert_array_equal(scores_indices, scores_masks)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_score_precomputed():
     # test for svm with precomputed kernel
     svm = SVC(kernel="precomputed")
@@ -592,6 +597,7 @@ def test_cross_val_score_precomputed():
                   linear_kernel.tolist(), y)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_score_fit_params():
     clf = MockClassifier()
     n_samples = X.shape[0]
@@ -785,6 +791,7 @@ def test_cross_val_score_multilabel():
     assert_almost_equal(score_samples, [1, 1 / 2, 3 / 4, 1 / 2, 1 / 4])
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict():
     boston = load_boston()
     X, y = boston.data, boston.target
@@ -834,6 +841,7 @@ def split(self, X, y=None, groups=None):
                          X, y, method='predict_proba', cv=KFold(2))
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict_decision_function_shape():
     X, y = make_classification(n_classes=2, n_samples=50, random_state=0)
 
@@ -880,6 +888,7 @@ def test_cross_val_predict_decision_function_shape():
                         cv=KFold(n_splits=3), method='decision_function')
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict_predict_proba_shape():
     X, y = make_classification(n_classes=2, n_samples=50, random_state=0)
 
@@ -894,6 +903,7 @@ def test_cross_val_predict_predict_proba_shape():
     assert_equal(preds.shape, (150, 3))
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict_predict_log_proba_shape():
     X, y = make_classification(n_classes=2, n_samples=50, random_state=0)
 
@@ -908,6 +918,7 @@ def test_cross_val_predict_predict_log_proba_shape():
     assert_equal(preds.shape, (150, 3))
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict_input_types():
     iris = load_iris()
     X, y = iris.data, iris.target
@@ -954,6 +965,7 @@ def test_cross_val_predict_input_types():
 
 
 @pytest.mark.filterwarnings('ignore: Using or importing the ABCs from')
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 # python3.7 deprecation warnings in pandas via matplotlib :-/
 def test_cross_val_predict_pandas():
     # check cross_val_score doesn't destroy pandas dataframe
@@ -972,6 +984,7 @@ def test_cross_val_predict_pandas():
         cross_val_predict(clf, X_df, y_ser)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_score_sparse_fit_params():
     iris = load_iris()
     X, y = iris.data, iris.target
@@ -1325,11 +1338,13 @@ def check_cross_val_predict_with_method(est):
         assert_array_equal(predictions, predictions_ystr)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict_with_method():
     check_cross_val_predict_with_method(LogisticRegression())
 
 
 @pytest.mark.filterwarnings('ignore: max_iter and tol parameters')
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict_method_checking():
     # Regression test for issue #9639. Tests that cross_val_predict does not
     # check estimator methods (e.g. predict_proba) before fitting
@@ -1338,6 +1353,7 @@ def test_cross_val_predict_method_checking():
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_gridsearchcv_cross_val_predict_with_method():
     est = GridSearchCV(LogisticRegression(random_state=42),
                        {'C': [0.1, 1]},
@@ -1365,6 +1381,7 @@ def get_expected_predictions(X, y, cv, classes, est, method):
     return expected_predictions
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict_class_subset():
 
     X = np.arange(200).reshape(100, 2)
@@ -1406,6 +1423,7 @@ def test_cross_val_predict_class_subset():
         assert_array_almost_equal(expected_predictions, predictions)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_score_memmap():
     # Ensure a scalar score of memmap type is accepted
     iris = load_iris()
@@ -1434,6 +1452,7 @@ def test_score_memmap():
 
 
 @pytest.mark.filterwarnings('ignore: Using or importing the ABCs from')
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_permutation_test_score_pandas():
     # check permutation_test_score doesn't destroy pandas dataframe
     types = [(MockDataFrame, MockDataFrame)]
diff --git a/sklearn/neighbors/approximate.py b/sklearn/neighbors/approximate.py
index 4a30bd815998..650af47e0d81 100644
--- a/sklearn/neighbors/approximate.py
+++ b/sklearn/neighbors/approximate.py
@@ -82,16 +82,53 @@ def _to_hash(projected):
         return out.reshape(projected.shape[0], -1)
 
     def fit_transform(self, X, y=None):
+        """
+        Parameters
+        ----------
+        X : array-like, shape = [n_samples, n_features]
+            Training vectors, where n_samples is the number of samples and
+            n_features is the number of predictors.
+        """
+
         self.fit(X)
         return self.transform(X)
 
     def transform(self, X):
+        """
+        Parameters
+        ----------
+        X : array-like, shape = [n_samples, n_features]
+            Training vectors, where n_samples is the number of samples and
+            n_features is the number of predictors.
+        """
         return self._to_hash(super(ProjectionToHashMixin, self).transform(X))
 
 
 class GaussianRandomProjectionHash(ProjectionToHashMixin,
                                    GaussianRandomProjection):
-    """Use GaussianRandomProjection to produce a cosine LSH fingerprint"""
+    """Use GaussianRandomProjection to produce a cosine LSH fingerprint
+
+    Parameters
+    ----------
+
+    n_components : int or 'auto', optional (default = 32)
+        Dimensionality of the target projection space.
+
+        n_components can be automatically adjusted according to the
+        number of samples in the dataset and the bound given by the
+        Johnson-Lindenstrauss lemma. In that case the quality of the
+        embedding is controlled by the ``eps`` parameter.
+
+        It should be noted that Johnson-Lindenstrauss lemma can yield
+        very conservative estimated of the required number of components
+        as it makes no assumption on the structure of the dataset.
+
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
+    """
     def __init__(self,
                  n_components=32,
                  random_state=None):
diff --git a/sklearn/neighbors/kde.py b/sklearn/neighbors/kde.py
index 140c72b526fa..ff5920b68ea5 100644
--- a/sklearn/neighbors/kde.py
+++ b/sklearn/neighbors/kde.py
@@ -121,7 +121,7 @@ def fit(self, X, y=None, sample_weight=None):
         X : array_like, shape (n_samples, n_features)
             List of n_features-dimensional data points.  Each row
             corresponds to a single data point.
-        sample_weight: array_like, shape (n_samples,), optional
+        sample_weight : array_like, shape (n_samples,), optional
             List of sample weights attached to the data X.
         """
         algorithm = self._choose_algorithm(self.algorithm, self.metric)
diff --git a/sklearn/neighbors/lof.py b/sklearn/neighbors/lof.py
index a2589f792331..3d1b40ec61db 100644
--- a/sklearn/neighbors/lof.py
+++ b/sklearn/neighbors/lof.py
@@ -100,6 +100,17 @@ class LocalOutlierFactor(NeighborsBase, KNeighborsMixin, UnsupervisedMixin,
         threshold on the decision function. If "auto", the decision function
         threshold is determined as in the original paper.
 
+        .. versionchanged:: 0.20
+           The default value of ``contamination`` will change from 0.1 in 0.20
+           to ``'auto'`` in 0.22.
+
+    novelty : boolean, default False
+        By default, LocalOutlierFactor is only meant to be used for outlier
+        detection (novelty=False). Set novelty to True if you want to use
+        LocalOutlierFactor for novelty detection. In this case be aware that
+        that you should only use predict, decision_function and score_samples
+        on new unseen data and not on the training set.
+
     n_jobs : int, optional (default=1)
         The number of parallel jobs to run for neighbors search.
         If ``-1``, then the number of jobs is set to the number of CPU cores.
@@ -137,25 +148,49 @@ class LocalOutlierFactor(NeighborsBase, KNeighborsMixin, UnsupervisedMixin,
     """
     def __init__(self, n_neighbors=20, algorithm='auto', leaf_size=30,
                  metric='minkowski', p=2, metric_params=None,
-                 contamination="legacy", n_jobs=1):
+                 contamination="legacy", novelty=False, n_jobs=1):
         super(LocalOutlierFactor, self).__init__(
-              n_neighbors=n_neighbors,
-              algorithm=algorithm,
-              leaf_size=leaf_size, metric=metric, p=p,
-              metric_params=metric_params, n_jobs=n_jobs)
-
-        if contamination == "legacy":
-            warnings.warn('default contamination parameter 0.1 will change '
-                          'in version 0.22 to "auto". This will change the '
-                          'predict method behavior.',
-                          DeprecationWarning)
+            n_neighbors=n_neighbors,
+            algorithm=algorithm,
+            leaf_size=leaf_size, metric=metric, p=p,
+            metric_params=metric_params, n_jobs=n_jobs)
         self.contamination = contamination
+        self.novelty = novelty
+
+    @property
+    def fit_predict(self):
+        """"Fits the model to the training set X and returns the labels.
+
+        Label is 1 for an inlier and -1 for an outlier according to the LOF
+        score and the contamination parameter.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features), default=None
+            The query sample or samples to compute the Local Outlier Factor
+            w.r.t. to the training samples.
+
+        Returns
+        -------
+        is_inlier : array, shape (n_samples,)
+            Returns -1 for anomalies/outliers and 1 for inliers.
+        """
+
+        # As fit_predict would be different from fit.predict, fit_predict is
+        # only available for outlier detection (novelty=False)
 
-    def fit_predict(self, X, y=None):
-        """"Fits the model to the training set X and returns the labels
-        (1 inlier, -1 outlier) on the training set according to the LOF score
-        and the contamination parameter.
+        if self.novelty:
+            msg = ('fit_predict is not available when novelty=True. Use '
+                   'novelty=False if you want to predict on the training set.')
+            raise AttributeError(msg)
 
+        return self._fit_predict
+
+    def _fit_predict(self, X, y=None):
+        """"Fits the model to the training set X and returns the labels.
+
+        Label is 1 for an inlier and -1 for an outlier according to the LOF
+        score and the contamination parameter.
 
         Parameters
         ----------
@@ -169,6 +204,9 @@ def fit_predict(self, X, y=None):
             Returns -1 for anomalies/outliers and 1 for inliers.
         """
 
+        # As fit_predict would be different from fit.predict, fit_predict is
+        # only available for outlier detection (novelty=False)
+
         return self.fit(X)._predict()
 
     def fit(self, X, y=None):
@@ -184,10 +222,19 @@ def fit(self, X, y=None):
         -------
         self : object
         """
-        if self.contamination not in ["auto", "legacy"]:  # rm legacy in 0.22
-            if not(0. < self.contamination <= .5):
+        if self.contamination == "legacy":
+            warnings.warn('default contamination parameter 0.1 will change '
+                          'in version 0.22 to "auto". This will change the '
+                          'predict method behavior.',
+                          FutureWarning)
+            self._contamination = 0.1
+        else:
+            self._contamination = self.contamination
+
+        if self._contamination != 'auto':
+            if not(0. < self._contamination <= .5):
                 raise ValueError("contamination must be in (0, 0.5], "
-                                 "got: %f" % self.contamination)
+                                 "got: %f" % self._contamination)
 
         super(LocalOutlierFactor, self).fit(X)
 
@@ -211,26 +258,47 @@ def fit(self, X, y=None):
 
         self.negative_outlier_factor_ = -np.mean(lrd_ratios_array, axis=1)
 
-        if self.contamination == "auto":
+        if self._contamination == "auto":
             # inliers score around -1 (the higher, the less abnormal).
             self.offset_ = -1.5
-        elif self.contamination == "legacy":  # to rm in 0.22
-            self.offset_ = scoreatpercentile(
-                self.negative_outlier_factor_, 100. * 0.1)
         else:
             self.offset_ = scoreatpercentile(
-                self.negative_outlier_factor_, 100. * self.contamination)
+                self.negative_outlier_factor_, 100. * self._contamination)
 
         return self
 
+    @property
+    def predict(self):
+        """Predict the labels (1 inlier, -1 outlier) of X according to LOF.
+
+        This method allows to generalize prediction to *new observations* (not
+        in the training set). Only available for novelty detection (when
+        novelty is set to True).
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+            The query sample or samples to compute the Local Outlier Factor
+            w.r.t. to the training samples.
+
+        Returns
+        -------
+        is_inlier : array, shape (n_samples,)
+            Returns -1 for anomalies/outliers and +1 for inliers.
+        """
+        if not self.novelty:
+            msg = ('predict is not available when novelty=False, use '
+                   'fit_predict if you want to predict on training data. Use '
+                   'novelty=True if you want to use LOF for novelty detection '
+                   'and predict on new unseen data.')
+            raise AttributeError(msg)
+
+        return self._predict
+
     def _predict(self, X=None):
         """Predict the labels (1 inlier, -1 outlier) of X according to LOF.
 
         If X is None, returns the same as fit_predict(X_train).
-        This method allows to generalize prediction to new observations (not
-        in the training set). As LOF originally does not deal with new data,
-        this method is kept private. In particular, fit(X)._predict(X) is not
-        the same as fit_predict(X).
 
         Parameters
         ----------
@@ -250,26 +318,61 @@ def _predict(self, X=None):
         if X is not None:
             X = check_array(X, accept_sparse='csr')
             is_inlier = np.ones(X.shape[0], dtype=int)
-            is_inlier[self._decision_function(X) < 0] = -1
+            is_inlier[self.decision_function(X) < 0] = -1
         else:
             is_inlier = np.ones(self._fit_X.shape[0], dtype=int)
             is_inlier[self.negative_outlier_factor_ < self.offset_] = -1
 
         return is_inlier
 
+    @property
+    def decision_function(self):
+        """Shifted opposite of the Local Outlier Factor of X.
+
+        Bigger is better, i.e. large values correspond to inliers.
+
+        The shift offset allows a zero threshold for being an outlier.
+        Only available for novelty detection (when novelty is set to True).
+        The argument X is supposed to contain *new data*: if X contains a
+        point from training, it considers the later in its own neighborhood.
+        Also, the samples in X are not considered in the neighborhood of any
+        point.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+            The query sample or samples to compute the Local Outlier Factor
+            w.r.t. the training samples.
+
+        Returns
+        -------
+        shifted_opposite_lof_scores : array, shape (n_samples,)
+            The shifted opposite of the Local Outlier Factor of each input
+            samples. The lower, the more abnormal. Negative scores represent
+            outliers, positive scores represent inliers.
+        """
+        if not self.novelty:
+            msg = ('decision_function is not available when novelty=False. '
+                   'Use novelty=True if you want to use LOF for novelty '
+                   'detection and compute decision_function for new unseen '
+                   'data. Note that the opposite LOF of the training samples '
+                   'is always available by considering the '
+                   'negative_outlier_factor_ attribute.')
+            raise AttributeError(msg)
+
+        return self._decision_function
+
     def _decision_function(self, X):
-        """Shifted opposite of the Local Outlier Factor of X
+        """Shifted opposite of the Local Outlier Factor of X.
 
         Bigger is better, i.e. large values correspond to inliers.
 
         The shift offset allows a zero threshold for being an outlier.
+        Only available for novelty detection (when novelty is set to True).
         The argument X is supposed to contain *new data*: if X contains a
-        point from training, it consider the later in its own neighborhood.
+        point from training, it considers the later in its own neighborhood.
         Also, the samples in X are not considered in the neighborhood of any
         point.
-        This method is kept private as the predict method is.
-        The decision function on training data is available by considering the
-        the negative_outlier_factor_ attribute.
 
         Parameters
         ----------
@@ -284,17 +387,59 @@ def _decision_function(self, X):
             samples. The lower, the more abnormal. Negative scores represent
             outliers, positive scores represent inliers.
         """
+
         return self._score_samples(X) - self.offset_
 
+    @property
+    def score_samples(self):
+        """Opposite of the Local Outlier Factor of X.
+
+        It is the opposite as as bigger is better, i.e. large values correspond
+        to inliers.
+
+        Only available for novelty detection (when novelty is set to True).
+        The argument X is supposed to contain *new data*: if X contains a
+        point from training, it considers the later in its own neighborhood.
+        Also, the samples in X are not considered in the neighborhood of any
+        point.
+        The score_samples on training data is available by considering the
+        the negative_outlier_factor_ attribute.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+            The query sample or samples to compute the Local Outlier Factor
+            w.r.t. the training samples.
+
+        Returns
+        -------
+        opposite_lof_scores : array, shape (n_samples,)
+            The opposite of the Local Outlier Factor of each input samples.
+            The lower, the more abnormal.
+        """
+        if not self.novelty:
+            msg = ('score_samples is not available when novelty=False. The '
+                   'scores of the training samples are always available '
+                   'through the negative_outlier_factor_ attribute. Use '
+                   'novelty=True if you want to use LOF for novelty detection '
+                   'and compute score_samples for new unseen data.')
+            raise AttributeError(msg)
+
+        return self._score_samples
+
     def _score_samples(self, X):
-        """Opposite of the Local Outlier Factor of X (as bigger is
-        better, i.e. large values correspond to inliers).
+        """Opposite of the Local Outlier Factor of X.
 
+        It is the opposite as as bigger is better, i.e. large values correspond
+        to inliers.
+
+        Only available for novelty detection (when novelty is set to True).
         The argument X is supposed to contain *new data*: if X contains a
-        point from training, it consider the later in its own neighborhood.
+        point from training, it considers the later in its own neighborhood.
         Also, the samples in X are not considered in the neighborhood of any
         point.
-        This method is kept private as the predict method is.
+        The score_samples on training data is available by considering the
+        the negative_outlier_factor_ attribute.
 
         Parameters
         ----------
diff --git a/sklearn/neighbors/tests/test_lof.py b/sklearn/neighbors/tests/test_lof.py
index a6fb572b6740..ed57a1d0fba2 100644
--- a/sklearn/neighbors/tests/test_lof.py
+++ b/sklearn/neighbors/tests/test_lof.py
@@ -3,6 +3,8 @@
 # License: BSD 3 clause
 
 from math import sqrt
+
+import pytest
 import numpy as np
 from sklearn import neighbors
 
@@ -15,7 +17,10 @@
 from sklearn.utils.testing import assert_greater, ignore_warnings
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_equal
-from sklearn.utils.testing import assert_warns_message, assert_raises
+from sklearn.utils.testing import assert_warns_message
+from sklearn.utils.testing import assert_raises
+from sklearn.utils.testing import assert_raises_regex
+from sklearn.utils.estimator_checks import check_estimator
 
 from sklearn.datasets import load_iris
 
@@ -29,8 +34,9 @@
 iris.target = iris.target[perm]
 
 
-@ignore_warnings(category=DeprecationWarning)
-# contamination changed to 'auto' 0.22
+@pytest.mark.filterwarnings(
+    'ignore:default contamination parameter 0.1:FutureWarning')
+# XXX: Remove in 0.22
 def test_lof():
     # Toy sample (the last two samples are outliers):
     X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], [5, 3], [-4, 2]]
@@ -47,10 +53,12 @@ def test_lof():
     clf = neighbors.LocalOutlierFactor(contamination=0.25,
                                        n_neighbors=5).fit(X)
     assert_array_equal(clf._predict(), 6 * [1] + 2 * [-1])
+    assert_array_equal(clf.fit_predict(X), 6 * [1] + 2 * [-1])
 
 
-@ignore_warnings(category=DeprecationWarning)
-# contamination changed to 'auto' 0.22
+@pytest.mark.filterwarnings(
+    'ignore:default contamination parameter 0.1:FutureWarning')
+# XXX: Remove in 0.22
 def test_lof_performance():
     # Generate train/test data
     rng = check_random_state(2)
@@ -62,39 +70,43 @@ def test_lof_performance():
     X_test = np.r_[X[100:], X_outliers]
     y_test = np.array([0] * 20 + [1] * 20)
 
-    # fit the model
-    clf = neighbors.LocalOutlierFactor().fit(X_train)
+    # fit the model for novelty detection
+    clf = neighbors.LocalOutlierFactor(novelty=True).fit(X_train)
 
     # predict scores (the lower, the more normal)
-    y_pred = -clf._decision_function(X_test)
+    y_pred = -clf.decision_function(X_test)
 
     # check that roc_auc is good
     assert_greater(roc_auc_score(y_test, y_pred), .99)
 
 
-@ignore_warnings(category=DeprecationWarning)
-# contamination changed to 'auto' 0.22
+@pytest.mark.filterwarnings(
+    'ignore:default contamination parameter 0.1:FutureWarning')
+# XXX: Remove in 0.22
 def test_lof_values():
     # toy samples:
     X_train = [[1, 1], [1, 2], [2, 1]]
     clf1 = neighbors.LocalOutlierFactor(n_neighbors=2,
-                                        contamination=0.1).fit(X_train)
-    clf2 = neighbors.LocalOutlierFactor(n_neighbors=2).fit(X_train)
+                                        contamination=0.1,
+                                        novelty=True).fit(X_train)
+    clf2 = neighbors.LocalOutlierFactor(n_neighbors=2,
+                                        novelty=True).fit(X_train)
     s_0 = 2. * sqrt(2.) / (1. + sqrt(2.))
     s_1 = (1. + sqrt(2)) * (1. / (4. * sqrt(2.)) + 1. / (2. + 2. * sqrt(2)))
     # check predict()
     assert_array_almost_equal(-clf1.negative_outlier_factor_, [s_0, s_1, s_1])
     assert_array_almost_equal(-clf2.negative_outlier_factor_, [s_0, s_1, s_1])
     # check predict(one sample not in train)
-    assert_array_almost_equal(-clf1._score_samples([[2., 2.]]), [s_0])
-    assert_array_almost_equal(-clf2._score_samples([[2., 2.]]), [s_0])
+    assert_array_almost_equal(-clf1.score_samples([[2., 2.]]), [s_0])
+    assert_array_almost_equal(-clf2.score_samples([[2., 2.]]), [s_0])
     # check predict(one sample already in train)
-    assert_array_almost_equal(-clf1._score_samples([[1., 1.]]), [s_1])
-    assert_array_almost_equal(-clf2._score_samples([[1., 1.]]), [s_1])
+    assert_array_almost_equal(-clf1.score_samples([[1., 1.]]), [s_1])
+    assert_array_almost_equal(-clf2.score_samples([[1., 1.]]), [s_1])
 
 
-@ignore_warnings(category=DeprecationWarning)
-# contamination changed to 'auto' 0.22
+@pytest.mark.filterwarnings(
+    'ignore:default contamination parameter 0.1:FutureWarning')
+# XXX: Remove in 0.22
 def test_lof_precomputed(random_state=42):
     """Tests LOF with a distance matrix."""
     # Note: smaller samples may result in spurious test success
@@ -104,24 +116,25 @@ def test_lof_precomputed(random_state=42):
     DXX = metrics.pairwise_distances(X, metric='euclidean')
     DYX = metrics.pairwise_distances(Y, X, metric='euclidean')
     # As a feature matrix (n_samples by n_features)
-    lof_X = neighbors.LocalOutlierFactor(n_neighbors=3)
+    lof_X = neighbors.LocalOutlierFactor(n_neighbors=3, novelty=True)
     lof_X.fit(X)
     pred_X_X = lof_X._predict()
-    pred_X_Y = lof_X._predict(Y)
+    pred_X_Y = lof_X.predict(Y)
 
     # As a dense distance matrix (n_samples by n_samples)
     lof_D = neighbors.LocalOutlierFactor(n_neighbors=3, algorithm='brute',
-                                         metric='precomputed')
+                                         metric='precomputed', novelty=True)
     lof_D.fit(DXX)
     pred_D_X = lof_D._predict()
-    pred_D_Y = lof_D._predict(DYX)
+    pred_D_Y = lof_D.predict(DYX)
 
     assert_array_almost_equal(pred_X_X, pred_D_X)
     assert_array_almost_equal(pred_X_Y, pred_D_Y)
 
 
-@ignore_warnings(category=DeprecationWarning)
-# contamination changed to 'auto' 0.22
+@pytest.mark.filterwarnings(
+    'ignore:default contamination parameter 0.1:FutureWarning')
+# XXX: Remove in 0.22
 def test_n_neighbors_attribute():
     X = iris.data
     clf = neighbors.LocalOutlierFactor(n_neighbors=500).fit(X)
@@ -134,19 +147,22 @@ def test_n_neighbors_attribute():
     assert_equal(clf.n_neighbors_, X.shape[0] - 1)
 
 
-@ignore_warnings(category=DeprecationWarning)
-# contamination changed to 'auto' 0.22
+@pytest.mark.filterwarnings(
+    'ignore:default contamination parameter 0.1:FutureWarning')
+# XXX: Remove in 0.22
 def test_score_samples():
     X_train = [[1, 1], [1, 2], [2, 1]]
     clf1 = neighbors.LocalOutlierFactor(n_neighbors=2,
-                                        contamination=0.1).fit(X_train)
-    clf2 = neighbors.LocalOutlierFactor(n_neighbors=2).fit(X_train)
-    assert_array_equal(clf1._score_samples([[2., 2.]]),
-                       clf1._decision_function([[2., 2.]]) + clf1.offset_)
-    assert_array_equal(clf2._score_samples([[2., 2.]]),
-                       clf2._decision_function([[2., 2.]]) + clf2.offset_)
-    assert_array_equal(clf1._score_samples([[2., 2.]]),
-                       clf2._score_samples([[2., 2.]]))
+                                        contamination=0.1,
+                                        novelty=True).fit(X_train)
+    clf2 = neighbors.LocalOutlierFactor(n_neighbors=2,
+                                        novelty=True).fit(X_train)
+    assert_array_equal(clf1.score_samples([[2., 2.]]),
+                       clf1.decision_function([[2., 2.]]) + clf1.offset_)
+    assert_array_equal(clf2.score_samples([[2., 2.]]),
+                       clf2.decision_function([[2., 2.]]) + clf2.offset_)
+    assert_array_equal(clf1.score_samples([[2., 2.]]),
+                       clf2.score_samples([[2., 2.]]))
 
 
 def test_contamination():
@@ -155,8 +171,84 @@ def test_contamination():
     assert_raises(ValueError, clf.fit, X)
 
 
-def test_deprecation():
-    assert_warns_message(DeprecationWarning,
+@pytest.mark.filterwarnings(
+    'ignore:default contamination parameter 0.1:FutureWarning')
+# XXX: Remove in 0.22
+def test_novelty_errors():
+    X = iris.data
+
+    # check errors for novelty=False
+    clf = neighbors.LocalOutlierFactor()
+    clf.fit(X)
+    # predict, decision_function and score_samples raise ValueError
+    for method in ['predict', 'decision_function', 'score_samples']:
+        msg = ('{} is not available when novelty=False'.format(method))
+        assert_raises_regex(AttributeError, msg, getattr, clf, method)
+
+    # check errors for novelty=True
+    clf = neighbors.LocalOutlierFactor(novelty=True)
+    msg = 'fit_predict is not available when novelty=True'
+    assert_raises_regex(AttributeError, msg, getattr, clf, 'fit_predict')
+
+
+@pytest.mark.filterwarnings(
+    'ignore:default contamination parameter 0.1:FutureWarning')
+# XXX: Remove in 0.22
+def test_novelty_training_scores():
+    # check that the scores of the training samples are still accessible
+    # when novelty=True through the negative_outlier_factor_ attribute
+    X = iris.data
+
+    # fit with novelty=False
+    clf_1 = neighbors.LocalOutlierFactor()
+    clf_1.fit(X)
+    scores_1 = clf_1.negative_outlier_factor_
+
+    # fit with novelty=True
+    clf_2 = neighbors.LocalOutlierFactor(novelty=True)
+    clf_2.fit(X)
+    scores_2 = clf_2.negative_outlier_factor_
+
+    assert_array_almost_equal(scores_1, scores_2)
+
+
+@pytest.mark.filterwarnings(
+    'ignore:default contamination parameter 0.1:FutureWarning')
+# XXX: Remove in 0.22
+def test_hasattr_prediction():
+    # check availability of prediction methods depending on novelty value.
+    X = [[1, 1], [1, 2], [2, 1]]
+
+    # when novelty=True
+    clf = neighbors.LocalOutlierFactor(novelty=True)
+    clf.fit(X)
+    assert hasattr(clf, 'predict')
+    assert hasattr(clf, 'decision_function')
+    assert hasattr(clf, 'score_samples')
+    assert not hasattr(clf, 'fit_predict')
+
+    # when novelty=False
+    clf = neighbors.LocalOutlierFactor(novelty=False)
+    clf.fit(X)
+    assert hasattr(clf, 'fit_predict')
+    assert not hasattr(clf, 'predict')
+    assert not hasattr(clf, 'decision_function')
+    assert not hasattr(clf, 'score_samples')
+
+
+@pytest.mark.filterwarnings(
+    'ignore:default contamination parameter 0.1:FutureWarning')
+# XXX: Remove in 0.22
+def test_novelty_true_common_tests():
+
+    # the common tests are run for the default LOF (novelty=False).
+    # here we run these common tests for LOF when novelty=True
+    check_estimator(neighbors.LocalOutlierFactor(novelty=True))
+
+
+def test_contamination_future_warning():
+    X = [[1, 1], [1, 2], [2, 1]]
+    assert_warns_message(FutureWarning,
                          'default contamination parameter 0.1 will change '
                          'in version 0.22 to "auto"',
-                         neighbors.LocalOutlierFactor, )
+                         neighbors.LocalOutlierFactor().fit, X)
diff --git a/sklearn/neighbors/tests/test_neighbors.py b/sklearn/neighbors/tests/test_neighbors.py
index e1acaa4c6f13..db786a3588c2 100644
--- a/sklearn/neighbors/tests/test_neighbors.py
+++ b/sklearn/neighbors/tests/test_neighbors.py
@@ -181,6 +181,7 @@ def test_precomputed(random_state=42):
         assert_array_almost_equal(pred_X, pred_D)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_precomputed_cross_validation():
     # Ensure array is split correctly
     rng = np.random.RandomState(0)
diff --git a/sklearn/preprocessing/data.py b/sklearn/preprocessing/data.py
index e3c72d688459..2b17f41010ee 100644
--- a/sklearn/preprocessing/data.py
+++ b/sklearn/preprocessing/data.py
@@ -1771,6 +1771,9 @@ class KernelCenterer(BaseEstimator, TransformerMixin):
 
     Read more in the :ref:`User Guide <kernel_centering>`.
     """
+    def __init__(self):
+        # Needed for backported inspect.signature compatibility with PyPy
+        pass
 
     def fit(self, K, y=None):
         """Fit KernelCenterer
diff --git a/sklearn/tests/test_calibration.py b/sklearn/tests/test_calibration.py
index c99f79bbebd3..e454633a3a29 100644
--- a/sklearn/tests/test_calibration.py
+++ b/sklearn/tests/test_calibration.py
@@ -26,6 +26,7 @@
 
 
 @pytest.mark.filterwarnings('ignore:The default value of n_estimators')
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_calibration():
     """Test calibration objects with isotonic and sigmoid"""
     n_samples = 100
@@ -101,6 +102,7 @@ def test_calibration():
         assert_raises(RuntimeError, clf_base_regressor.fit, X_train, y_train)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_sample_weight():
     n_samples = 100
     X, y = make_classification(n_samples=2 * n_samples, n_features=6,
diff --git a/sklearn/tests/test_common.py b/sklearn/tests/test_common.py
index 2cde1ca9a14a..8e5f020985b1 100644
--- a/sklearn/tests/test_common.py
+++ b/sklearn/tests/test_common.py
@@ -28,6 +28,7 @@
 from sklearn.cluster.bicluster import BiclusterMixin
 
 from sklearn.linear_model.base import LinearClassifierMixin
+from sklearn.utils import IS_PYPY
 from sklearn.utils.estimator_checks import (
     _yield_all_checks,
     set_checking_parameters,
@@ -163,6 +164,9 @@ def test_import_all_consistency():
     for modname in submods + ['sklearn']:
         if ".tests." in modname:
             continue
+        if IS_PYPY and ('_svmlight_format' in modname or
+                        'feature_extraction._hashing' in modname):
+            continue
         package = __import__(modname, fromlist="dummy")
         for name in getattr(package, '__all__', ()):
             if getattr(package, name, None) is None:
diff --git a/sklearn/tests/test_docstring_parameters.py b/sklearn/tests/test_docstring_parameters.py
index 69f72d934479..df139743d7c0 100644
--- a/sklearn/tests/test_docstring_parameters.py
+++ b/sklearn/tests/test_docstring_parameters.py
@@ -12,6 +12,7 @@
 
 import sklearn
 from sklearn.base import signature
+from sklearn.utils import IS_PYPY
 from sklearn.utils.testing import SkipTest
 from sklearn.utils.testing import check_docstring_parameters
 from sklearn.utils.testing import _get_func_name
@@ -27,13 +28,10 @@
 IGNORED_MODULES = (
     'cluster',
     'datasets',
-    'feature_selection',
-    'kernel_approximation',
     'model_selection',
     'multioutput',
     'setup',
     'utils',
-    'neighbors',
     # Deprecated modules
     'cross_validation',
     'grid_search',
@@ -146,6 +144,11 @@ def test_tabs():
     # Test that there are no tabs in our source files
     for importer, modname, ispkg in walk_packages(sklearn.__path__,
                                                   prefix='sklearn.'):
+
+        if IS_PYPY and ('_svmlight_format' in modname or
+                        'feature_extraction._hashing' in modname):
+            continue
+
         # because we don't import
         mod = importlib.import_module(modname)
         try:
diff --git a/sklearn/tests/test_impute.py b/sklearn/tests/test_impute.py
index a221feb7b453..b53f0f5012f0 100644
--- a/sklearn/tests/test_impute.py
+++ b/sklearn/tests/test_impute.py
@@ -463,6 +463,7 @@ def test_imputation_constant_pandas(dtype):
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_imputation_pipeline_grid_search():
     # Test imputation within a pipeline + gridsearch.
     X = sparse_random_matrix(100, 100, density=0.10)
@@ -735,6 +736,7 @@ def test_sampling_error_invalid_type(dtype):
         imputer.transform(X.astype(dtype=dtype))
 
 
+@pytest.mark.filterwarnings('ignore: in the future, full')
 def test_sampling_preserved_statistics():
     # check that: - filled values are drawn only within non-missing values
     #             - different random_states give different imputations
diff --git a/sklearn/tests/test_metaestimators.py b/sklearn/tests/test_metaestimators.py
index 36885ee8229d..1c2d5a0873cd 100644
--- a/sklearn/tests/test_metaestimators.py
+++ b/sklearn/tests/test_metaestimators.py
@@ -1,5 +1,5 @@
 """Common tests for metaestimators"""
-
+import pytest
 import functools
 
 import numpy as np
@@ -47,6 +47,7 @@ def __init__(self, name, construct, skip_methods=(),
 ]
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_metaestimator_delegation():
     # Ensures specified metaestimators have methods iff subestimator does
     def hides(method):
diff --git a/sklearn/tests/test_multiclass.py b/sklearn/tests/test_multiclass.py
index 010d1fcd92c8..08c3b9f01e16 100644
--- a/sklearn/tests/test_multiclass.py
+++ b/sklearn/tests/test_multiclass.py
@@ -332,6 +332,7 @@ def test_ovr_multilabel_dataset():
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_ovr_multilabel_predict_proba():
     base_clf = MultinomialNB(alpha=1)
     for au in (False, True):
@@ -425,6 +426,7 @@ def test_ovr_single_label_decision_function():
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_ovr_gridsearch():
     ovr = OneVsRestClassifier(LinearSVC(random_state=0))
     Cs = [0.1, 0.5, 0.8]
@@ -602,6 +604,7 @@ def test_ovo_decision_function():
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_ovo_gridsearch():
     ovo = OneVsOneClassifier(LinearSVC(random_state=0))
     Cs = [0.1, 0.5, 0.8]
@@ -697,6 +700,7 @@ def test_ecoc_fit_predict():
 
 
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_ecoc_gridsearch():
     ecoc = OutputCodeClassifier(LinearSVC(random_state=0),
                                 random_state=0)
@@ -747,6 +751,7 @@ def test_pairwise_attribute():
         assert_true(ovr_true._pairwise)
 
 
+@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_pairwise_cross_val_score():
     clf_precomputed = svm.SVC(kernel='precomputed')
     clf_notprecomputed = svm.SVC(kernel='linear')
diff --git a/sklearn/utils/__init__.py b/sklearn/utils/__init__.py
index 50963341f8ed..84f7ceae5dd5 100644
--- a/sklearn/utils/__init__.py
+++ b/sklearn/utils/__init__.py
@@ -1,8 +1,8 @@
 """
 The :mod:`sklearn.utils` module includes various utilities.
 """
-
 import numbers
+import platform
 
 import numpy as np
 from scipy.sparse import issparse
@@ -15,7 +15,7 @@
                          check_consistent_length, check_X_y, indexable,
                          check_symmetric)
 from .class_weight import compute_class_weight, compute_sample_weight
-from ._joblib import cpu_count, Parallel, Memory, delayed
+from ._joblib import cpu_count, Parallel, Memory, delayed, hash
 from ._joblib import parallel_backend
 from ..exceptions import DataConversionWarning
 from ..utils.fixes import _Sequence as Sequence
@@ -29,7 +29,10 @@
            "column_or_1d", "safe_indexing",
            "check_consistent_length", "check_X_y", 'indexable',
            "check_symmetric", "indices_to_mask", "deprecated",
-           "cpu_count", "Parallel", "Memory", "delayed", "parallel_backend"]
+           "cpu_count", "Parallel", "Memory", "delayed", "parallel_backend",
+           "hash"]
+
+IS_PYPY = platform.python_implementation() == 'PyPy'
 
 
 class Bunch(dict):
diff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py
index 23a46e2f11fc..7366841ec98c 100644
--- a/sklearn/utils/estimator_checks.py
+++ b/sklearn/utils/estimator_checks.py
@@ -14,6 +14,7 @@
 from scipy.stats import rankdata
 
 from sklearn.externals.six.moves import zip
+from sklearn.utils import IS_PYPY
 from sklearn.utils._joblib import hash, Memory
 from sklearn.utils.testing import assert_raises, _get_args
 from sklearn.utils.testing import assert_raises_regex
@@ -230,8 +231,9 @@ def _yield_clustering_checks(name, clusterer):
 
 def _yield_outliers_checks(name, estimator):
 
-    # checks for all outlier detectors
-    yield check_outliers_fit_predict
+    # checks for outlier detectors that have a fit_predict method
+    if hasattr(estimator, 'fit_predict'):
+        yield check_outliers_fit_predict
 
     # checks for estimators that can be used on a test set
     if hasattr(estimator, 'predict'):
@@ -428,7 +430,7 @@ def _is_pairwise_metric(estimator):
     out : bool
         True if _pairwise is set to True and False otherwise.
     """
-    metric = getattr(estimator,  "metric", None)
+    metric = getattr(estimator, "metric", None)
 
     return bool(metric == 'precomputed')
 
@@ -1228,6 +1230,11 @@ def check_estimators_pickle(name, estimator_orig):
     set_random_state(estimator)
     estimator.fit(X, y)
 
+    result = dict()
+    for method in check_methods:
+        if hasattr(estimator, method):
+            result[method] = getattr(estimator, method)(X)
+
     # pickle and unpickle!
     pickled_estimator = pickle.dumps(estimator)
     if estimator.__module__.startswith('sklearn.'):
@@ -1548,9 +1555,12 @@ def check_outliers_train(name, estimator_orig, readonly_memmap=True):
     assert_raises(ValueError, estimator.score_samples, X.T)
 
     # contamination parameter (not for OneClassSVM which has the nu parameter)
-    if hasattr(estimator, "contamination"):
+    if (hasattr(estimator, 'contamination')
+            and not hasattr(estimator, 'novelty')):
         # proportion of outliers equal to contamination parameter when not
-        # set to 'auto'
+        # set to 'auto'. This is true for the training set and cannot thus be
+        # checked as follows for estimators with a novelty parameter such as
+        # LocalOutlierFactor (tested in check_outliers_fit_predict)
         contamination = 0.1
         estimator.set_params(contamination=contamination)
         estimator.fit(X)
@@ -1597,24 +1607,25 @@ def check_estimators_unfitted(name, estimator_orig):
     # Common test for Regressors, Classifiers and Outlier detection estimators
     X, y = _boston_subset()
 
-    est = clone(estimator_orig)
+    estimator = clone(estimator_orig)
 
     msg = "fit"
-    if hasattr(est, 'predict'):
+
+    if hasattr(estimator, 'predict'):
         assert_raise_message((AttributeError, ValueError), msg,
-                             est.predict, X)
+                             estimator.predict, X)
 
-    if hasattr(est, 'decision_function'):
+    if hasattr(estimator, 'decision_function'):
         assert_raise_message((AttributeError, ValueError), msg,
-                             est.decision_function, X)
+                             estimator.decision_function, X)
 
-    if hasattr(est, 'predict_proba'):
+    if hasattr(estimator, 'predict_proba'):
         assert_raise_message((AttributeError, ValueError), msg,
-                             est.predict_proba, X)
+                             estimator.predict_proba, X)
 
-    if hasattr(est, 'predict_log_proba'):
+    if hasattr(estimator, 'predict_log_proba'):
         assert_raise_message((AttributeError, ValueError), msg,
-                             est.predict_log_proba, X)
+                             estimator.predict_log_proba, X)
 
 
 @ignore_warnings(category=(DeprecationWarning, FutureWarning))
@@ -1974,6 +1985,11 @@ def check_no_attributes_set_in_init(name, estimator):
         return
 
     init_params = _get_args(type(estimator).__init__)
+    if IS_PYPY:
+        # __init__ signature has additional objects in PyPy
+        for key in ['obj']:
+            if key in init_params:
+                init_params.remove(key)
     parents_init_params = [param for params_parent in
                            (_get_args(parent) for parent in
                             type(estimator).__mro__)
@@ -2326,7 +2342,9 @@ def check_outliers_fit_predict(name, estimator_orig):
     assert y_pred.dtype.kind == 'i'
     assert_array_equal(np.unique(y_pred), np.array([-1, 1]))
 
-    # check fit_predict = fit.predict when possible
+    # check fit_predict = fit.predict when the estimator has both a predict and
+    # a fit_predict method. recall that it is already assumed here that the
+    # estimator has a fit_predict method
     if hasattr(estimator, 'predict'):
         y_pred_2 = estimator.fit(X).predict(X)
         assert_array_equal(y_pred, y_pred_2)
diff --git a/sklearn/utils/metaestimators.py b/sklearn/utils/metaestimators.py
index f20f51889191..49b059b32459 100644
--- a/sklearn/utils/metaestimators.py
+++ b/sklearn/utils/metaestimators.py
@@ -41,7 +41,10 @@ def _set_params(self, attr, **params):
         if attr in params:
             setattr(self, attr, params.pop(attr))
         # 2. Step replacement
-        names, _ = zip(*getattr(self, attr))
+        items = getattr(self, attr)
+        names = []
+        if items:
+            names, _ = zip(*items)
         for name in list(six.iterkeys(params)):
             if '__' not in name and name in names:
                 self._replace_estimator(attr, name, params.pop(name))
diff --git a/sklearn/utils/testing.py b/sklearn/utils/testing.py
index bfae5d4662b1..ff91aa962417 100644
--- a/sklearn/utils/testing.py
+++ b/sklearn/utils/testing.py
@@ -47,7 +47,7 @@
 from sklearn.base import BaseEstimator
 from sklearn.externals import joblib
 from sklearn.utils.fixes import signature
-from sklearn.utils import deprecated
+from sklearn.utils import deprecated, IS_PYPY
 
 
 additional_names_in_all = []
@@ -625,7 +625,10 @@ def is_abstract(c):
     path = sklearn.__path__
     for importer, modname, ispkg in pkgutil.walk_packages(
             path=path, prefix='sklearn.', onerror=lambda x: None):
-        if (".tests." in modname):
+        if ".tests." in modname:
+            continue
+        if IS_PYPY and ('_svmlight_format' in modname or
+                        'feature_extraction._hashing' in modname):
             continue
         module = __import__(modname, fromlist="dummy")
         classes = inspect.getmembers(module, inspect.isclass)
@@ -706,6 +709,8 @@ def run_test(*args, **kwargs):
                                        reason='skipped on 32bit platforms')
     skip_travis = pytest.mark.skipif(os.environ.get('TRAVIS') == 'true',
                                      reason='skip on travis')
+    fails_if_pypy = pytest.mark.xfail(IS_PYPY, raises=NotImplementedError,
+                                      reason='not compatible with PyPy')
 
     #  Decorator for tests involving both BLAS calls and multiprocessing.
     #
