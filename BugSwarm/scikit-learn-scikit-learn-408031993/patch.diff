diff --git a/doc/datasets/index.rst b/doc/datasets/index.rst
index 8979f74f09..2f061aabe8 100755
--- a/doc/datasets/index.rst
+++ b/doc/datasets/index.rst
@@ -130,6 +130,7 @@ They can be loaded using the following functions:
    fetch_covtype
    fetch_rcv1
    fetch_kddcup99
+   fetch_california_housing
 
 .. toctree::
     :maxdepth: 2
@@ -141,18 +142,21 @@ They can be loaded using the following functions:
     covtype
     rcv1
     kddcup99
+    california_housing
 
-.. include:: ./olivetti_faces.rst
+.. include:: ../../sklearn/datasets/descr/olivetti_faces.rst
 
-.. include:: ./twenty_newsgroups.rst
+.. include:: ../../sklearn/datasets/descr/twenty_newsgroups.rst
 
-.. include:: ./labeled_faces.rst
+.. include:: ../../sklearn/datasets/descr/lfw.rst
 
-.. include:: ./covtype.rst
+.. include:: ../../sklearn/datasets/descr/covtype.rst
 
-.. include:: ./rcv1.rst
+.. include:: ../../sklearn/datasets/descr/rcv1.rst
 
-.. include:: ./kddcup99.rst
+.. include:: ../../sklearn/datasets/descr/kddcup99.rst
+
+.. include:: ../../sklearn/datasets/descr/california_housing.rst
 
 .. _sample_generators:
 
diff --git a/sklearn/datasets/base.py b/sklearn/datasets/base.py
index 9060ad5cbe..6f1ceef70a 100755
--- a/sklearn/datasets/base.py
+++ b/sklearn/datasets/base.py
@@ -264,7 +264,7 @@ def load_wine(return_X_y=False):
     Features            real, positive
     =================   ==============
 
-    Read more in the :ref:`User Guide <datasets>`.
+    Read more in the :ref:`User Guide <wine_dataset>`.
 
     Parameters
     ----------
@@ -339,7 +339,7 @@ def load_iris(return_X_y=False):
     Features            real, positive
     =================   ==============
 
-    Read more in the :ref:`User Guide <datasets>`.
+    Read more in the :ref:`User Guide <iris_dataset>`.
 
     Parameters
     ----------
@@ -414,6 +414,8 @@ def load_breast_cancer(return_X_y=False):
     Features            real, positive
     =================   ==============
 
+    Read more in the :ref:`User Guide <breast_cancer_dataset>`.
+
     Parameters
     ----------
     return_X_y : boolean, default=False
@@ -498,7 +500,7 @@ def load_digits(n_class=10, return_X_y=False):
     Features             integers 0-16
     =================   ==============
 
-    Read more in the :ref:`User Guide <datasets>`.
+    Read more in the :ref:`User Guide <digits_dataset>`.
 
     Parameters
     ----------
@@ -575,7 +577,7 @@ def load_diabetes(return_X_y=False):
     Targets             integer 25 - 346
     ==============      ==================
 
-    Read more in the :ref:`User Guide <datasets>`.
+    Read more in the :ref:`User Guide <diabetes_dataset>`.
 
     Parameters
     ----------
@@ -628,6 +630,8 @@ def load_linnerud(return_X_y=False):
     Targets           integer
     ==============    ============================
 
+    Read more in the :ref:`User Guide <linnerrud_dataset>`.
+
     Parameters
     ----------
     return_X_y : boolean, default=False.
@@ -690,6 +694,8 @@ def load_boston(return_X_y=False):
     Targets             real 5. - 50.
     ==============     ==============
 
+    Read more in the :ref:`User Guide <boston_dataset>`.
+
     Parameters
     ----------
     return_X_y : boolean, default=False.
@@ -760,6 +766,8 @@ def load_sample_images():
 
     Loads both, ``china`` and ``flower``.
 
+    Read more in the :ref:`User Guide <sample_images>`.
+
     Returns
     -------
     data : Bunch
@@ -801,6 +809,8 @@ def load_sample_images():
 def load_sample_image(image_name):
     """Load the numpy array of a single sample image
 
+    Read more in the :ref:`User Guide <sample_images>`.
+
     Parameters
     -----------
     image_name : {`china.jpg`, `flower.jpg`}
diff --git a/sklearn/datasets/california_housing.py b/sklearn/datasets/california_housing.py
index 8973ba59ad..76cb27dadd 100755
--- a/sklearn/datasets/california_housing.py
+++ b/sklearn/datasets/california_housing.py
@@ -21,7 +21,7 @@
 # Authors: Peter Prettenhofer
 # License: BSD 3 clause
 
-from os.path import exists
+from os.path import dirname, exists, join
 from os import makedirs, remove
 import tarfile
 
@@ -43,18 +43,21 @@
     checksum=('aaa5c9a6afe2225cc2aed2723682ae40'
               '3280c4a3695a2ddda4ffb5d8215ea681'))
 
-# Grab the module-level docstring to use as a description of the
-# dataset
-MODULE_DOCS = __doc__
-
 logger = logging.getLogger(__name__)
 
 
 def fetch_california_housing(data_home=None, download_if_missing=True,
                              return_X_y=False):
-    """Loader for the California housing dataset from StatLib.
+    """Load the California housing dataset (regression).
+
+    ==============     ==============
+    Samples total               20640
+    Dimensionality                  8
+    Features                     real
+    Target             real 0.15 - 5.
+    ==============     ==============
 
-    Read more in the :ref:`User Guide <datasets>`.
+    Read more in the :ref:`User Guide <california_housing_dataset>`.
 
     Parameters
     ----------
@@ -144,10 +147,14 @@ def fetch_california_housing(data_home=None, download_if_missing=True,
     # target in units of 100,000
     target = target / 100000.0
 
+    module_path = dirname(__file__)
+    with open(join(module_path, 'descr', 'california_housing.rst')) as dfile:
+        descr = dfile.read()
+
     if return_X_y:
         return data, target
 
     return Bunch(data=data,
                  target=target,
                  feature_names=feature_names,
-                 DESCR=MODULE_DOCS)
+                 DESCR=descr)
diff --git a/sklearn/datasets/covtype.py b/sklearn/datasets/covtype.py
index c7b880b116..a08f61f02b 100755
--- a/sklearn/datasets/covtype.py
+++ b/sklearn/datasets/covtype.py
@@ -16,7 +16,7 @@
 
 from gzip import GzipFile
 import logging
-from os.path import exists, join
+from os.path import dirname, exists, join
 from os import remove
 
 import numpy as np
@@ -43,9 +43,18 @@
 
 def fetch_covtype(data_home=None, download_if_missing=True,
                   random_state=None, shuffle=False, return_X_y=False):
-    """Load the covertype dataset, downloading it if necessary.
+    """Load the covertype dataset (classification).
 
-    Read more in the :ref:`User Guide <datasets>`.
+    Download it if necessary.
+
+    =================   ============
+    Classes                        7
+    Samples total             581012
+    Dimensionality                54
+    Features                     int
+    =================   ============
+
+    Read more in the :ref:`User Guide <covtype_dataset>`.
 
     Parameters
     ----------
@@ -127,7 +136,11 @@ def fetch_covtype(data_home=None, download_if_missing=True,
         X = X[ind]
         y = y[ind]
 
+    module_path = dirname(__file__)
+    with open(join(module_path, 'descr', 'covtype.rst')) as rst_file:
+        fdescr = rst_file.read()
+
     if return_X_y:
         return X, y
 
-    return Bunch(data=X, target=y, DESCR=__doc__)
+    return Bunch(data=X, target=y, DESCR=fdescr)
diff --git a/sklearn/datasets/descr/california_housing.rst b/sklearn/datasets/descr/california_housing.rst
new file mode 100755
index 0000000000..9ab3b679b6
--- /dev/null
+++ b/sklearn/datasets/descr/california_housing.rst
@@ -0,0 +1,40 @@
+.. _california_housing_dataset:
+
+California Housing dataset
+--------------------------
+
+**Data Set Characteristics:**
+
+    :Number of Instances: 20640
+
+    :Number of Attributes: 8 numeric, predictive attributes and the target
+
+    :Attribute Information:
+        - MedInc        median income in block
+        - HouseAge      median house age in block
+        - AveRooms      average number of rooms
+        - AveBedrms     average number of bedrooms
+        - Population    block population
+        - AveOccup      average house occupancy
+        - Latitude      house block latitude
+        - Longitude     house block longitude
+
+    :Missing Attribute Values: None
+
+This dataset was obtained from the StatLib repository.
+http://lib.stat.cmu.edu/datasets/
+
+The target variable is the median house value for California districts.
+
+This dataset was derived from the 1990 U.S. census, using one row per census
+block group. A block group is the smallest geographical unit for which the U.S.
+Census Bureau publishes sample data (a block group typically has a population
+of 600 to 3,000 people).
+
+It can be downloaded/loaded using the
+:func:`sklearn.datasets.fetch_california_housing` function.
+
+.. topic:: References
+
+    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,
+      Statistics and Probability Letters, 33 (1997) 291-297
diff --git a/sklearn/datasets/descr/covtype.rst b/sklearn/datasets/descr/covtype.rst
new file mode 100755
index 0000000000..08447403eb
--- /dev/null
+++ b/sklearn/datasets/descr/covtype.rst
@@ -0,0 +1,28 @@
+.. _covtype_dataset:
+
+Forest covertypes
+-----------------
+
+The samples in this dataset correspond to 30Ã—30m patches of forest in the US,
+collected for the task of predicting each patch's cover type,
+i.e. the dominant species of tree.
+There are seven covertypes, making this a multiclass classification problem.
+Each sample has 54 features, described on the
+`dataset's homepage <http://archive.ics.uci.edu/ml/datasets/Covertype>`__.
+Some of the features are boolean indicators,
+while others are discrete or continuous measurements.
+
+**Data Set Characteristics:**
+
+    =================   ============
+    Classes                        7
+    Samples total             581012
+    Dimensionality                54
+    Features                     int
+    =================   ============
+
+:func:`sklearn.datasets.fetch_covtype` will load the covertype dataset;
+it returns a dictionary-like object
+with the feature matrix in the ``data`` member
+and the target values in ``target``.
+The dataset will be downloaded from the web if necessary.
diff --git a/sklearn/datasets/descr/kddcup99.rst b/sklearn/datasets/descr/kddcup99.rst
new file mode 100755
index 0000000000..6e942246ea
--- /dev/null
+++ b/sklearn/datasets/descr/kddcup99.rst
@@ -0,0 +1,95 @@
+.. _kddcup99_dataset:
+
+Kddcup 99 dataset
+-----------------
+
+The KDD Cup '99 dataset was created by processing the tcpdump portions
+of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset,
+created by MIT Lincoln Lab [1]. The artificial data (described on the `dataset's
+homepage <http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html>`_) was
+generated using a closed network and hand-injected attacks to produce a
+large number of different types of attack with normal activity in the
+background. As the initial goal was to produce a large training set for
+supervised learning algorithms, there is a large proportion (80.1%) of
+abnormal data which is unrealistic in real world, and inappropriate for
+unsupervised anomaly detection which aims at detecting 'abnormal' data, ie
+
+1) qualitatively different from normal data
+
+2) in large minority among the observations.
+
+We thus transform the KDD Data set into two different data sets: SA and SF.
+
+-SA is obtained by simply selecting all the normal data, and a small
+proportion of abnormal data to gives an anomaly proportion of 1%.
+
+-SF is obtained as in [2]
+by simply picking up the data whose attribute logged_in is positive, thus
+focusing on the intrusion attack, which gives a proportion of 0.3% of
+attack.
+
+-http and smtp are two subsets of SF corresponding with third feature
+equal to 'http' (resp. to 'smtp')
+
+General KDD structure :
+
+    ================      ==========================================
+    Samples total         4898431
+    Dimensionality        41
+    Features              discrete (int) or continuous (float)
+    Targets               str, 'normal.' or name of the anomaly type
+    ================      ==========================================
+
+    SA structure :
+
+    ================      ==========================================
+    Samples total         976158
+    Dimensionality        41
+    Features              discrete (int) or continuous (float)
+    Targets               str, 'normal.' or name of the anomaly type
+    ================      ==========================================
+
+    SF structure :
+
+    ================      ==========================================
+    Samples total         699691
+    Dimensionality        4
+    Features              discrete (int) or continuous (float)
+    Targets               str, 'normal.' or name of the anomaly type
+    ================      ==========================================
+
+    http structure :
+
+    ================      ==========================================
+    Samples total         619052
+    Dimensionality        3
+    Features              discrete (int) or continuous (float)
+    Targets               str, 'normal.' or name of the anomaly type
+    ================      ==========================================
+
+    smtp structure :
+
+    ================      ==========================================
+    Samples total         95373
+    Dimensionality        3
+    Features              discrete (int) or continuous (float)
+    Targets               str, 'normal.' or name of the anomaly type
+    ================      ==========================================
+
+:func:`sklearn.datasets.fetch_kddcup99` will load the kddcup99 dataset; it
+returns a dictionary-like object with the feature matrix in the ``data`` member
+and the target values in ``target``. The dataset will be downloaded from the
+web if necessary.
+
+.. topic: References
+
+    .. [1] Analysis and Results of the 1999 DARPA Off-Line Intrusion
+           Detection Evaluation Richard Lippmann, Joshua W. Haines,
+           David J. Fried, Jonathan Korba, Kumar Das
+
+    .. [2] K. Yamanishi, J.-I. Takeuchi, G. Williams, and P. Milne. Online
+           unsupervised outlier detection using finite mixtures with
+           discounting learning algorithms. In Proceedings of the sixth
+           ACM SIGKDD international conference on Knowledge discovery
+           and data mining, pages 320-324. ACM Press, 2000.
+
diff --git a/sklearn/datasets/descr/lfw.rst b/sklearn/datasets/descr/lfw.rst
new file mode 100755
index 0000000000..e7fc35c3ca
--- /dev/null
+++ b/sklearn/datasets/descr/lfw.rst
@@ -0,0 +1,126 @@
+.. _labeled_faces_in_the_wild_dataset:
+
+The Labeled Faces in the Wild face recognition dataset
+------------------------------------------------------
+
+This dataset is a collection of JPEG pictures of famous people collected
+over the internet, all details are available on the official website:
+
+    http://vis-www.cs.umass.edu/lfw/
+
+Each picture is centered on a single face. The typical task is called
+Face Verification: given a pair of two pictures, a binary classifier
+must predict whether the two images are from the same person.
+
+An alternative task, Face Recognition or Face Identification is:
+given the picture of the face of an unknown person, identify the name
+of the person by referring to a gallery of previously seen pictures of
+identified persons.
+
+Both Face Verification and Face Recognition are tasks that are typically
+performed on the output of a model trained to perform Face Detection. The
+most popular model for Face Detection is called Viola-Jones and is
+implemented in the OpenCV library. The LFW faces were extracted by this
+face detector from various online websites.
+
+**Data Set Characteristics:**
+
+    =================   =======================
+    Classes                                5749
+    Samples total                         13233
+    Dimensionality                         5828
+    Features            real, between 0 and 255
+    =================   =======================
+
+Usage
+~~~~~
+
+``scikit-learn`` provides two loaders that will automatically download,
+cache, parse the metadata files, decode the jpeg and convert the
+interesting slices into memmapped numpy arrays. This dataset size is more
+than 200 MB. The first load typically takes more than a couple of minutes
+to fully decode the relevant part of the JPEG files into numpy arrays. If
+the dataset has  been loaded once, the following times the loading times
+less than 200ms by using a memmapped version memoized on the disk in the
+``~/scikit_learn_data/lfw_home/`` folder using ``joblib``.
+
+The first loader is used for the Face Identification task: a multi-class
+classification task (hence supervised learning)::
+
+  >>> from sklearn.datasets import fetch_lfw_people
+  >>> lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)
+
+  >>> for name in lfw_people.target_names:
+  ...     print(name)
+  ...
+  Ariel Sharon
+  Colin Powell
+  Donald Rumsfeld
+  George W Bush
+  Gerhard Schroeder
+  Hugo Chavez
+  Tony Blair
+
+The default slice is a rectangular shape around the face, removing
+most of the background::
+
+  >>> lfw_people.data.dtype
+  dtype('float32')
+
+  >>> lfw_people.data.shape
+  (1288, 1850)
+
+  >>> lfw_people.images.shape
+  (1288, 50, 37)
+
+Each of the ``1140`` faces is assigned to a single person id in the ``target``
+array::
+
+  >>> lfw_people.target.shape
+  (1288,)
+
+  >>> list(lfw_people.target[:10])
+  [5, 6, 3, 1, 0, 1, 3, 4, 3, 0]
+
+The second loader is typically used for the face verification task: each sample
+is a pair of two picture belonging or not to the same person::
+
+  >>> from sklearn.datasets import fetch_lfw_pairs
+  >>> lfw_pairs_train = fetch_lfw_pairs(subset='train')
+
+  >>> list(lfw_pairs_train.target_names)
+  ['Different persons', 'Same person']
+
+  >>> lfw_pairs_train.pairs.shape
+  (2200, 2, 62, 47)
+
+  >>> lfw_pairs_train.data.shape
+  (2200, 5828)
+
+  >>> lfw_pairs_train.target.shape
+  (2200,)
+
+Both for the :func:`sklearn.datasets.fetch_lfw_people` and
+:func:`sklearn.datasets.fetch_lfw_pairs` function it is
+possible to get an additional dimension with the RGB color channels by
+passing ``color=True``, in that case the shape will be
+``(2200, 2, 62, 47, 3)``.
+
+The :func:`sklearn.datasets.fetch_lfw_pairs` datasets is subdivided into
+3 subsets: the development ``train`` set, the development ``test`` set and
+an evaluation ``10_folds`` set meant to compute performance metrics using a
+10-folds cross validation scheme.
+
+.. topic:: References:
+
+ * `Labeled Faces in the Wild: A Database for Studying Face Recognition
+   in Unconstrained Environments.
+   <http://vis-www.cs.umass.edu/lfw/lfw.pdf>`_
+   Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.
+   University of Massachusetts, Amherst, Technical Report 07-49, October, 2007.
+
+
+Examples
+~~~~~~~~
+
+:ref:`sphx_glr_auto_examples_applications_plot_face_recognition.py`
diff --git a/sklearn/datasets/descr/olivetti_faces.rst b/sklearn/datasets/descr/olivetti_faces.rst
new file mode 100755
index 0000000000..c6193d5056
--- /dev/null
+++ b/sklearn/datasets/descr/olivetti_faces.rst
@@ -0,0 +1,44 @@
+.. _olivetti_faces_dataset:
+
+The Olivetti faces dataset
+--------------------------
+
+`This dataset contains a set of face images`_ taken between April 1992 and 
+April 1994 at AT&T Laboratories Cambridge. The
+:func:`sklearn.datasets.fetch_olivetti_faces` function is the data
+fetching / caching function that downloads the data
+archive from AT&T.
+
+.. _This dataset contains a set of face images: http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html
+
+As described on the original website:
+
+    There are ten different images of each of 40 distinct subjects. For some
+    subjects, the images were taken at different times, varying the lighting,
+    facial expressions (open / closed eyes, smiling / not smiling) and facial
+    details (glasses / no glasses). All the images were taken against a dark
+    homogeneous background with the subjects in an upright, frontal position 
+    (with tolerance for some side movement).
+
+**Data Set Characteristics:**
+
+    =================   =====================
+    Classes                                40
+    Samples total                         400
+    Dimensionality                       4096
+    Features            real, between 0 and 1
+    =================   =====================
+
+The image is quantized to 256 grey levels and stored as unsigned 8-bit 
+integers; the loader will convert these to floating point values on the 
+interval [0, 1], which are easier to work with for many algorithms.
+
+The "target" for this database is an integer from 0 to 39 indicating the
+identity of the person pictured; however, with only 10 examples per class, this
+relatively small dataset is more interesting from an unsupervised or
+semi-supervised perspective.
+
+The original dataset consisted of 92 x 112, while the version available here
+consists of 64x64 images.
+
+When using these images, please give credit to AT&T Laboratories Cambridge.
diff --git a/sklearn/datasets/descr/rcv1.rst b/sklearn/datasets/descr/rcv1.rst
new file mode 100755
index 0000000000..afaadbfb45
--- /dev/null
+++ b/sklearn/datasets/descr/rcv1.rst
@@ -0,0 +1,72 @@
+.. _rcv1_dataset:
+
+RCV1 dataset
+------------
+
+Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually 
+categorized newswire stories made available by Reuters, Ltd. for research 
+purposes. The dataset is extensively described in [1]_.
+
+**Data Set Characteristics:**
+
+    ==============     =====================
+    Classes                              103
+    Samples total                     804414
+    Dimensionality                     47236
+    Features           real, between 0 and 1
+    ==============     =====================
+
+:func:`sklearn.datasets.fetch_rcv1` will load the following 
+version: RCV1-v2, vectors, full sets, topics multilabels::
+
+    >>> from sklearn.datasets import fetch_rcv1
+    >>> rcv1 = fetch_rcv1()
+
+It returns a dictionary-like object, with the following attributes:
+
+``data``:
+The feature matrix is a scipy CSR sparse matrix, with 804414 samples and
+47236 features. Non-zero values contains cosine-normalized, log TF-IDF vectors.
+A nearly chronological split is proposed in [1]_: The first 23149 samples are
+the training set. The last 781265 samples are the testing set. This follows 
+the official LYRL2004 chronological split. The array has 0.16% of non zero 
+values::
+
+    >>> rcv1.data.shape
+    (804414, 47236)
+
+``target``:
+The target values are stored in a scipy CSR sparse matrix, with 804414 samples 
+and 103 categories. Each sample has a value of 1 in its categories, and 0 in 
+others. The array has 3.15% of non zero values::
+
+    >>> rcv1.target.shape
+    (804414, 103)
+
+``sample_id``:
+Each sample can be identified by its ID, ranging (with gaps) from 2286 
+to 810596::
+
+    >>> rcv1.sample_id[:3]
+    array([2286, 2287, 2288], dtype=uint32)
+
+``target_names``:
+The target values are the topics of each sample. Each sample belongs to at 
+least one topic, and to up to 17 topics. There are 103 topics, each 
+represented by a string. Their corpus frequencies span five orders of 
+magnitude, from 5 occurrences for 'GMIL', to 381327 for 'CCAT'::
+
+    >>> rcv1.target_names[:3].tolist()  # doctest: +SKIP
+    ['E11', 'ECAT', 'M11']
+
+The dataset will be downloaded from the `rcv1 homepage`_ if necessary.
+The compressed size is about 656 MB.
+
+.. _rcv1 homepage: http://jmlr.csail.mit.edu/papers/volume5/lewis04a/
+
+
+.. topic:: References
+
+    .. [1] Lewis, D. D., Yang, Y., Rose, T. G., & Li, F. (2004). 
+           RCV1: A new benchmark collection for text categorization research. 
+           The Journal of Machine Learning Research, 5, 361-397.
diff --git a/sklearn/datasets/descr/twenty_newsgroups.rst b/sklearn/datasets/descr/twenty_newsgroups.rst
new file mode 100755
index 0000000000..a9747d3224
--- /dev/null
+++ b/sklearn/datasets/descr/twenty_newsgroups.rst
@@ -0,0 +1,233 @@
+.. _20newsgroups_dataset:
+
+The 20 newsgroups text dataset
+------------------------------
+
+The 20 newsgroups dataset comprises around 18000 newsgroups posts on
+20 topics split in two subsets: one for training (or development)
+and the other one for testing (or for performance evaluation). The split
+between the train and test set is based upon a messages posted before
+and after a specific date.
+
+This module contains two loaders. The first one,
+:func:`sklearn.datasets.fetch_20newsgroups`,
+returns a list of the raw texts that can be fed to text feature
+extractors such as :class:`sklearn.feature_extraction.text.CountVectorizer`
+with custom parameters so as to extract feature vectors.
+The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,
+returns ready-to-use features, i.e., it is not necessary to use a feature
+extractor.
+
+**Data Set Characteristics:**
+
+    =================   ==========
+    Classes                     20
+    Samples total            18846
+    Dimensionality               1
+    Features                  text
+    =================   ==========
+
+Usage
+~~~~~
+
+The :func:`sklearn.datasets.fetch_20newsgroups` function is a data
+fetching / caching functions that downloads the data archive from
+the original `20 newsgroups website`_, extracts the archive contents
+in the ``~/scikit_learn_data/20news_home`` folder and calls the
+:func:`sklearn.datasets.load_files` on either the training or
+testing set folder, or both of them::
+
+  >>> from sklearn.datasets import fetch_20newsgroups
+  >>> newsgroups_train = fetch_20newsgroups(subset='train')
+
+  >>> from pprint import pprint
+  >>> pprint(list(newsgroups_train.target_names))
+  ['alt.atheism',
+   'comp.graphics',
+   'comp.os.ms-windows.misc',
+   'comp.sys.ibm.pc.hardware',
+   'comp.sys.mac.hardware',
+   'comp.windows.x',
+   'misc.forsale',
+   'rec.autos',
+   'rec.motorcycles',
+   'rec.sport.baseball',
+   'rec.sport.hockey',
+   'sci.crypt',
+   'sci.electronics',
+   'sci.med',
+   'sci.space',
+   'soc.religion.christian',
+   'talk.politics.guns',
+   'talk.politics.mideast',
+   'talk.politics.misc',
+   'talk.religion.misc']
+
+The real data lies in the ``filenames`` and ``target`` attributes. The target
+attribute is the integer index of the category::
+
+  >>> newsgroups_train.filenames.shape
+  (11314,)
+  >>> newsgroups_train.target.shape
+  (11314,)
+  >>> newsgroups_train.target[:10]
+  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])
+
+It is possible to load only a sub-selection of the categories by passing the
+list of the categories to load to the
+:func:`sklearn.datasets.fetch_20newsgroups` function::
+
+  >>> cats = ['alt.atheism', 'sci.space']
+  >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)
+
+  >>> list(newsgroups_train.target_names)
+  ['alt.atheism', 'sci.space']
+  >>> newsgroups_train.filenames.shape
+  (1073,)
+  >>> newsgroups_train.target.shape
+  (1073,)
+  >>> newsgroups_train.target[:10]
+  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])
+
+Converting text to vectors
+~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+In order to feed predictive or clustering models with the text data,
+one first need to turn the text into vectors of numerical values suitable
+for statistical analysis. This can be achieved with the utilities of the
+``sklearn.feature_extraction.text`` as demonstrated in the following
+example that extract `TF-IDF`_ vectors of unigram tokens
+from a subset of 20news::
+
+  >>> from sklearn.feature_extraction.text import TfidfVectorizer
+  >>> categories = ['alt.atheism', 'talk.religion.misc',
+  ...               'comp.graphics', 'sci.space']
+  >>> newsgroups_train = fetch_20newsgroups(subset='train',
+  ...                                       categories=categories)
+  >>> vectorizer = TfidfVectorizer()
+  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)
+  >>> vectors.shape
+  (2034, 34118)
+
+The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero
+components by sample in a more than 30000-dimensional space
+(less than .5% non-zero features)::
+
+  >>> vectors.nnz / float(vectors.shape[0])       # doctest: +ELLIPSIS
+  159.01327...
+
+:func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which 
+returns ready-to-use tfidf features instead of file names.
+
+.. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/
+.. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf
+
+
+Filtering text for more realistic training
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+It is easy for a classifier to overfit on particular things that appear in the
+20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very
+high F-scores, but their results would not generalize to other documents that
+aren't from this window of time.
+
+For example, let's look at the results of a multinomial Naive Bayes classifier,
+which is fast to train and achieves a decent F-score::
+
+  >>> from sklearn.naive_bayes import MultinomialNB
+  >>> from sklearn import metrics
+  >>> newsgroups_test = fetch_20newsgroups(subset='test',
+  ...                                      categories=categories)
+  >>> vectors_test = vectorizer.transform(newsgroups_test.data)
+  >>> clf = MultinomialNB(alpha=.01)
+  >>> clf.fit(vectors, newsgroups_train.target)
+  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)
+
+  >>> pred = clf.predict(vectors_test)
+  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  # doctest: +ELLIPSIS
+  0.88213...
+
+(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles
+the training and test data, instead of segmenting by time, and in that case
+multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious
+yet of what's going on inside this classifier?)
+
+Let's take a look at what the most informative features are:
+
+  >>> import numpy as np
+  >>> def show_top10(classifier, vectorizer, categories):
+  ...     feature_names = np.asarray(vectorizer.get_feature_names())
+  ...     for i, category in enumerate(categories):
+  ...         top10 = np.argsort(classifier.coef_[i])[-10:]
+  ...         print("%s: %s" % (category, " ".join(feature_names[top10])))
+  ...
+  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)
+  alt.atheism: edu it and in you that is of to the
+  comp.graphics: edu in graphics it is for and of to the
+  sci.space: edu it that is in and space to of the
+  talk.religion.misc: not it you in is that and to of the
+
+
+You can now see many things that these features have overfit to:
+
+- Almost every group is distinguished by whether headers such as
+  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.
+- Another significant feature involves whether the sender is affiliated with
+  a university, as indicated either by their headers or their signature.
+- The word "article" is a significant feature, based on how often people quote
+  previous posts like this: "In article [article ID], [name] <[e-mail address]>
+  wrote:"
+- Other features match the names and e-mail addresses of particular people who
+  were posting at the time.
+
+With such an abundance of clues that distinguish newsgroups, the classifiers
+barely have to identify topics from text at all, and they all perform at the
+same high level.
+
+For this reason, the functions that load 20 Newsgroups data provide a
+parameter called **remove**, telling it what kinds of information to strip out
+of each file. **remove** should be a tuple containing any subset of
+``('headers', 'footers', 'quotes')``, telling it to remove headers, signature
+blocks, and quotation blocks respectively.
+
+  >>> newsgroups_test = fetch_20newsgroups(subset='test',
+  ...                                      remove=('headers', 'footers', 'quotes'),
+  ...                                      categories=categories)
+  >>> vectors_test = vectorizer.transform(newsgroups_test.data)
+  >>> pred = clf.predict(vectors_test)
+  >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')  # doctest: +ELLIPSIS
+  0.77310...
+
+This classifier lost over a lot of its F-score, just because we removed
+metadata that has little to do with topic classification.
+It loses even more if we also strip this metadata from the training data:
+
+  >>> newsgroups_train = fetch_20newsgroups(subset='train',
+  ...                                       remove=('headers', 'footers', 'quotes'),
+  ...                                       categories=categories)
+  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)
+  >>> clf = MultinomialNB(alpha=.01)
+  >>> clf.fit(vectors, newsgroups_train.target)
+  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)
+
+  >>> vectors_test = vectorizer.transform(newsgroups_test.data)
+  >>> pred = clf.predict(vectors_test)
+  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  # doctest: +ELLIPSIS
+  0.76995...
+
+Some other classifiers cope better with this harder version of the task. Try
+running :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without
+the ``--filter`` option to compare the results.
+
+.. topic:: Recommendation
+
+  When evaluating text classifiers on the 20 Newsgroups data, you
+  should strip newsgroup-related metadata. In scikit-learn, you can do this by
+  setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be
+  lower because it is more realistic.
+
+.. topic:: Examples
+
+   * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`
+
+   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`
diff --git a/sklearn/datasets/kddcup99.py b/sklearn/datasets/kddcup99.py
index 77175a1710..c8ed0e3088 100755
--- a/sklearn/datasets/kddcup99.py
+++ b/sklearn/datasets/kddcup99.py
@@ -13,7 +13,7 @@
 from gzip import GzipFile
 import logging
 import os
-from os.path import exists, join
+from os.path import dirname, exists, join
 
 import numpy as np
 
@@ -48,80 +48,18 @@
 def fetch_kddcup99(subset=None, data_home=None, shuffle=False,
                    random_state=None,
                    percent10=True, download_if_missing=True, return_X_y=False):
-    """Load and return the kddcup 99 dataset (classification).
+    """Load the kddcup99 dataset (classification).
 
-    The KDD Cup '99 dataset was created by processing the tcpdump portions
-    of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset,
-    created by MIT Lincoln Lab [1]. The artificial data was generated using
-    a closed network and hand-injected attacks to produce a large number of
-    different types of attack with normal activity in the background.
-    As the initial goal was to produce a large training set for supervised
-    learning algorithms, there is a large proportion (80.1%) of abnormal
-    data which is unrealistic in real world, and inappropriate for unsupervised
-    anomaly detection which aims at detecting 'abnormal' data, ie
+    Download it if necessary.
 
-    1) qualitatively different from normal data.
+    =================   ====================================
+    Classes                                               23
+    Samples total                                    4898431
+    Dimensionality                                        41
+    Features            discrete (int) or continuous (float)
+    =================   ====================================
 
-    2) in large minority among the observations.
-
-    We thus transform the KDD Data set into two different data sets: SA and SF.
-
-    - SA is obtained by simply selecting all the normal data, and a small
-      proportion of abnormal data to gives an anomaly proportion of 1%.
-
-    - SF is obtained as in [2]
-      by simply picking up the data whose attribute logged_in is positive, thus
-      focusing on the intrusion attack, which gives a proportion of 0.3% of
-      attack.
-
-    - http and smtp are two subsets of SF corresponding with third feature
-      equal to 'http' (resp. to 'smtp')
-
-
-    General KDD structure :
-
-    ================      ==========================================
-    Samples total         4898431
-    Dimensionality        41
-    Features              discrete (int) or continuous (float)
-    Targets               str, 'normal.' or name of the anomaly type
-    ================      ==========================================
-
-    SA structure :
-
-    ================      ==========================================
-    Samples total         976158
-    Dimensionality        41
-    Features              discrete (int) or continuous (float)
-    Targets               str, 'normal.' or name of the anomaly type
-    ================      ==========================================
-
-    SF structure :
-
-    ================      ==========================================
-    Samples total         699691
-    Dimensionality        4
-    Features              discrete (int) or continuous (float)
-    Targets               str, 'normal.' or name of the anomaly type
-    ================      ==========================================
-
-    http structure :
-
-    ================      ==========================================
-    Samples total         619052
-    Dimensionality        3
-    Features              discrete (int) or continuous (float)
-    Targets               str, 'normal.' or name of the anomaly type
-    ================      ==========================================
-
-    smtp structure :
-
-    ================      ==========================================
-    Samples total         95373
-    Dimensionality        3
-    Features              discrete (int) or continuous (float)
-    Targets               str, 'normal.' or name of the anomaly type
-    ================      ==========================================
+    Read more in the :ref:`User Guide <kddcup99_dataset>`.
 
     .. versionadded:: 0.18
 
@@ -162,25 +100,13 @@ def fetch_kddcup99(subset=None, data_home=None, shuffle=False,
     -------
     data : Bunch
         Dictionary-like object, the interesting attributes are:
-        'data', the data to learn and 'target', the regression target for each
-        sample.
+         - 'data', the data to learn.
+         - 'target', the regression target for each sample.
+         - 'DESCR', a description of the dataset.
 
     (data, target) : tuple if ``return_X_y`` is True
 
         .. versionadded:: 0.20
-
-    References
-    ----------
-    .. [1] Analysis and Results of the 1999 DARPA Off-Line Intrusion
-           Detection Evaluation Richard Lippmann, Joshua W. Haines,
-           David J. Fried, Jonathan Korba, Kumar Das
-
-    .. [2] K. Yamanishi, J.-I. Takeuchi, G. Williams, and P. Milne. Online
-           unsupervised outlier detection using finite mixtures with
-           discounting learning algorithms. In Proceedings of the sixth
-           ACM SIGKDD international conference on Knowledge discovery
-           and data mining, pages 320-324. ACM Press, 2000.
-
     """
     data_home = get_data_home(data_home=data_home)
     kddcup99 = _fetch_brute_kddcup99(data_home=data_home,
@@ -236,10 +162,14 @@ def fetch_kddcup99(subset=None, data_home=None, shuffle=False,
     if shuffle:
         data, target = shuffle_method(data, target, random_state=random_state)
 
+    module_path = dirname(__file__)
+    with open(join(module_path, 'descr', 'kddcup99.rst')) as rst_file:
+        fdescr = rst_file.read()
+
     if return_X_y:
         return data, target
 
-    return Bunch(data=data, target=target)
+    return Bunch(data=data, target=target, DESCR=fdescr)
 
 
 def _fetch_brute_kddcup99(data_home=None,
@@ -375,7 +305,7 @@ def _fetch_brute_kddcup99(data_home=None,
         X = joblib.load(samples_path)
         y = joblib.load(targets_path)
 
-    return Bunch(data=X, target=y, DESCR=__doc__)
+    return Bunch(data=X, target=y)
 
 
 def _mkdirp(d):
diff --git a/sklearn/datasets/lfw.py b/sklearn/datasets/lfw.py
index b1cd055b1b..cf09f366ca 100755
--- a/sklearn/datasets/lfw.py
+++ b/sklearn/datasets/lfw.py
@@ -1,30 +1,15 @@
-"""Loader for the Labeled Faces in the Wild (LFW) dataset
+"""Labeled Faces in the Wild (LFW) dataset
 
 This dataset is a collection of JPEG pictures of famous people collected
 over the internet, all details are available on the official website:
 
     http://vis-www.cs.umass.edu/lfw/
-
-Each picture is centered on a single face. The typical task is called
-Face Verification: given a pair of two pictures, a binary classifier
-must predict whether the two images are from the same person.
-
-An alternative task, Face Recognition or Face Identification is:
-given the picture of the face of an unknown person, identify the name
-of the person by referring to a gallery of previously seen pictures of
-identified persons.
-
-Both Face Verification and Face Recognition are tasks that are typically
-performed on the output of a model trained to perform Face Detection. The
-most popular model for Face Detection is called Viola-Johns and is
-implemented in the OpenCV library. The LFW faces were extracted by this face
-detector from various online websites.
 """
 # Copyright (c) 2011 Olivier Grisel <olivier.grisel@ensta.org>
 # License: BSD 3 clause
 
 from os import listdir, makedirs, remove
-from os.path import join, exists, isdir
+from os.path import dirname, join, exists, isdir
 
 import logging
 from distutils.version import LooseVersion
@@ -259,23 +244,19 @@ def fetch_lfw_people(data_home=None, funneled=True, resize=0.5,
                      min_faces_per_person=0, color=False,
                      slice_=(slice(70, 195), slice(78, 172)),
                      download_if_missing=True, return_X_y=False):
-    """Loader for the Labeled Faces in the Wild (LFW) people dataset
+    """Load the Labeled Faces in the Wild (LFW) people dataset \
+(classification).
 
-    This dataset is a collection of JPEG pictures of famous people
-    collected on the internet, all details are available on the
-    official website:
+    Download it if necessary.
 
-        http://vis-www.cs.umass.edu/lfw/
+    =================   =======================
+    Classes                                5749
+    Samples total                         13233
+    Dimensionality                         5828
+    Features            real, between 0 and 255
+    =================   =======================
 
-    Each picture is centered on a single face. Each pixel of each channel
-    (color in RGB) is encoded by a float in range 0.0 - 1.0.
-
-    The task is called Face Recognition (or Identification): given the
-    picture of a face, find the name of the person given a training set
-    (gallery).
-
-    The original images are 250 x 250 pixels, but the default slice and resize
-    arguments reduce them to 62 x 47.
+    Read more in the :ref:`User Guide <labeled_faces_in_the_wild_dataset>`.
 
     Parameters
     ----------
@@ -361,13 +342,17 @@ def fetch_lfw_people(data_home=None, funneled=True, resize=0.5,
 
     X = faces.reshape(len(faces), -1)
 
+    module_path = dirname(__file__)
+    with open(join(module_path, 'descr', 'lfw.rst')) as rst_file:
+        fdescr = rst_file.read()
+
     if return_X_y:
         return X, target
 
     # pack the results as a Bunch instance
     return Bunch(data=X, images=faces,
                  target=target, target_names=target_names,
-                 DESCR="LFW faces dataset")
+                 DESCR=fdescr)
 
 
 #
@@ -429,20 +414,16 @@ def _fetch_lfw_pairs(index_file_path, data_folder_path, slice_=None,
 def fetch_lfw_pairs(subset='train', data_home=None, funneled=True, resize=0.5,
                     color=False, slice_=(slice(70, 195), slice(78, 172)),
                     download_if_missing=True):
-    """Loader for the Labeled Faces in the Wild (LFW) pairs dataset
-
-    This dataset is a collection of JPEG pictures of famous people
-    collected on the internet, all details are available on the
-    official website:
+    """Load the Labeled Faces in the Wild (LFW) pairs dataset (classification).
 
-        http://vis-www.cs.umass.edu/lfw/
+    Download it if necessary.
 
-    Each picture is centered on a single face. Each pixel of each channel
-    (color in RGB) is encoded by a float in range 0.0 - 1.0.
-
-    The task is called Face Verification: given a pair of two pictures,
-    a binary classifier must predict whether the two images are from
-    the same person.
+    =================   =======================
+    Classes                                5749
+    Samples total                         13233
+    Dimensionality                         5828
+    Features            real, between 0 and 255
+    =================   =======================
 
     In the official `README.txt`_ this task is described as the
     "Restricted" task.  As I am not sure as to implement the
@@ -453,7 +434,7 @@ def fetch_lfw_pairs(subset='train', data_home=None, funneled=True, resize=0.5,
     The original images are 250 x 250 pixels, but the default slice and resize
     arguments reduce them to 62 x 47.
 
-    Read more in the :ref:`User Guide <labeled_faces_in_the_wild>`.
+    Read more in the :ref:`User Guide <labeled_faces_in_the_wild_dataset>`.
 
     Parameters
     ----------
@@ -541,7 +522,11 @@ def fetch_lfw_pairs(subset='train', data_home=None, funneled=True, resize=0.5,
         index_file_path, data_folder_path, resize=resize, color=color,
         slice_=slice_)
 
+    module_path = dirname(__file__)
+    with open(join(module_path, 'descr', 'lfw.rst')) as rst_file:
+        fdescr = rst_file.read()
+
     # pack the results as a Bunch instance
     return Bunch(data=pairs.reshape(len(pairs), -1), pairs=pairs,
                  target=target, target_names=target_names,
-                 DESCR="'%s' segment of the LFW pairs dataset" % subset)
+                 DESCR=fdescr)
diff --git a/sklearn/datasets/olivetti_faces.py b/sklearn/datasets/olivetti_faces.py
index fd1bea5128..74915c6c69 100755
--- a/sklearn/datasets/olivetti_faces.py
+++ b/sklearn/datasets/olivetti_faces.py
@@ -1,6 +1,6 @@
 """Modified Olivetti faces dataset.
 
-The original database was available from
+The original database was available from (now defunct)
 
     http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html
 
@@ -8,21 +8,12 @@
 web page of Sam Roweis:
 
     http://www.cs.nyu.edu/~roweis/
-
-There are ten different images of each of 40 distinct subjects. For some
-subjects, the images were taken at different times, varying the lighting,
-facial expressions (open / closed eyes, smiling / not smiling) and facial
-details (glasses / no glasses). All the images were taken against a dark
-homogeneous background with the subjects in an upright, frontal position (with
-tolerance for some side movement).
-
-The original dataset consisted of 92 x 112, while the Roweis version
-consists of 64x64 images.
 """
+
 # Copyright (c) 2011 David Warde-Farley <wardefar at iro dot umontreal dot ca>
 # License: BSD 3 clause
 
-from os.path import exists
+from os.path import dirname, exists, join
 from os import makedirs, remove
 
 import numpy as np
@@ -43,16 +34,21 @@
     checksum=('b612fb967f2dc77c9c62d3e1266e0c73'
               'd5fca46a4b8906c18e454d41af987794'))
 
-# Grab the module-level docstring to use as a description of the
-# dataset
-MODULE_DOCS = __doc__
-
 
 def fetch_olivetti_faces(data_home=None, shuffle=False, random_state=0,
                          download_if_missing=True):
-    """Loader for the Olivetti faces data-set from AT&T.
+    """Load the Olivetti faces data-set from AT&T (classification).
 
-    Read more in the :ref:`User Guide <olivetti_faces>`.
+    Download it if necessary.
+
+    =================   =====================
+    Classes                                40
+    Samples total                         400
+    Dimensionality                       4096
+    Features            real, between 0 and 1
+    =================   =====================
+
+    Read more in the :ref:`User Guide <olivetti_faces_dataset>`.
 
     Parameters
     ----------
@@ -91,20 +87,6 @@ def fetch_olivetti_faces(data_home=None, shuffle=False, random_state=0,
 
     DESCR : string
         Description of the modified Olivetti Faces Dataset.
-
-    Notes
-    ------
-
-    This dataset consists of 10 pictures each of 40 individuals. The original
-    database was available from (now defunct)
-
-        http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html
-
-    The version retrieved here comes in MATLAB format from the personal
-    web page of Sam Roweis:
-
-        http://www.cs.nyu.edu/~roweis/
-
     """
     data_home = get_data_home(data_home=data_home)
     if not exists(data_home):
@@ -140,7 +122,12 @@ def fetch_olivetti_faces(data_home=None, shuffle=False, random_state=0,
         order = random_state.permutation(len(faces))
         faces = faces[order]
         target = target[order]
+
+    module_path = dirname(__file__)
+    with open(join(module_path, 'descr', 'covtype.rst')) as rst_file:
+        fdescr = rst_file.read()
+
     return Bunch(data=faces.reshape(len(faces), -1),
                  images=faces,
                  target=target,
-                 DESCR=MODULE_DOCS)
+                 DESCR=fdescr)
diff --git a/sklearn/datasets/rcv1.py b/sklearn/datasets/rcv1.py
index b0ef91972a..7890d7e18a 100755
--- a/sklearn/datasets/rcv1.py
+++ b/sklearn/datasets/rcv1.py
@@ -1,4 +1,8 @@
 """RCV1 dataset.
+
+The dataset page is available at
+
+    http://jmlr.csail.mit.edu/papers/volume5/lewis04a/
 """
 
 # Author: Tom Dupre la Tour
@@ -7,7 +11,7 @@
 import logging
 
 from os import remove
-from os.path import exists, join
+from os.path import dirname, exists, join
 from gzip import GzipFile
 
 import numpy as np
@@ -74,18 +78,20 @@
 
 def fetch_rcv1(data_home=None, subset='all', download_if_missing=True,
                random_state=None, shuffle=False, return_X_y=False):
-    """Load the RCV1 multilabel dataset, downloading it if necessary.
+    """Load the RCV1 multilabel dataset (classification).
+
+    Download it if necessary.
 
     Version: RCV1-v2, vectors, full sets, topics multilabels.
 
-    ==============     =====================
-    Classes                              103
-    Samples total                     804414
-    Dimensionality                     47236
-    Features           real, between 0 and 1
-    ==============     =====================
+    =================   =====================
+    Classes                               103
+    Samples total                      804414
+    Dimensionality                      47236
+    Features            real, between 0 and 1
+    =================   =====================
 
-    Read more in the :ref:`User Guide <datasets>`.
+    Read more in the :ref:`User Guide <rcv1_dataset>`.
 
     .. versionadded:: 0.17
 
@@ -143,13 +149,6 @@ def fetch_rcv1(data_home=None, subset='all', download_if_missing=True,
     (data, target) : tuple if ``return_X_y`` is True
 
         .. versionadded:: 0.20
-
-    References
-    ----------
-    Lewis, D. D., Yang, Y., Rose, T. G., & Li, F. (2004). RCV1: A new
-    benchmark collection for text categorization research. The Journal of
-    Machine Learning Research, 5, 361-397.
-
     """
     N_SAMPLES = 804414
     N_FEATURES = 47236
@@ -265,11 +264,15 @@ def fetch_rcv1(data_home=None, subset='all', download_if_missing=True,
     if shuffle:
         X, y, sample_id = shuffle_(X, y, sample_id, random_state=random_state)
 
+    module_path = dirname(__file__)
+    with open(join(module_path, 'descr', 'rcv1.rst')) as rst_file:
+        fdescr = rst_file.read()
+
     if return_X_y:
         return X, y
 
     return Bunch(data=X, target=y, sample_id=sample_id,
-                 target_names=categories, DESCR=__doc__)
+                 target_names=categories, DESCR=fdescr)
 
 
 def _inverse_permutation(p):
diff --git a/sklearn/datasets/twenty_newsgroups.py b/sklearn/datasets/twenty_newsgroups.py
index 38739e6e60..50c28c270f 100755
--- a/sklearn/datasets/twenty_newsgroups.py
+++ b/sklearn/datasets/twenty_newsgroups.py
@@ -20,22 +20,12 @@
 dataset and which features a point in time split between the train and
 test sets. The compressed dataset size is around 14 Mb compressed. Once
 uncompressed the train set is 52 MB and the test set is 34 MB.
-
-The data is downloaded, extracted and cached in the '~/scikit_learn_data'
-folder.
-
-The `fetch_20newsgroups` function will not vectorize the data into numpy
-arrays but the dataset lists the filenames of the posts and their categories
-as target labels.
-
-The `fetch_20newsgroups_vectorized` function will in addition do a simple
-tf-idf vectorization step.
-
 """
 # Copyright (c) 2011 Olivier Grisel <olivier.grisel@ensta.org>
 # License: BSD 3 clause
 
 import os
+from os.path import dirname, join
 import logging
 import tarfile
 import pickle
@@ -168,9 +158,19 @@ def fetch_20newsgroups(data_home=None, subset='train', categories=None,
                        shuffle=True, random_state=42,
                        remove=(),
                        download_if_missing=True):
-    """Load the filenames and data from the 20 newsgroups dataset.
+    """Load the filenames and data from the 20 newsgroups dataset \
+(classification).
 
-    Read more in the :ref:`User Guide <20newsgroups>`.
+    Download it if necessary.
+
+    =================   ==========
+    Classes                     20
+    Samples total            18846
+    Dimensionality               1
+    Features                  text
+    =================   ==========
+
+    Read more in the :ref:`User Guide <20newsgroups_dataset>`.
 
     Parameters
     ----------
@@ -213,6 +213,14 @@ def fetch_20newsgroups(data_home=None, subset='train', categories=None,
     download_if_missing : optional, True by default
         If False, raise an IOError if the data is not locally available
         instead of trying to download the data from the source site.
+
+    Returns
+    -------
+    bunch : Bunch object
+        bunch.data: list, length [n_samples]
+        bunch.target: array, shape [n_samples]
+        bunch.filenames: list, length [n_classes]
+        bunch.DESCR: a description of the dataset.
     """
 
     data_home = get_data_home(data_home=data_home)
@@ -260,7 +268,11 @@ def fetch_20newsgroups(data_home=None, subset='train', categories=None,
         raise ValueError(
             "subset can only be 'train', 'test' or 'all', got '%s'" % subset)
 
-    data.description = 'the 20 newsgroups by date dataset'
+    module_path = dirname(__file__)
+    with open(join(module_path, 'descr', 'twenty_newsgroups.rst')) as rst_file:
+        fdescr = rst_file.read()
+
+    data.DESCR = fdescr
 
     if 'headers' in remove:
         data.data = [strip_newsgroup_header(text) for text in data.data]
@@ -301,14 +313,24 @@ def fetch_20newsgroups(data_home=None, subset='train', categories=None,
 
 def fetch_20newsgroups_vectorized(subset="train", remove=(), data_home=None,
                                   download_if_missing=True, return_X_y=False):
-    """Load the 20 newsgroups dataset and transform it into tf-idf vectors.
+    """Load the 20 newsgroups dataset and transform it into tf-idf vectors \
+(classification).
+
+    Download it if necessary.
 
     This is a convenience function; the tf-idf transformation is done using the
     default settings for `sklearn.feature_extraction.text.Vectorizer`. For more
     advanced usage (stopword filtering, n-gram extraction, etc.), combine
     fetch_20newsgroups with a custom `Vectorizer` or `CountVectorizer`.
 
-    Read more in the :ref:`User Guide <20newsgroups>`.
+    =================   ==========
+    Classes                     20
+    Samples total            18846
+    Dimensionality          130107
+    Features                  real
+    =================   ==========
+
+    Read more in the :ref:`User Guide <20newsgroups_dataset>`.
 
     Parameters
     ----------
@@ -346,6 +368,7 @@ def fetch_20newsgroups_vectorized(subset="train", remove=(), data_home=None,
         bunch.data: sparse matrix, shape [n_samples, n_features]
         bunch.target: array, shape [n_samples]
         bunch.target_names: list, length [n_classes]
+        bunch.DESCR: a description of the dataset.
 
     (data, target) : tuple if ``return_X_y`` is True
 
@@ -404,7 +427,14 @@ def fetch_20newsgroups_vectorized(subset="train", remove=(), data_home=None,
         raise ValueError("%r is not a valid subset: should be one of "
                          "['train', 'test', 'all']" % subset)
 
+    module_path = dirname(__file__)
+    with open(join(module_path, 'descr', 'twenty_newsgroups.rst')) as rst_file:
+        fdescr = rst_file.read()
+
     if return_X_y:
         return data, target
 
-    return Bunch(data=data, target=target, target_names=target_names)
+    return Bunch(data=data,
+                 target=target,
+                 target_names=target_names,
+                 DESCR=fdescr)
diff --git a/sklearn/decomposition/sparse_pca.py b/sklearn/decomposition/sparse_pca.py
index d2c700a730..6b70826c82 100755
--- a/sklearn/decomposition/sparse_pca.py
+++ b/sklearn/decomposition/sparse_pca.py
@@ -434,7 +434,6 @@ def fit(self, X, y=None):
                           "compatibility mode will be removed in 0.22.",
                           DeprecationWarning)
 
-        self.n_samples = X.shape[0]
         if self.n_components is None:
             n_components = X.shape[1]
         else:
