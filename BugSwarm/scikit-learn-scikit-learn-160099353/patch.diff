diff --git a/sklearn/metrics/tests/test_common.py b/sklearn/metrics/tests/test_common.py
index edcd5218f2..c7fe05e615 100755
--- a/sklearn/metrics/tests/test_common.py
+++ b/sklearn/metrics/tests/test_common.py
@@ -956,7 +956,7 @@ def check_sample_weight_invariance(name, metric, y1, y2):
     # check that the weighted and unweighted scores are unequal
     weighted_score = metric(y1, y2, sample_weight=sample_weight)
     if name != "median_absolute_error":
-    # unweighted and weighted give same value for this metric. see #6217
+        # unweighted and weighted give same value for this metric. see #6217
         assert_not_equal(
             unweighted_score, weighted_score,
             msg="Unweighted and weighted scores are unexpectedly "
diff --git a/sklearn/metrics/tests/test_regression.py b/sklearn/metrics/tests/test_regression.py
index 272cacb2fe..6eec266e57 100755
--- a/sklearn/metrics/tests/test_regression.py
+++ b/sklearn/metrics/tests/test_regression.py
@@ -142,23 +142,23 @@ def test_regression_custom_weights():
     assert_almost_equal(maew, 0.475, decimal=3)
     assert_almost_equal(rw, 0.94, decimal=2)
     assert_almost_equal(evsw, 0.94, decimal=2)
-    
+
+
 def test_median_absolute_error_weights():
     y_tr = [3, -0.5, 2, 7]
     y_pr = [2.5, 0.0, 2, 8]
     sample_weight = [2, 3, 1, 4]
     # check that unit weights gives the same score as no weight
     unweighted_score = median_absolute_error(y_tr, y_pr, sample_weight=None)
-    assert_almost_equal(
-        unweighted_score, median_absolute_error(y_tr, y_pr,
-                        sample_weight=np.ones(shape=len(y_tr))),
-        err_msg="For median_absolute_error sample_weight=None is not "
-                "equivalent to sample_weight=ones" )
+    weighted_score = median_absolute_error(y_tr, y_pr,
+                                           sample_weight=np.ones(len(y_tr)))
+    assert_almost_equal(unweighted_score, weighted_score,
+                        err_msg="For median_absolute_error sample_weight=None"
+                        "is not equivalent to sample_weight=ones")
 
     # check that the weighted and unweighted scores are unequal
     weighted_score = median_absolute_error(y_tr, y_pr,
-                            sample_weight=sample_weight)
-    assert_not_equal(
-        unweighted_score, weighted_score,
-        msg="Unweighted and weighted scores are unexpectedly "
-            "equal (%f) for median_absolute_error" % weighted_score)
+                                           sample_weight=sample_weight)
+    assert_not_equal(unweighted_score, weighted_score,
+                     msg="Unweighted and weighted scores are unexpectedly "
+                     "equal (%f) for median_absolute_error" % weighted_score)
diff --git a/sklearn/utils/extmath.py b/sklearn/utils/extmath.py
index 0b302042d8..d9ed0c669e 100755
--- a/sklearn/utils/extmath.py
+++ b/sklearn/utils/extmath.py
@@ -896,7 +896,8 @@ def weighted_median(array, sample_weight):
     sample_weight = np.asarray(sample_weight)
     sorted_sample_weight = sample_weight[sorted_idx]
     weight_cdf = sorted_sample_weight.cumsum()
-    weighted_percentile = (weight_cdf - sorted_sample_weight/2.0) / weight_cdf[-1]
+    weighted_percentile = weight_cdf - sorted_sample_weight/2.0
+    weighted_percentile /= weight_cdf[-1]
     sorted_array = array[sorted_idx]
     weighted_median = np.interp(0.5, weighted_percentile, sorted_array)
     return weighted_median
