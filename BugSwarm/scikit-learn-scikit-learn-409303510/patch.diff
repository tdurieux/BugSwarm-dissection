diff --git a/.travis.yml b/.travis.yml
index 89ab1a3805..7196296a38 100755
--- a/.travis.yml
+++ b/.travis.yml
@@ -38,13 +38,13 @@ matrix:
            NUMPY_VERSION="1.10.4" SCIPY_VERSION="0.16.1" CYTHON_VERSION="0.25.2"
            PILLOW_VERSION="4.0.0" COVERAGE=true
       if: type != cron
-    # This environment tests the newest supported Anaconda release.
+    # This environment tests the latest available dependencies.
     # It runs tests requiring pandas and PyAMG.
     # It also runs with the site joblib instead of the vendored copy of joblib.
-    - env: DISTRIB="conda" PYTHON_VERSION="3.6.2" INSTALL_MKL="true"
-           NUMPY_VERSION="1.14.2" SCIPY_VERSION="1.0.0" PANDAS_VERSION="0.20.3"
-           CYTHON_VERSION="0.26.1" PYAMG_VERSION="3.3.2" PILLOW_VERSION="4.3.0"
-           JOBLIB_VERSION="0.12" COVERAGE=true
+    - env: DISTRIB="conda" PYTHON_VERSION="*" INSTALL_MKL="true"
+           NUMPY_VERSION="*" SCIPY_VERSION="*" PANDAS_VERSION="*"
+           CYTHON_VERSION="*" PYAMG_VERSION="*" PILLOW_VERSION="*"
+           JOBLIB_VERSION="*" COVERAGE=true
            CHECK_PYTEST_SOFT_DEPENDENCY="true" TEST_DOCSTRINGS="true"
            SKLEARN_SITE_JOBLIB=1
       if: type != cron
diff --git a/MANIFEST.in b/MANIFEST.in
index ed0ca0e872..db605f55f7 100755
--- a/MANIFEST.in
+++ b/MANIFEST.in
@@ -2,7 +2,7 @@ include *.rst
 recursive-include doc *
 recursive-include examples *
 recursive-include sklearn *.c *.h *.pyx *.pxd *.pxi
-recursive-include sklearn/datasets *.csv *.csv.gz *.rst *.jpg *.txt
+recursive-include sklearn/datasets *.csv *.csv.gz *.rst *.jpg *.txt *.arff.gz *.json.gz
 include COPYING
 include AUTHORS.rst
 include README.rst
diff --git a/Makefile b/Makefile
index ac242b12e5..c2da6ec5ce 100755
--- a/Makefile
+++ b/Makefile
@@ -24,7 +24,7 @@ inplace:
 	$(PYTHON) setup.py build_ext -i
 
 test-code: in
-	$(PYTEST) --showlocals -v sklearn
+	$(PYTEST) --showlocals -v sklearn --durations=20
 test-sphinxext:
 	$(PYTEST) --showlocals -v doc/sphinxext/
 test-doc:
diff --git a/doc/conf.py b/doc/conf.py
index fac3b9fc04..e0dc4c6f4a 100755
--- a/doc/conf.py
+++ b/doc/conf.py
@@ -237,6 +237,7 @@
     'scipy': ('https://docs.scipy.org/doc/scipy/reference', None),
     'matplotlib': ('https://matplotlib.org/', None),
     'pandas': ('https://pandas.pydata.org/pandas-docs/stable/', None),
+    'joblib': ('https://joblib.readthedocs.io/en/latest/', None),
 }
 
 sphinx_gallery_conf = {
diff --git a/doc/datasets/index.rst b/doc/datasets/index.rst
index 2f061aabe8..947e55f0c4 100755
--- a/doc/datasets/index.rst
+++ b/doc/datasets/index.rst
@@ -351,89 +351,6 @@ features::
 
  _`Faster API-compatible implementation`: https://github.com/mblondel/svmlight-loader
 
-..
-    For doctests:
-
-    >>> import numpy as np
-    >>> import os
-    >>> import tempfile
-    >>> # Create a temporary folder for the data fetcher
-    >>> custom_data_home = tempfile.mkdtemp()
-    >>> os.makedirs(os.path.join(custom_data_home, 'mldata'))
-
-
-.. _mldata:
-
-Downloading datasets from the mldata.org repository
----------------------------------------------------
-
-`mldata.org <http://mldata.org>`_ is a public repository for machine learning
-data, supported by the `PASCAL network <http://www.pascal-network.org>`_ .
-
-The ``sklearn.datasets`` package is able to directly download data
-sets from the repository using the function
-:func:`sklearn.datasets.fetch_mldata`.
-
-For example, to download the MNIST digit recognition database::
-
-  >>> from sklearn.datasets import fetch_mldata
-  >>> mnist = fetch_mldata('MNIST original', data_home=custom_data_home)
-
-The MNIST database contains a total of 70000 examples of handwritten digits
-of size 28x28 pixels, labeled from 0 to 9::
-
-  >>> mnist.data.shape
-  (70000, 784)
-  >>> mnist.target.shape
-  (70000,)
-  >>> np.unique(mnist.target)
-  array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])
-
-After the first download, the dataset is cached locally in the path
-specified by the ``data_home`` keyword argument, which defaults to
-``~/scikit_learn_data/``::
-
-  >>> os.listdir(os.path.join(custom_data_home, 'mldata'))
-  ['mnist-original.mat']
-
-Data sets in `mldata.org <http://mldata.org>`_ do not adhere to a strict
-naming or formatting convention. :func:`sklearn.datasets.fetch_mldata` is
-able to make sense of the most common cases, but allows to tailor the
-defaults to individual datasets:
-
-* The data arrays in `mldata.org <http://mldata.org>`_ are most often
-  shaped as ``(n_features, n_samples)``. This is the opposite of the
-  ``scikit-learn`` convention, so :func:`sklearn.datasets.fetch_mldata`
-  transposes the matrix by default. The ``transpose_data`` keyword controls
-  this behavior::
-
-    >>> iris = fetch_mldata('iris', data_home=custom_data_home)
-    >>> iris.data.shape
-    (150, 4)
-    >>> iris = fetch_mldata('iris', transpose_data=False,
-    ...                     data_home=custom_data_home)
-    >>> iris.data.shape
-    (4, 150)
-
-* For datasets with multiple columns, :func:`sklearn.datasets.fetch_mldata`
-  tries to identify the target and data columns and rename them to ``target``
-  and ``data``. This is done by looking for arrays named ``label`` and
-  ``data`` in the dataset, and failing that by choosing the first array to be
-  ``target`` and the second to be ``data``. This behavior can be changed with
-  the ``target_name`` and ``data_name`` keywords, setting them to a specific
-  name or index number (the name and order of the columns in the datasets
-  can be found at its `mldata.org <http://mldata.org>`_ under the tab "Data"::
-
-    >>> iris2 = fetch_mldata('datasets-UCI iris', target_name=1, data_name=0,
-    ...                      data_home=custom_data_home)
-    >>> iris3 = fetch_mldata('datasets-UCI iris', target_name='class',
-    ...                      data_name='double0', data_home=custom_data_home)
-
-
-..
-    >>> import shutil
-    >>> shutil.rmtree(custom_data_home)
-
 .. _external_datasets:
 
 Loading from external datasets
diff --git a/doc/datasets/openml.rst b/doc/datasets/openml.rst
new file mode 100755
index 0000000000..52dd453919
--- /dev/null
+++ b/doc/datasets/openml.rst
@@ -0,0 +1,148 @@
+..
+    For doctests:
+
+    >>> import numpy as np
+    >>> import os
+
+
+.. _openml:
+
+Downloading datasets from the openml.org repository
+===================================================
+
+`openml.org <https://openml.org>`_ is a public repository for machine learning
+data and experiments, that allows everybody to upload open datasets.
+
+The ``sklearn.datasets`` package is able to download datasets
+from the repository using the function
+:func:`sklearn.datasets.fetch_openml`.
+
+For example, to download a dataset of gene expressions in mice brains::
+
+  >>> from sklearn.datasets import fetch_openml
+  >>> mice = fetch_openml(name='miceprotein', version=4)
+
+To fully specify a dataset, you need to provide a name and a version, though
+the version is optional, see :ref:`openml_versions` below.
+The dataset contains a total of 1080 examples belonging to 8 different
+classes::
+
+  >>> mice.data.shape
+  (1080, 77)
+  >>> mice.target.shape
+  (1080,)
+  >>> np.unique(mice.target) # doctest: +NORMALIZE_WHITESPACE
+  array(['c-CS-m', 'c-CS-s', 'c-SC-m', 'c-SC-s', 't-CS-m', 't-CS-s', 't-SC-m', 't-SC-s'], dtype=object)
+
+You can get more information on the dataset by looking at the ``DESCR``
+and ``details`` attributes::
+
+  >>> print(mice.DESCR) # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS +SKIP
+  **Author**: Clara Higuera, Katheleen J. Gardiner, Krzysztof J. Cios
+  **Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression) - 2015
+  **Please cite**: Higuera C, Gardiner KJ, Cios KJ (2015) Self-Organizing
+  Feature Maps Identify Proteins Critical to Learning in a Mouse Model of Down
+  Syndrome. PLoS ONE 10(6): e0129126...
+
+  >>> mice.details # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS +SKIP
+  {'id': '40966', 'name': 'MiceProtein', 'version': '4', 'format': 'ARFF',
+  'upload_date': '2017-11-08T16:00:15', 'licence': 'Public',
+  'url': 'https://www.openml.org/data/v1/download/17928620/MiceProtein.arff',
+  'file_id': '17928620', 'default_target_attribute': 'class',
+  'row_id_attribute': 'MouseID',
+  'ignore_attribute': ['Genotype', 'Treatment', 'Behavior'],
+  'tag': ['OpenML-CC18', 'study_135', 'study_98', 'study_99'],
+  'visibility': 'public', 'status': 'active',
+  'md5_checksum': '3c479a6885bfa0438971388283a1ce32'}
+
+
+The ``DESCR`` contains a free-text description of the data, while ``details``
+contains a dictionary of meta-data stored by openml, like the dataset id.
+For more details, see the `OpenML documentation
+<https://docs.openml.org/#data>`_ The ``data_id`` of the mice protein dataset
+is 40966, and you can use this (or the name) to get more information on the
+dataset on the openml website::
+
+  >>> mice.url
+  'https://www.openml.org/d/40966'
+
+The ``data_id`` also uniquely identifies a dataset from OpenML::
+
+  >>> mice = fetch_openml(data_id=40966)
+  >>> mice.details # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS +SKIP
+  {'id': '4550', 'name': 'MiceProtein', 'version': '1', 'format': 'ARFF',
+  'creator': ...,
+  'upload_date': '2016-02-17T14:32:49', 'licence': 'Public', 'url':
+  'https://www.openml.org/data/v1/download/1804243/MiceProtein.ARFF', 'file_id':
+  '1804243', 'default_target_attribute': 'class', 'citation': 'Higuera C,
+  Gardiner KJ, Cios KJ (2015) Self-Organizing Feature Maps Identify Proteins
+  Critical to Learning in a Mouse Model of Down Syndrome. PLoS ONE 10(6):
+  e0129126. [Web Link] journal.pone.0129126', 'tag': ['OpenML100', 'study_14',
+  'study_34'], 'visibility': 'public', 'status': 'active', 'md5_checksum':
+  '3c479a6885bfa0438971388283a1ce32'}
+
+.. _openml_versions:
+
+Dataset Versions
+----------------
+
+A dataset is uniquely specified by its ``data_id``, but not necessarily by its
+name. Several different "versions" of a dataset with the same name can exist
+which can contain entirely different datasets.
+If a particular version of a dataset has been found to contain significant
+issues, it might be deactivated. Using a name to specify a dataset will yield
+the earliest version of a dataset that is still active. That means that
+``fetch_openml(name="miceprotein")`` can yield different results at different
+times if earlier versions become inactive.
+You can see that the dataset with ``data_id`` 40966 that we fetched above is
+the version 1 of the "miceprotein" dataset::
+
+  >>> mice.details['version']  #doctest: +SKIP
+  '1'
+
+In fact, this dataset only has one version. The iris dataset on the other hand
+has multiple versions::
+
+  >>> iris = fetch_openml(name="iris")
+  >>> iris.details['version']  #doctest: +SKIP
+  '1'
+  >>> iris.details['id']  #doctest: +SKIP
+  '61'
+
+  >>> iris_61 = fetch_openml(data_id=61)
+  >>> iris_61.details['version']
+  '1'
+  >>> iris_61.details['id']
+  '61'
+
+  >>> iris_969 = fetch_openml(data_id=969)
+  >>> iris_969.details['version']
+  '3'
+  >>> iris_969.details['id']
+  '969'
+
+Specifying the dataset by the name "iris" yields the lowest version, version 1,
+with the ``data_id`` 61. To make sure you always get this exact dataset, it is
+safest to specify it by the dataset ``data_id``. The other dataset, with
+``data_id`` 969, is version 3 (version 2 has become inactive), and contains a
+binarized version of the data::
+
+  >>> np.unique(iris_969.target)
+  array(['N', 'P'], dtype=object)
+
+You can also specify both the name and the version, which also uniquely
+identifies the dataset::
+
+  >>> iris_version_3 = fetch_openml(name="iris", version=3)
+  >>> iris_version_3.details['version']
+  '3'
+  >>> iris_version_3.details['id']
+  '969'
+
+
+.. topic:: References:
+
+ * Vanschoren, van Rijn, Bischl and Torgo
+   `"OpenML: networked science in machine learning"
+   <https://arxiv.org/pdf/1407.7722.pdf>`_,
+   ACM SIGKDD Explorations Newsletter, 15(2), 49-60, 2014.
diff --git a/doc/developers/contributing.rst b/doc/developers/contributing.rst
index a27bae14ba..31f5e5ef84 100755
--- a/doc/developers/contributing.rst
+++ b/doc/developers/contributing.rst
@@ -79,6 +79,7 @@ link to it from your website, or simply star to say "I use it":
    * `joblib <https://github.com/joblib/joblib/issues>`__
    * `sphinx-gallery <https://github.com/sphinx-gallery/sphinx-gallery/issues>`__
    * `numpydoc <https://github.com/numpy/numpydoc/issues>`__
+   * `liac-arff <https://github.com/renatopp/liac-arff>`__
 
    and larger projects:
 
@@ -140,6 +141,14 @@ feedback:
   your **Python, scikit-learn, numpy, and scipy versions**. This information
   can be found by running the following code snippet::
 
+    >>> import sklearn
+    >>> sklearn.show_versions()  # doctest: +SKIP
+
+  .. note::
+
+    This utility function is only available in scikit-learn v0.20+.
+    For previous versions, one has to explicitly run::
+
      import platform; print(platform.platform())
      import sys; print("Python", sys.version)
      import numpy; print("NumPy", numpy.__version__)
diff --git a/doc/developers/tips.rst b/doc/developers/tips.rst
index 9e66174a7e..9369b650fb 100755
--- a/doc/developers/tips.rst
+++ b/doc/developers/tips.rst
@@ -121,15 +121,11 @@ Issue: Self-contained example for bug
 Issue: Software versions
     ::
 
-        To help diagnose your issue, could you please paste the output of:
+        To help diagnose your issue, please paste the output of:
         ```py
-        import platform; print(platform.platform())
-        import sys; print("Python", sys.version)
-        import numpy; print("NumPy", numpy.__version__)
-        import scipy; print("SciPy", scipy.__version__)
-        import sklearn; print("Scikit-Learn", sklearn.__version__)
+        import sklearn; sklearn.show_versions()
         ```
-        ? Thanks.
+        Thanks.
 
 Issue: Code blocks
     ::
diff --git a/doc/faq.rst b/doc/faq.rst
index bef75f58e1..f6e557ef74 100755
--- a/doc/faq.rst
+++ b/doc/faq.rst
@@ -222,7 +222,10 @@ DBSCAN with Levenshtein distances::
     array([[0],
            [1],
            [2]])
-    >>> dbscan(X, metric=lev_metric, eps=5, min_samples=2)  # doctest: +SKIP
+    >>> # We need to specify algoritum='brute' as the default assumes
+    >>> # a continuous feature space.
+    >>> dbscan(X, metric=lev_metric, eps=5, min_samples=2, algorithm='brute')
+    ... # doctest: +SKIP
     ([0, 1], array([ 0,  0, -1]))
 
 (This uses the third-party edit distance package ``leven``.)
@@ -364,6 +367,7 @@ See also :ref:`sphx_glr_auto_examples_compose_plot_column_transformer_mixed_type
 example of working with heterogeneous (e.g. categorical and numeric) data.
 
 Why does Scikit-learn not directly work with, for example, pandas.DataFrame?
+----------------------------------------------------------------------------
 
 The homogeneous NumPy and SciPy data objects currently expected are most
 efficient to process for most operations. Extensive work would also be needed
diff --git a/doc/glossary.rst b/doc/glossary.rst
index 8a2628e849..4d93696c3a 100755
--- a/doc/glossary.rst
+++ b/doc/glossary.rst
@@ -1479,8 +1479,15 @@ functions or non-estimator constructors.
 
         ``n_jobs`` is an int, specifying the maximum number of concurrently
         running jobs.  If set to -1, all CPUs are used. If 1 is given, no
-        parallel computing code is used at all.  For n_jobs below -1, (n_cpus +
-        1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used.
+        joblib level parallelism is used at all, which is useful for
+        debugging. Even with ``n_jobs = 1``, parallelism may occur due to
+        numerical processing libraries (see :ref:`FAQ <faq_mkl_threading>`).
+        For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for
+        ``n_jobs = -2``, all CPUs but one are used.
+
+        ``n_jobs=None`` means *unset*; it will generally be interpreted as
+        ``n_jobs=1``, unless the current :class:`joblib.Parallel` backend
+        context specifies otherwise.
 
         The use of ``n_jobs``-based parallelism in estimators varies:
 
diff --git a/doc/modules/classes.rst b/doc/modules/classes.rst
index 1753bf9b40..57ccfb5cff 100755
--- a/doc/modules/classes.rst
+++ b/doc/modules/classes.rst
@@ -48,6 +48,7 @@ Functions
    config_context
    get_config
    set_config
+   show_versions
 
 .. _calibration_ref:
 
@@ -256,8 +257,8 @@ Loaders
    datasets.fetch_kddcup99
    datasets.fetch_lfw_pairs
    datasets.fetch_lfw_people
-   datasets.fetch_mldata
    datasets.fetch_olivetti_faces
+   datasets.fetch_openml
    datasets.fetch_rcv1
    datasets.fetch_species_distributions
    datasets.get_data_home
@@ -1511,6 +1512,7 @@ To be removed in 0.22
    :template: deprecated_function.rst
 
    covariance.graph_lasso
+   datasets.fetch_mldata
 
 
 To be removed in 0.21
diff --git a/doc/modules/clustering.rst b/doc/modules/clustering.rst
index e9dcebff10..968a66e67f 100755
--- a/doc/modules/clustering.rst
+++ b/doc/modules/clustering.rst
@@ -1282,7 +1282,7 @@ following equation [VEB2009]_. In this equation,
 :math:`b_j = |V_j|` (the number of elements in :math:`V_j`).
 
 
-.. math:: E[\text{MI}(U,V)]=\sum_{i=1}^|U| \sum_{j=1}^|V| \sum_{n_{ij}=(a_i+b_j-N)^+
+.. math:: E[\text{MI}(U,V)]=\sum_{i=1}^{|U|} \sum_{j=1}^{|V|} \sum_{n_{ij}=(a_i+b_j-N)^+
    }^{\min(a_i, b_j)} \frac{n_{ij}}{N}\log \left( \frac{ N.n_{ij}}{a_i b_j}\right)
    \frac{a_i!b_j!(N-a_i)!(N-b_j)!}{N!n_{ij}!(a_i-n_{ij})!(b_j-n_{ij})!
    (N-a_i-b_j+n_{ij})!}
diff --git a/doc/modules/compose.rst b/doc/modules/compose.rst
index 7ef7fac1ae..5a291bfaeb 100755
--- a/doc/modules/compose.rst
+++ b/doc/modules/compose.rst
@@ -342,7 +342,7 @@ and ``value`` is an estimator object::
     >>> estimators = [('linear_pca', PCA()), ('kernel_pca', KernelPCA())]
     >>> combined = FeatureUnion(estimators)
     >>> combined # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS
-    FeatureUnion(n_jobs=1,
+    FeatureUnion(n_jobs=None,
                  transformer_list=[('linear_pca', PCA(copy=True,...)),
                                    ('kernel_pca', KernelPCA(alpha=1.0,...))],
                  transformer_weights=None)
@@ -357,7 +357,7 @@ and ignored by setting to ``None``::
 
     >>> combined.set_params(kernel_pca=None)
     ... # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS
-    FeatureUnion(n_jobs=1,
+    FeatureUnion(n_jobs=None,
                  transformer_list=[('linear_pca', PCA(copy=True,...)),
                                    ('kernel_pca', None)],
                  transformer_weights=None)
@@ -423,7 +423,7 @@ By default, the remaining rating columns are ignored (``remainder='drop'``)::
   ...      remainder='drop')
 
   >>> column_trans.fit(X) # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
-  ColumnTransformer(n_jobs=1, remainder='drop', sparse_threshold=0.3,
+  ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,
       transformer_weights=None,
       transformers=...)
 
@@ -496,7 +496,7 @@ above example would be::
   ...     ('city', CountVectorizer(analyzer=lambda x: [x])),
   ...     ('title', CountVectorizer()))
   >>> column_trans # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
-  ColumnTransformer(n_jobs=1, remainder='drop', sparse_threshold=0.3,
+  ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,
            transformer_weights=None,
            transformers=[('countvectorizer-1', ...)
 
diff --git a/doc/modules/cross_validation.rst b/doc/modules/cross_validation.rst
index 328270b086..2d05e4b81c 100755
--- a/doc/modules/cross_validation.rst
+++ b/doc/modules/cross_validation.rst
@@ -323,6 +323,14 @@ Example of 2-fold cross-validation on a dataset with 4 samples::
   [2 3] [0 1]
   [0 1] [2 3]
 
+Here is a visualization of the cross-validation behavior. Note that
+:class:`KFold` is not affected by classes or groups.
+
+.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_004.png
+   :target: ../auto_examples/model_selection/plot_cv_indices.html
+   :align: center
+   :scale: 75%
+
 Each fold is constituted by two arrays: the first one is related to the
 *training set*, and the second one to the *test set*.
 Thus, one can create the training/test sets using numpy indexing::
@@ -471,6 +479,14 @@ Here is a usage example::
   [2 7 5 8 0 3 4] [6 1 9]
   [4 1 0 6 8 9 3] [5 2 7]
 
+Here is a visualization of the cross-validation behavior. Note that
+:class:`ShuffleSplit` is not affected by classes or groups.
+
+.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_006.png
+   :target: ../auto_examples/model_selection/plot_cv_indices.html
+   :align: center
+   :scale: 75%
+
 :class:`ShuffleSplit` is thus a good alternative to :class:`KFold` cross
 validation that allows a finer control on the number of iterations and
 the proportion of samples on each side of the train / test split.
@@ -506,6 +522,13 @@ two slightly unbalanced classes::
   [0 1 3 4 5 8 9] [2 6 7]
   [0 1 2 4 5 6 7] [3 8 9]
 
+Here is a visualization of the cross-validation behavior.
+
+.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_007.png
+   :target: ../auto_examples/model_selection/plot_cv_indices.html
+   :align: center
+   :scale: 75%
+
 :class:`RepeatedStratifiedKFold` can be used to repeat Stratified K-Fold n times
 with different randomization in each repetition.
 
@@ -517,6 +540,13 @@ Stratified Shuffle Split
 stratified splits, *i.e* which creates splits by preserving the same
 percentage for each target class as in the complete set.
 
+Here is a visualization of the cross-validation behavior.
+
+.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_009.png
+   :target: ../auto_examples/model_selection/plot_cv_indices.html
+   :align: center
+   :scale: 75%
+
 .. _group_cv:
 
 Cross-validation iterators for grouped data.
@@ -569,6 +599,12 @@ Each subject is in a different testing fold, and the same subject is never in
 both testing and training. Notice that the folds do not have exactly the same
 size due to the imbalance in the data.
 
+Here is a visualization of the cross-validation behavior.
+
+.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_005.png
+   :target: ../auto_examples/model_selection/plot_cv_indices.html
+   :align: center
+   :scale: 75%
 
 Leave One Group Out
 ^^^^^^^^^^^^^^^^^^^
@@ -645,6 +681,13 @@ Here is a usage example::
   [2 3 4 5] [0 1 6 7]
   [4 5 6 7] [0 1 2 3]
 
+Here is a visualization of the cross-validation behavior.
+
+.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_008.png
+   :target: ../auto_examples/model_selection/plot_cv_indices.html
+   :align: center
+   :scale: 75%
+
 This class is useful when the behavior of :class:`LeavePGroupsOut` is
 desired, but the number of groups is large enough that generating all
 possible partitions with :math:`P` groups withheld would be prohibitively
@@ -709,6 +752,12 @@ Example of 3-split time series cross-validation on a dataset with 6 samples::
   [0 1 2 3] [4]
   [0 1 2 3 4] [5]
 
+Here is a visualization of the cross-validation behavior.
+
+.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_010.png
+   :target: ../auto_examples/model_selection/plot_cv_indices.html
+   :align: center
+   :scale: 75%
 
 A note on shuffling
 ===================
diff --git a/doc/modules/ensemble.rst b/doc/modules/ensemble.rst
index 85fd8a30ba..a41c8201a3 100755
--- a/doc/modules/ensemble.rst
+++ b/doc/modules/ensemble.rst
@@ -218,7 +218,7 @@ setting ``oob_score=True``.
     The size of the model with the default parameters is :math:`O( M * N * log (N) )`,
     where :math:`M` is the number of trees and :math:`N` is the number of samples.
     In order to reduce the size of the model, you can change these parameters:
-    ``min_samples_split``, ``min_samples_leaf``, ``max_leaf_nodes`` and ``max_depth``.
+    ``min_samples_split``, ``max_leaf_nodes`` and ``max_depth``.
 
 Parallelization
 ---------------
@@ -260,11 +260,16 @@ respect to the predictability of the target variable. Features used at
 the top of the tree contribute to the final prediction decision of a
 larger fraction of the input samples. The **expected fraction of the
 samples** they contribute to can thus be used as an estimate of the
-**relative importance of the features**.
+**relative importance of the features**. In scikit-learn, the fraction of
+samples a feature contributes to is combined with the decrease in impurity
+from splitting them to create a normalized estimate of the predictive power
+of that feature.
 
-By **averaging** those expected activity rates over several randomized
+By **averaging** the estimates of predictive ability over several randomized
 trees one can **reduce the variance** of such an estimate and use it
-for feature selection.
+for feature selection. This is known as the mean decrease in impurity, or MDI.
+Refer to [L2014]_ for more information on MDI and feature importance
+evaluation with Random Forests.
 
 The following example shows a color-coded representation of the relative
 importances of each individual pixel for a face recognition task using
@@ -288,6 +293,12 @@ to the prediction function.
 
 .. _random_trees_embedding:
 
+.. topic:: References
+
+ .. [L2014] G. Louppe,
+         "Understanding Random Forests: From Theory to Practice",
+         PhD Thesis, U. of Liege, 2014.
+
 Totally Random Trees Embedding
 ------------------------------
 
@@ -382,9 +393,7 @@ The number of weak learners is controlled by the parameter ``n_estimators``. The
 the final combination. By default, weak learners are decision stumps. Different
 weak learners can be specified through the ``base_estimator`` parameter.
 The main parameters to tune to obtain good results are ``n_estimators`` and
-the complexity of the base estimators (e.g., its depth ``max_depth`` or
-minimum required number of samples at a leaf ``min_samples_leaf`` in case of
-decision trees).
+the complexity of the base estimators (e.g., its depth ``max_depth``).
 
 .. topic:: Examples:
 
@@ -964,7 +973,8 @@ The following example shows how to fit the majority rule classifier::
    >>> iris = datasets.load_iris()
    >>> X, y = iris.data[:, 1:3], iris.target
 
-   >>> clf1 = LogisticRegression(random_state=1)
+   >>> clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial',
+   ...                           random_state=1)
    >>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)
    >>> clf3 = GaussianNB()
 
@@ -973,10 +983,10 @@ The following example shows how to fit the majority rule classifier::
    >>> for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):
    ...     scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')
    ...     print("Accuracy: %0.2f (+/- %0.2f) [%s]" % (scores.mean(), scores.std(), label))
-   Accuracy: 0.90 (+/- 0.05) [Logistic Regression]
+   Accuracy: 0.95 (+/- 0.04) [Logistic Regression]
    Accuracy: 0.94 (+/- 0.04) [Random Forest]
    Accuracy: 0.91 (+/- 0.04) [naive Bayes]
-   Accuracy: 0.95 (+/- 0.05) [Ensemble]
+   Accuracy: 0.95 (+/- 0.04) [Ensemble]
 
 
 Weighted Average Probabilities (Soft Voting)
@@ -1049,7 +1059,8 @@ The `VotingClassifier` can also be used together with `GridSearch` in order
 to tune the hyperparameters of the individual estimators::
 
    >>> from sklearn.model_selection import GridSearchCV
-   >>> clf1 = LogisticRegression(random_state=1)
+   >>> clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial',
+   ...                           random_state=1)
    >>> clf2 = RandomForestClassifier(random_state=1)
    >>> clf3 = GaussianNB()
    >>> eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')
diff --git a/doc/modules/feature_extraction.rst b/doc/modules/feature_extraction.rst
index 97001f37fb..b3867373cb 100755
--- a/doc/modules/feature_extraction.rst
+++ b/doc/modules/feature_extraction.rst
@@ -919,7 +919,7 @@ concepts may not map one-to-one onto Lucene concepts.)
 
 To make the preprocessor, tokenizer and analyzers aware of the model
 parameters it is possible to derive from the class and override the
-``build_preprocessor``, ``build_tokenizer``` and ``build_analyzer``
+``build_preprocessor``, ``build_tokenizer`` and ``build_analyzer``
 factory methods instead of passing custom functions.
 
 Some tips and tricks:
diff --git a/doc/modules/kernel_approximation.rst b/doc/modules/kernel_approximation.rst
index 00d1569839..65a18bca9f 100755
--- a/doc/modules/kernel_approximation.rst
+++ b/doc/modules/kernel_approximation.rst
@@ -64,10 +64,9 @@ a linear algorithm, for example a linear SVM::
     SGDClassifier(alpha=0.0001, average=False, class_weight=None,
            early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,
            l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,
-           n_iter=None, n_iter_no_change=5, n_jobs=1, penalty='l2',
+           n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',
            power_t=0.5, random_state=None, shuffle=True, tol=None,
            validation_fraction=0.1, verbose=0, warm_start=False)
-
     >>> clf.score(X_features, y)
     1.0
 
diff --git a/doc/modules/label_propagation.rst b/doc/modules/label_propagation.rst
index 1aba742723..5737368b86 100755
--- a/doc/modules/label_propagation.rst
+++ b/doc/modules/label_propagation.rst
@@ -86,6 +86,7 @@ which can drastically reduce running times.
 
   * :ref:`sphx_glr_auto_examples_semi_supervised_plot_label_propagation_versus_svm_iris.py`
   * :ref:`sphx_glr_auto_examples_semi_supervised_plot_label_propagation_structure.py`
+  * :ref:`sphx_glr_auto_examples_semi_supervised_plot_label_propagation_digits.py`
   * :ref:`sphx_glr_auto_examples_semi_supervised_plot_label_propagation_digits_active_learning.py`
 
 .. topic:: References
diff --git a/doc/modules/linear_model.rst b/doc/modules/linear_model.rst
index dada7db109..ab6b299483 100755
--- a/doc/modules/linear_model.rst
+++ b/doc/modules/linear_model.rst
@@ -45,7 +45,9 @@ and will store the coefficients :math:`w` of the linear model in its
     >>> from sklearn import linear_model
     >>> reg = linear_model.LinearRegression()
     >>> reg.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
-    LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
+    ...                                       # doctest: +NORMALIZE_WHITESPACE
+    LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,
+                     normalize=False)
     >>> reg.coef_
     array([0.5, 0.5])
 
@@ -773,15 +775,20 @@ The "saga" solver [7]_ is a variant of "sag" that also supports the
 non-smooth `penalty="l1"` option. This is therefore the solver of choice
 for sparse multinomial logistic regression.
 
-In a nutshell, one may choose the solver with the following rules:
-
-=================================  =====================================
-Case                               Solver
-=================================  =====================================
-L1 penalty                         "liblinear" or "saga"
-Multinomial loss                   "lbfgs", "sag", "saga" or "newton-cg"
-Very Large dataset (`n_samples`)   "sag" or "saga"
-=================================  =====================================
+In a nutshell, the following table summarizes the solvers characteristics:
+
+============================   ===========  =======  ===========  =====  ======
+solver                         'liblinear'  'lbfgs'  'newton-cg'  'sag'  'saga'
+============================   ===========  =======  ===========  =====  ======
+Multinomial + L2 penalty       no           yes      yes          yes    yes
+OVR + L2 penalty               yes          yes      yes          yes    yes
+Multinomial + L1 penalty       no           no       no           no     yes
+OVR + L1 penalty               yes          no       no           no     yes
+============================   ===========  =======  ===========  =====  ======
+Penalize the intercept (bad)   yes          no       no           no     no
+Faster for large datasets      no           no       no           yes    yes
+Robust to unscaled datasets    yes          yes      yes          no     no
+============================   ===========  =======  ===========  =====  ======
 
 The "saga" solver is often the best choice. The "liblinear" solver is
 used by default for historical reasons.
diff --git a/doc/modules/model_evaluation.rst b/doc/modules/model_evaluation.rst
index 38902892b5..89bc3bb4a8 100755
--- a/doc/modules/model_evaluation.rst
+++ b/doc/modules/model_evaluation.rst
@@ -105,7 +105,7 @@ Usage examples:
     >>> model = svm.SVC()
     >>> cross_val_score(model, X, y, cv=5, scoring='wrong_choice')
     Traceback (most recent call last):
-    ValueError: 'wrong_choice' is not a valid scoring value. Valid options are ['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'balanced_accuracy', 'brier_score_loss', 'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'mutual_info_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'v_measure_score']
+    ValueError: 'wrong_choice' is not a valid scoring value. Use sorted(sklearn.metrics.SCORERS.keys()) to get valid options.
 
 .. note::
 
@@ -440,10 +440,10 @@ the total number of predictions).
 
 In contrast, if the conventional accuracy is above chance only because the
 classifier takes advantage of an imbalanced test set, then the balanced
-accuracy, as appropriate, will drop to :math:`\frac{1}{\text{n_classes}}`.
+accuracy, as appropriate, will drop to :math:`\frac{1}{\text{n\_classes}}`.
 
 The score ranges from 0 to 1, or when ``adjusted=True`` is used, it rescaled to
-the range :math:`\frac{1}{1 - \text{n_classes}}` to 1, inclusive, with
+the range :math:`\frac{1}{1 - \text{n\_classes}}` to 1, inclusive, with
 performance at random scoring 0.
 
 If :math:`y_i` is the true value of the :math:`i`-th sample, and :math:`w_i`
@@ -463,8 +463,9 @@ defined as:
 
 With ``adjusted=True``, balanced accuracy reports the relative increase from
 :math:`\texttt{balanced-accuracy}(y, \mathbf{0}, w) =
-\frac{1}{\text{n_classes}}`.  In the binary case, this is also known as
-`*Youden's J statistic* <https://en.wikipedia.org/wiki/Youden%27s_J_statistic>`_, or *informedness*.
+\frac{1}{\text{n\_classes}}`.  In the binary case, this is also known as
+`*Youden's J statistic* <https://en.wikipedia.org/wiki/Youden%27s_J_statistic>`_,
+or *informedness*.
 
 .. note::
 
@@ -590,13 +591,15 @@ and inferred labels::
    >>> y_pred = [0, 0, 2, 1, 0]
    >>> target_names = ['class 0', 'class 1', 'class 2']
    >>> print(classification_report(y_true, y_pred, target_names=target_names))
-                precision    recall  f1-score   support
+                 precision    recall  f1-score   support
    <BLANKLINE>
-       class 0       0.67      1.00      0.80         2
-       class 1       0.00      0.00      0.00         1
-       class 2       1.00      0.50      0.67         2
+        class 0       0.67      1.00      0.80         2
+        class 1       0.00      0.00      0.00         1
+        class 2       1.00      0.50      0.67         2
    <BLANKLINE>
-   avg / total       0.67      0.60      0.59         5
+      micro avg       0.60      0.60      0.60         5
+      macro avg       0.56      0.50      0.49         5
+   weighted avg       0.67      0.60      0.59         5
    <BLANKLINE>
 
 .. topic:: Example:
diff --git a/doc/modules/neural_networks_unsupervised.rst b/doc/modules/neural_networks_unsupervised.rst
index 262eba614c..914ef3d1f0 100755
--- a/doc/modules/neural_networks_unsupervised.rst
+++ b/doc/modules/neural_networks_unsupervised.rst
@@ -59,8 +59,8 @@ The energy function measures the quality of a joint assignment:
 
 .. math:: 
 
-   E(\mathbf{v}, \mathbf{h}) = \sum_i \sum_j w_{ij}v_ih_j + \sum_i b_iv_i
-     + \sum_j c_jh_j
+   E(\mathbf{v}, \mathbf{h}) = -\sum_i \sum_j w_{ij}v_ih_j - \sum_i b_iv_i
+     - \sum_j c_jh_j
 
 In the formula above, :math:`\mathbf{b}` and :math:`\mathbf{c}` are the
 intercept vectors for the visible and hidden layers, respectively. The
diff --git a/doc/modules/outlier_detection.rst b/doc/modules/outlier_detection.rst
index 6f7f658521..9dbe013bef 100755
--- a/doc/modules/outlier_detection.rst
+++ b/doc/modules/outlier_detection.rst
@@ -306,10 +306,10 @@ This strategy is illustrated below.
 .. topic:: Examples:
 
    * See :ref:`sphx_glr_auto_examples_neighbors_plot_lof_outlier_detection.py`
-   for an illustration of the use of :class:`neighbors.LocalOutlierFactor`.
+     for an illustration of the use of :class:`neighbors.LocalOutlierFactor`.
 
    * See :ref:`sphx_glr_auto_examples_plot_anomaly_comparison.py` for a
-   comparison with other anomaly detection methods.
+     comparison with other anomaly detection methods.
 
 .. topic:: References:
 
diff --git a/doc/modules/sgd.rst b/doc/modules/sgd.rst
index 55c25b9bb1..5792badf50 100755
--- a/doc/modules/sgd.rst
+++ b/doc/modules/sgd.rst
@@ -64,7 +64,7 @@ for the training samples::
     SGDClassifier(alpha=0.0001, average=False, class_weight=None,
                early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,
                l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,
-               n_iter=None, n_iter_no_change=5, n_jobs=1, penalty='l2',
+               n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',
                power_t=0.5, random_state=None, shuffle=True, tol=None,
                validation_fraction=0.1, verbose=0, warm_start=False)
 
diff --git a/doc/modules/tree.rst b/doc/modules/tree.rst
index 91b58b031b..5d448f86a3 100755
--- a/doc/modules/tree.rst
+++ b/doc/modules/tree.rst
@@ -330,29 +330,18 @@ Tips on practical use
     for each additional level the tree grows to.  Use ``max_depth`` to control
     the size of the tree to prevent overfitting.
 
-  * Use ``min_samples_split`` or ``min_samples_leaf`` to control the number of
-    samples at a leaf node.  A very small number will usually mean the tree
-    will overfit, whereas a large number will prevent the tree from learning
-    the data. Try ``min_samples_leaf=5`` as an initial value. If the sample size
-    varies greatly, a float number can be used as percentage in these two parameters.
-    The main difference between the two is that ``min_samples_leaf`` guarantees
-    a minimum number of samples in a leaf, while ``min_samples_split`` can
-    create arbitrary small leaves, though ``min_samples_split`` is more common
-    in the literature.
+  * Use ``min_samples_split`` to control the number of samples at a leaf node.
+    A very small number will usually mean the tree will overfit, whereas a
+    large number will prevent the tree from learning the data. If the sample
+    size varies greatly, a float number can be used as percentage in this
+    parameter. Note that ``min_samples_split`` can create arbitrarily
+    small leaves.
 
   * Balance your dataset before training to prevent the tree from being biased
     toward the classes that are dominant. Class balancing can be done by
     sampling an equal number of samples from each class, or preferably by
     normalizing the sum of the sample weights (``sample_weight``) for each
-    class to the same value. Also note that weight-based pre-pruning criteria,
-    such as ``min_weight_fraction_leaf``, will then be less biased toward
-    dominant classes than criteria that are not aware of the sample weights,
-    like ``min_samples_leaf``.
-
-  * If the samples are weighted, it will be easier to optimize the tree
-    structure using weight-based pre-pruning criterion such as
-    ``min_weight_fraction_leaf``, which ensure that leaf nodes contain at least
-    a fraction of the overall sum of the sample weights.
+    class to the same value.
 
   * All decision trees use ``np.float32`` arrays internally.
     If training data is not in this format, a copy of the dataset will be made.
diff --git a/doc/themes/scikit-learn/static/css/bootstrap.css b/doc/themes/scikit-learn/static/css/bootstrap.css
index 2ae4c14088..2cdd4ac5f9 100755
--- a/doc/themes/scikit-learn/static/css/bootstrap.css
+++ b/doc/themes/scikit-learn/static/css/bootstrap.css
@@ -909,6 +909,11 @@ a.badge:focus {
 .badge-warning[href] {
   background-color: #c67605;
 }
+.label-danger,
+.badge-danger {
+  /* XXX: backported from later bootstrap */
+  background-color: #d9534f;
+}
 .label-success,
 .badge-success {
   background-color: #468847;
diff --git a/doc/themes/scikit-learn/static/css/bootstrap.min.css b/doc/themes/scikit-learn/static/css/bootstrap.min.css
index 2f4146227e..0243215b66 100755
--- a/doc/themes/scikit-learn/static/css/bootstrap.min.css
+++ b/doc/themes/scikit-learn/static/css/bootstrap.min.css
@@ -164,6 +164,7 @@ a.label:hover,a.label:focus,a.badge:hover,a.badge:focus{color:#ffffff;text-decor
 .label-important[href],.badge-important[href]{background-color:#953b39;}
 .label-warning,.badge-warning{background-color:#f89406;}
 .label-warning[href],.badge-warning[href]{background-color:#c67605;}
+.label-danger,.badge-danger {/* XXX: backported from later bootstrap */background-color: #d9534f;}
 .label-success,.badge-success{background-color:#468847;}
 .label-success[href],.badge-success[href]{background-color:#356635;}
 .label-info,.badge-info{background-color:#3a87ad;}
diff --git a/doc/tutorial/basic/tutorial.rst b/doc/tutorial/basic/tutorial.rst
index 781495df99..18189ee385 100755
--- a/doc/tutorial/basic/tutorial.rst
+++ b/doc/tutorial/basic/tutorial.rst
@@ -318,8 +318,8 @@ Refitting and updating parameters
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 Hyper-parameters of an estimator can be updated after it has been constructed
-via the :func:`sklearn.pipeline.Pipeline.set_params` method. Calling ``fit()``
-more than once will overwrite what was learned by any previous ``fit()``::
+via the :term:`set_params()<set_params>` method. Calling ``fit()`` more than
+once will overwrite what was learned by any previous ``fit()``::
 
   >>> import numpy as np
   >>> from sklearn.svm import SVC
@@ -346,9 +346,10 @@ more than once will overwrite what was learned by any previous ``fit()``::
   >>> clf.predict(X_test)
   array([1, 0, 1, 1, 0])
 
-Here, the default kernel ``rbf`` is first changed to ``linear`` after the
-estimator has been constructed via ``SVC()``, and changed back to ``rbf`` to
-refit the estimator and to make a second prediction.
+Here, the default kernel ``rbf`` is first changed to ``linear`` via
+:func:`SVC.set_params()<sklearn.svm.SVC.set_params>` after the estimator has
+been constructed, and changed back to ``rbf`` to refit the estimator and to
+make a second prediction.
 
 Multiclass vs. multilabel fitting
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
diff --git a/doc/tutorial/statistical_inference/model_selection.rst b/doc/tutorial/statistical_inference/model_selection.rst
index f9fc2e2505..50e3a06b9a 100755
--- a/doc/tutorial/statistical_inference/model_selection.rst
+++ b/doc/tutorial/statistical_inference/model_selection.rst
@@ -269,9 +269,9 @@ parameter automatically by cross-validation::
     >>> y_diabetes = diabetes.target
     >>> lasso.fit(X_diabetes, y_diabetes)
     LassoCV(alphas=None, copy_X=True, cv=3, eps=0.001, fit_intercept=True,
-        max_iter=1000, n_alphas=100, n_jobs=1, normalize=False, positive=False,
-        precompute='auto', random_state=None, selection='cyclic', tol=0.0001,
-        verbose=False)
+        max_iter=1000, n_alphas=100, n_jobs=None, normalize=False,
+        positive=False, precompute='auto', random_state=None,
+        selection='cyclic', tol=0.0001, verbose=False)
     >>> # The estimator chose automatically its lambda:
     >>> lasso.alpha_ # doctest: +ELLIPSIS
     0.01229...
diff --git a/doc/tutorial/statistical_inference/supervised_learning.rst b/doc/tutorial/statistical_inference/supervised_learning.rst
index 49e69d9ec8..2c12f1038c 100755
--- a/doc/tutorial/statistical_inference/supervised_learning.rst
+++ b/doc/tutorial/statistical_inference/supervised_learning.rst
@@ -95,7 +95,7 @@ Scikit-learn documentation for more information about this type of classifier.)
     >>> knn = KNeighborsClassifier()
     >>> knn.fit(iris_X_train, iris_y_train) # doctest: +NORMALIZE_WHITESPACE
     KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
-               metric_params=None, n_jobs=1, n_neighbors=5, p=2,
+               metric_params=None, n_jobs=None, n_neighbors=5, p=2,
                weights='uniform')
     >>> knn.predict(iris_X_test)
     array([1, 2, 1, 0, 0, 0, 2, 1, 2, 0])
@@ -176,13 +176,16 @@ Linear models: :math:`y = X\beta + \epsilon`
     >>> from sklearn import linear_model
     >>> regr = linear_model.LinearRegression()
     >>> regr.fit(diabetes_X_train, diabetes_y_train)
-    LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
+    ...                                       # doctest: +NORMALIZE_WHITESPACE
+    LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,
+                     normalize=False)
     >>> print(regr.coef_)
     [   0.30349955 -237.63931533  510.53060544  327.73698041 -814.13170937
       492.81458798  102.84845219  184.60648906  743.51961675   76.09517222]
 
     >>> # The mean square error
-    >>> np.mean((regr.predict(diabetes_X_test)-diabetes_y_test)**2)# doctest: +ELLIPSIS
+    >>> np.mean((regr.predict(diabetes_X_test)-diabetes_y_test)**2)
+    ...                                                   # doctest: +ELLIPSIS
     2004.56760268...
 
     >>> # Explained variance score: 1 is perfect prediction
@@ -257,8 +260,11 @@ diabetes dataset rather than our synthetic data::
     >>> from __future__ import print_function
     >>> print([regr.set_params(alpha=alpha
     ...             ).fit(diabetes_X_train, diabetes_y_train,
-    ...             ).score(diabetes_X_test, diabetes_y_test) for alpha in alphas]) # doctest: +ELLIPSIS
-    [0.5851110683883..., 0.5852073015444..., 0.5854677540698..., 0.5855512036503..., 0.5830717085554..., 0.57058999437...]
+    ...             ).score(diabetes_X_test, diabetes_y_test)
+    ...        for alpha in alphas])
+    ...                            # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE
+    [0.5851110683883..., 0.5852073015444..., 0.5854677540698...,
+     0.5855512036503..., 0.5830717085554..., 0.57058999437...]
 
 
 .. note::
@@ -368,12 +374,13 @@ function or **logistic** function:
 
 ::
 
-    >>> logistic = linear_model.LogisticRegression(C=1e5)
-    >>> logistic.fit(iris_X_train, iris_y_train)
+    >>> log = linear_model.LogisticRegression(solver='lbfgs', C=1e5,
+    ...                                       multi_class='multinomial')
+    >>> log.fit(iris_X_train, iris_y_train)  # doctest: +NORMALIZE_WHITESPACE
     LogisticRegression(C=100000.0, class_weight=None, dual=False,
-              fit_intercept=True, intercept_scaling=1, max_iter=100,
-              multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
-              solver='liblinear', tol=0.0001, verbose=0, warm_start=False)
+        fit_intercept=True, intercept_scaling=1, max_iter=100,
+        multi_class='multinomial', n_jobs=None, penalty='l2', random_state=None,
+        solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)
 
 This is known as :class:`LogisticRegression`.
 
diff --git a/doc/tutorial/text_analytics/working_with_text_data.rst b/doc/tutorial/text_analytics/working_with_text_data.rst
index 24b0b5b3e3..589d83006f 100755
--- a/doc/tutorial/text_analytics/working_with_text_data.rst
+++ b/doc/tutorial/text_analytics/working_with_text_data.rst
@@ -379,7 +379,9 @@ utilities for more detailed performance analysis of the results::
                  sci.med       0.94      0.90      0.92       396
   soc.religion.christian       0.90      0.95      0.93       398
   <BLANKLINE>
-             avg / total       0.92      0.91      0.91      1502
+               micro avg       0.91      0.91      0.91      1502
+               macro avg       0.92      0.91      0.91      1502
+            weighted avg       0.92      0.91      0.91      1502
   <BLANKLINE>
 
   >>> metrics.confusion_matrix(twenty_test.target, predicted)
@@ -439,7 +441,7 @@ parameter combinations in parallel with the ``n_jobs`` parameter. If we give
 this parameter a value of ``-1``, grid search will detect how many cores
 are installed and use them all::
 
-  >>> gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)
+  >>> gs_clf = GridSearchCV(text_clf, parameters, cv=5, iid=False, n_jobs=-1)
 
 The grid search instance behaves like a normal ``scikit-learn``
 model. Let's perform the search on a smaller subset of the training data
@@ -463,7 +465,7 @@ mean score and the parameters setting corresponding to that score::
   ...
   clf__alpha: 0.001
   tfidf__use_idf: True
-  vect__ngram_range: (1, 1)
+  vect__ngram_range: (1, 2)
 
 A more detailed summary of the search is available at ``gs_clf.cv_results_``.
 
diff --git a/doc/whats_new/_contributors.rst b/doc/whats_new/_contributors.rst
index a55cc1bc8e..aeb8b0638d 100755
--- a/doc/whats_new/_contributors.rst
+++ b/doc/whats_new/_contributors.rst
@@ -4,6 +4,20 @@
     for core contributors, and occasionally for contributors who do not want
     their github page to be their URL target. Historically it was used to
     hyperlink all contributors' names, and ``:user:`` should now be preferred.
+    It also defines other ReST substitutions.
+
+.. role:: raw-html(raw)
+   :format: html
+
+.. role:: raw-latex(raw)
+   :format: latex
+
+.. |MajorFeature| replace:: :raw-html:`<span class="label label-success">Major Feature</span>` :raw-latex:`{\small\sc [Major Feature]}`
+.. |Feature| replace:: :raw-html:`<span class="label label-success">Feature</span>` :raw-latex:`{\small\sc [Feature]}`
+.. |Efficiency| replace:: :raw-html:`<span class="label label-info">Efficiency</span>` :raw-latex:`{\small\sc [Efficiency]}`
+.. |Enhancement| replace:: :raw-html:`<span class="label label-info">Enhancement</span>` :raw-latex:`{\small\sc [Enhancement]}`
+.. |Fix| replace:: :raw-html:`<span class="label label-danger">Fix</span>` :raw-latex:`{\small\sc [Fix]}`
+.. |API| replace:: :raw-html:`<span class="label label-warning">API Change</span>` :raw-latex:`{\small\sc [API Change]}`
 
 
 .. _Olivier Grisel: https://twitter.com/ogrisel
diff --git a/doc/whats_new/v0.20.rst b/doc/whats_new/v0.20.rst
index 7636b5fe89..aae4a3e1db 100755
--- a/doc/whats_new/v0.20.rst
+++ b/doc/whats_new/v0.20.rst
@@ -9,7 +9,7 @@ Version 0.20 (under development)
 
 This release packs in a mountain of bug fixes, features and enhancements for
 the Scikit-learn library, and improvements to the documentation and examples.
-Thanks to our many contributors!
+Thanks to our contributors!
 
 .. warning::
 
@@ -23,7 +23,7 @@ We have tried to improve our support for common data-science use-cases
 including missing values, categorical variables, heterogeneous data, and
 features/targets with unusual distributions.
 Missing values in features, represented by NaNs, are now accepted in
-column-wise preprocessing such as scalers.  Each feature is fitted disregarding
+column-wise preprocessing such as scalers. Each feature is fitted disregarding
 NaNs, and data containing NaNs can be transformed. The new :mod:`impute`
 module provides estimators for learning despite missing data.
 
@@ -50,6 +50,9 @@ by `Joel Nothman`_. The glossary is a reference resource to help users and
 contributors become familiar with the terminology and conventions used in
 Scikit-learn.
 
+Sorry if your contribution didn't make it into the highlights. There's a lot
+here...
+
 Changed models
 --------------
 
@@ -59,756 +62,899 @@ occurs due to changes in the modelling logic (bug fixes or enhancements), or in
 random sampling procedures.
 
 - :class:`decomposition.IncrementalPCA` in Python 2 (bug fix)
+- :class:`decomposition.SparsePCA` (bug fix)
+- :class:`ensemble.GradientBoostingClassifier` (bug fix affecting feature importances)
 - :class:`isotonic.IsotonicRegression` (bug fix)
 - :class:`linear_model.ARDRegression` (bug fix)
+- :class:`linear_model.LogisticRegressionCV` (bug fix)
 - :class:`linear_model.OrthogonalMatchingPursuit` (bug fix)
+- :class:`linear_model.PassiveAggressiveClassifier` (bug fix)
+- :class:`linear_model.PassiveAggressiveRegressor` (bug fix)
+- :class:`linear_model.Perceptron` (bug fix)
+- :class:`linear_model.SGDClassifier` (bug fix)
+- :class:`linear_model.SGDRegressor` (bug fix)
 - :class:`metrics.roc_auc_score` (bug fix)
 - :class:`metrics.roc_curve` (bug fix)
 - :class:`neural_network.BaseMultilayerPerceptron` (bug fix)
-- :class:`neural_network.MLPRegressor` (bug fix)
 - :class:`neural_network.MLPClassifier` (bug fix)
-- :class:`linear_model.SGDClassifier` (bug fix)
-- :class:`linear_model.SGDRegressor` (bug fix)
-- :class:`linear_model.PassiveAggressiveClassifier` (bug fix)
-- :class:`linear_model.PassiveAggressiveRegressor` (bug fix)
-- :class:`linear_model.Perceptron` (bug fix)
-- :class:`ensemble.gradient_boosting.GradientBoostingClassifier` (bug fix affecting feature importances)
-- :class:`linear_model.LogisticRegressionCV` (bug fix)
+- :class:`neural_network.MLPRegressor` (bug fix)
 - The v0.19.0 release notes failed to mention a backwards incompatibility with
   :class:`model_selection.StratifiedKFold` when ``shuffle=True`` due to
   :issue:`7823`.
-- :class:`decomposition.SparsePCA` (bug fix)
 
 Details are listed in the changelog below.
 
 (While we are trying to better inform users by providing this information, we
 cannot assure that this list is complete.)
 
+Known Major Bugs
+----------------
+
+* :issue:`11924`: :class:`LogisticRegressionCV` with `solver='lbfgs'` and
+  `multi_class='multinomial'` may be non-deterministic or otherwise broken on
+  MacOS. This appears to be the case on Travis CI servers, but has not been
+  confirmed on personal MacBooks! This issue has been present in previous
+  releases.
+
 Changelog
 ---------
 
 Support for Python 3.3 has been officially dropped.
 
-New features
-............
-
-Classifiers and regressors
-
-- :class:`ensemble.GradientBoostingClassifier` and
-  :class:`ensemble.GradientBoostingRegressor` now support early stopping
-  via ``n_iter_no_change``, ``validation_fraction`` and ``tol``. :issue:`7071`
-  by `Raghav RV`_
-
-- :class:`dummy.DummyRegressor` now has a ``return_std`` option in its
-  ``predict`` method. The returned standard deviations will be zeros.
-
-- Added :class:`multioutput.RegressorChain` for multi-target
-  regression. :issue:`9257` by :user:`Kumar Ashutosh <thechargedneutron>`.
-
-- Added :class:`naive_bayes.ComplementNB`, which implements the Complement
-  Naive Bayes classifier described in Rennie et al. (2003).
-  :issue:`8190` by :user:`Michael A. Alcorn <airalcorn2>`.
 
-- :class:`ensemble.BaggingRegressor` and :class:`ensemble.BaggingClassifier` can now
-  be fit with missing/non-finite values in X and/or multi-output Y to support
-  wrapping pipelines that perform their own imputation.
-  :issue:`9707` by :user:`Jimmy Wan <jimmywan>`.
+:mod:`sklearn.cluster`
+......................
 
-Preprocessing
+- |MajorFeature| A new clustering algorithm: :class:`cluster.OPTICS`: an
+  algoritm related to :class:`cluster.DBSCAN`, that has hyperparameters easier
+  to set and tat scales better, by :user:`Shane <espg>`.
 
-- Expanded :class:`preprocessing.OneHotEncoder` to allow to encode
-  categorical string features as a numeric array using a one-hot (or dummy)
-  encoding scheme, and added :class:`preprocessing.OrdinalEncoder` to
-  convert to ordinal integers.  Those two classes now handle
-  encoding of all feature types (also handles string-valued features) and
-  derives the categories based on the unique values in the features instead of
-  the maximum value in the features. :issue:`9151` and :issue:`10521` by
-  :user:`Vighnesh Birodkar <vighneshbirodkar>` and `Joris Van den Bossche`_.
+- |MajorFeature| :class:`cluster.AgglomerativeClustering` now supports Single
+  Linkage clustering via ``linkage='single'``. :issue:`9372` by :user:`Leland
+  McInnes <lmcinnes>` and :user:`Steve Astels <sastels>`.
 
-- Added :class:`preprocessing.KBinsDiscretizer` for turning
-  continuous features into categorical or one-hot encoded
-  features. :issue:`7668`, :issue:`9647`, :issue:`10195`,
-  :issue:`10192`, :issue:`11272`, :issue:`11467` and :issue:`11505`.
-  by :user:`Henry Lin <hlin117>`, `Hanmin Qin`_,
-  `Tom Dupre la Tour`_ and :user:`Giovanni Giuseppe Costa <ggc87>`.
+- |Feature| :class:`cluster.KMeans` and :class:`cluster.MiniBatchKMeans` now support
+  sample weights via new parameter ``sample_weight`` in ``fit`` function.
+  :issue:`10933` by :user:`Johannes Hansen <jnhansen>`.
 
-- Added :class:`compose.ColumnTransformer`, which allows to apply
-  different transformers to different columns of arrays or pandas
-  DataFrames. :issue:`9012` by `Andreas Müller`_ and `Joris Van den Bossche`_,
-  and :issue:`11315` by :user:`Thomas Fan <thomasjpfan>`.
+- |Efficiency| :class:`cluster.KMeans`, :class:`cluster.MiniBatchKMeans` and
+  :func:`cluster.k_means` passed with ``algorithm='full'`` now enforces
+  row-major ordering, improving runtime.
+  :issue:`10471` by :user:`Gaurav Dhingra <gxyd>`.
 
-- Added :class:`preprocessing.PowerTransformer`, which implements the
-  Yeo-Johnson and Box-Cox power transformations. Power transformations try to
-  find a set of feature-wise parametric transformations to approximately map
-  data to a Gaussian distribution centered at zero and with unit variance.
-  This is useful as a variance-stabilizing transformation in situations where
-  normality and homoscedasticity are desirable.
-  :issue:`10210` by :user:`Eric Chang <ericchang00>` and
-  :user:`Maniteja Nandana <maniteja123>`, and :issue:`11520` by :user:`Nicolas
-  Hug <nicolashug>`.
-
-- Added the :class:`compose.TransformedTargetRegressor` which transforms
-  the target y before fitting a regression model. The predictions are mapped
-  back to the original space via an inverse transform. :issue:`9041` by
-  `Andreas Müller`_ and :user:`Guillaume Lemaitre <glemaitre>`.
-
-- Added :class:`MissingIndicator` which generates a binary indicator for
-  missing values. :issue:`8075` by :user:`Maniteja Nandana <maniteja123>` and
-  :user:`Guillaume Lemaitre <glemaitre>`.
+- |Efficiency| :class:`cluster.DBSCAN` now is parallelized according to ``n_jobs``
+  regardless of ``algorithm``.
+  :issue:`8003` by :user:`Joël Billaud <recamshak>`.
 
-- :class:`linear_model.SGDClassifier`, :class:`linear_model.SGDRegressor`,
-  :class:`linear_model.PassiveAggressiveClassifier`,
-  :class:`linear_model.PassiveAggressiveRegressor` and
-  :class:`linear_model.Perceptron` now expose ``early_stopping``,
-  ``validation_fraction`` and ``n_iter_no_change`` parameters, to stop
-  optimization monitoring the score on a validation set. A new learning rate
-  ``"adaptive"`` strategy divides the learning rate by 5 each time
-  ``n_iter_no_change`` consecutive epochs fail to improve the model.
-  :issue:`9043` by `Tom Dupre la Tour`_.
+- |Enhancement| :class:`cluster.KMeans` now gives a warning, if the number of
+  distinct clusters found is smaller than ``n_clusters``. This may occur when
+  the number of distinct points in the data set is actually smaller than the
+  number of cluster one is looking for.
+  :issue:`10059` by :user:`Christian Braune <christianbraune79>`.
 
-Model evaluation
+- |Fix| Fixed a bug where the ``fit`` method of
+  :class:`cluster.AffinityPropagation` stored cluster
+  centers as 3d array instead of 2d array in case of non-convergence. For the
+  same class, fixed undefined and arbitrary behavior in case of training data
+  where all samples had equal similarity.
+  :issue:`9612`. By :user:`Jonatan Samoocha <jsamoocha>`.
 
-- Added the :func:`metrics.davies_bouldin_score` metric for unsupervised
-  evaluation of clustering models. :issue:`10827` by :user:`Luis Osa <logc>`.
+- |Fix| Fixed a bug in :func:`cluster.spectral_clustering` where the normalization of
+  the spectrum was using a division instead of a multiplication. :issue:`8129`
+  by :user:`Jan Margeta <jmargeta>`, :user:`Guillaume Lemaitre <glemaitre>`,
+  and :user:`Devansh D. <devanshdalal>`.
 
-- Added the :func:`metrics.balanced_accuracy_score` metric and a corresponding
-  ``'balanced_accuracy'`` scorer for binary and multiclass classification.
-  :issue:`8066` by :user:`xyguo` and :user:`Aman Dalmia <dalmia>`, and
-  :issue:`10587` by `Joel Nothman`_.
+- |Fix| Fixed a bug in :func:`cluster.k_means_elkan` where the returned
+  `iteration` was 1 less than the correct value. Also added the missing
+  `n_iter_` attribute in the docstring of :class:`cluster.KMeans`.
+  :issue:`11353` by :user:`Jeremie du Boisberranger <jeremiedbb>`.
 
-Decomposition, manifold learning and clustering
+- |API| Deprecate ``pooling_func`` unused parameter in
+  :class:`cluster.AgglomerativeClustering`.
+  :issue:`9875` by :user:`Kumar Ashutosh <thechargedneutron>`.
 
-- A new clustering algorithm: :class:`cluster.OPTICS`: an algoritm
-  related to :class:`cluster.DBSCAN`, that has hyperparameters easier to
-  set and tat scales better, by :user:`Shane <espg>`.
 
-- :class:`cluster.AgglomerativeClustering` now supports Single Linkage
-  clustering via ``linkage='single'``. :issue:`9372` by
-  :user:`Leland McInnes <lmcinnes>` and :user:`Steve Astels <sastels>`.
+:mod:`sklearn.compose`
+......................
 
-- :class:`cluster.KMeans` and :class:`cluster.MiniBatchKMeans` now support
-  sample weights via new parameter ``sample_weight`` in ``fit`` function.
-  :issue:`10933` by :user:`Johannes Hansen <jnhansen>`.
+- New module.
 
-- :mod:`dict_learning` functions and models now support positivity constraints.
-  This applies to the dictionary and sparse code.
-  :issue:`6374` by :user:`John Kirkham <jakirkham>`.
+- |MajorFeature| Added :class:`compose.ColumnTransformer`, which allows to
+  apply different transformers to different columns of arrays or pandas
+  DataFrames. :issue:`9012` by `Andreas Müller`_ and `Joris Van den Bossche`_,
+  and :issue:`11315` by :user:`Thomas Fan <thomasjpfan>`.
 
-- :class:`decomposition.SparsePCA` now exposes ``normalize_components``. When
-  set to True, the train and test data are centered with the train mean 
-  repsectively during the fit phase and the transform phase. This fixes the
-  behavior of SparsePCA. When set to False, which is the default, the previous
-  abnormal behaviour still holds. The False value is for backward
-  compatibility and should not be used.
-  :issue:`11585` by :user:`Ivan Panico <FollowKenny>`.
+- |MajorFeature| Added the :class:`compose.TransformedTargetRegressor` which
+  transforms the target y before fitting a regression model. The predictions
+  are mapped back to the original space via an inverse transform. :issue:`9041`
+  by `Andreas Müller`_ and :user:`Guillaume Lemaitre <glemaitre>`.
 
-Metrics
 
-- Partial AUC is available via ``max_fpr`` parameter in
-  :func:`metrics.roc_auc_score`. :issue:`3273` by
-  :user:`Alexander Niederbühl <Alexander-N>`.
 
-- Added control over the normalization in
-  :func:`metrics.normalized_mutual_information_score` and
-  :func:`metrics.adjusted_mutual_information_score` via the ``average_method``
-  parameter. In version 0.22, the default normalizer for each will become
-  the *arithmetic* mean of the entropies of each clustering. :issue:`11124` by
-  :user:`Arya McCarthy <aryamccarthy>`.
-- Added ``output_dict`` parameter in :func:`metrics.classification_report`
-  to return classification statistics as dictionary.
-  :issue:`11160` by :user:`Dan Barkhorn <danielbarkhorn>`.
+:mod:`sklearn.covariance`
+.........................
 
-Misc
+- |Efficiency| Runtime improvements to :class:`covariance.GraphicalLasso`.
+  :issue:`9858` by :user:`Steven Brown <stevendbrown>`.
 
-- A new configuration parameter, ``working_memory`` was added to control memory
-  consumption limits in chunked operations, such as the new
-  :func:`metrics.pairwise_distances_chunked`.  See :ref:`working_memory`.
-  :issue:`10280` by `Joel Nothman`_ and :user:`Aman Dalmia <dalmia>`.
+- |API| The :func:`covariance.graph_lasso`,
+  :class:`covariance.GraphLasso` and :class:`covariance.GraphLassoCV` have been
+  renamed to :func:`covariance.graphical_lasso`,
+  :class:`covariance.GraphicalLasso` and :class:`covariance.GraphicalLassoCV`
+  respectively and will be removed in version 0.22.
+  :issue:`9993` by :user:`Artiem Krinitsyn <artiemq>`
 
-- An environment variable to use the site joblib instead of the vendored
-  one was added (:ref:`environment_variable`). The main API of joblib is now
-  exposed in :mod:`sklearn.utils`.
-  :issue:`11166`by `Gael Varoquaux`_
 
-Enhancements
-............
+:mod:`sklearn.datasets`
+.......................
 
-Classifiers and regressors
+- |MajorFeature| Added :func:`datasets.fetch_openml` to fetch datasets from
+  `OpenML <http://openml.org>`. OpenML is a free, open data sharing platform
+  and will be used instead of mldata as it provides better service availability.
+  :issue:`9908` by `Andreas Müller`_ and :user:`Jan N. van Rijn <janvanrijn>`.
 
-- In :class:`gaussian_process.GaussianProcessRegressor`, method ``predict``
-  is faster when using ``return_std=True`` in particular more when called
-  several times in a row. :issue:`9234` by :user:`andrewww <andrewww>`
-  and :user:`Minghui Liu <minghui-liu>`.
+- |Feature| In :func:`datasets.make_blobs`, one can now pass a list to the
+  `n_samples` parameter to indicate the number of samples to generate per
+  cluster. :issue:`8617` by :user:`Maskani Filali Mohamed <maskani-moh>` and
+  :user:`Konstantinos Katrioplas <kkatrio>`.
 
-- Add `named_estimators_` parameter in
-  :class:`ensemble.VotingClassifier` to access fitted
-  estimators. :issue:`9157` by :user:`Herilalaina Rakotoarison <herilalaina>`.
+- |Feature| Add ``filename`` attribute to :mod:`datasets` that have a CSV file.
+  :issue:`9101` by :user:`alex-33 <alex-33>`
+  and :user:`Maskani Filali Mohamed <maskani-moh>`.
 
-- Add `var_smoothing` parameter in
-  :class:`naive_bayes.GaussianNB` to give a precise control over
-  variances calculation. :issue:`9681` by :user:`Dmitry Mottl <Mottl>`.
+- |Feature| ``return_X_y`` parameter has been added to several dataset loaders.
+  :issue:`10774` by :user:`Chris Catalfo <ccatalfo>`.
 
-- Add `n_iter_no_change` parameter in
-  :class:`neural_network.BaseMultilayerPerceptron`,
-  :class:`neural_network.MLPRegressor`, and
-  :class:`neural_network.MLPClassifier` to give control over
-  maximum number of epochs to not meet ``tol`` improvement.
-  :issue:`9456` by :user:`Nicholas Nadeau <nnadeau>`.
+- |Fix| Fixed a bug in :func:`datasets.load_boston` which had a wrong data
+  point. :issue:`10795` by :user:`Takeshi Yoshizawa <tarcusx>`.
 
-- A parameter ``check_inverse`` was added to
-  :class:`preprocessing.FunctionTransformer` to ensure that ``func`` and
-  ``inverse_func`` are the inverse of each other.
-  :issue:`9399` by :user:`Guillaume Lemaitre <glemaitre>`.
+- |Fix| Fixed a bug in :func:`datasets.load_iris` which had two wrong data points.
+  :issue:`11082` by :user:`Sadhana Srinivasan <rotuna>`
+  and :user:`Hanmin Qin <qinhanmin2014>`.
 
-- Add `sample_weight` parameter to the fit method of
-  :class:`linear_model.BayesianRidge` for weighted linear regression.
-  :issue:`10111` by :user:`Peter St. John <pstjohn>`.
+- |Fix| Fixed a bug in :func:`datasets.fetch_kddcup99`, where data were not
+  properly shuffled. :issue:`9731` by `Nicolas Goix`_.
 
-- :class:`dummy.DummyClassifier` and :class:`dummy.DummyRegressor` now
-  only require X to be an object with finite length or shape.
-  :issue:`9832` by :user:`Vrishank Bhardwaj <vrishank97>`.
+- |Fix| Fixed a bug in :func:`datasets.make_circles`, where no odd number of
+  data points could be generated. :issue:`10045` by :user:`Christian Braune
+  <christianbraune79>`.
 
-- Add `sample_weight` parameter to the fit method of
-  :class:`neighbors.KernelDensity` to enables weighting in kernel density
-  estimation.
-  :issue:`4394` by :user:`Samuel O. Ronsin <samronsin>`.
+- |API| Deprecated :func:`sklearn.datasets.fetch_mldata` to be removed in
+  version 0.22. mldata.org is no longer operational. Until removal it will
+  remain possible to load cached datasets. :issue:`11466` by `Joel Nothman`_.
 
-- :class:`neighbors.RadiusNeighborsRegressor` and
-  :class:`neighbors.RadiusNeighborsClassifier` are now
-  parallelized according to ``n_jobs`` regardless of ``algorithm``.
-  :issue:`8003` by :user:`Joël Billaud <recamshak>`.
+:mod:`sklearn.decomposition`
+............................
 
-- Memory usage improvement for :func:`_class_means` and :func:`_class_cov`
-  in :class:`discriminant_analysis`.
-  :issue:`10898` by :user:`Nanxin Chen <bobchennan>`.`
+- |Feature| :func:`decomposition.dict_learning` functions and models now
+  support positivity constraints. This applies to the dictionary and sparse
+  code. :issue:`6374` by :user:`John Kirkham <jakirkham>`.
 
-- :func:`manifold.t_sne.trustworthiness` accepts metrics other than
-  Euclidean. :issue:`9775` by :user:`William de Vazelhes <wdevazelhes>`.
+- |Feature| |Fix| :class:`decomposition.SparsePCA` now exposes
+  ``normalize_components``. When set to True, the train and test data are
+  centered with the train mean repsectively during the fit phase and the
+  transform phase. This fixes the behavior of SparsePCA. When set to False,
+  which is the default, the previous abnormal behaviour still holds. The False
+  value is for backward compatibility and should not be used. :issue:`11585`
+  by :user:`Ivan Panico <FollowKenny>`.
 
-- :mod:`Nearest neighbors <neighbors>` query methods are now more memory
-  efficient when ``algorithm='brute'``. :issue:`11136` by `Joel Nothman`_
-  and :user:`Aman Dalmia <dalmia>`.
+- |Efficiency| Efficiency improvements in :func:`decomposition.dict_learning`.
+  :issue:`11420` and others by :user:`John Kirkham <jakirkham>`.
 
-Cluster
+- |Fix| Fix for uninformative error in :class:`decomposition.IncrementalPCA`:
+  now an error is raised if the number of components is larger than the
+  chosen batch size. The ``n_components=None`` case was adapted accordingly.
+  :issue:`6452`. By :user:`Wally Gauze <wallygauze>`.
 
-- :class:`cluster.KMeans`, :class:`cluster.MiniBatchKMeans` and
-  :func:`cluster.k_means` passed with ``algorithm='full'`` now enforces
-  row-major ordering, improving runtime.
-  :issue:`10471` by :user:`Gaurav Dhingra <gxyd>`.
+- |Fix| Fixed a bug where the ``partial_fit`` method of
+  :class:`decomposition.IncrementalPCA` used integer division instead of float
+  division on Python 2.
+  :issue:`9492` by :user:`James Bourbeau <jrbourbeau>`.
+
+- |Fix| In :class:`decomposition.PCA` selecting a n_components parameter greater
+  than the number of samples now raises an error. Similarly, the
+  ``n_components=None`` case now selects the minimum of n_samples and
+  n_features.
+  :issue:`8484` by :user:`Wally Gauze <wallygauze>`.
+
+- |Fix| Fixed a bug in :class:`decomposition.PCA` where users will get
+  unexpected error with large datasets when ``n_components='mle'`` on Python 3
+  versions.
+  :issue:`9886` by :user:`Hanmin Qin <qinhanmin2014>`.
 
-- :class:`cluster.DBSCAN` now is parallelized according to ``n_jobs``
-  regardless of ``algorithm``.
-  :issue:`8003` by :user:`Joël Billaud <recamshak>`.
+- |Fix| Fixed an underflow in calculating KL-divergence for
+  :class:`decomposition.NMF` :issue:`10142` by `Tom Dupre la Tour`_.
 
-Datasets
+- |Fix| Fixed a bug in :class:`decomposition.SparseCoder` when running OMP
+  sparse coding in parallel using readonly memory mapped datastructures.
+  :issue:`5956` by :user:`Vighnesh Birodkar <vighneshbirodkar>` and
+  :user:`Olivier Grisel <ogrisel>`.
 
-- In :func:`datasets.make_blobs`, one can now pass a list to the `n_samples`
-  parameter to indicate the number of samples to generate per cluster.
-  :issue:`8617` by :user:`Maskani Filali Mohamed <maskani-moh>`
-  and :user:`Konstantinos Katrioplas <kkatrio>`.
 
-Preprocessing
+:mod:`sklearn.discriminant_analysis`
+....................................
 
-- :class:`preprocessing.PolynomialFeatures` now supports sparse input.
-  :issue:`10452` by :user:`Aman Dalmia <dalmia>` and `Joel Nothman`_.
+- |Efficiency| Memory usage improvement for :func:`_class_means` and
+  :func:`_class_cov` in :mod:`discriminant_analysis`. :issue:`10898` by
+  :user:`Nanxin Chen <bobchennan>`.`
 
-- Enable the call to :meth:`get_feature_names` in unfitted
-  :class:`feature_extraction.text.CountVectorizer` initialized with a
-  vocabulary. :issue:`10908` by :user:`Mohamed Maskani <maskani-moh>`.
 
-- :class:`preprocessing.OneHotEncoder` now supports the
-  :meth:`get_feature_names` method to obtain the transformed feature names.
-  :issue:`10181` by  :user:`Nirvan Anjirbag <Nirvan101>` and
-  `Joris Van den Bossche`_.
+:mod:`sklearn.dummy`
+....................
 
-- The ``transform`` method of :class:`sklearn.preprocessing.MultiLabelBinarizer`
-  now ignores any unknown classes. A warning is raised stating the unknown classes
-  classes found which are ignored.
-  :issue:`10913` by :user:`Rodrigo Agundez <rragundez>`.
+- |Feature| :class:`dummy.DummyRegressor` now has a ``return_std`` option in its
+  ``predict`` method. The returned standard deviations will be zeros.
 
-- NaN values are ignored and handled in the following preprocessing methods:
-  :class:`preprocessing.MaxAbsScaler`,
-  :class:`preprocessing.MinMaxScaler`,
-  :class:`preprocessing.RobustScaler`,
-  :class:`preprocessing.StandardScaler`,
-  :class:`preprocessing.PowerTransformer`,
-  :class:`preprocessing.QuantileTransformer` classes and
-  :func:`preprocessing.maxabs_scale`,
-  :func:`preprocessing.minmax_scale`,
-  :func:`preprocessing.robust_scale`,
-  :func:`preprocessing.scale`,
-  :func:`preprocessing.power_transform`,
-  :func:`preprocessing.quantile_transform` functions respectively addressed in
-  issues :issue:`11011`, :issue:`11005`, :issue:`11308`, :issue:`11206`,
-  :issue:`11306`, and :issue:`10437`.
-  By :user:`Lucija Gregov <LucijaGregov>` and
-  :user:`Guillaume Lemaitre <glemaitre>`.
+- |Feature| :class:`dummy.DummyClassifier` and :class:`dummy.DummyRegressor` now
+  only require X to be an object with finite length or shape. :issue:`9832` by
+  :user:`Vrishank Bhardwaj <vrishank97>`.
 
-- :class:`preprocessing.RobustScaler` and :func:`preprocessing.robust_scale`
-  can be fitted using sparse matrices.
-  :issue:`11308` by :user:`Guillaume Lemaitre <glemaitre>`.
 
-Model evaluation and meta-estimators
+:mod:`sklearn.ensemble`
+.......................
 
-- A scorer based on :func:`metrics.brier_score_loss` is also available.
-  :issue:`9521` by :user:`Hanmin Qin <qinhanmin2014>`.
+- |Feature| :class:`ensemble.BaggingRegressor` and
+  :class:`ensemble.BaggingClassifier` can now be fit with missing/non-finite
+  values in X and/or multi-output Y to support wrapping pipelines that perform
+  their own imputation. :issue:`9707` by :user:`Jimmy Wan <jimmywan>`.
 
-- The default of ``iid`` parameter of :class:`model_selection.GridSearchCV`
-  and :class:`model_selection.RandomizedSearchCV` will change from ``True`` to
-  ``False`` in version 0.22 to correspond to the standard definition of
-  cross-validation, and the parameter will be removed in version 0.24
-  altogether. This parameter is of greatest practical significance where the
-  sizes of different test sets in cross-validation were very unequal, i.e. in
-  group-based CV strategies. :issue:`9085` by :user:`Laurent Direr <ldirer>`
-  and `Andreas Müller`_.
+- |Feature| :class:`ensemble.GradientBoostingClassifier` and
+  :class:`ensemble.GradientBoostingRegressor` now support early stopping
+  via ``n_iter_no_change``, ``validation_fraction`` and ``tol``. :issue:`7071`
+  by `Raghav RV`_
 
-- The ``predict`` method of :class:`pipeline.Pipeline` now passes keyword
-  arguments on to the pipeline's last estimator, enabling the use of parameters
-  such as ``return_std`` in a pipeline with caution.
-  :issue:`9304` by :user:`Breno Freitas <brenolf>`.
+- |Feature| Add `named_estimators_` parameter in
+  :class:`ensemble.VotingClassifier` to access fitted estimators.
+  :issue:`9157` by :user:`Herilalaina Rakotoarison <herilalaina>`.
 
-- Add `return_estimator` parameter in :func:`model_selection.cross_validate` to
-  return estimators fitted on each split.
-  :issue:`9686` by :user:`Aurélien Bellet <bellet>`.
+- |Fix| Fixed a bug when fitting :class:`ensemble.GradientBoostingClassifier` or
+  :class:`ensemble.GradientBoostingRegressor` with ``warm_start=True`` which
+  previously raised a segmentation fault due to a non-conversion of CSC matrix
+  into CSR format expected by ``decision_function``. Similarly, Fortran-ordered
+  arrays are converted to C-ordered arrays in the dense case. :issue:`9991` by
+  :user:`Guillaume Lemaitre <glemaitre>`.
 
-- New ``refit_time_`` attribute will be stored in
-  :class:`model_selection.GridSearchCV` and
-  :class:`model_selection.RandomizedSearchCV` if ``refit`` is set to ``True``.
-  This will allow measuring the complete time it takes to perform
-  hyperparameter optimization and refitting the best model on the whole
-  dataset. :issue:`11310` by :user:`Matthias Feurer <mfeurer>`.
+- |Fix| Fixed a bug in :class:`ensemble.GradientBoostingRegressor`
+  and :class:`ensemble.GradientBoostingClassifier` to have
+  feature importances summed and then normalized, rather than normalizing on a
+  per-tree basis. The previous behavior over-weighted the Gini importance of
+  features that appear in later stages. This issue only affected feature
+  importances. :issue:`11176` by :user:`Gil Forsyth <gforsyth>`.
 
-- Expose `error_score` parameter in :func:`model_selection.cross_validate`,
-  :func:`model_selection.cross_val_score`,
-  :func:`model_selection.learning_curve` and
-  :func:`model_selection.validation_curve` to control the behavior triggered
-  when an error occurs in :func:`model_selection._fit_and_score`.
-  :issue:`11576` by :user:`Samuel O. Ronsin <samronsin>`.
+- |API| The default value of the ``n_estimators`` parameter of
+  :class:`ensemble.RandomForestClassifier`, :class:`ensemble.RandomForestRegressor`,
+  :class:`ensemble.ExtraTreesClassifier`, :class:`ensemble.ExtraTreesRegressor`,
+  and :class:`ensemble.RandomTreesEmbedding` will change from 10 in version 0.20
+  to 100 in 0.22. A FutureWarning is raised when the default value is used.
+  :issue:`11542` by :user:`Anna Ayzenshtat <annaayzenshtat>`.
 
-Decomposition and manifold learning
+- |API| Classes derived from :class:`ensemble.BaseBagging`. The attribute
+  ``estimators_samples_`` will return a list of arrays containing the indices
+  selected for each bootstrap instead of a list of arrays containing the mask
+  of the samples selected for each bootstrap. Indices allows to repeat samples
+  while mask does not allow this functionality.
+  :issue:`9524` by :user:`Guillaume Lemaitre <glemaitre>`.
 
-- Speed improvements for both 'exact' and 'barnes_hut' methods in
-  :class:`manifold.TSNE`. :issue:`10593` and :issue:`10610` by
-  `Tom Dupre la Tour`_.
+- |API| The parameters ``min_samples_leaf`` and ``min_weight_fraction_leaf`` in
+  tree-based ensembles are deprecated and will be removed (fixed to 1 and 0
+  respectively) in version 0.22.  These parameters were not effective for
+  regularization and at worst would produce bad splits.  :issue:`10773` by
+  :user:`Bob Chen <lasagnaman>` and `Joel Nothman`_.
 
-- Support sparse input in :meth:`manifold.Isomap.fit`. :issue:`8554` by
-  :user:`Leland McInnes <lmcinnes>`.
+- |Fix| :class:`ensemble.BaseBagging` where one could not deterministically
+  reproduce ``fit`` result using the object attributes when ``random_state``
+  is set. :issue:`9723` by :user:`Guillaume Lemaitre <glemaitre>`.
 
-Metrics
 
-- :func:`metrics.roc_auc_score` now supports binary ``y_true`` other than
-  ``{0, 1}`` or ``{-1, 1}``.
-  :issue:`9828` by :user:`Hanmin Qin <qinhanmin2014>`.
+:mod:`sklearn.feature_extraction`
+.................................
 
-- :func:`metrics.label_ranking_average_precision_score` now supports vector
-  ``sample_weight``.
-  :issue:`10845` by :user:`Jose Perez-Parras Toledano <jopepato>`.
+- |Feature| Enable the call to :term:`get_feature_names` in unfitted
+  :class:`feature_extraction.text.CountVectorizer` initialized with a
+  vocabulary. :issue:`10908` by :user:`Mohamed Maskani <maskani-moh>`.
 
-- Add ``dense_output`` parameter to :func:`metrics.pairwise.linear_kernel`.
-  When False and both inputs are sparse, will return a sparse matrix.
-  :issue:`10999` by :user:`Taylor G Smith <tgsmith61591>`.
+- |Enhancement| ``idf_`` can now be set on a
+  :class:`feature_extraction.text.TfidfTransformer`.
+  :issue:`10899` by :user:`Sergey Melderis <serega>`.
 
-- :func:`metrics.cluster.silhouette_score` and
-  :func:`metrics.cluster.silhouette_samples` are more memory efficient and run
-  faster. This avoids some reported freezes and MemoryErrors.
-  :issue:`11135` by `Joel Nothman`_.
+- |Fix| Fixed a bug in :func:`feature_extraction.image.extract_patches_2d` which
+  would throw an exception if ``max_patches`` was greater than or equal to the
+  number of all possible patches rather than simply returning the number of
+  possible patches. :issue:`10101` by :user:`Varun Agrawal <varunagrawal>`
 
-- :func:`metrics.average_precision_score` now supports binary ``y_true``
-  other than ``{0, 1}`` or ``{-1, 1}`` through ``pos_label`` parameter.
-  :issue:`9980` by :user:`Hanmin Qin <qinhanmin2014>`.
+- |Fix| Fixed a bug in :class:`feature_extraction.text.CountVectorizer`,
+  :class:`feature_extraction.text.TfidfVectorizer`,
+  :class:`feature_extraction.text.HashingVectorizer` to support 64 bit sparse
+  array indexing necessary to process large datasets with more than 2·10⁹ tokens
+  (words or n-grams). :issue:`9147` by :user:`Claes-Fredrik Mannby <mannby>`
+  and `Roman Yurchak`_.
 
-Linear, kernelized and related models
+- |Fix| Fixed bug in :class:`feature_extraction.text.TfidfVectorizer` which
+  was ignoring the parameter ``dtype``. In addition,
+  :class:`feature_extraction.text.TfidfTransformer` will preserve ``dtype``
+  for floating and raise a warning if ``dtype`` requested is integer.
+  :issue:`10441` by :user:`Mayur Kulkarni <maykulkarni>` and
+  :user:`Guillaume Lemaitre <glemaitre>`.
 
-- Deprecate ``random_state`` parameter in :class:`svm.OneClassSVM` as the
-  underlying implementation is not random.
-  :issue:`9497` by :user:`Albert Thomas <albertcthomas>`.
 
-Preprocessing and feature selection
+:mod:`sklearn.feature_selection`
+................................
 
-- Added select K best features functionality to
+- |Feature| Added select K best features functionality to
   :class:`feature_selection.SelectFromModel`.
   :issue:`6689` by :user:`Nihar Sheth <nsheth12>` and
   :user:`Quazi Rahman <qmaruf>`.
 
-Decomposition, manifold learning and clustering
+- |Feature| Added ``min_features_to_select`` parameter to
+  :class:`feature_selection.RFECV` to bound evaluated features counts.
+  :issue:`11293` by :user:`Brent Yi <brentyi>`.
 
-- Deprecate ``precomputed`` parameter in function
-  :func:`manifold.t_sne.trustworthiness`. Instead, the new parameter
-  ``metric`` should be used with any compatible metric including
-  'precomputed', in which case the input matrix ``X`` should be a matrix of
-  pairwise distances or squared distances. :issue:`9775` by
-  :user:`William de Vazelhes <wdevazelhes>`.
+- |Feature| :class:`feature_selection.RFECV`'s fit method now supports
+  :term:`groups`.  :issue:`9656` by :user:`Adam Greenhall <adamgreenhall>`.
 
-Utils
+- |Fix| Fixed computation of ``n_features_to_compute`` for edge case with tied
+  CV scores in :class:`feature_selection.RFECV`.
+  :issue:`9222` by :user:`Nick Hoh <nickypie>`.
 
-- Avoid copying the data in :func:`utils.check_array` when the input data is a
-  memmap (and ``copy=False``). :issue:`10663` by :user:`Arthur Mensch
-  <arthurmensch>` and :user:`Loïc Estève <lesteve>`.
+:mod:`sklearn.gaussian_process`
+...............................
 
-Miscellaneous
+- |Efficiency| In :class:`gaussian_process.GaussianProcessRegressor`, method
+  ``predict`` is faster when using ``return_std=True`` in particular more when
+  called several times in a row. :issue:`9234` by :user:`andrewww <andrewww>`
+  and :user:`Minghui Liu <minghui-liu>`.
 
-- Add ``filename`` attribute to datasets that have a CSV file.
-  :issue:`9101` by :user:`alex-33 <alex-33>`
-  and :user:`Maskani Filali Mohamed <maskani-moh>`.
 
-- Add almost complete PyPy 3 support. Known unsupported functionalities are
-  :func:`datasets.load_svmlight_file`, :class:`feature_extraction.FeatureHasher` and
-  :class:`feature_extraction.text.HashingVectorizer`.  For running on PyPy, PyPy3-v5.10+,
-  Numpy 1.14.0+, and scipy 1.1.0+ are required.
-  :issue:`11010` by :user:`Ronan Lamy <rlamy>` and `Roman Yurchak`_.
+:mod:`sklearn.impute`
+.....................
+
+- New module, adopting ``preprocessing.Imputer`` as
+  :class:`impute.SimpleImputer` with minor changes (see under preprocessing
+  below).
 
-Bug fixes
-.........
+- |MajorFeature| Added :class:`impute.MissingIndicator` which generates a
+  binary indicator for missing values. :issue:`8075` by :user:`Maniteja Nandana
+  <maniteja123>` and :user:`Guillaume Lemaitre <glemaitre>`.
 
-Classifiers and regressors
+- |Feature| The :class:`impute.SimpleImputer` has a new strategy,
+  ``'constant'``, to complete missing values with a fixed one, given by the
+  ``fill_value`` parameter. This strategy supports numeric and non-numeric
+  data, and so does the ``'most_frequent'`` strategy now. :issue:`11211` by
+  :user:`Jeremie du Boisberranger <jeremiedbb>`.
+
+
+:mod:`sklearn.isotonic`
+.......................
 
-- Fixed a bug in :class:`isotonic.IsotonicRegression` which incorrectly
+- |Fix| Fixed a bug in :class:`isotonic.IsotonicRegression` which incorrectly
   combined weights when fitting a model to data involving points with
   identical X values.
-  :issue:`9432` by :user:`Dallas Card <dallascard>`
+  :issue:`9484` by :user:`Dallas Card <dallascard>`
 
-- Fixed a bug in :class:`neural_network.BaseMultilayerPerceptron`,
-  :class:`neural_network.MLPRegressor`, and
-  :class:`neural_network.MLPClassifier` with new ``n_iter_no_change``
-  parameter now at 10 from previously hardcoded 2.
-  :issue:`9456` by :user:`Nicholas Nadeau <nnadeau>`.
 
-- Fixed a bug in :class:`neural_network.MLPRegressor` where fitting
-  quit unexpectedly early due to local minima or fluctuations.
-  :issue:`9456` by :user:`Nicholas Nadeau <nnadeau>`
+:mod:`sklearn.linear_model`
+...........................
 
-- Fixed a bug in :class:`naive_bayes.GaussianNB` which incorrectly raised
-  error for prior list which summed to 1.
-  :issue:`10005` by :user:`Gaurav Dhingra <gxyd>`.
+- |Feature| :class:`linear_model.SGDClassifier`,
+  :class:`linear_model.SGDRegressor`,
+  :class:`linear_model.PassiveAggressiveClassifier`,
+  :class:`linear_model.PassiveAggressiveRegressor` and
+  :class:`linear_model.Perceptron` now expose ``early_stopping``,
+  ``validation_fraction`` and ``n_iter_no_change`` parameters, to stop
+  optimization monitoring the score on a validation set. A new learning rate
+  ``"adaptive"`` strategy divides the learning rate by 5 each time
+  ``n_iter_no_change`` consecutive epochs fail to improve the model.
+  :issue:`9043` by `Tom Dupre la Tour`_.
+
+- |Feature| Add `sample_weight` parameter to the fit method of
+  :class:`linear_model.BayesianRidge` for weighted linear regression.
+  :issue:`10112` by :user:`Peter St. John <pstjohn>`.
 
-- Fixed a bug in :class:`linear_model.LogisticRegression` where when using the
-  parameter ``multi_class='multinomial'``, the ``predict_proba`` method was
+- |Fix| Fixed a bug in :func:`logistic.logistic_regression_path` to ensure
+  that the returned coefficients are correct when ``multiclass='multinomial'``.
+  Previously, some of the coefficients would override each other, leading to
+  incorrect results in :class:`linear_model.LogisticRegressionCV`.
+  :issue:`11724` by :user:`Nicolas Hug <NicolasHug>`.
+
+- |Fix| Fixed a bug in :class:`linear_model.LogisticRegression` where when using
+  the parameter ``multi_class='multinomial'``, the ``predict_proba`` method was
   returning incorrect probabilities in the case of binary outcomes.
   :issue:`9939` by :user:`Roger Westover <rwolst>`.
 
-- Fixed a bug in :class:`linear_model.LogisticRegressionCV` where the
+- |Fix| Fixed a bug in :class:`linear_model.LogisticRegressionCV` where the
   ``score`` method always computes accuracy, not the metric given by
   the ``scoring`` parameter.
   :issue:`10998` by :user:`Thomas Fan <thomasjpfan>`.
 
-- Fixed a bug in :class:`linear_model.LogisticRegressionCV` where the 'ovr'
-  strategy was always used to compute cross-validation scores in the
+- |Fix| Fixed a bug in :class:`linear_model.LogisticRegressionCV` where the
+  'ovr' strategy was always used to compute cross-validation scores in the
   multiclass setting, even if 'multinomial' was set.
   :issue:`8720` by :user:`William de Vazelhes <wdevazelhes>`.
 
-- Fixed a bug in :class:`linear_model.OrthogonalMatchingPursuit` that was
+- |Fix| Fixed a bug in :class:`linear_model.OrthogonalMatchingPursuit` that was
   broken when setting ``normalize=False``.
   :issue:`10071` by `Alexandre Gramfort`_.
 
-- Fixed a bug in :class:`linear_model.ARDRegression` which caused incorrectly
-  updated estimates for the standard deviation and the coefficients.
-  :issue:`10153` by :user:`Jörg Döpfert <jdoepfert>`.
-
-- Fixed a bug when fitting :class:`ensemble.GradientBoostingClassifier` or
-  :class:`ensemble.GradientBoostingRegressor` with ``warm_start=True`` which
-  previously raised a segmentation fault due to a non-conversion of CSC matrix
-  into CSR format expected by ``decision_function``. Similarly, Fortran-ordered
-  arrays are converted to C-ordered arrays in the dense case. :issue:`9991` by
-  :user:`Guillaume Lemaitre <glemaitre>`.
+- |Fix| Fixed a bug in :class:`linear_model.ARDRegression` which caused
+  incorrectly updated estimates for the standard deviation and the
+  coefficients. :issue:`10153` by :user:`Jörg Döpfert <jdoepfert>`.
 
-- Fixed a bug in :class:`neighbors.NearestNeighbors` where fitting a
-  NearestNeighbors model fails when a) the distance metric used is a
-  callable and b) the input to the NearestNeighbors model is sparse.
-  :issue:`9579` by :user:`Thomas Kober <tttthomasssss>`.
+- |Fix| Fixed a bug in :class:`linear_model.ARDRegression` and
+  :class:`linear_model.BayesianRidge` which caused NaN predictions when fitted
+  with a constant target.
+  :issue:`10095` by :user:`Jörg Döpfert <jdoepfert>`.
 
-- Fixed a bug in :class:`linear_model.RidgeClassifierCV` where
+- |Fix| Fixed a bug in :class:`linear_model.RidgeClassifierCV` where
   the parameter ``store_cv_values`` was not implemented though
   it was documented in ``cv_values`` as a way to set up the storage
   of cross-validation values for different alphas. :issue:`10297` by
   :user:`Mabel Villalba-Jiménez <mabelvj>`.
 
-- Fixed a bug in :class:`naive_bayes.MultinomialNB` which did not accept vector
-  valued pseudocounts (alpha).
-  :issue:`10346` by :user:`Tobias Madsen <TobiasMadsen>`
-
-- Fixed a bug in :class:`svm.SVC` where when the argument ``kernel`` is
-  unicode in Python2, the ``predict_proba`` method was raising an
-  unexpected TypeError given dense inputs.
-  :issue:`10412` by :user:`Jiongyan Zhang <qmick>`.
-
-- Fixed a bug in :class:`tree.BaseDecisionTree` with `splitter="best"`
-  where split threshold could become infinite when values in X were
-  near infinite. :issue:`10536` by :user:`Jonathan Ohayon <Johayon>`.
+- |Fix| Fixed a bug in :class:`linear_model.ElasticNet` which caused the input
+  to be overridden when using parameter ``copy_X=True`` and
+  ``check_input=False``. :issue:`10581` by :user:`Yacine Mazari <ymazari>`.
 
-- Fixed a bug in :class:`linear_model.ElasticNet` which caused the input to be
-  overridden when using parameter ``copy_X=True`` and ``check_input=False``.
-  :issue:`10581` by :user:`Yacine Mazari <ymazari>`.
-
-- Fixed a bug in :class:`sklearn.linear_model.Lasso`
+- |Fix| Fixed a bug in :class:`sklearn.linear_model.Lasso`
   where the coefficient had wrong shape when ``fit_intercept=False``.
   :issue:`10687` by :user:`Martin Hahn <martin-hahn>`.
 
-- Fixed a bug in :func:`sklearn.linear_model.LogisticRegression` where the
+- |Fix| Fixed a bug in :func:`sklearn.linear_model.LogisticRegression` where the
   multi_class='multinomial' with binary output with warm_start = True
   :issue:`10836` by :user:`Aishwarya Srinivasan <aishgrt1>`.
 
-- Fixed a bug in :class:`linear_model.RidgeCV` where using integer ``alphas``
-  raised an error. :issue:`10393` by :user:`Mabel Villalba-Jiménez <mabelvj>`.
+- |Fix| Fixed a bug in :class:`linear_model.RidgeCV` where using integer
+  ``alphas`` raised an error.
+  :issue:`10397` by :user:`Mabel Villalba-Jiménez <mabelvj>`.
+
+- |Fix| Fixed condition triggering gap computation in
+  :class:`linear_model.Lasso` and :class:`linear_model.ElasticNet` when working
+  with sparse matrices. :issue:`10992` by `Alexandre Gramfort`_.
+
+- |Fix| Fixed a bug in :class:`linear_model.SGDClassifier`,
+  :class:`linear_model.SGDRegressor`,
+  :class:`linear_model.PassiveAggressiveClassifier`,
+  :class:`linear_model.PassiveAggressiveRegressor` and
+  :class:`linear_model.Perceptron`, where the stopping criterion was stopping
+  the algorithm before convergence. A parameter `n_iter_no_change` was added
+  and set by default to 5. Previous behavior is equivalent to setting the
+  parameter to 1. :issue:`9043` by `Tom Dupre la Tour`_.
+
+- |Fix| Fixed a bug where liblinear and libsvm-based estimators would segfault
+  if passed a scipy.sparse matrix with 64-bit indices. They now raise a
+  ValueError.
+  :issue:`11327` by :user:`Karan Dhingra <kdhingra307>` and `Joel Nothman`_.
+
+- |API| The default values of the ``solver`` and ``multi_class`` parameters of
+  :class:`linear_model.LogisticRegression` will change respectively from
+  ``'liblinear'`` and ``'ovr'`` in version 0.20 to ``'lbfgs'`` and
+  ``'auto'`` in version 0.22. A FutureWarning is raised when the default
+  values are used. :issue:`11905` by `Tom Dupre la Tour`_ and `Joel Nothman`_.
+
+- |API| Deprecate ``positive=True`` option in :class:`linear_model.Lars` as
+  the underlying implementation is broken. Use :class:`linear_model.Lasso`
+  instead. :issue:`9837` by `Alexandre Gramfort`_.
+
+- |API| ``n_iter_`` may vary from previous releases in
+  :class:`linear_model.LogisticRegression` with ``solver='lbfgs'`` and
+  :class:`linear_model.HuberRegressor`. For Scipy <= 1.0.0, the optimizer could
+  perform more than the requested maximum number of iterations. Now both
+  estimators will report at most ``max_iter`` iterations even if more were
+  performed. :issue:`10723` by `Joel Nothman`_.
+
+
+:mod:`sklearn.manifold`
+.......................
+
+- |Efficiency| Speed improvements for both 'exact' and 'barnes_hut' methods in
+  :class:`manifold.TSNE`. :issue:`10593` and :issue:`10610` by
+  `Tom Dupre la Tour`_.
+
+- |Feature| Support sparse input in :meth:`manifold.Isomap.fit`.
+  :issue:`8554` by :user:`Leland McInnes <lmcinnes>`.
+
+- |Feature| :func:`manifold.t_sne.trustworthiness` accepts metrics other than
+  Euclidean. :issue:`9775` by :user:`William de Vazelhes <wdevazelhes>`.
+
+- |Fix| Fixed a bug in :func:`manifold.spectral_embedding` where the
+  normalization of the spectrum was using a division instead of a
+  multiplication. :issue:`8129` by :user:`Jan Margeta <jmargeta>`,
+  :user:`Guillaume Lemaitre <glemaitre>`, and :user:`Devansh D.
+  <devanshdalal>`.
+
+- |API| |Feature| Deprecate ``precomputed`` parameter in function
+  :func:`manifold.t_sne.trustworthiness`. Instead, the new parameter ``metric``
+  should be used with any compatible metric including 'precomputed', in which
+  case the input matrix ``X`` should be a matrix of pairwise distances or
+  squared distances. :issue:`9775` by :user:`William de Vazelhes
+  <wdevazelhes>`.
+
+- |API| Deprecate ``precomputed`` parameter in function
+  :func:`manifold.t_sne.trustworthiness`. Instead, the new parameter
+  ``metric`` should be used with any compatible metric including
+  'precomputed', in which case the input matrix ``X`` should be a matrix of
+  pairwise distances or squared distances. :issue:`9775` by
+  :user:`William de Vazelhes <wdevazelhes>`.
+
+
+:mod:`sklearn.metrics`
+......................
+
+- |MajorFeature| Added the :func:`metrics.davies_bouldin_score` metric for
+  evaluation of clustering models without a ground truth. :issue:`10827` by
+  :user:`Luis Osa <logc>`.
+
+- |MajorFeature| Added the :func:`metrics.balanced_accuracy_score` metric and
+  a corresponding ``'balanced_accuracy'`` scorer for binary and multiclass
+  classification. :issue:`8066` by :user:`xyguo` and :user:`Aman Dalmia
+  <dalmia>`, and :issue:`10587` by `Joel Nothman`_.
+
+- |Feature| Partial AUC is available via ``max_fpr`` parameter in
+  :func:`metrics.roc_auc_score`. :issue:`3840` by
+  :user:`Alexander Niederbühl <Alexander-N>`.
+
+- |Feature| A scorer based on :func:`metrics.brier_score_loss` is also
+  available. :issue:`9521` by :user:`Hanmin Qin <qinhanmin2014>`.
+
+- |Feature| Added control over the normalization in
+  :func:`metrics.normalized_mutual_info_score` and
+  :func:`metrics.adjusted_mutual_info_score` via the ``average_method``
+  parameter. In version 0.22, the default normalizer for each will become
+  the *arithmetic* mean of the entropies of each clustering. :issue:`11124` by
+  :user:`Arya McCarthy <aryamccarthy>`.
+
+- |Feature| Added ``output_dict`` parameter in :func:`metrics.classification_report`
+  to return classification statistics as dictionary.
+  :issue:`11160` by :user:`Dan Barkhorn <danielbarkhorn>`.
+
+- |Feature| :func:`metrics.classification_report` now reports all applicable averages on
+  the given data, including micro, macro and weighted average as well as samples
+  average for multilabel data. :issue:`11679` by :user:`Alexander Pacha <apacha>`. 
 
-- Fixed condition triggering gap computation in :class:`linear_model.Lasso`
-  and :class:`linear_model.ElasticNet` when working with sparse matrices.
-  :issue:`10992` by `Alexandre Gramfort`_.
+- |Feature| :func:`metrics.average_precision_score` now supports binary
+  ``y_true`` other than ``{0, 1}`` or ``{-1, 1}`` through ``pos_label``
+  parameter. :issue:`9980` by :user:`Hanmin Qin <qinhanmin2014>`.
 
-- Fixed a bug in :class:`linear_model.SGDClassifier`,
-  :class:`linear_model.SGDRegressor`,
-  :class:`linear_model.PassiveAggressiveClassifier`,
-  :class:`linear_model.PassiveAggressiveRegressor` and
-  :class:`linear_model.Perceptron`, where the stopping criterion was stopping
-  the algorithm before convergence. A parameter `n_iter_no_change` was added
-  and set by default to 5. Previous behavior is equivalent to setting the
-  parameter to 1. :issue:`9043` by `Tom Dupre la Tour`_.
+- |Feature| :func:`metrics.label_ranking_average_precision_score` now supports
+  ``sample_weight``.
+  :issue:`10845` by :user:`Jose Perez-Parras Toledano <jopepato>`.
 
-- Fixed a bug where liblinear and libsvm-based estimators would segfault if
-  passed a scipy.sparse matrix with 64-bit indices. They now raise a
-  ValueError.
-  :issue:`11327` by :user:`Karan Dhingra <kdhingra307>` and `Joel Nothman`_.
+- |Feature| Add ``dense_output`` parameter to :func:`metrics.pairwise.linear_kernel`.
+  When False and both inputs are sparse, will return a sparse matrix.
+  :issue:`10999` by :user:`Taylor G Smith <tgsmith61591>`.
 
-- Fixed a bug in :class:`ensemble.gradient_boosting.GradientBoostingRegressor`
-  and :class:`ensemble.gradient_boosting.GradientBoostingClassifier` to have
-  feature importances summed and then normalized, rather than normalizing on a
-  per-tree basis. The previous behavior over-weighted the Gini importance of
-  features that appear in later stages. This issue only affected feature
-  importances. :issue:`11176` by :user:`Gil Forsyth <gforsyth>`.
+- |Efficiency| :func:`metrics.silhouette_score` and
+  :func:`metrics.silhouette_samples` are more memory efficient and run
+  faster. This avoids some reported freezes and MemoryErrors.
+  :issue:`11135` by `Joel Nothman`_.
 
-- Fixed a bug in :class:`tree.MAE` to ensure sample weights are being used
-  during the calculation of tree MAE impurity. Previous behaviour could
-  cause suboptimal splits to be chosen since the impurity calculation
-  considered all samples to be of equal weight importance.
-  :issue:`11464` by :user:`John Stott <JohnStott>`.
+- |Fix| Fixed a bug in :func:`metrics.precision_recall_fscore_support`
+  when truncated `range(n_labels)` is passed as value for `labels`.
+  :issue:`10377` by :user:`Gaurav Dhingra <gxyd>`.
 
-Decomposition, manifold learning and clustering
+- |Fix| Fixed a bug due to floating point error in
+  :func:`metrics.roc_auc_score` with non-integer sample weights. :issue:`9786`
+  by :user:`Hanmin Qin <qinhanmin2014>`.
 
-- Fix for uninformative error in :class:`decomposition.IncrementalPCA`:
-  now an error is raised if the number of components is larger than the
-  chosen batch size. The ``n_components=None`` case was adapted accordingly.
-  :issue:`6452`. By :user:`Wally Gauze <wallygauze>`.
+- |Fix| Fixed a bug where :func:`metrics.roc_curve` sometimes starts on y-axis
+  instead of (0, 0), which is inconsistent with the document and other
+  implementations. Note that this will not influence the result from
+  :func:`metrics.roc_auc_score` :issue:`10093` by :user:`alexryndin
+  <alexryndin>` and :user:`Hanmin Qin <qinhanmin2014>`.
 
-- Fixed a bug where the ``partial_fit`` method of
-  :class:`decomposition.IncrementalPCA` used integer division instead of float
-  division on Python 2 versions. :issue:`9492` by
-  :user:`James Bourbeau <jrbourbeau>`.
+- |Fix| Fixed a bug to avoid integer overflow. Casted product to 64 bits integer in
+  :func:`metrics.mutual_info_score`.
+  :issue:`9772` by :user:`Kumar Ashutosh <thechargedneutron>`.
 
-- Fixed a bug where the ``fit`` method of
-  :class:`cluster.AffinityPropagation` stored cluster
-  centers as 3d array instead of 2d array in case of non-convergence. For the
-  same class, fixed undefined and arbitrary behavior in case of training data
-  where all samples had equal similarity.
-  :issue:`9612`. By :user:`Jonatan Samoocha <jsamoocha>`.
+- |Fix| Fixed a bug where :func:`metrics.average_precision_score` will sometimes return
+  ``nan`` when ``sample_weight`` contains 0.
+  :issue:`9980` by :user:`Hanmin Qin <qinhanmin2014>`.
 
-- In :class:`decomposition.PCA` selecting a n_components parameter greater than
-  the number of samples now raises an error.
-  Similarly, the ``n_components=None`` case now selects the minimum of
-  n_samples and n_features. :issue:`8484`. By :user:`Wally Gauze <wallygauze>`.
+- |Fix| Fixed a bug in :func:`metrics.fowlkes_mallows_score` to avoid integer
+  overflow. Casted return value of `contingency_matrix` to `int64` and computed
+  product of square roots rather than square root of product.
+  :issue:`9515` by :user:`Alan Liddell <aliddell>` and
+  :user:`Manh Dao <manhdao>`.
 
-- Fixed a bug in :func:`datasets.fetch_kddcup99`, where data were not properly
-  shuffled. :issue:`9731` by `Nicolas Goix`_.
+- |API| Deprecate ``reorder`` parameter in :func:`metrics.auc` as it's no
+  longer required for :func:`metrics.roc_auc_score`. Moreover using
+  ``reorder=True`` can hide bugs due to floating point error in the input.
+  :issue:`9851` by :user:`Hanmin Qin <qinhanmin2014>`.
 
-- Fixed a bug in :class:`decomposition.PCA` where users will get unexpected error
-  with large datasets when ``n_components='mle'`` on Python 3 versions.
-  :issue:`9886` by :user:`Hanmin Qin <qinhanmin2014>`.
+- |API| In :func:`metrics.normalized_mutual_info_score` and
+  :func:`metrics.adjusted_mutual_info_score`, warn that
+  ``average_method`` will have a new default value. In version 0.22, the
+  default normalizer for each will become the *arithmetic* mean of the
+  entropies of each clustering. Currently,
+  :func:`metrics.normalized_mutual_info_score` uses the default of
+  ``average_method='geometric'``, and
+  :func:`metrics.adjusted_mutual_info_score` uses the default of
+  ``average_method='max'`` to match their behaviors in version 0.19.
+  :issue:`11124` by :user:`Arya McCarthy <aryamccarthy>`.
 
-- Fixed a bug when setting parameters on meta-estimator, involving both a
-  wrapped estimator and its parameter. :issue:`9999` by :user:`Marcus Voss
-  <marcus-voss>` and `Joel Nothman`_.
+- |API| The ``batch_size`` parameter to :func:`metrics.pairwise_distances_argmin_min`
+  and :func:`metrics.pairwise_distances_argmin` is deprecated to be removed in
+  v0.22. It no longer has any effect, as batch size is determined by global
+  ``working_memory`` config. See :ref:`working_memory`. :issue:`10280` by `Joel
+  Nothman`_ and :user:`Aman Dalmia <dalmia>`.
 
-- ``k_means`` now gives a warning, if the number of distinct clusters found
-  is smaller than ``n_clusters``. This may occur when the number of distinct
-  points in the data set is actually smaller than the number of cluster one is
-  looking for. :issue:`10059` by :user:`Christian Braune <christianbraune79>`.
 
-- Fixed a bug in :func:`datasets.make_circles`, where no odd number of data
-  points could be generated. :issue:`10037`
-  by :user:`Christian Braune <christianbraune79>`.
+:mod:`sklearn.mixture`
+......................
 
-- Fixed a bug in :func:`cluster.spectral_clustering` where the normalization of
-  the spectrum was using a division instead of a multiplication. :issue:`8129`
-  by :user:`Jan Margeta <jmargeta>`, :user:`Guillaume Lemaitre <glemaitre>`,
-  and :user:`Devansh D. <devanshdalal>`.
+- |Feature| Added function :term:`fit_predict` to :class:`mixture.GaussianMixture`
+  and :class:`mixture.GaussianMixture`, which is essentially equivalent to
+  calling :term:`fit` and :term:`predict`. :issue:`10336` by :user:`Shu Haoran
+  <haoranShu>` and :user:`Andrew Peng <Andrew-peng>`.
 
-- Fixed a bug in :class:`mixture.BaseMixture` where the reported `n_iter_` was
+- |Fix| Fixed a bug in :class:`mixture.BaseMixture` where the reported `n_iter_` was
   missing an iteration. It affected :class:`mixture.GaussianMixture` and
   :class:`mixture.BayesianGaussianMixture`. :issue:`10740` by :user:`Erich
   Schubert <kno10>` and :user:`Guillaume Lemaitre <glemaitre>`.
 
-- Fixed a bug in :class:`mixture.BaseMixture` and its subclasses
+- |Fix| Fixed a bug in :class:`mixture.BaseMixture` and its subclasses
   :class:`mixture.GaussianMixture` and :class:`mixture.BayesianGaussianMixture`
   where the ``lower_bound_`` was not the max lower bound across all
   initializations (when ``n_init > 1``), but just the lower bound of the last
   initialization. :issue:`10869` by :user:`Aurélien Géron <ageron>`.
 
-- Fixed a bug in :class:`decomposition.SparseCoder` when running OMP sparse
-  coding in parallel using readonly memory mapped datastructures. :issue:`5956`
-  by :user:`Vighnesh Birodkar <vighneshbirodkar>` and
-  :user:`Olivier Grisel <ogrisel>`.
 
-- Fixed a bug in :func:`cluster.k_means_elkan` where the returned `iteration`
-  was 1 less than the correct value. Also added the missing `n_iter_` attribute
-  in the docstring of :class:`cluster.KMeans`. :issue:`11353` by
-  :user:`Jeremie du Boisberranger <jeremiedbb>`.
+:mod:`sklearn.model_selection`
+..............................
 
-Metrics
+- |Feature| Add `return_estimator` parameter in
+  :func:`model_selection.cross_validate` to return estimators fitted on each
+  split. :issue:`9686` by :user:`Aurélien Bellet <bellet>`.
 
-- Fixed a bug in :func:`metrics.precision_recall_fscore_support`
-  when truncated `range(n_labels)` is passed as value for `labels`.
-  :issue:`10377` by :user:`Gaurav Dhingra <gxyd>`.
+- |Feature| New ``refit_time_`` attribute will be stored in
+  :class:`model_selection.GridSearchCV` and
+  :class:`model_selection.RandomizedSearchCV` if ``refit`` is set to ``True``.
+  This will allow measuring the complete time it takes to perform
+  hyperparameter optimization and refitting the best model on the whole
+  dataset. :issue:`11310` by :user:`Matthias Feurer <mfeurer>`.
 
-- Fixed a bug due to floating point error in :func:`metrics.roc_auc_score` with
-  non-integer sample weights. :issue:`9786` by :user:`Hanmin Qin <qinhanmin2014>`.
+- |Feature| Expose `error_score` parameter in
+  :func:`model_selection.cross_validate`,
+  :func:`model_selection.cross_val_score`,
+  :func:`model_selection.learning_curve` and
+  :func:`model_selection.validation_curve` to control the behavior triggered
+  when an error occurs in :func:`model_selection._fit_and_score`.
+  :issue:`11576` by :user:`Samuel O. Ronsin <samronsin>`.
 
-- Fixed a bug where :func:`metrics.roc_curve` sometimes starts on y-axis instead
-  of (0, 0), which is inconsistent with the document and other implementations.
-  Note that this will not influence the result from :func:`metrics.roc_auc_score`
-  :issue:`10093` by :user:`alexryndin <alexryndin>`
-  and :user:`Hanmin Qin <qinhanmin2014>`.
+- |Feature| `BaseSearchCV` now has an experimental, private interface to
+  support customized parameter search strategies, through its ``_run_search``
+  method. See the implementations in :class:`model_selection.GridSearchCV` and
+  :class:`model_selection.RandomizedSearchCV` and please provide feedback if
+  you use this. Note that we do not assure the stability of this API beyond
+  version 0.20. :issue:`9599` by `Joel Nothman`_
 
-- Fixed a bug to avoid integer overflow. Casted product to 64 bits integer in
-  :func:`metrics.mutual_info_score`.
-  :issue:`9772` by :user:`Kumar Ashutosh <thechargedneutron>`.
+- |Enhancement| Add improved error message in
+  :func:`model_selection.cross_val_score` when multiple metrics are passed in
+  ``scoring`` keyword. :issue:`11006` by :user:`Ming Li <minggli>`.
 
-- Fixed a bug where :func:`metrics.average_precision_score` will sometimes return
-  ``nan`` when ``sample_weight`` contains 0.
-  :issue:`9980` by :user:`Hanmin Qin <qinhanmin2014>`.
+- |API| The default number of cross-validation folds ``cv`` and the default
+  number of splits ``n_splits`` in the :class:`model_selection.KFold`-like
+  splitters will change from 3 to 5 in 0.22 as 3-fold has a lot of variance.
+  :issue:`11557` by :user:`Alexandre Boucaud <aboucaud>`.
 
-- Fixed a bug in :func:`metrics.fowlkes_mallows_score` to avoid integer
-  overflow. Casted return value of `contingency_matrix` to `int64` and computed
-  product of square roots rather than square root of product.
-  :issue:`9515` by :user:`Alan Liddell <aliddell>` and
-  :user:`Manh Dao <manhdao>`.
+- |API| The default of ``iid`` parameter of :class:`model_selection.GridSearchCV`
+  and :class:`model_selection.RandomizedSearchCV` will change from ``True`` to
+  ``False`` in version 0.22 to correspond to the standard definition of
+  cross-validation, and the parameter will be removed in version 0.24
+  altogether. This parameter is of greatest practical significance where the
+  sizes of different test sets in cross-validation were very unequal, i.e. in
+  group-based CV strategies. :issue:`9085` by :user:`Laurent Direr <ldirer>`
+  and `Andreas Müller`_.
 
-Ensemble
+- |API| The default value of the ``error_score`` parameter in
+  :class:`model_selection.GridSearchCV` and
+  :class:`model_selection.RandomizedSearchCV` will change to ``np.NaN`` in
+  version 0.22. :issue:`10677` by :user:`Kirill Zhdanovich <Zhdanovich>`.
 
-- Fix allowing to obtain deterministic with :class:`BaseBagging` estimator,
-  when comparing results generated at fit time with the one using the object
-  attributes when ``random_state`` is set. :issue:`9723` by :user:`Guillaume
-  Lemaitre <glemaitre>`.
+- |API| Changed ValueError exception raised in
+  :class:`model_selection.ParameterSampler` to a UserWarning for case where the
+  class is instantiated with a greater value of ``n_iter`` than the total space
+  of parameters in the parameter grid. ``n_iter`` now acts as an upper bound on
+  iterations. :issue:`10982` by :user:`Juliet Lawton <julietcl>`
 
-Neighbors
+- |API| Invalid input for :class:`model_selection.ParameterGrid` now
+  raises TypeError.
+  :issue:`10928` by :user:`Solutus Immensus <solutusimmensus>`
 
-- Fixed a bug so ``predict`` in :class:`neighbors.RadiusNeighborsRegressor` can
-  handle empty neighbor set when using non uniform weights. Also raises a new
-  warning when no neighbors are found for samples.  :issue:`9655` by
-  :user:`Andreas Bjerre-Nielsen <abjer>`.
 
-- Fixed a bug in ``KDTree`` construction that results in faster construction
-  and querying times. :issue:`11556` by :user:`Jake VanderPlas <jakevdp>`
+:mod:`sklearn.multioutput`
+..........................
 
-Feature Extraction
+- |MajorFeature| Added :class:`multioutput.RegressorChain` for multi-target
+  regression. :issue:`9257` by :user:`Kumar Ashutosh <thechargedneutron>`.
 
-- Fixed a bug in :func:`feature_extraction.image.extract_patches_2d` which would
-  throw an exception if ``max_patches`` was greater than or equal to the number
-  of all possible patches rather than simply returning the number of possible
-  patches. :issue:`10100` by :user:`Varun Agrawal <varunagrawal>`
 
-- Fixed a bug in :class:`feature_extraction.text.CountVectorizer`,
-  :class:`feature_extraction.text.TfidfVectorizer`,
-  :class:`feature_extraction.text.HashingVectorizer` to support 64 bit sparse
-  array indexing necessary to process large datasets with more than 2·10⁹ tokens
-  (words or n-grams). :issue:`9147` by :user:`Claes-Fredrik Mannby <mannby>`
-  and `Roman Yurchak`_.
+:mod:`sklearn.naive_bayes`
+..........................
 
-- Fixed bug in :class:`feature_extraction.text.TFIDFVectorizer` which
-  was ignoring the parameter ``dtype``. In addition,
-  :class:`feature_extraction.text.TFIDFTransformer` will preserve ``dtype``
-  for floating and raise a warning if ``dtype`` requested is integer.
-  :issue:`10441` by :user:`Mayur Kulkarni <maykulkarni>` and
-  :user:`Guillaume Lemaitre <glemaitre>`.
+- |MajorFeature| Added :class:`naive_bayes.ComplementNB`, which implements the
+  Complement Naive Bayes classifier described in Rennie et al. (2003).
+  :issue:`8190` by :user:`Michael A. Alcorn <airalcorn2>`.
 
-Utils
+- |Feature| Add `var_smoothing` parameter in :class:`naive_bayes.GaussianNB`
+  to give a precise control over variances calculation.
+  :issue:`9681` by :user:`Dmitry Mottl <Mottl>`.
 
-- :func:`utils.check_array` yield a ``FutureWarning`` indicating
-  that arrays of bytes/strings will be interpreted as decimal numbers
-  beginning in version 0.22. :issue:`10229` by :user:`Ryan Lee <rtlee9>`
+- |Fix| Fixed a bug in :class:`naive_bayes.GaussianNB` which incorrectly
+  raised error for prior list which summed to 1.
+  :issue:`10005` by :user:`Gaurav Dhingra <gxyd>`.
 
-Preprocessing
+- |Fix| Fixed a bug in :class:`naive_bayes.MultinomialNB` which did not accept
+  vector valued pseudocounts (alpha).
+  :issue:`10346` by :user:`Tobias Madsen <TobiasMadsen>`
 
-- Fixed bugs in :class:`preprocessing.LabelEncoder` which would sometimes throw
-  errors when ``transform`` or ``inverse_transform`` was called with empty arrays.
-  :issue:`10458` by :user:`Mayur Kulkarni <maykulkarni>`.
 
-- Fix ValueError in :class:`preprocessing.LabelEncoder` when using
-  ``inverse_transform`` on unseen labels. :issue:`9816` by :user:`Charlie Newey
-  <newey01c>`.
+:mod:`sklearn.neighbors`
+........................
 
-- Fix bug in :class:`preprocessing.OneHotEncoder` which discarded the ``dtype``
-  when returning a sparse matrix output. :issue:`11042` by :user:`Daniel
-  Morales <DanielMorales9>`.
+- |Efficiency| :class:`neighbors.RadiusNeighborsRegressor` and
+  :class:`neighbors.RadiusNeighborsClassifier` are now
+  parallelized according to ``n_jobs`` regardless of ``algorithm``.
+  :issue:`10887` by :user:`Joël Billaud <recamshak>`.
 
-- Fix ``fit`` and ``partial_fit`` in :class:`preprocessing.StandardScaler` in
-  the rare case when `with_mean=False` and `with_std=False` which was crashing
-  by calling ``fit`` more than once and giving inconsistent results for
-  ``mean_`` whether the input was a sparse or a dense matrix. ``mean_`` will be
-  set to ``None`` with both sparse and dense inputs. ``n_samples_seen_`` will
-  be also reported for both input types.
-  :issue:`11235` by :user:`Guillaume Lemaitre <glemaitre>`.
+- |Efficiency| :mod:`Nearest neighbors <neighbors>` query methods are now more
+  memory efficient when ``algorithm='brute'``.
+  :issue:`11136` by `Joel Nothman`_ and :user:`Aman Dalmia <dalmia>`.
 
-Feature selection
+- |Feature| Add `sample_weight` parameter to the fit method of
+  :class:`neighbors.KernelDensity` to enable weighting in kernel density
+  estimation.
+  :issue:`4394` by :user:`Samuel O. Ronsin <samronsin>`.
 
-- Fixed computation of ``n_features_to_compute`` for edge case with tied CV
-  scores in :class:`feature_selection.RFECV`. :issue:`9222` by `Nick Hoh
-  <nickypie>`.
+- |Feature| Novelty detection with :class:`neighbors.LocalOutlierFactor`:
+  Add a ``novelty`` parameter to :class:`neighbors.LocalOutlierFactor`. When
+  ``novelty`` is set to True, :class:`neighbors.LocalOutlierFactor` can then
+  be used for novelty detection, i.e. predict on new unseen data. Available
+  prediction methods are ``predict``, ``decision_function`` and
+  ``score_samples``. By default, ``novelty`` is set to ``False``, and only
+  the ``fit_predict`` method is avaiable.
+  By :user:`Albert Thomas <albertcthomas>`.
 
-Model evaluation and meta-estimators
+- |Fix| Fixed a bug in :class:`neighbors.NearestNeighbors` where fitting a
+  NearestNeighbors model fails when a) the distance metric used is a
+  callable and b) the input to the NearestNeighbors model is sparse.
+  :issue:`9579` by :user:`Thomas Kober <tttthomasssss>`.
 
-- Add improved error message in :func:`model_selection.cross_val_score` when
-  multiple metrics are passed in ``scoring`` keyword.
-  :issue:`11006` by :user:`Ming Li <minggli>`.
+- |Fix| Fixed a bug so ``predict`` in
+  :class:`neighbors.RadiusNeighborsRegressor` can handle empty neighbor set
+  when using non uniform weights. Also raises a new warning when no neighbors
+  are found for samples. :issue:`9655` by :user:`Andreas Bjerre-Nielsen
+  <abjer>`.
 
-Datasets
+- |Fix| |Efficiency| Fixed a bug in ``KDTree`` construction that results in
+  faster construction and querying times.
+  :issue:`11556` by :user:`Jake VanderPlas <jakevdp>`
 
-- Fixed a bug in :func:`datasets.load_boston` which had a wrong data point.
-  :issue:`10801` by :user:`Takeshi Yoshizawa <tarcusx>`.
+- |Fix| Fixed a bug in `neighbors.KDTree` and `neighbors.BallTree` where
+  pickled tree objects would change their type to the super class `BinaryTree`.
+  :issue:`11774` by :user:`Nicolas Hug <NicolasHug>`.
 
-- Fixed a bug in :func:`datasets.load_iris` which had two wrong data points.
-  :issue:`11082` by :user:`Sadhana Srinivasan <rotuna>`
-  and :user:`Hanmin Qin <qinhanmin2014>`.
 
-API changes summary
--------------------
+:mod:`sklearn.neural_network`
+.............................
 
-Classifiers and regressors
+- |Feature| Add `n_iter_no_change` parameter in
+  :class:`neural_network.BaseMultilayerPerceptron`,
+  :class:`neural_network.MLPRegressor`, and
+  :class:`neural_network.MLPClassifier` to give control over
+  maximum number of epochs to not meet ``tol`` improvement.
+  :issue:`9456` by :user:`Nicholas Nadeau <nnadeau>`.
 
-- The default value of the ``n_estimators`` parameter of
-  :class:`ensemble.RandomForestClassifier`, :class:`ensemble.RandomForestRegressor`,
-  :class:`ensemble.ExtraTreesClassifier`, :class:`ensemble.ExtraTreesRegressor`,
-  and :class:`ensemble.RandomTreesEmbedding` will change from 10 in version 0.20
-  to 100 in 0.22. A FutureWarning is raised when the default value is used.
-  :issue:`11542` by :user:`Anna Ayzenshtat <annaayzenshtat>`.
+- |Fix| Fixed a bug in :class:`neural_network.BaseMultilayerPerceptron`,
+  :class:`neural_network.MLPRegressor`, and
+  :class:`neural_network.MLPClassifier` with new ``n_iter_no_change``
+  parameter now at 10 from previously hardcoded 2.
+  :issue:`9456` by :user:`Nicholas Nadeau <nnadeau>`.
 
-Linear, kernelized and related models
+- |Fix| Fixed a bug in :class:`neural_network.MLPRegressor` where fitting
+  quit unexpectedly early due to local minima or fluctuations.
+  :issue:`9456` by :user:`Nicholas Nadeau <nnadeau>`
 
-- Deprecate ``random_state`` parameter in :class:`svm.OneClassSVM` as the
-  underlying implementation is not random.
-  :issue:`9497` by :user:`Albert Thomas <albertcthomas>`.
 
-- Deprecate ``positive=True`` option in :class:`linear_model.Lars` as the
-  underlying implementation is broken. Use :class:`linear_model.Lasso` instead.
-  :issue:`9837` by `Alexandre Gramfort`_.
+:mod:`sklearn.pipeline`
+.......................
 
-- ``n_iter_`` may vary from previous releases in
-  :class:`linear_model.LogisticRegression` with ``solver='lbfgs'`` and
-  :class:`linear_model.HuberRegressor`.  For Scipy <= 1.0.0, the optimizer could
-  perform more than the requested maximum number of iterations. Now both
-  estimators will report at most ``max_iter`` iterations even if more were
-  performed. :issue:`10723` by `Joel Nothman`_.
+- |Feature| The ``predict`` method of :class:`pipeline.Pipeline` now passes
+  keyword arguments on to the pipeline's last estimator, enabling the use of
+  parameters such as ``return_std`` in a pipeline with caution.
+  :issue:`9304` by :user:`Breno Freitas <brenolf>`.
 
-- The default value of ``gamma`` parameter of :class:`svm.SVC`,
-  :class:`~svm.NuSVC`, :class:`~svm.SVR`, :class:`~svm.NuSVR`,
-  :class:`~svm.OneClassSVM` will change from ``'auto'`` to ``'scale'`` in
-  version 0.22 to account better for unscaled features. :issue:`8361` by
-  :user:`Gaurav Dhingra <gxyd>` and :user:`Ting Neo <neokt>`.
 
-- Added convergence warning to :class:`svm.LinearSVC` and
-  :class:`linear_model.LogisticRegression` when ``verbose`` is set to 0.
-  :issue:`10881` by :user:`Alexandre Sevin <AlexandreSev>`.
+:mod:`sklearn.preprocessing`
+............................
+
+- |MajorFeature| Expanded :class:`preprocessing.OneHotEncoder` to allow to
+  encode categorical string features as a numeric array using a one-hot (or
+  dummy) encoding scheme, and added :class:`preprocessing.OrdinalEncoder` to
+  convert to ordinal integers. Those two classes now handle encoding of all
+  feature types (also handles string-valued features) and derives the
+  categories based on the unique values in the features instead of the maximum
+  value in the features. :issue:`9151` and :issue:`10521` by :user:`Vighnesh
+  Birodkar <vighneshbirodkar>` and `Joris Van den Bossche`_.
+
+- |MajorFeature| Added :class:`preprocessing.KBinsDiscretizer` for turning
+  continuous features into categorical or one-hot encoded
+  features. :issue:`7668`, :issue:`9647`, :issue:`10195`,
+  :issue:`10192`, :issue:`11272`, :issue:`11467` and :issue:`11505`.
+  by :user:`Henry Lin <hlin117>`, `Hanmin Qin`_,
+  `Tom Dupre la Tour`_ and :user:`Giovanni Giuseppe Costa <ggc87>`.
+
+- |MajorFeature| Added :class:`preprocessing.PowerTransformer`, which
+  implements the Yeo-Johnson and Box-Cox power transformations. Power
+  transformations try to find a set of feature-wise parametric transformations
+  to approximately map data to a Gaussian distribution centered at zero and
+  with unit variance. This is useful as a variance-stabilizing transformation
+  in situations where normality and homoscedasticity are desirable.
+  :issue:`10210` by :user:`Eric Chang <chang>` and :user:`Maniteja
+  Nandana <maniteja123>`, and :issue:`11520` by :user:`Nicolas Hug
+  <nicolashug>`.
+
+- |MajorFeature| NaN values are ignored and handled in the following
+  preprocessing methods:
+  :class:`preprocessing.MaxAbsScaler`,
+  :class:`preprocessing.MinMaxScaler`,
+  :class:`preprocessing.RobustScaler`,
+  :class:`preprocessing.StandardScaler`,
+  :class:`preprocessing.PowerTransformer`,
+  :class:`preprocessing.QuantileTransformer` classes and
+  :func:`preprocessing.maxabs_scale`,
+  :func:`preprocessing.minmax_scale`,
+  :func:`preprocessing.robust_scale`,
+  :func:`preprocessing.scale`,
+  :func:`preprocessing.power_transform`,
+  :func:`preprocessing.quantile_transform` functions respectively addressed in
+  issues :issue:`11011`, :issue:`11005`, :issue:`11308`, :issue:`11206`,
+  :issue:`11306`, and :issue:`10437`.
+  By :user:`Lucija Gregov <LucijaGregov>` and
+  :user:`Guillaume Lemaitre <glemaitre>`.
+
+- |Feature| :class:`preprocessing.PolynomialFeatures` now supports sparse
+  input. :issue:`10452` by :user:`Aman Dalmia <dalmia>` and `Joel Nothman`_.
+
+- |Feature| :class:`preprocessing.RobustScaler` and
+  :func:`preprocessing.robust_scale` can be fitted using sparse matrices.
+  :issue:`11308` by :user:`Guillaume Lemaitre <glemaitre>`.
+
+- |Feature| :class:`preprocessing.OneHotEncoder` now supports the
+  :term:`get_feature_names` method to obtain the transformed feature names.
+  :issue:`10181` by :user:`Nirvan Anjirbag <Nirvan101>` and
+  `Joris Van den Bossche`_.
+
+- |Feature| A parameter ``check_inverse`` was added to
+  :class:`preprocessing.FunctionTransformer` to ensure that ``func`` and
+  ``inverse_func`` are the inverse of each other.
+  :issue:`9399` by :user:`Guillaume Lemaitre <glemaitre>`.
+
+- |Feature| The ``transform`` method of :class:`sklearn.preprocessing.MultiLabelBinarizer`
+  now ignores any unknown classes. A warning is raised stating the unknown classes
+  classes found which are ignored.
+  :issue:`10913` by :user:`Rodrigo Agundez <rragundez>`.
+
+- |Fix| Fixed bugs in :class:`preprocessing.LabelEncoder` which would
+  sometimes throw errors when ``transform`` or ``inverse_transform`` was called
+  with empty arrays. :issue:`10458` by :user:`Mayur Kulkarni <maykulkarni>`.
+
+- |Fix| Fix ValueError in :class:`preprocessing.LabelEncoder` when using
+  ``inverse_transform`` on unseen labels. :issue:`9816` by :user:`Charlie Newey
+  <newey01c>`.
 
-Preprocessing
+- |Fix| Fix bug in :class:`preprocessing.OneHotEncoder` which discarded the
+  ``dtype`` when returning a sparse matrix output.
+  :issue:`11042` by :user:`Daniel Morales <DanielMorales9>`.
+
+- |Fix| Fix ``fit`` and ``partial_fit`` in
+  :class:`preprocessing.StandardScaler` in the rare case when `with_mean=False`
+  and `with_std=False` which was crashing by calling ``fit`` more than once and
+  giving inconsistent results for ``mean_`` whether the input was a sparse or a
+  dense matrix. ``mean_`` will be set to ``None`` with both sparse and dense
+  inputs. ``n_samples_seen_`` will be also reported for both input types.
+  :issue:`11235` by :user:`Guillaume Lemaitre <glemaitre>`.
 
-- Deprecate ``n_values`` and ``categorical_features`` parameters and
+- |API| Deprecate ``n_values`` and ``categorical_features`` parameters and
   ``active_features_``, ``feature_indices_`` and ``n_values_`` attributes
   of :class:`preprocessing.OneHotEncoder`. The ``n_values`` parameter can be
   replaced with the new ``categories`` parameter, and the attributes with the
@@ -817,86 +963,99 @@ Preprocessing
   :class:`compose.ColumnTransformer`.
   :issue:`10521` by `Joris Van den Bossche`_.
 
-Decomposition, manifold learning and clustering
+- |API| Deprecate :class:`preprocessing.Imputer` and move
+  the corresponding module to :class:`impute.SimpleImputer`.
+  :issue:`9726` by :user:`Kumar Ashutosh
+  <thechargedneutron>`.
 
-- Deprecate ``precomputed`` parameter in function
-  :func:`manifold.t_sne.trustworthiness`. Instead, the new parameter
-  ``metric`` should be used with any compatible metric including
-  'precomputed', in which case the input matrix ``X`` should be a matrix of
-  pairwise distances or squared distances. :issue:`9775` by
-  :user:`William de Vazelhes <wdevazelhes>`.
+- |API| The ``axis`` parameter that was in
+  :class:`preprocessing.Imputer` is no longer present in
+  :class:`impute.SimpleImputer`. The behavior is equivalent
+  to ``axis=0`` (impute along columns). Row-wise
+  imputation can be performed with FunctionTransformer
+  (e.g., ``FunctionTransformer(lambda X:
+  SimpleImputer().fit_transform(X.T).T)``). :issue:`10829`
+  by :user:`Guillaume Lemaitre <glemaitre>` and
+  :user:`Gilberto Olimpio <gilbertoolimpio>`.
+
+- |API| The NaN marker for the missing values has been changed
+  between the :class:`preprocessing.Imputer` and the
+  :class:`impute.SimpleImputer`.
+  ``missing_values='NaN'`` should now be
+  ``missing_values=np.nan``. :issue:`11211` by
+  :user:`Jeremie du Boisberranger <jeremiedbb>`.
 
-- Added function :func:`fit_predict` to :class:`mixture.GaussianMixture` and
-  :class:`mixture.GaussianMixture`, which is essentially equivalent to calling
-  :func:`fit` and :func:`predict`. :issue:`10336` by
-  :user:`Shu Haoran <haoranShu>` and :user:`Andrew Peng <Andrew-peng>`.
+- |API| In :class:`preprocessing.FunctionTransformer`, the default of
+  ``validate`` will be from ``True`` to ``False`` in 0.22.
+  :issue:`10655` by :user:`Guillaume Lemaitre <glemaitre>`.
 
-Metrics
 
-- Deprecate ``reorder`` parameter in :func:`metrics.auc` as it's no longer required
-  for :func:`metrics.roc_auc_score`. Moreover using ``reorder=True`` can hide bugs
-  due to floating point error in the input.
-  :issue:`9851` by :user:`Hanmin Qin <qinhanmin2014>`.
+:mod:`sklearn.svm`
+..................
 
-- In :func:`metrics.normalized_mutual_information_score` and
-  :func:`metrics.adjusted_mutual_information_score`,
-  warn that ``average_method``
-  will have a new default value. In version 0.22, the default normalizer for each
-  will become the *arithmetic* mean of the entropies of each clustering. Currently,
-  :func:`metrics.normalized_mutual_information_score` uses the default of
-  ``average_method='geometric'``, and :func:`metrics.adjusted_mutual_information_score`
-  uses the default of ``average_method='max'`` to match their behaviors in
-  version 0.19.
-  :issue:`11124` by :user:`Arya McCarthy <aryamccarthy>`.
+- |Fix| Fixed a bug in :class:`svm.SVC` where when the argument ``kernel`` is
+  unicode in Python2, the ``predict_proba`` method was raising an
+  unexpected TypeError given dense inputs.
+  :issue:`10412` by :user:`Jiongyan Zhang <qmick>`.
 
-- The ``batch_size`` parameter to :func:`metrics.pairwise_distances_argmin_min`
-  and :func:`metrics.pairwise_distances_argmin` is deprecated to be removed in
-  v0.22.  It no longer has any effect, as batch size is determined by global
-  ``working_memory`` config. See :ref:`working_memory`. :issue:`10280` by `Joel
-  Nothman`_ and :user:`Aman Dalmia <dalmia>`.
+- |API| Deprecate ``random_state`` parameter in :class:`svm.OneClassSVM` as
+  the underlying implementation is not random.
+  :issue:`9497` by :user:`Albert Thomas <albertcthomas>`.
+
+- |API| The default value of ``gamma`` parameter of :class:`svm.SVC`,
+  :class:`~svm.NuSVC`, :class:`~svm.SVR`, :class:`~svm.NuSVR`,
+  :class:`~svm.OneClassSVM` will change from ``'auto'`` to ``'scale'`` in
+  version 0.22 to account better for unscaled features. :issue:`8361` by
+  :user:`Gaurav Dhingra <gxyd>` and :user:`Ting Neo <neokt>`.
 
-Cluster
 
-- Deprecate ``pooling_func`` unused parameter in
-  :class:`cluster.AgglomerativeClustering`. :issue:`9875` by :user:`Kumar Ashutosh
-  <thechargedneutron>`.
+:mod:`sklearn.tree`
+...................
 
-Ensemble
+- |Enhancement| Although private (and hence not assured API stability),
+  :class:`tree._criterion.ClassificationCriterion` and
+  :class:`tree._criterion.RegressionCriterion` may now be cimported and
+  extended. :issue:`10325` by :user:`Camil Staps <camilstaps>`.
 
-- Classes derived from :class:`ensemble.BaseBagging`. The attribute
-  ``estimators_samples_`` will return a list of arrays containing the indices
-  selected for each bootstrap instead of a list of arrays containing the mask
-  of the samples selected for each bootstrap. Indices allows to repeat samples
-  while mask does not allow this functionality. :issue:`9524` by
-  :user:`Guillaume Lemaitre <glemaitre>`.
+- |Fix| Fixed a bug in :class:`tree.BaseDecisionTree` with `splitter="best"`
+  where split threshold could become infinite when values in X were
+  near infinite. :issue:`10536` by :user:`Jonathan Ohayon <Johayon>`.
+
+- |Fix| Fixed a bug in :class:`tree.MAE` to ensure sample weights are being
+  used during the calculation of tree MAE impurity. Previous behaviour could
+  cause suboptimal splits to be chosen since the impurity calculation
+  considered all samples to be of equal weight importance.
+  :issue:`11464` by :user:`John Stott <JohnStott>`.
 
-Imputer
+- |API| The parameters ``min_samples_leaf`` and ``min_weight_fraction_leaf`` in
+  :class:`tree.DecisionTreeClassifier` and :class:`tree.DecisionTreeRegressor`
+  are deprecated and will be removed (fixed to 1 and 0 respectively) in version
+  0.22.  These parameters were not effective for regularization and at worst
+  would produce bad splits.  :issue:`10773` by :user:`Bob Chen <lasagnaman>`
+  and `Joel Nothman`_.
 
-- Deprecate :class:`preprocessing.Imputer` and move the corresponding module to
-  :class:`impute.SimpleImputer`. :issue:`9726` by :user:`Kumar Ashutosh
-  <thechargedneutron>`.
 
-- The ``axis`` parameter that was in :class:`preprocessing.Imputer` is no
-  longer present in :class:`impute.SimpleImputer`. The behavior is equivalent
-  to ``axis=0`` (impute along columns). Row-wise imputation can be performed
-  with FunctionTransformer (e.g., ``FunctionTransformer(lambda X:
-  SimpleImputer().fit_transform(X.T).T)``). :issue:`10829` by :user:`Guillaume
-  Lemaitre <glemaitre>` and :user:`Gilberto Olimpio <gilbertoolimpio>`.
+:mod:`sklearn.utils`
+....................
+
+- |Feature| :func:`utils.check_array` and :func:`utils.check_X_y` now have
+  ``accept_large_sparse`` to control whether scipy.sparse matrices with 64-bit
+  indices should be rejected.
+  :issue:`11327` by :user:`Karan Dhingra <kdhingra307>` and `Joel Nothman`_.
+
+- |Efficiency| |Fix| Avoid copying the data in :func:`utils.check_array` when
+  the input data is a memmap (and ``copy=False``). :issue:`10663` by
+  :user:`Arthur Mensch <arthurmensch>` and :user:`Loïc Estève <lesteve>`.
 
-- The :class:`impute.SimpleImputer` has a new strategy, ``'constant'``, to
-  complete missing values with a fixed one, given by the ``fill_value``
-  parameter. This strategy supports numeric and non-numeric data, and so does
-  the ``'most_frequent'`` strategy now. :issue:`11211` by :user:`Jeremie du
-  Boisberranger <jeremiedbb>`.
+- |API| :func:`utils.check_array` yield a ``FutureWarning`` indicating
+  that arrays of bytes/strings will be interpreted as decimal numbers
+  beginning in version 0.22. :issue:`10229` by :user:`Ryan Lee <rtlee9>`
 
-- The NaN marker for the missing values has been changed between the
-  :class:`preprocessing.Imputer` and the :class:`impute.SimpleImputer`.
-  ``missing_values='NaN'`` should now be ``missing_values=np.nan``.
-  :issue:`11211` by :user:`Jeremie du Boisberranger <jeremiedbb>`.
 
-Outlier Detection models
+Multiple modules
+................
 
-- More consistent outlier detection API:
+- |Feature| |API| More consistent outlier detection API:
   Add a ``score_samples`` method in :class:`svm.OneClassSVM`,
   :class:`ensemble.IsolationForest`, :class:`neighbors.LocalOutlierFactor`,
   :class:`covariance.EllipticEnvelope`. It allows to access raw score
@@ -912,16 +1071,7 @@ Outlier Detection models
   ``raw_values`` parameter is deprecated as the shifted Mahalanobis distance
   will be always returned in 0.22. :issue:`9015` by `Nicolas Goix`_.
 
-- Novelty detection with :class:`neighbors.LocalOutlierFactor`:
-  Add a ``novelty`` parameter to :class:`neighbors.LocalOutlierFactor`. When
-  ``novelty`` is set to True, :class:`neighbors.LocalOutlierFactor` can then 
-  be used for novelty detection, i.e. predict on new unseen data. Available
-  prediction methods are ``predict``, ``decision_function`` and
-  ``score_samples``. By default, ``novelty`` is set to ``False``, and only
-  the ``fit_predict`` method is avaiable.
-  By :user:`Albert Thomas <albertcthomas>`.
-
- - A ``behaviour`` parameter has been introduced in :class:`ensemble.IsolationForest`
+- |Feature| |API| A ``behaviour`` parameter has been introduced in :class:`ensemble.IsolationForest`
   to ensure backward compatibility.
   In the old behaviour, the ``decision_function`` is independent of the ``contamination``
   parameter. A threshold attribute depending on the ``contamination`` parameter is thus
@@ -932,17 +1082,11 @@ Outlier Detection models
   Beside, the behaviour parameter will be removed in 0.24.
   :issue:`11553` by `Nicolas Goix`_.
 
-Covariance
-
-- The :func:`covariance.graph_lasso`, :class:`covariance.GraphLasso` and
-  :class:`covariance.GraphLassoCV` have been renamed to
-  :func:`covariance.graphical_lasso`, :class:`covariance.GraphicalLasso` and
-  :class:`covariance.GraphicalLassoCV` respectively and will be removed in version 0.22.
-  :issue:`9993` by :user:`Artiem Krinitsyn <artiemq>`
-
-Misc
+- |API| Added convergence warning to :class:`svm.LinearSVC` and
+  :class:`linear_model.LogisticRegression` when ``verbose`` is set to 0.
+  :issue:`10881` by :user:`Alexandre Sevin <AlexandreSev>`.
 
-- Changed warning type from :class:`UserWarning` to
+- |API| Changed warning type from :class:`UserWarning` to
   :class:`exceptions.ConvergenceWarning` for failing convergence in
   :func:`linear_model.logistic_regression_path`,
   :class:`linear_model.RANSACRegressor`, :func:`linear_model.ridge_regression`,
@@ -950,40 +1094,71 @@ Misc
   :class:`gaussian_process.GaussianProcessClassifier`,
   :func:`decomposition.fastica`, :class:`cross_decomposition.PLSCanonical`,
   :class:`cluster.AffinityPropagation`, and :class:`cluster.Birch`.
-  :issue:`#10306` by :user:`Jonathan Siebert <jotasi>`.
+  :issue:`10306` by :user:`Jonathan Siebert <jotasi>`.
 
-- Changed ValueError exception raised in :class:`model_selection.ParameterSampler`
-  to a UserWarning for case where the class is instantiated with a greater value of
-  ``n_iter`` than the total space of parameters in the parameter grid. ``n_iter`` now
-  acts as an upper bound on iterations.
-  :issue:`#10982` by :user:`Juliet Lawton <julietcl>`
 
-- Invalid input for :class:`model_selection.ParameterGrid` now raises TypeError.
-  :issue:`10928` by :user:`Solutus Immensus <solutusimmensus>`
+Miscellaneous
+.............
 
-- :func:`utils.check_array` and :func:`utils.check_X_y` now have
-  ``accept_large_sparse`` to control whether scipy.sparse matrices with 64-bit
-  indices should be rejected.
-  :issue:`11327` by :user:`Karan Dhingra <kdhingra307>` and `Joel Nothman`_.
+- |MajorFeature| A new configuration parameter, ``working_memory`` was added
+  to control memory consumption limits in chunked operations, such as the new
+  :func:`metrics.pairwise_distances_chunked`. See :ref:`working_memory`.
+  :issue:`10280` by `Joel Nothman`_ and :user:`Aman Dalmia <dalmia>`.
 
-Preprocessing
+- |Feature| The version of :mod:`joblib` bundled with Scikit-learn is now 0.12.
+  This uses a new default multiprocessing implementation, named `loky
+  <https://github.com/tomMoral/loky>`_. While this may incur some memory and
+  communication overhead, it should provide greater cross-platform stability
+  than relying on Python standard library multiprocessing. :issue:`11741` by
+  the Joblib developers, especially :user:`Thomas Moreau <tomMoral>` and
+  `Olivier Grisel`_.
+
+- |Feature| An environment variable to use the site joblib instead of the
+  vendored one was added (:ref:`environment_variable`). The main API of joblib
+  is now exposed in :mod:`sklearn.utils`.
+  :issue:`11166` by `Gael Varoquaux`_.
+
+- |Feature| Add almost complete PyPy 3 support. Known unsupported
+  functionalities are :func:`datasets.load_svmlight_file`,
+  :class:`feature_extraction.FeatureHasher` and
+  :class:`feature_extraction.text.HashingVectorizer`. For running on PyPy,
+  PyPy3-v5.10+, Numpy 1.14.0+, and scipy 1.1.0+ are required.
+  :issue:`11010` by :user:`Ronan Lamy <rlamy>` and `Roman Yurchak`_.
 
-- In :class:`preprocessing.FunctionTransformer`, the default of ``validate``
-  will be from ``True`` to ``False`` in 0.22.
-  :issue:`10655` by :user:`Guillaume Lemaitre <glemaitre>`.
+- |Feature| A utility method :func:`sklearn.show_versions()` was added to
+  print out information relevant for debugging. It includes the user system,
+  the Python executable, the version of the main libraries and BLAS binding
+  information. :issue:`11596` by :user:`Alexandre Boucaud <aboucaud>`
 
-Model selection
+- |Fix| Fixed a bug when setting parameters on meta-estimator, involving both
+  a wrapped estimator and its parameter. :issue:`9999` by :user:`Marcus Voss
+  <marcus-voss>` and `Joel Nothman`_.
 
-- The default number of cross-validation folds ``cv`` and the default number of
-  splits ``n_splits`` in the :class:`model_selection.KFold`-like splitters will change
-  from 3 to 5 in 0.22 as 3-fold has a lot of variance.
-  :issue:`11557` by :user:`Alexandre Boucaud <aboucaud>`.
+- |Fix| Fixed a bug where calling :func:`sklearn.base.clone` was not thread
+  safe and could result in a "pop from empty list" error. :issue:`9569`
+  by `Andreas Müller`_.
+
+- |API| The default value of ``n_jobs`` is changed from ``1`` to ``None`` in
+  all related functions and classes. ``n_jobs=None`` means ``unset``. It will
+  generally be interpreted as ``n_jobs=1``, unless the current
+  ``joblib.Parallel`` backend context specifies otherwise (See
+  :term:`Glossary <n_jobs>` for additional information). Note that this change
+  happens immediately (i.e., without a deprecation cycle).
+  :issue:`11741` by `Olivier Grisel`_.
 
 Changes to estimator checks
 ---------------------------
 
 These changes mostly affect library developers.
 
+- Checks for transformers now apply if the estimator implements
+  :term:`transform`, regardless of whether it inherits from
+  :class:`sklearn.base.TransformerMixin`. :issue:`10474` by `Joel Nothman`_.
+
+- Classifiers are now checked for consistency between :term:`decision_function`
+  and categorical predictions.
+  :issue:`10500` by :user:`Narine Kokhlikyan <NarineK>`.
+
 - Allow tests in :func:`utils.estimator_checks.check_estimator` to test functions
   that accept pairwise data.
   :issue:`9701` by :user:`Kyle Johnson <gkjohns>`
@@ -1002,8 +1177,8 @@ These changes mostly affect library developers.
 
 - Add ``check_methods_subset_invariance`` to
   :func:`~utils.estimator_checks.check_estimator`, which checks that
-  estimator methods are invariant if applied to a data subset.  :issue:`10420`
-  by :user:`Jonathan Ohayon <Johayon>`
+  estimator methods are invariant if applied to a data subset.
+  :issue:`10428` by :user:`Jonathan Ohayon <Johayon>`
 
 - Add tests in :func:`utils.estimator_checks.check_estimator` to check that an
   estimator can handle read-only memmap input data. :issue:`10663` by
@@ -1012,3 +1187,7 @@ These changes mostly affect library developers.
 - ``check_sample_weights_pandas_series`` now uses 8 rather than 6 samples
   to accommodate for the default number of clusters in :class:`cluster.KMeans`.
   :issue:`10933` by :user:`Johannes Hansen <jnhansen>`.
+
+- Estimators are now checked for whether ``sample_weight=None`` equates to
+  ``sample_weight=np.ones(...)``.
+  :issue:`11558` by :user:`Sergul Aydore <sergulaydore>`.
diff --git a/examples/applications/plot_face_recognition.py b/examples/applications/plot_face_recognition.py
index 13a38d13bc..dce3df1d3e 100755
--- a/examples/applications/plot_face_recognition.py
+++ b/examples/applications/plot_face_recognition.py
@@ -108,7 +108,8 @@
 t0 = time()
 param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],
               'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }
-clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)
+clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'),
+                   param_grid, cv=5)
 clf = clf.fit(X_train_pca, y_train)
 print("done in %0.3fs" % (time() - t0))
 print("Best estimator found by grid search:")
diff --git a/examples/applications/plot_prediction_latency.py b/examples/applications/plot_prediction_latency.py
index f5a3b51735..6eac023b71 100755
--- a/examples/applications/plot_prediction_latency.py
+++ b/examples/applications/plot_prediction_latency.py
@@ -26,7 +26,6 @@
 
 from sklearn.preprocessing import StandardScaler
 from sklearn.model_selection import train_test_split
-from scipy.stats import scoreatpercentile
 from sklearn.datasets.samples_generator import make_regression
 from sklearn.ensemble.forest import RandomForestRegressor
 from sklearn.linear_model.ridge import Ridge
@@ -50,7 +49,7 @@ def atomic_benchmark_estimator(estimator, X_test, verbose=False):
         estimator.predict(instance)
         runtimes[i] = time.time() - start
     if verbose:
-        print("atomic_benchmark runtimes:", min(runtimes), scoreatpercentile(
+        print("atomic_benchmark runtimes:", min(runtimes), np.percentile(
             runtimes, 50), max(runtimes))
     return runtimes
 
@@ -65,7 +64,7 @@ def bulk_benchmark_estimator(estimator, X_test, n_bulk_repeats, verbose):
         runtimes[i] = time.time() - start
     runtimes = np.array(list(map(lambda x: x / float(n_instances), runtimes)))
     if verbose:
-        print("bulk_benchmark runtimes:", min(runtimes), scoreatpercentile(
+        print("bulk_benchmark runtimes:", min(runtimes), np.percentile(
             runtimes, 50), max(runtimes))
     return runtimes
 
@@ -207,8 +206,8 @@ def n_feature_influence(estimators, n_train, n_test, n_features, percentile):
             estimator.fit(X_train, y_train)
             gc.collect()
             runtimes = bulk_benchmark_estimator(estimator, X_test, 30, False)
-            percentiles[cls_name][n] = 1e6 * scoreatpercentile(runtimes,
-                                                               percentile)
+            percentiles[cls_name][n] = 1e6 * np.percentile(runtimes,
+                                                           percentile)
     return percentiles
 
 
diff --git a/examples/applications/plot_stock_market.py b/examples/applications/plot_stock_market.py
index 379efb8e4d..e2edc25b7e 100755
--- a/examples/applications/plot_stock_market.py
+++ b/examples/applications/plot_stock_market.py
@@ -166,7 +166,7 @@
 
 # #############################################################################
 # Learn a graphical structure from the correlations
-edge_model = covariance.GraphicalLassoCV()
+edge_model = covariance.GraphicalLassoCV(cv=5)
 
 # standardize the time series: using correlations rather than covariance
 # is more efficient for structure recovery
diff --git a/examples/cluster/plot_color_quantization.py b/examples/cluster/plot_color_quantization.py
index 7ef4ad6353..ccc45eff73 100755
--- a/examples/cluster/plot_color_quantization.py
+++ b/examples/cluster/plot_color_quantization.py
@@ -61,7 +61,7 @@
 print("done in %0.3fs." % (time() - t0))
 
 
-codebook_random = shuffle(image_array, random_state=0)[:n_colors + 1]
+codebook_random = shuffle(image_array, random_state=0)[:n_colors]
 print("Predicting color indices on the full image (random)")
 t0 = time()
 labels_random = pairwise_distances_argmin(codebook_random,
diff --git a/examples/compose/plot_digits_pipe.py b/examples/compose/plot_digits_pipe.py
index b95d2847ad..2352abba45 100755
--- a/examples/compose/plot_digits_pipe.py
+++ b/examples/compose/plot_digits_pipe.py
@@ -54,7 +54,7 @@
 # Parameters of pipelines can be set using ‘__’ separated parameter names:
 estimator = GridSearchCV(pipe,
                          dict(pca__n_components=n_components,
-                              logistic__C=Cs))
+                              logistic__C=Cs), cv=5)
 estimator.fit(X_digits, y_digits)
 
 plt.axvline(estimator.best_estimator_.named_steps['pca'].n_components,
diff --git a/examples/compose/plot_feature_union.py b/examples/compose/plot_feature_union.py
index 4798617f40..56d1e320c4 100755
--- a/examples/compose/plot_feature_union.py
+++ b/examples/compose/plot_feature_union.py
@@ -55,6 +55,6 @@
                   features__univ_select__k=[1, 2],
                   svm__C=[0.1, 1, 10])
 
-grid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)
+grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, verbose=10)
 grid_search.fit(X, y)
 print(grid_search.best_estimator_)
diff --git a/examples/covariance/plot_covariance_estimation.py b/examples/covariance/plot_covariance_estimation.py
index d33b77d68a..acbe567c53 100755
--- a/examples/covariance/plot_covariance_estimation.py
+++ b/examples/covariance/plot_covariance_estimation.py
@@ -83,7 +83,7 @@
 
 # GridSearch for an optimal shrinkage coefficient
 tuned_parameters = [{'shrinkage': shrinkages}]
-cv = GridSearchCV(ShrunkCovariance(), tuned_parameters)
+cv = GridSearchCV(ShrunkCovariance(), tuned_parameters, cv=5)
 cv.fit(X_train)
 
 # Ledoit-Wolf optimal shrinkage coefficient estimate
diff --git a/examples/covariance/plot_sparse_cov.py b/examples/covariance/plot_sparse_cov.py
index 313d2544c7..a2009bb330 100755
--- a/examples/covariance/plot_sparse_cov.py
+++ b/examples/covariance/plot_sparse_cov.py
@@ -83,7 +83,7 @@
 # Estimate the covariance
 emp_cov = np.dot(X.T, X) / n_samples
 
-model = GraphicalLassoCV()
+model = GraphicalLassoCV(cv=5)
 model.fit(X)
 cov_ = model.covariance_
 prec_ = model.precision_
diff --git a/examples/decomposition/plot_pca_vs_fa_model_selection.py b/examples/decomposition/plot_pca_vs_fa_model_selection.py
index b858434d91..9d395f70c3 100755
--- a/examples/decomposition/plot_pca_vs_fa_model_selection.py
+++ b/examples/decomposition/plot_pca_vs_fa_model_selection.py
@@ -69,20 +69,20 @@ def compute_scores(X):
     for n in n_components:
         pca.n_components = n
         fa.n_components = n
-        pca_scores.append(np.mean(cross_val_score(pca, X)))
-        fa_scores.append(np.mean(cross_val_score(fa, X)))
+        pca_scores.append(np.mean(cross_val_score(pca, X, cv=5)))
+        fa_scores.append(np.mean(cross_val_score(fa, X, cv=5)))
 
     return pca_scores, fa_scores
 
 
 def shrunk_cov_score(X):
     shrinkages = np.logspace(-2, 0, 30)
-    cv = GridSearchCV(ShrunkCovariance(), {'shrinkage': shrinkages})
-    return np.mean(cross_val_score(cv.fit(X).best_estimator_, X))
+    cv = GridSearchCV(ShrunkCovariance(), {'shrinkage': shrinkages}, cv=5)
+    return np.mean(cross_val_score(cv.fit(X).best_estimator_, X, cv=5))
 
 
 def lw_score(X):
-    return np.mean(cross_val_score(LedoitWolf(), X))
+    return np.mean(cross_val_score(LedoitWolf(), X, cv=5))
 
 
 for X, title in [(X_homo, 'Homoscedastic Noise'),
diff --git a/examples/ensemble/plot_adaboost_hastie_10_2.py b/examples/ensemble/plot_adaboost_hastie_10_2.py
index 4d48d13dd2..7fc00a77e3 100755
--- a/examples/ensemble/plot_adaboost_hastie_10_2.py
+++ b/examples/ensemble/plot_adaboost_hastie_10_2.py
@@ -43,11 +43,11 @@
 X_test, y_test = X[2000:], y[2000:]
 X_train, y_train = X[:2000], y[:2000]
 
-dt_stump = DecisionTreeClassifier(max_depth=1, min_samples_leaf=1)
+dt_stump = DecisionTreeClassifier(max_depth=1)
 dt_stump.fit(X_train, y_train)
 dt_stump_err = 1.0 - dt_stump.score(X_test, y_test)
 
-dt = DecisionTreeClassifier(max_depth=9, min_samples_leaf=1)
+dt = DecisionTreeClassifier(max_depth=9)
 dt.fit(X_train, y_train)
 dt_err = 1.0 - dt.score(X_test, y_test)
 
diff --git a/examples/ensemble/plot_gradient_boosting_oob.py b/examples/ensemble/plot_gradient_boosting_oob.py
index ea38b326ce..99f30e750b 100755
--- a/examples/ensemble/plot_gradient_boosting_oob.py
+++ b/examples/ensemble/plot_gradient_boosting_oob.py
@@ -55,7 +55,7 @@
 
 # Fit classifier with out-of-bag estimates
 params = {'n_estimators': 1200, 'max_depth': 3, 'subsample': 0.5,
-          'learning_rate': 0.01, 'min_samples_leaf': 1, 'random_state': 3}
+          'learning_rate': 0.01, 'random_state': 3}
 clf = ensemble.GradientBoostingClassifier(**params)
 
 clf.fit(X_train, y_train)
diff --git a/examples/ensemble/plot_gradient_boosting_quantile.py b/examples/ensemble/plot_gradient_boosting_quantile.py
index 6fb2731a51..99e7289710 100755
--- a/examples/ensemble/plot_gradient_boosting_quantile.py
+++ b/examples/ensemble/plot_gradient_boosting_quantile.py
@@ -41,8 +41,7 @@ def f(x):
 
 clf = GradientBoostingRegressor(loss='quantile', alpha=alpha,
                                 n_estimators=250, max_depth=3,
-                                learning_rate=.1, min_samples_leaf=9,
-                                min_samples_split=9)
+                                learning_rate=.1, min_samples_split=9)
 
 clf.fit(X, y)
 
diff --git a/examples/exercises/plot_cv_diabetes.py b/examples/exercises/plot_cv_diabetes.py
index 76b0d81b89..d68fd21bd7 100755
--- a/examples/exercises/plot_cv_diabetes.py
+++ b/examples/exercises/plot_cv_diabetes.py
@@ -29,7 +29,7 @@
 alphas = np.logspace(-4, -0.5, 30)
 
 tuned_parameters = [{'alpha': alphas}]
-n_folds = 3
+n_folds = 5
 
 clf = GridSearchCV(lasso, tuned_parameters, cv=n_folds, refit=False)
 clf.fit(X, y)
@@ -60,7 +60,7 @@
 # performs cross-validation on the training data it receives).
 # We use external cross-validation to see how much the automatically obtained
 # alphas differ across different cross-validation folds.
-lasso_cv = LassoCV(alphas=alphas, random_state=0)
+lasso_cv = LassoCV(alphas=alphas, cv=5, random_state=0)
 k_fold = KFold(3)
 
 print("Answer to the bonus question:",
diff --git a/examples/exercises/plot_cv_digits.py b/examples/exercises/plot_cv_digits.py
index a68f92afbd..f51bcc7e02 100755
--- a/examples/exercises/plot_cv_digits.py
+++ b/examples/exercises/plot_cv_digits.py
@@ -26,7 +26,7 @@
 scores_std = list()
 for C in C_s:
     svc.C = C
-    this_scores = cross_val_score(svc, X, y, n_jobs=1)
+    this_scores = cross_val_score(svc, X, y, cv=5, n_jobs=1)
     scores.append(np.mean(this_scores))
     scores_std.append(np.std(this_scores))
 
diff --git a/examples/feature_selection/plot_select_from_model_boston.py b/examples/feature_selection/plot_select_from_model_boston.py
index 17ef6d6bd0..400a736942 100755
--- a/examples/feature_selection/plot_select_from_model_boston.py
+++ b/examples/feature_selection/plot_select_from_model_boston.py
@@ -23,7 +23,7 @@
 X, y = boston['data'], boston['target']
 
 # We use the base estimator LassoCV since the L1 norm promotes sparsity of features.
-clf = LassoCV()
+clf = LassoCV(cv=5)
 
 # Set a minimum threshold of 0.25
 sfm = SelectFromModel(clf, threshold=0.25)
diff --git a/examples/gaussian_process/plot_gpr_co2.py b/examples/gaussian_process/plot_gpr_co2.py
index b0b271a364..8170de0189 100755
--- a/examples/gaussian_process/plot_gpr_co2.py
+++ b/examples/gaussian_process/plot_gpr_co2.py
@@ -8,7 +8,7 @@
 hyperparameter optimization using gradient ascent on the
 log-marginal-likelihood. The data consists of the monthly average atmospheric
 CO2 concentrations (in parts per million by volume (ppmv)) collected at the
-Mauna Loa Observatory in Hawaii, between 1958 and 1997. The objective is to
+Mauna Loa Observatory in Hawaii, between 1958 and 2001. The objective is to
 model the CO2 concentration as a function of the time t.
 
 The kernel is composed of several terms that are responsible for explaining
@@ -57,12 +57,12 @@
 explained by the model. The figure shows also that the model makes very
 confident predictions until around 2015.
 """
-print(__doc__)
-
 # Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
 #
 # License: BSD 3 clause
 
+from __future__ import division, print_function
+
 import numpy as np
 
 from matplotlib import pyplot as plt
@@ -70,11 +70,46 @@
 from sklearn.gaussian_process import GaussianProcessRegressor
 from sklearn.gaussian_process.kernels \
     import RBF, WhiteKernel, RationalQuadratic, ExpSineSquared
-from sklearn.datasets import fetch_mldata
+try:
+    from urllib.request import urlopen
+except ImportError:
+    # Python 2
+    from urllib2 import urlopen
+
+print(__doc__)
+
 
-data = fetch_mldata('mauna-loa-atmospheric-co2').data
-X = data[:, [1]]
-y = data[:, 0]
+def load_mauna_loa_atmospheric_c02():
+    url = ('http://cdiac.ess-dive.lbl.gov/'
+           'ftp/trends/co2/sio-keel-flask/maunaloa_c.dat')
+    months = []
+    ppmv_sums = []
+    counts = []
+    for line in urlopen(url):
+        line = line.decode('utf8')
+        if not line.startswith('MLO'):
+            # ignore headers
+            continue
+        station, date, weight, flag, ppmv = line.split()
+        y = date[:2]
+        m = date[2:4]
+        month_float = (int(('20' if y < '20' else '19') + y) +
+                       (int(m) - 1) / 12)
+        if not months or month_float != months[-1]:
+            months.append(month_float)
+            ppmv_sums.append(float(ppmv))
+            counts.append(1)
+        else:
+            # aggregate monthly sum to produce average
+            ppmv_sums[-1] += float(ppmv)
+            counts[-1] += 1
+
+    months = np.asarray(months).reshape(-1, 1)
+    avg_ppmvs = np.asarray(ppmv_sums) / counts
+    return months, avg_ppmvs
+
+
+X, y = load_mauna_loa_atmospheric_c02()
 
 # Kernel with parameters given in GPML book
 k1 = 66.0**2 * RBF(length_scale=67.0)  # long term smooth rising trend
diff --git a/examples/gaussian_process/plot_gpr_prior_posterior.py b/examples/gaussian_process/plot_gpr_prior_posterior.py
index 47163f77de..85d18041ad 100755
--- a/examples/gaussian_process/plot_gpr_prior_posterior.py
+++ b/examples/gaussian_process/plot_gpr_prior_posterior.py
@@ -29,7 +29,7 @@
                                 length_scale_bounds=(0.1, 10.0),
                                 periodicity_bounds=(1.0, 10.0)),
            ConstantKernel(0.1, (0.01, 10.0))
-               * (DotProduct(sigma_0=1.0, sigma_0_bounds=(0.0, 10.0)) ** 2),
+               * (DotProduct(sigma_0=1.0, sigma_0_bounds=(0.1, 10.0)) ** 2),
            1.0 * Matern(length_scale=1.0, length_scale_bounds=(1e-1, 10.0),
                         nu=1.5)]
 
diff --git a/examples/linear_model/plot_omp.py b/examples/linear_model/plot_omp.py
index f07b7d7233..8a3b52fc58 100755
--- a/examples/linear_model/plot_omp.py
+++ b/examples/linear_model/plot_omp.py
@@ -67,7 +67,7 @@
 
 # plot the noisy reconstruction with number of non-zeros set by CV
 ##################################################################
-omp_cv = OrthogonalMatchingPursuitCV()
+omp_cv = OrthogonalMatchingPursuitCV(cv=5)
 omp_cv.fit(X, y_noisy)
 coef = omp_cv.coef_
 idx_r, = coef.nonzero()
diff --git a/examples/linear_model/plot_sgd_early_stopping.py b/examples/linear_model/plot_sgd_early_stopping.py
index 31ce61f39d..4076fa5f6b 100755
--- a/examples/linear_model/plot_sgd_early_stopping.py
+++ b/examples/linear_model/plot_sgd_early_stopping.py
@@ -47,7 +47,7 @@
 import matplotlib.pyplot as plt
 
 from sklearn import linear_model
-from sklearn.datasets import fetch_mldata
+from sklearn.datasets import fetch_openml
 from sklearn.model_selection import train_test_split
 from sklearn.utils.testing import ignore_warnings
 from sklearn.exceptions import ConvergenceWarning
@@ -56,9 +56,10 @@
 print(__doc__)
 
 
-def load_mnist(n_samples=None, class_0=0, class_1=8):
+def load_mnist(n_samples=None, class_0='0', class_1='8'):
     """Load MNIST, select two classes, shuffle and return only n_samples."""
-    mnist = fetch_mldata('MNIST original')
+    # Load data from http://openml.org/d/554
+    mnist = fetch_openml('mnist_784', version=1)
 
     # take only two classes for binary classification
     mask = np.logical_or(mnist.target == class_0, mnist.target == class_1)
diff --git a/examples/linear_model/plot_sparse_logistic_regression_mnist.py b/examples/linear_model/plot_sparse_logistic_regression_mnist.py
index 5610f471b5..523f5683a5 100755
--- a/examples/linear_model/plot_sparse_logistic_regression_mnist.py
+++ b/examples/linear_model/plot_sparse_logistic_regression_mnist.py
@@ -20,7 +20,7 @@
 import matplotlib.pyplot as plt
 import numpy as np
 
-from sklearn.datasets import fetch_mldata
+from sklearn.datasets import fetch_openml
 from sklearn.linear_model import LogisticRegression
 from sklearn.model_selection import train_test_split
 from sklearn.preprocessing import StandardScaler
@@ -35,9 +35,9 @@
 t0 = time.time()
 train_samples = 5000
 
-mnist = fetch_mldata('MNIST original')
-X = mnist.data.astype('float64')
-y = mnist.target
+# Load data from https://www.openml.org/d/554
+X, y = fetch_openml('mnist_784', version=1, return_X_y=True)
+
 random_state = check_random_state(0)
 permutation = random_state.permutation(X.shape[0])
 X = X[permutation]
diff --git a/examples/mixture/plot_gmm_selection.py b/examples/mixture/plot_gmm_selection.py
index 3ccaba5262..0acee7366a 100755
--- a/examples/mixture/plot_gmm_selection.py
+++ b/examples/mixture/plot_gmm_selection.py
@@ -57,6 +57,7 @@
 bars = []
 
 # Plot the BIC scores
+plt.figure(figsize=(8, 6))
 spl = plt.subplot(2, 1, 1)
 for i, (cv_type, color) in enumerate(zip(cv_types, color_iter)):
     xpos = np.array(n_components_range) + .2 * (i - 2)
diff --git a/examples/model_selection/grid_search_text_feature_extraction.py b/examples/model_selection/grid_search_text_feature_extraction.py
index c3bd054c99..c220a43ed8 100755
--- a/examples/model_selection/grid_search_text_feature_extraction.py
+++ b/examples/model_selection/grid_search_text_feature_extraction.py
@@ -113,7 +113,8 @@
 
     # find the best parameters for both the feature extraction and the
     # classifier
-    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)
+    grid_search = GridSearchCV(pipeline, parameters, cv=5,
+                               n_jobs=-1, verbose=1)
 
     print("Performing grid search...")
     print("pipeline:", [name for name, _ in pipeline.steps])
diff --git a/examples/model_selection/plot_cv_indices.py b/examples/model_selection/plot_cv_indices.py
new file mode 100755
index 0000000000..078c3f5e54
--- /dev/null
+++ b/examples/model_selection/plot_cv_indices.py
@@ -0,0 +1,149 @@
+"""
+Visualizing cross-validation behavior in scikit-learn
+=====================================================
+
+Choosing the right cross-validation object is a crucial part of fitting a
+model properly. There are many ways to split data into training and test
+sets in order to avoid model overfitting, to standardize the number of
+groups in test sets, etc.
+
+This example visualizes the behavior of several common scikit-learn objects
+for comparison.
+"""
+
+from sklearn.model_selection import (TimeSeriesSplit, KFold, ShuffleSplit,
+                                     StratifiedKFold, GroupShuffleSplit,
+                                     GroupKFold, StratifiedShuffleSplit)
+import numpy as np
+import matplotlib.pyplot as plt
+from matplotlib.patches import Patch
+np.random.seed(1338)
+cmap_data = plt.cm.Paired
+cmap_cv = plt.cm.coolwarm
+n_splits = 4
+
+###############################################################################
+# Visualize our data
+# ------------------
+#
+# First, we must understand the structure of our data. It has 100 randomly
+# generated input datapoints, 3 classes split unevenly across datapoints,
+# and 10 "groups" split evenly across datapoints.
+#
+# As we'll see, some cross-validation objects do specific things with
+# labeled data, others behave differently with grouped data, and others
+# do not use this information.
+#
+# To begin, we'll visualize our data.
+
+# Generate the class/group data
+n_points = 100
+X = np.random.randn(100, 10)
+
+percentiles_classes = [.1, .3, .6]
+y = np.hstack([[ii] * int(100 * perc)
+               for ii, perc in enumerate(percentiles_classes)])
+
+# Evenly spaced groups repeated once
+groups = np.hstack([[ii] * 10 for ii in range(10)])
+
+
+def visualize_groups(classes, groups, name):
+    # Visualize dataset groups
+    fig, ax = plt.subplots()
+    ax.scatter(range(len(groups)),  [.5] * len(groups), c=groups, marker='_',
+               lw=50, cmap=cmap_data)
+    ax.scatter(range(len(groups)),  [3.5] * len(groups), c=classes, marker='_',
+               lw=50, cmap=cmap_data)
+    ax.set(ylim=[-1, 5], yticks=[.5, 3.5],
+           yticklabels=['Data\ngroup', 'Data\nclass'], xlabel="Sample index")
+
+
+visualize_groups(y, groups, 'no groups')
+
+###############################################################################
+# Define a function to visualize cross-validation behavior
+# --------------------------------------------------------
+#
+# We'll define a function that lets us visualize the behavior of each
+# cross-validation object. We'll perform 4 splits of the data. On each
+# split, we'll visualize the indices chosen for the training set
+# (in blue) and the test set (in red).
+
+
+def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):
+    """Create a sample plot for indices of a cross-validation object."""
+
+    # Generate the training/testing visualizations for each CV split
+    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):
+        # Fill in indices with the training/test groups
+        indices = np.array([np.nan] * len(X))
+        indices[tt] = 1
+        indices[tr] = 0
+
+        # Visualize the results
+        ax.scatter(range(len(indices)), [ii + .5] * len(indices),
+                   c=indices, marker='_', lw=lw, cmap=cmap_cv,
+                   vmin=-.2, vmax=1.2)
+
+    # Plot the data classes and groups at the end
+    ax.scatter(range(len(X)), [ii + 1.5] * len(X),
+               c=y, marker='_', lw=lw, cmap=cmap_data)
+
+    ax.scatter(range(len(X)), [ii + 2.5] * len(X),
+               c=group, marker='_', lw=lw, cmap=cmap_data)
+
+    # Formatting
+    yticklabels = list(range(n_splits)) + ['class', 'group']
+    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,
+           xlabel='Sample index', ylabel="CV iteration",
+           ylim=[n_splits+2.2, -.2], xlim=[0, 100])
+    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)
+    return ax
+
+
+###############################################################################
+# Let's see how it looks for the `KFold` cross-validation object:
+
+fig, ax = plt.subplots()
+cv = KFold(n_splits)
+plot_cv_indices(cv, X, y, groups, ax, n_splits)
+
+###############################################################################
+# As you can see, by default the KFold cross-validation iterator does not
+# take either datapoint class or group into consideration. We can change this
+# by using the ``StratifiedKFold`` like so.
+
+fig, ax = plt.subplots()
+cv = StratifiedKFold(n_splits)
+plot_cv_indices(cv, X, y, groups, ax, n_splits)
+
+###############################################################################
+# In this case, the cross-validation retained the same ratio of classes across
+# each CV split. Next we'll visualize this behavior for a number of CV
+# iterators.
+#
+# Visualize cross-validation indices for many CV objects
+# ------------------------------------------------------
+#
+# Let's visually compare the cross validation behavior for many
+# scikit-learn cross-validation objects. Below we will loop through several
+# common cross-validation objects, visualizing the behavior of each.
+#
+# Note how some use the group/class information while others do not.
+
+cvs = [KFold, GroupKFold, ShuffleSplit, StratifiedKFold,
+       GroupShuffleSplit, StratifiedShuffleSplit, TimeSeriesSplit]
+
+
+for cv in cvs:
+    this_cv = cv(n_splits=n_splits)
+    fig, ax = plt.subplots(figsize=(6, 3))
+    plot_cv_indices(this_cv, X, y, groups, ax, n_splits)
+
+    ax.legend([Patch(color=cmap_cv(.8)), Patch(color=cmap_cv(.02))],
+              ['Testing set', 'Training set'], loc=(1.02, .8))
+    # Make the legend fit
+    plt.tight_layout()
+    fig.subplots_adjust(right=.7)
+plt.show()
diff --git a/examples/model_selection/plot_learning_curve.py b/examples/model_selection/plot_learning_curve.py
index 6e022ebe27..4d86c323f5 100755
--- a/examples/model_selection/plot_learning_curve.py
+++ b/examples/model_selection/plot_learning_curve.py
@@ -25,7 +25,7 @@
 
 
 def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,
-                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):
+                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):
     """
     Generate a simple plot of the test and training learning curve.
 
@@ -63,8 +63,11 @@ def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validators that can be used here.
 
-    n_jobs : integer, optional
-        Number of jobs to run in parallel (default 1).
+    n_jobs : int or None, optional (default=None)
+        Number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     train_sizes : array-like, shape (n_ticks,), dtype float or int
         Relative or absolute numbers of training examples that will be used to
diff --git a/examples/model_selection/plot_multi_metric_evaluation.py b/examples/model_selection/plot_multi_metric_evaluation.py
index eaec4c8d18..e1b5cbb303 100755
--- a/examples/model_selection/plot_multi_metric_evaluation.py
+++ b/examples/model_selection/plot_multi_metric_evaluation.py
@@ -43,7 +43,7 @@
 # Setting refit='AUC', refits an estimator on the whole dataset with the
 # parameter setting that has the best cross-validated AUC score.
 # That estimator is made available at ``gs.best_estimator_`` along with
-# parameters like ``gs.best_score_``, ``gs.best_parameters_`` and
+# parameters like ``gs.best_score_``, ``gs.best_params_`` and
 # ``gs.best_index_``
 gs = GridSearchCV(DecisionTreeClassifier(random_state=42),
                   param_grid={'min_samples_split': range(2, 403, 10)},
diff --git a/examples/model_selection/plot_randomized_search.py b/examples/model_selection/plot_randomized_search.py
index bac6495090..2429c92e26 100755
--- a/examples/model_selection/plot_randomized_search.py
+++ b/examples/model_selection/plot_randomized_search.py
@@ -55,14 +55,13 @@ def report(results, n_top=3):
 param_dist = {"max_depth": [3, None],
               "max_features": sp_randint(1, 11),
               "min_samples_split": sp_randint(2, 11),
-              "min_samples_leaf": sp_randint(1, 11),
               "bootstrap": [True, False],
               "criterion": ["gini", "entropy"]}
 
 # run randomized search
 n_iter_search = 20
 random_search = RandomizedSearchCV(clf, param_distributions=param_dist,
-                                   n_iter=n_iter_search)
+                                   n_iter=n_iter_search, cv=5)
 
 start = time()
 random_search.fit(X, y)
@@ -74,12 +73,11 @@ def report(results, n_top=3):
 param_grid = {"max_depth": [3, None],
               "max_features": [1, 3, 10],
               "min_samples_split": [2, 3, 10],
-              "min_samples_leaf": [1, 3, 10],
               "bootstrap": [True, False],
               "criterion": ["gini", "entropy"]}
 
 # run grid search
-grid_search = GridSearchCV(clf, param_grid=param_grid)
+grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5)
 start = time()
 grid_search.fit(X, y)
 
diff --git a/examples/multioutput/plot_classifier_chain_yeast.py b/examples/multioutput/plot_classifier_chain_yeast.py
index 6a90e14dfc..cb3a5085e3 100755
--- a/examples/multioutput/plot_classifier_chain_yeast.py
+++ b/examples/multioutput/plot_classifier_chain_yeast.py
@@ -32,24 +32,23 @@
 with randomly ordered chains).
 """
 
-print(__doc__)
-
 # Author: Adam Kleczewski
 # License: BSD 3 clause
 
 import numpy as np
 import matplotlib.pyplot as plt
+from sklearn.datasets import fetch_openml
 from sklearn.multioutput import ClassifierChain
 from sklearn.model_selection import train_test_split
 from sklearn.multiclass import OneVsRestClassifier
 from sklearn.metrics import jaccard_similarity_score
 from sklearn.linear_model import LogisticRegression
-from sklearn.datasets import fetch_mldata
 
-# Load a multi-label dataset
-yeast = fetch_mldata('yeast')
-X = yeast['data']
-Y = yeast['target'].transpose().toarray()
+print(__doc__)
+
+# Load a multi-label dataset from https://www.openml.org/d/40597
+X, Y = fetch_openml('yeast', version=4, return_X_y=True)
+Y = Y == 'TRUE'
 X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2,
                                                     random_state=0)
 
diff --git a/examples/neighbors/plot_digits_kde_sampling.py b/examples/neighbors/plot_digits_kde_sampling.py
index 8367d16b95..ca44c96f13 100755
--- a/examples/neighbors/plot_digits_kde_sampling.py
+++ b/examples/neighbors/plot_digits_kde_sampling.py
@@ -27,7 +27,7 @@
 
 # use grid search cross-validation to optimize the bandwidth
 params = {'bandwidth': np.logspace(-1, 1, 20)}
-grid = GridSearchCV(KernelDensity(), params)
+grid = GridSearchCV(KernelDensity(), params, cv=5)
 grid.fit(data)
 
 print("best bandwidth: {0}".format(grid.best_estimator_.bandwidth))
diff --git a/examples/neural_networks/plot_mnist_filters.py b/examples/neural_networks/plot_mnist_filters.py
index 6c3b8b2284..ab50d4e59a 100755
--- a/examples/neural_networks/plot_mnist_filters.py
+++ b/examples/neural_networks/plot_mnist_filters.py
@@ -20,15 +20,16 @@
 for a very short time. Training longer would result in weights with a much
 smoother spatial appearance.
 """
-print(__doc__)
-
 import matplotlib.pyplot as plt
-from sklearn.datasets import fetch_mldata
+from sklearn.datasets import fetch_openml
 from sklearn.neural_network import MLPClassifier
 
-mnist = fetch_mldata("MNIST original")
+print(__doc__)
+
+# Load data from https://www.openml.org/d/554
+X, y = fetch_openml('mnist_784', version=1, return_X_y=True)
+
 # rescale the data, use the traditional train/test split
-X, y = mnist.data / 255., mnist.target
 X_train, X_test = X[:60000], X[60000:]
 y_train, y_test = y[:60000], y[60000:]
 
diff --git a/examples/plot_missing_values.py b/examples/plot_missing_values.py
index 777120053a..755943fb55 100755
--- a/examples/plot_missing_values.py
+++ b/examples/plot_missing_values.py
@@ -39,7 +39,7 @@ def get_results(dataset):
     # Estimate the score on the entire dataset, with no missing values
     estimator = RandomForestRegressor(random_state=0, n_estimators=100)
     full_scores = cross_val_score(estimator, X_full, y_full,
-                                  scoring='neg_mean_squared_error')
+                                  scoring='neg_mean_squared_error', cv=5)
 
     # Add missing values in 75% of the lines
     missing_rate = 0.75
@@ -57,7 +57,8 @@ def get_results(dataset):
     y_missing = y_full.copy()
     estimator = RandomForestRegressor(random_state=0, n_estimators=100)
     zero_impute_scores = cross_val_score(estimator, X_missing, y_missing,
-                                         scoring='neg_mean_squared_error')
+                                         scoring='neg_mean_squared_error',
+                                         cv=5)
 
     # Estimate the score after imputation (mean strategy) of the missing values
     X_missing = X_full.copy()
@@ -68,7 +69,8 @@ def get_results(dataset):
                    MissingIndicator(missing_values=0)),
         RandomForestRegressor(random_state=0, n_estimators=100))
     mean_impute_scores = cross_val_score(estimator, X_missing, y_missing,
-                                         scoring='neg_mean_squared_error')
+                                         scoring='neg_mean_squared_error',
+                                         cv=5)
 
 
     return ((full_scores.mean(), full_scores.std()),
diff --git a/examples/svm/plot_svm_anova.py b/examples/svm/plot_svm_anova.py
index 45599f31f5..08f9fddf71 100755
--- a/examples/svm/plot_svm_anova.py
+++ b/examples/svm/plot_svm_anova.py
@@ -45,7 +45,7 @@
 for percentile in percentiles:
     clf.set_params(anova__percentile=percentile)
     # Compute cross-validation score using 1 CPU
-    this_scores = cross_val_score(clf, X, y, n_jobs=1)
+    this_scores = cross_val_score(clf, X, y, cv=5, n_jobs=1)
     score_means.append(this_scores.mean())
     score_stds.append(this_scores.std())
 
diff --git a/examples/svm/plot_weighted_samples.py b/examples/svm/plot_weighted_samples.py
index 9cdb2dcb49..0549da7a38 100755
--- a/examples/svm/plot_weighted_samples.py
+++ b/examples/svm/plot_weighted_samples.py
@@ -45,7 +45,7 @@ def plot_decision_function(classifier, sample_weight, axis, title):
 sample_weight_last_ten[15:] *= 5
 sample_weight_last_ten[9] *= 15
 
-# for reference, first fit without class weights
+# for reference, first fit without sample weights
 
 # fit the model
 clf_weights = svm.SVC(gamma=1)
diff --git a/sklearn/__init__.py b/sklearn/__init__.py
index 4de0df5ed1..1d7cd2ef92 100755
--- a/sklearn/__init__.py
+++ b/sklearn/__init__.py
@@ -62,6 +62,8 @@
 else:
     from . import __check_build
     from .base import clone
+    from .utils._show_versions import show_versions
+
     __check_build  # avoid flakes unused variable error
 
     __all__ = ['calibration', 'cluster', 'covariance', 'cross_decomposition',
@@ -74,7 +76,8 @@
                'preprocessing', 'random_projection', 'semi_supervised',
                'svm', 'tree', 'discriminant_analysis', 'impute', 'compose',
                # Non-modules:
-               'clone', 'get_config', 'set_config', 'config_context']
+               'clone', 'get_config', 'set_config', 'config_context',
+               'show_versions']
 
 
 def setup_module(module):
diff --git a/sklearn/cluster/bicluster.py b/sklearn/cluster/bicluster.py
index 3be4f6cabf..8bbf735312 100755
--- a/sklearn/cluster/bicluster.py
+++ b/sklearn/cluster/bicluster.py
@@ -93,7 +93,7 @@ class BaseSpectral(six.with_metaclass(ABCMeta, BaseEstimator,
     @abstractmethod
     def __init__(self, n_clusters=3, svd_method="randomized",
                  n_svd_vecs=None, mini_batch=False, init="k-means++",
-                 n_init=10, n_jobs=1, random_state=None):
+                 n_init=10, n_jobs=None, random_state=None):
         self.n_clusters = n_clusters
         self.svd_method = svd_method
         self.n_svd_vecs = n_svd_vecs
@@ -228,15 +228,14 @@ class SpectralCoclustering(BaseSpectral):
         chosen and the algorithm runs once. Otherwise, the algorithm
         is run for each initialization and the best solution chosen.
 
-    n_jobs : int, optional, default: 1
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. This works by breaking
         down the pairwise matrix into n_jobs even slices and computing them in
         parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For n_jobs below -1,
-        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
-        are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None (default)
         Used for randomizing the singular value decomposition and the k-means
@@ -271,7 +270,7 @@ class SpectralCoclustering(BaseSpectral):
     array([0, 0], dtype=int32)
     >>> clustering # doctest: +NORMALIZE_WHITESPACE
     SpectralCoclustering(init='k-means++', mini_batch=False, n_clusters=2,
-               n_init=10, n_jobs=1, n_svd_vecs=None, random_state=0,
+               n_init=10, n_jobs=None, n_svd_vecs=None, random_state=0,
                svd_method='randomized')
 
     References
@@ -284,7 +283,7 @@ class SpectralCoclustering(BaseSpectral):
     """
     def __init__(self, n_clusters=3, svd_method='randomized',
                  n_svd_vecs=None, mini_batch=False, init='k-means++',
-                 n_init=10, n_jobs=1, random_state=None):
+                 n_init=10, n_jobs=None, random_state=None):
         super(SpectralCoclustering, self).__init__(n_clusters,
                                                    svd_method,
                                                    n_svd_vecs,
@@ -375,15 +374,14 @@ class SpectralBiclustering(BaseSpectral):
         chosen and the algorithm runs once. Otherwise, the algorithm
         is run for each initialization and the best solution chosen.
 
-    n_jobs : int, optional, default: 1
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. This works by breaking
         down the pairwise matrix into n_jobs even slices and computing them in
         parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For n_jobs below -1,
-        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
-        are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None (default)
         Used for randomizing the singular value decomposition and the k-means
@@ -419,7 +417,7 @@ class SpectralBiclustering(BaseSpectral):
     >>> clustering # doctest: +NORMALIZE_WHITESPACE
     SpectralBiclustering(init='k-means++', method='bistochastic',
                mini_batch=False, n_best=3, n_clusters=2, n_components=6,
-               n_init=10, n_jobs=1, n_svd_vecs=None, random_state=0,
+               n_init=10, n_jobs=None, n_svd_vecs=None, random_state=0,
                svd_method='randomized')
 
     References
@@ -433,7 +431,7 @@ class SpectralBiclustering(BaseSpectral):
     def __init__(self, n_clusters=3, method='bistochastic',
                  n_components=6, n_best=3, svd_method='randomized',
                  n_svd_vecs=None, mini_batch=False, init='k-means++',
-                 n_init=10, n_jobs=1, random_state=None):
+                 n_init=10, n_jobs=None, random_state=None):
         super(SpectralBiclustering, self).__init__(n_clusters,
                                                    svd_method,
                                                    n_svd_vecs,
diff --git a/sklearn/cluster/birch.py b/sklearn/cluster/birch.py
index 2af3ab7890..0292559857 100755
--- a/sklearn/cluster/birch.py
+++ b/sklearn/cluster/birch.py
@@ -74,7 +74,7 @@ def _split_node(node, threshold, branching_factor):
 
     farthest_idx = np.unravel_index(
         dist.argmax(), (n_clusters, n_clusters))
-    node1_dist, node2_dist = dist[[farthest_idx]]
+    node1_dist, node2_dist = dist[(farthest_idx,)]
 
     node1_closer = node1_dist < node2_dist
     for idx, subcluster in enumerate(node.subclusters_):
diff --git a/sklearn/cluster/dbscan_.py b/sklearn/cluster/dbscan_.py
index 91324907f5..f10890e10f 100755
--- a/sklearn/cluster/dbscan_.py
+++ b/sklearn/cluster/dbscan_.py
@@ -20,7 +20,8 @@
 
 
 def dbscan(X, eps=0.5, min_samples=5, metric='minkowski', metric_params=None,
-           algorithm='auto', leaf_size=30, p=2, sample_weight=None, n_jobs=1):
+           algorithm='auto', leaf_size=30, p=2, sample_weight=None,
+           n_jobs=None):
     """Perform DBSCAN clustering from vector array or distance matrix.
 
     Read more in the :ref:`User Guide <dbscan>`.
@@ -75,9 +76,11 @@ def dbscan(X, eps=0.5, min_samples=5, metric='minkowski', metric_params=None,
         weight may inhibit its eps-neighbor from being core.
         Note that weights are absolute, and default to 1.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Returns
     -------
@@ -228,9 +231,11 @@ class DBSCAN(BaseEstimator, ClusterMixin):
         The power of the Minkowski metric to be used to calculate distance
         between points.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+       ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -255,7 +260,7 @@ class DBSCAN(BaseEstimator, ClusterMixin):
     array([ 0,  0,  0,  1,  1, -1])
     >>> clustering # doctest: +NORMALIZE_WHITESPACE
     DBSCAN(algorithm='auto', eps=3, leaf_size=30, metric='euclidean',
-        metric_params=None, min_samples=2, n_jobs=1, p=None)
+        metric_params=None, min_samples=2, n_jobs=None, p=None)
 
     See also
     --------
@@ -296,7 +301,7 @@ class DBSCAN(BaseEstimator, ClusterMixin):
 
     def __init__(self, eps=0.5, min_samples=5, metric='euclidean',
                  metric_params=None, algorithm='auto', leaf_size=30, p=None,
-                 n_jobs=1):
+                 n_jobs=None):
         self.eps = eps
         self.min_samples = min_samples
         self.metric = metric
diff --git a/sklearn/cluster/k_means_.py b/sklearn/cluster/k_means_.py
index db599e5d0c..c402bf6c8b 100755
--- a/sklearn/cluster/k_means_.py
+++ b/sklearn/cluster/k_means_.py
@@ -25,12 +25,13 @@
 from ..utils.sparsefuncs import mean_variance_axis
 from ..utils.validation import _num_samples
 from ..utils import check_array
-from ..utils import check_random_state
 from ..utils import gen_batches
+from ..utils import check_random_state
 from ..utils.validation import check_is_fitted
 from ..utils.validation import FLOAT_DTYPES
 from ..utils import Parallel
 from ..utils import delayed
+from ..utils import effective_n_jobs
 from ..externals.six import string_types
 from ..exceptions import ConvergenceWarning
 from . import _k_means
@@ -184,8 +185,8 @@ def _check_sample_weight(X, sample_weight):
 
 def k_means(X, n_clusters, sample_weight=None, init='k-means++',
             precompute_distances='auto', n_init=10, max_iter=300,
-            verbose=False, tol=1e-4, random_state=None, copy_x=True, n_jobs=1,
-            algorithm="auto", return_n_iter=False):
+            verbose=False, tol=1e-4, random_state=None, copy_x=True,
+            n_jobs=None, algorithm="auto", return_n_iter=False):
     """K-means clustering algorithm.
 
     Read more in the :ref:`User Guide <k_means>`.
@@ -260,14 +261,13 @@ def k_means(X, n_clusters, sample_weight=None, init='k-means++',
         the data mean, in this case it will also not ensure that data is
         C-contiguous which may cause a significant slowdown.
 
-    n_jobs : int
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. This works by computing
         each of the n_init runs in parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For n_jobs below -1,
-        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
-        are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     algorithm : "auto", "full" or "elkan", default="auto"
         K-means algorithm to use. The classical EM-style algorithm is "full".
@@ -368,7 +368,7 @@ def k_means(X, n_clusters, sample_weight=None, init='k-means++',
     else:
         raise ValueError("Algorithm must be 'auto', 'full' or 'elkan', got"
                          " %s" % str(algorithm))
-    if n_jobs == 1:
+    if effective_n_jobs(n_jobs):
         # For a single thread, less memory is needed if we just store one set
         # of the best results (as opposed to one set per run per thread).
         for it in range(n_init):
@@ -833,14 +833,13 @@ class KMeans(BaseEstimator, ClusterMixin, TransformerMixin):
         the data mean, in this case it will also not ensure that data is
         C-contiguous which may cause a significant slowdown.
 
-    n_jobs : int
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. This works by computing
         each of the n_init runs in parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For n_jobs below -1,
-        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
-        are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     algorithm : "auto", "full" or "elkan", default="auto"
         K-means algorithm to use. The classical EM-style algorithm is "full".
@@ -913,7 +912,7 @@ class KMeans(BaseEstimator, ClusterMixin, TransformerMixin):
     def __init__(self, n_clusters=8, init='k-means++', n_init=10,
                  max_iter=300, tol=1e-4, precompute_distances='auto',
                  verbose=0, random_state=None, copy_x=True,
-                 n_jobs=1, algorithm='auto'):
+                 n_jobs=None, algorithm='auto'):
 
         self.n_clusters = n_clusters
         self.init = init
diff --git a/sklearn/cluster/mean_shift_.py b/sklearn/cluster/mean_shift_.py
index edd4ee9999..487545ac03 100755
--- a/sklearn/cluster/mean_shift_.py
+++ b/sklearn/cluster/mean_shift_.py
@@ -29,7 +29,7 @@
 
 
 def estimate_bandwidth(X, quantile=0.3, n_samples=None, random_state=0,
-                       n_jobs=1):
+                       n_jobs=None):
     """Estimate the bandwidth to use with the mean-shift algorithm.
 
     That this function takes time at least quadratic in n_samples. For large
@@ -53,9 +53,11 @@ def estimate_bandwidth(X, quantile=0.3, n_samples=None, random_state=0,
         deterministic.
         See :term:`Glossary <random_state>`.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Returns
     -------
@@ -107,7 +109,7 @@ def _mean_shift_single_seed(my_mean, X, nbrs, max_iter):
 
 def mean_shift(X, bandwidth=None, seeds=None, bin_seeding=False,
                min_bin_freq=1, cluster_all=True, max_iter=300,
-               n_jobs=1):
+               n_jobs=None):
     """Perform mean shift clustering of data using a flat kernel.
 
     Read more in the :ref:`User Guide <mean_shift>`.
@@ -152,14 +154,13 @@ def mean_shift(X, bandwidth=None, seeds=None, bin_seeding=False,
         Maximum number of iterations, per seed point before the clustering
         operation terminates (for that seed point), if has not converged yet.
 
-    n_jobs : int
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. This works by computing
         each of the n_init runs in parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For n_jobs below -1,
-        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
-        are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
         .. versionadded:: 0.17
            Parallel Execution using *n_jobs*.
@@ -334,14 +335,13 @@ class MeanShift(BaseEstimator, ClusterMixin):
         not within any kernel. Orphans are assigned to the nearest kernel.
         If false, then orphans are given cluster label -1.
 
-    n_jobs : int
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. This works by computing
         each of the n_init runs in parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For n_jobs below -1,
-        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
-        are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -364,7 +364,7 @@ class MeanShift(BaseEstimator, ClusterMixin):
     array([0, 1])
     >>> clustering # doctest: +NORMALIZE_WHITESPACE
     MeanShift(bandwidth=2, bin_seeding=False, cluster_all=True, min_bin_freq=1,
-         n_jobs=1, seeds=None)
+         n_jobs=None, seeds=None)
 
     Notes
     -----
@@ -392,7 +392,7 @@ class MeanShift(BaseEstimator, ClusterMixin):
 
     """
     def __init__(self, bandwidth=None, seeds=None, bin_seeding=False,
-                 min_bin_freq=1, cluster_all=True, n_jobs=1):
+                 min_bin_freq=1, cluster_all=True, n_jobs=None):
         self.bandwidth = bandwidth
         self.seeds = seeds
         self.bin_seeding = bin_seeding
diff --git a/sklearn/cluster/optics_.py b/sklearn/cluster/optics_.py
index ba772bc545..e10a92a759 100755
--- a/sklearn/cluster/optics_.py
+++ b/sklearn/cluster/optics_.py
@@ -26,7 +26,7 @@ def optics(X, min_samples=5, max_bound=np.inf, metric='euclidean',
            rejection_ratio=.7, similarity_threshold=0.4,
            significant_min=.003, min_cluster_size_ratio=.005,
            min_maxima_ratio=0.001, algorithm='ball_tree',
-           leaf_size=30, n_jobs=1):
+           leaf_size=30, n_jobs=None):
     """Perform OPTICS clustering from vector array
 
     OPTICS: Ordering Points To Identify the Clustering Structure
@@ -118,9 +118,11 @@ def optics(X, min_samples=5, max_bound=np.inf, metric='euclidean',
         required to store the tree. The optimal value depends on the
         nature of the problem.
 
-    n_jobs : int, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Returns
     -------
@@ -243,9 +245,11 @@ class OPTICS(BaseEstimator, ClusterMixin):
         required to store the tree. The optimal value depends on the
         nature of the problem.
 
-    n_jobs : int, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -285,7 +289,7 @@ def __init__(self, min_samples=5, max_bound=np.inf, metric='euclidean',
                  rejection_ratio=.7, similarity_threshold=0.4,
                  significant_min=.003, min_cluster_size_ratio=.005,
                  min_maxima_ratio=0.001, algorithm='ball_tree',
-                 leaf_size=30, n_jobs=1):
+                 leaf_size=30, n_jobs=None):
 
         self.max_bound = max_bound
         self.min_samples = min_samples
@@ -394,7 +398,7 @@ def _set_reach_dist(self, point_index, X, nbrs):
         # Keep n_jobs = 1 in the following lines...please
         if len(unproc) > 0:
             dists = pairwise_distances(P, np.take(X, unproc, axis=0),
-                                       self.metric, n_jobs=1).ravel()
+                                       self.metric, n_jobs=None).ravel()
 
             rdists = np.maximum(dists, self.core_distances_[point_index])
             new_reach = np.minimum(np.take(self.reachability_, unproc), rdists)
diff --git a/sklearn/cluster/spectral.py b/sklearn/cluster/spectral.py
index fd4334a277..31a2046dbf 100755
--- a/sklearn/cluster/spectral.py
+++ b/sklearn/cluster/spectral.py
@@ -358,9 +358,11 @@ class SpectralClustering(BaseEstimator, ClusterMixin):
         Parameters (keyword arguments) and values for kernel passed as
         callable object. Ignored by other kernels.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -385,7 +387,7 @@ class SpectralClustering(BaseEstimator, ClusterMixin):
     >>> clustering # doctest: +NORMALIZE_WHITESPACE
     SpectralClustering(affinity='rbf', assign_labels='discretize', coef0=1,
               degree=3, eigen_solver=None, eigen_tol=0.0, gamma=1.0,
-              kernel_params=None, n_clusters=2, n_init=10, n_jobs=1,
+              kernel_params=None, n_clusters=2, n_init=10, n_jobs=None,
               n_neighbors=10, random_state=0)
 
     Notes
@@ -426,7 +428,7 @@ class SpectralClustering(BaseEstimator, ClusterMixin):
     def __init__(self, n_clusters=8, eigen_solver=None, random_state=None,
                  n_init=10, gamma=1., affinity='rbf', n_neighbors=10,
                  eigen_tol=0.0, assign_labels='kmeans', degree=3, coef0=1,
-                 kernel_params=None, n_jobs=1):
+                 kernel_params=None, n_jobs=None):
         self.n_clusters = n_clusters
         self.eigen_solver = eigen_solver
         self.random_state = random_state
diff --git a/sklearn/cluster/tests/test_optics.py b/sklearn/cluster/tests/test_optics.py
index 597785083c..2116e75bf4 100755
--- a/sklearn/cluster/tests/test_optics.py
+++ b/sklearn/cluster/tests/test_optics.py
@@ -12,9 +12,10 @@
 from sklearn.metrics.cluster import contingency_matrix
 from sklearn.cluster.dbscan_ import DBSCAN
 from sklearn.utils.testing import assert_equal, assert_warns
-from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_raise_message
+from sklearn.utils.testing import assert_allclose
+from sklearn.utils import _IS_32BIT
 
 from sklearn.cluster.tests.common import generate_clustered_data
 
@@ -405,4 +406,10 @@ def test_reach_dists():
          1.364861, 0.459580, 1.025370, 0.980304, 0.607592, 0.533907, 1.134650,
          0.446161, 0.629962]
 
-    assert_array_almost_equal(clust.reachability_, np.array(v))
+    # FIXME: known failure in 32bit Linux; numerical imprecision results in
+    # different ordering in quick_scan
+    if _IS_32BIT:  # pragma: no cover
+        assert_allclose(clust.reachability_, np.array(v), rtol=1e-2)
+    else:
+        # we compare to truncated decimals, so use atol
+        assert_allclose(clust.reachability_, np.array(v), atol=1e-5)
diff --git a/sklearn/compose/_column_transformer.py b/sklearn/compose/_column_transformer.py
index cbd81ccc61..e09d2d09d7 100755
--- a/sklearn/compose/_column_transformer.py
+++ b/sklearn/compose/_column_transformer.py
@@ -93,8 +93,11 @@ class ColumnTransformer(_BaseComposition, TransformerMixin):
         the stacked result will be sparse or dense, respectively, and this
         keyword will be ignored.
 
-    n_jobs : int, optional
-        Number of jobs to run in parallel (default 1).
+    n_jobs : int or None, optional (default=None)
+        Number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     transformer_weights : dict, optional
         Multiplicative weights for features per transformer. The output of the
@@ -157,7 +160,7 @@ class ColumnTransformer(_BaseComposition, TransformerMixin):
     """
 
     def __init__(self, transformers, remainder='drop', sparse_threshold=0.3,
-                 n_jobs=1, transformer_weights=None):
+                 n_jobs=None, transformer_weights=None):
         self.transformers = transformers
         self.remainder = remainder
         self.sparse_threshold = sparse_threshold
@@ -666,8 +669,11 @@ def make_column_transformer(*transformers, **kwargs):
         non-specified columns will use the ``remainder`` estimator. The
         estimator must support `fit` and `transform`.
 
-    n_jobs : int, optional
-        Number of jobs to run in parallel (default 1).
+    n_jobs : int or None, optional (default=None)
+        Number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Returns
     -------
@@ -687,7 +693,7 @@ def make_column_transformer(*transformers, **kwargs):
     ...     (['numerical_column'], StandardScaler()),
     ...     (['categorical_column'], OneHotEncoder()))
     ...     # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
-    ColumnTransformer(n_jobs=1, remainder='drop', sparse_threshold=0.3,
+    ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,
              transformer_weights=None,
              transformers=[('standardscaler',
                             StandardScaler(...),
@@ -697,7 +703,7 @@ def make_column_transformer(*transformers, **kwargs):
                             ['categorical_column'])])
 
     """
-    n_jobs = kwargs.pop('n_jobs', 1)
+    n_jobs = kwargs.pop('n_jobs', None)
     remainder = kwargs.pop('remainder', 'drop')
     if kwargs:
         raise TypeError('Unknown keyword arguments: "{}"'
diff --git a/sklearn/compose/tests/test_column_transformer.py b/sklearn/compose/tests/test_column_transformer.py
index e6bf557765..f67806a52c 100755
--- a/sklearn/compose/tests/test_column_transformer.py
+++ b/sklearn/compose/tests/test_column_transformer.py
@@ -458,7 +458,7 @@ def test_column_transformer_get_set_params():
     ct = ColumnTransformer([('trans1', StandardScaler(), [0]),
                             ('trans2', StandardScaler(), [1])])
 
-    exp = {'n_jobs': 1,
+    exp = {'n_jobs': None,
            'remainder': 'drop',
            'sparse_threshold': 0.3,
            'trans1': ct.transformers[0][1],
@@ -478,7 +478,7 @@ def test_column_transformer_get_set_params():
     assert_false(ct.get_params()['trans1__with_mean'])
 
     ct.set_params(trans1='passthrough')
-    exp = {'n_jobs': 1,
+    exp = {'n_jobs': None,
            'remainder': 'drop',
            'sparse_threshold': 0.3,
            'trans1': 'passthrough',
@@ -662,6 +662,7 @@ def test_column_transformer_remainder():
     ct = make_column_transformer(([0], Trans()))
     assert ct.remainder == 'drop'
 
+
 @pytest.mark.parametrize("key", [[0], np.array([0]), slice(0, 1),
                                  np.array([True, False])])
 def test_column_transformer_remainder_numpy(key):
@@ -806,7 +807,7 @@ def test_column_transformer_get_set_params_with_remainder():
     ct = ColumnTransformer([('trans1', StandardScaler(), [0])],
                            remainder=StandardScaler())
 
-    exp = {'n_jobs': 1,
+    exp = {'n_jobs': None,
            'remainder': ct.remainder,
            'remainder__copy': True,
            'remainder__with_mean': True,
@@ -825,7 +826,7 @@ def test_column_transformer_get_set_params_with_remainder():
     assert not ct.get_params()['remainder__with_std']
 
     ct.set_params(trans1='passthrough')
-    exp = {'n_jobs': 1,
+    exp = {'n_jobs': None,
            'remainder': ct.remainder,
            'remainder__copy': True,
            'remainder__with_mean': True,
diff --git a/sklearn/covariance/elliptic_envelope.py b/sklearn/covariance/elliptic_envelope.py
index 7ea17b65aa..a150c032ed 100755
--- a/sklearn/covariance/elliptic_envelope.py
+++ b/sklearn/covariance/elliptic_envelope.py
@@ -106,8 +106,7 @@ def fit(self, X, y=None):
         y : (ignored)
         """
         super(EllipticEnvelope, self).fit(X)
-        self.offset_ = sp.stats.scoreatpercentile(
-            -self.dist_, 100. * self.contamination)
+        self.offset_ = np.percentile(-self.dist_, 100. * self.contamination)
         return self
 
     def decision_function(self, X, raw_values=None):
diff --git a/sklearn/covariance/graph_lasso_.py b/sklearn/covariance/graph_lasso_.py
index 0837acf4a3..b10e3c7f3f 100755
--- a/sklearn/covariance/graph_lasso_.py
+++ b/sklearn/covariance/graph_lasso_.py
@@ -520,8 +520,11 @@ class GraphicalLassoCV(GraphicalLasso):
         than number of samples. Elsewhere prefer cd which is more numerically
         stable.
 
-    n_jobs : int, optional
-        number of jobs to run in parallel (default 1).
+    n_jobs : int or None, optional (default=None)
+        number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : boolean, optional
         If verbose is True, the objective function and duality gap are
@@ -571,7 +574,7 @@ class GraphicalLassoCV(GraphicalLasso):
     """
 
     def __init__(self, alphas=4, n_refinements=4, cv='warn', tol=1e-4,
-                 enet_tol=1e-4, max_iter=100, mode='cd', n_jobs=1,
+                 enet_tol=1e-4, max_iter=100, mode='cd', n_jobs=None,
                  verbose=False, assume_centered=False):
         super(GraphicalLassoCV, self).__init__(
             mode=mode, tol=tol, verbose=verbose, enet_tol=enet_tol,
@@ -927,8 +930,11 @@ class GraphLassoCV(GraphicalLassoCV):
         than number of samples. Elsewhere prefer cd which is more numerically
         stable.
 
-    n_jobs : int, optional
-        number of jobs to run in parallel (default 1).
+    n_jobs : int or None, optional (default=None)
+        number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : boolean, optional
         If verbose is True, the objective function and duality gap are
diff --git a/sklearn/cross_decomposition/pls_.py b/sklearn/cross_decomposition/pls_.py
index 7566ee49d2..df7cb22b89 100755
--- a/sklearn/cross_decomposition/pls_.py
+++ b/sklearn/cross_decomposition/pls_.py
@@ -775,6 +775,25 @@ class PLSSVD(BaseEstimator, TransformerMixin):
     y_scores_ : array, [n_samples, n_components]
         Y scores.
 
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.cross_decomposition import PLSSVD
+    >>> X = np.array([[0., 0., 1.],
+    ...     [1.,0.,0.],
+    ...     [2.,2.,2.],
+    ...     [2.,5.,4.]])
+    >>> Y = np.array([[0.1, -0.2],
+    ...     [0.9, 1.1],
+    ...     [6.2, 5.9],
+    ...     [11.9, 12.3]])
+    >>> plsca = PLSSVD(n_components=2)
+    >>> plsca.fit(X, Y)
+    PLSSVD(copy=True, n_components=2, scale=True)
+    >>> X_c, Y_c = plsca.transform(X, Y)
+    >>> X_c.shape, Y_c.shape
+    ((4, 2), (4, 2))
+
     See also
     --------
     PLSCanonical
diff --git a/sklearn/datasets/__init__.py b/sklearn/datasets/__init__.py
index c43c0c4758..c7d78e6334 100755
--- a/sklearn/datasets/__init__.py
+++ b/sklearn/datasets/__init__.py
@@ -23,6 +23,7 @@
 from .twenty_newsgroups import fetch_20newsgroups
 from .twenty_newsgroups import fetch_20newsgroups_vectorized
 from .mldata import fetch_mldata, mldata_filename
+from .openml import fetch_openml
 from .samples_generator import make_classification
 from .samples_generator import make_multilabel_classification
 from .samples_generator import make_hastie_10_2
@@ -65,6 +66,7 @@
            'fetch_covtype',
            'fetch_rcv1',
            'fetch_kddcup99',
+           'fetch_openml',
            'get_data_home',
            'load_boston',
            'load_diabetes',
diff --git a/sklearn/datasets/mldata.py b/sklearn/datasets/mldata.py
index 1416208584..5948d04a8b 100755
--- a/sklearn/datasets/mldata.py
+++ b/sklearn/datasets/mldata.py
@@ -25,13 +25,19 @@
 
 from .base import get_data_home
 from ..utils import Bunch
+from ..utils import deprecated
 
 MLDATA_BASE_URL = "http://mldata.org/repository/data/download/matlab/%s"
 
 
+@deprecated('mldata_filename was deprecated in version 0.20 and will be '
+            'removed in version 0.22')
 def mldata_filename(dataname):
     """Convert a raw name for a data set in a mldata.org filename.
 
+    .. deprecated:: 0.20
+        Will be removed in version 0.22
+
     Parameters
     ----------
     dataname : str
@@ -46,10 +52,14 @@ def mldata_filename(dataname):
     return re.sub(r'[().]', '', dataname)
 
 
+@deprecated('fetch_mldata was deprecated in version 0.20 and will be removed '
+            'in version 0.22')
 def fetch_mldata(dataname, target_name='label', data_name='data',
                  transpose_data=True, data_home=None):
     """Fetch an mldata.org data set
 
+    mldata.org is no longer operational.
+
     If the file does not exist yet, it is downloaded from mldata.org .
 
     mldata.org does not have an enforced convention for storing data or
@@ -70,6 +80,9 @@ def fetch_mldata(dataname, target_name='label', data_name='data',
     mldata.org data sets may have multiple columns, which are stored in the
     Bunch object with their original name.
 
+    .. deprecated:: 0.20
+        Will be removed in version 0.22
+
     Parameters
     ----------
 
@@ -99,40 +112,6 @@ def fetch_mldata(dataname, target_name='label', data_name='data',
         'data', the data to learn, 'target', the classification labels,
         'DESCR', the full description of the dataset, and
         'COL_NAMES', the original names of the dataset columns.
-
-    Examples
-    --------
-    Load the 'iris' dataset from mldata.org:
-
-    >>> from sklearn.datasets.mldata import fetch_mldata
-    >>> import tempfile
-    >>> test_data_home = tempfile.mkdtemp()
-
-    >>> iris = fetch_mldata('iris', data_home=test_data_home)
-    >>> iris.target.shape
-    (150,)
-    >>> iris.data.shape
-    (150, 4)
-
-    Load the 'leukemia' dataset from mldata.org, which needs to be transposed
-    to respects the scikit-learn axes convention:
-
-    >>> leuk = fetch_mldata('leukemia', transpose_data=True,
-    ...                     data_home=test_data_home)
-    >>> leuk.data.shape
-    (72, 7129)
-
-    Load an alternative 'iris' dataset, which has different names for the
-    columns:
-
-    >>> iris2 = fetch_mldata('datasets-UCI iris', target_name=1,
-    ...                      data_name=0, data_home=test_data_home)
-    >>> iris3 = fetch_mldata('datasets-UCI iris',
-    ...                      target_name='class', data_name='double0',
-    ...                      data_home=test_data_home)
-
-    >>> import shutil
-    >>> shutil.rmtree(test_data_home)
     """
 
     # normalize dataset name
diff --git a/sklearn/datasets/openml.py b/sklearn/datasets/openml.py
new file mode 100755
index 0000000000..a35fa51307
--- /dev/null
+++ b/sklearn/datasets/openml.py
@@ -0,0 +1,575 @@
+import gzip
+import json
+import os
+import shutil
+from os.path import join
+from warnings import warn
+
+try:
+    # Python 3+
+    from urllib.request import urlopen
+except ImportError:
+    # Python 2
+    from urllib2 import urlopen
+
+
+import numpy as np
+import scipy.sparse
+
+from sklearn.externals import _arff
+from .base import get_data_home
+from ..externals.six import string_types, PY2
+from ..externals.six.moves.urllib.error import HTTPError
+from ..utils import Bunch
+
+__all__ = ['fetch_openml']
+
+_OPENML_PREFIX = "https://openml.org/"
+_SEARCH_NAME = "api/v1/json/data/list/data_name/{}/limit/2"
+_DATA_INFO = "api/v1/json/data/{}"
+_DATA_FEATURES = "api/v1/json/data/features/{}"
+_DATA_FILE = "data/v1/download/{}"
+
+
+def _open_openml_url(openml_path, data_home):
+    """
+    Returns a resource from OpenML.org. Caches it to data_home if required.
+
+    Parameters
+    ----------
+    openml_path : str
+        OpenML URL that will be accessed. This will be prefixes with
+        _OPENML_PREFIX
+
+    data_home : str
+        Directory to which the files will be cached. If None, no caching will
+        be applied.
+
+    Returns
+    -------
+    result : stream
+        A stream to the OpenML resource
+    """
+    if data_home is None:
+        return urlopen(_OPENML_PREFIX + openml_path)
+    local_path = os.path.join(data_home, 'openml.org', openml_path + ".gz")
+    if not os.path.exists(local_path):
+        try:
+            os.makedirs(os.path.dirname(local_path))
+        except OSError:
+            # potentially, the directory has been created already
+            pass
+
+        try:
+            with gzip.GzipFile(local_path, 'wb') as fdst:
+                fsrc = urlopen(_OPENML_PREFIX + openml_path)
+                shutil.copyfileobj(fsrc, fdst)
+                fsrc.close()
+        except Exception:
+            os.unlink(local_path)
+            raise
+    # XXX: unnecessary decompression on first access
+    return gzip.GzipFile(local_path, 'rb')
+
+
+def _get_json_content_from_openml_api(url, error_message, raise_if_error,
+                                      data_home):
+    """
+    Loads json data from the openml api
+
+    Parameters
+    ----------
+    url : str
+        The URL to load from. Should be an official OpenML endpoint
+
+    error_message : str or None
+        The error message to raise if an acceptable OpenML error is thrown
+        (acceptable error is, e.g., data id not found. Other errors, like 404's
+        will throw the native error message)
+
+    raise_if_error : bool
+        Whether to raise an error if OpenML returns an acceptable error (e.g.,
+        date not found). If this argument is set to False, a None is returned
+        in case of acceptable errors. Note that all other errors (e.g., 404)
+        will still be raised as normal.
+
+    data_home : str or None
+        Location to cache the response. None if no cache is required.
+
+    Returns
+    -------
+    json_data : json or None
+        the json result from the OpenML server if the call was successful;
+        None otherwise iff raise_if_error was set to False and the error was
+        ``acceptable``
+    """
+    data_found = True
+    try:
+        response = _open_openml_url(url, data_home)
+    except HTTPError as error:
+        # 412 is an OpenML specific error code, indicating a generic error
+        # (e.g., data not found)
+        if error.code == 412:
+            data_found = False
+        else:
+            raise error
+    if not data_found:
+        # not in except for nicer traceback
+        if raise_if_error:
+            raise ValueError(error_message)
+        else:
+            return None
+    json_data = json.loads(response.read().decode("utf-8"))
+    response.close()
+    return json_data
+
+
+def _split_sparse_columns(arff_data, include_columns):
+    """
+    obtains several columns from sparse arff representation. Additionally, the
+    column indices are re-labelled, given the columns that are not included.
+    (e.g., when including [1, 2, 3], the columns will be relabelled to
+    [0, 1, 2])
+
+    Parameters
+    ----------
+    arff_data : tuple
+        A tuple of three lists of equal size; first list indicating the value,
+        second the x coordinate and the third the y coordinate.
+
+    include_columns : list
+        A list of columns to include.
+
+    Returns
+    -------
+    arff_data_new : tuple
+        Subset of arff data with only the include columns indicated by the
+        include_columns argument.
+    """
+    arff_data_new = (list(), list(), list())
+    reindexed_columns = {column_idx: array_idx for array_idx, column_idx
+                         in enumerate(include_columns)}
+    for val, row_idx, col_idx in zip(arff_data[0], arff_data[1], arff_data[2]):
+        if col_idx in include_columns:
+            arff_data_new[0].append(val)
+            arff_data_new[1].append(row_idx)
+            arff_data_new[2].append(reindexed_columns[col_idx])
+    return arff_data_new
+
+
+def _sparse_data_to_array(arff_data, include_columns):
+    # turns the sparse data back into an array (can't use toarray() function,
+    # as this does only work on numeric data)
+    num_obs = max(arff_data[1]) + 1
+    y_shape = (num_obs, len(include_columns))
+    reindexed_columns = {column_idx: array_idx for array_idx, column_idx
+                         in enumerate(include_columns)}
+    # TODO: improve for efficiency
+    y = np.empty(y_shape, dtype=np.float64)
+    for val, row_idx, col_idx in zip(arff_data[0], arff_data[1], arff_data[2]):
+        if col_idx in include_columns:
+            y[row_idx, reindexed_columns[col_idx]] = val
+    return y
+
+
+def _convert_arff_data(arff_data, col_slice_x, col_slice_y):
+    """
+    converts the arff object into the appropriate matrix type (np.array or
+    scipy.sparse.csr_matrix) based on the 'data part' (i.e., in the
+    liac-arff dict, the object from the 'data' key)
+
+    Parameters
+    ----------
+    arff_data : list or dict
+        as obtained from liac-arff object
+
+    col_slice_x : list
+        The column indices that are sliced from the original array to return
+        as X data
+
+    col_slice_y : list
+        The column indices that are sliced from the original array to return
+        as y data
+
+    Returns
+    -------
+    X : np.array or scipy.sparse.csr_matrix
+    y : np.array
+    """
+    if isinstance(arff_data, list):
+        data = np.array(arff_data, dtype=np.float64)
+        X = np.array(data[:, col_slice_x], dtype=np.float64)
+        y = np.array(data[:, col_slice_y], dtype=np.float64)
+        return X, y
+    elif isinstance(arff_data, tuple):
+        arff_data_X = _split_sparse_columns(arff_data, col_slice_x)
+        num_obs = max(arff_data[1]) + 1
+        X_shape = (num_obs, len(col_slice_x))
+        X = scipy.sparse.coo_matrix(
+            (arff_data_X[0], (arff_data_X[1], arff_data_X[2])),
+            shape=X_shape, dtype=np.float64)
+        X = X.tocsr()
+        y = _sparse_data_to_array(arff_data, col_slice_y)
+        return X, y
+    else:
+        # This should never happen
+        raise ValueError('Unexpected Data Type obtained from arff.')
+
+
+def _get_data_info_by_name(name, version, data_home):
+    """
+    Utilizes the openml dataset listing api to find a dataset by
+    name/version
+    OpenML api function:
+    https://www.openml.org/api_docs#!/data/get_data_list_data_name_data_name
+
+    Parameters
+    ----------
+    name : str
+        name of the dataset
+
+    version : int or str
+        If version is an integer, the exact name/version will be obtained from
+        OpenML. If version is a string (value: "active") it will take the first
+        version from OpenML that is annotated as active. Any other string
+        values except "active" are treated as integer.
+
+    data_home : str or None
+        Location to cache the response. None if no cache is required.
+
+    Returns
+    -------
+    first_dataset : json
+        json representation of the first dataset object that adhired to the
+        search criteria
+
+    """
+    if version == "active":
+        # situation in which we return the oldest active version
+        url = _SEARCH_NAME.format(name) + "/status/active/"
+        error_msg = "No active dataset {} found.".format(name)
+        json_data = _get_json_content_from_openml_api(url, error_msg, True,
+                                                      data_home)
+        res = json_data['data']['dataset']
+        if len(res) > 1:
+            warn("Multiple active versions of the dataset matching the name"
+                 " {name} exist. Versions may be fundamentally different, "
+                 "returning version"
+                 " {version}.".format(name=name, version=res[0]['version']))
+        return res[0]
+
+    # an integer version has been provided
+    url = (_SEARCH_NAME + "/data_version/{}").format(name, version)
+    json_data = _get_json_content_from_openml_api(url, None, False,
+                                                  data_home)
+    if json_data is None:
+        # we can do this in 1 function call if OpenML does not require the
+        # specification of the dataset status (i.e., return datasets with a
+        # given name / version regardless of active, deactivated, etc. )
+        # TODO: feature request OpenML.
+        url += "/status/deactivated"
+        error_msg = "Dataset {} with version {} not found.".format(name,
+                                                                   version)
+        json_data = _get_json_content_from_openml_api(url, error_msg, True,
+                                                      data_home)
+
+    return json_data['data']['dataset'][0]
+
+
+def _get_data_description_by_id(data_id, data_home):
+    # OpenML API function: https://www.openml.org/api_docs#!/data/get_data_id
+    url = _DATA_INFO.format(data_id)
+    error_message = "Dataset with data_id {} not found.".format(data_id)
+    json_data = _get_json_content_from_openml_api(url, error_message, True,
+                                                  data_home)
+    return json_data['data_set_description']
+
+
+def _get_data_features(data_id, data_home):
+    # OpenML function:
+    # https://www.openml.org/api_docs#!/data/get_data_features_id
+    url = _DATA_FEATURES.format(data_id)
+    error_message = "Dataset with data_id {} not found.".format(data_id)
+    json_data = _get_json_content_from_openml_api(url, error_message, True,
+                                                  data_home)
+    return json_data['data_features']['feature']
+
+
+def _download_data_arff(file_id, sparse, data_home, encode_nominal=True):
+    # Accesses an ARFF file on the OpenML server. Documentation:
+    # https://www.openml.org/api_data_docs#!/data/get_download_id
+    # encode_nominal argument is to ensure unit testing, do not alter in
+    # production!
+    url = _DATA_FILE.format(file_id)
+    response = _open_openml_url(url, data_home)
+    if sparse is True:
+        return_type = _arff.COO
+    else:
+        return_type = _arff.DENSE
+
+    if PY2:
+        arff_file = _arff.load(response, encode_nominal=encode_nominal,
+                               return_type=return_type, )
+    else:
+        arff_file = _arff.loads(response.read().decode('utf-8'),
+                                encode_nominal=encode_nominal,
+                                return_type=return_type)
+    response.close()
+    return arff_file
+
+
+def _verify_target_data_type(features_dict, target_columns):
+    # verifies the data type of the y array in case there are multiple targets
+    # (throws an error if these targets do not comply with sklearn support)
+    if not isinstance(target_columns, list):
+        raise ValueError('target_column should be list, '
+                         'got: %s' % type(target_columns))
+    found_types = set()
+    for target_column in target_columns:
+        if target_column not in features_dict:
+            raise KeyError('Could not find target_column={}')
+        if features_dict[target_column]['data_type'] == "numeric":
+            found_types.add(np.float64)
+        else:
+            found_types.add(object)
+
+        # note: we compare to a string, not boolean
+        if features_dict[target_column]['is_ignore'] == 'true':
+            warn('target_column={} has flag is_ignore.'.format(
+                target_column))
+        if features_dict[target_column]['is_row_identifier'] == 'true':
+            warn('target_column={} has flag is_row_identifier.'.format(
+                target_column))
+    if len(found_types) > 1:
+        raise ValueError('Can only handle homogeneous multi-target datasets, '
+                         'i.e., all targets are either numeric or '
+                         'categorical.')
+
+
+def fetch_openml(name=None, version='active', data_id=None, data_home=None,
+                 target_column='default-target', cache=True, return_X_y=False):
+    """Fetch dataset from openml by name or dataset id.
+
+    Datasets are uniquely identified by either an integer ID or by a
+    combination of name and version (i.e. there might be multiple
+    versions of the 'iris' dataset). Please give either name or data_id
+    (not both). In case a name is given, a version can also be
+    provided.
+
+    .. note:: EXPERIMENTAL
+
+        The API is experimental in version 0.20 (particularly the return value
+        structure), and might have small backward-incompatible changes in
+        future releases.
+
+    Parameters
+    ----------
+    name : str or None
+        String identifier of the dataset. Note that OpenML can have multiple
+        datasets with the same name.
+
+    version : integer or 'active', default='active'
+        Version of the dataset. Can only be provided if also ``name`` is given.
+        If 'active' the oldest version that's still active is used. Since
+        there may be more than one active version of a dataset, and those
+        versions may fundamentally be different from one another, setting an
+        exact version is highly recommended.
+
+    data_id : int or None
+        OpenML ID of the dataset. The most specific way of retrieving a
+        dataset. If data_id is not given, name (and potential version) are
+        used to obtain a dataset.
+
+    data_home : string or None, default None
+        Specify another download and cache folder for the data sets. By default
+        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.
+
+    target_column : string, list or None, default 'default-target'
+        Specify the column name in the data to use as target. If
+        'default-target', the standard target column a stored on the server
+        is used. If ``None``, all columns are returned as data and the
+        target is ``None``. If list (of strings), all columns with these names
+        are returned as multi-target (Note: not all scikit-learn classifiers
+        can handle all types of multi-output combinations)
+
+    cache : boolean, default=True
+        Whether to cache downloaded datasets using joblib.
+
+    return_X_y : boolean, default=False.
+        If True, returns ``(data, target)`` instead of a Bunch object. See
+        below for more information about the `data` and `target` objects.
+
+    Returns
+    -------
+
+    data : Bunch
+        Dictionary-like object, with attributes:
+
+        data : np.array or scipy.sparse.csr_matrix of floats
+            The feature matrix. Categorical features are encoded as ordinals.
+        target : np.array
+            The regression target or classification labels, if applicable.
+            Dtype is float if numeric, and object if categorical.
+        DESCR : str
+            The full description of the dataset
+        feature_names : list
+            The names of the dataset columns
+        categories : dict
+            Maps each categorical feature name to a list of values, such
+            that the value encoded as i is ith in the list.
+        details : dict
+            More metadata from OpenML
+
+    (data, target) : tuple if ``return_X_y`` is True
+
+        .. note:: EXPERIMENTAL
+
+            This interface is **experimental** as at version 0.20 and
+            subsequent releases may change attributes without notice
+            (although there should only be minor changes to ``data``
+            and ``target``).
+
+        Missing values in the 'data' are represented as NaN's. Missing values
+        in 'target' are represented as NaN's (numerical target) or None
+        (categorical target)
+    """
+    data_home = get_data_home(data_home=data_home)
+    data_home = join(data_home, 'openml')
+    if cache is False:
+        # no caching will be applied
+        data_home = None
+
+    # check valid function arguments. data_id XOR (name, version) should be
+    # provided
+    if name is not None:
+        # OpenML is case-insensitive, but the caching mechanism is not
+        # convert all data names (str) to lower case
+        name = name.lower()
+        if data_id is not None:
+            raise ValueError(
+                "Dataset data_id={} and name={} passed, but you can only "
+                "specify a numeric data_id or a name, not "
+                "both.".format(data_id, name))
+        data_info = _get_data_info_by_name(name, version, data_home)
+        data_id = data_info['did']
+    elif data_id is not None:
+        # from the previous if statement, it is given that name is None
+        if version is not "active":
+            raise ValueError(
+                "Dataset data_id={} and version={} passed, but you can only "
+                "specify a numeric data_id or a version, not "
+                "both.".format(data_id, name))
+    else:
+        raise ValueError(
+            "Neither name nor data_id are provided. Please provide name or "
+            "data_id.")
+
+    data_description = _get_data_description_by_id(data_id, data_home)
+    if data_description['status'] != "active":
+        warn("Version {} of dataset {} is inactive, meaning that issues have "
+             "been found in the dataset. Try using a newer version from "
+             "this URL: {}".format(
+                data_description['version'],
+                data_description['name'],
+                data_description['url']))
+
+    # download data features, meta-info about column types
+    features_list = _get_data_features(data_id, data_home)
+
+    for feature in features_list:
+        if 'true' in (feature['is_ignore'], feature['is_row_identifier']):
+            continue
+        if feature['data_type'] == 'string':
+            raise ValueError('STRING attributes are not yet supported')
+
+    if target_column == "default-target":
+        # determines the default target based on the data feature results
+        # (which is currently more reliable than the data description;
+        # see issue: https://github.com/openml/OpenML/issues/768)
+        target_column = [feature['name'] for feature in features_list
+                         if feature['is_target'] == 'true']
+    elif isinstance(target_column, string_types):
+        # for code-simplicity, make target_column by default a list
+        target_column = [target_column]
+    elif target_column is None:
+        target_column = []
+    elif not isinstance(target_column, list):
+        raise TypeError("Did not recognize type of target_column"
+                        "Should be six.string_type, list or None. Got: "
+                        "{}".format(type(target_column)))
+    data_columns = [feature['name'] for feature in features_list
+                    if (feature['name'] not in target_column and
+                        feature['is_ignore'] != 'true' and
+                        feature['is_row_identifier'] != 'true')]
+
+    # prepare which columns and data types should be returned for the X and y
+    features_dict = {feature['name']: feature for feature in features_list}
+
+    # XXX: col_slice_y should be all nominal or all numeric
+    _verify_target_data_type(features_dict, target_column)
+
+    col_slice_y = [int(features_dict[col_name]['index'])
+                   for col_name in target_column]
+
+    col_slice_x = [int(features_dict[col_name]['index'])
+                   for col_name in data_columns]
+    for col_idx in col_slice_y:
+        feat = features_list[col_idx]
+        nr_missing = int(feat['number_of_missing_values'])
+        if nr_missing > 0:
+            raise ValueError('Target column {} has {} missing values. '
+                             'Missing values are not supported for target '
+                             'columns. '.format(feat['name'], nr_missing))
+
+    # determine arff encoding to return
+    return_sparse = False
+    if data_description['format'].lower() == 'sparse_arff':
+        return_sparse = True
+
+    # obtain the data
+    arff = _download_data_arff(data_description['file_id'], return_sparse,
+                               data_home)
+    arff_data = arff['data']
+    nominal_attributes = {k: v for k, v in arff['attributes']
+                          if isinstance(v, list)}
+    for feature in features_list:
+        if 'true' in (feature['is_row_identifier'],
+                      feature['is_ignore']) and (feature['name'] not in
+                                                 target_column):
+            del nominal_attributes[feature['name']]
+    X, y = _convert_arff_data(arff_data, col_slice_x, col_slice_y)
+
+    is_classification = {col_name in nominal_attributes
+                         for col_name in target_column}
+    if not is_classification:
+        # No target
+        pass
+    elif all(is_classification):
+        y = np.hstack([np.take(np.asarray(nominal_attributes.pop(col_name),
+                                          dtype='O'),
+                               y[:, i:i+1].astype(int))
+                       for i, col_name in enumerate(target_column)])
+    elif any(is_classification):
+        raise ValueError('Mix of nominal and non-nominal targets is not '
+                         'currently supported')
+
+    description = u"{}\n\nDownloaded from openml.org.".format(
+        data_description.pop('description'))
+
+    # reshape y back to 1-D array, if there is only 1 target column; back
+    # to None if there are not target columns
+    if y.shape[1] == 1:
+        y = y.reshape((-1,))
+    elif y.shape[1] == 0:
+        y = None
+
+    if return_X_y:
+        return X, y
+
+    bunch = Bunch(
+        data=X, target=y, feature_names=data_columns,
+        DESCR=description, details=data_description,
+        categories=nominal_attributes,
+        url="https://www.openml.org/d/{}".format(data_id))
+
+    return bunch
diff --git a/sklearn/datasets/tests/data/openml/2/api-v1-json-data-2.json.gz b/sklearn/datasets/tests/data/openml/2/api-v1-json-data-2.json.gz
new file mode 100755
index 0000000000..22dfb6ff61
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/2/api-v1-json-data-2.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/2/api-v1-json-data-features-2.json.gz b/sklearn/datasets/tests/data/openml/2/api-v1-json-data-features-2.json.gz
new file mode 100755
index 0000000000..cb3d275009
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/2/api-v1-json-data-features-2.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/2/api-v1-json-data-list-data_name-anneal-limit-2-data_version-1.json.gz b/sklearn/datasets/tests/data/openml/2/api-v1-json-data-list-data_name-anneal-limit-2-data_version-1.json.gz
new file mode 100755
index 0000000000..a95a8131dd
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/2/api-v1-json-data-list-data_name-anneal-limit-2-data_version-1.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/2/api-v1-json-data-list-data_name-anneal-limit-2-status-active-.json.gz b/sklearn/datasets/tests/data/openml/2/api-v1-json-data-list-data_name-anneal-limit-2-status-active-.json.gz
new file mode 100755
index 0000000000..e85c1b5ff9
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/2/api-v1-json-data-list-data_name-anneal-limit-2-status-active-.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/2/data-v1-download-1666876.arff.gz b/sklearn/datasets/tests/data/openml/2/data-v1-download-1666876.arff.gz
new file mode 100755
index 0000000000..cdf3254add
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/2/data-v1-download-1666876.arff.gz differ
diff --git a/sklearn/datasets/tests/data/openml/292/api-v1-json-data-292.json.gz b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-292.json.gz
new file mode 100755
index 0000000000..888140f92b
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-292.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/292/api-v1-json-data-40981.json.gz b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-40981.json.gz
new file mode 100755
index 0000000000..888140f92b
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-40981.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/292/api-v1-json-data-features-292.json.gz b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-features-292.json.gz
new file mode 100755
index 0000000000..29016cc36b
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-features-292.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/292/api-v1-json-data-features-40981.json.gz b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-features-40981.json.gz
new file mode 100755
index 0000000000..29016cc36b
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-features-40981.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/292/api-v1-json-data-list-data_name-australian-limit-2-data_version-1-status-deactivated.json.gz b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-list-data_name-australian-limit-2-data_version-1-status-deactivated.json.gz
new file mode 100755
index 0000000000..8cb61626e1
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-list-data_name-australian-limit-2-data_version-1-status-deactivated.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/292/api-v1-json-data-list-data_name-australian-limit-2-data_version-1.json.gz b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-list-data_name-australian-limit-2-data_version-1.json.gz
new file mode 100755
index 0000000000..0e2c4395f1
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-list-data_name-australian-limit-2-data_version-1.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/292/api-v1-json-data-list-data_name-australian-limit-2-status-active-.json.gz b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-list-data_name-australian-limit-2-status-active-.json.gz
new file mode 100755
index 0000000000..b91949b9e4
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/292/api-v1-json-data-list-data_name-australian-limit-2-status-active-.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/292/data-v1-download-49822.arff.gz b/sklearn/datasets/tests/data/openml/292/data-v1-download-49822.arff.gz
new file mode 100755
index 0000000000..6821829e1e
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/292/data-v1-download-49822.arff.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-40589.json.gz b/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-40589.json.gz
new file mode 100755
index 0000000000..9c71553ce5
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-40589.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-features-40589.json.gz b/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-features-40589.json.gz
new file mode 100755
index 0000000000..155460906a
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-features-40589.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-list-data_name-emotions-limit-2-data_version-3.json.gz b/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-list-data_name-emotions-limit-2-data_version-3.json.gz
new file mode 100755
index 0000000000..01e6648a91
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-list-data_name-emotions-limit-2-data_version-3.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-list-data_name-emotions-limit-2-status-active-.json.gz b/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-list-data_name-emotions-limit-2-status-active-.json.gz
new file mode 100755
index 0000000000..0fc8d5ba1f
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40589/api-v1-json-data-list-data_name-emotions-limit-2-status-active-.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40589/data-v1-download-4644182.arff.gz b/sklearn/datasets/tests/data/openml/40589/data-v1-download-4644182.arff.gz
new file mode 100755
index 0000000000..96ed11d969
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40589/data-v1-download-4644182.arff.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-40675.json.gz b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-40675.json.gz
new file mode 100755
index 0000000000..42b876f0a4
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-40675.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-features-40675.json.gz b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-features-40675.json.gz
new file mode 100755
index 0000000000..2d5c6f8a30
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-features-40675.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-list-data_name-glass2-limit-2-data_version-1-status-deactivated.json.gz b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-list-data_name-glass2-limit-2-data_version-1-status-deactivated.json.gz
new file mode 100755
index 0000000000..f038de4196
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-list-data_name-glass2-limit-2-data_version-1-status-deactivated.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-list-data_name-glass2-limit-2-data_version-1.json.gz b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-list-data_name-glass2-limit-2-data_version-1.json.gz
new file mode 100755
index 0000000000..df1665b1db
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-list-data_name-glass2-limit-2-data_version-1.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-list-data_name-glass2-limit-2-status-active-.json.gz b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-list-data_name-glass2-limit-2-status-active-.json.gz
new file mode 100755
index 0000000000..ff46d678f6
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40675/api-v1-json-data-list-data_name-glass2-limit-2-status-active-.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40675/data-v1-download-4965250.arff.gz b/sklearn/datasets/tests/data/openml/40675/data-v1-download-4965250.arff.gz
new file mode 100755
index 0000000000..c59c3b769e
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40675/data-v1-download-4965250.arff.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40945/api-v1-json-data-40945.json.gz b/sklearn/datasets/tests/data/openml/40945/api-v1-json-data-40945.json.gz
new file mode 100755
index 0000000000..aaafa4a2de
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40945/api-v1-json-data-40945.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40945/api-v1-json-data-features-40945.json.gz b/sklearn/datasets/tests/data/openml/40945/api-v1-json-data-features-40945.json.gz
new file mode 100755
index 0000000000..24cb46957f
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40945/api-v1-json-data-features-40945.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-40966.json.gz b/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-40966.json.gz
new file mode 100755
index 0000000000..02b25d717f
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-40966.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-features-40966.json.gz b/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-features-40966.json.gz
new file mode 100755
index 0000000000..a372f9a7be
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-features-40966.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-list-data_name-miceprotein-limit-2-data_version-4.json.gz b/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-list-data_name-miceprotein-limit-2-data_version-4.json.gz
new file mode 100755
index 0000000000..0931e0b2da
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-list-data_name-miceprotein-limit-2-data_version-4.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-list-data_name-miceprotein-limit-2-status-active-.json.gz b/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-list-data_name-miceprotein-limit-2-status-active-.json.gz
new file mode 100755
index 0000000000..190571cb65
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40966/api-v1-json-data-list-data_name-miceprotein-limit-2-status-active-.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/40966/data-v1-download-17928620.arff.gz b/sklearn/datasets/tests/data/openml/40966/data-v1-download-17928620.arff.gz
new file mode 100755
index 0000000000..43ec977bf6
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/40966/data-v1-download-17928620.arff.gz differ
diff --git a/sklearn/datasets/tests/data/openml/561/api-v1-json-data-561.json.gz b/sklearn/datasets/tests/data/openml/561/api-v1-json-data-561.json.gz
new file mode 100755
index 0000000000..e4df6060ca
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/561/api-v1-json-data-561.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/561/api-v1-json-data-features-561.json.gz b/sklearn/datasets/tests/data/openml/561/api-v1-json-data-features-561.json.gz
new file mode 100755
index 0000000000..54a3ab6a7a
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/561/api-v1-json-data-features-561.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/561/api-v1-json-data-list-data_name-cpu-limit-2-data_version-1.json.gz b/sklearn/datasets/tests/data/openml/561/api-v1-json-data-list-data_name-cpu-limit-2-data_version-1.json.gz
new file mode 100755
index 0000000000..872c5a8205
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/561/api-v1-json-data-list-data_name-cpu-limit-2-data_version-1.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/561/api-v1-json-data-list-data_name-cpu-limit-2-status-active-.json.gz b/sklearn/datasets/tests/data/openml/561/api-v1-json-data-list-data_name-cpu-limit-2-status-active-.json.gz
new file mode 100755
index 0000000000..99a631470e
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/561/api-v1-json-data-list-data_name-cpu-limit-2-status-active-.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/561/data-v1-download-52739.arff.gz b/sklearn/datasets/tests/data/openml/561/data-v1-download-52739.arff.gz
new file mode 100755
index 0000000000..eeb088c224
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/561/data-v1-download-52739.arff.gz differ
diff --git a/sklearn/datasets/tests/data/openml/61/api-v1-json-data-61.json.gz b/sklearn/datasets/tests/data/openml/61/api-v1-json-data-61.json.gz
new file mode 100755
index 0000000000..83c3ececcf
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/61/api-v1-json-data-61.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/61/api-v1-json-data-features-61.json.gz b/sklearn/datasets/tests/data/openml/61/api-v1-json-data-features-61.json.gz
new file mode 100755
index 0000000000..6df4cf0dad
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/61/api-v1-json-data-features-61.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/61/api-v1-json-data-list-data_name-iris-limit-2-data_version-1.json.gz b/sklearn/datasets/tests/data/openml/61/api-v1-json-data-list-data_name-iris-limit-2-data_version-1.json.gz
new file mode 100755
index 0000000000..71b0c876ad
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/61/api-v1-json-data-list-data_name-iris-limit-2-data_version-1.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/61/api-v1-json-data-list-data_name-iris-limit-2-status-active-.json.gz b/sklearn/datasets/tests/data/openml/61/api-v1-json-data-list-data_name-iris-limit-2-status-active-.json.gz
new file mode 100755
index 0000000000..7ea17070fb
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/61/api-v1-json-data-list-data_name-iris-limit-2-status-active-.json.gz differ
diff --git a/sklearn/datasets/tests/data/openml/61/data-v1-download-61.arff.gz b/sklearn/datasets/tests/data/openml/61/data-v1-download-61.arff.gz
new file mode 100755
index 0000000000..b05dadf99f
Binary files /dev/null and b/sklearn/datasets/tests/data/openml/61/data-v1-download-61.arff.gz differ
diff --git a/sklearn/datasets/tests/test_mldata.py b/sklearn/datasets/tests/test_mldata.py
index 65e10a8781..be0d994e9b 100755
--- a/sklearn/datasets/tests/test_mldata.py
+++ b/sklearn/datasets/tests/test_mldata.py
@@ -13,6 +13,7 @@
 from sklearn.utils.testing import assert_equal
 from sklearn.utils.testing import assert_raises
 from sklearn.utils.testing import assert_array_equal
+from sklearn.utils.testing import assert_warns
 
 import pytest
 
@@ -26,6 +27,7 @@ def tmpdata(tmpdir_factory):
     shutil.rmtree(str(tmpdir))
 
 
+@pytest.mark.filterwarnings('ignore::DeprecationWarning')
 def test_mldata_filename():
     cases = [('datasets-UCI iris', 'datasets-uci-iris'),
              ('news20.binary', 'news20binary'),
@@ -36,6 +38,7 @@ def test_mldata_filename():
         assert_equal(mldata_filename(name), desired)
 
 
+@pytest.mark.filterwarnings('ignore::DeprecationWarning')
 def test_download(tmpdata):
     """Test that fetch_mldata is able to download and cache a data set."""
     _urlopen_ref = datasets.mldata.urlopen
@@ -46,7 +49,8 @@ def test_download(tmpdata):
         },
     })
     try:
-        mock = fetch_mldata('mock', data_home=tmpdata)
+        mock = assert_warns(DeprecationWarning, fetch_mldata,
+                            'mock', data_home=tmpdata)
         for n in ["COL_NAMES", "DESCR", "target", "data"]:
             assert_in(n, mock)
 
@@ -54,11 +58,13 @@ def test_download(tmpdata):
         assert_equal(mock.data.shape, (150, 4))
 
         assert_raises(datasets.mldata.HTTPError,
+                      assert_warns, DeprecationWarning,
                       fetch_mldata, 'not_existing_name')
     finally:
         datasets.mldata.urlopen = _urlopen_ref
 
 
+@pytest.mark.filterwarnings('ignore::DeprecationWarning')
 def test_fetch_one_column(tmpdata):
     _urlopen_ref = datasets.mldata.urlopen
     try:
@@ -82,6 +88,7 @@ def test_fetch_one_column(tmpdata):
         datasets.mldata.urlopen = _urlopen_ref
 
 
+@pytest.mark.filterwarnings('ignore::DeprecationWarning')
 def test_fetch_multiple_column(tmpdata):
     _urlopen_ref = datasets.mldata.urlopen
     try:
diff --git a/sklearn/datasets/tests/test_openml.py b/sklearn/datasets/tests/test_openml.py
new file mode 100755
index 0000000000..c5be1b4160
--- /dev/null
+++ b/sklearn/datasets/tests/test_openml.py
@@ -0,0 +1,529 @@
+"""Test the openml loader.
+"""
+import gzip
+import json
+import numpy as np
+import os
+import re
+import scipy.sparse
+import sklearn
+
+from sklearn.datasets import fetch_openml
+from sklearn.datasets.openml import (_open_openml_url,
+                                     _get_data_description_by_id,
+                                     _download_data_arff)
+from sklearn.utils.testing import (assert_warns_message,
+                                   assert_raise_message)
+from sklearn.externals.six import string_types
+from sklearn.externals.six.moves.urllib.error import HTTPError
+from sklearn.datasets.tests.test_common import check_return_X_y
+from functools import partial
+
+
+currdir = os.path.dirname(os.path.abspath(__file__))
+# if True, urlopen will be monkey patched to only use local files
+test_offline = True
+test_gzip = True
+
+
+def _test_features_list(data_id):
+    # XXX Test is intended to verify/ensure correct decoding behavior
+    # Not usable with sparse data or datasets that have columns marked as
+    # {row_identifier, ignore}
+    def decode_column(data_bunch, col_idx):
+        col_name = data_bunch.feature_names[col_idx]
+        if col_name in data_bunch.categories:
+            # XXX: This would be faster with np.take, although it does not
+            # handle missing values fast (also not with mode='wrap')
+            cat = data_bunch.categories[col_name]
+            result = [cat[idx] if 0 <= idx < len(cat) else None for idx in
+                      data_bunch.data[:, col_idx].astype(int)]
+            return np.array(result, dtype='O')
+        else:
+            # non-nominal attribute
+            return data_bunch.data[:, col_idx]
+
+    data_bunch = fetch_openml(data_id=data_id, cache=False, target_column=None)
+
+    # also obtain decoded arff
+    data_description = _get_data_description_by_id(data_id, None)
+    sparse = data_description['format'].lower() == 'sparse_arff'
+    if sparse is True:
+        raise ValueError('This test is not intended for sparse data, to keep '
+                         'code relatively simple')
+    data_arff = _download_data_arff(data_description['file_id'],
+                                    sparse, None, False)
+    data_downloaded = np.array(data_arff['data'], dtype='O')
+
+    for i in range(len(data_bunch.feature_names)):
+        # XXX: Test per column, as this makes it easier to avoid problems with
+        # missing values
+
+        np.testing.assert_array_equal(data_downloaded[:, i],
+                                      decode_column(data_bunch, i))
+
+
+def _fetch_dataset_from_openml(data_id, data_name, data_version,
+                               target_column,
+                               expected_observations, expected_features,
+                               expected_missing,
+                               expected_data_dtype, expected_target_dtype,
+                               expect_sparse, compare_default_target):
+    # fetches a dataset in three various ways from OpenML, using the
+    # fetch_openml function, and does various checks on the validity of the
+    # result. Note that this function can be mocked (by invoking
+    # _monkey_patch_webbased_functions before invoking this function)
+    data_by_name_id = fetch_openml(name=data_name, version=data_version,
+                                   cache=False)
+    assert int(data_by_name_id.details['id']) == data_id
+
+    fetch_openml(name=data_name, cache=False)
+    # without specifying the version, there is no guarantee that the data id
+    # will be the same
+
+    # fetch with dataset id
+    data_by_id = fetch_openml(data_id=data_id, cache=False,
+                              target_column=target_column)
+    assert data_by_id.details['name'] == data_name
+    assert data_by_id.data.shape == (expected_observations, expected_features)
+    if isinstance(target_column, str):
+        # single target, so target is vector
+        assert data_by_id.target.shape == (expected_observations, )
+    elif isinstance(target_column, list):
+        # multi target, so target is array
+        assert data_by_id.target.shape == (expected_observations,
+                                           len(target_column))
+    assert data_by_id.data.dtype == np.float64
+    assert data_by_id.target.dtype == expected_target_dtype
+    assert len(data_by_id.feature_names) == expected_features
+    for feature in data_by_id.feature_names:
+        assert isinstance(feature, string_types)
+
+    # TODO: pass in a list of expected nominal features
+    for feature, categories in data_by_id.categories.items():
+        feature_idx = data_by_id.feature_names.index(feature)
+        values = np.unique(data_by_id.data[:, feature_idx])
+        values = values[np.isfinite(values)]
+        assert set(values) <= set(range(len(categories)))
+
+    if compare_default_target:
+        # check whether the data by id and data by id target are equal
+        data_by_id_default = fetch_openml(data_id=data_id, cache=False)
+        if data_by_id.data.dtype == np.float64:
+            np.testing.assert_allclose(data_by_id.data,
+                                       data_by_id_default.data)
+        else:
+            assert np.array_equal(data_by_id.data, data_by_id_default.data)
+        if data_by_id.target.dtype == np.float64:
+            np.testing.assert_allclose(data_by_id.target,
+                                       data_by_id_default.target)
+        else:
+            assert np.array_equal(data_by_id.target, data_by_id_default.target)
+
+    if expect_sparse:
+        assert isinstance(data_by_id.data, scipy.sparse.csr_matrix)
+    else:
+        assert isinstance(data_by_id.data, np.ndarray)
+        # np.isnan doesn't work on CSR matrix
+        assert (np.count_nonzero(np.isnan(data_by_id.data)) ==
+                expected_missing)
+
+    # test return_X_y option
+    fetch_func = partial(fetch_openml, data_id=data_id, cache=False,
+                         target_column=target_column)
+    check_return_X_y(data_by_id, fetch_func)
+    return data_by_id
+
+
+def _monkey_patch_webbased_functions(context, data_id, gziped_files):
+    url_prefix_data_description = "https://openml.org/api/v1/json/data/"
+    url_prefix_data_features = "https://openml.org/api/v1/json/data/features/"
+    url_prefix_download_data = "https://openml.org/data/v1/"
+    url_prefix_data_list = "https://openml.org/api/v1/json/data/list/"
+
+    path_suffix = ''
+    read_fn = open
+    if gziped_files:
+        path_suffix = '.gz'
+        read_fn = gzip.open
+
+    def _file_name(url, suffix):
+        return (re.sub(r'\W', '-', url[len("https://openml.org/"):])
+                + suffix + path_suffix)
+
+    def _mock_urlopen_data_description(url):
+        assert url.startswith(url_prefix_data_description)
+
+        path = os.path.join(currdir, 'data', 'openml', str(data_id),
+                            _file_name(url, '.json'))
+        return read_fn(path, 'rb')
+
+    def _mock_urlopen_data_features(url):
+        assert url.startswith(url_prefix_data_features)
+
+        path = os.path.join(currdir, 'data', 'openml', str(data_id),
+                            _file_name(url, '.json'))
+        return read_fn(path, 'rb')
+
+    def _mock_urlopen_download_data(url):
+        assert (url.startswith(url_prefix_download_data))
+
+        path = os.path.join(currdir, 'data', 'openml', str(data_id),
+                            _file_name(url, '.arff'))
+        return read_fn(path, 'rb')
+
+    def _mock_urlopen_data_list(url):
+        assert url.startswith(url_prefix_data_list)
+
+        json_file_path = os.path.join(currdir, 'data', 'openml',
+                                      str(data_id), _file_name(url, '.json'))
+        # load the file itself, to simulate a http error
+        json_data = json.loads(read_fn(json_file_path, 'rb').
+                               read().decode('utf-8'))
+        if 'error' in json_data:
+            raise HTTPError(url=None, code=412,
+                            msg='Simulated mock error',
+                            hdrs=None, fp=None)
+        return read_fn(json_file_path, 'rb')
+
+    def _mock_urlopen(url):
+        if url.startswith(url_prefix_data_list):
+            return _mock_urlopen_data_list(url)
+        elif url.startswith(url_prefix_data_features):
+            return _mock_urlopen_data_features(url)
+        elif url.startswith(url_prefix_download_data):
+            return _mock_urlopen_download_data(url)
+        elif url.startswith(url_prefix_data_description):
+            return _mock_urlopen_data_description(url)
+        else:
+            raise ValueError('Unknown mocking URL pattern: %s' % url)
+
+    # XXX: Global variable
+    if test_offline:
+        context.setattr(sklearn.datasets.openml, 'urlopen', _mock_urlopen)
+
+
+def test_fetch_openml_iris(monkeypatch):
+    # classification dataset with numeric only columns
+    data_id = 61
+    data_name = 'iris'
+    data_version = 1
+    target_column = 'class'
+    expected_observations = 150
+    expected_features = 4
+    expected_missing = 0
+
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    assert_warns_message(
+        UserWarning,
+        "Multiple active versions of the dataset matching the name"
+        " iris exist. Versions may be fundamentally different, "
+        "returning version 1.",
+        _fetch_dataset_from_openml,
+        **{'data_id': data_id, 'data_name': data_name,
+           'data_version': data_version,
+           'target_column': target_column,
+           'expected_observations': expected_observations,
+           'expected_features': expected_features,
+           'expected_missing': expected_missing,
+           'expect_sparse': False,
+           'expected_data_dtype': np.float64,
+           'expected_target_dtype': object,
+           'compare_default_target': True}
+    )
+
+
+def test_decode_iris():
+    data_id = 61
+    _test_features_list(data_id)
+
+
+def test_fetch_openml_iris_multitarget(monkeypatch):
+    # classification dataset with numeric only columns
+    data_id = 61
+    data_name = 'iris'
+    data_version = 1
+    target_column = ['sepallength', 'sepalwidth']
+    expected_observations = 150
+    expected_features = 3
+    expected_missing = 0
+
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    _fetch_dataset_from_openml(data_id, data_name, data_version, target_column,
+                               expected_observations, expected_features,
+                               expected_missing,
+                               object, np.float64, expect_sparse=False,
+                               compare_default_target=False)
+
+
+def test_fetch_openml_anneal(monkeypatch):
+    # classification dataset with numeric and categorical columns
+    data_id = 2
+    data_name = 'anneal'
+    data_version = 1
+    target_column = 'class'
+    # Not all original instances included for space reasons
+    expected_observations = 11
+    expected_features = 38
+    expected_missing = 267
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    _fetch_dataset_from_openml(data_id, data_name, data_version, target_column,
+                               expected_observations, expected_features,
+                               expected_missing,
+                               object, object, expect_sparse=False,
+                               compare_default_target=True)
+
+
+def test_decode_anneal():
+    data_id = 2
+    _test_features_list(data_id)
+
+
+def test_fetch_openml_anneal_multitarget(monkeypatch):
+    # classification dataset with numeric and categorical columns
+    data_id = 2
+    data_name = 'anneal'
+    data_version = 1
+    target_column = ['class', 'product-type', 'shape']
+    # Not all original instances included for space reasons
+    expected_observations = 11
+    expected_features = 36
+    expected_missing = 267
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    _fetch_dataset_from_openml(data_id, data_name, data_version, target_column,
+                               expected_observations, expected_features,
+                               expected_missing,
+                               object, object, expect_sparse=False,
+                               compare_default_target=False)
+
+
+def test_fetch_openml_cpu(monkeypatch):
+    # regression dataset with numeric and categorical columns
+    data_id = 561
+    data_name = 'cpu'
+    data_version = 1
+    target_column = 'class'
+    expected_observations = 209
+    expected_features = 7
+    expected_missing = 0
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    _fetch_dataset_from_openml(data_id, data_name, data_version, target_column,
+                               expected_observations, expected_features,
+                               expected_missing,
+                               object, np.float64, expect_sparse=False,
+                               compare_default_target=True)
+
+
+def test_decode_cpu():
+    data_id = 561
+    _test_features_list(data_id)
+
+
+def test_fetch_openml_australian(monkeypatch):
+    # sparse dataset
+    # Australian is the only sparse dataset that is reasonably small
+    # as it is inactive, we need to catch the warning. Due to mocking
+    # framework, it is not deactivated in our tests
+    data_id = 292
+    data_name = 'Australian'
+    data_version = 1
+    target_column = 'Y'
+    # Not all original instances included for space reasons
+    expected_observations = 85
+    expected_features = 14
+    expected_missing = 0
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    assert_warns_message(
+        UserWarning,
+        "Version 1 of dataset Australian is inactive,",
+        _fetch_dataset_from_openml,
+        **{'data_id': data_id, 'data_name': data_name,
+           'data_version': data_version,
+           'target_column': target_column,
+           'expected_observations': expected_observations,
+           'expected_features': expected_features,
+           'expected_missing': expected_missing,
+           'expect_sparse': True,
+           'expected_data_dtype': np.float64,
+           'expected_target_dtype': object,
+           'compare_default_target': False}  # numpy specific check
+    )
+
+
+def test_fetch_openml_miceprotein(monkeypatch):
+    # JvR: very important check, as this dataset defined several row ids
+    # and ignore attributes. Note that data_features json has 82 attributes,
+    # and row id (1), ignore attributes (3) have been removed (and target is
+    # stored in data.target)
+    data_id = 40966
+    data_name = 'MiceProtein'
+    data_version = 4
+    target_column = 'class'
+    # Not all original instances included for space reasons
+    expected_observations = 7
+    expected_features = 77
+    expected_missing = 7
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    _fetch_dataset_from_openml(data_id, data_name, data_version, target_column,
+                               expected_observations, expected_features,
+                               expected_missing,
+                               np.float64, object, expect_sparse=False,
+                               compare_default_target=True)
+
+
+def test_fetch_openml_emotions(monkeypatch):
+    # classification dataset with multiple targets (natively)
+    data_id = 40589
+    data_name = 'emotions'
+    data_version = 3
+    target_column = ['amazed.suprised', 'happy.pleased', 'relaxing.calm',
+                     'quiet.still', 'sad.lonely', 'angry.aggresive']
+    expected_observations = 13
+    expected_features = 72
+    expected_missing = 0
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+
+    _fetch_dataset_from_openml(data_id, data_name, data_version, target_column,
+                               expected_observations, expected_features,
+                               expected_missing,
+                               np.float64, object, expect_sparse=False,
+                               compare_default_target=True)
+
+
+def test_decode_emotions():
+    data_id = 40589
+    _test_features_list(data_id)
+
+
+def test_open_openml_url_cache(monkeypatch):
+    data_id = 61
+
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    openml_path = sklearn.datasets.openml._DATA_FILE.format(data_id)
+    test_directory = os.path.join(os.path.expanduser('~'), 'scikit_learn_data')
+    # first fill the cache
+    response1 = _open_openml_url(openml_path, test_directory)
+    # assert file exists
+    location = os.path.join(test_directory, 'openml.org', openml_path + '.gz')
+    assert os.path.isfile(location)
+    # redownload, to utilize cache
+    response2 = _open_openml_url(openml_path, test_directory)
+    assert response1.read() == response2.read()
+
+
+def test_fetch_openml_notarget(monkeypatch):
+    data_id = 61
+    target_column = None
+    expected_observations = 150
+    expected_features = 5
+
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    data = fetch_openml(data_id=data_id, target_column=target_column,
+                        cache=False)
+    assert data.data.shape == (expected_observations, expected_features)
+    assert data.target is None
+
+
+def test_fetch_openml_inactive(monkeypatch):
+    # fetch inactive dataset by id
+    data_id = 40675
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    glas2 = assert_warns_message(
+        UserWarning, "Version 1 of dataset glass2 is inactive,", fetch_openml,
+        data_id=data_id, cache=False)
+    # fetch inactive dataset by name and version
+    assert glas2.data.shape == (163, 9)
+    glas2_by_version = assert_warns_message(
+        UserWarning, "Version 1 of dataset glass2 is inactive,", fetch_openml,
+        data_id=None, name="glass2", version=1, cache=False)
+    assert int(glas2_by_version.details['id']) == data_id
+
+
+def test_fetch_nonexiting(monkeypatch):
+    # there is no active version of glass2
+    data_id = 40675
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    # Note that we only want to search by name (not data id)
+    assert_raise_message(ValueError, "No active dataset glass2 found",
+                         fetch_openml, name='glass2', cache=False)
+
+
+def test_raises_illegal_multitarget(monkeypatch):
+    data_id = 61
+    targets = ['sepalwidth', 'class']
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    # Note that we only want to search by name (not data id)
+    assert_raise_message(ValueError,
+                         "Can only handle homogeneous multi-target datasets,",
+                         fetch_openml, data_id=data_id,
+                         target_column=targets, cache=False)
+
+
+def test_warn_ignore_attribute(monkeypatch):
+    data_id = 40966
+    expected_row_id_msg = "target_column={} has flag is_row_identifier."
+    expected_ignore_msg = "target_column={} has flag is_ignore."
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    # single column test
+    assert_warns_message(UserWarning, expected_row_id_msg.format('MouseID'),
+                         fetch_openml, data_id=data_id,
+                         target_column='MouseID',
+                         cache=False)
+    assert_warns_message(UserWarning, expected_ignore_msg.format('Genotype'),
+                         fetch_openml, data_id=data_id,
+                         target_column='Genotype',
+                         cache=False)
+    # multi column test
+    assert_warns_message(UserWarning, expected_row_id_msg.format('MouseID'),
+                         fetch_openml, data_id=data_id,
+                         target_column=['MouseID', 'class'],
+                         cache=False)
+    assert_warns_message(UserWarning, expected_ignore_msg.format('Genotype'),
+                         fetch_openml, data_id=data_id,
+                         target_column=['Genotype', 'class'],
+                         cache=False)
+
+
+def test_string_attribute(monkeypatch):
+    data_id = 40945
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    # single column test
+    assert_raise_message(ValueError,
+                         'STRING attributes are not yet supported',
+                         fetch_openml, data_id=data_id, cache=False)
+
+
+def test_illegal_column(monkeypatch):
+    data_id = 61
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    assert_raise_message(KeyError, "Could not find target_column=",
+                         fetch_openml, data_id=data_id,
+                         target_column='undefined', cache=False)
+
+    assert_raise_message(KeyError, "Could not find target_column=",
+                         fetch_openml, data_id=data_id,
+                         target_column=['undefined', 'class'],
+                         cache=False)
+
+
+def test_fetch_openml_raises_missing_values_target(monkeypatch):
+    data_id = 2
+    _monkey_patch_webbased_functions(monkeypatch, data_id, test_gzip)
+    assert_raise_message(ValueError, "Target column ",
+                         fetch_openml, data_id=data_id, target_column='family')
+
+
+def test_fetch_openml_raises_illegal_argument():
+    assert_raise_message(ValueError, "Dataset data_id=",
+                         fetch_openml, data_id=-1, name="name")
+
+    assert_raise_message(ValueError, "Dataset data_id=",
+                         fetch_openml, data_id=-1, name=None,
+                         version="version")
+
+    assert_raise_message(ValueError, "Dataset data_id=",
+                         fetch_openml, data_id=-1, name="name",
+                         version="version")
+
+    assert_raise_message(ValueError, "Neither name nor data_id are provided. "
+                         "Please provide name or data_id.", fetch_openml)
diff --git a/sklearn/decomposition/dict_learning.py b/sklearn/decomposition/dict_learning.py
index 64315823e2..17054dd0a4 100755
--- a/sklearn/decomposition/dict_learning.py
+++ b/sklearn/decomposition/dict_learning.py
@@ -8,17 +8,16 @@
 import sys
 import itertools
 
-from math import sqrt, ceil
+from math import ceil
 
 import numpy as np
 from scipy import linalg
-from numpy.lib.stride_tricks import as_strided
 
 from ..base import BaseEstimator, TransformerMixin
-from ..utils import Parallel, delayed, cpu_count
+from ..utils import Parallel, delayed, effective_n_jobs
 from ..externals.six.moves import zip
 from ..utils import (check_array, check_random_state, gen_even_slices,
-                     gen_batches, _get_n_jobs)
+                     gen_batches)
 from ..utils.extmath import randomized_svd, row_norms
 from ..utils.validation import check_is_fitted
 from ..linear_model import Lasso, orthogonal_mp_gram, LassoLars, Lars
@@ -184,7 +183,7 @@ def _sparse_encode(X, dictionary, gram, cov=None, algorithm='lasso_lars',
 # XXX : could be moved to the linear_model module
 def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars',
                   n_nonzero_coefs=None, alpha=None, copy_cov=True, init=None,
-                  max_iter=1000, n_jobs=1, check_input=True, verbose=0,
+                  max_iter=1000, n_jobs=None, check_input=True, verbose=0,
                   positive=False):
     """Sparse coding
 
@@ -246,8 +245,11 @@ def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars',
     max_iter : int, 1000 by default
         Maximum number of iterations to perform if `algorithm='lasso_cd'`.
 
-    n_jobs : int, optional
+    n_jobs : int or None, optional (default=None)
         Number of parallel jobs to run.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     check_input : boolean, optional
         If False, the input arrays X and dictionary will not be checked.
@@ -299,7 +301,7 @@ def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars',
         if regularization is None:
             regularization = 1.
 
-    if n_jobs == 1 or algorithm == 'threshold':
+    if effective_n_jobs(n_jobs) or algorithm == 'threshold':
         code = _sparse_encode(X,
                               dictionary, gram, cov=cov,
                               algorithm=algorithm,
@@ -313,7 +315,7 @@ def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars',
 
     # Enter parallel code block
     code = np.empty((n_samples, n_components))
-    slices = list(gen_even_slices(n_samples, _get_n_jobs(n_jobs)))
+    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))
 
     code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(
         delayed(_sparse_encode)(
@@ -376,6 +378,7 @@ def _update_dict(dictionary, Y, code, verbose=False, return_r2=False,
     # Get BLAS functions
     gemm, = linalg.get_blas_funcs(('gemm',), (dictionary, code, Y))
     ger, = linalg.get_blas_funcs(('ger',), (dictionary, code))
+    nrm2, = linalg.get_blas_funcs(('nrm2',), (dictionary,))
     # Residuals, computed with BLAS for speed and efficiency
     # R <- -1.0 * U * V^T + 1.0 * Y
     # Outputs R as Fortran array for efficiency
@@ -383,12 +386,13 @@ def _update_dict(dictionary, Y, code, verbose=False, return_r2=False,
     for k in range(n_components):
         # R <- 1.0 * U_k * V_k^T + R
         R = ger(1.0, dictionary[:, k], code[k, :], a=R, overwrite_a=True)
-        dictionary[:, k] = np.dot(R, code[k, :].T)
+        dictionary[:, k] = np.dot(R, code[k, :])
         if positive:
             np.clip(dictionary[:, k], 0, None, out=dictionary[:, k])
         # Scale k'th atom
-        atom_norm_square = np.dot(dictionary[:, k], dictionary[:, k])
-        if atom_norm_square < 1e-20:
+        # (U_k * U_k) ** 0.5
+        atom_norm = nrm2(dictionary[:, k])
+        if atom_norm < 1e-10:
             if verbose == 1:
                 sys.stdout.write("+")
                 sys.stdout.flush()
@@ -399,26 +403,22 @@ def _update_dict(dictionary, Y, code, verbose=False, return_r2=False,
                 np.clip(dictionary[:, k], 0, None, out=dictionary[:, k])
             # Setting corresponding coefs to 0
             code[k, :] = 0.0
-            dictionary[:, k] /= sqrt(np.dot(dictionary[:, k],
-                                            dictionary[:, k]))
+            # (U_k * U_k) ** 0.5
+            atom_norm = nrm2(dictionary[:, k])
+            dictionary[:, k] /= atom_norm
         else:
-            dictionary[:, k] /= sqrt(atom_norm_square)
+            dictionary[:, k] /= atom_norm
             # R <- -1.0 * U_k * V_k^T + R
             R = ger(-1.0, dictionary[:, k], code[k, :], a=R, overwrite_a=True)
     if return_r2:
         R **= 2
-        # R is fortran-ordered. For numpy version < 1.6, sum does not
-        # follow the quick striding first, and is thus inefficient on
-        # fortran ordered data. We take a flat view of the data with no
-        # striding
-        R = as_strided(R, shape=(R.size, ), strides=(R.dtype.itemsize,))
-        R = np.sum(R)
+        R = R.sum()
         return dictionary, R
     return dictionary
 
 
 def dict_learning(X, n_components, alpha, max_iter=100, tol=1e-8,
-                  method='lars', n_jobs=1, dict_init=None, code_init=None,
+                  method='lars', n_jobs=None, dict_init=None, code_init=None,
                   callback=None, verbose=False, random_state=None,
                   return_n_iter=False, positive_dict=False,
                   positive_code=False):
@@ -459,8 +459,11 @@ def dict_learning(X, n_components, alpha, max_iter=100, tol=1e-8,
         Lasso solution (linear_model.Lasso). Lars will be faster if
         the estimated components are sparse.
 
-    n_jobs : int,
-        Number of parallel jobs to run, or -1 to autodetect.
+    n_jobs : int or None, optional (default=None)
+        Number of parallel jobs to run.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     dict_init : array of shape (n_components, n_features),
         Initial value for the dictionary for warm restart scenarios.
@@ -526,9 +529,6 @@ def dict_learning(X, n_components, alpha, max_iter=100, tol=1e-8,
     alpha = float(alpha)
     random_state = check_random_state(random_state)
 
-    if n_jobs == -1:
-        n_jobs = cpu_count()
-
     # Init the code and the dictionary with SVD of Y
     if code_init is not None and dict_init is not None:
         code = np.array(code_init, order='F')
@@ -605,11 +605,11 @@ def dict_learning(X, n_components, alpha, max_iter=100, tol=1e-8,
 
 def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,
                          return_code=True, dict_init=None, callback=None,
-                         batch_size=3, verbose=False, shuffle=True, n_jobs=1,
-                         method='lars', iter_offset=0, random_state=None,
-                         return_inner_stats=False, inner_stats=None,
-                         return_n_iter=False, positive_dict=False,
-                         positive_code=False):
+                         batch_size=3, verbose=False, shuffle=True,
+                         n_jobs=None, method='lars', iter_offset=0,
+                         random_state=None, return_inner_stats=False,
+                         inner_stats=None, return_n_iter=False,
+                         positive_dict=False, positive_code=False):
     """Solves a dictionary learning matrix factorization problem online.
 
     Finds the best dictionary and the corresponding sparse code for
@@ -657,8 +657,11 @@ def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,
     shuffle : boolean,
         Whether to shuffle the data before splitting it in batches.
 
-    n_jobs : int,
-        Number of parallel jobs to run, or -1 to autodetect.
+    n_jobs : int or None, optional (default=None)
+        Number of parallel jobs to run.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     method : {'lars', 'cd'}
         lars: uses the least angle regression method to solve the lasso problem
@@ -737,9 +740,6 @@ def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,
     alpha = float(alpha)
     random_state = check_random_state(random_state)
 
-    if n_jobs == -1:
-        n_jobs = cpu_count()
-
     # Init V with SVD of X
     if dict_init is not None:
         dictionary = dict_init
@@ -797,6 +797,7 @@ def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,
 
         this_code = sparse_encode(this_X, dictionary.T, algorithm=method,
                                   alpha=alpha, n_jobs=n_jobs,
+                                  check_input=False,
                                   positive=positive_code).T
 
         # Update the auxiliary variables
@@ -856,7 +857,7 @@ def _set_sparse_coding_params(self, n_components,
                                   transform_algorithm='omp',
                                   transform_n_nonzero_coefs=None,
                                   transform_alpha=None, split_sign=False,
-                                  n_jobs=1, positive_code=False):
+                                  n_jobs=None, positive_code=False):
         self.n_components = n_components
         self.transform_algorithm = transform_algorithm
         self.transform_n_nonzero_coefs = transform_n_nonzero_coefs
@@ -954,8 +955,11 @@ class SparseCoder(BaseEstimator, SparseCodingMixin):
         its negative part and its positive part. This can improve the
         performance of downstream classifiers.
 
-    n_jobs : int,
-        number of parallel jobs to run
+    n_jobs : int or None, optional (default=None)
+        Number of parallel jobs to run.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     positive_code : bool
         Whether to enforce positivity when finding the code.
@@ -979,7 +983,7 @@ class SparseCoder(BaseEstimator, SparseCodingMixin):
 
     def __init__(self, dictionary, transform_algorithm='omp',
                  transform_n_nonzero_coefs=None, transform_alpha=None,
-                 split_sign=False, n_jobs=1, positive_code=False):
+                 split_sign=False, n_jobs=None, positive_code=False):
         self._set_sparse_coding_params(dictionary.shape[0],
                                        transform_algorithm,
                                        transform_n_nonzero_coefs,
@@ -1074,8 +1078,11 @@ class DictionaryLearning(BaseEstimator, SparseCodingMixin):
         the reconstruction error targeted. In this case, it overrides
         `n_nonzero_coefs`.
 
-    n_jobs : int,
-        number of parallel jobs to run
+    n_jobs : int or None, optional (default=None)
+        Number of parallel jobs to run.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     code_init : array of shape (n_samples, n_components),
         initial value for the code, for warm restart
@@ -1135,7 +1142,7 @@ class DictionaryLearning(BaseEstimator, SparseCodingMixin):
     def __init__(self, n_components=None, alpha=1, max_iter=1000, tol=1e-8,
                  fit_algorithm='lars', transform_algorithm='omp',
                  transform_n_nonzero_coefs=None, transform_alpha=None,
-                 n_jobs=1, code_init=None, dict_init=None, verbose=False,
+                 n_jobs=None, code_init=None, dict_init=None, verbose=False,
                  split_sign=False, random_state=None,
                  positive_code=False, positive_dict=False):
 
@@ -1225,8 +1232,11 @@ class MiniBatchDictionaryLearning(BaseEstimator, SparseCodingMixin):
         Lasso solution (linear_model.Lasso). Lars will be faster if
         the estimated components are sparse.
 
-    n_jobs : int,
-        number of parallel jobs to run
+    n_jobs : int or None, optional (default=None)
+        Number of parallel jobs to run.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     batch_size : int,
         number of samples in each mini-batch
@@ -1319,7 +1329,7 @@ class MiniBatchDictionaryLearning(BaseEstimator, SparseCodingMixin):
 
     """
     def __init__(self, n_components=None, alpha=1, n_iter=1000,
-                 fit_algorithm='lars', n_jobs=1, batch_size=3,
+                 fit_algorithm='lars', n_jobs=None, batch_size=3,
                  shuffle=True, dict_init=None, transform_algorithm='omp',
                  transform_n_nonzero_coefs=None, transform_alpha=None,
                  verbose=False, split_sign=False, random_state=None,
diff --git a/sklearn/decomposition/factor_analysis.py b/sklearn/decomposition/factor_analysis.py
index 481a5e2322..eea477937e 100755
--- a/sklearn/decomposition/factor_analysis.py
+++ b/sklearn/decomposition/factor_analysis.py
@@ -108,6 +108,16 @@ class FactorAnalysis(BaseEstimator, TransformerMixin):
     n_iter_ : int
         Number of iterations run.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.decomposition import FactorAnalysis
+    >>> X, _ = load_digits(return_X_y=True)
+    >>> transformer = FactorAnalysis(n_components=7, random_state=0)
+    >>> X_transformed = transformer.fit_transform(X)
+    >>> X_transformed.shape
+    (1797, 7)
+
     References
     ----------
     .. David Barber, Bayesian Reasoning and Machine Learning,
diff --git a/sklearn/decomposition/fastica_.py b/sklearn/decomposition/fastica_.py
index f2484672ab..f64d4787b3 100755
--- a/sklearn/decomposition/fastica_.py
+++ b/sklearn/decomposition/fastica_.py
@@ -443,6 +443,17 @@ def my_g(x):
         maximum number of iterations run across all components. Else
         they are just the number of iterations taken to converge.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.decomposition import FastICA
+    >>> X, _ = load_digits(return_X_y=True)
+    >>> transformer = FastICA(n_components=7,
+    ...         random_state=0)
+    >>> X_transformed = transformer.fit_transform(X)
+    >>> X_transformed.shape
+    (1797, 7)
+
     Notes
     -----
     Implementation based on
diff --git a/sklearn/decomposition/incremental_pca.py b/sklearn/decomposition/incremental_pca.py
index 72f1326c58..05e6693051 100755
--- a/sklearn/decomposition/incremental_pca.py
+++ b/sklearn/decomposition/incremental_pca.py
@@ -100,6 +100,20 @@ class IncrementalPCA(_BasePCA):
         The number of samples processed by the estimator. Will be reset on
         new calls to fit, but increments across ``partial_fit`` calls.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.decomposition import IncrementalPCA
+    >>> X, _ = load_digits(return_X_y=True)
+    >>> transformer = IncrementalPCA(n_components=7, batch_size=200)
+    >>> # either partially fit on smaller batches of data
+    >>> transformer.partial_fit(X[:100, :])
+    IncrementalPCA(batch_size=200, copy=True, n_components=7, whiten=False)
+    >>> # or let the fit function itself divide the data into batches
+    >>> X_transformed = transformer.fit_transform(X)
+    >>> X_transformed.shape
+    (1797, 7)
+
     Notes
     -----
     Implements the incremental PCA model from:
diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py
index 3316ddb24d..133717e13f 100755
--- a/sklearn/decomposition/kernel_pca.py
+++ b/sklearn/decomposition/kernel_pca.py
@@ -89,9 +89,11 @@ class KernelPCA(BaseEstimator, TransformerMixin):
 
         .. versionadded:: 0.18
 
-    n_jobs : int, default=1
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run.
-        If `-1`, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
         .. versionadded:: 0.18
 
@@ -118,6 +120,16 @@ class KernelPCA(BaseEstimator, TransformerMixin):
         The data used to fit the model. If `copy_X=False`, then `X_fit_` is
         a reference. This attribute is used for the calls to transform.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.decomposition import KernelPCA
+    >>> X, _ = load_digits(return_X_y=True)
+    >>> transformer = KernelPCA(n_components=7, kernel='linear')
+    >>> X_transformed = transformer.fit_transform(X)
+    >>> X_transformed.shape
+    (1797, 7)
+
     References
     ----------
     Kernel PCA was introduced in:
@@ -131,7 +143,7 @@ def __init__(self, n_components=None, kernel="linear",
                  gamma=None, degree=3, coef0=1, kernel_params=None,
                  alpha=1.0, fit_inverse_transform=False, eigen_solver='auto',
                  tol=0, max_iter=None, remove_zero_eig=False,
-                 random_state=None, copy_X=True, n_jobs=1):
+                 random_state=None, copy_X=True, n_jobs=None):
         if fit_inverse_transform and kernel == 'precomputed':
             raise ValueError(
                 "Cannot fit_inverse_transform with a precomputed kernel.")
diff --git a/sklearn/decomposition/online_lda.py b/sklearn/decomposition/online_lda.py
index b8be3fbc68..5b48ea1a26 100755
--- a/sklearn/decomposition/online_lda.py
+++ b/sklearn/decomposition/online_lda.py
@@ -18,10 +18,10 @@
 
 from ..base import BaseEstimator, TransformerMixin
 from ..utils import (check_random_state, check_array,
-                     gen_batches, gen_even_slices, _get_n_jobs)
+                     gen_batches, gen_even_slices)
 from ..utils.fixes import logsumexp
 from ..utils.validation import check_non_negative
-from ..utils import Parallel, delayed
+from ..utils import Parallel, delayed, effective_n_jobs
 from ..externals.six.moves import xrange
 from ..exceptions import NotFittedError
 
@@ -215,9 +215,11 @@ class LatentDirichletAllocation(BaseEstimator, TransformerMixin):
         Max number of iterations for updating document topic distribution in
         the E-step.
 
-    n_jobs : int, optional (default=1)
-        The number of jobs to use in the E-step. If -1, all CPUs are used. For
-        ``n_jobs`` below -1, (n_cpus + 1 + n_jobs) are used.
+    n_jobs : int or None, optional (default=None)
+        The number of jobs to use in the E-step.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : int, optional (default=0)
         Verbosity level.
@@ -250,6 +252,22 @@ class LatentDirichletAllocation(BaseEstimator, TransformerMixin):
     n_iter_ : int
         Number of passes over the dataset.
 
+    Examples
+    --------
+    >>> from sklearn.decomposition import LatentDirichletAllocation
+    >>> from sklearn.datasets import make_multilabel_classification
+    >>> # This produces a feature matrix of token counts, similar to what
+    >>> # CountVectorizer would produce on text.
+    >>> X, _ = make_multilabel_classification(random_state=0)
+    >>> lda = LatentDirichletAllocation(n_components=5,
+    ...     random_state=0)
+    >>> lda.fit(X) # doctest: +ELLIPSIS
+    LatentDirichletAllocation(...)
+    >>> # get topics for some given samples:
+    >>> lda.transform(X[-2:])
+    array([[0.00360392, 0.25499205, 0.0036211 , 0.64236448, 0.09541846],
+           [0.15297572, 0.00362644, 0.44412786, 0.39568399, 0.003586  ]])
+
     References
     ----------
     [1] "Online Learning for Latent Dirichlet Allocation", Matthew D. Hoffman,
@@ -268,7 +286,7 @@ def __init__(self, n_components=10, doc_topic_prior=None,
                  learning_decay=.7, learning_offset=10., max_iter=10,
                  batch_size=128, evaluate_every=-1, total_samples=1e6,
                  perp_tol=1e-1, mean_change_tol=1e-3, max_doc_update_iter=100,
-                 n_jobs=1, verbose=0, random_state=None, n_topics=None):
+                 n_jobs=None, verbose=0, random_state=None, n_topics=None):
         self.n_components = n_components
         self.doc_topic_prior = doc_topic_prior
         self.topic_word_prior = topic_word_prior
@@ -374,7 +392,7 @@ def _e_step(self, X, cal_sstats, random_init, parallel=None):
         random_state = self.random_state_ if random_init else None
 
         # TODO: make Parallel._effective_n_jobs public instead?
-        n_jobs = _get_n_jobs(self.n_jobs)
+        n_jobs = effective_n_jobs(self.n_jobs)
         if parallel is None:
             parallel = Parallel(n_jobs=n_jobs, verbose=max(0,
                                 self.verbose - 1))
@@ -497,7 +515,7 @@ def partial_fit(self, X, y=None):
                 "the model was trained with feature size %d." %
                 (n_features, self.components_.shape[1]))
 
-        n_jobs = _get_n_jobs(self.n_jobs)
+        n_jobs = effective_n_jobs(self.n_jobs)
         with Parallel(n_jobs=n_jobs, verbose=max(0,
                       self.verbose - 1)) as parallel:
             for idx_slice in gen_batches(n_samples, batch_size):
@@ -538,7 +556,7 @@ def fit(self, X, y=None):
         self._init_latent_vars(n_features)
         # change to perplexity later
         last_bound = None
-        n_jobs = _get_n_jobs(self.n_jobs)
+        n_jobs = effective_n_jobs(self.n_jobs)
         with Parallel(n_jobs=n_jobs, verbose=max(0,
                       self.verbose - 1)) as parallel:
             for i in xrange(max_iter):
diff --git a/sklearn/decomposition/sparse_pca.py b/sklearn/decomposition/sparse_pca.py
index f38cc49882..95c9ab8960 100755
--- a/sklearn/decomposition/sparse_pca.py
+++ b/sklearn/decomposition/sparse_pca.py
@@ -48,8 +48,11 @@ class SparsePCA(BaseEstimator, TransformerMixin):
         Lasso solution (linear_model.Lasso). Lars will be faster if
         the estimated components are sparse.
 
-    n_jobs : int,
+    n_jobs : int or None, optional (default=None)
         Number of parallel jobs to run.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     U_init : array of shape (n_samples, n_components),
         Initial values for the loadings for warm restart scenarios.
@@ -96,6 +99,24 @@ class SparsePCA(BaseEstimator, TransformerMixin):
         Per-feature empirical mean, estimated from the training set.
         Equal to ``X.mean(axis=0)``.
 
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.datasets import make_friedman1
+    >>> from sklearn.decomposition import SparsePCA
+    >>> X, _ = make_friedman1(n_samples=200, n_features=30, random_state=0)
+    >>> transformer = SparsePCA(n_components=5,
+    ...         normalize_components=True,
+    ...         random_state=0)
+    >>> transformer.fit(X) # doctest: +ELLIPSIS
+    SparsePCA(...)
+    >>> X_transformed = transformer.transform(X)
+    >>> X_transformed.shape
+    (200, 5)
+    >>> # most values in the components_ are zero (sparsity)
+    >>> np.mean(transformer.components_ == 0) # doctest: +ELLIPSIS
+    0.9666...
+
     See also
     --------
     PCA
@@ -103,8 +124,8 @@ class SparsePCA(BaseEstimator, TransformerMixin):
     DictionaryLearning
     """
     def __init__(self, n_components=None, alpha=1, ridge_alpha=0.01,
-                 max_iter=1000, tol=1e-8, method='lars', n_jobs=1, U_init=None,
-                 V_init=None, verbose=False, random_state=None,
+                 max_iter=1000, tol=1e-8, method='lars', n_jobs=None,
+                 U_init=None, V_init=None, verbose=False, random_state=None,
                  normalize_components=False):
         self.n_components = n_components
         self.alpha = alpha
@@ -269,8 +290,11 @@ class MiniBatchSparsePCA(SparsePCA):
     shuffle : boolean,
         whether to shuffle the data before splitting it in batches
 
-    n_jobs : int,
-        number of parallel jobs to run, or -1 to autodetect.
+    n_jobs : int or None, optional (default=None)
+        Number of parallel jobs to run.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     method : {'lars', 'cd'}
         lars: uses the least angle regression method to solve the lasso problem
@@ -312,6 +336,25 @@ class MiniBatchSparsePCA(SparsePCA):
         Per-feature empirical mean, estimated from the training set.
         Equal to ``X.mean(axis=0)``.
 
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.datasets import make_friedman1
+    >>> from sklearn.decomposition import MiniBatchSparsePCA
+    >>> X, _ = make_friedman1(n_samples=200, n_features=30, random_state=0)
+    >>> transformer = MiniBatchSparsePCA(n_components=5,
+    ...         batch_size=50,
+    ...         normalize_components=True,
+    ...         random_state=0)
+    >>> transformer.fit(X) # doctest: +ELLIPSIS
+    MiniBatchSparsePCA(...)
+    >>> X_transformed = transformer.transform(X)
+    >>> X_transformed.shape
+    (200, 5)
+    >>> # most values in the components_ are zero (sparsity)
+    >>> np.mean(transformer.components_ == 0)
+    0.94
+
     See also
     --------
     PCA
@@ -320,7 +363,7 @@ class MiniBatchSparsePCA(SparsePCA):
     """
     def __init__(self, n_components=None, alpha=1, ridge_alpha=0.01,
                  n_iter=100, callback=None, batch_size=3, verbose=False,
-                 shuffle=True, n_jobs=1, method='lars', random_state=None,
+                 shuffle=True, n_jobs=None, method='lars', random_state=None,
                  normalize_components=False):
         super(MiniBatchSparsePCA, self).__init__(
             n_components=n_components, alpha=alpha, verbose=verbose,
diff --git a/sklearn/ensemble/bagging.py b/sklearn/ensemble/bagging.py
index f82221c959..51dce324a0 100755
--- a/sklearn/ensemble/bagging.py
+++ b/sklearn/ensemble/bagging.py
@@ -201,7 +201,7 @@ def __init__(self,
                  bootstrap_features=False,
                  oob_score=False,
                  warm_start=False,
-                 n_jobs=1,
+                 n_jobs=None,
                  random_state=None,
                  verbose=0):
         super(BaseBagging, self).__init__(
@@ -484,9 +484,11 @@ class BaggingClassifier(BaseBagging, ClassifierMixin):
         .. versionadded:: 0.17
            *warm_start* constructor parameter.
 
-    n_jobs : int, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of jobs to run in parallel for both `fit` and `predict`.
-        If -1, then the number of jobs is set to the number of cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
@@ -552,7 +554,7 @@ def __init__(self,
                  bootstrap_features=False,
                  oob_score=False,
                  warm_start=False,
-                 n_jobs=1,
+                 n_jobs=None,
                  random_state=None,
                  verbose=0):
 
@@ -860,9 +862,11 @@ class BaggingRegressor(BaseBagging, RegressorMixin):
         and add more estimators to the ensemble, otherwise, just fit
         a whole new ensemble. See :term:`the Glossary <warm_start>`.
 
-    n_jobs : int, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of jobs to run in parallel for both `fit` and `predict`.
-        If -1, then the number of jobs is set to the number of cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
@@ -920,7 +924,7 @@ def __init__(self,
                  bootstrap_features=False,
                  oob_score=False,
                  warm_start=False,
-                 n_jobs=1,
+                 n_jobs=None,
                  random_state=None,
                  verbose=0):
         super(BaggingRegressor, self).__init__(
diff --git a/sklearn/ensemble/base.py b/sklearn/ensemble/base.py
index 806a4e4cea..321031892d 100755
--- a/sklearn/ensemble/base.py
+++ b/sklearn/ensemble/base.py
@@ -11,8 +11,9 @@
 from ..base import clone
 from ..base import BaseEstimator
 from ..base import MetaEstimatorMixin
-from ..utils import _get_n_jobs, check_random_state
+from ..utils import check_random_state
 from ..externals import six
+from ..externals.joblib import effective_n_jobs
 from abc import ABCMeta, abstractmethod
 
 MAX_RAND_SEED = np.iinfo(np.int32).max
@@ -150,7 +151,7 @@ def __iter__(self):
 def _partition_estimators(n_estimators, n_jobs):
     """Private function used to partition estimators between jobs."""
     # Compute the number of jobs
-    n_jobs = min(_get_n_jobs(n_jobs), n_estimators)
+    n_jobs = min(effective_n_jobs(n_jobs), n_estimators)
 
     # Partition estimators between jobs
     n_estimators_per_job = np.full(n_jobs, n_estimators // n_jobs,
diff --git a/sklearn/ensemble/forest.py b/sklearn/ensemble/forest.py
index 5a9c3cac00..125f48d5b0 100755
--- a/sklearn/ensemble/forest.py
+++ b/sklearn/ensemble/forest.py
@@ -139,7 +139,7 @@ def __init__(self,
                  estimator_params=tuple(),
                  bootstrap=False,
                  oob_score=False,
-                 n_jobs=1,
+                 n_jobs=None,
                  random_state=None,
                  verbose=0,
                  warm_start=False,
@@ -175,7 +175,7 @@ def apply(self, X):
         """
         X = self._validate_X_predict(X)
         results = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
-                           backend="threading")(
+                           prefer="threads")(
             delayed(parallel_helper)(tree, 'apply', X, check_input=False)
             for tree in self.estimators_)
 
@@ -206,9 +206,9 @@ def decision_path(self, X):
         """
         X = self._validate_X_predict(X)
         indicators = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
-                              backend="threading")(
+                              prefer="threads")(
             delayed(parallel_helper)(tree, 'decision_path', X,
-                                      check_input=False)
+                                     check_input=False)
             for tree in self.estimators_)
 
         n_nodes = [0]
@@ -223,8 +223,8 @@ def fit(self, X, y, sample_weight=None):
         Parameters
         ----------
         X : array-like or sparse matrix of shape = [n_samples, n_features]
-            The training input samples. Internally, its dtype will be converted to
-            ``dtype=np.float32``. If a sparse matrix is provided, it will be
+            The training input samples. Internally, its dtype will be converted
+            to ``dtype=np.float32``. If a sparse matrix is provided, it will be
             converted into a sparse ``csc_matrix``.
 
         y : array-like, shape = [n_samples] or [n_samples, n_outputs]
@@ -321,12 +321,14 @@ def fit(self, X, y, sample_weight=None):
                                             random_state=random_state)
                 trees.append(tree)
 
-            # Parallel loop: we use the threading backend as the Cython code
+            # Parallel loop: we prefer the threading backend as the Cython code
             # for fitting the trees is internally releasing the Python GIL
-            # making threading always more efficient than multiprocessing in
-            # that case.
+            # making threading more efficient than multiprocessing in
+            # that case. However, we respect any parallel_backend contexts set
+            # at a higher level, since correctness does not rely on using
+            # threads.
             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
-                             backend="threading")(
+                             prefer="threads")(
                 delayed(_parallel_build_trees)(
                     t, self, X, y, sample_weight, i, len(trees),
                     verbose=self.verbose, class_weight=self.class_weight)
@@ -373,7 +375,7 @@ def feature_importances_(self):
         check_is_fitted(self, 'estimators_')
 
         all_importances = Parallel(n_jobs=self.n_jobs,
-                                   backend="threading")(
+                                   prefer="threads")(
             delayed(getattr)(tree, 'feature_importances_')
             for tree in self.estimators_)
 
@@ -410,7 +412,7 @@ def __init__(self,
                  estimator_params=tuple(),
                  bootstrap=False,
                  oob_score=False,
-                 n_jobs=1,
+                 n_jobs=None,
                  random_state=None,
                  verbose=0,
                  warm_start=False,
@@ -589,7 +591,7 @@ class in a leaf.
         all_proba = [np.zeros((X.shape[0], j), dtype=np.float64)
                      for j in np.atleast_1d(self.n_classes_)]
         lock = threading.Lock()
-        Parallel(n_jobs=n_jobs, verbose=self.verbose, backend="threading")(
+        Parallel(n_jobs=n_jobs, verbose=self.verbose, require="sharedmem")(
             delayed(_accumulate_prediction)(e.predict_proba, X, all_proba,
                                             lock)
             for e in self.estimators_)
@@ -649,7 +651,7 @@ def __init__(self,
                  estimator_params=tuple(),
                  bootstrap=False,
                  oob_score=False,
-                 n_jobs=1,
+                 n_jobs=None,
                  random_state=None,
                  verbose=0,
                  warm_start=False):
@@ -697,7 +699,7 @@ def predict(self, X):
 
         # Parallel loop
         lock = threading.Lock()
-        Parallel(n_jobs=n_jobs, verbose=self.verbose, backend="threading")(
+        Parallel(n_jobs=n_jobs, verbose=self.verbose, require="sharedmem")(
             delayed(_accumulate_prediction)(e.predict, X, [y_hat], lock)
             for e in self.estimators_)
 
@@ -782,8 +784,8 @@ class RandomForestClassifier(ForestClassifier):
     min_samples_split : int, float, optional (default=2)
         The minimum number of samples required to split an internal node:
 
-        - If int, then consider `min_samples_split` as the minimum number.
-        - If float, then `min_samples_split` is a fraction and
+        - If int, then consider ``min_samples_split`` as the minimum number.
+        - If float, then ``min_samples_split`` is a fraction and
           `ceil(min_samples_split * n_samples)` are the minimum
           number of samples for each split.
 
@@ -793,19 +795,28 @@ class RandomForestClassifier(ForestClassifier):
     min_samples_leaf : int, float, optional (default=1)
         The minimum number of samples required to be at a leaf node:
 
-        - If int, then consider `min_samples_leaf` as the minimum number.
-        - If float, then `min_samples_leaf` is a fraction and
+        - If int, then consider ``min_samples_leaf`` as the minimum number.
+        - If float, then ``min_samples_leaf`` is a fraction and
           `ceil(min_samples_leaf * n_samples)` are the minimum
           number of samples for each node.
 
         .. versionchanged:: 0.18
            Added float values for fractions.
+        .. deprecated:: 0.20
+           The parameter ``min_samples_leaf`` is deprecated in version 0.20 and
+           will be fixed to a value of 1 in version 0.22. It was not effective
+           for regularization and empirically, 1 is the best value.
 
     min_weight_fraction_leaf : float, optional (default=0.)
         The minimum weighted fraction of the sum total of weights (of all
         the input samples) required to be at a leaf node. Samples have
         equal weight when sample_weight is not provided.
 
+        .. deprecated:: 0.20
+           The parameter ``min_weight_fraction_leaf`` is deprecated in version
+           0.20. Its implementation, like ``min_samples_leaf``, is ineffective
+           for regularization.
+
     max_features : int, float, string or None, optional (default="auto")
         The number of features to consider when looking for the best split:
 
@@ -861,9 +872,11 @@ class RandomForestClassifier(ForestClassifier):
         Whether to use out-of-bag samples to estimate
         the generalization accuracy.
 
-    n_jobs : integer, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of jobs to run in parallel for both `fit` and `predict`.
-        If -1, then the number of jobs is set to the number of cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
@@ -950,9 +963,10 @@ class labels (multi-output problem).
     RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
                 max_depth=2, max_features='auto', max_leaf_nodes=None,
                 min_impurity_decrease=0.0, min_impurity_split=None,
-                min_samples_leaf=1, min_samples_split=2,
-                min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,
-                oob_score=False, random_state=0, verbose=0, warm_start=False)
+                min_samples_leaf='deprecated', min_samples_split=2,
+                min_weight_fraction_leaf='deprecated', n_estimators=100,
+                n_jobs=None, oob_score=False, random_state=0, verbose=0,
+                warm_start=False)
     >>> print(clf.feature_importances_)
     [0.14205973 0.76664038 0.0282433  0.06305659]
     >>> print(clf.predict([[0, 0, 0, 0]]))
@@ -961,7 +975,7 @@ class labels (multi-output problem).
     Notes
     -----
     The default values for the parameters controlling the size of the trees
-    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
+    (e.g. ``max_depth``, ``min_samples_split``, etc.) lead to fully grown and
     unpruned trees which can potentially be very large on some data sets. To
     reduce memory consumption, the complexity and size of the trees should be
     controlled by setting those parameter values.
@@ -987,15 +1001,15 @@ def __init__(self,
                  criterion="gini",
                  max_depth=None,
                  min_samples_split=2,
-                 min_samples_leaf=1,
-                 min_weight_fraction_leaf=0.,
+                 min_samples_leaf='deprecated',
+                 min_weight_fraction_leaf='deprecated',
                  max_features="auto",
                  max_leaf_nodes=None,
                  min_impurity_decrease=0.,
                  min_impurity_split=None,
                  bootstrap=True,
                  oob_score=False,
-                 n_jobs=1,
+                 n_jobs=None,
                  random_state=None,
                  verbose=0,
                  warm_start=False,
@@ -1065,8 +1079,8 @@ class RandomForestRegressor(ForestRegressor):
     min_samples_split : int, float, optional (default=2)
         The minimum number of samples required to split an internal node:
 
-        - If int, then consider `min_samples_split` as the minimum number.
-        - If float, then `min_samples_split` is a fraction and
+        - If int, then consider ``min_samples_split`` as the minimum number.
+        - If float, then ``min_samples_split`` is a fraction and
           `ceil(min_samples_split * n_samples)` are the minimum
           number of samples for each split.
 
@@ -1076,19 +1090,28 @@ class RandomForestRegressor(ForestRegressor):
     min_samples_leaf : int, float, optional (default=1)
         The minimum number of samples required to be at a leaf node:
 
-        - If int, then consider `min_samples_leaf` as the minimum number.
-        - If float, then `min_samples_leaf` is a fraction and
+        - If int, then consider ``min_samples_leaf`` as the minimum number.
+        - If float, then ``min_samples_leaf`` is a fraction and
           `ceil(min_samples_leaf * n_samples)` are the minimum
           number of samples for each node.
 
         .. versionchanged:: 0.18
            Added float values for fractions.
+        .. deprecated:: 0.20
+           The parameter ``min_samples_leaf`` is deprecated in version 0.20 and
+           will be fixed to a value of 1 in version 0.22. It was not effective
+           for regularization and empirically, 1 is the best value.
 
     min_weight_fraction_leaf : float, optional (default=0.)
         The minimum weighted fraction of the sum total of weights (of all
         the input samples) required to be at a leaf node. Samples have
         equal weight when sample_weight is not provided.
 
+        .. deprecated:: 0.20
+           The parameter ``min_weight_fraction_leaf`` is deprecated in version
+           0.20. Its implementation, like ``min_samples_leaf``, is ineffective
+           for regularization.
+
     max_features : int, float, string or None, optional (default="auto")
         The number of features to consider when looking for the best split:
 
@@ -1144,9 +1167,11 @@ class RandomForestRegressor(ForestRegressor):
         whether to use out-of-bag samples to estimate
         the R^2 on unseen data.
 
-    n_jobs : integer, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of jobs to run in parallel for both `fit` and `predict`.
-        If -1, then the number of jobs is set to the number of cores.
+        `None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
@@ -1195,9 +1220,10 @@ class RandomForestRegressor(ForestRegressor):
     RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,
                max_features='auto', max_leaf_nodes=None,
                min_impurity_decrease=0.0, min_impurity_split=None,
-               min_samples_leaf=1, min_samples_split=2,
-               min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,
-               oob_score=False, random_state=0, verbose=0, warm_start=False)
+               min_samples_leaf='deprecated', min_samples_split=2,
+               min_weight_fraction_leaf='deprecated', n_estimators=100,
+               n_jobs=None, oob_score=False, random_state=0, verbose=0,
+               warm_start=False)
     >>> print(regr.feature_importances_)
     [0.18146984 0.81473937 0.00145312 0.00233767]
     >>> print(regr.predict([[0, 0, 0, 0]]))
@@ -1206,7 +1232,7 @@ class RandomForestRegressor(ForestRegressor):
     Notes
     -----
     The default values for the parameters controlling the size of the trees
-    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
+    (e.g. ``max_depth``, ``min_samples_split``, etc.) lead to fully grown and
     unpruned trees which can potentially be very large on some data sets. To
     reduce memory consumption, the complexity and size of the trees should be
     controlled by setting those parameter values.
@@ -1218,11 +1244,18 @@ class RandomForestRegressor(ForestRegressor):
     search of the best split. To obtain a deterministic behaviour during
     fitting, ``random_state`` has to be fixed.
 
+    The default value ``max_features="auto"`` uses ``n_features`` 
+    rather than ``n_features / 3``. The latter was originally suggested in
+    [1], whereas the former was more recently justified empirically in [2].
+
     References
     ----------
 
     .. [1] L. Breiman, "Random Forests", Machine Learning, 45(1), 5-32, 2001.
 
+    .. [2] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized 
+           trees", Machine Learning, 63(1), 3-42, 2006.
+
     See also
     --------
     DecisionTreeRegressor, ExtraTreesRegressor
@@ -1232,15 +1265,15 @@ def __init__(self,
                  criterion="mse",
                  max_depth=None,
                  min_samples_split=2,
-                 min_samples_leaf=1,
-                 min_weight_fraction_leaf=0.,
+                 min_samples_leaf='deprecated',
+                 min_weight_fraction_leaf='deprecated',
                  max_features="auto",
                  max_leaf_nodes=None,
                  min_impurity_decrease=0.,
                  min_impurity_split=None,
                  bootstrap=True,
                  oob_score=False,
-                 n_jobs=1,
+                 n_jobs=None,
                  random_state=None,
                  verbose=0,
                  warm_start=False):
@@ -1301,8 +1334,8 @@ class ExtraTreesClassifier(ForestClassifier):
     min_samples_split : int, float, optional (default=2)
         The minimum number of samples required to split an internal node:
 
-        - If int, then consider `min_samples_split` as the minimum number.
-        - If float, then `min_samples_split` is a fraction and
+        - If int, then consider ``min_samples_split`` as the minimum number.
+        - If float, then ``min_samples_split`` is a fraction and
           `ceil(min_samples_split * n_samples)` are the minimum
           number of samples for each split.
 
@@ -1312,19 +1345,28 @@ class ExtraTreesClassifier(ForestClassifier):
     min_samples_leaf : int, float, optional (default=1)
         The minimum number of samples required to be at a leaf node:
 
-        - If int, then consider `min_samples_leaf` as the minimum number.
-        - If float, then `min_samples_leaf` is a fraction and
+        - If int, then consider ``min_samples_leaf`` as the minimum number.
+        - If float, then ``min_samples_leaf`` is a fraction and
           `ceil(min_samples_leaf * n_samples)` are the minimum
           number of samples for each node.
 
         .. versionchanged:: 0.18
            Added float values for fractions.
+        .. deprecated:: 0.20
+           The parameter ``min_samples_leaf`` is deprecated in version 0.20 and
+           will be fixed to a value of 1 in version 0.22. It was not effective
+           for regularization and empirically, 1 is the best value.
 
     min_weight_fraction_leaf : float, optional (default=0.)
         The minimum weighted fraction of the sum total of weights (of all
         the input samples) required to be at a leaf node. Samples have
         equal weight when sample_weight is not provided.
 
+        .. deprecated:: 0.20
+           The parameter ``min_weight_fraction_leaf`` is deprecated in version
+           0.20. Its implementation, like ``min_samples_leaf``, is ineffective
+           for regularization.
+
     max_features : int, float, string or None, optional (default="auto")
         The number of features to consider when looking for the best split:
 
@@ -1380,9 +1422,11 @@ class ExtraTreesClassifier(ForestClassifier):
         Whether to use out-of-bag samples to estimate
         the generalization accuracy.
 
-    n_jobs : integer, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of jobs to run in parallel for both `fit` and `predict`.
-        If -1, then the number of jobs is set to the number of cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
@@ -1457,7 +1501,7 @@ class labels (multi-output problem).
     Notes
     -----
     The default values for the parameters controlling the size of the trees
-    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
+    (e.g. ``max_depth``, ``min_samples_split``, etc.) lead to fully grown and
     unpruned trees which can potentially be very large on some data sets. To
     reduce memory consumption, the complexity and size of the trees should be
     controlled by setting those parameter values.
@@ -1465,8 +1509,8 @@ class labels (multi-output problem).
     References
     ----------
 
-    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized trees",
-           Machine Learning, 63(1), 3-42, 2006.
+    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized 
+           trees", Machine Learning, 63(1), 3-42, 2006.
 
     See also
     --------
@@ -1479,15 +1523,15 @@ def __init__(self,
                  criterion="gini",
                  max_depth=None,
                  min_samples_split=2,
-                 min_samples_leaf=1,
-                 min_weight_fraction_leaf=0.,
+                 min_samples_leaf='deprecated',
+                 min_weight_fraction_leaf='deprecated',
                  max_features="auto",
                  max_leaf_nodes=None,
                  min_impurity_decrease=0.,
                  min_impurity_split=None,
                  bootstrap=False,
                  oob_score=False,
-                 n_jobs=1,
+                 n_jobs=None,
                  random_state=None,
                  verbose=0,
                  warm_start=False,
@@ -1555,8 +1599,8 @@ class ExtraTreesRegressor(ForestRegressor):
     min_samples_split : int, float, optional (default=2)
         The minimum number of samples required to split an internal node:
 
-        - If int, then consider `min_samples_split` as the minimum number.
-        - If float, then `min_samples_split` is a fraction and
+        - If int, then consider ``min_samples_split`` as the minimum number.
+        - If float, then ``min_samples_split`` is a fraction and
           `ceil(min_samples_split * n_samples)` are the minimum
           number of samples for each split.
 
@@ -1566,19 +1610,28 @@ class ExtraTreesRegressor(ForestRegressor):
     min_samples_leaf : int, float, optional (default=1)
         The minimum number of samples required to be at a leaf node:
 
-        - If int, then consider `min_samples_leaf` as the minimum number.
-        - If float, then `min_samples_leaf` is a fraction and
+        - If int, then consider ``min_samples_leaf`` as the minimum number.
+        - If float, then ``min_samples_leaf`` is a fraction and
           `ceil(min_samples_leaf * n_samples)` are the minimum
           number of samples for each node.
 
         .. versionchanged:: 0.18
            Added float values for fractions.
+        .. deprecated:: 0.20
+           The parameter ``min_samples_leaf`` is deprecated in version 0.20 and
+           will be fixed to a value of 1 in version 0.22. It was not effective
+           for regularization and empirically, 1 is the best value.
 
     min_weight_fraction_leaf : float, optional (default=0.)
         The minimum weighted fraction of the sum total of weights (of all
         the input samples) required to be at a leaf node. Samples have
         equal weight when sample_weight is not provided.
 
+        .. deprecated:: 0.20
+           The parameter ``min_weight_fraction_leaf`` is deprecated in version
+           0.20. Its implementation, like ``min_samples_leaf``, is ineffective
+           for regularization.
+
     max_features : int, float, string or None, optional (default="auto")
         The number of features to consider when looking for the best split:
 
@@ -1633,9 +1686,11 @@ class ExtraTreesRegressor(ForestRegressor):
     oob_score : bool, optional (default=False)
         Whether to use out-of-bag samples to estimate the R^2 on unseen data.
 
-    n_jobs : integer, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of jobs to run in parallel for both `fit` and `predict`.
-        If -1, then the number of jobs is set to the number of cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
@@ -1674,7 +1729,7 @@ class ExtraTreesRegressor(ForestRegressor):
     Notes
     -----
     The default values for the parameters controlling the size of the trees
-    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
+    (e.g. ``max_depth``, ``min_samples_split``, etc.) lead to fully grown and
     unpruned trees which can potentially be very large on some data sets. To
     reduce memory consumption, the complexity and size of the trees should be
     controlled by setting those parameter values.
@@ -1695,15 +1750,15 @@ def __init__(self,
                  criterion="mse",
                  max_depth=None,
                  min_samples_split=2,
-                 min_samples_leaf=1,
-                 min_weight_fraction_leaf=0.,
+                 min_samples_leaf='deprecated',
+                 min_weight_fraction_leaf='deprecated',
                  max_features="auto",
                  max_leaf_nodes=None,
                  min_impurity_decrease=0.,
                  min_impurity_split=None,
                  bootstrap=False,
                  oob_score=False,
-                 n_jobs=1,
+                 n_jobs=None,
                  random_state=None,
                  verbose=0,
                  warm_start=False):
@@ -1765,8 +1820,8 @@ class RandomTreesEmbedding(BaseForest):
     min_samples_split : int, float, optional (default=2)
         The minimum number of samples required to split an internal node:
 
-        - If int, then consider `min_samples_split` as the minimum number.
-        - If float, then `min_samples_split` is a fraction and
+        - If int, then consider ``min_samples_split`` as the minimum number.
+        - If float, then ``min_samples_split`` is a fraction and
           `ceil(min_samples_split * n_samples)` is the minimum
           number of samples for each split.
 
@@ -1776,19 +1831,28 @@ class RandomTreesEmbedding(BaseForest):
     min_samples_leaf : int, float, optional (default=1)
         The minimum number of samples required to be at a leaf node:
 
-        - If int, then consider `min_samples_leaf` as the minimum number.
-        - If float, then `min_samples_leaf` is a fraction and
+        - If int, then consider ``min_samples_leaf`` as the minimum number.
+        - If float, then ``min_samples_leaf`` is a fraction and
           `ceil(min_samples_leaf * n_samples)` is the minimum
           number of samples for each node.
 
         .. versionchanged:: 0.18
            Added float values for fractions.
+        .. deprecated:: 0.20
+           The parameter ``min_samples_leaf`` is deprecated in version 0.20 and
+           will be fixed to a value of 1 in version 0.22. It was not effective
+           for regularization and empirically, 1 is the best value.
 
     min_weight_fraction_leaf : float, optional (default=0.)
         The minimum weighted fraction of the sum total of weights (of all
         the input samples) required to be at a leaf node. Samples have
         equal weight when sample_weight is not provided.
 
+        .. deprecated:: 0.20
+           The parameter ``min_weight_fraction_leaf`` is deprecated in version
+           0.20. Its implementation, like ``min_samples_leaf``, is ineffective
+           for regularization.
+
     max_leaf_nodes : int or None, optional (default=None)
         Grow trees with ``max_leaf_nodes`` in best-first fashion.
         Best nodes are defined as relative reduction in impurity.
@@ -1825,9 +1889,11 @@ class RandomTreesEmbedding(BaseForest):
         Whether or not to return a sparse CSR matrix, as default behavior,
         or to return a dense array compatible with dense pipeline operators.
 
-    n_jobs : integer, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of jobs to run in parallel for both `fit` and `predict`.
-        If -1, then the number of jobs is set to the number of cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
@@ -1862,13 +1928,13 @@ def __init__(self,
                  n_estimators='warn',
                  max_depth=5,
                  min_samples_split=2,
-                 min_samples_leaf=1,
-                 min_weight_fraction_leaf=0.,
+                 min_samples_leaf='deprecated',
+                 min_weight_fraction_leaf='deprecated',
                  max_leaf_nodes=None,
                  min_impurity_decrease=0.,
                  min_impurity_split=None,
                  sparse_output=True,
-                 n_jobs=1,
+                 n_jobs=None,
                  random_state=None,
                  verbose=0,
                  warm_start=False):
diff --git a/sklearn/ensemble/gradient_boosting.py b/sklearn/ensemble/gradient_boosting.py
index c76a5722c5..6e9cd843d5 100755
--- a/sklearn/ensemble/gradient_boosting.py
+++ b/sklearn/ensemble/gradient_boosting.py
@@ -25,6 +25,7 @@
 
 from abc import ABCMeta
 from abc import abstractmethod
+import warnings
 
 from .base import BaseEnsemble
 from ..base import ClassifierMixin
@@ -38,7 +39,6 @@
 import numbers
 import numpy as np
 
-from scipy import stats
 from scipy.sparse import csc_matrix
 from scipy.sparse import csr_matrix
 from scipy.sparse import issparse
@@ -91,7 +91,7 @@ def fit(self, X, y, sample_weight=None):
             Individual weights for each sample
         """
         if sample_weight is None:
-            self.quantile = stats.scoreatpercentile(y, self.alpha * 100.0)
+            self.quantile = np.percentile(y, self.alpha * 100.0)
         else:
             self.quantile = _weighted_percentile(y, sample_weight,
                                                  self.alpha * 100.0)
@@ -608,7 +608,7 @@ def __call__(self, y, pred, sample_weight=None):
         gamma = self.gamma
         if gamma is None:
             if sample_weight is None:
-                gamma = stats.scoreatpercentile(np.abs(diff), self.alpha * 100)
+                gamma = np.percentile(np.abs(diff), self.alpha * 100)
             else:
                 gamma = _weighted_percentile(np.abs(diff), sample_weight, self.alpha * 100)
 
@@ -641,7 +641,7 @@ def negative_gradient(self, y, pred, sample_weight=None, **kargs):
         pred = pred.ravel()
         diff = y - pred
         if sample_weight is None:
-            gamma = stats.scoreatpercentile(np.abs(diff), self.alpha * 100)
+            gamma = np.percentile(np.abs(diff), self.alpha * 100)
         else:
             gamma = _weighted_percentile(np.abs(diff), sample_weight, self.alpha * 100)
         gamma_mask = np.abs(diff) <= gamma
@@ -1125,13 +1125,13 @@ class BaseGradientBoosting(six.with_metaclass(ABCMeta, BaseEnsemble)):
 
     @abstractmethod
     def __init__(self, loss, learning_rate, n_estimators, criterion,
-                 min_samples_split, min_samples_leaf, min_weight_fraction_leaf,
+                 min_samples_split, min_weight_fraction_leaf,
                  max_depth, min_impurity_decrease, min_impurity_split,
                  init, subsample, max_features,
                  random_state, alpha=0.9, verbose=0, max_leaf_nodes=None,
-                 warm_start=False, presort='auto',
-                 validation_fraction=0.1, n_iter_no_change=None,
-                 tol=1e-4):
+                 min_samples_leaf='deprecated', warm_start=False,
+                 presort='auto', validation_fraction=0.1,
+                 n_iter_no_change=None, tol=1e-4):
 
         self.n_estimators = n_estimators
         self.learning_rate = learning_rate
@@ -1498,9 +1498,17 @@ def _fit_stages(self, X, y, y_pred, sample_weight, random_state,
         n_inbag = max(1, int(self.subsample * n_samples))
         loss_ = self.loss_
 
+        if self.min_weight_fraction_leaf != 'deprecated':
+            warnings.warn("'min_weight_fraction_leaf' is deprecated in 0.20 "
+                          "and will be fixed to a value of 0 in 0.22.",
+                          DeprecationWarning)
+            min_weight_fraction_leaf = self.min_weight_fraction_leaf
+        else:
+            min_weight_fraction_leaf = 0.
+
         # Set min_weight_leaf from min_weight_fraction_leaf
-        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:
-            min_weight_leaf = (self.min_weight_fraction_leaf *
+        if min_weight_fraction_leaf != 0. and sample_weight is not None:
+            min_weight_leaf = (min_weight_fraction_leaf *
                                np.sum(sample_weight))
         else:
             min_weight_leaf = 0.
@@ -1738,8 +1746,8 @@ class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):
     min_samples_split : int, float, optional (default=2)
         The minimum number of samples required to split an internal node:
 
-        - If int, then consider `min_samples_split` as the minimum number.
-        - If float, then `min_samples_split` is a fraction and
+        - If int, then consider ``min_samples_split`` as the minimum number.
+        - If float, then ``min_samples_split`` is a fraction and
           `ceil(min_samples_split * n_samples)` are the minimum
           number of samples for each split.
 
@@ -1749,19 +1757,28 @@ class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):
     min_samples_leaf : int, float, optional (default=1)
         The minimum number of samples required to be at a leaf node:
 
-        - If int, then consider `min_samples_leaf` as the minimum number.
-        - If float, then `min_samples_leaf` is a fraction and
+        - If int, then consider ``min_samples_leaf`` as the minimum number.
+        - If float, then ``min_samples_leaf`` is a fraction and
           `ceil(min_samples_leaf * n_samples)` are the minimum
           number of samples for each node.
 
         .. versionchanged:: 0.18
            Added float values for fractions.
+        .. deprecated:: 0.20
+           The parameter ``min_samples_leaf`` is deprecated in version 0.20 and
+           will be fixed to a value of 1 in version 0.22. It was not effective
+           for regularization and empirically, 1 is the best value.
 
     min_weight_fraction_leaf : float, optional (default=0.)
         The minimum weighted fraction of the sum total of weights (of all
         the input samples) required to be at a leaf node. Samples have
         equal weight when sample_weight is not provided.
 
+        .. deprecated:: 0.20
+           The parameter ``min_weight_fraction_leaf`` is deprecated in version
+           0.20. Its implementation, like ``min_samples_leaf``, is ineffective
+           for regularization.
+
     max_depth : integer, optional (default=3)
         maximum depth of the individual regression estimators. The maximum
         depth limits the number of nodes in the tree. Tune this parameter
@@ -1938,7 +1955,8 @@ class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):
 
     def __init__(self, loss='deviance', learning_rate=0.1, n_estimators=100,
                  subsample=1.0, criterion='friedman_mse', min_samples_split=2,
-                 min_samples_leaf=1, min_weight_fraction_leaf=0.,
+                 min_samples_leaf='deprecated',
+                 min_weight_fraction_leaf='deprecated',
                  max_depth=3, min_impurity_decrease=0.,
                  min_impurity_split=None, init=None,
                  random_state=None, max_features=None, verbose=0,
@@ -2193,8 +2211,8 @@ class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):
     min_samples_split : int, float, optional (default=2)
         The minimum number of samples required to split an internal node:
 
-        - If int, then consider `min_samples_split` as the minimum number.
-        - If float, then `min_samples_split` is a fraction and
+        - If int, then consider ``min_samples_split`` as the minimum number.
+        - If float, then ``min_samples_split`` is a fraction and
           `ceil(min_samples_split * n_samples)` are the minimum
           number of samples for each split.
 
@@ -2204,13 +2222,17 @@ class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):
     min_samples_leaf : int, float, optional (default=1)
         The minimum number of samples required to be at a leaf node:
 
-        - If int, then consider `min_samples_leaf` as the minimum number.
-        - If float, then `min_samples_leaf` is a fraction and
+        - If int, then consider ``min_samples_leaf`` as the minimum number.
+        - If float, then ``min_samples_leaf`` is a fraction and
           `ceil(min_samples_leaf * n_samples)` are the minimum
           number of samples for each node.
 
         .. versionchanged:: 0.18
            Added float values for fractions.
+        .. deprecated:: 0.20
+           The parameter ``min_samples_leaf`` is deprecated in version 0.20 and
+           will be fixed to a value of 1 in version 0.22. It was not effective
+           for regularization and empirically, 1 is the best value.
 
     min_weight_fraction_leaf : float, optional (default=0.)
         The minimum weighted fraction of the sum total of weights (of all
@@ -2388,7 +2410,8 @@ class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):
 
     def __init__(self, loss='ls', learning_rate=0.1, n_estimators=100,
                  subsample=1.0, criterion='friedman_mse', min_samples_split=2,
-                 min_samples_leaf=1, min_weight_fraction_leaf=0.,
+                 min_samples_leaf='deprecated',
+                 min_weight_fraction_leaf='deprecated',
                  max_depth=3, min_impurity_decrease=0.,
                  min_impurity_split=None, init=None, random_state=None,
                  max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None,
diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py
index 97e60d755a..4ab267fc73 100755
--- a/sklearn/ensemble/iforest.py
+++ b/sklearn/ensemble/iforest.py
@@ -5,7 +5,6 @@
 from __future__ import division
 
 import numpy as np
-import scipy as sp
 import warnings
 from warnings import warn
 from sklearn.utils.fixes import euler_gamma
@@ -85,9 +84,11 @@ class IsolationForest(BaseBagging, OutlierMixin):
         data sampled with replacement. If False, sampling without replacement
         is performed.
 
-    n_jobs : integer, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of jobs to run in parallel for both `fit` and `predict`.
-        If -1, then the number of jobs is set to the number of cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     behaviour : str, default='old'
         Behaviour of the ``decision_function`` which can be either 'old' or
@@ -134,7 +135,7 @@ class IsolationForest(BaseBagging, OutlierMixin):
     offset_ : float
         Offset used to define the decision function from the raw scores.
         We have the relation: ``decision_function = score_samples - offset_``.
-        Assuming behaviour == 'new', offset_ is defined as follows.
+        Assuming behaviour == 'new', ``offset_`` is defined as follows.
         When the contamination parameter is set to "auto", the offset is equal
         to -0.5 as the scores of inliers are close to 0 and the scores of
         outliers are close to -1. When a contamination parameter different
@@ -142,7 +143,7 @@ class IsolationForest(BaseBagging, OutlierMixin):
         the expected number of outliers (samples with decision function < 0)
         in training.
         Assuming the behaviour parameter is set to 'old', we always have
-        offset_ = -0.5, making the decision function independent from the
+        ``offset_ = -0.5``, making the decision function independent from the
         contamination parameter.
 
     References
@@ -161,7 +162,7 @@ def __init__(self,
                  contamination="legacy",
                  max_features=1.,
                  bootstrap=False,
-                 n_jobs=1,
+                 n_jobs=None,
                  behaviour='old',
                  random_state=None,
                  verbose=0):
@@ -267,8 +268,8 @@ def fit(self, X, y=None, sample_weight=None):
                                  "'auto' when behaviour == 'old'.")
 
             self.offset_ = -0.5
-            self._threshold_ = sp.stats.scoreatpercentile(
-                self.decision_function(X), 100. * self._contamination)
+            self._threshold_ = np.percentile(self.decision_function(X),
+                                             100. * self._contamination)
 
             return self
 
@@ -281,8 +282,8 @@ def fit(self, X, y=None, sample_weight=None):
 
         # else, define offset_ wrt contamination parameter, so that the
         # threshold_ attribute is implicitly 0 and is not needed anymore:
-        self.offset_ = sp.stats.scoreatpercentile(
-            self.score_samples(X), 100. * self._contamination)
+        self.offset_ = np.percentile(self.score_samples(X),
+                                     100. * self._contamination)
 
         return self
 
diff --git a/sklearn/ensemble/partial_dependence.py b/sklearn/ensemble/partial_dependence.py
index 3c1d91c863..f8d5ca7f24 100755
--- a/sklearn/ensemble/partial_dependence.py
+++ b/sklearn/ensemble/partial_dependence.py
@@ -165,7 +165,7 @@ def partial_dependence(gbrt, target_variables, grid=None, X=None,
 
 def plot_partial_dependence(gbrt, X, features, feature_names=None,
                             label=None, n_cols=3, grid_resolution=100,
-                            percentiles=(0.05, 0.95), n_jobs=1,
+                            percentiles=(0.05, 0.95), n_jobs=None,
                             verbose=0, ax=None, line_kw=None,
                             contour_kw=None, **fig_kw):
     """Partial dependence plots for ``features``.
@@ -203,9 +203,10 @@ def plot_partial_dependence(gbrt, X, features, feature_names=None,
     percentiles : (low, high), default=(0.05, 0.95)
         The lower and upper percentile used to create the extreme values
         for the PDP axes.
-    n_jobs : int
-        The number of CPUs to use to compute the PDs. -1 means 'all CPUs'.
-        Defaults to 1.
+    n_jobs : int or None, optional (default=None)
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
     verbose : int
         Verbose output during PD computations. Defaults to 0.
     ax : Matplotlib axis object, default None
diff --git a/sklearn/ensemble/tests/test_bagging.py b/sklearn/ensemble/tests/test_bagging.py
index 505ec2f17b..608df3dc43 100755
--- a/sklearn/ensemble/tests/test_bagging.py
+++ b/sklearn/ensemble/tests/test_bagging.py
@@ -293,6 +293,8 @@ def test_bootstrap_features():
         assert_greater(boston.data.shape[1], np.unique(features).shape[0])
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_probability():
     # Predict probabilities.
     rng = check_random_state(0)
@@ -712,6 +714,8 @@ def test_oob_score_consistency():
     assert_equal(bagging.fit(X, y).oob_score_, bagging.fit(X, y).oob_score_)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_estimators_samples():
     # Check that format of estimators_samples_ is correct and that results
     # generated at fit time can be identically reproduced at a later time
@@ -748,6 +752,8 @@ def test_estimators_samples():
     assert_array_almost_equal(orig_coefs, new_coefs)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_estimators_samples_deterministic():
     # This test is a regression test to check that with a random step
     # (e.g. SparseRandomProjection) and a given random state, the results
diff --git a/sklearn/ensemble/tests/test_forest.py b/sklearn/ensemble/tests/test_forest.py
index 0a14476da8..a470913f5f 100755
--- a/sklearn/ensemble/tests/test_forest.py
+++ b/sklearn/ensemble/tests/test_forest.py
@@ -21,6 +21,10 @@
 
 import pytest
 
+from sklearn.utils import parallel_backend
+from sklearn.utils import register_parallel_backend
+from sklearn.externals.joblib.parallel import LokyBackend
+
 from sklearn.utils.testing import assert_almost_equal
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_array_equal
@@ -758,13 +762,16 @@ def check_min_samples_leaf(name):
     ForestEstimator = FOREST_ESTIMATORS[name]
 
     # test boundary value
-    assert_raises(ValueError,
-                  ForestEstimator(min_samples_leaf=-1).fit, X, y)
-    assert_raises(ValueError,
-                  ForestEstimator(min_samples_leaf=0).fit, X, y)
+    with pytest.warns(DeprecationWarning, match='min_samples_leaf'):
+        assert_raises(ValueError,
+                      ForestEstimator(min_samples_leaf=-1).fit, X, y)
+    with pytest.warns(DeprecationWarning, match='min_samples_leaf'):
+        assert_raises(ValueError,
+                      ForestEstimator(min_samples_leaf=0).fit, X, y)
 
     est = ForestEstimator(min_samples_leaf=5, n_estimators=1, random_state=0)
-    est.fit(X, y)
+    with pytest.warns(DeprecationWarning, match='min_samples_leaf'):
+        est.fit(X, y)
     out = est.estimators_[0].tree_.apply(X)
     node_counts = np.bincount(out)
     # drop inner nodes
@@ -774,7 +781,8 @@ def check_min_samples_leaf(name):
 
     est = ForestEstimator(min_samples_leaf=0.25, n_estimators=1,
                           random_state=0)
-    est.fit(X, y)
+    with pytest.warns(DeprecationWarning, match='min_samples_leaf'):
+        est.fit(X, y)
     out = est.estimators_[0].tree_.apply(X)
     node_counts = np.bincount(out)
     # drop inner nodes
@@ -807,7 +815,9 @@ def check_min_weight_fraction_leaf(name):
         if "RandomForest" in name:
             est.bootstrap = False
 
-        est.fit(X, y, sample_weight=weights)
+        with pytest.warns(DeprecationWarning,
+                          match='min_weight_fraction_leaf'):
+            est.fit(X, y, sample_weight=weights)
         out = est.estimators_[0].tree_.apply(X)
         node_weights = np.bincount(out, weights=weights)
         # drop inner nodes
@@ -1263,3 +1273,31 @@ def test_nestimators_future_warning(forest):
     # When n_estimators is a valid value not equal to the default
     est = forest(n_estimators=100)
     est = assert_no_warnings(est.fit, X, y)
+
+
+class MyBackend(LokyBackend):
+    def __init__(self, *args, **kwargs):
+        self.count = 0
+        super(MyBackend, self).__init__(*args, **kwargs)
+
+    def start_call(self):
+        self.count += 1
+        return super(MyBackend, self).start_call()
+
+
+register_parallel_backend('testing', MyBackend)
+
+
+def test_backend_respected():
+    clf = RandomForestClassifier(n_estimators=10, n_jobs=2)
+
+    with parallel_backend("testing") as (ba, _):
+        clf.fit(X, y)
+
+    assert ba.count > 0
+
+    # predict_proba requires shared memory. Ensure that's honored.
+    with parallel_backend("testing") as (ba, _):
+        clf.predict_proba(X)
+
+    assert ba.count == 0
diff --git a/sklearn/ensemble/tests/test_gradient_boosting.py b/sklearn/ensemble/tests/test_gradient_boosting.py
index 6f7654c7d6..332ab89317 100755
--- a/sklearn/ensemble/tests/test_gradient_boosting.py
+++ b/sklearn/ensemble/tests/test_gradient_boosting.py
@@ -106,17 +106,29 @@ def test_classifier_parameter_checks():
     assert_raises(ValueError,
                   GradientBoostingClassifier(min_samples_split=1.1).fit, X, y)
 
-    assert_raises(ValueError,
-                  GradientBoostingClassifier(min_samples_leaf=0).fit, X, y)
-    assert_raises(ValueError,
-                  GradientBoostingClassifier(min_samples_leaf=-1.0).fit, X, y)
-
-    assert_raises(ValueError,
-                  GradientBoostingClassifier(min_weight_fraction_leaf=-1.).fit,
-                  X, y)
-    assert_raises(ValueError,
-                  GradientBoostingClassifier(min_weight_fraction_leaf=0.6).fit,
-                  X, y)
+    with pytest.warns(DeprecationWarning, match='min_samples_leaf'):
+        assert_raises(
+            ValueError,
+            GradientBoostingClassifier(min_samples_leaf=0).fit,
+            X, y
+        )
+    with pytest.warns(DeprecationWarning, match='min_samples_leaf'):
+        assert_raises(
+            ValueError,
+            GradientBoostingClassifier(min_samples_leaf=-1.0).fit,
+            X, y
+        )
+
+    with pytest.warns(DeprecationWarning, match='min_weight_fraction_leaf'):
+        assert_raises(ValueError,
+                      GradientBoostingClassifier(
+                          min_weight_fraction_leaf=-1.).fit,
+                      X, y)
+    with pytest.warns(DeprecationWarning, match='min_weight_fraction_leaf'):
+        assert_raises(ValueError,
+                      GradientBoostingClassifier(
+                          min_weight_fraction_leaf=0.6).fit,
+                      X, y)
 
     assert_raises(ValueError,
                   GradientBoostingClassifier(subsample=0.0).fit, X, y)
diff --git a/sklearn/ensemble/tests/test_voting_classifier.py b/sklearn/ensemble/tests/test_voting_classifier.py
index f5bfdbd101..c480d8381f 100755
--- a/sklearn/ensemble/tests/test_voting_classifier.py
+++ b/sklearn/ensemble/tests/test_voting_classifier.py
@@ -28,6 +28,8 @@
 X, y = iris.data[:, 1:3], iris.target
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_estimator_init():
     eclf = VotingClassifier(estimators=[])
     msg = ('Invalid `estimators` attribute, `estimators` should be'
@@ -59,6 +61,8 @@ def test_estimator_init():
     assert_raise_message(ValueError, msg, eclf.fit, X, y)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_predictproba_hardvoting():
     eclf = VotingClassifier(estimators=[('lr1', LogisticRegression()),
                                         ('lr2', LogisticRegression())],
@@ -67,6 +71,8 @@ def test_predictproba_hardvoting():
     assert_raise_message(AttributeError, msg, eclf.predict_proba, X)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_notfitted():
     eclf = VotingClassifier(estimators=[('lr1', LogisticRegression()),
                                         ('lr2', LogisticRegression())],
@@ -76,6 +82,8 @@ def test_notfitted():
     assert_raise_message(NotFittedError, msg, eclf.predict_proba, X)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_majority_label_iris():
     """Check classification by majority label on dataset iris."""
@@ -92,7 +100,8 @@ def test_majority_label_iris():
 @pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_tie_situation():
     """Check voting classifier selects smaller class label in tie situation."""
-    clf1 = LogisticRegression(random_state=123)
+    clf1 = LogisticRegression(random_state=123, multi_class='ovr',
+                              solver='liblinear')
     clf2 = RandomForestClassifier(random_state=123)
     eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)],
                             voting='hard')
@@ -101,6 +110,8 @@ def test_tie_situation():
     assert_equal(eclf.fit(X, y).predict(X)[73], 1)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_weights_iris():
     """Check classification by average probabilities on dataset iris."""
@@ -115,6 +126,8 @@ def test_weights_iris():
     assert_almost_equal(scores.mean(), 0.93, decimal=2)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_predict_on_toy_problem():
     """Manually check predicted class labels for toy dataset."""
@@ -148,6 +161,8 @@ def test_predict_on_toy_problem():
     assert_equal(all(eclf.fit(X, y).predict(X)), all([1, 1, 1, 2, 2, 2]))
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_predict_proba_on_toy_problem():
     """Calculate predicted probabilities on toy dataset."""
@@ -216,6 +231,8 @@ def test_multilabel():
         return
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_gridsearch():
     """Check GridSearch support."""
@@ -234,6 +251,8 @@ def test_gridsearch():
     grid.fit(iris.data, iris.target)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_parallel_fit():
     """Check parallel backend of VotingClassifier on toy dataset."""
@@ -256,6 +275,8 @@ def test_parallel_fit():
     assert_array_almost_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_sample_weight():
     """Tests sample_weight parameter of VotingClassifier"""
@@ -300,6 +321,8 @@ def fit(self, X, y, *args, **sample_weight):
     eclf.fit(X, y, sample_weight=np.ones((len(y),)))
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_set_params():
     """set_params should be able to set estimators"""
@@ -335,6 +358,8 @@ def test_set_params():
                  eclf1.get_params()["lr"].get_params()['C'])
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_set_estimator_none():
     """VotingClassifier set_params should be able to set estimators as None"""
@@ -390,6 +415,8 @@ def test_set_estimator_none():
     assert_array_equal(eclf2.transform(X1), np.array([[0], [1]]))
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_estimator_weights_format():
     # Test estimator weights inputs as list and array
@@ -408,6 +435,8 @@ def test_estimator_weights_format():
     assert_array_almost_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore:The default value of n_estimators')
 def test_transform():
     """Check transform method of VotingClassifier on toy dataset."""
diff --git a/sklearn/ensemble/voting_classifier.py b/sklearn/ensemble/voting_classifier.py
index 7ce8bcd80a..da08a163f3 100755
--- a/sklearn/ensemble/voting_classifier.py
+++ b/sklearn/ensemble/voting_classifier.py
@@ -59,9 +59,11 @@ class VotingClassifier(_BaseComposition, ClassifierMixin, TransformerMixin):
         predicted class labels (`hard` voting) or class probabilities
         before averaging (`soft` voting). Uses uniform weights if `None`.
 
-    n_jobs : int, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of jobs to run in parallel for ``fit``.
-        If -1, then the number of jobs is set to the number of cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     flatten_transform : bool, optional (default=None)
         Affects shape of transform output only when voting='soft'
@@ -90,7 +92,8 @@ class VotingClassifier(_BaseComposition, ClassifierMixin, TransformerMixin):
     >>> from sklearn.linear_model import LogisticRegression
     >>> from sklearn.naive_bayes import GaussianNB
     >>> from sklearn.ensemble import RandomForestClassifier, VotingClassifier
-    >>> clf1 = LogisticRegression(random_state=1)
+    >>> clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial',
+    ...                           random_state=1)
     >>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)
     >>> clf3 = GaussianNB()
     >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
@@ -121,7 +124,7 @@ class VotingClassifier(_BaseComposition, ClassifierMixin, TransformerMixin):
     >>>
     """
 
-    def __init__(self, estimators, voting='hard', weights=None, n_jobs=1,
+    def __init__(self, estimators, voting='hard', weights=None, n_jobs=None,
                  flatten_transform=None):
         self.estimators = estimators
         self.voting = voting
diff --git a/sklearn/externals/_arff.py b/sklearn/externals/_arff.py
new file mode 100755
index 0000000000..7fb445ef9d
--- /dev/null
+++ b/sklearn/externals/_arff.py
@@ -0,0 +1,1059 @@
+# -*- coding: utf-8 -*-
+# =============================================================================
+# Federal University of Rio Grande do Sul (UFRGS)
+# Connectionist Artificial Intelligence Laboratory (LIAC)
+# Renato de Pontes Pereira - rppereira@inf.ufrgs.br
+# =============================================================================
+# Copyright (c) 2011 Renato de Pontes Pereira, renato.ppontes at gmail dot com
+#
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
+#
+# The above copyright notice and this permission notice shall be included in
+# all copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+# SOFTWARE.
+# =============================================================================
+
+'''
+The liac-arff module implements functions to read and write ARFF files in
+Python. It was created in the Connectionist Artificial Intelligence Laboratory
+(LIAC), which takes place at the Federal University of Rio Grande do Sul 
+(UFRGS), in Brazil.
+
+ARFF (Attribute-Relation File Format) is an file format specially created for
+describe datasets which are commonly used for machine learning experiments and
+softwares. This file format was created to be used in Weka, the best 
+representative software for machine learning automated experiments.
+
+An ARFF file can be divided into two sections: header and data. The Header 
+describes the metadata of the dataset, including a general description of the 
+dataset, its name and its attributes. The source below is an example of a 
+header section in a XOR dataset::
+
+    % 
+    % XOR Dataset
+    % 
+    % Created by Renato Pereira
+    %            rppereira@inf.ufrgs.br
+    %            http://inf.ufrgs.br/~rppereira
+    % 
+    % 
+    @RELATION XOR
+
+    @ATTRIBUTE input1 REAL
+    @ATTRIBUTE input2 REAL
+    @ATTRIBUTE y REAL
+
+The Data section of an ARFF file describes the observations of the dataset, in 
+the case of XOR dataset::
+
+    @DATA
+    0.0,0.0,0.0
+    0.0,1.0,1.0
+    1.0,0.0,1.0
+    1.0,1.0,0.0
+    % 
+    % 
+    % 
+
+Notice that several lines are starting with an ``%`` symbol, denoting a 
+comment, thus, lines with ``%`` at the beginning will be ignored, except by the
+description part at the beginning of the file. The declarations ``@RELATION``, 
+``@ATTRIBUTE``, and ``@DATA`` are all case insensitive and obligatory.
+
+For more information and details about the ARFF file description, consult
+http://www.cs.waikato.ac.nz/~ml/weka/arff.html
+
+
+ARFF Files in Python
+~~~~~~~~~~~~~~~~~~~~
+
+This module uses built-ins python objects to represent a deserialized ARFF 
+file. A dictionary is used as the container of the data and metadata of ARFF,
+and have the following keys:
+
+- **description**: (OPTIONAL) a string with the description of the dataset.
+- **relation**: (OBLIGATORY) a string with the name of the dataset.
+- **attributes**: (OBLIGATORY) a list of attributes with the following 
+  template::
+
+    (attribute_name, attribute_type)
+
+  the attribute_name is a string, and attribute_type must be an string
+  or a list of strings.
+- **data**: (OBLIGATORY) a list of data instances. Each data instance must be 
+  a list with values, depending on the attributes.
+
+The above keys must follow the case which were described, i.e., the keys are 
+case sensitive. The attribute type ``attribute_type`` must be one of these 
+strings (they are not case sensitive): ``NUMERIC``, ``INTEGER``, ``REAL`` or 
+``STRING``. For nominal attributes, the ``atribute_type`` must be a list of 
+strings.
+
+In this format, the XOR dataset presented above can be represented as a python 
+object as::
+
+    xor_dataset = {
+        'description': 'XOR Dataset',
+        'relation': 'XOR',
+        'attributes': [
+            ('input1', 'REAL'),
+            ('input2', 'REAL'),
+            ('y', 'REAL'),
+        ],
+        'data': [
+            [0.0, 0.0, 0.0],
+            [0.0, 1.0, 1.0],
+            [1.0, 0.0, 1.0],
+            [1.0, 1.0, 0.0]
+        ]
+    }
+
+
+Features
+~~~~~~~~
+
+This module provides several features, including:
+
+- Read and write ARFF files using python built-in structures, such dictionaries
+  and lists;
+- Supports `scipy.sparse.coo <http://docs.scipy
+  .org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html#scipy.sparse.coo_matrix>`_
+  and lists of dictionaries as used by SVMLight
+- Supports the following attribute types: NUMERIC, REAL, INTEGER, STRING, and
+  NOMINAL;
+- Has an interface similar to other built-in modules such as ``json``, or 
+  ``zipfile``;
+- Supports read and write the descriptions of files;
+- Supports missing values and names with spaces;
+- Supports unicode values and names;
+- Fully compatible with Python 2.7+, Python 3.3+, pypy and pypy3;
+- Under `MIT License <http://opensource.org/licenses/MIT>`_
+
+'''
+__author__ = 'Renato de Pontes Pereira, Matthias Feurer, Joel Nothman'
+__author_email__ = ('renato.ppontes@gmail.com, '
+                    'feurerm@informatik.uni-freiburg.de, '
+                    'joel.nothman@gmail.com')
+__version__ = '2.3.1'
+
+import re
+import sys
+import csv
+
+# CONSTANTS ===================================================================
+_SIMPLE_TYPES = ['NUMERIC', 'REAL', 'INTEGER', 'STRING']
+
+_TK_DESCRIPTION = '%'
+_TK_COMMENT     = '%'
+_TK_RELATION    = '@RELATION'
+_TK_ATTRIBUTE   = '@ATTRIBUTE'
+_TK_DATA        = '@DATA'
+
+_RE_RELATION     = re.compile(r'^([^\{\}%,\s]*|\".*\"|\'.*\')$', re.UNICODE)
+_RE_ATTRIBUTE    = re.compile(r'^(\".*\"|\'.*\'|[^\{\}%,\s]*)\s+(.+)$', re.UNICODE)
+_RE_TYPE_NOMINAL = re.compile(r'^\{\s*((\".*\"|\'.*\'|\S*)\s*,\s*)*(\".*\"|\'.*\'|\S*)\s*\}$', re.UNICODE)
+_RE_QUOTE_CHARS = re.compile(r'["\'\\ \t%,]')
+_RE_ESCAPE_CHARS = re.compile(r'(?=["\'\\%])')  # don't need to capture anything
+_RE_SPARSE_LINE = re.compile(r'^\{.*\}$')
+_RE_NONTRIVIAL_DATA = re.compile('["\'{}\\s]')
+
+
+def _build_re_values():
+    quoted_re = r'''
+                    "      # open quote followed by zero or more of:
+                    (?:
+                        (?<!\\)    # no additional backslash
+                        (?:\\\\)*  # maybe escaped backslashes
+                        \\"        # escaped quote
+                    |
+                        \\[^"]     # escaping a non-quote
+                    |
+                        [^"\\]     # non-quote char
+                    )*
+                    "      # close quote
+                    '''
+    # a value is surrounded by " or by ' or contains no quotables
+    value_re = r'''(?:
+        %s|          # a value may be surrounded by "
+        %s|          # or by '
+        [^,\s"'{}]+  # or may contain no characters requiring quoting
+        )''' % (quoted_re,
+                quoted_re.replace('"', "'"))
+
+    # This captures (value, error) groups. Because empty values are allowed,
+    # we cannot just look for empty values to handle syntax errors.
+    # We presume the line has had ',' prepended...
+    dense = re.compile(r'''(?x)
+        ,                # may follow ','
+        \s*
+        ((?=,)|$|%(value_re)s)  # empty or value
+        |
+        (\S.*)           # error
+        ''' % {'value_re': value_re})
+
+    # This captures (key, value) groups and will have an empty key/value
+    # in case of syntax errors.
+    # It does not ensure that the line starts with '{' or ends with '}'.
+    sparse = re.compile(r'''(?x)
+        (?:^\s*\{|,)   # may follow ',', or '{' at line start
+        \s*
+        (\d+)          # attribute key
+        \s+
+        (%(value_re)s) # value
+        |
+        (?!}\s*$)      # not an error if it's }$
+        (?!^\s*{\s*}\s*$)  # not an error if it's ^{}$
+        \S.*           # error
+        ''' % {'value_re': value_re})
+    return dense, sparse
+
+
+_RE_DENSE_VALUES, _RE_SPARSE_KEY_VALUES = _build_re_values()
+
+
+def _unquote(v):
+    if v[:1] in ('"', "'"):
+        return re.sub(r'\\(.)', r'\1', v[1:-1])
+    elif v in ('?', ''):
+        return None
+    else:
+        return v
+
+
+def _parse_values(s):
+    '''(INTERNAL) Split a line into a list of values'''
+    if not _RE_NONTRIVIAL_DATA.search(s):
+        # Fast path for trivial cases (unfortunately we have to handle missing
+        # values because of the empty string case :(.)
+        return [None if s in ('?', '') else s
+                for s in next(csv.reader([s]))]
+
+    # _RE_DENSE_VALUES tokenizes despite quoting, whitespace, etc.
+    values, errors = zip(*_RE_DENSE_VALUES.findall(',' + s))
+    if not any(errors):
+        return [_unquote(v) for v in values]
+    if _RE_SPARSE_LINE.match(s):
+        try:
+            return {int(k): _unquote(v)
+                    for k, v in _RE_SPARSE_KEY_VALUES.findall(s)}
+        except ValueError as exc:
+            # an ARFF syntax error in sparse data
+            for match in _RE_SPARSE_KEY_VALUES.finditer(s):
+                if not match.group(1):
+                    raise BadLayout('Error parsing %r' % match.group())
+            raise BadLayout('Unknown parsing error')
+    else:
+        # an ARFF syntax error
+        for match in _RE_DENSE_VALUES.finditer(s):
+            if match.group(2):
+                raise BadLayout('Error parsing %r' % match.group())
+        raise BadLayout('Unknown parsing error')
+
+
+DENSE = 0   # Constant value representing a dense matrix
+COO = 1     # Constant value representing a sparse matrix in coordinate format
+LOD = 2     # Constant value representing a sparse matrix in list of
+            # dictionaries format
+_SUPPORTED_DATA_STRUCTURES = [DENSE, COO, LOD]
+
+# =============================================================================
+
+# COMPATIBILITY WITH PYTHON 3 =================================================
+PY3 = sys.version_info[0] == 3
+if PY3:
+    unicode = str
+    basestring = str
+    xrange = range
+# COMPABILITY WITH PYTHON 2 ===================================================
+# =============================================================================
+PY2 = sys.version_info[0] == 2
+if PY2:
+    from itertools import izip as zip
+
+# EXCEPTIONS ==================================================================
+class ArffException(Exception):
+    message = None
+
+    def __init__(self):
+        self.line = -1
+
+    def __str__(self):
+        return self.message%self.line
+
+class BadRelationFormat(ArffException):
+    '''Error raised when the relation declaration is in an invalid format.'''
+    message = 'Bad @RELATION format, at line %d.'
+
+class BadAttributeFormat(ArffException):
+    '''Error raised when some attribute declaration is in an invalid format.'''
+    message = 'Bad @ATTRIBUTE format, at line %d.'
+
+class BadDataFormat(ArffException):
+    '''Error raised when some data instance is in an invalid format.'''
+    def __init__(self, value):
+        super(BadDataFormat, self).__init__()
+        self.message = (
+            'Bad @DATA instance format in line %d: ' +
+            ('%s' % value)
+        )
+
+class BadAttributeType(ArffException):
+    '''Error raised when some invalid type is provided into the attribute 
+    declaration.'''
+    message = 'Bad @ATTRIBUTE type, at line %d.'
+
+class BadAttributeName(ArffException):
+    '''Error raised when an attribute name is provided twice the attribute
+    declaration.'''
+
+    def __init__(self, value, value2):
+        super(BadAttributeName, self).__init__()
+        self.message = (
+            ('Bad @ATTRIBUTE name %s at line' % value) +
+            ' %d, this name is already in use in line' +
+            (' %d.' % value2)
+        )
+
+class BadNominalValue(ArffException):
+    '''Error raised when a value in used in some data instance but is not 
+    declared into it respective attribute declaration.'''
+
+    def __init__(self, value):
+        super(BadNominalValue, self).__init__()
+        self.message = (
+            ('Data value %s not found in nominal declaration, ' % value)
+            + 'at line %d.'
+        )
+
+class BadNominalFormatting(ArffException):
+    '''Error raised when a nominal value with space is not properly quoted.'''
+    def __init__(self, value):
+        super(BadNominalFormatting, self).__init__()
+        self.message = (
+            ('Nominal data value "%s" not properly quoted in line ' % value) +
+            '%d.'
+        )
+
+class BadNumericalValue(ArffException):
+    '''Error raised when and invalid numerical value is used in some data 
+    instance.'''
+    message = 'Invalid numerical value, at line %d.'
+
+class BadStringValue(ArffException):
+    '''Error raise when a string contains space but is not quoted.'''
+    message = 'Invalid string value at line %d.'
+
+class BadLayout(ArffException):
+    '''Error raised when the layout of the ARFF file has something wrong.'''
+    message = 'Invalid layout of the ARFF file, at line %d.'
+
+    def __init__(self, msg=''):
+        super(BadLayout, self).__init__()
+        if msg:
+            self.message = BadLayout.message + ' ' + msg.replace('%', '%%')
+
+class BadObject(ArffException):
+    '''Error raised when the object representing the ARFF file has something 
+    wrong.'''
+
+    def __str__(self):
+        return 'Invalid object.'
+
+class BadObject(ArffException):
+    '''Error raised when the object representing the ARFF file has something 
+    wrong.'''
+    def __init__(self, msg=''):
+        self.msg = msg
+
+    def __str__(self):
+        return '%s'%self.msg
+# =============================================================================
+
+# INTERNAL ====================================================================
+def encode_string(s):
+    if _RE_QUOTE_CHARS.search(s):
+        return u"'%s'" % _RE_ESCAPE_CHARS.sub(r'\\', s)
+    return s
+
+
+class EncodedNominalConversor(object):
+    def __init__(self, values):
+        self.values = {v: i for i, v in enumerate(values)}
+        self.values[0] = 0
+
+    def __call__(self, value):
+        try:
+            return self.values[value]
+        except KeyError:
+            raise BadNominalValue(value)
+
+
+class NominalConversor(object):
+    def __init__(self, values):
+        self.values = set(values)
+        self.zero_value = values[0]
+
+    def __call__(self, value):
+        if value not in self.values:
+            if value == 0:
+                # Sparse decode
+                # See issue #52: nominals should take their first value when
+                # unspecified in a sparse matrix. Naturally, this is consistent
+                # with EncodedNominalConversor.
+                return self.zero_value
+            raise BadNominalValue(value)
+        return unicode(value)
+
+
+class Data(object):
+    '''Internal helper class to allow for different matrix types without
+    making the code a huge collection of if statements.'''
+    def __init__(self):
+        self.data = []
+
+    def decode_data(self, s, conversors):
+        values = _parse_values(s)
+
+        if isinstance(values, dict):
+            if max(values) >= len(conversors):
+                raise BadDataFormat(s)
+            # XXX: int 0 is used for implicit values, not '0'
+            values = [values[i] if i in values else 0 for i in
+                      xrange(len(conversors))]
+        else:
+            if len(values) != len(conversors):
+                raise BadDataFormat(s)
+
+        self.data.append(self._decode_values(values, conversors))
+
+    @staticmethod
+    def _decode_values(values, conversors):
+        try:
+            values = [None if value is None else conversor(value)
+                      for conversor, value
+                      in zip(conversors, values)]
+        except ValueError as exc:
+            if 'float: ' in str(exc):
+                raise BadNumericalValue()
+        return values
+
+    def _tuplify_sparse_data(self, x):
+        if len(x) != 2:
+            raise BadDataFormat(x)
+        return (int(x[0].strip('"').strip("'")), x[1])
+
+    def encode_data(self, data, attributes):
+        '''(INTERNAL) Encodes a line of data.
+
+        Data instances follow the csv format, i.e, attribute values are
+        delimited by commas. After converted from csv.
+
+        :param data: a list of values.
+        :param attributes: a list of attributes. Used to check if data is valid.
+        :return: a string with the encoded data line.
+        '''
+        current_row = 0
+
+        for inst in data:
+            if len(inst) != len(attributes):
+                raise BadObject(
+                    'Instance %d has %d attributes, expected %d' %
+                     (current_row, len(inst), len(attributes))
+                )
+
+            new_data = []
+            for value in inst:
+                if value is None or value == u'' or value != value:
+                    s = '?'
+                else:
+                    s = encode_string(unicode(value))
+                new_data.append(s)
+
+            current_row += 1
+            yield u','.join(new_data)
+
+class COOData(Data):
+    def __init__(self):
+        self.data = ([], [], [])
+        self._current_num_data_points = 0
+
+    def decode_data(self, s, conversors):
+        values = _parse_values(s)
+
+        if not isinstance(values, dict):
+            raise BadLayout()
+        if not values:
+            self._current_num_data_points += 1
+            return
+        col, values = zip(*sorted(values.items()))
+        try:
+            values = [value if value is None else conversors[key](value)
+                      for key, value in zip(col, values)]
+        except ValueError as exc:
+            if 'float: ' in str(exc):
+                raise BadNumericalValue()
+            raise
+        except IndexError:
+            # conversor out of range
+            raise BadDataFormat(s)
+        self.data[0].extend(values)
+        self.data[1].extend([self._current_num_data_points] * len(values))
+        self.data[2].extend(col)
+
+        self._current_num_data_points += 1
+
+    def encode_data(self, data, attributes):
+        num_attributes = len(attributes)
+        new_data = []
+        current_row = 0
+
+        row = data.row
+        col = data.col
+        data = data.data
+
+        # Check if the rows are sorted
+        if not all(row[i] <= row[i + 1] for i in xrange(len(row) - 1)):
+            raise ValueError("liac-arff can only output COO matrices with "
+                             "sorted rows.")
+
+        for v, col, row in zip(data, col, row):
+            if row > current_row:
+                # Add empty rows if necessary
+                while current_row < row:
+                    yield " ".join([u"{", u','.join(new_data), u"}"])
+                    new_data = []
+                    current_row += 1
+
+            if col >= num_attributes:
+                raise BadObject(
+                    'Instance %d has at least %d attributes, expected %d' %
+                    (current_row, col + 1, num_attributes)
+                )
+
+            if v is None or v == u'' or v != v:
+                s = '?'
+            else:
+                s = encode_string(unicode(v))
+            new_data.append("%d %s" % (col, s))
+
+        yield " ".join([u"{", u','.join(new_data), u"}"])
+
+class LODData(Data):
+    def __init__(self):
+        self.data = []
+
+    def decode_data(self, s, conversors):
+        values = _parse_values(s)
+        n_conversors = len(conversors)
+
+        if not isinstance(values, dict):
+            raise BadLayout()
+        try:
+            self.data.append({key: None if value is None else conversors[key](value)
+                              for key, value in values.items()})
+        except ValueError as exc:
+            if 'float: ' in str(exc):
+                raise BadNumericalValue()
+            raise
+        except IndexError:
+            # conversor out of range
+            raise BadDataFormat(s)
+
+    def encode_data(self, data, attributes):
+        current_row = 0
+
+        num_attributes = len(attributes)
+        for row in data:
+            new_data = []
+
+            if len(row) > 0 and max(row) >= num_attributes:
+                raise BadObject(
+                    'Instance %d has %d attributes, expected %d' %
+                    (current_row, max(row) + 1, num_attributes)
+                )
+
+            for col in sorted(row):
+                v = row[col]
+                if v is None or v == u'' or v != v:
+                    s = '?'
+                else:
+                    s = encode_string(unicode(v))
+                new_data.append("%d %s" % (col, s))
+
+            current_row += 1
+            yield " ".join([u"{", u','.join(new_data), u"}"])
+
+def _get_data_object_for_decoding(matrix_type):
+    if matrix_type == DENSE:
+        return Data()
+    elif matrix_type == COO:
+        return COOData()
+    elif matrix_type == LOD:
+        return LODData()
+    else:
+        raise ValueError("Matrix type %s not supported." % str(matrix_type))
+
+def _get_data_object_for_encoding(matrix):
+    # Probably a scipy.sparse
+    if hasattr(matrix, 'format'):
+        if matrix.format == 'coo':
+            return COOData()
+        else:
+            raise ValueError('Cannot guess matrix format!')
+    elif isinstance(matrix[0], dict):
+        return LODData()
+    else:
+        return Data()
+
+# =============================================================================
+
+# ADVANCED INTERFACE ==========================================================
+class ArffDecoder(object):
+    '''An ARFF decoder.'''
+
+    def __init__(self):
+        '''Constructor.'''
+        self._conversors = []
+        self._current_line = 0
+
+    def _decode_comment(self, s):
+        '''(INTERNAL) Decodes a comment line.
+
+        Comments are single line strings starting, obligatorily, with the ``%``
+        character, and can have any symbol, including whitespaces or special
+        characters.
+
+        This method must receive a normalized string, i.e., a string without
+        padding, including the "\r\n" characters. 
+
+        :param s: a normalized string.
+        :return: a string with the decoded comment.
+        '''
+        res = re.sub('^\%( )?', '', s)
+        return res
+
+    def _decode_relation(self, s):
+        '''(INTERNAL) Decodes a relation line.
+
+        The relation declaration is a line with the format ``@RELATION 
+        <relation-name>``, where ``relation-name`` is a string. The string must
+        start with alphabetic character and must be quoted if the name includes
+        spaces, otherwise this method will raise a `BadRelationFormat` exception.
+
+        This method must receive a normalized string, i.e., a string without
+        padding, including the "\r\n" characters. 
+
+        :param s: a normalized string.
+        :return: a string with the decoded relation name.
+        '''
+        _, v = s.split(' ', 1)
+        v = v.strip()
+
+        if not _RE_RELATION.match(v):
+            raise BadRelationFormat()
+
+        res = unicode(v.strip('"\''))
+        return res
+
+    def _decode_attribute(self, s):
+        '''(INTERNAL) Decodes an attribute line.
+
+        The attribute is the most complex declaration in an arff file. All 
+        attributes must follow the template::
+
+             @attribute <attribute-name> <datatype>
+
+        where ``attribute-name`` is a string, quoted if the name contains any 
+        whitespace, and ``datatype`` can be:
+
+        - Numerical attributes as ``NUMERIC``, ``INTEGER`` or ``REAL``.
+        - Strings as ``STRING``.
+        - Dates (NOT IMPLEMENTED).
+        - Nominal attributes with format:
+
+            {<nominal-name1>, <nominal-name2>, <nominal-name3>, ...} 
+
+        The nominal names follow the rules for the attribute names, i.e., they
+        must be quoted if the name contains whitespaces.
+
+        This method must receive a normalized string, i.e., a string without
+        padding, including the "\r\n" characters. 
+
+        :param s: a normalized string.
+        :return: a tuple (ATTRIBUTE_NAME, TYPE_OR_VALUES).
+        '''
+        _, v = s.split(' ', 1)
+        v = v.strip()
+
+        # Verify the general structure of declaration
+        m = _RE_ATTRIBUTE.match(v)
+        if not m:
+            raise BadAttributeFormat()
+
+        # Extracts the raw name and type
+        name, type_ = m.groups()
+
+        # Extracts the final name
+        name = unicode(name.strip('"\''))
+
+        # Extracts the final type
+        if _RE_TYPE_NOMINAL.match(type_):
+            try:
+                type_ = _parse_values(type_.strip('{} '))
+            except Exception:
+                raise BadAttributeType()
+            if isinstance(type_, dict):
+                raise BadAttributeType()
+
+        else:
+            # If not nominal, verify the type name
+            type_ = unicode(type_).upper()
+            if type_ not in ['NUMERIC', 'REAL', 'INTEGER', 'STRING']:
+                raise BadAttributeType()
+
+        return (name, type_)
+
+    def _decode(self, s, encode_nominal=False, matrix_type=DENSE):
+        '''Do the job the ``encode``.'''
+
+        # Make sure this method is idempotent
+        self._current_line = 0
+
+        # If string, convert to a list of lines
+        if isinstance(s, basestring):
+            s = s.strip('\r\n ').replace('\r\n', '\n').split('\n')
+
+        # Create the return object
+        obj = {
+            u'description': u'',
+            u'relation': u'',
+            u'attributes': [],
+            u'data': []
+        }
+        attribute_names = {}
+
+        # Create the data helper object
+        data = _get_data_object_for_decoding(matrix_type)
+
+        # Read all lines
+        STATE = _TK_DESCRIPTION
+        for row in s:
+            self._current_line += 1
+            # Ignore empty lines
+            row = row.strip(' \r\n')
+            if not row: continue
+
+            u_row = row.upper()
+
+            # DESCRIPTION -----------------------------------------------------
+            if u_row.startswith(_TK_DESCRIPTION) and STATE == _TK_DESCRIPTION:
+                obj['description'] += self._decode_comment(row) + '\n'
+            # -----------------------------------------------------------------
+
+            # RELATION --------------------------------------------------------
+            elif u_row.startswith(_TK_RELATION):
+                if STATE != _TK_DESCRIPTION:
+                    raise BadLayout()
+
+                STATE = _TK_RELATION
+                obj['relation'] = self._decode_relation(row)
+            # -----------------------------------------------------------------
+
+            # ATTRIBUTE -------------------------------------------------------
+            elif u_row.startswith(_TK_ATTRIBUTE):
+                if STATE != _TK_RELATION and STATE != _TK_ATTRIBUTE:
+                    raise BadLayout()
+
+                STATE = _TK_ATTRIBUTE
+
+                attr = self._decode_attribute(row)
+                if attr[0] in attribute_names:
+                    raise BadAttributeName(attr[0], attribute_names[attr[0]])
+                else:
+                    attribute_names[attr[0]] = self._current_line
+                obj['attributes'].append(attr)
+
+                if isinstance(attr[1], (list, tuple)):
+                    if encode_nominal:
+                        conversor = EncodedNominalConversor(attr[1])
+                    else:
+                        conversor = NominalConversor(attr[1])
+                else:
+                    CONVERSOR_MAP = {'STRING': unicode,
+                                     'INTEGER': lambda x: int(float(x)),
+                                     'NUMERIC': float,
+                                     'REAL': float}
+                    conversor = CONVERSOR_MAP[attr[1]]
+
+                self._conversors.append(conversor)
+            # -----------------------------------------------------------------
+
+            # DATA ------------------------------------------------------------
+            elif u_row.startswith(_TK_DATA):
+                if STATE != _TK_ATTRIBUTE:
+                    raise BadLayout()
+
+                STATE = _TK_DATA
+            # -----------------------------------------------------------------
+
+            # COMMENT ---------------------------------------------------------
+            elif u_row.startswith(_TK_COMMENT):
+                pass
+            # -----------------------------------------------------------------
+
+            # DATA INSTANCES --------------------------------------------------
+            elif STATE == _TK_DATA:
+                data.decode_data(row, self._conversors)
+            # -----------------------------------------------------------------
+
+            # UNKNOWN INFORMATION ---------------------------------------------
+            else:
+                raise BadLayout()
+            # -----------------------------------------------------------------
+
+        # Alter the data object
+        obj['data'] = data.data
+        if obj['description'].endswith('\n'):
+            obj['description'] = obj['description'][:-1]
+
+        return obj
+
+    def decode(self, s, encode_nominal=False, return_type=DENSE):
+        '''Returns the Python representation of a given ARFF file.
+
+        When a file object is passed as an argument, this method reads lines
+        iteratively, avoiding to load unnecessary information to the memory.
+
+        :param s: a string or file object with the ARFF file.
+        :param encode_nominal: boolean, if True perform a label encoding
+            while reading the .arff file.
+        :param return_type: determines the data structure used to store the
+            dataset. Can be one of `arff.DENSE`, `arff.COO` and `arff.LOD`.
+            Consult the section on `working with sparse data`_
+        '''
+        try:
+            return self._decode(s, encode_nominal=encode_nominal,
+                                matrix_type=return_type)
+        except ArffException as e:
+            e.line = self._current_line
+            raise e
+
+
+class ArffEncoder(object):
+    '''An ARFF encoder.'''
+
+    def _encode_comment(self, s=''):
+        '''(INTERNAL) Encodes a comment line.
+
+        Comments are single line strings starting, obligatorily, with the ``%``
+        character, and can have any symbol, including whitespaces or special
+        characters.
+
+        If ``s`` is None, this method will simply return an empty comment.
+
+        :param s: (OPTIONAL) string.
+        :return: a string with the encoded comment line.
+        '''
+        if s:
+            return u'%s %s'%(_TK_COMMENT, s)
+        else:
+            return u'%s' % _TK_COMMENT
+
+    def _encode_relation(self, name):
+        '''(INTERNAL) Decodes a relation line.
+
+        The relation declaration is a line with the format ``@RELATION 
+        <relation-name>``, where ``relation-name`` is a string. 
+
+        :param name: a string.
+        :return: a string with the encoded relation declaration.
+        '''
+        for char in ' %{},':
+            if char in name:
+                name = '"%s"'%name
+                break
+
+        return u'%s %s'%(_TK_RELATION, name)
+
+    def _encode_attribute(self, name, type_):
+        '''(INTERNAL) Encodes an attribute line.
+
+        The attribute follow the template::
+
+             @attribute <attribute-name> <datatype>
+
+        where ``attribute-name`` is a string, and ``datatype`` can be:
+
+        - Numerical attributes as ``NUMERIC``, ``INTEGER`` or ``REAL``.
+        - Strings as ``STRING``.
+        - Dates (NOT IMPLEMENTED).
+        - Nominal attributes with format:
+
+            {<nominal-name1>, <nominal-name2>, <nominal-name3>, ...} 
+
+        This method must receive a the name of the attribute and its type, if
+        the attribute type is nominal, ``type`` must be a list of values.
+
+        :param name: a string.
+        :param type_: a string or a list of string.
+        :return: a string with the encoded attribute declaration.
+        '''
+        for char in ' %{},':
+            if char in name:
+                name = '"%s"'%name
+                break
+
+        if isinstance(type_, (tuple, list)):
+            type_tmp = []
+            for i in range(len(type_)):
+                type_tmp.append(u'%s' % encode_string(type_[i]))
+            type_ = u'{%s}'%(u', '.join(type_tmp))
+
+        return u'%s %s %s'%(_TK_ATTRIBUTE, name, type_)
+
+    def encode(self, obj):
+        '''Encodes a given object to an ARFF file.
+
+        :param obj: the object containing the ARFF information.
+        :return: the ARFF file as an unicode string.
+        '''
+        data = [row for row in self.iter_encode(obj)]
+
+        return u'\n'.join(data)
+
+    def iter_encode(self, obj):
+        '''The iterative version of `arff.ArffEncoder.encode`.
+
+        This encodes iteratively a given object and return, one-by-one, the 
+        lines of the ARFF file.
+
+        :param obj: the object containing the ARFF information.
+        :return: (yields) the ARFF file as unicode strings.
+        '''
+        # DESCRIPTION
+        if obj.get('description', None):
+            for row in obj['description'].split('\n'):
+                yield self._encode_comment(row)
+
+        # RELATION
+        if not obj.get('relation'):
+            raise BadObject('Relation name not found or with invalid value.')
+
+        yield self._encode_relation(obj['relation'])
+        yield u''
+
+        # ATTRIBUTES
+        if not obj.get('attributes'):
+            raise BadObject('Attributes not found.')
+
+        attribute_names = set()
+        for attr in obj['attributes']:
+            # Verify for bad object format
+            if not isinstance(attr, (tuple, list)) or \
+               len(attr) != 2 or \
+               not isinstance(attr[0], basestring):
+                raise BadObject('Invalid attribute declaration "%s"'%str(attr))
+
+            if isinstance(attr[1], basestring):
+                # Verify for invalid types
+                if attr[1] not in _SIMPLE_TYPES:
+                    raise BadObject('Invalid attribute type "%s"'%str(attr))
+
+            # Verify for bad object format
+            elif not isinstance(attr[1], (tuple, list)):
+                raise BadObject('Invalid attribute type "%s"'%str(attr))
+
+            # Verify attribute name is not used twice
+            if attr[0] in attribute_names:
+                raise BadObject('Trying to use attribute name "%s" for the '
+                                'second time.' % str(attr[0]))
+            else:
+                attribute_names.add(attr[0])
+
+            yield self._encode_attribute(attr[0], attr[1])
+        yield u''
+        attributes = obj['attributes']
+
+        # DATA
+        yield _TK_DATA
+        if 'data' in obj:
+            data = _get_data_object_for_encoding(obj.get('data'))
+            for line in data.encode_data(obj.get('data'), attributes):
+                yield line
+
+        yield u''
+
+# =============================================================================
+
+# BASIC INTERFACE =============================================================
+def load(fp, encode_nominal=False, return_type=DENSE):
+    '''Load a file-like object containing the ARFF document and convert it into
+    a Python object. 
+
+    :param fp: a file-like object.
+    :param encode_nominal: boolean, if True perform a label encoding
+        while reading the .arff file.
+    :param return_type: determines the data structure used to store the
+        dataset. Can be one of `arff.DENSE`, `arff.COO` and `arff.LOD`.
+        Consult the section on `working with sparse data`_
+    :return: a dictionary.
+     '''
+    decoder = ArffDecoder()
+    return decoder.decode(fp, encode_nominal=encode_nominal,
+                          return_type=return_type)
+
+def loads(s, encode_nominal=False, return_type=DENSE):
+    '''Convert a string instance containing the ARFF document into a Python
+    object.
+
+    :param s: a string object.
+    :param encode_nominal: boolean, if True perform a label encoding
+        while reading the .arff file.
+    :param return_type: determines the data structure used to store the
+        dataset. Can be one of `arff.DENSE`, `arff.COO` and `arff.LOD`.
+        Consult the section on `working with sparse data`_
+    :return: a dictionary.
+    '''
+    decoder = ArffDecoder()
+    return decoder.decode(s, encode_nominal=encode_nominal,
+                          return_type=return_type)
+
+def dump(obj, fp):
+    '''Serialize an object representing the ARFF document to a given file-like 
+    object.
+
+    :param obj: a dictionary.
+    :param fp: a file-like object.
+    '''
+    encoder = ArffEncoder()
+    generator = encoder.iter_encode(obj)
+
+    last_row = next(generator)
+    for row in generator:
+        fp.write(last_row + u'\n')
+        last_row = row
+    fp.write(last_row)
+
+    return fp
+
+def dumps(obj):
+    '''Serialize an object representing the ARFF document, returning a string.
+
+    :param obj: a dictionary.
+    :return: a string with the ARFF document.
+    '''
+    encoder = ArffEncoder()
+    return encoder.encode(obj)
+# =============================================================================
diff --git a/sklearn/externals/joblib/__init__.py b/sklearn/externals/joblib/__init__.py
index 6561afbc21..1b5938350e 100755
--- a/sklearn/externals/joblib/__init__.py
+++ b/sklearn/externals/joblib/__init__.py
@@ -1,27 +1,25 @@
 """Joblib is a set of tools to provide **lightweight pipelining in
-Python**. In particular, joblib offers:
+Python**. In particular:
 
-1. transparent disk-caching of the output values and lazy re-evaluation
+1. transparent disk-caching of functions and lazy re-evaluation
    (memoize pattern)
 
 2. easy simple parallel computing
 
-3. logging and tracing of the execution
-
 Joblib is optimized to be **fast** and **robust** in particular on large
 data and has specific optimizations for `numpy` arrays. It is
 **BSD-licensed**.
 
 
-    ========================= ================================================
-    **User documentation:**        http://pythonhosted.org/joblib
+    ==================== ===============================================
+    **Documentation:**       http://pythonhosted.org/joblib
 
-    **Download packages:**         http://pypi.python.org/pypi/joblib#downloads
+    **Download:**            http://pypi.python.org/pypi/joblib#downloads
 
-    **Source code:**               http://github.com/joblib/joblib
+    **Source code:**         http://github.com/joblib/joblib
 
-    **Report issues:**             http://github.com/joblib/joblib/issues
-    ========================= ================================================
+    **Report issues:**       http://github.com/joblib/joblib/issues
+    ==================== ===============================================
 
 
 Vision
@@ -43,9 +41,8 @@
     good for resuming an application status or computational job, eg
     after a crash.
 
-Joblib strives to address these problems while **leaving your code and
-your flow control as unmodified as possible** (no framework, no new
-paradigms).
+Joblib addresses these problems while **leaving your code and your flow
+control as unmodified as possible** (no framework, no new paradigms).
 
 Main features
 ------------------
@@ -58,17 +55,18 @@
    inputs and  outputs: Python functions. Joblib can save their
    computation to disk and rerun it only if necessary::
 
-      >>> from sklearn.utils import Memory
-      >>> mem = Memory(cachedir='/tmp/joblib')
+      >>> from sklearn.externals.joblib import Memory
+      >>> cachedir = 'your_cache_dir_goes_here'
+      >>> mem = Memory(cachedir)
       >>> import numpy as np
       >>> a = np.vander(np.arange(3)).astype(np.float)
       >>> square = mem.cache(np.square)
       >>> b = square(a)                                   # doctest: +ELLIPSIS
       ________________________________________________________________________________
       [Memory] Calling square...
-      square(array([[ 0.,  0.,  1.],
-             [ 1.,  1.,  1.],
-             [ 4.,  2.,  1.]]))
+      square(array([[0., 0., 1.],
+             [1., 1., 1.],
+             [4., 2., 1.]]))
       ___________________________________________________________square - 0...s, 0.0min
 
       >>> c = square(a)
@@ -77,25 +75,18 @@
 2) **Embarrassingly parallel helper:** to make it easy to write readable
    parallel code and debug it quickly::
 
-      >>> from sklearn.utils import Parallel, delayed
+      >>> from sklearn.externals.joblib import Parallel, delayed
       >>> from math import sqrt
       >>> Parallel(n_jobs=1)(delayed(sqrt)(i**2) for i in range(10))
       [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]
 
 
-3) **Logging/tracing:** The different functionalities will
-   progressively acquire better logging mechanism to help track what
-   has been ran, and capture I/O easily. In addition, Joblib will
-   provide a few I/O primitives, to easily define logging and
-   display streams, and provide a way of compiling a report.
-   We want to be able to quickly inspect what has been run.
-
-4) **Fast compressed Persistence**: a replacement for pickle to work
+3) **Fast compressed Persistence**: a replacement for pickle to work
    efficiently on Python objects containing large data (
    *joblib.dump* & *joblib.load* ).
 
 ..
-    >>> import shutil ; shutil.rmtree('/tmp/joblib/')
+    >>> import shutil ; shutil.rmtree(cachedir)
 
 """
 
@@ -115,15 +106,16 @@
 # Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
 # 'X.Y.dev0' is the canonical version of 'X.Y.dev'
 #
-__version__ = '0.11'
+__version__ = '0.12.2'
 
 
-from .memory import Memory, MemorizedResult
+from .memory import Memory, MemorizedResult, register_store_backend
 from .logger import PrintTime
 from .logger import Logger
 from .hashing import hash
 from .numpy_pickle import dump
 from .numpy_pickle import load
+from .compressor import register_compressor
 from .parallel import Parallel
 from .parallel import delayed
 from .parallel import cpu_count
@@ -134,4 +126,5 @@
 
 __all__ = ['Memory', 'MemorizedResult', 'PrintTime', 'Logger', 'hash', 'dump',
            'load', 'Parallel', 'delayed', 'cpu_count', 'effective_n_jobs',
-           'register_parallel_backend', 'parallel_backend']
+           'register_parallel_backend', 'parallel_backend',
+           'register_store_backend', 'register_compressor']
diff --git a/sklearn/externals/joblib/_dask.py b/sklearn/externals/joblib/_dask.py
new file mode 100755
index 0000000000..92b9627d8e
--- /dev/null
+++ b/sklearn/externals/joblib/_dask.py
@@ -0,0 +1,259 @@
+from __future__ import print_function, division, absolute_import
+
+import contextlib
+
+from uuid import uuid4
+import weakref
+
+from .parallel import AutoBatchingMixin, ParallelBackendBase, BatchedCalls
+from .parallel import parallel_backend
+
+try:
+    import distributed
+except ImportError:
+    distributed = None
+
+if distributed is not None:
+    from distributed.client import Client, _wait
+    from distributed.utils import funcname, itemgetter
+    from distributed import get_client, secede, rejoin
+    from distributed.worker import thread_state
+    from distributed.sizeof import sizeof
+    from tornado import gen
+
+
+def is_weakrefable(obj):
+    try:
+        weakref.ref(obj)
+        return True
+    except TypeError:
+        return False
+
+
+class _WeakKeyDictionary:
+    """A variant of weakref.WeakKeyDictionary for unhashable objects.
+
+    This datastructure is used to store futures for broadcasted data objects
+    such as large numpy arrays or pandas dataframes that are not hashable and
+    therefore cannot be used as keys of traditional python dicts.
+
+    Futhermore using a dict with id(array) as key is not safe because the
+    Python is likely to reuse id of recently collected arrays.
+    """
+
+    def __init__(self):
+        self._data = {}
+
+    def __getitem__(self, obj):
+        ref, val = self._data[id(obj)]
+        if ref() is not obj:
+            # In case of a race condition with on_destroy.
+            raise KeyError(obj)
+        return val
+
+    def __setitem__(self, obj, value):
+        key = id(obj)
+        try:
+            ref, _ = self._data[key]
+            if ref() is not obj:
+                # In case of race condition with on_destroy.
+                raise KeyError(obj)
+        except KeyError:
+            # Insert the new entry in the mapping along with a weakref
+            # callback to automatically delete the entry from the mapping
+            # as soon as the object used as key is garbage collected.
+            def on_destroy(_):
+                del self._data[key]
+            ref = weakref.ref(obj, on_destroy)
+        self._data[key] = ref, value
+
+    def __len__(self):
+        return len(self._data)
+
+    def clear(self):
+        self._data.clear()
+
+
+def _funcname(x):
+    try:
+        if isinstance(x, BatchedCalls):
+            x = x.items[0][0]
+    except Exception:
+        pass
+    return funcname(x)
+
+
+class Batch(object):
+    def __init__(self, tasks):
+        self.tasks = tasks
+
+    def __call__(self, *data):
+        results = []
+        with parallel_backend('dask'):
+            for func, args, kwargs in self.tasks:
+                args = [a(data) if isinstance(a, itemgetter) else a
+                        for a in args]
+                kwargs = {k: v(data) if isinstance(v, itemgetter) else v
+                          for (k, v) in kwargs.items()}
+                results.append(func(*args, **kwargs))
+        return results
+
+    def __reduce__(self):
+        return Batch, (self.tasks,)
+
+
+class DaskDistributedBackend(ParallelBackendBase, AutoBatchingMixin):
+    MIN_IDEAL_BATCH_DURATION = 0.2
+    MAX_IDEAL_BATCH_DURATION = 1.0
+
+    def __init__(self, scheduler_host=None, scatter=None,
+                 client=None, loop=None, **submit_kwargs):
+        if client is None:
+            if scheduler_host:
+                client = Client(scheduler_host, loop=loop,
+                                set_as_default=False)
+            else:
+                try:
+                    client = get_client()
+                except ValueError:
+                    msg = ("To use Joblib with Dask first create a Dask Client"
+                           "\n\n"
+                           "    from dask.distributed import Client\n"
+                           "    client = Client()\n"
+                           "or\n"
+                           "    client = Client('scheduler-address:8786')")
+                    raise ValueError(msg)
+
+        self.client = client
+
+        if scatter is not None and not isinstance(scatter, (list, tuple)):
+            raise TypeError("scatter must be a list/tuple, got "
+                            "`%s`" % type(scatter).__name__)
+
+        if scatter is not None and len(scatter) > 0:
+            # Keep a reference to the scattered data to keep the ids the same
+            self._scatter = list(scatter)
+            scattered = self.client.scatter(scatter, broadcast=True)
+            self.data_futures = {id(x): f for x, f in zip(scatter, scattered)}
+        else:
+            self._scatter = []
+            self.data_futures = {}
+        self.task_futures = set()
+        self.submit_kwargs = submit_kwargs
+
+    def __reduce__(self):
+        return (DaskDistributedBackend, ())
+
+    def get_nested_backend(self):
+        return DaskDistributedBackend(client=self.client)
+
+    def configure(self, n_jobs=1, parallel=None, **backend_args):
+        return self.effective_n_jobs(n_jobs)
+
+    def start_call(self):
+        self.call_data_futures = _WeakKeyDictionary()
+
+    def stop_call(self):
+        # The explicit call to clear is required to break a cycling reference
+        # to the futures.
+        self.call_data_futures.clear()
+
+    def effective_n_jobs(self, n_jobs):
+        return sum(self.client.ncores().values())
+
+    def _to_func_args(self, func):
+        collected_futures = []
+        itemgetters = dict()
+
+        # Futures that are dynamically generated during a single call to
+        # Parallel.__call__.
+        call_data_futures = getattr(self, 'call_data_futures', None)
+
+        def maybe_to_futures(args):
+            for arg in args:
+                arg_id = id(arg)
+                if arg_id in itemgetters:
+                    yield itemgetters[arg_id]
+                    continue
+
+                f = self.data_futures.get(arg_id, None)
+                if f is None and call_data_futures is not None:
+                    try:
+                        f = call_data_futures[arg]
+                    except KeyError:
+                        if is_weakrefable(arg) and sizeof(arg) > 1e3:
+                            # Automatically scatter large objects to some of
+                            # the workers to avoid duplicated data transfers.
+                            # Rely on automated inter-worker data stealing if
+                            # more workers need to reuse this data
+                            # concurrently.
+                            [f] = self.client.scatter([arg])
+                            call_data_futures[arg] = f
+
+                if f is not None:
+                    getter = itemgetter(len(collected_futures))
+                    collected_futures.append(f)
+                    itemgetters[arg_id] = getter
+                    arg = getter
+                yield arg
+
+        tasks = []
+        for f, args, kwargs in func.items:
+            args = list(maybe_to_futures(args))
+            kwargs = dict(zip(kwargs.keys(),
+                              maybe_to_futures(kwargs.values())))
+            tasks.append((f, args, kwargs))
+
+        if not collected_futures:
+            return func, ()
+        return (Batch(tasks), collected_futures)
+
+    def apply_async(self, func, callback=None):
+        key = '%s-batch-%s' % (_funcname(func), uuid4().hex)
+        func, args = self._to_func_args(func)
+
+        future = self.client.submit(func, *args, key=key, **self.submit_kwargs)
+        self.task_futures.add(future)
+
+        @gen.coroutine
+        def callback_wrapper():
+            result = yield _wait([future])
+            self.task_futures.remove(future)
+            if callback is not None:
+                callback(result)  # gets called in separate thread
+
+        self.client.loop.add_callback(callback_wrapper)
+
+        ref = weakref.ref(future)  # avoid reference cycle
+
+        def get():
+            return ref().result()
+
+        future.get = get  # monkey patch to achieve AsyncResult API
+        return future
+
+    def abort_everything(self, ensure_ready=True):
+        """ Tell the client to cancel any task submitted via this instance
+
+        joblib.Parallel will never access those results
+        """
+        self.client.cancel(self.task_futures)
+        self.task_futures.clear()
+
+    @contextlib.contextmanager
+    def retrieval_context(self):
+        """Override ParallelBackendBase.retrieval_context to avoid deadlocks.
+
+        This removes thread from the worker's thread pool (using 'secede').
+        Seceding avoids deadlock in nested parallelism settings.
+        """
+        # See 'joblib.Parallel.__call__' and 'joblib.Parallel.retrieve' for how
+        # this is used.
+        if hasattr(thread_state, 'execution_state'):
+            # we are in a worker. Secede to avoid deadlock.
+            secede()
+
+        yield
+
+        if hasattr(thread_state, 'execution_state'):
+            rejoin()
diff --git a/sklearn/externals/joblib/_memmapping_reducer.py b/sklearn/externals/joblib/_memmapping_reducer.py
new file mode 100755
index 0000000000..5ba78195b2
--- /dev/null
+++ b/sklearn/externals/joblib/_memmapping_reducer.py
@@ -0,0 +1,434 @@
+"""
+Reducer using memory mapping for numpy arrays
+"""
+# Author: Thomas Moreau <thomas.moreau.2010@gmail.com>
+# Copyright: 2017, Thomas Moreau
+# License: BSD 3 clause
+
+from mmap import mmap
+import errno
+import os
+import stat
+import threading
+import atexit
+import tempfile
+import warnings
+import weakref
+from uuid import uuid4
+
+try:
+    WindowsError
+except NameError:
+    WindowsError = type(None)
+
+from pickle import whichmodule
+try:
+    # Python 2 compat
+    from cPickle import loads
+    from cPickle import dumps
+except ImportError:
+    from pickle import loads
+    from pickle import dumps
+
+from pickle import HIGHEST_PROTOCOL, PicklingError
+
+try:
+    import numpy as np
+    from numpy.lib.stride_tricks import as_strided
+except ImportError:
+    np = None
+
+from .numpy_pickle import load
+from .numpy_pickle import dump
+from .backports import make_memmap
+from .disk import delete_folder
+
+# Some system have a ramdisk mounted by default, we can use it instead of /tmp
+# as the default folder to dump big arrays to share with subprocesses.
+SYSTEM_SHARED_MEM_FS = '/dev/shm'
+
+# Minimal number of bytes available on SYSTEM_SHARED_MEM_FS to consider using
+# it as the default folder to dump big arrays to share with subprocesses.
+SYSTEM_SHARED_MEM_FS_MIN_SIZE = int(2e9)
+
+# Folder and file permissions to chmod temporary files generated by the
+# memmapping pool. Only the owner of the Python process can access the
+# temporary files and folder.
+FOLDER_PERMISSIONS = stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR
+FILE_PERMISSIONS = stat.S_IRUSR | stat.S_IWUSR
+
+
+class _WeakArrayKeyMap:
+    """A variant of weakref.WeakKeyDictionary for unhashable numpy arrays.
+
+    This datastructure will be used with numpy arrays as obj keys, therefore we
+    do not use the __get__ / __set__ methods to avoid any conflict with the
+    numpy fancy indexing syntax.
+    """
+
+    def __init__(self):
+        self._data = {}
+
+    def get(self, obj):
+        ref, val = self._data[id(obj)]
+        if ref() is not obj:
+            # In case of race condition with on_destroy: could never be
+            # triggered by the joblib tests with CPython.
+            raise KeyError(obj)
+        return val
+
+    def set(self, obj, value):
+        key = id(obj)
+        try:
+            ref, _ = self._data[key]
+            if ref() is not obj:
+                # In case of race condition with on_destroy: could never be
+                # triggered by the joblib tests with CPython.
+                raise KeyError(obj)
+        except KeyError:
+            # Insert the new entry in the mapping along with a weakref
+            # callback to automatically delete the entry from the mapping
+            # as soon as the object used as key is garbage collected.
+            def on_destroy(_):
+                del self._data[key]
+            ref = weakref.ref(obj, on_destroy)
+        self._data[key] = ref, value
+
+    def __getstate__(self):
+        raise PicklingError("_WeakArrayKeyMap is not pickleable")
+
+
+###############################################################################
+# Support for efficient transient pickling of numpy data structures
+
+
+def _get_backing_memmap(a):
+    """Recursively look up the original np.memmap instance base if any."""
+    b = getattr(a, 'base', None)
+    if b is None:
+        # TODO: check scipy sparse datastructure if scipy is installed
+        # a nor its descendants do not have a memmap base
+        return None
+
+    elif isinstance(b, mmap):
+        # a is already a real memmap instance.
+        return a
+
+    else:
+        # Recursive exploration of the base ancestry
+        return _get_backing_memmap(b)
+
+
+def _get_temp_dir(pool_folder_name, temp_folder=None):
+    """Get the full path to a subfolder inside the temporary folder.
+
+    Parameters
+    ----------
+    pool_folder_name : str
+        Sub-folder name used for the serialization of a pool instance.
+
+    temp_folder: str, optional
+        Folder to be used by the pool for memmapping large arrays
+        for sharing memory with worker processes. If None, this will try in
+        order:
+
+        - a folder pointed by the JOBLIB_TEMP_FOLDER environment
+          variable,
+        - /dev/shm if the folder exists and is writable: this is a
+          RAMdisk filesystem available by default on modern Linux
+          distributions,
+        - the default system temporary folder that can be
+          overridden with TMP, TMPDIR or TEMP environment
+          variables, typically /tmp under Unix operating systems.
+
+    Returns
+    -------
+    pool_folder : str
+       full path to the temporary folder
+    use_shared_mem : bool
+       whether the temporary folder is written to the system shared memory
+       folder or some other temporary folder.
+    """
+    use_shared_mem = False
+    if temp_folder is None:
+        temp_folder = os.environ.get('JOBLIB_TEMP_FOLDER', None)
+    if temp_folder is None:
+        if os.path.exists(SYSTEM_SHARED_MEM_FS):
+            try:
+                shm_stats = os.statvfs(SYSTEM_SHARED_MEM_FS)
+                available_nbytes = shm_stats.f_bsize * shm_stats.f_bavail
+                if available_nbytes > SYSTEM_SHARED_MEM_FS_MIN_SIZE:
+                    # Try to see if we have write access to the shared mem
+                    # folder only if it is reasonably large (that is 2GB or
+                    # more).
+                    temp_folder = SYSTEM_SHARED_MEM_FS
+                    pool_folder = os.path.join(temp_folder, pool_folder_name)
+                    if not os.path.exists(pool_folder):
+                        os.makedirs(pool_folder)
+                    use_shared_mem = True
+            except (IOError, OSError):
+                # Missing rights in the /dev/shm partition, fallback to regular
+                # temp folder.
+                temp_folder = None
+    if temp_folder is None:
+        # Fallback to the default tmp folder, typically /tmp
+        temp_folder = tempfile.gettempdir()
+    temp_folder = os.path.abspath(os.path.expanduser(temp_folder))
+    pool_folder = os.path.join(temp_folder, pool_folder_name)
+    return pool_folder, use_shared_mem
+
+
+def has_shareable_memory(a):
+    """Return True if a is backed by some mmap buffer directly or not."""
+    return _get_backing_memmap(a) is not None
+
+
+def _strided_from_memmap(filename, dtype, mode, offset, order, shape, strides,
+                         total_buffer_len):
+    """Reconstruct an array view on a memory mapped file."""
+    if mode == 'w+':
+        # Do not zero the original data when unpickling
+        mode = 'r+'
+
+    if strides is None:
+        # Simple, contiguous memmap
+        return make_memmap(filename, dtype=dtype, shape=shape, mode=mode,
+                           offset=offset, order=order)
+    else:
+        # For non-contiguous data, memmap the total enclosing buffer and then
+        # extract the non-contiguous view with the stride-tricks API
+        base = make_memmap(filename, dtype=dtype, shape=total_buffer_len,
+                           mode=mode, offset=offset, order=order)
+        return as_strided(base, shape=shape, strides=strides)
+
+
+def _reduce_memmap_backed(a, m):
+    """Pickling reduction for memmap backed arrays.
+
+    a is expected to be an instance of np.ndarray (or np.memmap)
+    m is expected to be an instance of np.memmap on the top of the ``base``
+    attribute ancestry of a. ``m.base`` should be the real python mmap object.
+    """
+    # offset that comes from the striding differences between a and m
+    a_start, a_end = np.byte_bounds(a)
+    m_start = np.byte_bounds(m)[0]
+    offset = a_start - m_start
+
+    # offset from the backing memmap
+    offset += m.offset
+
+    if m.flags['F_CONTIGUOUS']:
+        order = 'F'
+    else:
+        # The backing memmap buffer is necessarily contiguous hence C if not
+        # Fortran
+        order = 'C'
+
+    if a.flags['F_CONTIGUOUS'] or a.flags['C_CONTIGUOUS']:
+        # If the array is a contiguous view, no need to pass the strides
+        strides = None
+        total_buffer_len = None
+    else:
+        # Compute the total number of items to map from which the strided
+        # view will be extracted.
+        strides = a.strides
+        total_buffer_len = (a_end - a_start) // a.itemsize
+    return (_strided_from_memmap,
+            (m.filename, a.dtype, m.mode, offset, order, a.shape, strides,
+             total_buffer_len))
+
+
+def reduce_memmap(a):
+    """Pickle the descriptors of a memmap instance to reopen on same file."""
+    m = _get_backing_memmap(a)
+    if m is not None:
+        # m is a real mmap backed memmap instance, reduce a preserving striding
+        # information
+        return _reduce_memmap_backed(a, m)
+    else:
+        # This memmap instance is actually backed by a regular in-memory
+        # buffer: this can happen when using binary operators on numpy.memmap
+        # instances
+        return (loads, (dumps(np.asarray(a), protocol=HIGHEST_PROTOCOL),))
+
+
+class ArrayMemmapReducer(object):
+    """Reducer callable to dump large arrays to memmap files.
+
+    Parameters
+    ----------
+    max_nbytes: int
+        Threshold to trigger memmapping of large arrays to files created
+        a folder.
+    temp_folder: str
+        Path of a folder where files for backing memmapped arrays are created.
+    mmap_mode: 'r', 'r+' or 'c'
+        Mode for the created memmap datastructure. See the documentation of
+        numpy.memmap for more details. Note: 'w+' is coerced to 'r+'
+        automatically to avoid zeroing the data on unpickling.
+    verbose: int, optional, 0 by default
+        If verbose > 0, memmap creations are logged.
+        If verbose > 1, both memmap creations, reuse and array pickling are
+        logged.
+    prewarm: bool, optional, False by default.
+        Force a read on newly memmapped array to make sure that OS pre-cache it
+        memory. This can be useful to avoid concurrent disk access when the
+        same data array is passed to different worker processes.
+    """
+
+    def __init__(self, max_nbytes, temp_folder, mmap_mode, verbose=0,
+                 prewarm=True):
+        self._max_nbytes = max_nbytes
+        self._temp_folder = temp_folder
+        self._mmap_mode = mmap_mode
+        self.verbose = int(verbose)
+        self._prewarm = prewarm
+        self._memmaped_arrays = _WeakArrayKeyMap()
+
+    def __reduce__(self):
+        # The ArrayMemmapReducer is passed to the children processes: it needs
+        # to be pickled but the _WeakArrayKeyMap need to be skipped as it's
+        # only guaranteed to be consistent with the parent process memory
+        # garbage collection.
+        args = (self._max_nbytes, self._temp_folder, self._mmap_mode)
+        kwargs = {
+            'verbose': self.verbose,
+            'prewarm': self._prewarm,
+        }
+        return ArrayMemmapReducer, args, kwargs
+
+    def __call__(self, a):
+        m = _get_backing_memmap(a)
+        if m is not None:
+            # a is already backed by a memmap file, let's reuse it directly
+            return _reduce_memmap_backed(a, m)
+
+        if (not a.dtype.hasobject and self._max_nbytes is not None and
+                a.nbytes > self._max_nbytes):
+            # check that the folder exists (lazily create the pool temp folder
+            # if required)
+            try:
+                os.makedirs(self._temp_folder)
+                os.chmod(self._temp_folder, FOLDER_PERMISSIONS)
+            except OSError as e:
+                if e.errno != errno.EEXIST:
+                    raise e
+
+            try:
+                basename = self._memmaped_arrays.get(a)
+            except KeyError:
+                # Generate a new unique random filename. The process and thread
+                # ids are only useful for debugging purpose and to make it
+                # easier to cleanup orphaned files in case of hard process
+                # kill (e.g. by "kill -9" or segfault).
+                basename = "{}-{}-{}.pkl".format(
+                    os.getpid(), id(threading.current_thread()), uuid4().hex)
+                self._memmaped_arrays.set(a, basename)
+            filename = os.path.join(self._temp_folder, basename)
+
+            # In case the same array with the same content is passed several
+            # times to the pool subprocess children, serialize it only once
+
+            # XXX: implement an explicit reference counting scheme to make it
+            # possible to delete temporary files as soon as the workers are
+            # done processing this data.
+            if not os.path.exists(filename):
+                if self.verbose > 0:
+                    print("Memmapping (shape={}, dtype={}) to new file {}"
+                          .format(a.shape, a.dtype, filename))
+                for dumped_filename in dump(a, filename):
+                    os.chmod(dumped_filename, FILE_PERMISSIONS)
+
+                if self._prewarm:
+                    # Warm up the data by accessing it. This operation ensures
+                    # that the disk access required to create the memmapping
+                    # file are performed in the reducing process and avoids
+                    # concurrent memmap creation in multiple children
+                    # processes.
+                    load(filename, mmap_mode=self._mmap_mode).max()
+            elif self.verbose > 1:
+                print("Memmapping (shape={}, dtype={}) to old file {}"
+                      .format(a.shape, a.dtype, filename))
+
+            # The worker process will use joblib.load to memmap the data
+            return (load, (filename, self._mmap_mode))
+        else:
+            # do not convert a into memmap, let pickler do its usual copy with
+            # the default system pickler
+            if self.verbose > 1:
+                print("Pickling array (shape={}, dtype={})."
+                      .format(a.shape, a.dtype))
+            return (loads, (dumps(a, protocol=HIGHEST_PROTOCOL),))
+
+
+def get_memmapping_reducers(
+        pool_id, forward_reducers=None, backward_reducers=None,
+        temp_folder=None, max_nbytes=1e6, mmap_mode='r', verbose=0,
+        prewarm=False, **kwargs):
+    """Construct a pair of memmapping reducer linked to a tmpdir.
+
+    This function manage the creation and the clean up of the temporary folders
+    underlying the memory maps and should be use to get the reducers necessary
+    to construct joblib pool or executor.
+    """
+    if forward_reducers is None:
+        forward_reducers = dict()
+    if backward_reducers is None:
+        backward_reducers = dict()
+
+    # Prepare a sub-folder name for the serialization of this particular
+    # pool instance (do not create in advance to spare FS write access if
+    # no array is to be dumped):
+    pool_folder_name = "joblib_memmapping_folder_{}_{}".format(
+        os.getpid(), pool_id)
+    pool_folder, use_shared_mem = _get_temp_dir(pool_folder_name,
+                                                temp_folder)
+
+    # Register the garbage collector at program exit in case caller forgets
+    # to call terminate explicitly: note we do not pass any reference to
+    # self to ensure that this callback won't prevent garbage collection of
+    # the pool instance and related file handler resources such as POSIX
+    # semaphores and pipes
+    pool_module_name = whichmodule(delete_folder, 'delete_folder')
+
+    def _cleanup():
+        # In some cases the Python runtime seems to set delete_folder to
+        # None just before exiting when accessing the delete_folder
+        # function from the closure namespace. So instead we reimport
+        # the delete_folder function explicitly.
+        # https://github.com/joblib/joblib/issues/328
+        # We cannot just use from 'joblib.pool import delete_folder'
+        # because joblib should only use relative imports to allow
+        # easy vendoring.
+        delete_folder = __import__(
+            pool_module_name, fromlist=['delete_folder']).delete_folder
+        try:
+            delete_folder(pool_folder)
+        except WindowsError:
+            warnings.warn("Failed to clean temporary folder: {}"
+                          .format(pool_folder))
+
+    atexit.register(_cleanup)
+
+    if np is not None:
+        # Register smart numpy.ndarray reducers that detects memmap backed
+        # arrays and that is also able to dump to memmap large in-memory
+        # arrays over the max_nbytes threshold
+        if prewarm == "auto":
+            prewarm = not use_shared_mem
+        forward_reduce_ndarray = ArrayMemmapReducer(
+            max_nbytes, pool_folder, mmap_mode, verbose,
+            prewarm=prewarm)
+        forward_reducers[np.ndarray] = forward_reduce_ndarray
+        forward_reducers[np.memmap] = reduce_memmap
+
+        # Communication from child process to the parent process always
+        # pickles in-memory numpy.ndarray without dumping them as memmap
+        # to avoid confusing the caller and make it tricky to collect the
+        # temporary folder
+        backward_reduce_ndarray = ArrayMemmapReducer(
+            None, pool_folder, mmap_mode, verbose)
+        backward_reducers[np.ndarray] = backward_reduce_ndarray
+        backward_reducers[np.memmap] = reduce_memmap
+
+    return forward_reducers, backward_reducers, pool_folder
diff --git a/sklearn/externals/joblib/_parallel_backends.py b/sklearn/externals/joblib/_parallel_backends.py
index 7035f66e38..85312abec6 100755
--- a/sklearn/externals/joblib/_parallel_backends.py
+++ b/sklearn/externals/joblib/_parallel_backends.py
@@ -7,21 +7,39 @@
 import sys
 import warnings
 import threading
+import functools
+import contextlib
 from abc import ABCMeta, abstractmethod
 
 from .format_stack import format_exc
 from .my_exceptions import WorkerInterrupt, TransportableException
 from ._multiprocessing_helpers import mp
-from ._compat import with_metaclass
+from ._compat import with_metaclass, PY27
 if mp is not None:
-    from .pool import MemmapingPool
+    from .disk import delete_folder
+    from .pool import MemmappingPool
     from multiprocessing.pool import ThreadPool
+    from .executor import get_memmapping_executor
+
+    # Compat between concurrent.futures and multiprocessing TimeoutError
+    from multiprocessing import TimeoutError
+    from .externals.loky._base import TimeoutError as LokyTimeoutError
+    from .externals.loky import process_executor, cpu_count
 
 
 class ParallelBackendBase(with_metaclass(ABCMeta)):
     """Helper abc which defines all methods a ParallelBackend must implement"""
 
     supports_timeout = False
+    nesting_level = 0
+
+    def __init__(self, nesting_level=0):
+        self.nesting_level = nesting_level
+
+    SUPPORTED_CLIB_VARS = [
+        'OMP_NUM_THREADS', 'OPENBLAS_NUM_THREADS', 'MKL_NUM_THREADS',
+        'VECLIB_MAXIMUM_THREADS', 'NUMEXPR_NUM_THREADS'
+    ]
 
     @abstractmethod
     def effective_n_jobs(self, n_jobs):
@@ -45,7 +63,8 @@ def effective_n_jobs(self, n_jobs):
     def apply_async(self, func, callback=None):
         """Schedule a func to be run"""
 
-    def configure(self, n_jobs=1, parallel=None, **backend_args):
+    def configure(self, n_jobs=1, parallel=None, prefer=None, require=None,
+                  **backend_args):
         """Reconfigure the backend and return the number of workers.
 
         This makes it possible to reuse an existing backend instance for
@@ -54,8 +73,14 @@ def configure(self, n_jobs=1, parallel=None, **backend_args):
         self.parallel = parallel
         return self.effective_n_jobs(n_jobs)
 
+    def start_call(self):
+        """Call-back method called at the beginning of a Parallel call"""
+
+    def stop_call(self):
+        """Call-back method called at the end of a Parallel call"""
+
     def terminate(self):
-        """Shutdown the process or thread pool"""
+        """Shutdown the workers and free the shared memory."""
 
     def compute_batch_size(self):
         """Determine the optimal batch size"""
@@ -86,12 +111,56 @@ def abort_everything(self, ensure_ready=True):
         Setting ensure_ready to False is an optimization that can be leveraged
         when aborting tasks via killing processes from a local process pool
         managed by the backend it-self: if we expect no new tasks, there is no
-        point in re-creating a new working pool.
+        point in re-creating new workers.
         """
-        # Does nothing by default: to be overridden in subclasses when canceling
-        # tasks is possible.
+        # Does nothing by default: to be overridden in subclasses when
+        # canceling tasks is possible.
         pass
 
+    def get_nested_backend(self):
+        """Backend instance to be used by nested Parallel calls.
+
+        By default a thread-based backend is used for the first level of
+        nesting. Beyond, switch to sequential backend to avoid spawning too
+        many threads on the host.
+        """
+        nesting_level = getattr(self, 'nesting_level', 0) + 1
+        if nesting_level > 1:
+            return SequentialBackend(nesting_level=nesting_level)
+        else:
+            return ThreadingBackend(nesting_level=nesting_level)
+
+
+    @contextlib.contextmanager
+    def retrieval_context(self):
+        """Context manager to manage an execution context.
+
+        Calls to Parallel.retrieve will be made inside this context.
+
+        By default, this does nothing. It may be useful for subclasses to
+        handle nested parallelism. In particular, it may be required to avoid
+        deadlocks if a backend manages a fixed number of workers, when those
+        workers may be asked to do nested Parallel calls. Without
+        'retrieval_context' this could lead to deadlock, as all the workers
+        managed by the backend may be "busy" waiting for the nested parallel
+        calls to finish, but the backend has no free workers to execute those
+        tasks.
+        """
+        yield
+
+    @classmethod
+    def limit_clib_threads(cls, n_threads=1):
+        """Initializer to limit the number of threads used by some C-libraries.
+
+        This function set the number of threads to `n_threads` for OpenMP, MKL,
+        Accelerated and OpenBLAS libraries, that can be used with scientific
+        computing tools like numpy.
+        """
+        for var in cls.SUPPORTED_CLIB_VARS:
+            var_value = os.environ.get(var, None)
+            if var_value is None:
+                os.environ[var] = str(n_threads)
+
 
 class SequentialBackend(ParallelBackendBase):
     """A ParallelBackend which will execute all batches sequentially.
@@ -100,6 +169,9 @@ class SequentialBackend(ParallelBackendBase):
     overhead. Used when n_jobs == 1.
     """
 
+    uses_threads = True
+    supports_sharedmem = True
+
     def effective_n_jobs(self, n_jobs):
         """Determine the number of jobs which are going to run in parallel"""
         if n_jobs == 0:
@@ -113,10 +185,16 @@ def apply_async(self, func, callback=None):
             callback(result)
         return result
 
+    def get_nested_backend(self):
+        nested_level = getattr(self, 'nesting_level', 0) + 1
+        return SequentialBackend(nesting_level=nested_level)
+
 
 class PoolManagerMixin(object):
     """A helper class for managing pool of workers."""
 
+    _pool = None
+
     def effective_n_jobs(self, n_jobs):
         """Determine the number of jobs which are going to run in parallel"""
         if n_jobs == 0:
@@ -126,7 +204,7 @@ def effective_n_jobs(self, n_jobs):
             # to sequential mode
             return 1
         elif n_jobs < 0:
-            n_jobs = max(mp.cpu_count() + 1 + n_jobs, 1)
+            n_jobs = max(cpu_count() + 1 + n_jobs, 1)
         return n_jobs
 
     def terminate(self):
@@ -136,9 +214,14 @@ def terminate(self):
             self._pool.terminate()  # terminate does a join()
             self._pool = None
 
+    def _get_pool(self):
+        """Used by apply_async to make it possible to implement lazy init"""
+        return self._pool
+
     def apply_async(self, func, callback=None):
         """Schedule a func to be run"""
-        return self._pool.apply_async(SafeFunction(func), callback=callback)
+        return self._get_pool().apply_async(
+            SafeFunction(func), callback=callback)
 
     def abort_everything(self, ensure_ready=True):
         """Shutdown the pool and restart a new one with the same parameters"""
@@ -161,9 +244,13 @@ class AutoBatchingMixin(object):
     # on a single worker while other workers have no work to process any more.
     MAX_IDEAL_BATCH_DURATION = 2
 
-    # Batching counters
-    _effective_batch_size = 1
-    _smoothed_batch_duration = 0.0
+    # Batching counters default values
+    _DEFAULT_EFFECTIVE_BATCH_SIZE = 1
+    _DEFAULT_SMOOTHED_BATCH_DURATION = 0.0
+
+    def __init__(self):
+        self._effective_batch_size = self._DEFAULT_EFFECTIVE_BATCH_SIZE
+        self._smoothed_batch_duration = self._DEFAULT_SMOOTHED_BATCH_DURATION
 
     def compute_batch_size(self):
         """Determine the optimal batch size"""
@@ -207,7 +294,8 @@ def compute_batch_size(self):
             # CallBack as long as the batch_size is constant. Therefore
             # we need to reset the estimate whenever we re-tune the batch
             # size.
-            self._smoothed_batch_duration = 0
+            self._smoothed_batch_duration = \
+                self._DEFAULT_SMOOTHED_BATCH_DURATION
 
         return batch_size
 
@@ -217,7 +305,7 @@ def batch_completed(self, batch_size, duration):
             # Update the smoothed streaming estimate of the duration of a batch
             # from dispatch to completion
             old_duration = self._smoothed_batch_duration
-            if old_duration == 0:
+            if old_duration == self._DEFAULT_SMOOTHED_BATCH_DURATION:
                 # First record of duration for this batch size after the last
                 # reset.
                 new_duration = duration
@@ -227,6 +315,14 @@ def batch_completed(self, batch_size, duration):
                 new_duration = 0.8 * old_duration + 0.2 * duration
             self._smoothed_batch_duration = new_duration
 
+    def reset_batch_stats(self):
+        """Reset batch statistics to default values.
+
+        This avoids interferences with future jobs.
+        """
+        self._effective_batch_size = self._DEFAULT_EFFECTIVE_BATCH_SIZE
+        self._smoothed_batch_duration = self._DEFAULT_SMOOTHED_BATCH_DURATION
+
 
 class ThreadingBackend(PoolManagerMixin, ParallelBackendBase):
     """A ParallelBackend which will use a thread pool to execute batches in.
@@ -234,11 +330,18 @@ class ThreadingBackend(PoolManagerMixin, ParallelBackendBase):
     This is a low-overhead backend but it suffers from the Python Global
     Interpreter Lock if the called function relies a lot on Python objects.
     Mostly useful when the execution bottleneck is a compiled extension that
-    explicitly releases the GIL (for instance a Cython loop wrapped in a
-    "with nogil" block or an expensive call to a library such as NumPy).
+    explicitly releases the GIL (for instance a Cython loop wrapped in a "with
+    nogil" block or an expensive call to a library such as NumPy).
+
+    The actual thread pool is lazily initialized: the actual thread pool
+    construction is delayed to the first call to apply_async.
+
+    ThreadingBackend is used as the default backend for nested calls.
     """
 
     supports_timeout = True
+    uses_threads = True
+    supports_sharedmem = True
 
     def configure(self, n_jobs=1, parallel=None, **backend_args):
         """Build a process or thread pool and return the number of workers"""
@@ -247,9 +350,19 @@ def configure(self, n_jobs=1, parallel=None, **backend_args):
             # Avoid unnecessary overhead and use sequential backend instead.
             raise FallbackToBackend(SequentialBackend())
         self.parallel = parallel
-        self._pool = ThreadPool(n_jobs)
+        self._n_jobs = n_jobs
         return n_jobs
 
+    def _get_pool(self):
+        """Lazily initialize the thread pool
+
+        The actual pool of worker threads is only initialized at the first
+        call to apply_async.
+        """
+        if self._pool is None:
+            self._pool = ThreadPool(self._n_jobs)
+        return self._pool
+
 
 class MultiprocessingBackend(PoolManagerMixin, AutoBatchingMixin,
                              ParallelBackendBase):
@@ -283,17 +396,28 @@ def effective_n_jobs(self, n_jobs):
                     stacklevel=3)
             return 1
 
+        if process_executor._CURRENT_DEPTH > 0:
+            # Mixing loky and multiprocessing in nested loop is not supported
+            if n_jobs != 1:
+                warnings.warn(
+                    'Multiprocessing-backed parallel loops cannot be nested,'
+                    ' below loky, setting n_jobs=1',
+                    stacklevel=3)
+            return 1
+
         if not isinstance(threading.current_thread(), threading._MainThread):
             # Prevent posix fork inside in non-main posix threads
-            warnings.warn(
-                'Multiprocessing-backed parallel loops cannot be nested'
-                ' below threads, setting n_jobs=1',
-                stacklevel=3)
+            if n_jobs != 1:
+                warnings.warn(
+                    'Multiprocessing-backed parallel loops cannot be nested'
+                    ' below threads, setting n_jobs=1',
+                    stacklevel=3)
             return 1
 
         return super(MultiprocessingBackend, self).effective_n_jobs(n_jobs)
 
-    def configure(self, n_jobs=1, parallel=None, **backend_args):
+    def configure(self, n_jobs=1, parallel=None, prefer=None, require=None,
+                  **memmappingpool_args):
         """Build a process or thread pool and return the number of workers"""
         n_jobs = self.effective_n_jobs(n_jobs)
         if n_jobs == 1:
@@ -314,7 +438,8 @@ def configure(self, n_jobs=1, parallel=None, **backend_args):
 
         # Make sure to free as much memory as possible before forking
         gc.collect()
-        self._pool = MemmapingPool(n_jobs, **backend_args)
+        self._pool = MemmappingPool(
+            n_jobs, initializer=self.limit_clib_threads, **memmappingpool_args)
         self.parallel = parallel
         return n_jobs
 
@@ -324,6 +449,92 @@ def terminate(self):
         if self.JOBLIB_SPAWNED_PROCESS in os.environ:
             del os.environ[self.JOBLIB_SPAWNED_PROCESS]
 
+        self.reset_batch_stats()
+
+
+class LokyBackend(AutoBatchingMixin, ParallelBackendBase):
+    """Managing pool of workers with loky instead of multiprocessing."""
+
+    supports_timeout = True
+
+    def configure(self, n_jobs=1, parallel=None, prefer=None, require=None,
+                  idle_worker_timeout=300, **memmappingexecutor_args):
+        """Build a process executor and return the number of workers"""
+        n_jobs = self.effective_n_jobs(n_jobs)
+        if n_jobs == 1:
+            raise FallbackToBackend(SequentialBackend())
+
+        self._workers = get_memmapping_executor(
+            n_jobs, timeout=idle_worker_timeout,
+            initializer=self.limit_clib_threads,
+            **memmappingexecutor_args)
+        self.parallel = parallel
+        return n_jobs
+
+    def effective_n_jobs(self, n_jobs):
+        """Determine the number of jobs which are going to run in parallel"""
+        if n_jobs == 0:
+            raise ValueError('n_jobs == 0 in Parallel has no meaning')
+        elif mp is None or n_jobs is None:
+            # multiprocessing is not available or disabled, fallback
+            # to sequential mode
+            return 1
+        elif mp.current_process().daemon:
+            # Daemonic processes cannot have children
+            if n_jobs != 1:
+                warnings.warn(
+                    'Loky-backed parallel loops cannot be called in a'
+                    ' multiprocessing, setting n_jobs=1',
+                    stacklevel=3)
+            return 1
+        elif not isinstance(threading.current_thread(), threading._MainThread):
+            # Prevent posix fork inside in non-main posix threads
+            if n_jobs != 1:
+                warnings.warn(
+                    'Loky-backed parallel loops cannot be nested below '
+                    'threads, setting n_jobs=1',
+                    stacklevel=3)
+            return 1
+        elif n_jobs < 0:
+            n_jobs = max(cpu_count() + 1 + n_jobs, 1)
+        return n_jobs
+
+    def apply_async(self, func, callback=None):
+        """Schedule a func to be run"""
+        future = self._workers.submit(SafeFunction(func))
+        future.get = functools.partial(self.wrap_future_result, future)
+        if callback is not None:
+            future.add_done_callback(callback)
+        return future
+
+    @staticmethod
+    def wrap_future_result(future, timeout=None):
+        """Wrapper for Future.result to implement the same behaviour as
+        AsyncResults.get from multiprocessing."""
+        try:
+            return future.result(timeout=timeout)
+        except LokyTimeoutError:
+            raise TimeoutError()
+
+    def terminate(self):
+        if self._workers is not None:
+            # Terminate does not shutdown the workers as we want to reuse them
+            # in latter calls but we free as much memory as we can by deleting
+            # the shared memory
+            delete_folder(self._workers._temp_folder)
+            self._workers = None
+
+        self.reset_batch_stats()
+
+    def abort_everything(self, ensure_ready=True):
+        """Shutdown the workers and restart a new one with the same parameters
+        """
+        self._workers.shutdown(kill_workers=True)
+        delete_folder(self._workers._temp_folder)
+        self._workers = None
+        if ensure_ready:
+            self.configure(n_jobs=self.parallel.n_jobs, parallel=self.parallel)
+
 
 class ImmediateResult(object):
     def __init__(self, batch):
@@ -353,10 +564,17 @@ def __call__(self, *args, **kwargs):
             # something different, as multiprocessing does not
             # interrupt processing for a KeyboardInterrupt
             raise WorkerInterrupt()
-        except:
-            e_type, e_value, e_tb = sys.exc_info()
-            text = format_exc(e_type, e_value, e_tb, context=10, tb_offset=1)
-            raise TransportableException(text, e_type)
+        except BaseException:
+            if PY27:
+                # Capture the traceback of the worker to make it part of
+                # the final exception message.
+                e_type, e_value, e_tb = sys.exc_info()
+                text = format_exc(e_type, e_value, e_tb, context=10,
+                                  tb_offset=1)
+                raise TransportableException(text, e_type)
+            else:
+                # Rely on Python 3 built-in Remote Traceback reporting
+                raise
 
 
 class FallbackToBackend(Exception):
diff --git a/sklearn/externals/joblib/_store_backends.py b/sklearn/externals/joblib/_store_backends.py
new file mode 100755
index 0000000000..027fb9f9f7
--- /dev/null
+++ b/sklearn/externals/joblib/_store_backends.py
@@ -0,0 +1,412 @@
+"""Storage providers backends for Memory caching."""
+
+import re
+import os
+import os.path
+import datetime
+import json
+import shutil
+import warnings
+import collections
+import operator
+import threading
+from abc import ABCMeta, abstractmethod
+
+from ._compat import with_metaclass, _basestring
+from .backports import concurrency_safe_rename
+from .disk import mkdirp, memstr_to_bytes, rm_subdirs
+from . import numpy_pickle
+
+CacheItemInfo = collections.namedtuple('CacheItemInfo',
+                                       'path size last_access')
+
+
+def concurrency_safe_write(object_to_write, filename, write_func):
+    """Writes an object into a unique file in a concurrency-safe way."""
+    thread_id = id(threading.current_thread())
+    temporary_filename = '{}.thread-{}-pid-{}'.format(
+        filename, thread_id, os.getpid())
+    write_func(object_to_write, temporary_filename)
+
+    return temporary_filename
+
+
+class StoreBackendBase(with_metaclass(ABCMeta)):
+    """Helper Abstract Base Class which defines all methods that
+       a StorageBackend must implement."""
+
+    @abstractmethod
+    def _open_item(self, f, mode):
+        """Opens an item on the store and return a file-like object.
+
+        This method is private and only used by the StoreBackendMixin object.
+
+        Parameters
+        ----------
+        f: a file-like object
+            The file-like object where an item is stored and retrieved
+        mode: string, optional
+            the mode in which the file-like object is opened allowed valued are
+            'rb', 'wb'
+
+        Returns
+        -------
+        a file-like object
+        """
+
+    @abstractmethod
+    def _item_exists(self, location):
+        """Checks if an item location exists in the store.
+
+        This method is private and only used by the StoreBackendMixin object.
+
+        Parameters
+        ----------
+        location: string
+            The location of an item. On a filesystem, this corresponds to the
+            absolute path, including the filename, of a file.
+
+        Returns
+        -------
+        True if the item exists, False otherwise
+        """
+
+    @abstractmethod
+    def _move_item(self, src, dst):
+        """Moves an item from src to dst in the store.
+
+        This method is private and only used by the StoreBackendMixin object.
+
+        Parameters
+        ----------
+        src: string
+            The source location of an item
+        dst: string
+            The destination location of an item
+        """
+
+    @abstractmethod
+    def create_location(self, location):
+        """Creates a location on the store.
+
+        Parameters
+        ----------
+        location: string
+            The location in the store. On a filesystem, this corresponds to a
+            directory.
+        """
+
+    @abstractmethod
+    def clear_location(self, location):
+        """Clears a location on the store.
+
+        Parameters
+        ----------
+        location: string
+            The location in the store. On a filesystem, this corresponds to a
+            directory or a filename absolute path
+        """
+
+    @abstractmethod
+    def get_items(self):
+        """Returns the whole list of items available in the store.
+
+        Returns
+        -------
+        The list of items identified by their ids (e.g filename in a
+        filesystem).
+        """
+
+    @abstractmethod
+    def configure(self, location, verbose=0, backend_options=dict()):
+        """Configures the store.
+
+        Parameters
+        ----------
+        location: string
+            The base location used by the store. On a filesystem, this
+            corresponds to a directory.
+        verbose: int
+            The level of verbosity of the store
+        backend_options: dict
+            Contains a dictionnary of named paremeters used to configure the
+            store backend.
+        """
+
+
+class StoreBackendMixin(object):
+    """Class providing all logic for managing the store in a generic way.
+
+    The StoreBackend subclass has to implement 3 methods: create_location,
+    clear_location and configure. The StoreBackend also has to provide
+    a private _open_item, _item_exists and _move_item methods. The _open_item
+    method has to have the same signature as the builtin open and return a
+    file-like object.
+    """
+
+    def load_item(self, path, verbose=1, msg=None):
+        """Load an item from the store given its path as a list of
+           strings."""
+        full_path = os.path.join(self.location, *path)
+
+        if verbose > 1:
+            if verbose < 10:
+                print('{0}...'.format(msg))
+            else:
+                print('{0} from {1}'.format(msg, full_path))
+
+        mmap_mode = (None if not hasattr(self, 'mmap_mode')
+                     else self.mmap_mode)
+
+        filename = os.path.join(full_path, 'output.pkl')
+        if not self._item_exists(filename):
+            raise KeyError("Non-existing item (may have been "
+                           "cleared).\nFile %s does not exist" % filename)
+
+        # file-like object cannot be used when mmap_mode is set
+        if mmap_mode is None:
+            with self._open_item(filename, "rb") as f:
+                item = numpy_pickle.load(f)
+        else:
+            item = numpy_pickle.load(filename, mmap_mode=mmap_mode)
+        return item
+
+    def dump_item(self, path, item, verbose=1):
+        """Dump an item in the store at the path given as a list of
+           strings."""
+        try:
+            item_path = os.path.join(self.location, *path)
+            if not self._item_exists(item_path):
+                self.create_location(item_path)
+            filename = os.path.join(item_path, 'output.pkl')
+            if verbose > 10:
+                print('Persisting in %s' % item_path)
+
+            def write_func(to_write, dest_filename):
+                with self._open_item(dest_filename, "wb") as f:
+                    numpy_pickle.dump(to_write, f,
+                                      compress=self.compress)
+
+            self._concurrency_safe_write(item, filename, write_func)
+        except:  # noqa: E722
+            " Race condition in the creation of the directory "
+
+    def clear_item(self, path):
+        """Clear the item at the path, given as a list of strings."""
+        item_path = os.path.join(self.location, *path)
+        if self._item_exists(item_path):
+            self.clear_location(item_path)
+
+    def contains_item(self, path):
+        """Check if there is an item at the path, given as a list of
+           strings"""
+        item_path = os.path.join(self.location, *path)
+        filename = os.path.join(item_path, 'output.pkl')
+
+        return self._item_exists(filename)
+
+    def get_item_info(self, path):
+        """Return information about item."""
+        return {'location': os.path.join(self.location,
+                                         *path)}
+
+    def get_metadata(self, path):
+        """Return actual metadata of an item."""
+        try:
+            item_path = os.path.join(self.location, *path)
+            filename = os.path.join(item_path, 'metadata.json')
+            with self._open_item(filename, 'rb') as f:
+                return json.loads(f.read().decode('utf-8'))
+        except:  # noqa: E722
+            return {}
+
+    def store_metadata(self, path, metadata):
+        """Store metadata of a computation."""
+        try:
+            item_path = os.path.join(self.location, *path)
+            self.create_location(item_path)
+            filename = os.path.join(item_path, 'metadata.json')
+
+            def write_func(to_write, dest_filename):
+                with self._open_item(dest_filename, "wb") as f:
+                    f.write(json.dumps(to_write).encode('utf-8'))
+
+            self._concurrency_safe_write(metadata, filename, write_func)
+        except:  # noqa: E722
+            pass
+
+    def contains_path(self, path):
+        """Check cached function is available in store."""
+        func_path = os.path.join(self.location, *path)
+        return self.object_exists(func_path)
+
+    def clear_path(self, path):
+        """Clear all items with a common path in the store."""
+        func_path = os.path.join(self.location, *path)
+        if self._item_exists(func_path):
+            self.clear_location(func_path)
+
+    def store_cached_func_code(self, path, func_code=None):
+        """Store the code of the cached function."""
+        func_path = os.path.join(self.location, *path)
+        if not self._item_exists(func_path):
+            self.create_location(func_path)
+
+        if func_code is not None:
+            filename = os.path.join(func_path, "func_code.py")
+            with self._open_item(filename, 'wb') as f:
+                f.write(func_code.encode('utf-8'))
+
+    def get_cached_func_code(self, path):
+        """Store the code of the cached function."""
+        path += ['func_code.py', ]
+        filename = os.path.join(self.location, *path)
+        try:
+            with self._open_item(filename, 'rb') as f:
+                return f.read().decode('utf-8')
+        except:  # noqa: E722
+            raise
+
+    def get_cached_func_info(self, path):
+        """Return information related to the cached function if it exists."""
+        return {'location': os.path.join(self.location, *path)}
+
+    def clear(self):
+        """Clear the whole store content."""
+        self.clear_location(self.location)
+
+    def reduce_store_size(self, bytes_limit):
+        """Reduce store size to keep it under the given bytes limit."""
+        items_to_delete = self._get_items_to_delete(bytes_limit)
+
+        for item in items_to_delete:
+            if self.verbose > 10:
+                print('Deleting item {0}'.format(item))
+            try:
+                self.clear_location(item.path)
+            except OSError:
+                # Even with ignore_errors=True can shutil.rmtree
+                # can raise OSErrror with [Errno 116] Stale file
+                # handle if another process has deleted the folder
+                # already.
+                pass
+
+    def _get_items_to_delete(self, bytes_limit):
+        """Get items to delete to keep the store under a size limit."""
+        if isinstance(bytes_limit, _basestring):
+            bytes_limit = memstr_to_bytes(bytes_limit)
+
+        items = self.get_items()
+        size = sum(item.size for item in items)
+
+        to_delete_size = size - bytes_limit
+        if to_delete_size < 0:
+            return []
+
+        # We want to delete first the cache items that were accessed a
+        # long time ago
+        items.sort(key=operator.attrgetter('last_access'))
+
+        items_to_delete = []
+        size_so_far = 0
+
+        for item in items:
+            if size_so_far > to_delete_size:
+                break
+
+            items_to_delete.append(item)
+            size_so_far += item.size
+
+        return items_to_delete
+
+    def _concurrency_safe_write(self, to_write, filename, write_func):
+        """Writes an object into a file in a concurrency-safe way."""
+        temporary_filename = concurrency_safe_write(to_write,
+                                                    filename, write_func)
+        self._move_item(temporary_filename, filename)
+
+    def __repr__(self):
+        """Printable representation of the store location."""
+        return self.location
+
+
+class FileSystemStoreBackend(StoreBackendBase, StoreBackendMixin):
+    """A StoreBackend used with local or network file systems."""
+
+    _open_item = staticmethod(open)
+    _item_exists = staticmethod(os.path.exists)
+    _move_item = staticmethod(concurrency_safe_rename)
+
+    def clear_location(self, location):
+        """Delete location on store."""
+        if (location == self.location):
+            rm_subdirs(location)
+        else:
+            shutil.rmtree(location, ignore_errors=True)
+
+    def create_location(self, location):
+        """Create object location on store"""
+        mkdirp(location)
+
+    def get_items(self):
+        """Returns the whole list of items available in the store."""
+        items = []
+
+        for dirpath, _, filenames in os.walk(self.location):
+            is_cache_hash_dir = re.match('[a-f0-9]{32}',
+                                         os.path.basename(dirpath))
+
+            if is_cache_hash_dir:
+                output_filename = os.path.join(dirpath, 'output.pkl')
+                try:
+                    last_access = os.path.getatime(output_filename)
+                except OSError:
+                    try:
+                        last_access = os.path.getatime(dirpath)
+                    except OSError:
+                        # The directory has already been deleted
+                        continue
+
+                last_access = datetime.datetime.fromtimestamp(last_access)
+                try:
+                    full_filenames = [os.path.join(dirpath, fn)
+                                      for fn in filenames]
+                    dirsize = sum(os.path.getsize(fn)
+                                  for fn in full_filenames)
+                except OSError:
+                    # Either output_filename or one of the files in
+                    # dirpath does not exist any more. We assume this
+                    # directory is being cleaned by another process already
+                    continue
+
+                items.append(CacheItemInfo(dirpath, dirsize,
+                                           last_access))
+
+        return items
+
+    def configure(self, location, verbose=1, backend_options={}):
+        """Configure the store backend.
+
+        For this backend, valid store options are 'compress' and 'mmap_mode'
+        """
+
+        # setup location directory
+        self.location = location
+        if not os.path.exists(self.location):
+            mkdirp(self.location)
+
+        # item can be stored compressed for faster I/O
+        self.compress = backend_options['compress']
+
+        # FileSystemStoreBackend can be used with mmap_mode options under
+        # certain conditions.
+        mmap_mode = None
+        if 'mmap_mode' in backend_options:
+            mmap_mode = backend_options['mmap_mode']
+            if self.compress and mmap_mode is not None:
+                warnings.warn('Compressed items cannot be memmapped in a '
+                              'filesystem store. Option will be ignored.',
+                              stacklevel=2)
+
+        self.mmap_mode = mmap_mode
+        self.verbose = verbose
diff --git a/sklearn/externals/joblib/backports.py b/sklearn/externals/joblib/backports.py
index 7dd3df16f1..be6c9c506e 100755
--- a/sklearn/externals/joblib/backports.py
+++ b/sklearn/externals/joblib/backports.py
@@ -33,7 +33,8 @@ def make_memmap(filename, dtype='uint8', mode='r+', offset=0,
 
 
 if os.name == 'nt':
-    error_access_denied = 5
+    # https://github.com/joblib/joblib/issues/540
+    access_denied_errors = (5, 13)
     try:
         from os import replace
     except ImportError:
@@ -65,7 +66,7 @@ def concurrency_safe_rename(src, dst):
                 replace(src, dst)
                 break
             except Exception as exc:
-                if getattr(exc, 'winerror', None) == error_access_denied:
+                if getattr(exc, 'winerror', None) in access_denied_errors:
                     time.sleep(sleep_time)
                     total_sleep_time += sleep_time
                     sleep_time *= 2
diff --git a/sklearn/externals/joblib/compressor.py b/sklearn/externals/joblib/compressor.py
new file mode 100755
index 0000000000..7692fd9f28
--- /dev/null
+++ b/sklearn/externals/joblib/compressor.py
@@ -0,0 +1,594 @@
+"""Classes and functions for managing compressors."""
+
+import sys
+import io
+import zlib
+from distutils.version import LooseVersion
+
+from ._compat import _basestring, PY3_OR_LATER
+
+try:
+    from threading import RLock
+except ImportError:
+    from dummy_threading import RLock
+
+try:
+    import bz2
+except ImportError:
+    bz2 = None
+
+try:
+    import lzma
+except ImportError:
+    lzma = None
+
+try:
+    import lz4
+    if PY3_OR_LATER:
+        from lz4.frame import LZ4FrameFile
+except ImportError:
+    lz4 = None
+
+LZ4_NOT_INSTALLED_ERROR = ('LZ4 is not installed. Install it with pip: '
+                           'http://python-lz4.readthedocs.io/')
+
+# Registered compressors
+_COMPRESSORS = {}
+
+# Magic numbers of supported compression file formats.
+_ZFILE_PREFIX = b'ZF'  # used with pickle files created before 0.9.3.
+_ZLIB_PREFIX = b'\x78'
+_GZIP_PREFIX = b'\x1f\x8b'
+_BZ2_PREFIX = b'BZ'
+_XZ_PREFIX = b'\xfd\x37\x7a\x58\x5a'
+_LZMA_PREFIX = b'\x5d\x00'
+_LZ4_PREFIX = b'\x04\x22\x4D\x18'
+
+
+def register_compressor(compressor_name, compressor,
+                        force=False):
+    """Register a new compressor.
+
+    Parameters
+    -----------
+    compressor_name: str.
+        The name of the compressor.
+    compressor: CompressorWrapper
+        An instance of a 'CompressorWrapper'.
+    """
+    global _COMPRESSORS
+    if not isinstance(compressor_name, _basestring):
+        raise ValueError("Compressor name should be a string, "
+                         "'{}' given.".format(compressor_name))
+
+    if not isinstance(compressor, CompressorWrapper):
+        raise ValueError("Compressor should implement the CompressorWrapper "
+                         "interface, '{}' given.".format(compressor))
+
+    if (compressor.fileobj_factory is not None and
+            (not hasattr(compressor.fileobj_factory, 'read') or
+             not hasattr(compressor.fileobj_factory, 'write') or
+             not hasattr(compressor.fileobj_factory, 'seek') or
+             not hasattr(compressor.fileobj_factory, 'tell'))):
+        raise ValueError("Compressor 'fileobj_factory' attribute should "
+                         "implement the file object interface, '{}' given."
+                         .format(compressor.fileobj_factory))
+
+    if compressor_name in _COMPRESSORS and not force:
+        raise ValueError("Compressor '{}' already registered."
+                         .format(compressor_name))
+
+    _COMPRESSORS[compressor_name] = compressor
+
+
+class CompressorWrapper():
+    """A wrapper around a compressor file object.
+
+    Attributes
+    ----------
+    obj: a file-like object
+        The object must implement the buffer interface and will be used
+        internally to compress/decompress the data.
+    prefix: bytestring
+        A bytestring corresponding to the magic number that identifies the
+        file format associated to the compressor.
+    extention: str
+        The file extension used to automatically select this compressor during
+        a dump to a file.
+    """
+
+    def __init__(self, obj, prefix=b'', extension=''):
+        self.fileobj_factory = obj
+        self.prefix = prefix
+        self.extension = extension
+
+    def compressor_file(self, fileobj, compresslevel=None):
+        """Returns an instance of a compressor file object."""
+        if compresslevel is None:
+            return self.fileobj_factory(fileobj, 'wb')
+        else:
+            return self.fileobj_factory(fileobj, 'wb',
+                                        compresslevel=compresslevel)
+
+    def decompressor_file(self, fileobj):
+        """Returns an instance of a decompressor file object."""
+        return self.fileobj_factory(fileobj, 'rb')
+
+
+class BZ2CompressorWrapper(CompressorWrapper):
+
+    prefix = _BZ2_PREFIX
+    extension = '.bz2'
+
+    def __init__(self):
+        if bz2 is not None:
+            self.fileobj_factory = bz2.BZ2File
+        else:
+            self.fileobj_factory = None
+
+    def _check_versions(self):
+        if bz2 is None:
+            raise ValueError('bz2 module is not compiled on your python '
+                             'standard library.')
+
+    def compressor_file(self, fileobj, compresslevel=None):
+        """Returns an instance of a compressor file object."""
+        self._check_versions()
+        if compresslevel is None:
+            return self.fileobj_factory(fileobj, 'wb')
+        else:
+            return self.fileobj_factory(fileobj, 'wb',
+                                        compresslevel=compresslevel)
+
+    def decompressor_file(self, fileobj):
+        """Returns an instance of a decompressor file object."""
+        self._check_versions()
+        if PY3_OR_LATER:
+            fileobj = self.fileobj_factory(fileobj, 'rb')
+        else:
+            # In python 2, BZ2File doesn't support a fileobj opened in
+            # binary mode. In this case, we pass the filename.
+            fileobj = self.fileobj_factory(fileobj.name, 'rb')
+        return fileobj
+
+
+class LZMACompressorWrapper(CompressorWrapper):
+
+    prefix = _LZMA_PREFIX
+    extension = '.lzma'
+
+    def __init__(self):
+        if lzma is not None:
+            self.fileobj_factory = lzma.LZMAFile
+        else:
+            self.fileobj_factory = None
+
+    def compressor_file(self, fileobj, compresslevel=None):
+        """Returns an instance of a compressor file object."""
+        if compresslevel is None:
+            return self.fileobj_factory(fileobj, 'wb',
+                                        format=lzma.FORMAT_ALONE)
+        else:
+            return self.fileobj_factory(fileobj, 'wb',
+                                        format=lzma.FORMAT_ALONE,
+                                        preset=compresslevel)
+
+    def decompressor_file(self, fileobj):
+        """Returns an instance of a decompressor file object."""
+        if PY3_OR_LATER and lzma is not None:
+            # We support lzma only in python 3 because in python 2 users
+            # may have installed the pyliblzma package, which also provides
+            # the lzma module, but that unfortunately doesn't fully support
+            # the buffer interface required by joblib.
+            # See https://github.com/joblib/joblib/issues/403 for details.
+            return lzma.LZMAFile(fileobj, 'rb')
+        else:
+            raise NotImplementedError("Lzma decompression is not "
+                                      "supported for this version of "
+                                      "python ({}.{})"
+                                      .format(sys.version_info[0],
+                                              sys.version_info[1]))
+
+
+class XZCompressorWrapper(LZMACompressorWrapper):
+
+    prefix = _XZ_PREFIX
+    extension = '.xz'
+
+    def __init__(self):
+        if lzma is not None:
+            self.fileobj_factory = lzma.LZMAFile
+        else:
+            self.fileobj_factory = None
+
+    def compressor_file(self, fileobj, compresslevel=None):
+        """Returns an instance of a compressor file object."""
+        if compresslevel is None:
+            return self.fileobj_factory(fileobj, 'wb', check=lzma.CHECK_NONE)
+        else:
+            return self.fileobj_factory(fileobj, 'wb', check=lzma.CHECK_NONE,
+                                        preset=compresslevel)
+
+
+class LZ4CompressorWrapper(CompressorWrapper):
+
+    prefix = _LZ4_PREFIX
+    extension = '.lz4'
+
+    def __init__(self):
+        if PY3_OR_LATER and lz4 is not None:
+            self.fileobj_factory = LZ4FrameFile
+        else:
+            self.fileobj_factory = None
+
+    def _check_versions(self):
+        if not PY3_OR_LATER:
+            raise ValueError('lz4 compression is only available with '
+                             'python3+.')
+
+        if lz4 is None or LooseVersion(lz4.__version__) < LooseVersion('0.19'):
+            raise ValueError(LZ4_NOT_INSTALLED_ERROR)
+
+    def compressor_file(self, fileobj, compresslevel=None):
+        """Returns an instance of a compressor file object."""
+        self._check_versions()
+        if compresslevel is None:
+            return self.fileobj_factory(fileobj, 'wb')
+        else:
+            return self.fileobj_factory(fileobj, 'wb',
+                                        compression_level=compresslevel)
+
+    def decompressor_file(self, fileobj):
+        """Returns an instance of a decompressor file object."""
+        self._check_versions()
+        return self.fileobj_factory(fileobj, 'rb')
+
+
+###############################################################################
+#  base file compression/decompression object definition
+_MODE_CLOSED = 0
+_MODE_READ = 1
+_MODE_READ_EOF = 2
+_MODE_WRITE = 3
+_BUFFER_SIZE = 8192
+
+
+class BinaryZlibFile(io.BufferedIOBase):
+    """A file object providing transparent zlib (de)compression.
+
+    A BinaryZlibFile can act as a wrapper for an existing file object, or refer
+    directly to a named file on disk.
+
+    Note that BinaryZlibFile provides only a *binary* file interface: data read
+    is returned as bytes, and data to be written should be given as bytes.
+
+    This object is an adaptation of the BZ2File object and is compatible with
+    versions of python >= 2.7.
+
+    If filename is a str or bytes object, it gives the name
+    of the file to be opened. Otherwise, it should be a file object,
+    which will be used to read or write the compressed data.
+
+    mode can be 'rb' for reading (default) or 'wb' for (over)writing
+
+    If mode is 'wb', compresslevel can be a number between 1
+    and 9 specifying the level of compression: 1 produces the least
+    compression, and 9 produces the most compression. 3 is the default.
+    """
+
+    wbits = zlib.MAX_WBITS
+
+    def __init__(self, filename, mode="rb", compresslevel=3):
+        # This lock must be recursive, so that BufferedIOBase's
+        # readline(), readlines() and writelines() don't deadlock.
+        self._lock = RLock()
+        self._fp = None
+        self._closefp = False
+        self._mode = _MODE_CLOSED
+        self._pos = 0
+        self._size = -1
+        self.compresslevel = compresslevel
+
+        if not isinstance(compresslevel, int) or not (1 <= compresslevel <= 9):
+            raise ValueError("'compresslevel' must be an integer "
+                             "between 1 and 9. You provided 'compresslevel={}'"
+                             .format(compresslevel))
+
+        if mode == "rb":
+            self._mode = _MODE_READ
+            self._decompressor = zlib.decompressobj(self.wbits)
+            self._buffer = b""
+            self._buffer_offset = 0
+        elif mode == "wb":
+            self._mode = _MODE_WRITE
+            self._compressor = zlib.compressobj(self.compresslevel,
+                                                zlib.DEFLATED, self.wbits,
+                                                zlib.DEF_MEM_LEVEL, 0)
+        else:
+            raise ValueError("Invalid mode: %r" % (mode,))
+
+        if isinstance(filename, _basestring):
+            self._fp = io.open(filename, mode)
+            self._closefp = True
+        elif hasattr(filename, "read") or hasattr(filename, "write"):
+            self._fp = filename
+        else:
+            raise TypeError("filename must be a str or bytes object, "
+                            "or a file")
+
+    def close(self):
+        """Flush and close the file.
+
+        May be called more than once without error. Once the file is
+        closed, any other operation on it will raise a ValueError.
+        """
+        with self._lock:
+            if self._mode == _MODE_CLOSED:
+                return
+            try:
+                if self._mode in (_MODE_READ, _MODE_READ_EOF):
+                    self._decompressor = None
+                elif self._mode == _MODE_WRITE:
+                    self._fp.write(self._compressor.flush())
+                    self._compressor = None
+            finally:
+                try:
+                    if self._closefp:
+                        self._fp.close()
+                finally:
+                    self._fp = None
+                    self._closefp = False
+                    self._mode = _MODE_CLOSED
+                    self._buffer = b""
+                    self._buffer_offset = 0
+
+    @property
+    def closed(self):
+        """True if this file is closed."""
+        return self._mode == _MODE_CLOSED
+
+    def fileno(self):
+        """Return the file descriptor for the underlying file."""
+        self._check_not_closed()
+        return self._fp.fileno()
+
+    def seekable(self):
+        """Return whether the file supports seeking."""
+        return self.readable() and self._fp.seekable()
+
+    def readable(self):
+        """Return whether the file was opened for reading."""
+        self._check_not_closed()
+        return self._mode in (_MODE_READ, _MODE_READ_EOF)
+
+    def writable(self):
+        """Return whether the file was opened for writing."""
+        self._check_not_closed()
+        return self._mode == _MODE_WRITE
+
+    # Mode-checking helper functions.
+
+    def _check_not_closed(self):
+        if self.closed:
+            fname = getattr(self._fp, 'name', None)
+            msg = "I/O operation on closed file"
+            if fname is not None:
+                msg += " {}".format(fname)
+            msg += "."
+            raise ValueError(msg)
+
+    def _check_can_read(self):
+        if self._mode not in (_MODE_READ, _MODE_READ_EOF):
+            self._check_not_closed()
+            raise io.UnsupportedOperation("File not open for reading")
+
+    def _check_can_write(self):
+        if self._mode != _MODE_WRITE:
+            self._check_not_closed()
+            raise io.UnsupportedOperation("File not open for writing")
+
+    def _check_can_seek(self):
+        if self._mode not in (_MODE_READ, _MODE_READ_EOF):
+            self._check_not_closed()
+            raise io.UnsupportedOperation("Seeking is only supported "
+                                          "on files open for reading")
+        if not self._fp.seekable():
+            raise io.UnsupportedOperation("The underlying file object "
+                                          "does not support seeking")
+
+    # Fill the readahead buffer if it is empty. Returns False on EOF.
+    def _fill_buffer(self):
+        if self._mode == _MODE_READ_EOF:
+            return False
+        # Depending on the input data, our call to the decompressor may not
+        # return any data. In this case, try again after reading another block.
+        while self._buffer_offset == len(self._buffer):
+            try:
+                rawblock = (self._decompressor.unused_data or
+                            self._fp.read(_BUFFER_SIZE))
+                if not rawblock:
+                    raise EOFError
+            except EOFError:
+                # End-of-stream marker and end of file. We're good.
+                self._mode = _MODE_READ_EOF
+                self._size = self._pos
+                return False
+            else:
+                self._buffer = self._decompressor.decompress(rawblock)
+            self._buffer_offset = 0
+        return True
+
+    # Read data until EOF.
+    # If return_data is false, consume the data without returning it.
+    def _read_all(self, return_data=True):
+        # The loop assumes that _buffer_offset is 0. Ensure that this is true.
+        self._buffer = self._buffer[self._buffer_offset:]
+        self._buffer_offset = 0
+
+        blocks = []
+        while self._fill_buffer():
+            if return_data:
+                blocks.append(self._buffer)
+            self._pos += len(self._buffer)
+            self._buffer = b""
+        if return_data:
+            return b"".join(blocks)
+
+    # Read a block of up to n bytes.
+    # If return_data is false, consume the data without returning it.
+    def _read_block(self, n_bytes, return_data=True):
+        # If we have enough data buffered, return immediately.
+        end = self._buffer_offset + n_bytes
+        if end <= len(self._buffer):
+            data = self._buffer[self._buffer_offset: end]
+            self._buffer_offset = end
+            self._pos += len(data)
+            return data if return_data else None
+
+        # The loop assumes that _buffer_offset is 0. Ensure that this is true.
+        self._buffer = self._buffer[self._buffer_offset:]
+        self._buffer_offset = 0
+
+        blocks = []
+        while n_bytes > 0 and self._fill_buffer():
+            if n_bytes < len(self._buffer):
+                data = self._buffer[:n_bytes]
+                self._buffer_offset = n_bytes
+            else:
+                data = self._buffer
+                self._buffer = b""
+            if return_data:
+                blocks.append(data)
+            self._pos += len(data)
+            n_bytes -= len(data)
+        if return_data:
+            return b"".join(blocks)
+
+    def read(self, size=-1):
+        """Read up to size uncompressed bytes from the file.
+
+        If size is negative or omitted, read until EOF is reached.
+        Returns b'' if the file is already at EOF.
+        """
+        with self._lock:
+            self._check_can_read()
+            if size == 0:
+                return b""
+            elif size < 0:
+                return self._read_all()
+            else:
+                return self._read_block(size)
+
+    def readinto(self, b):
+        """Read up to len(b) bytes into b.
+
+        Returns the number of bytes read (0 for EOF).
+        """
+        with self._lock:
+            return io.BufferedIOBase.readinto(self, b)
+
+    def write(self, data):
+        """Write a byte string to the file.
+
+        Returns the number of uncompressed bytes written, which is
+        always len(data). Note that due to buffering, the file on disk
+        may not reflect the data written until close() is called.
+        """
+        with self._lock:
+            self._check_can_write()
+            # Convert data type if called by io.BufferedWriter.
+            if isinstance(data, memoryview):
+                data = data.tobytes()
+
+            compressed = self._compressor.compress(data)
+            self._fp.write(compressed)
+            self._pos += len(data)
+            return len(data)
+
+    # Rewind the file to the beginning of the data stream.
+    def _rewind(self):
+        self._fp.seek(0, 0)
+        self._mode = _MODE_READ
+        self._pos = 0
+        self._decompressor = zlib.decompressobj(self.wbits)
+        self._buffer = b""
+        self._buffer_offset = 0
+
+    def seek(self, offset, whence=0):
+        """Change the file position.
+
+        The new position is specified by offset, relative to the
+        position indicated by whence. Values for whence are:
+
+            0: start of stream (default); offset must not be negative
+            1: current stream position
+            2: end of stream; offset must not be positive
+
+        Returns the new file position.
+
+        Note that seeking is emulated, so depending on the parameters,
+        this operation may be extremely slow.
+        """
+        with self._lock:
+            self._check_can_seek()
+
+            # Recalculate offset as an absolute file position.
+            if whence == 0:
+                pass
+            elif whence == 1:
+                offset = self._pos + offset
+            elif whence == 2:
+                # Seeking relative to EOF - we need to know the file's size.
+                if self._size < 0:
+                    self._read_all(return_data=False)
+                offset = self._size + offset
+            else:
+                raise ValueError("Invalid value for whence: %s" % (whence,))
+
+            # Make it so that offset is the number of bytes to skip forward.
+            if offset < self._pos:
+                self._rewind()
+            else:
+                offset -= self._pos
+
+            # Read and discard data until we reach the desired position.
+            self._read_block(offset, return_data=False)
+
+            return self._pos
+
+    def tell(self):
+        """Return the current file position."""
+        with self._lock:
+            self._check_not_closed()
+            return self._pos
+
+
+class ZlibCompressorWrapper(CompressorWrapper):
+
+    def __init__(self):
+        CompressorWrapper.__init__(self, obj=BinaryZlibFile,
+                                   prefix=_ZLIB_PREFIX, extension='.z')
+
+
+class BinaryGzipFile(BinaryZlibFile):
+    """A file object providing transparent gzip (de)compression.
+
+    If filename is a str or bytes object, it gives the name
+    of the file to be opened. Otherwise, it should be a file object,
+    which will be used to read or write the compressed data.
+
+    mode can be 'rb' for reading (default) or 'wb' for (over)writing
+
+    If mode is 'wb', compresslevel can be a number between 1
+    and 9 specifying the level of compression: 1 produces the least
+    compression, and 9 produces the most compression. 3 is the default.
+    """
+
+    wbits = 31  # zlib compressor/decompressor wbits value for gzip format.
+
+
+class GzipCompressorWrapper(CompressorWrapper):
+
+    def __init__(self):
+        CompressorWrapper.__init__(self, obj=BinaryGzipFile,
+                                   prefix=_GZIP_PREFIX, extension='.gz')
diff --git a/sklearn/externals/joblib/disk.py b/sklearn/externals/joblib/disk.py
index 30ad100839..c90c3df360 100755
--- a/sklearn/externals/joblib/disk.py
+++ b/sklearn/externals/joblib/disk.py
@@ -8,11 +8,18 @@
 # License: BSD Style, 3 clauses.
 
 
-import errno
 import os
-import shutil
 import sys
 import time
+import errno
+import shutil
+import warnings
+
+
+try:
+    WindowsError
+except NameError:
+    WindowsError = OSError
 
 
 def disk_used(path):
@@ -57,8 +64,11 @@ def mkdirp(d):
 
 
 # if a rmtree operation fails in rm_subdirs, wait for this much time (in secs),
-# then retry once. if it still fails, raise the exception
+# then retry up to RM_SUBDIRS_N_RETRY times. If it still fails, raise the
+# exception. this mecanism ensures that the sub-process gc have the time to
+# collect and close the memmaps before we fail.
 RM_SUBDIRS_RETRY_TIME = 0.1
+RM_SUBDIRS_N_RETRY = 5
 
 
 def rm_subdirs(path, onerror=None):
@@ -80,7 +90,7 @@ def rm_subdirs(path, onerror=None):
     names = []
     try:
         names = os.listdir(path)
-    except os.error as err:
+    except os.error:
         if onerror is not None:
             onerror(os.listdir, path, sys.exc_info())
         else:
@@ -88,19 +98,27 @@ def rm_subdirs(path, onerror=None):
 
     for name in names:
         fullname = os.path.join(path, name)
-        if os.path.isdir(fullname):
-            if onerror is not None:
-                shutil.rmtree(fullname, False, onerror)
-            else:
-                # allow the rmtree to fail once, wait and re-try.
-                # if the error is raised again, fail
-                err_count = 0
-                while True:
-                    try:
-                        shutil.rmtree(fullname, False, None)
-                        break
-                    except os.error:
-                        if err_count > 0:
-                            raise
-                        err_count += 1
-                        time.sleep(RM_SUBDIRS_RETRY_TIME)
+        delete_folder(fullname, onerror=onerror)
+
+
+def delete_folder(folder_path, onerror=None):
+    """Utility function to cleanup a temporary folder if it still exists."""
+    if os.path.isdir(folder_path):
+        if onerror is not None:
+            shutil.rmtree(folder_path, False, onerror)
+        else:
+            # allow the rmtree to fail once, wait and re-try.
+            # if the error is raised again, fail
+            err_count = 0
+            while True:
+                try:
+                    shutil.rmtree(folder_path, False, None)
+                    break
+                except (OSError, WindowsError):
+                    err_count += 1
+                    if err_count > RM_SUBDIRS_N_RETRY:
+                        warnings.warn(
+                            "Unable to delete folder {} after {} tentatives."
+                            .format(folder_path, RM_SUBDIRS_N_RETRY))
+                        raise
+                    time.sleep(RM_SUBDIRS_RETRY_TIME)
diff --git a/sklearn/externals/joblib/executor.py b/sklearn/externals/joblib/executor.py
new file mode 100755
index 0000000000..c63472d608
--- /dev/null
+++ b/sklearn/externals/joblib/executor.py
@@ -0,0 +1,67 @@
+"""Utility function to construct a loky.ReusableExecutor with custom pickler.
+
+This module provides efficient ways of working with data stored in
+shared memory with numpy.memmap arrays without inducing any memory
+copy between the parent and child processes.
+"""
+# Author: Thomas Moreau <thomas.moreau.2010@gmail.com>
+# Copyright: 2017, Thomas Moreau
+# License: BSD 3 clause
+
+import random
+from .disk import delete_folder
+from ._memmapping_reducer import get_memmapping_reducers
+from .externals.loky.reusable_executor import get_reusable_executor
+
+
+_backend_args = None
+
+
+def get_memmapping_executor(n_jobs, timeout=300, initializer=None, initargs=(),
+                            **backend_args):
+    """Factory for ReusableExecutor with automatic memmapping for large numpy
+    arrays.
+    """
+    global _backend_args
+    reuse = _backend_args is None or _backend_args == backend_args
+    _backend_args = backend_args
+
+    id_executor = random.randint(0, int(1e10))
+    job_reducers, result_reducers, temp_folder = get_memmapping_reducers(
+        id_executor, **backend_args)
+    _executor = get_reusable_executor(n_jobs, job_reducers=job_reducers,
+                                      result_reducers=result_reducers,
+                                      reuse=reuse, timeout=timeout,
+                                      initializer=initializer,
+                                      initargs=initargs)
+    # If executor doesn't have a _temp_folder, it means it is a new executor
+    # and the reducers have been used. Else, the previous reducers are used
+    # and we should not change this attibute.
+    if not hasattr(_executor, "_temp_folder"):
+        _executor._temp_folder = temp_folder
+    else:
+        delete_folder(temp_folder)
+    return _executor
+
+
+class _TestingMemmappingExecutor():
+    """Wrapper around ReusableExecutor to ease memmapping testing with Pool
+    and Executor. This is only for testing purposes.
+    """
+    def __init__(self, n_jobs, **backend_args):
+        self._executor = get_memmapping_executor(n_jobs, **backend_args)
+        self._temp_folder = self._executor._temp_folder
+
+    def apply_async(self, func, args):
+        """Schedule a func to be run"""
+        future = self._executor.submit(func, *args)
+        future.get = future.result
+        return future
+
+    def terminate(self):
+        self._executor.shutdown()
+        delete_folder(self._temp_folder)
+
+    def map(self, f, *args):
+        res = self._executor.map(f, *args)
+        return list(res)
diff --git a/sklearn/externals/joblib/externals/__init__.py b/sklearn/externals/joblib/externals/__init__.py
new file mode 100755
index 0000000000..e69de29bb2
diff --git a/sklearn/externals/joblib/externals/cloudpickle/__init__.py b/sklearn/externals/joblib/externals/cloudpickle/__init__.py
new file mode 100755
index 0000000000..c8c8fa208a
--- /dev/null
+++ b/sklearn/externals/joblib/externals/cloudpickle/__init__.py
@@ -0,0 +1,5 @@
+from __future__ import absolute_import
+
+from .cloudpickle import *
+
+__version__ = '0.5.2'
diff --git a/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py b/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py
new file mode 100755
index 0000000000..e5aab0591f
--- /dev/null
+++ b/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py
@@ -0,0 +1,1098 @@
+"""
+This class is defined to override standard pickle functionality
+
+The goals of it follow:
+-Serialize lambdas and nested functions to compiled byte code
+-Deal with main module correctly
+-Deal with other non-serializable objects
+
+It does not include an unpickler, as standard python unpickling suffices.
+
+This module was extracted from the `cloud` package, developed by `PiCloud, Inc.
+<https://web.archive.org/web/20140626004012/http://www.picloud.com/>`_.
+
+Copyright (c) 2012, Regents of the University of California.
+Copyright (c) 2009 `PiCloud, Inc. <https://web.archive.org/web/20140626004012/http://www.picloud.com/>`_.
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions
+are met:
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in the
+      documentation and/or other materials provided with the distribution.
+    * Neither the name of the University of California, Berkeley nor the
+      names of its contributors may be used to endorse or promote
+      products derived from this software without specific prior written
+      permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+"""
+from __future__ import print_function
+
+import dis
+from functools import partial
+import imp
+import io
+import itertools
+import logging
+import opcode
+import operator
+import pickle
+import struct
+import sys
+import traceback
+import types
+import weakref
+
+
+# cloudpickle is meant for inter process communication: we expect all
+# communicating processes to run the same Python version hence we favor
+# communication speed over compatibility:
+DEFAULT_PROTOCOL = pickle.HIGHEST_PROTOCOL
+
+
+if sys.version < '3':
+    from pickle import Pickler
+    try:
+        from cStringIO import StringIO
+    except ImportError:
+        from StringIO import StringIO
+    PY3 = False
+else:
+    types.ClassType = type
+    from pickle import _Pickler as Pickler
+    from io import BytesIO as StringIO
+    PY3 = True
+
+
+def _make_cell_set_template_code():
+    """Get the Python compiler to emit LOAD_FAST(arg); STORE_DEREF
+
+    Notes
+    -----
+    In Python 3, we could use an easier function:
+
+    .. code-block:: python
+
+       def f():
+           cell = None
+
+           def _stub(value):
+               nonlocal cell
+               cell = value
+
+           return _stub
+
+        _cell_set_template_code = f()
+
+    This function is _only_ a LOAD_FAST(arg); STORE_DEREF, but that is
+    invalid syntax on Python 2. If we use this function we also don't need
+    to do the weird freevars/cellvars swap below
+    """
+    def inner(value):
+        lambda: cell  # make ``cell`` a closure so that we get a STORE_DEREF
+        cell = value
+
+    co = inner.__code__
+
+    # NOTE: we are marking the cell variable as a free variable intentionally
+    # so that we simulate an inner function instead of the outer function. This
+    # is what gives us the ``nonlocal`` behavior in a Python 2 compatible way.
+    if not PY3:
+        return types.CodeType(
+            co.co_argcount,
+            co.co_nlocals,
+            co.co_stacksize,
+            co.co_flags,
+            co.co_code,
+            co.co_consts,
+            co.co_names,
+            co.co_varnames,
+            co.co_filename,
+            co.co_name,
+            co.co_firstlineno,
+            co.co_lnotab,
+            co.co_cellvars,  # this is the trickery
+            (),
+        )
+    else:
+        return types.CodeType(
+            co.co_argcount,
+            co.co_kwonlyargcount,
+            co.co_nlocals,
+            co.co_stacksize,
+            co.co_flags,
+            co.co_code,
+            co.co_consts,
+            co.co_names,
+            co.co_varnames,
+            co.co_filename,
+            co.co_name,
+            co.co_firstlineno,
+            co.co_lnotab,
+            co.co_cellvars,  # this is the trickery
+            (),
+        )
+
+
+_cell_set_template_code = _make_cell_set_template_code()
+
+
+def cell_set(cell, value):
+    """Set the value of a closure cell.
+    """
+    return types.FunctionType(
+        _cell_set_template_code,
+        {},
+        '_cell_set_inner',
+        (),
+        (cell,),
+    )(value)
+
+
+#relevant opcodes
+STORE_GLOBAL = opcode.opmap['STORE_GLOBAL']
+DELETE_GLOBAL = opcode.opmap['DELETE_GLOBAL']
+LOAD_GLOBAL = opcode.opmap['LOAD_GLOBAL']
+GLOBAL_OPS = (STORE_GLOBAL, DELETE_GLOBAL, LOAD_GLOBAL)
+HAVE_ARGUMENT = dis.HAVE_ARGUMENT
+EXTENDED_ARG = dis.EXTENDED_ARG
+
+
+def islambda(func):
+    return getattr(func,'__name__') == '<lambda>'
+
+
+_BUILTIN_TYPE_NAMES = {}
+for k, v in types.__dict__.items():
+    if type(v) is type:
+        _BUILTIN_TYPE_NAMES[v] = k
+
+
+def _builtin_type(name):
+    return getattr(types, name)
+
+
+def _make__new__factory(type_):
+    def _factory():
+        return type_.__new__
+    return _factory
+
+
+# NOTE: These need to be module globals so that they're pickleable as globals.
+_get_dict_new = _make__new__factory(dict)
+_get_frozenset_new = _make__new__factory(frozenset)
+_get_list_new = _make__new__factory(list)
+_get_set_new = _make__new__factory(set)
+_get_tuple_new = _make__new__factory(tuple)
+_get_object_new = _make__new__factory(object)
+
+# Pre-defined set of builtin_function_or_method instances that can be
+# serialized.
+_BUILTIN_TYPE_CONSTRUCTORS = {
+    dict.__new__: _get_dict_new,
+    frozenset.__new__: _get_frozenset_new,
+    set.__new__: _get_set_new,
+    list.__new__: _get_list_new,
+    tuple.__new__: _get_tuple_new,
+    object.__new__: _get_object_new,
+}
+
+
+if sys.version_info < (3, 4):
+    def _walk_global_ops(code):
+        """
+        Yield (opcode, argument number) tuples for all
+        global-referencing instructions in *code*.
+        """
+        code = getattr(code, 'co_code', b'')
+        if not PY3:
+            code = map(ord, code)
+
+        n = len(code)
+        i = 0
+        extended_arg = 0
+        while i < n:
+            op = code[i]
+            i += 1
+            if op >= HAVE_ARGUMENT:
+                oparg = code[i] + code[i + 1] * 256 + extended_arg
+                extended_arg = 0
+                i += 2
+                if op == EXTENDED_ARG:
+                    extended_arg = oparg * 65536
+                if op in GLOBAL_OPS:
+                    yield op, oparg
+
+else:
+    def _walk_global_ops(code):
+        """
+        Yield (opcode, argument number) tuples for all
+        global-referencing instructions in *code*.
+        """
+        for instr in dis.get_instructions(code):
+            op = instr.opcode
+            if op in GLOBAL_OPS:
+                yield op, instr.arg
+
+
+class CloudPickler(Pickler):
+
+    dispatch = Pickler.dispatch.copy()
+
+    def __init__(self, file, protocol=None):
+        if protocol is None:
+            protocol = DEFAULT_PROTOCOL
+        Pickler.__init__(self, file, protocol=protocol)
+        # set of modules to unpickle
+        self.modules = set()
+        # map ids to dictionary. used to ensure that functions can share global env
+        self.globals_ref = {}
+
+    def dump(self, obj):
+        self.inject_addons()
+        try:
+            return Pickler.dump(self, obj)
+        except RuntimeError as e:
+            if 'recursion' in e.args[0]:
+                msg = """Could not pickle object as excessively deep recursion required."""
+                raise pickle.PicklingError(msg)
+
+    def save_memoryview(self, obj):
+        self.save(obj.tobytes())
+    dispatch[memoryview] = save_memoryview
+
+    if not PY3:
+        def save_buffer(self, obj):
+            self.save(str(obj))
+        dispatch[buffer] = save_buffer
+
+    def save_unsupported(self, obj):
+        raise pickle.PicklingError("Cannot pickle objects of type %s" % type(obj))
+    dispatch[types.GeneratorType] = save_unsupported
+
+    # itertools objects do not pickle!
+    for v in itertools.__dict__.values():
+        if type(v) is type:
+            dispatch[v] = save_unsupported
+
+    def save_module(self, obj):
+        """
+        Save a module as an import
+        """
+        mod_name = obj.__name__
+        # If module is successfully found then it is not a dynamically created module
+        if hasattr(obj, '__file__'):
+            is_dynamic = False
+        else:
+            try:
+                _find_module(mod_name)
+                is_dynamic = False
+            except ImportError:
+                is_dynamic = True
+
+        self.modules.add(obj)
+        if is_dynamic:
+            self.save_reduce(dynamic_subimport, (obj.__name__, vars(obj)), obj=obj)
+        else:
+            self.save_reduce(subimport, (obj.__name__,), obj=obj)
+    dispatch[types.ModuleType] = save_module
+
+    def save_codeobject(self, obj):
+        """
+        Save a code object
+        """
+        if PY3:
+            args = (
+                obj.co_argcount, obj.co_kwonlyargcount, obj.co_nlocals, obj.co_stacksize,
+                obj.co_flags, obj.co_code, obj.co_consts, obj.co_names, obj.co_varnames,
+                obj.co_filename, obj.co_name, obj.co_firstlineno, obj.co_lnotab, obj.co_freevars,
+                obj.co_cellvars
+            )
+        else:
+            args = (
+                obj.co_argcount, obj.co_nlocals, obj.co_stacksize, obj.co_flags, obj.co_code,
+                obj.co_consts, obj.co_names, obj.co_varnames, obj.co_filename, obj.co_name,
+                obj.co_firstlineno, obj.co_lnotab, obj.co_freevars, obj.co_cellvars
+            )
+        self.save_reduce(types.CodeType, args, obj=obj)
+    dispatch[types.CodeType] = save_codeobject
+
+    def save_function(self, obj, name=None):
+        """ Registered with the dispatch to handle all function types.
+
+        Determines what kind of function obj is (e.g. lambda, defined at
+        interactive prompt, etc) and handles the pickling appropriately.
+        """
+        if obj in _BUILTIN_TYPE_CONSTRUCTORS:
+            # We keep a special-cased cache of built-in type constructors at
+            # global scope, because these functions are structured very
+            # differently in different python versions and implementations (for
+            # example, they're instances of types.BuiltinFunctionType in
+            # CPython, but they're ordinary types.FunctionType instances in
+            # PyPy).
+            #
+            # If the function we've received is in that cache, we just
+            # serialize it as a lookup into the cache.
+            return self.save_reduce(_BUILTIN_TYPE_CONSTRUCTORS[obj], (), obj=obj)
+
+        write = self.write
+
+        if name is None:
+            name = obj.__name__
+        try:
+            # whichmodule() could fail, see
+            # https://bitbucket.org/gutworth/six/issues/63/importing-six-breaks-pickling
+            modname = pickle.whichmodule(obj, name)
+        except Exception:
+            modname = None
+        # print('which gives %s %s %s' % (modname, obj, name))
+        try:
+            themodule = sys.modules[modname]
+        except KeyError:
+            # eval'd items such as namedtuple give invalid items for their function __module__
+            modname = '__main__'
+
+        if modname == '__main__':
+            themodule = None
+
+        try:
+            lookedup_by_name = getattr(themodule, name, None)
+        except Exception:
+            lookedup_by_name = None
+
+        if themodule:
+            self.modules.add(themodule)
+            if lookedup_by_name is obj:
+                return self.save_global(obj, name)
+
+        # a builtin_function_or_method which comes in as an attribute of some
+        # object (e.g., itertools.chain.from_iterable) will end
+        # up with modname "__main__" and so end up here. But these functions
+        # have no __code__ attribute in CPython, so the handling for
+        # user-defined functions below will fail.
+        # So we pickle them here using save_reduce; have to do it differently
+        # for different python versions.
+        if not hasattr(obj, '__code__'):
+            if PY3:
+                rv = obj.__reduce_ex__(self.proto)
+            else:
+                if hasattr(obj, '__self__'):
+                    rv = (getattr, (obj.__self__, name))
+                else:
+                    raise pickle.PicklingError("Can't pickle %r" % obj)
+            return self.save_reduce(obj=obj, *rv)
+
+        # if func is lambda, def'ed at prompt, is in main, or is nested, then
+        # we'll pickle the actual function object rather than simply saving a
+        # reference (as is done in default pickler), via save_function_tuple.
+        if (islambda(obj)
+                or getattr(obj.__code__, 'co_filename', None) == '<stdin>'
+                or themodule is None):
+            self.save_function_tuple(obj)
+            return
+        else:
+            # func is nested
+            if lookedup_by_name is None or lookedup_by_name is not obj:
+                self.save_function_tuple(obj)
+                return
+
+        if obj.__dict__:
+            # essentially save_reduce, but workaround needed to avoid recursion
+            self.save(_restore_attr)
+            write(pickle.MARK + pickle.GLOBAL + modname + '\n' + name + '\n')
+            self.memoize(obj)
+            self.save(obj.__dict__)
+            write(pickle.TUPLE + pickle.REDUCE)
+        else:
+            write(pickle.GLOBAL + modname + '\n' + name + '\n')
+            self.memoize(obj)
+    dispatch[types.FunctionType] = save_function
+
+    def _save_subimports(self, code, top_level_dependencies):
+        """
+        Ensure de-pickler imports any package child-modules that
+        are needed by the function
+        """
+        # check if any known dependency is an imported package
+        for x in top_level_dependencies:
+            if isinstance(x, types.ModuleType) and hasattr(x, '__package__') and x.__package__:
+                # check if the package has any currently loaded sub-imports
+                prefix = x.__name__ + '.'
+                for name, module in sys.modules.items():
+                    # Older versions of pytest will add a "None" module to sys.modules.
+                    if name is not None and name.startswith(prefix):
+                        # check whether the function can address the sub-module
+                        tokens = set(name[len(prefix):].split('.'))
+                        if not tokens - set(code.co_names):
+                            # ensure unpickler executes this import
+                            self.save(module)
+                            # then discards the reference to it
+                            self.write(pickle.POP)
+
+    def save_dynamic_class(self, obj):
+        """
+        Save a class that can't be stored as module global.
+
+        This method is used to serialize classes that are defined inside
+        functions, or that otherwise can't be serialized as attribute lookups
+        from global modules.
+        """
+        clsdict = dict(obj.__dict__)  # copy dict proxy to a dict
+        clsdict.pop('__weakref__', None)
+
+        # On PyPy, __doc__ is a readonly attribute, so we need to include it in
+        # the initial skeleton class.  This is safe because we know that the
+        # doc can't participate in a cycle with the original class.
+        type_kwargs = {'__doc__': clsdict.pop('__doc__', None)}
+
+        # If type overrides __dict__ as a property, include it in the type kwargs.
+        # In Python 2, we can't set this attribute after construction.
+        __dict__ = clsdict.pop('__dict__', None)
+        if isinstance(__dict__, property):
+            type_kwargs['__dict__'] = __dict__
+
+        save = self.save
+        write = self.write
+
+        # We write pickle instructions explicitly here to handle the
+        # possibility that the type object participates in a cycle with its own
+        # __dict__. We first write an empty "skeleton" version of the class and
+        # memoize it before writing the class' __dict__ itself. We then write
+        # instructions to "rehydrate" the skeleton class by restoring the
+        # attributes from the __dict__.
+        #
+        # A type can appear in a cycle with its __dict__ if an instance of the
+        # type appears in the type's __dict__ (which happens for the stdlib
+        # Enum class), or if the type defines methods that close over the name
+        # of the type, (which is common for Python 2-style super() calls).
+
+        # Push the rehydration function.
+        save(_rehydrate_skeleton_class)
+
+        # Mark the start of the args tuple for the rehydration function.
+        write(pickle.MARK)
+
+        # Create and memoize an skeleton class with obj's name and bases.
+        tp = type(obj)
+        self.save_reduce(tp, (obj.__name__, obj.__bases__, type_kwargs), obj=obj)
+
+        # Now save the rest of obj's __dict__. Any references to obj
+        # encountered while saving will point to the skeleton class.
+        save(clsdict)
+
+        # Write a tuple of (skeleton_class, clsdict).
+        write(pickle.TUPLE)
+
+        # Call _rehydrate_skeleton_class(skeleton_class, clsdict)
+        write(pickle.REDUCE)
+
+    def save_function_tuple(self, func):
+        """  Pickles an actual func object.
+
+        A func comprises: code, globals, defaults, closure, and dict.  We
+        extract and save these, injecting reducing functions at certain points
+        to recreate the func object.  Keep in mind that some of these pieces
+        can contain a ref to the func itself.  Thus, a naive save on these
+        pieces could trigger an infinite loop of save's.  To get around that,
+        we first create a skeleton func object using just the code (this is
+        safe, since this won't contain a ref to the func), and memoize it as
+        soon as it's created.  The other stuff can then be filled in later.
+        """
+        if is_tornado_coroutine(func):
+            self.save_reduce(_rebuild_tornado_coroutine, (func.__wrapped__,),
+                             obj=func)
+            return
+
+        save = self.save
+        write = self.write
+
+        code, f_globals, defaults, closure_values, dct, base_globals = self.extract_func_data(func)
+
+        save(_fill_function)  # skeleton function updater
+        write(pickle.MARK)    # beginning of tuple that _fill_function expects
+
+        self._save_subimports(
+            code,
+            itertools.chain(f_globals.values(), closure_values or ()),
+        )
+
+        # create a skeleton function object and memoize it
+        save(_make_skel_func)
+        save((
+            code,
+            len(closure_values) if closure_values is not None else -1,
+            base_globals,
+        ))
+        write(pickle.REDUCE)
+        self.memoize(func)
+
+        # save the rest of the func data needed by _fill_function
+        state = {
+            'globals': f_globals,
+            'defaults': defaults,
+            'dict': dct,
+            'module': func.__module__,
+            'closure_values': closure_values,
+        }
+        if hasattr(func, '__qualname__'):
+            state['qualname'] = func.__qualname__
+        save(state)
+        write(pickle.TUPLE)
+        write(pickle.REDUCE)  # applies _fill_function on the tuple
+
+    _extract_code_globals_cache = (
+        weakref.WeakKeyDictionary()
+        if not hasattr(sys, "pypy_version_info")
+        else {})
+
+    @classmethod
+    def extract_code_globals(cls, co):
+        """
+        Find all globals names read or written to by codeblock co
+        """
+        out_names = cls._extract_code_globals_cache.get(co)
+        if out_names is None:
+            try:
+                names = co.co_names
+            except AttributeError:
+                # PyPy "builtin-code" object
+                out_names = set()
+            else:
+                out_names = set(names[oparg]
+                                for op, oparg in _walk_global_ops(co))
+
+                # see if nested function have any global refs
+                if co.co_consts:
+                    for const in co.co_consts:
+                        if type(const) is types.CodeType:
+                            out_names |= cls.extract_code_globals(const)
+
+            cls._extract_code_globals_cache[co] = out_names
+
+        return out_names
+
+    def extract_func_data(self, func):
+        """
+        Turn the function into a tuple of data necessary to recreate it:
+            code, globals, defaults, closure_values, dict
+        """
+        code = func.__code__
+
+        # extract all global ref's
+        func_global_refs = self.extract_code_globals(code)
+
+        # process all variables referenced by global environment
+        f_globals = {}
+        for var in func_global_refs:
+            if var in func.__globals__:
+                f_globals[var] = func.__globals__[var]
+
+        # defaults requires no processing
+        defaults = func.__defaults__
+
+        # process closure
+        closure = (
+            list(map(_get_cell_contents, func.__closure__))
+            if func.__closure__ is not None
+            else None
+        )
+
+        # save the dict
+        dct = func.__dict__
+
+        base_globals = self.globals_ref.get(id(func.__globals__), {})
+        self.globals_ref[id(func.__globals__)] = base_globals
+
+        return (code, f_globals, defaults, closure, dct, base_globals)
+
+    def save_builtin_function(self, obj):
+        if obj.__module__ == "__builtin__":
+            return self.save_global(obj)
+        return self.save_function(obj)
+    dispatch[types.BuiltinFunctionType] = save_builtin_function
+
+    def save_global(self, obj, name=None, pack=struct.pack):
+        """
+        Save a "global".
+
+        The name of this method is somewhat misleading: all types get
+        dispatched here.
+        """
+        if obj.__module__ == "__main__":
+            return self.save_dynamic_class(obj)
+
+        try:
+            return Pickler.save_global(self, obj, name=name)
+        except Exception:
+            if obj.__module__ == "__builtin__" or obj.__module__ == "builtins":
+                if obj in _BUILTIN_TYPE_NAMES:
+                    return self.save_reduce(
+                        _builtin_type, (_BUILTIN_TYPE_NAMES[obj],), obj=obj)
+
+            typ = type(obj)
+            if typ is not obj and isinstance(obj, (type, types.ClassType)):
+                return self.save_dynamic_class(obj)
+
+            raise
+
+    dispatch[type] = save_global
+    dispatch[types.ClassType] = save_global
+
+    def save_instancemethod(self, obj):
+        # Memoization rarely is ever useful due to python bounding
+        if obj.__self__ is None:
+            self.save_reduce(getattr, (obj.im_class, obj.__name__))
+        else:
+            if PY3:
+                self.save_reduce(types.MethodType, (obj.__func__, obj.__self__), obj=obj)
+            else:
+                self.save_reduce(types.MethodType, (obj.__func__, obj.__self__, obj.__self__.__class__),
+                         obj=obj)
+    dispatch[types.MethodType] = save_instancemethod
+
+    def save_inst(self, obj):
+        """Inner logic to save instance. Based off pickle.save_inst"""
+        cls = obj.__class__
+
+        # Try the dispatch table (pickle module doesn't do it)
+        f = self.dispatch.get(cls)
+        if f:
+            f(self, obj)  # Call unbound method with explicit self
+            return
+
+        memo = self.memo
+        write = self.write
+        save = self.save
+
+        if hasattr(obj, '__getinitargs__'):
+            args = obj.__getinitargs__()
+            len(args)  # XXX Assert it's a sequence
+            pickle._keep_alive(args, memo)
+        else:
+            args = ()
+
+        write(pickle.MARK)
+
+        if self.bin:
+            save(cls)
+            for arg in args:
+                save(arg)
+            write(pickle.OBJ)
+        else:
+            for arg in args:
+                save(arg)
+            write(pickle.INST + cls.__module__ + '\n' + cls.__name__ + '\n')
+
+        self.memoize(obj)
+
+        try:
+            getstate = obj.__getstate__
+        except AttributeError:
+            stuff = obj.__dict__
+        else:
+            stuff = getstate()
+            pickle._keep_alive(stuff, memo)
+        save(stuff)
+        write(pickle.BUILD)
+
+    if not PY3:
+        dispatch[types.InstanceType] = save_inst
+
+    def save_property(self, obj):
+        # properties not correctly saved in python
+        self.save_reduce(property, (obj.fget, obj.fset, obj.fdel, obj.__doc__), obj=obj)
+    dispatch[property] = save_property
+
+    def save_classmethod(self, obj):
+        orig_func = obj.__func__
+        self.save_reduce(type(obj), (orig_func,), obj=obj)
+    dispatch[classmethod] = save_classmethod
+    dispatch[staticmethod] = save_classmethod
+
+    def save_itemgetter(self, obj):
+        """itemgetter serializer (needed for namedtuple support)"""
+        class Dummy:
+            def __getitem__(self, item):
+                return item
+        items = obj(Dummy())
+        if not isinstance(items, tuple):
+            items = (items, )
+        return self.save_reduce(operator.itemgetter, items)
+
+    if type(operator.itemgetter) is type:
+        dispatch[operator.itemgetter] = save_itemgetter
+
+    def save_attrgetter(self, obj):
+        """attrgetter serializer"""
+        class Dummy(object):
+            def __init__(self, attrs, index=None):
+                self.attrs = attrs
+                self.index = index
+            def __getattribute__(self, item):
+                attrs = object.__getattribute__(self, "attrs")
+                index = object.__getattribute__(self, "index")
+                if index is None:
+                    index = len(attrs)
+                    attrs.append(item)
+                else:
+                    attrs[index] = ".".join([attrs[index], item])
+                return type(self)(attrs, index)
+        attrs = []
+        obj(Dummy(attrs))
+        return self.save_reduce(operator.attrgetter, tuple(attrs))
+
+    if type(operator.attrgetter) is type:
+        dispatch[operator.attrgetter] = save_attrgetter
+
+    def save_file(self, obj):
+        """Save a file"""
+        try:
+            import StringIO as pystringIO #we can't use cStringIO as it lacks the name attribute
+        except ImportError:
+            import io as pystringIO
+
+        if not hasattr(obj, 'name') or  not hasattr(obj, 'mode'):
+            raise pickle.PicklingError("Cannot pickle files that do not map to an actual file")
+        if obj is sys.stdout:
+            return self.save_reduce(getattr, (sys,'stdout'), obj=obj)
+        if obj is sys.stderr:
+            return self.save_reduce(getattr, (sys,'stderr'), obj=obj)
+        if obj is sys.stdin:
+            raise pickle.PicklingError("Cannot pickle standard input")
+        if obj.closed:
+            raise pickle.PicklingError("Cannot pickle closed files")
+        if hasattr(obj, 'isatty') and obj.isatty():
+            raise pickle.PicklingError("Cannot pickle files that map to tty objects")
+        if 'r' not in obj.mode and '+' not in obj.mode:
+            raise pickle.PicklingError("Cannot pickle files that are not opened for reading: %s" % obj.mode)
+
+        name = obj.name
+
+        retval = pystringIO.StringIO()
+
+        try:
+            # Read the whole file
+            curloc = obj.tell()
+            obj.seek(0)
+            contents = obj.read()
+            obj.seek(curloc)
+        except IOError:
+            raise pickle.PicklingError("Cannot pickle file %s as it cannot be read" % name)
+        retval.write(contents)
+        retval.seek(curloc)
+
+        retval.name = name
+        self.save(retval)
+        self.memoize(obj)
+
+    def save_ellipsis(self, obj):
+        self.save_reduce(_gen_ellipsis, ())
+
+    def save_not_implemented(self, obj):
+        self.save_reduce(_gen_not_implemented, ())
+
+    if PY3:
+        dispatch[io.TextIOWrapper] = save_file
+    else:
+        dispatch[file] = save_file
+
+    dispatch[type(Ellipsis)] = save_ellipsis
+    dispatch[type(NotImplemented)] = save_not_implemented
+
+    def save_weakset(self, obj):
+        self.save_reduce(weakref.WeakSet, (list(obj),))
+
+    dispatch[weakref.WeakSet] = save_weakset
+
+    def save_logger(self, obj):
+        self.save_reduce(logging.getLogger, (obj.name,), obj=obj)
+
+    dispatch[logging.Logger] = save_logger
+
+    """Special functions for Add-on libraries"""
+    def inject_addons(self):
+        """Plug in system. Register additional pickling functions if modules already loaded"""
+        pass
+
+
+# Tornado support
+
+def is_tornado_coroutine(func):
+    """
+    Return whether *func* is a Tornado coroutine function.
+    Running coroutines are not supported.
+    """
+    if 'tornado.gen' not in sys.modules:
+        return False
+    gen = sys.modules['tornado.gen']
+    if not hasattr(gen, "is_coroutine_function"):
+        # Tornado version is too old
+        return False
+    return gen.is_coroutine_function(func)
+
+
+def _rebuild_tornado_coroutine(func):
+    from tornado import gen
+    return gen.coroutine(func)
+
+
+# Shorthands for legacy support
+
+def dump(obj, file, protocol=None):
+    """Serialize obj as bytes streamed into file
+
+    protocol defaults to cloudpickle.DEFAULT_PROTOCOL which is an alias to
+    pickle.HIGHEST_PROTOCOL. This setting favors maximum communication speed
+    between processes running the same Python version.
+
+    Set protocol=pickle.DEFAULT_PROTOCOL instead if you need to ensure
+    compatibility with older versions of Python.
+    """
+    CloudPickler(file, protocol=protocol).dump(obj)
+
+
+def dumps(obj, protocol=None):
+    """Serialize obj as a string of bytes allocated in memory
+
+    protocol defaults to cloudpickle.DEFAULT_PROTOCOL which is an alias to
+    pickle.HIGHEST_PROTOCOL. This setting favors maximum communication speed
+    between processes running the same Python version.
+
+    Set protocol=pickle.DEFAULT_PROTOCOL instead if you need to ensure
+    compatibility with older versions of Python.
+    """
+    file = StringIO()
+    try:
+        cp = CloudPickler(file, protocol=protocol)
+        cp.dump(obj)
+        return file.getvalue()
+    finally:
+        file.close()
+
+
+# including pickles unloading functions in this namespace
+load = pickle.load
+loads = pickle.loads
+
+
+# hack for __import__ not working as desired
+def subimport(name):
+    __import__(name)
+    return sys.modules[name]
+
+
+def dynamic_subimport(name, vars):
+    mod = imp.new_module(name)
+    mod.__dict__.update(vars)
+    sys.modules[name] = mod
+    return mod
+
+
+# restores function attributes
+def _restore_attr(obj, attr):
+    for key, val in attr.items():
+        setattr(obj, key, val)
+    return obj
+
+
+def _get_module_builtins():
+    return pickle.__builtins__
+
+
+def print_exec(stream):
+    ei = sys.exc_info()
+    traceback.print_exception(ei[0], ei[1], ei[2], None, stream)
+
+
+def _modules_to_main(modList):
+    """Force every module in modList to be placed into main"""
+    if not modList:
+        return
+
+    main = sys.modules['__main__']
+    for modname in modList:
+        if type(modname) is str:
+            try:
+                mod = __import__(modname)
+            except Exception as e:
+                sys.stderr.write('warning: could not import %s\n.  '
+                                 'Your function may unexpectedly error due to this import failing;'
+                                 'A version mismatch is likely.  Specific error was:\n' % modname)
+                print_exec(sys.stderr)
+            else:
+                setattr(main, mod.__name__, mod)
+
+
+#object generators:
+def _genpartial(func, args, kwds):
+    if not args:
+        args = ()
+    if not kwds:
+        kwds = {}
+    return partial(func, *args, **kwds)
+
+def _gen_ellipsis():
+    return Ellipsis
+
+def _gen_not_implemented():
+    return NotImplemented
+
+
+def _get_cell_contents(cell):
+    try:
+        return cell.cell_contents
+    except ValueError:
+        # sentinel used by ``_fill_function`` which will leave the cell empty
+        return _empty_cell_value
+
+
+def instance(cls):
+    """Create a new instance of a class.
+
+    Parameters
+    ----------
+    cls : type
+        The class to create an instance of.
+
+    Returns
+    -------
+    instance : cls
+        A new instance of ``cls``.
+    """
+    return cls()
+
+
+@instance
+class _empty_cell_value(object):
+    """sentinel for empty closures
+    """
+    @classmethod
+    def __reduce__(cls):
+        return cls.__name__
+
+
+def _fill_function(*args):
+    """Fills in the rest of function data into the skeleton function object
+
+    The skeleton itself is create by _make_skel_func().
+    """
+    if len(args) == 2:
+        func = args[0]
+        state = args[1]
+    elif len(args) == 5:
+        # Backwards compat for cloudpickle v0.4.0, after which the `module`
+        # argument was introduced
+        func = args[0]
+        keys = ['globals', 'defaults', 'dict', 'closure_values']
+        state = dict(zip(keys, args[1:]))
+    elif len(args) == 6:
+        # Backwards compat for cloudpickle v0.4.1, after which the function
+        # state was passed as a dict to the _fill_function it-self.
+        func = args[0]
+        keys = ['globals', 'defaults', 'dict', 'module', 'closure_values']
+        state = dict(zip(keys, args[1:]))
+    else:
+        raise ValueError('Unexpected _fill_value arguments: %r' % (args,))
+
+    func.__globals__.update(state['globals'])
+    func.__defaults__ = state['defaults']
+    func.__dict__ = state['dict']
+    if 'module' in state:
+        func.__module__ = state['module']
+    if 'qualname' in state:
+        func.__qualname__ = state['qualname']
+
+    cells = func.__closure__
+    if cells is not None:
+        for cell, value in zip(cells, state['closure_values']):
+            if value is not _empty_cell_value:
+                cell_set(cell, value)
+
+    return func
+
+
+def _make_empty_cell():
+    if False:
+        # trick the compiler into creating an empty cell in our lambda
+        cell = None
+        raise AssertionError('this route should not be executed')
+
+    return (lambda: cell).__closure__[0]
+
+
+def _make_skel_func(code, cell_count, base_globals=None):
+    """ Creates a skeleton function object that contains just the provided
+        code and the correct number of cells in func_closure.  All other
+        func attributes (e.g. func_globals) are empty.
+    """
+    if base_globals is None:
+        base_globals = {}
+    base_globals['__builtins__'] = __builtins__
+
+    closure = (
+        tuple(_make_empty_cell() for _ in range(cell_count))
+        if cell_count >= 0 else
+        None
+    )
+    return types.FunctionType(code, base_globals, None, None, closure)
+
+
+def _rehydrate_skeleton_class(skeleton_class, class_dict):
+    """Put attributes from `class_dict` back on `skeleton_class`.
+
+    See CloudPickler.save_dynamic_class for more info.
+    """
+    for attrname, attr in class_dict.items():
+        setattr(skeleton_class, attrname, attr)
+    return skeleton_class
+
+
+def _find_module(mod_name):
+    """
+    Iterate over each part instead of calling imp.find_module directly.
+    This function is able to find submodules (e.g. sickit.tree)
+    """
+    path = None
+    for part in mod_name.split('.'):
+        if path is not None:
+            path = [path]
+        file, path, description = imp.find_module(part, path)
+        if file is not None:
+            file.close()
+    return path, description
+
+"""Constructors for 3rd party libraries
+Note: These can never be renamed due to client compatibility issues"""
+
+def _getobject(modname, attribute):
+    mod = __import__(modname, fromlist=[attribute])
+    return mod.__dict__[attribute]
+
+
+""" Use copy_reg to extend global pickle definitions """
+
+if sys.version_info < (3, 4):
+    method_descriptor = type(str.upper)
+
+    def _reduce_method_descriptor(obj):
+        return (getattr, (obj.__objclass__, obj.__name__))
+
+    try:
+        import copy_reg as copyreg
+    except ImportError:
+        import copyreg
+    copyreg.pickle(method_descriptor, _reduce_method_descriptor)
diff --git a/sklearn/externals/joblib/externals/loky/__init__.py b/sklearn/externals/joblib/externals/loky/__init__.py
new file mode 100755
index 0000000000..6480423cff
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/__init__.py
@@ -0,0 +1,12 @@
+r"""The :mod:`loky` module manages a pool of worker that can be re-used across time.
+It provides a robust and dynamic implementation os the
+:class:`ProcessPoolExecutor` and a function :func:`get_reusable_executor` which
+hide the pool management under the hood.
+"""
+from .reusable_executor import get_reusable_executor  # noqa: F401
+from .process_executor import ProcessPoolExecutor  # noqa: F401
+from .process_executor import BrokenProcessPool  # noqa: F401
+
+from .backend.context import cpu_count  # noqa: F401
+
+__version__ = '2.2.0'
diff --git a/sklearn/externals/joblib/externals/loky/_base.py b/sklearn/externals/joblib/externals/loky/_base.py
new file mode 100755
index 0000000000..ff4ac92cf4
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/_base.py
@@ -0,0 +1,610 @@
+###############################################################################
+# Backport concurrent.futures for python2.7/3.3
+#
+# author: Thomas Moreau and Olivier Grisel
+#
+# adapted from concurrent/futures/_base.py (17/02/2017)
+#  * Do not use yield from
+#  * Use old super syntax
+#
+# Copyright 2009 Brian Quinlan. All Rights Reserved.
+# Licensed to PSF under a Contributor Agreement.
+
+import sys
+import collections
+import logging
+import threading
+import time
+
+FIRST_COMPLETED = 'FIRST_COMPLETED'
+FIRST_EXCEPTION = 'FIRST_EXCEPTION'
+ALL_COMPLETED = 'ALL_COMPLETED'
+_AS_COMPLETED = '_AS_COMPLETED'
+
+# Possible future states (for internal use by the futures package).
+PENDING = 'PENDING'
+RUNNING = 'RUNNING'
+# The future was cancelled by the user...
+CANCELLED = 'CANCELLED'
+# ...and _Waiter.add_cancelled() was called by a worker.
+CANCELLED_AND_NOTIFIED = 'CANCELLED_AND_NOTIFIED'
+FINISHED = 'FINISHED'
+
+_FUTURE_STATES = [
+    PENDING,
+    RUNNING,
+    CANCELLED,
+    CANCELLED_AND_NOTIFIED,
+    FINISHED
+]
+
+_STATE_TO_DESCRIPTION_MAP = {
+    PENDING: "pending",
+    RUNNING: "running",
+    CANCELLED: "cancelled",
+    CANCELLED_AND_NOTIFIED: "cancelled",
+    FINISHED: "finished"
+}
+
+# Logger for internal use by the futures package.
+LOGGER = logging.getLogger("concurrent.futures")
+
+
+if sys.version_info[:2] < (3, 3):
+
+    class Error(Exception):
+        """Base class for all future-related exceptions."""
+        pass
+
+    class CancelledError(Error):
+        """The Future was cancelled."""
+        pass
+
+    class TimeoutError(Error):
+        """The operation exceeded the given deadline."""
+        pass
+else:
+    from concurrent.futures import CancelledError, TimeoutError
+
+
+class _Waiter(object):
+    """Provides the event that wait() and as_completed() block on."""
+    def __init__(self):
+        self.event = threading.Event()
+        self.finished_futures = []
+
+    def add_result(self, future):
+        self.finished_futures.append(future)
+
+    def add_exception(self, future):
+        self.finished_futures.append(future)
+
+    def add_cancelled(self, future):
+        self.finished_futures.append(future)
+
+
+class _AsCompletedWaiter(_Waiter):
+    """Used by as_completed()."""
+
+    def __init__(self):
+        super(_AsCompletedWaiter, self).__init__()
+        self.lock = threading.Lock()
+
+    def add_result(self, future):
+        with self.lock:
+            super(_AsCompletedWaiter, self).add_result(future)
+            self.event.set()
+
+    def add_exception(self, future):
+        with self.lock:
+            super(_AsCompletedWaiter, self).add_exception(future)
+            self.event.set()
+
+    def add_cancelled(self, future):
+        with self.lock:
+            super(_AsCompletedWaiter, self).add_cancelled(future)
+            self.event.set()
+
+
+class _FirstCompletedWaiter(_Waiter):
+    """Used by wait(return_when=FIRST_COMPLETED)."""
+
+    def add_result(self, future):
+        super(_FirstCompletedWaiter, self).add_result(future)
+        self.event.set()
+
+    def add_exception(self, future):
+        super(_FirstCompletedWaiter, self).add_exception(future)
+        self.event.set()
+
+    def add_cancelled(self, future):
+        super(_FirstCompletedWaiter, self).add_cancelled(future)
+        self.event.set()
+
+
+class _AllCompletedWaiter(_Waiter):
+    """Used by wait(return_when=FIRST_EXCEPTION and ALL_COMPLETED)."""
+
+    def __init__(self, num_pending_calls, stop_on_exception):
+        self.num_pending_calls = num_pending_calls
+        self.stop_on_exception = stop_on_exception
+        self.lock = threading.Lock()
+        super(_AllCompletedWaiter, self).__init__()
+
+    def _decrement_pending_calls(self):
+        with self.lock:
+            self.num_pending_calls -= 1
+            if not self.num_pending_calls:
+                self.event.set()
+
+    def add_result(self, future):
+        super(_AllCompletedWaiter, self).add_result(future)
+        self._decrement_pending_calls()
+
+    def add_exception(self, future):
+        super(_AllCompletedWaiter, self).add_exception(future)
+        if self.stop_on_exception:
+            self.event.set()
+        else:
+            self._decrement_pending_calls()
+
+    def add_cancelled(self, future):
+        super(_AllCompletedWaiter, self).add_cancelled(future)
+        self._decrement_pending_calls()
+
+
+class _AcquireFutures(object):
+    """A context manager that does an ordered acquire of Future conditions."""
+
+    def __init__(self, futures):
+        self.futures = sorted(futures, key=id)
+
+    def __enter__(self):
+        for future in self.futures:
+            future._condition.acquire()
+
+    def __exit__(self, *args):
+        for future in self.futures:
+            future._condition.release()
+
+
+def _create_and_install_waiters(fs, return_when):
+    if return_when == _AS_COMPLETED:
+        waiter = _AsCompletedWaiter()
+    elif return_when == FIRST_COMPLETED:
+        waiter = _FirstCompletedWaiter()
+    else:
+        pending_count = sum(
+                f._state not in [CANCELLED_AND_NOTIFIED, FINISHED] for f in fs)
+
+        if return_when == FIRST_EXCEPTION:
+            waiter = _AllCompletedWaiter(pending_count, stop_on_exception=True)
+        elif return_when == ALL_COMPLETED:
+            waiter = _AllCompletedWaiter(pending_count,
+                                         stop_on_exception=False)
+        else:
+            raise ValueError("Invalid return condition: %r" % return_when)
+
+    for f in fs:
+        f._waiters.append(waiter)
+
+    return waiter
+
+
+def as_completed(fs, timeout=None):
+    """An iterator over the given futures that yields each as it completes.
+
+    Args:
+        fs: The sequence of Futures (possibly created by different Executors)
+            to iterate over.
+        timeout: The maximum number of seconds to wait. If None, then there
+            is no limit on the wait time.
+
+    Returns:
+        An iterator that yields the given Futures as they complete (finished or
+        cancelled). If any given Futures are duplicated, they will be returned
+        once.
+
+    Raises:
+        TimeoutError: If the entire result iterator could not be generated
+            before the given timeout.
+    """
+    if timeout is not None:
+        end_time = timeout + time.time()
+
+    fs = set(fs)
+    with _AcquireFutures(fs):
+        finished = set(
+                f for f in fs
+                if f._state in [CANCELLED_AND_NOTIFIED, FINISHED])
+        pending = fs - finished
+        waiter = _create_and_install_waiters(fs, _AS_COMPLETED)
+
+    try:
+        for future in finished:
+            yield future
+
+        while pending:
+            if timeout is None:
+                wait_timeout = None
+            else:
+                wait_timeout = end_time - time.time()
+                if wait_timeout < 0:
+                    raise TimeoutError('%d (of %d) futures unfinished' % (
+                        len(pending), len(fs)))
+
+            waiter.event.wait(wait_timeout)
+
+            with waiter.lock:
+                finished = waiter.finished_futures
+                waiter.finished_futures = []
+                waiter.event.clear()
+
+            for future in finished:
+                yield future
+                pending.remove(future)
+
+    finally:
+        for f in fs:
+            with f._condition:
+                f._waiters.remove(waiter)
+
+
+DoneAndNotDoneFutures = collections.namedtuple(
+        'DoneAndNotDoneFutures', 'done not_done')
+
+
+def wait(fs, timeout=None, return_when=ALL_COMPLETED):
+    """Wait for the futures in the given sequence to complete.
+
+    Args:
+        fs: The sequence of Futures (possibly created by different Executors)
+            to wait upon.
+        timeout: The maximum number of seconds to wait. If None, then there
+            is no limit on the wait time.
+        return_when: Indicates when this function should return. The options
+            are:
+
+            FIRST_COMPLETED - Return when any future finishes or is
+                              cancelled.
+            FIRST_EXCEPTION - Return when any future finishes by raising an
+                              exception. If no future raises an exception
+                              then it is equivalent to ALL_COMPLETED.
+            ALL_COMPLETED -   Return when all futures finish or are cancelled.
+
+    Returns:
+        A named 2-tuple of sets. The first set, named 'done', contains the
+        futures that completed (is finished or cancelled) before the wait
+        completed. The second set, named 'not_done', contains uncompleted
+        futures.
+    """
+    with _AcquireFutures(fs):
+        done = set(f for f in fs
+                   if f._state in [CANCELLED_AND_NOTIFIED, FINISHED])
+        not_done = set(fs) - done
+
+        if (return_when == FIRST_COMPLETED) and done:
+            return DoneAndNotDoneFutures(done, not_done)
+        elif (return_when == FIRST_EXCEPTION) and done:
+            if any(f for f in done
+                   if not f.cancelled() and f.exception() is not None):
+                return DoneAndNotDoneFutures(done, not_done)
+
+        if len(done) == len(fs):
+            return DoneAndNotDoneFutures(done, not_done)
+
+        waiter = _create_and_install_waiters(fs, return_when)
+
+    waiter.event.wait(timeout)
+    for f in fs:
+        with f._condition:
+            f._waiters.remove(waiter)
+
+    done.update(waiter.finished_futures)
+    return DoneAndNotDoneFutures(done, set(fs) - done)
+
+
+class Future(object):
+    """Represents the result of an asynchronous computation."""
+
+    def __init__(self):
+        """Initializes the future. Should not be called by clients."""
+        self._condition = threading.Condition()
+        self._state = PENDING
+        self._result = None
+        self._exception = None
+        self._waiters = []
+        self._done_callbacks = []
+
+    def _invoke_callbacks(self):
+        for callback in self._done_callbacks:
+            try:
+                callback(self)
+            except BaseException:
+                LOGGER.exception('exception calling callback for %r', self)
+
+    def __repr__(self):
+        with self._condition:
+            if self._state == FINISHED:
+                if self._exception:
+                    return '<%s at %#x state=%s raised %s>' % (
+                        self.__class__.__name__,
+                        id(self),
+                        _STATE_TO_DESCRIPTION_MAP[self._state],
+                        self._exception.__class__.__name__)
+                else:
+                    return '<%s at %#x state=%s returned %s>' % (
+                        self.__class__.__name__,
+                        id(self),
+                        _STATE_TO_DESCRIPTION_MAP[self._state],
+                        self._result.__class__.__name__)
+            return '<%s at %#x state=%s>' % (
+                    self.__class__.__name__,
+                    id(self),
+                   _STATE_TO_DESCRIPTION_MAP[self._state])
+
+    def cancel(self):
+        """Cancel the future if possible.
+
+        Returns True if the future was cancelled, False otherwise. A future
+        cannot be cancelled if it is running or has already completed.
+        """
+        with self._condition:
+            if self._state in [RUNNING, FINISHED]:
+                return False
+
+            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
+                return True
+
+            self._state = CANCELLED
+            self._condition.notify_all()
+
+        self._invoke_callbacks()
+        return True
+
+    def cancelled(self):
+        """Return True if the future was cancelled."""
+        with self._condition:
+            return self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]
+
+    def running(self):
+        """Return True if the future is currently executing."""
+        with self._condition:
+            return self._state == RUNNING
+
+    def done(self):
+        """Return True of the future was cancelled or finished executing."""
+        with self._condition:
+            return self._state in [CANCELLED, CANCELLED_AND_NOTIFIED, FINISHED]
+
+    def __get_result(self):
+        if self._exception:
+            raise self._exception
+        else:
+            return self._result
+
+    def add_done_callback(self, fn):
+        """Attaches a callable that will be called when the future finishes.
+
+        Args:
+            fn: A callable that will be called with this future as its only
+                argument when the future completes or is cancelled. The
+                callable will always be called by a thread in the same process
+                in which it was added. If the future has already completed or
+                been cancelled then the callable will be called immediately.
+                These callables are called in the order that they were added.
+        """
+        with self._condition:
+            if self._state not in [CANCELLED, CANCELLED_AND_NOTIFIED,
+                                   FINISHED]:
+                self._done_callbacks.append(fn)
+                return
+        fn(self)
+
+    def result(self, timeout=None):
+        """Return the result of the call that the future represents.
+
+        Args:
+            timeout: The number of seconds to wait for the result if the future
+                isn't done. If None, then there is no limit on the wait time.
+
+        Returns:
+            The result of the call that the future represents.
+
+        Raises:
+            CancelledError: If the future was cancelled.
+            TimeoutError: If the future didn't finish executing before the
+                given timeout.
+            Exception: If the call raised then that exception will be raised.
+        """
+        with self._condition:
+            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
+                raise CancelledError()
+            elif self._state == FINISHED:
+                return self.__get_result()
+
+            self._condition.wait(timeout)
+
+            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
+                raise CancelledError()
+            elif self._state == FINISHED:
+                return self.__get_result()
+            else:
+                raise TimeoutError()
+
+    def exception(self, timeout=None):
+        """Return the exception raised by the call that the future represents.
+
+        Args:
+            timeout: The number of seconds to wait for the exception if the
+                future isn't done. If None, then there is no limit on the wait
+                time.
+
+        Returns:
+            The exception raised by the call that the future represents or None
+            if the call completed without raising.
+
+        Raises:
+            CancelledError: If the future was cancelled.
+            TimeoutError: If the future didn't finish executing before the
+                given timeout.
+        """
+
+        with self._condition:
+            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
+                raise CancelledError()
+            elif self._state == FINISHED:
+                return self._exception
+
+            self._condition.wait(timeout)
+
+            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
+                raise CancelledError()
+            elif self._state == FINISHED:
+                return self._exception
+            else:
+                raise TimeoutError()
+
+    # The following methods should only be used by Executors and in tests.
+    def set_running_or_notify_cancel(self):
+        """Mark the future as running or process any cancel notifications.
+
+        Should only be used by Executor implementations and unit tests.
+
+        If the future has been cancelled (cancel() was called and returned
+        True) then any threads waiting on the future completing (though calls
+        to as_completed() or wait()) are notified and False is returned.
+
+        If the future was not cancelled then it is put in the running state
+        (future calls to running() will return True) and True is returned.
+
+        This method should be called by Executor implementations before
+        executing the work associated with this future. If this method returns
+        False then the work should not be executed.
+
+        Returns:
+            False if the Future was cancelled, True otherwise.
+
+        Raises:
+            RuntimeError: if this method was already called or if set_result()
+                or set_exception() was called.
+        """
+        with self._condition:
+            if self._state == CANCELLED:
+                self._state = CANCELLED_AND_NOTIFIED
+                for waiter in self._waiters:
+                    waiter.add_cancelled(self)
+                # self._condition.notify_all() is not necessary because
+                # self.cancel() triggers a notification.
+                return False
+            elif self._state == PENDING:
+                self._state = RUNNING
+                return True
+            else:
+                LOGGER.critical('Future %s in unexpected state: %s',
+                                id(self),
+                                self._state)
+                raise RuntimeError('Future in unexpected state')
+
+    def set_result(self, result):
+        """Sets the return value of work associated with the future.
+
+        Should only be used by Executor implementations and unit tests.
+        """
+        with self._condition:
+            self._result = result
+            self._state = FINISHED
+            for waiter in self._waiters:
+                waiter.add_result(self)
+            self._condition.notify_all()
+        self._invoke_callbacks()
+
+    def set_exception(self, exception):
+        """Sets the result of the future as being the given exception.
+
+        Should only be used by Executor implementations and unit tests.
+        """
+        with self._condition:
+            self._exception = exception
+            self._state = FINISHED
+            for waiter in self._waiters:
+                waiter.add_exception(self)
+            self._condition.notify_all()
+        self._invoke_callbacks()
+
+
+class Executor(object):
+    """This is an abstract base class for concrete asynchronous executors."""
+
+    def submit(self, fn, *args, **kwargs):
+        """Submits a callable to be executed with the given arguments.
+
+        Schedules the callable to be executed as fn(*args, **kwargs) and
+        returns a Future instance representing the execution of the callable.
+
+        Returns:
+            A Future representing the given call.
+        """
+        raise NotImplementedError()
+
+    def map(self, fn, *iterables, **kwargs):
+        """Returns an iterator equivalent to map(fn, iter).
+
+        Args:
+            fn: A callable that will take as many arguments as there are
+                passed iterables.
+            timeout: The maximum number of seconds to wait. If None, then there
+                is no limit on the wait time.
+            chunksize: The size of the chunks the iterable will be broken into
+                before being passed to a child process. This argument is only
+                used by ProcessPoolExecutor; it is ignored by
+                ThreadPoolExecutor.
+
+        Returns:
+            An iterator equivalent to: map(func, *iterables) but the calls may
+            be evaluated out-of-order.
+
+        Raises:
+            TimeoutError: If the entire result iterator could not be generated
+                before the given timeout.
+            Exception: If fn(*args) raises for any values.
+        """
+        timeout = kwargs.get('timeout')
+        if timeout is not None:
+            end_time = timeout + time.time()
+
+        fs = [self.submit(fn, *args) for args in zip(*iterables)]
+
+        # Yield must be hidden in closure so that the futures are submitted
+        # before the first iterator value is required.
+        def result_iterator():
+            try:
+                for future in fs:
+                    if timeout is None:
+                        yield future.result()
+                    else:
+                        yield future.result(end_time - time.time())
+            finally:
+                for future in fs:
+                    future.cancel()
+        return result_iterator()
+
+    def shutdown(self, wait=True):
+        """Clean-up the resources associated with the Executor.
+
+        It is safe to call this method several times. Otherwise, no other
+        methods can be called after this one.
+
+        Args:
+            wait: If True then shutdown will not return until all running
+                futures have finished executing and the resources used by the
+                executor have been reclaimed.
+        """
+        pass
+
+    def __enter__(self):
+        return self
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        self.shutdown(wait=True)
+        return False
diff --git a/sklearn/externals/joblib/externals/loky/backend/__init__.py b/sklearn/externals/joblib/externals/loky/backend/__init__.py
new file mode 100755
index 0000000000..b5868d057a
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/__init__.py
@@ -0,0 +1,18 @@
+import os
+import sys
+
+from .context import get_context
+
+LOKY_PICKLER = os.environ.get("LOKY_PICKLER")
+
+if sys.version_info > (3, 4):
+
+    def _make_name():
+        name = '/loky-%i-%s' % (os.getpid(), next(synchronize.SemLock._rand))
+        return name
+
+    # monkey patch the name creation for multiprocessing
+    from multiprocessing import synchronize
+    synchronize.SemLock._make_name = staticmethod(_make_name)
+
+__all__ = ["get_context"]
diff --git a/sklearn/externals/joblib/externals/loky/backend/_posix_reduction.py b/sklearn/externals/joblib/externals/loky/backend/_posix_reduction.py
new file mode 100755
index 0000000000..e0e394d3cd
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/_posix_reduction.py
@@ -0,0 +1,76 @@
+###############################################################################
+# Extra reducers for Unix based system and connections objects
+#
+# author: Thomas Moreau and Olivier Grisel
+#
+# adapted from multiprocessing/reduction.py (17/02/2017)
+#  * Add adapted reduction for LokyProcesses and socket/Connection
+#
+import os
+import sys
+import socket
+import _socket
+
+from .reduction import register
+from .context import get_spawning_popen
+
+if sys.version_info >= (3, 3):
+    from multiprocessing.connection import Connection
+else:
+    from _multiprocessing import Connection
+
+
+HAVE_SEND_HANDLE = (hasattr(socket, 'CMSG_LEN') and
+                    hasattr(socket, 'SCM_RIGHTS') and
+                    hasattr(socket.socket, 'sendmsg'))
+
+
+def _mk_inheritable(fd):
+    if sys.version_info[:2] > (3, 3):
+        os.set_inheritable(fd, True)
+    return fd
+
+
+def DupFd(fd):
+    '''Return a wrapper for an fd.'''
+    popen_obj = get_spawning_popen()
+    if popen_obj is not None:
+        return popen_obj.DupFd(popen_obj.duplicate_for_child(fd))
+    elif HAVE_SEND_HANDLE and sys.version_info[:2] > (3, 3):
+        from multiprocessing import resource_sharer
+        return resource_sharer.DupFd(fd)
+    else:
+        raise TypeError(
+            'Cannot pickle connection object. This object can only be '
+            'passed when spawning a new process'
+        )
+
+
+if sys.version_info[:2] != (3, 3):
+    def _reduce_socket(s):
+        df = DupFd(s.fileno())
+        return _rebuild_socket, (df, s.family, s.type, s.proto)
+
+    def _rebuild_socket(df, family, type, proto):
+        fd = df.detach()
+        return socket.fromfd(fd, family, type, proto)
+else:
+    from multiprocessing.reduction import reduce_socket as _reduce_socket
+
+
+register(socket.socket, _reduce_socket)
+register(_socket.socket, _reduce_socket)
+
+
+if sys.version_info[:2] != (3, 3):
+    def reduce_connection(conn):
+        df = DupFd(conn.fileno())
+        return rebuild_connection, (df, conn.readable, conn.writable)
+
+    def rebuild_connection(df, readable, writable):
+        fd = df.detach()
+        return Connection(fd, readable, writable)
+else:
+    from multiprocessing.reduction import reduce_connection
+
+register(Connection, reduce_connection)
diff --git a/sklearn/externals/joblib/externals/loky/backend/_posix_wait.py b/sklearn/externals/joblib/externals/loky/backend/_posix_wait.py
new file mode 100755
index 0000000000..d935882dca
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/_posix_wait.py
@@ -0,0 +1,105 @@
+###############################################################################
+# Compat for wait function on UNIX based system
+#
+# author: Thomas Moreau and Olivier Grisel
+#
+# adapted from multiprocessing/connection.py (17/02/2017)
+#  * Backport wait function to python2.7
+#
+
+import platform
+import select
+import socket
+import errno
+SYSTEM = platform.system()
+
+try:
+    import ctypes
+except ImportError:  # pragma: no cover
+    ctypes = None  # noqa
+
+if SYSTEM == 'Darwin' and ctypes is not None:
+    from ctypes.util import find_library
+    libSystem = ctypes.CDLL(find_library('libSystem.dylib'))
+    CoreServices = ctypes.CDLL(find_library('CoreServices'),
+                               use_errno=True)
+    mach_absolute_time = libSystem.mach_absolute_time
+    mach_absolute_time.restype = ctypes.c_uint64
+    absolute_to_nanoseconds = CoreServices.AbsoluteToNanoseconds
+    absolute_to_nanoseconds.restype = ctypes.c_uint64
+    absolute_to_nanoseconds.argtypes = [ctypes.c_uint64]
+
+    def monotonic():
+        return absolute_to_nanoseconds(mach_absolute_time()) * 1e-9
+
+elif SYSTEM == 'Linux' and ctypes is not None:
+    # from stackoverflow:
+    # questions/1205722/how-do-i-get-monotonic-time-durations-in-python
+    import ctypes
+    import os
+
+    CLOCK_MONOTONIC = 1  # see <linux/time.h>
+
+    class timespec(ctypes.Structure):
+        _fields_ = [
+            ('tv_sec', ctypes.c_long),
+            ('tv_nsec', ctypes.c_long),
+        ]
+
+    librt = ctypes.CDLL('librt.so.1', use_errno=True)
+    clock_gettime = librt.clock_gettime
+    clock_gettime.argtypes = [
+        ctypes.c_int, ctypes.POINTER(timespec),
+    ]
+
+    def monotonic():  # noqa
+        t = timespec()
+        if clock_gettime(CLOCK_MONOTONIC, ctypes.pointer(t)) != 0:
+            errno_ = ctypes.get_errno()
+            raise OSError(errno_, os.strerror(errno_))
+        return t.tv_sec + t.tv_nsec * 1e-9
+else:  # pragma: no cover
+    from time import time as monotonic
+
+
+if hasattr(select, 'poll'):
+    def _poll(fds, timeout):
+        if timeout is not None:
+            timeout = int(timeout * 1000)  # timeout is in milliseconds
+        fd_map = {}
+        pollster = select.poll()
+        for fd in fds:
+            pollster.register(fd, select.POLLIN)
+            if hasattr(fd, 'fileno'):
+                fd_map[fd.fileno()] = fd
+            else:
+                fd_map[fd] = fd
+        ls = []
+        for fd, event in pollster.poll(timeout):
+            if event & select.POLLNVAL:  # pragma: no cover
+                raise ValueError('invalid file descriptor %i' % fd)
+            ls.append(fd_map[fd])
+        return ls
+else:
+    def _poll(fds, timeout):
+        return select.select(fds, [], [], timeout)[0]
+
+
+def wait(object_list, timeout=None):
+    '''
+    Wait till an object in object_list is ready/readable.
+    Returns list of those objects which are ready/readable.
+    '''
+    if timeout is not None:
+        if timeout <= 0:
+            return _poll(object_list, 0)
+        else:
+            deadline = monotonic() + timeout
+    while True:
+        try:
+            return _poll(object_list, timeout)
+        except (OSError, IOError, socket.error) as e:  # pragma: no cover
+            if e.errno != errno.EINTR:
+                raise
+        if timeout is not None:
+            timeout = deadline - monotonic()
diff --git a/sklearn/externals/joblib/externals/loky/backend/_win_reduction.py b/sklearn/externals/joblib/externals/loky/backend/_win_reduction.py
new file mode 100755
index 0000000000..142e6e7c80
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/_win_reduction.py
@@ -0,0 +1,99 @@
+###############################################################################
+# Extra reducers for Windows system and connections objects
+#
+# author: Thomas Moreau and Olivier Grisel
+#
+# adapted from multiprocessing/reduction.py (17/02/2017)
+#  * Add adapted reduction for LokyProcesses and socket/PipeConnection
+#
+import os
+import sys
+import socket
+from .reduction import register
+
+
+if sys.platform == 'win32':
+    if sys.version_info[:2] < (3, 3):
+        from _multiprocessing import PipeConnection
+    else:
+        import _winapi
+        from multiprocessing.connection import PipeConnection
+
+
+if sys.version_info[:2] >= (3, 4) and sys.platform == 'win32':
+    class DupHandle(object):
+        def __init__(self, handle, access, pid=None):
+            # duplicate handle for process with given pid
+            if pid is None:
+                pid = os.getpid()
+            proc = _winapi.OpenProcess(_winapi.PROCESS_DUP_HANDLE, False, pid)
+            try:
+                self._handle = _winapi.DuplicateHandle(
+                    _winapi.GetCurrentProcess(),
+                    handle, proc, access, False, 0)
+            finally:
+                _winapi.CloseHandle(proc)
+            self._access = access
+            self._pid = pid
+
+        def detach(self):
+            # retrieve handle from process which currently owns it
+            if self._pid == os.getpid():
+                return self._handle
+            proc = _winapi.OpenProcess(_winapi.PROCESS_DUP_HANDLE, False,
+                                       self._pid)
+            try:
+                return _winapi.DuplicateHandle(
+                    proc, self._handle, _winapi.GetCurrentProcess(),
+                    self._access, False, _winapi.DUPLICATE_CLOSE_SOURCE)
+            finally:
+                _winapi.CloseHandle(proc)
+
+    def reduce_pipe_connection(conn):
+        access = ((_winapi.FILE_GENERIC_READ if conn.readable else 0) |
+                  (_winapi.FILE_GENERIC_WRITE if conn.writable else 0))
+        dh = DupHandle(conn.fileno(), access)
+        return rebuild_pipe_connection, (dh, conn.readable, conn.writable)
+
+    def rebuild_pipe_connection(dh, readable, writable):
+        from multiprocessing.connection import PipeConnection
+        handle = dh.detach()
+        return PipeConnection(handle, readable, writable)
+    register(PipeConnection, reduce_pipe_connection)
+
+elif sys.platform == 'win32':
+    # Older Python versions
+    from multiprocessing.reduction import reduce_pipe_connection
+    register(PipeConnection, reduce_pipe_connection)
+
+
+if sys.version_info[:2] < (3, 3) and sys.platform == 'win32':
+    from _multiprocessing import win32
+    from multiprocessing.reduction import reduce_handle, rebuild_handle
+    close = win32.CloseHandle
+
+    def fromfd(handle, family, type_, proto=0):
+        s = socket.socket(family, type_, proto, fileno=handle)
+        if s.__class__ is not socket.socket:
+            s = socket.socket(_sock=s)
+        return s
+
+    def reduce_socket(s):
+        if not hasattr(socket, "fromfd"):
+            raise TypeError("sockets cannot be pickled on this system.")
+        reduced_handle = reduce_handle(s.fileno())
+        return _rebuild_socket, (reduced_handle, s.family, s.type, s.proto)
+
+    def _rebuild_socket(reduced_handle, family, type_, proto):
+        handle = rebuild_handle(reduced_handle)
+        s = fromfd(handle, family, type_, proto)
+        close(handle)
+        return s
+
+    register(socket.socket, reduce_socket)
+elif sys.version_info[:2] < (3, 4):
+    from multiprocessing.reduction import reduce_socket
+    register(socket.socket, reduce_socket)
+else:
+    from multiprocessing.reduction import _reduce_socket
+    register(socket.socket, _reduce_socket)
diff --git a/sklearn/externals/joblib/externals/loky/backend/_win_wait.py b/sklearn/externals/joblib/externals/loky/backend/_win_wait.py
new file mode 100755
index 0000000000..73271316d0
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/_win_wait.py
@@ -0,0 +1,58 @@
+###############################################################################
+# Compat for wait function on Windows system
+#
+# author: Thomas Moreau and Olivier Grisel
+#
+# adapted from multiprocessing/connection.py (17/02/2017)
+#  * Backport wait function to python2.7
+#
+
+import ctypes
+import sys
+from time import sleep
+
+
+if sys.platform == 'win32' and sys.version_info[:2] < (3, 3):
+    from _subprocess import WaitForSingleObject, WAIT_OBJECT_0
+
+    try:
+        from time import monotonic
+    except ImportError:
+        # Backward old for crappy old Python that did not have cross-platform
+        # monotonic clock by default.
+
+        # TODO: do we want to add support for cygwin at some point? See:
+        # https://github.com/atdt/monotonic/blob/master/monotonic.py
+        GetTickCount64 = ctypes.windll.kernel32.GetTickCount64
+        GetTickCount64.restype = ctypes.c_ulonglong
+
+        def monotonic():
+            """Monotonic clock, cannot go backward."""
+            return GetTickCount64() / 1000.0
+
+    def wait(handles, timeout=None):
+        """Backward compat for python2.7
+
+        This function wait for either:
+        * one connection is ready for read,
+        * one process handle has exited or got killed,
+        * timeout is reached. Note that this function has a precision of 2
+          msec.
+        """
+        if timeout is not None:
+            deadline = monotonic() + timeout
+
+        while True:
+            # We cannot use select as in windows it only support sockets
+            ready = []
+            for h in handles:
+                if type(h) in [int, long]:
+                    if WaitForSingleObject(h, 0) == WAIT_OBJECT_0:
+                        ready += [h]
+                elif h.poll(0):
+                    ready.append(h)
+            if len(ready) > 0:
+                return ready
+            sleep(.001)
+            if timeout is not None and deadline - monotonic() <= 0:
+                return []
diff --git a/sklearn/externals/joblib/externals/loky/backend/compat.py b/sklearn/externals/joblib/externals/loky/backend/compat.py
new file mode 100755
index 0000000000..6366b23d9f
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/compat.py
@@ -0,0 +1,26 @@
+# flake8: noqa
+###############################################################################
+# Compat file to import the correct modules for each platform and python
+# version.
+#
+# author: Thomas Moreau and Olivier grisel
+#
+import sys
+
+if sys.version_info[:2] >= (3, 3):
+    import queue
+    from _pickle import PicklingError
+else:
+    import Queue as queue
+    from pickle import PicklingError
+
+if sys.version_info >= (3, 4):
+    from multiprocessing.process import BaseProcess
+else:
+    from multiprocessing.process import Process as BaseProcess
+
+# Platform specific compat
+if sys.platform == "win32":
+    from .compat_win32 import *
+else:
+    from .compat_posix import *
diff --git a/sklearn/externals/joblib/externals/loky/backend/compat_posix.py b/sklearn/externals/joblib/externals/loky/backend/compat_posix.py
new file mode 100755
index 0000000000..c8e4e4a43c
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/compat_posix.py
@@ -0,0 +1,13 @@
+# flake8: noqa
+###############################################################################
+# Compat file to load the correct wait function
+#
+# author: Thomas Moreau and Olivier grisel
+#
+import sys
+
+# Compat wait
+if sys.version_info < (3, 3):
+    from ._posix_wait import wait
+else:
+    from multiprocessing.connection import wait
diff --git a/sklearn/externals/joblib/externals/loky/backend/compat_win32.py b/sklearn/externals/joblib/externals/loky/backend/compat_win32.py
new file mode 100755
index 0000000000..aa0a1fa919
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/compat_win32.py
@@ -0,0 +1,59 @@
+# flake8: noqa: F401
+import sys
+import numbers
+
+if sys.platform == "win32":
+    # Avoid import error by code introspection tools such as test runners
+    # trying to import this module while running on non-Windows systems.
+
+    # Compat Popen
+    if sys.version_info[:2] >= (3, 4):
+        from multiprocessing.popen_spawn_win32 import Popen
+    else:
+        from multiprocessing.forking import Popen
+
+    # wait compat
+    if sys.version_info[:2] < (3, 3):
+        from ._win_wait import wait
+    else:
+        from multiprocessing.connection import wait
+
+    # Compat _winapi
+    if sys.version_info[:2] >= (3, 4):
+        import _winapi
+    else:
+        import os
+        import msvcrt
+        if sys.version_info[:2] < (3, 3):
+            import _subprocess as win_api
+            from _multiprocessing import win32
+        else:
+            import _winapi as win_api
+
+        class _winapi:
+            CreateProcess = win_api.CreateProcess
+
+            @staticmethod
+            def CreatePipe(*args):
+                rfd, wfd = os.pipe()
+                _current_process = win_api.GetCurrentProcess()
+                rhandle = win_api.DuplicateHandle(
+                    _current_process, msvcrt.get_osfhandle(rfd),
+                    _current_process, 0, True,
+                    win_api.DUPLICATE_SAME_ACCESS)
+                if sys.version_info[:2] < (3, 3):
+                    rhandle = rhandle.Detach()
+                os.close(rfd)
+                return rhandle, wfd
+
+            @staticmethod
+            def CloseHandle(h):
+                if isinstance(h, numbers.Integral):
+                    # Cast long to int for 64-bit Python 2.7 under Windows
+                    h = int(h)
+                if sys.version_info[:2] < (3, 3):
+                    if not isinstance(h, int):
+                        h = h.Detach()
+                    win32.CloseHandle(h)
+                else:
+                    win_api.CloseHandle(h)
diff --git a/sklearn/externals/joblib/externals/loky/backend/context.py b/sklearn/externals/joblib/externals/loky/backend/context.py
new file mode 100755
index 0000000000..52df5589cb
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/context.py
@@ -0,0 +1,227 @@
+###############################################################################
+# Basic context management with LokyContext and  provides
+# compat for UNIX 2.7 and 3.3
+#
+# author: Thomas Moreau and Olivier Grisel
+#
+# adapted from multiprocessing/context.py
+#  * Create a context ensuring loky uses only objects that are compatible
+#  * Add LokyContext to the list of context of multiprocessing so loky can be
+#    used with multiprocessing.set_start_method
+#  * Add some compat function for python2.7 and 3.3.
+#
+from __future__ import division
+
+import os
+import sys
+import warnings
+import multiprocessing as mp
+
+
+from .process import LokyProcess
+
+if sys.version_info[:2] >= (3, 4):
+    from multiprocessing import get_context as get_mp_context
+    from multiprocessing.context import assert_spawning, set_spawning_popen
+    from multiprocessing.context import get_spawning_popen, BaseContext
+
+    def get_context(method="loky"):
+        if method == "fork":
+            warnings.warn("`fork` start method should not be used with `loky` "
+                          "as it does not respect POSIX. Try using `spawn` or "
+                          "`loky` instead.", UserWarning)
+        return get_mp_context(method)
+
+else:
+    METHODS = ['loky', 'loky_init_main']
+    if sys.platform != 'win32':
+        import threading
+        # Mecanism to check that the current thread is spawning a child process
+        _tls = threading.local()
+        popen_attr = 'spawning_popen'
+    else:
+        from multiprocessing.forking import Popen
+        _tls = Popen._tls
+        popen_attr = 'process_handle'
+
+    BaseContext = object
+
+    def get_spawning_popen():
+        return getattr(_tls, popen_attr, None)
+
+    def set_spawning_popen(popen):
+        setattr(_tls, popen_attr, popen)
+
+    def assert_spawning(obj):
+        if get_spawning_popen() is None:
+            raise RuntimeError(
+                '%s objects should only be shared between processes'
+                ' through inheritance' % type(obj).__name__
+            )
+
+    def get_context(method="loky"):
+        if method == "loky":
+            return LokyContext()
+        elif method == "loky_init_main":
+            return LokyInitMainContext()
+        else:
+            raise ValueError("Method {} is not implemented. The available "
+                             "methods are {}".format(method, METHODS))
+
+
+def cpu_count():
+    """Return the number of CPUs the current process can use.
+
+    The returned number of CPUs accounts for:
+     * the number of CPUs in the system, as given by
+       ``multiprocessing.cpu_count``
+     * the CPU affinity settings of the current process
+       (available with Python 3.4+ on some Unix systems)
+     * CFS scheduler CPU bandwidth limit
+       (available on Linux only)
+    and is given as the minimum of these three constraints.
+    It is also always larger or equal to 1.
+    """
+    import math
+
+    try:
+        cpu_count_mp = mp.cpu_count()
+    except NotImplementedError:
+        cpu_count_mp = 1
+
+    # Number of available CPUs given affinity settings
+    cpu_count_affinity = cpu_count_mp
+    if hasattr(os, 'sched_getaffinity'):
+        try:
+            cpu_count_affinity = len(os.sched_getaffinity(0))
+        except NotImplementedError:
+            pass
+
+    # CFS scheduler CPU bandwidth limit
+    # available in Linux since 2.6 kernel
+    cpu_count_cfs = cpu_count_mp
+    cfs_quota_fname = "/sys/fs/cgroup/cpu/cpu.cfs_quota_us"
+    cfs_period_fname = "/sys/fs/cgroup/cpu/cpu.cfs_period_us"
+    if os.path.exists(cfs_quota_fname) and os.path.exists(cfs_period_fname):
+        with open(cfs_quota_fname, 'r') as fh:
+            cfs_quota_us = int(fh.read())
+        with open(cfs_period_fname, 'r') as fh:
+            cfs_period_us = int(fh.read())
+
+        if cfs_quota_us > 0 and cfs_period_us > 0:
+            cpu_count_cfs = math.ceil(cfs_quota_us / cfs_period_us)
+            cpu_count_cfs = max(cpu_count_cfs, 1)
+
+    return min(cpu_count_mp, cpu_count_affinity, cpu_count_cfs)
+
+
+class LokyContext(BaseContext):
+    """Context relying on the LokyProcess."""
+    _name = 'loky'
+    Process = LokyProcess
+    cpu_count = staticmethod(cpu_count)
+
+    def Queue(self, maxsize=0, reducers=None):
+        '''Returns a queue object'''
+        from .queues import Queue
+        return Queue(maxsize, reducers=reducers,
+                     ctx=self.get_context())
+
+    def SimpleQueue(self, reducers=None):
+        '''Returns a queue object'''
+        from .queues import SimpleQueue
+        return SimpleQueue(reducers=reducers, ctx=self.get_context())
+
+    if sys.version_info[:2] < (3, 4):
+        """Compat for python2.7/3.3 for necessary methods in Context"""
+        def get_context(self):
+            return self
+
+        def get_start_method(self):
+            return "loky"
+
+        def Pipe(self, duplex=True):
+            '''Returns two connection object connected by a pipe'''
+            return mp.Pipe(duplex)
+
+        if sys.platform != "win32":
+            """Use the compat Manager for python2.7/3.3 on UNIX to avoid
+            relying on fork processes
+            """
+            def Manager(self):
+                """Returns a manager object"""
+                from .managers import LokyManager
+                m = LokyManager()
+                m.start()
+                return m
+        else:
+            """Compat for context on Windows and python2.7/3.3. Using regular
+            multiprocessing objects as it does not rely on fork.
+            """
+            from multiprocessing import synchronize
+            Semaphore = staticmethod(synchronize.Semaphore)
+            BoundedSemaphore = staticmethod(synchronize.BoundedSemaphore)
+            Lock = staticmethod(synchronize.Lock)
+            RLock = staticmethod(synchronize.RLock)
+            Condition = staticmethod(synchronize.Condition)
+            Event = staticmethod(synchronize.Event)
+            Manager = staticmethod(mp.Manager)
+
+    if sys.platform != "win32":
+        """For Unix platform, use our custom implementation of synchronize
+        relying on ctypes to interface with pthread semaphores.
+        """
+        def Semaphore(self, value=1):
+            """Returns a semaphore object"""
+            from . import synchronize
+            return synchronize.Semaphore(value=value)
+
+        def BoundedSemaphore(self, value):
+            """Returns a bounded semaphore object"""
+            from .synchronize import BoundedSemaphore
+            return BoundedSemaphore(value)
+
+        def Lock(self):
+            """Returns a lock object"""
+            from .synchronize import Lock
+            return Lock()
+
+        def RLock(self):
+            """Returns a recurrent lock object"""
+            from .synchronize import RLock
+            return RLock()
+
+        def Condition(self, lock=None):
+            """Returns a condition object"""
+            from .synchronize import Condition
+            return Condition(lock)
+
+        def Event(self):
+            """Returns an event object"""
+            from .synchronize import Event
+            return Event()
+
+
+class LokyInitMainContext(LokyContext):
+    """Extra context with LokyProcess, which does load the main module
+
+    This context is used for compatibility in the case ``cloudpickle`` is not
+    present on the running system. This permits to load functions defined in
+    the ``main`` module, using proper safeguards. The declaration of the
+    ``executor`` should be protected by ``if __name__ == "__main__":`` and the
+    functions and variable used from main should be out of this block.
+
+    This mimics the default behavior of multiprocessing under Windows and the
+    behavior of the ``spawn`` start method on a posix system for python3.4+.
+    For more details, see the end of the following section of python doc
+    https://docs.python.org/3/library/multiprocessing.html#multiprocessing-programming
+    """
+    def Process(self, *args, **kwargs):
+        kwargs.pop('init_main_module', True)
+        return LokyProcess(*args, init_main_module=True, **kwargs)
+
+
+if sys.version_info > (3, 4):
+    """Register loky context so it works with multiprocessing.get_context"""
+    mp.context._concrete_contexts['loky'] = LokyContext()
+    mp.context._concrete_contexts['loky_init_main'] = LokyInitMainContext()
diff --git a/sklearn/externals/joblib/externals/loky/backend/fork_exec.py b/sklearn/externals/joblib/externals/loky/backend/fork_exec.py
new file mode 100755
index 0000000000..eee2a1c80a
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/fork_exec.py
@@ -0,0 +1,43 @@
+###############################################################################
+# Launch a subprocess using forkexec and make sure only the needed fd are
+# shared in the two process.
+#
+# author: Thomas Moreau and Olivier Grisel
+#
+import os
+import sys
+
+if sys.platform == "darwin" and sys.version_info < (3, 3):
+    FileNotFoundError = OSError
+
+
+def close_fds(keep_fds):  # pragma: no cover
+    """Close all the file descriptors except those in keep_fds."""
+
+    # Make sure to keep stdout and stderr open for logging purpose
+    keep_fds = set(keep_fds).union([1, 2])
+
+    # We try to retrieve all the open fds
+    try:
+        open_fds = set(int(fd) for fd in os.listdir('/proc/self/fd'))
+    except FileNotFoundError:
+        import resource
+        max_nfds = resource.getrlimit(resource.RLIMIT_NOFILE)[0]
+        open_fds = set(fd for fd in range(3, max_nfds))
+        open_fds.add(0)
+
+    for i in open_fds - keep_fds:
+        try:
+            os.close(i)
+        except OSError:
+            pass
+
+
+def fork_exec(cmd, keep_fds):
+
+    pid = os.fork()
+    if pid == 0:  # pragma: no cover
+        close_fds(keep_fds)
+        os.execv(sys.executable, cmd)
+    else:
+        return pid
diff --git a/sklearn/externals/joblib/externals/loky/backend/managers.py b/sklearn/externals/joblib/externals/loky/backend/managers.py
new file mode 100755
index 0000000000..081f8976e4
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/managers.py
@@ -0,0 +1,51 @@
+###############################################################################
+# compat for UNIX 2.7 and 3.3
+# Manager with LokyContext server.
+# This avoids having a Manager using fork and breaks the fd.
+#
+# author: Thomas Moreau and Olivier Grisel
+#
+# based on multiprocessing/managers.py (17/02/2017)
+#  * Overload the start method to use LokyContext and launch a loky subprocess
+#
+
+import multiprocessing as mp
+from multiprocessing.managers import SyncManager, State
+from .process import LokyProcess as Process
+
+
+class LokyManager(SyncManager):
+    def start(self, initializer=None, initargs=()):
+        '''Spawn a server process for this manager object'''
+        assert self._state.value == State.INITIAL
+
+        if (initializer is not None
+                and not hasattr(initializer, '__call__')):
+            raise TypeError('initializer must be a callable')
+
+        # pipe over which we will retrieve address of server
+        reader, writer = mp.Pipe(duplex=False)
+
+        # spawn process which runs a server
+        self._process = Process(
+            target=type(self)._run_server,
+            args=(self._registry, self._address, bytes(self._authkey),
+                  self._serializer, writer, initializer, initargs),
+        )
+        ident = ':'.join(str(i) for i in self._process._identity)
+        self._process.name = type(self).__name__ + '-' + ident
+        self._process.start()
+
+        # get address of server
+        writer.close()
+        self._address = reader.recv()
+        reader.close()
+
+        # register a finalizer
+        self._state.value = State.STARTED
+        self.shutdown = mp.util.Finalize(
+            self, type(self)._finalize_manager,
+            args=(self._process, self._address, self._authkey,
+                  self._state, self._Client),
+            exitpriority=0
+        )
diff --git a/sklearn/externals/joblib/externals/loky/backend/popen_loky_posix.py b/sklearn/externals/joblib/externals/loky/backend/popen_loky_posix.py
new file mode 100755
index 0000000000..729c7c71fe
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/popen_loky_posix.py
@@ -0,0 +1,214 @@
+###############################################################################
+# Popen for LokyProcess.
+#
+# author: Thomas Moreau and Olivier Grisel
+#
+import os
+import sys
+import signal
+import pickle
+from io import BytesIO
+
+from . import reduction, spawn
+from .context import get_spawning_popen, set_spawning_popen
+from multiprocessing import util, process
+
+if sys.version_info[:2] < (3, 3):
+    ProcessLookupError = OSError
+
+if sys.platform != "win32":
+    from . import semaphore_tracker
+
+
+__all__ = []
+
+if sys.platform != "win32":
+    #
+    # Wrapper for an fd used while launching a process
+    #
+
+    class _DupFd(object):
+        def __init__(self, fd):
+            self.fd = reduction._mk_inheritable(fd)
+
+        def detach(self):
+            return self.fd
+
+    #
+    # Start child process using subprocess.Popen
+    #
+
+    __all__.append('Popen')
+
+    class Popen(object):
+        method = 'loky'
+        DupFd = _DupFd
+
+        def __init__(self, process_obj):
+            sys.stdout.flush()
+            sys.stderr.flush()
+            self.returncode = None
+            self._fds = []
+            self._launch(process_obj)
+
+        if sys.version_info < (3, 4):
+            @classmethod
+            def duplicate_for_child(cls, fd):
+                popen = get_spawning_popen()
+                popen._fds.append(fd)
+                return reduction._mk_inheritable(fd)
+
+        else:
+            def duplicate_for_child(self, fd):
+                self._fds.append(fd)
+                return reduction._mk_inheritable(fd)
+
+        def poll(self, flag=os.WNOHANG):
+            if self.returncode is None:
+                while True:
+                    try:
+                        pid, sts = os.waitpid(self.pid, flag)
+                    except OSError as e:
+                        # Child process not yet created. See #1731717
+                        # e.errno == errno.ECHILD == 10
+                        return None
+                    else:
+                        break
+                if pid == self.pid:
+                    if os.WIFSIGNALED(sts):
+                        self.returncode = -os.WTERMSIG(sts)
+                    else:
+                        assert os.WIFEXITED(sts)
+                        self.returncode = os.WEXITSTATUS(sts)
+            return self.returncode
+
+        def wait(self, timeout=None):
+            if sys.version_info < (3, 3):
+                import time
+                if timeout is None:
+                    return self.poll(0)
+                deadline = time.time() + timeout
+                delay = 0.0005
+                while 1:
+                    res = self.poll()
+                    if res is not None:
+                        break
+                    remaining = deadline - time.time()
+                    if remaining <= 0:
+                        break
+                    delay = min(delay * 2, remaining, 0.05)
+                    time.sleep(delay)
+                return res
+
+            if self.returncode is None:
+                if timeout is not None:
+                    from multiprocessing.connection import wait
+                    if not wait([self.sentinel], timeout):
+                        return None
+                # This shouldn't block if wait() returned successfully.
+                return self.poll(os.WNOHANG if timeout == 0.0 else 0)
+            return self.returncode
+
+        def terminate(self):
+            if self.returncode is None:
+                try:
+                    os.kill(self.pid, signal.SIGTERM)
+                except ProcessLookupError:
+                    pass
+                except OSError:
+                    if self.wait(timeout=0.1) is None:
+                        raise
+
+        def _launch(self, process_obj):
+
+            tracker_fd = semaphore_tracker._semaphore_tracker.getfd()
+
+            fp = BytesIO()
+            set_spawning_popen(self)
+            try:
+                prep_data = spawn.get_preparation_data(
+                    process_obj._name, process_obj.init_main_module)
+                reduction.dump(prep_data, fp)
+                reduction.dump(process_obj, fp)
+
+            finally:
+                set_spawning_popen(None)
+
+            try:
+                parent_r, child_w = os.pipe()
+                child_r, parent_w = os.pipe()
+                # for fd in self._fds:
+                #     _mk_inheritable(fd)
+
+                cmd_python = [sys.executable]
+                cmd_python += ['-m', self.__module__]
+                cmd_python += ['--process-name', str(process_obj.name)]
+                cmd_python += ['--pipe',
+                               str(reduction._mk_inheritable(child_r))]
+                reduction._mk_inheritable(child_w)
+                if tracker_fd is not None:
+                    cmd_python += ['--semaphore',
+                                   str(reduction._mk_inheritable(tracker_fd))]
+                self._fds.extend([child_r, child_w, tracker_fd])
+                util.debug("launch python with cmd:\n%s" % cmd_python)
+                from .fork_exec import fork_exec
+                pid = fork_exec(cmd_python, self._fds)
+                self.sentinel = parent_r
+
+                method = 'getbuffer'
+                if not hasattr(fp, method):
+                    method = 'getvalue'
+                with os.fdopen(parent_w, 'wb') as f:
+                    f.write(getattr(fp, method)())
+                self.pid = pid
+            finally:
+                if parent_r is not None:
+                    util.Finalize(self, os.close, (parent_r,))
+                for fd in (child_r, child_w):
+                    if fd is not None:
+                        os.close(fd)
+
+        @staticmethod
+        def thread_is_spawning():
+            return True
+
+
+if __name__ == '__main__':
+    import argparse
+    parser = argparse.ArgumentParser('Command line parser')
+    parser.add_argument('--pipe', type=int, required=True,
+                        help='File handle for the pipe')
+    parser.add_argument('--semaphore', type=int, required=True,
+                        help='File handle name for the semaphore tracker')
+    parser.add_argument('--process-name', type=str, default=None,
+                        help='Identifier for debugging purpose')
+
+    args = parser.parse_args()
+
+    info = dict()
+    semaphore_tracker._semaphore_tracker._fd = args.semaphore
+
+    exitcode = 1
+    try:
+        with os.fdopen(args.pipe, 'rb') as from_parent:
+            process.current_process()._inheriting = True
+            try:
+                prep_data = pickle.load(from_parent)
+                spawn.prepare(prep_data)
+                process_obj = pickle.load(from_parent)
+            finally:
+                del process.current_process()._inheriting
+
+        exitcode = process_obj._bootstrap()
+    except Exception as e:
+        print('\n\n' + '-' * 80)
+        print('{} failed with traceback: '.format(args.process_name))
+        print('-' * 80)
+        import traceback
+        print(traceback.format_exc())
+        print('\n' + '-' * 80)
+    finally:
+        if from_parent is not None:
+            from_parent.close()
+
+        sys.exit(exitcode)
diff --git a/sklearn/externals/joblib/externals/loky/backend/popen_loky_win32.py b/sklearn/externals/joblib/externals/loky/backend/popen_loky_win32.py
new file mode 100755
index 0000000000..1f3e909e78
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/popen_loky_win32.py
@@ -0,0 +1,99 @@
+import os
+import sys
+
+from .context import get_spawning_popen, set_spawning_popen
+from . import spawn
+from . import reduction
+from multiprocessing import util
+
+if sys.platform == "win32":
+    # Avoid import error by code introspection tools such as test runners
+    # trying to import this module while running on non-Windows systems.
+    import msvcrt
+    from .compat_win32 import _winapi
+    from .compat_win32 import Popen as _Popen
+else:
+    _Popen = object
+
+if sys.version_info[:2] < (3, 3):
+    from os import fdopen as open
+
+__all__ = ['Popen']
+
+#
+#
+#
+
+TERMINATE = 0x10000
+WINEXE = (sys.platform == 'win32' and getattr(sys, 'frozen', False))
+WINSERVICE = sys.executable.lower().endswith("pythonservice.exe")
+
+
+#
+# We define a Popen class similar to the one from subprocess, but
+# whose constructor takes a process object as its argument.
+#
+
+class Popen(_Popen):
+    '''
+    Start a subprocess to run the code of a process object
+    '''
+    method = 'loky'
+
+    def __init__(self, process_obj):
+        prep_data = spawn.get_preparation_data(
+            process_obj._name, process_obj.init_main_module)
+
+        # read end of pipe will be "stolen" by the child process
+        # -- see spawn_main() in spawn.py.
+        rhandle, wfd = _winapi.CreatePipe(None, 0)
+        if sys.version_info[:2] > (3, 3):
+            wfd = msvcrt.open_osfhandle(wfd, 0)
+
+        cmd = spawn.get_command_line(parent_pid=os.getpid(),
+                                     pipe_handle=rhandle)
+        cmd = ' '.join('"%s"' % x for x in cmd)
+
+        try:
+            with open(wfd, 'wb') as to_child:
+                # start process
+                try:
+                    inherit = sys.version_info[:2] < (3, 4)
+                    hp, ht, pid, tid = _winapi.CreateProcess(
+                        spawn.get_executable(), cmd,
+                        None, None, inherit, 0,
+                        None, None, None)
+                    _winapi.CloseHandle(ht)
+                except:
+                    _winapi.CloseHandle(rhandle)
+                    raise
+
+                # set attributes of self
+                self.pid = pid
+                self.returncode = None
+                self._handle = hp
+                self.sentinel = int(hp)
+                util.Finalize(self, _winapi.CloseHandle, (self.sentinel,))
+
+                # send information to child
+                set_spawning_popen(self)
+                if sys.version_info[:2] < (3, 4):
+                    Popen._tls.process_handle = int(hp)
+                try:
+                    reduction.dump(prep_data, to_child)
+                    reduction.dump(process_obj, to_child)
+                finally:
+                    set_spawning_popen(None)
+                    if sys.version_info[:2] < (3, 4):
+                        del Popen._tls.process_handle
+        except IOError as exc:
+            # IOError 22 happens when the launched subprocess terminated before
+            # wfd.close is called. Thus we can safely ignore it.
+            if exc.errno != 22:
+                raise
+            util.debug("While starting {}, ignored a IOError 22"
+                       .format(process_obj._name))
+
+    def duplicate_for_child(self, handle):
+        assert self is get_spawning_popen()
+        return reduction.duplicate(handle, self.sentinel)
diff --git a/sklearn/externals/joblib/externals/loky/backend/process.py b/sklearn/externals/joblib/externals/loky/backend/process.py
new file mode 100755
index 0000000000..401a46fa4f
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/process.py
@@ -0,0 +1,91 @@
+###############################################################################
+# LokyProcess implementation
+#
+# authors: Thomas Moreau and Olivier Grisel
+#
+# based on multiprocessing/process.py  (17/02/2017)
+# * Add some compatibility function for python2.7 and 3.3
+#
+import os
+import sys
+from .compat import BaseProcess
+
+
+class LokyProcess(BaseProcess):
+    _start_method = 'loky'
+
+    def __init__(self, group=None, target=None, name=None, args=(),
+                 kwargs={}, daemon=None, init_main_module=False):
+        if sys.version_info < (3, 3):
+            super(LokyProcess, self).__init__(
+                group=group, target=target, name=name, args=args,
+                kwargs=kwargs)
+            self.daemon = daemon
+        else:
+            super(LokyProcess, self).__init__(
+                group=group, target=target, name=name, args=args,
+                kwargs=kwargs, daemon=daemon)
+        self.authkey = self.authkey
+        self.init_main_module = init_main_module
+
+    @staticmethod
+    def _Popen(process_obj):
+        if sys.platform == "win32":
+            from .popen_loky_win32 import Popen
+        else:
+            from .popen_loky_posix import Popen
+        return Popen(process_obj)
+
+    if sys.version_info < (3, 3):
+        def start(self):
+            '''
+            Start child process
+            '''
+            from multiprocessing.process import _current_process, _cleanup
+            assert self._popen is None, 'cannot start a process twice'
+            assert self._parent_pid == os.getpid(), \
+                'can only start a process object created by current process'
+            _cleanup()
+            self._popen = self._Popen(self)
+            self._sentinel = self._popen.sentinel
+            _current_process._children.add(self)
+
+        @property
+        def sentinel(self):
+            '''
+            Return a file descriptor (Unix) or handle (Windows) suitable for
+            waiting for process termination.
+            '''
+            try:
+                return self._sentinel
+            except AttributeError:
+                raise ValueError("process not started")
+
+    if sys.version_info < (3, 4):
+        @property
+        def authkey(self):
+            return self._authkey
+
+        @authkey.setter
+        def authkey(self, authkey):
+            '''
+            Set authorization key of process
+            '''
+            self._authkey = AuthenticationKey(authkey)
+
+
+#
+# We subclass bytes to avoid accidental transmission of auth keys over network
+#
+
+class AuthenticationKey(bytes):
+    def __reduce__(self):
+        from .context import assert_spawning
+        try:
+            assert_spawning(self)
+        except RuntimeError:
+            raise TypeError(
+                'Pickling an AuthenticationKey object is '
+                'disallowed for security reasons'
+            )
+        return AuthenticationKey, (bytes(self),)
diff --git a/sklearn/externals/joblib/externals/loky/backend/queues.py b/sklearn/externals/joblib/externals/loky/backend/queues.py
new file mode 100755
index 0000000000..f640151687
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/queues.py
@@ -0,0 +1,239 @@
+###############################################################################
+# Queue and SimpleQueue implementation for loky
+#
+# authors: Thomas Moreau, Olivier Grisel
+#
+# based on multiprocessing/queues.py (16/02/2017)
+# * Add some compatibility function for python2.7 and 3.3 and makes sure
+#   it uses the right synchronization primitive.
+# * Add some custom reducers for the Queues/SimpleQueue to tweak the
+#   pickling process. (overload Queue._feed/SimpleQueue.put)
+#
+import os
+import sys
+import errno
+import weakref
+import threading
+
+from multiprocessing import util
+from multiprocessing import connection
+from multiprocessing.synchronize import SEM_VALUE_MAX
+from multiprocessing.queues import Full
+from multiprocessing.queues import _sentinel, Queue as mp_Queue
+from multiprocessing.queues import SimpleQueue as mp_SimpleQueue
+
+from .reduction import CustomizableLokyPickler
+from .context import assert_spawning, get_context
+
+
+__all__ = ['Queue', 'SimpleQueue', 'Full']
+
+
+class Queue(mp_Queue):
+
+    def __init__(self, maxsize=0, reducers=None, ctx=None):
+
+        if sys.version_info[:2] >= (3, 4):
+            super().__init__(maxsize=maxsize, ctx=ctx)
+        else:
+            if maxsize <= 0:
+                # Can raise ImportError (see issues #3770 and #23400)
+                maxsize = SEM_VALUE_MAX
+            if ctx is None:
+                ctx = get_context()
+            self._maxsize = maxsize
+            self._reader, self._writer = connection.Pipe(duplex=False)
+            self._rlock = ctx.Lock()
+            self._opid = os.getpid()
+            if sys.platform == 'win32':
+                self._wlock = None
+            else:
+                self._wlock = ctx.Lock()
+            self._sem = ctx.BoundedSemaphore(maxsize)
+
+            # For use by concurrent.futures
+            self._ignore_epipe = False
+
+            self._after_fork()
+
+            if sys.platform != 'win32':
+                util.register_after_fork(self, Queue._after_fork)
+
+        self._reducers = reducers
+
+    # Use custom queue set/get state to be able to reduce the custom reducers
+    def __getstate__(self):
+        assert_spawning(self)
+        return (self._ignore_epipe, self._maxsize, self._reader, self._writer,
+                self._reducers, self._rlock, self._wlock, self._sem,
+                self._opid)
+
+    def __setstate__(self, state):
+        (self._ignore_epipe, self._maxsize, self._reader, self._writer,
+         self._reducers, self._rlock, self._wlock, self._sem,
+         self._opid) = state
+        self._after_fork()
+
+    # Overload _start_thread to correctly call our custom _feed
+    def _start_thread(self):
+        util.debug('Queue._start_thread()')
+
+        # Start thread which transfers data from buffer to pipe
+        self._buffer.clear()
+        self._thread = threading.Thread(
+            target=Queue._feed,
+            args=(self._buffer, self._notempty, self._send_bytes,
+                  self._wlock, self._writer.close, self._reducers,
+                  self._ignore_epipe, self._on_queue_feeder_error, self._sem),
+            name='QueueFeederThread'
+        )
+        self._thread.daemon = True
+
+        util.debug('doing self._thread.start()')
+        self._thread.start()
+        util.debug('... done self._thread.start()')
+
+        # On process exit we will wait for data to be flushed to pipe.
+        #
+        # However, if this process created the queue then all
+        # processes which use the queue will be descendants of this
+        # process.  Therefore waiting for the queue to be flushed
+        # is pointless once all the child processes have been joined.
+        created_by_this_process = (self._opid == os.getpid())
+        if not self._joincancelled and not created_by_this_process:
+            self._jointhread = util.Finalize(
+                self._thread, Queue._finalize_join,
+                [weakref.ref(self._thread)],
+                exitpriority=-5
+            )
+
+        # Send sentinel to the thread queue object when garbage collected
+        self._close = util.Finalize(
+            self, Queue._finalize_close,
+            [self._buffer, self._notempty],
+            exitpriority=10
+        )
+
+    # Overload the _feed methods to use our custom pickling strategy.
+    @staticmethod
+    def _feed(buffer, notempty, send_bytes, writelock, close, reducers,
+              ignore_epipe, onerror, queue_sem):
+        util.debug('starting thread to feed data to pipe')
+        nacquire = notempty.acquire
+        nrelease = notempty.release
+        nwait = notempty.wait
+        bpopleft = buffer.popleft
+        sentinel = _sentinel
+        if sys.platform != 'win32':
+            wacquire = writelock.acquire
+            wrelease = writelock.release
+        else:
+            wacquire = None
+
+        while 1:
+            try:
+                nacquire()
+                try:
+                    if not buffer:
+                        nwait()
+                finally:
+                    nrelease()
+                try:
+                    while 1:
+                        obj = bpopleft()
+                        if obj is sentinel:
+                            util.debug('feeder thread got sentinel -- exiting')
+                            close()
+                            return
+
+                        # serialize the data before acquiring the lock
+                        obj = CustomizableLokyPickler.dumps(
+                            obj, reducers=reducers)
+                        if wacquire is None:
+                            send_bytes(obj)
+                        else:
+                            wacquire()
+                            try:
+                                send_bytes(obj)
+                            finally:
+                                wrelease()
+                except IndexError:
+                    pass
+            except BaseException as e:
+                if ignore_epipe and getattr(e, 'errno', 0) == errno.EPIPE:
+                    return
+                # Since this runs in a daemon thread the resources it uses
+                # may be become unusable while the process is cleaning up.
+                # We ignore errors which happen after the process has
+                # started to cleanup.
+                if util.is_exiting():
+                    util.info('error in queue thread: %s', e)
+                    return
+                else:
+                    queue_sem.release()
+                    onerror(e, obj)
+
+    def _on_queue_feeder_error(self, e, obj):
+        """
+        Private API hook called when feeding data in the background thread
+        raises an exception.  For overriding by concurrent.futures.
+        """
+        import traceback
+        traceback.print_exc()
+
+    if sys.version_info[:2] < (3, 4):
+        # Compat for python2.7/3.3 that use _send instead of _send_bytes
+        def _after_fork(self):
+            super(Queue, self)._after_fork()
+            self._send_bytes = self._writer.send_bytes
+
+
+class SimpleQueue(mp_SimpleQueue):
+
+    def __init__(self, reducers=None, ctx=None):
+        if sys.version_info[:2] >= (3, 4):
+            super().__init__(ctx=ctx)
+        else:
+            # Use the context to create the sync objects for python2.7/3.3
+            if ctx is None:
+                ctx = get_context()
+            self._reader, self._writer = connection.Pipe(duplex=False)
+            self._rlock = ctx.Lock()
+            self._poll = self._reader.poll
+            if sys.platform == 'win32':
+                self._wlock = None
+            else:
+                self._wlock = ctx.Lock()
+
+        # Add possiblity to use custom reducers
+        self._reducers = reducers
+
+    # Use custom queue set/get state to be able to reduce the custom reducers
+    def __getstate__(self):
+        assert_spawning(self)
+        return (self._reader, self._writer, self._reducers, self._rlock,
+                self._wlock)
+
+    def __setstate__(self, state):
+        (self._reader, self._writer, self._reducers, self._rlock,
+         self._wlock) = state
+
+    if sys.version_info[:2] < (3, 4):
+        # For python2.7/3.3, overload get to avoid creating deadlocks with
+        # unpickling errors.
+        def get(self):
+            with self._rlock:
+                res = self._reader.recv_bytes()
+            # unserialize the data after having released the lock
+            return CustomizableLokyPickler.loads(res)
+
+    # Overload put to use our customizable reducer
+    def put(self, obj):
+        # serialize the data before acquiring the lock
+        obj = CustomizableLokyPickler.dumps(obj, reducers=self._reducers)
+        if self._wlock is None:
+            # writes to a message oriented win32 pipe are atomic
+            self._writer.send_bytes(obj)
+        else:
+            with self._wlock:
+                self._writer.send_bytes(obj)
diff --git a/sklearn/externals/joblib/externals/loky/backend/reduction.py b/sklearn/externals/joblib/externals/loky/backend/reduction.py
new file mode 100755
index 0000000000..20eb581cbf
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/reduction.py
@@ -0,0 +1,206 @@
+###############################################################################
+# Customizable Pickler with some basic reducers
+#
+# author: Thomas Moreau
+#
+# adapted from multiprocessing/reduction.py (17/02/2017)
+#  * Replace the ForkingPickler with a similar _LokyPickler,
+#  * Add CustomizableLokyPickler to allow customizing pickling process
+#    on the fly.
+#
+import io
+import sys
+import functools
+import warnings
+from multiprocessing import util
+try:
+    # Python 2 compat
+    from cPickle import loads
+except ImportError:
+    from pickle import loads
+    import copyreg
+
+if sys.platform == "win32":
+    if sys.version_info[:2] > (3, 3):
+        from multiprocessing.reduction import duplicate
+    else:
+        from multiprocessing.forking import duplicate
+
+from pickle import HIGHEST_PROTOCOL
+from . import LOKY_PICKLER
+
+Pickler = None
+try:
+    if LOKY_PICKLER is None or LOKY_PICKLER == "":
+        from pickle import Pickler
+    elif LOKY_PICKLER == "cloudpickle":
+        from cloudpickle import CloudPickler as Pickler
+    elif LOKY_PICKLER == "dill":
+        from dill import Pickler
+    elif LOKY_PICKLER != "pickle":
+        from importlib import import_module
+        mpickle = import_module(LOKY_PICKLER)
+        Pickler = mpickle.Pickler
+    util.debug("Using default backend {} for pickling."
+               .format(LOKY_PICKLER if LOKY_PICKLER is not None
+                       else "pickle"))
+except ImportError:
+    warnings.warn("Failed to import {} as asked in LOKY_PICKLER. Make sure"
+                  " it is correctly installed on your system. Falling back"
+                  " to default builtin pickle.".format(LOKY_PICKLER))
+except AttributeError:  # pragma: no cover
+    warnings.warn("Failed to find Pickler object in module {}. The module "
+                  "specified in LOKY_PICKLER should implement a Pickler "
+                  "object. Falling back to default builtin pickle."
+                  .format(LOKY_PICKLER))
+
+
+if Pickler is None:
+    from pickle import Pickler
+
+
+###############################################################################
+# Enable custom pickling in Loky.
+# To allow instance customization of the pickling process, we use 2 classes.
+# _LokyPickler gives module level customization and CustomizablePickler permits
+# to use instance base custom reducers.  Only CustomizablePickler should be
+# used.
+
+class _LokyPickler(Pickler):
+    """Pickler that uses custom reducers.
+
+    HIGHEST_PROTOCOL is selected by default as this pickler is used
+    to pickle ephemeral datastructures for interprocess communication
+    hence no backward compatibility is required.
+
+    """
+
+    # We override the pure Python pickler as its the only way to be able to
+    # customize the dispatch table without side effects in Python 2.6
+    # to 3.2. For Python 3.3+ leverage the new dispatch_table
+    # feature from http://bugs.python.org/issue14166 that makes it possible
+    # to use the C implementation of the Pickler which is faster.
+
+    if hasattr(Pickler, 'dispatch'):
+        # Make the dispatch registry an instance level attribute instead of
+        # a reference to the class dictionary under Python 2
+        dispatch = Pickler.dispatch.copy()
+    else:
+        # Under Python 3 initialize the dispatch table with a copy of the
+        # default registry
+        dispatch_table = copyreg.dispatch_table.copy()
+
+    @classmethod
+    def register(cls, type, reduce_func):
+        """Attach a reducer function to a given type in the dispatch table."""
+        if hasattr(Pickler, 'dispatch'):
+            # Python 2 pickler dispatching is not explicitly customizable.
+            # Let us use a closure to workaround this limitation.
+            def dispatcher(cls, obj):
+                reduced = reduce_func(obj)
+                cls.save_reduce(obj=obj, *reduced)
+            cls.dispatch[type] = dispatcher
+        else:
+            cls.dispatch_table[type] = reduce_func
+
+
+class CustomizableLokyPickler(Pickler):
+    def __init__(self, writer, reducers=None, protocol=HIGHEST_PROTOCOL):
+        Pickler.__init__(self, writer, protocol=protocol)
+        if reducers is None:
+            reducers = {}
+        if hasattr(Pickler, 'dispatch'):
+            # Make the dispatch registry an instance level attribute instead of
+            # a reference to the class dictionary under Python 2
+            self.dispatch = _LokyPickler.dispatch.copy()
+        else:
+            # Under Python 3 initialize the dispatch table with a copy of the
+            # default registry
+            self.dispatch_table = _LokyPickler.dispatch_table.copy()
+        for type, reduce_func in reducers.items():
+            self.register(type, reduce_func)
+
+    def register(self, type, reduce_func):
+        """Attach a reducer function to a given type in the dispatch table."""
+        if hasattr(Pickler, 'dispatch'):
+            # Python 2 pickler dispatching is not explicitly customizable.
+            # Let us use a closure to workaround this limitation.
+            def dispatcher(self, obj):
+                reduced = reduce_func(obj)
+                self.save_reduce(obj=obj, *reduced)
+            self.dispatch[type] = dispatcher
+        else:
+            self.dispatch_table[type] = reduce_func
+
+    @classmethod
+    def loads(self, buf):
+        if sys.version_info < (3, 3) and isinstance(buf, io.BytesIO):
+            buf = buf.getvalue()
+        return loads(buf)
+
+    @classmethod
+    def dumps(cls, obj, reducers=None, protocol=None):
+        buf = io.BytesIO()
+        p = cls(buf, reducers=reducers, protocol=protocol)
+        p.dump(obj)
+        if sys.version_info < (3, 3):
+            return buf.getvalue()
+        return buf.getbuffer()
+
+
+def dump(obj, file, reducers=None, protocol=None):
+    '''Replacement for pickle.dump() using LokyPickler.'''
+    CustomizableLokyPickler(file, reducers=reducers,
+                            protocol=protocol).dump(obj)
+
+
+###############################################################################
+# Registers extra pickling routines to improve picklization  for loky
+
+register = _LokyPickler.register
+
+
+# make methods picklable
+def _reduce_method(m):
+    if m.__self__ is None:
+        return getattr, (m.__class__, m.__func__.__name__)
+    else:
+        return getattr, (m.__self__, m.__func__.__name__)
+
+
+class _C:
+    def f(self):
+        pass
+
+    @classmethod
+    def h(cls):
+        pass
+
+
+register(type(_C().f), _reduce_method)
+register(type(_C.h), _reduce_method)
+
+
+def _reduce_method_descriptor(m):
+    return getattr, (m.__objclass__, m.__name__)
+
+
+register(type(list.append), _reduce_method_descriptor)
+register(type(int.__add__), _reduce_method_descriptor)
+
+
+# Make partial func pickable
+def _reduce_partial(p):
+    return _rebuild_partial, (p.func, p.args, p.keywords or {})
+
+
+def _rebuild_partial(func, args, keywords):
+    return functools.partial(func, *args, **keywords)
+
+
+register(functools.partial, _reduce_partial)
+
+if sys.platform != "win32":
+    from ._posix_reduction import _mk_inheritable  # noqa: F401
+else:
+    from . import _win_reduction  # noqa: F401
diff --git a/sklearn/externals/joblib/externals/loky/backend/semaphore_tracker.py b/sklearn/externals/joblib/externals/loky/backend/semaphore_tracker.py
new file mode 100755
index 0000000000..f49423713c
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/semaphore_tracker.py
@@ -0,0 +1,235 @@
+###############################################################################
+# Server process to keep track of unlinked semaphores and clean them.
+#
+# author: Thomas Moreau
+#
+# adapted from multiprocessing/semaphore_tracker.py  (17/02/2017)
+#  * include custom spawnv_passfds to start the process
+#  * use custom unlink from our own SemLock implementation
+#  * add some VERBOSE logging
+#
+
+#
+# On Unix we run a server process which keeps track of unlinked
+# semaphores. The server ignores SIGINT and SIGTERM and reads from a
+# pipe.  Every other process of the program has a copy of the writable
+# end of the pipe, so we get EOF when all other processes have exited.
+# Then the server process unlinks any remaining semaphore names.
+#
+# This is important because the system only supports a limited number
+# of named semaphores, and they will not be automatically removed till
+# the next reboot.  Without this semaphore tracker process, "killall
+# python" would probably leave unlinked semaphores.
+#
+
+import os
+import signal
+import sys
+import threading
+import warnings
+
+from . import spawn
+from multiprocessing import util
+
+try:
+    from _multiprocessing import sem_unlink
+except ImportError:
+    from .semlock import sem_unlink
+
+__all__ = ['ensure_running', 'register', 'unregister']
+
+VERBOSE = False
+
+
+class SemaphoreTracker(object):
+
+    def __init__(self):
+        self._lock = threading.Lock()
+        self._fd = None
+        self._pid = None
+
+    def getfd(self):
+        self.ensure_running()
+        return self._fd
+
+    def ensure_running(self):
+        '''Make sure that semaphore tracker process is running.
+
+        This can be run from any process.  Usually a child process will use
+        the semaphore created by its parent.'''
+        with self._lock:
+            if self._fd is not None:
+                # semaphore tracker was launched before, is it still running?
+                if self._check_alive():
+                    # => still alive
+                    return
+                # => dead, launch it again
+                os.close(self._fd)
+                self._fd = None
+                self._pid = None
+
+                warnings.warn('semaphore_tracker: process died unexpectedly, '
+                              'relaunching.  Some semaphores might leak.')
+
+            fds_to_pass = []
+            try:
+                fds_to_pass.append(sys.stderr.fileno())
+            except Exception:
+                pass
+
+            cmd = 'from {} import main; main(%d)'.format(main.__module__)
+            r, w = os.pipe()
+            try:
+                fds_to_pass.append(r)
+                # process will out live us, so no need to wait on pid
+                exe = spawn.get_executable()
+                args = [exe] + util._args_from_interpreter_flags()
+                # In python 3.3, there is a bug which put `-RRRRR..` instead of
+                # `-R` in args. Replace it to get the correct flags.
+                # See https://github.com/python/cpython/blob/3.3/Lib/subprocess.py#L488
+                if sys.version_info[:2] <= (3, 3):
+                    import re
+                    for i in range(1, len(args)):
+                        args[i] = re.sub("-R+", "-R", args[i])
+                args += ['-c', cmd % r]
+                util.debug("launching Semaphore tracker: {}".format(args))
+                pid = spawnv_passfds(exe, args, fds_to_pass)
+            except BaseException:
+                os.close(w)
+                raise
+            else:
+                self._fd = w
+                self._pid = pid
+            finally:
+                os.close(r)
+
+    def _check_alive(self):
+        '''Check for the existence of the semaphore tracker process.'''
+        try:
+            self._send('PROBE', '')
+        except BrokenPipeError:
+            return False
+        else:
+            return True
+
+    def register(self, name):
+        '''Register name of semaphore with semaphore tracker.'''
+        self.ensure_running()
+        self._send('REGISTER', name)
+
+    def unregister(self, name):
+        '''Unregister name of semaphore with semaphore tracker.'''
+        self.ensure_running()
+        self._send('UNREGISTER', name)
+
+    def _send(self, cmd, name):
+        msg = '{0}:{1}\n'.format(cmd, name).encode('ascii')
+        if len(name) > 512:
+            # posix guarantees that writes to a pipe of less than PIPE_BUF
+            # bytes are atomic, and that PIPE_BUF >= 512
+            raise ValueError('name too long')
+        nbytes = os.write(self._fd, msg)
+        assert nbytes == len(msg)
+
+
+_semaphore_tracker = SemaphoreTracker()
+ensure_running = _semaphore_tracker.ensure_running
+register = _semaphore_tracker.register
+unregister = _semaphore_tracker.unregister
+getfd = _semaphore_tracker.getfd
+
+
+def main(fd):
+    '''Run semaphore tracker.'''
+    # protect the process from ^C and "killall python" etc
+    signal.signal(signal.SIGINT, signal.SIG_IGN)
+    signal.signal(signal.SIGTERM, signal.SIG_IGN)
+
+    for f in (sys.stdin, sys.stdout):
+        try:
+            f.close()
+        except Exception:
+            pass
+
+    if VERBOSE:  # pragma: no cover
+        sys.stderr.write("Main semaphore tracker is running\n")
+        sys.stderr.flush()
+
+    cache = set()
+    try:
+        # keep track of registered/unregistered semaphores
+        with os.fdopen(fd, 'rb') as f:
+            for line in f:
+                try:
+                    cmd, name = line.strip().split(b':')
+                    if cmd == b'REGISTER':
+                        name = name.decode('ascii')
+                        cache.add(name)
+                        if VERBOSE:  # pragma: no cover
+                            sys.stderr.write("[SemaphoreTracker] register {}\n"
+                                             .format(name))
+                            sys.stderr.flush()
+                    elif cmd == b'UNREGISTER':
+                        name = name.decode('ascii')
+                        cache.remove(name)
+                        if VERBOSE:  # pragma: no cover
+                            sys.stderr.write("[SemaphoreTracker] unregister {}"
+                                             ": cache({})\n"
+                                             .format(name, len(cache)))
+                            sys.stderr.flush()
+                    elif cmd == b'PROBE':
+                        pass
+                    else:
+                        raise RuntimeError('unrecognized command %r' % cmd)
+                except BaseException:
+                    try:
+                        sys.excepthook(*sys.exc_info())
+                    except BaseException:
+                        pass
+    finally:
+        # all processes have terminated; cleanup any remaining semaphores
+        if cache:
+            try:
+                warnings.warn('semaphore_tracker: There appear to be %d '
+                              'leaked semaphores to clean up at shutdown' %
+                              len(cache))
+            except Exception:
+                pass
+        for name in cache:
+            # For some reason the process which created and registered this
+            # semaphore has failed to unregister it. Presumably it has died.
+            # We therefore unlink it.
+            try:
+                try:
+                    sem_unlink(name)
+                    if VERBOSE:  # pragma: no cover
+                        sys.stderr.write("[SemaphoreTracker] unlink {}\n"
+                                         .format(name))
+                        sys.stderr.flush()
+                except Exception as e:
+                    warnings.warn('semaphore_tracker: %r: %r' % (name, e))
+            finally:
+                pass
+
+    if VERBOSE:  # pragma: no cover
+        sys.stderr.write("semaphore tracker shut down\n")
+        sys.stderr.flush()
+
+
+#
+# Start a program with only specified fds kept open
+#
+
+def spawnv_passfds(path, args, passfds):
+    passfds = sorted(passfds)
+    errpipe_read, errpipe_write = os.pipe()
+    try:
+        from .reduction import _mk_inheritable
+        _pass = []
+        for fd in passfds:
+            _pass += [_mk_inheritable(fd)]
+        from .fork_exec import fork_exec
+        return fork_exec(args, _pass)
+    finally:
+        os.close(errpipe_read)
+        os.close(errpipe_write)
diff --git a/sklearn/externals/joblib/externals/loky/backend/semlock.py b/sklearn/externals/joblib/externals/loky/backend/semlock.py
new file mode 100755
index 0000000000..2d35f6a271
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/semlock.py
@@ -0,0 +1,274 @@
+###############################################################################
+# Ctypes implementation for posix semaphore.
+#
+# author: Thomas Moreau and Olivier Grisel
+#
+# adapted from cpython/Modules/_multiprocessing/semaphore.c (17/02/2017)
+#  * use ctypes to access pthread semaphores and provide a full python
+#    semaphore management.
+#  * For OSX, as no sem_getvalue is not implemented, Semaphore with value > 1
+#    are not guaranteed to work.
+#  * Only work with LokyProcess on posix
+#
+import os
+import sys
+import time
+import errno
+import ctypes
+import tempfile
+import threading
+from ctypes.util import find_library
+
+# As we need to use ctypes return types for semlock object, failure value
+# needs to be cast to proper python value. Unix failure convention is to
+# return 0, whereas OSX returns -1
+SEM_FAILURE = ctypes.c_void_p(0).value
+if sys.platform == 'darwin':
+    SEM_FAILURE = ctypes.c_void_p(-1).value
+
+# Semaphore types
+RECURSIVE_MUTEX = 0
+SEMAPHORE = 1
+
+# Semaphore constants
+SEM_OFLAG = ctypes.c_int(os.O_CREAT | os.O_EXCL)
+SEM_PERM = ctypes.c_int(384)
+
+
+class timespec(ctypes.Structure):
+    _fields_ = [("tv_sec", ctypes.c_long), ("tv_nsec", ctypes.c_long)]
+
+
+if sys.platform != 'win32':
+    pthread = ctypes.CDLL(find_library('pthread'), use_errno=True)
+    pthread.sem_open.restype = ctypes.c_void_p
+    pthread.sem_close.argtypes = [ctypes.c_void_p]
+    pthread.sem_wait.argtypes = [ctypes.c_void_p]
+    pthread.sem_trywait.argtypes = [ctypes.c_void_p]
+    pthread.sem_post.argtypes = [ctypes.c_void_p]
+    pthread.sem_getvalue.argtypes = [ctypes.c_void_p, ctypes.c_void_p]
+    pthread.sem_unlink.argtypes = [ctypes.c_char_p]
+    if sys.platform != "darwin":
+        pthread.sem_timedwait.argtypes = [ctypes.c_void_p,
+                                          ctypes.POINTER(timespec)]
+
+try:
+    from threading import get_ident
+except ImportError:
+    def get_ident():
+        return threading.current_thread().ident
+
+
+if sys.version_info[:2] < (3, 3):
+    class FileExistsError(OSError):
+        pass
+
+    class FileNotFoundError(OSError):
+        pass
+
+
+def sem_unlink(name):
+    if pthread.sem_unlink(name.encode('ascii')) < 0:
+        raiseFromErrno()
+
+
+def _sem_open(name, value=None):
+    """ Construct or retrieve a semaphore with the given name
+
+    If value is None, try to retrieve an existing named semaphore.
+    Else create a new semaphore with the given value
+    """
+    if value is None:
+        handle = pthread.sem_open(ctypes.c_char_p(name), 0)
+    else:
+        handle = pthread.sem_open(ctypes.c_char_p(name), SEM_OFLAG, SEM_PERM,
+                                  ctypes.c_int(value))
+
+    if handle == SEM_FAILURE:
+        e = ctypes.get_errno()
+        if e == errno.EEXIST:
+            raise FileExistsError("a semaphore named %s already exists" % name)
+        elif e == errno.ENOENT:
+            raise FileNotFoundError('cannot find semaphore named %s' % name)
+        elif e == errno.ENOSYS:
+            raise NotImplementedError('No semaphore implementation on this '
+                                      'system')
+        else:
+            raiseFromErrno()
+
+    return handle
+
+
+def _sem_timedwait(handle, timeout):
+    t_start = time.time()
+    if sys.platform != "darwin":
+        sec = int(timeout)
+        tv_sec = int(t_start)
+        nsec = int(1e9 * (timeout - sec) + .5)
+        tv_nsec = int(1e9 * (t_start - tv_sec) + .5)
+        deadline = timespec(sec+tv_sec, nsec+tv_nsec)
+        deadline.tv_sec += int(deadline.tv_nsec / 1000000000)
+        deadline.tv_nsec %= 1000000000
+        return pthread.sem_timedwait(handle, ctypes.pointer(deadline))
+
+    # PERFORMANCE WARNING
+    # No sem_timedwait on OSX so we implement our own method. This method can
+    # degrade performances has the wait can have a latency up to 20 msecs
+    deadline = t_start + timeout
+    delay = 0
+    now = time.time()
+    while True:
+        # Poll the sem file
+        res = pthread.sem_trywait(handle)
+        if res == 0:
+            return 0
+        else:
+            e = ctypes.get_errno()
+            if e != errno.EAGAIN:
+                raiseFromErrno()
+
+        # check for timeout
+        now = time.time()
+        if now > deadline:
+            ctypes.set_errno(errno.ETIMEDOUT)
+            return -1
+
+        # calculate how much time left and check the delay is not too long
+        # -- maximum is 20 msecs
+        difference = (deadline - now)
+        delay = min(delay, 20e-3, difference)
+
+        # Sleep and increase delay
+        time.sleep(delay)
+        delay += 1e-3
+
+
+class SemLock(object):
+    """ctypes wrapper to the unix semaphore"""
+
+    _rand = tempfile._RandomNameSequence()
+
+    def __init__(self, kind, value, maxvalue, name=None, unlink_now=False):
+        self.count = 0
+        self.ident = 0
+        self.kind = kind
+        self.maxvalue = maxvalue
+        self.name = name
+        self.handle = _sem_open(self.name.encode('ascii'), value)
+
+    def __del__(self):
+        try:
+            res = pthread.sem_close(self.handle)
+            assert res == 0, "Issue while closing semaphores"
+        except AttributeError:
+            pass
+
+    def _is_mine(self):
+        return self.count > 0 and get_ident() == self.ident
+
+    def acquire(self, block=True, timeout=None):
+        if self.kind == RECURSIVE_MUTEX and self._is_mine():
+            self.count += 1
+            return True
+
+        if block and timeout is None:
+            res = pthread.sem_wait(self.handle)
+        elif not block or timeout <= 0:
+            res = pthread.sem_trywait(self.handle)
+        else:
+            res = _sem_timedwait(self.handle, timeout)
+        if res < 0:
+            e = ctypes.get_errno()
+            if e == errno.EINTR:
+                return None
+            elif e in [errno.EAGAIN, errno.ETIMEDOUT]:
+                return False
+            raiseFromErrno()
+        self.count += 1
+        self.ident = get_ident()
+        return True
+
+    def release(self):
+        if self.kind == RECURSIVE_MUTEX:
+            assert self._is_mine(), (
+                "attempt to release recursive lock not owned by thread")
+            if self.count > 1:
+                self.count -= 1
+                return
+            assert self.count == 1
+        else:
+            if sys.platform == 'darwin':
+                # Handle broken get_value for mac ==> only Lock will work
+                # as sem_get_value do not work properly
+                if self.maxvalue == 1:
+                    if pthread.sem_trywait(self.handle) < 0:
+                        e = ctypes.get_errno()
+                        if e != errno.EAGAIN:
+                            raise OSError(e, errno.errorcode[e])
+                    else:
+                        if pthread.sem_post(self.handle) < 0:
+                            raiseFromErrno()
+                        else:
+                            raise ValueError(
+                                "semaphore or lock released too many times")
+                else:
+                    import warnings
+                    warnings.warn("semaphore are broken on OSX, release might "
+                                  "increase its maximal value", RuntimeWarning)
+            else:
+                value = self._get_value()
+                if value >= self.maxvalue:
+                    raise ValueError(
+                        "semaphore or lock released too many times")
+
+        if pthread.sem_post(self.handle) < 0:
+            raiseFromErrno()
+
+        self.count -= 1
+
+    def _get_value(self):
+        value = ctypes.pointer(ctypes.c_int(-1))
+        if pthread.sem_getvalue(self.handle, value) < 0:
+            raiseFromErrno()
+        return value.contents.value
+
+    def _count(self):
+        return self.count
+
+    def _is_zero(self):
+        if sys.platform == 'darwin':
+            # Handle broken get_value for mac ==> only Lock will work
+            # as sem_get_value do not work properly
+            if pthread.sem_trywait(self.handle) < 0:
+                e = ctypes.get_errno()
+                if e == errno.EAGAIN:
+                    return True
+                raise OSError(e, errno.errorcode[e])
+            else:
+                if pthread.sem_post(self.handle) < 0:
+                    raiseFromErrno()
+                return False
+        else:
+            value = ctypes.pointer(ctypes.c_int(-1))
+            if pthread.sem_getvalue(self.handle, value) < 0:
+                raiseFromErrno()
+            return value.contents.value == 0
+
+    def _after_fork(self):
+        self.count = 0
+
+    @staticmethod
+    def _rebuild(handle, kind, maxvalue, name):
+        self = SemLock.__new__(SemLock)
+        self.count = 0
+        self.ident = 0
+        self.kind = kind
+        self.maxvalue = maxvalue
+        self.name = name
+        self.handle = _sem_open(name.encode('ascii'))
+        return self
+
+
+def raiseFromErrno():
+    e = ctypes.get_errno()
+    raise OSError(e, errno.errorcode[e])
diff --git a/sklearn/externals/joblib/externals/loky/backend/spawn.py b/sklearn/externals/joblib/externals/loky/backend/spawn.py
new file mode 100755
index 0000000000..a7e57b884c
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/spawn.py
@@ -0,0 +1,240 @@
+###############################################################################
+# Prepares and processes the data to setup the new process environment
+#
+# author: Thomas Moreau and Olivier Grisel
+#
+# adapted from multiprocessing/spawn.py (17/02/2017)
+#  * Improve logging data
+#
+import os
+import sys
+import runpy
+import types
+import multiprocessing as mp
+from multiprocessing import process, util
+
+
+if sys.platform != 'win32':
+    WINEXE = False
+    WINSERVICE = False
+else:
+    WINEXE = (sys.platform == 'win32' and getattr(sys, 'frozen', False))
+    WINSERVICE = sys.executable.lower().endswith("pythonservice.exe")
+
+if WINSERVICE:
+    _python_exe = os.path.join(sys.exec_prefix, 'python.exe')
+else:
+    _python_exe = sys.executable
+
+if sys.version_info[:2] < (3, 4):
+    def get_command_line(pipe_handle, **kwds):
+        '''
+        Returns prefix of command line used for spawning a child process
+        '''
+        if getattr(sys, 'frozen', False):
+            return ([sys.executable, '--multiprocessing-fork', pipe_handle])
+        else:
+            prog = 'from multiprocessing.forking import main; main()'
+            opts = util._args_from_interpreter_flags()
+            return [_python_exe] + opts + [
+                '-c', prog, '--multiprocessing-fork', pipe_handle]
+else:
+    from multiprocessing.spawn import get_command_line
+
+
+def get_executable():
+    return _python_exe
+
+
+def _check_not_importing_main():
+    if getattr(process.current_process(), '_inheriting', False):
+        raise RuntimeError('''
+        An attempt has been made to start a new process before the
+        current process has finished its bootstrapping phase.
+
+        This probably means that you are not using fork to start your
+        child processes and you have forgotten to use the proper idiom
+        in the main module:
+
+            if __name__ == '__main__':
+                freeze_support()
+                ...
+
+        The "freeze_support()" line can be omitted if the program
+        is not going to be frozen to produce an executable.''')
+
+
+def get_preparation_data(name, init_main_module=True):
+    '''
+    Return info about parent needed by child to unpickle process object
+    '''
+    _check_not_importing_main()
+    d = dict(
+        log_to_stderr=util._log_to_stderr,
+        authkey=bytes(process.current_process().authkey),
+    )
+
+    if util._logger is not None:
+        d['log_level'] = util._logger.getEffectiveLevel()
+        if len(util._logger.handlers) > 0:
+            h = util._logger.handlers[0]
+            d['log_fmt'] = h.formatter._fmt
+
+    sys_path = [p for p in sys.path]
+    try:
+        i = sys_path.index('')
+    except ValueError:
+        pass
+    else:
+        sys_path[i] = process.ORIGINAL_DIR
+
+    d.update(
+        name=name,
+        sys_path=sys_path,
+        sys_argv=sys.argv,
+        orig_dir=process.ORIGINAL_DIR,
+        dir=os.getcwd()
+    )
+
+    if sys.platform != "win32":
+        # Pass the semaphore_tracker pid to avoid re-spawning it in every child
+        from . import semaphore_tracker
+        semaphore_tracker.ensure_running()
+        d['tracker_pid'] = semaphore_tracker._semaphore_tracker._pid
+
+    # Figure out whether to initialise main in the subprocess as a module
+    # or through direct execution (or to leave it alone entirely)
+    if init_main_module:
+        main_module = sys.modules['__main__']
+        try:
+            main_mod_name = getattr(main_module.__spec__, "name", None)
+        except:
+            main_mod_name = None
+        if main_mod_name is not None:
+            d['init_main_from_name'] = main_mod_name
+        elif sys.platform != 'win32' or (not WINEXE and not WINSERVICE):
+            main_path = getattr(main_module, '__file__', None)
+            if main_path is not None:
+                if (not os.path.isabs(main_path) and
+                        process.ORIGINAL_DIR is not None):
+                    main_path = os.path.join(process.ORIGINAL_DIR, main_path)
+                d['init_main_from_path'] = os.path.normpath(main_path)
+                # Compat for python2.7
+                d['main_path'] = d['init_main_from_path']
+
+    return d
+
+
+#
+# Prepare current process
+#
+old_main_modules = []
+
+
+def prepare(data):
+    '''
+    Try to get current process ready to unpickle process object
+    '''
+    if 'name' in data:
+        process.current_process().name = data['name']
+
+    if 'authkey' in data:
+        process.current_process().authkey = data['authkey']
+
+    if 'log_to_stderr' in data and data['log_to_stderr']:
+        util.log_to_stderr()
+
+    if 'log_level' in data:
+        util.get_logger().setLevel(data['log_level'])
+
+    if 'log_fmt' in data:
+        import logging
+        util.get_logger().handlers[0].setFormatter(
+            logging.Formatter(data['log_fmt'])
+        )
+
+    if 'sys_path' in data:
+        sys.path = data['sys_path']
+
+    if 'sys_argv' in data:
+        sys.argv = data['sys_argv']
+
+    if 'dir' in data:
+        os.chdir(data['dir'])
+
+    if 'orig_dir' in data:
+        process.ORIGINAL_DIR = data['orig_dir']
+
+    if hasattr(mp, 'set_start_method'):
+        mp.set_start_method('loky', force=True)
+
+    if 'tacker_pid' in data:
+        from . import semaphore_tracker
+        semaphore_tracker._semaphore_tracker._pid = data["tracker_pid"]
+
+    if 'init_main_from_name' in data:
+        _fixup_main_from_name(data['init_main_from_name'])
+    elif 'init_main_from_path' in data:
+        _fixup_main_from_path(data['init_main_from_path'])
+
+
+# Multiprocessing module helpers to fix up the main module in
+# spawned subprocesses
+def _fixup_main_from_name(mod_name):
+    # __main__.py files for packages, directories, zip archives, etc, run
+    # their "main only" code unconditionally, so we don't even try to
+    # populate anything in __main__, nor do we make any changes to
+    # __main__ attributes
+    current_main = sys.modules['__main__']
+    if mod_name == "__main__" or mod_name.endswith(".__main__"):
+        return
+
+    # If this process was forked, __main__ may already be populated
+    if getattr(current_main.__spec__, "name", None) == mod_name:
+        return
+
+    # Otherwise, __main__ may contain some non-main code where we need to
+    # support unpickling it properly. We rerun it as __mp_main__ and make
+    # the normal __main__ an alias to that
+    old_main_modules.append(current_main)
+    main_module = types.ModuleType("__mp_main__")
+    main_content = runpy.run_module(mod_name,
+                                    run_name="__mp_main__",
+                                    alter_sys=True)
+    main_module.__dict__.update(main_content)
+    sys.modules['__main__'] = sys.modules['__mp_main__'] = main_module
+
+
+def _fixup_main_from_path(main_path):
+    # If this process was forked, __main__ may already be populated
+    current_main = sys.modules['__main__']
+
+    # Unfortunately, the main ipython launch script historically had no
+    # "if __name__ == '__main__'" guard, so we work around that
+    # by treating it like a __main__.py file
+    # See https://github.com/ipython/ipython/issues/4698
+    main_name = os.path.splitext(os.path.basename(main_path))[0]
+    if main_name == 'ipython':
+        return
+
+    # Otherwise, if __file__ already has the setting we expect,
+    # there's nothing more to do
+    if getattr(current_main, '__file__', None) == main_path:
+        return
+
+    # If the parent process has sent a path through rather than a module
+    # name we assume it is an executable script that may contain
+    # non-main code that needs to be executed
+    old_main_modules.append(current_main)
+    main_module = types.ModuleType("__mp_main__")
+    main_content = runpy.run_path(main_path,
+                                  run_name="__mp_main__")
+    main_module.__dict__.update(main_content)
+    sys.modules['__main__'] = sys.modules['__mp_main__'] = main_module
+
+
+def import_main_path(main_path):
+    '''
+    Set sys.modules['__main__'] to module at main_path
+    '''
+    _fixup_main_from_path(main_path)
diff --git a/sklearn/externals/joblib/externals/loky/backend/synchronize.py b/sklearn/externals/joblib/externals/loky/backend/synchronize.py
new file mode 100755
index 0000000000..4773b9dc87
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/synchronize.py
@@ -0,0 +1,381 @@
+###############################################################################
+# Synchronization primitives based on our SemLock implementation
+#
+# author: Thomas Moreau and Olivier Grisel
+#
+# adapted from multiprocessing/synchronize.py (17/02/2017)
+#  * Remove ctx argument for compatibility reason
+#  * Implementation of Condition/Event are necessary for compatibility
+#    with python2.7/3.3, Barrier should be reimplemented to for those
+#    version (but it is not used in loky).
+#
+
+import os
+import sys
+import tempfile
+import threading
+import _multiprocessing
+from time import time as _time
+
+from .context import assert_spawning
+from . import semaphore_tracker
+from multiprocessing import process
+from multiprocessing import util
+
+__all__ = [
+    'Lock', 'RLock', 'Semaphore', 'BoundedSemaphore', 'Condition', 'Event'
+    ]
+# Try to import the mp.synchronize module cleanly, if it fails
+# raise ImportError for platforms lacking a working sem_open implementation.
+# See issue 3770
+try:
+    if sys.version_info < (3, 4):
+        from .semlock import SemLock as _SemLock
+        from .semlock import sem_unlink
+    else:
+        from _multiprocessing import SemLock as _SemLock
+        from _multiprocessing import sem_unlink
+except (ImportError):
+    raise ImportError("This platform lacks a functioning sem_open" +
+                      " implementation, therefore, the required" +
+                      " synchronization primitives needed will not" +
+                      " function, see issue 3770.")
+
+if sys.version_info[:2] < (3, 3):
+    FileExistsError = OSError
+
+#
+# Constants
+#
+
+RECURSIVE_MUTEX, SEMAPHORE = list(range(2))
+SEM_VALUE_MAX = _multiprocessing.SemLock.SEM_VALUE_MAX
+
+
+#
+# Base class for semaphores and mutexes; wraps `_multiprocessing.SemLock`
+#
+
+class SemLock(object):
+
+    _rand = tempfile._RandomNameSequence()
+
+    def __init__(self, kind, value, maxvalue):
+        # unlink_now is only used on win32 or when we are using fork.
+        unlink_now = False
+        for i in range(100):
+            try:
+                self._semlock = _SemLock(
+                    kind, value, maxvalue, SemLock._make_name(),
+                    unlink_now)
+            except FileExistsError:  # pragma: no cover
+                pass
+            else:
+                break
+        else:  # pragma: no cover
+            raise FileExistsError('cannot find name for semaphore')
+
+        util.debug('created semlock with handle %s and name "%s"'
+                   % (self._semlock.handle, self._semlock.name))
+
+        self._make_methods()
+
+        def _after_fork(obj):
+            obj._semlock._after_fork()
+
+        util.register_after_fork(self, _after_fork)
+
+        # When the object is garbage collected or the
+        # process shuts down we unlink the semaphore name
+        semaphore_tracker.register(self._semlock.name)
+        util.Finalize(self, SemLock._cleanup, (self._semlock.name,),
+                      exitpriority=0)
+
+    @staticmethod
+    def _cleanup(name):
+        sem_unlink(name)
+        semaphore_tracker.unregister(name)
+
+    def _make_methods(self):
+        self.acquire = self._semlock.acquire
+        self.release = self._semlock.release
+
+    def __enter__(self):
+        return self._semlock.acquire()
+
+    def __exit__(self, *args):
+        return self._semlock.release()
+
+    def __getstate__(self):
+        assert_spawning(self)
+        sl = self._semlock
+        h = sl.handle
+        return (h, sl.kind, sl.maxvalue, sl.name)
+
+    def __setstate__(self, state):
+        self._semlock = _SemLock._rebuild(*state)
+        util.debug('recreated blocker with handle %r and name "%s"'
+                   % (state[0], state[3]))
+        self._make_methods()
+
+    @staticmethod
+    def _make_name():
+        # OSX does not support long names for semaphores
+        return '/loky-%i-%s' % (os.getpid(), next(SemLock._rand))
+
+
+#
+# Semaphore
+#
+
+class Semaphore(SemLock):
+
+    def __init__(self, value=1):
+        SemLock.__init__(self, SEMAPHORE, value, SEM_VALUE_MAX)
+
+    def get_value(self):
+        if sys.platform == 'darwin':
+            raise NotImplementedError("OSX does not implement sem_getvalue")
+        return self._semlock._get_value()
+
+    def __repr__(self):
+        try:
+            value = self._semlock._get_value()
+        except Exception:
+            value = 'unknown'
+        return '<%s(value=%s)>' % (self.__class__.__name__, value)
+
+
+#
+# Bounded semaphore
+#
+
+class BoundedSemaphore(Semaphore):
+
+    def __init__(self, value=1):
+        SemLock.__init__(self, SEMAPHORE, value, value)
+
+    def __repr__(self):
+        try:
+            value = self._semlock._get_value()
+        except Exception:
+            value = 'unknown'
+        return '<%s(value=%s, maxvalue=%s)>' % \
+               (self.__class__.__name__, value, self._semlock.maxvalue)
+
+
+#
+# Non-recursive lock
+#
+
+class Lock(SemLock):
+
+    def __init__(self):
+        super(Lock, self).__init__(SEMAPHORE, 1, 1)
+
+    def __repr__(self):
+        try:
+            if self._semlock._is_mine():
+                name = process.current_process().name
+                if threading.current_thread().name != 'MainThread':
+                    name += '|' + threading.current_thread().name
+            elif self._semlock._get_value() == 1:
+                name = 'None'
+            elif self._semlock._count() > 0:
+                name = 'SomeOtherThread'
+            else:
+                name = 'SomeOtherProcess'
+        except Exception:
+            name = 'unknown'
+        return '<%s(owner=%s)>' % (self.__class__.__name__, name)
+
+
+#
+# Recursive lock
+#
+
+class RLock(SemLock):
+
+    def __init__(self):
+        super(RLock, self).__init__(RECURSIVE_MUTEX, 1, 1)
+
+    def __repr__(self):
+        try:
+            if self._semlock._is_mine():
+                name = process.current_process().name
+                if threading.current_thread().name != 'MainThread':
+                    name += '|' + threading.current_thread().name
+                count = self._semlock._count()
+            elif self._semlock._get_value() == 1:
+                name, count = 'None', 0
+            elif self._semlock._count() > 0:
+                name, count = 'SomeOtherThread', 'nonzero'
+            else:
+                name, count = 'SomeOtherProcess', 'nonzero'
+        except Exception:
+            name, count = 'unknown', 'unknown'
+        return '<%s(%s, %s)>' % (self.__class__.__name__, name, count)
+
+
+#
+# Condition variable
+#
+
+class Condition(object):
+
+    def __init__(self, lock=None):
+        self._lock = lock or RLock()
+        self._sleeping_count = Semaphore(0)
+        self._woken_count = Semaphore(0)
+        self._wait_semaphore = Semaphore(0)
+        self._make_methods()
+
+    def __getstate__(self):
+        assert_spawning(self)
+        return (self._lock, self._sleeping_count,
+                self._woken_count, self._wait_semaphore)
+
+    def __setstate__(self, state):
+        (self._lock, self._sleeping_count,
+         self._woken_count, self._wait_semaphore) = state
+        self._make_methods()
+
+    def __enter__(self):
+        return self._lock.__enter__()
+
+    def __exit__(self, *args):
+        return self._lock.__exit__(*args)
+
+    def _make_methods(self):
+        self.acquire = self._lock.acquire
+        self.release = self._lock.release
+
+    def __repr__(self):
+        try:
+            num_waiters = (self._sleeping_count._semlock._get_value() -
+                           self._woken_count._semlock._get_value())
+        except Exception:
+            num_waiters = 'unknown'
+        return '<%s(%s, %s)>' % (self.__class__.__name__,
+                                 self._lock, num_waiters)
+
+    def wait(self, timeout=None):
+        assert self._lock._semlock._is_mine(), \
+               'must acquire() condition before using wait()'
+
+        # indicate that this thread is going to sleep
+        self._sleeping_count.release()
+
+        # release lock
+        count = self._lock._semlock._count()
+        for i in range(count):
+            self._lock.release()
+
+        try:
+            # wait for notification or timeout
+            return self._wait_semaphore.acquire(True, timeout)
+        finally:
+            # indicate that this thread has woken
+            self._woken_count.release()
+
+            # reacquire lock
+            for i in range(count):
+                self._lock.acquire()
+
+    def notify(self):
+        assert self._lock._semlock._is_mine(), 'lock is not owned'
+        assert not self._wait_semaphore.acquire(False)
+
+        # to take account of timeouts since last notify() we subtract
+        # woken_count from sleeping_count and rezero woken_count
+        while self._woken_count.acquire(False):
+            res = self._sleeping_count.acquire(False)
+            assert res
+
+        if self._sleeping_count.acquire(False):  # try grabbing a sleeper
+            self._wait_semaphore.release()       # wake up one sleeper
+            self._woken_count.acquire()          # wait for the sleeper to wake
+
+            # rezero _wait_semaphore in case a timeout just happened
+            self._wait_semaphore.acquire(False)
+
+    def notify_all(self):
+        assert self._lock._semlock._is_mine(), 'lock is not owned'
+        assert not self._wait_semaphore.acquire(False)
+
+        # to take account of timeouts since last notify*() we subtract
+        # woken_count from sleeping_count and rezero woken_count
+        while self._woken_count.acquire(False):
+            res = self._sleeping_count.acquire(False)
+            assert res
+
+        sleepers = 0
+        while self._sleeping_count.acquire(False):
+            self._wait_semaphore.release()        # wake up one sleeper
+            sleepers += 1
+
+        if sleepers:
+            for i in range(sleepers):
+                self._woken_count.acquire()       # wait for a sleeper to wake
+
+            # rezero wait_semaphore in case some timeouts just happened
+            while self._wait_semaphore.acquire(False):
+                pass
+
+    def wait_for(self, predicate, timeout=None):
+        result = predicate()
+        if result:
+            return result
+        if timeout is not None:
+            endtime = _time() + timeout
+        else:
+            endtime = None
+            waittime = None
+        while not result:
+            if endtime is not None:
+                waittime = endtime - _time()
+                if waittime <= 0:
+                    break
+            self.wait(waittime)
+            result = predicate()
+        return result
+
+
+#
+# Event
+#
+
+class Event(object):
+
+    def __init__(self):
+        self._cond = Condition(Lock())
+        self._flag = Semaphore(0)
+
+    def is_set(self):
+        with self._cond:
+            if self._flag.acquire(False):
+                self._flag.release()
+                return True
+            return False
+
+    def set(self):
+        with self._cond:
+            self._flag.acquire(False)
+            self._flag.release()
+            self._cond.notify_all()
+
+    def clear(self):
+        with self._cond:
+            self._flag.acquire(False)
+
+    def wait(self, timeout=None):
+        with self._cond:
+            if self._flag.acquire(False):
+                self._flag.release()
+            else:
+                self._cond.wait(timeout)
+
+            if self._flag.acquire(False):
+                self._flag.release()
+                return True
+            return False
diff --git a/sklearn/externals/joblib/externals/loky/backend/utils.py b/sklearn/externals/joblib/externals/loky/backend/utils.py
new file mode 100755
index 0000000000..d54098a816
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/backend/utils.py
@@ -0,0 +1,114 @@
+import os
+import sys
+import errno
+import signal
+import warnings
+import threading
+import subprocess
+try:
+    import psutil
+except ImportError:
+    psutil = None
+
+
+def _flag_current_thread_clean_exit():
+    """Put a ``_clean_exit`` flag on the current thread"""
+    thread = threading.current_thread()
+    thread._clean_exit = True
+
+
+def recursive_terminate(process, use_psutil=True):
+    if use_psutil and psutil is not None:
+        _recursive_terminate_with_psutil(process)
+    else:
+        _recursive_terminate_without_psutil(process)
+
+
+def _recursive_terminate_with_psutil(process, retries=5):
+    try:
+        children = psutil.Process(process.pid).children(recursive=True)
+    except psutil.NoSuchProcess:
+        return
+
+    for child in children:
+        try:
+            child.terminate()
+        except psutil.NoSuchProcess:
+            pass
+
+    gone, still_alive = psutil.wait_procs(children, timeout=5)
+    for child_process in still_alive:
+        child_process.kill()
+
+    process.terminate()
+    process.join()
+
+
+def _recursive_terminate_without_psutil(process):
+    """Terminate a process and its descendants.
+    """
+    try:
+        _recursive_terminate(process.pid)
+    except OSError as e:
+        warnings.warn("Failed to kill subprocesses on this platform. Please"
+                      "install psutil: https://github.com/giampaolo/psutil")
+        # In case we cannot introspect the children, we fall back to the
+        # classic Process.terminate.
+        process.terminate()
+    process.join()
+
+
+def _recursive_terminate(pid):
+    """Recursively kill the descendants of a process before killing it.
+    """
+
+    if sys.platform == "win32":
+        # On windows, the taskkill function with option `/T` terminate a given
+        # process pid and its children.
+        try:
+            subprocess.check_output(
+                ["taskkill", "/F", "/T", "/PID", str(pid)],
+                stderr=None)
+        except subprocess.CalledProcessError as e:
+            # In windows, taskkill return 1 for permission denied and 128, 255
+            # for no process found.
+            if e.returncode not in [1, 128, 255]:
+                raise
+            elif e.returncode == 1:
+                # Try to kill the process without its descendants if taskkill
+                # was denied permission. If this fails too, with an error
+                # different from process not found, let the top level function
+                # raise a warning and retry to kill the process.
+                try:
+                    os.kill(pid, signal.SIGTERM)
+                except OSError as e:
+                    if e.errno != errno.ESRCH:
+                        raise
+
+    else:
+        try:
+            children_pids = subprocess.check_output(
+                ["pgrep", "-P", str(pid)],
+                stderr=None
+            )
+        except subprocess.CalledProcessError as e:
+            # `ps` returns 1 when no child process has been found
+            if e.returncode == 1:
+                children_pids = b''
+            else:
+                raise
+
+        # Decode the result, split the cpid and remove the trailing line
+        children_pids = children_pids.decode().split('\n')[:-1]
+        for cpid in children_pids:
+            cpid = int(cpid)
+            _recursive_terminate(cpid)
+
+        try:
+            os.kill(pid, signal.SIGTERM)
+        except OSError as e:
+            # if OSError is raised with [Errno 3] no such process, the process
+            # is already terminated, else, raise the error and let the top
+            # level function raise a warning and retry to kill the process.
+            if e.errno != errno.ESRCH:
+                raise
diff --git a/sklearn/externals/joblib/externals/loky/process_executor.py b/sklearn/externals/joblib/externals/loky/process_executor.py
new file mode 100755
index 0000000000..6868f52706
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/process_executor.py
@@ -0,0 +1,1084 @@
+###############################################################################
+# Re-implementation of the ProcessPoolExecutor more robust to faults
+#
+# author: Thomas Moreau and Olivier Grisel
+#
+# adapted from concurrent/futures/process_pool_executor.py (17/02/2017)
+#  * Backport for python2.7/3.3,
+#  * Add an extra management thread to detect queue_management_thread failures,
+#  * Improve the shutdown process to avoid deadlocks,
+#  * Add timeout for workers,
+#  * More robust pickling process.
+#
+# Copyright 2009 Brian Quinlan. All Rights Reserved.
+# Licensed to PSF under a Contributor Agreement.
+
+"""Implements ProcessPoolExecutor.
+
+The follow diagram and text describe the data-flow through the system:
+
+|======================= In-process =====================|== Out-of-process ==|
+
++----------+     +----------+       +--------+     +-----------+    +---------+
+|          |  => | Work Ids |       |        |     | Call Q    |    | Process |
+|          |     +----------+       |        |     +-----------+    |  Pool   |
+|          |     | ...      |       |        |     | ...       |    +---------+
+|          |     | 6        |    => |        |  => | 5, call() | => |         |
+|          |     | 7        |       |        |     | ...       |    |         |
+| Process  |     | ...      |       | Local  |     +-----------+    | Process |
+|  Pool    |     +----------+       | Worker |                      |  #1..n  |
+| Executor |                        | Thread |                      |         |
+|          |     +----------- +     |        |     +-----------+    |         |
+|          | <=> | Work Items | <=> |        | <=  | Result Q  | <= |         |
+|          |     +------------+     |        |     +-----------+    |         |
+|          |     | 6: call()  |     |        |     | ...       |    |         |
+|          |     |    future  |     +--------+     | 4, result |    |         |
+|          |     | ...        |                    | 3, except |    |         |
++----------+     +------------+                    +-----------+    +---------+
+
+Executor.submit() called:
+- creates a uniquely numbered _WorkItem and adds it to the "Work Items" dict
+- adds the id of the _WorkItem to the "Work Ids" queue
+
+Local worker thread:
+- reads work ids from the "Work Ids" queue and looks up the corresponding
+  WorkItem from the "Work Items" dict: if the work item has been cancelled then
+  it is simply removed from the dict, otherwise it is repackaged as a
+  _CallItem and put in the "Call Q". New _CallItems are put in the "Call Q"
+  until "Call Q" is full. NOTE: the size of the "Call Q" is kept small because
+  calls placed in the "Call Q" can no longer be cancelled with Future.cancel().
+- reads _ResultItems from "Result Q", updates the future stored in the
+  "Work Items" dict and deletes the dict entry
+
+Process #1..n:
+- reads _CallItems from "Call Q", executes the calls, and puts the resulting
+  _ResultItems in "Result Q"
+"""
+
+
+__author__ = 'Thomas Moreau (thomas.moreau.2010@gmail.com)'
+
+
+import os
+import sys
+import types
+import weakref
+import warnings
+import itertools
+import traceback
+import threading
+import multiprocessing as mp
+from functools import partial
+from pickle import PicklingError
+from time import time
+import gc
+
+from . import _base
+from .backend import get_context
+from .backend.compat import queue
+from .backend.compat import wait
+from .backend.context import cpu_count
+from .backend.queues import Queue, SimpleQueue, Full
+from .backend.utils import recursive_terminate
+
+try:
+    from concurrent.futures.process import BrokenProcessPool as _BPPException
+except ImportError:
+    _BPPException = RuntimeError
+
+
+# Compatibility for python2.7
+if sys.version_info[0] == 2:
+    ProcessLookupError = OSError
+
+
+# Workers are created as daemon threads and processes. This is done to allow
+# the interpreter to exit when there are still idle processes in a
+# ProcessPoolExecutor's process pool (i.e. shutdown() was not called). However,
+# allowing workers to die with the interpreter has two undesirable properties:
+#   - The workers would still be running during interpreter shutdown,
+#     meaning that they would fail in unpredictable ways.
+#   - The workers could be killed while evaluating a work item, which could
+#     be bad if the callable being evaluated has external side-effects e.g.
+#     writing to a file.
+#
+# To work around this problem, an exit handler is installed which tells the
+# workers to exit when their work queues are empty and then waits until the
+# threads/processes finish.
+
+_threads_wakeups = weakref.WeakKeyDictionary()
+_global_shutdown = False
+
+# Mechanism to prevent infinite process spawning. When a worker of a
+# ProcessPoolExecutor nested in MAX_DEPTH Executor tries to create a new
+# Executor, a LokyRecursionError is raised
+MAX_DEPTH = int(os.environ.get("LOKY_MAX_DEPTH", 10))
+_CURRENT_DEPTH = 0
+
+# Minimum time interval between two consecutive memory usage checks.
+_MEMORY_CHECK_DELAY = 1.
+
+# Number of bytes of memory usage allowed over the reference process size.
+_MAX_MEMORY_LEAK_SIZE = int(1e8)
+
+try:
+    from psutil import Process
+
+    def _get_memory_usage(pid, force_gc=False):
+        if force_gc:
+            gc.collect()
+
+        return Process(pid).memory_info().rss
+
+except ImportError:
+    _get_memory_usage = None
+
+
+class _ThreadWakeup:
+    def __init__(self):
+        self._reader, self._writer = mp.Pipe(duplex=False)
+
+    def close(self):
+        self._writer.close()
+        self._reader.close()
+
+    def wakeup(self):
+        if sys.platform == "win32" and sys.version_info[:2] < (3, 4):
+            # Compat for python2.7 on windows, where poll return false for
+            # b"" messages. Use the slightly larger message b"0".
+            self._writer.send_bytes(b"0")
+        else:
+            self._writer.send_bytes(b"")
+
+    def clear(self):
+        while self._reader.poll():
+            self._reader.recv_bytes()
+
+
+class _ExecutorFlags(object):
+    """necessary references to maintain executor states without preventing gc
+
+    It permits to keep the information needed by queue_management_thread
+    and crash_detection_thread to maintain the pool without preventing the
+    garbage collection of unreferenced executors.
+    """
+    def __init__(self):
+
+        self.shutdown = False
+        self.broken = None
+        self.kill_workers = False
+        self.shutdown_lock = threading.Lock()
+
+    def flag_as_shutting_down(self, kill_workers=False):
+        with self.shutdown_lock:
+            self.shutdown = True
+            self.kill_workers = kill_workers
+
+    def flag_as_broken(self, broken):
+        with self.shutdown_lock:
+            self.shutdown = True
+            self.broken = broken
+
+
+def _python_exit():
+    global _global_shutdown
+    _global_shutdown = True
+    items = list(_threads_wakeups.items())
+    mp.util.debug("Interpreter shutting down. Waking up queue_manager_threads "
+                  "{}".format(items))
+    for thread, thread_wakeup in items:
+        if thread.is_alive():
+            thread_wakeup.wakeup()
+    for thread, _ in items:
+        thread.join()
+
+
+# Module variable to register the at_exit call
+process_pool_executor_at_exit = None
+
+# Controls how many more calls than processes will be queued in the call queue.
+# A smaller number will mean that processes spend more time idle waiting for
+# work while a larger number will make Future.cancel() succeed less frequently
+# (Futures in the call queue cannot be cancelled).
+EXTRA_QUEUED_CALLS = 1
+
+
+class _RemoteTraceback(Exception):
+    """Embed stringification of remote traceback in local traceback
+    """
+    def __init__(self, tb=None):
+        self.tb = tb
+
+    def __str__(self):
+        return self.tb
+
+
+class _ExceptionWithTraceback(BaseException):
+
+    def __init__(self, exc, tb=None):
+        if tb is None:
+            _, _, tb = sys.exc_info()
+        tb = traceback.format_exception(type(exc), exc, tb)
+        tb = ''.join(tb)
+        self.exc = exc
+        self.tb = '\n"""\n%s"""' % tb
+
+    def __reduce__(self):
+        return _rebuild_exc, (self.exc, self.tb)
+
+
+def _rebuild_exc(exc, tb):
+    exc.__cause__ = _RemoteTraceback(tb)
+    return exc
+
+
+class _WorkItem(object):
+
+    __slots__ = ["future", "fn", "args", "kwargs"]
+
+    def __init__(self, future, fn, args, kwargs):
+        self.future = future
+        self.fn = fn
+        self.args = args
+        self.kwargs = kwargs
+
+
+class _ResultItem(object):
+
+    def __init__(self, work_id, exception=None, result=None):
+        self.work_id = work_id
+        self.exception = exception
+        self.result = result
+
+
+class _CallItem(object):
+
+    def __init__(self, work_id, fn, args, kwargs):
+        self.work_id = work_id
+        self.fn = fn
+        self.args = args
+        self.kwargs = kwargs
+
+    def __repr__(self):
+        return "CallItem({}, {}, {}, {})".format(
+            self.work_id, self.fn, self.args, self.kwargs)
+
+    try:
+        # If cloudpickle is present on the system, use it to pickle the
+        # function. This permits to use interactive terminal for loky calls.
+        # TODO: Add option to deactivate, as it increases pickling time.
+        from .backend import LOKY_PICKLER
+        assert LOKY_PICKLER is None or LOKY_PICKLER == ""
+
+        import cloudpickle  # noqa: F401
+
+        def __getstate__(self):
+            from cloudpickle import dumps
+            if isinstance(self.fn, (types.FunctionType,
+                                    types.LambdaType,
+                                    partial)):
+                cp = True
+                fn = dumps(self.fn)
+            else:
+                cp = False
+                fn = self.fn
+            return (self.work_id, self.args, self.kwargs, fn, cp)
+
+        def __setstate__(self, state):
+            self.work_id, self.args, self.kwargs, self.fn, cp = state
+            if cp:
+                from cloudpickle import loads
+                self.fn = loads(self.fn)
+
+    except (ImportError, AssertionError) as e:
+        pass
+
+
+class _SafeQueue(Queue):
+    """Safe Queue set exception to the future object linked to a job"""
+    def __init__(self, max_size=0, ctx=None, pending_work_items=None,
+                 running_work_items=None, thread_wakeup=None, reducers=None):
+        self.thread_wakeup = thread_wakeup
+        self.pending_work_items = pending_work_items
+        self.running_work_items = running_work_items
+        super(_SafeQueue, self).__init__(max_size, reducers=reducers, ctx=ctx)
+
+    def _on_queue_feeder_error(self, e, obj):
+        if isinstance(obj, _CallItem):
+            # fromat traceback only on python3
+            pickling_error = PicklingError(
+                "Could not pickle the task to send it to the workers.")
+            tb = traceback.format_exception(
+                type(e), e, getattr(e, "__traceback__", None))
+            pickling_error.__cause__ = _RemoteTraceback(
+                '\n"""\n{}"""'.format(''.join(tb)))
+            work_item = self.pending_work_items.pop(obj.work_id, None)
+            self.running_work_items.remove(obj.work_id)
+            # work_item can be None if another process terminated. In this
+            # case, the queue_manager_thread fails all work_items with
+            # BrokenProcessPool
+            if work_item is not None:
+                work_item.future.set_exception(pickling_error)
+                del work_item
+            self.thread_wakeup.wakeup()
+        else:
+            super()._on_queue_feeder_error(e, obj)
+
+
+def _get_chunks(chunksize, *iterables):
+    """ Iterates over zip()ed iterables in chunks. """
+    if sys.version_info < (3, 3):
+        it = itertools.izip(*iterables)
+    else:
+        it = zip(*iterables)
+    while True:
+        chunk = tuple(itertools.islice(it, chunksize))
+        if not chunk:
+            return
+        yield chunk
+
+
+def _process_chunk(fn, chunk):
+    """ Processes a chunk of an iterable passed to map.
+
+    Runs the function passed to map() on a chunk of the
+    iterable passed to map.
+
+    This function is run in a separate process.
+
+    """
+    return [fn(*args) for args in chunk]
+
+
+def _sendback_result(result_queue, work_id, result=None, exception=None):
+    """Safely send back the given result or exception"""
+    try:
+        result_queue.put(_ResultItem(work_id, result=result,
+                                     exception=exception))
+    except BaseException as e:
+        exc = _ExceptionWithTraceback(e, getattr(e, "__traceback__", None))
+        result_queue.put(_ResultItem(work_id, exception=exc))
+
+
+def _process_worker(call_queue, result_queue, initializer, initargs,
+                    processes_management_lock, timeout, worker_exit_lock,
+                    current_depth):
+    """Evaluates calls from call_queue and places the results in result_queue.
+
+    This worker is run in a separate process.
+
+    Args:
+        call_queue: A ctx.Queue of _CallItems that will be read and
+            evaluated by the worker.
+        result_queue: A ctx.Queue of _ResultItems that will written
+            to by the worker.
+        initializer: A callable initializer, or None
+        initargs: A tuple of args for the initializer
+        process_management_lock: A ctx.Lock avoiding worker timeout while some
+            workers are being spawned.
+        timeout: maximum time to wait for a new item in the call_queue. If that
+            time is expired, the worker will shutdown.
+        worker_exit_lock: Lock to avoid flagging the executor as broken on
+            workers timeout.
+        current_depth: Nested parallelism level, to avoid infinite spawning.
+    """
+    if initializer is not None:
+        try:
+            initializer(*initargs)
+        except BaseException:
+            _base.LOGGER.critical('Exception in initializer:', exc_info=True)
+            # The parent will notice that the process stopped and
+            # mark the pool broken
+            return
+
+    # set the global _CURRENT_DEPTH mechanism to limit recursive call
+    global _CURRENT_DEPTH
+    _CURRENT_DEPTH = current_depth
+    _REFERENCE_PROCESS_SIZE = None
+    _LAST_MEMORY_CHECK = None
+    pid = os.getpid()
+
+    mp.util.debug('Worker started with timeout=%s' % timeout)
+    while True:
+        try:
+            call_item = call_queue.get(block=True, timeout=timeout)
+            if call_item is None:
+                mp.util.info("Shutting down worker on sentinel")
+        except queue.Empty:
+            mp.util.info("Shutting down worker after timeout %0.3fs"
+                         % timeout)
+            if processes_management_lock.acquire(block=False):
+                processes_management_lock.release()
+                call_item = None
+            else:
+                mp.util.info("Could not acquire processes_management_lock")
+                continue
+        except BaseException as e:
+            traceback.print_exc()
+            sys.exit(1)
+        if call_item is None:
+            # Notify queue management thread about clean worker shutdown
+            result_queue.put(pid)
+            with worker_exit_lock:
+                return
+        try:
+            r = call_item.fn(*call_item.args, **call_item.kwargs)
+        except BaseException as e:
+            exc = _ExceptionWithTraceback(e, getattr(e, "__traceback__", None))
+            result_queue.put(_ResultItem(call_item.work_id, exception=exc))
+        else:
+            _sendback_result(result_queue, call_item.work_id, result=r)
+
+        # Free the resource as soon as possible, to avoid holding onto
+        # open files or shared memory that is not needed anymore
+        del call_item
+
+        if _get_memory_usage is not None:
+            if _REFERENCE_PROCESS_SIZE is None:
+                # Make reference measurement after the first call
+                _REFERENCE_PROCESS_SIZE = _get_memory_usage(pid, force_gc=True)
+                _LAST_MEMORY_CHECK = time()
+                continue
+            if time() - _LAST_MEMORY_CHECK > _MEMORY_CHECK_DELAY:
+                mem_usage = _get_memory_usage(pid)
+                _LAST_MEMORY_CHECK = time()
+                if mem_usage - _REFERENCE_PROCESS_SIZE < _MAX_MEMORY_LEAK_SIZE:
+                    # Memory usage stays within bounds: everything is fine.
+                    continue
+
+                # Check again memory usage; this time take the measurement
+                # after a forced garbage collection to break any reference
+                # cycles.
+                mem_usage = _get_memory_usage(pid, force_gc=True)
+                _LAST_MEMORY_CHECK = time()
+                if mem_usage - _REFERENCE_PROCESS_SIZE < _MAX_MEMORY_LEAK_SIZE:
+                    # The GC managed to free the memory: everything is fine.
+                    continue
+
+                # The process is leaking memory: let the master process
+                # know that we need to start a new worker.
+                mp.util.info("Memory leak detected: shutting down worker")
+                result_queue.put(pid)
+                with worker_exit_lock:
+                    return
+
+
+def _add_call_item_to_queue(pending_work_items,
+                            running_work_items,
+                            work_ids,
+                            call_queue):
+    """Fills call_queue with _WorkItems from pending_work_items.
+
+    This function never blocks.
+
+    Args:
+        pending_work_items: A dict mapping work ids to _WorkItems e.g.
+            {5: <_WorkItem...>, 6: <_WorkItem...>, ...}
+        work_ids: A queue.Queue of work ids e.g. Queue([5, 6, ...]). Work ids
+            are consumed and the corresponding _WorkItems from
+            pending_work_items are transformed into _CallItems and put in
+            call_queue.
+        call_queue: A ctx.Queue that will be filled with _CallItems
+            derived from _WorkItems.
+    """
+    while True:
+        if call_queue.full():
+            return
+        try:
+            work_id = work_ids.get(block=False)
+        except queue.Empty:
+            return
+        else:
+            work_item = pending_work_items[work_id]
+
+            if work_item.future.set_running_or_notify_cancel():
+                running_work_items += [work_id]
+                call_queue.put(_CallItem(work_id,
+                                         work_item.fn,
+                                         work_item.args,
+                                         work_item.kwargs),
+                               block=True)
+            else:
+                del pending_work_items[work_id]
+                continue
+
+
+def _queue_management_worker(executor_reference,
+                             executor_flags,
+                             processes,
+                             pending_work_items,
+                             running_work_items,
+                             work_ids_queue,
+                             call_queue,
+                             result_queue,
+                             thread_wakeup,
+                             processes_management_lock):
+    """Manages the communication between this process and the worker processes.
+
+    This function is run in a local thread.
+
+    Args:
+        executor_reference: A weakref.ref to the ProcessPoolExecutor that owns
+            this thread. Used to determine if the ProcessPoolExecutor has been
+            garbage collected and that this function can exit.
+        executor_flags: A ExecutorFlags holding internal states of the
+            ProcessPoolExecutor. It permits to know if the executor is broken
+            even the object has been gc.
+        process: A list of the ctx.Process instances used as
+            workers.
+        pending_work_items: A dict mapping work ids to _WorkItems e.g.
+            {5: <_WorkItem...>, 6: <_WorkItem...>, ...}
+        work_ids_queue: A queue.Queue of work ids e.g. Queue([5, 6, ...]).
+        call_queue: A ctx.Queue that will be filled with _CallItems
+            derived from _WorkItems for processing by the process workers.
+        result_queue: A ctx.SimpleQueue of _ResultItems generated by the
+            process workers.
+        thread_wakeup: A _ThreadWakeup to allow waking up the
+            queue_manager_thread from the main Thread and avoid deadlocks
+            caused by permanently locked queues.
+    """
+    executor = None
+
+    def is_shutting_down():
+        # No more work items can be added if:
+        #   - The interpreter is shutting down OR
+        #   - The executor that own this worker is not broken AND
+        #        * The executor that owns this worker has been collected OR
+        #        * The executor that owns this worker has been shutdown.
+        # If the executor is broken, it should be detected in the next loop.
+        return (_global_shutdown or
+                ((executor is None or executor_flags.shutdown)
+                 and not executor_flags.broken))
+
+    def shutdown_all_workers():
+        mp.util.debug("queue management thread shutting down")
+        executor_flags.flag_as_shutting_down()
+        # Create a list to avoid RuntimeError due to concurrent modification of
+        # processes. nb_children_alive is thus an upper bound. Also release the
+        # processes' _worker_exit_lock to accelerate the shutdown procedure, as
+        # there is no need for hand-shake here.
+        with processes_management_lock:
+            n_children_alive = 0
+            for p in list(processes.values()):
+                p._worker_exit_lock.release()
+                n_children_alive += 1
+        n_children_to_stop = n_children_alive
+        n_sentinels_sent = 0
+        # Send the right number of sentinels, to make sure all children are
+        # properly terminated.
+        while n_sentinels_sent < n_children_to_stop and n_children_alive > 0:
+            for i in range(n_children_to_stop - n_sentinels_sent):
+                try:
+                    call_queue.put_nowait(None)
+                    n_sentinels_sent += 1
+                except Full:
+                    break
+            with processes_management_lock:
+                n_children_alive = sum(
+                    p.is_alive() for p in list(processes.values())
+                )
+
+        # Release the queue's resources as soon as possible. Flag the feeder
+        # thread for clean exit to avoid having the crash detection thread flag
+        # the Executor as broken during the shutdown. This is safe as either:
+        #  * We don't need to communicate with the workers anymore
+        #  * There is nothing left in the Queue buffer except None sentinels
+        mp.util.debug("closing call_queue")
+        call_queue.close()
+
+        mp.util.debug("joining processes")
+        # If .join() is not called on the created processes then
+        # some ctx.Queue methods may deadlock on Mac OS X.
+        while processes:
+            _, p = processes.popitem()
+            p.join()
+        mp.util.debug("queue management thread clean shutdown of worker "
+                      "processes: {}".format(list(processes)))
+
+    result_reader = result_queue._reader
+    wakeup_reader = thread_wakeup._reader
+    readers = [result_reader, wakeup_reader]
+
+    while True:
+        _add_call_item_to_queue(pending_work_items,
+                                running_work_items,
+                                work_ids_queue,
+                                call_queue)
+        # Wait for a result to be ready in the result_queue while checking
+        # that all worker processes are still running, or for a wake up
+        # signal send. The wake up signals come either from new tasks being
+        # submitted, from the executor being shutdown/gc-ed, or from the
+        # shutdown of the python interpreter.
+        worker_sentinels = [p.sentinel for p in processes.values()]
+        ready = wait(readers + worker_sentinels)
+
+        broken = ("A process in the executor was terminated abruptly", None)
+        if result_reader in ready:
+            try:
+                result_item = result_reader.recv()
+                broken = None
+            except BaseException as e:
+                tb = getattr(e, "__traceback__", None)
+                if tb is None:
+                    _, _, tb = sys.exc_info()
+                broken = ("A result has failed to un-serialize",
+                          traceback.format_exception(type(e), e, tb))
+        elif wakeup_reader in ready:
+            broken = None
+            result_item = None
+        thread_wakeup.clear()
+        if broken:
+            msg, cause = broken
+            # Mark the process pool broken so that submits fail right now.
+            executor_flags.flag_as_broken(
+                msg + ", the pool is not usable anymore.")
+            bpe = BrokenProcessPool(
+                msg + " while the future was running or pending.")
+            if cause is not None:
+                bpe.__cause__ = _RemoteTraceback(
+                    "\n'''\n{}'''".format(''.join(cause)))
+
+            # All futures in flight must be marked failed
+            for work_id, work_item in pending_work_items.items():
+                work_item.future.set_exception(bpe)
+                # Delete references to object. See issue16284
+                del work_item
+            pending_work_items.clear()
+
+            # Terminate remaining workers forcibly: the queues or their
+            # locks may be in a dirty state and block forever.
+            while processes:
+                _, p = processes.popitem()
+                mp.util.debug('terminate process {}'.format(p.name))
+                try:
+                    recursive_terminate(p)
+                except ProcessLookupError:  # pragma: no cover
+                    pass
+
+            shutdown_all_workers()
+            return
+        if isinstance(result_item, int):
+            # Clean shutdown of a worker using its PID, either on request
+            # by the executor.shutdown method or by the timeout of the worker
+            # itself: we should not mark the executor as broken.
+            with processes_management_lock:
+                p = processes.pop(result_item, None)
+
+            # p can be None is the executor is concurrently shutting down.
+            if p is not None:
+                p._worker_exit_lock.release()
+                p.join()
+                del p
+
+            # Make sure the executor have the right number of worker, even if a
+            # worker timeout while some jobs were submitted. If some work is
+            # pending or there is less processes than running items, we need to
+            # start a new Process and raise a warning.
+            n_pending = len(pending_work_items)
+            n_running = len(running_work_items)
+            if (n_pending - n_running > 0 or n_running > len(processes)):
+                executor = executor_reference()
+                if (executor is not None
+                        and len(processes) < executor._max_workers):
+                    warnings.warn(
+                        "A worker stopped while some jobs were given to the "
+                        "executor. This can be caused by a too short worker "
+                        "timeout or by a memory leak.", UserWarning
+                    )
+                    executor._adjust_process_count()
+                    executor = None
+
+        elif result_item is not None:
+            work_item = pending_work_items.pop(result_item.work_id, None)
+            # work_item can be None if another process terminated
+            if work_item is not None:
+                if result_item.exception:
+                    work_item.future.set_exception(result_item.exception)
+                else:
+                    work_item.future.set_result(result_item.result)
+                # Delete references to object. See issue16284
+                del work_item
+                running_work_items.remove(result_item.work_id)
+            # Delete reference to result_item
+            del result_item
+
+        # Check whether we should start shutting down.
+        executor = executor_reference()
+        # No more work items can be added if:
+        #   - The interpreter is shutting down OR
+        #   - The executor that owns this worker has been collected OR
+        #   - The executor that owns this worker has been shutdown.
+        if is_shutting_down():
+            # bpo-33097: Make sure that the executor is flagged as shutting
+            # down even if it is shutdown by the interpreter exiting.
+            with executor_flags.shutdown_lock:
+                executor_flags.shutdown = True
+            if executor_flags.kill_workers:
+                while pending_work_items:
+                    _, work_item = pending_work_items.popitem()
+                    work_item.future.set_exception(ShutdownExecutorError(
+                        "The Executor was shutdown before this job could "
+                        "complete."))
+                    del work_item
+                # Terminate remaining workers forcibly: the queues or their
+                # locks may be in a dirty state and block forever.
+                while processes:
+                    _, p = processes.popitem()
+                    recursive_terminate(p)
+                shutdown_all_workers()
+                return
+            # Since no new work items can be added, it is safe to shutdown
+            # this thread if there are no pending work items.
+            if not pending_work_items:
+                shutdown_all_workers()
+                return
+        elif executor_flags.broken:
+            return
+        executor = None
+
+
+_system_limits_checked = False
+_system_limited = None
+
+
+def _check_system_limits():
+    global _system_limits_checked, _system_limited
+    if _system_limits_checked:
+        if _system_limited:
+            raise NotImplementedError(_system_limited)
+    _system_limits_checked = True
+    try:
+        nsems_max = os.sysconf("SC_SEM_NSEMS_MAX")
+    except (AttributeError, ValueError):
+        # sysconf not available or setting not available
+        return
+    if nsems_max == -1:
+        # undetermined limit, assume that limit is determined
+        # by available memory only
+        return
+    if nsems_max >= 256:
+        # minimum number of semaphores available
+        # according to POSIX
+        return
+    _system_limited = ("system provides too few semaphores (%d available, "
+                       "256 necessary)" % nsems_max)
+    raise NotImplementedError(_system_limited)
+
+
+def _chain_from_iterable_of_lists(iterable):
+    """
+    Specialized implementation of itertools.chain.from_iterable.
+    Each item in *iterable* should be a list.  This function is
+    careful not to keep references to yielded objects.
+    """
+    for element in iterable:
+        element.reverse()
+        while element:
+            yield element.pop()
+
+
+def _check_max_depth(context):
+    # Limit the maxmal recursion level
+    global _CURRENT_DEPTH
+    if context.get_start_method() == "fork" and _CURRENT_DEPTH > 0:
+        raise LokyRecursionError(
+            "Could not spawn extra nested processes at depth superior to "
+            "MAX_DEPTH=1. It is not possible to increase this limit when "
+            "using the 'fork' start method.")
+
+    if 0 < MAX_DEPTH and _CURRENT_DEPTH + 1 > MAX_DEPTH:
+        raise LokyRecursionError(
+            "Could not spawn extra nested processes at depth superior to "
+            "MAX_DEPTH={}. If this is intendend, you can change this limit "
+            "with the LOKY_MAX_DEPTH environment variable.".format(MAX_DEPTH))
+
+
+class LokyRecursionError(RuntimeError):
+    """Raised when a process try to spawn too many levels of nested processes.
+    """
+
+
+class BrokenProcessPool(_BPPException):
+    """
+    Raised when a process in a ProcessPoolExecutor terminated abruptly
+    while a future was in the running state.
+    """
+
+
+# Alias for backward compat (for code written for loky 1.1.4 and earlier). Do
+# not use in new code.
+BrokenExecutor = BrokenProcessPool
+
+
+class ShutdownExecutorError(RuntimeError):
+
+    """
+    Raised when a ProcessPoolExecutor is shutdown while a future was in the
+    running or pending state.
+    """
+
+
+class ProcessPoolExecutor(_base.Executor):
+
+    _at_exit = None
+
+    def __init__(self, max_workers=None, job_reducers=None,
+                 result_reducers=None, timeout=None, context=None,
+                 initializer=None, initargs=()):
+        """Initializes a new ProcessPoolExecutor instance.
+
+        Args:
+            max_workers: int, optional (default: cpu_count())
+                The maximum number of processes that can be used to execute the
+                given calls. If None or not given then as many worker processes
+                will be created as the number of CPUs the current process
+                can use.
+            job_reducers, result_reducers: dict(type: reducer_func)
+                Custom reducer for pickling the jobs and the results from the
+                Executor. If only `job_reducers` is provided, `result_reducer`
+                will use the same reducers
+            timeout: int, optional (default: None)
+                Idle workers exit after timeout seconds. If a new job is
+                submitted after the timeout, the executor will start enough
+                new Python processes to make sure the pool of workers is full.
+            context: A multiprocessing context to launch the workers. This
+                object should provide SimpleQueue, Queue and Process.
+            initializer: An callable used to initialize worker processes.
+            initargs: A tuple of arguments to pass to the initializer.
+        """
+        _check_system_limits()
+
+        if max_workers is None:
+            self._max_workers = cpu_count()
+        else:
+            if max_workers <= 0:
+                raise ValueError("max_workers must be greater than 0")
+            self._max_workers = max_workers
+
+        if context is None:
+            context = get_context()
+        self._context = context
+
+        if initializer is not None and not callable(initializer):
+            raise TypeError("initializer must be a callable")
+        self._initializer = initializer
+        self._initargs = initargs
+
+        _check_max_depth(self._context)
+
+        if result_reducers is None:
+            result_reducers = job_reducers
+
+        # Timeout
+        self._timeout = timeout
+
+        # Internal variables of the ProcessPoolExecutor
+        self._processes = {}
+        self._queue_count = 0
+        self._pending_work_items = {}
+        self._running_work_items = []
+        self._work_ids = queue.Queue()
+        self._processes_management_lock = self._context.Lock()
+        self._queue_management_thread = None
+
+        # _ThreadWakeup is a communication channel used to interrupt the wait
+        # of the main loop of queue_manager_thread from another thread (e.g.
+        # when calling executor.submit or executor.shutdown). We do not use the
+        # _result_queue to send the wakeup signal to the queue_manager_thread
+        # as it could result in a deadlock if a worker process dies with the
+        # _result_queue write lock still acquired.
+        self._queue_management_thread_wakeup = _ThreadWakeup()
+
+        # Flag to hold the state of the Executor. This permits to introspect
+        # the Executor state even once it has been garbage collected.
+        self._flags = _ExecutorFlags()
+
+        # Finally setup the queues for interprocess communication
+        self._setup_queues(job_reducers, result_reducers)
+
+        mp.util.debug('ProcessPoolExecutor is setup')
+
+    def _setup_queues(self, job_reducers, result_reducers, queue_size=None):
+        # Make the call queue slightly larger than the number of processes to
+        # prevent the worker processes from idling. But don't make it too big
+        # because futures in the call queue cannot be cancelled.
+        if queue_size is None:
+            queue_size = 2 * self._max_workers + EXTRA_QUEUED_CALLS
+        self._call_queue = _SafeQueue(
+            max_size=queue_size, pending_work_items=self._pending_work_items,
+            running_work_items=self._running_work_items,
+            thread_wakeup=self._queue_management_thread_wakeup,
+            reducers=job_reducers, ctx=self._context)
+        # Killed worker processes can produce spurious "broken pipe"
+        # tracebacks in the queue's own worker thread. But we detect killed
+        # processes anyway, so silence the tracebacks.
+        self._call_queue._ignore_epipe = True
+
+        self._result_queue = SimpleQueue(reducers=result_reducers,
+                                         ctx=self._context)
+
+    def _start_queue_management_thread(self):
+        if self._queue_management_thread is None:
+            mp.util.debug('_start_queue_management_thread called')
+
+            # When the executor gets garbarge collected, the weakref callback
+            # will wake up the queue management thread so that it can terminate
+            # if there is no pending work item.
+            def weakref_cb(_,
+                           thread_wakeup=self._queue_management_thread_wakeup):
+                mp.util.debug('Executor collected: triggering callback for'
+                              ' QueueManager wakeup')
+                thread_wakeup.wakeup()
+
+            # Start the processes so that their sentinels are known.
+            self._queue_management_thread = threading.Thread(
+                target=_queue_management_worker,
+                args=(weakref.ref(self, weakref_cb),
+                      self._flags,
+                      self._processes,
+                      self._pending_work_items,
+                      self._running_work_items,
+                      self._work_ids,
+                      self._call_queue,
+                      self._result_queue,
+                      self._queue_management_thread_wakeup,
+                      self._processes_management_lock),
+                name="QueueManagerThread")
+            self._queue_management_thread.daemon = True
+            self._queue_management_thread.start()
+
+            # register this executor in a mechanism that ensures it will wakeup
+            # when the interpreter is exiting.
+            _threads_wakeups[self._queue_management_thread] = \
+                self._queue_management_thread_wakeup
+
+            global process_pool_executor_at_exit
+            if process_pool_executor_at_exit is None:
+                # Ensure that the _python_exit function will be called before
+                # the multiprocessing.Queue._close finalizers which have an
+                # exitpriority of 10.
+                process_pool_executor_at_exit = mp.util.Finalize(
+                    None, _python_exit, exitpriority=20)
+
+    def _adjust_process_count(self):
+        for _ in range(len(self._processes), self._max_workers):
+            worker_exit_lock = self._context.BoundedSemaphore(1)
+            worker_exit_lock.acquire()
+            p = self._context.Process(
+                target=_process_worker,
+                args=(self._call_queue,
+                      self._result_queue,
+                      self._initializer,
+                      self._initargs,
+                      self._processes_management_lock,
+                      self._timeout,
+                      worker_exit_lock,
+                      _CURRENT_DEPTH + 1))
+            p._worker_exit_lock = worker_exit_lock
+            p.start()
+            self._processes[p.pid] = p
+        mp.util.debug('Adjust process count : {}'.format(self._processes))
+
+    def _ensure_executor_running(self):
+        """ensures all workers and management thread are running
+        """
+        with self._processes_management_lock:
+            if len(self._processes) != self._max_workers:
+                self._adjust_process_count()
+            self._start_queue_management_thread()
+
+    def submit(self, fn, *args, **kwargs):
+        with self._flags.shutdown_lock:
+            if self._flags.broken:
+                raise BrokenProcessPool(self._flags.broken)
+            if self._flags.shutdown:
+                raise ShutdownExecutorError(
+                    'cannot schedule new futures after shutdown')
+
+            # Cannot submit a new calls once the interpreter is shutting down.
+            # This check avoids spawning new processes at exit.
+            if _global_shutdown:
+                raise RuntimeError('cannot schedule new futures after '
+                                   'interpreter shutdown')
+
+            f = _base.Future()
+            w = _WorkItem(f, fn, args, kwargs)
+
+            self._pending_work_items[self._queue_count] = w
+            self._work_ids.put(self._queue_count)
+            self._queue_count += 1
+            # Wake up queue management thread
+            self._queue_management_thread_wakeup.wakeup()
+
+            self._ensure_executor_running()
+            return f
+    submit.__doc__ = _base.Executor.submit.__doc__
+
+    def map(self, fn, *iterables, **kwargs):
+        """Returns an iterator equivalent to map(fn, iter).
+
+        Args:
+            fn: A callable that will take as many arguments as there are
+                passed iterables.
+            timeout: The maximum number of seconds to wait. If None, then there
+                is no limit on the wait time.
+            chunksize: If greater than one, the iterables will be chopped into
+                chunks of size chunksize and submitted to the process pool.
+                If set to one, the items in the list will be sent one at a
+                time.
+
+        Returns:
+            An iterator equivalent to: map(func, *iterables) but the calls may
+            be evaluated out-of-order.
+
+        Raises:
+            TimeoutError: If the entire result iterator could not be generated
+                before the given timeout.
+            Exception: If fn(*args) raises for any values.
+        """
+        timeout = kwargs.get('timeout', None)
+        chunksize = kwargs.get('chunksize', 1)
+        if chunksize < 1:
+            raise ValueError("chunksize must be >= 1.")
+
+        results = super(ProcessPoolExecutor, self).map(
+            partial(_process_chunk, fn), _get_chunks(chunksize, *iterables),
+            timeout=timeout)
+        return _chain_from_iterable_of_lists(results)
+
+    def shutdown(self, wait=True, kill_workers=False):
+        mp.util.debug('shutting down executor %s' % self)
+
+        self._flags.flag_as_shutting_down(kill_workers)
+        qmt = self._queue_management_thread
+        qmtw = self._queue_management_thread_wakeup
+        if qmt:
+            self._queue_management_thread = None
+            if qmtw:
+                self._queue_management_thread_wakeup = None
+            # Wake up queue management thread
+            if qmtw is not None:
+                try:
+                    qmtw.wakeup()
+                except OSError:
+                    # Can happen in case of concurrent calls to shutdown.
+                    pass
+            if wait:
+                qmt.join()
+
+        cq = self._call_queue
+        if cq:
+            self._call_queue = None
+            cq.close()
+            if wait:
+                cq.join_thread()
+        self._result_queue = None
+        self._processes_management_lock = None
+
+        if qmtw:
+            try:
+                qmtw.close()
+            except OSError:
+                # Can happen in case of concurrent calls to shutdown.
+                pass
+    shutdown.__doc__ = _base.Executor.shutdown.__doc__
diff --git a/sklearn/externals/joblib/externals/loky/reusable_executor.py b/sklearn/externals/joblib/externals/loky/reusable_executor.py
new file mode 100755
index 0000000000..30b217fd41
--- /dev/null
+++ b/sklearn/externals/joblib/externals/loky/reusable_executor.py
@@ -0,0 +1,205 @@
+###############################################################################
+# Reusable ProcessPoolExecutor
+#
+# author: Thomas Moreau and Olivier Grisel
+#
+import time
+import warnings
+import threading
+import multiprocessing as mp
+
+from .process_executor import ProcessPoolExecutor, EXTRA_QUEUED_CALLS
+from .backend.context import cpu_count
+from .backend import get_context
+
+__all__ = ['get_reusable_executor']
+
+# Python 2 compat helper
+STRING_TYPE = type("")
+
+# Singleton executor and id management
+_executor_lock = threading.RLock()
+_next_executor_id = 0
+_executor = None
+_executor_args = None
+
+
+def _get_next_executor_id():
+    """Ensure that each successive executor instance has a unique, monotonic id.
+
+    The purpose of this monotonic id is to help debug and test automated
+    instance creation.
+    """
+    global _next_executor_id
+    with _executor_lock:
+        executor_id = _next_executor_id
+        _next_executor_id += 1
+        return executor_id
+
+
+def get_reusable_executor(max_workers=None, context=None, timeout=10,
+                          kill_workers=False, reuse="auto",
+                          job_reducers=None, result_reducers=None,
+                          initializer=None, initargs=()):
+    """Return the current ReusableExectutor instance.
+
+    Start a new instance if it has not been started already or if the previous
+    instance was left in a broken state.
+
+    If the previous instance does not have the requested number of workers, the
+    executor is dynamically resized to adjust the number of workers prior to
+    returning.
+
+    Reusing a singleton instance spares the overhead of starting new worker
+    processes and importing common python packages each time.
+
+    ``max_workers`` controls the maximum number of tasks that can be running in
+    parallel in worker processes. By default this is set to the number of
+    CPUs on the host.
+
+    Setting ``timeout`` (in seconds) makes idle workers automatically shutdown
+    so as to release system resources. New workers are respawn upon submission
+    of new tasks so that ``max_workers`` are available to accept the newly
+    submitted tasks. Setting ``timeout`` to around 100 times the time required
+    to spawn new processes and import packages in them (on the order of 100ms)
+    ensures that the overhead of spawning workers is negligible.
+
+    Setting ``kill_workers=True`` makes it possible to forcibly interrupt
+    previously spawned jobs to get a new instance of the reusable executor
+    with new constructor argument values.
+
+    The ``job_reducers`` and ``result_reducers`` are used to customize the
+    pickling of tasks and results send to the executor.
+
+    When provided, the ``initializer`` is run first in newly spawned
+    processes with argument ``initargs``.
+    """
+    with _executor_lock:
+        global _executor, _executor_kwargs
+        executor = _executor
+
+        if max_workers is None:
+            if reuse is True and executor is not None:
+                max_workers = executor._max_workers
+            else:
+                max_workers = cpu_count()
+        elif max_workers <= 0:
+            raise ValueError(
+                "max_workers must be greater than 0, got {}."
+                .format(max_workers))
+
+        if isinstance(context, STRING_TYPE):
+            context = get_context(context)
+        if context is not None and context.get_start_method() == "fork":
+            raise ValueError("Cannot use reusable executor with the 'fork' "
+                             "context")
+
+        kwargs = dict(context=context, timeout=timeout,
+                      job_reducers=job_reducers,
+                      result_reducers=result_reducers,
+                      initializer=initializer, initargs=initargs)
+        if executor is None:
+            mp.util.debug("Create a executor with max_workers={}."
+                          .format(max_workers))
+            executor_id = _get_next_executor_id()
+            _executor_kwargs = kwargs
+            _executor = executor = _ReusablePoolExecutor(
+                _executor_lock, max_workers=max_workers,
+                executor_id=executor_id, **kwargs)
+        else:
+            if reuse == 'auto':
+                reuse = kwargs == _executor_kwargs
+            if (executor._flags.broken or executor._flags.shutdown
+                    or not reuse):
+                if executor._flags.broken:
+                    reason = "broken"
+                elif executor._flags.shutdown:
+                    reason = "shutdown"
+                else:
+                    reason = "arguments have changed"
+                mp.util.debug(
+                    "Creating a new executor with max_workers={} as the "
+                    "previous instance cannot be reused ({})."
+                    .format(max_workers, reason))
+                executor.shutdown(wait=True, kill_workers=kill_workers)
+                _executor = executor = _executor_kwargs = None
+                # Recursive call to build a new instance
+                return get_reusable_executor(max_workers=max_workers,
+                                             **kwargs)
+            else:
+                mp.util.debug("Reusing existing executor with max_workers={}."
+                              .format(executor._max_workers))
+                executor._resize(max_workers)
+
+    return executor
+
+
+class _ReusablePoolExecutor(ProcessPoolExecutor):
+    def __init__(self, submit_resize_lock, max_workers=None, context=None,
+                 timeout=None, executor_id=0, job_reducers=None,
+                 result_reducers=None, initializer=None, initargs=()):
+        super(_ReusablePoolExecutor, self).__init__(
+            max_workers=max_workers, context=context, timeout=timeout,
+            job_reducers=job_reducers, result_reducers=result_reducers,
+            initializer=initializer, initargs=initargs)
+        self.executor_id = executor_id
+        self._submit_resize_lock = submit_resize_lock
+
+    def submit(self, fn, *args, **kwargs):
+        with self._submit_resize_lock:
+            return super(_ReusablePoolExecutor, self).submit(
+                fn, *args, **kwargs)
+
+    def _resize(self, max_workers):
+        with self._submit_resize_lock:
+            if max_workers is None:
+                raise ValueError("Trying to resize with max_workers=None")
+            elif max_workers == self._max_workers:
+                return
+
+            if self._queue_management_thread is None:
+                # If the queue_management_thread has not been started
+                # then no processes have been spawned and we can just
+                # update _max_workers and return
+                self._max_workers = max_workers
+                return
+
+            self._wait_job_completion()
+
+            # Some process might have returned due to timeout so check how many
+            # children are still alive. Use the _process_management_lock to
+            # ensure that no process are spawned or timeout during the resize.
+            with self._processes_management_lock:
+                processes = list(self._processes.values())
+                nb_children_alive = sum(p.is_alive() for p in processes)
+                self._max_workers = max_workers
+                for _ in range(max_workers, nb_children_alive):
+                    self._call_queue.put(None)
+            while (len(self._processes) > max_workers
+                   and not self._flags.broken):
+                time.sleep(1e-3)
+
+            self._adjust_process_count()
+            processes = list(self._processes.values())
+            while not all([p.is_alive() for p in processes]):
+                time.sleep(1e-3)
+
+    def _wait_job_completion(self):
+        """Wait for the cache to be empty before resizing the pool."""
+        # Issue a warning to the user about the bad effect of this usage.
+        if len(self._pending_work_items) > 0:
+            warnings.warn("Trying to resize an executor with running jobs: "
+                          "waiting for jobs completion before resizing.",
+                          UserWarning)
+            mp.util.debug("Executor {} waiting for jobs completion before"
+                          " resizing".format(self.executor_id))
+        # Wait for the completion of the jobs
+        while len(self._pending_work_items) > 0:
+            time.sleep(1e-3)
+
+    def _setup_queues(self, job_reducers, result_reducers):
+        # As this executor can be resized, use a large queue size to avoid
+        # underestimating capacity and introducing overhead
+        queue_size = 2 * cpu_count() + EXTRA_QUEUED_CALLS
+        super(_ReusablePoolExecutor, self)._setup_queues(
+            job_reducers, result_reducers, queue_size=queue_size)
diff --git a/sklearn/externals/joblib/format_stack.py b/sklearn/externals/joblib/format_stack.py
index 4984ebb081..949ac7d957 100755
--- a/sklearn/externals/joblib/format_stack.py
+++ b/sklearn/externals/joblib/format_stack.py
@@ -355,7 +355,7 @@ def format_exc(etype, evalue, etb, context=5, tb_offset=0):
     # Get (safely) a string form of the exception info
     try:
         etype_str, evalue_str = map(str, (etype, evalue))
-    except:
+    except BaseException:
         # User exception is improperly defined.
         etype, evalue = str, sys.exc_info()[:2]
         etype_str, evalue_str = map(str, (etype, evalue))
diff --git a/sklearn/externals/joblib/func_inspect.py b/sklearn/externals/joblib/func_inspect.py
index 86e3a79bb7..e9320b43f1 100755
--- a/sklearn/externals/joblib/func_inspect.py
+++ b/sklearn/externals/joblib/func_inspect.py
@@ -11,12 +11,17 @@
 import warnings
 import re
 import os
+import collections
 
 from ._compat import _basestring
 from .logger import pformat
 from ._memory_helpers import open_py_source
 from ._compat import PY3_OR_LATER
 
+full_argspec_fields = ('args varargs varkw defaults kwonlyargs '
+                       'kwonlydefaults annotations')
+full_argspec_type = collections.namedtuple('FullArgSpec', full_argspec_fields)
+
 
 def get_func_code(func):
     """ Attempts to retrieve a reliable function code hash.
@@ -169,18 +174,13 @@ def getfullargspec(func):
         return inspect.getfullargspec(func)
     except AttributeError:
         arg_spec = inspect.getargspec(func)
-        import collections
-        tuple_fields = ('args varargs varkw defaults kwonlyargs '
-                        'kwonlydefaults annotations')
-        tuple_type = collections.namedtuple('FullArgSpec', tuple_fields)
-
-        return tuple_type(args=arg_spec.args,
-                          varargs=arg_spec.varargs,
-                          varkw=arg_spec.keywords,
-                          defaults=arg_spec.defaults,
-                          kwonlyargs=[],
-                          kwonlydefaults=None,
-                          annotations={})
+        return full_argspec_type(args=arg_spec.args,
+                                 varargs=arg_spec.varargs,
+                                 varkw=arg_spec.keywords,
+                                 defaults=arg_spec.defaults,
+                                 kwonlyargs=[],
+                                 kwonlydefaults=None,
+                                 annotations={})
 
 
 def _signature_str(function_name, arg_spec):
@@ -240,8 +240,10 @@ def filter_args(func, ignore_lst, args=(), kwargs=dict()):
     arg_spec = getfullargspec(func)
     arg_names = arg_spec.args + arg_spec.kwonlyargs
     arg_defaults = arg_spec.defaults or ()
-    arg_defaults = arg_defaults + tuple(arg_spec.kwonlydefaults[k]
-                                        for k in arg_spec.kwonlyargs)
+    if arg_spec.kwonlydefaults:
+        arg_defaults = arg_defaults + tuple(arg_spec.kwonlydefaults[k]
+                                            for k in arg_spec.kwonlyargs
+                                            if k in arg_spec.kwonlydefaults)
     arg_varargs = arg_spec.varargs
     arg_varkw = arg_spec.varkw
 
diff --git a/sklearn/externals/joblib/logger.py b/sklearn/externals/joblib/logger.py
index 9721512b6d..f30efef853 100755
--- a/sklearn/externals/joblib/logger.py
+++ b/sklearn/externals/joblib/logger.py
@@ -81,8 +81,7 @@ def debug(self, msg):
         logging.debug("[%s]: %s" % (self, msg))
 
     def format(self, obj, indent=0):
-        """ Return the formatted representation of the object.
-        """
+        """Return the formatted representation of the object."""
         return pformat(obj, indent=indent, depth=self.depth)
 
 
diff --git a/sklearn/externals/joblib/memory.py b/sklearn/externals/joblib/memory.py
index 14d7552535..ae187950bc 100755
--- a/sklearn/externals/joblib/memory.py
+++ b/sklearn/externals/joblib/memory.py
@@ -11,7 +11,6 @@
 
 from __future__ import with_statement
 import os
-import shutil
 import time
 import pydoc
 import re
@@ -19,13 +18,7 @@
 import traceback
 import warnings
 import inspect
-import json
 import weakref
-import io
-import operator
-import collections
-import datetime
-import threading
 
 # Local imports
 from . import hashing
@@ -34,15 +27,11 @@
 from .func_inspect import format_signature
 from ._memory_helpers import open_py_source
 from .logger import Logger, format_time, pformat
-from . import numpy_pickle
-from .disk import mkdirp, rm_subdirs, memstr_to_bytes
 from ._compat import _basestring, PY3_OR_LATER
-from .backports import concurrency_safe_rename
+from ._store_backends import StoreBackendBase, FileSystemStoreBackend
 
-FIRST_LINE_TEXT = "# first line:"
 
-CacheItemInfo = collections.namedtuple('CacheItemInfo',
-                                       'path size last_access')
+FIRST_LINE_TEXT = "# first line:"
 
 # TODO: The following object should have a data store object as a sub
 # object, and the interface to persist and query should be separated in
@@ -75,141 +64,106 @@ class JobLibCollisionWarning(UserWarning):
     """
 
 
-def _get_func_fullname(func):
-    """Compute the part of part associated with a function.
+_STORE_BACKENDS = {'local': FileSystemStoreBackend}
+
+
+def register_store_backend(backend_name, backend):
+    """Extend available store backends.
+
+    The Memory, MemorizeResult and MemorizeFunc objects are designed to be
+    agnostic to the type of store used behind. By default, the local file
+    system is used but this function gives the possibility to extend joblib's
+    memory pattern with other types of storage such as cloud storage (S3, GCS,
+    OpenStack, HadoopFS, etc) or blob DBs.
+
+    Parameters
+    ----------
+    backend_name: str
+        The name identifying the store backend being registered. For example,
+        'local' is used with FileSystemStoreBackend.
+    backend: StoreBackendBase subclass
+        The name of a class that implements the StoreBackendBase interface.
 
-    See code of_cache_key_to_dir() for details
     """
+    if not isinstance(backend_name, _basestring):
+        raise ValueError("Store backend name should be a string, "
+                         "'{0}' given.".format(backend_name))
+    if backend is None or not issubclass(backend, StoreBackendBase):
+        raise ValueError("Store backend should inherit "
+                         "StoreBackendBase, "
+                         "'{0}' given.".format(backend))
+
+    _STORE_BACKENDS[backend_name] = backend
+
+
+def _store_backend_factory(backend, location, verbose=0, backend_options={}):
+    """Return the correct store object for the given location."""
+    if isinstance(location, StoreBackendBase):
+        return location
+    elif isinstance(location, _basestring):
+        obj = None
+        location = os.path.expanduser(location)
+        # The location is not a local file system, we look in the
+        # registered backends if there's one matching the given backend
+        # name.
+        for backend_key, backend_obj in _STORE_BACKENDS.items():
+            if backend == backend_key:
+                obj = backend_obj()
+
+        # By default, we assume the FileSystemStoreBackend can be used if no
+        # matching backend could be found.
+        if obj is None:
+            raise TypeError('Unknown location {0} or backend {1}'.format(
+                            location, backend))
+
+        # The store backend is configured with the extra named parameters,
+        # some of them are specific to the underlying store backend.
+        obj.configure(location, verbose=verbose,
+                      backend_options=backend_options)
+        return obj
+
+    return None
+
+
+def _get_func_fullname(func):
+    """Compute the part of part associated with a function."""
     modules, funcname = get_func_name(func)
     modules.append(funcname)
     return os.path.join(*modules)
 
 
-def _cache_key_to_dir(cachedir, func, argument_hash):
-    """Compute directory associated with a given cache key.
-
-    func can be a function or a string as returned by _get_func_fullname().
-    """
-    parts = [cachedir]
+def _build_func_identifier(func):
+    """Build a roughly unique identifier for the cached function."""
+    parts = []
     if isinstance(func, _basestring):
         parts.append(func)
     else:
         parts.append(_get_func_fullname(func))
 
-    if argument_hash is not None:
-        parts.append(argument_hash)
+    # We reuse historical fs-like way of building a function identifier
     return os.path.join(*parts)
 
 
-def _load_output(output_dir, func_name, timestamp=None, metadata=None,
-                 mmap_mode=None, verbose=0):
-    """Load output of a computation."""
-    if verbose > 1:
-        signature = ""
-        try:
-            if metadata is not None:
-                args = ", ".join(['%s=%s' % (name, value)
-                                  for name, value
-                                  in metadata['input_args'].items()])
-                signature = "%s(%s)" % (os.path.basename(func_name),
-                                             args)
-            else:
-                signature = os.path.basename(func_name)
-        except KeyError:
-            pass
-
-        if timestamp is not None:
-            t = "% 16s" % format_time(time.time() - timestamp)
-        else:
-            t = ""
-
-        if verbose < 10:
-            print('[Memory]%s: Loading %s...' % (t, str(signature)))
+def _format_load_msg(func_id, args_id, timestamp=None, metadata=None):
+    """ Helper function to format the message when loading the results.
+    """
+    signature = ""
+    try:
+        if metadata is not None:
+            args = ", ".join(['%s=%s' % (name, value)
+                              for name, value
+                              in metadata['input_args'].items()])
+            signature = "%s(%s)" % (os.path.basename(func_id), args)
         else:
-            print('[Memory]%s: Loading %s from %s' % (
-                    t, str(signature), output_dir))
-
-    filename = os.path.join(output_dir, 'output.pkl')
-    if not os.path.isfile(filename):
-        raise KeyError(
-            "Non-existing cache value (may have been cleared).\n"
-            "File %s does not exist" % filename)
-    result = numpy_pickle.load(filename, mmap_mode=mmap_mode)
-
-    return result
-
-
-def _get_cache_items(root_path):
-    """Get cache information for reducing the size of the cache."""
-    cache_items = []
-
-    for dirpath, dirnames, filenames in os.walk(root_path):
-        is_cache_hash_dir = re.match('[a-f0-9]{32}', os.path.basename(dirpath))
-
-        if is_cache_hash_dir:
-            output_filename = os.path.join(dirpath, 'output.pkl')
-            try:
-                last_access = os.path.getatime(output_filename)
-            except OSError:
-                try:
-                    last_access = os.path.getatime(dirpath)
-                except OSError:
-                    # The directory has already been deleted
-                    continue
-
-            last_access = datetime.datetime.fromtimestamp(last_access)
-            try:
-                full_filenames = [os.path.join(dirpath, fn)
-                                  for fn in filenames]
-                dirsize = sum(os.path.getsize(fn)
-                              for fn in full_filenames)
-            except OSError:
-                # Either output_filename or one of the files in
-                # dirpath does not exist any more. We assume this
-                # directory is being cleaned by another process already
-                continue
-
-            cache_items.append(CacheItemInfo(dirpath, dirsize, last_access))
-
-    return cache_items
-
-
-def _get_cache_items_to_delete(root_path, bytes_limit):
-    """Get cache items to delete to keep the cache under a size limit."""
-    if isinstance(bytes_limit, _basestring):
-        bytes_limit = memstr_to_bytes(bytes_limit)
-
-    cache_items = _get_cache_items(root_path)
-    cache_size = sum(item.size for item in cache_items)
-
-    to_delete_size = cache_size - bytes_limit
-    if to_delete_size < 0:
-        return []
-
-    # We want to delete first the cache items that were accessed a
-    # long time ago
-    cache_items.sort(key=operator.attrgetter('last_access'))
-
-    cache_items_to_delete = []
-    size_so_far = 0
-
-    for item in cache_items:
-        if size_so_far > to_delete_size:
-            break
-
-        cache_items_to_delete.append(item)
-        size_so_far += item.size
-
-    return cache_items_to_delete
-
+            signature = os.path.basename(func_id)
+    except KeyError:
+        pass
 
-def concurrency_safe_write(to_write, filename, write_func):
-    """Writes an object into a file in a concurrency-safe way."""
-    thread_id = id(threading.current_thread())
-    temporary_filename = '{}.thread-{}-pid-{}'.format(
-        filename, thread_id, os.getpid())
-    write_func(to_write, temporary_filename)
-    concurrency_safe_rename(temporary_filename, filename)
+    if timestamp is not None:
+        ts_string = "{0: <16}".format(format_time(time.time() - timestamp))
+    else:
+        ts_string = ""
+    return '[Memory]{0}: Loading {1}'.format(ts_string, str(signature))
 
 
 # An in-memory store to avoid looking at the disk-based function
@@ -225,79 +179,86 @@ class MemorizedResult(Logger):
 
     Attributes
     ----------
-    cachedir: string
-        path to root of joblib cache
+    location: str
+        The location of joblib cache. Depends on the store backend used.
 
-    func: function or string
+    func: function or str
         function whose output is cached. The string case is intended only for
         instanciation based on the output of repr() on another instance.
         (namely eval(repr(memorized_instance)) works).
 
-    argument_hash: string
-        hash of the function arguments
+    argument_hash: str
+        hash of the function arguments.
+
+    backend: str
+        Type of store backend for reading/writing cache files.
+        Default is 'local'.
 
     mmap_mode: {None, 'r+', 'r', 'w+', 'c'}
         The memmapping mode used when loading from cache numpy arrays. See
         numpy.load for the meaning of the different values.
 
     verbose: int
-        verbosity level (0 means no message)
+        verbosity level (0 means no message).
 
     timestamp, metadata: string
-        for internal use only
+        for internal use only.
     """
-    def __init__(self, cachedir, func, argument_hash,
+    def __init__(self, location, func, args_id, backend='local',
                  mmap_mode=None, verbose=0, timestamp=None, metadata=None):
         Logger.__init__(self)
-        if isinstance(func, _basestring):
-            self.func = func
-        else:
-            self.func = _get_func_fullname(func)
-        self.argument_hash = argument_hash
-        self.cachedir = cachedir
+        self.func = func
+        self.func_id = _build_func_identifier(func)
+        self.args_id = args_id
+        self.store_backend = _store_backend_factory(backend, location,
+                                                    verbose=verbose)
         self.mmap_mode = mmap_mode
 
-        self._output_dir = _cache_key_to_dir(cachedir, self.func,
-                                             argument_hash)
-
         if metadata is not None:
             self.metadata = metadata
         else:
-            self.metadata = {}
-            # No error is relevant here.
-            try:
-                with open(os.path.join(self._output_dir, 'metadata.json'),
-                          'rb') as f:
-                    self.metadata = json.load(f)
-            except:
-                pass
+            self.metadata = self.store_backend.get_metadata(
+                [self.func_id, self.args_id])
 
         self.duration = self.metadata.get('duration', None)
         self.verbose = verbose
         self.timestamp = timestamp
 
+    @property
+    def argument_hash(self):
+        warnings.warn(
+            "The 'argument_hash' attribute has been deprecated in version "
+            "0.12 and will be removed in version 0.14.\n"
+            "Use `args_id` attribute instead.",
+            DeprecationWarning, stacklevel=2)
+        return self.args_id
+
     def get(self):
         """Read value from cache and return it."""
-        return _load_output(self._output_dir, _get_func_fullname(self.func),
-                            timestamp=self.timestamp,
-                            metadata=self.metadata, mmap_mode=self.mmap_mode,
-                            verbose=self.verbose)
+        if self.verbose:
+            msg = _format_load_msg(self.func_id, self.args_id,
+                                   timestamp=self.timestamp,
+                                   metadata=self.metadata)
+        else:
+            msg = None
+        return self.store_backend.load_item(
+            [self.func_id, self.args_id], msg=msg, verbose=self.verbose)
 
     def clear(self):
         """Clear value from cache"""
-        shutil.rmtree(self._output_dir, ignore_errors=True)
+        self.store_backend.clear_item([self.func_id, self.args_id])
 
     def __repr__(self):
-        return ('{class_name}(cachedir="{cachedir}", func="{func}", '
-                'argument_hash="{argument_hash}")'.format(
-                    class_name=self.__class__.__name__,
-                    cachedir=self.cachedir,
-                    func=self.func,
-                    argument_hash=self.argument_hash
-                    ))
-
+        return ('{class_name}(location="{location}", func="{func}", '
+                'args_id="{args_id}")'
+                .format(class_name=self.__class__.__name__,
+                        location=self.store_backend,
+                        func=self.func,
+                        args_id=self.args_id
+                        ))
     def __reduce__(self):
-        return (self.__class__, (self.cachedir, self.func, self.argument_hash),
+        return (self.__class__,
+                (self.store_backend, self.func, self.args_id),
                 {'mmap_mode': self.mmap_mode})
 
 
@@ -324,10 +285,9 @@ def clear(self):
 
     def __repr__(self):
         if self.valid:
-            return '{class_name}({value})'.format(
-                class_name=self.__class__.__name__,
-                value=pformat(self.value)
-                )
+            return ('{class_name}({value})'
+                    .format(class_name=self.__class__.__name__,
+                            value=pformat(self.value)))
         else:
             return self.__class__.__name__ + ' with no value'
 
@@ -368,10 +328,7 @@ def __reduce__(self):
         return (self.__class__, (self.func,))
 
     def __repr__(self):
-        return '%s(func=%s)' % (
-                    self.__class__.__name__,
-                    self.func
-            )
+        return '{0}(func={1})'.format(self.__class__.__name__, self.func)
 
     def clear(self, warn=True):
         # Argument "warn" is for compatibility with MemorizedFunc.clear
@@ -382,87 +339,73 @@ def clear(self, warn=True):
 # class `MemorizedFunc`
 ###############################################################################
 class MemorizedFunc(Logger):
-    """ Callable object decorating a function for caching its return value
-        each time it is called.
-
-        All values are cached on the filesystem, in a deep directory
-        structure. Methods are provided to inspect the cache or clean it.
+    """Callable object decorating a function for caching its return value
+    each time it is called.
 
-        Attributes
-        ----------
-        func: callable
-            The original, undecorated, function.
+    Methods are provided to inspect the cache or clean it.
 
-        cachedir: string
-            Path to the base cache directory of the memory context.
+    Attributes
+    ----------
+    func: callable
+        The original, undecorated, function.
 
-        ignore: list or None
-            List of variable names to ignore when choosing whether to
-            recompute.
+    location: string
+        The location of joblib cache. Depends on the store backend used.
 
-        mmap_mode: {None, 'r+', 'r', 'w+', 'c'}
-            The memmapping mode used when loading from cache
-            numpy arrays. See numpy.load for the meaning of the different
-            values.
+    backend: str
+        Type of store backend for reading/writing cache files.
+        Default is 'local', in which case the location is the path to a
+        disk storage.
 
-        compress: boolean, or integer
-            Whether to zip the stored data on disk. If an integer is
-            given, it should be between 1 and 9, and sets the amount
-            of compression. Note that compressed arrays cannot be
-            read by memmapping.
+    ignore: list or None
+        List of variable names to ignore when choosing whether to
+        recompute.
 
-        verbose: int, optional
-            The verbosity flag, controls messages that are issued as
-            the function is evaluated.
+    mmap_mode: {None, 'r+', 'r', 'w+', 'c'}
+        The memmapping mode used when loading from cache
+        numpy arrays. See numpy.load for the meaning of the different
+        values.
+
+    compress: boolean, or integer
+        Whether to zip the stored data on disk. If an integer is
+        given, it should be between 1 and 9, and sets the amount
+        of compression. Note that compressed arrays cannot be
+        read by memmapping.
+
+    verbose: int, optional
+        The verbosity flag, controls messages that are issued as
+        the function is evaluated.
     """
-    #-------------------------------------------------------------------------
+    # ------------------------------------------------------------------------
     # Public interface
-    #-------------------------------------------------------------------------
+    # ------------------------------------------------------------------------
 
-    def __init__(self, func, cachedir, ignore=None, mmap_mode=None,
-                 compress=False, verbose=1, timestamp=None):
-        """
-            Parameters
-            ----------
-            func: callable
-                The function to decorate
-            cachedir: string
-                The path of the base directory to use as a data store
-            ignore: list or None
-                List of variable names to ignore.
-            mmap_mode: {None, 'r+', 'r', 'w+', 'c'}, optional
-                The memmapping mode used when loading from cache
-                numpy arrays. See numpy.load for the meaning of the
-                arguments.
-            compress : boolean, or integer
-                Whether to zip the stored data on disk. If an integer is
-                given, it should be between 1 and 9, and sets the amount
-                of compression. Note that compressed arrays cannot be
-                read by memmapping.
-            verbose: int, optional
-                Verbosity flag, controls the debug messages that are issued
-                as functions are evaluated. The higher, the more verbose
-            timestamp: float, optional
-                The reference time from which times in tracing messages
-                are reported.
-        """
+    def __init__(self, func, location, backend='local', ignore=None,
+                 mmap_mode=None, compress=False, verbose=1, timestamp=None):
         Logger.__init__(self)
         self.mmap_mode = mmap_mode
+        self.compress = compress
         self.func = func
         if ignore is None:
             ignore = []
         self.ignore = ignore
-
         self._verbose = verbose
-        self.cachedir = cachedir
-        self.compress = compress
-        if compress and self.mmap_mode is not None:
-            warnings.warn('Compressed results cannot be memmapped',
-                          stacklevel=2)
+
+        # retrieve store object from backend type and location.
+        self.store_backend = _store_backend_factory(backend, location,
+                                                    verbose=verbose,
+                                                    backend_options=dict(
+                                                        compress=compress,
+                                                        mmap_mode=mmap_mode),
+                                                    )
+        if self.store_backend is not None:
+            # Create func directory on demand.
+            self.store_backend.\
+                store_cached_func_code([_build_func_identifier(self.func)])
+
         if timestamp is None:
             timestamp = time.time()
         self.timestamp = timestamp
-        mkdirp(self.cachedir)
         try:
             functools.update_wrapper(self, func)
         except:
@@ -496,32 +439,38 @@ def _cached_call(self, args, kwargs):
         """
         # Compare the function code with the previous to see if the
         # function code has changed
-        output_dir, argument_hash = self._get_output_dir(*args, **kwargs)
+        func_id, args_id = self._get_output_identifiers(*args, **kwargs)
         metadata = None
-        output_pickle_path = os.path.join(output_dir, 'output.pkl')
+        msg = None
         # FIXME: The statements below should be try/excepted
         if not (self._check_previous_func_code(stacklevel=4) and
-                os.path.isfile(output_pickle_path)):
+                self.store_backend.contains_item([func_id, args_id])):
             if self._verbose > 10:
                 _, name = get_func_name(self.func)
-                self.warn('Computing func %s, argument hash %s in '
-                          'directory %s'
-                        % (name, argument_hash, output_dir))
+                self.warn('Computing func {0}, argument hash {1} '
+                          'in location {2}'
+                          .format(name, args_id,
+                                  self.store_backend.
+                                  get_cached_func_info([func_id])['location']))
             out, metadata = self.call(*args, **kwargs)
             if self.mmap_mode is not None:
                 # Memmap the output at the first call to be consistent with
                 # later calls
-                out = _load_output(output_dir, _get_func_fullname(self.func),
-                                   timestamp=self.timestamp,
-                                   mmap_mode=self.mmap_mode,
-                                   verbose=self._verbose)
+                if self._verbose:
+                    msg = _format_load_msg(func_id, args_id,
+                                           timestamp=self.timestamp,
+                                           metadata=metadata)
+                out = self.store_backend.load_item([func_id, args_id], msg=msg,
+                                                   verbose=self._verbose)
         else:
             try:
                 t0 = time.time()
-                out = _load_output(output_dir, _get_func_fullname(self.func),
-                                   timestamp=self.timestamp,
-                                   metadata=metadata, mmap_mode=self.mmap_mode,
-                                   verbose=self._verbose)
+                if self._verbose:
+                    msg = _format_load_msg(func_id, args_id,
+                                           timestamp=self.timestamp,
+                                           metadata=metadata)
+                out = self.store_backend.load_item([func_id, args_id], msg=msg,
+                                                   verbose=self._verbose)
                 if self._verbose > 4:
                     t = time.time() - t0
                     _, name = get_func_name(self.func)
@@ -531,11 +480,12 @@ def _cached_call(self, args, kwargs):
                 # XXX: Should use an exception logger
                 _, signature = format_signature(self.func, *args, **kwargs)
                 self.warn('Exception while loading results for '
-                          '{}\n {}'.format(
-                              signature, traceback.format_exc()))
+                          '{}\n {}'.format(signature, traceback.format_exc()))
+
                 out, metadata = self.call(*args, **kwargs)
-                argument_hash = None
-        return (out, argument_hash, metadata)
+                args_id = None
+
+        return (out, args_id, metadata)
 
     def call_and_shelve(self, *args, **kwargs):
         """Call wrapped function, cache result and return a reference.
@@ -550,13 +500,12 @@ def call_and_shelve(self, *args, **kwargs):
         cached_result: MemorizedResult or NotMemorizedResult
             reference to the value returned by the wrapped function. The
             class "NotMemorizedResult" is used when there is no cache
-            activated (e.g. cachedir=None in Memory).
+            activated (e.g. location=None in Memory).
         """
-        _, argument_hash, metadata = self._cached_call(args, kwargs)
-
-        return MemorizedResult(self.cachedir, self.func, argument_hash,
-            metadata=metadata, verbose=self._verbose - 1,
-            timestamp=self.timestamp)
+        _, args_id, metadata = self._cached_call(args, kwargs)
+        return MemorizedResult(self.store_backend, self.func, args_id,
+                               metadata=metadata, verbose=self._verbose - 1,
+                               timestamp=self.timestamp)
 
     def __call__(self, *args, **kwargs):
         return self._cached_call(args, kwargs)[0]
@@ -566,44 +515,30 @@ def __reduce__(self):
             depending from it.
             In addition, when unpickling, we run the __init__
         """
-        return (self.__class__, (self.func, self.cachedir, self.ignore,
-                self.mmap_mode, self.compress, self._verbose))
+        return (self.__class__, (self.func, None),
+                {k: v for k, v in vars(self).items()
+                 if k not in ('timestamp', 'func')})
 
-    #-------------------------------------------------------------------------
+    # ------------------------------------------------------------------------
     # Private interface
-    #-------------------------------------------------------------------------
+    # ------------------------------------------------------------------------
 
     def _get_argument_hash(self, *args, **kwargs):
-        return hashing.hash(filter_args(self.func, self.ignore,
-                                         args, kwargs),
-                             coerce_mmap=(self.mmap_mode is not None))
+        return hashing.hash(filter_args(self.func, self.ignore, args, kwargs),
+                            coerce_mmap=(self.mmap_mode is not None))
 
-    def _get_output_dir(self, *args, **kwargs):
-        """ Return the directory in which are persisted the result
-            of the function called with the given arguments.
-        """
+    def _get_output_identifiers(self, *args, **kwargs):
+        """Return the func identifier and input parameter hash of a result."""
+        func_id = _build_func_identifier(self.func)
         argument_hash = self._get_argument_hash(*args, **kwargs)
-        output_dir = os.path.join(self._get_func_dir(self.func),
-                                  argument_hash)
-        return output_dir, argument_hash
-
-    get_output_dir = _get_output_dir  # backward compatibility
-
-    def _get_func_dir(self, mkdir=True):
-        """ Get the directory corresponding to the cache for the
-            function.
-        """
-        func_dir = _cache_key_to_dir(self.cachedir, self.func, None)
-        if mkdir:
-            mkdirp(func_dir)
-        return func_dir
+        return func_id, argument_hash
 
     def _hash_func(self):
         """Hash a function to key the online cache"""
         func_code_h = hash(getattr(self.func, '__code__', None))
         return id(self.func), hash(self.func), func_code_h
 
-    def _write_func_code(self, filename, func_code, first_line):
+    def _write_func_code(self, func_code, first_line):
         """ Write the function code and the filename to a file.
         """
         # We store the first line because the filename and the function
@@ -611,17 +546,18 @@ def _write_func_code(self, filename, func_code, first_line):
         # sometimes have several functions named the same way in a
         # file. This is bad practice, but joblib should be robust to bad
         # practice.
+        func_id = _build_func_identifier(self.func)
         func_code = u'%s %i\n%s' % (FIRST_LINE_TEXT, first_line, func_code)
-        with io.open(filename, 'w', encoding="UTF-8") as out:
-            out.write(func_code)
+        self.store_backend.store_cached_func_code([func_id], func_code)
+
         # Also store in the in-memory store of function hashes
         is_named_callable = False
         if PY3_OR_LATER:
-            is_named_callable = (hasattr(self.func, '__name__')
-                                 and self.func.__name__ != '<lambda>')
+            is_named_callable = (hasattr(self.func, '__name__') and
+                                 self.func.__name__ != '<lambda>')
         else:
-            is_named_callable = (hasattr(self.func, 'func_name')
-                                 and self.func.func_name != '<lambda>')
+            is_named_callable = (hasattr(self.func, 'func_name') and
+                                 self.func.func_name != '<lambda>')
         if is_named_callable:
             # Don't do this for lambda functions or strange callable
             # objects, as it ends up being too fragile
@@ -657,15 +593,14 @@ def _check_previous_func_code(self, stacklevel=2):
         # changing code and collision. We cannot inspect.getsource
         # because it is not reliable when using IPython's magic "%run".
         func_code, source_file, first_line = get_func_code(self.func)
-        func_dir = self._get_func_dir()
-        func_code_file = os.path.join(func_dir, 'func_code.py')
+        func_id = _build_func_identifier(self.func)
 
         try:
-            with io.open(func_code_file, encoding="UTF-8") as infile:
-                old_func_code, old_first_line = \
-                            extract_first_line(infile.read())
-        except IOError:
-                self._write_func_code(func_code_file, func_code, first_line)
+            old_func_code, old_first_line =\
+                extract_first_line(
+                    self.store_backend.get_cached_func_code([func_id]))
+        except (IOError, OSError):  # some backend can also raise OSError
+                self._write_func_code(func_code, first_line)
                 return False
         if old_func_code == func_code:
             return True
@@ -678,13 +613,14 @@ def _check_previous_func_code(self, stacklevel=2):
                                      win_characters=False)
         if old_first_line == first_line == -1 or func_name == '<lambda>':
             if not first_line == -1:
-                func_description = '%s (%s:%i)' % (func_name,
-                                                source_file, first_line)
+                func_description = ("{0} ({1}:{2})"
+                                    .format(func_name, source_file,
+                                            first_line))
             else:
                 func_description = func_name
             warnings.warn(JobLibCollisionWarning(
-                "Cannot detect name collisions for function '%s'"
-                        % func_description), stacklevel=stacklevel)
+                "Cannot detect name collisions for function '{0}'"
+                .format(func_description)), stacklevel=stacklevel)
 
         # Fetch the code at the old location and compare it. If it is the
         # same than the code store, we have a collision: the code in the
@@ -699,52 +635,52 @@ def _check_previous_func_code(self, stacklevel=2):
                     on_disk_func_code = f.readlines()[
                         old_first_line - 1:old_first_line - 1 + num_lines - 1]
                 on_disk_func_code = ''.join(on_disk_func_code)
-                possible_collision = (on_disk_func_code.rstrip()
-                                      == old_func_code.rstrip())
+                possible_collision = (on_disk_func_code.rstrip() ==
+                                      old_func_code.rstrip())
             else:
                 possible_collision = source_file.startswith('<doctest ')
             if possible_collision:
                 warnings.warn(JobLibCollisionWarning(
-                        'Possible name collisions between functions '
-                        "'%s' (%s:%i) and '%s' (%s:%i)" %
-                        (func_name, source_file, old_first_line,
-                        func_name, source_file, first_line)),
-                                stacklevel=stacklevel)
+                    'Possible name collisions between functions '
+                    "'%s' (%s:%i) and '%s' (%s:%i)" %
+                    (func_name, source_file, old_first_line,
+                     func_name, source_file, first_line)),
+                    stacklevel=stacklevel)
 
         # The function has changed, wipe the cache directory.
         # XXX: Should be using warnings, and giving stacklevel
         if self._verbose > 10:
             _, func_name = get_func_name(self.func, resolv_alias=False)
-            self.warn("Function %s (stored in %s) has changed." %
-                        (func_name, func_dir))
+            self.warn("Function {0} (identified by {1}) has changed"
+                      ".".format(func_name, func_id))
         self.clear(warn=True)
         return False
 
     def clear(self, warn=True):
-        """ Empty the function's cache.
-        """
-        func_dir = self._get_func_dir(mkdir=False)
+        """Empty the function's cache."""
+        func_id = _build_func_identifier(self.func)
+
         if self._verbose > 0 and warn:
-            self.warn("Clearing cache %s" % func_dir)
-        if os.path.exists(func_dir):
-            shutil.rmtree(func_dir, ignore_errors=True)
-        mkdirp(func_dir)
+            self.warn("Clearing function cache identified by %s" % func_id)
+        self.store_backend.clear_path([func_id, ])
+
         func_code, _, first_line = get_func_code(self.func)
-        func_code_file = os.path.join(func_dir, 'func_code.py')
-        self._write_func_code(func_code_file, func_code, first_line)
+        self._write_func_code(func_code, first_line)
 
     def call(self, *args, **kwargs):
         """ Force the execution of the function with the given arguments and
             persist the output values.
         """
         start_time = time.time()
-        output_dir, _ = self._get_output_dir(*args, **kwargs)
+        func_id, args_id = self._get_output_identifiers(*args, **kwargs)
         if self._verbose > 0:
             print(format_call(self.func, args, kwargs))
         output = self.func(*args, **kwargs)
-        self._persist_output(output, output_dir)
+        self.store_backend.dump_item(
+            [func_id, args_id], output, verbose=self._verbose)
+
         duration = time.time() - start_time
-        metadata = self._persist_input(output_dir, duration, args, kwargs)
+        metadata = self._persist_input(duration, args, kwargs)
 
         if self._verbose > 0:
             _, name = get_func_name(self.func)
@@ -752,23 +688,7 @@ def call(self, *args, **kwargs):
             print(max(0, (80 - len(msg))) * '_' + msg)
         return output, metadata
 
-    # Make public
-    def _persist_output(self, output, dir):
-        """ Persist the given output tuple in the directory.
-        """
-        try:
-            filename = os.path.join(dir, 'output.pkl')
-            mkdirp(dir)
-            write_func = functools.partial(numpy_pickle.dump,
-                                           compress=self.compress)
-            concurrency_safe_write(output, filename, write_func)
-            if self._verbose > 10:
-                print('Persisting in %s' % dir)
-        except OSError:
-            " Race condition in the creation of the directory "
-
-    def _persist_input(self, output_dir, duration, args, kwargs,
-                       this_duration_limit=0.5):
+    def _persist_input(self, duration, args, kwargs, this_duration_limit=0.5):
         """ Save a small summary of the call using json format in the
             output directory.
 
@@ -793,17 +713,9 @@ def _persist_input(self, output_dir, duration, args, kwargs,
         # This can fail due to race-conditions with multiple
         # concurrent joblibs removing the file or the directory
         metadata = {"duration": duration, "input_args": input_repr}
-        try:
-            mkdirp(output_dir)
-            filename = os.path.join(output_dir, 'metadata.json')
 
-            def write_func(output, dest_filename):
-                with open(dest_filename, 'w') as f:
-                    json.dump(output, f)
-
-            concurrency_safe_write(metadata, filename, write_func)
-        except Exception:
-            pass
+        func_id, args_id = self._get_output_identifiers(*args, **kwargs)
+        self.store_backend.store_metadata([func_id, args_id], metadata)
 
         this_duration = time.time() - start_time
         if this_duration > this_duration_limit:
@@ -828,17 +740,14 @@ def write_func(output, dest_filename):
 
     # XXX: Need a method to check if results are available.
 
-
-    #-------------------------------------------------------------------------
+    # ------------------------------------------------------------------------
     # Private `object` interface
-    #-------------------------------------------------------------------------
+    # ------------------------------------------------------------------------
 
     def __repr__(self):
-        return '%s(func=%s, cachedir=%s)' % (
-                    self.__class__.__name__,
-                    self.func,
-                    repr(self.cachedir),
-                    )
+        return ("{0}(func={1}, location={2})".format(self.__class__.__name__,
+                                                     self.func,
+                                                     self.store_backend,))
 
 
 ###############################################################################
@@ -851,54 +760,105 @@ class Memory(Logger):
         All values are cached on the filesystem, in a deep directory
         structure.
 
-        see :ref:`memory_reference`
+        Read more in the :ref:`User Guide <memory>`.
+
+        Parameters
+        ----------
+        location: str or None
+            The path of the base directory to use as a data store
+            or None. If None is given, no caching is done and
+            the Memory object is completely transparent. This option
+            replaces cachedir since version 0.12.
+
+        backend: str, optional
+            Type of store backend for reading/writing cache files.
+            Default: 'local'.
+            The 'local' backend is using regular filesystem operations to
+            manipulate data (open, mv, etc) in the backend.
+
+        mmap_mode: {None, 'r+', 'r', 'w+', 'c'}, optional
+            The memmapping mode used when loading from cache
+            numpy arrays. See numpy.load for the meaning of the
+            arguments.
+
+        compress: boolean, or integer, optional
+            Whether to zip the stored data on disk. If an integer is
+            given, it should be between 1 and 9, and sets the amount
+            of compression. Note that compressed arrays cannot be
+            read by memmapping.
+
+        verbose: int, optional
+            Verbosity flag, controls the debug messages that are issued
+            as functions are evaluated.
+
+        bytes_limit: int, optional
+            Limit in bytes of the size of the cache.
+
+        backend_options: dict, optional
+            Contains a dictionnary of named parameters used to configure
+            the store backend.
+
+        cachedir: str or None, optional
+
+            .. deprecated: 0.12
+                'cachedir' has been deprecated in 0.12 and will be
+                removed in 0.14. Use the 'location' parameter instead.
     """
-    #-------------------------------------------------------------------------
+    # ------------------------------------------------------------------------
     # Public interface
-    #-------------------------------------------------------------------------
+    # ------------------------------------------------------------------------
 
-    def __init__(self, cachedir, mmap_mode=None, compress=False, verbose=1,
-                 bytes_limit=None):
-        """
-            Parameters
-            ----------
-            cachedir: string or None
-                The path of the base directory to use as a data store
-                or None. If None is given, no caching is done and
-                the Memory object is completely transparent.
-            mmap_mode: {None, 'r+', 'r', 'w+', 'c'}, optional
-                The memmapping mode used when loading from cache
-                numpy arrays. See numpy.load for the meaning of the
-                arguments.
-            compress: boolean, or integer
-                Whether to zip the stored data on disk. If an integer is
-                given, it should be between 1 and 9, and sets the amount
-                of compression. Note that compressed arrays cannot be
-                read by memmapping.
-            verbose: int, optional
-                Verbosity flag, controls the debug messages that are issued
-                as functions are evaluated.
-            bytes_limit: int, optional
-                Limit in bytes of the size of the cache
-        """
+    def __init__(self, location=None, backend='local', mmap_mode=None,
+                 compress=False, verbose=1, bytes_limit=None,
+                 backend_options={}, cachedir=None):
         # XXX: Bad explanation of the None value of cachedir
         Logger.__init__(self)
         self._verbose = verbose
         self.mmap_mode = mmap_mode
         self.timestamp = time.time()
-        self.compress = compress
         self.bytes_limit = bytes_limit
+        self.backend = backend
         if compress and mmap_mode is not None:
             warnings.warn('Compressed results cannot be memmapped',
                           stacklevel=2)
-        if cachedir is None:
-            self.cachedir = None
-        else:
-            self.cachedir = os.path.join(cachedir, 'joblib')
-            mkdirp(self.cachedir)
-
-    def cache(self, func=None, ignore=None, verbose=None,
-                        mmap_mode=False):
+        if cachedir is not None:
+            if location is not None:
+                raise ValueError(
+                    'You set both "location={0!r} and "cachedir={1!r}". '
+                    "'cachedir' has been deprecated in version "
+                    "0.12 and will be removed in version 0.14.\n"
+                    'Please only set "location={0!r}"'.format(
+                        location, cachedir))
+
+            warnings.warn(
+                "The 'cachedir' parameter has been deprecated in version "
+                "0.12 and will be removed in version 0.14.\n"
+                'You provided "cachedir={0!r}", '
+                'use "location={0!r}" instead.'.format(cachedir),
+                DeprecationWarning, stacklevel=2)
+            location = cachedir
+
+        self.location = location
+        if isinstance(location, _basestring):
+            location = os.path.join(location, 'joblib')
+
+        self.store_backend = _store_backend_factory(
+            backend, location, verbose=self._verbose,
+            backend_options=dict(compress=compress, mmap_mode=mmap_mode,
+                                 **backend_options))
+
+    @property
+    def cachedir(self):
+        warnings.warn(
+            "The 'cachedir' attribute has been deprecated in version 0.12 "
+            "and will be removed in version 0.14.\n"
+            "Use os.path.join(memory.location, 'joblib') attribute instead.",
+            DeprecationWarning, stacklevel=2)
+        if self.location is None:
+            return None
+        return os.path.join(self.location, 'joblib')
+
+    def cache(self, func=None, ignore=None, verbose=None, mmap_mode=False):
         """ Decorates the given function func to only compute its return
             value for input arguments not cached on disk.
 
@@ -929,7 +889,7 @@ def cache(self, func=None, ignore=None, verbose=None,
             # arguments in decorators
             return functools.partial(self.cache, ignore=ignore,
                                      verbose=verbose, mmap_mode=mmap_mode)
-        if self.cachedir is None:
+        if self.store_backend is None:
             return NotMemorizedFunc(func)
         if verbose is None:
             verbose = self._verbose
@@ -937,38 +897,22 @@ def cache(self, func=None, ignore=None, verbose=None,
             mmap_mode = self.mmap_mode
         if isinstance(func, MemorizedFunc):
             func = func.func
-        return MemorizedFunc(func, cachedir=self.cachedir,
-                                   mmap_mode=mmap_mode,
-                                   ignore=ignore,
-                                   compress=self.compress,
-                                   verbose=verbose,
-                                   timestamp=self.timestamp)
+        return MemorizedFunc(func, self.store_backend, mmap_mode=mmap_mode,
+                             ignore=ignore, verbose=verbose,
+                             timestamp=self.timestamp)
 
     def clear(self, warn=True):
         """ Erase the complete cache directory.
         """
         if warn:
             self.warn('Flushing completely the cache')
-        if self.cachedir is not None:
-            rm_subdirs(self.cachedir)
+        if self.store_backend is not None:
+            self.store_backend.clear()
 
     def reduce_size(self):
-        """Remove cache folders to make cache size fit in ``bytes_limit``."""
-        if self.cachedir is not None and self.bytes_limit is not None:
-            cache_items_to_delete = _get_cache_items_to_delete(
-                self.cachedir, self.bytes_limit)
-
-            for cache_item in cache_items_to_delete:
-                if self._verbose > 10:
-                    print('Deleting cache item {}'.format(cache_item))
-                try:
-                    shutil.rmtree(cache_item.path, ignore_errors=True)
-                except OSError:
-                    # Even with ignore_errors=True can shutil.rmtree
-                    # can raise OSErrror with [Errno 116] Stale file
-                    # handle if another process has deleted the folder
-                    # already.
-                    pass
+        """Remove cache elements to make cache size fit in ``bytes_limit``."""
+        if self.bytes_limit is not None and self.store_backend is not None:
+            self.store_backend.reduce_store_size(self.bytes_limit)
 
     def eval(self, func, *args, **kwargs):
         """ Eval function func with arguments `*args` and `**kwargs`,
@@ -979,26 +923,23 @@ def eval(self, func, *args, **kwargs):
             up to date.
 
         """
-        if self.cachedir is None:
+        if self.store_backend is None:
             return func(*args, **kwargs)
         return self.cache(func)(*args, **kwargs)
 
-    #-------------------------------------------------------------------------
+    # ------------------------------------------------------------------------
     # Private `object` interface
-    #-------------------------------------------------------------------------
+    # ------------------------------------------------------------------------
 
     def __repr__(self):
-        return '%s(cachedir=%s)' % (
-                    self.__class__.__name__,
-                    repr(self.cachedir),
-                    )
+        return '{0}(location={1})'.format(
+            self.__class__.__name__, (repr(None) if self.store_backend is None
+                                      else repr(self.store_backend)))
 
     def __reduce__(self):
         """ We don't store the timestamp when pickling, to avoid the hash
             depending from it.
             In addition, when unpickling, we run the __init__
         """
-        # We need to remove 'joblib' from the end of cachedir
-        cachedir = self.cachedir[:-7] if self.cachedir is not None else None
-        return (self.__class__, (cachedir,
-                self.mmap_mode, self.compress, self._verbose))
+        return (self.__class__, (), {k: v for k, v in vars(self).items()
+                                     if k != 'timestamp'})
diff --git a/sklearn/externals/joblib/my_exceptions.py b/sklearn/externals/joblib/my_exceptions.py
index 3bda92f58a..765885e33c 100755
--- a/sklearn/externals/joblib/my_exceptions.py
+++ b/sklearn/externals/joblib/my_exceptions.py
@@ -4,9 +4,9 @@
 # Author: Gael Varoquaux < gael dot varoquaux at normalesup dot org >
 # Copyright: 2010, Gael Varoquaux
 # License: BSD 3 clause
-
 from ._compat import PY3_OR_LATER
 
+
 class JoblibException(Exception):
     """A simple exception with an error message that you can get to."""
     def __init__(self, *args):
@@ -47,6 +47,17 @@ def __init__(self, message, etype):
         self.message = message
         self.etype = etype
 
+    def unwrap(self, context_message=""):
+        report = """\
+%s
+---------------------------------------------------------------------------
+Joblib worker traceback:
+---------------------------------------------------------------------------
+%s""" % (context_message, self.message)
+        # Unwrap the exception to a JoblibException
+        exception_type = _mk_exception(self.etype)[0]
+        return exception_type(report)
+
 
 class WorkerInterrupt(Exception):
     """ An exception that is not KeyboardInterrupt to allow subprocesses
@@ -59,6 +70,10 @@ class WorkerInterrupt(Exception):
 
 
 def _mk_exception(exception, name=None):
+    if issubclass(exception, JoblibException):
+        # No need to wrap recursively JoblibException
+        return exception, exception.__name__
+
     # Create an exception inheriting from both JoblibException
     # and that exception
     if name is None:
diff --git a/sklearn/externals/joblib/numpy_pickle.py b/sklearn/externals/joblib/numpy_pickle.py
index 87a1a616cd..496429f50f 100755
--- a/sklearn/externals/joblib/numpy_pickle.py
+++ b/sklearn/externals/joblib/numpy_pickle.py
@@ -13,8 +13,11 @@
 except ImportError:
     Path = None
 
-from .numpy_pickle_utils import _COMPRESSORS
-from .numpy_pickle_utils import BinaryZlibFile
+from .compressor import lz4, LZ4_NOT_INSTALLED_ERROR
+from .compressor import _COMPRESSORS, register_compressor, BinaryZlibFile
+from .compressor import (ZlibCompressorWrapper, GzipCompressorWrapper,
+                         BZ2CompressorWrapper, LZMACompressorWrapper,
+                         XZCompressorWrapper, LZ4CompressorWrapper)
 from .numpy_pickle_utils import Unpickler, Pickler
 from .numpy_pickle_utils import _read_fileobject, _write_fileobject
 from .numpy_pickle_utils import _read_bytes, BUFFER_SIZE
@@ -28,6 +31,14 @@
 from ._compat import _basestring, PY3_OR_LATER
 from .backports import make_memmap
 
+# Register supported compressors
+register_compressor('zlib', ZlibCompressorWrapper())
+register_compressor('gzip', GzipCompressorWrapper())
+register_compressor('bz2', BZ2CompressorWrapper())
+register_compressor('lzma', LZMACompressorWrapper())
+register_compressor('xz', XZCompressorWrapper())
+register_compressor('lz4', LZ4CompressorWrapper())
+
 ###############################################################################
 # Utility objects for persistence.
 
@@ -209,7 +220,7 @@ class NumpyPickler(Pickler):
     ----------
     fp: file
         File object handle used for serializing the input object.
-    protocol: int
+    protocol: int, optional
         Pickle protocol used. Default is pickle.DEFAULT_PROTOCOL under
         python 3, pickle.HIGHEST_PROTOCOL otherwise.
     """
@@ -353,14 +364,17 @@ def load_build(self):
 def dump(value, filename, compress=0, protocol=None, cache_size=None):
     """Persist an arbitrary Python object into one file.
 
+    Read more in the :ref:`User Guide <persistence>`.
+
     Parameters
     -----------
     value: any Python object
         The object to store to disk.
-    filename: str or pathlib.Path
-        The path of the file in which it is to be stored. The compression
-        method corresponding to one of the supported filename extensions ('.z',
-        '.gz', '.bz2', '.xz' or '.lzma') will be used automatically.
+    filename: str, pathlib.Path, or file object.
+        The file object or path of the file in which it is to be stored.
+        The compression method corresponding to one of the supported filename
+        extensions ('.z', '.gz', '.bz2', '.xz' or '.lzma') will be used
+        automatically.
     compress: int from 0 to 9 or bool or 2-tuple, optional
         Optional compression level for the data. 0 or False is no compression.
         Higher value means more compression, but also slower read and
@@ -371,7 +385,7 @@ def dump(value, filename, compress=0, protocol=None, cache_size=None):
         between supported compressors (e.g 'zlib', 'gzip', 'bz2', 'lzma'
         'xz'), the second element must be an integer from 0 to 9, corresponding
         to the compression level.
-    protocol: positive int
+    protocol: int, optional
         Pickle protocol, see pickle.dump documentation for more details.
     cache_size: positive int, optional
         This option is deprecated in 0.10 and has no effect.
@@ -403,8 +417,9 @@ def dump(value, filename, compress=0, protocol=None, cache_size=None):
 
     compress_method = 'zlib'  # zlib is the default compression method.
     if compress is True:
-        # By default, if compress is enabled, we want to be using 3 by default
-        compress_level = 3
+        # By default, if compress is enabled, we want the default compress
+        # level of the compressor.
+        compress_level = None
     elif isinstance(compress, tuple):
         # a 2-tuple was set in compress
         if len(compress) != 2:
@@ -413,10 +428,19 @@ def dump(value, filename, compress=0, protocol=None, cache_size=None):
                 '(compress method, compress level), you passed {}'
                 .format(compress))
         compress_method, compress_level = compress
+    elif isinstance(compress, _basestring):
+        compress_method = compress
+        compress_level = None  # Use default compress level
+        compress = (compress_method, compress_level)
     else:
         compress_level = compress
 
-    if compress_level is not False and compress_level not in range(10):
+    if compress_method == 'lz4' and lz4 is None:
+        raise ValueError(LZ4_NOT_INSTALLED_ERROR)
+
+    if (compress_level is not None and
+            compress_level is not False and
+            compress_level not in range(10)):
         # Raising an error if a non valid compress level is given.
         raise ValueError(
             'Non valid compress level given: "{}". Possible values are '
@@ -441,25 +465,17 @@ def dump(value, filename, compress=0, protocol=None, cache_size=None):
         # In case no explicit compression was requested using both compression
         # method and level in a tuple and the filename has an explicit
         # extension, we select the corresponding compressor.
-        if filename.endswith('.z'):
-            compress_method = 'zlib'
-        elif filename.endswith('.gz'):
-            compress_method = 'gzip'
-        elif filename.endswith('.bz2'):
-            compress_method = 'bz2'
-        elif filename.endswith('.lzma'):
-            compress_method = 'lzma'
-        elif filename.endswith('.xz'):
-            compress_method = 'xz'
-        else:
-            # no matching compression method found, we unset the variable to
-            # be sure no compression level is set afterwards.
-            compress_method = None
+
+        # unset the variable to be sure no compression level is set afterwards.
+        compress_method = None
+        for name, compressor in _COMPRESSORS.items():
+            if filename.endswith(compressor.extension):
+                compress_method = name
 
         if compress_method in _COMPRESSORS and compress_level == 0:
-            # we choose a default compress_level of 3 in case it was not given
+            # we choose the default compress_level in case it was not given
             # as an argument (using compress).
-            compress_level = 3
+            compress_level = None
 
     if not PY3_OR_LATER and compress_method in ('lzma', 'xz'):
         raise NotImplementedError("{} compression is only available for "
@@ -530,14 +546,16 @@ def _unpickle(fobj, filename="", mmap_mode=None):
 def load(filename, mmap_mode=None):
     """Reconstruct a Python object from a file persisted with joblib.dump.
 
+    Read more in the :ref:`User Guide <persistence>`.
+
     Parameters
     -----------
-    filename: str or pathlib.Path
-        The path of the file from which to load the object
+    filename: str, pathlib.Path, or file object.
+        The file object or path of the file from which to load the object
     mmap_mode: {None, 'r+', 'r', 'w+', 'c'}, optional
         If not None, the arrays are memory-mapped from the disk. This
         mode has no effect for compressed files. Note that in this
-        case the reconstructed object might not longer match exactly
+        case the reconstructed object might no longer match exactly
         the originally pickled object.
 
     Returns
@@ -556,7 +574,7 @@ def load(filename, mmap_mode=None):
     dump. If the mmap_mode argument is given, it is passed to np.load and
     arrays are loaded as memmaps. As a consequence, the reconstructed
     object might not match the original pickled object. Note that if the
-    file was saved with compression, the arrays cannot be memmaped.
+    file was saved with compression, the arrays cannot be memmapped.
     """
     if Path is not None and isinstance(filename, Path):
         filename = str(filename)
diff --git a/sklearn/externals/joblib/numpy_pickle_utils.py b/sklearn/externals/joblib/numpy_pickle_utils.py
index 27d759be77..1ebf1aa61b 100755
--- a/sklearn/externals/joblib/numpy_pickle_utils.py
+++ b/sklearn/externals/joblib/numpy_pickle_utils.py
@@ -5,20 +5,14 @@
 # License: BSD Style, 3 clauses.
 
 import pickle
-import sys
 import io
-import zlib
-import gzip
 import warnings
 import contextlib
 from contextlib import closing
 
-from ._compat import PY3_OR_LATER, PY27, _basestring
-
-try:
-    from threading import RLock
-except ImportError:
-    from dummy_threading import RLock
+from ._compat import PY3_OR_LATER, PY27
+from .compressor import _ZFILE_PREFIX
+from .compressor import _COMPRESSORS
 
 if PY3_OR_LATER:
     Unpickler = pickle._Unpickler
@@ -33,12 +27,6 @@
 except ImportError:
     np = None
 
-try:
-    import lzma
-except ImportError:
-    lzma = None
-
-
 try:
     # The python standard library can be built without bz2 so we make bz2
     # usage optional.
@@ -48,30 +36,6 @@
 except ImportError:
     bz2 = None
 
-
-# Magic numbers of supported compression file formats.        '
-_ZFILE_PREFIX = b'ZF'  # used with pickle files created before 0.9.3.
-_ZLIB_PREFIX = b'\x78'
-_GZIP_PREFIX = b'\x1f\x8b'
-_BZ2_PREFIX = b'BZ'
-_XZ_PREFIX = b'\xfd\x37\x7a\x58\x5a'
-_LZMA_PREFIX = b'\x5d\x00'
-
-# Supported compressors
-_COMPRESSORS = ('zlib', 'bz2', 'lzma', 'xz', 'gzip')
-_COMPRESSOR_CLASSES = [gzip.GzipFile]
-
-if bz2 is not None:
-    _COMPRESSOR_CLASSES.append(bz2.BZ2File)
-
-if lzma is not None:
-    _COMPRESSOR_CLASSES.append(lzma.LZMAFile)
-
-# The max magic number length of supported compression file types.
-_MAX_PREFIX_LEN = max(len(prefix)
-                      for prefix in (_ZFILE_PREFIX, _GZIP_PREFIX, _BZ2_PREFIX,
-                                     _XZ_PREFIX, _LZMA_PREFIX))
-
 # Buffer size used in io.BufferedReader and io.BufferedWriter
 _IO_BUFFER_SIZE = 1024 ** 2
 
@@ -85,6 +49,13 @@ def _is_raw_file(fileobj):
         return isinstance(fileobj, file)  # noqa
 
 
+def _get_prefixes_max_len():
+    # Compute the max prefix len of registered compressors.
+    prefixes = [len(compressor.prefix) for compressor in _COMPRESSORS.values()]
+    prefixes += [len(_ZFILE_PREFIX)]
+    return max(prefixes)
+
+
 ###############################################################################
 # Cache file utilities
 def _detect_compressor(fileobj):
@@ -99,27 +70,22 @@ def _detect_compressor(fileobj):
     str in {'zlib', 'gzip', 'bz2', 'lzma', 'xz', 'compat', 'not-compressed'}
     """
     # Read the magic number in the first bytes of the file.
+    max_prefix_len = _get_prefixes_max_len()
     if hasattr(fileobj, 'peek'):
         # Peek allows to read those bytes without moving the cursor in the
-        # file which.
-        first_bytes = fileobj.peek(_MAX_PREFIX_LEN)
+        # file whic.
+        first_bytes = fileobj.peek(max_prefix_len)
     else:
         # Fallback to seek if the fileobject is not peekable.
-        first_bytes = fileobj.read(_MAX_PREFIX_LEN)
+        first_bytes = fileobj.read(max_prefix_len)
         fileobj.seek(0)
 
-    if first_bytes.startswith(_ZLIB_PREFIX):
-        return "zlib"
-    elif first_bytes.startswith(_GZIP_PREFIX):
-        return "gzip"
-    elif first_bytes.startswith(_BZ2_PREFIX):
-        return "bz2"
-    elif first_bytes.startswith(_LZMA_PREFIX):
-        return "lzma"
-    elif first_bytes.startswith(_XZ_PREFIX):
-        return "xz"
-    elif first_bytes.startswith(_ZFILE_PREFIX):
+    if first_bytes.startswith(_ZFILE_PREFIX):
         return "compat"
+    else:
+        for name, compressor in _COMPRESSORS.items():
+            if first_bytes.startswith(compressor.prefix):
+                return name
 
     return "not-compressed"
 
@@ -187,33 +153,13 @@ def _read_fileobject(fileobj, filename, mmap_mode=None):
                       DeprecationWarning, stacklevel=2)
         yield filename
     else:
-        # based on the compressor detected in the file, we open the
-        # correct decompressor file object, wrapped in a buffer.
-        if compressor == 'zlib':
-            fileobj = _buffered_read_file(BinaryZlibFile(fileobj, 'rb'))
-        elif compressor == 'gzip':
-            fileobj = _buffered_read_file(BinaryGzipFile(fileobj, 'rb'))
-        elif compressor == 'bz2' and bz2 is not None:
-            if PY3_OR_LATER:
-                fileobj = _buffered_read_file(bz2.BZ2File(fileobj, 'rb'))
-            else:
-                # In python 2, BZ2File doesn't support a fileobj opened in
-                # binary mode. In this case, we pass the filename.
-                fileobj = _buffered_read_file(bz2.BZ2File(fileobj.name, 'rb'))
-        elif (compressor == 'lzma' or compressor == 'xz'):
-            if PY3_OR_LATER and lzma is not None:
-                # We support lzma only in python 3 because in python 2 users
-                # may have installed the pyliblzma package, which also provides
-                # the lzma module, but that unfortunately doesn't fully support
-                # the buffer interface required by joblib.
-                # See https://github.com/joblib/joblib/issues/403 for details.
-                fileobj = _buffered_read_file(lzma.LZMAFile(fileobj, 'rb'))
-            else:
-                raise NotImplementedError("Lzma decompression is not "
-                                          "supported for this version of "
-                                          "python ({}.{})"
-                                          .format(sys.version_info[0],
-                                                  sys.version_info[1]))
+        if compressor in _COMPRESSORS:
+            # based on the compressor detected in the file, we open the
+            # correct decompressor file object, wrapped in a buffer.
+            compressor_wrapper = _COMPRESSORS[compressor]
+            inst = compressor_wrapper.decompressor_file(fileobj)
+            fileobj = _buffered_read_file(inst)
+
         # Checking if incompatible load parameters with the type of file:
         # mmap_mode cannot be used with compressed file or in memory buffers
         # such as io.BytesIO.
@@ -240,364 +186,15 @@ def _write_fileobject(filename, compress=("zlib", 3)):
     """Return the right compressor file object in write mode."""
     compressmethod = compress[0]
     compresslevel = compress[1]
-    if compressmethod == "gzip":
-        return _buffered_write_file(BinaryGzipFile(filename, 'wb',
-                                    compresslevel=compresslevel))
-    elif compressmethod == "bz2" and bz2 is not None:
-        return _buffered_write_file(bz2.BZ2File(filename, 'wb',
-                                                compresslevel=compresslevel))
-    elif lzma is not None and compressmethod == "xz":
-        return _buffered_write_file(lzma.LZMAFile(filename, 'wb',
-                                                  check=lzma.CHECK_NONE,
-                                                  preset=compresslevel))
-    elif lzma is not None and compressmethod == "lzma":
-        return _buffered_write_file(lzma.LZMAFile(filename, 'wb',
-                                                  preset=compresslevel,
-                                                  format=lzma.FORMAT_ALONE))
-    else:
-        return _buffered_write_file(BinaryZlibFile(filename, 'wb',
-                                    compresslevel=compresslevel))
-
-
-###############################################################################
-#  Joblib zlib compression file object definition
-
-_MODE_CLOSED = 0
-_MODE_READ = 1
-_MODE_READ_EOF = 2
-_MODE_WRITE = 3
-_BUFFER_SIZE = 8192
-
-
-class BinaryZlibFile(io.BufferedIOBase):
-    """A file object providing transparent zlib (de)compression.
-
-    A BinaryZlibFile can act as a wrapper for an existing file object, or refer
-    directly to a named file on disk.
-
-    Note that BinaryZlibFile provides only a *binary* file interface: data read
-    is returned as bytes, and data to be written should be given as bytes.
 
-    This object is an adaptation of the BZ2File object and is compatible with
-    versions of python >= 2.7.
-
-    If filename is a str or bytes object, it gives the name
-    of the file to be opened. Otherwise, it should be a file object,
-    which will be used to read or write the compressed data.
-
-    mode can be 'rb' for reading (default) or 'wb' for (over)writing
-
-    If mode is 'wb', compresslevel can be a number between 1
-    and 9 specifying the level of compression: 1 produces the least
-    compression, and 9 (default) produces the most compression.
-    """
-
-    wbits = zlib.MAX_WBITS
-
-    def __init__(self, filename, mode="rb", compresslevel=9):
-        # This lock must be recursive, so that BufferedIOBase's
-        # readline(), readlines() and writelines() don't deadlock.
-        self._lock = RLock()
-        self._fp = None
-        self._closefp = False
-        self._mode = _MODE_CLOSED
-        self._pos = 0
-        self._size = -1
-
-        if not isinstance(compresslevel, int) or not (1 <= compresslevel <= 9):
-            raise ValueError("'compresslevel' must be an integer "
-                             "between 1 and 9. You provided 'compresslevel={}'"
-                             .format(compresslevel))
-
-        if mode == "rb":
-            mode_code = _MODE_READ
-            self._decompressor = zlib.decompressobj(self.wbits)
-            self._buffer = b""
-            self._buffer_offset = 0
-        elif mode == "wb":
-            mode_code = _MODE_WRITE
-            self._compressor = zlib.compressobj(compresslevel,
-                                                zlib.DEFLATED,
-                                                self.wbits,
-                                                zlib.DEF_MEM_LEVEL,
-                                                0)
-        else:
-            raise ValueError("Invalid mode: %r" % (mode,))
-
-        if isinstance(filename, _basestring):
-            self._fp = io.open(filename, mode)
-            self._closefp = True
-            self._mode = mode_code
-        elif hasattr(filename, "read") or hasattr(filename, "write"):
-            self._fp = filename
-            self._mode = mode_code
-        else:
-            raise TypeError("filename must be a str or bytes object, "
-                            "or a file")
-
-    def close(self):
-        """Flush and close the file.
-
-        May be called more than once without error. Once the file is
-        closed, any other operation on it will raise a ValueError.
-        """
-        with self._lock:
-            if self._mode == _MODE_CLOSED:
-                return
-            try:
-                if self._mode in (_MODE_READ, _MODE_READ_EOF):
-                    self._decompressor = None
-                elif self._mode == _MODE_WRITE:
-                    self._fp.write(self._compressor.flush())
-                    self._compressor = None
-            finally:
-                try:
-                    if self._closefp:
-                        self._fp.close()
-                finally:
-                    self._fp = None
-                    self._closefp = False
-                    self._mode = _MODE_CLOSED
-                    self._buffer = b""
-                    self._buffer_offset = 0
-
-    @property
-    def closed(self):
-        """True if this file is closed."""
-        return self._mode == _MODE_CLOSED
-
-    def fileno(self):
-        """Return the file descriptor for the underlying file."""
-        self._check_not_closed()
-        return self._fp.fileno()
-
-    def seekable(self):
-        """Return whether the file supports seeking."""
-        return self.readable() and self._fp.seekable()
-
-    def readable(self):
-        """Return whether the file was opened for reading."""
-        self._check_not_closed()
-        return self._mode in (_MODE_READ, _MODE_READ_EOF)
-
-    def writable(self):
-        """Return whether the file was opened for writing."""
-        self._check_not_closed()
-        return self._mode == _MODE_WRITE
-
-    # Mode-checking helper functions.
-
-    def _check_not_closed(self):
-        if self.closed:
-            fname = getattr(self._fp, 'name', None)
-            msg = "I/O operation on closed file"
-            if fname is not None:
-                msg += " {}".format(fname)
-            msg += "."
-            raise ValueError(msg)
-
-    def _check_can_read(self):
-        if self._mode not in (_MODE_READ, _MODE_READ_EOF):
-            self._check_not_closed()
-            raise io.UnsupportedOperation("File not open for reading")
-
-    def _check_can_write(self):
-        if self._mode != _MODE_WRITE:
-            self._check_not_closed()
-            raise io.UnsupportedOperation("File not open for writing")
-
-    def _check_can_seek(self):
-        if self._mode not in (_MODE_READ, _MODE_READ_EOF):
-            self._check_not_closed()
-            raise io.UnsupportedOperation("Seeking is only supported "
-                                          "on files open for reading")
-        if not self._fp.seekable():
-            raise io.UnsupportedOperation("The underlying file object "
-                                          "does not support seeking")
-
-    # Fill the readahead buffer if it is empty. Returns False on EOF.
-    def _fill_buffer(self):
-        if self._mode == _MODE_READ_EOF:
-            return False
-        # Depending on the input data, our call to the decompressor may not
-        # return any data. In this case, try again after reading another block.
-        while self._buffer_offset == len(self._buffer):
-            try:
-                rawblock = (self._decompressor.unused_data or
-                            self._fp.read(_BUFFER_SIZE))
-
-                if not rawblock:
-                    raise EOFError
-            except EOFError:
-                # End-of-stream marker and end of file. We're good.
-                self._mode = _MODE_READ_EOF
-                self._size = self._pos
-                return False
-            else:
-                self._buffer = self._decompressor.decompress(rawblock)
-            self._buffer_offset = 0
-        return True
-
-    # Read data until EOF.
-    # If return_data is false, consume the data without returning it.
-    def _read_all(self, return_data=True):
-        # The loop assumes that _buffer_offset is 0. Ensure that this is true.
-        self._buffer = self._buffer[self._buffer_offset:]
-        self._buffer_offset = 0
-
-        blocks = []
-        while self._fill_buffer():
-            if return_data:
-                blocks.append(self._buffer)
-            self._pos += len(self._buffer)
-            self._buffer = b""
-        if return_data:
-            return b"".join(blocks)
-
-    # Read a block of up to n bytes.
-    # If return_data is false, consume the data without returning it.
-    def _read_block(self, n_bytes, return_data=True):
-        # If we have enough data buffered, return immediately.
-        end = self._buffer_offset + n_bytes
-        if end <= len(self._buffer):
-            data = self._buffer[self._buffer_offset: end]
-            self._buffer_offset = end
-            self._pos += len(data)
-            return data if return_data else None
-
-        # The loop assumes that _buffer_offset is 0. Ensure that this is true.
-        self._buffer = self._buffer[self._buffer_offset:]
-        self._buffer_offset = 0
-
-        blocks = []
-        while n_bytes > 0 and self._fill_buffer():
-            if n_bytes < len(self._buffer):
-                data = self._buffer[:n_bytes]
-                self._buffer_offset = n_bytes
-            else:
-                data = self._buffer
-                self._buffer = b""
-            if return_data:
-                blocks.append(data)
-            self._pos += len(data)
-            n_bytes -= len(data)
-        if return_data:
-            return b"".join(blocks)
-
-    def read(self, size=-1):
-        """Read up to size uncompressed bytes from the file.
-
-        If size is negative or omitted, read until EOF is reached.
-        Returns b'' if the file is already at EOF.
-        """
-        with self._lock:
-            self._check_can_read()
-            if size == 0:
-                return b""
-            elif size < 0:
-                return self._read_all()
-            else:
-                return self._read_block(size)
-
-    def readinto(self, b):
-        """Read up to len(b) bytes into b.
-
-        Returns the number of bytes read (0 for EOF).
-        """
-        with self._lock:
-            return io.BufferedIOBase.readinto(self, b)
-
-    def write(self, data):
-        """Write a byte string to the file.
-
-        Returns the number of uncompressed bytes written, which is
-        always len(data). Note that due to buffering, the file on disk
-        may not reflect the data written until close() is called.
-        """
-        with self._lock:
-            self._check_can_write()
-            # Convert data type if called by io.BufferedWriter.
-            if isinstance(data, memoryview):
-                data = data.tobytes()
-
-            compressed = self._compressor.compress(data)
-            self._fp.write(compressed)
-            self._pos += len(data)
-            return len(data)
-
-    # Rewind the file to the beginning of the data stream.
-    def _rewind(self):
-        self._fp.seek(0, 0)
-        self._mode = _MODE_READ
-        self._pos = 0
-        self._decompressor = zlib.decompressobj(self.wbits)
-        self._buffer = b""
-        self._buffer_offset = 0
-
-    def seek(self, offset, whence=0):
-        """Change the file position.
-
-        The new position is specified by offset, relative to the
-        position indicated by whence. Values for whence are:
-
-            0: start of stream (default); offset must not be negative
-            1: current stream position
-            2: end of stream; offset must not be positive
-
-        Returns the new file position.
-
-        Note that seeking is emulated, so depending on the parameters,
-        this operation may be extremely slow.
-        """
-        with self._lock:
-            self._check_can_seek()
-
-            # Recalculate offset as an absolute file position.
-            if whence == 0:
-                pass
-            elif whence == 1:
-                offset = self._pos + offset
-            elif whence == 2:
-                # Seeking relative to EOF - we need to know the file's size.
-                if self._size < 0:
-                    self._read_all(return_data=False)
-                offset = self._size + offset
-            else:
-                raise ValueError("Invalid value for whence: %s" % (whence,))
-
-            # Make it so that offset is the number of bytes to skip forward.
-            if offset < self._pos:
-                self._rewind()
-            else:
-                offset -= self._pos
-
-            # Read and discard data until we reach the desired position.
-            self._read_block(offset, return_data=False)
-
-            return self._pos
-
-    def tell(self):
-        """Return the current file position."""
-        with self._lock:
-            self._check_not_closed()
-            return self._pos
-
-
-class BinaryGzipFile(BinaryZlibFile):
-    """A file object providing transparent gzip (de)compression.
-
-    If filename is a str or bytes object, it gives the name
-    of the file to be opened. Otherwise, it should be a file object,
-    which will be used to read or write the compressed data.
-
-    mode can be 'rb' for reading (default) or 'wb' for (over)writing
-
-    If mode is 'wb', compresslevel can be a number between 1
-    and 9 specifying the level of compression: 1 produces the least
-    compression, and 9 (default) produces the most compression.
-    """
-
-    wbits = 31  # zlib compressor/decompressor wbits value for gzip format.
+    if compressmethod in _COMPRESSORS.keys():
+        file_instance = _COMPRESSORS[compressmethod].compressor_file(
+            filename, compresslevel=compresslevel)
+        return _buffered_write_file(file_instance)
+    else:
+        file_instance = _COMPRESSORS['zlib'].compressor_file(
+            filename, compresslevel=compresslevel)
+        return _buffered_write_file(file_instance)
 
 
 # Utility functions/variables from numpy required for writing arrays.
diff --git a/sklearn/externals/joblib/parallel.py b/sklearn/externals/joblib/parallel.py
index 96c90423e6..8be0ed2c6b 100755
--- a/sklearn/externals/joblib/parallel.py
+++ b/sklearn/externals/joblib/parallel.py
@@ -12,25 +12,25 @@
 from math import sqrt
 import functools
 import time
+import inspect
 import threading
 import itertools
 from numbers import Integral
-from contextlib import contextmanager
 import warnings
-try:
-    import cPickle as pickle
-except ImportError:
-    import pickle
+from functools import partial
 
 from ._multiprocessing_helpers import mp
 
 from .format_stack import format_outer_frames
 from .logger import Logger, short_format_time
-from .my_exceptions import TransportableException, _mk_exception
+from .my_exceptions import TransportableException
 from .disk import memstr_to_bytes
 from ._parallel_backends import (FallbackToBackend, MultiprocessingBackend,
-                                 ThreadingBackend, SequentialBackend)
+                                 ThreadingBackend, SequentialBackend,
+                                 LokyBackend)
 from ._compat import _basestring
+from .externals.cloudpickle import dumps, loads
+from .externals import loky
 
 # Make sure that those two classes are part of the public joblib.parallel API
 # so that 3rd party backend implementers can import them from here.
@@ -41,37 +41,108 @@
     'multiprocessing': MultiprocessingBackend,
     'threading': ThreadingBackend,
     'sequential': SequentialBackend,
+    'loky': LokyBackend,
 }
-
 # name of the backend used by default by Parallel outside of any context
 # managed by ``parallel_backend``.
-DEFAULT_BACKEND = 'multiprocessing'
+DEFAULT_BACKEND = 'loky'
 DEFAULT_N_JOBS = 1
+DEFAULT_THREAD_BACKEND = 'threading'
 
 # Thread local value that can be overridden by the ``parallel_backend`` context
 # manager
 _backend = threading.local()
 
+VALID_BACKEND_HINTS = ('processes', 'threads', None)
+VALID_BACKEND_CONSTRAINTS = ('sharedmem', None)
 
-def get_active_backend():
-    """Return the active default backend"""
-    active_backend_and_jobs = getattr(_backend, 'backend_and_jobs', None)
-    if active_backend_and_jobs is not None:
-        return active_backend_and_jobs
-    # We are outside of the scope of any parallel_backend context manager,
-    # create the default backend instance now
-    active_backend = BACKENDS[DEFAULT_BACKEND]()
-    return active_backend, DEFAULT_N_JOBS
+
+def _register_dask():
+    """ Register Dask Backend if called with parallel_backend("dask") """
+    try:
+        from ._dask import DaskDistributedBackend
+        register_parallel_backend('dask', DaskDistributedBackend)
+    except ImportError:
+        msg = ("To use the dask.distributed backend you must install both "
+               "the `dask` and distributed modules.\n\n"
+               "See http://dask.pydata.org/en/latest/install.html for more "
+               "information.")
+        raise ImportError(msg)
+
+
+EXTERNAL_BACKENDS = {
+    'dask': _register_dask,
+}
 
 
-@contextmanager
-def parallel_backend(backend, n_jobs=-1, **backend_params):
+def get_active_backend(prefer=None, require=None, verbose=0):
+    """Return the active default backend"""
+    if prefer not in VALID_BACKEND_HINTS:
+        raise ValueError("prefer=%r is not a valid backend hint, "
+                         "expected one of %r" % (prefer, VALID_BACKEND_HINTS))
+    if require not in VALID_BACKEND_CONSTRAINTS:
+        raise ValueError("require=%r is not a valid backend constraint, "
+                         "expected one of %r"
+                         % (require, VALID_BACKEND_CONSTRAINTS))
+
+    if prefer == 'processes' and require == 'sharedmem':
+        raise ValueError("prefer == 'processes' and require == 'sharedmem'"
+                         " are inconsistent settings")
+    backend_and_jobs = getattr(_backend, 'backend_and_jobs', None)
+    if backend_and_jobs is not None:
+        # Try to use the backend set by the user with the context manager.
+        backend, n_jobs = backend_and_jobs
+        supports_sharedmem = getattr(backend, 'supports_sharedmem', False)
+        if require == 'sharedmem' and not supports_sharedmem:
+            # This backend does not match the shared memory constraint:
+            # fallback to the default thead-based backend.
+            sharedmem_backend = BACKENDS[DEFAULT_THREAD_BACKEND]()
+            if verbose >= 10:
+                print("Using %s as joblib.Parallel backend instead of %s "
+                      "as the latter does not provide shared memory semantics."
+                      % (sharedmem_backend.__class__.__name__,
+                         backend.__class__.__name__))
+            return sharedmem_backend, DEFAULT_N_JOBS
+        else:
+            return backend_and_jobs
+
+    # We are outside of the scope of any parallel_backend context manager,
+    # create the default backend instance now.
+    backend = BACKENDS[DEFAULT_BACKEND]()
+    supports_sharedmem = getattr(backend, 'supports_sharedmem', False)
+    uses_threads = getattr(backend, 'uses_threads', False)
+    if ((require == 'sharedmem' and not supports_sharedmem) or
+            (prefer == 'threads' and not uses_threads)):
+        # Make sure the selected default backend match the soft hints and
+        # hard constraints:
+        backend = BACKENDS[DEFAULT_THREAD_BACKEND]()
+    return backend, DEFAULT_N_JOBS
+
+
+class parallel_backend(object):
     """Change the default backend used by Parallel inside a with block.
 
     If ``backend`` is a string it must match a previously registered
     implementation using the ``register_parallel_backend`` function.
 
-    Alternatively backend can be passed directly as an instance.
+    By default the following backends are available:
+
+    - 'loky': single-host, process-based parallelism (used by default),
+    - 'threading': single-host, thread-based parallelism,
+    - 'multiprocessing': legacy single-host, process-based parallelism.
+
+    'loky' is recommended to run functions that manipulate Python objects.
+    'threading' is a low-overhead alternative that is most efficient for
+    functions that release the Global Interpreter Lock: e.g. I/O-bound code or
+    CPU-bound code in a few calls to native code that explicitly releases the
+    GIL.
+
+    In addition, if the `dask` and `distributed` Python packages are installed,
+    it is possible to use the 'dask' backend for better scheduling of nested
+    parallel calls without over-subscription and potentially distribute
+    parallel calls over a networked cluster of several hosts.
+
+    Alternatively the backend can be passed directly as an instance.
 
     By default all available workers will be used (``n_jobs=-1``) unless the
     caller passes an explicit value for the ``n_jobs`` parameter.
@@ -93,19 +164,31 @@ def parallel_backend(backend, n_jobs=-1, **backend_params):
     .. versionadded:: 0.10
 
     """
-    if isinstance(backend, _basestring):
-        backend = BACKENDS[backend](**backend_params)
-    old_backend_and_jobs = getattr(_backend, 'backend_and_jobs', None)
-    try:
+    def __init__(self, backend, n_jobs=-1, **backend_params):
+        if isinstance(backend, _basestring):
+            if backend not in BACKENDS and backend in EXTERNAL_BACKENDS:
+                register = EXTERNAL_BACKENDS[backend]
+                register()
+
+            backend = BACKENDS[backend](**backend_params)
+
+        self.old_backend_and_jobs = getattr(_backend, 'backend_and_jobs', None)
+        self.new_backend_and_jobs = (backend, n_jobs)
+
         _backend.backend_and_jobs = (backend, n_jobs)
-        # return the backend instance to make it easier to write tests
-        yield backend, n_jobs
-    finally:
-        if old_backend_and_jobs is None:
+
+    def __enter__(self):
+        return self.new_backend_and_jobs
+
+    def __exit__(self, type, value, traceback):
+        self.unregister()
+
+    def unregister(self):
+        if self.old_backend_and_jobs is None:
             if getattr(_backend, 'backend_and_jobs', None) is not None:
                 del _backend.backend_and_jobs
         else:
-            _backend.backend_and_jobs = old_backend_and_jobs
+            _backend.backend_and_jobs = self.old_backend_and_jobs
 
 
 # Under Linux or OS X the default start method of multiprocessing
@@ -120,19 +203,96 @@ def parallel_backend(backend, n_jobs=-1, **backend_params):
     DEFAULT_MP_CONTEXT = None
 
 
+class CloudpickledObjectWrapper(object):
+    def __init__(self, obj):
+        self.pickled_obj = dumps(obj)
+
+    def __reduce__(self):
+        return loads, (self.pickled_obj,)
+
+
+def _need_pickle_wrapping(obj):
+    if isinstance(obj, list) and len(obj) >= 1:
+        # Make the assumption that the content of the list is homogeneously
+        # typed.
+        return _need_pickle_wrapping(obj[0])
+    elif isinstance(obj, dict) and len(obj) >= 1:
+        # Make the assumption that the content of the dict is homogeneously
+        # typed.
+        k, v = next(iter(obj.items()))
+        return _need_pickle_wrapping(v) or _need_pickle_wrapping(k)
+    elif isinstance(obj, partial):
+        return _need_pickle_wrapping(obj.func)
+
+    # Warning: obj.__module__ can be defined and set to None
+    module = getattr(obj, "__module__", None)
+    need_wrap = module is not None and "__main__" in module
+    if callable(obj):
+        # Need wrap if the object is a function defined in a local scope of
+        # another function.
+        func_code = getattr(obj, "__code__", "")
+        need_wrap |= getattr(func_code, "co_flags", 0) & inspect.CO_NESTED
+
+        # Need wrap if the obj is a lambda expression
+        func_name = getattr(obj, "__name__", "")
+        need_wrap |= "<lambda>" in func_name
+
+        # Need wrap if obj is a bound method of an instance of an
+        # interactively defined class
+        method_self = getattr(obj, '__self__', None)
+        if not need_wrap and method_self is not None:
+            # Recursively introspect the instanc of the method
+            return _need_pickle_wrapping(method_self)
+    return need_wrap
+
+
 class BatchedCalls(object):
     """Wrap a sequence of (func, args, kwargs) tuples as a single callable"""
 
-    def __init__(self, iterator_slice):
+    def __init__(self, iterator_slice, backend, pickle_cache=None):
         self.items = list(iterator_slice)
         self._size = len(self.items)
+        self._backend = backend
+        self._pickle_cache = pickle_cache if pickle_cache is not None else {}
 
     def __call__(self):
-        return [func(*args, **kwargs) for func, args, kwargs in self.items]
+        with parallel_backend(self._backend):
+            return [func(*args, **kwargs)
+                    for func, args, kwargs in self.items]
 
     def __len__(self):
         return self._size
 
+    @staticmethod
+    def _wrap_non_picklable_objects(obj, pickle_cache):
+        if not _need_pickle_wrapping(obj):
+            return obj
+        try:
+            wrapped_obj = pickle_cache.get(obj)
+            hashable = True
+        except TypeError:
+            # obj is not hashable: cannot be cached
+            wrapped_obj = None
+            hashable = False
+        if wrapped_obj is None:
+            wrapped_obj = CloudpickledObjectWrapper(obj)
+            if hashable:
+                pickle_cache[obj] = wrapped_obj
+        return wrapped_obj
+
+    def __getstate__(self):
+        items = [(self._wrap_non_picklable_objects(func, self._pickle_cache),
+                  [self._wrap_non_picklable_objects(a, self._pickle_cache)
+                   for a in args],
+                  {k: self._wrap_non_picklable_objects(a, self._pickle_cache)
+                   for k, a in kwargs.items()}
+                  )
+                 for func, args, kwargs in self.items]
+        return (items, self._size, self._backend)
+
+    def __setstate__(self, state):
+        self.items, self._size, self._backend = state
+
 
 ###############################################################################
 # CPU count that works also when multiprocessing has been disabled via
@@ -141,7 +301,8 @@ def cpu_count():
     """Return the number of CPUs."""
     if mp is None:
         return 1
-    return mp.cpu_count()
+
+    return loky.cpu_count()
 
 
 ###############################################################################
@@ -166,21 +327,15 @@ def _verbosity_filter(index, verbose):
 
 
 ###############################################################################
-def delayed(function, check_pickle=True):
-    """Decorator used to capture the arguments of a function.
-
-    Pass `check_pickle=False` when:
-
-    - performing a possibly repeated check is too costly and has been done
-      already once outside of the call to delayed.
-
-    - when used in conjunction `Parallel(backend='threading')`.
-
-    """
+def delayed(function, check_pickle=None):
+    """Decorator used to capture the arguments of a function."""
+    if check_pickle is not None:
+        warnings.warn('check_pickle is deprecated in joblib 0.12 and will be'
+                      ' removed in 0.13', DeprecationWarning)
     # Try to pickle the input function, to catch the problems early when
     # using with multiprocessing:
     if check_pickle:
-        pickle.dumps(function)
+        dumps(function)
 
     def delayed_function(*args, **kwargs):
         return function, args, kwargs
@@ -215,8 +370,9 @@ def __call__(self, out):
         self.parallel._backend.batch_completed(self.batch_size,
                                                this_batch_duration)
         self.parallel.print_progress()
-        if self.parallel._original_iterator is not None:
-            self.parallel.dispatch_next()
+        with self.parallel._lock:
+            if self.parallel._original_iterator is not None:
+                self.parallel.dispatch_next()
 
 
 ###############################################################################
@@ -245,18 +401,18 @@ def register_parallel_backend(name, factory, make_default=False):
 def effective_n_jobs(n_jobs=-1):
     """Determine the number of jobs that can actually run in parallel
 
-    n_jobs is the number of workers requested by the callers.
-    Passing n_jobs=-1 means requesting all available workers for instance
-    matching the number of CPU cores on the worker host(s).
+    n_jobs is the number of workers requested by the callers. Passing n_jobs=-1
+    means requesting all available workers for instance matching the number of
+    CPU cores on the worker host(s).
 
     This method should return a guesstimate of the number of workers that can
     actually perform work concurrently with the currently enabled default
     backend. The primary use case is to make it possible for the caller to know
     in how many chunks to slice the work.
 
-    In general working on larger data chunks is more efficient (less
-    scheduling overhead and better use of CPU cache prefetching heuristics)
-    as long as all the workers have enough work to do.
+    In general working on larger data chunks is more efficient (less scheduling
+    overhead and better use of CPU cache prefetching heuristics) as long as all
+    the workers have enough work to do.
 
     Warning: this function is experimental and subject to change in a future
     version of joblib.
@@ -272,9 +428,11 @@ def effective_n_jobs(n_jobs=-1):
 class Parallel(Logger):
     ''' Helper class for readable parallel mapping.
 
+        Read more in the :ref:`User Guide <parallel>`.
+
         Parameters
         -----------
-        n_jobs: int, default: 1
+        n_jobs: int, default: None
             The maximum number of concurrently running jobs, such as the number
             of Python worker processes when backend="multiprocessing"
             or the size of the thread-pool when backend="threading".
@@ -282,14 +440,19 @@ class Parallel(Logger):
             is used at all, which is useful for debugging. For n_jobs below -1,
             (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all
             CPUs but one are used.
-        backend: str, ParallelBackendBase instance or None, \
-                default: 'multiprocessing'
+            None is a marker for 'unset' that will be interpreted as n_jobs=1
+            (sequential execution) unless the call is performed under a
+            parallel_backend context manager that sets another value for
+            n_jobs.
+        backend: str, ParallelBackendBase instance or None, default: 'loky'
             Specify the parallelization backend implementation.
             Supported backends are:
 
-            - "multiprocessing" used by default, can induce some
+            - "loky" used by default, can induce some
               communication and memory overhead when exchanging input and
               output data with the worker Python processes.
+            - "multiprocessing" previous process-based backend based on
+              `multiprocessing.Pool`. Less robust than `loky`.
             - "threading" is a very low-overhead backend but it suffers
               from the Python Global Interpreter Lock if the called function
               relies a lot on Python objects. "threading" is mostly useful
@@ -300,6 +463,22 @@ class Parallel(Logger):
             - finally, you can register backends by calling
               register_parallel_backend. This will allow you to implement
               a backend of your liking.
+
+            It is not recommended to hard-code the backend name in a call to
+            Parallel in a library. Instead it is recommended to set soft hints
+            (prefer) or hard constraints (require) so as to make it possible
+            for library users to change the backend from the outside using the
+            parallel_backend context manager.
+        prefer: str in {'processes', 'threads'} or None, default: None
+            Soft hint to choose the default backend if no specific backend
+            was selected with the parallel_backend context manager. The
+            default process-based backend is 'loky' and the default
+            thread-based backend is 'threading'.
+        require: 'sharedmem' or None, default None
+            Hard constraint to select the backend. If set to 'sharedmem',
+            the selected backend will be single-host and thread-based even
+            if the user asked for a non-thread based backend with
+            parallel_backend.
         verbose: int, optional
             The verbosity level: if non zero, progress messages are
             printed. Above 50, the output is sent to stdout.
@@ -311,12 +490,13 @@ class Parallel(Logger):
         pre_dispatch: {'all', integer, or expression, as in '3*n_jobs'}
             The number of batches (of tasks) to be pre-dispatched.
             Default is '2*n_jobs'. When batch_size="auto" this is reasonable
-            default and the multiprocessing workers should never starve.
+            default and the workers should never starve.
         batch_size: int or 'auto', default: 'auto'
             The number of atomic tasks to dispatch at once to each
-            worker. When individual evaluations are very fast, multiprocessing
-            can be slower than sequential computation because of the overhead.
-            Batching fast computations together can mitigate this.
+            worker. When individual evaluations are very fast, dispatching
+            calls to workers can be slower than sequential computation because
+            of the overhead. Batching fast computations together can mitigate
+            this.
             The ``'auto'`` strategy keeps track of the time it takes for a batch
             to complete, and dynamically adjusts the batch size to keep the time
             on the order of half a second, using a heuristic. The initial batch
@@ -326,26 +506,26 @@ class Parallel(Logger):
             very little overhead and using larger batch size has not proved to
             bring any gain in that case.
         temp_folder: str, optional
-            Folder to be used by the pool for memmaping large arrays
+            Folder to be used by the pool for memmapping large arrays
             for sharing memory with worker processes. If None, this will try in
             order:
 
             - a folder pointed by the JOBLIB_TEMP_FOLDER environment
               variable,
             - /dev/shm if the folder exists and is writable: this is a
-              RAMdisk filesystem available by default on modern Linux
+              RAM disk filesystem available by default on modern Linux
               distributions,
             - the default system temporary folder that can be
               overridden with TMP, TMPDIR or TEMP environment
               variables, typically /tmp under Unix operating systems.
 
-            Only active when backend="multiprocessing".
+            Only active when backend="loky" or "multiprocessing".
         max_nbytes int, str, or None, optional, 1M by default
             Threshold on the size of arrays passed to the workers that
             triggers automated memory mapping in temp_folder. Can be an int
             in Bytes, or a human-readable string, e.g., '1M' for 1 megabyte.
-            Use None to disable memmaping of large arrays.
-            Only active when backend="multiprocessing".
+            Use None to disable memmapping of large arrays.
+            Only active when backend="loky" or "multiprocessing".
         mmap_mode: {None, 'r+', 'r', 'w+', 'c'}
             Memmapping mode for numpy arrays passed to workers.
             See 'max_nbytes' parameter documentation for more details.
@@ -353,10 +533,10 @@ class Parallel(Logger):
         Notes
         -----
 
-        This object uses the multiprocessing module to compute in
-        parallel the application of a function to many different
-        arguments. The main functionality it brings in addition to
-        using the raw multiprocessing API are (see examples for details):
+        This object uses workers to compute in parallel the application of a
+        function to many different arguments. The main functionality it brings
+        in addition to using the raw multiprocessing or concurrent.futures API
+        are (see examples for details):
 
         * More readable code, in particular since it avoids
           constructing list of arguments.
@@ -384,7 +564,7 @@ class Parallel(Logger):
         A simple example:
 
         >>> from math import sqrt
-        >>> from sklearn.utils import Parallel, delayed
+        >>> from sklearn.externals.joblib import Parallel, delayed
         >>> Parallel(n_jobs=1)(delayed(sqrt)(i**2) for i in range(10))
         [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]
 
@@ -392,7 +572,7 @@ class Parallel(Logger):
         values:
 
         >>> from math import modf
-        >>> from sklearn.utils import Parallel, delayed
+        >>> from sklearn.externals.joblib import Parallel, delayed
         >>> r = Parallel(n_jobs=1)(delayed(modf)(i/2.) for i in range(10))
         >>> res, i = zip(*r)
         >>> res
@@ -404,13 +584,11 @@ class Parallel(Logger):
         messages:
 
         >>> from time import sleep
-        >>> from sklearn.utils import Parallel, delayed
-        >>> r = Parallel(n_jobs=2, verbose=5)(delayed(sleep)(.1) for _ in range(10)) #doctest: +SKIP
-        [Parallel(n_jobs=2)]: Done   1 out of  10 | elapsed:    0.1s remaining:    0.9s
-        [Parallel(n_jobs=2)]: Done   3 out of  10 | elapsed:    0.2s remaining:    0.5s
-        [Parallel(n_jobs=2)]: Done   6 out of  10 | elapsed:    0.3s remaining:    0.2s
-        [Parallel(n_jobs=2)]: Done   9 out of  10 | elapsed:    0.5s remaining:    0.1s
-        [Parallel(n_jobs=2)]: Done  10 out of  10 | elapsed:    0.5s finished
+        >>> from sklearn.externals.joblib import Parallel, delayed
+        >>> r = Parallel(n_jobs=2, verbose=10)(delayed(sleep)(.2) for _ in range(10)) #doctest: +SKIP
+        [Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.6s
+        [Parallel(n_jobs=2)]: Done   4 tasks      | elapsed:    0.8s
+        [Parallel(n_jobs=2)]: Done  10 out of  10 | elapsed:    1.4s finished
 
         Traceback example, note how the line of the error is indicated
         as well as the values of the parameter passed to the function that
@@ -418,7 +596,7 @@ class Parallel(Logger):
         child process:
 
         >>> from heapq import nlargest
-        >>> from sklearn.utils import Parallel, delayed
+        >>> from sklearn.externals.joblib import Parallel, delayed
         >>> Parallel(n_jobs=2)(delayed(nlargest)(2, n) for n in (range(4), 'abcde', 3)) #doctest: +SKIP
         #...
         ---------------------------------------------------------------------------
@@ -445,11 +623,10 @@ class Parallel(Logger):
         Using pre_dispatch in a producer/consumer situation, where the
         data is generated on the fly. Note how the producer is first
         called 3 times before the parallel loop is initiated, and then
-        called to generate new data on the fly. In this case the total
-        number of iterations cannot be reported in the progress messages:
+        called to generate new data on the fly:
 
         >>> from math import sqrt
-        >>> from sklearn.utils import Parallel, delayed
+        >>> from sklearn.externals.joblib import Parallel, delayed
         >>> def producer():
         ...     for i in range(6):
         ...         print('Produced %s' % i)
@@ -466,18 +643,24 @@ class Parallel(Logger):
         [Parallel(n_jobs=2)]: Done 3 jobs     | elapsed:  0.0s
         Produced 5
         [Parallel(n_jobs=2)]: Done 4 jobs     | elapsed:  0.0s
-        [Parallel(n_jobs=2)]: Done 5 out of 6 | elapsed:  0.0s remaining: 0.0s
+        [Parallel(n_jobs=2)]: Done 6 out of 6 | elapsed:  0.0s remaining: 0.0s
         [Parallel(n_jobs=2)]: Done 6 out of 6 | elapsed:  0.0s finished
 
     '''
-    def __init__(self, n_jobs=1, backend=None, verbose=0, timeout=None,
+    def __init__(self, n_jobs=None, backend=None, verbose=0, timeout=None,
                  pre_dispatch='2 * n_jobs', batch_size='auto',
-                 temp_folder=None, max_nbytes='1M', mmap_mode='r'):
-        active_backend, default_n_jobs = get_active_backend()
-        if backend is None and n_jobs == 1:
+                 temp_folder=None, max_nbytes='1M', mmap_mode='r',
+                 prefer=None, require=None):
+        active_backend, context_n_jobs = get_active_backend(
+            prefer=prefer, require=require, verbose=verbose)
+        if backend is None and n_jobs is None:
             # If we are under a parallel_backend context manager, look up
             # the default number of jobs and use that instead:
-            n_jobs = default_n_jobs
+            n_jobs = context_n_jobs
+        if n_jobs is None:
+            # No specific context override and no specific value request:
+            # default to 1.
+            n_jobs = 1
         self.n_jobs = n_jobs
         self.verbose = verbose
         self.timeout = timeout
@@ -490,6 +673,8 @@ def __init__(self, n_jobs=1, backend=None, verbose=0, timeout=None,
             max_nbytes=max_nbytes,
             mmap_mode=mmap_mode,
             temp_folder=temp_folder,
+            prefer=prefer,
+            require=require,
             verbose=max(0, self.verbose - 50),
         )
         if DEFAULT_MP_CONTEXT is not None:
@@ -514,6 +699,11 @@ def __init__(self, n_jobs=1, backend=None, verbose=0, timeout=None,
                                  % (backend, sorted(BACKENDS.keys())))
             backend = backend_factory()
 
+        if (require == 'sharedmem' and
+                not getattr(backend, 'supports_sharedmem', False)):
+            raise ValueError("Backend %s does not support shared memory"
+                             % backend)
+
         if (batch_size == 'auto' or isinstance(batch_size, Integral) and
                 batch_size > 0):
             self.batch_size = batch_size
@@ -529,7 +719,7 @@ def __init__(self, n_jobs=1, backend=None, verbose=0, timeout=None,
 
         # This lock is used coordinate the main thread of this process with
         # the async callback thread of our the pool.
-        self._lock = threading.Lock()
+        self._lock = threading.RLock()
 
     def __enter__(self):
         self._managed_backend = True
@@ -585,8 +775,14 @@ def _dispatch(self, batch):
 
         dispatch_timestamp = time.time()
         cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)
-        job = self._backend.apply_async(batch, callback=cb)
-        self._jobs.append(job)
+        with self._lock:
+            job_idx = len(self._jobs)
+            job = self._backend.apply_async(batch, callback=cb)
+            # A job can complete so quickly than its callback is
+            # called before we get here, causing self._jobs to
+            # grow. To ensure correct results ordering, .insert is
+            # used (rather than .append) in the following line
+            self._jobs.insert(job_idx, job)
 
     def dispatch_next(self):
         """Dispatch more data for parallel processing
@@ -617,7 +813,9 @@ def dispatch_one_batch(self, iterator):
             batch_size = self.batch_size
 
         with self._lock:
-            tasks = BatchedCalls(itertools.islice(iterator, batch_size))
+            tasks = BatchedCalls(itertools.islice(iterator, batch_size),
+                                 self._backend.get_nested_backend(),
+                                 self._pickle_cache)
             if len(tasks) == 0:
                 # No more tasks available in the iterator: tell caller to stop.
                 return False
@@ -720,24 +918,14 @@ def retrieve(self):
                     ensure_ready = self._managed_backend
                     backend.abort_everything(ensure_ready=ensure_ready)
 
-                if not isinstance(exception, TransportableException):
-                    raise
-                else:
+                if isinstance(exception, TransportableException):
                     # Capture exception to add information on the local
                     # stack in addition to the distant stack
                     this_report = format_outer_frames(context=10,
                                                       stack_start=1)
-                    report = """Multiprocessing exception:
-%s
----------------------------------------------------------------------------
-Sub-process traceback:
----------------------------------------------------------------------------
-%s""" % (this_report, exception.message)
-                    # Convert this to a JoblibException
-                    exception_type = _mk_exception(exception.etype)[0]
-                    exception = exception_type(report)
-
-                    raise exception
+                    raise exception.unwrap(this_report)
+                else:
+                    raise
 
     def __call__(self, iterable):
         if self._jobs:
@@ -745,11 +933,15 @@ def __call__(self, iterable):
         # A flag used to abort the dispatching of jobs in case an
         # exception is found
         self._aborting = False
+
         if not self._managed_backend:
             n_jobs = self._initialize_backend()
         else:
             n_jobs = self._effective_n_jobs()
-
+        self._print("Using backend %s with %d concurrent workers.",
+                    (self._backend.__class__.__name__, n_jobs))
+        if hasattr(self._backend, 'start_call'):
+            self._backend.start_call()
         iterator = iter(iterable)
         pre_dispatch = self.pre_dispatch
 
@@ -772,30 +964,46 @@ def __call__(self, iterable):
         self.n_dispatched_batches = 0
         self.n_dispatched_tasks = 0
         self.n_completed_tasks = 0
+        # Use a caching dict for callables that are pickled with cloudpickle to
+        # improve performances. This cache is used only in the case of
+        # functions that are defined in the __main__ module, functions that are
+        # defined locally (inside another function) and lambda expressions.
+        self._pickle_cache = dict()
         try:
             # Only set self._iterating to True if at least a batch
             # was dispatched. In particular this covers the edge
-            # case of Parallel used with an exhausted iterator.
+            # case of Parallel used with an exhausted iterator. If
+            # self._original_iterator is None, then this means either
+            # that pre_dispatch == "all", n_jobs == 1 or that the first batch
+            # was very quick and its callback already dispatched all the
+            # remaining jobs.
+            self._iterating = False
+            if self.dispatch_one_batch(iterator):
+                self._iterating = self._original_iterator is not None
+
             while self.dispatch_one_batch(iterator):
-                self._iterating = True
-            else:
-                self._iterating = False
+                pass
 
             if pre_dispatch == "all" or n_jobs == 1:
                 # The iterable was consumed all at once by the above for loop.
                 # No need to wait for async callbacks to trigger to
                 # consumption.
                 self._iterating = False
-            self.retrieve()
+
+            with self._backend.retrieval_context():
+                self.retrieve()
             # Make sure that we get a last message telling us we are done
             elapsed_time = time.time() - self._start_time
             self._print('Done %3i out of %3i | elapsed: %s finished',
                         (len(self._output), len(self._output),
                          short_format_time(elapsed_time)))
         finally:
+            if hasattr(self._backend, 'stop_call'):
+                self._backend.stop_call()
             if not self._managed_backend:
                 self._terminate_backend()
             self._jobs = list()
+            self._pickle_cache = None
         output = self._output
         self._output = None
         return output
diff --git a/sklearn/externals/joblib/pool.py b/sklearn/externals/joblib/pool.py
index ef3838e7e5..396a3dfb4e 100755
--- a/sklearn/externals/joblib/pool.py
+++ b/sklearn/externals/joblib/pool.py
@@ -13,15 +13,7 @@
 # Copyright: 2012, Olivier Grisel
 # License: BSD 3 clause
 
-from mmap import mmap
-import errno
-import os
-import stat
 import sys
-import threading
-import atexit
-import tempfile
-import shutil
 import warnings
 from time import sleep
 
@@ -30,16 +22,6 @@
 except NameError:
     WindowsError = type(None)
 
-from pickle import whichmodule
-try:
-    # Python 2 compat
-    from cPickle import loads
-    from cPickle import dumps
-except ImportError:
-    from pickle import loads
-    from pickle import dumps
-    import copyreg
-
 # Customizable pure Python pickler in Python 2
 # customizable C-optimized pickler under Python 3.3+
 from pickle import Pickler
@@ -47,216 +29,21 @@
 from pickle import HIGHEST_PROTOCOL
 from io import BytesIO
 
+from .disk import delete_folder
+from ._memmapping_reducer import get_memmapping_reducers
 from ._multiprocessing_helpers import mp, assert_spawning
-# We need the class definition to derive from it not the multiprocessing.Pool
+
+# We need the class definition to derive from it, not the multiprocessing.Pool
 # factory function
 from multiprocessing.pool import Pool
 
 try:
     import numpy as np
-    from numpy.lib.stride_tricks import as_strided
 except ImportError:
     np = None
 
-from .numpy_pickle import load
-from .numpy_pickle import dump
-from .hashing import hash
-from .backports import make_memmap
-# Some system have a ramdisk mounted by default, we can use it instead of /tmp
-# as the default folder to dump big arrays to share with subprocesses
-SYSTEM_SHARED_MEM_FS = '/dev/shm'
-
-# Folder and file permissions to chmod temporary files generated by the
-# memmaping pool. Only the owner of the Python process can access the
-# temporary files and folder.
-FOLDER_PERMISSIONS = stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR
-FILE_PERMISSIONS = stat.S_IRUSR | stat.S_IWUSR
-
-###############################################################################
-# Support for efficient transient pickling of numpy data structures
-
-
-def _get_backing_memmap(a):
-    """Recursively look up the original np.memmap instance base if any."""
-    b = getattr(a, 'base', None)
-    if b is None:
-        # TODO: check scipy sparse datastructure if scipy is installed
-        # a nor its descendants do not have a memmap base
-        return None
-
-    elif isinstance(b, mmap):
-        # a is already a real memmap instance.
-        return a
-
-    else:
-        # Recursive exploration of the base ancestry
-        return _get_backing_memmap(b)
-
-
-def has_shareable_memory(a):
-    """Return True if a is backed by some mmap buffer directly or not."""
-    return _get_backing_memmap(a) is not None
-
-
-def _strided_from_memmap(filename, dtype, mode, offset, order, shape, strides,
-                         total_buffer_len):
-    """Reconstruct an array view on a memory mapped file."""
-    if mode == 'w+':
-        # Do not zero the original data when unpickling
-        mode = 'r+'
-
-    if strides is None:
-        # Simple, contiguous memmap
-        return make_memmap(filename, dtype=dtype, shape=shape, mode=mode,
-                           offset=offset, order=order)
-    else:
-        # For non-contiguous data, memmap the total enclosing buffer and then
-        # extract the non-contiguous view with the stride-tricks API
-        base = make_memmap(filename, dtype=dtype, shape=total_buffer_len,
-                           mode=mode, offset=offset, order=order)
-        return as_strided(base, shape=shape, strides=strides)
-
-
-def _reduce_memmap_backed(a, m):
-    """Pickling reduction for memmap backed arrays.
-
-    a is expected to be an instance of np.ndarray (or np.memmap)
-    m is expected to be an instance of np.memmap on the top of the ``base``
-    attribute ancestry of a. ``m.base`` should be the real python mmap object.
-    """
-    # offset that comes from the striding differences between a and m
-    a_start, a_end = np.byte_bounds(a)
-    m_start = np.byte_bounds(m)[0]
-    offset = a_start - m_start
-
-    # offset from the backing memmap
-    offset += m.offset
-
-    if m.flags['F_CONTIGUOUS']:
-        order = 'F'
-    else:
-        # The backing memmap buffer is necessarily contiguous hence C if not
-        # Fortran
-        order = 'C'
-
-    if a.flags['F_CONTIGUOUS'] or a.flags['C_CONTIGUOUS']:
-        # If the array is a contiguous view, no need to pass the strides
-        strides = None
-        total_buffer_len = None
-    else:
-        # Compute the total number of items to map from which the strided
-        # view will be extracted.
-        strides = a.strides
-        total_buffer_len = (a_end - a_start) // a.itemsize
-    return (_strided_from_memmap,
-            (m.filename, a.dtype, m.mode, offset, order, a.shape, strides,
-             total_buffer_len))
-
-
-def reduce_memmap(a):
-    """Pickle the descriptors of a memmap instance to reopen on same file."""
-    m = _get_backing_memmap(a)
-    if m is not None:
-        # m is a real mmap backed memmap instance, reduce a preserving striding
-        # information
-        return _reduce_memmap_backed(a, m)
-    else:
-        # This memmap instance is actually backed by a regular in-memory
-        # buffer: this can happen when using binary operators on numpy.memmap
-        # instances
-        return (loads, (dumps(np.asarray(a), protocol=HIGHEST_PROTOCOL),))
-
-
-class ArrayMemmapReducer(object):
-    """Reducer callable to dump large arrays to memmap files.
-
-    Parameters
-    ----------
-    max_nbytes: int
-        Threshold to trigger memmaping of large arrays to files created
-        a folder.
-    temp_folder: str
-        Path of a folder where files for backing memmaped arrays are created.
-    mmap_mode: 'r', 'r+' or 'c'
-        Mode for the created memmap datastructure. See the documentation of
-        numpy.memmap for more details. Note: 'w+' is coerced to 'r+'
-        automatically to avoid zeroing the data on unpickling.
-    verbose: int, optional, 0 by default
-        If verbose > 0, memmap creations are logged.
-        If verbose > 1, both memmap creations, reuse and array pickling are
-        logged.
-    prewarm: bool, optional, False by default.
-        Force a read on newly memmaped array to make sure that OS pre-cache it
-        memory. This can be useful to avoid concurrent disk access when the
-        same data array is passed to different worker processes.
-    """
-
-    def __init__(self, max_nbytes, temp_folder, mmap_mode, verbose=0,
-                 context_id=None, prewarm=True):
-        self._max_nbytes = max_nbytes
-        self._temp_folder = temp_folder
-        self._mmap_mode = mmap_mode
-        self.verbose = int(verbose)
-        self._prewarm = prewarm
-        if context_id is not None:
-            warnings.warn('context_id is deprecated and ignored in joblib'
-                          ' 0.9.4 and will be removed in 0.11',
-                          DeprecationWarning)
-
-    def __call__(self, a):
-        m = _get_backing_memmap(a)
-        if m is not None:
-            # a is already backed by a memmap file, let's reuse it directly
-            return _reduce_memmap_backed(a, m)
-
-        if (not a.dtype.hasobject
-                and self._max_nbytes is not None
-                and a.nbytes > self._max_nbytes):
-            # check that the folder exists (lazily create the pool temp folder
-            # if required)
-            try:
-                os.makedirs(self._temp_folder)
-                os.chmod(self._temp_folder, FOLDER_PERMISSIONS)
-            except OSError as e:
-                if e.errno != errno.EEXIST:
-                    raise e
-
-            # Find a unique, concurrent safe filename for writing the
-            # content of this array only once.
-            basename = "%d-%d-%s.pkl" % (
-                os.getpid(), id(threading.current_thread()), hash(a))
-            filename = os.path.join(self._temp_folder, basename)
-
-            # In case the same array with the same content is passed several
-            # times to the pool subprocess children, serialize it only once
-
-            # XXX: implement an explicit reference counting scheme to make it
-            # possible to delete temporary files as soon as the workers are
-            # done processing this data.
-            if not os.path.exists(filename):
-                if self.verbose > 0:
-                    print("Memmaping (shape=%r, dtype=%s) to new file %s" % (
-                        a.shape, a.dtype, filename))
-                for dumped_filename in dump(a, filename):
-                    os.chmod(dumped_filename, FILE_PERMISSIONS)
-
-                if self._prewarm:
-                    # Warm up the data to avoid concurrent disk access in
-                    # multiple children processes
-                    load(filename, mmap_mode=self._mmap_mode).max()
-            elif self.verbose > 1:
-                print("Memmaping (shape=%s, dtype=%s) to old file %s" % (
-                    a.shape, a.dtype, filename))
-
-            # The worker process will use joblib.load to memmap the data
-            return (load, (filename, self._mmap_mode))
-        else:
-            # do not convert a into memmap, let pickler do its usual copy with
-            # the default system pickler
-            if self.verbose > 1:
-                print("Pickling array (shape=%r, dtype=%s)." % (
-                    a.shape, a.dtype))
-            return (loads, (dumps(a, protocol=HIGHEST_PROTOCOL),))
+if sys.version_info[:2] > (2, 7):
+    import copyreg
 
 
 ###############################################################################
@@ -429,16 +216,7 @@ def _setup_queues(self):
         self._quick_get = self._outqueue._recv
 
 
-def delete_folder(folder_path):
-    """Utility function to cleanup a temporary folder if still existing."""
-    try:
-        if os.path.exists(folder_path):
-            shutil.rmtree(folder_path)
-    except WindowsError:
-        warnings.warn("Failed to clean temporary folder: %s" % folder_path)
-
-
-class MemmapingPool(PicklingPool):
+class MemmappingPool(PicklingPool):
     """Process pool that shares large arrays to avoid memory copy.
 
     This drop-in replacement for `multiprocessing.pool.Pool` makes
@@ -453,7 +231,7 @@ class MemmapingPool(PicklingPool):
 
     Furthermore large arrays from the parent process are automatically
     dumped to a temporary folder on the filesystem such as child
-    processes to access their content via memmaping (file system
+    processes to access their content via memmapping (file system
     backed shared memory).
 
     Note: it is important to call the terminate method to collect
@@ -468,7 +246,7 @@ class MemmapingPool(PicklingPool):
     initargs: tuple, optional
         Arguments passed to the initializer callable.
     temp_folder: str, optional
-        Folder to be used by the pool for memmaping large arrays
+        Folder to be used by the pool for memmapping large arrays
         for sharing memory with worker processes. If None, this will try in
         order:
         - a folder pointed by the JOBLIB_TEMP_FOLDER environment variable,
@@ -480,7 +258,7 @@ class MemmapingPool(PicklingPool):
     max_nbytes int or None, optional, 1e6 by default
         Threshold on the size of arrays passed to the workers that
         triggers automated memory mapping in temp_folder.
-        Use None to disable memmaping of large arrays.
+        Use None to disable memmapping of large arrays.
     mmap_mode: {'r+', 'r', 'w+', 'c'}
         Memmapping mode for numpy arrays passed to workers.
         See 'max_nbytes' parameter documentation for more details.
@@ -492,13 +270,14 @@ class MemmapingPool(PicklingPool):
         master process.
     verbose: int, optional
         Make it possible to monitor how the communication of numpy arrays
-        with the subprocess is handled (pickling or memmaping)
+        with the subprocess is handled (pickling or memmapping)
     prewarm: bool or str, optional, "auto" by default.
-        If True, force a read on newly memmaped array to make sure that OS pre-
-        cache it in memory. This can be useful to avoid concurrent disk access
-        when the same data array is passed to different worker processes.
-        If "auto" (by default), prewarm is set to True, unless the Linux shared
-        memory partition /dev/shm is available and used as temp_folder.
+        If True, force a read on newly memmapped array to make sure that OS
+        pre-cache it in memory. This can be useful to avoid concurrent disk
+        access when the same data array is passed to different worker
+        processes. If "auto" (by default), prewarm is set to True, unless the
+        Linux shared memory partition /dev/shm is available and used as temp
+        folder.
 
     `forward_reducers` and `backward_reducers` are expected to be
     dictionaries with key/values being `(type, callable)` pairs where
@@ -513,97 +292,31 @@ class MemmapingPool(PicklingPool):
     def __init__(self, processes=None, temp_folder=None, max_nbytes=1e6,
                  mmap_mode='r', forward_reducers=None, backward_reducers=None,
                  verbose=0, context_id=None, prewarm=False, **kwargs):
-        if forward_reducers is None:
-            forward_reducers = dict()
-        if backward_reducers is None:
-            backward_reducers = dict()
+
         if context_id is not None:
             warnings.warn('context_id is deprecated and ignored in joblib'
                           ' 0.9.4 and will be removed in 0.11',
                           DeprecationWarning)
 
-        # Prepare a sub-folder name for the serialization of this particular
-        # pool instance (do not create in advance to spare FS write access if
-        # no array is to be dumped):
-        use_shared_mem = False
-        pool_folder_name = "joblib_memmaping_pool_%d_%d" % (
-            os.getpid(), id(self))
-        if temp_folder is None:
-            temp_folder = os.environ.get('JOBLIB_TEMP_FOLDER', None)
-        if temp_folder is None:
-            if os.path.exists(SYSTEM_SHARED_MEM_FS):
-                try:
-                    temp_folder = SYSTEM_SHARED_MEM_FS
-                    pool_folder = os.path.join(temp_folder, pool_folder_name)
-                    if not os.path.exists(pool_folder):
-                        os.makedirs(pool_folder)
-                    use_shared_mem = True
-                except IOError:
-                    # Missing rights in the /dev/shm partition,
-                    # fallback to regular temp folder.
-                    temp_folder = None
-        if temp_folder is None:
-            # Fallback to the default tmp folder, typically /tmp
-            temp_folder = tempfile.gettempdir()
-        temp_folder = os.path.abspath(os.path.expanduser(temp_folder))
-        pool_folder = os.path.join(temp_folder, pool_folder_name)
-        self._temp_folder = pool_folder
-
-        # Register the garbage collector at program exit in case caller forgets
-        # to call terminate explicitly: note we do not pass any reference to
-        # self to ensure that this callback won't prevent garbage collection of
-        # the pool instance and related file handler resources such as POSIX
-        # semaphores and pipes
-        pool_module_name = whichmodule(delete_folder, 'delete_folder')
-
-        def _cleanup():
-            # In some cases the Python runtime seems to set delete_folder to
-            # None just before exiting when accessing the delete_folder
-            # function from the closure namespace. So instead we reimport
-            # the delete_folder function explicitly.
-            # https://github.com/joblib/joblib/issues/328
-            # We cannot just use from 'joblib.pool import delete_folder'
-            # because joblib should only use relative imports to allow
-            # easy vendoring.
-            delete_folder = __import__(
-                pool_module_name, fromlist=['delete_folder']).delete_folder
-            delete_folder(pool_folder)
-
-        atexit.register(_cleanup)
-
-        if np is not None:
-            # Register smart numpy.ndarray reducers that detects memmap backed
-            # arrays and that is else able to dump to memmap large in-memory
-            # arrays over the max_nbytes threshold
-            if prewarm == "auto":
-                prewarm = not use_shared_mem
-            forward_reduce_ndarray = ArrayMemmapReducer(
-                max_nbytes, pool_folder, mmap_mode, verbose,
+        forward_reducers, backward_reducers, self._temp_folder = \
+            get_memmapping_reducers(
+                id(self), temp_folder=temp_folder, max_nbytes=max_nbytes,
+                mmap_mode=mmap_mode, forward_reducers=forward_reducers,
+                backward_reducers=backward_reducers, verbose=verbose,
                 prewarm=prewarm)
-            forward_reducers[np.ndarray] = forward_reduce_ndarray
-            forward_reducers[np.memmap] = reduce_memmap
-
-            # Communication from child process to the parent process always
-            # pickles in-memory numpy.ndarray without dumping them as memmap
-            # to avoid confusing the caller and make it tricky to collect the
-            # temporary folder
-            backward_reduce_ndarray = ArrayMemmapReducer(
-                None, pool_folder, mmap_mode, verbose)
-            backward_reducers[np.ndarray] = backward_reduce_ndarray
-            backward_reducers[np.memmap] = reduce_memmap
 
         poolargs = dict(
             processes=processes,
             forward_reducers=forward_reducers,
             backward_reducers=backward_reducers)
         poolargs.update(kwargs)
-        super(MemmapingPool, self).__init__(**poolargs)
+        super(MemmappingPool, self).__init__(**poolargs)
 
     def terminate(self):
         n_retries = 10
         for i in range(n_retries):
             try:
-                super(MemmapingPool, self).terminate()
+                super(MemmappingPool, self).terminate()
                 break
             except OSError as e:
                 if isinstance(e, WindowsError):
diff --git a/sklearn/externals/joblib/testing.py b/sklearn/externals/joblib/testing.py
new file mode 100755
index 0000000000..5426c63386
--- /dev/null
+++ b/sklearn/externals/joblib/testing.py
@@ -0,0 +1,79 @@
+"""
+Helper for testing.
+"""
+
+import sys
+import warnings
+import os.path
+import re
+import subprocess
+import threading
+
+import pytest
+import _pytest
+
+from sklearn.externals.joblib._compat import PY3_OR_LATER
+
+
+raises = pytest.raises
+warns = pytest.warns
+SkipTest = _pytest.runner.Skipped
+skipif = pytest.mark.skipif
+fixture = pytest.fixture
+parametrize = pytest.mark.parametrize
+timeout = pytest.mark.timeout
+
+
+def warnings_to_stdout():
+    """ Redirect all warnings to stdout.
+    """
+    showwarning_orig = warnings.showwarning
+
+    def showwarning(msg, cat, fname, lno, file=None, line=0):
+        showwarning_orig(msg, cat, os.path.basename(fname), line, sys.stdout)
+
+    warnings.showwarning = showwarning
+    # warnings.simplefilter('always')
+
+
+def check_subprocess_call(cmd, timeout=5, stdout_regex=None,
+                          stderr_regex=None):
+    """Runs a command in a subprocess with timeout in seconds.
+
+    Also checks returncode is zero, stdout if stdout_regex is set, and
+    stderr if stderr_regex is set.
+    """
+    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE,
+                            stderr=subprocess.PIPE)
+
+    def kill_process():
+        warnings.warn("Timeout running {}".format(cmd))
+        proc.kill()
+
+    timer = threading.Timer(timeout, kill_process)
+    try:
+        timer.start()
+        stdout, stderr = proc.communicate()
+
+        if PY3_OR_LATER:
+            stdout, stderr = stdout.decode(), stderr.decode()
+        if proc.returncode != 0:
+            message = (
+                'Non-zero return code: {}.\nStdout:\n{}\n'
+                'Stderr:\n{}').format(
+                    proc.returncode, stdout, stderr)
+            raise ValueError(message)
+
+        if (stdout_regex is not None and
+                not re.search(stdout_regex, stdout)):
+            raise ValueError(
+                "Unexpected stdout: {!r} does not match:\n{!r}".format(
+                    stdout_regex, stdout))
+        if (stderr_regex is not None and
+                not re.search(stderr_regex, stderr)):
+            raise ValueError(
+                "Unexpected stderr: {!r} does not match:\n{!r}".format(
+                    stderr_regex, stderr))
+
+    finally:
+        timer.cancel()
diff --git a/sklearn/externals/setup.py b/sklearn/externals/setup.py
index 936f032722..452f7d25d0 100755
--- a/sklearn/externals/setup.py
+++ b/sklearn/externals/setup.py
@@ -5,5 +5,9 @@ def configuration(parent_package='', top_path=None):
     from numpy.distutils.misc_util import Configuration
     config = Configuration('externals', parent_package, top_path)
     config.add_subpackage('joblib')
+    config.add_subpackage('joblib/externals')
+    config.add_subpackage('joblib/externals/loky')
+    config.add_subpackage('joblib/externals/loky/backend')
+    config.add_subpackage('joblib/externals/cloudpickle')
 
     return config
diff --git a/sklearn/feature_extraction/image.py b/sklearn/feature_extraction/image.py
index 1fe28d6739..b6e68537c0 100755
--- a/sklearn/feature_extraction/image.py
+++ b/sklearn/feature_extraction/image.py
@@ -286,7 +286,7 @@ def extract_patches(arr, patch_shape=8, extraction_step=1):
 
     patch_strides = arr.strides
 
-    slices = [slice(None, None, st) for st in extraction_step]
+    slices = tuple(slice(None, None, st) for st in extraction_step)
     indexing_strides = arr[slices].strides
 
     patch_indices_shape = ((np.array(arr.shape) - np.array(patch_shape)) //
diff --git a/sklearn/feature_extraction/tests/test_image.py b/sklearn/feature_extraction/tests/test_image.py
index 4908cbde56..516c18c2b9 100755
--- a/sklearn/feature_extraction/tests/test_image.py
+++ b/sklearn/feature_extraction/tests/test_image.py
@@ -304,9 +304,9 @@ def test_extract_patches_strided():
         ndim = len(image_shape)
 
         assert_true(patches.shape[:ndim] == expected_view)
-        last_patch_slices = [slice(i, i + j, None) for i, j in
-                             zip(last_patch, patch_size)]
-        assert_true((patches[[slice(-1, None, None)] * ndim] ==
+        last_patch_slices = tuple(slice(i, i + j, None) for i, j in
+                                  zip(last_patch, patch_size))
+        assert_true((patches[(-1, None, None) * ndim] ==
                     image[last_patch_slices].squeeze()).all())
 
 
diff --git a/sklearn/feature_selection/from_model.py b/sklearn/feature_selection/from_model.py
index f6f9c2776b..3e2efdbeb1 100755
--- a/sklearn/feature_selection/from_model.py
+++ b/sklearn/feature_selection/from_model.py
@@ -16,12 +16,13 @@ def _get_feature_importances(estimator, norm_order=1):
     """Retrieve or aggregate feature importances from estimator"""
     importances = getattr(estimator, "feature_importances_", None)
 
-    if importances is None and hasattr(estimator, "coef_"):
+    coef_ = getattr(estimator, "coef_", None)
+    if importances is None and coef_ is not None:
         if estimator.coef_.ndim == 1:
-            importances = np.abs(estimator.coef_)
+            importances = np.abs(coef_)
 
         else:
-            importances = np.linalg.norm(estimator.coef_, axis=0,
+            importances = np.linalg.norm(coef_, axis=0,
                                          ord=norm_order)
 
     elif importances is None:
diff --git a/sklearn/feature_selection/rfe.py b/sklearn/feature_selection/rfe.py
index b02ae49c00..6aa4c10174 100755
--- a/sklearn/feature_selection/rfe.py
+++ b/sklearn/feature_selection/rfe.py
@@ -15,7 +15,7 @@
 from ..base import MetaEstimatorMixin
 from ..base import clone
 from ..base import is_classifier
-from ..utils import Parallel, delayed
+from ..utils import Parallel, delayed, effective_n_jobs
 from ..model_selection import check_cv
 from ..model_selection._validation import _score
 from ..metrics.scorer import check_scoring
@@ -60,12 +60,12 @@ class RFE(BaseEstimator, MetaEstimatorMixin, SelectorMixin):
         are selected.
 
     step : int or float, optional (default=1)
-        If greater than or equal to 1, then `step` corresponds to the (integer)
-        number of features to remove at each iteration.
-        If within (0.0, 1.0), then `step` corresponds to the percentage
+        If greater than or equal to 1, then ``step`` corresponds to the
+        (integer) number of features to remove at each iteration.
+        If within (0.0, 1.0), then ``step`` corresponds to the percentage
         (rounded down) of features to remove at each iteration.
 
-    verbose : int, default=0
+    verbose : int, (default=0)
         Controls verbosity of output.
 
     Attributes
@@ -335,10 +335,18 @@ class RFECV(RFE, MetaEstimatorMixin):
         attribute or through a ``feature_importances_`` attribute.
 
     step : int or float, optional (default=1)
-        If greater than or equal to 1, then `step` corresponds to the (integer)
-        number of features to remove at each iteration.
-        If within (0.0, 1.0), then `step` corresponds to the percentage
+        If greater than or equal to 1, then ``step`` corresponds to the
+        (integer) number of features to remove at each iteration.
+        If within (0.0, 1.0), then ``step`` corresponds to the percentage
         (rounded down) of features to remove at each iteration.
+        Note that the last iteration may remove fewer than ``step`` features in
+        order to reach ``min_features_to_select``.
+
+    min_features_to_select : int, (default=1)
+        The minimum number of features to be selected. This number of features
+        will always be scored, even if the difference between the original
+        feature count and ``min_features_to_select`` isn't divisible by
+        ``step``.
 
     cv : int, cross-validation generator or an iterable, optional
         Determines the cross-validation splitting strategy.
@@ -358,21 +366,22 @@ class RFECV(RFE, MetaEstimatorMixin):
         cross-validation strategies that can be used here.
 
         .. versionchanged:: 0.20
-            ``cv`` default value if None will change from 3-fold to 5-fold
+            ``cv`` default value of None will change from 3-fold to 5-fold
             in v0.22.
 
-    scoring : string, callable or None, optional, default: None
+    scoring : string, callable or None, optional, (default=None)
         A string (see model evaluation documentation) or
         a scorer callable object / function with signature
         ``scorer(estimator, X, y)``.
 
-    verbose : int, default=0
+    verbose : int, (default=0)
         Controls verbosity of output.
 
-    n_jobs : int, default 1
+    n_jobs : int or None, optional (default=None)
         Number of cores to run in parallel while fitting across folds.
-        Defaults to 1 core. If `n_jobs=-1`, then number of jobs is set
-        to number of cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -399,7 +408,8 @@ class RFECV(RFE, MetaEstimatorMixin):
 
     Notes
     -----
-    The size of ``grid_scores_`` is equal to ceil((n_features - 1) / step) + 1,
+    The size of ``grid_scores_`` is equal to
+    ``ceil((n_features - min_features_to_select) / step) + 1``,
     where step is the number of features removed at each iteration.
 
     Examples
@@ -431,14 +441,15 @@ class RFECV(RFE, MetaEstimatorMixin):
            for cancer classification using support vector machines",
            Mach. Learn., 46(1-3), 389--422, 2002.
     """
-    def __init__(self, estimator, step=1, cv='warn', scoring=None, verbose=0,
-                 n_jobs=1):
+    def __init__(self, estimator, step=1, min_features_to_select=1, cv='warn',
+                 scoring=None, verbose=0, n_jobs=None):
         self.estimator = estimator
         self.step = step
         self.cv = cv
         self.scoring = scoring
         self.verbose = verbose
         self.n_jobs = n_jobs
+        self.min_features_to_select = min_features_to_select
 
     def fit(self, X, y, groups=None):
         """Fit the RFE model and automatically tune the number of selected
@@ -464,7 +475,6 @@ def fit(self, X, y, groups=None):
         cv = check_cv(self.cv, y, is_classifier(self.estimator))
         scorer = check_scoring(self.estimator, scoring=self.scoring)
         n_features = X.shape[1]
-        n_features_to_select = 1
 
         if 0.0 < self.step < 1.0:
             step = int(max(1, self.step * n_features))
@@ -473,8 +483,10 @@ def fit(self, X, y, groups=None):
         if step <= 0:
             raise ValueError("Step must be >0")
 
+        # Build an RFE object, which will evaluate and score each possible
+        # feature count, down to self.min_features_to_select
         rfe = RFE(estimator=self.estimator,
-                  n_features_to_select=n_features_to_select,
+                  n_features_to_select=self.min_features_to_select,
                   step=self.step, verbose=self.verbose)
 
         # Determine the number of subsets of features by fitting across
@@ -489,10 +501,11 @@ def fit(self, X, y, groups=None):
         # and provides bound methods as scorers is not broken with the
         # addition of n_jobs parameter in version 0.18.
 
-        if self.n_jobs == 1:
+        if effective_n_jobs(self.n_jobs) == 1:
             parallel, func = list, _rfe_single_fit
         else:
-            parallel, func, = Parallel(n_jobs=self.n_jobs), delayed(_rfe_single_fit)
+            parallel = Parallel(n_jobs=self.n_jobs)
+            func = delayed(_rfe_single_fit)
 
         scores = parallel(
             func(rfe, self.estimator, X, y, train, test, scorer)
@@ -503,7 +516,7 @@ def fit(self, X, y, groups=None):
         argmax_idx = len(scores) - np.argmax(scores_rev) - 1
         n_features_to_select = max(
             n_features - (argmax_idx * step),
-            n_features_to_select)
+            self.min_features_to_select)
 
         # Re-execute an elimination with best_k over the whole set
         rfe = RFE(estimator=self.estimator,
diff --git a/sklearn/feature_selection/tests/test_from_model.py b/sklearn/feature_selection/tests/test_from_model.py
index e6bb76c5e1..47e62eb8e7 100755
--- a/sklearn/feature_selection/tests/test_from_model.py
+++ b/sklearn/feature_selection/tests/test_from_model.py
@@ -8,7 +8,6 @@
 from sklearn.utils.testing import assert_greater
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_array_equal
-from sklearn.utils.testing import assert_almost_equal
 from sklearn.utils.testing import assert_allclose
 from sklearn.utils.testing import assert_raises
 from sklearn.utils.testing import skip_if_32bit
@@ -178,6 +177,8 @@ def test_feature_importances():
         assert_array_almost_equal(X_new, X[:, feature_mask])
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_sample_weight():
     # Ensure sample weights are passed to underlying estimator
     X, y = datasets.make_classification(
@@ -214,6 +215,8 @@ def test_coef_default_threshold():
     assert_array_almost_equal(X_new, X[:, mask])
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @skip_if_32bit
 def test_2d_coef():
     X, y = datasets.make_classification(
diff --git a/sklearn/feature_selection/tests/test_rfe.py b/sklearn/feature_selection/tests/test_rfe.py
index 29854bb1df..41b4a9e767 100755
--- a/sklearn/feature_selection/tests/test_rfe.py
+++ b/sklearn/feature_selection/tests/test_rfe.py
@@ -1,6 +1,8 @@
 """
 Testing Recursive feature elimination
 """
+from __future__ import division
+
 import pytest
 import numpy as np
 from numpy.testing import assert_array_almost_equal, assert_array_equal
@@ -229,6 +231,26 @@ def test_rfecv_verbose_output():
     assert_greater(len(verbose_output.readline()), 0)
 
 
+def test_rfecv_grid_scores_size():
+    generator = check_random_state(0)
+    iris = load_iris()
+    X = np.c_[iris.data, generator.normal(size=(len(iris.data), 6))]
+    y = list(iris.target)   # regression test: list should be supported
+
+    # Non-regression test for varying combinations of step and
+    # min_features_to_select.
+    for step, min_features_to_select in [[2, 1], [2, 2], [3, 3]]:
+        rfecv = RFECV(estimator=MockClassifier(), step=step,
+                      min_features_to_select=min_features_to_select, cv=5)
+        rfecv.fit(X, y)
+
+        score_len = np.ceil(
+            (X.shape[1] - min_features_to_select) / step) + 1
+        assert len(rfecv.grid_scores_) == score_len
+        assert len(rfecv.ranking_) == X.shape[1]
+        assert rfecv.n_features_ >= min_features_to_select
+
+
 @pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_rfe_estimator_tags():
     rfe = RFE(SVC(kernel='linear'))
diff --git a/sklearn/feature_selection/univariate_selection.py b/sklearn/feature_selection/univariate_selection.py
index 360c312b55..6534667339 100755
--- a/sklearn/feature_selection/univariate_selection.py
+++ b/sklearn/feature_selection/univariate_selection.py
@@ -389,6 +389,17 @@ class SelectPercentile(_BaseFilter):
     pvalues_ : array-like, shape=(n_features,)
         p-values of feature scores, None if `score_func` returned only scores.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.feature_selection import SelectPercentile, chi2
+    >>> X, y = load_digits(return_X_y=True)
+    >>> X.shape
+    (1797, 64)
+    >>> X_new = SelectPercentile(chi2, percentile=10).fit_transform(X, y)
+    >>> X_new.shape
+    (1797, 7)
+
     Notes
     -----
     Ties between features with equal scores will be broken in an unspecified
@@ -427,8 +438,7 @@ def _get_support_mask(self):
             return np.zeros(len(self.scores_), dtype=np.bool)
 
         scores = _clean_nans(self.scores_)
-        threshold = stats.scoreatpercentile(scores,
-                                            100 - self.percentile)
+        threshold = np.percentile(scores, 100 - self.percentile)
         mask = scores > threshold
         ties = np.where(scores == threshold)[0]
         if len(ties):
@@ -463,6 +473,17 @@ class SelectKBest(_BaseFilter):
     pvalues_ : array-like, shape=(n_features,)
         p-values of feature scores, None if `score_func` returned only scores.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.feature_selection import SelectKBest, chi2
+    >>> X, y = load_digits(return_X_y=True)
+    >>> X.shape
+    (1797, 64)
+    >>> X_new = SelectKBest(chi2, k=20).fit_transform(X, y)
+    >>> X_new.shape
+    (1797, 20)
+
     Notes
     -----
     Ties between features with equal scores will be broken in an unspecified
@@ -536,6 +557,17 @@ class SelectFpr(_BaseFilter):
     pvalues_ : array-like, shape=(n_features,)
         p-values of feature scores.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_breast_cancer
+    >>> from sklearn.feature_selection import SelectFpr, chi2
+    >>> X, y = load_breast_cancer(return_X_y=True)
+    >>> X.shape
+    (569, 30)
+    >>> X_new = SelectFpr(chi2, alpha=0.01).fit_transform(X, y)
+    >>> X_new.shape
+    (569, 16)
+
     See also
     --------
     f_classif: ANOVA F-value between label/feature for classification tasks.
@@ -579,6 +611,16 @@ class SelectFdr(_BaseFilter):
     alpha : float, optional
         The highest uncorrected p-value for features to keep.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_breast_cancer
+    >>> from sklearn.feature_selection import SelectFdr, chi2
+    >>> X, y = load_breast_cancer(return_X_y=True)
+    >>> X.shape
+    (569, 30)
+    >>> X_new = SelectFdr(chi2, alpha=0.01).fit_transform(X, y)
+    >>> X_new.shape
+    (569, 16)
 
     Attributes
     ----------
@@ -638,6 +680,17 @@ class SelectFwe(_BaseFilter):
     alpha : float, optional
         The highest uncorrected p-value for features to keep.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_breast_cancer
+    >>> from sklearn.feature_selection import SelectFwe, chi2
+    >>> X, y = load_breast_cancer(return_X_y=True)
+    >>> X.shape
+    (569, 30)
+    >>> X_new = SelectFwe(chi2, alpha=0.01).fit_transform(X, y)
+    >>> X_new.shape
+    (569, 15)
+
     Attributes
     ----------
     scores_ : array-like, shape=(n_features,)
@@ -700,6 +753,18 @@ class GenericUnivariateSelect(_BaseFilter):
     pvalues_ : array-like, shape=(n_features,)
         p-values of feature scores, None if `score_func` returned scores only.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_breast_cancer
+    >>> from sklearn.feature_selection import GenericUnivariateSelect, chi2
+    >>> X, y = load_breast_cancer(return_X_y=True)
+    >>> X.shape
+    (569, 30)
+    >>> transformer = GenericUnivariateSelect(chi2, 'k_best', param=20)
+    >>> X_new = transformer.fit_transform(X, y)
+    >>> X_new.shape
+    (569, 20)
+
     See also
     --------
     f_classif: ANOVA F-value between label/feature for classification tasks.
diff --git a/sklearn/gaussian_process/gpc.py b/sklearn/gaussian_process/gpc.py
index 51da24fccc..bca6bc506d 100755
--- a/sklearn/gaussian_process/gpc.py
+++ b/sklearn/gaussian_process/gpc.py
@@ -534,11 +534,11 @@ def optimizer(obj_func, initial_theta, bounds):
         Note that "one_vs_one" does not support predicting probability
         estimates.
 
-    n_jobs : int, optional, default: 1
-        The number of jobs to use for the computation. If -1 all CPUs are used.
-        If 1 is given, no parallel computing code is used at all, which is
-        useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are
-        used. Thus for n_jobs = -2, all CPUs but one are used.
+    n_jobs : int or None, optional (default=None)
+        The number of jobs to use for the computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -558,12 +558,27 @@ def optimizer(obj_func, initial_theta, bounds):
     n_classes_ : int
         The number of classes in the training data
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_iris
+    >>> from sklearn.gaussian_process import GaussianProcessClassifier
+    >>> from sklearn.gaussian_process.kernels import RBF
+    >>> X, y = load_iris(return_X_y=True)
+    >>> kernel = 1.0 * RBF(1.0)
+    >>> gpc = GaussianProcessClassifier(kernel=kernel,
+    ...         random_state=0).fit(X, y)
+    >>> gpc.score(X, y) # doctest: +ELLIPSIS
+    0.9866...
+    >>> gpc.predict_proba(X[:2,:])
+    array([[0.83548752, 0.03228706, 0.13222543],
+           [0.79064206, 0.06525643, 0.14410151]])
+
     .. versionadded:: 0.18
     """
     def __init__(self, kernel=None, optimizer="fmin_l_bfgs_b",
                  n_restarts_optimizer=0, max_iter_predict=100,
                  warm_start=False, copy_X_train=True, random_state=None,
-                 multi_class="one_vs_rest", n_jobs=1):
+                 multi_class="one_vs_rest", n_jobs=None):
         self.kernel = kernel
         self.optimizer = optimizer
         self.n_restarts_optimizer = n_restarts_optimizer
diff --git a/sklearn/gaussian_process/gpr.py b/sklearn/gaussian_process/gpr.py
index 27043c46dd..ac2c0a46b6 100755
--- a/sklearn/gaussian_process/gpr.py
+++ b/sklearn/gaussian_process/gpr.py
@@ -132,6 +132,20 @@ def optimizer(obj_func, initial_theta, bounds):
     log_marginal_likelihood_value_ : float
         The log-marginal-likelihood of ``self.kernel_.theta``
 
+    Examples
+    --------
+    >>> from sklearn.datasets import make_friedman2
+    >>> from sklearn.gaussian_process import GaussianProcessRegressor
+    >>> from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel
+    >>> X, y = make_friedman2(n_samples=500, noise=0, random_state=0)
+    >>> kernel = DotProduct() + WhiteKernel()
+    >>> gpr = GaussianProcessRegressor(kernel=kernel,
+    ...         random_state=0).fit(X, y)
+    >>> gpr.score(X, y) # doctest: +ELLIPSIS
+    0.3680...
+    >>> gpr.predict(X[:2,:], return_std=True) # doctest: +ELLIPSIS
+    (array([653.0..., 592.1...]), array([316.6..., 316.6...]))
+
     """
     def __init__(self, kernel=None, alpha=1e-10,
                  optimizer="fmin_l_bfgs_b", n_restarts_optimizer=0,
diff --git a/sklearn/kernel_approximation.py b/sklearn/kernel_approximation.py
index cbd8dcf51a..79d915fa1e 100755
--- a/sklearn/kernel_approximation.py
+++ b/sklearn/kernel_approximation.py
@@ -58,7 +58,7 @@ class RBFSampler(BaseEstimator, TransformerMixin):
     SGDClassifier(alpha=0.0001, average=False, class_weight=None,
            early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,
            l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,
-           n_iter=None, n_iter_no_change=5, n_jobs=1, penalty='l2',
+           n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',
            power_t=0.5, random_state=None, shuffle=True, tol=None,
            validation_fraction=0.1, verbose=0, warm_start=False)
     >>> clf.score(X_features, y)
@@ -167,7 +167,7 @@ class SkewedChi2Sampler(BaseEstimator, TransformerMixin):
     SGDClassifier(alpha=0.0001, average=False, class_weight=None,
            early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,
            l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=10,
-           n_iter=None, n_iter_no_change=5, n_jobs=1, penalty='l2',
+           n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',
            power_t=0.5, random_state=None, shuffle=True, tol=None,
            validation_fraction=0.1, verbose=0, warm_start=False)
     >>> clf.score(X_features, y)
@@ -274,6 +274,25 @@ class AdditiveChi2Sampler(BaseEstimator, TransformerMixin):
     sample_interval : float, optional
         Sampling interval. Must be specified when sample_steps not in {1,2,3}.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.linear_model import SGDClassifier
+    >>> from sklearn.kernel_approximation import AdditiveChi2Sampler
+    >>> X, y = load_digits(return_X_y=True)
+    >>> chi2sampler = AdditiveChi2Sampler(sample_steps=2)
+    >>> X_transformed = chi2sampler.fit_transform(X, y)
+    >>> clf = SGDClassifier(max_iter=5, random_state=0)
+    >>> clf.fit(X_transformed, y)
+    SGDClassifier(alpha=0.0001, average=False, class_weight=None,
+           early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,
+           l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,
+           n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',
+           power_t=0.5, random_state=0, shuffle=True, tol=None,
+           validation_fraction=0.1, verbose=0, warm_start=False)
+    >>> clf.score(X_transformed, y) # doctest: +ELLIPSIS
+    0.9543...
+
     Notes
     -----
     This estimator approximates a slightly different version of the additive
diff --git a/sklearn/linear_model/base.py b/sklearn/linear_model/base.py
index 2890cef43e..30a28cd507 100755
--- a/sklearn/linear_model/base.py
+++ b/sklearn/linear_model/base.py
@@ -381,10 +381,12 @@ class LinearRegression(LinearModel, RegressorMixin):
     copy_X : boolean, optional, default True
         If True, X will be copied; else, it may be overwritten.
 
-    n_jobs : int, optional, default 1
-        The number of jobs to use for the computation.
-        If -1 all CPUs are used. This will only provide speedup for
-        n_targets > 1 and sufficient large problems.
+    n_jobs : int or None, optional (default=None)
+        The number of jobs to use for the computation. This will only provide
+        speedup for n_targets > 1 and sufficient large problems.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -405,7 +407,7 @@ class LinearRegression(LinearModel, RegressorMixin):
     """
 
     def __init__(self, fit_intercept=True, normalize=False, copy_X=True,
-                 n_jobs=1):
+                 n_jobs=None):
         self.fit_intercept = fit_intercept
         self.normalize = normalize
         self.copy_X = copy_X
diff --git a/sklearn/linear_model/cd_fast.pyx b/sklearn/linear_model/cd_fast.pyx
index a51d1bdbdb..cd044824b4 100755
--- a/sklearn/linear_model/cd_fast.pyx
+++ b/sklearn/linear_model/cd_fast.pyx
@@ -141,10 +141,10 @@ cdef extern from "cblas.h":
 @cython.boundscheck(False)
 @cython.wraparound(False)
 @cython.cdivision(True)
-def enet_coordinate_descent(np.ndarray[floating, ndim=1] w,
+def enet_coordinate_descent(floating[::1] w,
                             floating alpha, floating beta,
-                            np.ndarray[floating, ndim=2, mode='fortran'] X,
-                            np.ndarray[floating, ndim=1, mode='c'] y,
+                            floating[::1, :] X,
+                            floating[::1] y,
                             int max_iter, floating tol,
                             object rng, bint random=0, bint positive=0):
     """Cython version of the coordinate descent algorithm
@@ -159,28 +159,29 @@ def enet_coordinate_descent(np.ndarray[floating, ndim=1] w,
     # fused types version of BLAS functions
     if floating is float:
         dtype = np.float32
+        gemv = sgemv
         dot = sdot
         axpy = saxpy
         asum = sasum
+        copy = scopy
     else:
         dtype = np.float64
+        gemv = dgemv
         dot = ddot
         axpy = daxpy
         asum = dasum
+        copy = dcopy
 
     # get the data information into easy vars
     cdef unsigned int n_samples = X.shape[0]
     cdef unsigned int n_features = X.shape[1]
 
-    # get the number of tasks indirectly, using strides
-    cdef unsigned int n_tasks = y.strides[0] / sizeof(floating)
-
     # compute norms of the columns of X
-    cdef np.ndarray[floating, ndim=1] norm_cols_X = (X**2).sum(axis=0)
+    cdef floating[::1] norm_cols_X = np.square(X).sum(axis=0)
 
     # initial value of the residuals
-    cdef np.ndarray[floating, ndim=1] R = np.empty(n_samples, dtype=dtype)
-    cdef np.ndarray[floating, ndim=1] XtA = np.empty(n_features, dtype=dtype)
+    cdef floating[::1] R = np.empty(n_samples, dtype=dtype)
+    cdef floating[::1] XtA = np.empty(n_features, dtype=dtype)
 
     cdef floating tmp
     cdef floating w_ii
@@ -202,23 +203,20 @@ def enet_coordinate_descent(np.ndarray[floating, ndim=1] w,
     cdef UINT32_t rand_r_state_seed = rng.randint(0, RAND_R_MAX)
     cdef UINT32_t* rand_r_state = &rand_r_state_seed
 
-    cdef floating *X_data = <floating*> X.data
-    cdef floating *y_data = <floating*> y.data
-    cdef floating *w_data = <floating*> w.data
-    cdef floating *R_data = <floating*> R.data
-    cdef floating *XtA_data = <floating*> XtA.data
-
     if alpha == 0 and beta == 0:
         warnings.warn("Coordinate descent with no regularization may lead to unexpected"
             " results and is discouraged.")
 
     with nogil:
         # R = y - np.dot(X, w)
-        for i in range(n_samples):
-            R[i] = y[i] - dot(n_features, &X_data[i], n_samples, w_data, 1)
+        copy(n_samples, &y[0], 1, &R[0], 1)
+        gemv(CblasColMajor, CblasNoTrans,
+             n_samples, n_features, -1.0, &X[0, 0], n_samples,
+             &w[0], 1,
+             1.0, &R[0], 1)
 
         # tol *= np.dot(y, y)
-        tol *= dot(n_samples, y_data, n_tasks, y_data, n_tasks)
+        tol *= dot(n_samples, &y[0], 1, &y[0], 1)
 
         for n_iter in range(max_iter):
             w_max = 0.0
@@ -236,11 +234,10 @@ def enet_coordinate_descent(np.ndarray[floating, ndim=1] w,
 
                 if w_ii != 0.0:
                     # R += w_ii * X[:,ii]
-                    axpy(n_samples, w_ii, &X_data[ii * n_samples], 1,
-                         R_data, 1)
+                    axpy(n_samples, w_ii, &X[0, ii], 1, &R[0], 1)
 
                 # tmp = (X[:,ii]*R).sum()
-                tmp = dot(n_samples, &X_data[ii * n_samples], 1, R_data, 1)
+                tmp = dot(n_samples, &X[0, ii], 1, &R[0], 1)
 
                 if positive and tmp < 0:
                     w[ii] = 0.0
@@ -250,8 +247,7 @@ def enet_coordinate_descent(np.ndarray[floating, ndim=1] w,
 
                 if w[ii] != 0.0:
                     # R -=  w[ii] * X[:,ii] # Update residual
-                    axpy(n_samples, -w[ii], &X_data[ii * n_samples], 1,
-                         R_data, 1)
+                    axpy(n_samples, -w[ii], &X[0, ii], 1, &R[0], 1)
 
                 # update the maximum absolute coefficient update
                 d_w_ii = fabs(w[ii] - w_ii)
@@ -270,19 +266,19 @@ def enet_coordinate_descent(np.ndarray[floating, ndim=1] w,
 
                 # XtA = np.dot(X.T, R) - beta * w
                 for i in range(n_features):
-                    XtA[i] = dot(n_samples, &X_data[i * n_samples],
-                                 1, R_data, 1) - beta * w[i]
+                    XtA[i] = (dot(n_samples, &X[0, i], 1, &R[0], 1)
+                              - beta * w[i])
 
                 if positive:
-                    dual_norm_XtA = max(n_features, XtA_data)
+                    dual_norm_XtA = max(n_features, &XtA[0])
                 else:
-                    dual_norm_XtA = abs_max(n_features, XtA_data)
+                    dual_norm_XtA = abs_max(n_features, &XtA[0])
 
                 # R_norm2 = np.dot(R, R)
-                R_norm2 = dot(n_samples, R_data, 1, R_data, 1)
+                R_norm2 = dot(n_samples, &R[0], 1, &R[0], 1)
 
                 # w_norm2 = np.dot(w, w)
-                w_norm2 = dot(n_features, w_data, 1, w_data, 1)
+                w_norm2 = dot(n_features, &w[0], 1, &w[0], 1)
 
                 if (dual_norm_XtA > alpha):
                     const = alpha / dual_norm_XtA
@@ -292,11 +288,11 @@ def enet_coordinate_descent(np.ndarray[floating, ndim=1] w,
                     const = 1.0
                     gap = R_norm2
 
-                l1_norm = asum(n_features, w_data, 1)
+                l1_norm = asum(n_features, &w[0], 1)
 
                 # np.dot(R.T, y)
                 gap += (alpha * l1_norm
-                        - const * dot(n_samples, R_data, 1, y_data, n_tasks)
+                        - const * dot(n_samples, &R[0], 1, &y[0], 1)
                         + 0.5 * beta * (1 + const ** 2) * (w_norm2))
 
                 if gap < tol:
@@ -308,7 +304,7 @@ def enet_coordinate_descent(np.ndarray[floating, ndim=1] w,
 @cython.boundscheck(False)
 @cython.wraparound(False)
 @cython.cdivision(True)
-def sparse_enet_coordinate_descent(floating [:] w,
+def sparse_enet_coordinate_descent(floating [::1] w,
                             floating alpha, floating beta,
                             np.ndarray[floating, ndim=1, mode='c'] X_data,
                             np.ndarray[int, ndim=1, mode='c'] X_indices,
@@ -336,9 +332,6 @@ def sparse_enet_coordinate_descent(floating [:] w,
     cdef unsigned int startptr = X_indptr[0]
     cdef unsigned int endptr
 
-    # get the number of tasks indirectly, using strides
-    cdef unsigned int n_tasks
-
     # initial value of the residuals
     cdef floating[:] R = y.copy()
 
@@ -348,12 +341,10 @@ def sparse_enet_coordinate_descent(floating [:] w,
     # fused types version of BLAS functions
     if floating is float:
         dtype = np.float32
-        n_tasks = y.strides[0] / sizeof(float)
         dot = sdot
         asum = sasum
     else:
         dtype = np.float64
-        n_tasks = y.strides[0] / sizeof(DOUBLE)
         dot = ddot
         asum = dasum
 
@@ -514,7 +505,7 @@ def sparse_enet_coordinate_descent(floating [:] w,
                 gap += (alpha * l1_norm - const * dot(
                             n_samples,
                             &R[0], 1,
-                            &y[0], n_tasks
+                            &y[0], 1
                             )
                         + 0.5 * beta * (1 + const ** 2) * w_norm2)
 
@@ -528,7 +519,8 @@ def sparse_enet_coordinate_descent(floating [:] w,
 @cython.boundscheck(False)
 @cython.wraparound(False)
 @cython.cdivision(True)
-def enet_coordinate_descent_gram(floating[:] w, floating alpha, floating beta,
+def enet_coordinate_descent_gram(floating[::1] w,
+                                 floating alpha, floating beta,
                                  np.ndarray[floating, ndim=2, mode='c'] Q,
                                  np.ndarray[floating, ndim=1, mode='c'] q,
                                  np.ndarray[floating, ndim=1] y,
diff --git a/sklearn/linear_model/coordinate_descent.py b/sklearn/linear_model/coordinate_descent.py
index 0205daa063..6fa71f2ddd 100755
--- a/sklearn/linear_model/coordinate_descent.py
+++ b/sklearn/linear_model/coordinate_descent.py
@@ -18,7 +18,7 @@
 from ..utils import check_array, check_X_y
 from ..utils.validation import check_random_state
 from ..model_selection import check_cv
-from ..utils import Parallel, delayed
+from ..utils import Parallel, delayed, effective_n_jobs
 from ..externals import six
 from ..externals.six.moves import xrange
 from ..utils.extmath import safe_sparse_dot
@@ -98,8 +98,8 @@ def _alpha_grid(X, y, Xy=None, l1_ratio=1.0, fit_intercept=True,
             # Workaround to find alpha_max for sparse matrices.
             # since we should not destroy the sparsity of such matrices.
             _, _, X_offset, _, X_scale = _preprocess_data(X, y, fit_intercept,
-                                                      normalize,
-                                                      return_mean=True)
+                                                          normalize,
+                                                          return_mean=True)
             mean_dot = X_offset * np.sum(y)
 
     if Xy.ndim == 1:
@@ -447,7 +447,7 @@ def enet_path(X, y, l1_ratio=0.5, eps=1e-3, n_alphas=100, alphas=None,
                          dtype=X.dtype)
 
     if coef_init is None:
-        coef_ = np.asfortranarray(np.zeros(coefs.shape[:-1], dtype=X.dtype))
+        coef_ = np.zeros(coefs.shape[:-1], dtype=X.dtype, order='F')
     else:
         coef_ = np.asfortranarray(coef_init, dtype=X.dtype)
 
@@ -753,8 +753,9 @@ def fit(self, X, y, check_input=True):
                           precompute=precompute, Xy=this_Xy,
                           fit_intercept=False, normalize=False, copy_X=True,
                           verbose=False, tol=self.tol, positive=self.positive,
-                          X_offset=X_offset, X_scale=X_scale, return_n_iter=True,
-                          coef_init=coef_[k], max_iter=self.max_iter,
+                          X_offset=X_offset, X_scale=X_scale,
+                          return_n_iter=True, coef_init=coef_[k],
+                          max_iter=self.max_iter,
                           random_state=self.random_state,
                           selection=self.selection,
                           check_input=False)
@@ -1053,7 +1054,7 @@ class LinearModelCV(six.with_metaclass(ABCMeta, LinearModel)):
     @abstractmethod
     def __init__(self, eps=1e-3, n_alphas=100, alphas=None, fit_intercept=True,
                  normalize=False, precompute='auto', max_iter=1000, tol=1e-4,
-                 copy_X=True, cv='warn', verbose=False, n_jobs=1,
+                 copy_X=True, cv='warn', verbose=False, n_jobs=None,
                  positive=False, random_state=None, selection='cyclic'):
         self.eps = eps
         self.n_alphas = n_alphas
@@ -1183,7 +1184,7 @@ def fit(self, X, y):
         path_params['copy_X'] = copy_X
         # We are not computing in parallel, we can modify X
         # inplace in the folds
-        if not (self.n_jobs == 1 or self.n_jobs is None):
+        if effective_n_jobs(self.n_jobs) > 1:
             path_params['copy_X'] = False
 
         # init cross-validation generator
@@ -1202,7 +1203,7 @@ def fit(self, X, y):
                 for this_l1_ratio, this_alphas in zip(l1_ratios, alphas)
                 for train, test in folds)
         mse_paths = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
-                             backend="threading")(jobs)
+                             prefer="threads")(jobs)
         mse_paths = np.reshape(mse_paths, (n_l1_ratio, len(folds), -1))
         mean_mse = np.mean(mse_paths, axis=1)
         self.mse_path_ = np.squeeze(np.rollaxis(mse_paths, 2, 1))
@@ -1319,9 +1320,11 @@ class LassoCV(LinearModelCV, RegressorMixin):
     verbose : bool or integer
         Amount of verbosity.
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the cross validation. If ``-1``, use
-        all the CPUs.
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the cross validation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     positive : bool, optional
         If positive, restrict regression coefficients to be positive
@@ -1386,7 +1389,7 @@ class LassoCV(LinearModelCV, RegressorMixin):
 
     def __init__(self, eps=1e-3, n_alphas=100, alphas=None, fit_intercept=True,
                  normalize=False, precompute='auto', max_iter=1000, tol=1e-4,
-                 copy_X=True, cv='warn', verbose=False, n_jobs=1,
+                 copy_X=True, cv='warn', verbose=False, n_jobs=None,
                  positive=False, random_state=None, selection='cyclic'):
         super(LassoCV, self).__init__(
             eps=eps, n_alphas=n_alphas, alphas=alphas,
@@ -1479,9 +1482,11 @@ class ElasticNetCV(LinearModelCV, RegressorMixin):
     verbose : bool or integer
         Amount of verbosity.
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the cross validation. If ``-1``, use
-        all the CPUs.
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the cross validation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     positive : bool, optional
         When set to ``True``, forces the coefficients to be positive.
@@ -1535,7 +1540,7 @@ class ElasticNetCV(LinearModelCV, RegressorMixin):
     >>> regr = ElasticNetCV(cv=5, random_state=0)
     >>> regr.fit(X, y)
     ElasticNetCV(alphas=None, copy_X=True, cv=5, eps=0.001, fit_intercept=True,
-           l1_ratio=0.5, max_iter=1000, n_alphas=100, n_jobs=1,
+           l1_ratio=0.5, max_iter=1000, n_alphas=100, n_jobs=None,
            normalize=False, positive=False, precompute='auto', random_state=0,
            selection='cyclic', tol=0.0001, verbose=0)
     >>> print(regr.alpha_) # doctest: +ELLIPSIS
@@ -1583,7 +1588,7 @@ class ElasticNetCV(LinearModelCV, RegressorMixin):
     def __init__(self, l1_ratio=0.5, eps=1e-3, n_alphas=100, alphas=None,
                  fit_intercept=True, normalize=False, precompute='auto',
                  max_iter=1000, tol=1e-4, cv='warn', copy_X=True,
-                 verbose=0, n_jobs=1, positive=False, random_state=None,
+                 verbose=0, n_jobs=None, positive=False, random_state=None,
                  selection='cyclic'):
         self.l1_ratio = l1_ratio
         self.eps = eps
@@ -1612,13 +1617,13 @@ class MultiTaskElasticNet(Lasso):
 
     The optimization objective for MultiTaskElasticNet is::
 
-        (1 / (2 * n_samples)) * ||Y - XW||^Fro_2
+        (1 / (2 * n_samples)) * ||Y - XW||_Fro^2
         + alpha * l1_ratio * ||W||_21
         + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2
 
     Where::
 
-        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}
+        ||W||_21 = sum_i sqrt(sum_j w_ij ^ 2)
 
     i.e. the sum of norm of each row.
 
@@ -2013,10 +2018,12 @@ class MultiTaskElasticNetCV(LinearModelCV, RegressorMixin):
     verbose : bool or integer
         Amount of verbosity.
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the cross validation. If ``-1``, use
-        all the CPUs. Note that this is used only if multiple values for
-        l1_ratio are given.
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the cross validation. Note that this is
+        used only if multiple values for l1_ratio are given.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional, default None
         The seed of the pseudo random number generator that selects a random
@@ -2067,7 +2074,7 @@ class MultiTaskElasticNetCV(LinearModelCV, RegressorMixin):
     ... #doctest: +NORMALIZE_WHITESPACE
     MultiTaskElasticNetCV(alphas=None, copy_X=True, cv=3, eps=0.001,
            fit_intercept=True, l1_ratio=0.5, max_iter=1000, n_alphas=100,
-           n_jobs=1, normalize=False, random_state=None, selection='cyclic',
+           n_jobs=None, normalize=False, random_state=None, selection='cyclic',
            tol=0.0001, verbose=0)
     >>> print(clf.coef_)
     [[0.52875032 0.46958558]
@@ -2093,7 +2100,8 @@ class MultiTaskElasticNetCV(LinearModelCV, RegressorMixin):
     def __init__(self, l1_ratio=0.5, eps=1e-3, n_alphas=100, alphas=None,
                  fit_intercept=True, normalize=False,
                  max_iter=1000, tol=1e-4, cv='warn', copy_X=True,
-                 verbose=0, n_jobs=1, random_state=None, selection='cyclic'):
+                 verbose=0, n_jobs=None, random_state=None,
+                 selection='cyclic'):
         self.l1_ratio = l1_ratio
         self.eps = eps
         self.n_alphas = n_alphas
@@ -2184,10 +2192,12 @@ class MultiTaskLassoCV(LinearModelCV, RegressorMixin):
     verbose : bool or integer
         Amount of verbosity.
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the cross validation. If ``-1``, use
-        all the CPUs. Note that this is used only if multiple values for
-        l1_ratio are given.
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the cross validation. Note that this is
+        used only if multiple values for l1_ratio are given.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional, default None
         The seed of the pseudo random number generator that selects a random
@@ -2242,7 +2252,7 @@ class MultiTaskLassoCV(LinearModelCV, RegressorMixin):
 
     def __init__(self, eps=1e-3, n_alphas=100, alphas=None, fit_intercept=True,
                  normalize=False, max_iter=1000, tol=1e-4, copy_X=True,
-                 cv='warn', verbose=False, n_jobs=1, random_state=None,
+                 cv='warn', verbose=False, n_jobs=None, random_state=None,
                  selection='cyclic'):
         super(MultiTaskLassoCV, self).__init__(
             eps=eps, n_alphas=n_alphas, alphas=alphas,
diff --git a/sklearn/linear_model/least_angle.py b/sklearn/linear_model/least_angle.py
index bae65fc889..d139560260 100755
--- a/sklearn/linear_model/least_angle.py
+++ b/sklearn/linear_model/least_angle.py
@@ -606,7 +606,8 @@ def __init__(self, fit_intercept=True, verbose=False, normalize=True,
         self.copy_X = copy_X
         self.fit_path = fit_path
 
-    def _get_gram(self, precompute, X, y):
+    @staticmethod
+    def _get_gram(precompute, X, y):
         if (not hasattr(precompute, '__array__')) and (
                 (precompute is True) or
                 (precompute == 'auto' and X.shape[0] > X.shape[1]) or
@@ -1021,9 +1022,11 @@ class LarsCV(Lars):
         The maximum number of points on the path used to compute the
         residuals in the cross-validation
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the cross validation. If ``-1``, use
-        all the CPUs
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the cross validation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     eps : float, optional
         The machine-precision regularization in the computation of the
@@ -1076,7 +1079,7 @@ class LarsCV(Lars):
 
     def __init__(self, fit_intercept=True, verbose=False, max_iter=500,
                  normalize=True, precompute='auto', cv='warn',
-                 max_n_alphas=1000, n_jobs=1, eps=np.finfo(np.float).eps,
+                 max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
                  copy_X=True, positive=False):
         self.max_iter = max_iter
         self.cv = cv
@@ -1234,9 +1237,11 @@ class LassoLarsCV(LarsCV):
         The maximum number of points on the path used to compute the
         residuals in the cross-validation
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the cross validation. If ``-1``, use
-        all the CPUs
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the cross validation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     eps : float, optional
         The machine-precision regularization in the computation of the
@@ -1306,7 +1311,7 @@ class LassoLarsCV(LarsCV):
 
     def __init__(self, fit_intercept=True, verbose=False, max_iter=500,
                  normalize=True, precompute='auto', cv='warn',
-                 max_n_alphas=1000, n_jobs=1, eps=np.finfo(np.float).eps,
+                 max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
                  copy_X=True, positive=False):
         self.fit_intercept = fit_intercept
         self.verbose = verbose
diff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py
index 895d5faa00..01a4f78ab0 100755
--- a/sklearn/linear_model/logistic.py
+++ b/sklearn/linear_model/logistic.py
@@ -32,7 +32,7 @@
 from ..exceptions import (NotFittedError, ConvergenceWarning,
                           ChangedBehaviorWarning)
 from ..utils.multiclass import check_classification_targets
-from ..utils import Parallel, delayed
+from ..utils import Parallel, delayed, effective_n_jobs
 from ..model_selection import check_cv
 from ..externals import six
 from ..metrics import get_scorer
@@ -61,8 +61,8 @@ def _intercept_dot(w, X, y):
         Coefficient vector without the intercept weight (w[-1]) if the
         intercept should be fit. Unchanged otherwise.
 
-    X : {array-like, sparse matrix}, shape (n_samples, n_features)
-        Training data. Unchanged.
+    c : float
+        The intercept.
 
     yz : float
         y * np.dot(X, w).
@@ -424,35 +424,61 @@ def hessp(v):
     return grad, hessp
 
 
-def _check_solver_option(solver, multi_class, penalty, dual):
-    if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:
-        raise ValueError("Logistic Regression supports only liblinear, "
-                         "newton-cg, lbfgs, sag and saga solvers, got %s"
-                         % solver)
-
-    if multi_class not in ['multinomial', 'ovr']:
-        raise ValueError("multi_class should be either multinomial or "
-                         "ovr, got %s" % multi_class)
-
+def _check_solver(solver, penalty, dual):
+    if solver == 'warn':
+        solver = 'liblinear'
+        warnings.warn("Default solver will be changed to 'lbfgs' in 0.22. "
+                      "Specify a solver to silence this warning.",
+                      FutureWarning)
+
+    all_solvers = ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']
+    if solver not in all_solvers:
+        raise ValueError("Logistic Regression supports only solvers in %s, got"
+                         " %s." % (all_solvers, solver))
+
+    all_penalties = ['l1', 'l2']
+    if penalty not in all_penalties:
+        raise ValueError("Logistic Regression supports only penalties in %s,"
+                         " got %s." % (all_penalties, penalty))
+
+    if solver not in ['liblinear', 'saga'] and penalty != 'l2':
+        raise ValueError("Solver %s supports only l2 penalties, "
+                         "got %s penalty." % (solver, penalty))
+    if solver != 'liblinear' and dual:
+        raise ValueError("Solver %s supports only "
+                         "dual=False, got dual=%s" % (solver, dual))
+    return solver
+
+
+def _check_multi_class(multi_class, solver, n_classes):
+    if multi_class == 'warn':
+        multi_class = 'ovr'
+        if n_classes > 2:
+            warnings.warn("Default multi_class will be changed to 'auto' in"
+                          " 0.22. Specify the multi_class option to silence "
+                          "this warning.", FutureWarning)
+    if multi_class == 'auto':
+        if solver == 'liblinear':
+            multi_class = 'ovr'
+        elif n_classes > 2:
+            multi_class = 'multinomial'
+        else:
+            multi_class = 'ovr'
+    if multi_class not in ('multinomial', 'ovr'):
+        raise ValueError("multi_class should be 'multinomial', 'ovr' or "
+                         "'auto'. Got %s." % multi_class)
     if multi_class == 'multinomial' and solver == 'liblinear':
         raise ValueError("Solver %s does not support "
                          "a multinomial backend." % solver)
+    return multi_class
 
-    if solver not in ['liblinear', 'saga']:
-        if penalty != 'l2':
-            raise ValueError("Solver %s supports only l2 penalties, "
-                             "got %s penalty." % (solver, penalty))
-    if solver != 'liblinear':
-        if dual:
-            raise ValueError("Solver %s supports only "
-                             "dual=False, got dual=%s" % (solver, dual))
 
 
 def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
                              max_iter=100, tol=1e-4, verbose=0,
                              solver='lbfgs', coef=None,
                              class_weight=None, dual=False, penalty='l2',
-                             intercept_scaling=1., multi_class='ovr',
+                             intercept_scaling=1., multi_class='warn',
                              random_state=None, check_input=True,
                              max_squared_sum=None, sample_weight=None):
     """Compute a Logistic Regression model for a list of regularization
@@ -471,7 +497,7 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
     X : array-like or sparse matrix, shape (n_samples, n_features)
         Input data.
 
-    y : array-like, shape (n_samples,)
+    y : array-like, shape (n_samples,) or (n_samples, n_targets)
         Input data, target values.
 
     pos_class : int, None
@@ -540,12 +566,18 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
         To lessen the effect of regularization on synthetic feature weight
         (and therefore on the intercept) intercept_scaling has to be increased.
 
-    multi_class : str, {'ovr', 'multinomial'}
-        Multiclass option can be either 'ovr' or 'multinomial'. If the option
-        chosen is 'ovr', then a binary problem is fit for each label. Else
-        the loss minimised is the multinomial loss fit across
-        the entire probability distribution. Does not work for 'liblinear'
-        solver.
+    multi_class : str, {'ovr', 'multinomial', 'auto'}, default: 'ovr'
+        If the option chosen is 'ovr', then a binary problem is fit for each
+        label. For 'multinomial' the loss minimised is the multinomial loss fit
+        across the entire probability distribution, *even when the data is
+        binary*. 'multinomial' is unavailable when solver='liblinear'.
+        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',
+        and otherwise selects 'multinomial'.
+
+        .. versionadded:: 0.18
+           Stochastic Average Gradient descent solver for 'multinomial' case.
+        .. versionchanged:: 0.20
+            Default will change from 'ovr' to 'auto' in 0.22.
 
     random_state : int, RandomState instance or None, optional, default None
         The seed of the pseudo random number generator to use when shuffling
@@ -572,7 +604,9 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
     coefs : ndarray, shape (n_cs, n_features) or (n_cs, n_features + 1)
         List of coefficients for the Logistic Regression model. If
         fit_intercept is set to True then the second dimension will be
-        n_features + 1, where the last item represents the intercept.
+        n_features + 1, where the last item represents the intercept. For
+        ``multiclass='multinomial'``, the shape is (n_classes, n_cs,
+        n_features) or (n_classes, n_cs, n_features + 1).
 
     Cs : ndarray
         Grid of Cs used for cross-validation.
@@ -591,7 +625,7 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
     if isinstance(Cs, numbers.Integral):
         Cs = np.logspace(-4, 4, Cs)
 
-    _check_solver_option(solver, multi_class, penalty, dual)
+    solver = _check_solver(solver, penalty, dual)
 
     # Preprocessing.
     if check_input:
@@ -600,9 +634,11 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
         y = check_array(y, ensure_2d=False, dtype=None)
         check_consistent_length(X, y)
     _, n_features = X.shape
+
     classes = np.unique(y)
     random_state = check_random_state(random_state)
 
+    multi_class = _check_multi_class(multi_class, solver, len(classes))
     if pos_class is None and multi_class != 'multinomial':
         if (classes.size > 2):
             raise ValueError('To fit OvR, use the pos_class argument')
@@ -759,16 +795,17 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
                              "'newton-cg', 'sag'}, got '%s' instead" % solver)
 
         if multi_class == 'multinomial':
-            multi_w0 = np.reshape(w0, (classes.size, -1))
-            if classes.size == 2:
+            n_classes = max(2, classes.size)
+            multi_w0 = np.reshape(w0, (n_classes, -1))
+            if n_classes == 2:
                 multi_w0 = multi_w0[1][np.newaxis, :]
-            coefs.append(multi_w0)
+            coefs.append(multi_w0.copy())
         else:
             coefs.append(w0.copy())
 
         n_iter[i] = n_iter_i
 
-    return coefs, np.array(Cs), n_iter
+    return np.array(coefs), np.array(Cs), n_iter
 
 
 # helper function for LogisticCV
@@ -777,7 +814,7 @@ def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,
                           max_iter=100, tol=1e-4, class_weight=None,
                           verbose=0, solver='lbfgs', penalty='l2',
                           dual=False, intercept_scaling=1.,
-                          multi_class='ovr', random_state=None,
+                          multi_class='warn', random_state=None,
                           max_squared_sum=None, sample_weight=None):
     """Computes scores across logistic_regression_path
 
@@ -862,11 +899,10 @@ def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,
         (and therefore on the intercept) intercept_scaling has to be increased.
 
     multi_class : str, {'ovr', 'multinomial'}
-        Multiclass option can be either 'ovr' or 'multinomial'. If the option
-        chosen is 'ovr', then a binary problem is fit for each label. Else
-        the loss minimised is the multinomial loss fit across
-        the entire probability distribution. Does not work for 'liblinear'
-        solver.
+        If the option chosen is 'ovr', then a binary problem is fit for each
+        label. For 'multinomial' the loss minimised is the multinomial loss fit
+        across the entire probability distribution, *even when the data is
+        binary*. 'multinomial' is unavailable when solver='liblinear'.
 
     random_state : int, RandomState instance or None, optional, default None
         The seed of the pseudo random number generator to use when shuffling
@@ -901,8 +937,6 @@ def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,
     n_iter : array, shape(n_cs,)
         Actual number of iteration for each Cs.
     """
-    _check_solver_option(solver, multi_class, penalty, dual)
-
     X_train = X[train]
     X_test = X[test]
     y_train = y[train]
@@ -923,7 +957,7 @@ def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,
         check_input=False, max_squared_sum=max_squared_sum,
         sample_weight=sample_weight)
 
-    log_reg = LogisticRegression(multi_class=multi_class)
+    log_reg = LogisticRegression(solver=solver, multi_class=multi_class)
 
     # The score method of Logistic Regression has a classes_ attribute.
     if multi_class == 'ovr':
@@ -1043,8 +1077,9 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,
         instance used by `np.random`. Used when ``solver`` == 'sag' or
         'liblinear'.
 
-    solver : str, {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},
-        default: 'liblinear'
+    solver : str, {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, \
+             default: 'liblinear'.
+
         Algorithm to use in the optimization problem.
 
         - For small datasets, 'liblinear' is a good choice, whereas 'sag' and
@@ -1063,20 +1098,25 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,
            Stochastic Average Gradient descent solver.
         .. versionadded:: 0.19
            SAGA solver.
+        .. versionchanged:: 0.20
+            Default will change from 'liblinear' to 'lbfgs' in 0.22.
 
     max_iter : int, default: 100
         Useful only for the newton-cg, sag and lbfgs solvers.
         Maximum number of iterations taken for the solvers to converge.
 
-    multi_class : str, {'ovr', 'multinomial'}, default: 'ovr'
-        Multiclass option can be either 'ovr' or 'multinomial'. If the option
-        chosen is 'ovr', then a binary problem is fit for each label. Else
-        the loss minimised is the multinomial loss fit across
-        the entire probability distribution. Does not work for 'liblinear'
-        solver.
+    multi_class : str, {'ovr', 'multinomial', 'auto'}, default: 'ovr'
+        If the option chosen is 'ovr', then a binary problem is fit for each
+        label. For 'multinomial' the loss minimised is the multinomial loss fit
+        across the entire probability distribution, *even when the data is
+        binary*. 'multinomial' is unavailable when solver='liblinear'.
+        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',
+        and otherwise selects 'multinomial'.
 
         .. versionadded:: 0.18
            Stochastic Average Gradient descent solver for 'multinomial' case.
+        .. versionchanged:: 0.20
+            Default will change from 'ovr' to 'auto' in 0.22.
 
     verbose : int, default: 0
         For the liblinear and lbfgs solvers set verbose to any positive
@@ -1090,11 +1130,13 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,
         .. versionadded:: 0.17
            *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.
 
-    n_jobs : int, default: 1
+    n_jobs : int or None, optional (default=None)
         Number of CPU cores used when parallelizing over classes if
         multi_class='ovr'". This parameter is ignored when the ``solver`` is
         set to 'liblinear' regardless of whether 'multi_class' is specified or
-        not. If given a value of -1, all cores are used.
+        not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`
+        context. ``-1`` means using all processors.
+        See :term:`Glossary <n_jobs>` for more details.
 
     Attributes
     ----------
@@ -1125,11 +1167,25 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,
             In SciPy <= 1.0.0 the number of lbfgs iterations may exceed
             ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_iris
+    >>> from sklearn.linear_model import LogisticRegression
+    >>> X, y = load_iris(return_X_y=True)
+    >>> clf = LogisticRegression(random_state=0, solver='lbfgs',
+    ...                          multi_class='multinomial').fit(X, y)
+    >>> clf.predict(X[:2, :])
+    array([0, 0])
+    >>> clf.predict_proba(X[:2, :]) # doctest: +ELLIPSIS
+    array([[9.8...e-01, 1.8...e-02, 1.4...e-08],
+           [9.7...e-01, 2.8...e-02, ...e-08]])
+    >>> clf.score(X, y)
+    0.97...
+
     See also
     --------
     SGDClassifier : incrementally trained logistic regression (when given
         the parameter ``loss="log"``).
-    sklearn.svm.LinearSVC : learns SVM models using the same algorithm.
     LogisticRegressionCV : Logistic regression with built-in cross validation
 
     Notes
@@ -1166,8 +1222,8 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,
 
     def __init__(self, penalty='l2', dual=False, tol=1e-4, C=1.0,
                  fit_intercept=True, intercept_scaling=1, class_weight=None,
-                 random_state=None, solver='liblinear', max_iter=100,
-                 multi_class='ovr', verbose=0, warm_start=False, n_jobs=1):
+                 random_state=None, solver='warn', max_iter=100,
+                 multi_class='warn', verbose=0, warm_start=False, n_jobs=None):
 
         self.penalty = penalty
         self.dual = dual
@@ -1193,7 +1249,7 @@ def fit(self, X, y, sample_weight=None):
             Training vector, where n_samples is the number of samples and
             n_features is the number of features.
 
-        y : array-like, shape (n_samples,)
+        y : array-like, shape (n_samples,) or (n_samples, n_targets)
             Target vector relative to X.
 
         sample_weight : array-like, shape (n_samples,) optional
@@ -1217,25 +1273,27 @@ def fit(self, X, y, sample_weight=None):
             raise ValueError("Tolerance for stopping criteria must be "
                              "positive; got (tol=%r)" % self.tol)
 
-        if self.solver in ['newton-cg']:
+        solver = _check_solver(self.solver, self.penalty, self.dual)
+
+        if solver in ['newton-cg']:
             _dtype = [np.float64, np.float32]
         else:
             _dtype = np.float64
 
         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order="C",
-                         accept_large_sparse=self.solver != 'liblinear')
+                         accept_large_sparse=solver != 'liblinear')
         check_classification_targets(y)
         self.classes_ = np.unique(y)
         n_samples, n_features = X.shape
 
-        _check_solver_option(self.solver, self.multi_class, self.penalty,
-                             self.dual)
+        multi_class = _check_multi_class(self.multi_class, solver,
+                                         len(self.classes_))
 
-        if self.solver == 'liblinear':
-            if self.n_jobs != 1:
+        if solver == 'liblinear':
+            if effective_n_jobs(self.n_jobs) != 1:
                 warnings.warn("'n_jobs' > 1 does not have any effect when"
                               " 'solver' is set to 'liblinear'. Got 'n_jobs'"
-                              " = {}.".format(self.n_jobs))
+                              " = {}.".format(effective_n_jobs(self.n_jobs)))
             self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                 X, y, self.C, self.fit_intercept, self.intercept_scaling,
                 self.class_weight, self.penalty, self.dual, self.verbose,
@@ -1244,7 +1302,7 @@ def fit(self, X, y, sample_weight=None):
             self.n_iter_ = np.array([n_iter_])
             return self
 
-        if self.solver in ['sag', 'saga']:
+        if solver in ['sag', 'saga']:
             max_squared_sum = row_norms(X, squared=True).max()
         else:
             max_squared_sum = None
@@ -1273,7 +1331,7 @@ def fit(self, X, y, sample_weight=None):
         self.intercept_ = np.zeros(n_classes)
 
         # Hack so that we iterate only once for the multinomial case.
-        if self.multi_class == 'multinomial':
+        if multi_class == 'multinomial':
             classes_ = [None]
             warm_start_coef = [warm_start_coef]
         if warm_start_coef is None:
@@ -1283,16 +1341,16 @@ def fit(self, X, y, sample_weight=None):
 
         # The SAG solver releases the GIL so it's more efficient to use
         # threads for this solver.
-        if self.solver in ['sag', 'saga']:
-            backend = 'threading'
+        if solver in ['sag', 'saga']:
+            prefer = 'threads'
         else:
-            backend = 'multiprocessing'
+            prefer = 'processes'
         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
-                               backend=backend)(
+                               prefer=prefer)(
             path_func(X, y, pos_class=class_, Cs=[self.C],
                       fit_intercept=self.fit_intercept, tol=self.tol,
-                      verbose=self.verbose, solver=self.solver,
-                      multi_class=self.multi_class, max_iter=self.max_iter,
+                      verbose=self.verbose, solver=solver,
+                      multi_class=multi_class, max_iter=self.max_iter,
                       class_weight=self.class_weight, check_input=False,
                       random_state=self.random_state, coef=warm_start_coef_,
                       penalty=self.penalty,
@@ -1303,7 +1361,7 @@ def fit(self, X, y, sample_weight=None):
         fold_coefs_, _, n_iter_ = zip(*fold_coefs_)
         self.n_iter_ = np.asarray(n_iter_, dtype=np.int32)[:, 0]
 
-        if self.multi_class == 'multinomial':
+        if multi_class == 'multinomial':
             self.coef_ = fold_coefs_[0][0]
         else:
             self.coef_ = np.asarray(fold_coefs_)
@@ -1341,7 +1399,11 @@ def predict_proba(self, X):
         """
         if not hasattr(self, "coef_"):
             raise NotFittedError("Call fit before prediction")
-        if self.multi_class == "ovr":
+
+        ovr = (self.multi_class in ["ovr", "warn"] or
+               (self.multi_class == 'auto' and (self.classes_.size <= 2 or
+                                                self.solver == 'liblinear')))
+        if ovr:
             return super(LogisticRegression, self)._predict_proba_lr(X)
         else:
             decision = self.decision_function(X)
@@ -1434,8 +1496,9 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,
         that can be used, look at :mod:`sklearn.metrics`. The
         default scoring option used is 'accuracy'.
 
-    solver : str, {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},
-        default: 'lbfgs'
+    solver : str, {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, \
+             default: 'lbfgs'.
+
         Algorithm to use in the optimization problem.
 
         - For small datasets, 'liblinear' is a good choice, whereas 'sag' and
@@ -1477,9 +1540,11 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,
         .. versionadded:: 0.17
            class_weight == 'balanced'
 
-    n_jobs : int, optional
-        Number of CPU cores used during the cross-validation loop. If given
-        a value of -1, all cores are used.
+    n_jobs : int or None, optional (default=None)
+        Number of CPU cores used during the cross-validation loop.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : int
         For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any
@@ -1505,15 +1570,18 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,
         To lessen the effect of regularization on synthetic feature weight
         (and therefore on the intercept) intercept_scaling has to be increased.
 
-    multi_class : str, {'ovr', 'multinomial'}
-        Multiclass option can be either 'ovr' or 'multinomial'. If the option
-        chosen is 'ovr', then a binary problem is fit for each label. Else
-        the loss minimised is the multinomial loss fit across
-        the entire probability distribution. Does not work for 'liblinear'
-        solver.
+    multi_class : str, {'ovr', 'multinomial', 'auto'}, default: 'ovr'
+        If the option chosen is 'ovr', then a binary problem is fit for each
+        label. For 'multinomial' the loss minimised is the multinomial loss fit
+        across the entire probability distribution, *even when the data is
+        binary*. 'multinomial' is unavailable when solver='liblinear'.
+        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',
+        and otherwise selects 'multinomial'.
 
         .. versionadded:: 0.18
            Stochastic Average Gradient descent solver for 'multinomial' case.
+        .. versionchanged:: 0.20
+            Default will change from 'ovr' to 'auto' in 0.22.
 
     random_state : int, RandomState instance or None, optional, default None
         If int, random_state is the seed used by the random number generator;
@@ -1568,16 +1636,29 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,
         Actual number of iterations for all classes, folds and Cs.
         In the binary or multinomial cases, the first dimension is equal to 1.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_iris
+    >>> from sklearn.linear_model import LogisticRegressionCV
+    >>> X, y = load_iris(return_X_y=True)
+    >>> clf = LogisticRegressionCV(cv=5, random_state=0,
+    ...                            multi_class='multinomial').fit(X, y)
+    >>> clf.predict(X[:2, :])
+    array([0, 0])
+    >>> clf.predict_proba(X[:2, :]).shape
+    (2, 3)
+    >>> clf.score(X, y) # doctest: +ELLIPSIS
+    0.98...
+
     See also
     --------
     LogisticRegression
 
     """
-
     def __init__(self, Cs=10, fit_intercept=True, cv='warn', dual=False,
                  penalty='l2', scoring=None, solver='lbfgs', tol=1e-4,
-                 max_iter=100, class_weight=None, n_jobs=1, verbose=0,
-                 refit=True, intercept_scaling=1., multi_class='ovr',
+                 max_iter=100, class_weight=None, n_jobs=None, verbose=0,
+                 refit=True, intercept_scaling=1., multi_class='warn',
                  random_state=None):
         self.Cs = Cs
         self.fit_intercept = fit_intercept
@@ -1605,7 +1686,7 @@ def fit(self, X, y, sample_weight=None):
             Training vector, where n_samples is the number of samples and
             n_features is the number of features.
 
-        y : array-like, shape (n_samples,)
+        y : array-like, shape (n_samples,) or (n_samples, n_targets)
             Target vector relative to X.
 
         sample_weight : array-like, shape (n_samples,) optional
@@ -1616,8 +1697,7 @@ def fit(self, X, y, sample_weight=None):
         -------
         self : object
         """
-        _check_solver_option(self.solver, self.multi_class, self.penalty,
-                             self.dual)
+        solver = _check_solver(self.solver, self.penalty, self.dual)
 
         if not isinstance(self.max_iter, numbers.Number) or self.max_iter < 0:
             raise ValueError("Maximum number of iteration must be positive;"
@@ -1628,7 +1708,7 @@ def fit(self, X, y, sample_weight=None):
 
         X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64,
                          order="C",
-                         accept_large_sparse=self.solver != 'liblinear')
+                         accept_large_sparse=solver != 'liblinear')
         check_classification_targets(y)
 
         class_weight = self.class_weight
@@ -1644,7 +1724,10 @@ def fit(self, X, y, sample_weight=None):
         classes = self.classes_ = label_encoder.classes_
         encoded_labels = label_encoder.transform(label_encoder.classes_)
 
-        if self.solver in ['sag', 'saga']:
+        multi_class = _check_multi_class(self.multi_class, solver,
+                                         len(classes))
+
+        if solver in ['sag', 'saga']:
             max_squared_sum = row_norms(X, squared=True).max()
         else:
             max_squared_sum = None
@@ -1670,7 +1753,7 @@ def fit(self, X, y, sample_weight=None):
 
         # We need this hack to iterate only once over labels, in the case of
         # multi_class = multinomial, without changing the value of the labels.
-        if self.multi_class == 'multinomial':
+        if multi_class == 'multinomial':
             iter_encoded_labels = iter_classes = [None]
         else:
             iter_encoded_labels = encoded_labels
@@ -1688,17 +1771,17 @@ def fit(self, X, y, sample_weight=None):
         # The SAG solver releases the GIL so it's more efficient to use
         # threads for this solver.
         if self.solver in ['sag', 'saga']:
-            backend = 'threading'
+            prefer = 'threads'
         else:
-            backend = 'multiprocessing'
+            prefer = 'processes'
         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
-                               backend=backend)(
+                               prefer=prefer)(
             path_func(X, y, train, test, pos_class=label, Cs=self.Cs,
                       fit_intercept=self.fit_intercept, penalty=self.penalty,
-                      dual=self.dual, solver=self.solver, tol=self.tol,
+                      dual=self.dual, solver=solver, tol=self.tol,
                       max_iter=self.max_iter, verbose=self.verbose,
                       class_weight=class_weight, scoring=self.scoring,
-                      multi_class=self.multi_class,
+                      multi_class=multi_class,
                       intercept_scaling=self.intercept_scaling,
                       random_state=self.random_state,
                       max_squared_sum=max_squared_sum,
@@ -1707,7 +1790,7 @@ def fit(self, X, y, sample_weight=None):
             for label in iter_encoded_labels
             for train, test in folds)
 
-        if self.multi_class == 'multinomial':
+        if multi_class == 'multinomial':
             multi_coefs_paths, Cs, multi_scores, n_iter_ = zip(*fold_coefs_)
             multi_coefs_paths = np.asarray(multi_coefs_paths)
             multi_scores = np.asarray(multi_scores)
@@ -1744,14 +1827,14 @@ def fit(self, X, y, sample_weight=None):
         self.intercept_ = np.zeros(n_classes)
 
         # hack to iterate only once for multinomial case.
-        if self.multi_class == 'multinomial':
+        if multi_class == 'multinomial':
             scores = multi_scores
             coefs_paths = multi_coefs_paths
 
         for index, (cls, encoded_label) in enumerate(
                 zip(iter_classes, iter_encoded_labels)):
 
-            if self.multi_class == 'ovr':
+            if multi_class == 'ovr':
                 # The scores_ / coefs_paths_ dict have unencoded class
                 # labels as their keys
                 scores = self.scores_[cls]
@@ -1762,7 +1845,7 @@ def fit(self, X, y, sample_weight=None):
 
                 C_ = self.Cs_[best_index]
                 self.C_.append(C_)
-                if self.multi_class == 'multinomial':
+                if multi_class == 'multinomial':
                     coef_init = np.mean(coefs_paths[:, best_index, :, :],
                                         axis=0)
                 else:
@@ -1771,12 +1854,12 @@ def fit(self, X, y, sample_weight=None):
                 # Note that y is label encoded and hence pos_class must be
                 # the encoded label / None (for 'multinomial')
                 w, _, _ = logistic_regression_path(
-                    X, y, pos_class=encoded_label, Cs=[C_], solver=self.solver,
+                    X, y, pos_class=encoded_label, Cs=[C_], solver=solver,
                     fit_intercept=self.fit_intercept, coef=coef_init,
                     max_iter=self.max_iter, tol=self.tol,
                     penalty=self.penalty,
                     class_weight=class_weight,
-                    multi_class=self.multi_class,
+                    multi_class=multi_class,
                     verbose=max(0, self.verbose - 1),
                     random_state=self.random_state,
                     check_input=False, max_squared_sum=max_squared_sum,
@@ -1791,7 +1874,7 @@ def fit(self, X, y, sample_weight=None):
                              for i in range(len(folds))], axis=0)
                 self.C_.append(np.mean(self.Cs_[best_indices]))
 
-            if self.multi_class == 'multinomial':
+            if multi_class == 'multinomial':
                 self.C_ = np.tile(self.C_, n_classes)
                 self.coef_ = w[:, :X.shape[1]]
                 if self.fit_intercept:
diff --git a/sklearn/linear_model/omp.py b/sklearn/linear_model/omp.py
index ec3cb7efc8..a0f6d49490 100755
--- a/sklearn/linear_model/omp.py
+++ b/sklearn/linear_model/omp.py
@@ -789,9 +789,11 @@ class OrthogonalMatchingPursuitCV(LinearModel, RegressorMixin):
             ``cv`` default value if None will change from 3-fold to 5-fold
             in v0.22.
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the cross validation. If ``-1``, use
-        all the CPUs
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the cross validation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : boolean or integer, optional
         Sets the verbosity amount
@@ -826,7 +828,7 @@ class OrthogonalMatchingPursuitCV(LinearModel, RegressorMixin):
 
     """
     def __init__(self, copy=True, fit_intercept=True, normalize=True,
-                 max_iter=None, cv='warn', n_jobs=1, verbose=False):
+                 max_iter=None, cv='warn', n_jobs=None, verbose=False):
         self.copy = copy
         self.fit_intercept = fit_intercept
         self.normalize = normalize
diff --git a/sklearn/linear_model/passive_aggressive.py b/sklearn/linear_model/passive_aggressive.py
index 7fd15d1710..22f1c0fbba 100755
--- a/sklearn/linear_model/passive_aggressive.py
+++ b/sklearn/linear_model/passive_aggressive.py
@@ -68,10 +68,12 @@ class PassiveAggressiveClassifier(BaseSGDClassifier):
         hinge: equivalent to PA-I in the reference paper.
         squared_hinge: equivalent to PA-II in the reference paper.
 
-    n_jobs : integer, optional
+    n_jobs : int or None, optional (default=None)
         The number of CPUs to use to do the OVA (One Versus All, for
-        multi-class problems) computation. -1 means 'all CPUs'. Defaults
-        to 1.
+        multi-class problems) computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional, default=None
         The seed of the pseudo random number generator to use when shuffling
@@ -141,7 +143,7 @@ class PassiveAggressiveClassifier(BaseSGDClassifier):
     >>> clf.fit(X, y)
     PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,
                   early_stopping=False, fit_intercept=True, loss='hinge',
-                  max_iter=1000, n_iter=None, n_iter_no_change=5, n_jobs=1,
+                  max_iter=1000, n_iter=None, n_iter_no_change=5, n_jobs=None,
                   random_state=0, shuffle=True, tol=None,
                   validation_fraction=0.1, verbose=0, warm_start=False)
     >>> print(clf.coef_)
@@ -167,7 +169,7 @@ class PassiveAggressiveClassifier(BaseSGDClassifier):
     def __init__(self, C=1.0, fit_intercept=True, max_iter=None, tol=None,
                  early_stopping=False, validation_fraction=0.1,
                  n_iter_no_change=5, shuffle=True, verbose=0, loss="hinge",
-                 n_jobs=1, random_state=None, warm_start=False,
+                 n_jobs=None, random_state=None, warm_start=False,
                  class_weight=None, average=False, n_iter=None):
         super(PassiveAggressiveClassifier, self).__init__(
             penalty=None,
diff --git a/sklearn/linear_model/perceptron.py b/sklearn/linear_model/perceptron.py
index 1a01e1ba22..1bc06f4f17 100755
--- a/sklearn/linear_model/perceptron.py
+++ b/sklearn/linear_model/perceptron.py
@@ -47,10 +47,12 @@ class Perceptron(BaseSGDClassifier):
     eta0 : double
         Constant by which the updates are multiplied. Defaults to 1.
 
-    n_jobs : integer, optional
+    n_jobs : int or None, optional (default=None)
         The number of CPUs to use to do the OVA (One Versus All, for
-        multi-class problems) computation. -1 means 'all CPUs'. Defaults
-        to 1.
+        multi-class problems) computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional, default None
         The seed of the pseudo random number generator to use when shuffling
@@ -123,6 +125,20 @@ class Perceptron(BaseSGDClassifier):
     ``Perceptron()`` is equivalent to `SGDClassifier(loss="perceptron",
     eta0=1, learning_rate="constant", penalty=None)`.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.linear_model import Perceptron
+    >>> X, y = load_digits(return_X_y=True)
+    >>> clf = Perceptron(tol=1e-3, random_state=0)
+    >>> clf.fit(X, y)
+    Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,
+          fit_intercept=True, max_iter=None, n_iter=None, n_iter_no_change=5,
+          n_jobs=None, penalty=None, random_state=0, shuffle=True, tol=0.001,
+          validation_fraction=0.1, verbose=0, warm_start=False)
+    >>> clf.score(X, y) # doctest: +ELLIPSIS
+    0.946...
+
     See also
     --------
 
@@ -135,7 +151,7 @@ class Perceptron(BaseSGDClassifier):
     """
     def __init__(self, penalty=None, alpha=0.0001, fit_intercept=True,
                  max_iter=None, tol=None, shuffle=True, verbose=0, eta0=1.0,
-                 n_jobs=1, random_state=0, early_stopping=False,
+                 n_jobs=None, random_state=0, early_stopping=False,
                  validation_fraction=0.1, n_iter_no_change=5,
                  class_weight=None, warm_start=False, n_iter=None):
         super(Perceptron, self).__init__(
diff --git a/sklearn/linear_model/randomized_l1.py b/sklearn/linear_model/randomized_l1.py
index f75a59db5e..40ebe3c578 100755
--- a/sklearn/linear_model/randomized_l1.py
+++ b/sklearn/linear_model/randomized_l1.py
@@ -33,7 +33,7 @@
 # Randomized linear model: feature selection
 
 def _resample_model(estimator_func, X, y, scaling=.5, n_resampling=200,
-                    n_jobs=1, verbose=False, pre_dispatch='3*n_jobs',
+                    n_jobs=None, verbose=False, pre_dispatch='3*n_jobs',
                     random_state=None, sample_fraction=.75, **params):
     random_state = check_random_state(random_state)
     # We are generating 1 - weights, and not weights
@@ -257,9 +257,11 @@ class RandomizedLasso(BaseRandomizedLinearModel):
         If None, the random number generator is the RandomState instance used
         by `np.random`.
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the resampling. If '-1', use
-        all the CPUs
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the resampling.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     pre_dispatch : int, or string, optional
         Controls the number of jobs that get dispatched during parallel
@@ -316,7 +318,7 @@ def __init__(self, alpha='aic', scaling=.5, sample_fraction=.75,
                  normalize=True, precompute='auto',
                  max_iter=500,
                  eps=np.finfo(np.float).eps, random_state=None,
-                 n_jobs=1, pre_dispatch='3*n_jobs',
+                 n_jobs=None, pre_dispatch='3*n_jobs',
                  memory=None):
         self.alpha = alpha
         self.scaling = scaling
@@ -380,7 +382,8 @@ def _randomized_logistic(X, y, weights, mask, C=1., verbose=False,
     for this_C, this_scores in zip(C, scores.T):
         # XXX : would be great to do it with a warm_start ...
         clf = LogisticRegression(C=this_C, tol=tol, penalty='l1', dual=False,
-                                 fit_intercept=fit_intercept)
+                                 fit_intercept=fit_intercept,
+                                 solver='liblinear', multi_class='ovr')
         clf.fit(X, y)
         this_scores[:] = np.any(
             np.abs(clf.coef_) > 10 * np.finfo(np.float).eps, axis=0)
@@ -451,9 +454,11 @@ class RandomizedLogisticRegression(BaseRandomizedLinearModel):
         If None, the random number generator is the RandomState instance used
         by `np.random`.
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the resampling. If '-1', use
-        all the CPUs
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the resampling.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     pre_dispatch : int, or string, optional
         Controls the number of jobs that get dispatched during parallel
@@ -510,7 +515,7 @@ def __init__(self, C=1, scaling=.5, sample_fraction=.75,
                  fit_intercept=True, verbose=False,
                  normalize=True,
                  random_state=None,
-                 n_jobs=1, pre_dispatch='3*n_jobs',
+                 n_jobs=None, pre_dispatch='3*n_jobs',
                  memory=None):
         self.C = C
         self.scaling = scaling
@@ -572,7 +577,7 @@ def _lasso_stability_path(X, y, mask, weights, eps):
 def lasso_stability_path(X, y, scaling=0.5, random_state=None,
                          n_resampling=200, n_grid=100,
                          sample_fraction=0.75,
-                         eps=4 * np.finfo(np.float).eps, n_jobs=1,
+                         eps=4 * np.finfo(np.float).eps, n_jobs=None,
                          verbose=False):
     """Stability path based on randomized Lasso estimates
 
@@ -608,9 +613,11 @@ def lasso_stability_path(X, y, scaling=0.5, random_state=None,
     eps : float, optional
         Smallest value of alpha / alpha_max considered
 
-    n_jobs : integer, optional
-        Number of CPUs to use during the resampling. If '-1', use
-        all the CPUs
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the resampling.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : boolean or integer, optional
         Sets the verbosity amount
diff --git a/sklearn/linear_model/ransac.py b/sklearn/linear_model/ransac.py
index f9c372d8ed..9dcd044d1f 100755
--- a/sklearn/linear_model/ransac.py
+++ b/sklearn/linear_model/ransac.py
@@ -74,6 +74,8 @@ class RANSACRegressor(BaseEstimator, MetaEstimatorMixin, RegressorMixin):
            which is used for the stop criterion defined by `stop_score`.
            Additionally, the score is used to decide which of two equally
            large consensus sets is chosen as the better one.
+         * `predict(X)`: Returns predicted values using the linear model,
+           which is used to compute residual error using loss function.
 
         If `base_estimator` is None, then
         ``base_estimator=sklearn.linear_model.LinearRegression()`` is used for
diff --git a/sklearn/linear_model/ridge.py b/sklearn/linear_model/ridge.py
index 6ff0bd4f93..089540efed 100755
--- a/sklearn/linear_model/ridge.py
+++ b/sklearn/linear_model/ridge.py
@@ -779,6 +779,15 @@ class RidgeClassifier(LinearClassifierMixin, _BaseRidge):
         Actual number of iterations for each target. Available only for
         sag and lsqr solvers. Other solvers will return None.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_breast_cancer
+    >>> from sklearn.linear_model import RidgeClassifier
+    >>> X, y = load_breast_cancer(return_X_y=True)
+    >>> clf = RidgeClassifier().fit(X, y)
+    >>> clf.score(X, y) # doctest: +ELLIPSIS
+    0.9595...
+
     See also
     --------
     Ridge : Ridge regression
@@ -1249,6 +1258,15 @@ class RidgeCV(_BaseRidgeCV, RegressorMixin):
     alpha_ : float
         Estimated regularization parameter.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_diabetes
+    >>> from sklearn.linear_model import RidgeCV
+    >>> X, y = load_diabetes(return_X_y=True)
+    >>> clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)
+    >>> clf.score(X, y) # doctest: +ELLIPSIS
+    0.5166...
+
     See also
     --------
     Ridge : Ridge regression
@@ -1339,6 +1357,15 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):
     alpha_ : float
         Estimated regularization parameter
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_breast_cancer
+    >>> from sklearn.linear_model import RidgeClassifierCV
+    >>> X, y = load_breast_cancer(return_X_y=True)
+    >>> clf = RidgeClassifierCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)
+    >>> clf.score(X, y) # doctest: +ELLIPSIS
+    0.9630...
+
     See also
     --------
     Ridge : Ridge regression
diff --git a/sklearn/linear_model/sag.py b/sklearn/linear_model/sag.py
index 39b817da1b..3e8861f26d 100755
--- a/sklearn/linear_model/sag.py
+++ b/sklearn/linear_model/sag.py
@@ -212,13 +212,15 @@ def sag_solver(X, y, sample_weight=None, loss='log', alpha=1., beta=0.,
 
     >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
     >>> y = np.array([1, 1, 2, 2])
-    >>> clf = linear_model.LogisticRegression(solver='sag')
+    >>> clf = linear_model.LogisticRegression(
+    ...     solver='sag', multi_class='multinomial')
     >>> clf.fit(X, y)
     ... #doctest: +NORMALIZE_WHITESPACE
     LogisticRegression(C=1.0, class_weight=None, dual=False,
         fit_intercept=True, intercept_scaling=1, max_iter=100,
-        multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
-        solver='sag', tol=0.0001, verbose=0, warm_start=False)
+        multi_class='multinomial', n_jobs=None, penalty='l2',
+        random_state=None, solver='sag', tol=0.0001, verbose=0,
+        warm_start=False)
 
     References
     ----------
diff --git a/sklearn/linear_model/stochastic_gradient.py b/sklearn/linear_model/stochastic_gradient.py
index 20107c233d..93a2a6c912 100755
--- a/sklearn/linear_model/stochastic_gradient.py
+++ b/sklearn/linear_model/stochastic_gradient.py
@@ -318,7 +318,7 @@ def _validation_score(self, coef, intercept):
 def _prepare_fit_binary(est, y, i):
     """Initialization for fit_binary.
 
-    Returns y, coef, intercept.
+    Returns y, coef, intercept, average_coef, average_intercept.
     """
     y_i = np.ones(y.shape, dtype=np.float64, order="C")
     y_i[y != est.classes_[i]] = -1.0
@@ -464,7 +464,7 @@ class BaseSGDClassifier(six.with_metaclass(ABCMeta, BaseSGD,
     @abstractmethod
     def __init__(self, loss="hinge", penalty='l2', alpha=0.0001,
                  l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None,
-                 shuffle=True, verbose=0, epsilon=DEFAULT_EPSILON, n_jobs=1,
+                 shuffle=True, verbose=0, epsilon=DEFAULT_EPSILON, n_jobs=None,
                  random_state=None, learning_rate="optimal", eta0=0.0,
                  power_t=0.5, early_stopping=False,
                  validation_fraction=0.1, n_iter_no_change=5,
@@ -481,7 +481,7 @@ def __init__(self, loss="hinge", penalty='l2', alpha=0.0001,
             n_iter_no_change=n_iter_no_change, warm_start=warm_start,
             average=average, n_iter=n_iter)
         self.class_weight = class_weight
-        self.n_jobs = int(n_jobs)
+        self.n_jobs = n_jobs
 
     @property
     @deprecated("Attribute loss_function was deprecated in version 0.19 and "
@@ -613,7 +613,7 @@ def _fit_multiclass(self, X, y, alpha, C, learning_rate,
         strategy is called OVA: One Versus All.
         """
         # Use joblib to fit OvA in parallel.
-        result = Parallel(n_jobs=self.n_jobs, backend="threading",
+        result = Parallel(n_jobs=self.n_jobs, prefer="threads",
                           verbose=self.verbose)(
             delayed(fit_binary)(self, i, X, y, alpha, C, learning_rate,
                                 max_iter, self._expanded_class_weight[i],
@@ -806,10 +806,12 @@ class SGDClassifier(BaseSGDClassifier):
         For epsilon-insensitive, any differences between the current prediction
         and the correct label are ignored if they are less than this threshold.
 
-    n_jobs : integer, optional
+    n_jobs : int or None, optional (default=None)
         The number of CPUs to use to do the OVA (One Versus All, for
-        multi-class problems) computation. -1 means 'all CPUs'. Defaults
-        to 1.
+        multi-class problems) computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional (default=None)
         The seed of the pseudo random number generator to use when shuffling
@@ -927,7 +929,7 @@ class SGDClassifier(BaseSGDClassifier):
     SGDClassifier(alpha=0.0001, average=False, class_weight=None,
            early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,
            l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=1000,
-           n_iter=None, n_iter_no_change=5, n_jobs=1, penalty='l2',
+           n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',
            power_t=0.5, random_state=None, shuffle=True, tol=None,
            validation_fraction=0.1, verbose=0, warm_start=False)
 
@@ -942,7 +944,7 @@ class SGDClassifier(BaseSGDClassifier):
 
     def __init__(self, loss="hinge", penalty='l2', alpha=0.0001, l1_ratio=0.15,
                  fit_intercept=True, max_iter=None, tol=None, shuffle=True,
-                 verbose=0, epsilon=DEFAULT_EPSILON, n_jobs=1,
+                 verbose=0, epsilon=DEFAULT_EPSILON, n_jobs=None,
                  random_state=None, learning_rate="optimal", eta0=0.0,
                  power_t=0.5, early_stopping=False, validation_fraction=0.1,
                  n_iter_no_change=5, class_weight=None, warm_start=False,
diff --git a/sklearn/linear_model/tests/test_least_angle.py b/sklearn/linear_model/tests/test_least_angle.py
index 8545ecd988..9c9a883f96 100755
--- a/sklearn/linear_model/tests/test_least_angle.py
+++ b/sklearn/linear_model/tests/test_least_angle.py
@@ -80,7 +80,6 @@ def test_simple_precomputed():
 
 def test_all_precomputed():
     # Test that lars_path with precomputed Gram and Xy gives the right answer
-    X, y = diabetes.data, diabetes.target
     G = np.dot(X.T, X)
     Xy = np.dot(X.T, y)
     for method in 'lar', 'lasso':
@@ -188,7 +187,6 @@ def test_no_path_all_precomputed():
         [linear_model.Lars, linear_model.LarsCV, linear_model.LassoLarsIC])
 def test_lars_precompute(classifier):
     # Check for different values of precompute
-    X, y = diabetes.data, diabetes.target
     G = np.dot(X.T, X)
 
     clf = classifier(precompute=G)
diff --git a/sklearn/linear_model/tests/test_logistic.py b/sklearn/linear_model/tests/test_logistic.py
index 8677ec80ad..4195405b86 100755
--- a/sklearn/linear_model/tests/test_logistic.py
+++ b/sklearn/linear_model/tests/test_logistic.py
@@ -1,15 +1,18 @@
+import os
+import sys
 import numpy as np
 import scipy.sparse as sp
 from scipy import linalg, optimize, sparse
 
 import pytest
 
+from sklearn.base import clone
 from sklearn.datasets import load_iris, make_classification
 from sklearn.metrics import log_loss
 from sklearn.metrics.scorer import get_scorer
 from sklearn.model_selection import StratifiedKFold
 from sklearn.preprocessing import LabelEncoder
-from sklearn.utils import compute_class_weight
+from sklearn.utils import compute_class_weight, _IS_32BIT
 from sklearn.utils.testing import assert_almost_equal
 from sklearn.utils.testing import assert_allclose
 from sklearn.utils.testing import assert_array_almost_equal
@@ -22,6 +25,7 @@
 from sklearn.utils.testing import assert_warns
 from sklearn.utils.testing import ignore_warnings
 from sklearn.utils.testing import assert_warns_message
+from sklearn.utils.testing import assert_no_warnings
 
 from sklearn.exceptions import ConvergenceWarning
 from sklearn.exceptions import ChangedBehaviorWarning
@@ -57,6 +61,8 @@ def check_predictions(clf, X, y):
     assert_array_equal(probabilities.argmax(axis=1), y)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_predict_2_classes():
     # Simple sanity check on a 2 classes dataset
     # Make sure it predicts the correct result on simple datasets.
@@ -72,6 +78,7 @@ def test_predict_2_classes():
                                          random_state=0), X_sp, Y1)
 
 
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_error():
     # Test for appropriate exception on errors
     msg = "Penalty term must be positive"
@@ -95,6 +102,7 @@ def test_error():
         assert_raise_message(ValueError, msg, LR(max_iter="test").fit, X, Y1)
 
 
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_logistic_cv_mock_scorer():
 
     class MockScorer(object):
@@ -130,7 +138,7 @@ def __call__(self, model, X, y, sample_weight=None):
 
 
 def test_logistic_cv_score_does_not_warn_by_default():
-    lr = LogisticRegressionCV(cv=2)
+    lr = LogisticRegressionCV(cv=2, multi_class='ovr')
     lr.fit(X, Y1)
 
     with pytest.warns(None) as record:
@@ -142,7 +150,7 @@ def test_lr_liblinear_warning():
     n_samples, n_features = iris.data.shape
     target = iris.target_names[iris.target]
 
-    lr = LogisticRegression(solver='liblinear', n_jobs=2)
+    lr = LogisticRegression(solver='liblinear', multi_class='ovr', n_jobs=2)
     assert_warns_message(UserWarning,
                          "'n_jobs' > 1 does not have any effect when"
                          " 'solver' is set to 'liblinear'. Got 'n_jobs'"
@@ -150,6 +158,8 @@ def test_lr_liblinear_warning():
                          lr.fit, iris.data, target)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_predict_3_classes():
     check_predictions(LogisticRegression(C=10), X, Y2)
     check_predictions(LogisticRegression(C=10), X_sp, Y2)
@@ -164,7 +174,8 @@ def test_predict_iris():
     # Test that both multinomial and OvR solvers handle
     # multiclass data correctly and give good accuracy
     # score (>0.95) for the training data.
-    for clf in [LogisticRegression(C=len(iris.data)),
+    for clf in [LogisticRegression(C=len(iris.data), solver='liblinear',
+                                   multi_class='ovr'),
                 LogisticRegression(C=len(iris.data), solver='lbfgs',
                                    multi_class='multinomial'),
                 LogisticRegression(C=len(iris.data), solver='newton-cg',
@@ -198,12 +209,13 @@ def test_multinomial_validation(solver):
 def test_check_solver_option(LR):
     X, y = iris.data, iris.target
 
-    msg = ('Logistic Regression supports only liblinear, newton-cg, '
-           'lbfgs, sag and saga solvers, got wrong_name')
-    lr = LR(solver="wrong_name")
+    msg = ("Logistic Regression supports only solvers in ['liblinear', "
+           "'newton-cg', 'lbfgs', 'sag', 'saga'], got wrong_name.")
+    lr = LR(solver="wrong_name", multi_class="ovr")
     assert_raise_message(ValueError, msg, lr.fit, X, y)
 
-    msg = "multi_class should be either multinomial or ovr, got wrong_name"
+    msg = ("multi_class should be 'multinomial', 'ovr' or 'auto'. "
+           "Got wrong_name")
     lr = LR(solver='newton-cg', multi_class="wrong_name")
     assert_raise_message(ValueError, msg, lr.fit, X, y)
 
@@ -216,15 +228,40 @@ def test_check_solver_option(LR):
     for solver in ['newton-cg', 'lbfgs', 'sag']:
         msg = ("Solver %s supports only l2 penalties, got l1 penalty." %
                solver)
-        lr = LR(solver=solver, penalty='l1')
+        lr = LR(solver=solver, penalty='l1', multi_class='ovr')
         assert_raise_message(ValueError, msg, lr.fit, X, y)
     for solver in ['newton-cg', 'lbfgs', 'sag', 'saga']:
         msg = ("Solver %s supports only dual=False, got dual=True" %
                solver)
-        lr = LR(solver=solver, dual=True)
+        lr = LR(solver=solver, dual=True, multi_class='ovr')
         assert_raise_message(ValueError, msg, lr.fit, X, y)
 
 
+@pytest.mark.parametrize('model, params, warn_solver',
+                         [(LogisticRegression, {}, True),
+                          (LogisticRegressionCV, {'cv': 5}, False)])
+def test_logistic_regression_warnings(model, params, warn_solver):
+    clf_solver_warning = model(multi_class='ovr', **params)
+    clf_multi_class_warning = model(solver='lbfgs', **params)
+    clf_no_warnings = model(solver='lbfgs', multi_class='ovr', **params)
+
+    solver_warning_msg = "Default solver will be changed to 'lbfgs'"
+    multi_class_warning_msg = "Default multi_class will be changed to 'auto"
+
+    if warn_solver:
+        assert_warns_message(FutureWarning, solver_warning_msg,
+                             clf_solver_warning.fit, iris.data, iris.target)
+    else:
+        assert_no_warnings(clf_no_warnings.fit, iris.data, iris.target)
+
+    assert_warns_message(FutureWarning, multi_class_warning_msg,
+                         clf_multi_class_warning.fit, iris.data, iris.target)
+    # But no warning when binary target:
+    assert_no_warnings(clf_multi_class_warning.fit,
+                       iris.data, iris.target == 0)
+    assert_no_warnings(clf_no_warnings.fit, iris.data, iris.target)
+
+
 @pytest.mark.parametrize('solver', ['lbfgs', 'newton-cg', 'sag', 'saga'])
 def test_multinomial_binary(solver):
     # Test multinomial LR on a binary problem.
@@ -259,11 +296,13 @@ def test_multinomial_binary_probabilities():
 
     expected_proba_class_1 = (np.exp(decision) /
                               (np.exp(decision) + np.exp(-decision)))
-    expected_proba = np.c_[1-expected_proba_class_1, expected_proba_class_1]
+    expected_proba = np.c_[1 - expected_proba_class_1, expected_proba_class_1]
 
     assert_almost_equal(proba, expected_proba)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_sparsify():
     # Test sparsify and densify members.
     n_samples, n_features = iris.data.shape
@@ -287,6 +326,8 @@ def test_sparsify():
     assert_array_almost_equal(pred_d_d, pred_d_s)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_inconsistent_input():
     # Test that an exception is raised on inconsistent input
     rng = np.random.RandomState(0)
@@ -305,6 +346,8 @@ def test_inconsistent_input():
                   rng.random_sample((3, 12)))
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_write_parameters():
     # Test that we can write to coef_ and intercept_
     clf = LogisticRegression(random_state=0)
@@ -314,6 +357,8 @@ def test_write_parameters():
     assert_array_almost_equal(clf.decision_function(X), 0)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_nan():
     # Test proper NaN handling.
     # Regression test for Issue #252: fit used to go into an infinite loop.
@@ -336,12 +381,11 @@ def test_consistency_path():
     for solver in ['sag', 'saga']:
         coefs, Cs, _ = f(logistic_regression_path)(
             X, y, Cs=Cs, fit_intercept=False, tol=1e-5, solver=solver,
-            max_iter=1000,
-            random_state=0)
+            max_iter=1000, multi_class='ovr', random_state=0)
         for i, C in enumerate(Cs):
             lr = LogisticRegression(C=C, fit_intercept=False, tol=1e-5,
-                                    solver=solver,
-                                    random_state=0)
+                                    solver=solver, multi_class='ovr',
+                                    random_state=0, max_iter=1000)
             lr.fit(X, y)
             lr_coef = lr.coef_.ravel()
             assert_array_almost_equal(lr_coef, coefs[i], decimal=4,
@@ -352,9 +396,10 @@ def test_consistency_path():
         Cs = [1e3]
         coefs, Cs, _ = f(logistic_regression_path)(
             X, y, Cs=Cs, fit_intercept=True, tol=1e-6, solver=solver,
-            intercept_scaling=10000., random_state=0)
+            intercept_scaling=10000., random_state=0, multi_class='ovr')
         lr = LogisticRegression(C=Cs[0], fit_intercept=True, tol=1e-4,
-                                intercept_scaling=10000., random_state=0)
+                                intercept_scaling=10000., random_state=0,
+                                multi_class='ovr', solver=solver)
         lr.fit(X, y)
         lr_coef = np.concatenate([lr.coef_.ravel(), lr.intercept_])
         assert_array_almost_equal(lr_coef, coefs[0], decimal=4,
@@ -373,11 +418,14 @@ def test_logistic_regression_path_convergence_fail():
 def test_liblinear_dual_random_state():
     # random_state is relevant for liblinear solver only if dual=True
     X, y = make_classification(n_samples=20, random_state=0)
-    lr1 = LogisticRegression(random_state=0, dual=True, max_iter=1, tol=1e-15)
+    lr1 = LogisticRegression(random_state=0, dual=True, max_iter=1, tol=1e-15,
+                             solver='liblinear', multi_class='ovr')
     lr1.fit(X, y)
-    lr2 = LogisticRegression(random_state=0, dual=True, max_iter=1, tol=1e-15)
+    lr2 = LogisticRegression(random_state=0, dual=True, max_iter=1, tol=1e-15,
+                             solver='liblinear', multi_class='ovr')
     lr2.fit(X, y)
-    lr3 = LogisticRegression(random_state=8, dual=True, max_iter=1, tol=1e-15)
+    lr3 = LogisticRegression(random_state=8, dual=True, max_iter=1, tol=1e-15,
+                             solver='liblinear', multi_class='ovr')
     lr3.fit(X, y)
 
     # same result for same random state
@@ -477,9 +525,10 @@ def test_logistic_cv():
     X_ref -= X_ref.mean()
     X_ref /= X_ref.std()
     lr_cv = LogisticRegressionCV(Cs=[1.], fit_intercept=False,
-                                 solver='liblinear')
+                                 solver='liblinear', multi_class='ovr')
     lr_cv.fit(X_ref, y)
-    lr = LogisticRegression(C=1., fit_intercept=False)
+    lr = LogisticRegression(C=1., fit_intercept=False,
+                            solver='liblinear', multi_class='ovr')
     lr.fit(X_ref, y)
     assert_array_almost_equal(lr.coef_, lr_cv.coef_)
 
@@ -568,6 +617,7 @@ def test_multinomial_logistic_regression_string_inputs():
     assert_equal(sorted(np.unique(lr_cv_str.predict(X_ref))), ['bar', 'baz'])
 
 
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_logistic_cv_sparse():
     X, y = make_classification(n_samples=50, n_features=5,
@@ -630,11 +680,11 @@ def test_ovr_multinomial_iris():
     precomputed_folds = list(cv.split(train, target))
 
     # Train clf on the original dataset where classes 0 and 1 are separated
-    clf = LogisticRegressionCV(cv=precomputed_folds)
+    clf = LogisticRegressionCV(cv=precomputed_folds, multi_class='ovr')
     clf.fit(train, target)
 
     # Conflate classes 0 and 1 and train clf1 on this modified dataset
-    clf1 = LogisticRegressionCV(cv=precomputed_folds)
+    clf1 = LogisticRegressionCV(cv=precomputed_folds, multi_class='ovr')
     target_copy = target.copy()
     target_copy[target_copy == 0] = 1
     clf1.fit(train, target_copy)
@@ -680,13 +730,12 @@ def test_ovr_multinomial_iris():
 def test_logistic_regression_solvers():
     X, y = make_classification(n_features=10, n_informative=5, random_state=0)
 
-    ncg = LogisticRegression(solver='newton-cg', fit_intercept=False)
-    lbf = LogisticRegression(solver='lbfgs', fit_intercept=False)
-    lib = LogisticRegression(fit_intercept=False)
-    sag = LogisticRegression(solver='sag', fit_intercept=False,
-                             random_state=42)
-    saga = LogisticRegression(solver='saga', fit_intercept=False,
-                              random_state=42)
+    params = dict(fit_intercept=False, random_state=42, multi_class='ovr')
+    ncg = LogisticRegression(solver='newton-cg', **params)
+    lbf = LogisticRegression(solver='lbfgs', **params)
+    lib = LogisticRegression(solver='liblinear', **params)
+    sag = LogisticRegression(solver='sag', **params)
+    saga = LogisticRegression(solver='saga', **params)
     ncg.fit(X, y)
     lbf.fit(X, y)
     sag.fit(X, y)
@@ -708,13 +757,13 @@ def test_logistic_regression_solvers_multiclass():
     X, y = make_classification(n_samples=20, n_features=20, n_informative=10,
                                n_classes=3, random_state=0)
     tol = 1e-7
-    ncg = LogisticRegression(solver='newton-cg', fit_intercept=False, tol=tol)
-    lbf = LogisticRegression(solver='lbfgs', fit_intercept=False, tol=tol)
-    lib = LogisticRegression(fit_intercept=False, tol=tol)
-    sag = LogisticRegression(solver='sag', fit_intercept=False, tol=tol,
-                             max_iter=1000, random_state=42)
-    saga = LogisticRegression(solver='saga', fit_intercept=False, tol=tol,
-                              max_iter=10000, random_state=42)
+    params = dict(fit_intercept=False, tol=tol, random_state=42,
+                  multi_class='ovr')
+    ncg = LogisticRegression(solver='newton-cg', **params)
+    lbf = LogisticRegression(solver='lbfgs', **params)
+    lib = LogisticRegression(solver='liblinear', **params)
+    sag = LogisticRegression(solver='sag', max_iter=1000, **params)
+    saga = LogisticRegression(solver='saga', max_iter=10000, **params)
     ncg.fit(X, y)
     lbf.fit(X, y)
     sag.fit(X, y)
@@ -744,20 +793,25 @@ def test_logistic_regressioncv_class_weights():
 
             clf_lbf = LogisticRegressionCV(solver='lbfgs', Cs=1,
                                            fit_intercept=False,
+                                           multi_class='ovr',
                                            class_weight=class_weight)
             clf_ncg = LogisticRegressionCV(solver='newton-cg', Cs=1,
                                            fit_intercept=False,
+                                           multi_class='ovr',
                                            class_weight=class_weight)
             clf_lib = LogisticRegressionCV(solver='liblinear', Cs=1,
                                            fit_intercept=False,
+                                           multi_class='ovr',
                                            class_weight=class_weight)
             clf_sag = LogisticRegressionCV(solver='sag', Cs=1,
                                            fit_intercept=False,
+                                           multi_class='ovr',
                                            class_weight=class_weight,
                                            tol=1e-5, max_iter=10000,
                                            random_state=0)
             clf_saga = LogisticRegressionCV(solver='saga', Cs=1,
                                             fit_intercept=False,
+                                            multi_class='ovr',
                                             class_weight=class_weight,
                                             tol=1e-5, max_iter=10000,
                                             random_state=0)
@@ -784,27 +838,29 @@ def test_logistic_regression_sample_weights():
         # not passing them at all (default None)
         for solver in ['lbfgs', 'liblinear']:
             clf_sw_none = LR(solver=solver, fit_intercept=False,
-                             random_state=42)
+                             random_state=42, multi_class='ovr')
             clf_sw_none.fit(X, y)
             clf_sw_ones = LR(solver=solver, fit_intercept=False,
-                             random_state=42)
+                             random_state=42, multi_class='ovr')
             clf_sw_ones.fit(X, y, sample_weight=np.ones(y.shape[0]))
             assert_array_almost_equal(
                 clf_sw_none.coef_, clf_sw_ones.coef_, decimal=4)
 
         # Test that sample weights work the same with the lbfgs,
         # newton-cg, and 'sag' solvers
-        clf_sw_lbfgs = LR(solver='lbfgs', fit_intercept=False, random_state=42)
+        clf_sw_lbfgs = LR(solver='lbfgs', fit_intercept=False, random_state=42,
+                          multi_class='ovr')
         clf_sw_lbfgs.fit(X, y, sample_weight=sample_weight)
-        clf_sw_n = LR(solver='newton-cg', fit_intercept=False, random_state=42)
+        clf_sw_n = LR(solver='newton-cg', fit_intercept=False, random_state=42,
+                      multi_class='ovr')
         clf_sw_n.fit(X, y, sample_weight=sample_weight)
         clf_sw_sag = LR(solver='sag', fit_intercept=False, tol=1e-10,
-                        random_state=42)
+                        random_state=42, multi_class='ovr')
         # ignore convergence warning due to small dataset
         with ignore_warnings():
             clf_sw_sag.fit(X, y, sample_weight=sample_weight)
         clf_sw_liblinear = LR(solver='liblinear', fit_intercept=False,
-                              random_state=42)
+                              random_state=42, multi_class='ovr')
         clf_sw_liblinear.fit(X, y, sample_weight=sample_weight)
         assert_array_almost_equal(
             clf_sw_lbfgs.coef_, clf_sw_n.coef_, decimal=4)
@@ -818,9 +874,11 @@ def test_logistic_regression_sample_weights():
         # to be 2 for all instances of class 2
         for solver in ['lbfgs', 'liblinear']:
             clf_cw_12 = LR(solver=solver, fit_intercept=False,
-                           class_weight={0: 1, 1: 2}, random_state=42)
+                           class_weight={0: 1, 1: 2}, random_state=42,
+                           multi_class='ovr')
             clf_cw_12.fit(X, y)
-            clf_sw_12 = LR(solver=solver, fit_intercept=False, random_state=42)
+            clf_sw_12 = LR(solver=solver, fit_intercept=False, random_state=42,
+                           multi_class='ovr')
             clf_sw_12.fit(X, y, sample_weight=sample_weight)
             assert_array_almost_equal(
                 clf_cw_12.coef_, clf_sw_12.coef_, decimal=4)
@@ -829,21 +887,21 @@ def test_logistic_regression_sample_weights():
     # since the patched liblinear code is different.
     clf_cw = LogisticRegression(
         solver="liblinear", fit_intercept=False, class_weight={0: 1, 1: 2},
-        penalty="l1", tol=1e-5, random_state=42)
+        penalty="l1", tol=1e-5, random_state=42, multi_class='ovr')
     clf_cw.fit(X, y)
     clf_sw = LogisticRegression(
         solver="liblinear", fit_intercept=False, penalty="l1", tol=1e-5,
-        random_state=42)
+        random_state=42, multi_class='ovr')
     clf_sw.fit(X, y, sample_weight)
     assert_array_almost_equal(clf_cw.coef_, clf_sw.coef_, decimal=4)
 
     clf_cw = LogisticRegression(
         solver="liblinear", fit_intercept=False, class_weight={0: 1, 1: 2},
-        penalty="l2", dual=True, random_state=42)
+        penalty="l2", dual=True, random_state=42, multi_class='ovr')
     clf_cw.fit(X, y)
     clf_sw = LogisticRegression(
         solver="liblinear", fit_intercept=False, penalty="l2", dual=True,
-        random_state=42)
+        random_state=42, multi_class='ovr')
     clf_sw.fit(X, y, sample_weight)
     assert_array_almost_equal(clf_cw.coef_, clf_sw.coef_, decimal=4)
 
@@ -974,7 +1032,8 @@ def test_liblinear_decision_function_zero():
     # See Issue: https://github.com/scikit-learn/scikit-learn/issues/3600
     # and the PR https://github.com/scikit-learn/scikit-learn/pull/3623
     X, y = make_classification(n_samples=5, n_features=5, random_state=0)
-    clf = LogisticRegression(fit_intercept=False)
+    clf = LogisticRegression(fit_intercept=False, solver='liblinear',
+                             multi_class='ovr')
     clf.fit(X, y)
 
     # Dummy data such that the decision function becomes zero.
@@ -987,10 +1046,11 @@ def test_liblinear_logregcv_sparse():
     # Test LogRegCV with solver='liblinear' works for sparse matrices
 
     X, y = make_classification(n_samples=10, n_features=5, random_state=0)
-    clf = LogisticRegressionCV(solver='liblinear')
+    clf = LogisticRegressionCV(solver='liblinear', multi_class='ovr')
     clf.fit(sparse.csr_matrix(X), y)
 
 
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_saga_sparse():
     # Test LogRegCV with solver='liblinear' works for sparse matrices
@@ -1004,13 +1064,16 @@ def test_logreg_intercept_scaling():
     # Test that the right error message is thrown when intercept_scaling <= 0
 
     for i in [-1, 0]:
-        clf = LogisticRegression(intercept_scaling=i)
+        clf = LogisticRegression(intercept_scaling=i, solver='liblinear',
+                                 multi_class='ovr')
         msg = ('Intercept scaling is %r but needs to be greater than 0.'
                ' To disable fitting an intercept,'
                ' set fit_intercept=False.' % clf.intercept_scaling)
         assert_raise_message(ValueError, msg, clf.fit, X, Y1)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_logreg_intercept_scaling_zero():
     # Test that intercept_scaling is ignored when fit_intercept is False
 
@@ -1031,12 +1094,12 @@ def test_logreg_l1():
     X_constant = np.ones(shape=(n_samples, 2))
     X = np.concatenate((X, X_noise, X_constant), axis=1)
     lr_liblinear = LogisticRegression(penalty="l1", C=1.0, solver='liblinear',
-                                      fit_intercept=False,
+                                      fit_intercept=False, multi_class='ovr',
                                       tol=1e-10)
     lr_liblinear.fit(X, y)
 
     lr_saga = LogisticRegression(penalty="l1", C=1.0, solver='saga',
-                                 fit_intercept=False,
+                                 fit_intercept=False, multi_class='ovr',
                                  max_iter=1000, tol=1e-10)
     lr_saga.fit(X, y)
     assert_array_almost_equal(lr_saga.coef_, lr_liblinear.coef_)
@@ -1062,12 +1125,12 @@ def test_logreg_l1_sparse_data():
     X = sparse.csr_matrix(X)
 
     lr_liblinear = LogisticRegression(penalty="l1", C=1.0, solver='liblinear',
-                                      fit_intercept=False,
+                                      fit_intercept=False, multi_class='ovr',
                                       tol=1e-10)
     lr_liblinear.fit(X, y)
 
     lr_saga = LogisticRegression(penalty="l1", C=1.0, solver='saga',
-                                 fit_intercept=False,
+                                 fit_intercept=False, multi_class='ovr',
                                  max_iter=1000, tol=1e-10)
     lr_saga.fit(X, y)
     assert_array_almost_equal(lr_saga.coef_, lr_liblinear.coef_)
@@ -1078,19 +1141,20 @@ def test_logreg_l1_sparse_data():
 
     # Check that solving on the sparse and dense data yield the same results
     lr_saga_dense = LogisticRegression(penalty="l1", C=1.0, solver='saga',
-                                       fit_intercept=False,
+                                       fit_intercept=False, multi_class='ovr',
                                        max_iter=1000, tol=1e-10)
     lr_saga_dense.fit(X.toarray(), y)
     assert_array_almost_equal(lr_saga.coef_, lr_saga_dense.coef_)
 
 
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_logreg_cv_penalty():
     # Test that the correct penalty is passed to the final fit.
     X, y = make_classification(n_samples=50, n_features=20, random_state=0)
-    lr_cv = LogisticRegressionCV(penalty="l1", Cs=[1.0], solver='liblinear')
+    lr_cv = LogisticRegressionCV(penalty="l1", Cs=[1.0], solver='saga')
     lr_cv.fit(X, y)
-    lr = LogisticRegression(penalty="l1", C=1.0, solver='liblinear')
+    lr = LogisticRegression(penalty="l1", C=1.0, solver='saga')
     lr.fit(X, y)
     assert_equal(np.count_nonzero(lr_cv.coef_), np.count_nonzero(lr.coef_))
 
@@ -1255,7 +1319,8 @@ def test_saga_vs_liblinear():
                 assert_array_almost_equal(saga.coef_, liblinear.coef_, 3)
 
 
-def test_dtype_match():
+@pytest.mark.parametrize('multi_class', ['ovr', 'multinomial'])
+def test_dtype_match(multi_class):
     # Test that np.float32 input data is not cast to np.float64 when possible
 
     X_32 = np.array(X).astype(np.float32)
@@ -1264,28 +1329,33 @@ def test_dtype_match():
     y_64 = np.array(Y1).astype(np.float64)
     X_sparse_32 = sp.csr_matrix(X, dtype=np.float32)
 
-    for solver in ['newton-cg']:
-        for multi_class in ['ovr', 'multinomial']:
+    solver = 'newton-cg'
+
+    # Check type consistency
+    lr_32 = LogisticRegression(solver=solver, multi_class=multi_class,
+                               random_state=42)
+    lr_32.fit(X_32, y_32)
+    assert_equal(lr_32.coef_.dtype, X_32.dtype)
+
+    # check consistency with sparsity
+    lr_32_sparse = LogisticRegression(solver=solver,
+                                      multi_class=multi_class,
+                                      random_state=42)
+    lr_32_sparse.fit(X_sparse_32, y_32)
+    assert_equal(lr_32_sparse.coef_.dtype, X_sparse_32.dtype)
 
-            # Check type consistency
-            lr_32 = LogisticRegression(solver=solver, multi_class=multi_class,
-                                       random_state=42)
-            lr_32.fit(X_32, y_32)
-            assert_equal(lr_32.coef_.dtype, X_32.dtype)
+    # Check accuracy consistency
+    lr_64 = LogisticRegression(solver=solver, multi_class=multi_class,
+                               random_state=42)
+    lr_64.fit(X_64, y_64)
+    assert_equal(lr_64.coef_.dtype, X_64.dtype)
 
-            # check consistency with sparsity
-            lr_32_sparse = LogisticRegression(solver=solver,
-                                              multi_class=multi_class,
-                                              random_state=42)
-            lr_32_sparse.fit(X_sparse_32, y_32)
-            assert_equal(lr_32_sparse.coef_.dtype, X_sparse_32.dtype)
+    rtol = 1e-6
+    if os.name == 'nt' and _IS_32BIT:
+        # FIXME
+        rtol = 1e-2
 
-            # Check accuracy consistency
-            lr_64 = LogisticRegression(solver=solver, multi_class=multi_class,
-                                       random_state=42)
-            lr_64.fit(X_64, y_64)
-            assert_equal(lr_64.coef_.dtype, X_64.dtype)
-            assert_almost_equal(lr_32.coef_, lr_64.coef_.astype(np.float32))
+    assert_allclose(lr_32.coef_, lr_64.coef_.astype(np.float32), rtol=rtol)
 
 
 def test_warm_start_converge_LR():
@@ -1307,3 +1377,70 @@ def test_warm_start_converge_LR():
         lr_ws.fit(X, y)
     lr_ws_loss = log_loss(y, lr_ws.predict_proba(X))
     assert_allclose(lr_no_ws_loss, lr_ws_loss, rtol=1e-5)
+
+
+def test_logistic_regression_path_coefs_multinomial():
+    # Make sure that the returned coefs by logistic_regression_path when
+    # multi_class='multinomial' don't override each other (used to be a
+    # bug).
+    X, y = make_classification(n_samples=200, n_classes=3, n_informative=2,
+                               n_redundant=0, n_clusters_per_class=1,
+                               random_state=0, n_features=2)
+    Cs = [.00001, 1, 10000]
+    coefs, _, _ = logistic_regression_path(X, y, penalty='l1', Cs=Cs,
+                                           solver='saga', random_state=0,
+                                           multi_class='multinomial')
+
+    with pytest.raises(AssertionError):
+        assert_array_almost_equal(coefs[0], coefs[1], decimal=1)
+    with pytest.raises(AssertionError):
+        assert_array_almost_equal(coefs[0], coefs[2], decimal=1)
+    with pytest.raises(AssertionError):
+        assert_array_almost_equal(coefs[1], coefs[2], decimal=1)
+
+
+@pytest.mark.parametrize('est', [LogisticRegression(random_state=0),
+                                 LogisticRegressionCV(random_state=0, cv=3),
+                                 ])
+@pytest.mark.parametrize('solver', ['liblinear', 'lbfgs', 'newton-cg', 'sag',
+                                    'saga'])
+def test_logistic_regression_multi_class_auto(est, solver):
+    # check multi_class='auto' => multi_class='ovr' iff binary y or liblinear
+
+    def fit(X, y, **kw):
+        return clone(est).set_params(**kw).fit(X, y)
+
+    X = iris.data[::10]
+    X2 = iris.data[1::10]
+    y_multi = iris.target[::10]
+    y_bin = y_multi == 0
+    est_auto_bin = fit(X, y_bin, multi_class='auto', solver=solver)
+    est_ovr_bin = fit(X, y_bin, multi_class='ovr', solver=solver)
+    assert np.allclose(est_auto_bin.coef_, est_ovr_bin.coef_)
+    assert np.allclose(est_auto_bin.predict_proba(X2),
+                       est_ovr_bin.predict_proba(X2))
+
+    est_auto_multi = fit(X, y_multi, multi_class='auto', solver=solver)
+    if solver == 'liblinear':
+        est_ovr_multi = fit(X, y_multi, multi_class='ovr', solver=solver)
+        assert np.allclose(est_auto_multi.coef_, est_ovr_multi.coef_)
+        assert np.allclose(est_auto_multi.predict_proba(X2),
+                           est_ovr_multi.predict_proba(X2))
+    else:
+        est_multi_multi = fit(X, y_multi, multi_class='multinomial',
+                              solver=solver)
+        if sys.platform == 'darwin' and solver == 'lbfgs':
+            pytest.xfail('Issue #11924: LogisticRegressionCV(solver="lbfgs", '
+                         'multi_class="multinomial") is nondterministic on '
+                         'MacOS.')  # pragma: no cover
+        assert np.allclose(est_auto_multi.coef_, est_multi_multi.coef_)
+        assert np.allclose(est_auto_multi.predict_proba(X2),
+                           est_multi_multi.predict_proba(X2))
+
+        # Make sure multi_class='ovr' is distinct from ='multinomial'
+        assert not np.allclose(est_auto_bin.coef_,
+                               fit(X, y_bin, multi_class='multinomial',
+                                   solver=solver).coef_)
+        assert not np.allclose(est_auto_bin.coef_,
+                               fit(X, y_multi, multi_class='multinomial',
+                                   solver=solver).coef_)
diff --git a/sklearn/linear_model/tests/test_sag.py b/sklearn/linear_model/tests/test_sag.py
index ca99a81a73..8f4dbc8794 100755
--- a/sklearn/linear_model/tests/test_sag.py
+++ b/sklearn/linear_model/tests/test_sag.py
@@ -247,7 +247,8 @@ def test_classifier_matching():
             n_iter = 300
         clf = LogisticRegression(solver=solver, fit_intercept=fit_intercept,
                                  tol=1e-11, C=1. / alpha / n_samples,
-                                 max_iter=n_iter, random_state=10)
+                                 max_iter=n_iter, random_state=10,
+                                 multi_class='ovr')
         clf.fit(X, y)
 
         weights, intercept = sag_sparse(X, y, step_size, alpha, n_iter=n_iter,
@@ -311,11 +312,12 @@ def test_sag_pobj_matches_logistic_regression():
 
     clf1 = LogisticRegression(solver='sag', fit_intercept=False, tol=.0000001,
                               C=1. / alpha / n_samples, max_iter=max_iter,
-                              random_state=10)
+                              random_state=10, multi_class='ovr')
     clf2 = clone(clf1)
     clf3 = LogisticRegression(fit_intercept=False, tol=.0000001,
                               C=1. / alpha / n_samples, max_iter=max_iter,
-                              random_state=10)
+                              random_state=10, multi_class='ovr',
+                              solver='lbfgs')
 
     clf1.fit(X, y)
     clf2.fit(sp.csr_matrix(X), y)
@@ -507,7 +509,7 @@ def test_sag_classifier_computed_correctly():
 
     clf1 = LogisticRegression(solver='sag', C=1. / alpha / n_samples,
                               max_iter=n_iter, tol=tol, random_state=77,
-                              fit_intercept=fit_intercept)
+                              fit_intercept=fit_intercept, multi_class='ovr')
     clf2 = clone(clf1)
 
     clf1.fit(X, y)
@@ -547,7 +549,7 @@ def test_sag_multiclass_computed_correctly():
 
     clf1 = LogisticRegression(solver='sag', C=1. / alpha / n_samples,
                               max_iter=max_iter, tol=tol, random_state=77,
-                              fit_intercept=fit_intercept)
+                              fit_intercept=fit_intercept, multi_class='ovr')
     clf2 = clone(clf1)
 
     clf1.fit(X, y)
@@ -591,6 +593,7 @@ def test_sag_multiclass_computed_correctly():
         assert_almost_equal(clf2.intercept_[i], intercept2[i], decimal=1)
 
 
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_classifier_results():
     """tests if classifier results match target"""
     alpha = .1
@@ -634,7 +637,7 @@ def test_binary_classifier_class_weight():
     class_weight = {1: .45, -1: .55}
     clf1 = LogisticRegression(solver='sag', C=1. / alpha / n_samples,
                               max_iter=n_iter, tol=tol, random_state=77,
-                              fit_intercept=fit_intercept,
+                              fit_intercept=fit_intercept, multi_class='ovr',
                               class_weight=class_weight)
     clf2 = clone(clf1)
 
@@ -681,7 +684,7 @@ def test_multiclass_classifier_class_weight():
 
     clf1 = LogisticRegression(solver='sag', C=1. / alpha / n_samples,
                               max_iter=max_iter, tol=tol, random_state=77,
-                              fit_intercept=fit_intercept,
+                              fit_intercept=fit_intercept, multi_class='ovr',
                               class_weight=class_weight)
     clf2 = clone(clf1)
     clf1.fit(X, y)
@@ -728,6 +731,7 @@ def test_multiclass_classifier_class_weight():
         assert_almost_equal(clf2.intercept_[i], intercept2[i], decimal=1)
 
 
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_classifier_single_class():
     """tests if ValueError is thrown with only one class"""
     X = [[1, 2], [3, 4]]
@@ -740,6 +744,7 @@ def test_classifier_single_class():
                          X, y)
 
 
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_step_size_alpha_error():
     X = [[0, 0], [0, 0]]
     y = [1, -1]
diff --git a/sklearn/linear_model/theil_sen.py b/sklearn/linear_model/theil_sen.py
index de71e24cc6..0f3b19164b 100755
--- a/sklearn/linear_model/theil_sen.py
+++ b/sklearn/linear_model/theil_sen.py
@@ -20,7 +20,7 @@
 from .base import LinearModel
 from ..base import RegressorMixin
 from ..utils import check_random_state
-from ..utils import check_X_y, _get_n_jobs
+from ..utils import check_X_y, effective_n_jobs
 from ..utils import Parallel, delayed
 from ..externals.six.moves import xrange as range
 from ..exceptions import ConvergenceWarning
@@ -249,9 +249,11 @@ class TheilSenRegressor(LinearModel, RegressorMixin):
         random number generator; If None, the random number generator is the
         RandomState instance used by `np.random`.
 
-    n_jobs : integer, optional, default 1
-        Number of CPUs to use during the cross validation. If ``-1``, use
-        all the CPUs.
+    n_jobs : int or None, optional (default=None)
+        Number of CPUs to use during the cross validation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : boolean, optional, default False
         Verbose mode when fitting the model.
@@ -283,7 +285,7 @@ class TheilSenRegressor(LinearModel, RegressorMixin):
 
     def __init__(self, fit_intercept=True, copy_X=True,
                  max_subpopulation=1e4, n_subsamples=None, max_iter=300,
-                 tol=1.e-3, random_state=None, n_jobs=1, verbose=False):
+                 tol=1.e-3, random_state=None, n_jobs=None, verbose=False):
         self.fit_intercept = fit_intercept
         self.copy_X = copy_X
         self.max_subpopulation = int(max_subpopulation)
@@ -368,7 +370,7 @@ def fit(self, X, y):
                                            replace=False)
                        for _ in range(self.n_subpopulation_)]
 
-        n_jobs = _get_n_jobs(self.n_jobs)
+        n_jobs = effective_n_jobs(self.n_jobs)
         index_list = np.array_split(indices, n_jobs)
         weights = Parallel(n_jobs=n_jobs,
                            verbose=self.verbose)(
diff --git a/sklearn/manifold/isomap.py b/sklearn/manifold/isomap.py
index 5d6ae04ce4..bbb83a5ed8 100755
--- a/sklearn/manifold/isomap.py
+++ b/sklearn/manifold/isomap.py
@@ -58,9 +58,11 @@ class Isomap(BaseEstimator, TransformerMixin):
         Algorithm to use for nearest neighbors search,
         passed to neighbors.NearestNeighbors instance.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -80,6 +82,18 @@ class Isomap(BaseEstimator, TransformerMixin):
     dist_matrix_ : array-like, shape (n_samples, n_samples)
         Stores the geodesic distance matrix of training data.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.manifold import Isomap
+    >>> X, _ = load_digits(return_X_y=True)
+    >>> X.shape
+    (1797, 64)
+    >>> embedding = Isomap(n_components=2)
+    >>> X_transformed = embedding.fit_transform(X[:100])
+    >>> X_transformed.shape
+    (100, 2)
+
     References
     ----------
 
@@ -89,7 +103,7 @@ class Isomap(BaseEstimator, TransformerMixin):
 
     def __init__(self, n_neighbors=5, n_components=2, eigen_solver='auto',
                  tol=0, max_iter=None, path_method='auto',
-                 neighbors_algorithm='auto', n_jobs=1):
+                 neighbors_algorithm='auto', n_jobs=None):
         self.n_neighbors = n_neighbors
         self.n_components = n_components
         self.eigen_solver = eigen_solver
diff --git a/sklearn/manifold/locally_linear.py b/sklearn/manifold/locally_linear.py
index 0d174e785f..c3afdac2cb 100755
--- a/sklearn/manifold/locally_linear.py
+++ b/sklearn/manifold/locally_linear.py
@@ -64,7 +64,7 @@ def barycenter_weights(X, Z, reg=1e-3):
     return B
 
 
-def barycenter_kneighbors_graph(X, n_neighbors, reg=1e-3, n_jobs=1):
+def barycenter_kneighbors_graph(X, n_neighbors, reg=1e-3, n_jobs=None):
     """Computes the barycenter weighted graph of k-Neighbors for points in X
 
     Parameters
@@ -81,9 +81,11 @@ def barycenter_kneighbors_graph(X, n_neighbors, reg=1e-3, n_jobs=1):
         problem. Only relevant if mode='barycenter'. If None, use the
         default.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Returns
     -------
@@ -186,7 +188,7 @@ def null_space(M, k, k_skip=1, eigen_solver='arpack', tol=1E-6, max_iter=100,
 def locally_linear_embedding(
         X, n_neighbors, n_components, reg=1e-3, eigen_solver='auto', tol=1e-6,
         max_iter=100, method='standard', hessian_tol=1E-4, modified_tol=1E-12,
-        random_state=None, n_jobs=1):
+        random_state=None, n_jobs=None):
     """Perform a Locally Linear Embedding analysis on the data.
 
     Read more in the :ref:`User Guide <locally_linear_embedding>`.
@@ -253,9 +255,11 @@ def locally_linear_embedding(
         If None, the random number generator is the RandomState instance used
         by `np.random`. Used when ``solver`` == 'arpack'.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Returns
     -------
@@ -582,9 +586,11 @@ class LocallyLinearEmbedding(BaseEstimator, TransformerMixin):
         If None, the random number generator is the RandomState instance used
         by `np.random`. Used when ``eigen_solver`` == 'arpack'.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -598,6 +604,18 @@ class LocallyLinearEmbedding(BaseEstimator, TransformerMixin):
         Stores nearest neighbors instance, including BallTree or KDtree
         if applicable.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.manifold import LocallyLinearEmbedding
+    >>> X, _ = load_digits(return_X_y=True)
+    >>> X.shape
+    (1797, 64)
+    >>> embedding = LocallyLinearEmbedding(n_components=2)
+    >>> X_transformed = embedding.fit_transform(X[:100])
+    >>> X_transformed.shape
+    (100, 2)
+
     References
     ----------
 
@@ -617,7 +635,7 @@ class LocallyLinearEmbedding(BaseEstimator, TransformerMixin):
     def __init__(self, n_neighbors=5, n_components=2, reg=1E-3,
                  eigen_solver='auto', tol=1E-6, max_iter=100,
                  method='standard', hessian_tol=1E-4, modified_tol=1E-12,
-                 neighbors_algorithm='auto', random_state=None, n_jobs=1):
+                 neighbors_algorithm='auto', random_state=None, n_jobs=None):
         self.n_neighbors = n_neighbors
         self.n_components = n_components
         self.reg = reg
diff --git a/sklearn/manifold/mds.py b/sklearn/manifold/mds.py
index 130dc9a0e9..ecfe22af28 100755
--- a/sklearn/manifold/mds.py
+++ b/sklearn/manifold/mds.py
@@ -14,6 +14,7 @@
 from ..utils import check_random_state, check_array, check_symmetric
 from ..utils import Parallel
 from ..utils import delayed
+from ..utils import effective_n_jobs
 from ..isotonic import IsotonicRegression
 
 
@@ -132,7 +133,7 @@ def _smacof_single(dissimilarities, metric=True, n_components=2, init=None,
 
 
 def smacof(dissimilarities, metric=True, n_components=2, init=None, n_init=8,
-           n_jobs=1, max_iter=300, verbose=0, eps=1e-3, random_state=None,
+           n_jobs=None, max_iter=300, verbose=0, eps=1e-3, random_state=None,
            return_n_iter=False):
     """Computes multidimensional scaling using the SMACOF algorithm.
 
@@ -177,15 +178,14 @@ def smacof(dissimilarities, metric=True, n_components=2, init=None, n_init=8,
         determined by the run with the smallest final stress. If ``init`` is
         provided, this option is overridden and a single run is performed.
 
-    n_jobs : int, optional, default: 1
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. If multiple
         initializations are used (``n_init``), each run of the algorithm is
         computed in parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For ``n_jobs`` below -1,
-        (``n_cpus + 1 + n_jobs``) are used. Thus for ``n_jobs = -2``, all CPUs
-        but one are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     max_iter : int, optional, default: 300
         Maximum number of iterations of the SMACOF algorithm for a single run.
@@ -245,7 +245,7 @@ def smacof(dissimilarities, metric=True, n_components=2, init=None, n_init=8,
 
     best_pos, best_stress = None, None
 
-    if n_jobs == 1:
+    if effective_n_jobs(n_jobs) == 1:
         for it in range(n_init):
             pos, stress, n_iter_ = _smacof_single(
                 dissimilarities, metric=metric,
@@ -304,15 +304,14 @@ class MDS(BaseEstimator):
         Relative tolerance with respect to stress at which to declare
         convergence.
 
-    n_jobs : int, optional, default: 1
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. If multiple
         initializations are used (``n_init``), each run of the algorithm is
         computed in parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For ``n_jobs`` below -1,
-        (``n_cpus + 1 + n_jobs``) are used. Thus for ``n_jobs = -2``, all CPUs
-        but one are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional, default: None
         The generator used to initialize the centers.  If int, random_state is
@@ -332,13 +331,24 @@ class MDS(BaseEstimator):
 
     Attributes
     ----------
-    embedding_ : array-like, shape (n_components, n_samples)
+    embedding_ : array-like, shape (n_samples, n_components)
         Stores the position of the dataset in the embedding space.
 
     stress_ : float
         The final value of the stress (sum of squared distance of the
         disparities and the distances for all constrained points).
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.manifold import MDS
+    >>> X, _ = load_digits(return_X_y=True)
+    >>> X.shape
+    (1797, 64)
+    >>> embedding = MDS(n_components=2)
+    >>> X_transformed = embedding.fit_transform(X[:100])
+    >>> X_transformed.shape
+    (100, 2)
 
     References
     ----------
@@ -353,7 +363,7 @@ class MDS(BaseEstimator):
 
     """
     def __init__(self, n_components=2, metric=True, n_init=4,
-                 max_iter=300, verbose=0, eps=1e-3, n_jobs=1,
+                 max_iter=300, verbose=0, eps=1e-3, n_jobs=None,
                  random_state=None, dissimilarity="euclidean"):
         self.n_components = n_components
         self.dissimilarity = dissimilarity
diff --git a/sklearn/manifold/spectral_embedding_.py b/sklearn/manifold/spectral_embedding_.py
index 3dca967a54..71c822e3b4 100755
--- a/sklearn/manifold/spectral_embedding_.py
+++ b/sklearn/manifold/spectral_embedding_.py
@@ -382,9 +382,11 @@ class SpectralEmbedding(BaseEstimator):
     n_neighbors : int, default : max(n_samples/10 , 1)
         Number of nearest neighbors for nearest_neighbors graph building.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -395,6 +397,18 @@ class SpectralEmbedding(BaseEstimator):
     affinity_matrix_ : array, shape = (n_samples, n_samples)
         Affinity_matrix constructed from samples or precomputed.
 
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.manifold import SpectralEmbedding
+    >>> X, _ = load_digits(return_X_y=True)
+    >>> X.shape
+    (1797, 64)
+    >>> embedding = SpectralEmbedding(n_components=2)
+    >>> X_transformed = embedding.fit_transform(X[:100])
+    >>> X_transformed.shape
+    (100, 2)
+
     References
     ----------
 
@@ -413,7 +427,7 @@ class SpectralEmbedding(BaseEstimator):
 
     def __init__(self, n_components=2, affinity="nearest_neighbors",
                  gamma=None, random_state=None, eigen_solver=None,
-                 n_neighbors=None, n_jobs=1):
+                 n_neighbors=None, n_jobs=None):
         self.n_components = n_components
         self.affinity = affinity
         self.gamma = gamma
diff --git a/sklearn/manifold/t_sne.py b/sklearn/manifold/t_sne.py
index bcaaa68e99..1c69036d0d 100755
--- a/sklearn/manifold/t_sne.py
+++ b/sklearn/manifold/t_sne.py
@@ -614,7 +614,7 @@ class TSNE(BaseEstimator):
         Using t-SNE. Journal of Machine Learning Research 9:2579-2605, 2008.
 
     [2] van der Maaten, L.J.P. t-Distributed Stochastic Neighbor Embedding
-        http://homepage.tudelft.nl/19j49/t-SNE.html
+        https://lvdmaaten.github.io/tsne/
 
     [3] L.J.P. van der Maaten. Accelerating t-SNE using Tree-Based Algorithms.
         Journal of Machine Learning Research 15(Oct):3221-3245, 2014.
diff --git a/sklearn/metrics/classification.py b/sklearn/metrics/classification.py
index 2cd01acdd2..c8130783c2 100755
--- a/sklearn/metrics/classification.py
+++ b/sklearn/metrics/classification.py
@@ -628,8 +628,9 @@ def f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary',
 
         F1 = 2 * (precision * recall) / (precision + recall)
 
-    In the multi-class and multi-label case, this is the weighted average of
-    the F1 score of each class.
+    In the multi-class and multi-label case, this is the average of
+    the F1 score of each class with weighting depending on the ``average``
+    parameter.
 
     Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.
 
@@ -1483,9 +1484,12 @@ def classification_report(y_true, y_pred, labels=None, target_names=None,
               ...
             }
 
-        The reported averages are a prevalence-weighted macro-average across
-        classes (equivalent to :func:`precision_recall_fscore_support` with
-        ``average='weighted'``).
+        The reported averages include micro average (averaging the
+        total true positives, false negatives and false positives), macro
+        average (averaging the unweighted mean per label), weighted average
+        (averaging the support-weighted mean per label) and sample average
+        (only for multilabel classification). See also
+        :func:`precision_recall_fscore_support` for more details on averages.
 
         Note that in binary classification, recall of the positive class
         is also known as "sensitivity"; recall of the negative class is
@@ -1498,17 +1502,20 @@ def classification_report(y_true, y_pred, labels=None, target_names=None,
     >>> y_pred = [0, 0, 2, 2, 1]
     >>> target_names = ['class 0', 'class 1', 'class 2']
     >>> print(classification_report(y_true, y_pred, target_names=target_names))
-                 precision    recall  f1-score   support
+                  precision    recall  f1-score   support
     <BLANKLINE>
-        class 0       0.50      1.00      0.67         1
-        class 1       0.00      0.00      0.00         1
-        class 2       1.00      0.67      0.80         3
+         class 0       0.50      1.00      0.67         1
+         class 1       0.00      0.00      0.00         1
+         class 2       1.00      0.67      0.80         3
     <BLANKLINE>
-    avg / total       0.70      0.60      0.61         5
+       micro avg       0.60      0.60      0.60         5
+       macro avg       0.50      0.56      0.49         5
+    weighted avg       0.70      0.60      0.61         5
     <BLANKLINE>
-
     """
 
+    y_type, y_true, y_pred = _check_targets(y_true, y_pred)
+
     labels_given = True
     if labels is None:
         labels = unique_labels(y_true, y_pred)
@@ -1528,53 +1535,59 @@ class 2       1.00      0.67      0.80         3
                 "target_names, {1}. Try specifying the labels "
                 "parameter".format(len(labels), len(target_names))
             )
-
-    last_line_heading = 'avg / total'
-
     if target_names is None:
         target_names = [u'%s' % l for l in labels]
-    name_width = max(len(cn) for cn in target_names)
-    width = max(name_width, len(last_line_heading), digits)
 
     headers = ["precision", "recall", "f1-score", "support"]
-    head_fmt = u'{:>{width}s} ' + u' {:>9}' * len(headers)
-    report = head_fmt.format(u'', *headers, width=width)
-    report += u'\n\n'
-
+    # compute per-class results without averaging
     p, r, f1, s = precision_recall_fscore_support(y_true, y_pred,
                                                   labels=labels,
                                                   average=None,
                                                   sample_weight=sample_weight)
-
-    row_fmt = u'{:>{width}s} ' + u' {:>9.{digits}f}' * 3 + u' {:>9}\n'
     rows = zip(target_names, p, r, f1, s)
 
-    avg_total = [np.average(p, weights=s),
-                 np.average(r, weights=s),
-                 np.average(f1, weights=s),
-                 np.sum(s)]
+    if y_type.startswith('multilabel'):
+        average_options = ('micro', 'macro', 'weighted', 'samples')
+    else:
+        average_options = ('micro', 'macro', 'weighted')
 
     if output_dict:
         report_dict = {label[0]: label[1:] for label in rows}
-
         for label, scores in report_dict.items():
-            report_dict[label] = dict(zip(headers, scores))
-
-        report_dict['avg / total'] = dict(zip(headers, avg_total))
+            report_dict[label] = dict(zip(headers,
+                                          [i.item() for i in scores]))
+    else:
+        longest_last_line_heading = 'weighted avg'
+        name_width = max(len(cn) for cn in target_names)
+        width = max(name_width, len(longest_last_line_heading), digits)
+        head_fmt = u'{:>{width}s} ' + u' {:>9}' * len(headers)
+        report = head_fmt.format(u'', *headers, width=width)
+        report += u'\n\n'
+        row_fmt = u'{:>{width}s} ' + u' {:>9.{digits}f}' * 3 + u' {:>9}\n'
+        for row in rows:
+            report += row_fmt.format(*row, width=width, digits=digits)
+        report += u'\n'
+
+    # compute all applicable averages
+    for average in average_options:
+        line_heading = average + ' avg'
+        # compute averages with specified averaging method
+        avg_p, avg_r, avg_f1, _ = precision_recall_fscore_support(
+            y_true, y_pred, labels=labels,
+            average=average, sample_weight=sample_weight)
+        avg = [avg_p, avg_r, avg_f1, np.sum(s)]
+
+        if output_dict:
+            report_dict[line_heading] = dict(
+                zip(headers, [i.item() for i in avg]))
+        else:
+            report += row_fmt.format(line_heading, *avg,
+                                     width=width, digits=digits)
 
+    if output_dict:
         return report_dict
-
-    for row in rows:
-        report += row_fmt.format(*row, width=width, digits=digits)
-
-    report += u'\n'
-
-    # append averages
-    report += row_fmt.format(last_line_heading,
-                             *avg_total,
-                             width=width, digits=digits)
-
-    return report
+    else:
+        return report
 
 
 def hamming_loss(y_true, y_pred, labels=None, sample_weight=None):
diff --git a/sklearn/metrics/cluster/supervised.py b/sklearn/metrics/cluster/supervised.py
index 2f084c4fce..8483ef11f8 100755
--- a/sklearn/metrics/cluster/supervised.py
+++ b/sklearn/metrics/cluster/supervised.py
@@ -767,14 +767,14 @@ def normalized_mutual_info_score(labels_true, labels_pred,
                                  average_method='warn'):
     """Normalized Mutual Information between two clusterings.
 
-    Normalized Mutual Information (NMI) is an normalization of the Mutual
+    Normalized Mutual Information (NMI) is a normalization of the Mutual
     Information (MI) score to scale the results between 0 (no mutual
     information) and 1 (perfect correlation). In this function, mutual
     information is normalized by some generalized mean of ``H(labels_true)``
     and ``H(labels_pred))``, defined by the `average_method`.
 
     This measure is not adjusted for chance. Therefore
-    :func:`adjusted_mustual_info_score` might be preferred.
+    :func:`adjusted_mutual_info_score` might be preferred.
 
     This metric is independent of the absolute values of the labels:
     a permutation of the class or cluster label values won't change the
diff --git a/sklearn/metrics/pairwise.py b/sklearn/metrics/pairwise.py
index 01c3cde04a..2e56255af0 100755
--- a/sklearn/metrics/pairwise.py
+++ b/sklearn/metrics/pairwise.py
@@ -26,7 +26,7 @@
 from ..preprocessing import normalize
 from ..utils import Parallel
 from ..utils import delayed
-from ..utils import cpu_count
+from ..utils import effective_n_jobs
 
 from .pairwise_fast import _chi2_kernel_fast, _sparse_manhattan
 
@@ -1061,21 +1061,18 @@ def distance_metrics():
 def _parallel_pairwise(X, Y, func, n_jobs, **kwds):
     """Break the pairwise matrix in n_jobs even slices
     and compute them in parallel"""
-    if n_jobs < 0:
-        n_jobs = max(cpu_count() + 1 + n_jobs, 1)
 
     if Y is None:
         Y = X
 
-    if n_jobs == 1:
-        # Special case to avoid picklability checks in delayed
+    if effective_n_jobs(n_jobs) == 1:
         return func(X, Y, **kwds)
 
     # TODO: in some cases, backend='threading' may be appropriate
     fd = delayed(func)
     ret = Parallel(n_jobs=n_jobs, verbose=0)(
         fd(X, Y[s], **kwds)
-        for s in gen_even_slices(Y.shape[0], n_jobs))
+        for s in gen_even_slices(_num_samples(Y), effective_n_jobs(n_jobs)))
 
     return np.hstack(ret)
 
@@ -1142,7 +1139,7 @@ def _check_chunk_size(reduced, chunk_size):
 
 
 def pairwise_distances_chunked(X, Y=None, reduce_func=None,
-                               metric='euclidean', n_jobs=1,
+                               metric='euclidean', n_jobs=None,
                                working_memory=None, **kwds):
     """Generate a distance matrix chunk by chunk with optional reduction
 
@@ -1184,15 +1181,14 @@ def pairwise_distances_chunked(X, Y=None, reduce_func=None,
         should take two arrays from X as input and return a value indicating
         the distance between them.
 
-    n_jobs : int
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. This works by breaking
         down the pairwise matrix into n_jobs even slices and computing them in
         parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For n_jobs below -1,
-        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
-        are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     working_memory : int, optional
         The sought maximum memory for temporary distance matrix chunks.
@@ -1291,7 +1287,7 @@ def pairwise_distances_chunked(X, Y=None, reduce_func=None,
         yield D_chunk
 
 
-def pairwise_distances(X, Y=None, metric="euclidean", n_jobs=1, **kwds):
+def pairwise_distances(X, Y=None, metric="euclidean", n_jobs=None, **kwds):
     """ Compute the distance matrix from a vector array X and optional Y.
 
     This method takes either a vector array or a distance matrix, and returns
@@ -1347,15 +1343,14 @@ def pairwise_distances(X, Y=None, metric="euclidean", n_jobs=1, **kwds):
         should take two arrays from X as input and return a value indicating
         the distance between them.
 
-    n_jobs : int
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. This works by breaking
         down the pairwise matrix into n_jobs even slices and computing them in
         parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For n_jobs below -1,
-        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
-        are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     **kwds : optional keyword parameters
         Any further parameters are passed directly to the distance function.
@@ -1372,9 +1367,9 @@ def pairwise_distances(X, Y=None, metric="euclidean", n_jobs=1, **kwds):
 
     See also
     --------
-    pairwise_distances_chunked : performs the same calculation as this funtion,
-        but returns a generator of chunks of the distance matrix, in order to
-        limit memory usage.
+    pairwise_distances_chunked : performs the same calculation as this
+        function, but returns a generator of chunks of the distance matrix, in
+        order to limit memory usage.
     paired_distances : Computes the distances between corresponding
                        elements of two arrays
     """
@@ -1397,10 +1392,9 @@ def pairwise_distances(X, Y=None, metric="euclidean", n_jobs=1, **kwds):
                             " support sparse matrices.")
 
         dtype = bool if metric in PAIRWISE_BOOLEAN_FUNCTIONS else None
-
         X, Y = check_pairwise_arrays(X, Y, dtype=dtype)
 
-        if n_jobs == 1 and X is Y:
+        if effective_n_jobs(n_jobs) == 1 and X is Y:
             return distance.squareform(distance.pdist(X, metric=metric,
                                                       **kwds))
         func = partial(distance.cdist, metric=metric, **kwds)
@@ -1478,7 +1472,7 @@ def kernel_metrics():
 
 
 def pairwise_kernels(X, Y=None, metric="linear", filter_params=False,
-                     n_jobs=1, **kwds):
+                     n_jobs=None, **kwds):
     """Compute the kernel between arrays X and optional array Y.
 
     This method takes either a vector array or a kernel matrix, and returns
@@ -1519,15 +1513,14 @@ def pairwise_kernels(X, Y=None, metric="linear", filter_params=False,
     filter_params : boolean
         Whether to filter invalid parameters or not.
 
-    n_jobs : int
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation. This works by breaking
         down the pairwise matrix into n_jobs even slices and computing them in
         parallel.
 
-        If -1 all CPUs are used. If 1 is given, no parallel computing code is
-        used at all, which is useful for debugging. For n_jobs below -1,
-        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
-        are used.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     **kwds : optional keyword parameters
         Any further parameters are passed directly to the kernel function.
diff --git a/sklearn/metrics/scorer.py b/sklearn/metrics/scorer.py
index 021f1eca37..2661a379b4 100755
--- a/sklearn/metrics/scorer.py
+++ b/sklearn/metrics/scorer.py
@@ -216,8 +216,8 @@ def get_scorer(scoring):
             scorer = SCORERS[scoring]
         except KeyError:
             raise ValueError('%r is not a valid scoring value. '
-                             'Valid options are %s'
-                             % (scoring, sorted(SCORERS.keys())))
+                             'Use sorted(sklearn.metrics.SCORERS.keys()) '
+                             'to get valid options.' % (scoring))
     else:
         scorer = scoring
     return scorer
diff --git a/sklearn/metrics/tests/test_classification.py b/sklearn/metrics/tests/test_classification.py
index 512e609345..c07f9d66aa 100755
--- a/sklearn/metrics/tests/test_classification.py
+++ b/sklearn/metrics/tests/test_classification.py
@@ -123,16 +123,35 @@ def test_classification_report_dictionary_output():
                                      'recall': 0.90000000000000002,
                                      'f1-score': 0.57142857142857151,
                                      'support': 20},
-                       'avg / total': {'precision': 0.51375351084147847,
-                                       'recall': 0.53333333333333333,
-                                       'f1-score': 0.47310435663627154,
-                                       'support': 75}}
+                       'macro avg': {'f1-score': 0.5099797365754813,
+                                     'precision': 0.5260083136726211,
+                                     'recall': 0.596146953405018,
+                                     'support': 75},
+                       'micro avg': {'f1-score': 0.5333333333333333,
+                                     'precision': 0.5333333333333333,
+                                     'recall': 0.5333333333333333,
+                                     'support': 75},
+                       'weighted avg': {'f1-score': 0.47310435663627154,
+                                        'precision': 0.5137535108414785,
+                                        'recall': 0.5333333333333333,
+                                        'support': 75}}
 
     report = classification_report(
         y_true, y_pred, labels=np.arange(len(iris.target_names)),
         target_names=iris.target_names, output_dict=True)
 
-    assert_dict_equal(report, expected_report)
+    # assert the 2 dicts are equal.
+    assert(report.keys() == expected_report.keys())
+    for key in expected_report:
+        assert report[key].keys() == expected_report[key].keys()
+        for metric in expected_report[key]:
+            assert_almost_equal(expected_report[key][metric],
+                                report[key][metric])
+
+    assert type(expected_report['setosa']['precision']) == float
+    assert type(expected_report['macro avg']['precision']) == float
+    assert type(expected_report['setosa']['support']) == int
+    assert type(expected_report['macro avg']['support']) == int
 
 
 def test_multilabel_accuracy_score_subset_accuracy():
@@ -728,27 +747,55 @@ def test_classification_report_multiclass():
 
     # print classification report with class names
     expected_report = """\
-             precision    recall  f1-score   support
+              precision    recall  f1-score   support
 
-     setosa       0.83      0.79      0.81        24
- versicolor       0.33      0.10      0.15        31
-  virginica       0.42      0.90      0.57        20
+      setosa       0.83      0.79      0.81        24
+  versicolor       0.33      0.10      0.15        31
+   virginica       0.42      0.90      0.57        20
 
-avg / total       0.51      0.53      0.47        75
+   micro avg       0.53      0.53      0.53        75
+   macro avg       0.53      0.60      0.51        75
+weighted avg       0.51      0.53      0.47        75
 """
     report = classification_report(
         y_true, y_pred, labels=np.arange(len(iris.target_names)),
         target_names=iris.target_names)
     assert_equal(report, expected_report)
+
+
+def test_classification_report_multiclass_balanced():
+    y_true, y_pred = [0, 0, 0, 1, 1, 1, 2, 2, 2], [0, 1, 2, 0, 1, 2, 0, 1, 2]
+
+    expected_report = """\
+              precision    recall  f1-score   support
+
+           0       0.33      0.33      0.33         3
+           1       0.33      0.33      0.33         3
+           2       0.33      0.33      0.33         3
+
+   micro avg       0.33      0.33      0.33         9
+   macro avg       0.33      0.33      0.33         9
+weighted avg       0.33      0.33      0.33         9
+"""
+    report = classification_report(y_true, y_pred)
+    assert_equal(report, expected_report)
+
+
+def test_classification_report_multiclass_with_label_detection():
+    iris = datasets.load_iris()
+    y_true, y_pred, _ = make_prediction(dataset=iris, binary=False)
+
     # print classification report with label detection
     expected_report = """\
-             precision    recall  f1-score   support
+              precision    recall  f1-score   support
 
-          0       0.83      0.79      0.81        24
-          1       0.33      0.10      0.15        31
-          2       0.42      0.90      0.57        20
+           0       0.83      0.79      0.81        24
+           1       0.33      0.10      0.15        31
+           2       0.42      0.90      0.57        20
 
-avg / total       0.51      0.53      0.47        75
+   micro avg       0.53      0.53      0.53        75
+   macro avg       0.53      0.60      0.51        75
+weighted avg       0.51      0.53      0.47        75
 """
     report = classification_report(y_true, y_pred)
     assert_equal(report, expected_report)
@@ -761,30 +808,20 @@ def test_classification_report_multiclass_with_digits():
 
     # print classification report with class names
     expected_report = """\
-             precision    recall  f1-score   support
+              precision    recall  f1-score   support
 
-     setosa    0.82609   0.79167   0.80851        24
- versicolor    0.33333   0.09677   0.15000        31
-  virginica    0.41860   0.90000   0.57143        20
+      setosa    0.82609   0.79167   0.80851        24
+  versicolor    0.33333   0.09677   0.15000        31
+   virginica    0.41860   0.90000   0.57143        20
 
-avg / total    0.51375   0.53333   0.47310        75
+   micro avg    0.53333   0.53333   0.53333        75
+   macro avg    0.52601   0.59615   0.50998        75
+weighted avg    0.51375   0.53333   0.47310        75
 """
     report = classification_report(
         y_true, y_pred, labels=np.arange(len(iris.target_names)),
         target_names=iris.target_names, digits=5)
     assert_equal(report, expected_report)
-    # print classification report with label detection
-    expected_report = """\
-             precision    recall  f1-score   support
-
-          0       0.83      0.79      0.81        24
-          1       0.33      0.10      0.15        31
-          2       0.42      0.90      0.57        20
-
-avg / total       0.51      0.53      0.47        75
-"""
-    report = classification_report(y_true, y_pred)
-    assert_equal(report, expected_report)
 
 
 def test_classification_report_multiclass_with_string_label():
@@ -794,25 +831,29 @@ def test_classification_report_multiclass_with_string_label():
     y_pred = np.array(["blue", "green", "red"])[y_pred]
 
     expected_report = """\
-             precision    recall  f1-score   support
+              precision    recall  f1-score   support
 
-       blue       0.83      0.79      0.81        24
-      green       0.33      0.10      0.15        31
-        red       0.42      0.90      0.57        20
+        blue       0.83      0.79      0.81        24
+       green       0.33      0.10      0.15        31
+         red       0.42      0.90      0.57        20
 
-avg / total       0.51      0.53      0.47        75
+   micro avg       0.53      0.53      0.53        75
+   macro avg       0.53      0.60      0.51        75
+weighted avg       0.51      0.53      0.47        75
 """
     report = classification_report(y_true, y_pred)
     assert_equal(report, expected_report)
 
     expected_report = """\
-             precision    recall  f1-score   support
+              precision    recall  f1-score   support
 
-          a       0.83      0.79      0.81        24
-          b       0.33      0.10      0.15        31
-          c       0.42      0.90      0.57        20
+           a       0.83      0.79      0.81        24
+           b       0.33      0.10      0.15        31
+           c       0.42      0.90      0.57        20
 
-avg / total       0.51      0.53      0.47        75
+   micro avg       0.53      0.53      0.53        75
+   macro avg       0.53      0.60      0.51        75
+weighted avg       0.51      0.53      0.47        75
 """
     report = classification_report(y_true, y_pred,
                                    target_names=["a", "b", "c"])
@@ -827,13 +868,15 @@ def test_classification_report_multiclass_with_unicode_label():
     y_pred = labels[y_pred]
 
     expected_report = u"""\
-             precision    recall  f1-score   support
+              precision    recall  f1-score   support
 
-      blue\xa2       0.83      0.79      0.81        24
-     green\xa2       0.33      0.10      0.15        31
-       red\xa2       0.42      0.90      0.57        20
+       blue\xa2       0.83      0.79      0.81        24
+      green\xa2       0.33      0.10      0.15        31
+        red\xa2       0.42      0.90      0.57        20
 
-avg / total       0.51      0.53      0.47        75
+   micro avg       0.53      0.53      0.53        75
+   macro avg       0.53      0.60      0.51        75
+weighted avg       0.51      0.53      0.47        75
 """
     report = classification_report(y_true, y_pred)
     assert_equal(report, expected_report)
@@ -842,7 +885,7 @@ def test_classification_report_multiclass_with_unicode_label():
 def test_classification_report_multiclass_with_long_string_label():
     y_true, y_pred, _ = make_prediction(binary=False)
 
-    labels = np.array(["blue", "green"*5, "red"])
+    labels = np.array(["blue", "green" * 5, "red"])
     y_true = labels[y_true]
     y_pred = labels[y_pred]
 
@@ -853,7 +896,9 @@ def test_classification_report_multiclass_with_long_string_label():
 greengreengreengreengreen       0.33      0.10      0.15        31
                       red       0.42      0.90      0.57        20
 
-              avg / total       0.51      0.53      0.47        75
+                micro avg       0.53      0.53      0.53        75
+                macro avg       0.53      0.60      0.51        75
+             weighted avg       0.51      0.53      0.47        75
 """
 
     report = classification_report(y_true, y_pred)
@@ -901,14 +946,17 @@ def test_multilabel_classification_report():
                                                random_state=1)
 
     expected_report = """\
-             precision    recall  f1-score   support
+              precision    recall  f1-score   support
 
-          0       0.50      0.67      0.57        24
-          1       0.51      0.74      0.61        27
-          2       0.29      0.08      0.12        26
-          3       0.52      0.56      0.54        27
+           0       0.50      0.67      0.57        24
+           1       0.51      0.74      0.61        27
+           2       0.29      0.08      0.12        26
+           3       0.52      0.56      0.54        27
 
-avg / total       0.45      0.51      0.46       104
+   micro avg       0.50      0.51      0.50       104
+   macro avg       0.45      0.51      0.46       104
+weighted avg       0.45      0.51      0.46       104
+ samples avg       0.46      0.42      0.40       104
 """
 
     report = classification_report(y_true, y_pred)
diff --git a/sklearn/metrics/tests/test_score_objects.py b/sklearn/metrics/tests/test_score_objects.py
index f418a9375d..da04b4215d 100755
--- a/sklearn/metrics/tests/test_score_objects.py
+++ b/sklearn/metrics/tests/test_score_objects.py
@@ -334,6 +334,8 @@ def test_regression_scorers():
     assert_almost_equal(score1, score2)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_thresholded_scorers():
     # Test scorers that take thresholds.
     X, y = make_blobs(random_state=0, centers=2)
@@ -504,6 +506,8 @@ def test_scorer_memmap_input(name):
     check_scorer_memmap(name)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_scoring_is_not_metric():
     assert_raises_regexp(ValueError, 'make_scorer', check_scoring,
                          LogisticRegression(), f1_score)
diff --git a/sklearn/model_selection/_search.py b/sklearn/model_selection/_search.py
index 2da6c3f6ec..b4cd9d068f 100755
--- a/sklearn/model_selection/_search.py
+++ b/sklearn/model_selection/_search.py
@@ -408,11 +408,12 @@ def __repr__(self):
 
 class BaseSearchCV(six.with_metaclass(ABCMeta, BaseEstimator,
                                       MetaEstimatorMixin)):
-    """Base class for hyper parameter search with cross-validation."""
+    """Abstract base class for hyper parameter search with cross-validation.
+    """
 
     @abstractmethod
     def __init__(self, estimator, scoring=None,
-                 fit_params=None, n_jobs=1, iid='warn',
+                 fit_params=None, n_jobs=None, iid='warn',
                  refit=True, cv='warn', verbose=0, pre_dispatch='2*n_jobs',
                  error_score='raise-deprecating', return_train_score=True):
 
@@ -466,7 +467,7 @@ def _check_is_fitted(self, method_name):
                                  'with refit=False. %s is '
                                  'available only after refitting on the best '
                                  'parameters. You can refit an estimator '
-                                 'manually using the ``best_parameters_`` '
+                                 'manually using the ``best_params_`` '
                                  'attribute'
                                  % (type(self).__name__, method_name))
         else:
@@ -579,6 +580,40 @@ def classes_(self):
         self._check_is_fitted("classes_")
         return self.best_estimator_.classes_
 
+    @abstractmethod
+    def _run_search(self, evaluate_candidates):
+        """Repeatedly calls `evaluate_candidates` to conduct a search.
+
+        This method, implemented in sub-classes, makes it is possible to
+        customize the the scheduling of evaluations: GridSearchCV and
+        RandomizedSearchCV schedule evaluations for their whole parameter
+        search space at once but other more sequential approaches are also
+        possible: for instance is possible to iteratively schedule evaluations
+        for new regions of the parameter search space based on previously
+        collected evaluation results. This makes it possible to implement
+        Bayesian optimization or more generally sequential model-based
+        optimization by deriving from the BaseSearchCV abstract base class.
+
+        Parameters
+        ----------
+        evaluate_candidates : callable
+            This callback accepts a list of candidates, where each candidate is
+            a dict of parameter settings. It returns a dict of all results so
+            far, formatted like ``cv_results_``.
+
+        Examples
+        --------
+
+        ::
+
+            def _run_search(self, evaluate_candidates):
+                'Try C=0.1 only if C=1 is better than C=10'
+                all_results = evaluate_candidates([{'C': 1}, {'C': 10}])
+                score = all_results['mean_test_score']
+                if score[0] < score[1]:
+                    evaluate_candidates([{'C': 0.1}])
+        """
+
     def fit(self, X, y=None, groups=None, **fit_params):
         """Run fit with all sets of parameters.
 
@@ -638,29 +673,86 @@ def fit(self, X, y=None, groups=None, **fit_params):
 
         X, y, groups = indexable(X, y, groups)
         n_splits = cv.get_n_splits(X, y, groups)
-        # Regenerate parameter iterable for each fit
-        candidate_params = list(self._get_param_iterator())
-        n_candidates = len(candidate_params)
-        if self.verbose > 0:
-            print("Fitting {0} folds for each of {1} candidates, totalling"
-                  " {2} fits".format(n_splits, n_candidates,
-                                     n_candidates * n_splits))
 
         base_estimator = clone(self.estimator)
-        pre_dispatch = self.pre_dispatch
-
-        out = Parallel(
-            n_jobs=self.n_jobs, verbose=self.verbose,
-            pre_dispatch=pre_dispatch
-        )(delayed(_fit_and_score)(clone(base_estimator), X, y, scorers, train,
-                                  test, self.verbose, parameters,
-                                  fit_params=fit_params,
-                                  return_train_score=self.return_train_score,
-                                  return_n_test_samples=True,
-                                  return_times=True, return_parameters=False,
-                                  error_score=self.error_score)
-          for parameters, (train, test) in product(candidate_params,
-                                                   cv.split(X, y, groups)))
+
+        parallel = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
+                            pre_dispatch=self.pre_dispatch)
+
+        fit_and_score_kwargs = dict(scorer=scorers,
+                                    fit_params=fit_params,
+                                    return_train_score=self.return_train_score,
+                                    return_n_test_samples=True,
+                                    return_times=True,
+                                    return_parameters=False,
+                                    error_score=self.error_score,
+                                    verbose=self.verbose)
+        results_container = [{}]
+        with parallel:
+            all_candidate_params = []
+            all_out = []
+
+            def evaluate_candidates(candidate_params):
+                candidate_params = list(candidate_params)
+                n_candidates = len(candidate_params)
+
+                if self.verbose > 0:
+                    print("Fitting {0} folds for each of {1} candidates,"
+                          " totalling {2} fits".format(
+                              n_splits, n_candidates, n_candidates * n_splits))
+
+                out = parallel(delayed(_fit_and_score)(clone(base_estimator),
+                                                       X, y,
+                                                       train=train, test=test,
+                                                       parameters=parameters,
+                                                       **fit_and_score_kwargs)
+                               for parameters, (train, test)
+                               in product(candidate_params,
+                                          cv.split(X, y, groups)))
+
+                all_candidate_params.extend(candidate_params)
+                all_out.extend(out)
+
+                # XXX: When we drop Python 2 support, we can use nonlocal
+                # instead of results_container
+                results_container[0] = self._format_results(
+                    all_candidate_params, scorers, n_splits, all_out)
+                return results_container[0]
+
+            self._run_search(evaluate_candidates)
+
+        results = results_container[0]
+
+        # For multi-metric evaluation, store the best_index_, best_params_ and
+        # best_score_ iff refit is one of the scorer names
+        # In single metric evaluation, refit_metric is "score"
+        if self.refit or not self.multimetric_:
+            self.best_index_ = results["rank_test_%s" % refit_metric].argmin()
+            self.best_params_ = results["params"][self.best_index_]
+            self.best_score_ = results["mean_test_%s" % refit_metric][
+                self.best_index_]
+
+        if self.refit:
+            self.best_estimator_ = clone(base_estimator).set_params(
+                **self.best_params_)
+            refit_start_time = time.time()
+            if y is not None:
+                self.best_estimator_.fit(X, y, **fit_params)
+            else:
+                self.best_estimator_.fit(X, **fit_params)
+            refit_end_time = time.time()
+            self.refit_time_ = refit_end_time - refit_start_time
+
+        # Store the only scorer not as a dict for single metric evaluation
+        self.scorer_ = scorers if self.multimetric_ else scorers['score']
+
+        self.cv_results_ = results
+        self.n_splits_ = n_splits
+
+        return self
+
+    def _format_results(self, candidate_params, scorers, n_splits, out):
+        n_candidates = len(candidate_params)
 
         # if one choose to see train score, "out" will contain train score info
         if self.return_train_score:
@@ -729,7 +821,19 @@ def _store(key_name, array, weights=None, splits=False, rank=False):
                                       dtype=np.int)
         iid = self.iid
         if self.iid == 'warn':
-            if len(np.unique(test_sample_counts)) > 1:
+            warn = False
+            for scorer_name in scorers.keys():
+                scores = test_scores[scorer_name].reshape(n_candidates,
+                                                          n_splits)
+                means_weighted = np.average(scores, axis=1,
+                                            weights=test_sample_counts)
+                means_unweighted = np.average(scores, axis=1)
+                if not np.allclose(means_weighted, means_unweighted,
+                                   rtol=1e-4, atol=1e-4):
+                    warn = True
+                    break
+
+            if warn:
                 warnings.warn("The default of the `iid` parameter will change "
                               "from True to False in version 0.22 and will be"
                               " removed in 0.24. This will change numeric"
@@ -746,7 +850,6 @@ def _store(key_name, array, weights=None, splits=False, rank=False):
                 prev_keys = set(results.keys())
                 _store('train_%s' % scorer_name, train_scores[scorer_name],
                        splits=True)
-
                 if self.return_train_score == 'warn':
                     for key in set(results.keys()) - prev_keys:
                         message = (
@@ -757,33 +860,7 @@ def _store(key_name, array, weights=None, splits=False, rank=False):
                         # warn on key access
                         results.add_warning(key, message, FutureWarning)
 
-        # For multi-metric evaluation, store the best_index_, best_params_ and
-        # best_score_ iff refit is one of the scorer names
-        # In single metric evaluation, refit_metric is "score"
-        if self.refit or not self.multimetric_:
-            self.best_index_ = results["rank_test_%s" % refit_metric].argmin()
-            self.best_params_ = candidate_params[self.best_index_]
-            self.best_score_ = results["mean_test_%s" % refit_metric][
-                self.best_index_]
-
-        if self.refit:
-            self.best_estimator_ = clone(base_estimator).set_params(
-                **self.best_params_)
-            refit_start_time = time.time()
-            if y is not None:
-                self.best_estimator_.fit(X, y, **fit_params)
-            else:
-                self.best_estimator_.fit(X, **fit_params)
-            refit_end_time = time.time()
-            self.refit_time_ = refit_end_time - refit_start_time
-
-        # Store the only scorer not as a dict for single metric evaluation
-        self.scorer_ = scorers if self.multimetric_ else scorers['score']
-
-        self.cv_results_ = results
-        self.n_splits_ = n_splits
-
-        return self
+        return results
 
 
 class GridSearchCV(BaseSearchCV):
@@ -838,8 +915,11 @@ class GridSearchCV(BaseSearchCV):
            0.19 and will be removed in version 0.21. Pass fit parameters to
            the ``fit`` method instead.
 
-    n_jobs : int, default=1
+    n_jobs : int or None, optional (default=None)
         Number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     pre_dispatch : int, or string, optional
         Controls the number of jobs that get dispatched during parallel
@@ -904,7 +984,7 @@ class GridSearchCV(BaseSearchCV):
         ``GridSearchCV`` instance.
 
         Also for multiple metric evaluation, the attributes ``best_index_``,
-        ``best_score_`` and ``best_parameters_`` will only be available if
+        ``best_score_`` and ``best_params_`` will only be available if
         ``refit`` is set and all of them will be determined w.r.t this specific
         scorer.
 
@@ -951,7 +1031,7 @@ class GridSearchCV(BaseSearchCV):
                          kernel='rbf', max_iter=-1, probability=False,
                          random_state=None, shrinking=True, tol=...,
                          verbose=False),
-           fit_params=None, iid=..., n_jobs=1,
+           fit_params=None, iid=..., n_jobs=None,
            param_grid=..., pre_dispatch=..., refit=..., return_train_score=...,
            scoring=..., verbose=...)
     >>> sorted(clf.cv_results_.keys())
@@ -1095,7 +1175,7 @@ class GridSearchCV(BaseSearchCV):
     """
 
     def __init__(self, estimator, param_grid, scoring=None, fit_params=None,
-                 n_jobs=1, iid='warn', refit=True, cv='warn', verbose=0,
+                 n_jobs=None, iid='warn', refit=True, cv='warn', verbose=0,
                  pre_dispatch='2*n_jobs', error_score='raise-deprecating',
                  return_train_score="warn"):
         super(GridSearchCV, self).__init__(
@@ -1106,9 +1186,9 @@ def __init__(self, estimator, param_grid, scoring=None, fit_params=None,
         self.param_grid = param_grid
         _check_param_grid(param_grid)
 
-    def _get_param_iterator(self):
-        """Return ParameterGrid instance for the given param_grid"""
-        return ParameterGrid(self.param_grid)
+    def _run_search(self, evaluate_candidates):
+        """Search all candidates in param_grid"""
+        evaluate_candidates(ParameterGrid(self.param_grid))
 
 
 class RandomizedSearchCV(BaseSearchCV):
@@ -1182,8 +1262,11 @@ class RandomizedSearchCV(BaseSearchCV):
            0.19 and will be removed in version 0.21. Pass fit parameters to
            the ``fit`` method instead.
 
-    n_jobs : int, default=1
+    n_jobs : int or None, optional (default=None)
         Number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     pre_dispatch : int, or string, optional
         Controls the number of jobs that get dispatched during parallel
@@ -1248,7 +1331,7 @@ class RandomizedSearchCV(BaseSearchCV):
         ``RandomizedSearchCV`` instance.
 
         Also for multiple metric evaluation, the attributes ``best_index_``,
-        ``best_score_`` and ``best_parameters_`` will only be available if
+        ``best_score_`` and ``best_params_`` will only be available if
         ``refit`` is set and all of them will be determined w.r.t this specific
         scorer.
 
@@ -1412,9 +1495,10 @@ class RandomizedSearchCV(BaseSearchCV):
     """
 
     def __init__(self, estimator, param_distributions, n_iter=10, scoring=None,
-                 fit_params=None, n_jobs=1, iid='warn', refit=True, cv='warn',
-                 verbose=0, pre_dispatch='2*n_jobs', random_state=None,
-                 error_score='raise-deprecating', return_train_score="warn"):
+                 fit_params=None, n_jobs=None, iid='warn', refit=True,
+                 cv='warn', verbose=0, pre_dispatch='2*n_jobs',
+                 random_state=None, error_score='raise-deprecating',
+                 return_train_score="warn"):
         self.param_distributions = param_distributions
         self.n_iter = n_iter
         self.random_state = random_state
@@ -1424,8 +1508,8 @@ def __init__(self, estimator, param_distributions, n_iter=10, scoring=None,
             pre_dispatch=pre_dispatch, error_score=error_score,
             return_train_score=return_train_score)
 
-    def _get_param_iterator(self):
-        """Return ParameterSampler instance for the given distributions"""
-        return ParameterSampler(
+    def _run_search(self, evaluate_candidates):
+        """Search n_iter candidates from param_distributions"""
+        evaluate_candidates(ParameterSampler(
             self.param_distributions, self.n_iter,
-            random_state=self.random_state)
+            random_state=self.random_state))
diff --git a/sklearn/model_selection/_split.py b/sklearn/model_selection/_split.py
index 50b2ce8711..75c8e5d239 100755
--- a/sklearn/model_selection/_split.py
+++ b/sklearn/model_selection/_split.py
@@ -714,7 +714,7 @@ class TimeSeriesSplit(_BaseKFold):
     Parameters
     ----------
     n_splits : int, default=3
-        Number of splits. Must be at least 1.
+        Number of splits. Must be at least 2.
 
         .. versionchanged:: 0.20
             ``n_splits`` default value will change from 3 to 5 in v0.22.
@@ -1939,7 +1939,7 @@ def check_cv(cv='warn', y=None, classifier=False):
         The return value is a cross-validator which generates the train/test
         splits via the ``split`` method.
     """
-    if cv is 'warn':
+    if cv is None or cv is 'warn':
         warnings.warn(CV_WARNING, FutureWarning)
         cv = 3
 
diff --git a/sklearn/model_selection/_validation.py b/sklearn/model_selection/_validation.py
index a513c660f5..4ddfc5edac 100755
--- a/sklearn/model_selection/_validation.py
+++ b/sklearn/model_selection/_validation.py
@@ -39,7 +39,7 @@
 
 
 def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv='warn',
-                   n_jobs=1, verbose=0, fit_params=None,
+                   n_jobs=None, verbose=0, fit_params=None,
                    pre_dispatch='2*n_jobs', return_train_score="warn",
                    return_estimator=False, error_score='raise-deprecating'):
     """Evaluate metric(s) by cross-validation and also record fit/score times.
@@ -97,9 +97,11 @@ def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv='warn',
             ``cv`` default value if None will change from 3-fold to 5-fold
             in v0.22.
 
-    n_jobs : integer, optional
-        The number of CPUs to use to do the computation. -1 means
-        'all CPUs'.
+    n_jobs : int or None, optional (default=None)
+        The number of CPUs to use to do the computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : integer, optional
         The verbosity level.
@@ -272,7 +274,7 @@ def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv='warn',
 
 
 def cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv='warn',
-                    n_jobs=1, verbose=0, fit_params=None,
+                    n_jobs=None, verbose=0, fit_params=None,
                     pre_dispatch='2*n_jobs', error_score='raise-deprecating'):
     """Evaluate a score by cross-validation
 
@@ -319,9 +321,11 @@ def cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv='warn',
             ``cv`` default value if None will change from 3-fold to 5-fold
             in v0.22.
 
-    n_jobs : integer, optional
-        The number of CPUs to use to do the computation. -1 means
-        'all CPUs'.
+    n_jobs : int or None, optional (default=None)
+        The number of CPUs to use to do the computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : integer, optional
         The verbosity level.
@@ -646,9 +650,9 @@ def _multimetric_score(estimator, X_test, y_test, scorers):
     return scores
 
 
-def cross_val_predict(estimator, X, y=None, groups=None, cv='warn', n_jobs=1,
-                      verbose=0, fit_params=None, pre_dispatch='2*n_jobs',
-                      method='predict'):
+def cross_val_predict(estimator, X, y=None, groups=None, cv='warn',
+                      n_jobs=None, verbose=0, fit_params=None,
+                      pre_dispatch='2*n_jobs', method='predict'):
     """Generate cross-validated estimates for each input data point
 
     It is not appropriate to pass these predictions into an evaluation
@@ -692,9 +696,11 @@ def cross_val_predict(estimator, X, y=None, groups=None, cv='warn', n_jobs=1,
             ``cv`` default value if None will change from 3-fold to 5-fold
             in v0.22.
 
-    n_jobs : integer, optional
-        The number of CPUs to use to do the computation. -1 means
-        'all CPUs'.
+    n_jobs : int or None, optional (default=None)
+        The number of CPUs to use to do the computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     verbose : integer, optional
         The verbosity level.
@@ -927,7 +933,7 @@ def _index_param_value(X, v, indices):
 
 
 def permutation_test_score(estimator, X, y, groups=None, cv='warn',
-                           n_permutations=100, n_jobs=1, random_state=0,
+                           n_permutations=100, n_jobs=None, random_state=0,
                            verbose=0, scoring=None):
     """Evaluate the significance of a cross-validated score with permutations
 
@@ -984,9 +990,11 @@ def permutation_test_score(estimator, X, y, groups=None, cv='warn',
     n_permutations : integer, optional
         Number of times to permute ``y``.
 
-    n_jobs : integer, optional
-        The number of CPUs to use to do the computation. -1 means
-        'all CPUs'.
+    n_jobs : int or None, optional (default=None)
+        The number of CPUs to use to do the computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     random_state : int, RandomState instance or None, optional (default=0)
         If int, random_state is the seed used by the random number generator;
@@ -1068,8 +1076,8 @@ def _shuffle(y, groups, random_state):
 
 def learning_curve(estimator, X, y, groups=None,
                    train_sizes=np.linspace(0.1, 1.0, 5), cv='warn',
-                   scoring=None, exploit_incremental_learning=False, n_jobs=1,
-                   pre_dispatch="all", verbose=0, shuffle=False,
+                   scoring=None, exploit_incremental_learning=False,
+                   n_jobs=None, pre_dispatch="all", verbose=0, shuffle=False,
                    random_state=None,  error_score='raise-deprecating'):
     """Learning curve.
 
@@ -1140,8 +1148,11 @@ def learning_curve(estimator, X, y, groups=None,
         If the estimator supports incremental learning, this will be
         used to speed up fitting for different training set sizes.
 
-    n_jobs : integer, optional
-        Number of jobs to run in parallel (default 1).
+    n_jobs : int or None, optional (default=None)
+        Number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     pre_dispatch : integer or string, optional
         Number of predispatched jobs for parallel execution (default is
@@ -1323,7 +1334,7 @@ def _incremental_fit_estimator(estimator, X, y, classes, train, test,
 
 
 def validation_curve(estimator, X, y, param_name, param_range, groups=None,
-                     cv='warn', scoring=None, n_jobs=1, pre_dispatch="all",
+                     cv='warn', scoring=None, n_jobs=None, pre_dispatch="all",
                      verbose=0, error_score='raise-deprecating'):
     """Validation curve.
 
@@ -1384,8 +1395,11 @@ def validation_curve(estimator, X, y, param_name, param_range, groups=None,
         a scorer callable object / function with signature
         ``scorer(estimator, X, y)``.
 
-    n_jobs : integer, optional
-        Number of jobs to run in parallel (default 1).
+    n_jobs : int or None, optional (default=None)
+        Number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     pre_dispatch : integer or string, optional
         Number of predispatched jobs for parallel execution (default is
diff --git a/sklearn/model_selection/tests/test_search.py b/sklearn/model_selection/tests/test_search.py
index 5652164b4f..0409794bf0 100755
--- a/sklearn/model_selection/tests/test_search.py
+++ b/sklearn/model_selection/tests/test_search.py
@@ -26,6 +26,7 @@
 from sklearn.utils.testing import assert_false, assert_true
 from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_array_almost_equal
+from sklearn.utils.testing import assert_allclose
 from sklearn.utils.testing import assert_almost_equal
 from sklearn.utils.testing import assert_greater_equal
 from sklearn.utils.testing import ignore_warnings
@@ -54,6 +55,7 @@
 from sklearn.model_selection import RandomizedSearchCV
 from sklearn.model_selection import ParameterGrid
 from sklearn.model_selection import ParameterSampler
+from sklearn.model_selection._search import BaseSearchCV
 
 from sklearn.model_selection._validation import FitFailedWarning
 
@@ -936,8 +938,8 @@ def test_random_search_cv_results():
         check_cv_results_array_types(search, param_keys, score_keys)
         check_cv_results_keys(cv_results, param_keys, score_keys, n_cand)
         # For random_search, all the param array vals should be unmasked
-        assert_false(any(cv_results['param_C'].mask) or
-                     any(cv_results['param_gamma'].mask))
+        assert_false(any(np.ma.getmaskarray(cv_results['param_C'])) or
+                     any(np.ma.getmaskarray(cv_results['param_gamma'])))
 
 
 @ignore_warnings(category=DeprecationWarning)
@@ -1627,6 +1629,55 @@ def test_transform_inverse_transform_round_trip():
     assert_array_equal(X, X_round_trip)
 
 
+def test_custom_run_search():
+    def check_results(results, gscv):
+        exp_results = gscv.cv_results_
+        assert sorted(results.keys()) == sorted(exp_results)
+        for k in results:
+            if not k.endswith('_time'):
+                # XXX: results['params'] is a list :|
+                results[k] = np.asanyarray(results[k])
+                if results[k].dtype.kind == 'O':
+                    assert_array_equal(exp_results[k], results[k],
+                                       err_msg='Checking ' + k)
+                else:
+                    assert_allclose(exp_results[k], results[k],
+                                    err_msg='Checking ' + k)
+
+    def fit_grid(param_grid):
+        return GridSearchCV(clf, param_grid, cv=5,
+                            return_train_score=True).fit(X, y)
+
+    class CustomSearchCV(BaseSearchCV):
+        def __init__(self, estimator, **kwargs):
+            super(CustomSearchCV, self).__init__(estimator, **kwargs)
+
+        def _run_search(self, evaluate):
+            results = evaluate([{'max_depth': 1}, {'max_depth': 2}])
+            check_results(results, fit_grid({'max_depth': [1, 2]}))
+            results = evaluate([{'min_samples_split': 5},
+                                {'min_samples_split': 10}])
+            check_results(results, fit_grid([{'max_depth': [1, 2]},
+                                             {'min_samples_split': [5, 10]}]))
+
+    # Using regressor to make sure each score differs
+    clf = DecisionTreeRegressor(random_state=0)
+    X, y = make_classification(n_samples=100, n_informative=4,
+                               random_state=0)
+    mycv = CustomSearchCV(clf, cv=5, return_train_score=True).fit(X, y)
+    gscv = fit_grid([{'max_depth': [1, 2]},
+                     {'min_samples_split': [5, 10]}])
+
+    results = mycv.cv_results_
+    check_results(results, gscv)
+    for attr in dir(gscv):
+        if attr[0].islower() and attr[-1:] == '_' and \
+           attr not in {'cv_results_', 'best_estimator_',
+                        'refit_time_'}:
+            assert getattr(gscv, attr) == getattr(mycv, attr), \
+                   "Attribute %s not equal" % attr
+
+
 def test_deprecated_grid_search_iid():
     depr_message = ("The default of the `iid` parameter will change from True "
                     "to False in version 0.22")
diff --git a/sklearn/model_selection/tests/test_split.py b/sklearn/model_selection/tests/test_split.py
index 9ba26c0fdf..710c194cfc 100755
--- a/sklearn/model_selection/tests/test_split.py
+++ b/sklearn/model_selection/tests/test_split.py
@@ -1444,7 +1444,7 @@ def test_nsplit_default_warn():
 def test_check_cv_default_warn():
     # Test that warnings are raised. Will be removed in 0.22
     assert_warns_message(FutureWarning, CV_WARNING, check_cv)
-
+    assert_warns_message(FutureWarning, CV_WARNING, check_cv, None)
     assert_no_warnings(check_cv, cv=5)
 
 
diff --git a/sklearn/model_selection/tests/test_validation.py b/sklearn/model_selection/tests/test_validation.py
index bb7e736eb3..0d7a05f39d 100755
--- a/sklearn/model_selection/tests/test_validation.py
+++ b/sklearn/model_selection/tests/test_validation.py
@@ -790,6 +790,8 @@ def test_cross_val_score_multilabel():
     assert_almost_equal(score_samples, [1, 1 / 2, 3 / 4, 1 / 2, 1 / 4])
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict():
     boston = load_boston()
@@ -840,6 +842,8 @@ def split(self, X, y=None, groups=None):
                          X, y, method='predict_proba', cv=KFold(2))
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict_decision_function_shape():
     X, y = make_classification(n_classes=2, n_samples=50, random_state=0)
@@ -887,6 +891,8 @@ def test_cross_val_predict_decision_function_shape():
                         cv=KFold(n_splits=3), method='decision_function')
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict_predict_proba_shape():
     X, y = make_classification(n_classes=2, n_samples=50, random_state=0)
@@ -902,6 +908,8 @@ def test_cross_val_predict_predict_proba_shape():
     assert_equal(preds.shape, (150, 3))
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict_predict_log_proba_shape():
     X, y = make_classification(n_classes=2, n_samples=50, random_state=0)
@@ -917,6 +925,8 @@ def test_cross_val_predict_predict_log_proba_shape():
     assert_equal(preds.shape, (150, 3))
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict_input_types():
     iris = load_iris()
@@ -1336,6 +1346,8 @@ def check_cross_val_predict_with_method(est):
         assert_array_equal(predictions, predictions_ystr)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict_with_method():
     check_cross_val_predict_with_method(LogisticRegression())
@@ -1350,6 +1362,8 @@ def test_cross_val_predict_method_checking():
     check_cross_val_predict_with_method(est)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore: The default of the `iid`')
 @pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_gridsearchcv_cross_val_predict_with_method():
@@ -1379,11 +1393,13 @@ def get_expected_predictions(X, y, cv, classes, est, method):
     return expected_predictions
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
 def test_cross_val_predict_class_subset():
 
     X = np.arange(200).reshape(100, 2)
-    y = np.array([x//10 for x in range(100)])
+    y = np.array([x // 10 for x in range(100)])
     classes = 10
 
     kfold3 = KFold(n_splits=3)
diff --git a/sklearn/multiclass.py b/sklearn/multiclass.py
index 7c84fcf9c6..0361dfa4d4 100755
--- a/sklearn/multiclass.py
+++ b/sklearn/multiclass.py
@@ -157,11 +157,11 @@ class OneVsRestClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):
         An estimator object implementing `fit` and one of `decision_function`
         or `predict_proba`.
 
-    n_jobs : int, optional, default: 1
-        The number of jobs to use for the computation. If -1 all CPUs are used.
-        If 1 is given, no parallel computing code is used at all, which is
-        useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are
-        used. Thus for n_jobs = -2, all CPUs but one are used.
+    n_jobs : int or None, optional (default=None)
+        The number of jobs to use for the computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -176,7 +176,7 @@ class OneVsRestClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):
     multilabel_ : boolean
         Whether a OneVsRestClassifier is a multilabel classifier.
     """
-    def __init__(self, estimator, n_jobs=1):
+    def __init__(self, estimator, n_jobs=None):
         self.estimator = estimator
         self.n_jobs = n_jobs
 
@@ -456,11 +456,11 @@ class OneVsOneClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):
         An estimator object implementing `fit` and one of `decision_function`
         or `predict_proba`.
 
-    n_jobs : int, optional, default: 1
-        The number of jobs to use for the computation. If -1 all CPUs are used.
-        If 1 is given, no parallel computing code is used at all, which is
-        useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are
-        used. Thus for n_jobs = -2, all CPUs but one are used.
+    n_jobs : int or None, optional (default=None)
+        The number of jobs to use for the computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -471,7 +471,7 @@ class OneVsOneClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):
         Array containing labels.
     """
 
-    def __init__(self, estimator, n_jobs=1):
+    def __init__(self, estimator, n_jobs=None):
         self.estimator = estimator
         self.n_jobs = n_jobs
 
@@ -662,11 +662,11 @@ class OutputCodeClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):
         random_state is the random number generator; If None, the random number
         generator is the RandomState instance used by `np.random`.
 
-    n_jobs : int, optional, default: 1
-        The number of jobs to use for the computation. If -1 all CPUs are used.
-        If 1 is given, no parallel computing code is used at all, which is
-        useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are
-        used. Thus for n_jobs = -2, all CPUs but one are used.
+    n_jobs : int or None, optional (default=None)
+        The number of jobs to use for the computation.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -698,7 +698,8 @@ class OutputCodeClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):
        2008.
     """
 
-    def __init__(self, estimator, code_size=1.5, random_state=None, n_jobs=1):
+    def __init__(self, estimator, code_size=1.5, random_state=None,
+                 n_jobs=None):
         self.estimator = estimator
         self.code_size = code_size
         self.random_state = random_state
diff --git a/sklearn/multioutput.py b/sklearn/multioutput.py
index c2e40d044b..1b0fdd19e1 100755
--- a/sklearn/multioutput.py
+++ b/sklearn/multioutput.py
@@ -63,7 +63,7 @@ def _partial_fit_estimator(estimator, X, y, classes=None, sample_weight=None,
 class MultiOutputEstimator(six.with_metaclass(ABCMeta, BaseEstimator,
                                               MetaEstimatorMixin)):
     @abstractmethod
-    def __init__(self, estimator, n_jobs=1):
+    def __init__(self, estimator, n_jobs=None):
         self.estimator = estimator
         self.n_jobs = n_jobs
 
@@ -209,15 +209,18 @@ class MultiOutputRegressor(MultiOutputEstimator, RegressorMixin):
     estimator : estimator object
         An estimator object implementing `fit` and `predict`.
 
-    n_jobs : int, optional, default=1
-        The number of jobs to run in parallel for `fit`. If -1,
-        then the number of jobs is set to the number of cores.
+    n_jobs : int or None, optional (default=None)
+        The number of jobs to run in parallel for `fit`.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
+
         When individual estimators are fast to train or predict
         using `n_jobs>1` can result in slower performance due
         to the overhead of spawning processes.
     """
 
-    def __init__(self, estimator, n_jobs=1):
+    def __init__(self, estimator, n_jobs=None):
         super(MultiOutputRegressor, self).__init__(estimator, n_jobs)
 
     @if_delegate_has_method('estimator')
@@ -295,13 +298,12 @@ class MultiOutputClassifier(MultiOutputEstimator, ClassifierMixin):
     estimator : estimator object
         An estimator object implementing `fit`, `score` and `predict_proba`.
 
-    n_jobs : int, optional, default=1
-        The number of jobs to use for the computation. If -1 all CPUs are used.
-        If 1 is given, no parallel computing code is used at all, which is
-        useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are
-        used. Thus for n_jobs = -2, all CPUs but one are used.
+    n_jobs : int or None, optional (default=None)
         The number of jobs to use for the computation.
         It does each target variable in y in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -309,7 +311,7 @@ class MultiOutputClassifier(MultiOutputEstimator, ClassifierMixin):
         Estimators used for predictions.
     """
 
-    def __init__(self, estimator, n_jobs=1):
+    def __init__(self, estimator, n_jobs=None):
         super(MultiOutputClassifier, self).__init__(estimator, n_jobs)
 
     def predict_proba(self, X):
diff --git a/sklearn/neighbors/base.py b/sklearn/neighbors/base.py
index f1bf43f059..fcb221b037 100755
--- a/sklearn/neighbors/base.py
+++ b/sklearn/neighbors/base.py
@@ -20,13 +20,12 @@
 from ..base import BaseEstimator
 from ..metrics import pairwise_distances_chunked
 from ..metrics.pairwise import PAIRWISE_DISTANCE_FUNCTIONS
-from ..utils import check_X_y, check_array, _get_n_jobs, gen_even_slices
+from ..utils import check_X_y, check_array, gen_even_slices
 from ..utils.multiclass import check_classification_targets
 from ..utils.validation import check_is_fitted
 from ..externals import six
-from ..utils import Parallel, delayed
+from ..utils import Parallel, delayed, effective_n_jobs
 from ..utils._joblib import __version__ as joblib_version
-from ..exceptions import NotFittedError
 from ..exceptions import DataConversionWarning
 
 VALID_METRICS = dict(ball_tree=BallTree.valid_metrics,
@@ -109,7 +108,7 @@ class NeighborsBase(six.with_metaclass(ABCMeta, BaseEstimator)):
     @abstractmethod
     def __init__(self, n_neighbors=None, radius=None,
                  algorithm='auto', leaf_size=30, metric='minkowski',
-                 p=2, metric_params=None, n_jobs=1):
+                 p=2, metric_params=None, n_jobs=None):
 
         self.n_neighbors = n_neighbors
         self.radius = radius
@@ -403,7 +402,7 @@ class from an array representing our data set and ask who's
         n_samples, _ = X.shape
         sample_range = np.arange(n_samples)[:, None]
 
-        n_jobs = _get_n_jobs(self.n_jobs)
+        n_jobs = effective_n_jobs(self.n_jobs)
         if self._fit_method == 'brute':
 
             reduce_func = partial(self._kneighbors_reduce_func,
@@ -428,9 +427,11 @@ class from an array representing our data set and ask who's
                 # Deal with change of API in joblib
                 delayed_query = delayed(self._tree.query,
                                         check_pickle=False)
+                parallel_kwargs = {"backend": "threading"}
             else:
                 delayed_query = delayed(self._tree.query)
-            result = Parallel(n_jobs, backend='threading')(
+                parallel_kwargs = {"prefer": "threads"}
+            result = Parallel(n_jobs, **parallel_kwargs)(
                 delayed_query(
                     X[s], n_neighbors, return_distance)
                 for s in gen_even_slices(X.shape[0], n_jobs)
@@ -705,15 +706,16 @@ class from an array representing our data set and ask who's
                     "%s does not work with sparse matrices. Densify the data, "
                     "or set algorithm='brute'" % self._fit_method)
 
-            n_jobs = _get_n_jobs(self.n_jobs)
-
+            n_jobs = effective_n_jobs(self.n_jobs)
             if LooseVersion(joblib_version) < LooseVersion('0.12'):
                 # Deal with change of API in joblib
                 delayed_query = delayed(self._tree.query_radius,
                                         check_pickle=False)
+                parallel_kwargs = {"backend": "threading"}
             else:
                 delayed_query = delayed(self._tree.query_radius)
-            results = Parallel(n_jobs, backend='threading')(
+                parallel_kwargs = {"prefer": "threads"}
+            results = Parallel(n_jobs, **parallel_kwargs)(
                 delayed_query(X[s], radius, return_distance)
                 for s in gen_even_slices(X.shape[0], n_jobs)
             )
diff --git a/sklearn/neighbors/binary_tree.pxi b/sklearn/neighbors/binary_tree.pxi
index ede29c2bb4..abb03edb04 100755
--- a/sklearn/neighbors/binary_tree.pxi
+++ b/sklearn/neighbors/binary_tree.pxi
@@ -292,7 +292,7 @@ Additional keywords are passed to the distance metric class.
 
 Attributes
 ----------
-data : np.ndarray
+data : memory view
     The training data
 
 Examples
@@ -486,7 +486,7 @@ cdef DTYPE_t _log_kernel_norm(DTYPE_t h, ITYPE_t d,
     elif kernel == EXPONENTIAL_KERNEL:
         factor = logSn(d - 1) + lgamma(d)
     elif kernel == LINEAR_KERNEL:
-        factor = logVn(d) - np.log1p(d)
+        factor = logVn(d) - log(d + 1.)
     elif kernel == COSINE_KERNEL:
         # this is derived from a chain rule integration
         factor = 0
@@ -1119,7 +1119,7 @@ cdef class BinaryTree:
         """
         reduce method used for pickling
         """
-        return (newObj, (BinaryTree,), self.__getstate__())
+        return (newObj, (type(self),), self.__getstate__())
 
     def __getstate__(self):
         """
@@ -1136,7 +1136,8 @@ cdef class BinaryTree:
                 int(self.n_leaves),
                 int(self.n_splits),
                 int(self.n_calls),
-                self.dist_metric)
+                self.dist_metric,
+                self.sample_weight)
 
     def __setstate__(self, state):
         """
@@ -1162,6 +1163,7 @@ cdef class BinaryTree:
         self.dist_metric = state[11]
         self.euclidean = (self.dist_metric.__class__.__name__
                           == 'EuclideanDistance')
+        self.sample_weight = state[12]
 
     def get_tree_stats(self):
         return (self.n_trims, self.n_leaves, self.n_splits)
diff --git a/sklearn/neighbors/classification.py b/sklearn/neighbors/classification.py
index 83ee27cccd..e84f9751d0 100755
--- a/sklearn/neighbors/classification.py
+++ b/sklearn/neighbors/classification.py
@@ -75,9 +75,11 @@ class KNeighborsClassifier(NeighborsBase, KNeighborsMixin,
     metric_params : dict, optional (default = None)
         Additional keyword arguments for the metric function.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
         Doesn't affect :meth:`fit` method.
 
     Examples
@@ -117,7 +119,7 @@ class KNeighborsClassifier(NeighborsBase, KNeighborsMixin,
 
     def __init__(self, n_neighbors=5,
                  weights='uniform', algorithm='auto', leaf_size=30,
-                 p=2, metric='minkowski', metric_params=None, n_jobs=1,
+                 p=2, metric='minkowski', metric_params=None, n_jobs=None,
                  **kwargs):
 
         super(KNeighborsClassifier, self).__init__(
@@ -288,9 +290,11 @@ class RadiusNeighborsClassifier(NeighborsBase, RadiusNeighborsMixin,
     metric_params : dict, optional (default = None)
         Additional keyword arguments for the metric function.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Examples
     --------
@@ -320,7 +324,8 @@ class RadiusNeighborsClassifier(NeighborsBase, RadiusNeighborsMixin,
 
     def __init__(self, radius=1.0, weights='uniform',
                  algorithm='auto', leaf_size=30, p=2, metric='minkowski',
-                 outlier_label=None, metric_params=None, n_jobs=1, **kwargs):
+                 outlier_label=None, metric_params=None, n_jobs=None,
+                 **kwargs):
         super(RadiusNeighborsClassifier, self).__init__(
               radius=radius,
               algorithm=algorithm,
@@ -378,10 +383,10 @@ def predict(self, X):
                 mode = np.array([stats.mode(pl)[0]
                                  for pl in pred_labels[inliers]], dtype=np.int)
             else:
-                mode = np.array([weighted_mode(pl, w)[0]
-                                 for (pl, w)
-                                 in zip(pred_labels[inliers], weights[inliers])],
-                                dtype=np.int)
+                mode = np.array(
+                    [weighted_mode(pl, w)[0]
+                     for (pl, w) in zip(pred_labels[inliers], weights[inliers])
+                     ], dtype=np.int)
 
             mode = mode.ravel()
 
diff --git a/sklearn/neighbors/graph.py b/sklearn/neighbors/graph.py
index b794e2059a..3999ff458e 100755
--- a/sklearn/neighbors/graph.py
+++ b/sklearn/neighbors/graph.py
@@ -32,7 +32,7 @@ def _query_include_self(X, include_self):
 
 
 def kneighbors_graph(X, n_neighbors, mode='connectivity', metric='minkowski',
-                     p=2, metric_params=None, include_self=False, n_jobs=1):
+                     p=2, metric_params=None, include_self=False, n_jobs=None):
     """Computes the (weighted) graph of k-Neighbors for points in X
 
     Read more in the :ref:`User Guide <unsupervised_neighbors>`.
@@ -70,9 +70,11 @@ def kneighbors_graph(X, n_neighbors, mode='connectivity', metric='minkowski',
         itself. If `None`, then True is used for mode='connectivity' and False
         for mode='distance' as this will preserve backwards compatibility.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Returns
     -------
@@ -104,7 +106,8 @@ def kneighbors_graph(X, n_neighbors, mode='connectivity', metric='minkowski',
 
 
 def radius_neighbors_graph(X, radius, mode='connectivity', metric='minkowski',
-                           p=2, metric_params=None, include_self=False, n_jobs=1):
+                           p=2, metric_params=None, include_self=False,
+                           n_jobs=None):
     """Computes the (weighted) graph of Neighbors for points in X
 
     Neighborhoods are restricted the points at a distance lower than
@@ -145,9 +148,11 @@ def radius_neighbors_graph(X, radius, mode='connectivity', metric='minkowski',
         itself. If `None`, then True is used for mode='connectivity' and False
         for mode='distance' as this will preserve backwards compatibility.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Returns
     -------
@@ -158,7 +163,8 @@ def radius_neighbors_graph(X, radius, mode='connectivity', metric='minkowski',
     --------
     >>> X = [[0], [3], [1]]
     >>> from sklearn.neighbors import radius_neighbors_graph
-    >>> A = radius_neighbors_graph(X, 1.5, mode='connectivity', include_self=True)
+    >>> A = radius_neighbors_graph(X, 1.5, mode='connectivity',
+    ...                            include_self=True)
     >>> A.toarray()
     array([[1., 0., 1.],
            [0., 1., 0.],
diff --git a/sklearn/neighbors/lof.py b/sklearn/neighbors/lof.py
index 3d1b40ec61..68fe777b3c 100755
--- a/sklearn/neighbors/lof.py
+++ b/sklearn/neighbors/lof.py
@@ -4,7 +4,6 @@
 
 import numpy as np
 import warnings
-from scipy.stats import scoreatpercentile
 
 from .base import NeighborsBase
 from .base import KNeighborsMixin
@@ -111,9 +110,11 @@ class LocalOutlierFactor(NeighborsBase, KNeighborsMixin, UnsupervisedMixin,
         that you should only use predict, decision_function and score_samples
         on new unseen data and not on the training set.
 
-    n_jobs : int, optional (default=1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
         Affects only :meth:`kneighbors` and :meth:`kneighbors_graph` methods.
 
 
@@ -121,7 +122,7 @@ class LocalOutlierFactor(NeighborsBase, KNeighborsMixin, UnsupervisedMixin,
     ----------
     negative_outlier_factor_ : numpy array, shape (n_samples,)
         The opposite LOF of the training samples. The higher, the more normal.
-        Inliers tend to have a LOF score close to 1 (`negative_outlier_factor_`
+        Inliers tend to have a LOF score close to 1 (``negative_outlier_factor_``
         close to -1), while outliers tend to have a larger LOF score.
 
         The local outlier factor (LOF) of a sample captures its
@@ -148,7 +149,7 @@ class LocalOutlierFactor(NeighborsBase, KNeighborsMixin, UnsupervisedMixin,
     """
     def __init__(self, n_neighbors=20, algorithm='auto', leaf_size=30,
                  metric='minkowski', p=2, metric_params=None,
-                 contamination="legacy", novelty=False, n_jobs=1):
+                 contamination="legacy", novelty=False, n_jobs=None):
         super(LocalOutlierFactor, self).__init__(
             n_neighbors=n_neighbors,
             algorithm=algorithm,
@@ -262,8 +263,8 @@ def fit(self, X, y=None):
             # inliers score around -1 (the higher, the less abnormal).
             self.offset_ = -1.5
         else:
-            self.offset_ = scoreatpercentile(
-                self.negative_outlier_factor_, 100. * self._contamination)
+            self.offset_ = np.percentile(self.negative_outlier_factor_,
+                                         100. * self._contamination)
 
         return self
 
@@ -403,7 +404,7 @@ def score_samples(self):
         Also, the samples in X are not considered in the neighborhood of any
         point.
         The score_samples on training data is available by considering the
-        the negative_outlier_factor_ attribute.
+        the ``negative_outlier_factor_`` attribute.
 
         Parameters
         ----------
@@ -439,7 +440,7 @@ def _score_samples(self, X):
         Also, the samples in X are not considered in the neighborhood of any
         point.
         The score_samples on training data is available by considering the
-        the negative_outlier_factor_ attribute.
+        the ``negative_outlier_factor_`` attribute.
 
         Parameters
         ----------
diff --git a/sklearn/neighbors/regression.py b/sklearn/neighbors/regression.py
index d9f5682d49..be4f835ff2 100755
--- a/sklearn/neighbors/regression.py
+++ b/sklearn/neighbors/regression.py
@@ -82,9 +82,11 @@ class KNeighborsRegressor(NeighborsBase, KNeighborsMixin,
     metric_params : dict, optional (default = None)
         Additional keyword arguments for the metric function.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
         Doesn't affect :meth:`fit` method.
 
     Examples
@@ -122,7 +124,7 @@ class KNeighborsRegressor(NeighborsBase, KNeighborsMixin,
 
     def __init__(self, n_neighbors=5, weights='uniform',
                  algorithm='auto', leaf_size=30,
-                 p=2, metric='minkowski', metric_params=None, n_jobs=1,
+                 p=2, metric='minkowski', metric_params=None, n_jobs=None,
                  **kwargs):
         super(KNeighborsRegressor, self).__init__(
               n_neighbors=n_neighbors,
@@ -238,9 +240,11 @@ class RadiusNeighborsRegressor(NeighborsBase, RadiusNeighborsMixin,
     metric_params : dict, optional (default = None)
         Additional keyword arguments for the metric function.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Examples
     --------
@@ -270,7 +274,7 @@ class RadiusNeighborsRegressor(NeighborsBase, RadiusNeighborsMixin,
 
     def __init__(self, radius=1.0, weights='uniform',
                  algorithm='auto', leaf_size=30,
-                 p=2, metric='minkowski', metric_params=None, n_jobs=1,
+                 p=2, metric='minkowski', metric_params=None, n_jobs=None,
                  **kwargs):
         super(RadiusNeighborsRegressor, self).__init__(
               radius=radius,
diff --git a/sklearn/neighbors/tests/test_ball_tree.py b/sklearn/neighbors/tests/test_ball_tree.py
index de0d166fb8..52ed63ae41 100755
--- a/sklearn/neighbors/tests/test_ball_tree.py
+++ b/sklearn/neighbors/tests/test_ball_tree.py
@@ -228,6 +228,8 @@ def check_pickle_protocol(protocol):
         assert_array_almost_equal(ind1_pyfunc, ind2_pyfunc)
         assert_array_almost_equal(dist1_pyfunc, dist2_pyfunc)
 
+        assert isinstance(bt2, BallTree)
+
     for protocol in (0, 1, 2):
         check_pickle_protocol(protocol)
 
diff --git a/sklearn/neighbors/tests/test_kd_tree.py b/sklearn/neighbors/tests/test_kd_tree.py
index 46cddc711e..18d2138021 100755
--- a/sklearn/neighbors/tests/test_kd_tree.py
+++ b/sklearn/neighbors/tests/test_kd_tree.py
@@ -187,6 +187,7 @@ def check_pickle_protocol(protocol):
         ind2, dist2 = kdt2.query(X)
         assert_array_almost_equal(ind1, ind2)
         assert_array_almost_equal(dist1, dist2)
+        assert isinstance(kdt2, KDTree)
 
     check_pickle_protocol(protocol)
 
diff --git a/sklearn/neighbors/tests/test_kde.py b/sklearn/neighbors/tests/test_kde.py
index 022cbce136..990942c9ef 100755
--- a/sklearn/neighbors/tests/test_kde.py
+++ b/sklearn/neighbors/tests/test_kde.py
@@ -10,6 +10,7 @@
 from sklearn.datasets import make_blobs
 from sklearn.model_selection import GridSearchCV
 from sklearn.preprocessing import StandardScaler
+from sklearn.externals import joblib
 
 
 def compute_kernel_slow(Y, X, kernel, h):
@@ -202,3 +203,23 @@ def test_kde_sample_weights():
                     kde.fit(X, sample_weight=(scale_factor * weights))
                     scores_scaled_weight = kde.score_samples(test_points)
                     assert_allclose(scores_scaled_weight, scores_weight)
+
+
+def test_pickling(tmpdir):
+    # Make sure that predictions are the same before and after pickling. Used
+    # to be a bug because sample_weights wasn't pickled and the resulting tree
+    # would miss some info.
+
+    kde = KernelDensity()
+    data = np.reshape([1., 2., 3.], (-1, 1))
+    kde.fit(data)
+
+    X = np.reshape([1.1, 2.1], (-1, 1))
+    scores = kde.score_samples(X)
+
+    file_path = str(tmpdir.join('dump.pkl'))
+    joblib.dump(kde, file_path)
+    kde = joblib.load(file_path)
+    scores_pickled = kde.score_samples(X)
+
+    assert_allclose(scores, scores_pickled)
diff --git a/sklearn/neighbors/unsupervised.py b/sklearn/neighbors/unsupervised.py
index db19e8df6b..9d41b640f9 100755
--- a/sklearn/neighbors/unsupervised.py
+++ b/sklearn/neighbors/unsupervised.py
@@ -74,9 +74,11 @@ class NearestNeighbors(NeighborsBase, KNeighborsMixin,
     metric_params : dict, optional (default = None)
         Additional keyword arguments for the metric function.
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run for neighbors search.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Examples
     --------
@@ -114,7 +116,7 @@ class NearestNeighbors(NeighborsBase, KNeighborsMixin,
 
     def __init__(self, n_neighbors=5, radius=1.0,
                  algorithm='auto', leaf_size=30, metric='minkowski',
-                 p=2, metric_params=None, n_jobs=1, **kwargs):
+                 p=2, metric_params=None, n_jobs=None, **kwargs):
         super(NearestNeighbors, self).__init__(
               n_neighbors=n_neighbors,
               radius=radius,
diff --git a/sklearn/pipeline.py b/sklearn/pipeline.py
index 53b33fb55e..87d7180e0c 100755
--- a/sklearn/pipeline.py
+++ b/sklearn/pipeline.py
@@ -640,8 +640,11 @@ class FeatureUnion(_BaseComposition, TransformerMixin):
         List of transformer objects to be applied to the data. The first
         half of each tuple is the name of the transformer.
 
-    n_jobs : int, optional
-        Number of jobs to run in parallel (default 1).
+    n_jobs : int or None, optional (default=None)
+        Number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     transformer_weights : dict, optional
         Multiplicative weights for features per transformer.
@@ -663,7 +666,8 @@ class FeatureUnion(_BaseComposition, TransformerMixin):
     array([[ 1.5       ,  3.0...,  0.8...],
            [-1.5       ,  5.7..., -0.4...]])
     """
-    def __init__(self, transformer_list, n_jobs=1, transformer_weights=None):
+    def __init__(self, transformer_list, n_jobs=None,
+                 transformer_weights=None):
         self.transformer_list = transformer_list
         self.n_jobs = n_jobs
         self.transformer_weights = transformer_weights
@@ -843,8 +847,11 @@ def make_union(*transformers, **kwargs):
     ----------
     *transformers : list of estimators
 
-    n_jobs : int, optional
-        Number of jobs to run in parallel (default 1).
+    n_jobs : int or None, optional (default=None)
+        Number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Returns
     -------
@@ -860,7 +867,7 @@ def make_union(*transformers, **kwargs):
     >>> from sklearn.decomposition import PCA, TruncatedSVD
     >>> from sklearn.pipeline import make_union
     >>> make_union(PCA(), TruncatedSVD())    # doctest: +NORMALIZE_WHITESPACE
-    FeatureUnion(n_jobs=1,
+    FeatureUnion(n_jobs=None,
            transformer_list=[('pca',
                               PCA(copy=True, iterated_power='auto',
                                   n_components=None, random_state=None,
@@ -871,7 +878,7 @@ def make_union(*transformers, **kwargs):
                               random_state=None, tol=0.0))],
            transformer_weights=None)
     """
-    n_jobs = kwargs.pop('n_jobs', 1)
+    n_jobs = kwargs.pop('n_jobs', None)
     if kwargs:
         # We do not currently support `transformer_weights` as we may want to
         # change its type spec in make_union
diff --git a/sklearn/preprocessing/data.py b/sklearn/preprocessing/data.py
index 226dbc823b..0a33f9140f 100755
--- a/sklearn/preprocessing/data.py
+++ b/sklearn/preprocessing/data.py
@@ -831,6 +831,20 @@ class MaxAbsScaler(BaseEstimator, TransformerMixin):
         The number of samples processed by the estimator. Will be reset on
         new calls to fit, but increments across ``partial_fit`` calls.
 
+    Examples
+    --------
+    >>> from sklearn.preprocessing import MaxAbsScaler
+    >>> X = [[ 1., -1.,  2.],
+    ...      [ 2.,  0.,  0.],
+    ...      [ 0.,  1., -1.]]
+    >>> transformer = MaxAbsScaler().fit(X)
+    >>> transformer
+    MaxAbsScaler(copy=True)
+    >>> transformer.transform(X)
+    array([[ 0.5, -1. ,  1. ],
+           [ 1. ,  0. ,  0. ],
+           [ 0. ,  1. , -0.5]])
+
     See also
     --------
     maxabs_scale: Equivalent function without the estimator API.
@@ -1068,6 +1082,21 @@ class RobustScaler(BaseEstimator, TransformerMixin):
         .. versionadded:: 0.17
            *scale_* attribute.
 
+    Examples
+    --------
+    >>> from sklearn.preprocessing import RobustScaler
+    >>> X = [[ 1., -2.,  2.],
+    ...      [ -2.,  1.,  3.],
+    ...      [ 4.,  1., -2.]]
+    >>> transformer = RobustScaler().fit(X)
+    >>> transformer
+    RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,
+           with_scaling=True)
+    >>> transformer.transform(X)
+    array([[ 0. , -2. ,  0. ],
+           [-1. ,  0. ,  0.4],
+           [ 1. ,  0. , -1.6]])
+
     See also
     --------
     robust_scale: Equivalent function without the estimator API.
@@ -1580,6 +1609,20 @@ class Normalizer(BaseEstimator, TransformerMixin):
         copy (if the input is already a numpy array or a scipy.sparse
         CSR matrix).
 
+    Examples
+    --------
+    >>> from sklearn.preprocessing import Normalizer
+    >>> X = [[4, 1, 2, 2],
+    ...      [1, 3, 9, 3],
+    ...      [5, 7, 5, 1]]
+    >>> transformer = Normalizer().fit(X) # fit does nothing.
+    >>> transformer
+    Normalizer(copy=True, norm='l2')
+    >>> transformer.transform(X)
+    array([[0.8, 0.2, 0.4, 0.4],
+           [0.1, 0.3, 0.9, 0.3],
+           [0.5, 0.7, 0.5, 0.1]])
+
     Notes
     -----
     This estimator is stateless (besides constructor parameters), the
@@ -1707,6 +1750,20 @@ class Binarizer(BaseEstimator, TransformerMixin):
         set to False to perform inplace binarization and avoid a copy (if
         the input is already a numpy array or a scipy.sparse CSR matrix).
 
+    Examples
+    --------
+    >>> from sklearn.preprocessing import Binarizer
+    >>> X = [[ 1., -1.,  2.],
+    ...      [ 2.,  0.,  0.],
+    ...      [ 0.,  1., -1.]]
+    >>> transformer = Binarizer().fit(X) # fit does nothing.
+    >>> transformer
+    Binarizer(copy=True, threshold=0.0)
+    >>> transformer.transform(X)
+    array([[1., 0., 1.],
+           [1., 0., 0.],
+           [0., 1., 0.]])
+
     Notes
     -----
     If the input is a sparse matrix, only the non-zero values are subject
@@ -1771,7 +1828,28 @@ class KernelCenterer(BaseEstimator, TransformerMixin):
     sklearn.preprocessing.StandardScaler(with_std=False).
 
     Read more in the :ref:`User Guide <kernel_centering>`.
+
+    Examples
+    --------
+    >>> from sklearn.preprocessing import KernelCenterer
+    >>> from sklearn.metrics.pairwise import pairwise_kernels
+    >>> X = [[ 1., -2.,  2.],
+    ...      [ -2.,  1.,  3.],
+    ...      [ 4.,  1., -2.]]
+    >>> K = pairwise_kernels(X, metric='linear')
+    >>> K
+    array([[  9.,   2.,  -2.],
+           [  2.,  14., -13.],
+           [ -2., -13.,  21.]])
+    >>> transformer = KernelCenterer().fit(K)
+    >>> transformer
+    KernelCenterer()
+    >>> transformer.transform(K)
+    array([[  5.,   0.,  -5.],
+           [  0.,  14., -14.],
+           [ -5., -14.,  19.]])
     """
+
     def __init__(self):
         # Needed for backported inspect.signature compatibility with PyPy
         pass
diff --git a/sklearn/random_projection.py b/sklearn/random_projection.py
index daa3c0243d..4a2edada4c 100755
--- a/sklearn/random_projection.py
+++ b/sklearn/random_projection.py
@@ -150,7 +150,7 @@ def _check_input_size(n_components, n_features):
                          n_components)
     if n_features <= 0:
         raise ValueError("n_features must be strictly positive, got %d" %
-                         n_components)
+                         n_features)
 
 
 def gaussian_random_matrix(n_components, n_features, random_state=None):
@@ -465,6 +465,16 @@ class GaussianRandomProjection(BaseRandomProjection):
     components_ : numpy array of shape [n_components, n_features]
         Random matrix used for the projection.
 
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.random_projection import GaussianRandomProjection
+    >>> X = np.random.rand(100, 10000)
+    >>> transformer = GaussianRandomProjection()
+    >>> X_new = transformer.fit_transform(X)
+    >>> X_new.shape
+    (100, 3947)
+
     See Also
     --------
     SparseRandomProjection
@@ -577,6 +587,20 @@ class SparseRandomProjection(BaseRandomProjection):
     density_ : float in range 0.0 - 1.0
         Concrete density computed from when density = "auto".
 
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.random_projection import SparseRandomProjection
+    >>> np.random.seed(42)
+    >>> X = np.random.rand(100, 10000)
+    >>> transformer = SparseRandomProjection()
+    >>> X_new = transformer.fit_transform(X)
+    >>> X_new.shape
+    (100, 3947)
+    >>> # very few components are non-zero
+    >>> np.mean(transformer.components_ != 0) # doctest: +ELLIPSIS
+    0.0100...
+
     See Also
     --------
     GaussianRandomProjection
diff --git a/sklearn/semi_supervised/label_propagation.py b/sklearn/semi_supervised/label_propagation.py
index 778686f53d..ff32005399 100755
--- a/sklearn/semi_supervised/label_propagation.py
+++ b/sklearn/semi_supervised/label_propagation.py
@@ -101,13 +101,15 @@ class BaseLabelPropagation(six.with_metaclass(ABCMeta, BaseEstimator,
         Convergence tolerance: threshold to consider the system at steady
         state
 
-    n_jobs : int, optional (default = 1)
+   n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
     """
 
     def __init__(self, kernel='rbf', gamma=20, n_neighbors=7,
-                 alpha=1, max_iter=30, tol=1e-3, n_jobs=1):
+                 alpha=1, max_iter=30, tol=1e-3, n_jobs=None):
 
         self.max_iter = max_iter
         self.tol = tol
@@ -334,9 +336,11 @@ class LabelPropagation(BaseLabelPropagation):
         Convergence tolerance: threshold to consider the system at steady
         state
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -384,7 +388,7 @@ class LabelPropagation(BaseLabelPropagation):
     _variant = 'propagation'
 
     def __init__(self, kernel='rbf', gamma=20, n_neighbors=7,
-                 alpha=None, max_iter=1000, tol=1e-3, n_jobs=1):
+                 alpha=None, max_iter=1000, tol=1e-3, n_jobs=None):
         super(LabelPropagation, self).__init__(
             kernel=kernel, gamma=gamma, n_neighbors=n_neighbors, alpha=alpha,
             max_iter=max_iter, tol=tol, n_jobs=n_jobs)
@@ -452,9 +456,11 @@ class LabelSpreading(BaseLabelPropagation):
       Convergence tolerance: threshold to consider the system at steady
       state
 
-    n_jobs : int, optional (default = 1)
+    n_jobs : int or None, optional (default=None)
         The number of parallel jobs to run.
-        If ``-1``, then the number of jobs is set to the number of CPU cores.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
 
     Attributes
     ----------
@@ -502,7 +508,7 @@ class LabelSpreading(BaseLabelPropagation):
     _variant = 'spreading'
 
     def __init__(self, kernel='rbf', gamma=20, n_neighbors=7, alpha=0.2,
-                 max_iter=30, tol=1e-3, n_jobs=1):
+                 max_iter=30, tol=1e-3, n_jobs=None):
 
         # this one has different base parameters
         super(LabelSpreading, self).__init__(kernel=kernel, gamma=gamma,
diff --git a/sklearn/semi_supervised/tests/test_label_propagation.py b/sklearn/semi_supervised/tests/test_label_propagation.py
index 8cd0cce41d..51b725030c 100755
--- a/sklearn/semi_supervised/tests/test_label_propagation.py
+++ b/sklearn/semi_supervised/tests/test_label_propagation.py
@@ -113,8 +113,10 @@ def test_label_propagation_closed_form():
     clf.fit(X, y)
     # adopting notation from Zhu et al 2002
     T_bar = clf._build_graph()
-    Tuu = T_bar[np.meshgrid(unlabelled_idx, unlabelled_idx, indexing='ij')]
-    Tul = T_bar[np.meshgrid(unlabelled_idx, labelled_idx, indexing='ij')]
+    Tuu = T_bar[tuple(np.meshgrid(unlabelled_idx, unlabelled_idx,
+                      indexing='ij'))]
+    Tul = T_bar[tuple(np.meshgrid(unlabelled_idx, labelled_idx,
+                                  indexing='ij'))]
     Y = Y[:, :-1]
     Y_l = Y[labelled_idx, :]
     Y_u = np.dot(np.dot(np.linalg.inv(np.eye(Tuu.shape[0]) - Tuu), Tul), Y_l)
diff --git a/sklearn/svm/bounds.py b/sklearn/svm/bounds.py
index f1897e3d8e..3c37f60193 100755
--- a/sklearn/svm/bounds.py
+++ b/sklearn/svm/bounds.py
@@ -32,8 +32,6 @@ def l1_min_c(X, y, loss='squared_hinge', fit_intercept=True,
         Specifies the loss function.
         With 'squared_hinge' it is the squared hinge loss (a.k.a. L2 loss).
         With 'log' it is the loss of logistic regression models.
-        'l2' is accepted as an alias for 'squared_hinge', for backward
-        compatibility reasons, but should not be used in new code.
 
     fit_intercept : bool, default: True
         Specifies if the intercept should be fitted by the model.
@@ -52,7 +50,7 @@ def l1_min_c(X, y, loss='squared_hinge', fit_intercept=True,
         minimum value for C
     """
     if loss not in ('squared_hinge', 'log'):
-        raise ValueError('loss type not in ("squared_hinge", "log", "l2")')
+        raise ValueError('loss type not in ("squared_hinge", "log")')
 
     X = check_array(X, accept_sparse='csc')
     check_consistent_length(X, y)
diff --git a/sklearn/svm/tests/test_bounds.py b/sklearn/svm/tests/test_bounds.py
index d02c53b05d..fffd7fc787 100755
--- a/sklearn/svm/tests/test_bounds.py
+++ b/sklearn/svm/tests/test_bounds.py
@@ -45,7 +45,8 @@ def check_l1_min_c(X, y, loss, fit_intercept=True, intercept_scaling=None):
     min_c = l1_min_c(X, y, loss, fit_intercept, intercept_scaling)
 
     clf = {
-        'log': LogisticRegression(penalty='l1'),
+        'log': LogisticRegression(penalty='l1', solver='liblinear',
+                                  multi_class='ovr'),
         'squared_hinge': LinearSVC(loss='squared_hinge',
                                    penalty='l1', dual=False),
     }[loss]
diff --git a/sklearn/svm/tests/test_sparse.py b/sklearn/svm/tests/test_sparse.py
index 5fa83050a9..2628cd741a 100755
--- a/sklearn/svm/tests/test_sparse.py
+++ b/sklearn/svm/tests/test_sparse.py
@@ -1,3 +1,4 @@
+import pytest
 import numpy as np
 from scipy import sparse
 from numpy.testing import (assert_array_almost_equal, assert_array_equal,
@@ -10,7 +11,9 @@
 from sklearn.utils.extmath import safe_sparse_dot
 from sklearn.utils.testing import (assert_raises, assert_true, assert_false,
                                    assert_warns, assert_raise_message,
-                                   ignore_warnings)
+                                   ignore_warnings, skip_if_32bit)
+import pytest
+
 
 # test sample 1
 X = np.array([[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]])
@@ -71,6 +74,7 @@ def check_svm_model_equal(dense_svm, sparse_svm, X_train, y_train, X_test):
         assert_raise_message(ValueError, msg, dense_svm.predict, X_test)
 
 
+@skip_if_32bit
 def test_svc():
     """Check that sparse SVC gives the same result as SVC"""
     # many class dataset:
@@ -234,6 +238,8 @@ def test_linearsvc_iris():
     assert_array_equal(pred, sp_clf.predict(iris.data))
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_weight():
     # Test class weights
     X_, y_ = make_classification(n_samples=200, n_features=100,
@@ -265,21 +271,21 @@ def test_sparse_liblinear_intercept_handling():
     test_svm.test_dense_liblinear_intercept_handling(svm.LinearSVC)
 
 
-def test_sparse_oneclasssvm():
+@pytest.mark.parametrize("datasets_index", range(4))
+@pytest.mark.parametrize("kernel", ["linear", "poly", "rbf", "sigmoid"])
+@skip_if_32bit
+def test_sparse_oneclasssvm(datasets_index, kernel):
     # Check that sparse OneClassSVM gives the same result as dense OneClassSVM
     # many class dataset:
     X_blobs, _ = make_blobs(n_samples=100, centers=10, random_state=0)
     X_blobs = sparse.csr_matrix(X_blobs)
-
     datasets = [[X_sp, None, T], [X2_sp, None, T2],
                 [X_blobs[:80], None, X_blobs[80:]],
                 [iris.data, None, iris.data]]
-    kernels = ["linear", "poly", "rbf", "sigmoid"]
-    for dataset in datasets:
-        for kernel in kernels:
-            clf = svm.OneClassSVM(gamma='scale', kernel=kernel)
-            sp_clf = svm.OneClassSVM(gamma='scale', kernel=kernel)
-            check_svm_model_equal(clf, sp_clf, *dataset)
+    dataset = datasets[datasets_index]
+    clf = svm.OneClassSVM(gamma='scale', kernel=kernel)
+    sp_clf = svm.OneClassSVM(gamma='scale', kernel=kernel)
+    check_svm_model_equal(clf, sp_clf, *dataset)
 
 
 def test_sparse_realdata():
diff --git a/sklearn/svm/tests/test_svm.py b/sklearn/svm/tests/test_svm.py
index 6187a08f7b..4a8e4ef735 100755
--- a/sklearn/svm/tests/test_svm.py
+++ b/sklearn/svm/tests/test_svm.py
@@ -5,6 +5,8 @@
 """
 import numpy as np
 import itertools
+import pytest
+
 from numpy.testing import assert_array_equal, assert_array_almost_equal
 from numpy.testing import assert_almost_equal
 from numpy.testing import assert_allclose
@@ -403,6 +405,8 @@ def test_svr_predict():
     assert_array_almost_equal(dec.ravel(), reg.predict(X).ravel())
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_weight():
     # Test class weights
     clf = svm.SVC(gamma='scale', class_weight={1: 0.1})
@@ -442,6 +446,8 @@ def test_sample_weights():
     assert_array_almost_equal(dual_coef_no_weight, clf.dual_coef_)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @ignore_warnings(category=UndefinedMetricWarning)
 def test_auto_weight():
     # Test class weights for imbalanced data
diff --git a/sklearn/tests/test_metaestimators.py b/sklearn/tests/test_metaestimators.py
index 253e0c9647..6558e9de53 100755
--- a/sklearn/tests/test_metaestimators.py
+++ b/sklearn/tests/test_metaestimators.py
@@ -47,7 +47,8 @@ def __init__(self, name, construct, skip_methods=(),
                                 'predict']),
     DelegatorData('Self-Training', lambda est: SelfTraining(est),
                   skip_methods=['transform', 'inverse_transform', 'score',
-                                'predict_log_proba', 'decision_funtion'])
+                                'predict_proba', 'predict_log_proba',
+                                'predict', 'decision_function']),
 ]
 
 
diff --git a/sklearn/tests/test_multiclass.py b/sklearn/tests/test_multiclass.py
index 08c3b9f01e..130c43b3eb 100755
--- a/sklearn/tests/test_multiclass.py
+++ b/sklearn/tests/test_multiclass.py
@@ -187,6 +187,8 @@ def test_ovr_fit_predict_sparse():
         assert_array_equal(dec_pred, clf_sprs.predict(X_test).toarray())
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_ovr_always_present():
     # Test that ovr works with classes that are always present or absent.
     # Note: tests is the case where _ConstantPredictor is utilised
@@ -244,6 +246,8 @@ def test_ovr_multiclass():
         assert_array_equal(y_pred, [0, 0, 1])
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_ovr_binary():
     # Toy dataset where features correspond directly to labels.
     X = np.array([[0, 0, 5], [0, 5, 0], [3, 0, 0], [0, 0, 6], [6, 0, 0]])
diff --git a/sklearn/tests/test_multioutput.py b/sklearn/tests/test_multioutput.py
index 83e3794d78..1eb5a7e48f 100755
--- a/sklearn/tests/test_multioutput.py
+++ b/sklearn/tests/test_multioutput.py
@@ -1,5 +1,6 @@
 from __future__ import division
 
+import pytest
 import numpy as np
 import scipy.sparse as sp
 
@@ -277,7 +278,8 @@ def test_multiclass_multioutput_estimator_predict_proba():
 
     Y = np.concatenate([y1, y2], axis=1)
 
-    clf = MultiOutputClassifier(LogisticRegression(random_state=seed))
+    clf = MultiOutputClassifier(LogisticRegression(
+        multi_class='ovr', solver='liblinear', random_state=seed))
 
     clf.fit(X, Y)
 
@@ -383,6 +385,8 @@ def test_classifier_chain_fit_and_predict_with_linear_svc():
     assert not hasattr(classifier_chain, 'predict_proba')
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_classifier_chain_fit_and_predict_with_sparse_data():
     # Fit classifier chain with sparse data
     X, Y = generate_multilabel_dataset_with_correlations()
@@ -399,6 +403,8 @@ def test_classifier_chain_fit_and_predict_with_sparse_data():
     assert_array_equal(Y_pred_sparse, Y_pred_dense)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_classifier_chain_vs_independent_models():
     # Verify that an ensemble of classifier chains (each of length
     # N) can achieve a higher Jaccard similarity score than N independent
@@ -421,6 +427,8 @@ def test_classifier_chain_vs_independent_models():
                    jaccard_similarity_score(Y_test, Y_pred_ovr))
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_base_chain_fit_and_predict():
     # Fit base chain and verify predict performance
     X, Y = generate_multilabel_dataset_with_correlations()
@@ -440,6 +448,8 @@ def test_base_chain_fit_and_predict():
     assert isinstance(chains[1], ClassifierMixin)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_base_chain_fit_and_predict_with_sparse_data_and_cv():
     # Fit base chain with sparse data cross_val_predict
     X, Y = generate_multilabel_dataset_with_correlations()
@@ -452,6 +462,8 @@ def test_base_chain_fit_and_predict_with_sparse_data_and_cv():
         assert_equal(Y_pred.shape, Y.shape)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_base_chain_random_order():
     # Fit base chain with random order
     X, Y = generate_multilabel_dataset_with_correlations()
@@ -472,6 +484,8 @@ def test_base_chain_random_order():
             assert_array_almost_equal(est1.coef_, est2.coef_)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_base_chain_crossval_fit_and_predict():
     # Fit chain with cross_val_predict and verify predict
     # performance
diff --git a/sklearn/tests/test_pipeline.py b/sklearn/tests/test_pipeline.py
index a36d3e17e3..8a15238ede 100755
--- a/sklearn/tests/test_pipeline.py
+++ b/sklearn/tests/test_pipeline.py
@@ -6,6 +6,7 @@
 import shutil
 import time
 
+import pytest
 import numpy as np
 from scipy import sparse
 
@@ -234,6 +235,8 @@ def test_pipeline_init_tuple():
     pipe.score(X)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_pipeline_methods_anova():
     # Test the various methods of the pipeline (anova).
     iris = load_iris()
@@ -784,6 +787,8 @@ def test_feature_union_feature_names():
                          'get_feature_names', ft.get_feature_names)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_classes_property():
     iris = load_iris()
     X = iris.data
@@ -887,6 +892,8 @@ def test_step_name_validation():
                                  [[1]], [1])
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_set_params_nested_pipeline():
     estimator = Pipeline([
         ('a', Pipeline([
diff --git a/sklearn/tree/tests/test_tree.py b/sklearn/tree/tests/test_tree.py
index 10fb73b181..68b5040374 100755
--- a/sklearn/tree/tests/test_tree.py
+++ b/sklearn/tree/tests/test_tree.py
@@ -507,16 +507,28 @@ def test_error():
         assert_raises(ValueError, est.predict_proba, X2)
 
     for name, TreeEstimator in ALL_TREES.items():
-        assert_raises(ValueError, TreeEstimator(min_samples_leaf=-1).fit, X, y)
-        assert_raises(ValueError, TreeEstimator(min_samples_leaf=.6).fit, X, y)
-        assert_raises(ValueError, TreeEstimator(min_samples_leaf=0.).fit, X, y)
-        assert_raises(ValueError, TreeEstimator(min_samples_leaf=3.).fit, X, y)
-        assert_raises(ValueError,
-                      TreeEstimator(min_weight_fraction_leaf=-1).fit,
-                      X, y)
-        assert_raises(ValueError,
-                      TreeEstimator(min_weight_fraction_leaf=0.51).fit,
-                      X, y)
+        with pytest.warns(DeprecationWarning, match='min_samples_leaf'):
+            assert_raises(ValueError,
+                          TreeEstimator(min_samples_leaf=-1).fit, X, y)
+        with pytest.warns(DeprecationWarning, match='min_samples_leaf'):
+            assert_raises(ValueError,
+                          TreeEstimator(min_samples_leaf=.6).fit, X, y)
+        with pytest.warns(DeprecationWarning, match='min_samples_leaf'):
+            assert_raises(ValueError,
+                          TreeEstimator(min_samples_leaf=0.).fit, X, y)
+        with pytest.warns(DeprecationWarning, match='min_samples_leaf'):
+            assert_raises(ValueError,
+                          TreeEstimator(min_samples_leaf=3.).fit, X, y)
+        with pytest.warns(DeprecationWarning,
+                          match='min_weight_fraction_leaf'):
+            assert_raises(ValueError,
+                          TreeEstimator(min_weight_fraction_leaf=-1).fit,
+                          X, y)
+        with pytest.warns(DeprecationWarning,
+                          match='min_weight_fraction_leaf'):
+            assert_raises(ValueError,
+                          TreeEstimator(min_weight_fraction_leaf=0.51).fit,
+                          X, y)
         assert_raises(ValueError, TreeEstimator(min_samples_split=-1).fit,
                       X, y)
         assert_raises(ValueError, TreeEstimator(min_samples_split=0.0).fit,
@@ -574,7 +586,7 @@ def test_error():
 
 def test_min_samples_split():
     """Test min_samples_split parameter"""
-    X = np.asfortranarray(iris.data.astype(tree._tree.DTYPE))
+    X = np.asfortranarray(iris.data, dtype=tree._tree.DTYPE)
     y = iris.target
 
     # test both DepthFirstTreeBuilder and BestFirstTreeBuilder
@@ -607,7 +619,7 @@ def test_min_samples_split():
 
 def test_min_samples_leaf():
     # Test if leaves contain more than leaf_count training examples
-    X = np.asfortranarray(iris.data.astype(tree._tree.DTYPE))
+    X = np.asfortranarray(iris.data, dtype=tree._tree.DTYPE)
     y = iris.target
 
     # test both DepthFirstTreeBuilder and BestFirstTreeBuilder
@@ -619,7 +631,8 @@ def test_min_samples_leaf():
         est = TreeEstimator(min_samples_leaf=5,
                             max_leaf_nodes=max_leaf_nodes,
                             random_state=0)
-        est.fit(X, y)
+        with pytest.warns(DeprecationWarning, match='min_samples_leaf'):
+            est.fit(X, y)
         out = est.tree_.apply(X)
         node_counts = np.bincount(out)
         # drop inner nodes
@@ -631,7 +644,8 @@ def test_min_samples_leaf():
         est = TreeEstimator(min_samples_leaf=0.1,
                             max_leaf_nodes=max_leaf_nodes,
                             random_state=0)
-        est.fit(X, y)
+        with pytest.warns(DeprecationWarning, match='min_samples_leaf'):
+            est.fit(X, y)
         out = est.tree_.apply(X)
         node_counts = np.bincount(out)
         # drop inner nodes
@@ -660,7 +674,9 @@ def check_min_weight_fraction_leaf(name, datasets, sparse=False):
         est = TreeEstimator(min_weight_fraction_leaf=frac,
                             max_leaf_nodes=max_leaf_nodes,
                             random_state=0)
-        est.fit(X, y, sample_weight=weights)
+        with pytest.warns(DeprecationWarning,
+                          match='min_weight_fraction_leaf'):
+            est.fit(X, y, sample_weight=weights)
 
         if sparse:
             out = est.tree_.apply(X.tocsr())
@@ -685,7 +701,9 @@ def check_min_weight_fraction_leaf(name, datasets, sparse=False):
         est = TreeEstimator(min_weight_fraction_leaf=frac,
                             max_leaf_nodes=max_leaf_nodes,
                             random_state=0)
-        est.fit(X, y)
+        with pytest.warns(DeprecationWarning,
+                          match='min_weight_fraction_leaf'):
+            est.fit(X, y)
 
         if sparse:
             out = est.tree_.apply(X.tocsr())
@@ -731,7 +749,8 @@ def check_min_weight_fraction_leaf_with_min_samples_leaf(name, datasets,
                             max_leaf_nodes=max_leaf_nodes,
                             min_samples_leaf=5,
                             random_state=0)
-        est.fit(X, y)
+        with pytest.warns(DeprecationWarning):
+            est.fit(X, y)
 
         if sparse:
             out = est.tree_.apply(X.tocsr())
@@ -756,7 +775,8 @@ def check_min_weight_fraction_leaf_with_min_samples_leaf(name, datasets,
                             max_leaf_nodes=max_leaf_nodes,
                             min_samples_leaf=.1,
                             random_state=0)
-        est.fit(X, y)
+        with pytest.warns(DeprecationWarning, match='min_samples_leaf'):
+            est.fit(X, y)
 
         if sparse:
             out = est.tree_.apply(X.tocsr())
@@ -792,7 +812,7 @@ def test_min_impurity_split():
     # test if min_impurity_split creates leaves with impurity
     # [0, min_impurity_split) when min_samples_leaf = 1 and
     # min_samples_split = 2.
-    X = np.asfortranarray(iris.data.astype(tree._tree.DTYPE))
+    X = np.asfortranarray(iris.data, dtype=tree._tree.DTYPE)
     y = iris.target
 
     # test both DepthFirstTreeBuilder and BestFirstTreeBuilder
@@ -1412,10 +1432,16 @@ def check_sparse_parameters(tree, dataset):
     assert_array_almost_equal(s.predict(X), d.predict(X))
 
     # Check min_samples_leaf
-    d = TreeEstimator(random_state=0,
-                      min_samples_leaf=X_sparse.shape[0] // 2).fit(X, y)
-    s = TreeEstimator(random_state=0,
-                      min_samples_leaf=X_sparse.shape[0] // 2).fit(X_sparse, y)
+    with pytest.warns(DeprecationWarning, match='min_samples_leaf'):
+        d = TreeEstimator(
+                random_state=0,
+                min_samples_leaf=X_sparse.shape[0] // 2
+            ).fit(X, y)
+    with pytest.warns(DeprecationWarning, match='min_samples_leaf'):
+        s = TreeEstimator(
+                random_state=0,
+                min_samples_leaf=X_sparse.shape[0] // 2
+            ).fit(X_sparse, y)
     assert_tree_equal(d.tree_, s.tree_,
                       "{0} with dense and sparse format gave different "
                       "trees".format(tree))
@@ -1560,7 +1586,8 @@ def _check_min_weight_leaf_split_level(TreeEstimator, X, y, sample_weight):
     assert_equal(est.tree_.max_depth, 1)
 
     est = TreeEstimator(random_state=0, min_weight_fraction_leaf=0.4)
-    est.fit(X, y, sample_weight=sample_weight)
+    with pytest.warns(DeprecationWarning, match='min_weight_fraction_leaf'):
+        est.fit(X, y, sample_weight=sample_weight)
     assert_equal(est.tree_.max_depth, 0)
 
 
diff --git a/sklearn/tree/tree.py b/sklearn/tree/tree.py
index 7105a86ce0..437dc197c7 100755
--- a/sklearn/tree/tree.py
+++ b/sklearn/tree/tree.py
@@ -85,26 +85,26 @@ def __init__(self,
                  splitter,
                  max_depth,
                  min_samples_split,
-                 min_samples_leaf,
                  min_weight_fraction_leaf,
                  max_features,
                  max_leaf_nodes,
                  random_state,
                  min_impurity_decrease,
                  min_impurity_split,
+                 min_samples_leaf='deprecated',
                  class_weight=None,
                  presort=False):
         self.criterion = criterion
         self.splitter = splitter
         self.max_depth = max_depth
         self.min_samples_split = min_samples_split
-        self.min_samples_leaf = min_samples_leaf
         self.min_weight_fraction_leaf = min_weight_fraction_leaf
         self.max_features = max_features
         self.random_state = random_state
         self.max_leaf_nodes = max_leaf_nodes
         self.min_impurity_decrease = min_impurity_decrease
         self.min_impurity_split = min_impurity_split
+        self.min_samples_leaf = min_samples_leaf
         self.class_weight = class_weight
         self.presort = presort
 
@@ -173,18 +173,24 @@ def fit(self, X, y, sample_weight=None, check_input=True,
         max_leaf_nodes = (-1 if self.max_leaf_nodes is None
                           else self.max_leaf_nodes)
 
-        if isinstance(self.min_samples_leaf, (numbers.Integral, np.integer)):
-            if not 1 <= self.min_samples_leaf:
+        if self.min_samples_leaf != 'deprecated':
+            warnings.warn("'min_samples_leaf' is deprecated in 0.20 and "
+                          "will be fixed to a value of 1 in 0.22.",
+                          DeprecationWarning)
+            min_samples_leaf = self.min_samples_leaf
+        else:
+            min_samples_leaf = 1
+        if isinstance(min_samples_leaf, (numbers.Integral, np.integer)):
+            if not 1 <= min_samples_leaf:
                 raise ValueError("min_samples_leaf must be at least 1 "
                                  "or in (0, 0.5], got %s"
-                                 % self.min_samples_leaf)
-            min_samples_leaf = self.min_samples_leaf
+                                 % min_samples_leaf)
         else:  # float
-            if not 0. < self.min_samples_leaf <= 0.5:
+            if not 0. < min_samples_leaf <= 0.5:
                 raise ValueError("min_samples_leaf must be at least 1 "
                                  "or in (0, 0.5], got %s"
-                                 % self.min_samples_leaf)
-            min_samples_leaf = int(ceil(self.min_samples_leaf * n_samples))
+                                 % min_samples_leaf)
+            min_samples_leaf = int(ceil(min_samples_leaf * n_samples))
 
         if isinstance(self.min_samples_split, (numbers.Integral, np.integer)):
             if not 2 <= self.min_samples_split:
@@ -234,7 +240,15 @@ def fit(self, X, y, sample_weight=None, check_input=True,
         if len(y) != n_samples:
             raise ValueError("Number of labels=%d does not match "
                              "number of samples=%d" % (len(y), n_samples))
-        if not 0 <= self.min_weight_fraction_leaf <= 0.5:
+
+        if self.min_weight_fraction_leaf != 'deprecated':
+            warnings.warn("'min_weight_fraction_leaf' is deprecated in 0.20 "
+                          "and will be fixed to a value of 0 in 0.22.",
+                          DeprecationWarning)
+            min_weight_fraction_leaf = self.min_weight_fraction_leaf
+        else:
+            min_weight_fraction_leaf = 0
+        if not 0 <= min_weight_fraction_leaf <= 0.5:
             raise ValueError("min_weight_fraction_leaf must in [0, 0.5]")
         if max_depth <= 0:
             raise ValueError("max_depth must be greater than zero. ")
@@ -269,10 +283,10 @@ def fit(self, X, y, sample_weight=None, check_input=True,
 
         # Set min_weight_leaf from min_weight_fraction_leaf
         if sample_weight is None:
-            min_weight_leaf = (self.min_weight_fraction_leaf *
+            min_weight_leaf = (min_weight_fraction_leaf *
                                n_samples)
         else:
-            min_weight_leaf = (self.min_weight_fraction_leaf *
+            min_weight_leaf = (min_weight_fraction_leaf *
                                np.sum(sample_weight))
 
         if self.min_impurity_split is not None:
@@ -539,8 +553,8 @@ class DecisionTreeClassifier(BaseDecisionTree, ClassifierMixin):
     min_samples_split : int, float, optional (default=2)
         The minimum number of samples required to split an internal node:
 
-        - If int, then consider `min_samples_split` as the minimum number.
-        - If float, then `min_samples_split` is a fraction and
+        - If int, then consider ``min_samples_split`` as the minimum number.
+        - If float, then ``min_samples_split`` is a fraction and
           `ceil(min_samples_split * n_samples)` are the minimum
           number of samples for each split.
 
@@ -550,19 +564,28 @@ class DecisionTreeClassifier(BaseDecisionTree, ClassifierMixin):
     min_samples_leaf : int, float, optional (default=1)
         The minimum number of samples required to be at a leaf node:
 
-        - If int, then consider `min_samples_leaf` as the minimum number.
-        - If float, then `min_samples_leaf` is a fraction and
+        - If int, then consider ``min_samples_leaf`` as the minimum number.
+        - If float, then ``min_samples_leaf`` is a fraction and
           `ceil(min_samples_leaf * n_samples)` are the minimum
           number of samples for each node.
 
         .. versionchanged:: 0.18
            Added float values for fractions.
+        .. deprecated:: 0.20
+           The parameter ``min_samples_leaf`` is deprecated in version 0.20 and
+           will be fixed to a value of 1 in version 0.22. It was not effective
+           for regularization and empirically, 1 is the best value.
 
     min_weight_fraction_leaf : float, optional (default=0.)
         The minimum weighted fraction of the sum total of weights (of all
         the input samples) required to be at a leaf node. Samples have
         equal weight when sample_weight is not provided.
 
+        .. deprecated:: 0.20
+           The parameter ``min_weight_fraction_leaf`` is deprecated in version
+           0.20. Its implementation, like ``min_samples_leaf``, is ineffective
+           for regularization.
+
     max_features : int, float, string or None, optional (default=None)
         The number of features to consider when looking for the best split:
 
@@ -680,7 +703,7 @@ class DecisionTreeClassifier(BaseDecisionTree, ClassifierMixin):
     Notes
     -----
     The default values for the parameters controlling the size of the trees
-    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
+    (e.g. ``max_depth``, ``min_samples_split``, etc.) lead to fully grown and
     unpruned trees which can potentially be very large on some data sets. To
     reduce memory consumption, the complexity and size of the trees should be
     controlled by setting those parameter values.
@@ -728,8 +751,8 @@ def __init__(self,
                  splitter="best",
                  max_depth=None,
                  min_samples_split=2,
-                 min_samples_leaf=1,
-                 min_weight_fraction_leaf=0.,
+                 min_samples_leaf='deprecated',
+                 min_weight_fraction_leaf='deprecated',
                  max_features=None,
                  random_state=None,
                  max_leaf_nodes=None,
@@ -907,8 +930,8 @@ class DecisionTreeRegressor(BaseDecisionTree, RegressorMixin):
     min_samples_split : int, float, optional (default=2)
         The minimum number of samples required to split an internal node:
 
-        - If int, then consider `min_samples_split` as the minimum number.
-        - If float, then `min_samples_split` is a fraction and
+        - If int, then consider ``min_samples_split`` as the minimum number.
+        - If float, then ``min_samples_split`` is a fraction and
           `ceil(min_samples_split * n_samples)` are the minimum
           number of samples for each split.
 
@@ -918,19 +941,28 @@ class DecisionTreeRegressor(BaseDecisionTree, RegressorMixin):
     min_samples_leaf : int, float, optional (default=1)
         The minimum number of samples required to be at a leaf node:
 
-        - If int, then consider `min_samples_leaf` as the minimum number.
-        - If float, then `min_samples_leaf` is a fraction and
+        - If int, then consider ``min_samples_leaf`` as the minimum number.
+        - If float, then ``min_samples_leaf`` is a fraction and
           `ceil(min_samples_leaf * n_samples)` are the minimum
           number of samples for each node.
 
         .. versionchanged:: 0.18
            Added float values for fractions.
+        .. deprecated:: 0.20
+           The parameter ``min_samples_leaf`` is deprecated in version 0.20 and
+           will be fixed to a value of 1 in version 0.22. It was not effective
+           for regularization and empirically, 1 is the best value.
 
     min_weight_fraction_leaf : float, optional (default=0.)
         The minimum weighted fraction of the sum total of weights (of all
         the input samples) required to be at a leaf node. Samples have
         equal weight when sample_weight is not provided.
 
+        .. deprecated:: 0.20
+           The parameter ``min_weight_fraction_leaf`` is deprecated in version
+           0.20. Its implementation, like ``min_samples_leaf``, is ineffective
+           for regularization.
+
     max_features : int, float, string or None, optional (default=None)
         The number of features to consider when looking for the best split:
 
@@ -1019,7 +1051,7 @@ class DecisionTreeRegressor(BaseDecisionTree, RegressorMixin):
     Notes
     -----
     The default values for the parameters controlling the size of the trees
-    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
+    (e.g. ``max_depth``, ``min_samples_split``, etc.) lead to fully grown and
     unpruned trees which can potentially be very large on some data sets. To
     reduce memory consumption, the complexity and size of the trees should be
     controlled by setting those parameter values.
@@ -1067,8 +1099,8 @@ def __init__(self,
                  splitter="best",
                  max_depth=None,
                  min_samples_split=2,
-                 min_samples_leaf=1,
-                 min_weight_fraction_leaf=0.,
+                 min_samples_leaf='deprecated',
+                 min_weight_fraction_leaf='deprecated',
                  max_features=None,
                  random_state=None,
                  max_leaf_nodes=None,
@@ -1165,8 +1197,8 @@ class ExtraTreeClassifier(DecisionTreeClassifier):
     min_samples_split : int, float, optional (default=2)
         The minimum number of samples required to split an internal node:
 
-        - If int, then consider `min_samples_split` as the minimum number.
-        - If float, then `min_samples_split` is a fraction and
+        - If int, then consider ``min_samples_split`` as the minimum number.
+        - If float, then ``min_samples_split`` is a fraction and
           `ceil(min_samples_split * n_samples)` are the minimum
           number of samples for each split.
 
@@ -1176,19 +1208,28 @@ class ExtraTreeClassifier(DecisionTreeClassifier):
     min_samples_leaf : int, float, optional (default=1)
         The minimum number of samples required to be at a leaf node:
 
-        - If int, then consider `min_samples_leaf` as the minimum number.
-        - If float, then `min_samples_leaf` is a fraction and
+        - If int, then consider ``min_samples_leaf`` as the minimum number.
+        - If float, then ``min_samples_leaf`` is a fraction and
           `ceil(min_samples_leaf * n_samples)` are the minimum
           number of samples for each node.
 
         .. versionchanged:: 0.18
            Added float values for fractions.
+        .. deprecated:: 0.20
+           The parameter ``min_samples_leaf`` is deprecated in version 0.20 and
+           will be fixed to a value of 1 in version 0.22. It was not effective
+           for regularization and empirically, 1 is the best value.
 
     min_weight_fraction_leaf : float, optional (default=0.)
         The minimum weighted fraction of the sum total of weights (of all
         the input samples) required to be at a leaf node. Samples have
         equal weight when sample_weight is not provided.
 
+        .. deprecated:: 0.20
+           The parameter ``min_weight_fraction_leaf`` is deprecated in version
+           0.20. Its implementation, like ``min_samples_leaf``, is ineffective
+           for regularization.
+
     max_features : int, float, string or None, optional (default="auto")
         The number of features to consider when looking for the best split:
 
@@ -1272,7 +1313,7 @@ class ExtraTreeClassifier(DecisionTreeClassifier):
     Notes
     -----
     The default values for the parameters controlling the size of the trees
-    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
+    (e.g. ``max_depth``, ``min_samples_split``, etc.) lead to fully grown and
     unpruned trees which can potentially be very large on some data sets. To
     reduce memory consumption, the complexity and size of the trees should be
     controlled by setting those parameter values.
@@ -1288,8 +1329,8 @@ def __init__(self,
                  splitter="random",
                  max_depth=None,
                  min_samples_split=2,
-                 min_samples_leaf=1,
-                 min_weight_fraction_leaf=0.,
+                 min_samples_leaf='deprecated',
+                 min_weight_fraction_leaf='deprecated',
                  max_features="auto",
                  random_state=None,
                  max_leaf_nodes=None,
@@ -1349,8 +1390,8 @@ class ExtraTreeRegressor(DecisionTreeRegressor):
     min_samples_split : int, float, optional (default=2)
         The minimum number of samples required to split an internal node:
 
-        - If int, then consider `min_samples_split` as the minimum number.
-        - If float, then `min_samples_split` is a fraction and
+        - If int, then consider ``min_samples_split`` as the minimum number.
+        - If float, then ``min_samples_split`` is a fraction and
           `ceil(min_samples_split * n_samples)` are the minimum
           number of samples for each split.
 
@@ -1360,19 +1401,28 @@ class ExtraTreeRegressor(DecisionTreeRegressor):
     min_samples_leaf : int, float, optional (default=1)
         The minimum number of samples required to be at a leaf node:
 
-        - If int, then consider `min_samples_leaf` as the minimum number.
-        - If float, then `min_samples_leaf` is a fraction and
+        - If int, then consider ``min_samples_leaf`` as the minimum number.
+        - If float, then ``min_samples_leaf`` is a fraction and
           `ceil(min_samples_leaf * n_samples)` are the minimum
           number of samples for each node.
 
         .. versionchanged:: 0.18
            Added float values for fractions.
+        .. deprecated:: 0.20
+           The parameter ``min_samples_leaf`` is deprecated in version 0.20 and
+           will be fixed to a value of 1 in version 0.22. It was not effective
+           for regularization and empirically, 1 is the best value.
 
     min_weight_fraction_leaf : float, optional (default=0.)
         The minimum weighted fraction of the sum total of weights (of all
         the input samples) required to be at a leaf node. Samples have
         equal weight when sample_weight is not provided.
 
+        .. deprecated:: 0.20
+           The parameter ``min_weight_fraction_leaf`` is deprecated in version
+           0.20. Its implementation, like ``min_samples_leaf``, is ineffective
+           for regularization.
+
     max_features : int, float, string or None, optional (default="auto")
         The number of features to consider when looking for the best split:
 
@@ -1436,7 +1486,7 @@ class ExtraTreeRegressor(DecisionTreeRegressor):
     Notes
     -----
     The default values for the parameters controlling the size of the trees
-    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
+    (e.g. ``max_depth``, ``min_samples_split``, etc.) lead to fully grown and
     unpruned trees which can potentially be very large on some data sets. To
     reduce memory consumption, the complexity and size of the trees should be
     controlled by setting those parameter values.
@@ -1452,8 +1502,8 @@ def __init__(self,
                  splitter="random",
                  max_depth=None,
                  min_samples_split=2,
-                 min_samples_leaf=1,
-                 min_weight_fraction_leaf=0.,
+                 min_samples_leaf='deprecated',
+                 min_weight_fraction_leaf='deprecated',
                  max_features="auto",
                  random_state=None,
                  min_impurity_decrease=0.,
diff --git a/sklearn/utils/__init__.py b/sklearn/utils/__init__.py
index c5db6633b6..4c22752030 100755
--- a/sklearn/utils/__init__.py
+++ b/sklearn/utils/__init__.py
@@ -3,6 +3,7 @@
 """
 import numbers
 import platform
+import struct
 
 import numpy as np
 from scipy.sparse import issparse
@@ -16,7 +17,8 @@
                          check_symmetric)
 from .class_weight import compute_class_weight, compute_sample_weight
 from ._joblib import cpu_count, Parallel, Memory, delayed, hash
-from ._joblib import parallel_backend
+from ._joblib import parallel_backend, register_parallel_backend
+from ._joblib import effective_n_jobs
 from ..exceptions import DataConversionWarning
 from ..utils.fixes import _Sequence as Sequence
 from .deprecation import deprecated
@@ -30,9 +32,10 @@
            "check_consistent_length", "check_X_y", 'indexable',
            "check_symmetric", "indices_to_mask", "deprecated",
            "cpu_count", "Parallel", "Memory", "delayed", "parallel_backend",
-           "hash"]
+           "register_parallel_backend", "hash", "effective_n_jobs"]
 
 IS_PYPY = platform.python_implementation() == 'PyPy'
+_IS_32BIT = 8 * struct.calcsize("P") == 32
 
 
 class Bunch(dict):
@@ -477,45 +480,6 @@ def gen_even_slices(n, n_packs, n_samples=None):
             start = end
 
 
-def _get_n_jobs(n_jobs):
-    """Get number of jobs for the computation.
-
-    This function reimplements the logic of joblib to determine the actual
-    number of jobs depending on the cpu count. If -1 all CPUs are used.
-    If 1 is given, no parallel computing code is used at all, which is useful
-    for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used.
-    Thus for n_jobs = -2, all CPUs but one are used.
-
-    Parameters
-    ----------
-    n_jobs : int
-        Number of jobs stated in joblib convention.
-
-    Returns
-    -------
-    n_jobs : int
-        The actual number of jobs as positive integer.
-
-    Examples
-    --------
-    >>> from sklearn.utils import _get_n_jobs
-    >>> _get_n_jobs(4)
-    4
-    >>> jobs = _get_n_jobs(-2)
-    >>> assert jobs == max(cpu_count() - 1, 1)
-    >>> _get_n_jobs(0)
-    Traceback (most recent call last):
-    ...
-    ValueError: Parameter n_jobs == 0 has no meaning.
-    """
-    if n_jobs < 0:
-        return max(cpu_count() + 1 + n_jobs, 1)
-    elif n_jobs == 0:
-        raise ValueError('Parameter n_jobs == 0 has no meaning.')
-    else:
-        return n_jobs
-
-
 def tosequence(x):
     """Cast iterable x to a Sequence, avoiding a copy if possible.
 
diff --git a/sklearn/utils/_logistic_sigmoid.pyx b/sklearn/utils/_logistic_sigmoid.pyx
index 9c7b8d0a20..58809eb7c1 100755
--- a/sklearn/utils/_logistic_sigmoid.pyx
+++ b/sklearn/utils/_logistic_sigmoid.pyx
@@ -13,9 +13,9 @@ ctypedef np.float64_t DTYPE_t
 cdef DTYPE_t _inner_log_logistic_sigmoid(DTYPE_t x):
     """Log of the logistic sigmoid function log(1 / (1 + e ** -x))"""
     if x > 0:
-        return -np.log1p(exp(-x))
+        return -log(1 + exp(-x))
     else:
-        return x - np.log1p(exp(x))
+        return x - log(1 + exp(x))
 
 
 def _log_logistic_sigmoid(int n_samples, int n_features, 
diff --git a/sklearn/utils/_show_versions.py b/sklearn/utils/_show_versions.py
new file mode 100755
index 0000000000..5973e8afb8
--- /dev/null
+++ b/sklearn/utils/_show_versions.py
@@ -0,0 +1,119 @@
+"""
+Utility methods to print system info for debugging
+
+adapted from :func:`pandas.show_versions`
+"""
+# License: BSD 3 clause
+
+import platform
+import sys
+import importlib
+
+
+def _get_sys_info():
+    """System information
+
+    Return
+    ------
+    sys_info : dict
+        system and Python version information
+
+    """
+    python = sys.version.replace('\n', ' ')
+
+    blob = [
+        ("python", python),
+        ('executable', sys.executable),
+        ("machine", platform.platform()),
+    ]
+
+    return dict(blob)
+
+
+def _get_deps_info():
+    """Overview of the installed version of main dependencies
+
+    Returns
+    -------
+    deps_info: dict
+        version information on relevant Python libraries
+
+    """
+    deps = [
+        "pip",
+        "setuptools",
+        "sklearn",
+        "numpy",
+        "scipy",
+        "Cython",
+        "pandas",
+    ]
+
+    def get_version(module):
+        return module.__version__
+
+    deps_info = {}
+
+    for modname in deps:
+        try:
+            if modname in sys.modules:
+                mod = sys.modules[modname]
+            else:
+                mod = importlib.import_module(modname)
+            ver = get_version(mod)
+            deps_info[modname] = ver
+        except ImportError:
+            deps_info[modname] = None
+
+    return deps_info
+
+
+def _get_blas_info():
+    """Information on system BLAS
+
+    Uses the `scikit-learn` builtin method
+    :func:`sklearn._build_utils.get_blas_info` which may fail from time to time
+
+    Returns
+    -------
+    blas_info: dict
+        system BLAS information
+
+    """
+    from .._build_utils import get_blas_info
+
+    cblas_libs, blas_dict = get_blas_info()
+
+    macros = ['{key}={val}'.format(key=a, val=b)
+              for (a, b) in blas_dict.get('define_macros', [])]
+
+    blas_blob = [
+        ('macros', ', '.join(macros)),
+        ('lib_dirs', ':'.join(blas_dict.get('library_dirs', ''))),
+        ('cblas_libs', ', '.join(cblas_libs)),
+    ]
+
+    return dict(blas_blob)
+
+
+def show_versions():
+    "Print useful debugging information"
+
+    sys_info = _get_sys_info()
+    deps_info = _get_deps_info()
+    blas_info = _get_blas_info()
+
+    print('\nSystem')
+    print('------')
+    for k, stat in sys_info.items():
+        print("{k:>10}: {stat}".format(k=k, stat=stat))
+
+    print('\nBLAS')
+    print('----')
+    for k, stat in blas_info.items():
+        print("{k:>10}: {stat}".format(k=k, stat=stat))
+
+    print('\nPython deps')
+    print('-----------')
+    for k, stat in deps_info.items():
+        print("{k:>10}: {stat}".format(k=k, stat=stat))
diff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py
index dac884a317..54369033a7 100755
--- a/sklearn/utils/estimator_checks.py
+++ b/sklearn/utils/estimator_checks.py
@@ -6,7 +6,6 @@
 import traceback
 import pickle
 from copy import deepcopy
-import struct
 from functools import partial
 
 import numpy as np
@@ -14,7 +13,7 @@
 from scipy.stats import rankdata
 
 from sklearn.externals.six.moves import zip
-from sklearn.utils import IS_PYPY
+from sklearn.utils import IS_PYPY, _IS_32BIT
 from sklearn.utils._joblib import hash, Memory
 from sklearn.utils.testing import assert_raises, _get_args
 from sklearn.utils.testing import assert_raises_regex
@@ -404,11 +403,6 @@ def __array__(self, dtype=None):
         return self.data
 
 
-def _is_32bit():
-    """Detect if process is 32bit Python."""
-    return struct.calcsize('P') * 8 == 32
-
-
 def _is_pairwise(estimator):
     """Returns True if estimator has a _pairwise attribute set to True.
 
@@ -943,7 +937,7 @@ def check_transformers_unfitted(name, transformer):
 
 
 def _check_transformer(name, transformer_orig, X, y):
-    if name in ('CCA', 'LocallyLinearEmbedding', 'KernelPCA') and _is_32bit():
+    if name in ('CCA', 'LocallyLinearEmbedding', 'KernelPCA') and _IS_32BIT:
         # Those transformers yield non-deterministic output when executed on
         # a 32bit Python. The same transformers are stable on 64bit Python.
         # FIXME: try to isolate a minimalistic reproduction case only depending
@@ -1021,7 +1015,7 @@ def _check_transformer(name, transformer_orig, X, y):
 
 @ignore_warnings
 def check_pipeline_consistency(name, estimator_orig):
-    if name in ('CCA', 'LocallyLinearEmbedding', 'KernelPCA') and _is_32bit():
+    if name in ('CCA', 'LocallyLinearEmbedding', 'KernelPCA') and _IS_32BIT:
         # Those transformers yield non-deterministic output when executed on
         # a 32bit Python. The same transformers are stable on 64bit Python.
         # FIXME: try to isolate a minimalistic reproduction case only depending
@@ -1545,7 +1539,7 @@ def check_outliers_train(name, estimator_orig, readonly_memmap=True):
     y_scores = estimator.score_samples(X)
     assert y_scores.shape == (n_samples,)
     y_dec = y_scores - estimator.offset_
-    assert_array_equal(y_dec, decision)
+    assert_allclose(y_dec, decision)
 
     # raises error on malformed input for score_samples
     assert_raises(ValueError, estimator.score_samples, X.T)
diff --git a/sklearn/utils/stats.py b/sklearn/utils/stats.py
index 43f37bb95a..82b8912b78 100755
--- a/sklearn/utils/stats.py
+++ b/sklearn/utils/stats.py
@@ -22,4 +22,6 @@ def _weighted_percentile(array, sample_weight, percentile=50):
     weight_cdf = stable_cumsum(sample_weight[sorted_idx])
     percentile_idx = np.searchsorted(
         weight_cdf, (percentile / 100.) * weight_cdf[-1])
+    # in rare cases, percentile_idx equals to len(sorted_idx)
+    percentile_idx = np.clip(percentile_idx, 0, len(sorted_idx)-1)
     return array[sorted_idx[percentile_idx]]
diff --git a/sklearn/utils/testing.py b/sklearn/utils/testing.py
index 473e4b3e9c..f2a834152e 100755
--- a/sklearn/utils/testing.py
+++ b/sklearn/utils/testing.py
@@ -15,7 +15,6 @@
 import pkgutil
 import warnings
 import sys
-import struct
 import functools
 
 import scipy as sp
@@ -47,7 +46,7 @@
 from sklearn.base import BaseEstimator
 from sklearn.externals import joblib
 from sklearn.utils.fixes import signature
-from sklearn.utils import deprecated, IS_PYPY
+from sklearn.utils import deprecated, IS_PYPY, _IS_32BIT
 
 
 additional_names_in_all = []
@@ -468,9 +467,13 @@ def assert_allclose_dense_sparse(x, y, rtol=1e-07, atol=1e-9, err_msg=''):
                          " not a sparse matrix and an array.")
 
 
+@deprecated('deprecated in version 0.20 to be removed in version 0.22')
 def fake_mldata(columns_dict, dataname, matfile, ordering=None):
     """Create a fake mldata data set.
 
+    .. deprecated:: 0.20
+        Will be removed in version 0.22
+
     Parameters
     ----------
     columns_dict : dict, keys=str, values=ndarray
@@ -508,6 +511,7 @@ def fake_mldata(columns_dict, dataname, matfile, ordering=None):
     scipy.io.savemat(matfile, datasets, oned_as='column')
 
 
+@deprecated('deprecated in version 0.20 to be removed in version 0.22')
 class mock_mldata_urlopen(object):
     """Object that mocks the urlopen function to fake requests to mldata.
 
@@ -515,6 +519,9 @@ class mock_mldata_urlopen(object):
     creates a fake dataset in a StringIO object and returns it. Otherwise, it
     raises an HTTPError.
 
+    .. deprecated:: 0.20
+        Will be removed in version 0.22
+
     Parameters
     ----------
     mock_datasets : dict
@@ -582,7 +589,8 @@ def uninstall_mldata_mock():
                    "RFE", "RFECV", "BaseEnsemble", "ClassifierChain",
                    "RegressorChain", "SelfTraining"]
 # estimators that there is no way to default-construct sensibly
-OTHER = ["Pipeline", "FeatureUnion", "GridSearchCV", "RandomizedSearchCV",
+OTHER = ["Pipeline", "FeatureUnion",
+         "GridSearchCV", "RandomizedSearchCV",
          "SelectFromModel", "ColumnTransformer"]
 
 # some strange ones
@@ -749,7 +757,7 @@ def run_test(*args, **kwargs):
 try:
     import pytest
 
-    skip_if_32bit = pytest.mark.skipif(8 * struct.calcsize("P") == 32,
+    skip_if_32bit = pytest.mark.skipif(_IS_32BIT,
                                        reason='skipped on 32bit platforms')
     skip_travis = pytest.mark.skipif(os.environ.get('TRAVIS') == 'true',
                                      reason='skip on travis')
diff --git a/sklearn/utils/tests/test_class_weight.py b/sklearn/utils/tests/test_class_weight.py
index 1dfedad9bc..c2d03595fb 100755
--- a/sklearn/utils/tests/test_class_weight.py
+++ b/sklearn/utils/tests/test_class_weight.py
@@ -1,4 +1,5 @@
 import numpy as np
+import pytest
 
 from sklearn.linear_model import LogisticRegression
 from sklearn.datasets import make_blobs
@@ -65,6 +66,8 @@ def test_compute_class_weight_dict():
                          classes, y)
 
 
+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22
+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 def test_compute_class_weight_invariance():
     # Test that results with class_weight="balanced" is invariant wrt
     # class imbalance if the number of samples is identical.
diff --git a/sklearn/utils/tests/test_show_versions.py b/sklearn/utils/tests/test_show_versions.py
new file mode 100755
index 0000000000..f55bc8b945
--- /dev/null
+++ b/sklearn/utils/tests/test_show_versions.py
@@ -0,0 +1,32 @@
+
+from sklearn.utils._show_versions import _get_sys_info
+from sklearn.utils._show_versions import _get_deps_info
+from sklearn.utils._show_versions import show_versions
+
+
+def test_get_sys_info():
+    sys_info = _get_sys_info()
+
+    assert 'python' in sys_info
+    assert 'executable' in sys_info
+    assert 'machine' in sys_info
+
+
+def test_get_deps_info():
+    deps_info = _get_deps_info()
+
+    assert 'pip' in deps_info
+    assert 'setuptools' in deps_info
+    assert 'sklearn' in deps_info
+    assert 'numpy' in deps_info
+    assert 'scipy' in deps_info
+    assert 'Cython' in deps_info
+    assert 'pandas' in deps_info
+
+
+def test_show_versions_with_blas(capsys):
+    show_versions()
+    out, err = capsys.readouterr()
+    assert 'python' in out
+    assert 'numpy' in out
+    assert 'BLAS' in out
