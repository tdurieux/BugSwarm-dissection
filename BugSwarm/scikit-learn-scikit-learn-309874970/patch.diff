diff --git a/.travis.yml b/.travis.yml
index 5cf2d2ea78bb..aea19fc6b36b 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -22,6 +22,7 @@ matrix:
   include:
     # This environment tests that scikit-learn can be built against
     # versions of numpy, scipy with ATLAS that comes with Ubuntu Trusty 14.04
+    # i.e. numpy 1.8.2 and scipy 0.13.3
     - env: DISTRIB="ubuntu" PYTHON_VERSION="2.7" CYTHON_VERSION="0.23.5"
            COVERAGE=true
       if: type != cron
@@ -32,9 +33,9 @@ matrix:
             - python-scipy
             - libatlas3-base
             - libatlas-dev
-    # This environment tests the oldest supported anaconda env
-    - env: DISTRIB="conda" PYTHON_VERSION="2.7" INSTALL_MKL="false"
-           NUMPY_VERSION="1.8.2" SCIPY_VERSION="0.13.3" CYTHON_VERSION="0.23.5"
+    # Python 3.4 build
+    - env: DISTRIB="conda" PYTHON_VERSION="3.4" INSTALL_MKL="false"
+           NUMPY_VERSION="1.10.4" SCIPY_VERSION="0.16.1" CYTHON_VERSION="0.25.2"
            COVERAGE=true
       if: type != cron
     # This environment tests the newest supported Anaconda release (5.0.0)
diff --git a/README.rst b/README.rst
index 7afc95fb6bb8..7b9d1101518d 100644
--- a/README.rst
+++ b/README.rst
@@ -49,7 +49,7 @@ Dependencies
 
 scikit-learn requires:
 
-- Python (>= 2.7 or >= 3.3)
+- Python (>= 2.7 or >= 3.4)
 - NumPy (>= 1.8.2)
 - SciPy (>= 0.13.3)
 
diff --git a/doc/developers/advanced_installation.rst b/doc/developers/advanced_installation.rst
index c03298e7ec4c..ee5c83e34965 100644
--- a/doc/developers/advanced_installation.rst
+++ b/doc/developers/advanced_installation.rst
@@ -35,7 +35,7 @@ Installing an official release
 
 Scikit-learn requires:
 
-- Python (>= 2.7 or >= 3.3),
+- Python (>= 2.7 or >= 3.4),
 - NumPy (>= 1.8.2),
 - SciPy (>= 0.13.3).
 
diff --git a/doc/install.rst b/doc/install.rst
index e4240b41d857..6fecec888c24 100644
--- a/doc/install.rst
+++ b/doc/install.rst
@@ -15,7 +15,7 @@ Installing the latest release
 
 Scikit-learn requires:
 
-- Python (>= 2.7 or >= 3.3),
+- Python (>= 2.7 or >= 3.4),
 - NumPy (>= 1.8.2),
 - SciPy (>= 0.13.3).
 
diff --git a/doc/modules/model_evaluation.rst b/doc/modules/model_evaluation.rst
index c7dea387e6b8..7505ec181955 100644
--- a/doc/modules/model_evaluation.rst
+++ b/doc/modules/model_evaluation.rst
@@ -1365,29 +1365,33 @@ implements label ranking average precision (LRAP). This metric is linked to
 the :func:`average_precision_score` function, but is based on the notion of
 label ranking instead of precision and recall.
 
-Label ranking average precision (LRAP) is the average over each ground truth
-label assigned to each sample, of the ratio of true vs. total labels with lower
-score. This metric will yield better scores if you are able to give better rank
-to the labels associated with each sample. The obtained score is always strictly
-greater than 0, and the best value is 1. If there is exactly one relevant
-label per sample, label ranking average precision is equivalent to the `mean
+Label ranking average precision (LRAP) averages over the samples the answer to
+the following question: for each ground truth label, what fraction of
+higher-ranked labels were true labels? This performance measure will be higher
+if you are able to give better rank to the labels associated with each sample.
+The obtained score is always strictly greater than 0, and the best value is 1.
+If there is exactly one relevant label per sample, label ranking average
+precision is equivalent to the `mean
 reciprocal rank <https://en.wikipedia.org/wiki/Mean_reciprocal_rank>`_.
 
 Formally, given a binary indicator matrix of the ground truth labels
-:math:`y \in \mathcal{R}^{n_\text{samples} \times n_\text{labels}}` and the
-score associated with each label
-:math:`\hat{f} \in \mathcal{R}^{n_\text{samples} \times n_\text{labels}}`,
+:math:`y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}`
+and the score associated with each label
+:math:`\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}`,
 the average precision is defined as
 
 .. math::
   LRAP(y, \hat{f}) = \frac{1}{n_{\text{samples}}}
-    \sum_{i=0}^{n_{\text{samples}} - 1} \frac{1}{|y_i|}
+    \sum_{i=0}^{n_{\text{samples}} - 1} \frac{1}{||y_i||_0}
     \sum_{j:y_{ij} = 1} \frac{|\mathcal{L}_{ij}|}{\text{rank}_{ij}}
 
 
-with :math:`\mathcal{L}_{ij} = \left\{k: y_{ik} = 1, \hat{f}_{ik} \geq \hat{f}_{ij} \right\}`,
-:math:`\text{rank}_{ij} = \left|\left\{k: \hat{f}_{ik} \geq \hat{f}_{ij} \right\}\right|`
-and :math:`|\cdot|` is the l0 norm or the cardinality of the set.
+where
+:math:`\mathcal{L}_{ij} = \left\{k: y_{ik} = 1, \hat{f}_{ik} \geq \hat{f}_{ij} \right\}`,
+:math:`\text{rank}_{ij} = \left|\left\{k: \hat{f}_{ik} \geq \hat{f}_{ij} \right\}\right|`,
+:math:`|\cdot|` computes the cardinality of the set (i.e., the number of
+elements in the set), and :math:`||\cdot||_0` is the :math:`\ell_0` "norm"
+(which computes the number of nonzero elements in a vector).
 
 Here is a small example of usage of this function::
 
@@ -1406,8 +1410,8 @@ Ranking loss
 The :func:`label_ranking_loss` function computes the ranking loss which
 averages over the samples the number of label pairs that are incorrectly
 ordered, i.e. true labels have a lower score than false labels, weighted by
-the inverse number of false and true labels. The lowest achievable
-ranking loss is zero.
+the inverse of the number of ordered pairs of false and true labels.
+The lowest achievable ranking loss is zero.
 
 Formally, given a binary indicator matrix of the ground truth labels
 :math:`y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}` and the
@@ -1417,10 +1421,12 @@ the ranking loss is defined as
 
 .. math::
   \text{ranking\_loss}(y, \hat{f}) =  \frac{1}{n_{\text{samples}}}
-    \sum_{i=0}^{n_{\text{samples}} - 1} \frac{1}{|y_i|(n_\text{labels} - |y_i|)}
-    \left|\left\{(k, l): \hat{f}_{ik} < \hat{f}_{il}, y_{ik} = 1, y_{il} = 0 \right\}\right|
+    \sum_{i=0}^{n_{\text{samples}} - 1} \frac{1}{||y_i||_0(n_\text{labels} - ||y_i||_0)}
+    \left|\left\{(k, l): \hat{f}_{ik} \leq \hat{f}_{il}, y_{ik} = 1, y_{il} = 0 \right\}\right|
 
-where :math:`|\cdot|` is the :math:`\ell_0` norm or the cardinality of the set.
+where :math:`|\cdot|` computes the cardinality of the set (i.e., the number of
+elements in the set) and :math:`||\cdot||_0` is the :math:`\ell_0` "norm"
+(which computes the number of nonzero elements in a vector).
 
 Here is a small example of usage of this function::
 
diff --git a/doc/modules/preprocessing.rst b/doc/modules/preprocessing.rst
index 9e925b6a562a..809179f3f974 100644
--- a/doc/modules/preprocessing.rst
+++ b/doc/modules/preprocessing.rst
@@ -569,9 +569,9 @@ Continuing the example above::
 
   >>> enc = preprocessing.UnaryEncoder()
   >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]) # doctest: +ELLIPSIS
-  UnaryEncoder(dtype=<... 'numpy.float64'>, handle_unknown='error',
-         n_values='auto', ordinal_features='all', sparse=True)
-  >>> enc.transform([[0, 1, 1]]).toarray()
+  UnaryEncoder(dtype=<... 'numpy.float64'>, handle_greater='error',
+         n_values='auto', ordinal_features='all', sparse=False)
+  >>> enc.transform([[0, 1, 1]])
   array([[ 0.,  1.,  0.,  1.,  0.,  0.]])
 
 By default, how many values each feature can take is inferred automatically
@@ -590,9 +590,9 @@ categorical features, one has to explicitly set ``n_values``. For example,::
   >>> # Note that there are missing categorical values for the 2nd and 3rd
   >>> # features
   >>> enc.fit([[1, 2, 3], [0, 2, 0]])  # doctest: +ELLIPSIS
-  UnaryEncoder(dtype=<... 'numpy.float64'>, handle_unknown='error',
-         n_values=[2, 3, 4], ordinal_features='all', sparse=True)
-  >>> enc.transform([[1, 1, 2]]).toarray()
+  UnaryEncoder(dtype=<... 'numpy.float64'>, handle_greater='error',
+         n_values=[2, 3, 4], ordinal_features='all', sparse=False)
+  >>> enc.transform([[1, 1, 2]])
   array([[ 1.,  1.,  0.,  1.,  1.,  0.]])
 
 .. _imputation:
diff --git a/doc/whats_new/_contributors.rst b/doc/whats_new/_contributors.rst
index fd12247043e0..3dbcac534058 100644
--- a/doc/whats_new/_contributors.rst
+++ b/doc/whats_new/_contributors.rst
@@ -151,3 +151,5 @@
 .. _Arthur Mensch: https://amensch.fr
 
 .. _Joris Van den Bossche: https://github.com/jorisvandenbossche
+
+.. _Roman Yurchak: https://github.com/rth
diff --git a/doc/whats_new/v0.20.rst b/doc/whats_new/v0.20.rst
index e9c2254bc222..afd3ed299585 100644
--- a/doc/whats_new/v0.20.rst
+++ b/doc/whats_new/v0.20.rst
@@ -33,6 +33,8 @@ cannot assure that this list is complete.)
 Changelog
 ---------
 
+Support for Python 3.3 has been officially dropped.
+
 New features
 ............
 
@@ -146,6 +148,13 @@ Classifiers and regressors
   updated estimates for the standard deviation and the coefficients.
   :issue:`10153` by :user:`Jörg Döpfert <jdoepfert>`.
 
+- Fixed a bug when fitting :class:`ensemble.GradientBoostingClassifier` or
+  :class:`ensemble.GradientBoostingRegressor` with ``warm_start=True`` which
+  previously raised a segmentation fault due to a non-conversion of CSC matrix
+  into CSR format expected by ``decision_function``. Similarly, Fortran-ordered
+  arrays are converted to C-ordered arrays in the dense case. :issue:`9991` by
+  :user:`Guillaume Lemaitre <glemaitre>`.
+
 Decomposition, manifold learning and clustering
 
 - Fix for uninformative error in :class:`decomposition.IncrementalPCA`:
@@ -180,6 +189,11 @@ Decomposition, manifold learning and clustering
 - Fixed a bug when setting parameters on meta-estimator, involving both a
   wrapped estimator and its parameter. :issue:`9999` by :user:`Marcus Voss
   <marcus-voss>` and `Joel Nothman`_.
+  
+- ``k_means`` now gives a warning, if the number of distinct clusters found
+  is smaller than ``n_clusters``. This may occur when the number of distinct 
+  points in the data set is actually smaller than the number of cluster one is 
+  looking for. :issue:`10059` by :user:`Christian Braune <christianbraune79>`.
 
 - Fixed a bug in :func:`datasets.make_circles`, where no odd number of data
   points could be generated. :issue:`10037` by :user:`Christian Braune
@@ -209,8 +223,16 @@ Feature Extraction
   throw an exception if ``max_patches`` was greater than or equal to the number
   of all possible patches rather than simply returning the number of possible
   patches. :issue:`10100` by :user:`Varun Agrawal <varunagrawal>`
+  
+- Fixed a bug in :class:`feature_extraction.text.CountVectorizer`,
+  :class:`feature_extraction.text.TfidfVectorizer`,
+  :class:`feature_extraction.text.HashingVectorizer` to support 64 bit sparse
+  array indexing necessary to process large datasets with more than 2·10⁹ tokens
+  (words or n-grams). :issue:`9147` by :user:`Claes-Fredrik Mannby <mannby>`
+  and `Roman Yurchak`_.
 
 	
+
 API changes summary
 -------------------
 
diff --git a/sklearn/cluster/k_means_.py b/sklearn/cluster/k_means_.py
index 0da014417270..fd31d5f84d94 100644
--- a/sklearn/cluster/k_means_.py
+++ b/sklearn/cluster/k_means_.py
@@ -31,7 +31,7 @@
 from ..externals.joblib import Parallel
 from ..externals.joblib import delayed
 from ..externals.six import string_types
-
+from ..exceptions import ConvergenceWarning
 from . import _k_means
 from ._k_means_elkan import k_means_elkan
 
@@ -374,6 +374,13 @@ def k_means(X, n_clusters, init='k-means++', precompute_distances='auto',
             X += X_mean
         best_centers += X_mean
 
+    distinct_clusters = len(set(best_labels))
+    if distinct_clusters < n_clusters:
+        warnings.warn("Number of distinct clusters ({}) found smaller than "
+                      "n_clusters ({}). Possibly due to duplicate points "
+                      "in X.".format(distinct_clusters, n_clusters),
+                      ConvergenceWarning, stacklevel=2)
+
     if return_n_iter:
         return best_centers, best_labels, best_inertia, best_n_iter
     else:
diff --git a/sklearn/cluster/tests/test_k_means.py b/sklearn/cluster/tests/test_k_means.py
index 080a31ba52f9..f0f2b56bd591 100644
--- a/sklearn/cluster/tests/test_k_means.py
+++ b/sklearn/cluster/tests/test_k_means.py
@@ -15,9 +15,10 @@
 from sklearn.utils.testing import assert_greater
 from sklearn.utils.testing import assert_less
 from sklearn.utils.testing import assert_warns
+from sklearn.utils.testing import assert_warns_message
 from sklearn.utils.testing import if_safe_multiprocessing_with_blas
 from sklearn.utils.testing import assert_raise_message
-
+from sklearn.exceptions import ConvergenceWarning
 
 from sklearn.utils.extmath import row_norms
 from sklearn.metrics.cluster import v_measure_score
@@ -867,3 +868,22 @@ def test_sparse_validate_centers():
     msg = "The shape of the initial centers \(\(4L?, 4L?\)\) " \
           "does not match the number of clusters 3"
     assert_raises_regex(ValueError, msg, classifier.fit, X)
+
+
+def test_less_centers_than_unique_points():
+    X = np.asarray([[0, 0],
+                    [0, 1],
+                    [1, 0],
+                    [1, 0]])  # last point is duplicated
+
+    km = KMeans(n_clusters=4).fit(X)
+
+    # only three distinct points, so only three clusters
+    # can have points assigned to them
+    assert_equal(set(km.labels_), set(range(3)))
+
+    # k_means should warn that fewer labels than cluster
+    # centers have been used
+    msg = ("Number of distinct clusters (3) found smaller than "
+           "n_clusters (4). Possibly due to duplicate points in X.")
+    assert_warns_message(ConvergenceWarning, msg, k_means, X, n_clusters=4)
diff --git a/sklearn/ensemble/_gradient_boosting.pyx b/sklearn/ensemble/_gradient_boosting.pyx
index 71371f5c24a4..cd749eae8e9b 100644
--- a/sklearn/ensemble/_gradient_boosting.pyx
+++ b/sklearn/ensemble/_gradient_boosting.pyx
@@ -203,11 +203,14 @@ def predict_stages(np.ndarray[object, ndim=2] estimators,
     cdef Tree tree
 
     if issparse(X):
+        if X.format != 'csr':
+            raise ValueError("When X is a sparse matrix, a CSR format is"
+                             " expected, got {!r}".format(type(X)))
         _predict_regression_tree_stages_sparse(estimators, X, scale, out)
     else:
-        if not isinstance(X, np.ndarray):
-            raise ValueError("X should be in np.ndarray or csr_matrix format,"
-                             "got %s" % type(X))
+        if not isinstance(X, np.ndarray) or np.isfortran(X):
+            raise ValueError("X should be C-ordered np.ndarray,"
+                             " got {}".format(type(X)))
 
         for i in range(n_estimators):
             for k in range(K):
diff --git a/sklearn/ensemble/gradient_boosting.py b/sklearn/ensemble/gradient_boosting.py
index 31a82b9ce285..c82d47c9bb34 100644
--- a/sklearn/ensemble/gradient_boosting.py
+++ b/sklearn/ensemble/gradient_boosting.py
@@ -887,6 +887,11 @@ def _check_params(self):
                              "integer. %r was passed"
                              % self.n_iter_no_change)
 
+        allowed_presort = ('auto', True, False)
+        if self.presort not in allowed_presort:
+            raise ValueError("'presort' should be in {}. Got {!r} instead."
+                             .format(allowed_presort, self.presort))
+
     def _init_state(self):
         """Initialize model state and allocate model state data structures. """
 
@@ -1034,24 +1039,27 @@ def fit(self, X, y, sample_weight=None, monitor=None):
                                  % (self.n_estimators,
                                     self.estimators_.shape[0]))
             begin_at_stage = self.estimators_.shape[0]
+            # The requirements of _decision_function (called in two lines
+            # below) are more constrained than fit. It accepts only CSR
+            # matrices.
+            X = check_array(X, dtype=DTYPE, order="C", accept_sparse='csr')
             y_pred = self._decision_function(X)
             self._resize_state()
 
-        X_idx_sorted = None
+        if self.presort is True and issparse(X):
+            raise ValueError(
+                "Presorting is not supported for sparse matrices.")
+
         presort = self.presort
         # Allow presort to be 'auto', which means True if the dataset is dense,
         # otherwise it will be False.
-        if presort == 'auto' and issparse(X):
-            presort = False
-        elif presort == 'auto':
-            presort = True
-
-        if presort == True:
-            if issparse(X):
-                raise ValueError("Presorting is not supported for sparse matrices.")
-            else:
-                X_idx_sorted = np.asfortranarray(np.argsort(X, axis=0),
-                                                 dtype=np.int32)
+        if presort == 'auto':
+            presort = not issparse(X)
+
+        X_idx_sorted = None
+        if presort:
+            X_idx_sorted = np.asfortranarray(np.argsort(X, axis=0),
+                                             dtype=np.int32)
 
         # fit the boosting stages
         n_stages = self._fit_stages(X, y, y_pred, sample_weight, self._rng,
diff --git a/sklearn/ensemble/tests/test_gradient_boosting.py b/sklearn/ensemble/tests/test_gradient_boosting.py
index f4594529e034..4c59b33e675e 100644
--- a/sklearn/ensemble/tests/test_gradient_boosting.py
+++ b/sklearn/ensemble/tests/test_gradient_boosting.py
@@ -16,6 +16,7 @@
 from sklearn.ensemble import GradientBoostingClassifier
 from sklearn.ensemble import GradientBoostingRegressor
 from sklearn.ensemble.gradient_boosting import ZeroEstimator
+from sklearn.ensemble._gradient_boosting import predict_stages
 from sklearn.metrics import mean_squared_error
 from sklearn.model_selection import train_test_split
 from sklearn.utils import check_random_state, tosequence
@@ -82,7 +83,6 @@ def test_classification_toy():
 
 def test_classifier_parameter_checks():
     # Check input parameter validation for GradientBoostingClassifier.
-
     assert_raises(ValueError,
                   GradientBoostingClassifier(n_estimators=0).fit, X, y)
     assert_raises(ValueError,
@@ -140,6 +140,13 @@ def test_classifier_parameter_checks():
                       loss='deviance').fit(X, y),
                   X, [0, 0, 0, 0])
 
+    allowed_presort = ('auto', True, False)
+    assert_raise_message(ValueError,
+                         "'presort' should be in {}. "
+                         "Got 'invalid' instead.".format(allowed_presort),
+                         GradientBoostingClassifier(presort='invalid')
+                         .fit, X, y)
+
 
 def test_regressor_parameter_checks():
     # Check input parameter validation for GradientBoostingRegressor
@@ -158,6 +165,12 @@ def test_regressor_parameter_checks():
                          " or an integer. 'invalid' was passed",
                          GradientBoostingRegressor(n_iter_no_change='invalid')
                          .fit, X, y)
+    allowed_presort = ('auto', True, False)
+    assert_raise_message(ValueError,
+                         "'presort' should be in {}. "
+                         "Got 'invalid' instead.".format(allowed_presort),
+                         GradientBoostingRegressor(presort='invalid')
+                         .fit, X, y)
 
 
 def test_loss_function():
@@ -379,6 +392,25 @@ def test_check_inputs_predict():
     assert_raises(ValueError, clf.predict, x)
 
 
+def test_check_inputs_predict_stages():
+    # check that predict_stages through an error if the type of X is not
+    # supported
+    x, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)
+    x_sparse_csc = csc_matrix(x)
+    clf = GradientBoostingClassifier(n_estimators=100, random_state=1)
+    clf.fit(x, y)
+    score = np.zeros((y.shape)).reshape(-1, 1)
+    assert_raise_message(ValueError,
+                         "When X is a sparse matrix, a CSR format is expected",
+                         predict_stages, clf.estimators_, x_sparse_csc,
+                         clf.learning_rate, score)
+    x_fortran = np.asfortranarray(x)
+    assert_raise_message(ValueError,
+                         "X should be C-ordered np.ndarray",
+                         predict_stages, clf.estimators_, x_fortran,
+                         clf.learning_rate, score)
+
+
 def test_check_max_features():
     # test if max_features is valid.
     clf = GradientBoostingRegressor(n_estimators=100, random_state=1,
@@ -849,6 +881,56 @@ def test_warm_start_oob():
                                   est.oob_improvement_[:100])
 
 
+def test_warm_start_sparse():
+    # Test that all sparse matrix types are supported
+    X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)
+    sparse_matrix_type = [csr_matrix, csc_matrix, coo_matrix]
+    for Cls in [GradientBoostingRegressor, GradientBoostingClassifier]:
+        est_dense = Cls(n_estimators=100, max_depth=1, subsample=0.5,
+                        random_state=1, warm_start=True)
+        est_dense.fit(X, y)
+        est_dense.predict(X)
+        est_dense.set_params(n_estimators=200)
+        est_dense.fit(X, y)
+        y_pred_dense = est_dense.predict(X)
+
+        for sparse_constructor in sparse_matrix_type:
+            X_sparse = sparse_constructor(X)
+
+            est_sparse = Cls(n_estimators=100, max_depth=1, subsample=0.5,
+                             random_state=1, warm_start=True)
+            est_sparse.fit(X_sparse, y)
+            est_sparse.predict(X)
+            est_sparse.set_params(n_estimators=200)
+            est_sparse.fit(X_sparse, y)
+            y_pred_sparse = est_sparse.predict(X)
+
+            assert_array_almost_equal(est_dense.oob_improvement_[:100],
+                                      est_sparse.oob_improvement_[:100])
+            assert_array_almost_equal(y_pred_dense, y_pred_sparse)
+
+
+def test_warm_start_fortran():
+    # Test that feeding a X in Fortran-ordered is giving the same results as
+    # in C-ordered
+    X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)
+    for Cls in [GradientBoostingRegressor, GradientBoostingClassifier]:
+        est_c = Cls(n_estimators=1, random_state=1, warm_start=True)
+        est_fortran = Cls(n_estimators=1, random_state=1, warm_start=True)
+
+        est_c.fit(X, y)
+        est_c.set_params(n_estimators=11)
+        est_c.fit(X, y)
+
+        X_fortran = np.asfortranarray(X)
+        est_fortran.fit(X_fortran, y)
+        est_fortran.set_params(n_estimators=11)
+        est_fortran.fit(X_fortran, y)
+
+        assert_array_almost_equal(est_c.predict(X),
+                                  est_fortran.predict(X))
+
+
 def early_stopping_monitor(i, est, locals):
     """Returns True on the 10th iteration. """
     if i == 9:
diff --git a/sklearn/feature_extraction/_hashing.pyx b/sklearn/feature_extraction/_hashing.pyx
index e39aeafa0868..c462dd8a2471 100644
--- a/sklearn/feature_extraction/_hashing.pyx
+++ b/sklearn/feature_extraction/_hashing.pyx
@@ -1,6 +1,7 @@
 # Author: Lars Buitinck
 # License: BSD 3 clause
 
+import sys
 import array
 from cpython cimport array
 cimport cython
@@ -9,6 +10,7 @@ cimport numpy as np
 import numpy as np
 
 from sklearn.utils.murmurhash cimport murmurhash3_bytes_s32
+from sklearn.utils.fixes import sp_version
 
 np.import_array()
 
@@ -33,12 +35,20 @@ def transform(raw_X, Py_ssize_t n_features, dtype, bint alternate_sign=1):
     cdef array.array indices
     cdef array.array indptr
     indices = array.array("i")
-    indptr = array.array("i", [0])
+    if sys.version_info >= (3, 3):
+        indices_array_dtype = "q"
+        indices_np_dtype = np.longlong
+    else:
+        # On Windows with PY2.7 long int would still correspond to 32 bit. 
+        indices_array_dtype = "l"
+        indices_np_dtype = np.int_
+
+    indptr = array.array(indices_array_dtype, [0])
 
     # Since Python array does not understand Numpy dtypes, we grow the indices
     # and values arrays ourselves. Use a Py_ssize_t capacity for safety.
     cdef Py_ssize_t capacity = 8192     # arbitrary
-    cdef np.int32_t size = 0
+    cdef np.int64_t size = 0
     cdef np.ndarray values = np.empty(capacity, dtype=dtype)
 
     for x in raw_X:
@@ -79,4 +89,18 @@ def transform(raw_X, Py_ssize_t n_features, dtype, bint alternate_sign=1):
         indptr[len(indptr) - 1] = size
 
     indices_a = np.frombuffer(indices, dtype=np.int32)
-    return (indices_a, np.frombuffer(indptr, dtype=np.int32), values[:size])
+    indptr_a = np.frombuffer(indptr, dtype=indices_np_dtype)
+
+    if indptr[-1] > 2147483648:  # = 2**31
+        if sp_version < (0, 14):
+            raise ValueError(('sparse CSR array has {} non-zero '
+                              'elements and requires 64 bit indexing, '
+                              ' which is unsupported with scipy {}. '
+                              'Please upgrade to scipy >=0.14')
+                             .format(indptr[-1], '.'.join(sp_version)))
+        # both indices and indptr have the same dtype in CSR arrays
+        indices_a = indices_a.astype(np.int64)
+    else:
+        indptr_a = indptr_a.astype(np.int32)
+
+    return (indices_a, indptr_a, values[:size])
diff --git a/sklearn/feature_extraction/text.py b/sklearn/feature_extraction/text.py
index 417aeef2f8bc..a1e0845abe9a 100644
--- a/sklearn/feature_extraction/text.py
+++ b/sklearn/feature_extraction/text.py
@@ -30,6 +30,7 @@
 from .hashing import FeatureHasher
 from .stop_words import ENGLISH_STOP_WORDS
 from ..utils.validation import check_is_fitted
+from ..utils.fixes import sp_version
 
 __all__ = ['CountVectorizer',
            'ENGLISH_STOP_WORDS',
@@ -784,7 +785,8 @@ def _count_vocab(self, raw_documents, fixed_vocab):
 
         analyze = self.build_analyzer()
         j_indices = []
-        indptr = _make_int_array()
+        indptr = []
+
         values = _make_int_array()
         indptr.append(0)
         for doc in raw_documents:
@@ -811,8 +813,20 @@ def _count_vocab(self, raw_documents, fixed_vocab):
                 raise ValueError("empty vocabulary; perhaps the documents only"
                                  " contain stop words")
 
-        j_indices = np.asarray(j_indices, dtype=np.intc)
-        indptr = np.frombuffer(indptr, dtype=np.intc)
+        if indptr[-1] > 2147483648:  # = 2**31 - 1
+            if sp_version >= (0, 14):
+                indices_dtype = np.int64
+            else:
+                raise ValueError(('sparse CSR array has {} non-zero '
+                                  'elements and requires 64 bit indexing, '
+                                  ' which is unsupported with scipy {}. '
+                                  'Please upgrade to scipy >=0.14')
+                                 .format(indptr[-1], '.'.join(sp_version)))
+
+        else:
+            indices_dtype = np.int32
+        j_indices = np.asarray(j_indices, dtype=indices_dtype)
+        indptr = np.asarray(indptr, dtype=indices_dtype)
         values = np.frombuffer(values, dtype=np.intc)
 
         X = sp.csr_matrix((values, j_indices, indptr),
diff --git a/sklearn/linear_model/passive_aggressive.py b/sklearn/linear_model/passive_aggressive.py
index 9c8d111371f7..37eb5b2ca58c 100644
--- a/sklearn/linear_model/passive_aggressive.py
+++ b/sklearn/linear_model/passive_aggressive.py
@@ -63,6 +63,10 @@ class PassiveAggressiveClassifier(BaseSGDClassifier):
         When set to True, reuse the solution of the previous call to fit as
         initialization, otherwise, just erase the previous solution.
 
+        Repeatedly calling fit or partial_fit when warm_start is True can
+        result in a different solution than when calling fit a single time
+        because of the way the data is shuffled.
+
     class_weight : dict, {class_label: weight} or "balanced" or None, optional
         Preset for the class_weight fit parameter.
 
@@ -282,6 +286,10 @@ class PassiveAggressiveRegressor(BaseSGDRegressor):
         When set to True, reuse the solution of the previous call to fit as
         initialization, otherwise, just erase the previous solution.
 
+        Repeatedly calling fit or partial_fit when warm_start is True can
+        result in a different solution than when calling fit a single time
+        because of the way the data is shuffled.
+
     average : bool or int, optional
         When set to True, computes the averaged SGD weights and stores the
         result in the ``coef_`` attribute. If set to an int greater than 1,
diff --git a/sklearn/linear_model/stochastic_gradient.py b/sklearn/linear_model/stochastic_gradient.py
index 1cd37b953060..3fdf17763a5c 100644
--- a/sklearn/linear_model/stochastic_gradient.py
+++ b/sklearn/linear_model/stochastic_gradient.py
@@ -724,6 +724,14 @@ class SGDClassifier(BaseSGDClassifier):
         When set to True, reuse the solution of the previous call to fit as
         initialization, otherwise, just erase the previous solution.
 
+        Repeatedly calling fit or partial_fit when warm_start is True can
+        result in a different solution than when calling fit a single time
+        because of the way the data is shuffled.
+        If a dynamic learning rate is used, the learning rate is adapted
+        depending on the number of samples already seen. Calling ``fit`` resets
+        this counter, while ``partial_fit`` will result in increasing the
+        existing counter.
+
     average : bool or int, optional
         When set to True, computes the averaged SGD weights and stores the
         result in the ``coef_`` attribute. If set to an int greater than 1,
@@ -1273,6 +1281,14 @@ class SGDRegressor(BaseSGDRegressor):
         When set to True, reuse the solution of the previous call to fit as
         initialization, otherwise, just erase the previous solution.
 
+        Repeatedly calling fit or partial_fit when warm_start is True can
+        result in a different solution than when calling fit a single time
+        because of the way the data is shuffled.
+        If a dynamic learning rate is used, the learning rate is adapted
+        depending on the number of samples already seen. Calling ``fit`` resets
+        this counter, while ``partial_fit``  will result in increasing the
+        existing counter.
+
     average : bool or int, optional
         When set to True, computes the averaged SGD weights and stores the
         result in the ``coef_`` attribute. If set to an int greater than 1,
diff --git a/sklearn/tree/tests/test_tree.py b/sklearn/tree/tests/test_tree.py
index ec8eb263c6d5..0e49dfae3ba0 100644
--- a/sklearn/tree/tests/test_tree.py
+++ b/sklearn/tree/tests/test_tree.py
@@ -31,6 +31,7 @@
 from sklearn.utils.testing import assert_warns
 from sklearn.utils.testing import assert_warns_message
 from sklearn.utils.testing import ignore_warnings
+from sklearn.utils.testing import assert_raise_message
 
 from sklearn.utils.validation import check_random_state
 
@@ -502,7 +503,6 @@ def test_error():
         assert_raises(ValueError, est.predict_proba, X2)
 
     for name, TreeEstimator in ALL_TREES.items():
-        # Invalid values for parameters
         assert_raises(ValueError, TreeEstimator(min_samples_leaf=-1).fit, X, y)
         assert_raises(ValueError, TreeEstimator(min_samples_leaf=.6).fit, X, y)
         assert_raises(ValueError, TreeEstimator(min_samples_leaf=0.).fit, X, y)
@@ -1624,6 +1624,18 @@ def test_presort_sparse():
         yield check_presort_sparse, est, sparse_matrix(X), y
 
 
+def test_invalid_presort():
+    classes = (DecisionTreeRegressor, DecisionTreeClassifier)
+    allowed_presort = ('auto', True, False)
+    invalid_presort = 'invalid'
+    msg = ("'presort' should be in {}. "
+           "Got {!r} instead.".format(allowed_presort, invalid_presort))
+    for cls in classes:
+        est = cls(presort=invalid_presort)
+        assert_raise_message(ValueError, msg,
+                             est.fit, X, y)
+
+
 def test_decision_path_hardcoded():
     X = iris.data
     y = iris.target
diff --git a/sklearn/tree/tree.py b/sklearn/tree/tree.py
index 789ffb8b61ca..50b81dfdfda0 100644
--- a/sklearn/tree/tree.py
+++ b/sklearn/tree/tree.py
@@ -292,18 +292,21 @@ def fit(self, X, y, sample_weight=None, check_input=True,
             raise ValueError("min_impurity_decrease must be greater than "
                              "or equal to 0")
 
-        presort = self.presort
-        # Allow presort to be 'auto', which means True if the dataset is dense,
-        # otherwise it will be False.
-        if self.presort == 'auto' and issparse(X):
-            presort = False
-        elif self.presort == 'auto':
-            presort = True
+        allowed_presort = ('auto', True, False)
+        if self.presort not in allowed_presort:
+            raise ValueError("'presort' should be in {}. Got {!r} instead."
+                             .format(allowed_presort, self.presort))
 
-        if presort is True and issparse(X):
+        if self.presort is True and issparse(X):
             raise ValueError("Presorting is not supported for sparse "
                              "matrices.")
 
+        presort = self.presort
+        # Allow presort to be 'auto', which means True if the dataset is dense,
+        # otherwise it will be False.
+        if self.presort == 'auto':
+            presort = not issparse(X)
+
         # If multiple trees are built on the same dataset, we only want to
         # presort once. Splitters now can accept presorted indices if desired,
         # but do not handle any presorting themselves. Ensemble algorithms
