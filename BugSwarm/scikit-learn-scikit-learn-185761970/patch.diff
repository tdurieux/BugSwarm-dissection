diff --git a/doc/whats_new.rst b/doc/whats_new.rst
index e6f52aed34..d78dd9e210 100755
--- a/doc/whats_new.rst
+++ b/doc/whats_new.rst
@@ -41,6 +41,11 @@ New features
      Kullback-Leibler divergence and the Itakura-Saito divergence.
      By `Tom Dupre la Tour`_.
 
+   - Added :func:`metrics.mean_squared_log_error`, which computes 
+     the mean square error of the logarithmic transformation of targets, 
+     particularly useful for targets with an exponential trend.
+     :issue:`7655` by :user:`Karan Desai <karandesai-96>`.
+
 Enhancements
 ............
 
@@ -98,7 +103,7 @@ Enhancements
    - Added ability to set ``n_jobs`` parameter to :func:`pipeline.make_union`.
      A ``TypeError`` will be raised for any other kwargs. :issue:`8028`
      by :user:`Alexander Booth <alexandercbooth>`.
-     
+
    - Added type checking to the ``accept_sparse`` parameter in
      :mod:`sklearn.utils.validation` methods. This parameter now accepts only
      boolean, string, or list/tuple of strings. ``accept_sparse=None`` is deprecated
@@ -140,12 +145,24 @@ Bug fixes
      where the ``perplexity`` method was returning incorrect results because
      the ``transform`` method returns normalized document topic distributions
      as of version 0.18. :issue:`7954` by :user:`Gary Foreman <garyForeman>`.
-     
+
    - Fix a bug where :class:`sklearn.ensemble.GradientBoostingClassifier` and
      :class:`sklearn.ensemble.GradientBoostingRegressor` ignored the
      ``min_impurity_split`` parameter.
      :issue:`8006` by :user:`Sebastian PÃ¶lsterl <sebp>`.
 
+   - Fix a bug where
+     :class:`sklearn.ensemble.gradient_boosting.QuantileLossFunction` computed
+     negative errors for negative values of ``ytrue - ypred`` leading to
+     wrong values when calling ``__call__``.
+     :issue:`8087` by :user:`Alexis Mignon <AlexisMignon>`
+
+   - Fix :func:`sklearn.multioutput.MultiOutputClassifier.predict_proba` to
+     return a list of 2d arrays, rather than a 3d array. In the case where
+     different target columns had different numbers of classes, a `ValueError`
+     would be raised on trying to stack matrices with different dimensions.
+     :issue:`8093` by :user:`Peter Bull <pjbull>`.
+
 API changes summary
 -------------------
 
@@ -154,13 +171,22 @@ API changes summary
      ensemble estimators (deriving from :class:`ensemble.BaseEnsemble`)
      now only have ``self.estimators_`` available after ``fit``.
      :issue:`7464` by `Lars Buitinck`_ and `Loic Esteve`_.
-     
+
    - Deprecate the ``doc_topic_distr`` argument of the ``perplexity`` method
      in :class:`sklearn.decomposition.LatentDirichletAllocation` because the
      user no longer has access to the unnormalized document topic distribution
      needed for the perplexity calculation. :issue:`7954` by
      :user:`Gary Foreman <garyForeman>`.
 
+   - The :func:`sklearn.multioutput.MultiOutputClassifier.predict_proba`
+     function used to return a 3d array (``n_samples``, ``n_classes``,
+     ``n_outputs``). In the case where different target columns had different
+     numbers of classes, a `ValueError` would be raised on trying to stack
+     matrices with different dimensions. This function now returns a list of
+     arrays where the length of the list is ``n_outputs``, and each array is
+     (``n_samples``, ``n_classes``) for that particular output.
+     :issue:`8093` by :user:`Peter Bull <pjbull>`.
+
 .. _changes_0_18_1:
 
 Version 0.18.1
diff --git a/sklearn/ensemble/gradient_boosting.py b/sklearn/ensemble/gradient_boosting.py
index a337ee9891..37ba1ccf92 100755
--- a/sklearn/ensemble/gradient_boosting.py
+++ b/sklearn/ensemble/gradient_boosting.py
@@ -418,10 +418,10 @@ def __call__(self, y, pred, sample_weight=None):
 
         mask = y > pred
         if sample_weight is None:
-            loss = (alpha * diff[mask].sum() +
+            loss = (alpha * diff[mask].sum() -
                     (1.0 - alpha) * diff[~mask].sum()) / y.shape[0]
         else:
-            loss = ((alpha * np.sum(sample_weight[mask] * diff[mask]) +
+            loss = ((alpha * np.sum(sample_weight[mask] * diff[mask]) -
                     (1.0 - alpha) * np.sum(sample_weight[~mask] * diff[~mask])) /
                     sample_weight.sum())
         return loss
diff --git a/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py b/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py
index ead2c29b56..244872175b 100755
--- a/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py
+++ b/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py
@@ -15,6 +15,7 @@
 from sklearn.ensemble.gradient_boosting import RegressionLossFunction
 from sklearn.ensemble.gradient_boosting import LOSS_FUNCTIONS
 from sklearn.ensemble.gradient_boosting import _weighted_percentile
+from sklearn.ensemble.gradient_boosting import QuantileLossFunction
 
 
 def test_binomial_deviance():
@@ -141,6 +142,16 @@ def test_weighted_percentile_zero_weight():
     assert score == 1.0
 
 
+def test_quantile_loss_function():
+    # Non regression test for the QuantileLossFunction object
+    # There was a sign problem when evaluating the function
+    # for negative values of 'ytrue - ypred'
+    x = np.asarray([-1.0, 0.0, 1.0])
+    y_found = QuantileLossFunction(1, 0.9)(x, np.zeros_like(x))
+    y_expected = np.asarray([0.1, 0.0, 0.9]).mean()
+    np.testing.assert_allclose(y_found, y_expected)
+
+
 def test_sample_weight_deviance():
     # Test if deviance supports sample weights.
     rng = check_random_state(13)
diff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py
index 708715a192..565acba536 100755
--- a/sklearn/linear_model/logistic.py
+++ b/sklearn/linear_model/logistic.py
@@ -1369,10 +1369,13 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,
         l2 penalty with liblinear solver. Prefer dual=False when
         n_samples > n_features.
 
-    scoring : callabale
-        Scoring function to use as cross-validation criteria. For a list of
-        scoring functions that can be used, look at :mod:`sklearn.metrics`.
-        The default scoring option used is accuracy_score.
+    scoring : string, callable, or None
+        A string (see model evaluation documentation) or
+        a scorer callable object / function with signature
+        ``scorer(estimator, X, y)``. For a list of scoring functions
+        that can be used, look at :mod:`sklearn.metrics`. The
+        default scoring option used is accuracy_score.
+
 
     solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag'}
         Algorithm to use in the optimization problem.
diff --git a/sklearn/multioutput.py b/sklearn/multioutput.py
index e650bff25b..001bda251b 100755
--- a/sklearn/multioutput.py
+++ b/sklearn/multioutput.py
@@ -214,16 +214,18 @@ def predict_proba(self, X):
 
         Returns
         -------
-        T : (sparse) array-like, shape = (n_samples, n_classes, n_outputs)
-            The class probabilities of the samples for each of the outputs
+        p : array of shape = [n_samples, n_classes], or a list of n_outputs \
+            such arrays if n_outputs > 1.
+            The class probabilities of the input samples. The order of the
+            classes corresponds to that in the attribute `classes_`.
         """
         check_is_fitted(self, 'estimators_')
         if not hasattr(self.estimator, "predict_proba"):
             raise ValueError("The base estimator should implement"
                              "predict_proba method")
 
-        results = np.dstack([estimator.predict_proba(X) for estimator in
-                            self.estimators_])
+        results = [estimator.predict_proba(X) for estimator in
+                   self.estimators_]
         return results
 
     def score(self, X, y):
diff --git a/sklearn/tests/test_multioutput.py b/sklearn/tests/test_multioutput.py
index ea9575429f..208477c9cf 100755
--- a/sklearn/tests/test_multioutput.py
+++ b/sklearn/tests/test_multioutput.py
@@ -10,7 +10,7 @@
 from sklearn import datasets
 from sklearn.base import clone
 from sklearn.ensemble import GradientBoostingRegressor, RandomForestClassifier
-from sklearn.linear_model import Lasso
+from sklearn.linear_model import Lasso, LogisticRegression
 from sklearn.svm import LinearSVC
 from sklearn.multiclass import OneVsRestClassifier
 from sklearn.multioutput import MultiOutputRegressor, MultiOutputClassifier
@@ -118,9 +118,13 @@ def test_multi_output_classification():
     assert_equal((n_samples, n_outputs), predictions.shape)
 
     predict_proba = multi_target_forest.predict_proba(X)
-    assert_equal((n_samples, n_classes, n_outputs), predict_proba.shape)
 
-    assert_array_equal(np.argmax(predict_proba, axis=1), predictions)
+    assert len(predict_proba) == n_outputs
+    for class_probabilities in predict_proba:
+        assert_equal((n_samples, n_classes), class_probabilities.shape)
+
+    assert_array_equal(np.argmax(np.dstack(predict_proba), axis=1),
+                       predictions)
 
     # train the forest with each column and assert that predictions are equal
     for i in range(3):
@@ -128,7 +132,7 @@ def test_multi_output_classification():
         forest_.fit(X, y[:, i])
         assert_equal(list(forest_.predict(X)), list(predictions[:, i]))
         assert_array_equal(list(forest_.predict_proba(X)),
-                           list(predict_proba[:, :, i]))
+                           list(predict_proba[i]))
 
 
 def test_multiclass_multioutput_estimator():
@@ -150,6 +154,41 @@ def test_multiclass_multioutput_estimator():
                      list(predictions[:, i]))
 
 
+def test_multiclass_multioutput_estimator_predict_proba():
+    seed = 542
+
+    # make test deterministic
+    rng = np.random.RandomState(seed)
+
+    # random features
+    X = rng.normal(size=(5, 5))
+
+    # random labels
+    y1 = np.array(['b', 'a', 'a', 'b', 'a']).reshape(5, 1)  # 2 classes
+    y2 = np.array(['d', 'e', 'f', 'e', 'd']).reshape(5, 1)  # 3 classes
+
+    Y = np.concatenate([y1, y2], axis=1)
+
+    clf = MultiOutputClassifier(LogisticRegression(random_state=seed))
+
+    clf.fit(X, Y)
+
+    y_result = clf.predict_proba(X)
+    y_actual = [np.array([[0.23481764, 0.76518236],
+                          [0.67196072, 0.32803928],
+                          [0.54681448, 0.45318552],
+                          [0.34883923, 0.65116077],
+                          [0.73687069, 0.26312931]]),
+                np.array([[0.5171785, 0.23878628, 0.24403522],
+                          [0.22141451, 0.64102704, 0.13755846],
+                          [0.16751315, 0.18256843, 0.64991843],
+                          [0.27357372, 0.55201592, 0.17441036],
+                          [0.65745193, 0.26062899, 0.08191907]])]
+
+    for i in range(len(y_actual)):
+        assert_almost_equal(y_result[i], y_actual[i])
+
+
 def test_multi_output_classification_sample_weights():
     # weighted classifier
     Xw = [[1, 2, 3], [4, 5, 6]]
diff --git a/sklearn/tree/_tree.pyx b/sklearn/tree/_tree.pyx
index e4fa4dca02..f61679641e 100755
--- a/sklearn/tree/_tree.pyx
+++ b/sklearn/tree/_tree.pyx
@@ -676,7 +676,7 @@ cdef class Tree:
                 capacity = 2 * self.capacity
 
         safe_realloc(&self.nodes, capacity)
-        safe_realloc(&self.value, capacity)
+        safe_realloc(&self.value, capacity * self.value_stride)
 
         # value memory is initialised to 0 to enable classifier argmax
         if capacity > self.capacity:
