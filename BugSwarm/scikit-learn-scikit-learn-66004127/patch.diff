diff --git a/doc/modules/preprocessing.rst b/doc/modules/preprocessing.rst
index a25fd9fb49..d7ed663b51 100755
--- a/doc/modules/preprocessing.rst
+++ b/doc/modules/preprocessing.rst
@@ -426,3 +426,44 @@ values than observed values.
 
 :class:`Imputer` can be used in a Pipeline as a way to build a composite
 estimator that supports imputation. See :ref:`example_missing_values.py`
+
+.. _polynomial_features:
+
+Generating polynomial features
+==============================
+
+Often it's useful to add complexity to the model by considering nonlinear features of the input data. A simple and common method to use is polynomial features, which can get features' high-order and interaction terms. It is implemented in :class:`PolynomialFeatures`::
+
+    >>> import numpy as np
+    >>> from sklearn.preprocessing import PolynomialFeatures
+    >>> X = np.arange(6).reshape(3, 2)
+    >>> X                                                 # doctest: +ELLIPSIS
+    array([[0, 1],
+           [2, 3],
+           [4, 5]])
+    >>> poly = PolynomialFeatures(2)
+    >>> poly.fit_transform(X)                             # doctest: +ELLIPSIS
+    array([[ 1,  0,  1,  0,  0,  1],
+           [ 1,  2,  3,  4,  6,  9],
+           [ 1,  4,  5, 16, 20, 25]])
+
+The features of X have been transformed from :math:`(X_1, X_2)` to :math:`(1, X_1, X_2, X_1^2, X_1X_2, X_2^2)`.
+
+In some cases, only interaction terms among features are required, and it can be gotten with the setting ``interaction_only=True``::
+
+    >>> X = np.arange(9).reshape(3, 3)
+    >>> X                                                 # doctest: +ELLIPSIS
+    array([[0, 1, 2],
+           [3, 4, 5],
+           [6, 7, 8]])
+    >>> poly = PolynomialFeatures(degree=3, interaction_only=True)
+    >>> poly.fit_transform(X)                             # doctest: +ELLIPSIS
+    array([[  1,   0,   1,   2,   0,   0,   2,   0],
+           [  1,   3,   4,   5,  12,  15,  20,  60],
+           [  1,   6,   7,   8,  42,  48,  56, 336]])
+
+The features of X have been transformed from :math:`(X_1, X_2, X_3)` to :math:`(1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)`.
+
+Note that polynomial features are used implicitily in `kernel methods <http://en.wikipedia.org/wiki/Kernel_method>`_ (e.g., :class:`sklearn.svm.SVC`, :class:`sklearn.decomposition.KernelPCA`) when using polynomial :ref:`svm_kernels`.
+
+See :ref:`example_linear_model_plot_polynomial_interpolation.py` for Ridge regression using created polynomial features.  
diff --git a/sklearn/grid_search.py b/sklearn/grid_search.py
index 9b3d3e80a0..36a8f33eee 100755
--- a/sklearn/grid_search.py
+++ b/sklearn/grid_search.py
@@ -70,6 +70,8 @@ class ParameterGrid(object):
     ...                               {'kernel': 'rbf', 'gamma': 1},
     ...                               {'kernel': 'rbf', 'gamma': 10}]
     True
+    >>> ParameterGrid(grid)[1] == {'kernel': 'rbf', 'gamma': 1}
+    True
 
     See also
     --------
@@ -112,24 +114,42 @@ def __len__(self):
                    for p in self.param_grid)
 
     def __getitem__(self, ind):
-        """
-        >>> grid = ParameterGrid([{'kernel': ['linear']},
-        ...                       {'kernel': ['rbf'], 'gamma': [1, 10]}])
-        >>> [grid[i] for i in range(len(grid))] == list(grid)
-        True
+        """Get the parameters that would be ``ind``th in iteration
+
+        Parameters
+        ----------
+        ind : int
+            The iteration index
+
+        Returns
+        -------
+        params : dict of string to any
+            Equal to list(self)[ind]
         """
         for sub_grid in self.param_grid:
             # XXX: could memoize information used here
-            keys, values = zip(*sorted(sub_grid.items()))
-            sizes = [len(v) for v in values]
+            if not sub_grid:
+                if ind == 0:
+                    return {}
+                else:
+                    ind -= 1
+                    continue
+            keys, values_lists = zip(*sorted(sub_grid.items()))
+            sizes = [len(v_list) for v_list in values_lists]
+            # For sizes = [5, 3, 2], modulo = [30,  6,  2,  1]
+            # The grid has size 30;
+            # the first param changes every 6, the second every 2, etc.
             modulo = np.cumprod(np.hstack([1, sizes[::-1]]))[::-1]
             if ind >= modulo[0]:
+                # Try the next grid
                 ind -= modulo[0]
             else:
                 offsets = ind // modulo[1:] % sizes
-                return {k: v[o] for k, v, o in zip(keys, values, offsets)}
+                return dict((key, v_list[offset])
+                            for key, v_list, offset
+                            in zip(keys, values_lists, offsets))
 
-        raise ValueError
+        raise IndexError('ParameterGrid index out of range')
 
 
 class ParameterSampler(object):
@@ -201,7 +221,7 @@ def __iter__(self):
         rnd = check_random_state(self.random_state)
 
         if all_lists:
-            # get complete grid and yield from it
+            # look up sampled parameter settings in parameter grid
             param_grid = ParameterGrid(self.param_distributions)
             grid_size = len(param_grid)
 
diff --git a/sklearn/linear_model/ridge.py b/sklearn/linear_model/ridge.py
index 77c3e10dbb..24d9f7a7e3 100755
--- a/sklearn/linear_model/ridge.py
+++ b/sklearn/linear_model/ridge.py
@@ -871,10 +871,7 @@ def fit(self, X, y, sample_weight=None):
                 raise ValueError("cv!=None and store_cv_values=True "
                                  " are incompatible")
             parameters = {'alpha': self.alphas}
-            # FIXME: sample_weight must be split into training/validation data
-            #        too!
-            #fit_params = {'sample_weight' : sample_weight}
-            fit_params = {}
+            fit_params = {'sample_weight' : sample_weight}
             gs = GridSearchCV(Ridge(fit_intercept=self.fit_intercept),
                               parameters, fit_params=fit_params, cv=self.cv)
             gs.fit(X, y)
diff --git a/sklearn/linear_model/tests/test_ridge.py b/sklearn/linear_model/tests/test_ridge.py
index daa8f8bbcc..649970fec3 100755
--- a/sklearn/linear_model/tests/test_ridge.py
+++ b/sklearn/linear_model/tests/test_ridge.py
@@ -27,6 +27,8 @@
 from sklearn.linear_model.ridge import _solve_cholesky
 from sklearn.linear_model.ridge import _solve_cholesky_kernel
 
+from sklearn.grid_search import GridSearchCV
+
 from sklearn.cross_validation import KFold
 
 
@@ -527,6 +529,32 @@ def test_ridgecv_store_cv_values():
     assert_equal(r.cv_values_.shape, (n_samples, n_responses, n_alphas))
 
 
+def test_ridgecv_sample_weight():
+    rng = np.random.RandomState(0)
+    alphas = (0.1, 1.0, 10.0)
+
+    # There are different algorithms for n_samples > n_features
+    # and the opposite, so test them both.
+    for n_samples, n_features in ((6, 5), (5, 10)):
+        y = rng.randn(n_samples)
+        X = rng.randn(n_samples, n_features)
+        sample_weight = 1 + rng.rand(n_samples)
+
+        cv = KFold(n_samples, 5)
+        ridgecv = RidgeCV(alphas=alphas, cv=cv)
+        ridgecv.fit(X, y, sample_weight=sample_weight)
+
+        # Check using GridSearchCV directly
+        parameters = {'alpha': alphas}
+        fit_params = {'sample_weight': sample_weight}
+        gs = GridSearchCV(Ridge(), parameters, fit_params=fit_params,
+                          cv=cv)
+        gs.fit(X, y)
+
+        assert_equal(ridgecv.alpha_, gs.best_estimator_.alpha)
+        assert_array_almost_equal(ridgecv.coef_, gs.best_estimator_.coef_)
+
+
 def test_raises_value_error_if_sample_weights_greater_than_1d():
     # Sample weights must be either scalar or 1D
 
diff --git a/sklearn/tests/test_grid_search.py b/sklearn/tests/test_grid_search.py
index d9038187df..78c6bfc8ea 100755
--- a/sklearn/tests/test_grid_search.py
+++ b/sklearn/tests/test_grid_search.py
@@ -92,6 +92,10 @@ def score(self):
 y = np.array([1, 1, 2, 2])
 
 
+def assert_grid_iter_equals_getitem(grid):
+    assert_equal(list(grid), [grid[i] for i in range(len(grid))])
+
+
 def test_parameter_grid():
     # Test basic properties of ParameterGrid.
     params1 = {"foo": [1, 2, 3]}
@@ -99,6 +103,7 @@ def test_parameter_grid():
     assert_true(isinstance(grid1, Iterable))
     assert_true(isinstance(grid1, Sized))
     assert_equal(len(grid1), 3)
+    assert_grid_iter_equals_getitem(grid1)
 
     params2 = {"foo": [4, 2],
                "bar": ["ham", "spam", "eggs"]}
@@ -113,14 +118,19 @@ def test_parameter_grid():
                      set(("bar", x, "foo", y)
                          for x, y in product(params2["bar"], params2["foo"])))
 
+    assert_grid_iter_equals_getitem(grid2)
+
     # Special case: empty grid (useful to get default estimator settings)
     empty = ParameterGrid({})
     assert_equal(len(empty), 1)
     assert_equal(list(empty), [{}])
+    assert_grid_iter_equals_getitem(empty)
+    assert_raises(IndexError, lambda: empty[1])
 
     has_empty = ParameterGrid([{'C': [1, 10]}, {}])
     assert_equal(len(has_empty), 3)
     assert_equal(list(has_empty), [{'C': 1}, {'C': 10}, {}])
+    assert_grid_iter_equals_getitem(has_empty)
 
 
 def test_grid_search():
