diff --git a/examples/cluster/plot_kmeans_digits.py b/examples/cluster/plot_kmeans_digits.py
index b1cd3626b6..1e6fbbc019 100755
--- a/examples/cluster/plot_kmeans_digits.py
+++ b/examples/cluster/plot_kmeans_digits.py
@@ -52,15 +52,14 @@
       % (n_digits, n_samples, n_features))
 
 
-print(79 * '_')
-print('% 9s' % 'init'
-      '    time  inertia    homo   compl  v-meas     ARI AMI  silhouette')
+print(82 * '_')
+print('init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\tsilhouette')
 
 
 def bench_k_means(estimator, name, data):
     t0 = time()
     estimator.fit(data)
-    print('% 9s   %.2fs    %i   %.3f   %.3f   %.3f   %.3f   %.3f    %.3f'
+    print('%-9s\t%.2fs\t%i\t%.3f\t%.3f\t%.3f\t%.3f\t%.3f\t%.3f'
           % (name, (time() - t0), estimator.inertia_,
              metrics.homogeneity_score(labels, estimator.labels_),
              metrics.completeness_score(labels, estimator.labels_),
@@ -83,7 +82,7 @@ def bench_k_means(estimator, name, data):
 bench_k_means(KMeans(init=pca.components_, n_clusters=n_digits, n_init=1),
               name="PCA-based",
               data=data)
-print(79 * '_')
+print(82 * '_')
 
 ###############################################################################
 # Visualize the results on PCA-reduced data
diff --git a/sklearn/ensemble/bagging.py b/sklearn/ensemble/bagging.py
index 07d834f58d..60c4490b0f 100755
--- a/sklearn/ensemble/bagging.py
+++ b/sklearn/ensemble/bagging.py
@@ -543,7 +543,7 @@ class BaggingClassifier(BaseBagging, ClassifierMixin):
     >>> bagging = BaggingClassifier(LinearSVC(random_state=0),
     ...                             max_samples=0.5, max_features=0.5,
     ...                             oob_score=True)
-    >>> cross_val_score(bagging, X, y).mean()              # doctest +ELLIPSIS
+    >>> cross_val_score(bagging, X, y).mean()             # doctest: +ELLIPSIS
     0.8...
 
     References
diff --git a/sklearn/ensemble/forest.py b/sklearn/ensemble/forest.py
index f7c288f697..924f90e3c4 100755
--- a/sklearn/ensemble/forest.py
+++ b/sklearn/ensemble/forest.py
@@ -1080,7 +1080,7 @@ class RandomForestRegressor(ForestRegressor):
     >>> boston = load_boston()
     >>> X, y = boston.data, boston.target
     >>> cross_val_score(forest, X, y, cv=5).mean()        # doctest: +ELLIPSIS
-    0.7...
+    0.53...
 
 
     References
@@ -1288,7 +1288,7 @@ class labels (multi-output problem).
 
     >>> clf = ExtraTreesClassifier(n_estimators=5, max_depth=None,
     ...                            min_samples_split=1, random_state=0)
-    >>> scores = cross_val_score(clf, X, y).mean()        # doctest: +ELLIPSIS
+    >>> cross_val_score(clf, X, y).mean()        # doctest: +ELLIPSIS
     0.99...
 
     References
@@ -1625,7 +1625,7 @@ class RandomTreesEmbedding(BaseForest):
     The dimensionality before transforming: 4
     >>> X_transformed = rte.fit_transform(X, y)
     >>> type(X_transformed)
-    scipy.sparse.csr.csr_matrix
+    <class 'scipy.sparse.csr.csr_matrix'>
     >>> print("The dimensionality after transforming: %d"
     ...       % X_transformed.shape[1])
     The dimensionality after transforming: 235
