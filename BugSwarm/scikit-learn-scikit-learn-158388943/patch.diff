diff --git a/sklearn/feature_selection/from_model.py b/sklearn/feature_selection/from_model.py
index a0b7aba8bc..bbd379c9d1 100755
--- a/sklearn/feature_selection/from_model.py
+++ b/sklearn/feature_selection/from_model.py
@@ -7,7 +7,7 @@
 from ..base import TransformerMixin, BaseEstimator, clone
 from ..externals import six
 
-from ..utils import safe_mask, check_array, deprecated, check_X_y
+from ..utils import safe_mask, check_array, deprecated
 from ..utils.validation import check_is_fitted
 from ..exceptions import NotFittedError
 
@@ -173,10 +173,6 @@ class SelectFromModel(BaseEstimator, SelectorMixin):
         Otherwise train the model using ``fit`` and then ``transform`` to do
         feature selection.
 
-    max_features : int, between 0 and number of features, default None.
-        If provided, first n most important features are
-        kept, ignoring threshold parameter.
-
     Attributes
     ----------
     `estimator_`: an estimator
@@ -187,28 +183,10 @@ class SelectFromModel(BaseEstimator, SelectorMixin):
     `threshold_`: float
         The threshold value used for feature selection.
     """
-    def __init__(self, estimator, threshold=None, prefit=False,
-                 max_features=None):
+    def __init__(self, estimator, threshold=None, prefit=False):
         self.estimator = estimator
         self.threshold = threshold
         self.prefit = prefit
-        self.max_features = max_features
-
-    def _check_max_features(self, X, max_features):
-        if self.max_features is not None:
-            if isinstance(self.max_features, int):
-                if 0 <= self.max_features <= X.shape[1]:
-                    return
-            elif self.max_features == 'all':
-                    return
-            raise ValueError(
-                    "max_features should be >=0, <= n_features; got %r."
-                    " Use max_features='all' to return all features."
-                    % self.max_features)
-
-    def _check_params(self, X, y):
-        X, y = check_X_y(X, y)
-        self._check_max_features(X, self.max_features)
 
     def _get_support_mask(self):
         # SelectFromModel can directly call on transform.
@@ -223,15 +201,7 @@ def _get_support_mask(self):
         scores = _get_feature_importances(estimator)
         self.threshold_ = _calculate_threshold(estimator, scores,
                                                self.threshold)
-        mask = np.zeros_like(scores, dtype=bool)
-        if self.max_features == 'all':
-            self.max_features = scores.size
-        candidate_indices = np.argsort(-scores,
-                                       kind='mergesort')[:self.max_features]
-        mask[candidate_indices] = True
-        if self.threshold is not None:
-            mask[scores < self.threshold_] = False
-        return mask
+        return scores >= self.threshold_
 
     def fit(self, X, y=None, **fit_params):
         """Fit the SelectFromModel meta-transformer.
@@ -252,7 +222,6 @@ def fit(self, X, y=None, **fit_params):
         self : object
             Returns self.
         """
-        self._check_params(X, y)
         if self.prefit:
             raise NotFittedError(
                 "Since 'prefit=True', call transform directly")
diff --git a/sklearn/feature_selection/tests/test_from_model.py b/sklearn/feature_selection/tests/test_from_model.py
index 0a943983db..06dc3d7f42 100755
--- a/sklearn/feature_selection/tests/test_from_model.py
+++ b/sklearn/feature_selection/tests/test_from_model.py
@@ -10,7 +10,6 @@
 from sklearn.utils.testing import assert_almost_equal
 from sklearn.utils.testing import assert_warns
 from sklearn.utils.testing import skip_if_32bit
-from sklearn.utils.testing import assert_equal
 
 from sklearn import datasets
 from sklearn.linear_model import LogisticRegression, SGDClassifier, Lasso
@@ -18,7 +17,6 @@
 from sklearn.feature_selection import SelectFromModel
 from sklearn.ensemble import RandomForestClassifier
 from sklearn.linear_model import PassiveAggressiveClassifier
-from sklearn.base import BaseEstimator
 
 iris = datasets.load_iris()
 data, y = iris.data, iris.target
@@ -65,112 +63,6 @@ def test_input_estimator_unchanged():
     assert_true(transformer.estimator is est)
 
 
-def check_invalid_max_features(est, X, y):
-    max_features = X.shape[1]
-    for invalid_max_n_feature in [-1, max_features + 1, 'gobbledigook']:
-        transformer = SelectFromModel(estimator=est,
-                                      max_features=invalid_max_n_feature)
-        assert_raises(ValueError, transformer.fit, X, y)
-
-
-def check_valid_max_features(est, X, y):
-    max_features = X.shape[1]
-    for valid_max_n_feature in [0, max_features, 'all', 5]:
-        transformer = SelectFromModel(estimator=est,
-                                      max_features=valid_max_n_feature)
-        X_new = transformer.fit_transform(X, y)
-        if valid_max_n_feature == 'all':
-            valid_max_n_feature = max_features
-        assert_equal(X_new.shape[1], valid_max_n_feature)
-
-
-class FixedImportanceEstimator(BaseEstimator):
-    def __init__(self, importances):
-        self.importances = importances
-
-    def fit(self, X, y=None):
-        self.feature_importances_ = np.array(self.importances)
-
-
-def check_max_features(est, X, y):
-    X = X.copy()
-    max_features = X.shape[1]
-
-    check_valid_max_features(est, X, y)
-    check_invalid_max_features(est, X, y)
-
-    transformer1 = SelectFromModel(estimator=est, max_features='all')
-    transformer2 = SelectFromModel(estimator=est,
-                                   max_features=max_features)
-    X_new1 = transformer1.fit_transform(X, y)
-    X_new2 = transformer2.fit_transform(X, y)
-    assert_array_equal(X_new1, X_new2)
-
-    # Test max_features against actual model.
-
-    transformer1 = SelectFromModel(estimator=Lasso(alpha=0.025))
-    X_new1 = transformer1.fit_transform(X, y)
-    for n_features in range(1, X_new1.shape[1] + 1):
-        transformer2 = SelectFromModel(estimator=Lasso(alpha=0.025),
-                                       max_features=n_features)
-        X_new2 = transformer2.fit_transform(X, y)
-        assert_array_equal(X_new1[:, :n_features], X_new2)
-        assert_array_equal(transformer1.estimator_.coef_,
-                           transformer2.estimator_.coef_)
-
-    # Test if max_features can break tie among feature importance
-
-    feature_importances = np.array([4, 4, 4, 4, 3, 3, 3, 2, 2, 1])
-    for n_features in range(1, max_features + 1):
-        transformer = SelectFromModel(
-            FixedImportanceEstimator(feature_importances),
-            max_features=n_features)
-        X_new = transformer.fit_transform(X, y)
-        selected_feature_indices = np.where(transformer._get_support_mask())[0]
-        assert_array_equal(selected_feature_indices, np.arange(n_features))
-        assert_equal(X_new.shape[1], n_features)
-
-
-def check_threshold_and_max_features(est, X, y):
-    transformer1 = SelectFromModel(estimator=est, max_features=3)
-    X_new1 = transformer1.fit_transform(X, y)
-
-    transformer2 = SelectFromModel(estimator=est, threshold=0.04)
-    X_new2 = transformer2.fit_transform(X, y)
-
-    transformer3 = SelectFromModel(estimator=est, max_features=3,
-                                   threshold=0.04)
-    X_new3 = transformer3.fit_transform(X, y)
-    assert_equal(X_new3.shape[1], min(X_new1.shape[1], X_new2.shape[1]))
-    selected_indices = \
-        transformer3.transform(np.arange(X.shape[1]))[np.newaxis, :]
-    assert_array_equal(X_new3, X[:, selected_indices[0][0]])
-
-    """
-    If threshold and max_features are not provided, all features are
-    returned, use threshold=None if it is not required.
-    """
-    transformer = SelectFromModel(estimator=Lasso(alpha=0.1))
-    X_new = transformer.fit_transform(X, y)
-    assert_array_equal(X, X_new)
-
-    transformer = SelectFromModel(estimator=Lasso(alpha=0.1), max_features=3)
-    X_new = transformer.fit_transform(X, y)
-    assert_equal(X_new.shape[1], 3)
-
-    # Threshold will be applied if it is not None
-    transformer = SelectFromModel(estimator=Lasso(alpha=0.1), threshold=1e-5)
-    X_new = transformer.fit_transform(X, y)
-    mask = np.abs(transformer.estimator_.coef_) > 1e-5
-    assert_array_equal(X_new, X[:, mask])
-
-    transformer = SelectFromModel(estimator=Lasso(alpha=0.1), threshold=1e-5,
-                                  max_features=4)
-    X_new = transformer.fit_transform(X, y)
-    mask = np.abs(transformer.estimator_.coef_) > 1e-5
-    assert_array_equal(X_new, X[:, mask])
-
-
 @skip_if_32bit
 def test_feature_importances():
     X, y = datasets.make_classification(
@@ -203,16 +95,12 @@ def test_feature_importances():
     assert_almost_equal(importances, importances_bis)
 
     # For the Lasso and related models, the threshold defaults to 1e-5
-    transformer = SelectFromModel(estimator=Lasso(alpha=0.1), threshold=1e-5)
+    transformer = SelectFromModel(estimator=Lasso(alpha=0.1))
     transformer.fit(X, y)
     X_new = transformer.transform(X)
     mask = np.abs(transformer.estimator_.coef_) > 1e-5
     assert_array_equal(X_new, X[:, mask])
 
-    # Test max_features parameter using various values
-    check_max_features(est, X, y)
-    check_threshold_and_max_features(est, X, y)
-
 
 def test_partial_fit():
     est = PassiveAggressiveClassifier(random_state=0, shuffle=False)
