diff --git a/build_tools/travis/flake8_diff.sh b/build_tools/travis/flake8_diff.sh
index 84495b339a..9781f7e6a5 100755
--- a/build_tools/travis/flake8_diff.sh
+++ b/build_tools/travis/flake8_diff.sh
@@ -137,12 +137,9 @@ check_files() {
 if [[ "$MODIFIED_FILES" == "no_match" ]]; then
     echo "No file outside sklearn/externals and doc/sphinxext/sphinx_gallery has been modified"
 else
-    # Default ignore PEP8 violations are from flake8 3.3.0
-    DEFAULT_IGNORED_PEP8=E121,E123,E126,E226,E24,E704,W503,W504
-    check_files "$(echo "$MODIFIED_FILES" | grep -v ^examples)" \
-           --ignore $DEFAULT_IGNORED_PEP8
-    # Examples are allowed to not have imports at top of file
+
+    check_files "$(echo "$MODIFIED_FILES" | grep -v ^examples)"
     check_files "$(echo "$MODIFIED_FILES" | grep ^examples)" \
-           --ignore $DEFAULT_IGNORED_PEP8 --ignore E402
+        --config ./examples/.flake8
 fi
 echo -e "No problem detected by flake8\n"
diff --git a/conftest.py b/conftest.py
index 25275e11aa..c4bed49c6d 100755
--- a/conftest.py
+++ b/conftest.py
@@ -9,6 +9,6 @@
 # the doctests pass
 import numpy as np
 try:
-    np.set_printoptions(sign='legacy')
+    np.set_printoptions(legacy=True)
 except TypeError:
     pass
diff --git a/doc/datasets/index.rst b/doc/datasets/index.rst
index f9b400ba83..1316d596f5 100755
--- a/doc/datasets/index.rst
+++ b/doc/datasets/index.rst
@@ -64,7 +64,7 @@ require to download any file from some external website.
    load_breast_cancer
 
 These datasets are useful to quickly illustrate the behavior of the
-various algorithms implemented in the scikit. They are however often too
+various algorithms implemented in scikit-learn. They are however often too
 small to be representative of real world machine learning tasks.
 
 .. _sample_images:
@@ -72,7 +72,7 @@ small to be representative of real world machine learning tasks.
 Sample images
 =============
 
-The scikit also embed a couple of sample JPEG images published under Creative
+Scikit-learn also embed a couple of sample JPEG images published under Creative
 Commons license by their authors. Those image can be useful to test algorithms
 and pipeline on 2D data.
 
diff --git a/doc/developers/maintainer.rst b/doc/developers/maintainer.rst
index ff639d5500..c645a5c71d 100755
--- a/doc/developers/maintainer.rst
+++ b/doc/developers/maintainer.rst
@@ -1,10 +1,10 @@
 Maintainer / core-developer information
 ========================================
 
-For more information see https://github.com/scikit-learn/scikit-learn/wiki/How-to-make-a-release
-
 Making a release
 ------------------
+For more information see https://github.com/scikit-learn/scikit-learn/wiki/How-to-make-a-release
+
 
 1. Update docs:
 
@@ -55,3 +55,27 @@ Making a release
 
 
 7. FOR FINAL RELEASE: Update the release date in What's New
+
+Travis Cron jobs
+----------------
+
+From `<https://docs.travis-ci.com/user/cron-jobs>`_: Travis CI cron jobs work
+similarly to the cron utility, they run builds at regular scheduled intervals
+independently of whether any commits were pushed to the repository. Cron jobs
+always fetch the most recent commit on a particular branch and build the project
+at that state. Cron jobs can run daily, weekly or monthly, which in practice
+means up to an hour after the selected time span, and you cannot set them to run
+at a specific time.
+
+For scikit-learn, Cron jobs are used for builds that we do not want to run in
+each PR. As an example the build with the dev versions of numpy and scipy is
+run as a Cron job. Most of the time when this numpy-dev build fail, it is
+related to a numpy change and not a scikit-learn one, so it would not make sense
+to blame the PR author for the Travis failure.
+
+The definition of what gets run in the Cron job is done in the .travis.yml
+config file, exactly the same way as the other Travis jobs. We use a ``if: type
+= cron`` filter in order for the build to be run only in Cron jobs.
+
+The branch targetted by the Cron job and the frequency of the Cron job is set
+via the web UI at https://www.travis-ci.org/scikit-learn/scikit-learn/settings.
diff --git a/doc/developers/performance.rst b/doc/developers/performance.rst
index 692e7ca1f9..d3d6204ec3 100755
--- a/doc/developers/performance.rst
+++ b/doc/developers/performance.rst
@@ -94,7 +94,7 @@ loads and prepare you data and then use the IPython integrated profiler
 for interactively exploring the relevant part for the code.
 
 Suppose we want to profile the Non Negative Matrix Factorization module
-of the scikit. Let us setup a new IPython session and load the digits
+of scikit-learn. Let us setup a new IPython session and load the digits
 dataset and as in the :ref:`sphx_glr_auto_examples_classification_plot_digits_classification.py` example::
 
   In [1]: from sklearn.decomposition import NMF
diff --git a/doc/faq.rst b/doc/faq.rst
index fea4efa010..527850a8cb 100755
--- a/doc/faq.rst
+++ b/doc/faq.rst
@@ -86,6 +86,8 @@ arrays such as pandas DataFrame are also acceptable.
 For more information on loading your data files into these usable data 
 structures, please refer to :ref:`loading external datasets <external_datasets>`.
 
+.. _new_algorithms_inclusion_criteria:
+
 What are the inclusion criteria for new algorithms ?
 ----------------------------------------------------
 
@@ -108,9 +110,12 @@ proposed algorithm should outperform the methods that are already implemented
 in scikit-learn at least in some areas.
 
 Also note that your implementation need not be in scikit-learn to be used
-together with scikit-learn tools. You can implement your favorite algorithm in
-a scikit-learn compatible way, upload it to github and let us know. We will
-list it under :ref:`related_projects`.
+together with scikit-learn tools. You can implement your favorite algorithm
+in a scikit-learn compatible way, upload it to GitHub and let us know. We
+will be happy to list it under :ref:`related_projects`. If you already have
+a package on GitHub following the scikit-learn API, you may also be
+interested to look at `scikit-learn-contrib
+<http://scikit-learn-contrib.github.io>`_.
 
 .. _selectiveness:
 
@@ -123,8 +128,10 @@ The package relies on core developers using their free time to
 fix bugs, maintain code and review contributions.
 Any algorithm that is added needs future attention by the developers,
 at which point the original author might long have lost interest.
-Also see `this thread on the mailing list
-<https://sourceforge.net/p/scikit-learn/mailman/scikit-learn-general/thread/CAAkaFLWcBG+gtsFQzpTLfZoCsHMDv9UG5WaqT0LwUApte0TVzg@mail.gmail.com/#msg33104380>`_.
+See also :ref:`new_algorithms_inclusion_criteria`. For a great read about 
+long-term maintenance issues in open-source software, look at 
+`the Executive Summary of Roads and Bridges
+<https://www.fordfoundation.org/media/2976/roads-and-bridges-the-unseen-labor-behind-our-digital-infrastructure.pdf#page=8>`_
 
 Why did you remove HMMs from scikit-learn?
 --------------------------------------------
diff --git a/doc/modules/dp-derivation.rst b/doc/modules/dp-derivation.rst
index 4509e0fa32..0625884c27 100755
--- a/doc/modules/dp-derivation.rst
+++ b/doc/modules/dp-derivation.rst
@@ -23,7 +23,7 @@ complex, or even more. For this reason we present here a full
 derivation of the inference algorithm and all the update and
 lower-bound equations. If you're not interested in learning how to
 derive similar algorithms yourself and you're not interested in
-changing/debugging the implementation in the scikit this document is
+changing/debugging the implementation in scikit-learn this document is
 not for you.
 
 The complexity of this implementation is linear in the number of
diff --git a/doc/modules/model_persistence.rst b/doc/modules/model_persistence.rst
index 1efe4a8bcd..d64657717b 100755
--- a/doc/modules/model_persistence.rst
+++ b/doc/modules/model_persistence.rst
@@ -13,7 +13,7 @@ security and maintainability issues when working with pickle serialization.
 Persistence example
 -------------------
 
-It is possible to save a model in the scikit by using Python's built-in
+It is possible to save a model in scikit-learn by using Python's built-in
 persistence model, namely `pickle <https://docs.python.org/2/library/pickle.html>`_::
 
   >>> from sklearn import svm
@@ -35,7 +35,7 @@ persistence model, namely `pickle <https://docs.python.org/2/library/pickle.html
   >>> y[0]
   0
 
-In the specific case of the scikit, it may be more interesting to use
+In the specific case of scikit-learn, it may be more interesting to use
 joblib's replacement of pickle (``joblib.dump`` & ``joblib.load``),
 which is more efficient on objects that carry large numpy arrays internally as
 is often the case for fitted scikit-learn estimators, but can only pickle to the
diff --git a/doc/presentations.rst b/doc/presentations.rst
index 8b5d3bdc89..6fe17a69f4 100755
--- a/doc/presentations.rst
+++ b/doc/presentations.rst
@@ -37,7 +37,7 @@ Videos
   <http://videolectures.net/icml2010_varaquaux_scik/>`_ by `Gael Varoquaux`_ at
   ICML 2010
 
-    A three minute video from a very early stage of the scikit, explaining the
+    A three minute video from a very early stage of scikit-learn, explaining the
     basic idea and approach we are following.
 
 - `Introduction to statistical learning with scikit-learn <http://archive.org/search.php?query=scikit-learn>`_
diff --git a/doc/tutorial/basic/tutorial.rst b/doc/tutorial/basic/tutorial.rst
index 89600953a8..7c6058591b 100755
--- a/doc/tutorial/basic/tutorial.rst
+++ b/doc/tutorial/basic/tutorial.rst
@@ -209,7 +209,7 @@ example that you can run and study:
 Model persistence
 -----------------
 
-It is possible to save a model in the scikit by using Python's built-in
+It is possible to save a model in scikit-learn by using Python's built-in
 persistence model, namely `pickle <https://docs.python.org/2/library/pickle.html>`_::
 
   >>> from sklearn import svm
@@ -231,7 +231,7 @@ persistence model, namely `pickle <https://docs.python.org/2/library/pickle.html
   >>> y[0]
   0
 
-In the specific case of the scikit, it may be more interesting to use
+In the specific case of scikit-learn, it may be more interesting to use
 joblib's replacement of pickle (``joblib.dump`` & ``joblib.load``),
 which is more efficient on big data, but can only pickle to the disk
 and not to a string::
diff --git a/doc/tutorial/statistical_inference/settings.rst b/doc/tutorial/statistical_inference/settings.rst
index 1b1e477c5c..e3c4ca8fea 100755
--- a/doc/tutorial/statistical_inference/settings.rst
+++ b/doc/tutorial/statistical_inference/settings.rst
@@ -12,7 +12,7 @@ list of multi-dimensional observations. We say that the first axis of
 these arrays is the **samples** axis, while the second is the
 **features** axis.
 
-.. topic:: A simple example shipped with the scikit: iris dataset
+.. topic:: A simple example shipped with scikit-learn: iris dataset
 
     ::
 
@@ -46,7 +46,7 @@ needs to be preprocessed in order to be used by scikit-learn.
         >>> plt.imshow(digits.images[-1], cmap=plt.cm.gray_r) #doctest: +SKIP
         <matplotlib.image.AxesImage object at ...>
 
-    To use this dataset with the scikit, we transform each 8x8 image into a
+    To use this dataset with scikit-learn, we transform each 8x8 image into a
     feature vector of length 64 ::
 
         >>> data = digits.images.reshape((digits.images.shape[0], -1))
diff --git a/doc/tutorial/statistical_inference/unsupervised_learning.rst b/doc/tutorial/statistical_inference/unsupervised_learning.rst
index 0ad16c1803..cef8fbe780 100755
--- a/doc/tutorial/statistical_inference/unsupervised_learning.rst
+++ b/doc/tutorial/statistical_inference/unsupervised_learning.rst
@@ -171,7 +171,7 @@ Connectivity-constrained clustering
 .....................................
 
 With agglomerative clustering, it is possible to specify which samples can be
-clustered together by giving a connectivity graph. Graphs in the scikit
+clustered together by giving a connectivity graph. Graphs in scikit-learn
 are represented by their adjacency matrix. Often, a sparse matrix is used.
 This can be useful, for instance, to retrieve connected regions (sometimes
 also referred to as connected components) when
diff --git a/doc/whats_new/v0.20.rst b/doc/whats_new/v0.20.rst
index a01ffe41f9..58506cf8aa 100755
--- a/doc/whats_new/v0.20.rst
+++ b/doc/whats_new/v0.20.rst
@@ -81,6 +81,10 @@ Classifiers and regressors
   ``inverse_func`` are the inverse of each other.
   :issue:`9399` by :user:`Guillaume Lemaitre <glemaitre>`.
 
+- Add `sample_weight` parameter to the fit method of
+  :class:`linear_model.BayesianRidge` for weighted linear regression.
+  :issue:`10111` by :user:`Peter St. John <pstjohn>`.
+
 Model evaluation and meta-estimators
 
 - A scorer based on :func:`metrics.brier_score_loss` is also available.
@@ -192,3 +196,10 @@ Cluster
 - Deprecate ``pooling_func`` unused parameter in
   :class:`cluster.AgglomerativeClustering`. :issue:`9875` by :user:`Kumar Ashutosh
   <thechargedneutron>`.
+
+Changes to estimator checks
+---------------------------
+
+- Allow tests in :func:`estimator_checks.check_estimator` to test functions
+  that accept pairwise data.
+  :issue:`9701` by :user:`Kyle Johnson <gkjohns>`
diff --git a/examples/.flake8 b/examples/.flake8
new file mode 100755
index 0000000000..703bf15e79
--- /dev/null
+++ b/examples/.flake8
@@ -0,0 +1,5 @@
+# Examples specific flake8 configuration
+
+[flake8]
+# Same ignore as project-wide plus E402 (imports not at top of file)
+ignore=E121,E123,E126,E24,E226,E704,W503,W504,E402
diff --git a/examples/README.txt b/examples/README.txt
index 45f038ddcd..4f467efb61 100755
--- a/examples/README.txt
+++ b/examples/README.txt
@@ -6,4 +6,4 @@ Examples
 General examples
 ----------------
 
-General-purpose and introductory examples for the scikit.
+General-purpose and introductory examples for scikit-learn.
diff --git a/examples/applications/wikipedia_principal_eigenvector.py b/examples/applications/wikipedia_principal_eigenvector.py
index 175c105944..3ef921bb3d 100755
--- a/examples/applications/wikipedia_principal_eigenvector.py
+++ b/examples/applications/wikipedia_principal_eigenvector.py
@@ -23,7 +23,7 @@
     https://en.wikipedia.org/wiki/Power_iteration
 
 Here the computation is achieved thanks to Martinsson's Randomized SVD
-algorithm implemented in the scikit.
+algorithm implemented in scikit-learn.
 
 The graph data is fetched from the DBpedia dumps. DBpedia is an extraction
 of the latent structured data of the Wikipedia content.
diff --git a/examples/plot_missing_values.py b/examples/plot_missing_values.py
index 5e7ef8fd68..5ae8af2b54 100755
--- a/examples/plot_missing_values.py
+++ b/examples/plot_missing_values.py
@@ -16,34 +16,35 @@
 
 Script output::
 
-  Score with the entire dataset = 0.56
-  Score without the samples containing missing values = 0.48
-  Score after imputation of the missing values (mean) = 0.57
-  Score after imputation of the missing values (NMF) = 0.51
+  Score with the entire dataset = 0.88
+  Score without the samples containing missing values = 0.68
+  Score after imputation of the missing values (mean) = 0.88
+  Score after imputation of the missing values (NMF) = 0.87
 
 In this case, imputing helps the classifier get close to the original score.
 
 """
 import numpy as np
 
-from sklearn.datasets import load_boston
-from sklearn.ensemble import RandomForestRegressor
+from sklearn.datasets import fetch_olivetti_faces
+from sklearn.ensemble import RandomForestClassifier
 from sklearn.pipeline import Pipeline
 from sklearn.preprocessing import Imputer
-from sklearn.decomposition import ImputerNMF
+from sklearn.decomposition import NMF
 from sklearn.model_selection import cross_val_score
 
 print(__doc__)
 
 rng = np.random.RandomState(0)
 
-dataset = load_boston()
+dataset = fetch_olivetti_faces()
 X_full, y_full = dataset.data, dataset.target
+
 n_samples = X_full.shape[0]
 n_features = X_full.shape[1]
 
 # Estimate the score on the entire dataset, with no missing values
-estimator = RandomForestRegressor(random_state=0, n_estimators=100)
+estimator = RandomForestClassifier(random_state=0, n_estimators=100)
 score = cross_val_score(estimator, X_full, y_full).mean()
 print("Score with the entire dataset = %.2f" % score)
 
@@ -60,7 +61,7 @@
 # Estimate the score without the lines containing missing values
 X_filtered = X_full[~missing_samples, :]
 y_filtered = y_full[~missing_samples]
-estimator = RandomForestRegressor(random_state=0, n_estimators=100)
+estimator = RandomForestClassifier(random_state=0, n_estimators=100)
 score = cross_val_score(estimator, X_filtered, y_filtered).mean()
 print("Score without the samples containing missing values = %.2f" % score)
 
@@ -71,18 +72,15 @@
 estimator = Pipeline([("imputer", Imputer(missing_values=np.nan,
                                           strategy="mean",
                                           axis=0)),
-                      ("forest", RandomForestRegressor(random_state=0,
-                                                       n_estimators=100))])
+                      ("forest", RandomForestClassifier(random_state=0,
+                                                        n_estimators=100))])
 score = cross_val_score(estimator, X_missing, y_missing).mean()
-
 print("Score after imputation of the missing values (mean) = %.2f" % score)
 
 # Estimate the score after imputation using non-negative matrix factorization
-estimator = Pipeline([("imputer", ImputerNMF(missing_values=np.nan,
-                                             n_components=3,
-                                             max_iter=2000,
-                                             random_state=0)),
-                      ("forest", RandomForestRegressor(random_state=0,
-                                                       n_estimators=100))])
+estimator = Pipeline([("nmf", NMF(solver='mu', init='random', n_components=19,
+                                  random_state=0, max_iter=1000)),
+                      ("forest", RandomForestClassifier(random_state=0,
+                                                        n_estimators=100))])
 score = cross_val_score(estimator, X_missing, y_missing).mean()
 print("Score after imputation of the missing values (NMF) = %.2f" % score)
diff --git a/setup.cfg b/setup.cfg
index 378905311e..02b3015e87 100755
--- a/setup.cfg
+++ b/setup.cfg
@@ -38,6 +38,10 @@ artifact_indexes=
     # https://ci.appveyor.com/project/sklearn-ci/scikit-learn/
     http://windows-wheels.scikit-learn.org/
 
+[flake8]
+# Default flake8 3.5 ignored flags
+ignore=E121,E123,E126,E226,E24,E704,W503,W504
+
 # Uncomment the following under windows to build using:
 # http://sourceforge.net/projects/mingw/
 
diff --git a/sklearn/__check_build/__init__.py b/sklearn/__check_build/__init__.py
index 5a4018789a..6c1cdfd9fc 100755
--- a/sklearn/__check_build/__init__.py
+++ b/sklearn/__check_build/__init__.py
@@ -1,5 +1,5 @@
 """ Module to give helpful messages to the user that did not
-compile the scikit properly.
+compile scikit-learn properly.
 """
 import os
 
diff --git a/sklearn/__init__.py b/sklearn/__init__.py
index c45728106a..5f2278d1c8 100755
--- a/sklearn/__init__.py
+++ b/sklearn/__init__.py
@@ -127,7 +127,7 @@ def config_context(**new_config):
 
 if __SKLEARN_SETUP__:
     sys.stderr.write('Partial import of sklearn during the build process.\n')
-    # We are not importing the rest of the scikit during the build
+    # We are not importing the rest of scikit-learn during the build
     # process, as it may not be compiled yet
 else:
     from . import __check_build
diff --git a/sklearn/base.py b/sklearn/base.py
index 81c7e5dae7..6f59cea3c7 100755
--- a/sklearn/base.py
+++ b/sklearn/base.py
@@ -551,7 +551,6 @@ def is_classifier(estimator):
 def is_regressor(estimator):
     """Returns True if the given estimator is (probably) a regressor.
 
-
     Parameters
     ----------
     estimator : object
diff --git a/sklearn/decomposition/__init__.py b/sklearn/decomposition/__init__.py
index 352f566703..faca56b91b 100755
--- a/sklearn/decomposition/__init__.py
+++ b/sklearn/decomposition/__init__.py
@@ -4,7 +4,7 @@
 this module can be regarded as dimensionality reduction techniques.
 """
 
-from .nmf import NMF, non_negative_factorization, ImputerNMF
+from .nmf import NMF, non_negative_factorization
 from .pca import PCA, RandomizedPCA
 from .incremental_pca import IncrementalPCA
 from .kernel_pca import KernelPCA
@@ -25,7 +25,6 @@
            'MiniBatchDictionaryLearning',
            'MiniBatchSparsePCA',
            'NMF',
-           'ImputerNMF',
            'PCA',
            'RandomizedPCA',
            'SparseCoder',
diff --git a/sklearn/decomposition/nmf.py b/sklearn/decomposition/nmf.py
index 060c221a4e..048bc2d0a9 100755
--- a/sklearn/decomposition/nmf.py
+++ b/sklearn/decomposition/nmf.py
@@ -231,7 +231,7 @@ def _beta_divergence(X, W, H, beta, square_root=False):
         return res
 
 
-def _special_dot_X(W, H, X):
+def _special_dot_X(W, H, X, out=None):
     """Computes np.dot(W, H) in a special way:
 
     - If X is sparse, np.dot(W, H) is computed only where X is non zero,
@@ -247,11 +247,11 @@ def _special_dot_X(W, H, X):
         WH = sp.coo_matrix((dot_vals, (ii, jj)), shape=X.shape)
         return WH.tocsr()
     elif isinstance(X, np.ma.masked_array):
-        WH = np.ma.masked_array(np.dot(W, H), mask=X.mask)
-        WH.unshare_mask()
+        WH = np.ma.masked_array(np.dot(W, H, out=out), mask=X.mask)
+        WH._sharedmask = False
         return WH
     else:
-        return np.dot(W, H)
+        return np.dot(W, H, out=out)
 
 
 def _safe_dot(X, Ht):
@@ -608,7 +608,8 @@ def _fit_coordinate_descent(X, W, H, tol=1e-4, max_iter=200, l1_reg_W=0,
 
 
 def _multiplicative_update_w(X, W, H, beta_loss, l1_reg_W, l2_reg_W, gamma,
-                             H_sum=None, HHt=None, XHt=None, update_H=True):
+                             WH, H_sum=None, HHt=None, XHt=None,
+                             update_H=True):
     """update W in Multiplicative Update NMF"""
     X_mask = X.mask if isinstance(X, np.ma.masked_array) else False
 
@@ -629,13 +630,13 @@ def _multiplicative_update_w(X, W, H, beta_loss, l1_reg_W, l2_reg_W, gamma,
                 HHt = np.dot(H, H.T)
             denominator = np.dot(W, HHt)
         else:
-            WH = _special_dot_X(W, H, X)
+            WH = _special_dot_X(W, H, X, out=WH)
             denominator = _safe_dot(WH, H.T)
 
     else:
         # Numerator
         # if X is sparse, compute WH only where X is non zero
-        WH_safe_X = _special_dot_X(W, H, X)
+        WH_safe_X = _special_dot_X(W, H, X, out=WH)
         if sp.issparse(X):
             WH_safe_X_data = WH_safe_X.data
             X_data = X.data
@@ -718,7 +719,8 @@ def _multiplicative_update_w(X, W, H, beta_loss, l1_reg_W, l2_reg_W, gamma,
     return delta_W, H_sum, HHt, XHt
 
 
-def _multiplicative_update_h(X, W, H, beta_loss, l1_reg_H, l2_reg_H, gamma):
+def _multiplicative_update_h(X, W, H, beta_loss, l1_reg_H, l2_reg_H, gamma,
+                             WH):
     """update H in Multiplicative Update NMF"""
     X_mask = X.mask if isinstance(X, np.ma.masked_array) else False
 
@@ -728,12 +730,12 @@ def _multiplicative_update_h(X, W, H, beta_loss, l1_reg_H, l2_reg_H, gamma):
             denominator = np.dot(np.dot(W.T, W), H)
         else:
             numerator = _safe_dot(W.T, X)
-            WH = _special_dot_X(W, H, X)
+            WH = _special_dot_X(W, H, X, out=WH)
             denominator = _safe_dot(W.T, WH)
 
     else:
         # Numerator
-        WH_safe_X = _special_dot_X(W, H, X)
+        WH_safe_X = _special_dot_X(W, H, X, out=WH)
         if sp.issparse(X):
             WH_safe_X_data = WH_safe_X.data
             X_data = X.data
@@ -895,6 +897,9 @@ def _fit_multiplicative_update(X, W, H, beta_loss='frobenius',
     else:
         gamma = 1.
 
+    # allocate memory for the product np.dot(W, H)
+    WH = np.empty(X.shape) if not sp.issparse(X) else None
+
     # transform in a numpy masked array if X contains missing (NaN) values
     if not sp.issparse(X):
         X_mask = np.isnan(X)
@@ -910,7 +915,7 @@ def _fit_multiplicative_update(X, W, H, beta_loss='frobenius',
         # update W
         # H_sum, HHt and XHt are saved and reused if not update_H
         delta_W, H_sum, HHt, XHt = _multiplicative_update_w(
-            X, W, H, beta_loss, l1_reg_W, l2_reg_W, gamma,
+            X, W, H, beta_loss, l1_reg_W, l2_reg_W, gamma, WH,
             H_sum, HHt, XHt, update_H)
         W *= delta_W
 
@@ -921,7 +926,7 @@ def _fit_multiplicative_update(X, W, H, beta_loss='frobenius',
         # update H
         if update_H:
             delta_H = _multiplicative_update_h(X, W, H, beta_loss, l1_reg_H,
-                                               l2_reg_H, gamma)
+                                               l2_reg_H, gamma, WH)
             H *= delta_H
 
             # These values will be recomputed since H changed
@@ -1449,203 +1454,3 @@ def inverse_transform(self, W):
         """
         check_is_fitted(self, 'n_components_')
         return np.dot(W, self.components_)
-
-
-def _get_mask(X, value_to_mask):
-    """Compute the boolean mask X == missing_values."""
-    if value_to_mask == "NaN" or np.isnan(value_to_mask):
-        return np.isnan(X)
-    else:
-        return X == value_to_mask
-
-
-class ImputerNMF(BaseEstimator, TransformerMixin):
-    """Imputation transformer for completing missing values, using NMF
-
-    Parameters
-    ----------
-    missing_values : integer or "NaN", optional (default="NaN")
-        The placeholder for the missing values. All occurrences of
-        `missing_values` will be imputed. For missing values encoded as np.nan,
-        use the string value "NaN".
-
-    n_components : int or None
-        Number of components, if n_components is not set all features
-        are kept.
-
-    init :  'random' | 'custom', default 'random'
-        Method used to initialize the procedure.
-        Valid options:
-
-        - 'random': non-negative random matrices, scaled with:
-            sqrt(X.mean() / n_components)
-
-        - 'custom': use custom matrices W and H
-
-    beta_loss : float or string, default 'frobenius'
-        String must be in {'frobenius', 'kullback-leibler', 'itakura-saito'}.
-        Beta divergence to be minimized, measuring the distance between X
-        and the dot product WH. Note that values different from 'frobenius'
-        (or 2) and 'kullback-leibler' (or 1) lead to significantly slower
-        fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input
-        matrix X cannot contain zeros.
-
-    tol : float, default: 1e-4
-        Tolerance of the stopping condition.
-
-    max_iter : integer, default: 200
-        Maximum number of iterations before timing out.
-
-    random_state : int, RandomState instance or None, optional, default: None
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
-
-    alpha : double, default: 0.
-        Constant that multiplies the regularization terms. Set it to zero to
-        have no regularization.
-
-    l1_ratio : double, default: 0.
-        The regularization mixing parameter, with 0 <= l1_ratio <= 1.
-        For l1_ratio = 0 the penalty is an elementwise L2 penalty
-        (aka Frobenius Norm).
-        For l1_ratio = 1 it is an elementwise L1 penalty.
-        For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.
-
-    verbose : bool, default=False
-        Whether to be verbose.
-
-    Attributes
-    ----------
-    components_ : array, [n_components, n_features]
-        Factorization matrix, sometimes called 'dictionary'.
-
-    activations_ : array, [n_samples, n_components]
-        Factorization matrix.
-
-    reconstruction_err_ : number
-        Frobenius norm of the matrix difference, or beta-divergence, between
-        the training data ``X`` and the reconstructed data ``WH`` from
-        the fitted model.
-
-    mask_ : array, [n_samples, n_features]
-        Boolean array of masked values in the last transformed array.
-
-    n_iter_ : int
-        Actual number of iterations.
-
-    """
-
-    def __init__(self, missing_values="NaN", n_components=None, init='random',
-                 beta_loss='frobenius', tol=1e-4, max_iter=200,
-                 random_state=None, alpha=0., l1_ratio=0., verbose=0):
-        self.missing_values = missing_values
-        self.n_components = n_components
-        self.init = init
-        self.beta_loss = beta_loss
-        self.tol = tol
-        self.max_iter = max_iter
-        self.random_state = random_state
-        self.alpha = alpha
-        self.l1_ratio = l1_ratio
-        self.verbose = verbose
-
-    def fit_transform(self, X, y=None, W=None, H=None):
-        """Learn a NMF model for the data X and returns the transformed data.
-
-        This is more efficient than calling fit followed by transform.
-
-        Parameters
-        ----------
-        X : {array-like, sparse matrix}, shape (n_samples, n_features)
-            Data matrix to be decomposed
-
-        y : Ignored
-
-        W : array-like, shape (n_samples, n_components)
-            If init='custom', it is used as initial guess for the solution.
-
-        H : array-like, shape (n_components, n_features)
-            If init='custom', it is used as initial guess for the solution.
-
-        Returns
-        -------
-        X : array, shape (n_samples, n_features)
-            Transformed data, computed through np.dot(W, H)
-        """
-        # XXX: copy=False and pass a masked array to non_negative_factorization
-        X = check_array(X, dtype=float, force_all_finite=False, copy=True)
-        mask = _get_mask(X, self.missing_values)
-        X[mask] = np.nan
-
-        W, H, n_iter_ = non_negative_factorization(
-            X=X, W=W, H=H, n_components=self.n_components,
-            init=self.init, update_H=True, solver='mu',
-            beta_loss=self.beta_loss, tol=self.tol, max_iter=self.max_iter,
-            alpha=self.alpha, l1_ratio=self.l1_ratio, regularization='both',
-            random_state=self.random_state, verbose=self.verbose,
-            shuffle=False)
-
-        self.reconstruction_err_ = _beta_divergence(X, W, H, self.beta_loss,
-                                                    square_root=True)
-        self.mask_ = mask
-        self.n_components_ = H.shape[0]
-        self.components_ = H
-        self.activations_ = W
-        self.n_iter_ = n_iter_
-
-        X[mask] = np.dot(W, H)[mask]
-        return X
-
-    def fit(self, X, y=None, **params):
-        """Learn a NMF model for the data X.
-
-        Parameters
-        ----------
-        X : {array-like, sparse matrix}, shape (n_samples, n_features)
-            Data matrix to be decomposed
-
-        y : Ignored
-
-        Returns
-        -------
-        self
-        """
-        self.fit_transform(X, **params)
-        return self
-
-    def transform(self, X):
-        """Transform the data X according to the fitted NMF model
-
-        Parameters
-        ----------
-        X : array-like, shape (n_samples, n_features)
-            Data matrix to be transformed by the model
-
-        Returns
-        -------
-        X : array, shape (n_samples, n_features)
-            Transformed data
-        """
-        check_is_fitted(self, 'n_components_')
-
-        # XXX: copy=False and pass a masked array to non_negative_factorization
-        X = check_array(X, dtype=float, force_all_finite=False, copy=True)
-        mask = _get_mask(X, self.missing_values)
-        X[mask] = np.nan
-
-        W, _, n_iter_ = non_negative_factorization(
-            X=X, W=None, H=self.components_, n_components=self.n_components_,
-            init=self.init, update_H=False, solver='mu',
-            beta_loss=self.beta_loss, tol=self.tol, max_iter=self.max_iter,
-            alpha=self.alpha, l1_ratio=self.l1_ratio, regularization='both',
-            random_state=self.random_state, verbose=self.verbose,
-            shuffle=False)
-
-        self.activations_ = W
-        self.mask_ = mask
-        self.n_iter_ = n_iter_
-
-        X[mask] = np.dot(W, self.components_)[mask]
-        return X
diff --git a/sklearn/linear_model/bayes.py b/sklearn/linear_model/bayes.py
index a094eec0cd..e754613cda 100755
--- a/sklearn/linear_model/bayes.py
+++ b/sklearn/linear_model/bayes.py
@@ -11,7 +11,7 @@
 from scipy import linalg
 from scipy.linalg import pinvh
 
-from .base import LinearModel
+from .base import LinearModel, _rescale_data
 from ..base import RegressorMixin
 from ..utils.extmath import fast_logdet
 from ..utils import check_X_y
@@ -140,7 +140,7 @@ def __init__(self, n_iter=300, tol=1.e-3, alpha_1=1.e-6, alpha_2=1.e-6,
         self.copy_X = copy_X
         self.verbose = verbose
 
-    def fit(self, X, y):
+    def fit(self, X, y, sample_weight=None):
         """Fit the model
 
         Parameters
@@ -150,13 +150,25 @@ def fit(self, X, y):
         y : numpy array of shape [n_samples]
             Target values. Will be cast to X's dtype if necessary
 
+        sample_weight : numpy array of shape [n_samples]
+            Individual weights for each sample
+
+            .. versionadded:: 0.20
+               parameter *sample_weight* support to BayesianRidge.
+
         Returns
         -------
         self : returns an instance of self.
         """
         X, y = check_X_y(X, y, dtype=np.float64, y_numeric=True)
         X, y, X_offset_, y_offset_, X_scale_ = self._preprocess_data(
-            X, y, self.fit_intercept, self.normalize, self.copy_X)
+            X, y, self.fit_intercept, self.normalize, self.copy_X,
+            sample_weight=sample_weight)
+
+        if sample_weight is not None:
+            # Sample weight can be implemented via a simple rescaling.
+            X, y = _rescale_data(X, y, sample_weight)
+
         self.X_offset_ = X_offset_
         self.X_scale_ = X_scale_
         n_samples, n_features = X.shape
diff --git a/sklearn/linear_model/tests/test_bayes.py b/sklearn/linear_model/tests/test_bayes.py
index 492f77d693..5337c0a19c 100755
--- a/sklearn/linear_model/tests/test_bayes.py
+++ b/sklearn/linear_model/tests/test_bayes.py
@@ -50,6 +50,21 @@ def test_bayesian_ridge_parameter():
     assert_almost_equal(rr_model.intercept_, br_model.intercept_)
 
 
+def test_bayesian_sample_weights():
+    # Test correctness of the sample_weights method
+    X = np.array([[1, 1], [3, 4], [5, 7], [4, 1], [2, 6], [3, 10], [3, 2]])
+    y = np.array([1, 2, 3, 2, 0, 4, 5]).T
+    w = np.array([4, 3, 3, 1, 1, 2, 3]).T
+
+    # A Ridge regression model using an alpha value equal to the ratio of
+    # lambda_ and alpha_ from the Bayesian Ridge model must be identical
+    br_model = BayesianRidge(compute_score=True).fit(X, y, sample_weight=w)
+    rr_model = Ridge(alpha=br_model.lambda_ / br_model.alpha_).fit(
+        X, y, sample_weight=w)
+    assert_array_almost_equal(rr_model.coef_, br_model.coef_)
+    assert_almost_equal(rr_model.intercept_, br_model.intercept_)
+
+
 def test_toy_bayesian_ridge_object():
     # Test BayesianRidge on toy
     X = np.array([[1], [2], [6], [8], [10]])
diff --git a/sklearn/neighbors/regression.py b/sklearn/neighbors/regression.py
index bd2ffb9b82..b13f16cfd3 100755
--- a/sklearn/neighbors/regression.py
+++ b/sklearn/neighbors/regression.py
@@ -9,6 +9,7 @@
 # License: BSD 3 clause (C) INRIA, University of Amsterdam
 
 import numpy as np
+from scipy.sparse import issparse
 
 from .base import _get_weights, _check_weights, NeighborsBase, KNeighborsMixin
 from .base import RadiusNeighborsMixin, SupervisedFloatMixin
@@ -139,6 +140,11 @@ def predict(self, X):
         y : array of int, shape = [n_samples] or [n_samples, n_outputs]
             Target values
         """
+        if issparse(X) and self.metric == 'precomputed':
+            raise ValueError(
+                "Sparse matrices not supported for prediction with "
+                "precomputed kernels. Densify your matrix."
+            )
         X = check_array(X, accept_sparse='csr')
 
         neigh_dist, neigh_ind = self.kneighbors(X)
diff --git a/sklearn/neighbors/tests/test_neighbors.py b/sklearn/neighbors/tests/test_neighbors.py
index 052c83c71d..ceb5341201 100755
--- a/sklearn/neighbors/tests/test_neighbors.py
+++ b/sklearn/neighbors/tests/test_neighbors.py
@@ -2,7 +2,7 @@
 
 import numpy as np
 from scipy.sparse import (bsr_matrix, coo_matrix, csc_matrix, csr_matrix,
-                          dok_matrix, lil_matrix)
+                          dok_matrix, lil_matrix, issparse)
 
 from sklearn import metrics
 from sklearn import neighbors, datasets
@@ -731,10 +731,22 @@ def test_kneighbors_regressor_sparse(n_samples=40,
         knn = neighbors.KNeighborsRegressor(n_neighbors=n_neighbors,
                                             algorithm='auto')
         knn.fit(sparsemat(X), y)
+
+        knn_pre = neighbors.KNeighborsRegressor(n_neighbors=n_neighbors,
+                                                metric='precomputed')
+        knn_pre.fit(pairwise_distances(X, metric='euclidean'), y)
+
         for sparsev in SPARSE_OR_DENSE:
             X2 = sparsev(X)
             assert_true(np.mean(knn.predict(X2).round() == y) > 0.95)
 
+            X2_pre = sparsev(pairwise_distances(X, metric='euclidean'))
+            if issparse(sparsev(X2_pre)):
+                assert_raises(ValueError, knn_pre.predict, X2_pre)
+            else:
+                assert_true(
+                    np.mean(knn_pre.predict(X2_pre).round() == y) > 0.95)
+
 
 def test_neighbors_iris():
     # Sanity checks on the iris dataset
diff --git a/sklearn/preprocessing/label.py b/sklearn/preprocessing/label.py
index 530f376c19..88f1774367 100755
--- a/sklearn/preprocessing/label.py
+++ b/sklearn/preprocessing/label.py
@@ -160,7 +160,7 @@ class LabelBinarizer(BaseEstimator, TransformerMixin):
     """Binarize labels in a one-vs-all fashion
 
     Several regression and binary classification algorithms are
-    available in the scikit. A simple way to extend these algorithms
+    available in scikit-learn. A simple way to extend these algorithms
     to the multi-class classification case is to use the so-called
     one-vs-all scheme.
 
@@ -393,7 +393,7 @@ def label_binarize(y, classes, neg_label=0, pos_label=1, sparse_output=False):
     """Binarize labels in a one-vs-all fashion
 
     Several regression and binary classification algorithms are
-    available in the scikit. A simple way to extend these algorithms
+    available in scikit-learn. A simple way to extend these algorithms
     to the multi-class classification case is to use the so-called
     one-vs-all scheme.
 
diff --git a/sklearn/tests/test_common.py b/sklearn/tests/test_common.py
index dde6f4c41c..908240cdaf 100755
--- a/sklearn/tests/test_common.py
+++ b/sklearn/tests/test_common.py
@@ -77,7 +77,7 @@ def test_non_meta_estimators():
 
 def test_configure():
     # Smoke test the 'configure' step of setup, this tests all the
-    # 'configure' functions in the setup.pys in the scikit
+    # 'configure' functions in the setup.pys in scikit-learn
     cwd = os.getcwd()
     setup_path = os.path.abspath(os.path.join(sklearn.__path__[0], '..'))
     setup_filename = os.path.join(setup_path, 'setup.py')
diff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py
index fdbecc358b..40fcb1fdd0 100755
--- a/sklearn/utils/estimator_checks.py
+++ b/sklearn/utils/estimator_checks.py
@@ -37,6 +37,7 @@
 
 from sklearn.base import (clone, TransformerMixin, ClusterMixin,
                           BaseEstimator, is_classifier, is_regressor)
+
 from sklearn.metrics import accuracy_score, adjusted_rand_score, f1_score
 
 from sklearn.random_projection import BaseRandomProjection
@@ -48,6 +49,8 @@
 from sklearn.exceptions import DataConversionWarning
 from sklearn.exceptions import SkipTestWarning
 from sklearn.model_selection import train_test_split
+from sklearn.metrics.pairwise import (rbf_kernel, linear_kernel,
+                                      pairwise_distances)
 
 from sklearn.utils import shuffle
 from sklearn.utils.fixes import signature
@@ -355,10 +358,56 @@ def _is_32bit():
     return struct.calcsize('P') * 8 == 32
 
 
+def _is_pairwise(estimator):
+    """Returns True if estimator has a _pairwise attribute set to True.
+
+    Parameters
+    ----------
+    estimator : object
+        Estimator object to test.
+
+    Returns
+    -------
+    out : bool
+        True if _pairwise is set to True and False otherwise.
+    """
+    return bool(getattr(estimator, "_pairwise", False))
+
+
+def _is_pairwise_metric(estimator):
+    """Returns True if estimator accepts pairwise metric.
+
+    Parameters
+    ----------
+    estimator : object
+        Estimator object to test.
+
+    Returns
+    -------
+    out : bool
+        True if _pairwise is set to True and False otherwise.
+    """
+    metric = getattr(estimator,  "metric", None)
+
+    return bool(metric == 'precomputed')
+
+
+def pairwise_estimator_convert_X(X, estimator, kernel=linear_kernel):
+
+    if _is_pairwise_metric(estimator):
+        return pairwise_distances(X, metric='euclidean')
+    if _is_pairwise(estimator):
+        return kernel(X, X)
+
+    return X
+
+
 def check_estimator_sparse_data(name, estimator_orig):
+
     rng = np.random.RandomState(0)
     X = rng.rand(40, 10)
     X[X < .8] = 0
+    X = pairwise_estimator_convert_X(X, estimator_orig)
     X_csr = sparse.csr_matrix(X)
     y = (4 * rng.rand(40)).astype(np.int)
     # catch deprecation warnings
@@ -383,8 +432,8 @@ def check_estimator_sparse_data(name, estimator_orig):
             if hasattr(estimator, 'predict_proba'):
                 probs = estimator.predict_proba(X)
                 assert_equal(probs.shape, (X.shape[0], 4))
-        except TypeError as e:
-            if 'sparse' not in repr(e):
+        except (TypeError, ValueError) as e:
+            if 'sparse' not in repr(e).lower():
                 print("Estimator %s doesn't seem to fail gracefully on "
                       "sparse data: error message state explicitly that "
                       "sparse input is not supported if this is not the case."
@@ -405,7 +454,8 @@ def check_sample_weights_pandas_series(name, estimator_orig):
     if has_fit_parameter(estimator, "sample_weight"):
         try:
             import pandas as pd
-            X = pd.DataFrame([[1, 1], [1, 2], [1, 3], [2, 1], [2, 2], [2, 3]])
+            X = np.array([[1, 1], [1, 2], [1, 3], [2, 1], [2, 2], [2, 3]])
+            X = pd.DataFrame(pairwise_estimator_convert_X(X, estimator_orig))
             y = pd.Series([1, 1, 1, 2, 2, 2])
             weights = pd.Series([1] * 6)
             try:
@@ -426,7 +476,8 @@ def check_sample_weights_list(name, estimator_orig):
     if has_fit_parameter(estimator_orig, "sample_weight"):
         estimator = clone(estimator_orig)
         rnd = np.random.RandomState(0)
-        X = rnd.uniform(size=(10, 3))
+        X = pairwise_estimator_convert_X(rnd.uniform(size=(10, 3)),
+                                         estimator_orig)
         y = np.arange(10) % 3
         y = multioutput_estimator_convert_y_2d(estimator, y)
         sample_weight = [3] * 10
@@ -438,7 +489,8 @@ def check_sample_weights_list(name, estimator_orig):
 def check_dtype_object(name, estimator_orig):
     # check that estimators treat dtype object as numeric if possible
     rng = np.random.RandomState(0)
-    X = rng.rand(40, 10).astype(object)
+    X = pairwise_estimator_convert_X(rng.rand(40, 10), estimator_orig)
+    X = X.astype(object)
     y = (X[:, 0] * 4).astype(np.int)
     estimator = clone(estimator_orig)
     y = multioutput_estimator_convert_y_2d(estimator, y)
@@ -485,6 +537,8 @@ def check_dict_unchanged(name, estimator_orig):
     else:
         X = 2 * rnd.uniform(size=(20, 3))
 
+    X = pairwise_estimator_convert_X(X, estimator_orig)
+
     y = X[:, 0].astype(np.int)
     estimator = clone(estimator_orig)
     y = multioutput_estimator_convert_y_2d(estimator, y)
@@ -522,6 +576,7 @@ def check_dont_overwrite_parameters(name, estimator_orig):
     estimator = clone(estimator_orig)
     rnd = np.random.RandomState(0)
     X = 3 * rnd.uniform(size=(20, 3))
+    X = pairwise_estimator_convert_X(X, estimator_orig)
     y = X[:, 0].astype(np.int)
     y = multioutput_estimator_convert_y_2d(estimator, y)
 
@@ -568,6 +623,7 @@ def check_fit2d_predict1d(name, estimator_orig):
     # check by fitting a 2d array and predicting with a 1d array
     rnd = np.random.RandomState(0)
     X = 3 * rnd.uniform(size=(20, 3))
+    X = pairwise_estimator_convert_X(X, estimator_orig)
     y = X[:, 0].astype(np.int)
     estimator = clone(estimator_orig)
     y = multioutput_estimator_convert_y_2d(estimator, y)
@@ -621,6 +677,7 @@ def check_fit2d_1feature(name, estimator_orig):
     # informative message
     rnd = np.random.RandomState(0)
     X = 3 * rnd.uniform(size=(10, 1))
+    X = pairwise_estimator_convert_X(X, estimator_orig)
     y = X[:, 0].astype(np.int)
     estimator = clone(estimator_orig)
     y = multioutput_estimator_convert_y_2d(estimator, y)
@@ -793,6 +850,7 @@ def check_pipeline_consistency(name, estimator_orig):
     X, y = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],
                       random_state=0, n_features=2, cluster_std=0.1)
     X -= X.min()
+    X = pairwise_estimator_convert_X(X, estimator_orig, kernel=rbf_kernel)
     estimator = clone(estimator_orig)
     y = multioutput_estimator_convert_y_2d(estimator, y)
     set_random_state(estimator)
@@ -817,6 +875,7 @@ def check_fit_score_takes_y(name, estimator_orig):
     # in fit and score so they can be used in pipelines
     rnd = np.random.RandomState(0)
     X = rnd.uniform(size=(10, 3))
+    X = pairwise_estimator_convert_X(X, estimator_orig)
     y = np.arange(10) % 3
     estimator = clone(estimator_orig)
     y = multioutput_estimator_convert_y_2d(estimator, y)
@@ -842,6 +901,7 @@ def check_fit_score_takes_y(name, estimator_orig):
 def check_estimators_dtypes(name, estimator_orig):
     rnd = np.random.RandomState(0)
     X_train_32 = 3 * rnd.uniform(size=(20, 5)).astype(np.float32)
+    X_train_32 = pairwise_estimator_convert_X(X_train_32, estimator_orig)
     X_train_64 = X_train_32.astype(np.float64)
     X_train_int_64 = X_train_32.astype(np.int64)
     X_train_int_32 = X_train_32.astype(np.int32)
@@ -887,7 +947,8 @@ def check_estimators_empty_data_messages(name, estimator_orig):
 def check_estimators_nan_inf(name, estimator_orig):
     # Checks that Estimator X's do not contain NaN or inf.
     rnd = np.random.RandomState(0)
-    X_train_finite = rnd.uniform(size=(10, 3))
+    X_train_finite = pairwise_estimator_convert_X(rnd.uniform(size=(10, 3)),
+                                                  estimator_orig)
     X_train_nan = rnd.uniform(size=(10, 3))
     X_train_nan[0, 0] = np.nan
     X_train_inf = rnd.uniform(size=(10, 3))
@@ -964,6 +1025,7 @@ def check_estimators_pickle(name, estimator_orig):
 
     # some estimators can't do features less than 0
     X -= X.min()
+    X = pairwise_estimator_convert_X(X, estimator_orig, kernel=rbf_kernel)
 
     estimator = clone(estimator_orig)
 
@@ -1138,6 +1200,7 @@ def check_classifiers_train(name, classifier_orig):
         classifier = clone(classifier_orig)
         if name in ['BernoulliNB', 'MultinomialNB', 'ComplementNB']:
             X -= X.min()
+        X = pairwise_estimator_convert_X(X, classifier_orig)
         set_random_state(classifier)
         # raises error on malformed input for fit
         with assert_raises(ValueError, msg="The classifer {} does not"
@@ -1159,11 +1222,18 @@ def check_classifiers_train(name, classifier_orig):
             assert_greater(accuracy_score(y, y_pred), 0.83)
 
         # raises error on malformed input for predict
-        with assert_raises(ValueError, msg="The classifier {} does not"
-                           " raise an error when the number of features "
-                           "in predict is different from the number of"
-                           " features in fit.".format(name)):
-            classifier.predict(X.T)
+        if _is_pairwise(classifier):
+            with assert_raises(ValueError, msg="The classifier {} does not"
+                               " raise an error when shape of X"
+                               "in predict is not equal to (n_test_samples,"
+                               "n_training_samples)".format(name)):
+                classifier.predict(X.reshape(-1, 1))
+        else:
+            with assert_raises(ValueError, msg="The classifier {} does not"
+                               " raise an error when the number of features "
+                               "in predict is different from the number of"
+                               " features in fit.".format(name)):
+                classifier.predict(X.T)
         if hasattr(classifier, "decision_function"):
             try:
                 # decision_function agrees with predict
@@ -1179,12 +1249,21 @@ def check_classifiers_train(name, classifier_orig):
                     assert_array_equal(np.argmax(decision, axis=1), y_pred)
 
                 # raises error on malformed input for decision_function
-                with assert_raises(ValueError, msg="The classifier {} does"
-                                   " not raise an error when the number of "
-                                   "features in decision_function is "
-                                   "different from the number of features"
-                                   " in fit.".format(name)):
-                    classifier.decision_function(X.T)
+                if _is_pairwise(classifier):
+                    with assert_raises(ValueError, msg="The classifier {} does"
+                                       " not raise an error when the  "
+                                       "shape of X in decision_function is "
+                                       "not equal to (n_test_samples, "
+                                       "n_training_samples) in fit."
+                                       .format(name)):
+                        classifier.decision_function(X.reshape(-1, 1))
+                else:
+                    with assert_raises(ValueError, msg="The classifier {} does"
+                                       " not raise an error when the number "
+                                       "of features in decision_function is "
+                                       "different from the number of features"
+                                       " in fit.".format(name)):
+                        classifier.decision_function(X.T)
             except NotImplementedError:
                 pass
         if hasattr(classifier, "predict_proba"):
@@ -1195,11 +1274,20 @@ def check_classifiers_train(name, classifier_orig):
             # check that probas for all classes sum to one
             assert_allclose(np.sum(y_prob, axis=1), np.ones(n_samples))
             # raises error on malformed input for predict_proba
-            with assert_raises(ValueError, msg="The classifier {} does not"
-                               " raise an error when the number of features "
-                               "in predict_proba is different from the number "
-                               "of features in fit.".format(name)):
-                classifier.predict_proba(X.T)
+            if _is_pairwise(classifier_orig):
+                with assert_raises(ValueError, msg="The classifier {} does not"
+                                   " raise an error when the shape of X"
+                                   "in predict_proba is not equal to "
+                                   "(n_test_samples, n_training_samples)."
+                                   .format(name)):
+                    classifier.predict_proba(X.reshape(-1, 1))
+            else:
+                with assert_raises(ValueError, msg="The classifier {} does not"
+                                   " raise an error when the number of "
+                                   "features in predict_proba is different "
+                                   "from the number of features in fit."
+                                   .format(name)):
+                    classifier.predict_proba(X.T)
             if hasattr(classifier, "predict_log_proba"):
                 # predict_log_proba is a transformation of predict_proba
                 y_log_prob = classifier.predict_log_proba(X)
@@ -1213,6 +1301,7 @@ def check_estimators_fit_returns_self(name, estimator_orig):
     X, y = make_blobs(random_state=0, n_samples=9, n_features=4)
     # some want non-negative input
     X -= X.min()
+    X = pairwise_estimator_convert_X(X, estimator_orig)
 
     estimator = clone(estimator_orig)
     y = multioutput_estimator_convert_y_2d(estimator, y)
@@ -1260,7 +1349,7 @@ def check_supervised_y_2d(name, estimator_orig):
         # These only work on 2d, so this test makes no sense
         return
     rnd = np.random.RandomState(0)
-    X = rnd.uniform(size=(10, 3))
+    X = pairwise_estimator_convert_X(rnd.uniform(size=(10, 3)), estimator_orig)
     y = np.arange(10) % 3
     estimator = clone(estimator_orig)
     set_random_state(estimator)
@@ -1294,6 +1383,7 @@ def check_classifiers_classes(name, classifier_orig):
     # We need to make sure that we have non negative data, for things
     # like NMF
     X -= X.min() - .1
+    X = pairwise_estimator_convert_X(X, classifier_orig)
     y_names = np.array(["one", "two", "three"])[y]
 
     for y_names in [y_names, y_names.astype('O')]:
@@ -1325,7 +1415,7 @@ def check_classifiers_classes(name, classifier_orig):
 @ignore_warnings(category=(DeprecationWarning, FutureWarning))
 def check_regressors_int(name, regressor_orig):
     X, _ = _boston_subset()
-    X = X[:50]
+    X = pairwise_estimator_convert_X(X[:50], regressor_orig)
     rnd = np.random.RandomState(0)
     y = rnd.randint(3, size=X.shape[0])
     y = multioutput_estimator_convert_y_2d(regressor_orig, y)
@@ -1353,6 +1443,7 @@ def check_regressors_int(name, regressor_orig):
 @ignore_warnings(category=(DeprecationWarning, FutureWarning))
 def check_regressors_train(name, regressor_orig):
     X, y = _boston_subset()
+    X = pairwise_estimator_convert_X(X, regressor_orig)
     y = StandardScaler().fit_transform(y.reshape(-1, 1))  # X is already scaled
     y = y.ravel()
     regressor = clone(regressor_orig)
@@ -1429,6 +1520,12 @@ def check_class_weight_classifiers(name, classifier_orig):
         X, y = make_blobs(centers=n_centers, random_state=0, cluster_std=20)
         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,
                                                             random_state=0)
+
+        # can't use gram_if_pairwise() here, setting up gram matrix manually
+        if _is_pairwise(classifier_orig):
+            X_test = rbf_kernel(X_test, X_train)
+            X_train = rbf_kernel(X_train, X_train)
+
         n_centers = len(np.unique(y_train))
 
         if n_centers == 2:
@@ -1512,6 +1609,7 @@ def check_estimators_overwrite_params(name, estimator_orig):
     X, y = make_blobs(random_state=0, n_samples=9)
     # some want non-negative input
     X -= X.min()
+    X = pairwise_estimator_convert_X(X, estimator_orig, kernel=rbf_kernel)
     estimator = clone(estimator_orig)
     y = multioutput_estimator_convert_y_2d(estimator, y)
 
@@ -1586,6 +1684,7 @@ def check_sparsify_coefficients(name, estimator_orig):
 @ignore_warnings(category=DeprecationWarning)
 def check_classifier_data_not_an_array(name, estimator_orig):
     X = np.array([[3, 0], [0, 1], [0, 2], [1, 1], [1, 2], [2, 1]])
+    X = pairwise_estimator_convert_X(X, estimator_orig)
     y = [1, 1, 1, 2, 2, 2]
     y = multioutput_estimator_convert_y_2d(estimator_orig, y)
     check_estimators_data_not_an_array(name, estimator_orig, X, y)
@@ -1594,6 +1693,7 @@ def check_classifier_data_not_an_array(name, estimator_orig):
 @ignore_warnings(category=DeprecationWarning)
 def check_regressor_data_not_an_array(name, estimator_orig):
     X, y = _boston_subset(n_samples=50)
+    X = pairwise_estimator_convert_X(X, estimator_orig)
     y = multioutput_estimator_convert_y_2d(estimator_orig, y)
     check_estimators_data_not_an_array(name, estimator_orig, X, y)
 
diff --git a/sklearn/utils/tests/test_estimator_checks.py b/sklearn/utils/tests/test_estimator_checks.py
index 1b3a1ea7e5..2323f8a634 100755
--- a/sklearn/utils/tests/test_estimator_checks.py
+++ b/sklearn/utils/tests/test_estimator_checks.py
@@ -18,6 +18,8 @@
 from sklearn.cluster import MiniBatchKMeans
 from sklearn.decomposition import NMF
 from sklearn.linear_model import MultiTaskElasticNet
+from sklearn.svm import SVC
+from sklearn.neighbors import KNeighborsRegressor
 from sklearn.utils.validation import check_X_y, check_array
 
 
@@ -251,3 +253,16 @@ def __init__(self):
                         check_no_fit_attributes_set_in_init,
                         'estimator_name',
                         NonConformantEstimator)
+
+
+def test_check_estimator_pairwise():
+    # check that check_estimator() works on estimator with _pairwise
+    # kernel or  metric
+
+    # test precomputed kernel
+    est = SVC(kernel='precomputed')
+    check_estimator(est)
+
+    # test precomputed metric
+    est = KNeighborsRegressor(metric='precomputed')
+    check_estimator(est)
