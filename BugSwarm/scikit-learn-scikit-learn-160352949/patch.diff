diff --git a/examples/preprocessing/plot_boxcox.py b/examples/preprocessing/plot_boxcox.py
new file mode 100644
index 000000000000..5e9ef587afe1
--- /dev/null
+++ b/examples/preprocessing/plot_boxcox.py
@@ -0,0 +1,48 @@
+"""
+============================================
+BoxCox transformation on Boston housing data
+============================================
+
+This example shows the effect of boxcox transform on the boston
+housing dataset.
+
+"""
+
+import numpy as np
+import matplotlib.pyplot as plt
+
+from sklearn.datasets import load_boston
+from sklearn.preprocessing import BoxCoxTransformer
+rng = np.random.RandomState(0)
+
+dataset = load_boston()
+X = dataset.data
+y = dataset.target
+n_samples = X.shape[0]
+
+X = X[:, np.all(X > 0, axis=0)]  # restrict X to positive features
+X_train = X[:n_samples // 2]
+X_test = X[n_samples // 2:]
+
+bct = BoxCoxTransformer(feature_indices=None)
+bct.fit(X_train)
+X_tr = bct.transform(X_test)
+
+num_bins = 50
+
+for i in [0, 10]:
+    fig, (ax1, ax2) = plt.subplots(2, 1)
+    ax1.hist(X_test[:, i], num_bins, normed=1, facecolor='green', alpha=0.5)
+    ax1.set_title('Probplot before Box-Cox transformation %d' % i)
+    ax1.set_xlabel('')
+    ax1.set_ylabel('Probability')
+
+    ax2.hist(X_tr[:, i], num_bins, normed=1, facecolor='green', alpha=0.5)
+    ax2.set_title('Probplot after Box-Cox transformation %d' % i)
+    ax2.set_xlabel('')
+    ax2.set_ylabel('Probability')
+
+    # Tweak spacing to prevent clipping of ylabel
+    plt.subplots_adjust(left=0.15)
+
+plt.show()
diff --git a/sklearn/preprocessing/__init__.py b/sklearn/preprocessing/__init__.py
index cabbd469c10d..ffa23bdd8733 100644
--- a/sklearn/preprocessing/__init__.py
+++ b/sklearn/preprocessing/__init__.py
@@ -20,7 +20,8 @@
 from .data import maxabs_scale
 from .data import minmax_scale
 from .data import OneHotEncoder
-
+from .data import boxcox
+from .data import BoxCoxTransformer
 from .data import PolynomialFeatures
 
 from .label import label_binarize
@@ -54,4 +55,6 @@
     'maxabs_scale',
     'minmax_scale',
     'label_binarize',
+    'boxcox',
+    'BoxCoxTransformer',
 ]
diff --git a/sklearn/preprocessing/data.py b/sklearn/preprocessing/data.py
index e7f242cdedc5..f66d2c12f882 100644
--- a/sklearn/preprocessing/data.py
+++ b/sklearn/preprocessing/data.py
@@ -12,9 +12,11 @@
 
 import numpy as np
 from scipy import sparse
+from scipy import stats
 
 from ..base import BaseEstimator, TransformerMixin
 from ..externals import six
+from ..externals.joblib import Parallel, delayed
 from ..utils import check_array
 from ..utils import deprecated
 from ..utils.extmath import row_norms
@@ -48,7 +50,7 @@
     'scale',
     'robust_scale',
     'maxabs_scale',
-    'minmax_scale',
+    'minmax_scale'
 ]
 
 DEPRECATION_MSG_1D = (
@@ -1673,7 +1675,7 @@ def add_dummy_feature(X, value=1.0):
         return np.hstack((np.ones((n_samples, 1)) * value, X))
 
 
-def _transform_selected(X, transform, selected="all", copy=True):
+def _transform_selected(X, transform, selected="all", copy=True, order=False):
     """Apply a transform function to portion of selected features
 
     Parameters
@@ -1690,6 +1692,10 @@ def _transform_selected(X, transform, selected="all", copy=True):
     selected: "all" or array of indices or mask
         Specify which features to apply the transform to.
 
+    order: boolean, default False
+        Specify whether the initial order of features has
+        to be maintained in the output
+
     Returns
     -------
     X : array or sparse matrix, shape=(n_samples, n_features_new)
@@ -1719,10 +1725,15 @@ def _transform_selected(X, transform, selected="all", copy=True):
         X_sel = transform(X[:, ind[sel]])
         X_not_sel = X[:, ind[not_sel]]
 
-        if sparse.issparse(X_sel) or sparse.issparse(X_not_sel):
-            return sparse.hstack((X_sel, X_not_sel))
+        if order:
+            # As of now, X is expected to be dense array
+            X[:, ind[sel]] = X_sel
+            return X
         else:
-            return np.hstack((X_sel, X_not_sel))
+            if sparse.issparse(X_sel) or sparse.issparse(X_not_sel):
+                return sparse.hstack((X_sel, X_not_sel))
+            else:
+                return np.hstack((X_sel, X_not_sel))
 
 
 class OneHotEncoder(BaseEstimator, TransformerMixin):
@@ -1946,3 +1957,151 @@ def transform(self, X):
         """
         return _transform_selected(X, self._transform,
                                    self.categorical_features, copy=True)
+
+
+def _boxcox(x, lambda_x=None):
+    if lambda_x is None:
+        x, lambda_x = stats.boxcox(x, lambda_x)
+        return x, lambda_x
+    else:
+        x = stats.boxcox(x, lambda_x)
+        return x
+
+
+def boxcox(X, copy=True):
+    """BoxCox transform to the input data
+
+    Apply boxcox transform on individual features with lambda
+    that maximizes the log-likelihood function for each feature
+
+    Parameters
+    ----------
+    X : array-like, shape (n_samples, n_features)
+        The data to be transformed. Should contain only positive data.
+
+    copy : boolean, optional, default is True
+        set to False to perform inplace transformation and avoid a
+        copy (if the input is already a numpy array or a scipy.sparse
+        CSR matrix and if axis is 1).
+
+    Returns
+    -------
+    X_tr : array-like, shape (n_samples, n_features)
+        The transformed data.
+
+    References
+    ----------
+    G.E.P. Box and D.R. Cox, "An Analysis of Transformations", Journal of the
+    Royal Statistical Society B, 26, 211-252 (1964).
+    """
+    X = check_array(X, ensure_2d=True, dtype=FLOAT_DTYPES, copy=copy)
+    if any(np.any(X <= 0, axis=0)):
+        raise ValueError("BoxCox transform can only be applied "
+                         "on positive data")
+    n_features = X.shape[1]
+    outputs = Parallel(n_jobs=-1)(delayed(_boxcox)(X[:, i], lambda_x=None)
+                                  for i in range(n_features))
+    output = np.concatenate([o[0][..., np.newaxis] for o in outputs], axis=1)
+    return output
+
+
+class BoxCoxTransformer(BaseEstimator, TransformerMixin):
+    """BoxCox features individually.
+
+    Each feature (i.e. each column of the data matrix) will be applied
+    boxcox transform with lambda evaluated to maximise the log-likelihood
+
+    Parameters
+    ----------
+    feature_indices: None | array of int or mask, default None
+        Specify what features are treated are to be transformed.
+
+        - None (default): All features are to be transformed.
+        - array of int: Array of feature indices to be transformed..
+        - mask: Array of length n_features and with dtype=bool.
+
+    copy : boolean, optional, default True
+        Set to False to perform inplace computation.
+
+    Attributes
+    ----------
+    feature_indices_ : array of int
+        The indices of the features to be transformed
+    lambdas_ : array of float, shape (n_transformed_features,)
+        The parameters of the BoxCox transform for the selected features.
+    n_features_ : int
+        Number of features in input during fit
+
+    Notes
+    -----
+    The Box-Cox transform is given by::
+
+        y = (x ** lmbda - 1.) / lmbda,  for lmbda > 0
+            log(x),                     for lmbda = 0
+
+    `boxcox` requires the input data to be positive.
+
+    This estimator is stateless (besides constructor parameters), the
+    fit method does nothing but is useful when used in a pipeline.
+    """
+    def __init__(self, feature_indices=None, n_jobs=1, copy=True):
+        self.feature_indices = feature_indices
+        self.n_jobs = n_jobs
+        self.copy = copy
+
+    def fit(self, X, y=None):
+        """Do nothing and return the estimator unchanged
+
+        Parameters
+        ----------
+        X : array-like, shape [n_samples, n_features]
+            The data to fit by apply boxcox transform,
+            to each of the features and learn the lambda.
+
+        """
+        X = check_array(X, ensure_2d=True, dtype=FLOAT_DTYPES)
+        self.n_features_ = X.shape[1]
+        if self.feature_indices is None:
+            self.feature_indices_ = np.arange(self.n_features_)
+        else:
+            self.feature_indices_ = np.copy(self.feature_indices)
+            if self.feature_indices_.dtype == np.bool:
+                self.feature_indices_ = np.where(self.feature_indices_)[0]
+        if any(np.any(X[:, self.feature_indices_] <= 0, axis=0)):
+            raise ValueError("BoxCox transform can only be applied "
+                             "on positive data")
+        out = Parallel(n_jobs=self.n_jobs)(delayed(_boxcox)(X[:, i],
+                                           lambda_x=None)
+                                           for i in self.feature_indices_)
+        self.lambdas_ = np.array([o[1] for o in out])
+        return self
+
+    def transform(self, X, y=None):
+        """Scale each non zero row of X to unit norm
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+            The data to apply boxcox transform.
+
+        Returns
+        -------
+        X_tr : array-like, shape (n_samples, n_features)
+            The transformed data.
+        """
+        X = check_array(X, ensure_2d=True, dtype=FLOAT_DTYPES, copy=self.copy)
+        if any(np.any(X[:, self.feature_indices_] <= 0, axis=0)):
+            raise ValueError("BoxCox transform can only be applied "
+                             "on positive data")
+        if X.shape[1] != self.n_features_:
+            raise ValueError("X has a different shape than during fitting.")
+        X_tr = _transform_selected(X, self._transform, self.feature_indices_,
+                                   copy=False, order=True)
+        return X_tr
+
+    def _transform(self, X):
+        outputs = Parallel(n_jobs=self.n_jobs)(
+            delayed(_boxcox)(X[:, i], self.lambdas_[i])
+            for i in range(len(self.feature_indices_)))
+        output = np.concatenate([o[..., np.newaxis] for o in outputs], axis=1)
+        return output
diff --git a/sklearn/preprocessing/tests/test_data.py b/sklearn/preprocessing/tests/test_data.py
index 39360ebdd779..7dc9b20f7b6d 100644
--- a/sklearn/preprocessing/tests/test_data.py
+++ b/sklearn/preprocessing/tests/test_data.py
@@ -8,6 +8,7 @@
 import warnings
 import numpy as np
 import numpy.linalg as la
+import scipy.stats as stats
 from scipy import sparse
 from distutils.version import LooseVersion
 
@@ -49,6 +50,8 @@
 from sklearn.preprocessing.data import robust_scale
 from sklearn.preprocessing.data import add_dummy_feature
 from sklearn.preprocessing.data import PolynomialFeatures
+from sklearn.preprocessing.data import boxcox
+from sklearn.preprocessing.data import BoxCoxTransformer
 from sklearn.exceptions import DataConversionWarning
 
 from sklearn.pipeline import Pipeline
@@ -1654,3 +1657,50 @@ def test_fit_cold_start():
         # with a different shape, this may break the scaler unless the internal
         # state is reset
         scaler.fit_transform(X_2d)
+
+
+def test_boxcox_transform():
+    # Apply boxcox transform and check if it is
+    # applying individually on each feature
+    X = np.array([[4, 2, 1], [1, 6, 3], [1, 5, 2], [3, 1, 3]])
+    Xtr = boxcox(X)
+    n_features = X.shape[1]
+    for k in range(n_features):
+        assert_array_equal(stats.boxcox(X[:, k])[0], Xtr[:, k])
+        assert_array_equal(boxcox(X[:, k:k + 1]).ravel(),
+                           Xtr[:, k])
+
+
+def test_boxcox_transformer():
+    rng = np.random.RandomState(42)
+    X = rng.randn(3000, 2)
+    lambda0 = 0.1
+    X[:, 0] = np.exp(X[:, 0])
+    X[:, 1] = ((X[:, 1] * lambda0 + 1)) ** (1. / lambda0)
+
+    feature_indices = [0]
+    bct = BoxCoxTransformer(feature_indices=feature_indices)
+
+    bct.fit(X)
+    assert_true(len(bct.lambdas_), 1)
+    X_tr = bct.transform(X)
+    assert_true(X_tr.shape, X.shape)
+    assert_true(np.min(X_tr[:, 0]) < 0.)
+    assert_true(np.min(X_tr[:, 1]) > 0.)
+
+    bct.set_params(feature_indices=np.array([False, True]))
+    bct.fit(X)
+    assert_true(len(bct.lambdas_), 1)
+    X_tr = bct.transform(X)
+    assert_true(X_tr.shape, X.shape)
+    assert_true(np.min(X_tr[:, 0]) > 0.)
+    assert_true(np.min(X_tr[:, 1]) < 0.)
+
+    bct.set_params(feature_indices=None)
+    bct.fit(X)
+    assert_true(len(bct.lambdas_), 2)
+    X_tr = bct.transform(X)
+    assert_true(X_tr.shape, X.shape)
+    assert_true(np.min(X_tr[:, 0]) < 0.)
+    assert_true(np.min(X_tr[:, 1]) < 0.)
+    assert_array_almost_equal(bct.lambdas_, [0., lambda0], 2)
diff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py
index 8c117ae696ec..b94b1e81a4f9 100644
--- a/sklearn/utils/estimator_checks.py
+++ b/sklearn/utils/estimator_checks.py
@@ -522,7 +522,7 @@ def check_transformer_general(name, Transformer):
     X, y = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],
                       random_state=0, n_features=2, cluster_std=0.1)
     X = StandardScaler().fit_transform(X)
-    X -= X.min()
+    X -= X.min() - .1
     _check_transformer(name, Transformer, X, y)
     _check_transformer(name, Transformer, X.tolist(), y.tolist())
 
@@ -633,7 +633,7 @@ def check_pipeline_consistency(name, Estimator):
     # check that make_pipeline(est) gives same score as est
     X, y = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],
                       random_state=0, n_features=2, cluster_std=0.1)
-    X -= X.min()
+    X -= X.min() - .1
     y = multioutput_estimator_convert_y_2d(name, y)
     estimator = Estimator()
     set_testing_parameters(estimator)
@@ -687,7 +687,11 @@ def check_fit_score_takes_y(name, Estimator):
 @ignore_warnings
 def check_estimators_dtypes(name, Estimator):
     rnd = np.random.RandomState(0)
-    X_train_32 = 3 * rnd.uniform(size=(20, 5)).astype(np.float32)
+    if name in ["BoxCoxTransformer", "SpectralCoclustering",
+                "SpectralBiclustering"]:
+        X_train_32 = 3 * rnd.uniform(1., 2., size=(20, 5)).astype(np.float32)
+    else:
+        X_train_32 = 3 * rnd.uniform(size=(20, 5)).astype(np.float32)
     X_train_64 = X_train_32.astype(np.float64)
     X_train_int_64 = X_train_32.astype(np.int64)
     X_train_int_32 = X_train_32.astype(np.int32)
@@ -815,7 +819,7 @@ def check_estimators_pickle(name, Estimator):
                       random_state=0, n_features=2, cluster_std=0.1)
 
     # some estimators can't do features less than 0
-    X -= X.min()
+    X -= X.min() - .1
 
     # some estimators only take multioutputs
     y = multioutput_estimator_convert_y_2d(name, y)
@@ -1029,7 +1033,7 @@ def check_estimators_fit_returns_self(name, Estimator):
     X, y = make_blobs(random_state=0, n_samples=9, n_features=4)
     y = multioutput_estimator_convert_y_2d(name, y)
     # some want non-negative input
-    X -= X.min()
+    X -= X.min() - .1
 
     estimator = Estimator()
 
@@ -1321,7 +1325,7 @@ def check_estimators_overwrite_params(name, Estimator):
     X, y = make_blobs(random_state=0, n_samples=9)
     y = multioutput_estimator_convert_y_2d(name, y)
     # some want non-negative input
-    X -= X.min()
+    X -= X.min() - .1
     with warnings.catch_warnings(record=True):
         # catch deprecation warnings
         estimator = Estimator()
