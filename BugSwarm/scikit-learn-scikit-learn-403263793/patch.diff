diff --git a/doc/modules/clustering.rst b/doc/modules/clustering.rst
index f83bb4bfdd..1faea9fe27 100755
--- a/doc/modules/clustering.rst
+++ b/doc/modules/clustering.rst
@@ -873,16 +873,18 @@ larger parent cluster.
 .. topic:: Comparison with DBSCAN
     
     The results from OPTICS ``extract_dbscan`` method and DBSCAN are not quite
-    identical. This is in part because the first sample processed by OPTICS
-    will always have a reachability distance that is set to ``inf``, and will
-    thus generally be marked as noise regardless of the surrounding density of
-    other samples. This affects adjacent points when they are considered as
-    candidates for being marked as *core samples*. While this effect is quite
-    local to the starting point of the dataset and is unlikely to be noticed
-    on even moderately large datasets, it is worth also noting that non-core
-    boundry points may switch cluster labels on the rare occasion that they
-    are equidistant to a competeing cluster due to how the graph is read from
-    left to right when assigning labels. 
+    identical. Specifically, while *core_samples* returned from both OPTICS
+    and DBSCAN are guaranteed to be identical, labeling of periphery and noise
+    points is not. This is in part because the first sample processed by
+    OPTICS will always have a reachability distance that is set to ``inf``,
+    and will thus generally be marked as noise rather than periphery. This
+    affects adjacent points when they are considered as candidates for being
+    marked as either periphery or noise. While this effect is quite local to
+    the starting point of the dataset and is unlikely to be noticed on even
+    moderately large datasets, it is worth also noting that non-core boundry
+    points may switch cluster labels on the rare occasion that they are
+    equidistant to a competeing cluster due to how the graph is read from left
+    to right when assigning labels. 
 
     Note that for any single value of ``eps``, DBSCAN will tend to have a
     shorter run time than OPTICS; however, for repeated runs at varying ``eps``
diff --git a/sklearn/cluster/tests/test_optics.py b/sklearn/cluster/tests/test_optics.py
index 84ddcdea9d..597785083c 100755
--- a/sklearn/cluster/tests/test_optics.py
+++ b/sklearn/cluster/tests/test_optics.py
@@ -97,7 +97,7 @@ def test_dbscan_optics_parity(eps, min_samples):
     core_optics, labels_optics = op.extract_dbscan(eps)
 
     # calculate dbscan labels
-    db = DBSCAN(eps=0.3, min_samples=min_samples).fit(X)
+    db = DBSCAN(eps=eps, min_samples=min_samples).fit(X)
 
     contingency = contingency_matrix(db.labels_, labels_optics)
     agree = min(np.sum(np.max(contingency, axis=0)),
diff --git a/sklearn/kernel_approximation.py b/sklearn/kernel_approximation.py
index 68b2e82772..a0720a8568 100755
--- a/sklearn/kernel_approximation.py
+++ b/sklearn/kernel_approximation.py
@@ -44,6 +44,26 @@ class RBFSampler(BaseEstimator, TransformerMixin):
         If None, the random number generator is the RandomState instance used
         by `np.random`.
 
+    Examples
+    --------
+    >>> from sklearn.kernel_approximation import RBFSampler
+    >>> from sklearn.linear_model import SGDClassifier
+    >>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]
+    >>> y = [0, 0, 1, 1]
+    >>> rbf_feature = RBFSampler(gamma=1, random_state=1)
+    >>> X_features = rbf_feature.fit_transform(X)
+    >>> clf = SGDClassifier(max_iter=5)
+    >>> clf.fit(X_features, y)
+    ... # doctest: +NORMALIZE_WHITESPACE
+    SGDClassifier(alpha=0.0001, average=False, class_weight=None,
+           early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,
+           l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,
+           n_iter=None, n_iter_no_change=5, n_jobs=1, penalty='l2',
+           power_t=0.5, random_state=None, shuffle=True, tol=None,
+           validation_fraction=0.1, verbose=0, warm_start=False)
+    >>> clf.score(X_features, y)
+    1.0
+
     Notes
     -----
     See "Random Features for Large-Scale Kernel Machines" by A. Rahimi and
@@ -132,6 +152,27 @@ class SkewedChi2Sampler(BaseEstimator, TransformerMixin):
         If None, the random number generator is the RandomState instance used
         by `np.random`.
 
+    Examples
+    --------
+    >>> from sklearn.kernel_approximation import SkewedChi2Sampler
+    >>> from sklearn.linear_model import SGDClassifier
+    >>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]
+    >>> y = [0, 0, 1, 1]
+    >>> chi2_feature = SkewedChi2Sampler(skewedness=.01,
+    ...                                  n_components=10,
+    ...                                  random_state=0)
+    >>> X_features = chi2_feature.fit_transform(X, y)
+    >>> clf = SGDClassifier(max_iter=10)
+    >>> clf.fit(X_features, y)
+    SGDClassifier(alpha=0.0001, average=False, class_weight=None,
+           early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,
+           l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=10,
+           n_iter=None, n_iter_no_change=5, n_jobs=1, penalty='l2',
+           power_t=0.5, random_state=None, shuffle=True, tol=None,
+           validation_fraction=0.1, verbose=0, warm_start=False)
+    >>> clf.score(X_features, y)
+    1.0
+
     References
     ----------
     See "Random Fourier Approximations for Skewed Multiplicative Histogram
