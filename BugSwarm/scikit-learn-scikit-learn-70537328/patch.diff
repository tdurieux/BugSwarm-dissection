diff --git a/examples/applications/face_recognition.py b/examples/applications/face_recognition.py
index 2422ec67617e..be466e9532cd 100644
--- a/examples/applications/face_recognition.py
+++ b/examples/applications/face_recognition.py
@@ -12,17 +12,17 @@
 
 Expected results for the top 5 most represented people in the dataset::
 
-                     precision    recall  f1-score   support
-
-  Gerhard_Schroeder       0.91      0.75      0.82        28
-    Donald_Rumsfeld       0.84      0.82      0.83        33
-         Tony_Blair       0.65      0.82      0.73        34
-       Colin_Powell       0.78      0.88      0.83        58
-      George_W_Bush       0.93      0.86      0.90       129
-
-        avg / total       0.86      0.84      0.85       282
+                   precision    recall  f1-score   support
 
+     Ariel Sharon       0.67      0.92      0.77        13
+     Colin Powell       0.75      0.78      0.76        60
+  Donald Rumsfeld       0.78      0.67      0.72        27
+    George W Bush       0.86      0.86      0.86       146
+Gerhard Schroeder       0.76      0.76      0.76        25
+      Hugo Chavez       0.67      0.67      0.67        15
+       Tony Blair       0.81      0.69      0.75        36
 
+      avg / total       0.80      0.80      0.80       322
 
 """
 from __future__ import print_function
@@ -75,7 +75,7 @@
 
 # split into a training and testing set
 X_train, X_test, y_train, y_test = train_test_split(
-    X, y, test_size=0.25)
+    X, y, test_size=0.25, random_state=42)
 
 
 ###############################################################################
diff --git a/examples/cluster/plot_kmeans_digits.py b/examples/cluster/plot_kmeans_digits.py
index cdd5073937ef..c653d6998af0 100644
--- a/examples/cluster/plot_kmeans_digits.py
+++ b/examples/cluster/plot_kmeans_digits.py
@@ -96,8 +96,8 @@ def bench_k_means(estimator, name, data):
 h = .02     # point in the mesh [x_min, m_max]x[y_min, y_max].
 
 # Plot the decision boundary. For that, we will assign a color to each
-x_min, x_max = reduced_data[:, 0].min() + 1, reduced_data[:, 0].max() - 1
-y_min, y_max = reduced_data[:, 1].min() + 1, reduced_data[:, 1].max() - 1
+x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1
+y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1
 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
 
 # Obtain labels for each point in mesh. Use last trained model.
diff --git a/sklearn/decomposition/nmf.py b/sklearn/decomposition/nmf.py
index 61c319a21fe9..a482a7647904 100644
--- a/sklearn/decomposition/nmf.py
+++ b/sklearn/decomposition/nmf.py
@@ -267,7 +267,7 @@ class ProjectedGradientNMF(BaseEstimator, TransformerMixin):
 
     init :  'nndsvd' |  'nndsvda' | 'nndsvdar' | 'random'
         Method used to initialize the procedure.
-        Default: 'nndsvdar' if n_components < n_features, otherwise random.
+        Default: 'nndsvd' if n_components < n_features, otherwise random.
         Valid options::
 
             'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD)
diff --git a/sklearn/ensemble/forest.py b/sklearn/ensemble/forest.py
index 5bf53ac23c2c..b915ed8edf8f 100644
--- a/sklearn/ensemble/forest.py
+++ b/sklearn/ensemble/forest.py
@@ -71,6 +71,22 @@ class calls the ``fit`` method of each sub-estimator on random samples
 
 MAX_INT = np.iinfo(np.int32).max
 
+def _generate_sample_indices(random_state, n_samples):
+    """Private function used to _parallel_build_trees function."""
+    random_instance = check_random_state(random_state)
+    sample_indices = random_instance.randint(0, n_samples, n_samples)
+
+    return sample_indices
+
+def _generate_unsampled_indices(random_state, n_samples):
+    """Private function used to forest._set_oob_score fuction."""
+    sample_indices = _generate_sample_indices(random_state, n_samples)
+    sample_counts = bincount(sample_indices, minlength=n_samples)
+    unsampled_mask = sample_counts == 0
+    indices_range = np.arange(n_samples)
+    unsampled_indices = indices_range[unsampled_mask]
+
+    return unsampled_indices
 
 def _parallel_build_trees(tree, forest, X, y, sample_weight, tree_idx, n_trees,
                           verbose=0, class_weight=None):
@@ -85,8 +101,7 @@ def _parallel_build_trees(tree, forest, X, y, sample_weight, tree_idx, n_trees,
         else:
             curr_sample_weight = sample_weight.copy()
 
-        random_state = check_random_state(tree.random_state)
-        indices = random_state.randint(0, n_samples, n_samples)
+        indices = _generate_sample_indices(tree.random_state, n_samples)
         sample_counts = bincount(indices, minlength=n_samples)
         curr_sample_weight *= sample_counts
 
@@ -98,9 +113,6 @@ def _parallel_build_trees(tree, forest, X, y, sample_weight, tree_idx, n_trees,
             curr_sample_weight *= compute_sample_weight('balanced', y, indices)
 
         tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)
-
-        tree.indices_ = sample_counts > 0.
-
     else:
         tree.fit(X, y, sample_weight=sample_weight, check_input=False)
 
@@ -371,19 +383,17 @@ def _set_oob_score(self, X, y):
         for k in range(self.n_outputs_):
             predictions.append(np.zeros((n_samples, n_classes_[k])))
 
-        sample_indices = np.arange(n_samples)
         for estimator in self.estimators_:
-            mask = np.ones(n_samples, dtype=np.bool)
-            mask[estimator.indices_] = False
-            mask_indices = sample_indices[mask]
-            p_estimator = estimator.predict_proba(X[mask_indices, :],
+            unsampled_indices = _generate_unsampled_indices(
+                estimator.random_state, n_samples)
+            p_estimator = estimator.predict_proba(X[unsampled_indices, :],
                                                   check_input=False)
 
             if self.n_outputs_ == 1:
                 p_estimator = [p_estimator]
 
             for k in range(self.n_outputs_):
-                predictions[k][mask_indices, :] += p_estimator[k]
+                predictions[k][unsampled_indices, :] += p_estimator[k]
 
         for k in range(self.n_outputs_):
             if (predictions[k].sum(axis=1) == 0).any():
@@ -653,18 +663,17 @@ def _set_oob_score(self, X, y):
         predictions = np.zeros((n_samples, self.n_outputs_))
         n_predictions = np.zeros((n_samples, self.n_outputs_))
 
-        sample_indices = np.arange(n_samples)
         for estimator in self.estimators_:
-            mask = np.ones(n_samples, dtype=np.bool)
-            mask[estimator.indices_] = False
-            mask_indices = sample_indices[mask]
-            p_estimator = estimator.predict(X[mask_indices, :], check_input=False)
+            unsampled_indices = _generate_unsampled_indices(
+                estimator.random_state, n_samples)
+            p_estimator = estimator.predict(
+                X[unsampled_indices, :], check_input=False)
 
             if self.n_outputs_ == 1:
                 p_estimator = p_estimator[:, np.newaxis]
 
-            predictions[mask_indices, :] += p_estimator
-            n_predictions[mask_indices, :] += 1
+            predictions[unsampled_indices, :] += p_estimator
+            n_predictions[unsampled_indices, :] += 1
 
         if (n_predictions == 0).any():
             warn("Some inputs do not have OOB scores. "
diff --git a/sklearn/linear_model/ransac.py b/sklearn/linear_model/ransac.py
index e3466b026c83..717cbde80514 100644
--- a/sklearn/linear_model/ransac.py
+++ b/sklearn/linear_model/ransac.py
@@ -292,7 +292,7 @@ def fit(self, X, y):
                 continue
             if n_inliers_subset == 0:
                 raise ValueError("No inliers found, possible cause is "
-                    "setting residual_threshold ({}) too low.".format(
+                    "setting residual_threshold ({0}) too low.".format(
                     self.residual_threshold))
 
             # extract inlier data set
diff --git a/sklearn/svm/base.py b/sklearn/svm/base.py
index df8ee5d80cb5..7c2b9fb7888f 100644
--- a/sklearn/svm/base.py
+++ b/sklearn/svm/base.py
@@ -751,14 +751,14 @@ def _get_liblinear_solver_type(multi_class, penalty, loss, dual):
         # FIME penalty.lower() --> penalty in 0.18
         _solver_dual = _solver_pen.get(penalty.lower(), None)
         if _solver_dual is None:
-            error_string = ("The combination of penalty='%s'"
+            error_string = ("The combination of penalty='%s' "
                             "and loss='%s' is not supported"
                             % (penalty, loss))
         else:
             solver_num = _solver_dual.get(dual, None)
             if solver_num is None:
-                error_string = ("loss='%s' and penalty='%s'"
-                                "are not supported when dual=%s"
+                error_string = ("The combination of penalty='%s' and "
+                                "loss='%s' are not supported when dual=%s"
                                 % (penalty, loss, dual))
             else:
                 return solver_num
