diff --git a/examples/text/document_classification_20newsgroups.py b/examples/text/document_classification_20newsgroups.py
index 23656b56f2..f34bbd10cb 100755
--- a/examples/text/document_classification_20newsgroups.py
+++ b/examples/text/document_classification_20newsgroups.py
@@ -34,6 +34,7 @@
 from sklearn.datasets import fetch_20newsgroups
 from sklearn.feature_extraction.text import TfidfVectorizer
 from sklearn.feature_extraction.text import HashingVectorizer
+from sklearn.feature_selection import SelectFromModel
 from sklearn.feature_selection import SelectKBest, chi2
 from sklearn.linear_model import RidgeClassifier
 from sklearn.pipeline import Pipeline
@@ -259,8 +260,8 @@ def benchmark(clf):
     print('=' * 80)
     print("%s penalty" % penalty.upper())
     # Train Liblinear model
-    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,
-                                            dual=False, tol=1e-3)))
+    results.append(benchmark(LinearSVC(penalty=penalty, dual=False,
+                                       tol=1e-3)))
 
     # Train SGD model
     results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,
@@ -288,9 +289,9 @@ def benchmark(clf):
 # The smaller C, the stronger the regularization.
 # The more regularization, the more sparsity.
 results.append(benchmark(Pipeline([
-  ('feature_selection', LinearSVC(penalty="l1", dual=False, tol=1e-3)),
-  ('classification', LinearSVC())
-])))
+  ('feature_selection', SelectFromModel(LinearSVC(penalty="l1", dual=False,
+                                                  tol=1e-3))),
+  ('classification', LinearSVC(penalty="l2"))])))
 
 # make some plots
 
diff --git a/sklearn/preprocessing/data.py b/sklearn/preprocessing/data.py
index 5835f6526b..a2d2b1ecf6 100755
--- a/sklearn/preprocessing/data.py
+++ b/sklearn/preprocessing/data.py
@@ -2028,11 +2028,7 @@ def fit(self, X, y=None):
 
         if y is not None:
             for i in range(len_data):
-                X_key_1 = X[i]
-                X_key_2 = X_key_1.take(inclusion_used)
-                X_key_3 = X_key_2
-                X_key = tuple(X_key_3)
-                # X_key = tuple(X.take(i, axis=0).take(inclusion_used).flatten())
+                X_key = tuple(X[i].take(inclusion_used))
                 y_key = tuple(y.take([i]))
                 self.count_cache_[X_key][y_key] += 1
                 self.y_set_.add(y_key)
@@ -2088,7 +2084,6 @@ def transform(self, X):
         for i in range(len_data):
             for j, y_key in self.y_set_:
                 X_key = tuple(X[i].take(inclusion_used))
-
                 transformed[i, num_features + j] = \
                     self.count_cache_[X_key][y_key]
 
diff --git a/sklearn/preprocessing/tests/test_data.py b/sklearn/preprocessing/tests/test_data.py
index cc9fbfad94..57c52e19fe 100755
--- a/sklearn/preprocessing/tests/test_data.py
+++ b/sklearn/preprocessing/tests/test_data.py
@@ -1685,7 +1685,7 @@ def test_count_featurizer_ft_y_standard():
     y = np.array([0, 0, 1, 0])
     cf_1 = CountFeaturizer()
     assert_array_equal(
-        cf_1.fit_transform(X),
+        cf_1.fit_transform(X, y=y),
         np.array([[0, 2, 1, 1, 0], [1, 0, 3, 1, 0],
                  [1, 0, 2, 1, 1], [1, 0, 2, 1, 1]]))
 
@@ -1699,11 +1699,11 @@ def test_count_featurizer_ft_y_standard_inclusion():
     cf_inclusion_1 = CountFeaturizer(inclusion=[0])
     cf_inclusion_2 = CountFeaturizer(inclusion=[0, 1, 2])
     assert_array_equal(
-        cf_inclusion_1.fit_transform(X),
+        cf_inclusion_1.fit_transform(X, y=y),
         np.array([[0, 2, 1, 1, 0], [1, 0, 3, 2, 1],
                  [1, 0, 2, 2, 1], [1, 0, 2, 2, 1]]))
 
     assert_array_equal(
-        cf_inclusion_2.fit_transform(X),
+        cf_inclusion_2.fit_transform(X, y=y),
         np.array([[0, 2, 1, 1, 0], [1, 0, 3, 1, 0],
                  [1, 0, 2, 1, 1], [1, 0, 2, 1, 1]]))
