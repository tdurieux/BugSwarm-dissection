diff --git a/src/edu/washington/escience/myria/io/AmazonS3Source.java b/src/edu/washington/escience/myria/io/AmazonS3Source.java
index 3a08ca448c..e9119258b5 100755
--- a/src/edu/washington/escience/myria/io/AmazonS3Source.java
+++ b/src/edu/washington/escience/myria/io/AmazonS3Source.java
@@ -23,7 +23,7 @@
 import com.google.common.base.MoreObjects;
 
 /**
- * 
+ *
  */
 @NotThreadSafe
 public class AmazonS3Source implements DataSource, Serializable {
@@ -31,7 +31,8 @@
   /** Required for Java serialization. */
   private static final long serialVersionUID = 1L;
   /** The logger for debug, trace, etc. messages in this class. */
-  private static final org.slf4j.Logger LOGGER = org.slf4j.LoggerFactory.getLogger(AmazonS3Source.class);
+  private static final org.slf4j.Logger LOGGER =
+      org.slf4j.LoggerFactory.getLogger(AmazonS3Source.class);
 
   private final URI s3Uri;
   private transient ClientConfiguration clientConfig;
@@ -45,14 +46,17 @@
   private final String key;
 
   @JsonCreator
-  public AmazonS3Source(@JsonProperty(value = "uri", required = true) final String uri) throws URIException {
+  public AmazonS3Source(@JsonProperty(value = "uri", required = true) final String uri)
+      throws URIException {
     this(uri, null, null);
   }
 
   @JsonCreator
-  public AmazonS3Source(@JsonProperty(value = "uri", required = true) final String uri,
+  public AmazonS3Source(
+      @JsonProperty(value = "uri", required = true) final String uri,
       @Nullable @JsonProperty(value = "startRange", required = false) final Long startRange,
-      @Nullable @JsonProperty(value = "endRange", required = false) final Long endRange) throws URIException {
+      @Nullable @JsonProperty(value = "endRange", required = false) final Long endRange)
+      throws URIException {
     s3Uri = URI.create(Objects.requireNonNull(uri, "Parameter uri to UriSource may not be null"));
     if (!s3Uri.getScheme().equals("s3")) {
       throw new URIException("URI must contain an S3 scheme");
diff --git a/src/edu/washington/escience/myria/io/DataSource.java b/src/edu/washington/escience/myria/io/DataSource.java
index 4761d98e68..94675b0bde 100755
--- a/src/edu/washington/escience/myria/io/DataSource.java
+++ b/src/edu/washington/escience/myria/io/DataSource.java
@@ -13,9 +13,12 @@
  */
 @JsonTypeInfo(use = JsonTypeInfo.Id.NAME, include = JsonTypeInfo.As.PROPERTY, property = "dataType")
 @JsonSubTypes({
-    @Type(name = "Bytes", value = ByteArraySource.class), @Type(name = "File", value = FileSource.class),
-    @Type(name = "S3", value = AmazonS3Source.class), @Type(name = "URI", value = UriSource.class),
-    @Type(name = "Empty", value = EmptySource.class) })
+  @Type(name = "Bytes", value = ByteArraySource.class),
+  @Type(name = "File", value = FileSource.class),
+  @Type(name = "S3", value = AmazonS3Source.class),
+  @Type(name = "URI", value = UriSource.class),
+  @Type(name = "Empty", value = EmptySource.class)
+})
 public interface DataSource {
   /**
    * Returns an {@link InputStream} providing read access to the bits in the specified data source.
diff --git a/src/edu/washington/escience/myria/io/UriSource.java b/src/edu/washington/escience/myria/io/UriSource.java
index ad25c10d58..df9c1a6732 100755
--- a/src/edu/washington/escience/myria/io/UriSource.java
+++ b/src/edu/washington/escience/myria/io/UriSource.java
@@ -53,15 +53,22 @@ public UriSource(@JsonProperty(value = "uri", required = true) final String uri)
     /* Force using the Hadoop S3A FileSystem */
     if (parsedUri.getScheme().equals("s3")) {
       parsedUri =
-          new URI("s3a", parsedUri.getUserInfo(), parsedUri.getHost(), parsedUri.getPort(), parsedUri.getPath(),
-              parsedUri.getQuery(), parsedUri.getFragment());
+          new URI(
+              "s3a",
+              parsedUri.getUserInfo(),
+              parsedUri.getHost(),
+              parsedUri.getPort(),
+              parsedUri.getPath(),
+              parsedUri.getQuery(),
+              parsedUri.getFragment());
     }
   }
 
   @Override
   public InputStream getInputStream() throws IOException {
-    return (parsedUri.getScheme().equals("http") || parsedUri.getScheme().equals("https")) ? parsedUri.toURL()
-        .openConnection().getInputStream() : getHadoopFileSystemInputStream(parsedUri);
+    return (parsedUri.getScheme().equals("http") || parsedUri.getScheme().equals("https"))
+        ? parsedUri.toURL().openConnection().getInputStream()
+        : getHadoopFileSystemInputStream(parsedUri);
   }
 
   /**
@@ -87,5 +94,4 @@ private static InputStream getHadoopFileSystemInputStream(final URI uri) throws
 
     return new SequenceInputStream(java.util.Collections.enumeration(streams));
   }
-
-}
\ No newline at end of file
+}
diff --git a/src/edu/washington/escience/myria/operator/CSVFileScanFragment.java b/src/edu/washington/escience/myria/operator/CSVFileScanFragment.java
index 6a59996bf7..0ba47d2a39 100755
--- a/src/edu/washington/escience/myria/operator/CSVFileScanFragment.java
+++ b/src/edu/washington/escience/myria/operator/CSVFileScanFragment.java
@@ -33,7 +33,7 @@
 import edu.washington.escience.myria.util.DateTimeUtils;
 
 /**
- * 
+ *
  */
 public class CSVFileScanFragment extends LeafOperator {
 
@@ -72,38 +72,88 @@
   /**
    * The logger for debug, trace, etc. messages in this class.
    */
-  private static final org.slf4j.Logger LOGGER = org.slf4j.LoggerFactory.getLogger(CSVFileScanFragment.class);
-
-  public CSVFileScanFragment(final String filename, final Schema schema, final long startByteRange,
-      final long endByteRange, final boolean isLastWorker) {
+  private static final org.slf4j.Logger LOGGER =
+      org.slf4j.LoggerFactory.getLogger(CSVFileScanFragment.class);
+
+  public CSVFileScanFragment(
+      final String filename,
+      final Schema schema,
+      final long startByteRange,
+      final long endByteRange,
+      final boolean isLastWorker) {
     this(filename, schema, startByteRange, endByteRange, isLastWorker, null, null, null, null);
   }
 
-  public CSVFileScanFragment(final DataSource source, final Schema schema, final long startByteRange,
-      final long endByteRange, final boolean isLastWorker) {
+  public CSVFileScanFragment(
+      final DataSource source,
+      final Schema schema,
+      final long startByteRange,
+      final long endByteRange,
+      final boolean isLastWorker) {
     this(source, schema, startByteRange, endByteRange, isLastWorker, null, null, null, null);
   }
 
-  public CSVFileScanFragment(final String filename, final Schema schema, final long startByteRange,
-      final long endByteRange, final boolean isLastWorker, final Character delimiter) {
-    this(new FileSource(filename), schema, startByteRange, endByteRange, isLastWorker, delimiter, null, null, null);
+  public CSVFileScanFragment(
+      final String filename,
+      final Schema schema,
+      final long startByteRange,
+      final long endByteRange,
+      final boolean isLastWorker,
+      final Character delimiter) {
+    this(
+        new FileSource(filename),
+        schema,
+        startByteRange,
+        endByteRange,
+        isLastWorker,
+        delimiter,
+        null,
+        null,
+        null);
   }
 
-  public CSVFileScanFragment(final DataSource source, final Schema schema, final long startByteRange,
-      final long endByteRange, final boolean isLastWorker, final Character delimiter) {
+  public CSVFileScanFragment(
+      final DataSource source,
+      final Schema schema,
+      final long startByteRange,
+      final long endByteRange,
+      final boolean isLastWorker,
+      final Character delimiter) {
     this(source, schema, startByteRange, endByteRange, isLastWorker, delimiter, null, null, null);
   }
 
-  public CSVFileScanFragment(final String filename, final Schema schema, final long startByteRange,
-      final long endByteRange, final boolean isLastWorker, @Nullable final Character delimiter,
-      @Nullable final Character quote, @Nullable final Character escape, @Nullable final Integer numberOfSkippedLines) {
-    this(new FileSource(filename), schema, startByteRange, endByteRange, isLastWorker, delimiter, quote, escape,
+  public CSVFileScanFragment(
+      final String filename,
+      final Schema schema,
+      final long startByteRange,
+      final long endByteRange,
+      final boolean isLastWorker,
+      @Nullable final Character delimiter,
+      @Nullable final Character quote,
+      @Nullable final Character escape,
+      @Nullable final Integer numberOfSkippedLines) {
+    this(
+        new FileSource(filename),
+        schema,
+        startByteRange,
+        endByteRange,
+        isLastWorker,
+        delimiter,
+        quote,
+        escape,
         numberOfSkippedLines);
   }
 
-  public CSVFileScanFragment(final DataSource source, final Schema schema, final long partitionStartByteRange,
-      final long partitionEndByteRange, final boolean isLastWorker, @Nullable final Character delimiter,
-      @Nullable final Character quote, @Nullable final Character escape, @Nullable final Integer numberOfSkippedLines) {
+  public CSVFileScanFragment(
+      final DataSource source,
+      final Schema schema,
+      final long partitionStartByteRange,
+      final long partitionEndByteRange,
+      final boolean isLastWorker,
+      @Nullable final Character delimiter,
+      @Nullable final Character quote,
+      @Nullable final Character escape,
+      @Nullable final Integer numberOfSkippedLines) {
     this.source = (AmazonS3Source) Preconditions.checkNotNull(source, "source");
     this.schema = Preconditions.checkNotNull(schema, "schema");
 
@@ -115,7 +165,6 @@ public CSVFileScanFragment(final DataSource source, final Schema schema, final l
     this.partitionStartByteRange = partitionStartByteRange;
     this.partitionEndByteRange = partitionEndByteRange;
     this.isLastWorker = isLastWorker;
-
   }
 
   @Override
@@ -143,7 +192,9 @@ protected TupleBatch fetchNextReady() throws IOException, DbException {
       CSVRecord record = iterator.next();
       // This covers the case where the first row of a worker matches the schema. We only want to read this row if the
       // previous character is '\n' or '\r'
-      if (record.size() == schema.numColumns() && lineNumber - 1 == 0 && partitionStartByteRange != 0) {
+      if (record.size() == schema.numColumns()
+          && lineNumber - 1 == 0
+          && partitionStartByteRange != 0) {
         InputStreamReader startStreamReader = new InputStreamReader(partitionInputStream);
         char currentChar = (char) startStreamReader.read();
         if (currentChar != '\n' && currentChar != '\r') {
@@ -159,7 +210,8 @@ else if (record.size() == schema.numColumns() && !iterator.hasNext() && !isLastW
         while (!newLineFound) {
           movingEndByte += MyriaConstants.BYTE_OVERLAP_PARALLEL_INGEST;
           // Create a stream to look for the new line
-          InputStream trailingEndInputStream = source.getInputStream(partitionEndByteRange, movingEndByte);
+          InputStream trailingEndInputStream =
+              source.getInputStream(partitionEndByteRange, movingEndByte);
           InputStreamReader startStreamReader = new InputStreamReader(trailingEndInputStream);
           int dataChar = startStreamReader.read();
           while (dataChar != -1) {
@@ -169,11 +221,16 @@ else if (record.size() == schema.numColumns() && !iterator.hasNext() && !isLastW
               // Re-initialize the parser with the last row only
               InputStream beginningOfRecord =
                   source.getInputStream(bytePositionAtBeginningOfRecord, partitionEndByteRange);
-              InputStream concatenateEndOfRecord = source.getInputStream(partitionEndByteRange + 1, movingEndByte);
-              partitionInputStream = new SequenceInputStream(beginningOfRecord, concatenateEndOfRecord);
+              InputStream concatenateEndOfRecord =
+                  source.getInputStream(partitionEndByteRange + 1, movingEndByte);
+              partitionInputStream =
+                  new SequenceInputStream(beginningOfRecord, concatenateEndOfRecord);
               parser =
-                  new CSVParser(new BufferedReader(new InputStreamReader(partitionInputStream)), CSVFormat.newFormat(
-                      delimiter).withQuote(quote).withEscape(escape), bytePositionAtBeginningOfRecord, 0);
+                  new CSVParser(
+                      new BufferedReader(new InputStreamReader(partitionInputStream)),
+                      CSVFormat.newFormat(delimiter).withQuote(quote).withEscape(escape),
+                      bytePositionAtBeginningOfRecord,
+                      0);
               iterator = parser.iterator();
               onLastRow = true;
               record = iterator.next();
@@ -194,11 +251,13 @@ else if (record.size() == schema.numColumns() && !iterator.hasNext() && !isLastW
             partitionStartByteRange += bytePositionAtBeginningOfRecord;
           }
           partitionEndByteRange += byteOverlap;
-          InputStream overlapStream = source.getInputStream(partitionStartByteRange, partitionEndByteRange);
+          InputStream overlapStream =
+              source.getInputStream(partitionStartByteRange, partitionEndByteRange);
           partitionInputStream = new SequenceInputStream(partitionInputStream, overlapStream);
           parser =
-              new CSVParser(new BufferedReader(new InputStreamReader(partitionInputStream)), CSVFormat.newFormat(
-                  delimiter).withQuote(quote).withEscape(escape));
+              new CSVParser(
+                  new BufferedReader(new InputStreamReader(partitionInputStream)),
+                  CSVFormat.newFormat(delimiter).withQuote(quote).withEscape(escape));
           iterator = parser.iterator();
           byteOverlap *= 2;
         } else {
@@ -236,22 +295,27 @@ else if (record.size() == schema.numColumns() && !iterator.hasNext() && !isLastW
                 break;
             }
           } catch (final IllegalArgumentException e) {
-            throw new DbException("Error parsing column " + column + " of row " + lineNumber + ", expected type: "
-                + schema.getColumnType(column) + ", scanned value: " + cell, e);
+            throw new DbException(
+                "Error parsing column "
+                    + column
+                    + " of row "
+                    + lineNumber
+                    + ", expected type: "
+                    + schema.getColumnType(column)
+                    + ", scanned value: "
+                    + cell,
+                e);
           }
           if (onLastRow) {
             parser.close();
-
           }
         }
       }
-
     }
 
     LOGGER.debug("Scanned {} input lines", lineNumber - lineNumberBegin);
 
     return buffer.popAny();
-
   }
 
   @Override
@@ -277,8 +341,9 @@ protected void init(final ImmutableMap<String, Object> execEnvVars) throws DbExc
       }
       partitionInputStream = source.getInputStream(partitionStartByteRange, partitionEndByteRange);
       parser =
-          new CSVParser(new BufferedReader(new InputStreamReader(partitionInputStream)), CSVFormat.newFormat(delimiter)
-              .withQuote(quote).withEscape(escape));
+          new CSVParser(
+              new BufferedReader(new InputStreamReader(partitionInputStream)),
+              CSVFormat.newFormat(delimiter).withQuote(quote).withEscape(escape));
       iterator = parser.iterator();
       for (int i = 0; i < numberOfSkippedLines; i++) {
         iterator.next();
@@ -287,4 +352,4 @@ protected void init(final ImmutableMap<String, Object> execEnvVars) throws DbExc
       throw new DbException(e);
     }
   }
-}
\ No newline at end of file
+}
diff --git a/src/edu/washington/escience/myria/parallel/Server.java b/src/edu/washington/escience/myria/parallel/Server.java
index 45774165bc..9e4d68b2b8 100755
--- a/src/edu/washington/escience/myria/parallel/Server.java
+++ b/src/edu/washington/escience/myria/parallel/Server.java
@@ -30,7 +30,6 @@
 import javax.annotation.Nullable;
 import javax.inject.Inject;
 
-
 import org.apache.commons.httpclient.URIException;
 import org.apache.commons.io.FilenameUtils;
 
@@ -921,16 +920,22 @@ public DatasetStatus ingestDataset(
 
   /**
    * Parallel Ingest
-   * 
+   *
    * @param relationKey the name of the dataset.
    * @param workersToIngest restrict the workers to ingest data (null for all)
    * @throws URIException
    * @throws DbException
    * @throws InterruptedException
    */
-  public DatasetStatus parallelIngestDataset(final RelationKey relationKey, final Schema schema,
-      @Nullable final Character delimiter, @Nullable final Character quote, @Nullable final Character escape,
-      @Nullable final Integer numberOfSkippedLines, final String S3URI, final Set<Integer> workersToIngest)
+  public DatasetStatus parallelIngestDataset(
+      final RelationKey relationKey,
+      final Schema schema,
+      @Nullable final Character delimiter,
+      @Nullable final Character quote,
+      @Nullable final Character escape,
+      @Nullable final Integer numberOfSkippedLines,
+      final String S3URI,
+      final Set<Integer> workersToIngest)
       throws URIException, DbException, InterruptedException {
     /* Figure out the workers we will use */
 
@@ -972,7 +977,15 @@ public DatasetStatus parallelIngestDataset(final RelationKey relationKey, final
       long endRange = startRange + partitionSize;
 
       CSVFileScanFragment scanFragment =
-          new CSVFileScanFragment(s3Source, schema, startRange, endRange, isLastWorker, delimiter, quote, escape,
+          new CSVFileScanFragment(
+              s3Source,
+              schema,
+              startRange,
+              endRange,
+              isLastWorker,
+              delimiter,
+              quote,
+              escape,
               numberOfSkippedLines);
       workerPlans.put(workerID, new SubQueryPlan(new DbInsert(scanFragment, relationKey, true)));
       workerCounterID++;
@@ -981,8 +994,12 @@ public DatasetStatus parallelIngestDataset(final RelationKey relationKey, final
     ListenableFuture<Query> qf;
     try {
       qf =
-          queryManager.submitQuery("ingest " + relationKey.toString(), "ingest " + relationKey.toString(), "ingest "
-              + relationKey.toString(getDBMS()), new SubQueryPlan(new SinkRoot(new EOSSource())), workerPlans);
+          queryManager.submitQuery(
+              "ingest " + relationKey.toString(),
+              "ingest " + relationKey.toString(),
+              "ingest " + relationKey.toString(getDBMS()),
+              new SubQueryPlan(new SinkRoot(new EOSSource())),
+              workerPlans);
     } catch (CatalogException e) {
       throw new DbException("Error submitting query", e);
     }
