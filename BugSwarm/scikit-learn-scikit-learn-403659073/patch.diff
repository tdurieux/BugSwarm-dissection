diff --git a/doc/developers/contributing.rst b/doc/developers/contributing.rst
index 9c20363485..854dbe4442 100755
--- a/doc/developers/contributing.rst
+++ b/doc/developers/contributing.rst
@@ -698,7 +698,7 @@ Here's a simple example of code using some of the above guidelines::
 
         Parameters
         ----------
-        X : array-like, shape = (n_samples, n_features)
+        X : array-like, shape (n_samples, n_features)
             array representing the data
         random_state : RandomState or an int seed (0 by default)
             A random number generator instance to define the state of the
@@ -706,7 +706,7 @@ Here's a simple example of code using some of the above guidelines::
 
         Returns
         -------
-        x : numpy array, shape = (n_features,)
+        x : numpy array, shape (n_features,)
             A random point selected from X
         """
         X = check_array(X)
@@ -1037,11 +1037,9 @@ the predict method.
 ============= ======================================================
 Parameters
 ============= ======================================================
-X             array-like, with shape = [N, D], where N is the number
-              of samples and D is the number of features.
+X             array-like, shape (n_samples, n_features)
 
-y             array, with shape = [N], where N is the number of
-              samples.
+y             array, shape (n_samples,)
 
 kwargs        optional data-dependent parameters.
 ============= ======================================================
diff --git a/examples/plot_isotonic_regression.py b/examples/plot_isotonic_regression.py
index fd076b5afa..2411aa1d95 100755
--- a/examples/plot_isotonic_regression.py
+++ b/examples/plot_isotonic_regression.py
@@ -28,7 +28,7 @@
 n = 100
 x = np.arange(n)
 rs = check_random_state(0)
-y = rs.randint(-50, 50, size=(n,)) + 50. * np.log(1 + np.arange(n))
+y = rs.randint(-50, 50, size=(n,)) + 50. * np.log1p(np.arange(n))
 
 # #############################################################################
 # Fit IsotonicRegression and LinearRegression models
diff --git a/sklearn/cluster/hierarchical.py b/sklearn/cluster/hierarchical.py
index c462f2f2cd..f630fa990b 100755
--- a/sklearn/cluster/hierarchical.py
+++ b/sklearn/cluster/hierarchical.py
@@ -733,6 +733,20 @@ class AgglomerativeClustering(BaseEstimator, ClusterMixin):
         at the i-th iteration, children[i][0] and children[i][1]
         are merged to form node `n_samples + i`
 
+    Examples
+    --------
+    >>> from sklearn.cluster import AgglomerativeClustering
+    >>> import numpy as np
+    >>> X = np.array([[1, 2], [1, 4], [1, 0],
+    ...               [4, 2], [4, 4], [4, 0]])
+    >>> clustering = AgglomerativeClustering().fit(X)
+    >>> clustering # doctest: +NORMALIZE_WHITESPACE
+    AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto',
+                connectivity=None, linkage='ward', memory=None, n_clusters=2,
+                pooling_func='deprecated')
+    >>> clustering.labels_
+    array([1, 1, 1, 0, 0, 0])
+
     """
 
     def __init__(self, n_clusters=2, affinity="euclidean",
diff --git a/sklearn/gaussian_process/gpc.py b/sklearn/gaussian_process/gpc.py
index e1f37617b6..51da24fccc 100755
--- a/sklearn/gaussian_process/gpc.py
+++ b/sklearn/gaussian_process/gpc.py
@@ -409,7 +409,7 @@ def _posterior_mode(self, K, return_temporaries=False):
             # Line 10: Compute log marginal likelihood in loop and use as
             #          convergence criterion
             lml = -0.5 * a.T.dot(f) \
-                - np.log(1 + np.exp(-(self.y_train_ * 2 - 1) * f)).sum() \
+                - np.log1p(np.exp(-(self.y_train_ * 2 - 1) * f)).sum() \
                 - np.log(np.diag(L)).sum()
             # Check if we have converged (log marginal likelihood does
             # not decrease)
diff --git a/sklearn/neighbors/binary_tree.pxi b/sklearn/neighbors/binary_tree.pxi
index 6b736d39fa..3e17f1b93d 100755
--- a/sklearn/neighbors/binary_tree.pxi
+++ b/sklearn/neighbors/binary_tree.pxi
@@ -486,7 +486,7 @@ cdef DTYPE_t _log_kernel_norm(DTYPE_t h, ITYPE_t d,
     elif kernel == EXPONENTIAL_KERNEL:
         factor = logSn(d - 1) + lgamma(d)
     elif kernel == LINEAR_KERNEL:
-        factor = logVn(d) - log(d + 1.)
+        factor = logVn(d) - np.log1p(d)
     elif kernel == COSINE_KERNEL:
         # this is derived from a chain rule integration
         factor = 0
diff --git a/sklearn/preprocessing/_discretization.py b/sklearn/preprocessing/_discretization.py
index 75fa74fbe8..82ce907170 100755
--- a/sklearn/preprocessing/_discretization.py
+++ b/sklearn/preprocessing/_discretization.py
@@ -188,6 +188,12 @@ def fit(self, X, y=None):
         self.bin_edges_ = bin_edges
         self.n_bins_ = n_bins
 
+        if self.encode != 'ordinal':
+            encode_sparse = self.encode == 'onehot'
+            self.ohe_encoder_ = OneHotEncoder(
+                categories=[np.arange(i) for i in self.n_bins_],
+                sparse=encode_sparse)
+
         return self
 
     def _validate_n_bins(self, n_features):
@@ -262,9 +268,6 @@ def transform(self, X):
         if self.encode == 'ordinal':
             return Xt
 
-        encode_sparse = self.encode == 'onehot'
-        self.ohe_encoder_ = OneHotEncoder(categories=[np.arange(i) for i in self.n_bins_],
-                                      sparse=encode_sparse)
         return self.ohe_encoder_.fit_transform(Xt)
 
     def inverse_transform(self, Xt):
diff --git a/sklearn/utils/_logistic_sigmoid.pyx b/sklearn/utils/_logistic_sigmoid.pyx
index 58809eb7c1..9c7b8d0a20 100755
--- a/sklearn/utils/_logistic_sigmoid.pyx
+++ b/sklearn/utils/_logistic_sigmoid.pyx
@@ -13,9 +13,9 @@ ctypedef np.float64_t DTYPE_t
 cdef DTYPE_t _inner_log_logistic_sigmoid(DTYPE_t x):
     """Log of the logistic sigmoid function log(1 / (1 + e ** -x))"""
     if x > 0:
-        return -log(1 + exp(-x))
+        return -np.log1p(exp(-x))
     else:
-        return x - log(1 + exp(x))
+        return x - np.log1p(exp(x))
 
 
 def _log_logistic_sigmoid(int n_samples, int n_features, 
